file_path,api_count,code
start_notebook.py,0,"b'#!/usr/bin/env python\nimport os\n\nos.system(""jupyter notebook --ip=\'0.0.0.0\'"")'"
config/download_pdc_data.py,0,"b'import os\nimport sys\nimport yaml\n\nprint sys.argv\n\nif len(sys.argv) < 2:\n    print ""expected arg to specify which composite dataset .yaml to download data for""\n    print ""for example:""\n    print ""    python download_pdc_data.py /path/to/pytorch-dense-correspondence/config/dense_correspondence/dataset/composite/caterpillar_upright.yaml""\n    quit()\n\n\nlogs = []\ndatasets_to_download_config = yaml.load(file(sys.argv[1]))\npdc_root_dir = os.getcwd()\n\n\nif len(sys.argv) >= 3:\n    data_dir = sys.argv[2]\nelse:\n    data_dir = pdc_root_dir\n\npdc_data_dir = os.path.join(data_dir, ""pdc"")\nprint ""data_dir"", data_dir\nprint ""pdc_data_dir"", pdc_data_dir\n\ndef add_datset(path):\n    global logs\n    single_object_dataset = yaml.load(file(path))\n    for j in single_object_dataset[""train""]:\n        if j not in logs:\n            logs.append(j)\n    for j in single_object_dataset[""test""]:\n        if j not in logs:\n            logs.append(j)\n\nfor i in datasets_to_download_config[\'single_object_scenes_config_files\']:\n    path = os.path.join(pdc_root_dir, ""config/dense_correspondence/dataset/single_object"", i)\n    add_datset(path)\n\nfor i in datasets_to_download_config[\'multi_object_scenes_config_files\']:\n    path = os.path.join(pdc_root_dir, ""config/dense_correspondence/dataset/multi_object"", i)\n    add_datset(path)\n\n\n\nif not os.path.isdir(pdc_data_dir):\n    os.makedirs(pdc_data_dir)\n\n\ndownload_url_base = ""https://data.csail.mit.edu/labelfusion/pdccompressed/""\n\n\nif not os.path.isdir(os.path.join(pdc_data_dir, ""evaluation_labeled_data"")):\n    print ""downloading labeled data""\n    os.chdir(data_dir)\n    # note the evaluation_labeled_data_compressed.tar.gz"" actually has the form\n    # pdc/evaluation_labeled_data so you need to be careful when unpacking it\n    os.system(""wget -q ""+download_url_base+""evaluation_labeled_data_compressed.tar.gz"")\n    evaluation_data_compressed_path = os.path.join(data_dir, ""evaluation_labeled_data_compressed.tar.gz"")\n    os.system(""tar -zxf "" + evaluation_data_compressed_path)\n    os.system(""rm "" + evaluation_data_compressed_path)\nelse:\n    print ""already have labeled data -- skipping ...""\n\n\nprint ""Will download these logs if do not already have:""\nprint logs\n\nlogs_proto_dir = os.path.join(pdc_data_dir, ""logs_proto"")\nif not os.path.isdir(logs_proto_dir):\n    os.makedirs(logs_proto_dir)\n\n## uncompress each log\nfor i, log in enumerate(logs):\n    os.chdir(pdc_data_dir)\n    log_path = log+"".tar.gz""\n    log_path = os.path.join(logs_proto_dir, log)\n    log_compressed_path = os.path.join(logs_proto_dir, log+"".tar.gz"")\n    if os.path.isdir(os.path.join(logs_proto_dir, log)):\n        print ""already have log"",log,"" -- skipping ...""\n        continue\n    print ""downloading   log"", i+1, ""of"", len(logs)\n    print ""log_compressed_path"", log_compressed_path\n    os.system(""rm -rf "" + log_compressed_path)\n    \n    # change to logs_proto dir\n    os.chdir(logs_proto_dir)\n\n    # download the compressed log\n    os.system(""wget "" + download_url_base+""logs_proto/""+log + "".tar.gz"")\n\n    # extract the log\n    os.chdir(data_dir)\n    os.system(""tar -zvxf"" + log_compressed_path)\n    os.system(""rm -rf "" + log_compressed_path)\n\nprint ""done""\n'"
dense_correspondence/__init__.py,0,b''
docker/docker_build.py,0,"b'#!/usr/bin/env python\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport getpass\n\nif __name__==""__main__"":\n\n\tprint(""building docker container . . . "")\n\tuser_name = getpass.getuser()\n\tdefault_image_name = user_name + ""-pytorch-dense-correspondence""\n\n\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(""-i"", ""--image"", type=str,\n\t\thelp=""name for the newly created docker image"", default=default_image_name)\n\n\tparser.add_argument(""-d"", ""--dry_run"", action=\'store_true\', help=""(optional) perform a dry_run, print the command that would have been executed but don\'t execute it."")\n\n\tparser.add_argument(""-pw"", ""--password"", type=str,\n                        help=""(optional) password for the user"", default=""password"")\n\n\tparser.add_argument(\'-uid\',\'--user_id\', type=int, help=""(optional) user id for this user"", default=os.getuid())\n\tparser.add_argument(\'-gid\',\'--group_id\', type=int, help=""(optional) user gid for this user"", default=os.getgid())\n\n\tparser.add_argument(\'-p\', ""--passthrough"", type=str, help=""(optional) passthrough arguments to add to the docker build"")\n\n\targs = parser.parse_args()\n\tprint(""building docker image named "", args.image)\n\tcmd = ""docker build --build-arg USER_NAME=%(user_name)s \\\n\t\t\t--build-arg USER_PASSWORD=%(password)s \\\n\t\t\t--build-arg USER_ID=%(user_id)s \\\n\t\t\t--build-arg USER_GID=%(group_id)s"" \\\n\t\t\t%{\'user_name\': user_name, \'password\': args.password, \'user_id\': args.user_id, \'group_id\': args.group_id}\n\n\tif args.passthrough:\n\t\tcmd += "" "" + args.passthrough\n\n\tcmd += "" -t %s -f pytorch-dense-correspondence.dockerfile ."" % args.image\n\t\n\n\tprint(""command = \\n \\n"", cmd)\n\tprint("""")\n\n\t# build the docker image\n\tif not args.dry_run:\n\t\tprint(""executing shell command"")\n\t\tos.system(cmd)\n\telse:\n\t\tprint(""dry run, not executing command"")'"
docker/docker_run.py,0,"b'#!/usr/bin/env python\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport socket\nimport getpass\nimport yaml\n\nif __name__==""__main__"":\n    user_name = getpass.getuser()\n    default_image_name = user_name + \'-pytorch-dense-correspondence\'\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""-i"", ""--image"", type=str,\n        help=""(required) name of the image that this container is derived from"", default=default_image_name)\n\n    parser.add_argument(""-c"", ""--container"", type=str, default=""pytorch-container"", help=""(optional) name of the container"")\\\n\n    parser.add_argument(""-d"", ""--dry_run"", action=\'store_true\', help=""(optional) perform a dry_run, print the command that would have been executed but don\'t execute it."")\n\n    parser.add_argument(""-e"", ""--entrypoint"", type=str, default="""", help=""(optional) thing to run in container"")\n\n    parser.add_argument(""-p"", ""--passthrough"", type=str, default="""", help=""(optional) extra string that will be tacked onto the docker run command, allows you to pass extra options. Make sure to put this in quotes and leave a space before the first character"")\n\n    args = parser.parse_args()\n    print(""running docker container derived from image %s"" %args.image)\n    source_dir=os.path.join(os.getcwd(),""../"")\n    config_file = os.path.join(source_dir, \'config\', \'docker_run_config.yaml\')\n\n    print(source_dir)\n\n    image_name = args.image\n    home_directory = \'/home/\' + user_name\n    dense_correspondence_source_dir = os.path.join(home_directory, \'code\')\n\n    cmd = ""xhost +local:root \\n""\n    cmd += ""nvidia-docker run ""\n    if args.container:\n        cmd += "" --name %(container_name)s "" % {\'container_name\': args.container}\n\n    cmd += "" -e DISPLAY -e QT_X11_NO_MITSHM=1 -v /tmp/.X11-unix:/tmp/.X11-unix:rw ""     # enable graphics \n    cmd += "" -v %(source_dir)s:%(home_directory)s/code ""  \\\n        % {\'source_dir\': source_dir, \'home_directory\': home_directory}              # mount source\n    cmd += "" -v ~/.ssh:%(home_directory)s/.ssh "" % {\'home_directory\': home_directory}   # mount ssh keys\n    cmd += "" -v /media:/media "" #mount media\n    cmd += "" -v ~/.torch:%(home_directory)s/.torch "" % {\'home_directory\': home_directory}  # mount torch folder \n                                                        # where pytorch standard models (i.e. resnet34) are stored\n\n    cmd += "" --user %s "" % user_name                                                    # login as current user\n\n    # uncomment below to mount your data volume\n    config_yaml = yaml.load(file(config_file))\n    host_name = socket.gethostname()\n    cmd += "" -v %s:%s/data "" %(config_yaml[host_name][user_name][\'path_to_data_directory\'], home_directory)\n\n    # expose UDP ports\n    cmd += "" -p 8888:8888 ""\n    cmd += "" --ipc=host ""\n\n    # share host machine network\n    cmd += "" --network=host ""\n\n    cmd += "" "" + args.passthrough + "" ""\n\n    cmd += "" --privileged -v /dev/bus/usb:/dev/bus/usb "" # allow usb access\n\n    cmd += "" --rm "" # remove the image when you exit\n\n\n\n\n    if args.entrypoint and args.entrypoint != """":\n        cmd += ""--entrypoint=\\""%(entrypoint)s\\"" "" % {""entrypoint"": args.entrypoint}\n    else:\n        cmd += ""-it ""\n    cmd += args.image\n    cmd_endxhost = ""xhost -local:root""\n\n    print(""command = \\n \\n"", cmd, ""\\n"", cmd_endxhost)\n    print("""")\n\n    # build the docker image\n    if not args.dry_run:\n        print(""executing shell command"")\n        code = os.system(cmd)\n        print(""Executed with code "", code)\n        os.system(cmd_endxhost)\n        # Squash return code to 0/1, as\n        # Docker\'s very large return codes\n        # were tricking Jenkins\' failure\n        # detection\n        exit(code != 0)\n    else:\n        print(""dry run, not executing command"")\n        exit(0)\n'"
modules/__init__.py,0,b''
dense_correspondence/correspondence_tools/__init__.py,0,b''
dense_correspondence/correspondence_tools/correspondence_augmentation.py,9,"b'""""""\n\nThe purpose of this file is to perform data augmentation for images\nand lists of pixel positions in them.\n\n- For operations on the images, we can use functions optimized \nfor image data.\n\n- For operations on a list of pixel indices, we need a matching\nimplementation.\n\n""""""\n\nfrom PIL import Image, ImageOps\nimport numpy as np\nimport random\nimport torch\n\ndef random_image_and_indices_mutation(images, uv_pixel_positions):\n    """"""\n    This function takes a list of images and a list of pixel positions in the image, \n    and picks some subset of available mutations.\n\n    :param images: a list of images (for example the rgb, depth, and mask) for which the \n                        **same** mutation will be applied\n    :type  images: list of PIL.image.image\n\n    :param uv_pixel_positions: pixel locations (u, v) in the image. \n    \tSee doc/coordinate_conventions.md for definition of (u, v)\n\n    :type  uv_pixel_positions: a tuple of torch Tensors, each of length n, i.e:\n\n    \t(u_pixel_positions, v_pixel_positions)\n\n    \tWhere each of the elements of the tuple are torch Tensors of length n\n\n    \tNote: aim is to support both torch.LongTensor and torch.FloatTensor,\n    \t      and return the mutated_uv_pixel_positions with same type\n\n    :return mutated_image_list, mutated_uv_pixel_positions\n    \t:rtype: list of PIL.image.image, tuple of torch Tensors\n\n    """"""\n\n    # Current augmentation is:\n    # 50% do nothing\n    # 50% rotate the image 180 degrees (by applying flip vertical then flip horizontal) \n\n    if random.random() < 0.5:\n        return images, uv_pixel_positions\n\n    else:\n        mutated_images, mutated_uv_pixel_positions = flip_vertical(images, uv_pixel_positions)\n        mutated_images, mutated_uv_pixel_positions = flip_horizontal(mutated_images, mutated_uv_pixel_positions)\n\n        return mutated_images, mutated_uv_pixel_positions\n\n\ndef flip_vertical(images, uv_pixel_positions):\n    """"""\n    Fip the images and the pixel positions vertically (flip up/down)\n\n    See random_image_and_indices_mutation() for documentation of args and return types.\n\n    """"""\n    mutated_images = [ImageOps.flip(image) for image in images]\n    v_pixel_positions = uv_pixel_positions[1]\n    mutated_v_pixel_positions = (image.height-1) - v_pixel_positions\n    mutated_uv_pixel_positions = (uv_pixel_positions[0], mutated_v_pixel_positions)\n    return mutated_images, mutated_uv_pixel_positions\n\ndef flip_horizontal(images, uv_pixel_positions):\n    """"""\n    Randomly flip the image and the pixel positions horizontall (flip left/right)\n\n    See random_image_and_indices_mutation() for documentation of args and return types.\n\n    """"""\n\n    mutated_images = [ImageOps.mirror(image) for image in images]\n    u_pixel_positions = uv_pixel_positions[0]\n    mutated_u_pixel_positions = (image.width-1) - u_pixel_positions\n    mutated_uv_pixel_positions = (mutated_u_pixel_positions, uv_pixel_positions[1])\n    return mutated_images, mutated_uv_pixel_positions\n\ndef random_domain_randomize_background(image_rgb, image_mask):\n    """"""\n    Ranomly call domain_randomize_background\n    """"""\n    if random.random() < 0.5:\n        return image_rgb\n    else:\n        return domain_randomize_background(image_rgb, image_mask)\n\n\ndef domain_randomize_background(image_rgb, image_mask):\n    """"""\n    This function applies domain randomization to the non-masked part of the image.\n\n    :param image_rgb: rgb image for which the non-masked parts of the image will \n                        be domain randomized\n    :type  image_rgb: PIL.image.image\n\n    :param image_mask: mask of part of image to be left alone, all else will be domain randomized\n    :type image_mask: PIL.image.image\n\n    :return domain_randomized_image_rgb:\n    :rtype: PIL.image.image\n    """"""\n    # First, mask the rgb image\n    image_rgb_numpy = np.asarray(image_rgb)\n    image_mask_numpy = np.asarray(image_mask)\n    three_channel_mask = np.zeros_like(image_rgb_numpy)\n    three_channel_mask[:,:,0] = three_channel_mask[:,:,1] = three_channel_mask[:,:,2] = image_mask\n    image_rgb_numpy = image_rgb_numpy * three_channel_mask\n\n    # Next, domain randomize all non-masked parts of image\n    three_channel_mask_complement = np.ones_like(three_channel_mask) - three_channel_mask\n    random_rgb_image = get_random_image(image_rgb_numpy.shape)\n    random_rgb_background = three_channel_mask_complement * random_rgb_image\n\n    domain_randomized_image_rgb = image_rgb_numpy + random_rgb_background\n    return Image.fromarray(domain_randomized_image_rgb)\n\ndef get_random_image(shape):\n    """"""\n    Expects something like shape=(480,640,3)\n\n    :param shape: tuple of shape for numpy array, for example from my_array.shape\n    :type shape: tuple of ints\n\n    :return random_image:\n    :rtype: np.ndarray\n    """"""\n    if random.random() < 0.5:\n        rand_image = get_random_solid_color_image(shape)\n    else:\n        rgb1 = get_random_solid_color_image(shape)\n        rgb2 = get_random_solid_color_image(shape)\n        vertical = bool(np.random.uniform() > 0.5)\n        rand_image = get_gradient_image(rgb1, rgb2, vertical=vertical)\n\n    if random.random() < 0.5:\n        return rand_image\n    else:\n        return add_noise(rand_image)\n\ndef get_random_rgb():\n    """"""\n    :return random rgb colors, each in range 0 to 255, for example [13, 25, 255]\n    :rtype: numpy array with dtype=np.uint8\n    """"""\n    return np.array(np.random.uniform(size=3) * 255, dtype=np.uint8)\n\ndef get_random_solid_color_image(shape):\n    """"""\n    Expects something like shape=(480,640,3)\n\n    :return random solid color image:\n    :rtype: numpy array of specificed shape, with dtype=np.uint8\n    """"""\n    return np.ones(shape,dtype=np.uint8)*get_random_rgb()\n\ndef get_random_entire_image(shape, max_pixel_uint8):\n    """"""\n    Expects something like shape=(480,640,3)\n\n    Returns an array of that shape, with values in range [0..max_pixel_uint8)\n\n    :param max_pixel_uint8: maximum value in the image\n    :type max_pixel_uint8: int\n\n    :return random solid color image:\n    :rtype: numpy array of specificed shape, with dtype=np.uint8\n    """"""\n    return np.array(np.random.uniform(size=shape) * max_pixel_uint8, dtype=np.uint8)\n\n# this gradient code roughly taken from: \n# https://github.com/openai/mujoco-py/blob/master/mujoco_py/modder.py\ndef get_gradient_image(rgb1, rgb2, vertical):\n    """"""\n    Interpolates between two images rgb1 and rgb2\n\n    :param rgb1, rgb2: two numpy arrays of shape (H,W,3)\n\n    :return interpolated image:\n    :rtype: same as rgb1 and rgb2\n    """"""\n    bitmap = np.zeros_like(rgb1)\n    h, w = rgb1.shape[0], rgb1.shape[1]\n    if vertical:\n        p = np.tile(np.linspace(0, 1, h)[:, None], (1, w))\n    else:\n        p = np.tile(np.linspace(0, 1, w), (h, 1))\n\n    for i in range(3):\n        bitmap[:, :, i] = rgb2[:, :, i] * p + rgb1[:, :, i] * (1.0 - p)\n\n    return bitmap\n\ndef add_noise(rgb_image):\n    """"""\n    Adds noise, and subtracts noise to the rgb_image\n\n    :param rgb_image: image to which noise will be added \n    :type rgb_image: numpy array of shape (H,W,3)\n\n    :return image with noise:\n    :rtype: same as rgb_image\n\n    ## Note: do not need to clamp, since uint8 will just overflow -- not bad\n    """"""\n    max_noise_to_add_or_subtract = 50\n    return rgb_image + get_random_entire_image(rgb_image.shape, max_noise_to_add_or_subtract) - get_random_entire_image(rgb_image.shape, max_noise_to_add_or_subtract) \n\n\ndef merge_images_with_occlusions(image_a, image_b, mask_a, mask_b, matches_pair_a, matches_pair_b):\n    """"""\n    This function will take image_a and image_b and ""merge"" them.\n\n    It will do this by:\n    - randomly selecting either image_a or image_b to be the background\n    - using the mask for the image that is not the background, it will put the other image on top.\n    - critically there are two separate sets of matches, one is associated with image_a and some other image,\n        and the other is associated with image_b and some other image.\n    - both of these sets of matches must be pruned for any occlusions that occur.\n\n    :param image_a, image_b: the two images to merge\n    :type image_a, image_b: each a PIL.image.image\n    :param mask_a, mask_b: the masks for these images\n    :type mask_a, mask_b: each a PIL.image.image\n    :param matches_a, matches_b:\n    :type matches_a, mathces_b: each a tuple of torch Tensors, each of length n, i.e:\n\n        (u_pixel_positions, v_pixel_positions)\n\n        Where each of the elements of the tuple are torch Tensors of length n\n\n        Note: only support torch.LongTensors\n\n    :return: merged image, merged_mask, pruned_matches_a, pruned_associated_matches_a, pruned_matches_b, pruned_associated_matches_b\n    :rtype: PIL.image.image, numpy array, rest are same types as matches_a and matches_b\n\n    """"""\n\n    if random.random() < 0.5:\n        foreground = ""B""\n        background_image, background_mask, background_matches_pair = image_a, mask_a, matches_pair_a\n        foreground_image, foreground_mask, foreground_matches_pair = image_b, mask_b, matches_pair_b\n    else:\n        foreground = ""A""\n        background_image, background_mask, background_matches_pair = image_b, mask_b, matches_pair_b\n        foreground_image, foreground_mask, foreground_matches_pair = image_a, mask_a, matches_pair_a\n\n    # First, mask the foreground rgb image\n    foreground_image_numpy = np.asarray(foreground_image)\n    foreground_mask_numpy  = np.asarray(foreground_mask)\n    three_channel_mask = np.zeros_like(foreground_image_numpy)\n    three_channel_mask[:,:,0] = three_channel_mask[:,:,1] = three_channel_mask[:,:,2] = foreground_mask\n    foreground_image_numpy = foreground_image_numpy * three_channel_mask\n\n    # Next, zero out this portion in the background image\n    background_image_numpy = np.asarray(background_image)\n    three_channel_mask_complement = np.ones_like(three_channel_mask) - three_channel_mask\n    background_image_numpy = three_channel_mask_complement * background_image_numpy\n\n    # Finally, merge these two images\n    merged_image_numpy = foreground_image_numpy + background_image_numpy\n\n    # Prune occluded matches\n    background_matches_pair = prune_matches_if_occluded(foreground_mask_numpy, background_matches_pair)\n \n    if foreground == ""A"":\n        matches_a            = foreground_matches_pair[0]\n        associated_matches_a = foreground_matches_pair[1]\n        matches_b            = background_matches_pair[0]\n        associated_matches_b = background_matches_pair[1]\n    elif foreground == ""B"":\n        matches_a            = background_matches_pair[0]\n        associated_matches_a = background_matches_pair[1]\n        matches_b            = foreground_matches_pair[0]\n        associated_matches_b = foreground_matches_pair[1]\n    else:\n        raise ValueError(""Should not be here?"")\n\n    merged_masked_numpy = foreground_mask_numpy + np.asarray(background_mask)\n    merged_masked_numpy = merged_masked_numpy.clip(0,1) # in future, could preserve identities of masks\n    return Image.fromarray(merged_image_numpy), merged_masked_numpy, matches_a, associated_matches_a, matches_b, associated_matches_b\n\n\ndef prune_matches_if_occluded(foreground_mask_numpy, background_matches_pair):\n    """"""\n    Checks if any of the matches have been occluded.\n\n    If yes, prunes them from the list of matches.\n\n    NOTE:\n    - background_matches is a tuple\n    - the first element of the tuple HAS to be the one that we are actually checking for occlusions\n    - the second element of the tuple must also get pruned\n\n    :param foreground_mask_numpy: The mask of the foreground image\n    :type foreground_mask_numpy: numpy 2d array of shape (H,W)\n    :param background_matches: a tuple of torch Tensors, each of length n, i.e:\n\n        (u_pixel_positions, v_pixel_positions)\n\n        Where each of the elements of the tuple are torch Tensors of length n\n\n        Note: only support torch.LongTensors\n    """"""\n\n    background_matches_a = background_matches_pair[0] \n    background_matches_b = background_matches_pair[1]\n\n    idxs_to_keep  = []\n    \n    # this is slow but works\n    for i in range(len(background_matches_a[0])):\n        u = background_matches_a[0][i]\n        v = background_matches_a[1][i]\n\n        if foreground_mask_numpy[v,u] == 0:\n            idxs_to_keep.append(i)\n\n    if len(idxs_to_keep) == 0:\n        return (None, None)\n\n    idxs_to_keep = torch.LongTensor(idxs_to_keep)\n    background_matches_a = (torch.index_select(background_matches_a[0], 0, idxs_to_keep), torch.index_select(background_matches_a[1], 0, idxs_to_keep))\n    background_matches_b = (torch.index_select(background_matches_b[0], 0, idxs_to_keep), torch.index_select(background_matches_b[1], 0, idxs_to_keep))\n\n    return (background_matches_a, background_matches_b)\n\ndef merge_matches(matches_one, matches_two):\n    """"""\n    :param matches_one, matches_two: each a tuple of torch Tensors, each of length n, i.e:\n\n        (u_pixel_positions, v_pixel_positions)\n\n        Where each of the elements of the tuple are torch Tensors of length n\n\n        Note: only support torch.LongTensors\n    """"""\n    concatenated_u = torch.cat((matches_one[0], matches_two[0]))\n    concatenated_v = torch.cat((matches_one[1], matches_two[1]))\n    return (concatenated_u, concatenated_v)\n\n\n\n\n\n\n    \n\n'"
dense_correspondence/correspondence_tools/correspondence_finder.py,98,"b'# torch\nimport torch\n\n# system\nimport numpy as numpy\nimport numpy as np\nfrom numpy.linalg import inv\nimport random\nimport warnings\n\n# io\nfrom PIL import Image\n\n# torchvision\nimport sys\nsys.path.insert(0, \'../pytorch-segmentation-detection/vision/\') # from subrepo\nfrom torchvision import transforms\n\n\nfrom dense_correspondence_manipulation.utils.constants import *\n\n# turns out to be faster to do this match generation on the CPU\n# for the general size of params we expect\n# also this will help by not taking up GPU memory, \n# allowing batch sizes to stay large\ndtype_float = torch.FloatTensor\ndtype_long = torch.LongTensor\n\ndef pytorch_rand_select_pixel(width,height,num_samples=1):\n    two_rand_numbers = torch.rand(2,num_samples)\n    two_rand_numbers[0,:] = two_rand_numbers[0,:]*width\n    two_rand_numbers[1,:] = two_rand_numbers[1,:]*height\n    two_rand_ints    = torch.floor(two_rand_numbers).type(dtype_long)\n    return (two_rand_ints[0], two_rand_ints[1])\n\ndef get_default_K_matrix():\n    K = numpy.zeros((3,3))\n    K[0,0] = 533.6422696034836 # focal x\n    K[1,1] = 534.7824445233571 # focal y\n    K[0,2] = 319.4091030774892 # principal point x\n    K[1,2] = 236.4374299691866 # principal point y\n    K[2,2] = 1.0\n    return K\n\ndef get_body_to_rdf():\n    body_to_rdf = numpy.zeros((3,3))\n    body_to_rdf[0,1] = -1.0\n    body_to_rdf[1,2] = -1.0\n    body_to_rdf[2,0] = 1.0\n    return body_to_rdf\n\ndef invert_transform(transform4):\n    transform4_copy = numpy.copy(transform4)\n    R = transform4_copy[0:3,0:3]\n    R = numpy.transpose(R)\n    transform4_copy[0:3,0:3] = R\n    t = transform4_copy[0:3,3]\n    inv_t = -1.0 * numpy.transpose(R).dot(t)\n    transform4_copy[0:3,3] = inv_t\n    return transform4_copy\n\ndef apply_transform_torch(vec3, transform4):\n    ones_row = torch.ones_like(vec3[0,:]).type(dtype_float).unsqueeze(0)\n    vec4 = torch.cat((vec3,ones_row),0)\n    vec4 = transform4.mm(vec4)\n    return vec4[0:3]\n\ndef random_sample_from_masked_image(img_mask, num_samples):\n    """"""\n    Samples num_samples (row, column) convention pixel locations from the masked image\n    Note this is not in (u,v) format, but in same format as img_mask\n    :param img_mask: numpy.ndarray\n        - masked image, we will select from the non-zero entries\n        - shape is H x W\n    :param num_samples: int\n        - number of random indices to return\n    :return: List of np.array\n    """"""\n    idx_tuple = img_mask.nonzero()\n    num_nonzero = len(idx_tuple[0])\n    if num_nonzero == 0:\n        empty_list = []\n        return empty_list\n    rand_inds = random.sample(range(0,num_nonzero), num_samples)\n\n    sampled_idx_list = []\n    for i, idx in enumerate(idx_tuple):\n        sampled_idx_list.append(idx[rand_inds])\n\n    return sampled_idx_list\n\ndef random_sample_from_masked_image_torch(img_mask, num_samples):\n    """"""\n\n    :param img_mask: Numpy array [H,W] or torch.Tensor with shape [H,W]\n    :type img_mask:\n    :param num_samples: an integer\n    :type num_samples:\n    :return: tuple of torch.LongTensor in (u,v) format. Each torch.LongTensor has shape\n    [num_samples]\n    :rtype:\n    """"""\n\n    image_height, image_width = img_mask.shape\n\n    if isinstance(img_mask, np.ndarray):\n        img_mask_torch = torch.from_numpy(img_mask).float()\n    else:\n        img_mask_torch = img_mask\n\n    # This code would randomly subsample from the mask\n    mask = img_mask_torch.view(image_width*image_height,1).squeeze(1)\n    mask_indices_flat = torch.nonzero(mask)\n    if len(mask_indices_flat) == 0:\n        return (None, None)\n\n    rand_numbers = torch.rand(num_samples)*len(mask_indices_flat)\n    rand_indices = torch.floor(rand_numbers).long()\n    uv_vec_flattened = torch.index_select(mask_indices_flat, 0, rand_indices).squeeze(1)\n    uv_vec = utils.flattened_pixel_locations_to_u_v(uv_vec_flattened, image_width)\n    return uv_vec\n\ndef pinhole_projection_image_to_world(uv, z, K):\n    """"""\n    Takes a (u,v) pixel location to it\'s 3D location in camera frame.\n    See https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html for a detailed explanation.\n\n    :param uv: pixel location in image\n    :type uv:\n    :param z: depth, in camera frame\n    :type z: float\n    :param K: 3 x 3 camera intrinsics matrix\n    :type K: numpy.ndarray\n    :return: (x,y,z) in camera frame\n    :rtype: numpy.array size (3,)\n    """"""\n\n    warnings.warn(""Potentially incorrect implementation"", category=DeprecationWarning)\n\n\n    u_v_1 = np.array([uv[0], uv[1], 1])\n    K_inv = inv(K)\n    pos = z * K_inv.dot(u_v_1)\n    return pos\n\n\ndef pinhole_projection_image_to_camera_coordinates(uv, z, K):\n    """"""\n    Takes a (u,v) pixel location to it\'s 3D location in camera frame.\n    See https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html for a detailed explanation.\n\n    :param uv: pixel location in image\n    :type uv:\n    :param z: depth, in camera frame\n    :type z: float\n    :param K: 3 x 3 camera intrinsics matrix\n    :type K: numpy.ndarray\n    :return: (x,y,z) in camera frame\n    :rtype: numpy.array size (3,)\n    """"""\n\n    u_v_1 = np.array([uv[0], uv[1], 1])\n    K_inv = inv(K)\n\n    pos = z * K_inv.dot(u_v_1)\n    return pos\n\ndef pinhole_projection_image_to_camera_coordinates_vectorized(uv, z, K):\n    """"""\n    Same as pinhole_projection_image_to_camera_coordinates but where\n    uv, z can be vectors\n\n    Takes a (u,v) pixel location to it\'s 3D location in camera frame.\n    See https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html for a detailed explanation.\n\n    N = z.size, number of pixels\n\n    :param uv:\n    :type uv: list of numpy.arrays each of shape [N,]\n    :param z: depth value\n    :type z: np.array of shape [N,]. This assumes that z > 0\n    :param K: 3 x 3 camera intrinsics matrix\n    :type K:\n    :return:\n    :rtype:\n    """"""\n    N = z.size\n    uv_homog = np.zeros([3, N])\n    uv_homog[0,:] = uv[0]\n    uv_homog[1,:] = uv[1]\n    uv_homog[2,:] = np.ones(N)\n\n\n    K_inv = inv(K)\n    pos = z * K_inv.dot(uv_homog) # 3 x N\n\n    return np.transpose(pos)\n\n\n\ndef pinhole_projection_image_to_world_coordinates(uv, z, K, camera_to_world):\n    """"""\n    Takes a (u,v) pixel location to it\'s 3D location in camera frame.\n    See https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html for a detailed explanation.\n\n    :param uv: pixel location in image\n    :type uv:\n    :param z: depth, in camera frame\n    :type z: float\n    :param K: 3 x 3 camera intrinsics matrix\n    :type K: numpy.ndarray\n    :param camera_to_world: 4 x 4 homogeneous transform\n    :type camera_to_world: numpy array\n    :return: (x,y,z) in world\n    :rtype: numpy.array size (3,)\n    """"""\n\n    pos_in_camera_frame = pinhole_projection_image_to_camera_coordinates(uv, z, K)\n    pos_in_camera_frame_homog = np.append(pos_in_camera_frame, 1)\n    pos_in_world_homog = camera_to_world.dot(pos_in_camera_frame_homog)\n    return pos_in_world_homog[:3]\n\n\n\ndef pinhole_projection_world_to_image(world_pos, K, camera_to_world=None):\n    """"""\n    Projects from world position to camera coordinates\n    See https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html\n    :param world_pos:\n    :type world_pos:\n    :param K:\n    :type K:\n    :return:\n    :rtype:\n    """"""\n\n\n\n    world_pos_vec = np.append(world_pos, 1)\n\n    # transform to camera frame if camera_to_world is not None\n    if camera_to_world is not None:\n        world_pos_vec = np.dot(np.linalg.inv(camera_to_world), world_pos_vec)\n\n    # scaled position is [X/Z, Y/Z, 1] where X,Y,Z is the position in camera frame\n    scaled_pos = np.array([world_pos_vec[0]/world_pos_vec[2], world_pos_vec[1]/world_pos_vec[2], 1])\n    uv = np.dot(K, scaled_pos)[:2]\n    return uv\n\n\n\n# in torch 0.3 we don\'t yet have torch.where(), although this\n# is there in 0.4 (not yet stable release)\n# for more see: https://discuss.pytorch.org/t/how-can-i-do-the-operation-the-same-as-np-where/1329/8\ndef where(cond, x_1, x_2):\n    """"""\n    We follow the torch.where implemented in 0.4.\n    See http://pytorch.org/docs/master/torch.html?highlight=where#torch.where\n\n    For more discussion see https://discuss.pytorch.org/t/how-can-i-do-the-operation-the-same-as-np-where/1329/8\n\n\n    Return a tensor of elements selected from either x_1 or x_2, depending on condition.\n    :param cond: cond should be tensor with entries [0,1]\n    :type cond:\n    :param x_1: torch.Tensor\n    :type x_1:\n    :param x_2: torch.Tensor\n    :type x_2:\n    :return:\n    :rtype:\n    """"""\n    cond = cond.type(dtype_float)    \n    return (cond * x_1) + ((1-cond) * x_2)\n\ndef create_non_correspondences(uv_b_matches, img_b_shape, num_non_matches_per_match=100, img_b_mask=None):\n    """"""\n    Takes in pixel matches (uv_b_matches) that correspond to matches in another image, and generates non-matches by just sampling in image space.\n\n    Optionally, the non-matches can be sampled from a mask for image b.\n\n    Returns non-matches as pixel positions in image b.\n\n    Please see \'coordinate_conventions.md\' documentation for an explanation of pixel coordinate conventions.\n\n    ## Note that arg uv_b_matches are the outputs of batch_find_pixel_correspondences()\n\n    :param uv_b_matches: tuple of torch.FloatTensors, where each FloatTensor is length n, i.e.:\n        (torch.FloatTensor, torch.FloatTensor)\n\n    :param img_b_shape: tuple of (H,W) which is the shape of the image\n\n    (optional)\n    :param num_non_matches_per_match: int\n\n    (optional)\n    :param img_b_mask: torch.FloatTensor (can be cuda or not)\n        - masked image, we will select from the non-zero entries\n        - shape is H x W\n     \n    :return: tuple of torch.FloatTensors, i.e. (torch.FloatTensor, torch.FloatTensor).\n        - The first element of the tuple is all ""u"" pixel positions, and the right element of the tuple is all ""v"" positions\n        - Each torch.FloatTensor is of shape torch.Shape([num_matches, non_matches_per_match])\n        - This shape makes it so that each row of the non-matches corresponds to the row for the match in uv_a\n    """"""\n    image_width  = img_b_shape[1]\n    image_height = img_b_shape[0]\n\n    if uv_b_matches == None:\n        return None\n\n    num_matches = len(uv_b_matches[0])\n\n    def get_random_uv_b_non_matches():\n        return pytorch_rand_select_pixel(width=image_width,height=image_height, \n            num_samples=num_matches*num_non_matches_per_match)\n\n    if img_b_mask is not None:\n        img_b_mask_flat = img_b_mask.view(-1,1).squeeze(1)\n        mask_b_indices_flat = torch.nonzero(img_b_mask_flat)\n        if len(mask_b_indices_flat) == 0:\n            print ""warning, empty mask b""\n            uv_b_non_matches = get_random_uv_b_non_matches()\n        else:\n            num_samples = num_matches*num_non_matches_per_match\n            rand_numbers_b = torch.rand(num_samples)*len(mask_b_indices_flat)\n            rand_indices_b = torch.floor(rand_numbers_b).long()\n            randomized_mask_b_indices_flat = torch.index_select(mask_b_indices_flat, 0, rand_indices_b).squeeze(1)\n            uv_b_non_matches = (randomized_mask_b_indices_flat%image_width, randomized_mask_b_indices_flat/image_width)\n    else:\n        uv_b_non_matches = get_random_uv_b_non_matches()\n    \n    # for each in uv_a, we want non-matches\n    # first just randomly sample ""non_matches""\n    # we will later move random samples that were too close to being matches\n    uv_b_non_matches = (uv_b_non_matches[0].view(num_matches,num_non_matches_per_match), uv_b_non_matches[1].view(num_matches,num_non_matches_per_match))\n\n    # uv_b_matches can now be used to make sure no ""non_matches"" are too close\n    # to preserve tensor size, rather than pruning, we can perturb these in pixel space\n    copied_uv_b_matches_0 = torch.t(uv_b_matches[0].repeat(num_non_matches_per_match, 1))\n    copied_uv_b_matches_1 = torch.t(uv_b_matches[1].repeat(num_non_matches_per_match, 1))\n\n    diffs_0 = copied_uv_b_matches_0 - uv_b_non_matches[0].type(dtype_float)\n    diffs_1 = copied_uv_b_matches_1 - uv_b_non_matches[1].type(dtype_float)\n\n    diffs_0_flattened = diffs_0.view(-1,1)\n    diffs_1_flattened = diffs_1.view(-1,1)\n\n    diffs_0_flattened = torch.abs(diffs_0_flattened).squeeze(1)\n    diffs_1_flattened = torch.abs(diffs_1_flattened).squeeze(1)\n\n\n    need_to_be_perturbed = torch.zeros_like(diffs_0_flattened)\n    ones = torch.zeros_like(diffs_0_flattened)\n    num_pixels_too_close = 1.0\n    threshold = torch.ones_like(diffs_0_flattened)*num_pixels_too_close\n\n    # determine which pixels are too close to being matches\n    need_to_be_perturbed = where(diffs_0_flattened < threshold, ones, need_to_be_perturbed)\n    need_to_be_perturbed = where(diffs_1_flattened < threshold, ones, need_to_be_perturbed)\n\n    minimal_perturb        = num_pixels_too_close/2\n    minimal_perturb_vector = (torch.rand(len(need_to_be_perturbed))*2).floor()*(minimal_perturb*2)-minimal_perturb\n    std_dev = 10\n    random_vector = torch.randn(len(need_to_be_perturbed))*std_dev + minimal_perturb_vector\n    perturb_vector = need_to_be_perturbed*random_vector\n\n    uv_b_non_matches_0_flat = uv_b_non_matches[0].view(-1,1).type(dtype_float).squeeze(1)\n    uv_b_non_matches_1_flat = uv_b_non_matches[1].view(-1,1).type(dtype_float).squeeze(1)\n\n    uv_b_non_matches_0_flat = uv_b_non_matches_0_flat + perturb_vector\n    uv_b_non_matches_1_flat = uv_b_non_matches_1_flat + perturb_vector\n\n    # now just need to wrap around any that went out of bounds\n\n    # handle wrapping in width\n    lower_bound = 0.0\n    upper_bound = image_width*1.0 - 1\n    lower_bound_vec = torch.ones_like(uv_b_non_matches_0_flat) * lower_bound\n    upper_bound_vec = torch.ones_like(uv_b_non_matches_0_flat) * upper_bound\n\n    uv_b_non_matches_0_flat = where(uv_b_non_matches_0_flat > upper_bound_vec, \n        uv_b_non_matches_0_flat - upper_bound_vec, \n        uv_b_non_matches_0_flat)\n\n    uv_b_non_matches_0_flat = where(uv_b_non_matches_0_flat < lower_bound_vec, \n        uv_b_non_matches_0_flat + upper_bound_vec, \n        uv_b_non_matches_0_flat)\n\n    # handle wrapping in height\n    lower_bound = 0.0\n    upper_bound = image_height*1.0 - 1\n    lower_bound_vec = torch.ones_like(uv_b_non_matches_1_flat) * lower_bound\n    upper_bound_vec = torch.ones_like(uv_b_non_matches_1_flat) * upper_bound\n\n    uv_b_non_matches_1_flat = where(uv_b_non_matches_1_flat > upper_bound_vec, \n        uv_b_non_matches_1_flat - upper_bound_vec, \n        uv_b_non_matches_1_flat)\n\n    uv_b_non_matches_1_flat = where(uv_b_non_matches_1_flat < lower_bound_vec, \n        uv_b_non_matches_1_flat + upper_bound_vec, \n        uv_b_non_matches_1_flat)\n\n    return (uv_b_non_matches_0_flat.view(num_matches, num_non_matches_per_match),\n        uv_b_non_matches_1_flat.view(num_matches, num_non_matches_per_match))\n\n# Optionally, uv_a specifies the pixels in img_a for which to find matches\n# If uv_a is not set, then random correspondences are attempted to be found\ndef batch_find_pixel_correspondences(img_a_depth, img_a_pose, img_b_depth, img_b_pose, \n                                        uv_a=None, num_attempts=20, device=\'CPU\', img_a_mask=None, K=None):\n    """"""\n    Computes pixel correspondences in batch\n\n    :param img_a_depth: depth image for image a\n    :type  img_a_depth: numpy 2d array (H x W) encoded as a uint16\n    --\n    :param img_a_pose:  pose for image a, in right-down-forward optical frame\n    :type  img_a_pose:  numpy 2d array, 4 x 4 (homogeneous transform)\n    --\n    :param img_b_depth: depth image for image b\n    :type  img_b_depth: numpy 2d array (H x W) encoded as a uint16\n    -- \n    :param img_b_pose:  pose for image a, in right-down-forward optical frame\n    :type  img_b_pose:  numpy 2d array, 4 x 4 (homogeneous transform)\n    -- \n    :param uv_a:        optional arg, a tuple of (u,v) pixel positions for which to find matches\n    :type  uv_a:        each element of tuple is either an int, or a list-like (castable to torch.LongTensor)\n    --\n    :param num_attempts: if random sampling, how many pixels will be _attempted_ to find matches for.  Note that\n                            this is not the same as asking for a specific number of matches, since many attempted matches\n                            will either be occluded or outside of field-of-view. \n    :type  num_attempts: int\n    --\n    :param device:      either \'CPU\' or \'CPU\'\n    :type  device:      string\n    --\n    :param img_a_mask:  optional arg, an image where each nonzero pixel will be used as a mask\n    :type  img_a_mask:  ndarray, of shape (H, W)\n    --\n    :param K:           optional arg, an image where each nonzero pixel will be used as a mask\n    :type  K:           ndarray, of shape (H, W)\n    --\n    :return:            ""Tuple of tuples"", i.e. pixel position tuples for image a and image b (uv_a, uv_b). \n                        Each of these is a tuple of pixel positions\n    :rtype:             Each of uv_a is a tuple of torch.FloatTensors\n    """"""\n    assert (img_a_depth.shape == img_b_depth.shape)\n    image_width  = img_a_depth.shape[1]\n    image_height = img_b_depth.shape[0]\n\n    global dtype_float\n    global dtype_long\n    if device == \'CPU\':\n        dtype_float = torch.FloatTensor\n        dtype_long = torch.LongTensor\n    if device ==\'GPU\':\n        dtype_float = torch.cuda.FloatTensor\n        dtype_long = torch.cuda.LongTensor\n\n    if uv_a is None:\n        uv_a = pytorch_rand_select_pixel(width=image_width,height=image_height, num_samples=num_attempts)\n    else:\n        uv_a = (torch.LongTensor([uv_a[0]]).type(dtype_long), torch.LongTensor([uv_a[1]]).type(dtype_long))\n        num_attempts = 1\n\n    if img_a_mask is None:\n        uv_a_vec = (torch.ones(num_attempts).type(dtype_long)*uv_a[0],torch.ones(num_attempts).type(dtype_long)*uv_a[1])\n        uv_a_vec_flattened = uv_a_vec[1]*image_width+uv_a_vec[0]\n    else:\n        img_a_mask = torch.from_numpy(img_a_mask).type(dtype_float)  \n        \n        # Option A: This next line samples from img mask\n        uv_a_vec = random_sample_from_masked_image_torch(img_a_mask, num_samples=num_attempts)\n        if uv_a_vec[0] is None:\n            return (None, None)\n        \n        # Option B: These 4 lines grab ALL from img mask\n        # mask_a = img_a_mask.squeeze(0)\n        # mask_a = mask_a/torch.max(mask_a)\n        # nonzero = (torch.nonzero(mask_a)).type(dtype_long)\n        # uv_a_vec = (nonzero[:,1], nonzero[:,0])\n\n        # Always use this line        \n        uv_a_vec_flattened = uv_a_vec[1]*image_width+uv_a_vec[0]\n\n\n    if K is None:\n        K = get_default_K_matrix()\n\n    K_inv = inv(K)\n    body_to_rdf = get_body_to_rdf()\n    rdf_to_body = inv(body_to_rdf)\n\n    img_a_depth_torch = torch.from_numpy(img_a_depth).type(dtype_float)\n    img_a_depth_torch = torch.squeeze(img_a_depth_torch, 0)\n    img_a_depth_torch = img_a_depth_torch.view(-1,1)\n\n    \n    depth_vec = torch.index_select(img_a_depth_torch, 0, uv_a_vec_flattened)*1.0/DEPTH_IM_SCALE\n    depth_vec = depth_vec.squeeze(1)\n    \n    # Prune based on\n    # Case 1: depth is zero (for this data, this means no-return)\n    nonzero_indices = torch.nonzero(depth_vec)\n    if nonzero_indices.dim() == 0:\n        return (None, None)\n    nonzero_indices = nonzero_indices.squeeze(1)\n    depth_vec = torch.index_select(depth_vec, 0, nonzero_indices)\n\n    # prune u_vec and v_vec, then multiply by already pruned depth_vec\n    u_a_pruned = torch.index_select(uv_a_vec[0], 0, nonzero_indices)\n    u_vec = u_a_pruned.type(dtype_float)*depth_vec\n\n    v_a_pruned = torch.index_select(uv_a_vec[1], 0, nonzero_indices)\n    v_vec = v_a_pruned.type(dtype_float)*depth_vec\n\n    z_vec = depth_vec\n\n    full_vec = torch.stack((u_vec, v_vec, z_vec))\n\n    K_inv_torch = torch.from_numpy(K_inv).type(dtype_float)\n    point_camera_frame_rdf_vec = K_inv_torch.mm(full_vec)\n\n    point_world_frame_rdf_vec = apply_transform_torch(point_camera_frame_rdf_vec, torch.from_numpy(img_a_pose).type(dtype_float))\n    point_camera_2_frame_rdf_vec = apply_transform_torch(point_world_frame_rdf_vec, torch.from_numpy(invert_transform(img_b_pose)).type(dtype_float))\n\n    K_torch = torch.from_numpy(K).type(dtype_float)\n    vec2_vec = K_torch.mm(point_camera_2_frame_rdf_vec)\n\n    u2_vec = vec2_vec[0]/vec2_vec[2]\n    v2_vec = vec2_vec[1]/vec2_vec[2]\n\n    maybe_z2_vec = point_camera_2_frame_rdf_vec[2]\n\n    z2_vec = vec2_vec[2]\n\n    # Prune based on\n    # Case 2: the pixels projected into image b are outside FOV\n    # u2_vec bounds should be: 0, image_width\n    # v2_vec bounds should be: 0, image_height\n\n    ## do u2-based pruning\n    u2_vec_lower_bound = 0.0\n    epsilon = 1e-3\n    u2_vec_upper_bound = image_width*1.0 - epsilon  # careful, needs to be epsilon less!!\n    lower_bound_vec = torch.ones_like(u2_vec) * u2_vec_lower_bound\n    upper_bound_vec = torch.ones_like(u2_vec) * u2_vec_upper_bound\n    zeros_vec       = torch.zeros_like(u2_vec)\n\n    u2_vec = where(u2_vec < lower_bound_vec, zeros_vec, u2_vec)\n    u2_vec = where(u2_vec > upper_bound_vec, zeros_vec, u2_vec)\n    in_bound_indices = torch.nonzero(u2_vec)\n    if in_bound_indices.dim() == 0:\n        return (None, None)\n    in_bound_indices = in_bound_indices.squeeze(1)\n\n    # apply pruning\n    u2_vec = torch.index_select(u2_vec, 0, in_bound_indices)\n    v2_vec = torch.index_select(v2_vec, 0, in_bound_indices)\n    z2_vec = torch.index_select(z2_vec, 0, in_bound_indices)\n    u_a_pruned = torch.index_select(u_a_pruned, 0, in_bound_indices) # also prune from first list\n    v_a_pruned = torch.index_select(v_a_pruned, 0, in_bound_indices) # also prune from first list\n\n    ## do v2-based pruning\n    v2_vec_lower_bound = 0.0\n    v2_vec_upper_bound = image_height*1.0 - epsilon\n    lower_bound_vec = torch.ones_like(v2_vec) * v2_vec_lower_bound\n    upper_bound_vec = torch.ones_like(v2_vec) * v2_vec_upper_bound\n    zeros_vec       = torch.zeros_like(v2_vec)    \n\n    v2_vec = where(v2_vec < lower_bound_vec, zeros_vec, v2_vec)\n    v2_vec = where(v2_vec > upper_bound_vec, zeros_vec, v2_vec)\n    in_bound_indices = torch.nonzero(v2_vec)\n    if in_bound_indices.dim() == 0:\n        return (None, None)\n    in_bound_indices = in_bound_indices.squeeze(1)\n\n    # apply pruning\n    u2_vec = torch.index_select(u2_vec, 0, in_bound_indices)\n    v2_vec = torch.index_select(v2_vec, 0, in_bound_indices)\n    z2_vec = torch.index_select(z2_vec, 0, in_bound_indices)\n    u_a_pruned = torch.index_select(u_a_pruned, 0, in_bound_indices) # also prune from first list\n    v_a_pruned = torch.index_select(v_a_pruned, 0, in_bound_indices) # also prune from first list\n\n    # Prune based on\n    # Case 3: the pixels in image b are occluded, OR there is no depth return in image b so we aren\'t sure\n\n    img_b_depth_torch = torch.from_numpy(img_b_depth).type(dtype_float)\n    img_b_depth_torch = torch.squeeze(img_b_depth_torch, 0)\n    img_b_depth_torch = img_b_depth_torch.view(-1,1)\n\n    uv_b_vec_flattened = (v2_vec.type(dtype_long)*image_width+u2_vec.type(dtype_long))  # simply round to int -- good enough \n                                                                       # occlusion check for smooth surfaces\n\n    depth2_vec = torch.index_select(img_b_depth_torch, 0, uv_b_vec_flattened)*1.0/1000\n    depth2_vec = depth2_vec.squeeze(1)\n\n    # occlusion margin, in meters\n    occlusion_margin = 0.003\n    z2_vec = z2_vec - occlusion_margin\n    zeros_vec = torch.zeros_like(depth2_vec)\n\n    depth2_vec = where(depth2_vec < zeros_vec, zeros_vec, depth2_vec) # to be careful, prune any negative depths\n    depth2_vec = where(depth2_vec < z2_vec, zeros_vec, depth2_vec)    # prune occlusions\n    non_occluded_indices = torch.nonzero(depth2_vec)\n    if non_occluded_indices.dim() == 0:\n        return (None, None)\n    non_occluded_indices = non_occluded_indices.squeeze(1)\n    depth2_vec = torch.index_select(depth2_vec, 0, non_occluded_indices)\n\n    # apply pruning\n    u2_vec = torch.index_select(u2_vec, 0, non_occluded_indices)\n    v2_vec = torch.index_select(v2_vec, 0, non_occluded_indices)\n    u_a_pruned = torch.index_select(u_a_pruned, 0, non_occluded_indices) # also prune from first list\n    v_a_pruned = torch.index_select(v_a_pruned, 0, non_occluded_indices) # also prune from first list\n\n    uv_b_vec = (u2_vec, v2_vec)\n    uv_a_vec = (u_a_pruned, v_a_pruned)\n    return (uv_a_vec, uv_b_vec)\n'"
dense_correspondence/correspondence_tools/correspondence_plotter.py,0,"b'import matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle\n\ndef plot_correspondences(images, uv_a, uv_b, use_previous_plot=None, circ_color=\'g\', show=True):\n    if use_previous_plot is None:\n        fig, axes = plt.subplots(nrows=2, ncols=2)\n    else:\n        fig, axes = use_previous_plot[0], use_previous_plot[1]\n\n    fig.set_figheight(10)\n    fig.set_figwidth(15)\n    pixel_locs = [uv_a, uv_b, uv_a, uv_b]\n    axes = axes.flat[0:]\n    if use_previous_plot is not None:\n        axes = [axes[1], axes[3]]\n        images = [images[1], images[3]]\n        pixel_locs = [pixel_locs[1], pixel_locs[3]]\n    for ax, img, pixel_loc in zip(axes[0:], images, pixel_locs):\n        ax.set_aspect(\'equal\')\n        if isinstance(pixel_loc[0], int) or isinstance(pixel_loc[0], float):\n            circ = Circle(pixel_loc, radius=10, facecolor=circ_color, edgecolor=\'white\', fill=True ,linewidth = 2.0, linestyle=\'solid\')\n            ax.add_patch(circ)\n        else:\n            for x,y in zip(pixel_loc[0],pixel_loc[1]):\n                circ = Circle((x,y), radius=10, facecolor=circ_color, edgecolor=\'white\', fill=True ,linewidth = 2.0, linestyle=\'solid\')\n                ax.add_patch(circ)\n        ax.imshow(img)\n    if show:\n        plt.show()\n        return None\n    else:\n        return fig, axes\n\ndef plot_correspondences_from_dir(log_dir, img_a, img_b, uv_a, uv_b, use_previous_plot=None, circ_color=\'g\', show=True):\n    img1_filename = log_dir+""/images/""+img_a+""_rgb.png""\n    img2_filename = log_dir+""/images/""+img_b+""_rgb.png""\n    img1_depth_filename = log_dir+""/images/""+img_a+""_depth.png""\n    img2_depth_filename = log_dir+""/images/""+img_b+""_depth.png""\n    images = [img1_filename, img2_filename, img1_depth_filename, img2_depth_filename]\n    images = [mpimg.imread(x) for x in images]\n    return plot_correspondences(images, uv_a, uv_b, use_previous_plot=use_previous_plot, circ_color=circ_color, show=show)\n\ndef plot_correspondences_direct(img_a_rgb, img_a_depth, img_b_rgb, img_b_depth, uv_a, uv_b, use_previous_plot=None, circ_color=\'g\', show=True):\n    """"""\n\n    Plots rgb and depth image pair along with circles at pixel locations\n    :param img_a_rgb: PIL.Image.Image\n    :param img_a_depth: PIL.Image.Image\n    :param img_b_rgb: PIL.Image.Image\n    :param img_b_depth: PIL.Image.Image\n    :param uv_a: (u,v) pixel location, or list of pixel locations\n    :param uv_b: (u,v) pixel location, or list of pixel locations\n    :param use_previous_plot:\n    :param circ_color: str\n    :param show:\n    :return:\n    """"""\n    images = [img_a_rgb, img_b_rgb, img_a_depth, img_b_depth]\n    return plot_correspondences(images, uv_a, uv_b, use_previous_plot=use_previous_plot, circ_color=circ_color, show=show)\n    \n'"
dense_correspondence/dataset/__init__.py,0,b''
dense_correspondence/dataset/dense_correspondence_dataset_masked.py,25,"b'import torch\nimport torch.utils.data as data\n\nimport os\nimport copy\nimport math\nimport yaml\nimport logging\nimport numpy as np\nimport random\nimport glob\nfrom PIL import Image\n\nimport sys\nimport dense_correspondence_manipulation.utils.utils as utils\nutils.add_dense_correspondence_to_python_path()\n\nfrom torchvision import transforms\nfrom pytorch_segmentation_detection.transforms import ComposeJoint\nimport dense_correspondence.correspondence_tools.correspondence_finder as correspondence_finder\nimport dense_correspondence.correspondence_tools.correspondence_augmentation as correspondence_augmentation\n\n\n\n# This implements a subclass for a data.Dataset class in PyTorch\n# to load in data for dense descriptor training\n#\n# in particular note:\n# __len__     is overloaded\n# __getitem__ is overloaded\n#\n# For more info see:\n# http://pytorch.org/docs/master/data.html#torch.utils.data.Dataset\n\nclass ImageType:\n    RGB = 0\n    DEPTH = 1\n    MASK = 2\n\nclass DenseCorrespondenceDataset(data.Dataset):\n\n    def __init__(self, debug=False):\n        \n        self.debug = debug\n        self.mode = ""train""\n        self.both_to_tensor = ComposeJoint(\n            [\n                [transforms.ToTensor(), transforms.ToTensor()]\n            ])\n\n      \n    def __len__(self):\n        return self.num_images_total\n    \n    def __getitem__(self, index):\n        """"""\n        The method through which the dataset is accessed for training.\n\n        The index param is not currently used, and instead each dataset[i] is the result of\n        a random sampling over:\n        - random scene\n        - random rgbd frame from that scene\n        - random rgbd frame (different enough pose) from that scene\n        - various randomization in the match generation and non-match generation procedure\n\n        returns a large amount of variables, separated by commas.\n\n        0th return arg: the type of data sampled (this can be used as a flag for different loss functions)\n        0th rtype: string\n\n        1st, 2nd return args: image_a_rgb, image_b_rgb\n        1st, 2nd rtype: 3-dimensional torch.FloatTensor of shape (image_height, image_width, 3)\n\n        3rd, 4th return args: matches_a, matches_b\n        3rd, 4th rtype: 1-dimensional torch.LongTensor of shape (num_matches)\n\n        5th, 6th return args: non_matches_a, non_matches_b\n        5th, 6th rtype: 1-dimensional torch.LongTensor of shape (num_non_matches)\n\n        Return values 3,4,5,6 are all in the ""single index"" format for pixels. That is\n\n        (u,v) --> n = u + image_width * v\n\n        """"""\n\n        # stores metadata about this data\n        metadata = dict()\n\n\n        # pick a scene\n        scene_name = self.get_random_scene_name()\n        metadata[\'scene_name\'] = scene_name\n\n        # image a\n        image_a_idx = self.get_random_image_index(scene_name)\n        image_a_rgb, image_a_depth, image_a_mask, image_a_pose = self.get_rgbd_mask_pose(scene_name, image_a_idx)\n\n        metadata[\'image_a_idx\'] = image_a_idx\n\n        # image b\n        image_b_idx = self.get_img_idx_with_different_pose(scene_name, image_a_pose, num_attempts=50)\n        metadata[\'image_b_idx\'] = image_b_idx\n        if image_b_idx is None:\n            logging.info(""no frame with sufficiently different pose found, returning"")\n            # TODO: return something cleaner than no-data\n            image_a_rgb_tensor = self.rgb_image_to_tensor(image_a_rgb)\n            return self.return_empty_data(image_a_rgb_tensor, image_a_rgb_tensor)\n\n        image_b_rgb, image_b_depth, image_b_mask, image_b_pose = self.get_rgbd_mask_pose(scene_name, image_b_idx)\n\n        image_a_depth_numpy = np.asarray(image_a_depth)\n        image_b_depth_numpy = np.asarray(image_b_depth)\n\n        # find correspondences\n        uv_a, uv_b = correspondence_finder.batch_find_pixel_correspondences(image_a_depth_numpy, image_a_pose, \n                                                                           image_b_depth_numpy, image_b_pose, \n                                                                           num_attempts=self.num_matching_attempts, img_a_mask=np.asarray(image_a_mask))\n\n        if uv_a is None:\n            logging.info(""no matches found, returning"")\n            image_a_rgb_tensor = self.rgb_image_to_tensor(image_a_rgb)\n            return self.return_empty_data(image_a_rgb_tensor, image_a_rgb_tensor)\n\n        if self.debug:\n            # downsample so can plot\n            num_matches_to_plot = 10\n            indexes_to_keep = (torch.rand(num_matches_to_plot)*len(uv_a[0])).floor().type(torch.LongTensor)\n            uv_a = (torch.index_select(uv_a[0], 0, indexes_to_keep), torch.index_select(uv_a[1], 0, indexes_to_keep))\n            uv_b = (torch.index_select(uv_b[0], 0, indexes_to_keep), torch.index_select(uv_b[1], 0, indexes_to_keep))\n\n        # data augmentation\n        if self._domain_randomize:\n            image_a_rgb = correspondence_augmentation.random_domain_randomize_background(image_a_rgb, image_a_mask)\n            image_b_rgb = correspondence_augmentation.random_domain_randomize_background(image_b_rgb, image_b_mask)\n\n\n        if not self.debug:\n            [image_a_rgb], uv_a                 = correspondence_augmentation.random_image_and_indices_mutation([image_a_rgb], uv_a)\n            [image_b_rgb, image_b_mask], uv_b   = correspondence_augmentation.random_image_and_indices_mutation([image_b_rgb, image_b_mask], uv_b)\n        else: # also mutate depth just for plotting\n            [image_a_rgb, image_a_depth], uv_a               = correspondence_augmentation.random_image_and_indices_mutation([image_a_rgb, image_a_depth], uv_a)\n            [image_b_rgb, image_b_depth, image_b_mask], uv_b = correspondence_augmentation.random_image_and_indices_mutation([image_b_rgb, image_b_depth, image_b_mask], uv_b)\n            image_a_depth_numpy = np.asarray(image_a_depth)\n            image_b_depth_numpy = np.asarray(image_b_depth)\n\n        # find non_correspondences\n\n        if index%2:\n            metadata[\'non_match_type\'] = \'masked\'\n            logging.debug(""masking non-matches"")\n            image_b_mask = torch.from_numpy(np.asarray(image_b_mask)).type(torch.FloatTensor)\n        else:\n            metadata[\'non_match_type\'] = \'non_masked\'\n            logging.debug(""not masking non-matches"")\n            image_b_mask = None\n            \n        image_b_shape = image_b_depth_numpy.shape\n        image_width  = image_b_shape[1]\n        image_height = image_b_shape[1]\n\n        uv_b_non_matches = correspondence_finder.create_non_correspondences(uv_b, image_b_shape, \n            num_non_matches_per_match=self.num_non_matches_per_match, img_b_mask=image_b_mask)\n\n        if self.debug:\n            # only want to bring in plotting code if in debug mode\n            import correspondence_plotter\n\n            # Just show all images \n            uv_a_long = (torch.t(uv_a[0].repeat(self.num_non_matches_per_match, 1)).contiguous().view(-1,1), \n                     torch.t(uv_a[1].repeat(self.num_non_matches_per_match, 1)).contiguous().view(-1,1))\n            uv_b_non_matches_long = (uv_b_non_matches[0].view(-1,1), uv_b_non_matches[1].view(-1,1) )\n            \n            # Show correspondences\n            if uv_a is not None:\n                fig, axes = correspondence_plotter.plot_correspondences_direct(image_a_rgb, image_a_depth_numpy, image_b_rgb, image_b_depth_numpy, uv_a, uv_b, show=False)\n                correspondence_plotter.plot_correspondences_direct(image_a_rgb, image_a_depth_numpy, image_b_rgb, image_b_depth_numpy,\n                                                  uv_a_long, uv_b_non_matches_long,\n                                                  use_previous_plot=(fig,axes),\n                                                  circ_color=\'r\')\n\n\n        # image_a_rgb, image_b_rgb = self.both_to_tensor([image_a_rgb, image_b_rgb])\n\n        # convert PIL.Image to torch.FloatTensor\n        image_a_rgb = self.rgb_image_to_tensor(image_a_rgb)\n        image_b_rgb = self.rgb_image_to_tensor(image_b_rgb)\n\n        uv_a_long = (torch.t(uv_a[0].repeat(self.num_non_matches_per_match, 1)).contiguous().view(-1,1), \n                     torch.t(uv_a[1].repeat(self.num_non_matches_per_match, 1)).contiguous().view(-1,1))\n        uv_b_non_matches_long = (uv_b_non_matches[0].view(-1,1), uv_b_non_matches[1].view(-1,1) )\n\n        # flatten correspondences and non_correspondences\n        matches_a = uv_a[1].long()*image_width+uv_a[0].long()\n        matches_b = uv_b[1].long()*image_width+uv_b[0].long()\n        non_matches_a = uv_a_long[1].long()*image_width+uv_a_long[0].long()\n        non_matches_a = non_matches_a.squeeze(1)\n        non_matches_b = uv_b_non_matches_long[1].long()*image_width+uv_b_non_matches_long[0].long()\n        non_matches_b = non_matches_b.squeeze(1)\n\n        return ""matches"", image_a_rgb, image_b_rgb, matches_a, matches_b, non_matches_a, non_matches_b, metadata\n\n    def return_empty_data(self, image_a_rgb, image_b_rgb, metadata=None):\n        if metadata is None:\n            metadata = dict()\n\n        empty = DenseCorrespondenceDataset.empty_tensor()\n        return -1, image_a_rgb, image_b_rgb, empty, empty, empty, empty, empty, empty, empty, empty, metadata\n\n    @staticmethod\n    def empty_tensor():\n        """"""\n        Makes a placeholder tensor\n        :return:\n        :rtype:\n        """"""\n        return torch.LongTensor([-1])\n\n    @staticmethod\n    def is_empty(tensor):\n        """"""\n        Tells if the tensor is the same as that created by empty_tensor()\n        """"""\n        return ((len(tensor) == 1) and (tensor[0] == -1))\n\n    def get_rgbd_mask_pose(self, scene_name, img_idx):\n        """"""\n        Returns rgb image, depth image, mask and pose.\n        :param scene_name:\n        :type scene_name: str\n        :param img_idx:\n        :type img_idx: int\n        :return: rgb, depth, mask, pose\n        :rtype: PIL.Image.Image, PIL.Image.Image, PIL.Image.Image, a 4x4 numpy array\n        """"""\n        rgb_file = self.get_image_filename(scene_name, img_idx, ImageType.RGB)\n        rgb = self.get_rgb_image(rgb_file)\n\n        depth_file = self.get_image_filename(scene_name, img_idx, ImageType.DEPTH)\n        depth = self.get_depth_image(depth_file)\n\n        mask_file = self.get_image_filename(scene_name, img_idx, ImageType.MASK)\n        mask = self.get_mask_image(mask_file)\n\n        pose = self.get_pose_from_scene_name_and_idx(scene_name, img_idx)\n\n        return rgb, depth, mask, pose\n\n    def get_random_rgbd_mask_pose(self):\n        """"""\n        Simple wrapper method for `get_rgbd_mask_pose`.\n        Returns rgb image, depth image, mask and pose.\n        :return: rgb, depth, mask, pose\n        :rtype: PIL.Image.Image, PIL.Image.Image, PIL.Image.Image, a 4x4 numpy array\n        """"""\n        scene_name = self.get_random_scene_name()\n        img_idx = self.get_random_image_index(scene_name)\n        return self.get_rgbd_mask_pose(scene_name, img_idx)\n\n\n    def get_img_idx_with_different_pose(self, scene_name, pose_a, threshold=0.2, angle_threshold=20, num_attempts=10):\n        """"""\n        Try to get an image with a different pose to the one passed in. If one can\'t be found\n        then return None\n        :param scene_name:\n        :type scene_name:\n        :param pose_a:\n        :type pose_a:\n        :param threshold:\n        :type threshold:\n        :param num_attempts:\n        :type num_attempts:\n        :return: an index with a different-enough pose\n        :rtype: int or None\n        """"""\n\n        counter = 0\n        while counter < num_attempts:\n            img_idx = self.get_random_image_index(scene_name)\n            pose = self.get_pose_from_scene_name_and_idx(scene_name, img_idx)\n\n            diff = utils.compute_distance_between_poses(pose_a, pose)\n            angle_diff = utils.compute_angle_between_poses(pose_a, pose)\n            if (diff > threshold) or (angle_diff > angle_threshold):\n                return img_idx\n            counter += 1\n\n        return None\n\n\n    def rgb_image_to_tensor(self, img):\n        """"""\n        Transforms a PIL.Image to a torch.FloatTensor.\n        Performs normalization of mean and std dev\n        :param img: input image\n        :type img: PIL.Image\n        :return:\n        :rtype:\n        """"""\n        raise NotImplementedError(""subclass must implement this method"")\n\n\n    @staticmethod\n    def load_rgb_image(rgb_filename):\n        """"""\n        Returns PIL.Image.Image\n        :param rgb_filename:\n        :type rgb_filename:\n        :return:\n        :rtype: PIL.Image.Image\n        """"""\n        return Image.open(rgb_filename).convert(\'RGB\')\n\n    @staticmethod\n    def load_mask_image(mask_filename):\n        """"""\n        Loads the mask image, returns a PIL.Image.Image\n        :param mask_filename:\n        :type mask_filename:\n        :return:\n        :rtype: PIL.Image.Image\n        """"""\n        return Image.open(mask_filename)\n\n    def get_rgb_image(self, rgb_filename):\n        """"""\n        :param depth_filename: string of full path to depth image\n        :return: PIL.Image.Image, in particular an \'RGB\' PIL image\n        """"""\n        return Image.open(rgb_filename).convert(\'RGB\')\n\n    def get_rgb_image_from_scene_name_and_idx(self, scene_name, img_idx):\n        """"""\n        Returns an rgb image given a scene_name and image index\n        :param scene_name:\n        :param img_idx: str or int\n        :return: PIL.Image.Image\n        """"""\n        img_filename = self.get_image_filename(scene_name, img_idx, ImageType.RGB)\n        return self.get_rgb_image(img_filename)\n\n    def get_depth_image(self, depth_filename):\n        """"""\n        :param depth_filename: string of full path to depth image\n        :return: PIL.Image.Image\n        """"""\n        return Image.open(depth_filename)\n\n    def get_depth_image_from_scene_name_and_idx(self, scene_name, img_idx):\n        """"""\n        Returns a depth image given a scene_name and image index\n        :param scene_name:\n        :param img_idx: str or int\n        :return: PIL.Image.Image\n        """"""\n        img_filename = self.get_image_filename(scene_name, img_idx, ImageType.DEPTH)\n        return self.get_depth_image(img_filename)\n\n    def get_mask_image(self, mask_filename):\n        """"""\n        :param mask_filename: string of full path to mask image\n        :return: PIL.Image.Image\n        """"""\n        return Image.open(mask_filename)\n\n    def get_mask_image_from_scene_name_and_idx(self, scene_name, img_idx):\n        """"""\n        Returns a depth image given a scene_name and image index\n        :param scene_name:\n        :param img_idx: str or int\n        :return: PIL.Image.Image\n        """"""\n        img_filename = self.get_image_filename(scene_name, img_idx, ImageType.MASK)\n        return self.get_mask_image(img_filename)\n\n    def get_image_filename(self, scene_name, img_index, image_type):\n        raise NotImplementedError(""Implement in superclass"")\n\n    def load_all_pose_data(self):\n        """"""\n        Efficiently pre-loads all pose data for the scenes. This is because when used as\n        part of torch DataLoader in threaded way it behaves strangely\n        :return:\n        :rtype:\n        """"""\n        raise NotImplementedError(""subclass must implement this method"")\n\n    def get_pose_from_scene_name_and_idx(self, scene_name, idx):\n        """"""\n\n        :param scene_name: str\n        :param img_idx: int\n        :return: 4 x 4 numpy array\n        """"""\n        raise NotImplementedError(""subclass must implement this method"")\n\n    # this function cowbody copied from:\n    # https://www.lfd.uci.edu/~gohlke/code/transformations.py.html\n    def quaternion_matrix(self, quaternion):\n        _EPS = np.finfo(float).eps * 4.0\n        q = np.array(quaternion, dtype=np.float64, copy=True)\n        n = np.dot(q, q)\n        if n < _EPS:\n            return np.identity(4)\n        q *= math.sqrt(2.0 / n)\n        q = np.outer(q, q)\n        return np.array([\n            [1.0-q[2, 2]-q[3, 3],     q[1, 2]-q[3, 0],     q[1, 3]+q[2, 0], 0.0],\n            [    q[1, 2]+q[3, 0], 1.0-q[1, 1]-q[3, 3],     q[2, 3]-q[1, 0], 0.0],\n            [    q[1, 3]-q[2, 0],     q[2, 3]+q[1, 0], 1.0-q[1, 1]-q[2, 2], 0.0],\n            [                0.0,                 0.0,                 0.0, 1.0]])\n\n    def elasticfusion_pose_to_homogeneous_transform(self, lf_pose):\n        homogeneous_transform = self.quaternion_matrix([lf_pose[6], lf_pose[3], lf_pose[4], lf_pose[5]])\n        homogeneous_transform[0,3] = lf_pose[0]\n        homogeneous_transform[1,3] = lf_pose[1]\n        homogeneous_transform[2,3] = lf_pose[2]\n        return homogeneous_transform\n\n    def get_pose_list(self, scene_directory, pose_list_filename):\n        posegraph_filename = os.path.join(scene_directory, pose_list_filename)\n        with open(posegraph_filename) as f:\n            content = f.readlines()\n        pose_list = [x.strip().split() for x in content]\n        return pose_list\n\n    def get_full_path_for_scene(self, scene_name):\n        raise NotImplementedError(""subclass must implement this method"")\n\n    def get_random_scene_name(self):\n        """"""\n        Returns a random scene_name\n        The result will depend on whether we are in test or train mode\n        :return:\n        :rtype:\n        """"""\n        return random.choice(self.scenes)\n\n    def get_random_image_index(self, scene_name):\n        """"""\n        Returns a random image index from a given scene\n        :param scene_name:\n        :type scene_name:\n        :return:\n        :rtype:\n        """"""\n        raise NotImplementedError(""subclass must implement this method"")\n\n    def get_random_scene_directory(self):\n        scene_name = self.get_random_scene_name()\n        # can later add biases for scenes, for example based on # images?\n        scene_directory = self.get_full_path_for_scene(scene_name)\n        return scene_directory\n\n    def scene_generator(self, mode=None):\n        """"""\n        Returns an generator that traverses all the scenes\n        :return:\n        :rtype:\n        """"""\n\n        NotImplementedError(""subclass must implement this method"")\n\n    def init_length(self):\n        """"""\n        Computes the total number of images and scenes in this dataset.\n        Sets the result to the class variables self.num_images_total and self._num_scenes\n        :return:\n        :rtype:\n        """"""\n        \n        self.num_images_total = 0\n        self._num_scenes = 0\n        for scene_name in self.scene_generator():\n            scene_directory = self.get_full_path_for_scene(scene_name)\n            rgb_images_regex = os.path.join(scene_directory, ""images/*_rgb.png"")\n            all_rgb_images_in_scene = glob.glob(rgb_images_regex)\n            num_images_this_scene = len(all_rgb_images_in_scene)\n            self.num_images_total += num_images_this_scene\n            self._num_scenes += 1\n\n    def load_from_config_yaml(self, key):\n\n        this_file_path = os.path.dirname(__file__)\n        yaml_path = os.path.join(this_file_path, ""config.yaml"")\n\n        with open(yaml_path, \'r\') as stream:\n            try:\n                config_dict = yaml.load(stream)\n            except yaml.YAMLError as exc:\n                print(exc)\n\n        import getpass\n        username = getpass.getuser()\n\n        relative_path = config_dict[username][key]\n        full_path = os.path.join(os.environ[\'HOME\'], relative_path)\n        return full_path\n\n    def use_all_available_scenes(self):\n        self.scenes = [os.path.basename(x) for x in glob.glob(self.logs_root_path+""*"")]\n\n    def set_train_test_split_from_yaml(self, yaml_config_file_full_path):\n        """"""\n        Sets self.train and self.test attributes from config file\n        :param yaml_config_file_full_path:\n        :return:\n        """"""\n        if isinstance(yaml_config_file_full_path, str):\n            with open(yaml_config_file_full_path, \'r\') as stream:\n                try:\n                    config_dict = yaml.load(stream)\n                except yaml.YAMLError as exc:\n                    print(exc)\n        else:\n            config_dict = yaml_config_file_full_path\n\n        self.train = config_dict[""train""]\n        self.test  = config_dict[""test""]\n        self.set_train_mode()\n\n    def set_parameters_from_training_config(self, training_config):\n        """"""\n        Some parameters that are really associated only with training, for example\n        those associated with random sampling during the training process,\n        should be passed in from a training.yaml config file.\n\n        :param training_config: a dict() holding params\n        """"""\n\n        if (self.mode == ""train"") and (training_config[""training""][""domain_randomize""]):\n            logging.info(""enabling domain randomization"")\n            self.enable_domain_randomization()\n        else:\n            self.disable_domain_randomization()\n\n        # self._training_config = copy.deepcopy(training_config[""training""])\n\n        self.num_matching_attempts = int(training_config[\'training\'][\'num_matching_attempts\'])\n        self.sample_matches_only_off_mask = training_config[\'training\'][\'sample_matches_only_off_mask\']\n\n        self.num_non_matches_per_match = training_config[\'training\'][""num_non_matches_per_match""]\n\n\n        self.num_masked_non_matches_per_match     = int(training_config[\'training\'][""fraction_masked_non_matches""] * self.num_non_matches_per_match)\n\n        self.num_background_non_matches_per_match = int(training_config[\'training\'][\n                                                    ""fraction_background_non_matches""] * self.num_non_matches_per_match)\n\n        self.cross_scene_num_samples              = training_config[\'training\'][""cross_scene_num_samples""]\n\n        self._use_image_b_mask_inv = training_config[""training""][""use_image_b_mask_inv""] \n\n        from spartan_dataset_masked import SpartanDatasetDataType\n\n        self._data_load_types = []\n        self._data_load_type_probabilities = []\n\n        p = training_config[""training""][""data_type_probabilities""][""SINGLE_OBJECT_WITHIN_SCENE""] \n        if p > 0:\n            print ""using SINGLE_OBJECT_WITHIN_SCENE""\n            self._data_load_types.append(SpartanDatasetDataType.SINGLE_OBJECT_WITHIN_SCENE)\n            self._data_load_type_probabilities.append(p)\n\n        p = training_config[""training""][""data_type_probabilities""][""SINGLE_OBJECT_ACROSS_SCENE""]\n        if p > 0:\n            print ""using SINGLE_OBJECT_ACROSS_SCENE""\n            self._data_load_types.append(SpartanDatasetDataType.SINGLE_OBJECT_ACROSS_SCENE)\n            self._data_load_type_probabilities.append(p)\n\n        p = training_config[""training""][""data_type_probabilities""][""DIFFERENT_OBJECT""]\n        if p > 0:\n            print ""using DIFFERENT_OBJECT""\n            self._data_load_types.append(SpartanDatasetDataType.DIFFERENT_OBJECT)\n            self._data_load_type_probabilities.append(p)\n\n        p = training_config[""training""][""data_type_probabilities""][""MULTI_OBJECT""]\n        if p > 0:\n            print ""using MULTI_OBJECT""\n            self._data_load_types.append(SpartanDatasetDataType.MULTI_OBJECT)\n            self._data_load_type_probabilities.append(p)\n\n        p = training_config[""training""][""data_type_probabilities""][""SYNTHETIC_MULTI_OBJECT""]\n        if p > 0:\n            print ""using SYNTHETIC_MULTI_OBJECT""\n            self._data_load_types.append(SpartanDatasetDataType.SYNTHETIC_MULTI_OBJECT)\n            self._data_load_type_probabilities.append(p)\n\n        self._data_load_type_probabilities = np.array(self._data_load_type_probabilities)\n        self._data_load_type_probabilities /= np.sum(self._data_load_type_probabilities)\n\n    def set_train_mode(self):\n        self.mode = ""train""\n\n    def set_test_mode(self):\n        self.mode = ""test""\n\n    def enable_domain_randomization(self):\n        """"""\n        Turns on background domain randomization\n        :return:\n        :rtype:\n        """"""\n        self._domain_randomize = True\n\n    def disable_domain_randomization(self):\n        """"""\n        Turns off background domain randomization\n        :return:\n        :rtype:\n        """"""\n        self._domain_randomize = False\n\n\n    def compute_image_mean_and_std_dev(self, num_image_samples=10):\n        """"""\n        Computes the image_mean and std_dev using the specified number of samples.\n        Returns two torch.FloatTensor objects, each of size [3]\n        :param num_image_samples:\n        :type num_image_samples:\n        :return:\n        :rtype:\n        """"""\n\n        def get_random_image():\n            scene_name = self.get_random_scene_name()\n            img_idx = self.get_random_image_index(scene_name)\n            img_filename = self.get_image_filename(scene_name, img_idx, ImageType.RGB)\n            img = self.get_rgb_image(img_filename)\n            return img\n\n        def get_image_mean(img_tensor):\n            """"""\n\n            :param img_tensor: torch.FloatTensor with shape [3, 480, 640]\n            :type img_tensor:\n            :return: torch.FloatTensor with shape [3]\n            :rtype:\n            """"""\n            img_mean = torch.mean(img_tensor, 1)\n            img_mean = torch.mean(img_mean, 1)\n            return img_mean\n\n        def get_image_std_dev(img_tensor):\n            shape = img_tensor.shape\n            img_height = shape[1]\n            img_width = shape[2]\n\n            v = img_tensor.view(-1, img_height * img_width)\n            std_dev = torch.std(v, 1)\n            return std_dev\n\n\n\n        to_tensor = transforms.ToTensor()\n\n\n        img_mean_sum = None\n\n        img_mean_sum = torch.zeros([3])\n\n        for i in xrange(0, num_image_samples):\n            img = get_random_image()\n            img_tensor = to_tensor(img)\n            single_img_mean = get_image_mean(img_tensor)\n\n            if img_mean_sum is None:\n                img_mean_sum = torch.zeros_like(single_img_mean)\n\n\n            img_mean_sum = img_mean_sum + single_img_mean\n\n\n        std_dev_sum = None\n\n        for i in xrange(0, num_image_samples):\n            img = get_random_image()\n            img_tensor = to_tensor(img)\n            single_std_dev = get_image_std_dev(img_tensor)\n\n            if std_dev_sum is None:\n                std_dev_sum = torch.zeros_like(single_std_dev)\n\n\n            std_dev_sum += single_std_dev\n\n\n        img_mean = 1.0/num_image_samples * img_mean_sum\n        std_dev = 1.0/num_image_samples * std_dev_sum\n\n        return img_mean, std_dev\n\n\n\n\n    @property\n    def test_scene_directories(self):\n        """"""\n        Get the list of testing scene directories\n        :return: list of strings\n        """"""\n        return self.test\n\n    @property\n    def train_scene_directories(self):\n        """"""\n        Get the list of training scene directories\n        :return: list of strings\n        """"""\n        return self.train\n    """"""\n    Debug\n    """"""\n    def debug_show_data(self, image_a_rgb, image_a_depth, image_a_pose,\n                              image_b_rgb, image_b_depth, image_b_pose):\n        plt.imshow(image_a_rgb)\n        plt.show()\n        plt.imshow(image_a_depth)\n        plt.show()\n        print ""image_a_pose"", image_a_pose\n        plt.imshow(image_b_rgb)\n        plt.show()\n        plt.imshow(image_b_depth)\n        plt.show()\n        print ""image_b_pose"", image_b_pose'"
dense_correspondence/dataset/labelfusion_masked.py,0,"b'from dense_correspondence_dataset_masked import DenseCorrespondenceDataset\n\nimport os\n\nclass LabelFusionDataset(DenseCorrespondenceDataset):\n    def __init__(self, debug):\n    \tself.logs_root_path = self.load_from_config_yaml(""relative_path_to_labelfusion_logs_test"")\n\n        # 5 drill scenes\n        self.scenes = [""2017-06-13-12"",\n                       ""2017-06-13-01"",\n                       ""2017-06-13-15"",\n                       ""2017-06-13-16"",\n                       ""2017-06-13-20""]\n\n        #self.scenes = [""2017-06-13-12""] # just drill scene in tool area\n\n        self.init_length()\n        print ""Using LabelFusionDataset with:""\n        print ""   - number of scenes:"", len(self.scenes)\n        print ""   - total images:    "", self.num_images_total\n\n        DenseCorrespondenceDataset.__init__(self, debug=debug)\n\n    def get_pose(self, rgb_filename):\n    \ttime_filename = self.get_time_filename(rgb_filename)\n        time = self.get_time(time_filename)\n        scene_directory = time_filename.split(""images"")[0]\n        pose_list = self.get_pose_list(scene_directory, ""posegraph.posegraph"")\n        pose_elasticfusion = self.get_pose_from_list(time, pose_list)\n        pose_matrix4 = self.elasticfusion_pose_to_homogeneous_transform(pose_elasticfusion)\n        return pose_matrix4\n\n    def get_time_filename(self, rgb_filename):\n        prefix = rgb_filename.split(""rgb"")[0]\n        time_filename = prefix+""utime.txt""\n        return time_filename\n\n    def get_mask_filename(self, rgb_filename):\n        prefix = rgb_filename.split(""rgb"")[0]\n        mask_filename = prefix+""labels.png""\n        return mask_filename\n\n    def get_time(self, time_filename):\n        with open (time_filename) as f:\n            content = f.readlines()\n        return int(content[0])/1e6\n\n    def get_pose_from_list(self, time, pose_list):\n        if (time <= float(pose_list[0][0])):\n            pose = pose_list[0]\n            pose = [float(x) for x in pose[1:]]\n            return pose\n        for pose in pose_list:\n            if (time <= float(pose[0])):\n                pose = [float(x) for x in pose[1:]]\n                return pose\n        print ""did not find matching pose, must be at end of list""\n        return pose'"
dense_correspondence/dataset/scene_structure.py,0,"b'import os\n\nimport dense_correspondence_manipulation.utils.utils as utils\n\nclass SceneStructure(object):\n\n    def __init__(self, processed_folder_dir):\n        self._processed_folder_dir = processed_folder_dir\n\n    @property\n    def fusion_reconstruction_file(self):\n        """"""\n        The filepath for the fusion reconstruction\n        :return:\n        :rtype:\n        """"""\n        return os.path.join(self._processed_folder_dir, \'fusion_mesh.ply\')\n\n    @property\n    def foreground_fusion_reconstruction_file(self):\n        """"""\n        The filepath for the fusion reconstruction corresponding only to the\n        foreground. Note, this may not exist if you haven\'t done some processing\n        :return:\n        :rtype:\n        """"""\n        return os.path.join(self._processed_folder_dir, \'fusion_mesh_foreground.ply\')\n\n    @property\n    def camera_info_file(self):\n        """"""\n        Full filepath for yaml file containing camera intrinsics parameters\n        :return:\n        :rtype:\n        """"""\n        return os.path.join(self._processed_folder_dir, \'images\', \'camera_info.yaml\')\n\n\n    @property\n    def camera_pose_file(self):\n        """"""\n        Full filepath for yaml file containing the camera poses\n        :return:\n        :rtype:\n        """"""\n        return os.path.join(self._processed_folder_dir, \'images\', \'pose_data.yaml\')\n\n    @property\n    def rendered_images_dir(self):\n        return os.path.join(self._processed_folder_dir, \'rendered_images\')\n\n    @property\n    def images_dir(self):\n        return os.path.join(self._processed_folder_dir, \'images\')\n\n    @property\n    def metadata_file(self):\n        return os.path.join(self.images_dir, \'metadata.yaml\')\n\n\n    def mesh_descriptors_dir(self, network_name):\n        """"""\n        Directory where we store descriptors corresponding to a particular network\n        :param network_name:\n        :type network_name:\n        :return:\n        :rtype:\n        """"""\n        return os.path.join(self._processed_folder_dir, \'mesh_descriptors\', network_name)\n\n    def mesh_cells_image_filename(self, img_idx):\n        """"""\n        Returns the full filename for the cell labels image\n        :param img_idx:\n        :type img_idx:\n        :return:\n        :rtype:\n        """"""\n        filename = utils.getPaddedString(img_idx) + ""_mesh_cells.png""\n        return os.path.join(self.rendered_images_dir, filename)\n\n\n    def mesh_descriptors_filename(self, network_name, img_idx):\n        """"""\n        Returns the full filename for the .npz file that contains two arrays\n\n        .npz reference https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.savez.html#numpy.savez\n\n        D = descriptor dimension\n\n        - cell_ids: np.array of size N, dtype=np.int64\n        - cell_descriptors: np.array with np.shape = [N,D dtype = np.float64\n        -\n        :param img_idx:\n        :type img_idx:\n        :return:\n        :rtype:\n        """"""\n\n        filename = utils.getPaddedString(img_idx) + ""_mesh_descriptors.npz""\n        return os.path.join(self.mesh_descriptors_dir(network_name), filename)\n\n    def mesh_descriptor_statistics_filename(self, network_name):\n        """"""\n        Filename containing mesh descriptor statistics\n\n        N = number of cells for which we have descriptor information\n\n        - cell_valid: np.array of size N, dtype=np.int64. Value is the \n        index of that cell in the ply file description\n        - cell_descriptor_mean: np.array with np.shape = [N,D] dtype = np.float64\n        - cell_location: Location of the cell in object frame np.array with\n                        np.shape = [N,3], dtype=np.float64\n\n        :param: network_name\n        :return: filename\n        :rtype: str\n        """"""\n        return os.path.join(self.mesh_descriptors_dir(network_name), ""mesh_descriptor_stats.npz"")\n\n    @staticmethod\n    def descriptor_image_filename(img_idx):\n        filename = utils.getPaddedString(img_idx) + ""_descriptor_image.npy""\n        return filename\n\n\n\n\n\n'"
dense_correspondence/dataset/spartan_dataset_masked.py,29,"b'from dense_correspondence_dataset_masked import DenseCorrespondenceDataset, ImageType\n\nimport os\nimport numpy as np\nimport logging\nimport glob\nimport random\nimport copy\n\nimport torch\n\n# note that this is the torchvision provided by the warmspringwinds\n# pytorch-segmentation-detection repo. It is a fork of pytorch/vision\nfrom torchvision import transforms\n\nimport dense_correspondence_manipulation.utils.utils as utils\nfrom dense_correspondence_manipulation.utils.utils import CameraIntrinsics\n\n\nimport dense_correspondence_manipulation.utils.constants as constants\n\n\nutils.add_dense_correspondence_to_python_path()\nimport dense_correspondence.correspondence_tools.correspondence_finder as correspondence_finder\nimport dense_correspondence.correspondence_tools.correspondence_augmentation as correspondence_augmentation\n\nfrom dense_correspondence.dataset.scene_structure import SceneStructure\n\n\n\nclass SpartanDatasetDataType:\n    SINGLE_OBJECT_WITHIN_SCENE = 0\n    SINGLE_OBJECT_ACROSS_SCENE = 1\n    DIFFERENT_OBJECT = 2\n    MULTI_OBJECT = 3\n    SYNTHETIC_MULTI_OBJECT = 4\n\n\nclass SpartanDataset(DenseCorrespondenceDataset):\n\n    PADDED_STRING_WIDTH = 6\n\n    def __init__(self, debug=False, mode=""train"", config=None, config_expanded=None, verbose=False):\n        """"""\n        :param config: This is for creating a dataset from a composite dataset config file.\n            This is of the form:\n\n                logs_root_path: logs_proto # path relative to utils.get_data_dir()\n\n                single_object_scenes_config_files:\n                - caterpillar_17_scenes.yaml\n                - baymax.yaml\n\n                multi_object_scenes_config_files:\n                - multi_object.yaml\n\n        :type config: dict()\n\n        :param config_expanded: When a config is read, it is parsed into an expanded form\n            which is actually used as self._config.  See the function _setup_scene_data()\n            for how this is done.  We want to save this expanded config to disk as it contains\n            all config information.  If loading a previously-used dataset configuration, we want\n            to pass in the config_expanded.\n        :type config_expanded: dict()\n        """"""\n\n        DenseCorrespondenceDataset.__init__(self, debug=debug)\n\n        # Otherwise, all of these parameters should be set in\n        # set_parameters_from_training_config()\n        # which is called from training.py\n        # and parameters are populated in training.yaml\n        if self.debug:\n            # NOTE: these are not the same as the numbers\n            # that get plotted in debug mode.\n            # This is just so the dataset will ""run"".\n            self._domain_randomize = False\n            self.num_masked_non_matches_per_match = 5\n            self.num_background_non_matches_per_match = 5\n            self.cross_scene_num_samples = 1000\n            self._use_image_b_mask_inv = True\n            self.num_matching_attempts = 10000\n            self.sample_matches_only_off_mask = True\n\n        self._verbose = verbose\n\n        if config is not None:\n            self._setup_scene_data(config)\n        elif config_expanded is not None:\n            self._parse_expanded_config(config_expanded)\n        else:\n            raise ValueError(""You need to give me either a config or config_expanded"")\n\n        self._pose_data = dict()\n        self._initialize_rgb_image_to_tensor()\n\n        if mode == ""test"":\n            self.set_test_mode()\n        elif mode == ""train"":\n            self.set_train_mode()\n        else:\n            raise ValueError(""mode should be one of [test, train]"")\n\n        self.init_length()\n        print ""Using SpartanDataset:""\n        print ""   - in"", self.mode, ""mode""\n        print ""   - number of scenes"", self._num_scenes\n        print ""   - total images:    "", self.num_images_total\n\n\n    def __getitem__(self, index):\n        """"""\n        This overloads __getitem__ and is what is actually returned\n        using a torch dataloader.\n\n        This small function randomly chooses one of our different\n        img pair types, then returns that type of data.\n        """"""\n\n\n        data_load_type = self._get_data_load_type()\n\n        # Case 0: Same scene, same object\n        if data_load_type == SpartanDatasetDataType.SINGLE_OBJECT_WITHIN_SCENE:\n            if self._verbose:\n                print ""Same scene, same object""\n            return self.get_single_object_within_scene_data()\n\n        # Case 1: Same object, different scene\n        if data_load_type == SpartanDatasetDataType.SINGLE_OBJECT_ACROSS_SCENE:\n            if self._verbose:\n                print ""Same object, different scene""\n            return self.get_single_object_across_scene_data()\n\n        # Case 2: Different object\n        if data_load_type == SpartanDatasetDataType.DIFFERENT_OBJECT:\n            if self._verbose:\n                print ""Different object""\n            return self.get_different_object_data()\n\n        # Case 3: Multi object\n        if data_load_type == SpartanDatasetDataType.MULTI_OBJECT:\n            if self._verbose:\n                print ""Multi object""\n            return self.get_multi_object_within_scene_data()\n\n        # Case 4: Synthetic multi object\n        if data_load_type == SpartanDatasetDataType.SYNTHETIC_MULTI_OBJECT:\n            if self._verbose:\n                print ""Synthetic multi object""\n            return self.get_synthetic_multi_object_within_scene_data()\n\n\n    def _setup_scene_data(self, config):\n        """"""\n        Initializes the data for all the different types of scenes\n\n        Creates two class attributes\n\n        self._single_object_scene_dict\n\n        Each entry of self._single_object_scene_dict is a dict with keys {""test"", ""train""}. The\n        values are lists of scenes\n\n        self._single_object_scene_dict has (key, value) = (object_id, scene config for that object)\n\n        self._multi_object_scene_dict has (key, value) = (""train""/""test"", list of scenes)\n\n        Note that the scenes have absolute paths here\n        """"""\n\n        self.logs_root_path = utils.convert_data_relative_path_to_absolute_path(config[\'logs_root_path\'], assert_path_exists=True)\n\n\n        self._single_object_scene_dict = dict()\n\n        prefix = os.path.join(utils.getDenseCorrespondenceSourceDir(), \'config\', \'dense_correspondence\',\n                                       \'dataset\')\n\n        for config_file in config[""single_object_scenes_config_files""]:\n            config_file = os.path.join(prefix, \'single_object\', config_file)\n            single_object_scene_config = utils.getDictFromYamlFilename(config_file)\n            object_id = single_object_scene_config[""object_id""]\n\n            # check if we already have this object in our dataset or not\n            if object_id not in self._single_object_scene_dict:\n                self._single_object_scene_dict[object_id] = single_object_scene_config\n            else:\n                existing_config = self._single_object_scene_dict[object_id]\n                merged_config = SpartanDataset.merge_single_object_configs([existing_config, single_object_scene_config])\n                self._single_object_scene_dict[object_id] = merged_config\n\n            # will have test and train entries\n        # each one is a list of scenes\n        self._multi_object_scene_dict = {""train"": [], ""test"": [], ""evaluation_labeled_data_path"": []}\n\n        for config_file in config[""multi_object_scenes_config_files""]:\n            config_file = os.path.join(prefix, \'multi_object\', config_file)\n            multi_object_scene_config = utils.getDictFromYamlFilename(config_file)\n\n            for key, val in self._multi_object_scene_dict.iteritems():\n                for item in multi_object_scene_config[key]:\n                    val.append(item)\n\n        self._config = dict()\n        self._config[""logs_root_path""] = config[\'logs_root_path\']\n        self._config[""single_object""] = self._single_object_scene_dict\n        self._config[""multi_object""] = self._multi_object_scene_dict\n\n        self._setup_data_load_types()\n\n    def _parse_expanded_config(self, config_expanded):\n        """"""\n        If we have previously saved to disk a dict with:\n        ""single_object"", and\n        ""multi_object"" keys,\n        then we want to recreate a config from these.\n        """"""\n        self._config = config_expanded\n        self._single_object_scene_dict = self._config[""single_object""]\n        self._multi_object_scene_dict = self._config[""multi_object""]\n        self.logs_root_path = utils.convert_data_relative_path_to_absolute_path(self._config[""logs_root_path""], assert_path_exists=True)\n\n    def _setup_data_load_types(self):\n\n        self._data_load_types = []\n        self._data_load_type_probabilities = []\n        if self.debug:\n            #self._data_load_types.append(SpartanDatasetDataType.SINGLE_OBJECT_WITHIN_SCENE)\n            # self._data_load_types.append(SpartanDatasetDataType.SINGLE_OBJECT_ACROSS_SCENE)\n            # self._data_load_types.append(SpartanDatasetDataType.DIFFERENT_OBJECT)\n            # self._data_load_types.append(SpartanDatasetDataType.MULTI_OBJECT)\n            self._data_load_types.append(SpartanDatasetDataType.SYNTHETIC_MULTI_OBJECT)\n            self._data_load_type_probabilities.append(1)\n\n    def _get_data_load_type(self):\n        """"""\n        Gets a random data load type from the allowable types\n        :return: SpartanDatasetDataType\n        :rtype:\n        """"""\n        return np.random.choice(self._data_load_types, 1, p=self._data_load_type_probabilities)[0]\n\n    def scene_generator(self, mode=None):\n        """"""\n        Returns a generator that traverses all the scenes\n        :return:\n        :rtype:\n        """"""\n        if mode is None:\n            mode = self.mode\n\n        for object_id, single_object_scene_dict in self._single_object_scene_dict.iteritems():\n            for scene_name in single_object_scene_dict[mode]:\n                yield scene_name\n\n        for scene_name in self._multi_object_scene_dict[mode]:\n            yield scene_name\n\n    def get_scene_list(self, mode=None):\n        """"""\n        Returns a list of all scenes in this dataset\n        :return:\n        :rtype:\n        """"""\n        scene_generator = self.scene_generator(mode=mode)\n        scene_list = []\n        for scene_name in scene_generator:\n            scene_list.append(scene_name)\n\n        return scene_list\n    \n    def get_list_of_objects(self):\n        """"""\n        Returns a list of object ids\n        :return: list of object_ids\n        :rtype:\n        """"""\n        return self._single_object_scene_dict.keys()\n\n    def get_scene_list_for_object(self, object_id, mode=None):\n        """"""\n        Returns list of scenes for a given object. Return test/train\n        scenes depending on the mode\n        :param object_id:\n        :type object_id: string\n        :param mode: either ""test"" or ""train""\n        :type mode:\n        :return:\n        :rtype:\n        """"""\n        if mode is None:\n            mode = self.mode\n\n        return copy.copy(self._single_object_scene_dict[object_id][mode])\n\n    def _initialize_rgb_image_to_tensor(self):\n        """"""\n        Sets up the RGB PIL.Image --> torch.FloatTensor transform\n        :return: None\n        :rtype:\n        """"""\n        norm_transform = transforms.Normalize(self.get_image_mean(), self.get_image_std_dev())\n        self._rgb_image_to_tensor = transforms.Compose([transforms.ToTensor(), norm_transform])\n\n    def get_full_path_for_scene(self, scene_name):\n        """"""\n        Returns the full path to the processed logs folder\n        :param scene_name:\n        :type scene_name:\n        :return:\n        :rtype:\n        """"""\n        return os.path.join(self.logs_root_path, scene_name, \'processed\')\n\n\n    def load_all_pose_data(self):\n        """"""\n        Efficiently pre-loads all pose data for the scenes. This is because when used as\n        part of torch DataLoader in threaded way it behaves strangely\n        :return:\n        :rtype:\n        """"""\n\n        for scene_name in self.scene_generator():\n            self.get_pose_data(scene_name)\n\n    def get_pose_data(self, scene_name):\n        """"""\n        Checks if have not already loaded the pose_data.yaml for this scene,\n        if haven\'t then loads it. Then returns the dict of the pose_data.yaml.\n        :type scene_name: str\n        :return: a dict() of the pose_data.yaml for the scene.\n        :rtype: dict()\n        """"""\n        if scene_name not in self._pose_data:\n            logging.info(""Loading pose data for scene %s"" %(scene_name) )\n            pose_data_filename = os.path.join(self.get_full_path_for_scene(scene_name),\n                                              \'images\', \'pose_data.yaml\')\n            self._pose_data[scene_name] = utils.getDictFromYamlFilename(pose_data_filename)\n\n        return self._pose_data[scene_name]\n\n\n    def get_pose_from_scene_name_and_idx(self, scene_name, idx):\n        """"""\n        :param scene_name: str\n        :param img_idx: int\n        :return: 4 x 4 numpy array\n        """"""\n        idx = int(idx)\n        scene_pose_data = self.get_pose_data(scene_name)\n        pose_data = scene_pose_data[idx][\'camera_to_world\']\n        return utils.homogenous_transform_from_dict(pose_data)\n\n\n    def get_image_filename(self, scene_name, img_index, image_type):\n        """"""\n        Get the image filename for that scene and image index\n        :param scene_name: str\n        :param img_index: str or int\n        :param image_type: ImageType\n        :return:\n        """"""\n\n        # @todo(manuelli) check that scene_name actually exists\n\n        scene_directory = self.get_full_path_for_scene(scene_name)\n\n        if image_type == ImageType.RGB:\n            images_dir = os.path.join(scene_directory, \'images\')\n            file_extension = ""_rgb.png""\n        elif image_type == ImageType.DEPTH:\n            images_dir = os.path.join(scene_directory, \'rendered_images\')\n            file_extension = ""_depth.png""\n        elif image_type == ImageType.MASK:\n            images_dir = os.path.join(scene_directory, \'image_masks\')\n            file_extension = ""_mask.png""\n        else:\n            raise ValueError(""unsupported image type"")\n\n        if isinstance(img_index, int):\n            img_index = utils.getPaddedString(img_index, width=SpartanDataset.PADDED_STRING_WIDTH)\n\n        scene_directory = self.get_full_path_for_scene(scene_name)\n        if not os.path.isdir(scene_directory):\n            raise ValueError(""scene_name = %s doesn\'t exist"" %(scene_name))\n\n        return os.path.join(images_dir, img_index + file_extension)\n\n    def get_camera_intrinsics(self, scene_name=None):\n        """"""\n        Returns the camera matrix for that scene\n        :param scene_name:\n        :type scene_name:\n        :return:\n        :rtype:\n        """"""\n\n        if scene_name is None:\n            scene_directory = self.get_random_scene_directory()\n        else:\n            scene_directory = os.path.join(self.logs_root_path, scene_name)\n\n        camera_info_file = os.path.join(scene_directory, \'processed\', \'images\', \'camera_info.yaml\')\n        return CameraIntrinsics.from_yaml_file(camera_info_file)\n\n    def get_random_image_index(self, scene_name):\n        """"""\n        Returns a random image index from a given scene\n        :param scene_name:\n        :type scene_name:\n        :return:\n        :rtype:\n        """"""\n        pose_data = self.get_pose_data(scene_name)\n        image_idxs = pose_data.keys() # list of integers\n        random.choice(image_idxs)\n        random_idx = random.choice(image_idxs)\n        return random_idx\n\n    def get_random_object_id(self):\n        """"""\n        Returns a random object_id\n        :return:\n        :rtype:\n        """"""\n        object_id_list = self._single_object_scene_dict.keys()\n        return random.choice(object_id_list)\n\n    def get_random_object_id_and_int(self):\n        """"""\n        Returns a random object_id (a string) and its ""int"" (i.e. numerical unique id)\n        :return:\n        :rtype:\n        """"""\n        object_id_list = self._single_object_scene_dict.keys()\n        random_object_id = random.choice(object_id_list)\n        object_id_int = sorted(self._single_object_scene_dict.keys()).index(random_object_id)\n        return random_object_id, object_id_int\n\n    def get_random_single_object_scene_name(self, object_id):\n        """"""\n        Returns a random scene name for that object\n        :param object_id: str\n        :type object_id:\n        :return: str\n        :rtype:\n        """"""\n        scene_list = self._single_object_scene_dict[object_id][self.mode]\n        return random.choice(scene_list)\n\n    def get_different_scene_for_object(self, object_id, scene_name):\n        """"""\n        Return a different scene name\n        :param object_id:\n        :type object_id:\n        :return:\n        :rtype:\n        """"""\n\n        scene_list = self._single_object_scene_dict[object_id][self.mode]\n        if len(scene_list) == 1:\n            raise ValueError(""There is only one scene of this object, can\'t sample a different one"")\n\n        idx_array = np.arange(0, len(scene_list))\n        rand_idxs = np.random.choice(idx_array, 2, replace=False)\n\n        for idx in rand_idxs:\n            scene_name_b = scene_list[idx]\n            if scene_name != scene_name_b:\n                return scene_name_b\n\n        raise ValueError(""It (should) be impossible to get here!!!!"")\n\n    def get_two_different_object_ids(self):\n        """"""\n        Returns two different random object ids\n        :return: two object ids\n        :rtype: two strings separated by commas\n        """"""\n\n        object_id_list = self._single_object_scene_dict.keys()\n        if len(object_id_list) == 1:\n            raise ValueError(""There is only one object, can\'t sample a different one"")\n\n        idx_array = np.arange(0, len(object_id_list))\n        rand_idxs = np.random.choice(idx_array, 2, replace=False)\n\n        object_1_id = object_id_list[rand_idxs[0]]\n        object_2_id = object_id_list[rand_idxs[1]]\n\n        assert object_1_id != object_2_id\n        return object_1_id, object_2_id\n\n    def get_random_multi_object_scene_name(self):\n        """"""\n        Returns a random multi object scene name\n        :return:\n        :rtype:\n        """"""\n        return random.choice(self._multi_object_scene_dict[self.mode])\n\n\n    def get_number_of_unique_single_objects(self):\n        """"""\n        Returns the number of unique objects in this dataset with single object scenes\n        :return:\n        :rtype:\n        """"""\n        return len(self._single_object_scene_dict.keys())\n\n    def has_multi_object_scenes(self):\n        """"""\n        Returns true if there are multi-object scenes in this datase\n        :return:\n        :rtype:\n        """"""\n        return len(self._multi_object_scene_dict[""train""]) > 0\n\n    def get_random_scene_name(self):\n        """"""\n        Gets a random scene name across both single and multi object\n        """"""\n        types = []\n        if self.has_multi_object_scenes():\n            for _ in range(len(self._multi_object_scene_dict[self.mode])):\n                types.append(""multi"")\n        if self.get_number_of_unique_single_objects() > 0:\n            for _ in range(self.get_number_of_unique_single_objects()):\n                types.append(""single"")\n\n        if len(types) == 0:\n            raise ValueError(""I don\'t think you have any scenes?"")\n        scene_type = random.choice(types)\n\n        if scene_type == ""multi"":\n            return self.get_random_multi_object_scene_name()\n        if scene_type == ""single"":\n            object_id = self.get_random_object_id()\n            return self.get_random_single_object_scene_name(object_id)\n\n    def get_single_object_within_scene_data(self):\n        """"""\n        Simple wrapper around get_within_scene_data(), for the single object case\n        """"""\n        if self.get_number_of_unique_single_objects() == 0:\n            raise ValueError(""There are no single object scenes in this dataset"")\n\n        object_id = self.get_random_object_id()\n        scene_name = self.get_random_single_object_scene_name(object_id)\n\n        metadata = dict()\n        metadata[""object_id""] = object_id\n        metadata[""object_id_int""] = sorted(self._single_object_scene_dict.keys()).index(object_id)\n        metadata[""scene_name""] = scene_name\n        metadata[""type""] = SpartanDatasetDataType.SINGLE_OBJECT_WITHIN_SCENE\n\n        return self.get_within_scene_data(scene_name, metadata)\n\n    def get_multi_object_within_scene_data(self):\n        """"""\n        Simple wrapper around get_within_scene_data(), for the multi object case\n        """"""\n\n        if not self.has_multi_object_scenes():\n            raise ValueError(""There are no multi object scenes in this dataset"")\n\n        scene_name = self.get_random_multi_object_scene_name()\n\n        metadata = dict()\n        metadata[""scene_name""] = scene_name\n        metadata[""type""] = SpartanDatasetDataType.MULTI_OBJECT\n\n        return self.get_within_scene_data(scene_name, metadata)\n\n    def get_within_scene_data(self, scene_name, metadata, for_synthetic_multi_object=False):\n        """"""\n        The method through which the dataset is accessed for training.\n\n        Each call is is the result of\n        a random sampling over:\n        - random scene\n        - random rgbd frame from that scene\n        - random rgbd frame (different enough pose) from that scene\n        - various randomization in the match generation and non-match generation procedure\n\n        returns a large amount of variables, separated by commas.\n\n        0th return arg: the type of data sampled (this can be used as a flag for different loss functions)\n        0th rtype: string\n\n        1st, 2nd return args: image_a_rgb, image_b_rgb\n        1st, 2nd rtype: 3-dimensional torch.FloatTensor of shape (image_height, image_width, 3)\n\n        3rd, 4th return args: matches_a, matches_b\n        3rd, 4th rtype: 1-dimensional torch.LongTensor of shape (num_matches)\n\n        5th, 6th return args: masked_non_matches_a, masked_non_matches_b\n        5th, 6th rtype: 1-dimensional torch.LongTensor of shape (num_non_matches)\n\n        7th, 8th return args: non_masked_non_matches_a, non_masked_non_matches_b\n        7th, 8th rtype: 1-dimensional torch.LongTensor of shape (num_non_matches)\n\n        7th, 8th return args: non_masked_non_matches_a, non_masked_non_matches_b\n        7th, 8th rtype: 1-dimensional torch.LongTensor of shape (num_non_matches)\n\n        9th, 10th return args: blind_non_matches_a, blind_non_matches_b\n        9th, 10th rtype: 1-dimensional torch.LongTensor of shape (num_non_matches)\n\n        11th return arg: metadata useful for plotting, and-or other flags for loss functions\n        11th rtype: dict\n\n        Return values 3,4,5,6,7,8,9,10 are all in the ""single index"" format for pixels. That is\n\n        (u,v) --> n = u + image_width * v\n\n        If no datapoints were found for some type of match or non-match then we return\n        our ""special"" empty tensor. Note that due to the way the pytorch data loader\n        functions you cannot return an empty tensor like torch.FloatTensor([]). So we\n        return SpartanDataset.empty_tensor()\n\n        """"""\n\n        SD = SpartanDataset\n\n        image_a_idx = self.get_random_image_index(scene_name)\n        image_a_rgb, image_a_depth, image_a_mask, image_a_pose = self.get_rgbd_mask_pose(scene_name, image_a_idx)\n\n        metadata[\'image_a_idx\'] = image_a_idx\n\n        # image b\n        image_b_idx = self.get_img_idx_with_different_pose(scene_name, image_a_pose, num_attempts=50)\n        metadata[\'image_b_idx\'] = image_b_idx\n        if image_b_idx is None:\n            logging.info(""no frame with sufficiently different pose found, returning"")\n            # TODO: return something cleaner than no-data\n            image_a_rgb_tensor = self.rgb_image_to_tensor(image_a_rgb)\n            return self.return_empty_data(image_a_rgb_tensor, image_a_rgb_tensor)\n\n        image_b_rgb, image_b_depth, image_b_mask, image_b_pose = self.get_rgbd_mask_pose(scene_name, image_b_idx)\n\n        image_a_depth_numpy = np.asarray(image_a_depth)\n        image_b_depth_numpy = np.asarray(image_b_depth)\n\n        if self.sample_matches_only_off_mask:\n            correspondence_mask = np.asarray(image_a_mask)\n        else:\n            correspondence_mask = None\n\n        # find correspondences\n        uv_a, uv_b = correspondence_finder.batch_find_pixel_correspondences(image_a_depth_numpy, image_a_pose,\n                                                                            image_b_depth_numpy, image_b_pose,\n                                                                            img_a_mask=correspondence_mask,\n                                                                            num_attempts=self.num_matching_attempts)\n\n        if for_synthetic_multi_object:\n            return image_a_rgb, image_b_rgb, image_a_depth, image_b_depth, image_a_mask, image_b_mask, uv_a, uv_b\n\n\n        if uv_a is None:\n            logging.info(""no matches found, returning"")\n            image_a_rgb_tensor = self.rgb_image_to_tensor(image_a_rgb)\n            return self.return_empty_data(image_a_rgb_tensor, image_a_rgb_tensor)\n\n\n        # data augmentation\n        if self._domain_randomize:\n            image_a_rgb = correspondence_augmentation.random_domain_randomize_background(image_a_rgb, image_a_mask)\n            image_b_rgb = correspondence_augmentation.random_domain_randomize_background(image_b_rgb, image_b_mask)\n\n        if not self.debug:\n            [image_a_rgb, image_a_mask], uv_a = correspondence_augmentation.random_image_and_indices_mutation([image_a_rgb, image_a_mask], uv_a)\n            [image_b_rgb, image_b_mask], uv_b = correspondence_augmentation.random_image_and_indices_mutation(\n                [image_b_rgb, image_b_mask], uv_b)\n        else:  # also mutate depth just for plotting\n            [image_a_rgb, image_a_depth, image_a_mask], uv_a = correspondence_augmentation.random_image_and_indices_mutation(\n                [image_a_rgb, image_a_depth, image_a_mask], uv_a)\n            [image_b_rgb, image_b_depth, image_b_mask], uv_b = correspondence_augmentation.random_image_and_indices_mutation(\n                [image_b_rgb, image_b_depth, image_b_mask], uv_b)\n\n        image_a_depth_numpy = np.asarray(image_a_depth)\n        image_b_depth_numpy = np.asarray(image_b_depth)\n\n\n        # find non_correspondences\n        image_b_mask_torch = torch.from_numpy(np.asarray(image_b_mask)).type(torch.FloatTensor)\n        image_b_shape = image_b_depth_numpy.shape\n        image_width = image_b_shape[1]\n        image_height = image_b_shape[0]\n\n        uv_b_masked_non_matches = \\\n            correspondence_finder.create_non_correspondences(uv_b,\n                                                             image_b_shape,\n                                                             num_non_matches_per_match=self.num_masked_non_matches_per_match,\n                                                                            img_b_mask=image_b_mask_torch)\n\n\n        if self._use_image_b_mask_inv:\n            image_b_mask_inv = 1 - image_b_mask_torch\n        else:\n            image_b_mask_inv = None\n\n        uv_b_background_non_matches = correspondence_finder.create_non_correspondences(uv_b,\n                                                                            image_b_shape,\n                                                                            num_non_matches_per_match=self.num_background_non_matches_per_match,\n                                                                            img_b_mask=image_b_mask_inv)\n\n\n\n        # convert PIL.Image to torch.FloatTensor\n        image_a_rgb_PIL = image_a_rgb\n        image_b_rgb_PIL = image_b_rgb\n        image_a_rgb = self.rgb_image_to_tensor(image_a_rgb)\n        image_b_rgb = self.rgb_image_to_tensor(image_b_rgb)\n\n        matches_a = SD.flatten_uv_tensor(uv_a, image_width)\n        matches_b = SD.flatten_uv_tensor(uv_b, image_width)\n\n        # Masked non-matches\n        uv_a_masked_long, uv_b_masked_non_matches_long = self.create_non_matches(uv_a, uv_b_masked_non_matches, self.num_masked_non_matches_per_match)\n\n        masked_non_matches_a = SD.flatten_uv_tensor(uv_a_masked_long, image_width).squeeze(1)\n        masked_non_matches_b = SD.flatten_uv_tensor(uv_b_masked_non_matches_long, image_width).squeeze(1)\n\n\n        # Non-masked non-matches\n        uv_a_background_long, uv_b_background_non_matches_long = self.create_non_matches(uv_a, uv_b_background_non_matches,\n                                                                            self.num_background_non_matches_per_match)\n\n        background_non_matches_a = SD.flatten_uv_tensor(uv_a_background_long, image_width).squeeze(1)\n        background_non_matches_b = SD.flatten_uv_tensor(uv_b_background_non_matches_long, image_width).squeeze(1)\n\n\n        # make blind non matches\n        matches_a_mask = SD.mask_image_from_uv_flat_tensor(matches_a, image_width, image_height)\n        image_a_mask_torch = torch.from_numpy(np.asarray(image_a_mask)).long()\n        mask_a_flat = image_a_mask_torch.view(-1,1).squeeze(1)\n        blind_non_matches_a = (mask_a_flat - matches_a_mask).nonzero()\n\n        no_blind_matches_found = False\n        if len(blind_non_matches_a) == 0:\n            no_blind_matches_found = True\n        else:\n\n            blind_non_matches_a = blind_non_matches_a.squeeze(1)\n            num_blind_samples = blind_non_matches_a.size()[0]\n\n            if num_blind_samples > 0:\n                # blind_uv_b is a tuple of torch.LongTensor\n                # make sure we check that blind_uv_b is not None and that it is non-empty\n\n\n                blind_uv_b = correspondence_finder.random_sample_from_masked_image_torch(image_b_mask_torch, num_blind_samples)\n\n                if blind_uv_b[0] is None:\n                    no_blind_matches_found = True\n                elif len(blind_uv_b[0]) == 0:\n                    no_blind_matches_found = True\n                else:\n                    blind_non_matches_b = utils.uv_to_flattened_pixel_locations(blind_uv_b, image_width)\n\n                    if len(blind_non_matches_b) == 0:\n                        no_blind_matches_found = True\n            else:\n                no_blind_matches_found = True\n\n        if no_blind_matches_found:\n            blind_non_matches_a = blind_non_matches_b = SD.empty_tensor()\n\n\n        if self.debug:\n            # downsample so can plot\n            num_matches_to_plot = 10\n            plot_uv_a, plot_uv_b = SD.subsample_tuple_pair(uv_a, uv_b, num_samples=num_matches_to_plot)\n\n            plot_uv_a_masked_long, plot_uv_b_masked_non_matches_long = SD.subsample_tuple_pair(uv_a_masked_long, uv_b_masked_non_matches_long, num_samples=num_matches_to_plot*3)\n\n            plot_uv_a_background_long, plot_uv_b_background_non_matches_long = SD.subsample_tuple_pair(uv_a_background_long, uv_b_background_non_matches_long, num_samples=num_matches_to_plot*3)\n\n            blind_uv_a = utils.flattened_pixel_locations_to_u_v(blind_non_matches_a, image_width)\n            plot_blind_uv_a, plot_blind_uv_b = SD.subsample_tuple_pair(blind_uv_a, blind_uv_b, num_samples=num_matches_to_plot*10)\n\n\n        if self.debug:\n            # only want to bring in plotting code if in debug mode\n            import dense_correspondence.correspondence_tools.correspondence_plotter as correspondence_plotter\n\n            # Show correspondences\n            if uv_a is not None:\n                fig, axes = correspondence_plotter.plot_correspondences_direct(image_a_rgb_PIL, image_a_depth_numpy,\n                                                                               image_b_rgb_PIL, image_b_depth_numpy,\n                                                                               plot_uv_a, plot_uv_b, show=False)\n\n                correspondence_plotter.plot_correspondences_direct(image_a_rgb_PIL, image_a_depth_numpy,\n                                                                   image_b_rgb_PIL, image_b_depth_numpy,\n                                                                   plot_uv_a_masked_long, plot_uv_b_masked_non_matches_long,\n                                                                   use_previous_plot=(fig, axes),\n                                                                   circ_color=\'r\')\n\n                fig, axes = correspondence_plotter.plot_correspondences_direct(image_a_rgb_PIL, image_a_depth_numpy,\n                                                                               image_b_rgb_PIL, image_b_depth_numpy,\n                                                                               plot_uv_a, plot_uv_b, show=False)\n\n                correspondence_plotter.plot_correspondences_direct(image_a_rgb_PIL, image_a_depth_numpy,\n                                                                   image_b_rgb_PIL, image_b_depth_numpy,\n                                                                   plot_uv_a_background_long, plot_uv_b_background_non_matches_long,\n                                                                   use_previous_plot=(fig, axes),\n                                                                   circ_color=\'b\')\n\n\n                correspondence_plotter.plot_correspondences_direct(image_a_rgb_PIL, image_a_depth_numpy,\n                                                                   image_b_rgb_PIL, image_b_depth_numpy,\n                                                                   plot_blind_uv_a, plot_blind_uv_b,\n                                                                   circ_color=\'k\', show=True)\n\n                # Mask-plotting city\n                import matplotlib.pyplot as plt\n                plt.imshow(np.asarray(image_a_mask))\n                plt.title(""Mask of img a object pixels"")\n                plt.show()\n\n                plt.imshow(np.asarray(image_a_mask) - 1)\n                plt.title(""Mask of img a background"")\n                plt.show()\n\n                temp = matches_a_mask.view(image_height, -1)\n                plt.imshow(temp)\n                plt.title(""Mask of img a object pixels for which there was a match"")\n                plt.show()\n\n                temp2 = (mask_a_flat - matches_a_mask).view(image_height, -1)\n                plt.imshow(temp2)\n                plt.title(""Mask of img a object pixels for which there was NO match"")\n                plt.show()\n\n\n\n        return metadata[""type""], image_a_rgb, image_b_rgb, matches_a, matches_b, masked_non_matches_a, masked_non_matches_b, background_non_matches_a, background_non_matches_b, blind_non_matches_a, blind_non_matches_b, metadata\n\n    def create_non_matches(self, uv_a, uv_b_non_matches, multiplier):\n        """"""\n        Simple wrapper for repeated code\n        :param uv_a:\n        :type uv_a:\n        :param uv_b_non_matches:\n        :type uv_b_non_matches:\n        :param multiplier:\n        :type multiplier:\n        :return:\n        :rtype:\n        """"""\n        uv_a_long = (torch.t(uv_a[0].repeat(multiplier, 1)).contiguous().view(-1, 1),\n                     torch.t(uv_a[1].repeat(multiplier, 1)).contiguous().view(-1, 1))\n\n        uv_b_non_matches_long = (uv_b_non_matches[0].view(-1, 1), uv_b_non_matches[1].view(-1, 1))\n\n        return uv_a_long, uv_b_non_matches_long\n\n    def get_single_object_across_scene_data(self):\n        """"""\n        Simple wrapper for get_across_scene_data(), for the single object case\n        """"""\n        metadata = dict()\n        object_id = self.get_random_object_id()\n        scene_name_a = self.get_random_single_object_scene_name(object_id)\n        scene_name_b = self.get_different_scene_for_object(object_id, scene_name_a)\n        metadata[""object_id""] = object_id\n        metadata[""scene_name_a""] = scene_name_a\n        metadata[""scene_name_b""] = scene_name_b\n        metadata[""type""] = SpartanDatasetDataType.SINGLE_OBJECT_ACROSS_SCENE\n        return self.get_across_scene_data(scene_name_a, scene_name_b, metadata)\n\n    def get_different_object_data(self):\n        """"""\n        Simple wrapper for get_across_scene_data(), for the different object case\n        """"""\n        metadata = dict()\n        object_id_a, object_id_b = self.get_two_different_object_ids()\n        scene_name_a = self.get_random_single_object_scene_name(object_id_a)\n        scene_name_b = self.get_random_single_object_scene_name(object_id_b)\n\n        metadata[""object_id_a""]  = object_id_a\n        metadata[""scene_name_a""] = scene_name_a\n        metadata[""object_id_b""]  = object_id_b\n        metadata[""scene_name_b""] = scene_name_b\n        metadata[""type""] = SpartanDatasetDataType.DIFFERENT_OBJECT\n        return self.get_across_scene_data(scene_name_a, scene_name_b, metadata)\n\n    def get_synthetic_multi_object_within_scene_data(self):\n        """"""\n        Synthetic case\n        """"""\n\n        object_id_a, object_id_b = self.get_two_different_object_ids()\n        scene_name_a = self.get_random_single_object_scene_name(object_id_a)\n        scene_name_b = self.get_random_single_object_scene_name(object_id_b)\n\n        metadata = dict()\n        metadata[""object_id_a""]  = object_id_a\n        metadata[""scene_name_a""] = scene_name_a\n        metadata[""object_id_b""]  = object_id_b\n        metadata[""scene_name_b""] = scene_name_b\n        metadata[""type""] = SpartanDatasetDataType.SYNTHETIC_MULTI_OBJECT\n\n        image_a1_rgb, image_a2_rgb, image_a1_depth, image_a2_depth,\\\n        image_a1_mask, image_a2_mask, uv_a1, uv_a2 =\\\n         self.get_within_scene_data(scene_name_a, metadata, for_synthetic_multi_object=True)\n\n        if uv_a1 is None:\n            logging.info(""no matches found, returning"")\n            image_a1_rgb_tensor = self.rgb_image_to_tensor(image_a1_rgb)\n            return self.return_empty_data(image_a1_rgb_tensor, image_a1_rgb_tensor)\n\n        image_b1_rgb, image_b2_rgb, image_b1_depth, image_b2_depth,\\\n        image_b1_mask, image_b2_mask, uv_b1, uv_b2 =\\\n         self.get_within_scene_data(scene_name_b, metadata, for_synthetic_multi_object=True)\n\n        if uv_b1 is None:\n            logging.info(""no matches found, returning"")\n            image_b1_rgb_tensor = self.rgb_image_to_tensor(image_b1_rgb)\n            return self.return_empty_data(image_b1_rgb_tensor, image_b1_rgb_tensor)\n\n        uv_a1 = (uv_a1[0].long(), uv_a1[1].long())\n        uv_a2 = (uv_a2[0].long(), uv_a2[1].long())\n        uv_b1 = (uv_b1[0].long(), uv_b1[1].long())\n        uv_b2 = (uv_b2[0].long(), uv_b2[1].long())\n\n        matches_pair_a = (uv_a1, uv_a2)\n        matches_pair_b = (uv_b1, uv_b2)\n        merged_rgb_1, merged_mask_1, uv_a1, uv_a2, uv_b1, uv_b2 =\\\n         correspondence_augmentation.merge_images_with_occlusions(image_a1_rgb, image_b1_rgb,\n                                                                  image_a1_mask, image_b1_mask,\n                                                                  matches_pair_a, matches_pair_b)\n\n        if (uv_a1 is None) or (uv_a2 is None) or (uv_b1 is None) or (uv_b2 is None):\n            logging.info(""something got fully occluded, returning"")\n            image_b1_rgb_tensor = self.rgb_image_to_tensor(image_b1_rgb)\n            return self.return_empty_data(image_b1_rgb_tensor, image_b1_rgb_tensor)\n\n        matches_pair_a = (uv_a2, uv_a1)\n        matches_pair_b = (uv_b2, uv_b1)\n        merged_rgb_2, merged_mask_2, uv_a2, uv_a1, uv_b2, uv_b1 =\\\n         correspondence_augmentation.merge_images_with_occlusions(image_a2_rgb, image_b2_rgb,\n                                                                  image_a2_mask, image_b2_mask,\n                                                                  matches_pair_a, matches_pair_b)\n\n        if (uv_a1 is None) or (uv_a2 is None) or (uv_b1 is None) or (uv_b2 is None):\n            logging.info(""something got fully occluded, returning"")\n            image_b1_rgb_tensor = self.rgb_image_to_tensor(image_b1_rgb)\n            return self.return_empty_data(image_b1_rgb_tensor, image_b1_rgb_tensor)\n\n        matches_1 = correspondence_augmentation.merge_matches(uv_a1, uv_b1)\n        matches_2 = correspondence_augmentation.merge_matches(uv_a2, uv_b2)\n        matches_2 = (matches_2[0].float(), matches_2[1].float())\n\n        # find non_correspondences\n        merged_mask_2_torch = torch.from_numpy(merged_mask_2).type(torch.FloatTensor)\n        image_b_shape = merged_mask_2_torch.shape\n        image_width = image_b_shape[1]\n        image_height = image_b_shape[0]\n\n        matches_2_masked_non_matches = \\\n            correspondence_finder.create_non_correspondences(matches_2,\n                                                             image_b_shape,\n                                                             num_non_matches_per_match=self.num_masked_non_matches_per_match,\n                                                                            img_b_mask=merged_mask_2_torch)\n        if self._use_image_b_mask_inv:\n            merged_mask_2_torch_inv = 1 - merged_mask_2_torch\n        else:\n            merged_mask_2_torch_inv = None\n\n        matches_2_background_non_matches = correspondence_finder.create_non_correspondences(matches_2,\n                                                                            image_b_shape,\n                                                                            num_non_matches_per_match=self.num_background_non_matches_per_match,\n                                                                            img_b_mask=merged_mask_2_torch_inv)\n\n\n        SD = SpartanDataset\n        # convert PIL.Image to torch.FloatTensor\n        merged_rgb_1_PIL = merged_rgb_1\n        merged_rgb_2_PIL = merged_rgb_2\n        merged_rgb_1 = self.rgb_image_to_tensor(merged_rgb_1)\n        merged_rgb_2 = self.rgb_image_to_tensor(merged_rgb_2)\n\n        matches_a = SD.flatten_uv_tensor(matches_1, image_width)\n        matches_b = SD.flatten_uv_tensor(matches_2, image_width)\n\n        # Masked non-matches\n        uv_a_masked_long, uv_b_masked_non_matches_long = self.create_non_matches(matches_1, matches_2_masked_non_matches, self.num_masked_non_matches_per_match)\n\n        masked_non_matches_a = SD.flatten_uv_tensor(uv_a_masked_long, image_width).squeeze(1)\n        masked_non_matches_b = SD.flatten_uv_tensor(uv_b_masked_non_matches_long, image_width).squeeze(1)\n\n        # Non-masked non-matches\n        uv_a_background_long, uv_b_background_non_matches_long = self.create_non_matches(matches_1, matches_2_background_non_matches,\n                                                                            self.num_background_non_matches_per_match)\n\n        background_non_matches_a = SD.flatten_uv_tensor(uv_a_background_long, image_width).squeeze(1)\n        background_non_matches_b = SD.flatten_uv_tensor(uv_b_background_non_matches_long, image_width).squeeze(1)\n\n\n        if self.debug:\n            import dense_correspondence.correspondence_tools.correspondence_plotter as correspondence_plotter\n            num_matches_to_plot = 10\n\n            print ""PRE-MERGING""\n            plot_uv_a1, plot_uv_a2 = SpartanDataset.subsample_tuple_pair(uv_a1, uv_a2, num_samples=num_matches_to_plot)\n\n            # correspondence_plotter.plot_correspondences_direct(image_a1_rgb, np.asarray(image_a1_depth),\n            #                                                        image_a2_rgb, np.asarray(image_a2_depth),\n            #                                                        plot_uv_a1, plot_uv_a2,\n            #                                                        circ_color=\'g\', show=True)\n\n            plot_uv_b1, plot_uv_b2 = SpartanDataset.subsample_tuple_pair(uv_b1, uv_b2, num_samples=num_matches_to_plot)\n\n            # correspondence_plotter.plot_correspondences_direct(image_b1_rgb, np.asarray(image_b1_depth),\n            #                                                        image_b2_rgb, np.asarray(image_b2_depth),\n            #                                                        plot_uv_b1, plot_uv_b2,\n            #                                                        circ_color=\'g\', show=True)\n\n            print ""MERGED""\n            plot_uv_1, plot_uv_2 = SpartanDataset.subsample_tuple_pair(matches_1, matches_2, num_samples=num_matches_to_plot)\n            plot_uv_a_masked_long, plot_uv_b_masked_non_matches_long =\\\n                SpartanDataset.subsample_tuple_pair(uv_a_masked_long, uv_b_masked_non_matches_long, num_samples=num_matches_to_plot)\n\n            plot_uv_a_background_long, plot_uv_b_background_non_matches_long =\\\n                SpartanDataset.subsample_tuple_pair(uv_a_background_long, uv_b_background_non_matches_long, num_samples=num_matches_to_plot)\n\n            fig, axes = correspondence_plotter.plot_correspondences_direct(merged_rgb_1_PIL, np.asarray(image_b1_depth),\n                                                                   merged_rgb_2_PIL, np.asarray(image_b2_depth),\n                                                                   plot_uv_1, plot_uv_2,\n                                                                   circ_color=\'g\', show=False)\n\n            correspondence_plotter.plot_correspondences_direct(merged_rgb_1_PIL, np.asarray(image_b1_depth),\n                                                               merged_rgb_2_PIL, np.asarray(image_b2_depth),\n                                                               plot_uv_a_masked_long, plot_uv_b_masked_non_matches_long,\n                                                               use_previous_plot=(fig, axes),\n                                                               circ_color=\'r\', show=True)\n\n            fig, axes = correspondence_plotter.plot_correspondences_direct(merged_rgb_1_PIL, np.asarray(image_b1_depth),\n                                                                   merged_rgb_2_PIL, np.asarray(image_b2_depth),\n                                                                   plot_uv_1, plot_uv_2,\n                                                                   circ_color=\'g\', show=False)\n\n            correspondence_plotter.plot_correspondences_direct(merged_rgb_1_PIL, np.asarray(image_b1_depth),\n                                                               merged_rgb_2_PIL, np.asarray(image_b2_depth),\n                                                               plot_uv_a_background_long, plot_uv_b_background_non_matches_long,\n                                                               use_previous_plot=(fig, axes),\n                                                               circ_color=\'b\')\n\n\n        return metadata[""type""], merged_rgb_1, merged_rgb_2, matches_a, matches_b, masked_non_matches_a, masked_non_matches_b, background_non_matches_a, background_non_matches_b, SD.empty_tensor(), SD.empty_tensor(), metadata\n\n\n    def get_across_scene_data(self, scene_name_a, scene_name_b, metadata):\n        """"""\n        Essentially just returns a bunch of samples off the masks from scene_name_a, and scene_name_b.\n\n        Since this data is across scene, we can\'t generate matches.\n\n        Return args are for returning directly from __getitem__\n\n        See get_within_scene_data() for documentation of return args.\n\n        :param scene_name_a, scene_name_b: Names of scenes from which to each randomly sample an image\n        :type scene_name_a, scene_name_b: strings\n        :param metadata: a dict() holding metadata of the image pair, both for logging and for different downstream loss functions\n        :type metadata: dict()\n        """"""\n\n        SD = SpartanDataset\n\n        if self.get_number_of_unique_single_objects() == 0:\n            raise ValueError(""There are no single object scenes in this dataset"")\n\n        image_a_idx = self.get_random_image_index(scene_name_a)\n        image_a_rgb, image_a_depth, image_a_mask, image_a_pose = self.get_rgbd_mask_pose(scene_name_a, image_a_idx)\n\n        metadata[\'image_a_idx\'] = image_a_idx\n\n        # image b\n        image_b_idx = self.get_random_image_index(scene_name_b)\n        image_b_rgb, image_b_depth, image_b_mask, image_b_pose = self.get_rgbd_mask_pose(scene_name_b, image_b_idx)\n        metadata[\'image_b_idx\'] = image_b_idx\n\n        # sample random indices from mask in image a\n        num_samples = self.cross_scene_num_samples\n        blind_uv_a = correspondence_finder.random_sample_from_masked_image_torch(np.asarray(image_a_mask), num_samples)\n        # sample random indices from mask in image b\n        blind_uv_b = correspondence_finder.random_sample_from_masked_image_torch(np.asarray(image_b_mask), num_samples)\n\n        if (blind_uv_a[0] is None) or (blind_uv_b[0] is None):\n            image_a_rgb_tensor = self.rgb_image_to_tensor(image_a_rgb)\n            return self.return_empty_data(image_a_rgb_tensor, image_a_rgb_tensor)\n\n        # data augmentation\n        if self._domain_randomize:\n            image_a_rgb = correspondence_augmentation.random_domain_randomize_background(image_a_rgb, image_a_mask)\n            image_b_rgb = correspondence_augmentation.random_domain_randomize_background(image_b_rgb, image_b_mask)\n\n        if not self.debug:\n            [image_a_rgb, image_a_mask], blind_uv_a = correspondence_augmentation.random_image_and_indices_mutation([image_a_rgb, image_a_mask], blind_uv_a)\n            [image_b_rgb, image_b_mask], blind_uv_b = correspondence_augmentation.random_image_and_indices_mutation(\n                [image_b_rgb, image_b_mask], blind_uv_b)\n        else:  # also mutate depth just for plotting\n            [image_a_rgb, image_a_depth, image_a_mask], blind_uv_a = correspondence_augmentation.random_image_and_indices_mutation(\n                [image_a_rgb, image_a_depth, image_a_mask], blind_uv_a)\n            [image_b_rgb, image_b_depth, image_b_mask], blind_uv_b = correspondence_augmentation.random_image_and_indices_mutation(\n                [image_b_rgb, image_b_depth, image_b_mask], blind_uv_b)\n\n        image_a_depth_numpy = np.asarray(image_a_depth)\n        image_b_depth_numpy = np.asarray(image_b_depth)\n\n        image_b_shape = image_b_depth_numpy.shape\n        image_width = image_b_shape[1]\n        image_height = image_b_shape[0]\n\n        blind_uv_a_flat = SD.flatten_uv_tensor(blind_uv_a, image_width)\n        blind_uv_b_flat = SD.flatten_uv_tensor(blind_uv_b, image_width)\n\n        # convert PIL.Image to torch.FloatTensor\n        image_a_rgb_PIL = image_a_rgb\n        image_b_rgb_PIL = image_b_rgb\n        image_a_rgb = self.rgb_image_to_tensor(image_a_rgb)\n        image_b_rgb = self.rgb_image_to_tensor(image_b_rgb)\n\n        empty_tensor = SD.empty_tensor()\n\n        if self.debug and ((blind_uv_a[0] is not None) and (blind_uv_b[0] is not None)):\n            import dense_correspondence.correspondence_tools.correspondence_plotter as correspondence_plotter\n            num_matches_to_plot = 10\n\n            plot_blind_uv_a, plot_blind_uv_b = SD.subsample_tuple_pair(blind_uv_a, blind_uv_b, num_samples=num_matches_to_plot*10)\n\n            correspondence_plotter.plot_correspondences_direct(image_a_rgb_PIL, image_a_depth_numpy,\n                                                                   image_b_rgb_PIL, image_b_depth_numpy,\n                                                                   plot_blind_uv_a, plot_blind_uv_b,\n                                                                   circ_color=\'k\', show=True)\n\n        return metadata[""type""], image_a_rgb, image_b_rgb, empty_tensor, empty_tensor, empty_tensor, empty_tensor, empty_tensor, empty_tensor, blind_uv_a_flat, blind_uv_b_flat, metadata\n\n    def get_image_mean(self):\n        """"""\n        Returns dataset image_mean\n        :return: list\n        :rtype:\n        """"""\n\n        # if ""image_normalization"" not in self.config:\n        #     return constants.DEFAULT_IMAGE_MEAN\n\n        # return self.config[""image_normalization""][""mean""]\n\n\n        return constants.DEFAULT_IMAGE_MEAN\n\n    def get_image_std_dev(self):\n        """"""\n        Returns dataset image std_dev\n        :return: list\n        :rtype:\n        """"""\n\n        # if ""image_normalization"" not in self.config:\n        #     return constants.DEFAULT_IMAGE_STD_DEV\n\n        # return self.config[""image_normalization""][""std_dev""]\n\n        return constants.DEFAULT_IMAGE_STD_DEV\n\n\n\n    def rgb_image_to_tensor(self, img):\n        """"""\n        Transforms a PIL.Image to a torch.FloatTensor.\n        Performs normalization of mean and std dev\n        :param img: input image\n        :type img: PIL.Image\n        :return:\n        :rtype:\n        """"""\n\n        return self._rgb_image_to_tensor(img)\n\n    def get_first_image_index(self, scene_name):\n        """"""\n        Gets the image index for the ""first"" image in that scene.\n        Correctly handles the case where we did a close-up data collection\n        :param scene_name:\n        :type scene_name: string\n        :return: index of first image in scene\n        :rtype: int\n        """"""\n        full_path_for_scene = self.get_full_path_for_scene(scene_name)\n\n        ss = SceneStructure(full_path_for_scene)\n        metadata_file = ss.metadata_file\n        first_image_index = None\n        if os.path.exists(metadata_file):\n            metadata = utils.getDictFromYamlFilename(metadata_file)\n            if len(metadata[\'close_up_image_indices\']) > 0:\n                first_image_index = min(metadata[\'close_up_image_indices\'])\n            else:\n                first_image_index = min(metadata[\'normal_image_indices\'])\n        else:\n            pose_data = self.get_pose_data(scene_name)\n            first_image_index = min(pose_data.keys())\n\n        return first_image_index\n\n    @property\n    def config(self):\n        return self._config\n    \n    @staticmethod\n    def merge_single_object_configs(config_list):\n        """"""\n        Given a list of single object configs, merge them. This basically concatenates\n        all the fields (\'train\', \'test\', \'logs_root_path\')\n\n        Asserts that \'object_id\' is the same for all of the configs\n        Asserts that `logs_root_path` is the same for all the configs\n        :param config_list:\n        :type config_list:\n        :return: single object config\n        :rtype: dict\n        """"""\n        config = config_list[0]\n        logs_root_path = config[\'logs_root_path\']\n        object_id = config[\'object_id\']\n\n        train_scenes = []\n        test_scenes = []\n        evaluation_labeled_data_path = []\n\n        for config in config_list:\n            assert (config[\'object_id\'] == object_id)\n            assert (config[\'logs_root_path\'] == logs_root_path)\n            train_scenes += config[\'train\']\n            test_scenes += config[\'test\']\n            evaluation_labeled_data_path += config[\'evaluation_labeled_data_path\']\n\n\n\n        merged_config = dict()\n        merged_config[\'logs_root_path\'] = logs_root_path\n        merged_config[\'object_id\'] = object_id\n        merged_config[\'train\'] = train_scenes\n        merged_config[\'test\'] = test_scenes\n        merged_config[\'evaluation_labeled_data_path\'] = evaluation_labeled_data_path\n\n        return merged_config\n\n    @staticmethod\n    def flatten_uv_tensor(uv_tensor, image_width):\n        """"""\n        Flattens a uv_tensor to single dimensional tensor\n        :param uv_tensor:\n        :type uv_tensor:\n        :return:\n        :rtype:\n        """"""\n        return uv_tensor[1].long() * image_width + uv_tensor[0].long()\n\n    @staticmethod\n    def mask_image_from_uv_flat_tensor(uv_flat_tensor, image_width, image_height):\n        """"""\n        Returns a torch.LongTensor with shape [image_width*image_height]. It has a 1 exactly\n        at the indices specified by uv_flat_tensor\n        :param uv_flat_tensor:\n        :type uv_flat_tensor:\n        :param image_width:\n        :type image_width:\n        :param image_height:\n        :type image_height:\n        :return:\n        :rtype:\n        """"""\n        image_flat = torch.zeros(image_width*image_height).long()\n        image_flat[uv_flat_tensor] = 1\n        return image_flat\n\n\n    @staticmethod\n    def subsample_tuple(uv, num_samples):\n        """"""\n        Subsamples a tuple of (torch.Tensor, torch.Tensor)\n        """"""\n        indexes_to_keep = (torch.rand(num_samples) * len(uv[0])).floor().type(torch.LongTensor)\n        return (torch.index_select(uv[0], 0, indexes_to_keep), torch.index_select(uv[1], 0, indexes_to_keep))\n\n    @staticmethod\n    def subsample_tuple_pair(uv_a, uv_b, num_samples):\n        """"""\n        Subsamples a pair of tuples, i.e. (torch.Tensor, torch.Tensor), (torch.Tensor, torch.Tensor)\n        """"""\n        assert len(uv_a[0]) == len(uv_b[0])\n        indexes_to_keep = (torch.rand(num_samples) * len(uv_a[0])).floor().type(torch.LongTensor)\n        uv_a_downsampled = (torch.index_select(uv_a[0], 0, indexes_to_keep), torch.index_select(uv_a[1], 0, indexes_to_keep))\n        uv_b_downsampled = (torch.index_select(uv_b[0], 0, indexes_to_keep), torch.index_select(uv_b[1], 0, indexes_to_keep))\n        return uv_a_downsampled, uv_b_downsampled\n\n\n    @staticmethod\n    def make_default_10_scenes_drill():\n        """"""\n        Makes a default SpartanDatase from the 10_scenes_drill data\n        :return:\n        :rtype:\n        """"""\n        config_file = os.path.join(utils.getDenseCorrespondenceSourceDir(), \'config\', \'dense_correspondence\',\n                                   \'dataset\',\n                                   \'10_drill_scenes.yaml\')\n\n        config = utils.getDictFromYamlFilename(config_file)\n        dataset = SpartanDataset(mode=""train"", config=config)\n        return dataset\n\n    @staticmethod\n    def make_default_caterpillar():\n        """"""\n        Makes a default SpartanDatase from the 10_scenes_drill data\n        :return:\n        :rtype:\n        """"""\n        config_file = os.path.join(utils.getDenseCorrespondenceSourceDir(), \'config\', \'dense_correspondence\',\n                                   \'dataset\', \'composite\',\n                                   \'caterpillar_only.yaml\')\n\n        config = utils.getDictFromYamlFilename(config_file)\n        dataset = SpartanDataset(mode=""train"", config=config)\n        return dataset\n'"
dense_correspondence/evaluation/__init__.py,0,b''
dense_correspondence/evaluation/evaluation.py,16,"b'#!/usr/bin/python\n\n\nimport os\nimport dense_correspondence_manipulation.utils.utils as utils\nimport logging\nutils.add_dense_correspondence_to_python_path()\nimport matplotlib.pyplot as plt\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport random\nimport scipy.stats as ss\nimport itertools\n\nimport torch\nfrom torch.autograd import Variable\nfrom torchvision import transforms\n\n\nfrom dense_correspondence_manipulation.utils.constants import *\nfrom dense_correspondence_manipulation.utils.utils import CameraIntrinsics\nfrom dense_correspondence.dataset.spartan_dataset_masked import SpartanDataset\nimport dense_correspondence.correspondence_tools.correspondence_plotter as correspondence_plotter\nimport dense_correspondence.correspondence_tools.correspondence_finder as correspondence_finder\nfrom dense_correspondence.network.dense_correspondence_network import DenseCorrespondenceNetwork\nfrom dense_correspondence.loss_functions.pixelwise_contrastive_loss import PixelwiseContrastiveLoss\nimport dense_correspondence_manipulation.utils.visualization as vis_utils\n\nimport dense_correspondence.evaluation.plotting as dc_plotting\n\nfrom dense_correspondence.correspondence_tools.correspondence_finder import random_sample_from_masked_image\n\nfrom dense_correspondence.evaluation.utils import PandaDataFrameWrapper\n\n# why don\'t we have scene_name\nclass DCNEvaluationPandaTemplate(PandaDataFrameWrapper):\n    columns = [\'scene_name\',\n               \'scene_name_a\',\n               \'scene_name_b\',\n               \'object_id_a\',\n               \'object_id_b\',\n                \'img_a_idx\',\n                \'img_b_idx\',\n                \'is_valid\',\n                \'is_valid_masked\',\n                \'norm_diff_descriptor_ground_truth\',\n                \'norm_diff_descriptor\',\n                \'norm_diff_descriptor_masked\',\n                \'norm_diff_ground_truth_3d\',\n                \'norm_diff_pred_3d\',\n                \'norm_diff_pred_3d_masked\',\n                \'pixel_match_error_l2\',\n                \'pixel_match_error_l2_masked\',\n                \'pixel_match_error_l1\',\n                \'fraction_pixels_closer_than_ground_truth\',\n                \'fraction_pixels_closer_than_ground_truth_masked\',\n                \'average_l2_distance_for_false_positives\',\n                \'average_l2_distance_for_false_positives_masked\',\n               \'keypoint_name\' # (optional) name of the keypoint\n               ]\n\n    def __init__(self):\n        PandaDataFrameWrapper.__init__(self, DCNEvaluationPandaTemplate.columns)\n\nclass DCNEvaluationPandaTemplateAcrossObject(PandaDataFrameWrapper):\n    columns = [\'scene_name_a\',\n            \'scene_name_b\',\n            \'img_a_idx\',\n            \'img_b_idx\',\n            \'object_id_a\',\n            \'object_id_b\',\n            \'norm_diff_descriptor_best_match\']\n\n    def __init__(self):\n        PandaDataFrameWrapper.__init__(self, DCNEvaluationPandaTemplateAcrossObject.columns)\n\nclass SIFTKeypointMatchPandaTemplate(PandaDataFrameWrapper):\n    columns = [\'scene_name\',\n               \'img_a_idx\',\n               \'img_b_idx\',\n               \'is_valid\',\n               \'norm_diff_pred_3d\']\n\n    def __init__(self):\n        PandaDataFrameWrapper.__init__(self, SIFTKeypointMatchPandaTemplate.columns)\n\n\nclass DenseCorrespondenceEvaluation(object):\n    """"""\n    Samples image pairs from the given scenes. Then uses the network to compute dense\n    descriptors. Records the results of this in a Pandas.DataFrame object.\n    """"""\n\n\n    def __init__(self, config):\n        self._config = config\n        self._dataset = None\n\n\n    @property\n    def config(self):\n        return self._configs\n\n    def load_network_from_config(self, name):\n        """"""\n        Loads a network from config file. Puts it in eval mode by default\n        :param name:\n        :type name:\n        :return: DenseCorrespondenceNetwork\n        :rtype:\n        """"""\n        if name not in self._config[""networks""]:\n            raise ValueError(""Network %s is not in config file"" %(name))\n\n\n        path_to_network_params = self._config[""networks""][name][""path_to_network_params""]\n        path_to_network_params = utils.convert_data_relative_path_to_absolute_path(path_to_network_params, assert_path_exists=True)\n        model_folder = os.path.dirname(path_to_network_params)\n\n        dcn = DenseCorrespondenceNetwork.from_model_folder(model_folder, model_param_file=path_to_network_params)\n        dcn.eval()\n        return dcn\n\n    def load_dataset_for_network(self, network_name):\n        """"""\n        Loads a dataset for the network specified in the config file\n        :param network_name: string\n        :type network_name:\n        :return: SpartanDataset\n        :rtype:\n        """"""\n        if network_name not in self._config[""networks""]:\n            raise ValueError(""Network %s is not in config file"" %(network_name))\n\n        network_folder = os.path.dirname(self._config[""networks""][network_name][""path_to_network_params""])\n        network_folder = utils.convert_data_relative_path_to_absolute_path(network_folder, assert_path_exists=True)\n        dataset_config = utils.getDictFromYamlFilename(os.path.join(network_folder, ""dataset.yaml""))\n\n        dataset = SpartanDataset(config=dataset_config)\n        return dataset\n\n\n    def load_dataset(self):\n        """"""\n        Loads a SpartanDatasetMasked object\n        For now we use a default one\n        :return:\n        :rtype: SpartanDatasetMasked\n        """"""\n\n        config_file = os.path.join(utils.getDenseCorrespondenceSourceDir(), \'config\', \'dense_correspondence\', \'dataset\',\n                                   \'spartan_dataset_masked.yaml\')\n\n        config = utils.getDictFromYamlFilename(config_file)\n\n        dataset = SpartanDataset(mode=""test"", config=config)\n\n        return dataset\n\n    @property\n    def dataset(self):\n        if self._dataset is None:\n            self._dataset = self.load_dataset()\n        return self._dataset\n\n    @dataset.setter\n    def dataset(self, value):\n        self._dataset = value\n\n    def get_output_dir(self):\n        return utils.convert_data_relative_path_to_absolute_path(self._config[\'output_dir\'])\n\n    @staticmethod\n    def get_image_pair_with_poses_diff_above_threshold(dataset, scene_name, threshold=0.05,\n                                                       max_num_attempts=100):\n        """"""\n        Given a dataset and scene name find a random pair of images with\n        poses that are different above a threshold\n        :param dataset:\n        :type dataset:\n        :param scene_name:\n        :type scene_name:\n        :param threshold:\n        :type threshold:\n        :param max_num_attempts:\n        :type max_num_attempts:\n        :return:\n        :rtype:\n        """"""\n        img_a_idx = dataset.get_random_image_index(scene_name)\n        pose_a = dataset.get_pose_from_scene_name_and_idx(scene_name, img_a_idx)\n        pos_a = pose_a[0:3, 3]\n\n        for i in xrange(0, max_num_attempts):\n            img_b_idx = dataset.get_random_image_index(scene_name)\n            pose_b = dataset.get_pose_from_scene_name_and_idx(scene_name, img_b_idx)\n            pos_b = pose_b[0:3, 3]\n\n            if np.linalg.norm(pos_a - pos_b) > threshold:\n                return (img_a_idx, img_b_idx)\n\n        return None\n\n\n    def evaluate_single_network(self, network_name, mode=""train"", save=True):\n        """"""\n        Evaluates a single network, this network should be in the config\n        :param network_name:\n        :type network_name:\n        :return:\n        :rtype:\n        """"""\n        DCE = DenseCorrespondenceEvaluation\n\n        dcn = self.load_network_from_config(network_name)\n        dcn.eval()\n        dataset = self.dataset\n\n        if mode == ""train"":\n            dataset.set_train_mode()\n        if mode == ""test"":\n            dataset.set_test_mode()\n\n        num_image_pairs = self._config[\'params\'][\'num_image_pairs\']\n        num_matches_per_image_pair = self._config[\'params\'][\'num_matches_per_image_pair\']\n\n        pd_dataframe_list, df = DCE.evaluate_network(dcn, dataset, num_image_pairs=num_image_pairs,\n                                                     num_matches_per_image_pair=num_matches_per_image_pair)\n\n\n        # save pandas.DataFrame to csv\n        if save:\n            output_dir = os.path.join(self.get_output_dir(), network_name, mode)\n            data_file = os.path.join(output_dir, ""data.csv"")\n            if not os.path.isdir(output_dir):\n                os.makedirs(output_dir)\n\n            df.to_csv(data_file)\n\n\n\n    def evaluate_single_network_cross_scene(self, network_name, save=True):\n        """"""\n        Simple wrapper that uses class config and then calls static method\n        """"""\n        dcn = self.load_network_from_config(network_name)\n        dcn.eval()\n        dataset = dcn.load_training_dataset()\n        DenseCorrespondenceEvaluation.evaluate_network_cross_scene(dcn, dataset, save=save)\n\n    @staticmethod\n    def evaluate_network_cross_scene(dcn=None, dataset=None, save=True):\n        """"""\n        This will search for the ""evaluation_labeled_data_path"" in the dataset.yaml,\n        and use pairs of images that have been human-labeled across scenes.\n        """"""\n\n        utils.reset_random_seed()\n\n        cross_scene_data = DenseCorrespondenceEvaluation.parse_cross_scene_data(dataset)\n\n        pd_dataframe_list = []\n        for annotated_pair in cross_scene_data:\n\n            scene_name_a = annotated_pair[""image_a""][""scene_name""]\n            scene_name_b = annotated_pair[""image_b""][""scene_name""]\n\n            if not os.path.isdir(dataset.get_full_path_for_scene(scene_name_a))\\\n            or not os.path.isdir(dataset.get_full_path_for_scene(scene_name_b)):\n                print(""at least one of these scene names does not exist:"", scene_name_a, scene_name_b)\n                continue\n\n            \n            image_a_idx = annotated_pair[""image_a""][""image_idx""]\n            image_b_idx = annotated_pair[""image_b""][""image_idx""]\n\n            img_a_pixels = annotated_pair[""image_a""][""pixels""]\n            img_b_pixels = annotated_pair[""image_b""][""pixels""]\n\n\n            dataframe_list_temp =\\\n                DenseCorrespondenceEvaluation.single_cross_scene_image_pair_quantitative_analysis(dcn,\n                dataset, scene_name_a, image_a_idx, scene_name_b, image_b_idx,\n                img_a_pixels, img_b_pixels)\n\n            assert dataframe_list_temp is not None\n\n            pd_dataframe_list += dataframe_list_temp\n\n\n        df = pd.concat(pd_dataframe_list)\n        # save pandas.DataFrame to csv\n        if save:\n            output_dir = os.path.join(self.get_output_dir(), network_name, ""cross-scene"")\n            data_file = os.path.join(output_dir, ""data.csv"")\n            if not os.path.isdir(output_dir):\n                os.makedirs(output_dir)\n\n            df.to_csv(data_file)\n        return df\n\n\n    @staticmethod\n    def evaluate_network_across_objects(dcn=None, dataset=None, num_image_pairs=25):\n        """"""\n        This grabs different objects and computes a small set of statistics on their distribution.\n        """"""\n\n        utils.reset_random_seed()\n\n        pd_dataframe_list = []\n        for i in xrange(num_image_pairs):\n\n            object_id_a, object_id_b = dataset.get_two_different_object_ids()\n            scene_name_a = dataset.get_random_single_object_scene_name(object_id_a)\n            scene_name_b = dataset.get_random_single_object_scene_name(object_id_b)\n\n            image_a_idx = dataset.get_random_image_index(scene_name_a)\n            image_b_idx = dataset.get_random_image_index(scene_name_b)\n\n            dataframe_list_temp =\\\n                DenseCorrespondenceEvaluation.single_across_object_image_pair_quantitative_analysis(dcn,\n                dataset, scene_name_a, scene_name_b, image_a_idx, image_b_idx, object_id_a, object_id_b)\n\n            # if the list is empty, don\'t bother +=ing it, just continue\n            if len(dataframe_list_temp) == 0:\n                continue\n\n            assert dataframe_list_temp is not None\n\n            pd_dataframe_list += dataframe_list_temp\n\n\n        df = pd.concat(pd_dataframe_list)\n\n        return df\n\n\n    def evaluate_single_network_cross_instance(self, network_name, full_path_cross_instance_labels, save=False):\n        """"""\n        Simple wrapper that uses class config and then calls static method\n        """"""\n        dcn = self.load_network_from_config(network_name)\n        dcn.eval()\n        dataset = dcn.load_training_dataset()\n        return DenseCorrespondenceEvaluation.evaluate_network_cross_instance(dcn, dataset, full_path_cross_instance_labels, save=False)\n\n    @staticmethod\n    def evaluate_network_cross_instance(dcn=None, dataset=None, full_path_cross_instance_labels=None, save=False):\n        """"""\n        This will grab the .yaml specified via its full path (full_path_cross_instance_labels)\n        and use globally class-consistent keypoints that have been human-labeled across instances.\n        """"""\n\n        utils.reset_random_seed()\n\n        cross_instance_keypoint_labels = utils.getDictFromYamlFilename(full_path_cross_instance_labels)\n\n        print cross_instance_keypoint_labels\n\n        # keypoints = dict()\n        # for label in cross_instance_keypoint_labels:\n        #     for keypoint_label in label[\'image\'][\'pixels\']:\n        #         if keypoint_label[\'keypoint\'] not in keypoints:\n        #             print ""Found new keypoint:"", keypoint_label[\'keypoint\']\n\n\n        pd_dataframe_list = []\n\n        # generate all pairs of images\n        import itertools\n        for subset in itertools.combinations(cross_instance_keypoint_labels, 2):\n            print(subset)\n\n            scene_name_a = subset[0][""image""][""scene_name""]\n            scene_name_b = subset[1][""image""][""scene_name""]\n\n            image_a_idx = subset[0][""image""][""image_idx""]\n            image_b_idx = subset[1][""image""][""image_idx""]\n\n            img_a_pixels = subset[0][""image""][""pixels""]\n            img_b_pixels = subset[1][""image""][""pixels""]\n\n            dataframe_list_temp =\\\n                DenseCorrespondenceEvaluation.single_cross_scene_image_pair_quantitative_analysis(dcn,\n                dataset, scene_name_a, image_a_idx, scene_name_b, image_b_idx,\n                img_a_pixels, img_b_pixels)\n\n            assert dataframe_list_temp is not None\n\n            pd_dataframe_list += dataframe_list_temp\n\n\n        df = pd.concat(pd_dataframe_list)\n        # save pandas.DataFrame to csv\n        if save:\n            output_dir = os.path.join(self.get_output_dir(), network_name, ""cross-instance"")\n            data_file = os.path.join(output_dir, ""data.csv"")\n            if not os.path.isdir(output_dir):\n                os.makedirs(output_dir)\n\n            df.to_csv(data_file)\n        return df\n\n    @staticmethod\n    def evaluate_network_cross_scene_keypoints(dcn, dataset, full_path_cross_instance_labels):\n        """"""\n        Evaluates the network on keypoint annotations across scenes\n        :param dcn:\n        :type dcn:\n        :param dataset:\n        :type dataset:\n        :param full_path_cross_instance_labels:\n        :type full_path_cross_instance_labels:\n        :return:\n        :rtype: pandas.DataFrame\n        """"""\n\n        utils.reset_random_seed()\n\n        cross_instance_keypoint_labels = utils.getDictFromYamlFilename(full_path_cross_instance_labels)\n\n        print ""num cross instance labels"", len(cross_instance_keypoint_labels)\n\n        # Two-layer dict with:\n        # - key:   the scene_name\n        # - key:   the image_idx\n        # - value: the descriptor image \n        descriptor_images = dict()\n\n        # generate all descriptor images\n        for keypoint_label in cross_instance_keypoint_labels:\n            scene_name = keypoint_label[""scene_name""]\n            image_idx = keypoint_label[""image_idx""]\n            if scene_name not in descriptor_images:\n                descriptor_images[scene_name] = dict()\n            if image_idx in descriptor_images[scene_name]:\n                continue\n            rgb, _, _, _ = dataset.get_rgbd_mask_pose(scene_name, image_idx)\n            rgb_tensor = dataset.rgb_image_to_tensor(rgb)\n            res = dcn.forward_single_image_tensor(rgb_tensor).data.cpu().numpy()\n            \n            descriptor_images[scene_name][image_idx] = res\n\n\n        pd_dataframe_list = []\n\n        # generate all pairs of images\n        counter = 0\n        for subset in itertools.combinations(cross_instance_keypoint_labels, 2):\n            counter += 1\n            keypoint_data_a = subset[0]\n            keypoint_data_b = subset[1]\n            \n            res_a = descriptor_images[keypoint_data_a[""scene_name""]][keypoint_data_a[""image_idx""]]\n            res_b = descriptor_images[keypoint_data_b[""scene_name""]][keypoint_data_b[""image_idx""]]\n\n            dataframe_list_temp = \\\n                DenseCorrespondenceEvaluation.single_image_pair_cross_scene_keypoints_quantitative_analysis(dcn, dataset, keypoint_data_a, keypoint_data_b, res_a, res_b)\n\n            if dataframe_list_temp is None:\n                print ""no matches found, skipping""\n                continue\n\n            pd_dataframe_list += dataframe_list_temp\n\n\n        print ""num_pairs considered"", counter\n        df = pd.concat(pd_dataframe_list)\n\n        return df\n\n    @staticmethod\n    def evaluate_network(dcn, dataset, num_image_pairs=25, num_matches_per_image_pair=100):\n        """"""\n\n        :param nn: A neural network DenseCorrespondenceNetwork\n        :param test_dataset: DenseCorrespondenceDataset\n            the dataset to draw samples from\n        :return:\n        """"""\n        utils.reset_random_seed()\n\n        DCE = DenseCorrespondenceEvaluation\n        dcn.eval()\n\n        logging_rate = 5\n\n\n        pd_dataframe_list = []\n        for i in xrange(0, num_image_pairs):\n\n\n            scene_name = dataset.get_random_scene_name()\n\n            # grab random scene\n            if i % logging_rate == 0:\n                print ""computing statistics for image %d of %d, scene_name %s"" %(i, num_image_pairs, scene_name)\n                print ""scene""\n\n\n            idx_pair = DCE.get_image_pair_with_poses_diff_above_threshold(dataset, scene_name)\n\n            if idx_pair is None:\n                logging.info(""no satisfactory image pair found, continuing"")\n                continue\n\n            img_idx_a, img_idx_b = idx_pair\n\n            dataframe_list_temp =\\\n                DCE.single_same_scene_image_pair_quantitative_analysis(dcn, dataset, scene_name,\n                                                            img_idx_a,\n                                                            img_idx_b,\n                                                            num_matches=num_matches_per_image_pair,\n                                                            debug=False)\n\n            if dataframe_list_temp is None:\n                print ""no matches found, skipping""\n                continue\n\n            pd_dataframe_list += dataframe_list_temp\n            # pd_series_list.append(series_list_temp)\n\n\n        df = pd.concat(pd_dataframe_list)\n        return pd_dataframe_list, df\n\n    @staticmethod\n    def plot_descriptor_colormaps(res_a, res_b, descriptor_image_stats=None,\n                                  mask_a=None, mask_b=None, plot_masked=False,descriptor_norm_type=""mask_image""):\n        """"""\n        Plots the colormaps of descriptors for a pair of images\n        :param res_a: descriptors for img_a\n        :type res_a: numpy.ndarray\n        :param res_b:\n        :type res_b: numpy.ndarray\n        :param descriptor_norm_type: what type of normalization to use for the\n        full descriptor image\n        :type : str\n        :return: None\n        :rtype: None\n        """"""\n\n        if plot_masked:\n            nrows = 2\n            ncols = 2\n        else:\n            nrows = 1\n            ncols = 2\n\n        fig, axes = plt.subplots(nrows=nrows, ncols=ncols)\n        fig.set_figheight(5)\n        fig.set_figwidth(15)\n\n        if descriptor_image_stats is None:\n            res_a_norm, res_b_norm = dc_plotting.normalize_descriptor_pair(res_a, res_b)\n        else:\n            res_a_norm = dc_plotting.normalize_descriptor(res_a, descriptor_image_stats[descriptor_norm_type])\n            res_b_norm = dc_plotting.normalize_descriptor(res_b, descriptor_image_stats[descriptor_norm_type])\n\n\n        if plot_masked:\n            ax = axes[0,0]\n        else:\n            ax = axes[0]\n\n        ax.imshow(res_a_norm)\n\n\n        if plot_masked:\n            ax = axes[0,1]\n        else:\n            ax = axes[1]\n\n        ax.imshow(res_b_norm)\n\n        if plot_masked:\n            assert mask_a is not None\n            assert mask_b is not None\n\n            fig.set_figheight(10)\n            fig.set_figwidth(15)\n\n            D = np.shape(res_a)[2]\n            mask_a_repeat = np.repeat(mask_a[:,:,np.newaxis], D, axis=2)\n            mask_b_repeat = np.repeat(mask_b[:,:,np.newaxis], D, axis=2)\n            res_a_mask = mask_a_repeat * res_a\n            res_b_mask = mask_b_repeat * res_b\n\n            if descriptor_image_stats is None:\n                res_a_norm_mask, res_b_norm_mask = dc_plotting.normalize_masked_descriptor_pair(res_a, res_b, mask_a, mask_b)\n            else:\n                res_a_norm_mask = dc_plotting.normalize_descriptor(res_a_mask, descriptor_image_stats[\'mask_image\'])\n                res_b_norm_mask = dc_plotting.normalize_descriptor(res_b_mask, descriptor_image_stats[\'mask_image\'])\n\n            res_a_norm_mask = res_a_norm_mask * mask_a_repeat\n            res_b_norm_mask = res_b_norm_mask * mask_b_repeat\n\n            axes[1,0].imshow(res_a_norm_mask)\n            axes[1,1].imshow(res_b_norm_mask)\n\n    @staticmethod\n    def clip_pixel_to_image_size_and_round(uv, image_width, image_height):\n        u = min(int(round(uv[0])), image_width - 1)\n        v = min(int(round(uv[1])), image_height - 1)\n        return (u,v)\n\n    @staticmethod\n    def single_cross_scene_image_pair_quantitative_analysis(dcn, dataset, scene_name_a,\n                                               img_a_idx, scene_name_b, img_b_idx,\n                                               img_a_pixels, img_b_pixels):\n        """"""\n        Quantitative analsys of a dcn on a pair of images from different scenes (requires human labeling).\n\n        There is a bit of code copy from single_same_scene_image_pair_quantitative_analysis, but\n        it\'s a bit of a different structure, since matches are passed in and we need to try to generate more\n        views of these sparse human labeled pixel matches.\n\n        :param dcn:\n        :type dcn: DenseCorrespondenceNetwork\n        :param dataset:\n        :type dataset: SpartanDataset\n        :param scene_name:\n        :type scene_name: str\n        :param img_a_idx:\n        :type img_a_idx: int\n        :param img_b_idx:\n        :type img_b_idx: int\n        :param img_a_pixels, img_b_pixels: lists of dicts, where each dict contains keys for ""u"" and ""v""\n                the lists should be the same length and index i from each list constitutes a match pair\n        :return: Dict with relevant data\n        :rtype:\n        """"""\n\n        rgb_a, depth_a, mask_a, pose_a = dataset.get_rgbd_mask_pose(scene_name_a, img_a_idx)\n\n        rgb_b, depth_b, mask_b, pose_b = dataset.get_rgbd_mask_pose(scene_name_b, img_b_idx)\n\n        depth_a = np.asarray(depth_a)\n        depth_b = np.asarray(depth_b)\n        mask_a = np.asarray(mask_a)\n        mask_b = np.asarray(mask_b)\n\n        # compute dense descriptors\n        rgb_a_tensor = dataset.rgb_image_to_tensor(rgb_a)\n        rgb_b_tensor = dataset.rgb_image_to_tensor(rgb_b)\n\n        # these are Variables holding torch.FloatTensors, first grab the data, then convert to numpy\n        res_a = dcn.forward_single_image_tensor(rgb_a_tensor).data.cpu().numpy()\n        res_b = dcn.forward_single_image_tensor(rgb_b_tensor).data.cpu().numpy()\n\n        camera_intrinsics_a = dataset.get_camera_intrinsics(scene_name_a)\n        camera_intrinsics_b = dataset.get_camera_intrinsics(scene_name_b)\n        if not np.allclose(camera_intrinsics_a.K, camera_intrinsics_b.K):\n            print ""Currently cannot handle two different camera K matrices in different scenes!""\n            print ""But you could add this...""\n        camera_intrinsics_matrix = camera_intrinsics_a.K\n\n        assert len(img_a_pixels) == len(img_b_pixels)\n\n        print ""Expanding amount of matches between:""\n        print ""scene_name_a"", scene_name_a\n        print ""scene_name_b"", scene_name_b\n        print ""originally had"", len(img_a_pixels), ""matches""\n\n        image_height, image_width = dcn.image_shape\n        DCE = DenseCorrespondenceEvaluation\n\n        dataframe_list = []\n\n        # Loop over the labeled pixel matches once, before using different views\n        # This lets us keep depth_a, depth_b, res_a, res_b without reloading\n        for i in range(len(img_a_pixels)):\n            print ""now, index of pixel match:"", i\n            uv_a = (img_a_pixels[i][""u""], img_a_pixels[i][""v""])\n            uv_b = (img_b_pixels[i][""u""], img_b_pixels[i][""v""])\n            uv_a = DCE.clip_pixel_to_image_size_and_round(uv_a, image_width, image_height)\n            uv_b = DCE.clip_pixel_to_image_size_and_round(uv_b, image_width, image_height)\n            print uv_a\n            print uv_b\n\n            # Reminder: this function wants only a single uv_a, uv_b\n            pd_template = DenseCorrespondenceEvaluation.compute_descriptor_match_statistics(depth_a,\n                                                        depth_b, mask_a, mask_b, uv_a, uv_b, pose_a,pose_b, res_a,\n                                                        res_b, camera_intrinsics_matrix,\n                                                        rgb_a=rgb_a, rgb_b=rgb_b, debug=False)\n\n            pd_template.set_value(\'scene_name\', scene_name_a+""+""+scene_name_b)\n            pd_template.set_value(\'img_a_idx\', int(img_a_idx))\n            pd_template.set_value(\'img_b_idx\', int(img_b_idx))\n\n            dataframe_list.append(pd_template.dataframe)\n\n        # Loop a second time over the labeled pixel matches\n        # But this time try,\n        #  for each I labeled pixel match pairs,\n        #       for each J different views for image a, and\n        #       for each K different views for image b\n        # This will lead to I*J+I*K attempts at new pairs!\n        # Could also do the cubic version...\n        J = 10\n        K = 10\n\n        # Loop over labeled pixel matches\n        for i in range(len(img_a_pixels)):\n            uv_a = (img_a_pixels[i][""u""], img_a_pixels[i][""v""])\n            uv_b = (img_b_pixels[i][""u""], img_b_pixels[i][""v""])\n            uv_a = DCE.clip_pixel_to_image_size_and_round(uv_a, image_width, image_height)\n            uv_b = DCE.clip_pixel_to_image_size_and_round(uv_b, image_width, image_height)\n\n            # Loop over J different views for image a\n            for j in range(J):\n                different_view_a_idx = dataset.get_img_idx_with_different_pose(scene_name_a, pose_a, num_attempts=50)\n                if different_view_a_idx is None:\n                    logging.info(""no frame with sufficiently different pose found, continuing"")\n                    continue\n                diff_rgb_a, diff_depth_a, diff_mask_a, diff_pose_a = dataset.get_rgbd_mask_pose(scene_name_a, different_view_a_idx)\n                diff_depth_a = np.asarray(diff_depth_a)\n                diff_mask_a = np.asarray(diff_mask_a)\n                (uv_a_vec, diff_uv_a_vec) = correspondence_finder.batch_find_pixel_correspondences(depth_a, pose_a, diff_depth_a, diff_pose_a,\n                                                               uv_a=uv_a)\n                if uv_a_vec is None:\n                    logging.info(""no matches found, continuing"")\n                    continue\n                elif uv_a_vec[0].numel() == 0:\n                    logging.info(""no matches found, continuing"")\n                    continue\n\n                diff_rgb_a_tensor = dataset.rgb_image_to_tensor(diff_rgb_a)\n                diff_res_a = dcn.forward_single_image_tensor(diff_rgb_a_tensor).data.cpu().numpy()\n\n\n                diff_uv_a = (diff_uv_a_vec[0][0], diff_uv_a_vec[1][0])\n                diff_uv_a = DCE.clip_pixel_to_image_size_and_round(diff_uv_a, image_width, image_height)\n\n                pd_template = DenseCorrespondenceEvaluation.compute_descriptor_match_statistics(diff_depth_a,\n                                                        depth_b, diff_mask_a, mask_b, diff_uv_a, uv_b, diff_pose_a, pose_b,\n                                                        diff_res_a, res_b, camera_intrinsics_matrix,\n                                                        rgb_a=diff_rgb_a, rgb_b=rgb_b, debug=False)\n                pd_template.set_value(\'scene_name\', scene_name_a+""+""+scene_name_b)\n                pd_template.set_value(\'img_a_idx\', int(different_view_a_idx))\n                pd_template.set_value(\'img_b_idx\', int(img_b_idx))\n\n                dataframe_list.append(pd_template.dataframe)\n\n            # Loop over K different views for image b\n            for k in range(K):\n                different_view_b_idx = dataset.get_img_idx_with_different_pose(scene_name_b, pose_b, num_attempts=50)\n                if different_view_b_idx is None:\n                    logging.info(""no frame with sufficiently different pose found, continuing"")\n                    continue\n                diff_rgb_b, diff_depth_b, diff_mask_b, diff_pose_b = dataset.get_rgbd_mask_pose(scene_name_b, different_view_b_idx)\n                diff_depth_b = np.asarray(diff_depth_b)\n                diff_mask_b = np.asarray(diff_mask_b)\n                (uv_b_vec, diff_uv_b_vec) = correspondence_finder.batch_find_pixel_correspondences(depth_b, pose_b, diff_depth_b, diff_pose_b,\n                                                               uv_a=uv_b)\n                if uv_b_vec is None:\n                    logging.info(""no matches found, continuing"")\n                    continue\n                elif uv_b_vec[0].numel() == 0:\n                    logging.info(""no matches found, continuing"")\n                    continue\n\n                diff_rgb_b_tensor = dataset.rgb_image_to_tensor(diff_rgb_b)\n                diff_res_b = dcn.forward_single_image_tensor(diff_rgb_b_tensor).data.cpu().numpy()\n\n                diff_uv_b = (diff_uv_b_vec[0][0], diff_uv_b_vec[1][0])\n                diff_uv_b = DCE.clip_pixel_to_image_size_and_round(diff_uv_b, image_width, image_height)\n\n                pd_template = DenseCorrespondenceEvaluation.compute_descriptor_match_statistics(depth_a,\n                                                        diff_depth_b, mask_a, diff_mask_b, uv_a, diff_uv_b, pose_a, diff_pose_b,\n                                                        res_a, diff_res_b, camera_intrinsics_matrix,\n                                                        rgb_a=rgb_a, rgb_b=diff_rgb_b, debug=False)\n                pd_template.set_value(\'scene_name\', scene_name_a+""+""+scene_name_b)\n                pd_template.set_value(\'img_a_idx\', int(img_a_idx))\n                pd_template.set_value(\'img_b_idx\', int(different_view_b_idx))\n\n                dataframe_list.append(pd_template.dataframe)\n\n        return dataframe_list\n\n    @staticmethod\n    def single_across_object_image_pair_quantitative_analysis(dcn, dataset, scene_name_a, scene_name_b,\n                                                img_a_idx, img_b_idx, object_id_a, object_id_b, num_uv_a_samples=100,\n                                                debug=False):\n        """"""\n        Quantitative analysis of a dcn on a pair of images from the same scene.\n\n        :param dcn:\n        :type dcn: DenseCorrespondenceNetwork\n        :param dataset:\n        :type dataset: SpartanDataset\n        :param scene_name:\n        :type scene_name: str\n        :param img_a_idx:\n        :type img_a_idx: int\n        :param img_b_idx:\n        :type img_b_idx: int\n        :param camera_intrinsics_matrix: Optionally set camera intrinsics, otherwise will get it from the dataset\n        :type camera_intrinsics_matrix: 3 x 3 numpy array\n        :return: Dict with relevant data\n        :rtype:\n        """"""\n\n\n        rgb_a, depth_a, mask_a, _ = dataset.get_rgbd_mask_pose(scene_name_a, img_a_idx)\n        rgb_b, depth_b, mask_b, _ = dataset.get_rgbd_mask_pose(scene_name_b, img_b_idx)\n\n        depth_a = np.asarray(depth_a)\n        depth_b = np.asarray(depth_b)\n        mask_a = np.asarray(mask_a)\n        mask_b = np.asarray(mask_b)\n\n        # compute dense descriptors\n        rgb_a_tensor = dataset.rgb_image_to_tensor(rgb_a)\n        rgb_b_tensor = dataset.rgb_image_to_tensor(rgb_b)\n\n        # these are Variables holding torch.FloatTensors, first grab the data, then convert to numpy\n        res_a = dcn.forward_single_image_tensor(rgb_a_tensor).data.cpu().numpy()\n        res_b = dcn.forward_single_image_tensor(rgb_b_tensor).data.cpu().numpy()\n\n        # container to hold a list of pandas dataframe\n        # will eventually combine them all with concat\n        dataframe_list = []\n\n        logging_rate = 100\n\n        image_height, image_width = dcn.image_shape\n\n        DCE = DenseCorrespondenceEvaluation\n\n        sampled_idx_list = random_sample_from_masked_image(mask_a, num_uv_a_samples)\n        # If the list is empty, return an empty list\n        if len(sampled_idx_list) == 0:\n            return dataframe_list\n\n        for i in range(num_uv_a_samples):\n\n            uv_a = [sampled_idx_list[1][i], sampled_idx_list[0][i]]\n\n            pd_template = DCE.compute_descriptor_match_statistics_no_ground_truth(uv_a, res_a,\n                                                                  res_b,\n                                                                  rgb_a=rgb_a,\n                                                                  rgb_b=rgb_b,\n                                                                  depth_a=depth_a,\n                                                                  depth_b=depth_b,\n                                                                  debug=debug)\n\n            pd_template.set_value(\'scene_name_a\', scene_name_a)\n            pd_template.set_value(\'scene_name_b\', scene_name_b)\n            pd_template.set_value(\'object_id_a\', object_id_a)\n            pd_template.set_value(\'object_id_b\', object_id_b)\n            pd_template.set_value(\'img_a_idx\', int(img_a_idx))\n            pd_template.set_value(\'img_b_idx\', int(img_b_idx))\n\n            dataframe_list.append(pd_template.dataframe)\n\n        return dataframe_list\n\n    @staticmethod\n    def single_same_scene_image_pair_quantitative_analysis(dcn, dataset, scene_name,\n                                                img_a_idx, img_b_idx,\n                                                camera_intrinsics_matrix=None,\n                                                num_matches=100,\n                                                debug=False):\n        """"""\n        Quantitative analysis of a dcn on a pair of images from the same scene.\n\n        :param dcn:\n        :type dcn: DenseCorrespondenceNetwork\n        :param dataset:\n        :type dataset: SpartanDataset\n        :param scene_name:\n        :type scene_name: str\n        :param img_a_idx:\n        :type img_a_idx: int\n        :param img_b_idx:\n        :type img_b_idx: int\n        :param camera_intrinsics_matrix: Optionally set camera intrinsics, otherwise will get it from the dataset\n        :type camera_intrinsics_matrix: 3 x 3 numpy array\n        :return: List of pandas DataFrame objects\n        :rtype:\n        """"""\n\n        rgb_a, depth_a, mask_a, pose_a = dataset.get_rgbd_mask_pose(scene_name, img_a_idx)\n\n        rgb_b, depth_b, mask_b, pose_b = dataset.get_rgbd_mask_pose(scene_name, img_b_idx)\n\n        depth_a = np.asarray(depth_a)\n        depth_b = np.asarray(depth_b)\n        mask_a = np.asarray(mask_a)\n        mask_b = np.asarray(mask_b)\n\n        # compute dense descriptors\n        rgb_a_tensor = dataset.rgb_image_to_tensor(rgb_a)\n        rgb_b_tensor = dataset.rgb_image_to_tensor(rgb_b)\n\n        # these are Variables holding torch.FloatTensors, first grab the data, then convert to numpy\n        res_a = dcn.forward_single_image_tensor(rgb_a_tensor).data.cpu().numpy()\n        res_b = dcn.forward_single_image_tensor(rgb_b_tensor).data.cpu().numpy()\n\n        if camera_intrinsics_matrix is None:\n            camera_intrinsics = dataset.get_camera_intrinsics(scene_name)\n            camera_intrinsics_matrix = camera_intrinsics.K\n\n        # find correspondences\n        (uv_a_vec, uv_b_vec) = correspondence_finder.batch_find_pixel_correspondences(depth_a, pose_a, depth_b, pose_b,\n                                                               device=\'CPU\', img_a_mask=mask_a)\n\n        if uv_a_vec is None:\n            print ""no matches found, returning""\n            return None\n\n        # container to hold a list of pandas dataframe\n        # will eventually combine them all with concat\n        dataframe_list = []\n\n        total_num_matches = len(uv_a_vec[0])\n        num_matches = min(num_matches, total_num_matches)\n        match_list = random.sample(range(0, total_num_matches), num_matches)\n\n        if debug:\n            match_list = [50]\n\n        logging_rate = 100\n\n        image_height, image_width = dcn.image_shape\n\n        DCE = DenseCorrespondenceEvaluation\n\n        for i in match_list:\n            uv_a = (uv_a_vec[0][i], uv_a_vec[1][i])\n            uv_b_raw = (uv_b_vec[0][i], uv_b_vec[1][i])\n            uv_b = DCE.clip_pixel_to_image_size_and_round(uv_b_raw, image_width, image_height)\n\n            pd_template = DCE.compute_descriptor_match_statistics(depth_a,\n                                                                  depth_b,\n                                                                  mask_a,\n                                                                  mask_b,\n                                                                  uv_a,\n                                                                  uv_b,\n                                                                  pose_a,\n                                                                  pose_b,\n                                                                  res_a,\n                                                                  res_b,\n                                                                  camera_intrinsics_matrix,\n                                                                  rgb_a=rgb_a,\n                                                                  rgb_b=rgb_b,\n                                                                  debug=debug)\n\n            pd_template.set_value(\'scene_name\', scene_name)\n            pd_template.set_value(\'img_a_idx\', int(img_a_idx))\n            pd_template.set_value(\'img_b_idx\', int(img_b_idx))\n\n            dataframe_list.append(pd_template.dataframe)\n\n        return dataframe_list\n\n    @staticmethod\n    def is_depth_valid(depth):\n        """"""\n        Checks if depth value is valid, usually missing depth values are either 0 or MAX_RANGE\n        :param depth: depth in meters\n        :type depth:\n        :return:\n        :rtype: bool\n        """"""\n\n        MAX_DEPTH = 10.0\n\n        return ((depth > 0) and (depth < MAX_DEPTH))\n\n\n\n    @staticmethod\n    def compute_descriptor_match_statistics_no_ground_truth(uv_a, res_a, res_b, rgb_a=None, rgb_b=None,\n                                                            depth_a=None, depth_b=None, debug=False):\n        """"""\n        Computes statistics of descriptor pixelwise match when there is zero ground truth data.\n\n        :param res_a: descriptor for image a, of shape (H,W,D)\n        :type res_a: numpy array\n        :param res_b: descriptor for image b, of shape (H,W,D)\n        :param debug: whether or not to print visualization\n        :type debug:\n        """"""\n\n        DCE = DenseCorrespondenceEvaluation\n\n        # compute best match\n        uv_b, best_match_diff, norm_diffs =\\\n            DenseCorrespondenceNetwork.find_best_match(uv_a, res_a,\n                                                       res_b, debug=debug)\n\n        if debug:\n            correspondence_plotter.plot_correspondences_direct(rgb_a, depth_a, rgb_b, depth_b,\n                                                               uv_a, uv_b, show=True)\n\n\n        pd_template = DCNEvaluationPandaTemplateAcrossObject()\n        pd_template.set_value(\'norm_diff_descriptor_best_match\', best_match_diff)\n        return pd_template\n\n\n    @staticmethod\n    def compute_descriptor_match_statistics(depth_a, depth_b, mask_a, mask_b, uv_a, uv_b, pose_a, pose_b,\n                                            res_a, res_b, camera_matrix, params=None,\n                                            rgb_a=None, rgb_b=None, debug=False):\n        """"""\n        Computes statistics of descriptor pixelwise match.\n\n        :param uv_a: a single pixel index in (u,v) coordinates, from image a\n        :type uv_a: tuple of 2 ints\n        :param uv_b: a single pixel index in (u,v) coordinates, from image b\n        :type uv_b: tuple of 2 ints\n        :param camera_matrix: camera intrinsics matrix\n        :type camera_matrix: 3 x 3 numpy array\n        :param rgb_a:\n        :type rgb_a:\n        :param rgb_b:\n        :type rgb_b:\n        :param depth_a: depth is assumed to be in mm (see conversion to meters below)\n        :type depth_a: numpy array\n        :param depth_b:\n        :type depth_b:\n        :param pose_a:\n        :type pose_a: 4 x 4 numpy array\n        :param pose_b:\n        :type pose_b:\n        :param res_a: descriptor for image a, of shape (H,W,D)\n        :type res_a: numpy array\n        :param res_b: descriptor for image b, of shape (H,W,D)\n        :type res_b: numpy array\n        :param params:\n        :type params:\n        :param debug: whether or not to print visualization\n        :type debug:\n        :return: Dense\n        :rtype: DCNEvaluationPandaTemplate\n        """"""\n\n        DCE = DenseCorrespondenceEvaluation\n\n        # compute best match\n        uv_b_pred, best_match_diff, norm_diffs =\\\n            DenseCorrespondenceNetwork.find_best_match(uv_a, res_a,\n                                                       res_b, debug=debug)\n\n        # norm_diffs shape is (H,W)\n\n        # compute best match on mask only\n        mask_b_inv = 1-mask_b\n        masked_norm_diffs = norm_diffs + mask_b_inv*1e6\n\n        best_match_flattened_idx_masked = np.argmin(masked_norm_diffs)\n        best_match_xy_masked = np.unravel_index(best_match_flattened_idx_masked, masked_norm_diffs.shape)\n        best_match_diff_masked = masked_norm_diffs[best_match_xy_masked]\n        uv_b_pred_masked = (best_match_xy_masked[1], best_match_xy_masked[0])\n\n        # compute pixel space difference\n        pixel_match_error_l2 = np.linalg.norm((np.array(uv_b) - np.array(uv_b_pred)), ord=2)\n        pixel_match_error_l2_masked = np.linalg.norm((np.array(uv_b) - np.array(uv_b_pred_masked)), ord=2)\n        pixel_match_error_l1 = np.linalg.norm((np.array(uv_b) - np.array(uv_b_pred)), ord=1)\n\n\n        # extract the ground truth descriptors\n        des_a = res_a[uv_a[1], uv_a[0], :]\n        des_b_ground_truth = res_b[uv_b[1], uv_b[0], :]\n        norm_diff_descriptor_ground_truth = np.linalg.norm(des_a - des_b_ground_truth)\n\n        # from Schmidt et al 2017:\n        """"""\n        We then determine the number of pixels in the target image that are closer in\n        descriptor space to the source point than the manually-labelled corresponding point.\n        """"""\n        # compute this\n        (v_indices_better_than_ground_truth, u_indices_better_than_ground_truth) = np.where(norm_diffs < norm_diff_descriptor_ground_truth)\n        num_pixels_closer_than_ground_truth = len(u_indices_better_than_ground_truth)\n        num_pixels_in_image = res_a.shape[0] * res_a.shape[1]\n        fraction_pixels_closer_than_ground_truth = num_pixels_closer_than_ground_truth*1.0/num_pixels_in_image\n\n        (v_indices_better_than_ground_truth_masked, u_indices_better_than_ground_truth_masked) = np.where(masked_norm_diffs < norm_diff_descriptor_ground_truth)\n        num_pixels_closer_than_ground_truth_masked = len(u_indices_better_than_ground_truth_masked)\n        num_pixels_in_masked_image = len(np.nonzero(mask_b)[0])\n        fraction_pixels_closer_than_ground_truth_masked = num_pixels_closer_than_ground_truth_masked*1.0/num_pixels_in_masked_image\n\n        # new metric: average l2 distance of the pixels better than ground truth\n        if num_pixels_closer_than_ground_truth == 0:\n            average_l2_distance_for_false_positives = 0.0\n        else:\n            l2_distances = np.sqrt((u_indices_better_than_ground_truth - uv_b[0])**2 + (v_indices_better_than_ground_truth - uv_b[1])**2)\n            average_l2_distance_for_false_positives = np.average(l2_distances)\n\n        # new metric: average l2 distance of the pixels better than ground truth\n        if num_pixels_closer_than_ground_truth_masked == 0:\n            average_l2_distance_for_false_positives_masked = 0.0\n        else:\n            l2_distances_masked = np.sqrt((u_indices_better_than_ground_truth_masked - uv_b[0])**2 + (v_indices_better_than_ground_truth_masked - uv_b[1])**2)\n            average_l2_distance_for_false_positives_masked = np.average(l2_distances_masked)\n\n        # extract depth values, note the indexing order of u,v has to be reversed\n        uv_a_depth = depth_a[uv_a[1], uv_a[0]] / DEPTH_IM_SCALE # check if this is not None\n        uv_b_depth = depth_b[uv_b[1], uv_b[0]] / DEPTH_IM_SCALE\n        uv_b_pred_depth = depth_b[uv_b_pred[1], uv_b_pred[0]] / DEPTH_IM_SCALE\n        uv_b_pred_depth_is_valid = DenseCorrespondenceEvaluation.is_depth_valid(uv_b_pred_depth)\n        uv_b_pred_depth_masked = depth_b[uv_b_pred_masked[1], uv_b_pred_masked[0]] / DEPTH_IM_SCALE\n        uv_b_pred_depth_is_valid_masked = DenseCorrespondenceEvaluation.is_depth_valid(uv_b_pred_depth_masked)\n        is_valid = uv_b_pred_depth_is_valid\n        is_valid_masked = uv_b_pred_depth_is_valid_masked\n\n        uv_a_pos = DCE.compute_3d_position(uv_a, uv_a_depth, camera_matrix, pose_a)\n        uv_b_pos = DCE.compute_3d_position(uv_b, uv_b_depth, camera_matrix, pose_b)\n        uv_b_pred_pos = DCE.compute_3d_position(uv_b_pred, uv_b_pred_depth, camera_matrix, pose_b)\n        uv_b_pred_pos_masked = DCE.compute_3d_position(uv_b_pred_masked, uv_b_pred_depth_masked, camera_matrix, pose_b)\n\n        diff_ground_truth_3d = uv_b_pos - uv_a_pos\n\n        diff_pred_3d = uv_b_pos - uv_b_pred_pos\n        diff_pred_3d_masked = uv_b_pos - uv_b_pred_pos_masked\n\n        if DCE.is_depth_valid(uv_b_depth):\n            norm_diff_ground_truth_3d = np.linalg.norm(diff_ground_truth_3d)\n        else:\n            norm_diff_ground_truth_3d = np.nan\n\n        if DCE.is_depth_valid(uv_b_depth) and DCE.is_depth_valid(uv_b_pred_depth):\n            norm_diff_pred_3d = np.linalg.norm(diff_pred_3d)\n        else:\n            norm_diff_pred_3d = np.nan\n\n        if DCE.is_depth_valid(uv_b_depth) and DCE.is_depth_valid(uv_b_pred_depth_masked):\n            norm_diff_pred_3d_masked = np.linalg.norm(diff_pred_3d_masked)\n        else:\n            norm_diff_pred_3d_masked = np.nan\n\n        if debug:\n\n            fig, axes = correspondence_plotter.plot_correspondences_direct(rgb_a, depth_a, rgb_b, depth_b,\n                                                               uv_a, uv_b, show=False)\n\n            correspondence_plotter.plot_correspondences_direct(rgb_a, depth_a, rgb_b, depth_b,\n                                                               uv_a, uv_b_pred,\n                                                               use_previous_plot=(fig, axes),\n                                                               show=True,\n                                                               circ_color=\'purple\')\n\n        pd_template = DCNEvaluationPandaTemplate()\n        pd_template.set_value(\'norm_diff_descriptor\', best_match_diff)\n        pd_template.set_value(\'norm_diff_descriptor_masked\', best_match_diff_masked)\n        pd_template.set_value(\'is_valid\', is_valid)\n        pd_template.set_value(\'is_valid_masked\', is_valid_masked)\n\n        pd_template.set_value(\'norm_diff_ground_truth_3d\', norm_diff_ground_truth_3d)\n\n        if is_valid:\n            pd_template.set_value(\'norm_diff_pred_3d\', norm_diff_pred_3d)\n        else:\n            pd_template.set_value(\'norm_diff_pred_3d\', np.nan)\n\n        if is_valid_masked:\n            pd_template.set_value(\'norm_diff_pred_3d_masked\', norm_diff_pred_3d_masked)\n        else:\n            pd_template.set_value(\'norm_diff_pred_3d_masked\', np.nan)\n\n        pd_template.set_value(\'norm_diff_descriptor_ground_truth\', norm_diff_descriptor_ground_truth)\n\n        pd_template.set_value(\'pixel_match_error_l2\', pixel_match_error_l2)\n        pd_template.set_value(\'pixel_match_error_l2_masked\', pixel_match_error_l2_masked)\n        pd_template.set_value(\'pixel_match_error_l1\', pixel_match_error_l1)\n\n        pd_template.set_value(\'fraction_pixels_closer_than_ground_truth\', fraction_pixels_closer_than_ground_truth)\n        pd_template.set_value(\'fraction_pixels_closer_than_ground_truth_masked\', fraction_pixels_closer_than_ground_truth_masked)\n        pd_template.set_value(\'average_l2_distance_for_false_positives\', average_l2_distance_for_false_positives)\n        pd_template.set_value(\'average_l2_distance_for_false_positives_masked\', average_l2_distance_for_false_positives_masked)\n\n\n        return pd_template\n\n    @staticmethod\n    def compute_3d_position(uv, depth, camera_intrinsics_matrix, camera_to_world):\n        """"""\n\n\n        :param uv: pixel-location in (row, column) ordering\n        :type uv:\n        :param depth: depth-value\n        :type depth:\n        :param camera_intrinsics_matrix: the camera intrinsics matrix\n        :type camera_intrinsics_matrix:\n        :param camera_to_world: camera to world transform as a homogenous transform matrix\n        :type camera_to_world: 4 x 4 numpy array\n        :return:\n        :rtype: np.array with shape (3,)\n        """"""\n        pos_in_camera_frame = correspondence_finder.pinhole_projection_image_to_world(uv, depth, camera_intrinsics_matrix)\n\n        pos_in_world_frame = np.dot(camera_to_world, np.append(pos_in_camera_frame, 1))[:3]\n\n        return pos_in_world_frame\n\n    @staticmethod\n    def single_same_scene_image_pair_qualitative_analysis(dcn, dataset, scene_name,\n                                               img_a_idx, img_b_idx,\n                                               num_matches=10):\n        """"""\n        Wrapper for single_image_pair_qualitative_analysis, when images are from same scene.\n\n        See that function for remaining documentation.\n\n        :param scene_name: scene name to use\n        :param img_a_idx: index of image_a in the dataset\n        :param img_b_idx: index of image_b in the datset\n\n        :type scene_name: str\n        :type img_a_idx: int\n        :type img_b_idx: int\n\n        :return: None\n        """"""\n\n        rgb_a, _, mask_a, _ = dataset.get_rgbd_mask_pose(scene_name, img_a_idx)\n\n        rgb_b, _, mask_b, _ = dataset.get_rgbd_mask_pose(scene_name, img_b_idx)\n\n        DenseCorrespondenceEvaluation.single_image_pair_qualitative_analysis(dcn, dataset, rgb_a, rgb_b, mask_a, mask_b, num_matches)\n\n    @staticmethod\n    def single_cross_scene_image_pair_qualitative_analysis(dcn, dataset, scene_name_a,\n                                               img_a_idx, scene_name_b, img_b_idx,\n                                               num_matches=10):\n        """"""\n        Wrapper for single_image_pair_qualitative_analysis, when images are NOT from same scene.\n\n        See that function for remaining documentation.\n\n        :param scene_name: scene name to use\n        :param img_a_idx: index of image_a in the dataset\n        :param img_b_idx: index of image_b in the datset\n\n        :type scene_name: str\n        :type img_a_idx: int\n        :type img_b_idx: int\n\n        :return: the images a and b\n        :rtype: PIL.Image, PIL.Image\n        """"""\n\n        rgb_a, _, mask_a, _ = dataset.get_rgbd_mask_pose(scene_name_a, img_a_idx)\n\n        rgb_b, _, mask_b, _ = dataset.get_rgbd_mask_pose(scene_name_b, img_b_idx)\n\n        DenseCorrespondenceEvaluation.single_image_pair_qualitative_analysis(dcn, dataset, rgb_a, rgb_b, mask_a, mask_b, num_matches)\n        return rgb_a, rgb_b\n\n    @staticmethod\n    def single_image_pair_keypoint_qualitative_analysis(dcn, dataset, keypoint_data_a,\n                                                        keypoint_data_b,\n                                                        heatmap_kernel_variance=0.25,\n                                                        blend_weight_original_image=0.3,\n                                                        plot_title=""Keypoints""):\n        """"""\n        Wrapper for qualitative analysis of a pair of images using keypoint annotations\n        :param dcn:\n        :type dcn:\n        :param dataset:\n        :type dataset:\n        :param keypoint_data_a: pandas Series\n        :type keypoint_data_a:\n        :param keypoint_data_b:\n        :type keypoint_data_b:\n        :return:\n        :rtype:\n        """"""\n        DCE = DenseCorrespondenceEvaluation\n\n        image_height, image_width = dcn.image_shape\n\n        scene_name_a = keypoint_data_a[\'scene_name\']\n        img_a_idx = keypoint_data_a[\'image_idx\']\n        uv_a = (keypoint_data_a[\'u\'], keypoint_data_a[\'v\'])\n        uv_a = DCE.clip_pixel_to_image_size_and_round(uv_a, image_width, image_height)\n\n        scene_name_b = keypoint_data_b[\'scene_name\']\n        img_b_idx = keypoint_data_b[\'image_idx\']\n        uv_b = (keypoint_data_b[\'u\'], keypoint_data_b[\'v\'])\n        uv_b = DCE.clip_pixel_to_image_size_and_round(uv_b, image_width, image_height)\n\n\n        rgb_a, _, mask_a, _ = dataset.get_rgbd_mask_pose(scene_name_a, img_a_idx)\n\n        rgb_b, _, mask_b, _ = dataset.get_rgbd_mask_pose(scene_name_b, img_b_idx)\n\n        mask_a = np.asarray(mask_a)\n        mask_b = np.asarray(mask_b)\n\n        # compute dense descriptors\n        rgb_a_tensor = dataset.rgb_image_to_tensor(rgb_a)\n        rgb_b_tensor = dataset.rgb_image_to_tensor(rgb_b)\n\n        # these are Variables holding torch.FloatTensors, first grab the data, then convert to numpy\n        res_a = dcn.forward_single_image_tensor(rgb_a_tensor).data.cpu().numpy()\n        res_b = dcn.forward_single_image_tensor(rgb_b_tensor).data.cpu().numpy()\n\n        best_match_uv, best_match_diff, norm_diffs = \\\n        DenseCorrespondenceNetwork.find_best_match(uv_a, res_a, res_b, debug=False)\n\n\n\n        # visualize image and then heatmap\n        diam = 0.03\n        dist = 0.01\n        kp1 = []\n        kp2 = []\n        kp1.append(cv2.KeyPoint(uv_a[0], uv_a[1], diam))\n        kp2.append(cv2.KeyPoint(best_match_uv[0], best_match_uv[1], diam))\n\n\n        matches = [] # list of cv2.DMatch\n        matches.append(cv2.DMatch(0,0,dist))\n\n        gray_a_numpy = cv2.cvtColor(np.asarray(rgb_a), cv2.COLOR_BGR2GRAY)\n        gray_b_numpy = cv2.cvtColor(np.asarray(rgb_b), cv2.COLOR_BGR2GRAY)\n        img3 = cv2.drawMatches(gray_a_numpy, kp1, gray_b_numpy, kp2, matches, flags=2, outImg=gray_b_numpy, matchColor=(255,0,0))\n\n        fig, axes = plt.subplots(nrows=2, ncols=1)\n        fig.set_figheight(10)\n        fig.set_figwidth(15)\n        axes[0].imshow(img3)\n        axes[0].set_title(plot_title)\n\n        # visualize the heatmap\n        heatmap_color = vis_utils.compute_gaussian_kernel_heatmap_from_norm_diffs(norm_diffs, heatmap_kernel_variance)\n\n        # convert heatmap to RGB (it\'s in BGR now)\n        heatmap_color_rgb = cv2.cvtColor(heatmap_color, cv2.COLOR_BGR2RGB)\n\n\n        alpha = blend_weight_original_image\n        beta = 1-alpha\n        blended = cv2.addWeighted(np.asarray(rgb_b), alpha, heatmap_color_rgb, beta, 0)\n\n        axes[1].imshow(blended)\n        return rgb_a, rgb_b\n\n    @staticmethod\n    def single_image_pair_qualitative_analysis(dcn, dataset, rgb_a, rgb_b, mask_a, mask_b,\n                                               num_matches):\n        """"""\n        Computes qualtitative assessment of DCN performance for a pair of\n        images\n\n        :param dcn: dense correspondence network to use\n        :param dataset: dataset to use\n        :param rgb_a, rgb_b: two rgb images for which to do matching\n        :param mask_a, mask_b: masks of these two images\n        :param num_matches: number of matches to generate\n\n        :type dcn: DenseCorrespondenceNetwork\n        :type dataset: DenseCorrespondenceDataset\n        :type rgb_a, rgb_b: PIL.Images\n        :type mask_a, mask_b: PIL.Images\n        :type num_matches: int\n\n        :return: None\n        """"""\n\n        mask_a = np.asarray(mask_a)\n        mask_b = np.asarray(mask_b)\n\n        # compute dense descriptors\n        rgb_a_tensor = dataset.rgb_image_to_tensor(rgb_a)\n        rgb_b_tensor = dataset.rgb_image_to_tensor(rgb_b)\n\n        # these are Variables holding torch.FloatTensors, first grab the data, then convert to numpy\n        res_a = dcn.forward_single_image_tensor(rgb_a_tensor).data.cpu().numpy()\n        res_b = dcn.forward_single_image_tensor(rgb_b_tensor).data.cpu().numpy()\n\n\n        # sample points on img_a. Compute best matches on img_b\n        # note that this is in (x,y) format\n        # TODO: if this mask is empty, this function will not be happy\n        # de-prioritizing since this is only for qualitative evaluation plots\n        sampled_idx_list = random_sample_from_masked_image(mask_a, num_matches)\n\n        # list of cv2.KeyPoint\n        kp1 = []\n        kp2 = []\n        matches = []  # list of cv2.DMatch\n\n        # placeholder constants for opencv\n        diam = 0.01\n        dist = 0.01\n\n        try:\n            descriptor_image_stats = dcn.descriptor_image_stats\n        except:\n            print ""Could not find descriptor image stats...""\n            print ""Only normalizing pairs of images!""\n            descriptor_image_stats = None\n\n        for i in xrange(0, num_matches):\n            # convert to (u,v) format\n            pixel_a = [sampled_idx_list[1][i], sampled_idx_list[0][i]]\n            best_match_uv, best_match_diff, norm_diffs =\\\n                DenseCorrespondenceNetwork.find_best_match(pixel_a, res_a,\n                                                                                                     res_b)\n\n            # be careful, OpenCV format is  (u,v) = (right, down)\n            kp1.append(cv2.KeyPoint(pixel_a[0], pixel_a[1], diam))\n            kp2.append(cv2.KeyPoint(best_match_uv[0], best_match_uv[1], diam))\n            matches.append(cv2.DMatch(i, i, dist))\n\n        gray_a_numpy = cv2.cvtColor(np.asarray(rgb_a), cv2.COLOR_BGR2GRAY)\n        gray_b_numpy = cv2.cvtColor(np.asarray(rgb_b), cv2.COLOR_BGR2GRAY)\n        img3 = cv2.drawMatches(gray_a_numpy, kp1, gray_b_numpy, kp2, matches, flags=2, outImg=gray_b_numpy)\n        fig, axes = plt.subplots(nrows=1, ncols=1)\n        fig.set_figheight(10)\n        fig.set_figwidth(15)\n        axes.imshow(img3)\n\n        # show colormap if possible (i.e. if descriptor dimension is 1 or 3)\n        if dcn.descriptor_dimension in [1,3]:\n            DenseCorrespondenceEvaluation.plot_descriptor_colormaps(res_a, res_b,\n                                                                    descriptor_image_stats=descriptor_image_stats,\n                                                                    mask_a=mask_a,\n                                                                    mask_b=mask_b,\n                                                                    plot_masked=True)\n\n        plt.show()\n\n    @staticmethod\n    def single_image_pair_cross_scene_keypoints_quantitative_analysis(dcn, dataset, keypoint_data_a,\n                                                                      keypoint_data_b, res_a=None, res_b=None):\n        """"""\n        Quantitative analysis of cross instance keypoint annotations. This is used in the\n        class consistent setting\n\n        :param dcn:\n        :type dcn:\n        :param dataset:\n        :type dataset:\n        :param keypoint_data_a:\n        :type keypoint_data_a:\n        :param keypoint_data_b:\n        :type keypoint_data_b:\n        :return: List of pandas DataFrame objects\n        :rtype:\n        """"""\n\n        DCE = DenseCorrespondenceEvaluation\n\n        scene_name_a = keypoint_data_a[\'scene_name\']\n        object_id_a = keypoint_data_a[\'object_id\']\n        img_a_idx = keypoint_data_a[\'image_idx\']\n\n        scene_name_b = keypoint_data_b[\'scene_name\']\n        object_id_b = keypoint_data_b[\'object_id\']\n        img_b_idx = keypoint_data_b[\'image_idx\']\n\n        rgb_a, depth_a, mask_a, pose_a = dataset.get_rgbd_mask_pose(scene_name_a, img_a_idx)\n\n        rgb_b, depth_b, mask_b, pose_b = dataset.get_rgbd_mask_pose(scene_name_b, img_b_idx)\n\n        depth_a = np.asarray(depth_a)\n        depth_b = np.asarray(depth_b)\n        mask_a = np.asarray(mask_a)\n        mask_b = np.asarray(mask_b)\n\n        if res_a is None and res_b is None:\n            # compute dense descriptors\n            rgb_a_tensor = dataset.rgb_image_to_tensor(rgb_a)\n            rgb_b_tensor = dataset.rgb_image_to_tensor(rgb_b)\n\n            # these are Variables holding torch.FloatTensors, first grab the data, then convert to numpy\n            res_a = dcn.forward_single_image_tensor(rgb_a_tensor).data.cpu().numpy()\n            res_b = dcn.forward_single_image_tensor(rgb_b_tensor).data.cpu().numpy()\n\n\n        # vectors to allow re-ordering\n        rgb = [rgb_a, rgb_b]\n        depth = [depth_a, depth_b]\n        mask = [mask_a, mask_b]\n        scene_name = [scene_name_a, scene_name_b]\n        img_idx = [img_a_idx, img_b_idx]\n        pose = [pose_a, pose_b]\n        res = [res_a, res_b]\n        object_id = [object_id_a, object_id_b]\n\n        camera_intrinsics_a = dataset.get_camera_intrinsics(scene_name_a)\n        camera_intrinsics_b = dataset.get_camera_intrinsics(scene_name_b)\n        if not np.allclose(camera_intrinsics_a.K, camera_intrinsics_b.K):\n            print ""Currently cannot handle two different camera K matrices in different scenes!""\n            print ""But you could add this...""\n        camera_intrinsics_matrix = camera_intrinsics_a.K\n\n        image_height, image_width = dcn.image_shape\n        DCE = DenseCorrespondenceEvaluation\n        dataframe_list = []\n\n        ordering = [""standard"", ""reverse""]\n\n        for kp_name, data_a in keypoint_data_a[\'keypoints\'].iteritems():\n            if kp_name not in keypoint_data_b[\'keypoints\']:\n                raise ValueError(""keypoint %s appears in one list of annotated data but""\n                                 ""not the other"" %(kp_name))\n\n            data_b = keypoint_data_b[\'keypoints\'][kp_name]\n\n            data = [data_a, data_b]\n\n            for order in ordering:\n                if order == ""standard"":\n                    idx_1 = 0\n                    idx_2 = 1\n                elif order == ""reverse"":\n                    idx_1 = 1\n                    idx_2 = 0\n                else:\n                    raise ValueError(""you should never get here"")\n\n\n                uv_1 = DCE.clip_pixel_to_image_size_and_round((data[idx_1][\'u\'], data[idx_2][\'v\']), image_width, image_height)\n                uv_2 = DCE.clip_pixel_to_image_size_and_round((data[idx_2][\'u\'], data[idx_2][\'v\']), image_width,\n                                                              image_height)\n\n                pd_template = DenseCorrespondenceEvaluation.compute_descriptor_match_statistics(depth[idx_1],\n                                                                                                depth[idx_2],\n                                                                                                mask[idx_1],\n                                                                                                mask[idx_2],\n                                                                                                uv_1,\n                                                                                                uv_2,\n                                                                                                pose[idx_1],\n                                                                                                pose[idx_2],\n                                                                                                res[idx_1],\n                                                                                                res[idx_2],\n                                                                                                camera_intrinsics_matrix,\n                                                                                                rgb_a=rgb[idx_1], rgb_b=rgb[idx_2],\n                                                                                                debug=False)\n\n                pd_template.set_value(\'img_a_idx\', img_idx[idx_1])\n                pd_template.set_value(\'img_b_idx\', img_idx[idx_2])\n                pd_template.set_value(\'scene_name_a\', scene_name[idx_1])\n                pd_template.set_value(\'scene_name_b\', scene_name[idx_2])\n                pd_template.set_value(\'object_id_a\', object_id[idx_1])\n                pd_template.set_value(\'object_id_b\', object_id[idx_2])\n                pd_template.set_value(\'keypoint_name\', kp_name)\n\n\n                dataframe_list.append(pd_template.dataframe)\n\n        return dataframe_list\n\n    @staticmethod\n    def compute_sift_keypoints(img, mask=None):\n        """"""\n        Compute SIFT keypoints given a grayscale img\n        :param img:\n        :type img:\n        :param mask:\n        :type mask:\n        :return:\n        :rtype:\n        """"""\n\n        # convert to grayscale image if needed\n        if len(img.shape) > 2:\n            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        else:\n            gray = img\n\n        sift = cv2.xfeatures2d.SIFT_create()\n        kp = sift.detect(gray, mask)\n        kp, des = sift.compute(gray, kp)\n        img_w_kp = 0 * img\n        cv2.drawKeypoints(gray, kp, img_w_kp)\n        return kp, des, gray, img_w_kp\n\n\n    @staticmethod\n    def single_image_pair_sift_analysis(dataset, scene_name,\n                                        img_a_idx, img_b_idx,\n                                        cross_match_threshold=0.75,\n                                        num_matches=10,\n                                        visualize=True,\n                                        camera_intrinsics_matrix=None):\n        """"""\n        Computes SIFT features and does statistics\n        :param dcn:\n        :type dcn:\n        :param dataset:\n        :type dataset:\n        :param scene_name:\n        :type scene_name:\n        :param img_a_idx:\n        :type img_a_idx:\n        :param img_b_idx:\n        :type img_b_idx:\n        :param num_matches:\n        :type num_matches:\n        :return:\n        :rtype:\n        """"""\n\n        DCE = DenseCorrespondenceEvaluation\n\n        rgb_a, depth_a, mask_a, pose_a = dataset.get_rgbd_mask_pose(scene_name, img_a_idx)\n        rgb_a = np.array(rgb_a) # converts PIL image to rgb\n\n        rgb_b, depth_b, mask_b, pose_b = dataset.get_rgbd_mask_pose(scene_name, img_b_idx)\n        rgb_b = np.array(rgb_b) # converts PIL image to rgb\n\n        kp1, des1, gray1, img_1_kp = DCE.compute_sift_keypoints(rgb_a, mask_a)\n        kp2, des2, gray2, img_2_kp = DCE.compute_sift_keypoints(rgb_b, mask_b)\n\n\n        img1 = gray1\n        img2 = gray2\n\n        if visualize:\n            fig, axes = plt.subplots(nrows=1, ncols=2)\n            fig.set_figheight(10)\n            fig.set_figwidth(15)\n            axes[0].imshow(img_1_kp)\n            axes[1].imshow(img_2_kp)\n            plt.title(""SIFT Keypoints"")\n            plt.show()\n\n        # compute matches\n        # Match descriptors.\n        # BFMatcher with default params\n        bf = cv2.BFMatcher()\n        matches = bf.knnMatch(des1, des2, k=2)  # Sort them in the order of their distance.\n        total_num_matches = len(matches)\n\n        # Apply ratio test\n        good = []\n        for m, n in matches:\n            # m is the best match\n            # n is the second best match\n\n            if m.distance < 0.75 * n.distance:\n                good.append([m])\n\n\n        if visualize:\n            good_vis = random.sample(good, 5)\n            outImg = 0 * img1 # placeholder\n            fig, axes = plt.subplots(nrows=1, ncols=1)\n            fig.set_figheight(10)\n            fig.set_figwidth(15)\n            img3 = cv2.drawMatchesKnn(img1, kp1, img2, kp2, good_vis, outImg, flags=2)\n            plt.imshow(img3)\n            plt.title(""SIFT Keypoint Matches"")\n            plt.show()\n\n\n        if camera_intrinsics_matrix is None:\n            camera_intrinsics = dataset.get_camera_intrinsics(scene_name)\n            camera_intrinsics_matrix = camera_intrinsics.K\n\n        dataframe_list = []\n\n        for idx, val in enumerate(good):\n            match = val[0]\n            kp_a = kp1[match.queryIdx]\n            kp_b = kp2[match.trainIdx]\n            df = DCE.compute_single_sift_match_statistics(depth_a, depth_b, kp_a, kp_b,\n                                                     pose_a, pose_b, camera_intrinsics_matrix)\n\n            dataframe_list.append(df)\n\n\n\n        returnData = dict()\n        returnData[\'kp1\'] = kp1\n        returnData[\'kp2\'] = kp2\n        returnData[\'matches\'] = matches\n        returnData[\'good\'] = good\n        returnData[\'dataframe_list\'] = dataframe_list\n\n        return returnData\n\n\n\n\n\n    @staticmethod\n    def compute_single_sift_match_statistics(depth_a, depth_b, kp_a, kp_b, pose_a, pose_b,\n                                            camera_matrix, params=None,\n                                            rgb_a=None, rgb_b=None, debug=False):\n        """"""\n        Compute some statistics of the SIFT match\n\n        :param depth_a:\n        :type depth_a:\n        :param depth_b:\n        :type depth_b:\n        :param kp_a: kp_a.pt is the (u,v) = (column, row) coordinates in the image\n        :type kp_a: cv2.KeyPoint\n        :param kp_b:\n        :type kp_b:\n        :param pose_a:\n        :type pose_a:\n        :param pose_b:\n        :type pose_b:\n        :param camera_matrix:\n        :type camera_matrix:\n        :param params:\n        :type params:\n        :param rgb_a:\n        :type rgb_a:\n        :param rgb_b:\n        :type rgb_b:\n        :param debug:\n        :type debug:\n        :return:\n        :rtype:\n        """"""\n\n        DCE = DenseCorrespondenceEvaluation\n        # first compute location of kp_a in world frame\n\n        image_height, image_width = depth_a.shape[0], depth_a.shape[1]\n\n        def clip_pixel_to_image_size_and_round(uv):\n            u = min(int(round(uv[0])), image_width - 1)\n            v = min(int(round(uv[1])), image_height - 1)\n            return [u,v]\n\n        uv_a = clip_pixel_to_image_size_and_round((kp_a.pt[0], kp_a.pt[1]))\n        uv_a_depth = depth_a[uv_a[1], uv_a[0]] / DEPTH_IM_SCALE\n        # print ""uv_a"", uv_a\n        # print ""uv_a_depth"", uv_a_depth\n        # print ""camera_matrix"", camera_matrix\n        # print ""pose_a"", pose_a\n        kp_a_3d = DCE.compute_3d_position(uv_a, uv_a_depth, camera_matrix, pose_a)\n\n\n        uv_b = clip_pixel_to_image_size_and_round((kp_b.pt[0], kp_b.pt[1]))\n        uv_b_depth = depth_b[uv_b[1], uv_b[0]] / DEPTH_IM_SCALE\n        uv_b_depth_valid = DCE.is_depth_valid(uv_b_depth)\n        kp_b_3d = DCE.compute_3d_position(uv_b, uv_b_depth, camera_matrix, pose_b)\n\n\n        # uv_b_ground_truth = correspondence_finder.pinhole_projection_world_to_image(kp_b_3d, camera_matrix, camera_to_world=pose_b)\n\n        is_valid = uv_b_depth_valid\n\n        if debug:\n            print ""\\n\\n""\n            print ""uv_a"", uv_a\n            print ""kp_a_3d"", kp_a_3d\n            print ""kp_b_3d"", kp_b_3d\n            print ""is_valid"", is_valid\n\n\n\n        norm_diff_pred_3d = np.linalg.norm(kp_b_3d - kp_a_3d)\n\n        pd_template = SIFTKeypointMatchPandaTemplate()\n        pd_template.set_value(\'is_valid\', is_valid)\n\n        if is_valid:\n            pd_template.set_value(\'norm_diff_pred_3d\', norm_diff_pred_3d)\n\n        return pd_template\n\n\n    @staticmethod\n    def single_image_pair_keypoint_analysis(dcn, dataset, scene_name,\n                                                img_a_idx, img_b_idx,\n                                                params=None,\n                                                camera_intrinsics_matrix=None, visualize=True):\n\n        DCE = DenseCorrespondenceEvaluation\n        # first compute SIFT stuff\n        sift_data = DCE.single_image_pair_sift_analysis(dataset, scene_name,\n                                        img_a_idx, img_b_idx, visualize=visualize)\n\n        kp1 = sift_data[\'kp1\']\n        kp2 = sift_data[\'kp2\']\n\n        rgb_a, depth_a, mask_a, pose_a = dataset.get_rgbd_mask_pose(scene_name, img_a_idx)\n        rgb_a = np.array(rgb_a)  # converts PIL image to rgb\n\n        rgb_b, depth_b, mask_b, pose_b = dataset.get_rgbd_mask_pose(scene_name, img_b_idx)\n        rgb_b = np.array(rgb_b)  # converts PIL image to rgb\n\n\n        # compute the best matches among the SIFT keypoints\n        des1 = dcn.evaluate_descriptor_at_keypoints(rgb_a, kp1)\n        des2 = dcn.evaluate_descriptor_at_keypoints(rgb_b, kp2)\n\n        bf = cv2.BFMatcher()\n        matches = bf.knnMatch(des1, des2, k=2)  # Sort them in the order of their distance.\n        total_num_matches = len(matches)\n\n        good = []\n        for idx, val in enumerate(matches):\n            m, n = val\n            if (m.distance < 0.5 * n.distance) and m.distance < 0.01:\n                print ""\\n\\n""\n                print ""m.distance"", m.distance\n                print ""n.distance"", n.distance\n                good.append([m])\n\n\n            #\n            # if idx > 5:\n            #     return\n\n\n        print ""total keypoints = "", len(kp1)\n        print ""num good matches = "", len(good)\n        print ""SIFT good matches = "", len(sift_data[\'good\'])\n        if visualize:\n            img1 = cv2.cvtColor(rgb_a, cv2.COLOR_BGR2GRAY)\n            img2 = cv2.cvtColor(rgb_b, cv2.COLOR_BGR2GRAY)\n\n\n            good_vis = random.sample(good, 5)\n            outImg = 0 * img1 # placeholder\n            fig, axes = plt.subplots(nrows=1, ncols=1)\n            fig.set_figheight(10)\n            fig.set_figwidth(15)\n            img3 = cv2.drawMatchesKnn(img1, kp1, img2, kp2, good_vis, outImg, flags=2)\n            plt.imshow(img3)\n            plt.title(""Dense Correspondence Keypoint Matches"")\n            plt.show()\n\n        returnData = dict()\n        returnData[\'kp1\'] = kp1\n        returnData[\'kp2\'] = kp2\n        returnData[\'matches\'] = matches\n        returnData[\'des1\'] = des1\n        returnData[\'des2\'] = des2\n\n        return returnData\n        return returnData\n\n\n    @staticmethod\n    def parse_cross_scene_data(dataset):\n        """"""\n        This takes a dataset.config, and concatenates together\n        a list of all of the cross scene data annotated pairs.\n        """"""\n        evaluation_labeled_data_paths = []\n\n        # add the multi object list\n        # Note: (manuelli) why is this treated differently than the single object\n        # case?\n        evaluation_labeled_data_paths += dataset.config[""multi_object""][""evaluation_labeled_data_path""]\n        \n        # add all of the single object lists\n        for object_key, val in dataset.config[""single_object""].iteritems():\n            if ""evaluation_labeled_data_path"" in val:\n                evaluation_labeled_data_paths += val[""evaluation_labeled_data_path""]\n\n        if len(evaluation_labeled_data_paths) == 0:\n            print ""Could not find labeled cross scene data for this dataset.""\n            print ""It needs to be set in the dataset.yaml of the folder from which""\n            print ""this network is loaded from.""\n            return\n\n        cross_scene_data = []\n\n        for path in evaluation_labeled_data_paths:\n            cross_scene_data_full_path = utils.convert_data_relative_path_to_absolute_path(path, assert_path_exists=True)\n            this_cross_scene_data = utils.getDictFromYamlFilename(cross_scene_data_full_path)\n            cross_scene_data += this_cross_scene_data\n\n        return cross_scene_data\n\n    @staticmethod\n    def evaluate_network_qualitative_cross_scene(dcn, dataset, draw_human_annotations=True):\n        """"""\n        This will search for the ""evaluation_labeled_data_path"" in the dataset.yaml,\n        and use pairs of images that have been human-labeled across scenes.\n        """"""\n\n        dcn.eval()\n\n        cross_scene_data = DenseCorrespondenceEvaluation.parse_cross_scene_data(dataset)\n        \n        for annotated_pair in cross_scene_data:\n\n            scene_name_a = annotated_pair[""image_a""][""scene_name""]\n            scene_name_b = annotated_pair[""image_b""][""scene_name""]\n            # some labeled cross scene data may not be in the configured dataset\n            # this checks that the scenes actually exist (i.e., have been downloaded)\n            if not os.path.isdir(dataset.get_full_path_for_scene(scene_name_a))\\\n            or not os.path.isdir(dataset.get_full_path_for_scene(scene_name_b)):\n                print(""at least one of these scene names does not exist:"", scene_name_a, scene_name_b)\n                continue\n            print(""these scene names exist:"", scene_name_a, scene_name_b)    \n            image_a_idx = annotated_pair[""image_a""][""image_idx""]\n            image_b_idx = annotated_pair[""image_b""][""image_idx""]\n\n            rgb_a, rgb_b = DenseCorrespondenceEvaluation.single_cross_scene_image_pair_qualitative_analysis(\\\n                dcn, dataset, scene_name_a, image_a_idx, scene_name_b, image_b_idx)\n\n\n            if draw_human_annotations:\n                img_a_points_picked = annotated_pair[""image_a""][""pixels""]\n                img_b_points_picked = annotated_pair[""image_b""][""pixels""]\n\n                # note here: converting the rgb_a to numpy format, but not inverting\n                # the RGB <--> BGR colors as cv2 would expect, because all I\'m going to do is then\n                # plot this as an image in matplotlib, in which case\n                # would just need to switch the colors back.\n                rgb_a = dc_plotting.draw_correspondence_points_cv2(np.asarray(rgb_a), img_a_points_picked)\n                rgb_b = dc_plotting.draw_correspondence_points_cv2(np.asarray(rgb_b), img_b_points_picked)\n\n                fig, axes = plt.subplots(nrows=1, ncols=2)\n                fig.set_figheight(10)\n                fig.set_figwidth(15)\n                axes[0].imshow(rgb_a)\n                axes[1].imshow(rgb_b)\n                plt.show()\n\n\n    @staticmethod \n    def get_random_image_pairs(dataset):\n        """"""\n        Given a dataset, chose a random scene, and a handful of image pairs from\n        that scene.\n\n        :param dataset: dataset from which to draw a scene and image pairs\n        :type dataset: SpartanDataset\n\n        :return: scene_name, img_pairs\n        :rtype: str, list of lists, where each of the lists are [img_a_idx, img_b_idx], for example:\n            [[113,220],\n             [114,225]]\n        """"""\n        scene_name = dataset.get_random_scene_name()\n        img_pairs = []\n        for _ in range(5):\n            img_a_idx  = dataset.get_random_image_index(scene_name)\n            pose_a     = dataset.get_pose_from_scene_name_and_idx(scene_name, img_a_idx)\n            img_b_idx  = dataset.get_img_idx_with_different_pose(scene_name, pose_a, num_attempts=100)\n            if img_b_idx is None:\n                continue\n            img_pairs.append([img_a_idx, img_b_idx])\n        return scene_name, img_pairs\n\n    @staticmethod\n    def get_random_scenes_and_image_pairs(dataset):\n        """"""\n        Given a dataset, chose a variety of random scenes and image pairs\n\n        :param dataset: dataset from which to draw a scene and image pairs\n        :type dataset: SpartanDataset\n\n        :return: scene_names, img_pairs\n        :rtype: list[str], list of lists, where each of the lists are [img_a_idx, img_b_idx], for example:\n            [[113,220],\n             [114,225]]\n        """"""\n\n        scene_names = []\n\n        img_pairs = []\n        for _ in range(5):\n            scene_name = dataset.get_random_scene_name()\n            img_a_idx = dataset.get_random_image_index(scene_name)\n            pose_a = dataset.get_pose_from_scene_name_and_idx(scene_name, img_a_idx)\n            img_b_idx = dataset.get_img_idx_with_different_pose(scene_name, pose_a, num_attempts=100)\n            if img_b_idx is None:\n                continue\n            img_pairs.append([img_a_idx, img_b_idx])\n            scene_names.append(scene_name)\n\n        return scene_names, img_pairs\n\n    @staticmethod\n    def evaluate_network_qualitative(dcn, dataset, num_image_pairs=5, randomize=False,\n                                     scene_type=None):\n\n        dcn.eval()\n        # Train Data\n        print ""\\n\\n-----------Train Data Evaluation----------------""\n        if randomize:\n            scene_names, img_pairs = DenseCorrespondenceEvaluation.get_random_scenes_and_image_pairs(dataset)\n        else:\n            if scene_type == ""caterpillar"":\n                scene_name = \'2018-04-10-16-06-26\'\n                img_pairs = []\n                img_pairs.append([0,753])\n                img_pairs.append([812, 1218])\n                img_pairs.append([1430, 1091])\n                img_pairs.append([1070, 649])\n            elif scene_type == ""drill"":\n                scene_name = \'13_drill_long_downsampled\'\n                img_pairs = []\n                img_pairs.append([0, 737])\n                img_pairs.append([409, 1585])\n                img_pairs.append([2139, 1041])\n                img_pairs.append([235, 1704])\n            else:\n                raise ValueError(""scene_type must be one of [drill, caterpillar], it was %s)"" %(scene_type))\n\n            scene_names = [scene_name]*len(img_pairs)\n\n        for scene_name, img_pair in zip(scene_names, img_pairs):\n            print ""Image pair (%d, %d)"" %(img_pair[0], img_pair[1])\n            DenseCorrespondenceEvaluation.single_same_scene_image_pair_qualitative_analysis(dcn,\n                                                                                 dataset,\n                                                                                 scene_name,\n                                                                                 img_pair[0],\n                                                                                 img_pair[1])\n\n        # Test Data\n        print ""\\n\\n-----------Test Data Evaluation----------------""\n        dataset.set_test_mode()\n        if randomize:\n            scene_names, img_pairs = DenseCorrespondenceEvaluation.get_random_scenes_and_image_pairs(dataset)\n        else:\n            if scene_type == ""caterpillar"":\n                scene_name = \'2018-04-10-16-08-46\'\n                img_pairs = []\n                img_pairs.append([0, 754])\n                img_pairs.append([813, 1219])\n                img_pairs.append([1429, 1092])\n                img_pairs.append([1071, 637])\n            elif scene_type == ""drill"":\n                scene_name = \'06_drill_long_downsampled\'\n                img_pairs = []\n                img_pairs.append([0, 617])\n                img_pairs.append([270, 786])\n                img_pairs.append([1001, 2489])\n                img_pairs.append([1536, 1917])\n            else:\n                raise ValueError(""scene_type must be one of [drill, caterpillar], it was %s)"" % (scene_type))\n\n            scene_names = [scene_name] * len(img_pairs)\n\n\n        for scene_name, img_pair in zip(scene_names, img_pairs):\n            print ""Image pair (%d, %d)"" %(img_pair[0], img_pair[1])\n            DenseCorrespondenceEvaluation.single_same_scene_image_pair_qualitative_analysis(dcn,\n                                                                                 dataset,\n                                                                                 scene_name,\n                                                                                 img_pair[0],\n                                                                                 img_pair[1])\n\n        if scene_type == ""caterpillar"":\n            # Train Data\n            print ""\\n\\n-----------More Test Data Evaluation----------------""\n            if randomize:\n                scene_name, img_pairs = DenseCorrespondenceEvaluation.get_random_image_pairs(dataset)\n            else:\n\n                scene_name = \'2018-04-16-14-25-19\'\n                img_pairs = []\n                img_pairs.append([0,1553])\n                img_pairs.append([1729, 2386])\n                img_pairs.append([2903, 1751])\n                img_pairs.append([841, 771])\n\n            for img_pair in img_pairs:\n                print ""Image pair (%d, %d)"" %(img_pair[0], img_pair[1])\n                DenseCorrespondenceEvaluation.single_same_scene_image_pair_qualitative_analysis(dcn,\n                                                                                 dataset,\n                                                                                 scene_name,\n                                                                                 img_pair[0],\n                                                                                 img_pair[1])\n\n\n    @staticmethod\n    def compute_loss_on_dataset(dcn, data_loader, loss_config, num_iterations=500,):\n        """"""\n\n        Computes the loss for the given number of iterations\n\n        :param dcn:\n        :type dcn:\n        :param data_loader:\n        :type data_loader:\n        :param num_iterations:\n        :type num_iterations:\n        :return:\n        :rtype:\n        """"""\n        dcn.eval()\n\n        # loss_vec = np.zeros(num_iterations)\n        loss_vec = []\n        match_loss_vec = []\n        non_match_loss_vec = []\n        counter = 0\n        pixelwise_contrastive_loss = PixelwiseContrastiveLoss(dcn.image_shape, config=loss_config)\n\n        batch_size = 1\n\n        for i, data in enumerate(data_loader, 0):\n\n            # get the inputs\n            data_type, img_a, img_b, matches_a, matches_b, non_matches_a, non_matches_b, metadata = data\n            data_type = data_type[0]\n\n            if len(matches_a[0]) == 0:\n                print ""didn\'t have any matches, continuing""\n                continue\n\n            img_a = Variable(img_a.cuda(), requires_grad=False)\n            img_b = Variable(img_b.cuda(), requires_grad=False)\n\n            if data_type == ""matches"":\n                matches_a = Variable(matches_a.cuda().squeeze(0), requires_grad=False)\n                matches_b = Variable(matches_b.cuda().squeeze(0), requires_grad=False)\n                non_matches_a = Variable(non_matches_a.cuda().squeeze(0), requires_grad=False)\n                non_matches_b = Variable(non_matches_b.cuda().squeeze(0), requires_grad=False)\n\n            # run both images through the network\n            image_a_pred = dcn.forward(img_a)\n            image_a_pred = dcn.process_network_output(image_a_pred, batch_size)\n\n            image_b_pred = dcn.forward(img_b)\n            image_b_pred = dcn.process_network_output(image_b_pred, batch_size)\n\n            # get loss\n            if data_type == ""matches"":\n                loss, match_loss, non_match_loss = \\\n                    pixelwise_contrastive_loss.get_loss(image_a_pred,\n                                                        image_b_pred,\n                                                        matches_a,\n                                                        matches_b,\n                                                        non_matches_a,\n                                                        non_matches_b)\n\n\n\n                loss_vec.append(loss.data[0])\n                non_match_loss_vec.append(non_match_loss.data[0])\n                match_loss_vec.append(match_loss.data[0])\n\n\n            if i > num_iterations:\n                break\n\n        loss_vec = np.array(loss_vec)\n        match_loss_vec = np.array(match_loss_vec)\n        non_match_loss_vec = np.array(non_match_loss_vec)\n\n        loss = np.average(loss_vec)\n        match_loss = np.average(match_loss_vec)\n        non_match_loss = np.average(non_match_loss_vec)\n\n        return loss, match_loss, non_match_loss\n\n\n\n    @staticmethod\n    def compute_descriptor_statistics_on_dataset(dcn, dataset, num_images=100,\n                                                 save_to_file=True, filename=None):\n        """"""\n        Computes the statistics of the descriptors on the dataset\n        :param dcn:\n        :type dcn:\n        :param dataset:\n        :type dataset:\n        :param save_to_file:\n        :type save_to_file:\n        :return:\n        :rtype:\n        """"""\n\n        utils.reset_random_seed()\n\n        dcn.eval()\n        to_tensor = transforms.ToTensor()\n\n        # compute the per-channel mean\n        def compute_descriptor_statistics(res, mask_tensor):\n            """"""\n            Computes\n            :param res: The output of the DCN\n            :type res: torch.FloatTensor with shape [H,W,D]\n            :return: min, max, mean\n            :rtype: each is torch.FloatTensor of shape [D]\n            """"""\n            # convert to [W*H, D]\n            D = res.shape[2]\n\n            # convert to torch.FloatTensor instead of variable\n            if isinstance(res, torch.autograd.Variable):\n                res = res.data\n\n            res_reshape = res.contiguous().view(-1,D)\n            channel_mean = res_reshape.mean(0) # shape [D]\n            channel_min, _ = res_reshape.min(0) # shape [D]\n            channel_max, _ = res_reshape.max(0) # shape [D]\n\n            \n            mask_flat = mask_tensor.view(-1,1).squeeze(1)\n\n            # now do the same for the masked image\n            # gracefully handle the case where the mask is all zeros\n            mask_indices_flat = torch.nonzero(mask_flat)\n            if len(mask_indices_flat) == 0:\n                return None, None     \n\n            mask_indices_flat = mask_indices_flat.squeeze(1)\n            \n                \n            # print ""mask_flat.shape"", mask_flat.shape\n\n            res_masked_flat = res_reshape.index_select(0, mask_indices_flat) # shape [mask_size, D]\n            mask_channel_mean = res_masked_flat.mean(0)\n            mask_channel_min, _ = res_masked_flat.min(0)\n            mask_channel_max, _ = res_masked_flat.max(0)\n\n\n            entire_image_stats = (channel_min, channel_max, channel_mean)\n            mask_image_stats = (mask_channel_min, mask_channel_max, mask_channel_mean)\n            return entire_image_stats, mask_image_stats\n\n        def compute_descriptor_std_dev(res, channel_mean):\n            """"""\n            Computes the std deviation of a descriptor image, given a channel mean\n            :param res:\n            :type res:\n            :param channel_mean:\n            :type channel_mean:\n            :return:\n            :rtype:\n            """"""\n            D = res.shape[2]\n            res_reshape = res.view(-1, D) # shape [W*H,D]\n            v = res - channel_mean\n            std_dev = torch.std(v, 0) # shape [D]\n            return std_dev\n\n        def update_stats(stats_dict, single_img_stats):\n            """"""\n            Update the running mean, min and max\n            :param stats_dict:\n            :type stats_dict:\n            :param single_img_stats:\n            :type single_img_stats:\n            :return:\n            :rtype:\n            """"""\n\n            min_temp, max_temp, mean_temp = single_img_stats\n\n            if stats_dict[\'min\'] is None:\n                stats_dict[\'min\'] = min_temp\n            else:\n                stats_dict[\'min\'] = torch.min(stats_dict[\'min\'], min_temp)\n\n            if stats_dict[\'max\'] is None:\n                stats_dict[\'max\'] = max_temp\n            else:\n                stats_dict[\'max\'] = torch.max(stats_dict[\'max\'], max_temp)\n\n            if stats_dict[\'mean\'] is None:\n                stats_dict[\'mean\'] = mean_temp\n            else:\n                stats_dict[\'mean\'] += mean_temp\n\n\n        stats = dict()\n        stats[\'entire_image\'] = {\'mean\': None, \'max\': None, \'min\': None}\n        stats[\'mask_image\'] = {\'mean\': None, \'max\': None, \'min\': None}\n\n        for i in xrange(0,num_images):\n            rgb, depth, mask, _ = dataset.get_random_rgbd_mask_pose()\n            img_tensor = dataset.rgb_image_to_tensor(rgb)\n            res = dcn.forward_single_image_tensor(img_tensor)  # [H, W, D]\n\n            mask_tensor = to_tensor(mask).cuda()\n            entire_image_stats, mask_image_stats = compute_descriptor_statistics(res, mask_tensor)\n\n\n            # handles the case of an empty mask\n            if mask_image_stats is None:\n                logging.info(""Mask was empty, skipping"")\n                continue\n\n\n            update_stats(stats[\'entire_image\'], entire_image_stats)\n            update_stats(stats[\'mask_image\'], mask_image_stats)\n\n\n        for key, val in stats.iteritems():\n            val[\'mean\'] = 1.0/num_images * val[\'mean\']\n            for field in val:\n                val[field] = val[field].tolist()\n\n        if save_to_file:\n            if filename is None:\n                path_to_params_folder = dcn.config[\'path_to_network_params_folder\']\n                path_to_params_folder = utils.convert_data_relative_path_to_absolute_path(path_to_params_folder)\n                filename = os.path.join(path_to_params_folder, \'descriptor_statistics.yaml\')\n\n            utils.saveToYaml(stats, filename)\n\n\n\n        return stats\n\n\n    @staticmethod\n    def run_evaluation_on_network(model_folder, num_image_pairs=100,\n                                  num_matches_per_image_pair=100,\n                                  save_folder_name=""analysis"",\n                                  compute_descriptor_statistics=True, \n                                  cross_scene=True,\n                                  dataset=None,\n                                  iteration=None):\n        """"""\n        Runs all the quantitative evaluations on the model folder\n        Creates a folder model_folder/analysis that stores the information.\n\n        Performs several steps:\n\n        1. compute dataset descriptor stats\n        2. compute quantitative eval csv files\n        3. make quantitative plots, save as a png for easy viewing\n\n\n        :param model_folder:\n        :type model_folder:\n        :return:\n        :rtype:\n        """"""\n\n        utils.reset_random_seed()\n\n        DCE = DenseCorrespondenceEvaluation\n\n        model_folder = utils.convert_data_relative_path_to_absolute_path(model_folder, assert_path_exists=True)\n\n        # save it to a csv file\n        output_dir = os.path.join(model_folder, save_folder_name)\n        train_output_dir = os.path.join(output_dir, ""train"")\n        test_output_dir = os.path.join(output_dir, ""test"")\n        cross_scene_output_dir = os.path.join(output_dir, ""cross_scene"")\n\n        # create the necessary directories\n        for dir in [output_dir, train_output_dir, test_output_dir, cross_scene_output_dir]:\n            if not os.path.isdir(dir):\n                os.makedirs(dir)\n\n\n        dcn = DenseCorrespondenceNetwork.from_model_folder(model_folder, iteration=iteration)\n        dcn.eval()\n\n        if dataset is None:\n            dataset = dcn.load_training_dataset()\n\n        # compute dataset statistics\n        if compute_descriptor_statistics:\n            logging.info(""Computing descriptor statistics on dataset"")\n            DCE.compute_descriptor_statistics_on_dataset(dcn, dataset, num_images=100, save_to_file=True)\n\n\n        # evaluate on training data and on test data\n        logging.info(""Evaluating network on train data"")\n        dataset.set_train_mode()\n        pd_dataframe_list, df = DCE.evaluate_network(dcn, dataset, num_image_pairs=num_image_pairs,\n                                                     num_matches_per_image_pair=num_matches_per_image_pair)\n\n        train_csv = os.path.join(train_output_dir, ""data.csv"")\n        df.to_csv(train_csv)\n\n        logging.info(""Evaluating network on test data"")\n        dataset.set_test_mode()\n        pd_dataframe_list, df = DCE.evaluate_network(dcn, dataset, num_image_pairs=num_image_pairs,\n                                                     num_matches_per_image_pair=num_matches_per_image_pair)\n\n        test_csv = os.path.join(test_output_dir, ""data.csv"")\n        df.to_csv(test_csv)\n\n\n        if cross_scene:\n            logging.info(""Evaluating network on cross scene data"")\n            df = DCE.evaluate_network_cross_scene(dcn=dcn, dataset=dataset, save=False)\n            cross_scene_csv = os.path.join(cross_scene_output_dir, ""data.csv"")\n            df.to_csv(cross_scene_csv)\n\n        logging.info(""Making plots"")\n        DCEP = DenseCorrespondenceEvaluationPlotter\n        fig_axes = DCEP.run_on_single_dataframe(train_csv, label=""train"", save=False)\n        fig_axes = DCEP.run_on_single_dataframe(test_csv, label=""test"", save=False, previous_fig_axes=fig_axes)\n        if cross_scene:\n            fig_axes = DCEP.run_on_single_dataframe(cross_scene_csv, label=""cross_scene"", save=False,\n                                                    previous_fig_axes=fig_axes)\n\n        fig, _ = fig_axes        \n        save_fig_file = os.path.join(output_dir, ""quant_plots.png"")\n        fig.savefig(save_fig_file)\n\n        # only do across object analysis if have multiple single objects\n        if dataset.get_number_of_unique_single_objects() > 1:\n            across_object_output_dir = os.path.join(output_dir, ""across_object"")\n            if not os.path.isdir(across_object_output_dir):\n                os.makedirs(across_object_output_dir)\n            logging.info(""Evaluating network on across object data"")\n            df = DCE.evaluate_network_across_objects(dcn=dcn, dataset=dataset)\n            across_object_csv = os.path.join(across_object_output_dir, ""data.csv"")\n            df.to_csv(across_object_csv)\n            DCEP.run_on_single_dataframe_across_objects(across_object_csv, label=""across_object"", save=True)\n\n\n        logging.info(""Finished running evaluation on network"")\n\n    @staticmethod\n    def run_cross_instance_keypoint_evaluation_on_network(model_folder, path_to_cross_instance_labels,\n                                  save_folder_name=""analysis/cross_scene_keypoints"",\n                                  dataset=None, save=True):\n        """"""\n        Runs cross instance keypoint evaluation on the given network.\n        Creates a folder model_folder/<save_folder_name> that stores the information.\n\n        :param model_folder: folder where trained network is\n        :type model_folder:\n        :param path_to_cross_instance_labels: path to location of cross instance data. Can be full path\n        or path relative to the data directory\n        :type path_to_cross_instance_labels: str\n        :param save_folder_name: place where data is being saved\n        :type save_folder_name: str\n        :param dataset: SpartanDataset\n        :type dataset:\n        :return:\n        :rtype:\n        """"""\n        utils.reset_random_seed()\n\n        DCE = DenseCorrespondenceEvaluation\n\n        model_folder = utils.convert_data_relative_path_to_absolute_path(model_folder, assert_path_exists=True)\n\n\n\n        dcn = DenseCorrespondenceNetwork.from_model_folder(model_folder)\n        dcn.eval()\n\n        if dataset is None:\n            dataset = dcn.load_training_dataset()\n\n        path_to_cross_instance_labels = utils.convert_data_relative_path_to_absolute_path(path_to_cross_instance_labels, assert_path_exists=True)\n        df = DCE.evaluate_network_cross_scene_keypoints(dcn, dataset, path_to_cross_instance_labels)\n\n\n        if save:\n            # save it to a csv file\n            output_dir = os.path.join(model_folder, save_folder_name)\n            # create the necessary directories\n            for dir in [output_dir]:\n                if not os.path.isdir(dir):\n                    os.makedirs(dir)\n\n            save_filename = os.path.join(output_dir, \'data.csv\')\n            df.to_csv(save_filename)\n\n        logging.info(""Finished running cross scene keypoint evaluation"")\n        return df\n\n\n    @staticmethod\n    def make_2d_cluster_plot(dcn, dataset, plot_background=False):\n        """"""\n        This function randomly samples many points off of different objects and the background,\n        and makes an object-labeled scatter plot of where these descriptors are.\n        """"""\n\n        print ""Checking to make sure this is a 2D or 3D descriptor""\n        print ""If you\'d like you could add projection methods for higher dimension descriptors""\n        assert ((dcn.descriptor_dimension == 2) or (dcn.descriptor_dimension == 3))\n\n        if dcn.descriptor_dimension == 3:\n            use_3d = True\n            d = 3\n            print ""This descriptor_dimension is 3d""\n            print ""I\'m going to make 3 plots for you: xy, yz, xz""\n        else:\n            use_3d = False\n            d = 2\n\n        # randomly grab object ID, and scene\n\n        # Fixing random state for reproducibility\n        np.random.seed(19680801)\n\n        descriptors_known_objects_samples = dict()\n        if use_3d:\n            descriptors_known_objects_samples_xy = dict()\n            descriptors_known_objects_samples_yz = dict()\n            descriptors_known_objects_samples_xz = dict()\n\n        descriptors_background_samples = np.zeros((0,d))\n\n        if use_3d:\n            descriptors_background_samples_xy = np.zeros((0,2))\n            descriptors_background_samples_yz = np.zeros((0,2))\n            descriptors_background_samples_xz = np.zeros((0,2))\n\n        num_objects = dataset.get_number_of_unique_single_objects()\n        num_samples_per_image = 100\n\n        for i in range(100):\n            object_id, object_id_int = dataset.get_random_object_id_and_int()\n\n            scene_name = dataset.get_random_single_object_scene_name(object_id)\n            img_idx = dataset.get_random_image_index(scene_name)\n            rgb = dataset.get_rgb_image_from_scene_name_and_idx(scene_name, img_idx)\n            mask = dataset.get_mask_image_from_scene_name_and_idx(scene_name, img_idx)\n\n            mask_torch = torch.from_numpy(np.asarray(mask)).long()\n            mask_inv = 1 - mask_torch\n\n            object_uv_samples     = correspondence_finder.random_sample_from_masked_image_torch(mask_torch, num_samples_per_image)\n            background_uv_samples = correspondence_finder.random_sample_from_masked_image_torch(mask_inv, num_samples_per_image/num_objects)\n\n            object_u_samples = object_uv_samples[0].numpy()\n            object_v_samples = object_uv_samples[1].numpy()\n\n            background_u_samples = background_uv_samples[0].numpy()\n            background_v_samples = background_uv_samples[1].numpy()\n\n            # This snippet will plot where the samples are coming from in the image\n            # plt.scatter(object_u_samples, object_v_samples, c=""g"", alpha=0.5, label=""object"")\n            # plt.scatter(background_u_samples, background_v_samples, c=""k"", alpha=0.5, label=""background"")\n            # plt.legend()\n            # plt.show()\n\n            img_tensor = dataset.rgb_image_to_tensor(rgb)\n            res = dcn.forward_single_image_tensor(img_tensor)  # [H, W, D]\n            res = res.data.cpu().numpy()\n\n            descriptors_object = np.zeros((len(object_u_samples),d))\n            for j in range(len(object_u_samples)):\n                descriptors_object[j,:] = res[object_v_samples[j], object_u_samples[j], :]\n            if use_3d:\n                descriptors_object_xy = np.zeros((len(object_u_samples),2))\n                descriptors_object_yz = np.zeros((len(object_u_samples),2))\n                descriptors_object_xz = np.zeros((len(object_u_samples),2))\n                for j in range(len(object_u_samples)):\n                    descriptors_object_xy[j,:] = res[object_v_samples[j], object_u_samples[j], 0:2]\n                    descriptors_object_yz[j,:] = res[object_v_samples[j], object_u_samples[j], 1:3]\n                    descriptors_object_xz[j,:] = res[object_v_samples[j], object_u_samples[j], 0::2]\n\n            descriptors_background = np.zeros((len(background_u_samples),d))\n            for j in range(len(background_u_samples)):\n                descriptors_background[j,:] = res[background_v_samples[j], background_u_samples[j], :]\n            if use_3d:\n                descriptors_background_xy = np.zeros((len(background_u_samples),2))\n                descriptors_background_yz = np.zeros((len(background_u_samples),2))\n                descriptors_background_xz = np.zeros((len(background_u_samples),2))\n                for j in range(len(background_u_samples)):\n                    descriptors_background_xy[j,:] = res[background_v_samples[j], background_u_samples[j], 0:2]\n                    descriptors_background_yz[j,:] = res[background_v_samples[j], background_u_samples[j], 1:3]\n                    descriptors_background_xz[j,:] = res[background_v_samples[j], background_u_samples[j], 0::2]\n\n\n            # This snippet will plot the descriptors just from this image\n            # plt.scatter(descriptors_object[:,0], descriptors_object[:,1], c=""g"", alpha=0.5, label=object_id)\n            # plt.scatter(descriptors_background[:,0], descriptors_background[:,1], c=""k"", alpha=0.5, label=""background"")\n            # plt.legend()\n            # plt.show()\n\n            if object_id not in descriptors_known_objects_samples:\n                descriptors_known_objects_samples[object_id] = descriptors_object\n                if use_3d:\n                    descriptors_known_objects_samples_xy[object_id] = descriptors_object_xy\n                    descriptors_known_objects_samples_yz[object_id] = descriptors_object_yz\n                    descriptors_known_objects_samples_xz[object_id] = descriptors_object_xz\n            else:\n                descriptors_known_objects_samples[object_id] = np.vstack((descriptors_known_objects_samples[object_id], descriptors_object))\n                if use_3d:\n                    descriptors_known_objects_samples_xy[object_id] = np.vstack((descriptors_known_objects_samples_xy[object_id], descriptors_object_xy))\n                    descriptors_known_objects_samples_yz[object_id] = np.vstack((descriptors_known_objects_samples_yz[object_id], descriptors_object_yz))\n                    descriptors_known_objects_samples_xz[object_id] = np.vstack((descriptors_known_objects_samples_xz[object_id], descriptors_object_xz))\n\n            descriptors_background_samples = np.vstack((descriptors_background_samples, descriptors_background))\n            if use_3d:\n                descriptors_background_samples_xy = np.vstack((descriptors_background_samples_xy, descriptors_background_xy))\n                descriptors_background_samples_yz = np.vstack((descriptors_background_samples_yz, descriptors_background_yz))\n                descriptors_background_samples_xz = np.vstack((descriptors_background_samples_xz, descriptors_background_xz))\n\n\n\n        print ""ALL""\n        if not use_3d:\n            for key, value in descriptors_known_objects_samples.iteritems():\n                plt.scatter(value[:,0], value[:,1], alpha=0.5, label=key)\n\n            if plot_background:\n                plt.scatter(descriptors_background_samples[:,0], descriptors_background_samples[:,1], alpha=0.5, label=""background"")\n            plt.legend()\n            plt.show()\n        \n        if use_3d:\n            for key, value in descriptors_known_objects_samples_xy.iteritems():\n                plt.scatter(value[:,0], value[:,1], alpha=0.5, label=key)\n            if plot_background:\n                plt.scatter(descriptors_background_samples_xy[:,0], descriptors_background_samples_xy[:,1], alpha=0.5, label=""background"")\n            plt.legend()\n            plt.show()\n\n            for key, value in descriptors_known_objects_samples_yz.iteritems():\n                plt.scatter(value[:,0], value[:,1], alpha=0.5, label=key)\n            if plot_background:\n                plt.scatter(descriptors_background_samples_yz[:,0], descriptors_background_samples_yz[:,1], alpha=0.5, label=""background"")\n            plt.legend()\n            plt.show()\n\n            for key, value in descriptors_known_objects_samples_xz.iteritems():\n                plt.scatter(value[:,0], value[:,1], alpha=0.5, label=key)\n            if plot_background:\n                plt.scatter(descriptors_background_samples_xz[:,0], descriptors_background_samples_xz[:,1], alpha=0.5, label=""background"")\n            plt.legend()\n            plt.show()\n\n        print ""done""\n\n    @staticmethod\n    def make_default():\n        """"""\n        Makes a DenseCorrespondenceEvaluation object using the default config\n        :return:\n        :rtype: DenseCorrespondenceEvaluation\n        """"""\n        config_filename = os.path.join(utils.getDenseCorrespondenceSourceDir(), \'config\', \'dense_correspondence\', \'evaluation\', \'evaluation.yaml\')\n        config = utils.getDictFromYamlFilename(config_filename)\n        return DenseCorrespondenceEvaluation(config)\n\n    ############ TESTING ################\n\n\n    @staticmethod\n    def test(dcn, dataset, data_idx=1, visualize=False, debug=False, match_idx=10):\n\n        scene_name = \'13_drill_long_downsampled\'\n        img_idx_a = utils.getPaddedString(0)\n        img_idx_b = utils.getPaddedString(737)\n\n        DenseCorrespondenceEvaluation.single_same_scene_image_pair_qualitative_analysis(dcn, dataset,\n                                                                             scene_name, img_idx_a,\n                                                                             img_idx_b)\n\nclass DenseCorrespondenceEvaluationPlotter(object):\n    """"""\n    This class contains plotting utilities. They are all\n    encapsulated as static methods\n\n    """"""\n\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def make_cdf_plot(ax, data, num_bins, label=None, x_axis_scale_factor=1):\n        """"""\n        Plots the empirical CDF of the data\n        :param ax: axis of a matplotlib plot to plot on\n        :param data:\n        :type data:\n        :param num_bins:\n        :type num_bins:\n        :return:\n        :rtype:\n        """"""\n        cumhist, l, b, e = ss.cumfreq(data, num_bins)\n        cumhist *= 1.0 / len(data)\n        x_axis = l + b * np.arange(0, num_bins)\n        x_axis /= x_axis_scale_factor\n        plot = ax.plot(x_axis, cumhist, label=label)\n        return plot\n\n    @staticmethod\n    def make_pixel_match_error_plot(ax, df, label=None, num_bins=100, masked=False):\n        """"""\n        :param ax: axis of a matplotlib plot to plot on\n        :param df: pandas dataframe, i.e. generated from quantitative \n        :param num_bins:\n        :type num_bins:\n        :return:\n        :rtype:\n        """"""\n        DCEP = DenseCorrespondenceEvaluationPlotter\n\n        \n        if masked:\n            data_string = \'pixel_match_error_l2_masked\'\n        else:\n            data_string = \'pixel_match_error_l2\' \n\n        data = df[data_string]\n\n        # rescales the pixel distance to be relative to the diagonal of the image\n        x_axis_scale_factor = 800\n\n        plot = DCEP.make_cdf_plot(ax, data, num_bins=num_bins, label=label, x_axis_scale_factor=x_axis_scale_factor)\n        if masked:\n            ax.set_xlabel(\'Pixel match error (masked), L2 (pixel distance)\')\n        else:\n            ax.set_xlabel(\'Pixel match error (fraction of image), L2 (pixel distance)\')\n        ax.set_ylabel(\'Fraction of images\')\n\n        # ax.set_xlim([0,200])\n        return plot\n\n    @staticmethod\n    def make_across_object_best_match_plot(ax, df, label=None, num_bins=100):\n        """"""\n        :param ax: axis of a matplotlib plot to plot on\n        :param df: pandas dataframe, i.e. generated from quantitative \n        :param num_bins:\n        :type num_bins:\n        :return:\n        :rtype:\n        """"""\n        DCEP = DenseCorrespondenceEvaluationPlotter\n\n        data = df[\'norm_diff_descriptor_best_match\']\n\n        plot = DCEP.make_cdf_plot(ax, data, num_bins=num_bins, label=label)\n        ax.set_xlabel(\'Best descriptor match, L2 norm\')\n        ax.set_ylabel(\'Fraction of pixel samples from images\')\n        return plot\n\n    @staticmethod\n    def make_descriptor_accuracy_plot(ax, df, label=None, num_bins=100, masked=False):\n        """"""\n        Makes a plot of best match accuracy.\n        Drops nans\n        :param ax: axis of a matplotlib plot to plot on\n        :param df:\n        :type df:\n        :param num_bins:\n        :type num_bins:\n        :return:\n        :rtype:\n        """"""\n        DCEP = DenseCorrespondenceEvaluationPlotter\n\n        if masked:\n            data_string = \'norm_diff_pred_3d_masked\'\n        else:\n            data_string = \'norm_diff_pred_3d\' \n\n\n        data = df[data_string]\n        data = data.dropna()\n        data *= 100 # convert to cm\n\n        plot = DCEP.make_cdf_plot(ax, data, num_bins=num_bins, label=label)\n        if masked:\n            ax.set_xlabel(\'3D match error (masked), L2 (cm)\')\n        else:\n            ax.set_xlabel(\'3D match error, L2 (cm)\')\n        ax.set_ylabel(\'Fraction of images\')\n        #ax.set_title(""3D Norm Diff Best Match"")\n        return plot\n\n    @staticmethod\n    def make_norm_diff_ground_truth_plot(ax, df, label=None, num_bins=100, masked=False):\n        """"""\n        :param ax: axis of a matplotlib plot to plot on\n        :param df:\n        :type df:\n        :param num_bins:\n        :type num_bins:\n        :return:\n        :rtype:\n        """"""\n        DCEP = DenseCorrespondenceEvaluationPlotter\n\n        data = df[\'norm_diff_descriptor_ground_truth\']\n        \n        plot = DCEP.make_cdf_plot(ax, data, num_bins=num_bins, label=label)\n        ax.set_xlabel(\'Descriptor match error, L2\')\n        ax.set_ylabel(\'Fraction of images\')\n        return plot\n\n    @staticmethod\n    def make_fraction_false_positives_plot(ax, df, label=None, num_bins=100, masked=False):\n        """"""\n        :param ax: axis of a matplotlib plot to plot on\n        :param df:\n        :type df:\n        :param num_bins:\n        :type num_bins:\n        :return:\n        :rtype:\n        """"""\n        DCEP = DenseCorrespondenceEvaluationPlotter\n\n        if masked:\n            data_string = \'fraction_pixels_closer_than_ground_truth_masked\'\n        else:\n            data_string = \'fraction_pixels_closer_than_ground_truth\' \n\n        data = df[data_string]\n        \n        plot = DCEP.make_cdf_plot(ax, data, num_bins=num_bins, label=label)\n        \n        if masked:\n            ax.set_xlabel(\'Fraction false positives (masked)\')\n        else:\n            ax.set_xlabel(\'Fraction false positives\')    \n\n        ax.set_ylabel(\'Fraction of images\')\n        ax.set_xlim([0, 1])\n        ax.set_ylim([0, 1])\n        return plot\n\n    @staticmethod\n    def make_average_l2_false_positives_plot(ax, df, label=None, num_bins=100, masked=False):\n        """"""\n        :param ax: axis of a matplotlib plot to plot on\n        :param df:\n        :type df:\n        :param num_bins:\n        :type num_bins:\n        :return:\n        :rtype:\n        """"""\n        DCEP = DenseCorrespondenceEvaluationPlotter\n\n        if masked:\n            data_string = \'average_l2_distance_for_false_positives_masked\'\n        else:\n            data_string = \'average_l2_distance_for_false_positives\'\n\n        data = df[data_string]\n        \n        plot = DCEP.make_cdf_plot(ax, data, num_bins=num_bins, label=label)\n        if masked:\n            ax.set_xlabel(\'Average l2 pixel distance for false positives (masked)\')\n        else:\n            ax.set_xlabel(\'Average l2 pixel distance for false positives\')\n        ax.set_ylabel(\'Fraction of images\')\n        # ax.set_xlim([0,200])\n        return plot\n\n    @staticmethod\n    def compute_area_above_curve(df, field, num_bins=100):\n        """"""\n        Computes AOC for the entries in that field\n        :param df:\n        :type df: Pandas.DataFrame\n        :param field: specifies which column of the DataFrame to use\n        :type field: str\n        :return:\n        :rtype:\n        """"""\n\n        data = df[field]\n        data = data.dropna()\n\n        cumhist, l, b, e = ss.cumfreq(data, num_bins)\n        cumhist *= 1.0 / len(data)\n\n        # b is bin width\n        area_above_curve = b * np.sum((1-cumhist))\n        return area_above_curve\n\n    @staticmethod\n    def run_on_single_dataframe(path_to_df_csv, label=None, output_dir=None, save=True, previous_fig_axes=None, dataframe=None):\n        """"""\n        This method is intended to be called from an ipython notebook for plotting.\n\n        Usage notes:\n        - after calling this function, you can still change many things about the plot\n        - for example you can still call plt.title(""New title"") to change the title\n        - if you\'d like to plot multiple lines on the same axes, then take the return arg of a previous call to this function, \n        - and pass it into previous_plot, i.e.:\n            fig = run_on_single_dataframe(""thing1.csv"")\n            run_on_single_dataframe(""thing2.csv"", previous_plot=fig)\n            plt.title(""both things"")\n            plt.show()\n        - if you\'d like each line to have a label in the plot, then use pass a string to label, i.e.:\n            fig = run_on_single_dataframe(""thing1.csv"", label=""thing1"")\n            run_on_single_dataframe(""thing2.csv"", label=""thing2"", previous_plot=fig)\n            plt.title(""both things"")\n            plt.show()\n\n        :param dataframe: The pandas dataframe, object\n        :type dataframe:\n        :param path_to_df_csv: full path to csv file\n        :type path_to_df_csv: string\n        :param label: name that will show up labeling this line in the legend\n        :type label: string\n        :param save: whether or not you want to save a .png\n        :type save: bool\n        :param previous_plot: a previous matplotlib figure to keep building on\n        :type previous_plot: None or matplotlib figure \n        """"""\n        DCEP = DenseCorrespondenceEvaluationPlotter\n\n        if dataframe is None:\n            path_to_csv = utils.convert_data_relative_path_to_absolute_path(path_to_df_csv,\n                assert_path_exists=True)\n            df = pd.read_csv(path_to_csv, index_col=0, parse_dates=True)\n            if output_dir is None:\n                output_dir = os.path.dirname(path_to_csv)\n        else:\n            df = dataframe\n            if save and (output_dir is None):\n                raise ValueError(""You must pass in an output directory"")\n\n\n\n\n\n        if \'is_valid_masked\' not in df:\n            use_masked_plots = False\n        else:\n            use_masked_plots = True\n\n\n        if previous_fig_axes==None:\n            N = 5\n            if use_masked_plots:\n                fig, axes = plt.subplots(nrows=N, ncols=2, figsize=(15,N*5))\n            else:\n                fig, axes = plt.subplots(N, figsize=(10,N*5))\n        else:\n            [fig, axes] = previous_fig_axes\n        \n        \n        def get_ax(axes, index):\n            if use_masked_plots:\n                return axes[index,0]\n            else:\n                return axes[index]\n\n        # pixel match error\n        ax = get_ax(axes, 0)\n        plot = DCEP.make_pixel_match_error_plot(ax, df, label=label)\n        if use_masked_plots:\n            plot = DCEP.make_pixel_match_error_plot(axes[0,1], df, label=label, masked=True)\n        ax.legend()\n       \n        # 3D match error\n        ax = get_ax(axes, 1)\n        plot = DCEP.make_descriptor_accuracy_plot(ax, df, label=label)\n        if use_masked_plots:\n            plot = DCEP.make_descriptor_accuracy_plot(axes[1,1], df, label=label, masked=True)            \n\n\n        # if save:\n        #     fig_file = os.path.join(output_dir, ""norm_diff_pred_3d.png"")\n        #     fig.savefig(fig_file)\n\n        aac = DCEP.compute_area_above_curve(df, \'norm_diff_pred_3d\')\n        d = dict()\n        d[\'norm_diff_3d_area_above_curve\'] = float(aac)\n\n        # norm difference of the ground truth match (should be 0)\n        ax = get_ax(axes,2)\n        plot = DCEP.make_norm_diff_ground_truth_plot(ax, df, label=label)\n\n        # fraction false positives\n        ax = get_ax(axes,3)\n        plot = DCEP.make_fraction_false_positives_plot(ax, df, label=label)\n        if use_masked_plots:\n            plot = DCEP.make_fraction_false_positives_plot(axes[3,1], df, label=label, masked=True)\n\n        # average l2 false positives\n        ax = get_ax(axes, 4)\n        plot = DCEP.make_average_l2_false_positives_plot(ax, df, label=label)\n        if use_masked_plots:\n            plot = DCEP.make_average_l2_false_positives_plot(axes[4,1], df, label=label, masked=True)\n\n        if save:\n            yaml_file = os.path.join(output_dir, \'stats.yaml\')\n            utils.saveToYaml(d, yaml_file)\n        return [fig, axes]\n\n    @staticmethod\n    def run_on_single_dataframe_across_objects(path_to_df_csv, label=None, output_dir=None, save=True, previous_fig_axes=None):\n        """"""\n        This method is intended to be called from an ipython notebook for plotting.\n\n        See run_on_single_dataframe() for documentation.\n\n        The only difference is that for this one, we only have across object data. \n        """"""\n        DCEP = DenseCorrespondenceEvaluationPlotter\n\n        path_to_csv = utils.convert_data_relative_path_to_absolute_path(path_to_df_csv,\n            assert_path_exists=True)\n\n        if output_dir is None:\n            output_dir = os.path.dirname(path_to_csv)\n\n        df = pd.read_csv(path_to_csv, index_col=0, parse_dates=True)\n\n        if previous_fig_axes==None:\n            N = 1\n            fig, ax = plt.subplots(N, figsize=(10,N*5))\n        else:\n            [fig, ax] = previous_fig_axes\n        \n        \n        # pixel match error\n        plot = DCEP.make_across_object_best_match_plot(ax, df, label=label)\n        ax.legend()\n       \n        if save:\n            fig_file = os.path.join(output_dir, ""across_objects.png"")\n            fig.savefig(fig_file)\n        \n        return [fig, ax]\n        \n\n\ndef run():\n    pass\n\ndef main(config):\n    eval = DenseCorrespondenceEvaluation(config)\n    dcn = eval.load_network_from_config(""10_scenes_drill"")\n    test_dataset = SpartanDataset(mode=""test"")\n\n    DenseCorrespondenceEvaluation.test(dcn, test_dataset)\n\ndef test():\n    config_filename = os.path.join(utils.getDenseCorrespondenceSourceDir(), \'config\', \'evaluation\', \'evaluation.yaml\')\n    config = utils.getDictFromYamlFilename(config_filename)\n    default_config = utils.get_defaults_config()\n    utils.set_cuda_visible_devices(default_config[\'cuda_visible_devices\'])\n\n    main(config)\n\nif __name__ == ""__main__"":\n    test()\n'"
dense_correspondence/evaluation/plotting.py,0,"b'import numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\n\ndef normalize_descriptor(res, stats=None):\n    """"""\n    Normalizes the descriptor into RGB color space\n    :param res: numpy.array [H,W,D]\n        Output of the network, per-pixel dense descriptor\n    :param stats: dict, with fields [\'min\', \'max\', \'mean\'], which are used to normalize descriptor\n    :return: numpy.array\n        normalized descriptor\n    """"""\n\n    if stats is None:\n        res_min = res.min()\n        res_max = res.max()\n    else:\n        res_min = np.array(stats[\'min\'])\n        res_max = np.array(stats[\'max\'])\n\n    normed_res = np.clip(res, res_min, res_max)\n    eps = 1e-10\n    scale = (res_max - res_min) + eps\n    normed_res = (normed_res - res_min) / scale\n    return normed_res\n\ndef normalize_descriptor_pair(res_a, res_b):\n    """"""\n    Normalizes the descriptor into RGB color space\n    :param res_a, res_b: numpy.array [H,W,D]\n        Two outputs of the network, per-pixel dense descriptor\n    :return: numpy.array, numpy.array\n        normalized descriptors\n    """"""\n\n    # needs to be per-channel\n    D = np.shape(res_a)[-1]\n    normed_res_a = np.zeros_like(res_a)\n    normed_res_b = np.zeros_like(res_b)\n\n    for d in xrange(D):\n        both_min = min(np.min(res_a[:,:,d]), np.min(res_b[:,:,d]))\n        both_max = max(np.max(res_a[:,:,d]), np.max(res_b[:,:,d]))\n        scale_factor = both_max - both_min\n        normed_res_a[:,:,d] = (res_a[:,:,d] - both_min)/scale_factor\n        normed_res_b[:,:,d] = (res_b[:,:,d] - both_min)/scale_factor\n\n        \n    return normed_res_a, normed_res_b\n\ndef normalize_masked_descriptor_pair(res_a, res_b, mask_a, mask_b):\n    # needs to be per-channel\n    D = np.shape(res_a)[-1]\n\n    mask_a= np.repeat(mask_a[:,:,np.newaxis], D, axis=2)\n    mask_b = np.repeat(mask_b[:,:,np.newaxis], D, axis=2)\n\n    res_a_temp = res_a * mask_a\n    res_b_temp = res_b * mask_b\n\n    normed_res_a = np.zeros_like(res_a)\n    normed_res_b = np.zeros_like(res_b)\n\n    for d in xrange(D):\n        res_a_d = res_a_temp[:,:,d]\n        res_b_d = res_b_temp[:,:,d]\n\n\n\n        res_a_min = np.min(res_a_d[np.nonzero(res_a_d)])\n        res_b_min = np.min(res_b_d[np.nonzero(res_b_d)])\n\n        res_a_max = np.max(res_a_d[np.nonzero(res_a_d)])\n        res_b_max = np.max(res_b_d[np.nonzero(res_b_d)])\n\n        both_min = min(res_a_min, res_b_min)\n        both_max = max(res_a_max, res_b_max)\n        scale_factor = both_max - both_min\n        normed_res_a[:,:,d] = (res_a[:,:,d] - both_min)/scale_factor\n        normed_res_b[:,:,d] = (res_b[:,:,d] - both_min)/scale_factor\n\n\n    normed_res_a = normed_res_a * mask_a\n    normed_res_b = normed_res_b * mask_b\n        \n    return normed_res_a, normed_res_b\n\n\ndef pil_image_to_cv2(pil_image):\n    """"""\n    Converts a PIL.Image to cv2 format\n    :param pil_image: RGB PIL Image\n    :type pil_image: PIL.Image\n    :return: a numpy array that cv2 likes\n    :rtype: numpy array \n    """"""\n    return np.array(pil_image)[:, :, ::-1].copy() # open and convert between BGR and RGB\n\ndef draw_correspondence_points_cv2(img, pixels):\n    """"""\n    Draws nice reticles on the img, at each of pixels\n\n    :param img: a cv2 image (really, just a numpy array, for example the return of pil_image_to_cv2)\n    :type img: numpy array\n    :param pixels: a list of dicts where each dict contains ""u"", ""v"" keys\n    :type pixels: list\n\n    :return: the img with the pixel reticles drawn on it\n    :rtype: a cv2 image\n    """"""\n    label_colors = [(255,0,0), (0,255,0), (0,0,255), (255,0,255), (0,125,125), (125,125,0), (200,255,50), (255, 125, 220), (10, 125, 255)]\n    num_colors = len(label_colors)\n    for index, pixel in enumerate(pixels):\n        color = label_colors[index % num_colors]\n        img = draw_reticle_cv2(img, int(pixel[""u""]), int(pixel[""v""]), color)\n    return img\n\ndef draw_reticle_cv2(img, u, v, label_color):\n    """"""\n    Draws a nice reticle at pixel location u, v.\n\n    The reticle is part white, and part label_color\n\n    :param img: a cv2 image (really, just a numpy array, for example the return of pil_image_to_cv2)\n    :type img: numpy array\n    :param u, v: u,v pixel position\n    :type u, v: int, int\n    :param label_color: b, g, r color\n    :type label_color: tuple of 3 ints in range 0 to 255.  for example: (255,255,255) for white\n\n    :return: the img with a new pixel reticle drawn on it\n    :rtype: a cv2 image\n    """"""\n    white = (255,255,255)\n    black = (0,0,0)\n    cv2.circle(img,(u,v),10,label_color,1)\n    cv2.circle(img,(u,v),11,white,1)\n    cv2.circle(img,(u,v),12,label_color,1)\n    cv2.line(img,(u,v+1),(u,v+3),white,1)\n    cv2.line(img,(u+1,v),(u+3,v),white,1)\n    cv2.line(img,(u,v-1),(u,v-3),white,1)\n    cv2.line(img,(u-1,v),(u-3,v),white,1)\n    return img'"
dense_correspondence/evaluation/utils.py,0,"b'# system\nimport os\nimport numpy as np\nimport pandas as pd\nimport time\nimport shutil\n\n\nimport dense_correspondence_manipulation.utils.utils as utils\nutils.add_dense_correspondence_to_python_path()\nfrom dense_correspondence.dataset.spartan_dataset_masked import SpartanDataset\n\nclass PandaDataFrameWrapper(object):\n    """"""\n    A simple wrapper for a PandaSeries that protects from read/write errors\n    """"""\n\n    def __init__(self, columns):\n        data = [np.nan] * len(columns)\n        self._columns = columns\n        self._df = pd.DataFrame(data=[data], columns=columns)\n\n    def set_value(self, key, value):\n        if key not in self._columns:\n            raise KeyError(""%s is not in the index"" %(key))\n\n        self._df[key] = value\n\n    def get_value(self, key):\n        return self._df[key]\n\n    @property\n    def dataframe(self):\n        return self._df\n\n    @dataframe.setter\n    def dataframe(self, value):\n        self._series = value\n\n\n\nclass KeypointAnnotationsPandasTemplate(PandaDataFrameWrapper):\n    columns = [\'scene_name\',\n               \'image_idx\',\n               \'keypoint_name\',\n               \'object_id\',\n               \'u\',\n               \'v\'\n               ]\n\n    def __init__(self):\n        PandaDataFrameWrapper.__init__(self, KeypointAnnotationsPandasTemplate.columns)\n\n\n    @staticmethod\n    def convert_keypoint_annotations_file_to_dataframe(keypoint_annotations):\n        """"""\n\n        :param keypoint_annotations: list of dicts of keypoint annotations in format\n        specified in README. i.e. one dict looks like\n\n        - image_idx: 3705\n              keypoints:\n                bottom_of_shoelaces:\n                  keypoint: bottom_of_shoelaces\n                  u: 396.5\n                  v: 308.0\n                heel:\n                  keypoint: heel\n                  u: 387.5\n                  v: 107.0\n                toe:\n                  keypoint: toe\n                  u: 406.0\n                  v: 373.5\n                top_of_shoelaces:\n                  keypoint: top_of_shoelaces\n                  u: 391.5\n                  v: 238.5\n              object_id: shoe_red_nike\n              scene_name: 2018-11-16-21-00-00\n\n        :type keypoint_annotations:\n        :return: pandas DataFrame\n        :rtype:\n        """"""\n\n        pd_dataframe_list = []\n\n        for d in keypoint_annotations:\n            for keypoint, keypoint_data in d[\'keypoints\'].iteritems():\n                dft = KeypointAnnotationsPandasTemplate()\n                dft.set_value(\'scene_name\', d[\'scene_name\'])\n                dft.set_value(\'image_idx\', d[\'image_idx\'])\n                dft.set_value(\'object_id\', d[\'object_id\'])\n                dft.set_value(\'keypoint_name\', keypoint_data[\'keypoint\'])\n                dft.set_value(\'u\', keypoint_data[\'u\'])\n                dft.set_value(\'v\', keypoint_data[\'v\'])\n\n                pd_dataframe_list.append(dft.dataframe)\n\n\n        df = pd.concat(pd_dataframe_list)\n\n        return df\n\n\n\ndef extract_descriptor_images_for_scene(dcn, dataset, scene_name, save_dir,\n                                        overwrite=False):\n    """"""\n    Save the descriptor images for a scene at the given directory\n    :param dcn:\n    :type dcn:\n    :param dataset:\n    :type dataset:\n    :param scene_name:\n    :type scene_name:\n    :param save_dir: Absolute path of where to save images\n    :type save_dir:\n    :return:\n    :rtype:\n    """"""\n\n    pose_data = dataset.get_pose_data(scene_name)\n    image_idxs = pose_data.keys()\n    image_idxs.sort()\n\n    num_images = len(pose_data)\n\n    logging_frequency = 50\n    start_time = time.time()\n\n    # make the mesh_descriptors dir if it doesn\'t already exist\n    if os.path.exists(save_dir):\n        if not overwrite:\n            raise ValueError(""save_dir %s already exists and overwrite is False"" %(save_dir))\n        else:\n            shutil.rmtree(save_dir)\n\n    os.makedirs(save_dir)\n\n    for counter, img_idx in enumerate(image_idxs):\n\n        if (counter % logging_frequency) == 0:\n            print ""processing image %d of %d"" % (counter, num_images)\n\n        rgb_img = dataset.get_rgb_image_from_scene_name_and_idx(scene_name, img_idx)\n\n        # note that this has already been normalized\n        rgb_img_tensor = dataset.rgb_image_to_tensor(rgb_img)\n        res = dcn.forward_single_image_tensor(rgb_img_tensor).data\n        descriptor_image_filename = utils.getPaddedString(img_idx, width=SpartanDataset.PADDED_STRING_WIDTH) + ""_descriptor.npy""\n\n        full_filepath = os.path.join(save_dir, descriptor_image_filename)\n        np.save(full_filepath, res.cpu())\n\n\n    elapsed_time = time.time() - start_time\n    print ""computing descriptor images took %d seconds"" % (elapsed_time)'"
dense_correspondence/loss_functions/__init__.py,0,b''
dense_correspondence/loss_functions/loss_composer.py,2,"b'from dense_correspondence.dataset.spartan_dataset_masked import SpartanDataset, SpartanDatasetDataType\nfrom dense_correspondence.loss_functions.pixelwise_contrastive_loss import PixelwiseContrastiveLoss\n\nimport torch\nfrom torch.autograd import Variable\n\ndef get_loss(pixelwise_contrastive_loss, match_type, \n              image_a_pred, image_b_pred,\n              matches_a,     matches_b,\n              masked_non_matches_a, masked_non_matches_b,\n              background_non_matches_a, background_non_matches_b,\n              blind_non_matches_a, blind_non_matches_b):\n    """"""\n    This function serves the purpose of:\n    - parsing the different types of SpartanDatasetDataType...\n    - parsing different types of matches / non matches..\n    - into different pixelwise contrastive loss functions\n\n    :return args: loss, match_loss, masked_non_match_loss, \\\n                background_non_match_loss, blind_non_match_loss\n    :rtypes: each pytorch Variables\n\n    """"""\n    verbose = False\n\n    if (match_type == SpartanDatasetDataType.SINGLE_OBJECT_WITHIN_SCENE).all():\n        if verbose:\n            print ""applying SINGLE_OBJECT_WITHIN_SCENE loss""\n        return get_within_scene_loss(pixelwise_contrastive_loss, image_a_pred, image_b_pred,\n                                            matches_a,    matches_b,\n                                            masked_non_matches_a, masked_non_matches_b,\n                                            background_non_matches_a, background_non_matches_b,\n                                            blind_non_matches_a, blind_non_matches_b)\n\n    if (match_type == SpartanDatasetDataType.SINGLE_OBJECT_ACROSS_SCENE).all():\n        if verbose:\n            print ""applying SINGLE_OBJECT_ACROSS_SCENE loss""\n        return get_same_object_across_scene_loss(pixelwise_contrastive_loss, image_a_pred, image_b_pred,\n                                            blind_non_matches_a, blind_non_matches_b)\n\n    if (match_type == SpartanDatasetDataType.DIFFERENT_OBJECT).all():\n        if verbose:\n            print ""applying DIFFERENT_OBJECT loss""\n        return get_different_object_loss(pixelwise_contrastive_loss, image_a_pred, image_b_pred,\n                                            blind_non_matches_a, blind_non_matches_b)\n\n\n    if (match_type == SpartanDatasetDataType.MULTI_OBJECT).all():\n        if verbose:\n            print ""applying MULTI_OBJECT loss""\n        return get_within_scene_loss(pixelwise_contrastive_loss, image_a_pred, image_b_pred,\n                                            matches_a,    matches_b,\n                                            masked_non_matches_a, masked_non_matches_b,\n                                            background_non_matches_a, background_non_matches_b,\n                                            blind_non_matches_a, blind_non_matches_b)\n\n    if (match_type == SpartanDatasetDataType.SYNTHETIC_MULTI_OBJECT).all():\n        if verbose:\n            print ""applying SYNTHETIC_MULTI_OBJECT loss""\n        return get_within_scene_loss(pixelwise_contrastive_loss, image_a_pred, image_b_pred,\n                                            matches_a,    matches_b,\n                                            masked_non_matches_a, masked_non_matches_b,\n                                            background_non_matches_a, background_non_matches_b,\n                                            blind_non_matches_a, blind_non_matches_b)\n\n    else:\n        raise ValueError(""Should only have above scenes?"")\n\n\ndef get_within_scene_loss(pixelwise_contrastive_loss, image_a_pred, image_b_pred,\n                                        matches_a,    matches_b,\n                                        masked_non_matches_a, masked_non_matches_b,\n                                        background_non_matches_a, background_non_matches_b,\n                                        blind_non_matches_a, blind_non_matches_b):\n    """"""\n    Simple wrapper for pixelwise_contrastive_loss functions.  Args and return args documented above in get_loss()\n    """"""\n    pcl = pixelwise_contrastive_loss\n\n    match_loss, masked_non_match_loss, num_masked_hard_negatives =\\\n        pixelwise_contrastive_loss.get_loss_matched_and_non_matched_with_l2(image_a_pred,         image_b_pred,\n                                                                          matches_a,            matches_b,\n                                                                          masked_non_matches_a, masked_non_matches_b,\n                                                                          M_descriptor=pcl._config[""M_masked""])\n\n    if pcl._config[""use_l2_pixel_loss_on_background_non_matches""]:\n        background_non_match_loss, num_background_hard_negatives =\\\n            pixelwise_contrastive_loss.non_match_loss_with_l2_pixel_norm(image_a_pred, image_b_pred, matches_b, \n                background_non_matches_a, background_non_matches_b, M_descriptor=pcl._config[""M_background""])    \n        \n    else:\n        background_non_match_loss, num_background_hard_negatives =\\\n            pixelwise_contrastive_loss.non_match_loss_descriptor_only(image_a_pred, image_b_pred,\n                                                                    background_non_matches_a, background_non_matches_b,\n                                                                    M_descriptor=pcl._config[""M_background""])\n        \n        \n\n    blind_non_match_loss = zero_loss()\n    num_blind_hard_negatives = 1\n    if not (SpartanDataset.is_empty(blind_non_matches_a.data)):\n        blind_non_match_loss, num_blind_hard_negatives =\\\n            pixelwise_contrastive_loss.non_match_loss_descriptor_only(image_a_pred, image_b_pred,\n                                                                    blind_non_matches_a, blind_non_matches_b,\n                                                                    M_descriptor=pcl._config[""M_masked""])\n        \n\n\n    total_num_hard_negatives = num_masked_hard_negatives + num_background_hard_negatives\n    total_num_hard_negatives = max(total_num_hard_negatives, 1)\n\n    if pcl._config[""scale_by_hard_negatives""]:\n        scale_factor = total_num_hard_negatives\n\n        masked_non_match_loss_scaled = masked_non_match_loss*1.0/max(num_masked_hard_negatives, 1)\n\n        background_non_match_loss_scaled = background_non_match_loss*1.0/max(num_background_hard_negatives, 1)\n\n        blind_non_match_loss_scaled = blind_non_match_loss*1.0/max(num_blind_hard_negatives, 1)\n    else:\n        # we are not currently using blind non-matches\n        num_masked_non_matches = max(len(masked_non_matches_a),1)\n        num_background_non_matches = max(len(background_non_matches_a),1)\n        num_blind_non_matches = max(len(blind_non_matches_a),1)\n        scale_factor = num_masked_non_matches + num_background_non_matches\n\n\n        masked_non_match_loss_scaled = masked_non_match_loss*1.0/num_masked_non_matches\n\n        background_non_match_loss_scaled = background_non_match_loss*1.0/num_background_non_matches\n\n        blind_non_match_loss_scaled = blind_non_match_loss*1.0/num_blind_non_matches\n\n\n\n    non_match_loss = 1.0/scale_factor * (masked_non_match_loss + background_non_match_loss)\n\n    loss = pcl._config[""match_loss_weight""] * match_loss + \\\n    pcl._config[""non_match_loss_weight""] * non_match_loss\n\n    \n\n    return loss, match_loss, masked_non_match_loss_scaled, background_non_match_loss_scaled, blind_non_match_loss_scaled\n\ndef get_within_scene_loss_triplet(pixelwise_contrastive_loss, image_a_pred, image_b_pred,\n                                        matches_a,    matches_b,\n                                        masked_non_matches_a, masked_non_matches_b,\n                                        background_non_matches_a, background_non_matches_b,\n                                        blind_non_matches_a, blind_non_matches_b):\n    """"""\n    Simple wrapper for pixelwise_contrastive_loss functions.  Args and return args documented above in get_loss()\n    """"""\n    \n    pcl = pixelwise_contrastive_loss\n\n    masked_triplet_loss =\\\n        pixelwise_contrastive_loss.get_triplet_loss(image_a_pred, image_b_pred, matches_a, \n            matches_b, masked_non_matches_a, masked_non_matches_b, pcl._config[""alpha_triplet""])\n        \n    background_triplet_loss =\\\n        pixelwise_contrastive_loss.get_triplet_loss(image_a_pred, image_b_pred, matches_a, \n            matches_b, background_non_matches_a, background_non_matches_b, pcl._config[""alpha_triplet""])\n\n    total_loss = masked_triplet_loss + background_triplet_loss\n\n    return total_loss, zero_loss(), zero_loss(), zero_loss(), zero_loss()\n\ndef get_different_object_loss(pixelwise_contrastive_loss, image_a_pred, image_b_pred,\n                              blind_non_matches_a, blind_non_matches_b):\n    """"""\n    Simple wrapper for pixelwise_contrastive_loss functions.  Args and return args documented above in get_loss()\n    """"""\n\n    scale_by_hard_negatives = pixelwise_contrastive_loss.config[""scale_by_hard_negatives_DIFFERENT_OBJECT""]\n    blind_non_match_loss = zero_loss()\n    if not (SpartanDataset.is_empty(blind_non_matches_a.data)):\n        M_descriptor = pixelwise_contrastive_loss.config[""M_background""]\n\n        blind_non_match_loss, num_hard_negatives =\\\n            pixelwise_contrastive_loss.non_match_loss_descriptor_only(image_a_pred, image_b_pred,\n                                                                    blind_non_matches_a, blind_non_matches_b,\n                                                                    M_descriptor=M_descriptor)\n        \n        if scale_by_hard_negatives:\n            scale_factor = max(num_hard_negatives, 1)\n        else:\n            scale_factor = max(len(blind_non_matches_a), 1)\n\n        blind_non_match_loss = 1.0/scale_factor * blind_non_match_loss\n    loss = blind_non_match_loss\n    return loss, zero_loss(), zero_loss(), zero_loss(), blind_non_match_loss\n\ndef get_same_object_across_scene_loss(pixelwise_contrastive_loss, image_a_pred, image_b_pred,\n                              blind_non_matches_a, blind_non_matches_b):\n    """"""\n    Simple wrapper for pixelwise_contrastive_loss functions.  Args and return args documented above in get_loss()\n    """"""\n    blind_non_match_loss = zero_loss()\n    if not (SpartanDataset.is_empty(blind_non_matches_a.data)):\n        blind_non_match_loss, num_hard_negatives =\\\n            pixelwise_contrastive_loss.non_match_loss_descriptor_only(image_a_pred, image_b_pred,\n                                                                    blind_non_matches_a, blind_non_matches_b,\n                                                                    M_descriptor=pcl._config[""M_masked""], invert=True)\n\n    if pixelwise_contrastive_loss._config[""scale_by_hard_negatives""]:\n        scale_factor = max(num_hard_negatives, 1)\n    else:\n        scale_factor = max(len(blind_non_matches_a), 1)\n\n    loss = 1.0/scale_factor * blind_non_match_loss\n    blind_non_match_loss_scaled = 1.0/scale_factor * blind_non_match_loss\n    return loss, zero_loss(), zero_loss(), zero_loss(), blind_non_match_loss\n\ndef zero_loss():\n    return Variable(torch.FloatTensor([0]).cuda())\n\ndef is_zero_loss(loss):\n    return loss.item() < 1e-20\n\n\n'"
dense_correspondence/loss_functions/pixelwise_contrastive_loss.py,56,"b'import torch\nfrom torch.autograd import Variable\n\n\nclass PixelwiseContrastiveLoss(object):\n\n    def __init__(self, image_shape, config=None):\n    \tself.type = ""pixelwise_contrastive""\n        self.image_width  = image_shape[1]\n        self.image_height = image_shape[0]\n\n        assert config is not None\n        self._config = config\n\n        self._debug_data = dict()\n\n        self._debug = False\n\n    @property\n    def debug(self):\n        return self._debug\n\n    @property\n    def config(self):\n        return self._config\n\n    @debug.setter\n    def debug(self, value):\n        self._debug = value\n\n    @property\n    def debug_data(self):\n        return self._debug_data\n\n    def get_loss_matched_and_non_matched_with_l2(self, image_a_pred, image_b_pred, matches_a, matches_b, non_matches_a, non_matches_b,\n                 M_descriptor=None, M_pixel=None, non_match_loss_weight=1.0, use_l2_pixel_loss=None):\n        """"""\n        Computes the loss function\n\n        DCN = Dense Correspondence Network\n        num_images = number of images in this batch\n        num_matches = number of matches\n        num_non_matches = number of non-matches\n        W = image width\n        H = image height\n        D = descriptor dimension\n\n\n        match_loss = 1/num_matches \\sum_{num_matches} ||descriptor_a - descriptor_b||_2^2\n        non_match_loss = 1/num_non_matches \\sum_{num_non_matches} max(0, M_margin - ||descriptor_a - descriptor_b||_2)^2\n\n        loss = match_loss + non_match_loss\n\n        :param image_a_pred: Output of DCN network on image A.\n        :type image_a_pred: torch.Variable(torch.FloatTensor) shape [1, W * H, D]\n        :param image_b_pred: same as image_a_pred\n        :type image_b_pred:\n        :param matches_a: torch.Variable(torch.LongTensor) has shape [num_matches,],  a (u,v) pair is mapped\n        to (u,v) ---> image_width * v + u, this matches the shape of one dimension of image_a_pred\n        :type matches_a: torch.Variable(torch.FloatTensor)\n        :param matches_b: same as matches_a\n        :type matches_b:\n        :param non_matches_a: torch.Variable(torch.FloatTensor) has shape [num_non_matches,],  a (u,v) pair is mapped\n        to (u,v) ---> image_width * v + u, this matches the shape of image_a_pred\n        :type non_matches_a: torch.Variable(torch.FloatTensor)\n        :param non_matches_b: same as non_matches_a\n        :type non_matches_b:\n        :return: loss, match_loss, non_match_loss\n        :rtype: torch.Variable(torch.FloatTensor) each of shape torch.Size([1])\n        """"""\n\n        PCL = PixelwiseContrastiveLoss\n\n        if M_descriptor is None:\n            M_descriptor = self._config[""M_descriptor""]\n\n        if M_pixel is None:\n            M_pixel = self._config[""M_pixel""]\n\n\n        if use_l2_pixel_loss is None:\n            use_l2_pixel_loss = self._config[\'use_l2_pixel_loss_on_masked_non_matches\']\n\n\n        match_loss, _, _ = PCL.match_loss(image_a_pred, image_b_pred, matches_a, matches_b)\n\n\n\n        if use_l2_pixel_loss:\n            non_match_loss, num_hard_negatives =\\\n                self.non_match_loss_with_l2_pixel_norm(image_a_pred, image_b_pred, matches_b,\n                                                       non_matches_a, non_matches_b,\n                                                       M_descriptor=M_descriptor,\n                                                       M_pixel=M_pixel)\n        else:\n            # version with no l2 pixel term\n            non_match_loss, num_hard_negatives = self.non_match_loss_descriptor_only(image_a_pred, image_b_pred, non_matches_a, non_matches_b, M_descriptor=M_descriptor)\n\n\n\n        return match_loss, non_match_loss, num_hard_negatives\n\n    @staticmethod\n    def get_triplet_loss(image_a_pred, image_b_pred, matches_a, matches_b, non_matches_a, non_matches_b, alpha):\n        """"""\n        Computes the loss function\n\n        \\sum_{triplets} ||D(I_a, u_a, I_b, u_{b,match})||_2^2 - ||D(I_a, u_a, I_b, u_{b,non-match)||_2^2 + alpha \n\n        """"""\n        num_matches = matches_a.size()[0]\n        num_non_matches = non_matches_a.size()[0]\n        multiplier = num_non_matches / num_matches\n\n        ## non_matches_a is already replicated up to be the right size\n        ## non_matches_b is also that side\n        ## matches_a is just a smaller version of non_matches_a\n        ## matches_b is the only thing that needs to be replicated up in size\n\n        matches_b_long =  torch.t(matches_b.repeat(multiplier, 1)).contiguous().view(-1)\n                         \n        matches_a_descriptors = torch.index_select(image_a_pred, 1, non_matches_a)\n        matches_b_descriptors      = torch.index_select(image_b_pred, 1, matches_b_long)\n        non_matches_b_descriptors  = torch.index_select(image_b_pred, 1, non_matches_b)\n\n        triplet_losses = (matches_a_descriptors - matches_b_descriptors).pow(2) - (matches_a_descriptors - non_matches_b_descriptors).pow(2) + alpha\n        triplet_loss = 1.0 / num_non_matches * torch.clamp(triplet_losses, min=0).sum()\n\n        return triplet_loss\n\n    @staticmethod\n    def match_loss(image_a_pred, image_b_pred, matches_a, matches_b):\n        """"""\n        Computes the match loss given by\n\n        1/num_matches * \\sum_{matches} ||D(I_a, u_a, I_b, u_b)||_2^2\n\n        :param image_a_pred: Output of DCN network on image A.\n        :type image_a_pred: torch.Variable(torch.FloatTensor) shape [1, W * H, D]\n        :param image_b_pred: same as image_a_pred\n        :type image_b_pred:\n        :param matches_a: torch.Variable(torch.LongTensor) has shape [num_matches,],  a (u,v) pair is mapped\n        to (u,v) ---> image_width * v + u, this matches the shape of one dimension of image_a_pred\n        :type matches_a: torch.Variable(torch.FloatTensor)\n        :param matches_b: same as matches_b\n\n        :return: match_loss, matches_a_descriptors, matches_b_descriptors\n        :rtype: torch.Variable(),\n\n        matches_a_descriptors is torch.FloatTensor with shape torch.Shape([num_matches, descriptor_dimension])\n        """"""\n\n        num_matches = matches_a.size()[0]\n        matches_a_descriptors = torch.index_select(image_a_pred, 1, matches_a)\n        matches_b_descriptors = torch.index_select(image_b_pred, 1, matches_b)\n\n        # crazily enough, if there is only one element to index_select into\n        # above, then the first dimension is collapsed down, and we end up \n        # with shape [D,], where we want [1,D]\n        # this unsqueeze fixes that case\n        if len(matches_a) == 1:\n            matches_a_descriptors = matches_a_descriptors.unsqueeze(0)\n            matches_b_descriptors = matches_b_descriptors.unsqueeze(0)\n\n        match_loss = 1.0 / num_matches * (matches_a_descriptors - matches_b_descriptors).pow(2).sum()\n\n        return match_loss, matches_a_descriptors, matches_b_descriptors\n\n\n    @staticmethod\n    def non_match_descriptor_loss(image_a_pred, image_b_pred, non_matches_a, non_matches_b, M=0.5, invert=False):\n        """"""\n        Computes the max(0, M - D(I_a,I_b,u_a,u_b))^2 term\n\n        This is effectively:       ""a and b should be AT LEAST M away from each other""\n        With invert=True, this is: ""a and b should be AT MOST  M away from each other"" \n\n         :param image_a_pred: Output of DCN network on image A.\n        :type image_a_pred: torch.Variable(torch.FloatTensor) shape [1, W * H, D]\n        :param image_b_pred: same as image_a_pred\n        :type image_b_pred:\n        :param non_matches_a: torch.Variable(torch.FloatTensor) has shape [num_non_matches,],  a (u,v) pair is mapped\n        to (u,v) ---> image_width * v + u, this matches the shape of image_a_pred\n        :type non_matches_a: torch.Variable(torch.FloatTensor)\n        :param non_matches_b: same as non_matches_a\n        :param M: the margin\n        :type M: float\n        :return: torch.FloatTensor with shape torch.Shape([num_non_matches])\n        :rtype:\n        """"""\n\n        non_matches_a_descriptors = torch.index_select(image_a_pred, 1, non_matches_a).squeeze()\n        non_matches_b_descriptors = torch.index_select(image_b_pred, 1, non_matches_b).squeeze()\n\n        # crazily enough, if there is only one element to index_select into\n        # above, then the first dimension is collapsed down, and we end up \n        # with shape [D,], where we want [1,D]\n        # this unsqueeze fixes that case\n        if len(non_matches_a) == 1:\n            non_matches_a_descriptors = non_matches_a_descriptors.unsqueeze(0)\n            non_matches_b_descriptors = non_matches_b_descriptors.unsqueeze(0)\n\n        norm_degree = 2\n        non_match_loss = (non_matches_a_descriptors - non_matches_b_descriptors).norm(norm_degree, 1)\n        if not invert:\n            non_match_loss = torch.clamp(M - non_match_loss, min=0).pow(2)\n        else:\n            non_match_loss = torch.clamp(non_match_loss - M, min=0).pow(2)\n\n        hard_negative_idxs = torch.nonzero(non_match_loss)\n        num_hard_negatives = len(hard_negative_idxs)\n\n        return non_match_loss, num_hard_negatives, non_matches_a_descriptors, non_matches_b_descriptors\n\n    def non_match_loss_with_l2_pixel_norm(self, image_a_pred, image_b_pred, matches_b,\n                                          non_matches_a, non_matches_b, M_descriptor=0.5,\n                                          M_pixel=None):\n\n        """"""\n\n        Computes the total non_match_loss with an l2_pixel norm term\n\n        :param image_a_pred: Output of DCN network on image A.\n        :type image_a_pred: torch.Variable(torch.FloatTensor) shape [1, W * H, D]\n        :param image_b_pred: same as image_a_pred\n        :type image_b_pred:\n        :param matches_a: torch.Variable(torch.LongTensor) has shape [num_matches,],  a (u,v) pair is mapped\n        to (u,v) ---> image_width * v + u, this matches the shape of one dimension of image_a_pred\n        :type matches_a: torch.Variable(torch.FloatTensor)\n        :param matches_b: same as matches_a\n        :type matches_b:\n        :param non_matches_a: torch.Variable(torch.FloatTensor) has shape [num_non_matches,],  a (u,v) pair is mapped\n        to (u,v) ---> image_width * v + u, this matches the shape of image_a_pred\n        :type non_matches_a: torch.Variable(torch.FloatTensor)\n        :param non_matches_b: same as non_matches_a\n\n        :param M_descriptor: margin for descriptor loss term\n        :type M_descriptor: float\n        :param M_pixel: margin for pixel loss term\n        :type M_pixel: float\n        :return: non_match_loss, num_hard_negatives\n        :rtype: torch.Variable, int\n        """"""\n\n        if M_descriptor is None:\n            M_descriptor = self._config[""M_descriptor""]\n\n        if M_pixel is None:\n            M_pixel = self._config[""M_pixel""]\n\n        PCL = PixelwiseContrastiveLoss\n\n        num_non_matches = non_matches_a.size()[0]\n\n        non_match_descriptor_loss, num_hard_negatives, _, _ = PCL.non_match_descriptor_loss(image_a_pred, image_b_pred, non_matches_a, non_matches_b, M=M_descriptor)\n\n        non_match_pixel_l2_loss, _, _ = self.l2_pixel_loss(matches_b, non_matches_b, M_pixel=M_pixel)\n\n\n\n\n        non_match_loss = (non_match_descriptor_loss * non_match_pixel_l2_loss).sum()\n\n        if self.debug:\n            self._debug_data[\'num_hard_negatives\'] = num_hard_negatives\n            self._debug_data[\'fraction_hard_negatives\'] = num_hard_negatives * 1.0/num_non_matches\n\n\n        return non_match_loss, num_hard_negatives\n\n    def non_match_loss_descriptor_only(self, image_a_pred, image_b_pred, non_matches_a, non_matches_b, M_descriptor=0.5, invert=False):\n        """"""\n        Computes the non-match loss, only using the desciptor norm\n        :param image_a_pred:\n        :type image_a_pred:\n        :param image_b_pred:\n        :type image_b_pred:\n        :param non_matches_a:\n        :type non_matches_a:\n        :param non_matches_b:\n        :type non_matches_b:\n        :param M:\n        :type M:\n        :return: non_match_loss, num_hard_negatives\n        :rtype: torch.Variable, int\n        """"""\n        PCL = PixelwiseContrastiveLoss\n\n        if M_descriptor is None:\n            M_descriptor = self._config[""M_descriptor""]\n\n        non_match_loss_vec, num_hard_negatives, _, _ = PCL.non_match_descriptor_loss(image_a_pred, image_b_pred, non_matches_a,\n                                                                 non_matches_b, M=M_descriptor, invert=invert)\n\n        num_non_matches = long(non_match_loss_vec.size()[0])\n\n\n        non_match_loss = non_match_loss_vec.sum()\n\n        if self._debug:\n            self._debug_data[\'num_hard_negatives\'] = num_hard_negatives\n            self._debug_data[\'fraction_hard_negatives\'] = num_hard_negatives * 1.0/num_non_matches\n\n        return non_match_loss, num_hard_negatives\n\n\n    def l2_pixel_loss(self, matches_b, non_matches_b, M_pixel=None):\n        """"""\n        Apply l2 loss in pixel space.\n\n        This weights non-matches more if they are ""far away"" in pixel space.\n\n        :param matches_b: A torch.LongTensor with shape torch.Shape([num_matches])\n        :param non_matches_b: A torch.LongTensor with shape torch.Shape([num_non_matches])\n        :return l2 loss per sample: A torch.FloatTensorof with shape torch.Shape([num_matches])\n        """"""\n\n        if M_pixel is None:\n            M_pixel = self._config[\'M_pixel\']\n\n        num_non_matches_per_match = len(non_matches_b)/len(matches_b)\n\n        ground_truth_pixels_for_non_matches_b = torch.t(matches_b.repeat(num_non_matches_per_match,1)).contiguous().view(-1,1)\n\n        ground_truth_u_v_b = self.flattened_pixel_locations_to_u_v(ground_truth_pixels_for_non_matches_b)\n        sampled_u_v_b      = self.flattened_pixel_locations_to_u_v(non_matches_b.unsqueeze(1))\n\n        # each element is always within [0,1], you have 1 if you are at least M_pixel away in\n        # L2 norm in pixel space\n        norm_degree = 2\n        squared_l2_pixel_loss = 1.0/M_pixel * torch.clamp((ground_truth_u_v_b - sampled_u_v_b).float().norm(norm_degree,1), max=M_pixel)\n\n\n        return squared_l2_pixel_loss, ground_truth_u_v_b, sampled_u_v_b\n        \n\n    \n    def flattened_pixel_locations_to_u_v(self, flat_pixel_locations):\n        """"""\n        :param flat_pixel_locations: A torch.LongTensor of shape torch.Shape([n,1]) where each element\n         is a flattened pixel index, i.e. some integer between 0 and 307,200 for a 640x480 image\n\n        :type flat_pixel_locations: torch.LongTensor\n\n        :return A torch.LongTensor of shape (n,2) where the first column is the u coordinates of\n        the pixel and the second column is the v coordinate\n\n        """"""\n        u_v_pixel_locations = flat_pixel_locations.repeat(1,2)\n        u_v_pixel_locations[:,0] = u_v_pixel_locations[:,0]%self.image_width \n        u_v_pixel_locations[:,1] = u_v_pixel_locations[:,1]/self.image_width\n        return u_v_pixel_locations\n\n    def get_l2_pixel_loss_original(self):\n        pass\n\n    def get_loss_original(self, image_a_pred, image_b_pred, matches_a,\n                          matches_b, non_matches_a, non_matches_b,\n                          M_margin=0.5, non_match_loss_weight=1.0):\n\n        # this is pegged to it\'s implemenation at sha 87abdb63bb5b99d9632f5c4360b5f6f1cf54245f\n        """"""\n        Computes the loss function\n        DCN = Dense Correspondence Network\n        num_images = number of images in this batch\n        num_matches = number of matches\n        num_non_matches = number of non-matches\n        W = image width\n        H = image height\n        D = descriptor dimension\n        match_loss = 1/num_matches \\sum_{num_matches} ||descriptor_a - descriptor_b||_2^2\n        non_match_loss = 1/num_non_matches \\sum_{num_non_matches} max(0, M_margin - ||descriptor_a - descriptor_b||_2^2 )\n        loss = match_loss + non_match_loss\n        :param image_a_pred: Output of DCN network on image A.\n        :type image_a_pred: torch.Variable(torch.FloatTensor) shape [1, W * H, D]\n        :param image_b_pred: same as image_a_pred\n        :type image_b_pred:\n        :param matches_a: torch.Variable(torch.LongTensor) has shape [num_matches,],  a (u,v) pair is mapped\n        to (u,v) ---> image_width * v + u, this matches the shape of one dimension of image_a_pred\n        :type matches_a: torch.Variable(torch.FloatTensor)\n        :param matches_b: same as matches_b\n        :type matches_b:\n        :param non_matches_a: torch.Variable(torch.FloatTensor) has shape [num_non_matches,],  a (u,v) pair is mapped\n        to (u,v) ---> image_width * v + u, this matches the shape of image_a_pred\n        :type non_matches_a: torch.Variable(torch.FloatTensor)\n        :param non_matches_b: same as non_matches_a\n        :type non_matches_b:\n        :return: loss, match_loss, non_match_loss\n        :rtype: torch.Variable(torch.FloatTensor) each of shape torch.Size([1])\n        """"""\n\n        num_matches = matches_a.size()[0]\n        num_non_matches = non_matches_a.size()[0]\n\n\n        matches_a_descriptors = torch.index_select(image_a_pred, 1, matches_a)\n        matches_b_descriptors = torch.index_select(image_b_pred, 1, matches_b)\n\n        match_loss = 1.0/num_matches * (matches_a_descriptors - matches_b_descriptors).pow(2).sum()\n\n        # add loss via non_matches\n        non_matches_a_descriptors = torch.index_select(image_a_pred, 1, non_matches_a)\n        non_matches_b_descriptors = torch.index_select(image_b_pred, 1, non_matches_b)\n        pixel_wise_loss = (non_matches_a_descriptors - non_matches_b_descriptors).pow(2).sum(dim=2)\n        pixel_wise_loss = torch.add(torch.neg(pixel_wise_loss), M_margin)\n        zeros_vec = torch.zeros_like(pixel_wise_loss)\n        non_match_loss = non_match_loss_weight * 1.0/num_non_matches * torch.max(zeros_vec, pixel_wise_loss).sum()\n\n        loss = match_loss + non_match_loss\n\n        return loss, match_loss, non_match_loss\n'"
dense_correspondence/network/__init__.py,0,b''
dense_correspondence/network/dense_correspondence_network.py,12,"b'#!/usr/bin/python\n\nimport sys, os\nimport numpy as np\nimport warnings\nimport logging\nimport dense_correspondence_manipulation.utils.utils as utils\nutils.add_dense_correspondence_to_python_path()\n\n\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms\nimport pytorch_segmentation_detection.models.resnet_dilated as resnet_dilated\nfrom dense_correspondence.dataset.spartan_dataset_masked import SpartanDataset\n\n\n\nclass DenseCorrespondenceNetwork(nn.Module):\n\n    IMAGE_TO_TENSOR = valid_transform = transforms.Compose([transforms.ToTensor(), ])\n\n    def __init__(self, fcn, descriptor_dimension, image_width=640,\n                 image_height=480, normalize=False):\n        """"""\n\n        :param fcn:\n        :type fcn:\n        :param descriptor_dimension:\n        :type descriptor_dimension:\n        :param image_width:\n        :type image_width:\n        :param image_height:\n        :type image_height:\n        :param normalize: If True normalizes the feature vectors to lie on unit ball\n        :type normalize:\n        """"""\n\n        super(DenseCorrespondenceNetwork, self).__init__()\n\n        self._fcn = fcn\n        self._descriptor_dimension = descriptor_dimension\n        self._image_width = image_width\n        self._image_height = image_height\n\n        # this defaults to the identity transform\n        self._image_mean = np.zeros(3)\n        self._image_std_dev = np.ones(3)\n\n        # defaults to no image normalization, assume it is done by dataset loader instead\n\n        self.config = dict()\n\n        self._descriptor_image_stats = None\n        self._normalize = normalize\n        self._constructed_from_model_folder = False\n\n\n    @property\n    def fcn(self):\n        return self._fcn\n\n    @property\n    def config(self):\n        return self._config\n\n    @config.setter\n    def config(self, value):\n        self._config = value\n\n    @property\n    def descriptor_dimension(self):\n        return self._descriptor_dimension\n\n    @property\n    def image_shape(self):\n        return [self._image_height, self._image_width]\n\n    @property\n    def image_mean(self):\n        return self._image_mean\n\n    @image_mean.setter\n    def image_mean(self, value):\n        """"""\n        Sets the image mean used in normalizing the images before\n        being passed through the network\n        :param value: list of floats\n        :type value:\n        :return:\n        :rtype:\n        """"""\n        self._image_mean = value\n        self.config[\'image_mean\'] = value\n        self._update_normalize_tensor_transform()\n\n    @property\n    def image_std_dev(self):\n        return self._image_std_dev\n\n    @image_std_dev.setter\n    def image_std_dev(self, value):\n        """"""\n        Sets the image std dev used in normalizing the images before\n        being passed through the network\n        :param value: list of floats\n        :type value:\n        :return:\n        :rtype:\n        """"""\n        self._image_std_dev = value\n        self.config[\'image_std_dev\'] = value\n        self._update_normalize_tensor_transform()\n\n    @property\n    def image_to_tensor(self):\n        return self._image_to_tensor\n\n    @image_to_tensor.setter\n    def image_to_tensor(self, value):\n        self._image_to_tensor = value\n\n    @property\n    def normalize_tensor_transform(self):\n        return self._normalize_tensor_transform\n\n    @property\n    def path_to_network_params_folder(self):\n        if not \'path_to_network_params_folder\' in self.config:\n            raise ValueError(""DenseCorrespondenceNetwork: Config doesn\'t have a `path_to_network_params_folder`""\n                             ""entry"")\n\n        return self.config[\'path_to_network_params_folder\']\n\n\n    @property\n    def descriptor_image_stats(self):\n        """"""\n        Returns the descriptor normalization parameters, if possible.\n        If they have not yet been loaded then it loads them\n        :return:\n        :rtype:\n        """"""\n\n        # if it isn\'t already set, then attempt to load it\n        if self._descriptor_image_stats is None:\n            path_to_params = utils.convert_to_absolute_path(self.path_to_network_params_folder)\n            descriptor_stats_file = os.path.join(path_to_params, ""descriptor_statistics.yaml"")\n            self._descriptor_image_stats = utils.getDictFromYamlFilename(descriptor_stats_file)\n\n\n        return self._descriptor_image_stats\n\n    @property\n    def constructed_from_model_folder(self):\n        """"""\n        Returns True if this model was constructed from\n        :return:\n        :rtype:\n        """"""\n        return self._constructed_from_model_folder\n\n    @constructed_from_model_folder.setter\n    def constructed_from_model_folder(self, value):\n        self._constructed_from_model_folder = value\n\n    @property\n    def unique_identifier(self):\n        """"""\n        Return the unique identifier for this network, if it has one.\n        If no identifier.yaml found (or we don\'t even have a model params folder)\n        then return None\n        :return:\n        :rtype:\n        """"""\n\n        try:\n            path_to_network_params_folder = self.path_to_network_params_folder\n        except ValueError:\n            return None\n\n        identifier_file = os.path.join(path_to_network_params_folder, \'identifier.yaml\')\n        if not os.path.exists(identifier_file):\n            return None\n\n        if not self.constructed_from_model_folder:\n            return None\n\n\n\n        d = utils.getDictFromYamlFilename(identifier_file)\n        unique_identifier = d[\'id\'] + ""+"" + self.config[\'model_param_filename_tail\']\n        return unique_identifier\n\n    def _update_normalize_tensor_transform(self):\n        """"""\n        Updates the image to tensor transform using the current image mean and\n        std dev\n        :return: None\n        :rtype:\n        """"""\n        self._normalize_tensor_transform = transforms.Normalize(self.image_mean, self.image_std_dev)\n\n\n    def forward_on_img(self, img, cuda=True):\n        """"""\n        Runs the network forward on an image\n        :param img: img is an image as a numpy array in opencv format [0,255]\n        :return:\n        """"""\n        img_tensor = DenseCorrespondenceNetwork.IMAGE_TO_TENSOR(img)\n\n        if cuda:\n            img_tensor.cuda()\n\n        return self.forward(img_tensor)\n\n\n    def forward_on_img_tensor(self, img):\n        """"""\n        Deprecated, use `forward` instead\n        Runs the network forward on an img_tensor\n        :param img: (C x H X W) in range [0.0, 1.0]\n        :return:\n        """"""\n        warnings.warn(""use forward method instead"", DeprecationWarning)\n\n        img = img.unsqueeze(0)\n        img = torch.tensor(img, device=torch.device(""cuda""))\n        res = self.fcn(img)\n        res = res.squeeze(0)\n        res = res.permute(1, 2, 0)\n        res = res.data.cpu().numpy().squeeze()\n\n        return res\n\n    def forward(self, img_tensor):\n        """"""\n        Simple forward pass on the network.\n\n        Does NOT normalize the image\n\n        D = descriptor dimension\n        N = batch size\n\n        :param img_tensor: input tensor img.shape = [N, D, H , W] where\n                    N is the batch size\n        :type img_tensor: torch.Variable or torch.Tensor\n        :return: torch.Variable with shape [N, D, H, W],\n        :rtype:\n        """"""\n\n        res = self.fcn(img_tensor)\n        if self._normalize:\n            #print ""normalizing descriptor norm""\n            norm = torch.norm(res, 2, 1) # [N,1,H,W]\n            res = res/norm\n\n\n\n        return res\n\n    def forward_single_image_tensor(self, img_tensor):\n        """"""\n        Simple forward pass on the network.\n\n        Assumes the image has already been normalized (i.e. subtract mean, divide by std dev)\n\n        Color channel should be RGB\n\n        :param img_tensor: torch.FloatTensor with shape [3,H,W]\n        :type img_tensor:\n        :return: torch.FloatTensor with shape  [H, W, D]\n        :rtype:\n        """"""\n\n        assert len(img_tensor.shape) == 3\n\n\n        # transform to shape [1,3,H,W]\n        img_tensor = img_tensor.unsqueeze(0)\n\n        # make sure it\'s on the GPU\n        img_tensor = torch.tensor(img_tensor, device=torch.device(""cuda""))\n\n\n        res = self.forward(img_tensor) # shape [1,D,H,W]\n        # print ""res.shape 1"", res.shape\n\n\n        res = res.squeeze(0) # shape [D,H,W]\n        # print ""res.shape 2"", res.shape\n\n        res = res.permute(1,2,0) # shape [H,W,D]\n        # print ""res.shape 3"", res.shape\n\n        return res\n\n\n\n    def process_network_output(self, image_pred, N):\n        """"""\n        Processes the network output into a new shape\n\n        :param image_pred: output of the network img.shape = [N,descriptor_dim, H , W]\n        :type image_pred: torch.Tensor\n        :param N: batch size\n        :type N: int\n        :return: same as input, new shape is [N, W*H, descriptor_dim]\n        :rtype:\n        """"""\n\n        W = self._image_width\n        H = self._image_height\n        image_pred = image_pred.view(N, self.descriptor_dimension, W * H)\n        image_pred = image_pred.permute(0, 2, 1)\n        return image_pred\n\n    def clip_pixel_to_image_size_and_round(self, uv):\n        """"""\n        Clips pixel to image coordinates and converts to int\n        :param uv:\n        :type uv:\n        :return:\n        :rtype:\n        """"""\n        u = min(int(round(uv[0])), self._image_width - 1)\n        v = min(int(round(uv[1])), self._image_height - 1)\n        return [u, v]\n\n    def load_training_dataset(self):\n        """"""\n        Loads the dataset that this was trained on\n        :return: a dataset object, loaded with the config as set in the dataset.yaml\n        :rtype: SpartanDataset\n        """"""\n\n        network_params_folder = self.path_to_network_params_folder\n        network_params_folder = utils.convert_to_absolute_path(network_params_folder)\n        dataset_config_file = os.path.join(network_params_folder, \'dataset.yaml\')\n        config = utils.getDictFromYamlFilename(dataset_config_file)\n        return SpartanDataset(config_expanded=config)\n\n\n    @staticmethod\n    def get_unet(config):\n        """"""\n        Returns a Unet nn.module that satisifies the fcn properties stated in get_fcn() docstring\n        """"""\n        dc_source_dir = utils.getDenseCorrespondenceSourceDir()\n        sys.path.append(os.path.join(dc_source_dir, \'external/unet-pytorch\'))\n        from unet_model import UNet\n        model = UNet(num_classes=config[""descriptor_dimension""]).cuda()\n        return model\n\n\n    @staticmethod\n    def get_fcn(config):\n        """"""\n        Returns a pytorch nn.module that satisfies these properties:\n\n        1. autodiffs\n        2. has forward() overloaded\n        3. can accept a ~Nx3xHxW (should double check)\n        4. outputs    a ~NxDxHxW (should double check)\n\n        :param config: Dict with dcn configuration parameters\n\n        """"""\n        \n        if config[""backbone""][""model_class""] == ""Resnet"":\n            resnet_model = config[""backbone""][""resnet_name""]\n            fcn = getattr(resnet_dilated, resnet_model)(num_classes=config[\'descriptor_dimension\'])\n        \n        elif config[""backbone""][""model_class""] == ""Unet"":\n            fcn = DenseCorrespondenceNetwork.get_unet(config)\n\n        else:\n            raise ValueError(""Can\'t build backbone network.  I don\'t know this backbone model class!"")\n        \n        return fcn\n\n    @staticmethod\n    def from_config(config, load_stored_params=True, model_param_file=None):\n        """"""\n        Load a network from a configuration\n\n\n        :param config: Dict specifying details of the network architecture\n\n        :param load_stored_params: whether or not to load stored params, if so there should be\n            a ""path_to_network"" entry in the config\n        :type load_stored_params: bool\n\n        e.g.\n            path_to_network: /home/manuelli/code/dense_correspondence/recipes/trained_models/10_drill_long_3d\n            parameter_file: dense_resnet_34_8s_03505.pth\n            descriptor_dimensionality: 3\n            image_width: 640\n            image_height: 480\n\n        :return: DenseCorrespondenceNetwork\n        :rtype:\n        """"""\n\n        if ""backbone"" not in config:  \n            # default to CoRL 2018 backbone!\n            config[""backbone""] = dict()\n            config[""backbone""][""model_class""] = ""Resnet""\n            config[""backbone""][""resnet_name""] = ""Resnet34_8s""\n\n        fcn = DenseCorrespondenceNetwork.get_fcn(config)\n\n        if \'normalize\' in config:\n            normalize = config[\'normalize\']\n        else:\n            normalize = False\n\n        dcn = DenseCorrespondenceNetwork(fcn, config[\'descriptor_dimension\'],\n                                          image_width=config[\'image_width\'],\n                                          image_height=config[\'image_height\'],\n                                         normalize=normalize)\n\n        if load_stored_params:\n            assert model_param_file is not None\n            config[\'model_param_file\'] = model_param_file # should be an absolute path\n            try:\n                dcn.load_state_dict(torch.load(model_param_file))\n            except:\n                logging.info(""loading params with the new style failed, falling back to dcn.fcn.load_state_dict"")\n                dcn.fcn.load_state_dict(torch.load(model_param_file))\n\n        dcn.cuda()\n        dcn.train()\n        dcn.config = config\n        return dcn\n\n    @staticmethod\n    def from_model_folder(model_folder, load_stored_params=True, model_param_file=None,\n        iteration=None):\n        """"""\n        Loads a DenseCorrespondenceNetwork from a model folder\n        :param model_folder: the path to the folder where the model is stored. This direction contains\n        files like\n\n            - 003500.pth\n            - training.yaml\n\n        :type model_folder:\n        :return: a DenseCorrespondenceNetwork objecc t\n        :rtype:\n        """"""\n\n        from_model_folder = False\n        model_folder = utils.convert_to_absolute_path(model_folder)\n\n        if model_param_file is None:\n            model_param_file, _, _ = utils.get_model_param_file_from_directory(model_folder, iteration=iteration)\n            from_model_folder = True\n\n        model_param_file = utils.convert_to_absolute_path(model_param_file)\n\n        training_config_filename = os.path.join(model_folder, ""training.yaml"")\n        training_config = utils.getDictFromYamlFilename(training_config_filename)\n        config = training_config[""dense_correspondence_network""]\n        config[""path_to_network_params_folder""] = model_folder\n        config[""model_param_filename_tail""] = os.path.split(model_param_file)[1]\n\n\n\n\n        dcn = DenseCorrespondenceNetwork.from_config(config,\n                                                     load_stored_params=load_stored_params,\n                                                     model_param_file=model_param_file)\n\n\n        # whether or not network was constructed from model folder\n        dcn.constructed_from_model_folder = from_model_folder\n\n\n\n        dcn.model_folder = model_folder\n        return dcn\n\n    @staticmethod\n    def find_best_match(pixel_a, res_a, res_b, debug=False):\n        """"""\n        Compute the correspondences between the pixel_a location in image_a\n        and image_b\n\n        :param pixel_a: vector of (u,v) pixel coordinates\n        :param res_a: array of dense descriptors res_a.shape = [H,W,D]\n        :param res_b: array of dense descriptors\n        :param pixel_b: Ground truth . . .\n        :return: (best_match_uv, best_match_diff, norm_diffs)\n        best_match_idx is again in (u,v) = (right, down) coordinates\n\n        """"""\n\n        descriptor_at_pixel = res_a[pixel_a[1], pixel_a[0]]\n        height, width, _ = res_a.shape\n\n        if debug:\n            print ""height: "", height\n            print ""width: "", width\n            print ""res_b.shape: "", res_b.shape\n\n\n        # non-vectorized version\n        # norm_diffs = np.zeros([height, width])\n        # for i in xrange(0, height):\n        #     for j in xrange(0, width):\n        #         norm_diffs[i,j] = np.linalg.norm(res_b[i,j] - descriptor_at_pixel)**2\n\n        norm_diffs = np.sqrt(np.sum(np.square(res_b - descriptor_at_pixel), axis=2))\n\n        best_match_flattened_idx = np.argmin(norm_diffs)\n        best_match_xy = np.unravel_index(best_match_flattened_idx, norm_diffs.shape)\n        best_match_diff = norm_diffs[best_match_xy]\n\n        best_match_uv = (best_match_xy[1], best_match_xy[0])\n\n        return best_match_uv, best_match_diff, norm_diffs\n\n    @staticmethod\n    def find_best_match_for_descriptor(descriptor, res):\n        """"""\n        Compute the correspondences between the given descriptor and the descriptor image\n        res\n        :param descriptor:\n        :type descriptor:\n        :param res: array of dense descriptors res = [H,W,D]\n        :type res: numpy array with shape [H,W,D]\n        :return: (best_match_uv, best_match_diff, norm_diffs)\n        best_match_idx is again in (u,v) = (right, down) coordinates\n        :rtype:\n        """"""\n        height, width, _ = res.shape\n\n        norm_diffs = np.sqrt(np.sum(np.square(res - descriptor), axis=2))\n\n        best_match_flattened_idx = np.argmin(norm_diffs)\n        best_match_xy = np.unravel_index(best_match_flattened_idx, norm_diffs.shape)\n        best_match_diff = norm_diffs[best_match_xy]\n\n        best_match_uv = (best_match_xy[1], best_match_xy[0])\n\n        return best_match_uv, best_match_diff, norm_diffs\n\n\n    def evaluate_descriptor_at_keypoints(self, res, keypoint_list):\n        """"""\n\n        :param res: result of evaluating the network\n        :type res: torch.FloatTensor [D,W,H]\n        :param img:\n        :type img: img_tensor\n        :param kp: list of cv2.KeyPoint\n        :type kp:\n        :return: numpy.ndarray (N,D) N = num keypoints, D = descriptor dimension\n        This is the same format as sift.compute from OpenCV\n        :rtype:\n        """"""\n\n        raise NotImplementedError(""This function is currently broken"")\n\n        N = len(keypoint_list)\n        D = self.descriptor_dimension\n        des = np.zeros([N,D])\n\n        for idx, kp in enumerate(keypoint_list):\n            uv = self.clip_pixel_to_image_size_and_round([kp.pt[0], kp.pt[1]])\n            des[idx,:] = res[uv[1], uv[0], :]\n\n        # cast to float32, need this in order to use cv2.BFMatcher() with bf.knnMatch\n        des = np.array(des, dtype=np.float32)\n        return des\n\n'"
dense_correspondence/test/__init__.py,0,b''
dense_correspondence/test/numpy_correspondence_finder.py,0,"b'# io\nimport imageio\n\n# math\nimport numpy as numpy\nimport math\nimport random\nfrom numpy.linalg import inv\n\ndef rand_select_pixel(width,height):\n    x = random.randint(0,width)\n    y = random.randint(0,height)\n    return (x,y)\n\ndef find_pixel_correspondence(log_dir, img_a, img_b, uv_a=None):\n    img1_filename = log_dir+""/images/""+img_a+""_rgb.png""\n    img2_filename = log_dir+""/images/""+img_b+""_rgb.png""\n    img1_depth_filename = log_dir+""/images/""+img_a+""_depth.png""\n    img2_depth_filename = log_dir+""/images/""+img_b+""_depth.png""\n    img1_time_filename = log_dir+""/images/""+img_a+""_utime.txt""\n    img2_time_filename = log_dir+""/images/""+img_b+""_utime.txt""\n\n    def get_time(time_filename):\n        with open (time_filename) as f:\n            content = f.readlines()\n        return int(content[0])/1e6\n\n    img1_time = get_time(img1_time_filename)\n    img2_time = get_time(img2_time_filename)\n\n    posegraph_filename = log_dir+""/posegraph.posegraph""\n    with open(posegraph_filename) as f:\n        content = f.readlines()\n    pose_list = [x.strip().split() for x in content] \n\n    def get_pose(time, pose_list):\n        if (time <= float(pose_list[0][0])):\n            pose = pose_list[0]\n            pose = [float(x) for x in pose[1:]]\n            return pose\n        for pose in pose_list:\n            if (time <= float(pose[0])):\n                pose = [float(x) for x in pose[1:]]\n                return pose\n        print ""did not find matching pose""\n\n    img1_pose = get_pose(img1_time, pose_list)\n    img2_pose = get_pose(img2_time, pose_list)\n\n    _EPS = numpy.finfo(float).eps * 4.0\n    def quaternion_matrix(quaternion):\n        q = numpy.array(quaternion, dtype=numpy.float64, copy=True)\n        n = numpy.dot(q, q)\n        if n < _EPS:\n            return numpy.identity(4)\n        q *= math.sqrt(2.0 / n)\n        q = numpy.outer(q, q)\n        return numpy.array([\n            [1.0-q[2, 2]-q[3, 3],     q[1, 2]-q[3, 0],     q[1, 3]+q[2, 0], 0.0],\n            [    q[1, 2]+q[3, 0], 1.0-q[1, 1]-q[3, 3],     q[2, 3]-q[1, 0], 0.0],\n            [    q[1, 3]-q[2, 0],     q[2, 3]+q[1, 0], 1.0-q[1, 1]-q[2, 2], 0.0],\n            [                0.0,                 0.0,                 0.0, 1.0]])\n\n    def labelfusion_pose_to_homogeneous_transform(lf_pose):\n        homogeneous_transform = quaternion_matrix([lf_pose[6], lf_pose[3], lf_pose[4], lf_pose[5]])\n        homogeneous_transform[0,3] = lf_pose[0]\n        homogeneous_transform[1,3] = lf_pose[1]\n        homogeneous_transform[2,3] = lf_pose[2]\n        return homogeneous_transform\n\n    img1_pose_4 = labelfusion_pose_to_homogeneous_transform(img1_pose)\n    img2_pose_4 = labelfusion_pose_to_homogeneous_transform(img2_pose)\n\n    img1_depth = imageio.imread(img1_depth_filename)\n\n    if uv_a is None:\n        uv_a = rand_select_pixel(width=640,height=480)\n    print uv_a\n\n    body_to_rdf = numpy.zeros((3,3))\n    body_to_rdf[0,1] = -1.0\n    body_to_rdf[1,2] = -1.0\n    body_to_rdf[2,0] = 1.0\n    rdf_to_body = inv(body_to_rdf)\n\n    K = numpy.zeros((3,3))\n    K[0,0] = 528.0 # focal x\n    K[1,1] = 528.0 # focal y\n    K[0,2] = 320.0 # principal point x\n    K[1,2] = 240.0 # principal point y\n    K[2,2] = 1.0\n    K_inv = inv(K)\n\n    depth = img1_depth[uv_a[::-1]]*1.0/1000\n    print ""depth, "", depth\n    u = uv_a[0]\n    v = uv_a[1]\n\n    x = u*depth\n    y = v*depth\n    z = depth\n    vec = numpy.array([x,y,z])\n\n    point_camera_frame_rdf = K_inv.dot(vec)\n\n    def invert_transform(transform4):\n        transform4_copy = numpy.copy(transform4)\n        R = transform4_copy[0:3,0:3]\n        R = numpy.transpose(R)\n        transform4_copy[0:3,0:3] = R\n        t = transform4_copy[0:3,3]\n        inv_t = -1.0 * numpy.transpose(R).dot(t)\n        transform4_copy[0:3,3] = inv_t\n        return transform4_copy\n\n    def apply_transform(vec3, transform4):\n        vec4 = numpy.array([vec3[0], vec3[1], vec3[2], 1.0])\n        vec4 = transform4.dot(vec4)\n        return numpy.array([vec4[0], vec4[1], vec4[2]])\n\n    point_world_frame_rdf = apply_transform(point_camera_frame_rdf, img1_pose_4)\n    point_camera_2_frame_rdf = apply_transform(point_world_frame_rdf, invert_transform(img2_pose_4))\n\n    vec2 = K.dot(point_camera_2_frame_rdf)\n    u2 = vec2[0]/vec2[2]\n    v2 = vec2[1]/vec2[2]\n    uv_b = (u2, v2)\n    print uv_b\n    return (uv_a, uv_b)'"
dense_correspondence/training/__init__.py,0,b''
dense_correspondence/training/training.py,10,"b'# system\nimport numpy as np\nimport os\nimport fnmatch\nimport gc\nimport logging\nimport time\nimport shutil\nimport subprocess\nimport copy\n\n# torch\nimport torch\nfrom torchvision import transforms\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.optim as optim\n\n\nimport tensorboard_logger\n\n\n\n\n# dense correspondence\nimport dense_correspondence_manipulation.utils.utils as utils\nutils.add_dense_correspondence_to_python_path()\nimport pytorch_segmentation_detection.models.fcn as fcns\nimport pytorch_segmentation_detection.models.resnet_dilated as resnet_dilated\nfrom pytorch_segmentation_detection.transforms import (ComposeJoint,\n                                                       RandomHorizontalFlipJoint,\n                                                       RandomScaleJoint,\n                                                       CropOrPad,\n                                                       ResizeAspectRatioPreserve,\n                                                       RandomCropJoint,\n                                                       Split2D)\n\nfrom dense_correspondence.dataset.spartan_dataset_masked import SpartanDataset, SpartanDatasetDataType\nfrom dense_correspondence.network.dense_correspondence_network import DenseCorrespondenceNetwork\n\nfrom dense_correspondence.loss_functions.pixelwise_contrastive_loss import PixelwiseContrastiveLoss\nimport dense_correspondence.loss_functions.loss_composer as loss_composer\nfrom dense_correspondence.evaluation.evaluation import DenseCorrespondenceEvaluation\n\n\nclass DenseCorrespondenceTraining(object):\n\n    def __init__(self, config=None, dataset=None, dataset_test=None):\n        if config is None:\n            config = DenseCorrespondenceTraining.load_default_config()\n\n        self._config = config\n        self._dataset = dataset\n        self._dataset_test = dataset_test\n\n        self._dcn = None\n        self._optimizer = None\n\n    def setup(self):\n        """"""\n        Initializes the object\n        :return:\n        :rtype:\n        """"""\n        self.load_dataset()\n        self.setup_logging_dir()\n        self.setup_tensorboard()\n\n\n    @property\n    def dataset(self):\n        return self._dataset\n\n    @dataset.setter\n    def dataset(self, value):\n        self._dataset = value\n\n    def load_dataset(self):\n        """"""\n        Loads a dataset, construct a trainloader.\n        Additionally creates a dataset and DataLoader for the test data\n        :return:\n        :rtype:\n        """"""\n\n        batch_size = self._config[\'training\'][\'batch_size\']\n        num_workers = self._config[\'training\'][\'num_workers\']\n\n        if self._dataset is None:\n            self._dataset = SpartanDataset.make_default_10_scenes_drill()\n\n        \n        self._dataset.load_all_pose_data()\n        self._dataset.set_parameters_from_training_config(self._config)\n\n        self._data_loader = torch.utils.data.DataLoader(self._dataset, batch_size=batch_size,\n                                          shuffle=True, num_workers=num_workers, drop_last=True)\n\n        # create a test dataset\n        if self._config[""training""][""compute_test_loss""]:\n            if self._dataset_test is None:\n                self._dataset_test = SpartanDataset(mode=""test"", config=self._dataset.config)\n\n            \n            self._dataset_test.load_all_pose_data()\n            self._dataset_test.set_parameters_from_training_config(self._config)\n\n            self._data_loader_test = torch.utils.data.DataLoader(self._dataset_test, batch_size=batch_size,\n                                          shuffle=True, num_workers=2, drop_last=True)\n\n    def load_dataset_from_config(self, config):\n        """"""\n        Loads train and test datasets from the given config\n        :param config: Dict gotten from a YAML file\n        :type config:\n        :return: None\n        :rtype:\n        """"""\n        self._dataset = SpartanDataset(mode=""train"", config=config)\n        self._dataset_test = SpartanDataset(mode=""test"", config=config)\n        self.load_dataset()\n\n    def build_network(self):\n        """"""\n        Builds the DenseCorrespondenceNetwork\n        :return:\n        :rtype: DenseCorrespondenceNetwork\n        """"""\n\n        return DenseCorrespondenceNetwork.from_config(self._config[\'dense_correspondence_network\'],\n                                                      load_stored_params=False)\n\n    def _construct_optimizer(self, parameters):\n        """"""\n        Constructs the optimizer\n        :param parameters: Parameters to adjust in the optimizer\n        :type parameters:\n        :return: Adam Optimizer with params from the config\n        :rtype: torch.optim\n        """"""\n\n        learning_rate = float(self._config[\'training\'][\'learning_rate\'])\n        weight_decay = float(self._config[\'training\'][\'weight_decay\'])\n        optimizer = optim.Adam(parameters, lr=learning_rate, weight_decay=weight_decay)\n        return optimizer\n\n    def _get_current_loss(self, logging_dict):\n        """"""\n        Gets the current loss for both test and train\n        :return:\n        :rtype: dict\n        """"""\n        d = dict()\n        d[\'train\'] = dict()\n        d[\'test\'] = dict()\n\n        for key, val in d.iteritems():\n            for field in logging_dict[key].keys():\n                vec = logging_dict[key][field]\n\n                if len(vec) > 0:\n                    val[field] = vec[-1]\n                else:\n                    val[field] = -1 # placeholder\n\n\n        return d\n\n    def load_pretrained(self, model_folder, iteration=None):\n        """"""\n        Loads network and optimizer parameters from a previous training run.\n\n        Note: It is up to the user to ensure that the model parameters match.\n        e.g. width, height, descriptor dimension etc.\n\n        :param model_folder: location of the folder containing the param files 001000.pth. Can be absolute or relative path. If relative then it is relative to pdc/trained_models/\n        :type model_folder:\n        :param iteration: which index to use, e.g. 3500, if None it loads the latest one\n        :type iteration:\n        :return: iteration\n        :rtype:\n        """"""\n\n        if not os.path.isdir(model_folder):\n            pdc_path = utils.getPdcPath()\n            model_folder = os.path.join(pdc_path, ""trained_models"", model_folder)\n\n        # find idx.pth and idx.pth.opt files\n        if iteration is None:\n            files = os.listdir(model_folder)\n            model_param_file = sorted(fnmatch.filter(files, \'*.pth\'))[-1]\n            iteration = int(model_param_file.split(""."")[0])\n            optim_param_file = sorted(fnmatch.filter(files, \'*.pth.opt\'))[-1]\n        else:\n            prefix = utils.getPaddedString(iteration, width=6)\n            model_param_file = prefix + "".pth""\n            optim_param_file = prefix + "".pth.opt""\n\n        print ""model_param_file"", model_param_file\n        model_param_file = os.path.join(model_folder, model_param_file)\n        optim_param_file = os.path.join(model_folder, optim_param_file)\n\n\n        self._dcn = self.build_network()\n        self._dcn.load_state_dict(torch.load(model_param_file))\n        self._dcn.cuda()\n        self._dcn.train()\n\n        self._optimizer = self._construct_optimizer(self._dcn.parameters())\n        self._optimizer.load_state_dict(torch.load(optim_param_file))\n\n        return iteration\n\n    def run_from_pretrained(self, model_folder, iteration=None, learning_rate=None):\n        """"""\n        Wrapper for load_pretrained(), then run()\n        """"""\n        iteration = self.load_pretrained(model_folder, iteration)\n        if iteration is None:\n            iteration = 0\n\n        if learning_rate is not None:\n            self._config[""training""][""learning_rate_starting_from_pretrained""] = learning_rate\n            self.set_learning_rate(self._optimizer, learning_rate)\n\n        self.run(loss_current_iteration=iteration, use_pretrained=True)\n\n    def run(self, loss_current_iteration=0, use_pretrained=False):\n        """"""\n        Runs the training\n        :return:\n        :rtype:\n        """"""\n\n        start_iteration = copy.copy(loss_current_iteration)\n\n        DCE = DenseCorrespondenceEvaluation\n\n        self.setup()\n        self.save_configs()\n\n        if not use_pretrained:\n            # create new network and optimizer\n            self._dcn = self.build_network()\n            self._optimizer = self._construct_optimizer(self._dcn.parameters())\n        else:\n            logging.info(""using pretrained model"")\n            if (self._dcn is None):\n                raise ValueError(""you must set self._dcn if use_pretrained=True"")\n            if (self._optimizer is None):\n                raise ValueError(""you must set self._optimizer if use_pretrained=True"")\n\n        # make sure network is using cuda and is in train mode\n        dcn = self._dcn\n        dcn.cuda()\n        dcn.train()\n\n        optimizer = self._optimizer\n        batch_size = self._data_loader.batch_size\n\n        pixelwise_contrastive_loss = PixelwiseContrastiveLoss(image_shape=dcn.image_shape, config=self._config[\'loss_function\'])\n        pixelwise_contrastive_loss.debug = True\n\n        loss = match_loss = non_match_loss = 0\n\n        max_num_iterations = self._config[\'training\'][\'num_iterations\'] + start_iteration\n        logging_rate = self._config[\'training\'][\'logging_rate\']\n        save_rate = self._config[\'training\'][\'save_rate\']\n        compute_test_loss_rate = self._config[\'training\'][\'compute_test_loss_rate\']\n\n        # logging\n        self._logging_dict = dict()\n        self._logging_dict[\'train\'] = {""iteration"": [], ""loss"": [], ""match_loss"": [],\n                                           ""masked_non_match_loss"": [], \n                                           ""background_non_match_loss"": [],\n                                           ""blind_non_match_loss"": [],\n                                           ""learning_rate"": [],\n                                           ""different_object_non_match_loss"": []}\n\n        self._logging_dict[\'test\'] = {""iteration"": [], ""loss"": [], ""match_loss"": [],\n                                           ""non_match_loss"": []}\n\n        # save network before starting\n        if not use_pretrained:\n            self.save_network(dcn, optimizer, 0)\n\n        # from training_progress_visualizer import TrainingProgressVisualizer\n        # TPV = TrainingProgressVisualizer()\n\n        for epoch in range(50):  # loop over the dataset multiple times\n\n            for i, data in enumerate(self._data_loader, 0):\n                loss_current_iteration += 1\n                start_iter = time.time()\n\n                match_type, \\\n                img_a, img_b, \\\n                matches_a, matches_b, \\\n                masked_non_matches_a, masked_non_matches_b, \\\n                background_non_matches_a, background_non_matches_b, \\\n                blind_non_matches_a, blind_non_matches_b, \\\n                metadata = data\n\n                if (match_type == -1).all():\n                    print ""\\n empty data, continuing \\n""\n                    continue\n\n\n                data_type = metadata[""type""][0]\n                \n                img_a = Variable(img_a.cuda(), requires_grad=False)\n                img_b = Variable(img_b.cuda(), requires_grad=False)\n\n                matches_a = Variable(matches_a.cuda().squeeze(0), requires_grad=False)\n                matches_b = Variable(matches_b.cuda().squeeze(0), requires_grad=False)\n                masked_non_matches_a = Variable(masked_non_matches_a.cuda().squeeze(0), requires_grad=False)\n                masked_non_matches_b = Variable(masked_non_matches_b.cuda().squeeze(0), requires_grad=False)\n\n                background_non_matches_a = Variable(background_non_matches_a.cuda().squeeze(0), requires_grad=False)\n                background_non_matches_b = Variable(background_non_matches_b.cuda().squeeze(0), requires_grad=False)\n\n                blind_non_matches_a = Variable(blind_non_matches_a.cuda().squeeze(0), requires_grad=False)\n                blind_non_matches_b = Variable(blind_non_matches_b.cuda().squeeze(0), requires_grad=False)\n\n                optimizer.zero_grad()\n                self.adjust_learning_rate(optimizer, loss_current_iteration)\n\n                # run both images through the network\n                image_a_pred = dcn.forward(img_a)\n                image_a_pred = dcn.process_network_output(image_a_pred, batch_size)\n\n                image_b_pred = dcn.forward(img_b)\n                image_b_pred = dcn.process_network_output(image_b_pred, batch_size)\n\n                # get loss\n                loss, match_loss, masked_non_match_loss, \\\n                background_non_match_loss, blind_non_match_loss = loss_composer.get_loss(pixelwise_contrastive_loss, match_type,\n                                                                                image_a_pred, image_b_pred,\n                                                                                matches_a,     matches_b,\n                                                                                masked_non_matches_a, masked_non_matches_b,\n                                                                                background_non_matches_a, background_non_matches_b,\n                                                                                blind_non_matches_a, blind_non_matches_b)\n                \n\n                loss.backward()\n                optimizer.step()\n\n                #if i % 10 == 0:\n                # TPV.update(self._dataset, dcn, loss_current_iteration, now_training_object_id=metadata[""object_id""])\n\n                elapsed = time.time() - start_iter\n\n                def update_plots(loss, match_loss, masked_non_match_loss, background_non_match_loss, blind_non_match_loss):\n                    """"""\n                    Updates the tensorboard plots with current loss function information\n                    :return:\n                    :rtype:\n                    """"""\n\n\n\n                    learning_rate = DenseCorrespondenceTraining.get_learning_rate(optimizer)\n                    self._logging_dict[\'train\'][\'learning_rate\'].append(learning_rate)\n                    self._tensorboard_logger.log_value(""learning rate"", learning_rate, loss_current_iteration)\n\n\n                    # Don\'t update any plots if the entry corresponding to that term\n                    # is a zero loss\n                    if not loss_composer.is_zero_loss(match_loss):\n                        self._logging_dict[\'train\'][\'match_loss\'].append(match_loss.item())\n                        self._tensorboard_logger.log_value(""train match loss"", match_loss.item(), loss_current_iteration)\n\n                    if not loss_composer.is_zero_loss(masked_non_match_loss):\n                        self._logging_dict[\'train\'][\'masked_non_match_loss\'].append(masked_non_match_loss.item())\n\n                        self._tensorboard_logger.log_value(""train masked non match loss"", masked_non_match_loss.item(), loss_current_iteration)\n\n                    if not loss_composer.is_zero_loss(background_non_match_loss):\n                        self._logging_dict[\'train\'][\'background_non_match_loss\'].append(background_non_match_loss.item())\n                        self._tensorboard_logger.log_value(""train background non match loss"", background_non_match_loss.item(), loss_current_iteration)\n\n                    if not loss_composer.is_zero_loss(blind_non_match_loss):\n\n                        if data_type == SpartanDatasetDataType.SINGLE_OBJECT_WITHIN_SCENE:\n                            self._tensorboard_logger.log_value(""train blind SINGLE_OBJECT_WITHIN_SCENE"", blind_non_match_loss.item(), loss_current_iteration)\n\n                        if data_type == SpartanDatasetDataType.DIFFERENT_OBJECT:\n                            self._tensorboard_logger.log_value(""train blind DIFFERENT_OBJECT"", blind_non_match_loss.item(), loss_current_iteration)\n\n\n                    # loss is never zero\n                    if data_type == SpartanDatasetDataType.SINGLE_OBJECT_WITHIN_SCENE:\n                        self._tensorboard_logger.log_value(""train loss SINGLE_OBJECT_WITHIN_SCENE"", loss.item(), loss_current_iteration)\n\n                    elif data_type == SpartanDatasetDataType.DIFFERENT_OBJECT:\n                        self._tensorboard_logger.log_value(""train loss DIFFERENT_OBJECT"", loss.item(), loss_current_iteration)\n\n                    elif data_type == SpartanDatasetDataType.SINGLE_OBJECT_ACROSS_SCENE:\n                        self._tensorboard_logger.log_value(""train loss SINGLE_OBJECT_ACROSS_SCENE"", loss.item(), loss_current_iteration)\n\n                    elif data_type == SpartanDatasetDataType.MULTI_OBJECT:\n                        self._tensorboard_logger.log_value(""train loss MULTI_OBJECT"", loss.item(), loss_current_iteration)\n                    \n                    elif data_type == SpartanDatasetDataType.SYNTHETIC_MULTI_OBJECT:\n                        self._tensorboard_logger.log_value(""train loss SYNTHETIC_MULTI_OBJECT"", loss.item(), loss_current_iteration)\n                    else:\n                        raise ValueError(""unknown data type"")\n\n\n                    if data_type == SpartanDatasetDataType.DIFFERENT_OBJECT:\n                        self._tensorboard_logger.log_value(""train different object"", loss.item(), loss_current_iteration)\n\n                update_plots(loss, match_loss, masked_non_match_loss, background_non_match_loss, blind_non_match_loss)\n\n                if loss_current_iteration % save_rate == 0:\n                    self.save_network(dcn, optimizer, loss_current_iteration, logging_dict=self._logging_dict)\n\n                if loss_current_iteration % logging_rate == 0:\n                    logging.info(""Training on iteration %d of %d"" %(loss_current_iteration, max_num_iterations))\n\n                    logging.info(""single iteration took %.3f seconds"" %(elapsed))\n\n                    percent_complete = loss_current_iteration * 100.0/(max_num_iterations - start_iteration)\n                    logging.info(""Training is %d percent complete\\n"" %(percent_complete))\n\n\n                # don\'t compute the test loss on the first few times through the loop\n                if self._config[""training""][""compute_test_loss""] and (loss_current_iteration % compute_test_loss_rate == 0) and loss_current_iteration > 5:\n                    logging.info(""Computing test loss"")\n\n                    # delete the loss, match_loss, non_match_loss variables so that\n                    # pytorch can use that GPU memory\n                    del loss, match_loss, masked_non_match_loss, background_non_match_loss, blind_non_match_loss\n                    gc.collect()\n\n                    dcn.eval()\n                    test_loss, test_match_loss, test_non_match_loss = DCE.compute_loss_on_dataset(dcn,\n                                                                                                  self._data_loader_test, self._config[\'loss_function\'], num_iterations=self._config[\'training\'][\'test_loss_num_iterations\'])\n\n                    # delete these variables so we can free GPU memory\n                    del test_loss, test_match_loss, test_non_match_loss\n\n                    # make sure to set the network back to train mode\n                    dcn.train()\n\n                if loss_current_iteration % self._config[\'training\'][\'garbage_collect_rate\'] == 0:\n                    logging.debug(""running garbage collection"")\n                    gc_start = time.time()\n                    gc.collect()\n                    gc_elapsed = time.time() - gc_start\n                    logging.debug(""garbage collection took %.2d seconds"" %(gc_elapsed))\n\n                if loss_current_iteration > max_num_iterations:\n                    logging.info(""Finished testing after %d iterations"" % (max_num_iterations))\n                    self.save_network(dcn, optimizer, loss_current_iteration, logging_dict=self._logging_dict)\n                    return\n\n\n    def setup_logging_dir(self):\n        """"""\n        Sets up the directory where logs will be stored and config\n        files written\n        :return: full path of logging dir\n        :rtype: str\n        """"""\n\n        if \'logging_dir_name\' in self._config[\'training\']:\n            dir_name = self._config[\'training\'][\'logging_dir_name\']\n        else:\n            dir_name = utils.get_current_time_unique_name() +""_"" + str(self._config[\'dense_correspondence_network\'][\'descriptor_dimension\']) + ""d""\n\n        self._logging_dir_name = dir_name\n\n        self._logging_dir = os.path.join(utils.convert_data_relative_path_to_absolute_path(self._config[\'training\'][\'logging_dir\']), dir_name)\n\n        print ""logging_dir:"", self._logging_dir\n\n        if os.path.isdir(self._logging_dir):\n            shutil.rmtree(self._logging_dir)\n\n        if not os.path.isdir(self._logging_dir):\n            os.makedirs(self._logging_dir)\n\n        # make the tensorboard log directory\n        self._tensorboard_log_dir = os.path.join(self._logging_dir, ""tensorboard"")\n        if not os.path.isdir(self._tensorboard_log_dir):\n            os.makedirs(self._tensorboard_log_dir)\n\n        return self._logging_dir\n\n    @property\n    def logging_dir(self):\n        """"""\n        Sets up the directory where logs will be stored and config\n        files written\n        :return: full path of logging dir\n        :rtype: str\n        """"""\n        return self._logging_dir\n\n    def save_network(self, dcn, optimizer, iteration, logging_dict=None):\n        """"""\n        Saves network parameters to logging directory\n        :return:\n        :rtype: None\n        """"""\n\n        network_param_file = os.path.join(self._logging_dir, utils.getPaddedString(iteration, width=6) + "".pth"")\n        optimizer_param_file = network_param_file + "".opt""\n        torch.save(dcn.state_dict(), network_param_file)\n        torch.save(optimizer.state_dict(), optimizer_param_file)\n\n        # also save loss history stuff\n        if logging_dict is not None:\n            log_history_file = os.path.join(self._logging_dir, utils.getPaddedString(iteration, width=6) + ""_log_history.yaml"")\n            utils.saveToYaml(logging_dict, log_history_file)\n\n            current_loss_file = os.path.join(self._logging_dir, \'loss.yaml\')\n            current_loss_data = self._get_current_loss(logging_dict)\n\n            utils.saveToYaml(current_loss_data, current_loss_file)\n\n\n\n    def save_configs(self):\n        """"""\n        Saves config files to the logging directory\n        :return:\n        :rtype: None\n        """"""\n        training_params_file = os.path.join(self._logging_dir, \'training.yaml\')\n        utils.saveToYaml(self._config, training_params_file)\n\n        dataset_params_file = os.path.join(self._logging_dir, \'dataset.yaml\')\n        utils.saveToYaml(self._dataset.config, dataset_params_file)\n\n        # make unique identifier\n        identifier_file = os.path.join(self._logging_dir, \'identifier.yaml\')\n        identifier_dict = dict()\n        identifier_dict[\'id\'] = utils.get_unique_string()\n        utils.saveToYaml(identifier_dict, identifier_file)\n\n\n    def adjust_learning_rate(self, optimizer, iteration):\n        """"""\n        Adjusts the learning rate according to the schedule\n        :param optimizer:\n        :type optimizer:\n        :param iteration:\n        :type iteration:\n        :return:\n        :rtype:\n        """"""\n\n        steps_between_learning_rate_decay = self._config[\'training\'][\'steps_between_learning_rate_decay\']\n        if iteration % steps_between_learning_rate_decay == 0:\n            for param_group in optimizer.param_groups:\n                param_group[\'lr\'] = param_group[\'lr\'] * self._config[""training""][""learning_rate_decay""]\n\n    @staticmethod\n    def set_learning_rate(optimizer, learning_rate):\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = learning_rate\n\n    @staticmethod\n    def get_learning_rate(optimizer):\n        for param_group in optimizer.param_groups:\n            lr = param_group[\'lr\']\n            break\n\n        return lr\n\n    def setup_tensorboard(self):\n        """"""\n        Starts the tensorboard server and sets up the plotting\n        :return:\n        :rtype:\n        """"""\n\n        # start tensorboard\n        # cmd = ""python -m tensorboard.main""\n        logging.info(""setting up tensorboard_logger"")\n        cmd = ""tensorboard --logdir=%s"" %(self._tensorboard_log_dir)\n        self._tensorboard_logger = tensorboard_logger.Logger(self._tensorboard_log_dir)\n        logging.info(""tensorboard logger started"")\n\n\n    @staticmethod\n    def load_default_config():\n        dc_source_dir = utils.getDenseCorrespondenceSourceDir()\n        config_file = os.path.join(dc_source_dir, \'config\', \'dense_correspondence\',\n                                   \'training\', \'training.yaml\')\n\n        config = utils.getDictFromYamlFilename(config_file)\n        return config\n\n    @staticmethod\n    def make_default():\n        dataset = SpartanDataset.make_default_caterpillar()\n        return DenseCorrespondenceTraining(dataset=dataset)\n\n'"
dense_correspondence/training/training_script.py,0,"b'import dense_correspondence_manipulation.utils.utils as utils\nutils.add_dense_correspondence_to_python_path()\nfrom dense_correspondence.training.training import *\nimport sys\nimport logging\n\n#utils.set_default_cuda_visible_devices()\n# utils.set_cuda_visible_devices([0]) # use this to manually set CUDA_VISIBLE_DEVICES\n\nfrom dense_correspondence.training.training import DenseCorrespondenceTraining\nfrom dense_correspondence.dataset.spartan_dataset_masked import SpartanDataset\nlogging.basicConfig(level=logging.INFO)\n\nfrom dense_correspondence.evaluation.evaluation import DenseCorrespondenceEvaluation\n\nconfig_filename = os.path.join(utils.getDenseCorrespondenceSourceDir(), \'config\', \'dense_correspondence\', \n                               \'dataset\', \'composite\', \'caterpillar_only.yaml\')\nconfig = utils.getDictFromYamlFilename(config_filename)\n\ntrain_config_file = os.path.join(utils.getDenseCorrespondenceSourceDir(), \'config\', \'dense_correspondence\', \n                               \'training\', \'training.yaml\')\n\ntrain_config = utils.getDictFromYamlFilename(train_config_file)\ndataset = SpartanDataset(config=config)\n\nlogging_dir = ""code/data_volume/pdc/trained_models/2018-10-15/""\nnum_iterations = (1500/4)-1\nd = 2 # the descriptor dimension\nname = ""shoes_progress_actually_iterative_%d"" %(d)\ntrain_config[""training""][""logging_dir_name""] = name\ntrain_config[""training""][""logging_dir""] = logging_dir\ntrain_config[""dense_correspondence_network""][""descriptor_dimension""] = d\ntrain_config[""training""][""num_iterations""] = num_iterations\n\nTRAIN = True\nEVALUATE = True\n\n# All of the saved data for this network will be located in the\n# code/data_volume/pdc/trained_models/tutorials/caterpillar_3 folder\n\n\n### NON ITERATIVE\n\n# if TRAIN:\n#     print ""training descriptor of dimension %d"" %(d)\n#     train = DenseCorrespondenceTraining(dataset=dataset, config=train_config)\n#     train.run()\n#     print ""finished training descriptor of dimension %d"" %(d)\n\n# quit()\n\n### ITERATIVE\n\n\n# First \nconfig_filename = os.path.join(utils.getDenseCorrespondenceSourceDir(), \'config\', \'dense_correspondence\', \n                               \'dataset\', \'composite\', \'shoe_train_1_green_nike.yaml\')\nconfig = utils.getDictFromYamlFilename(config_filename)\ndataset = SpartanDataset(config=config)\n\nprint ""training descriptor of dimension %d"" %(d)\ntrain = DenseCorrespondenceTraining(dataset=dataset, config=train_config)\ntrain.run()\nprint ""finished training descriptor of dimension %d"" %(d)\n\n# Second \nconfig_filename = os.path.join(utils.getDenseCorrespondenceSourceDir(), \'config\', \'dense_correspondence\', \n                               \'dataset\', \'composite\', \'shoe_train_1_gray_nike.yaml\')\nconfig = utils.getDictFromYamlFilename(config_filename)\ndataset = SpartanDataset(config=config)\n\nprint ""training descriptor of dimension %d"" %(d)\ntrain_config[""training""][""logging_dir_name""] = name+""1""\ntrain = DenseCorrespondenceTraining(dataset=dataset, config=train_config)\ntrain.run_from_pretrained(""2018-10-15/""+name)\nprint ""finished training descriptor of dimension %d"" %(d)\n\n# Third \nconfig_filename = os.path.join(utils.getDenseCorrespondenceSourceDir(), \'config\', \'dense_correspondence\', \n                               \'dataset\', \'composite\', \'shoe_train_1_red_nike.yaml\')\nconfig = utils.getDictFromYamlFilename(config_filename)\ndataset = SpartanDataset(config=config)\n\nprint ""training descriptor of dimension %d"" %(d)\ntrain_config[""training""][""logging_dir_name""] = name+""2""\ntrain = DenseCorrespondenceTraining(dataset=dataset, config=train_config)\ntrain.run_from_pretrained(""2018-10-15/""+name+""1"")\nprint ""finished training descriptor of dimension %d"" %(d)\n\n# Fourth\nconfig_filename = os.path.join(utils.getDenseCorrespondenceSourceDir(), \'config\', \'dense_correspondence\', \n                               \'dataset\', \'composite\', \'shoe_train_1_brown_boot.yaml\')\nconfig = utils.getDictFromYamlFilename(config_filename)\ndataset = SpartanDataset(config=config)\n\nprint ""training descriptor of dimension %d"" %(d)\ntrain_config[""training""][""logging_dir_name""] = name+""3""\ntrain = DenseCorrespondenceTraining(dataset=dataset, config=train_config)\ntrain.run_from_pretrained(""2018-10-15/""+name+""2"")\nprint ""finished training descriptor of dimension %d"" %(d)'"
modules/dense_correspondence_manipulation/__init__.py,0,b''
modules/user-interaction-heatmap-visualization/live_heatmap_visualization.py,0,"b'import sys\nimport os\nimport cv2\nimport numpy as np\nimport copy\n\nimport dense_correspondence_manipulation.utils.utils as utils\ndc_source_dir = utils.getDenseCorrespondenceSourceDir()\nsys.path.append(dc_source_dir)\nsys.path.append(os.path.join(dc_source_dir, ""dense_correspondence"", ""correspondence_tools""))\nfrom dense_correspondence.dataset.spartan_dataset_masked import SpartanDataset, ImageType\n\nimport dense_correspondence\nfrom dense_correspondence.evaluation.evaluation import *\nfrom dense_correspondence.evaluation.plotting import normalize_descriptor\nfrom dense_correspondence.network.dense_correspondence_network import DenseCorrespondenceNetwork\n\n\nimport dense_correspondence_manipulation.utils.visualization as vis_utils\n\n\nfrom dense_correspondence_manipulation.simple_pixel_correspondence_labeler.annotate_correspondences import label_colors, draw_reticle, pil_image_to_cv2, drawing_scale_config, numpy_to_cv2\n\n\n\n\nCOLOR_RED = np.array([0, 0, 255])\nCOLOR_GREEN = np.array([0,255,0])\n\nutils.set_default_cuda_visible_devices()\neval_config_filename = os.path.join(utils.getDenseCorrespondenceSourceDir(), \'config\', \'dense_correspondence\', \'evaluation\', \'evaluation.yaml\')\nEVAL_CONFIG = utils.getDictFromYamlFilename(eval_config_filename)\n\n\n\nLOAD_SPECIFIC_DATASET = False\n\nclass HeatmapVisualization(object):\n    """"""\n    Launches a live interactive heatmap visualization.\n\n    Edit config/dense_correspondence/heatmap_vis/heatmap.yaml to specify which networks\n    to visualize. Specifically add the network you want to visualize to the ""networks"" list.\n    Make sure that this network appears in the file pointed to by EVAL_CONFIG\n\n    Usage: Launch this file with python after sourcing the environment with\n    `use_pytorch_dense_correspondence`\n\n    Then `python live_heatmap_visualization.py`.\n\n    Keypresses:\n        n: new set of images\n        s: swap images\n        p: pause/un-pause\n    """"""\n\n    def __init__(self, config):\n        self._config = config\n        self._dce = DenseCorrespondenceEvaluation(EVAL_CONFIG)\n        self._load_networks()\n        self._reticle_color = COLOR_GREEN\n        self._paused = False\n        if LOAD_SPECIFIC_DATASET:\n            self.load_specific_dataset() # uncomment if you want to load a specific dataset\n\n    def _load_networks(self):\n        # we will use the dataset for the first network in the series\n        self._dcn_dict = dict()\n\n        self._dataset = None\n        self._network_reticle_color = dict()\n\n        for idx, network_name in enumerate(self._config[""networks""]):\n            dcn = self._dce.load_network_from_config(network_name)\n            dcn.eval()\n            self._dcn_dict[network_name] = dcn\n            # self._network_reticle_color[network_name] = label_colors[idx]\n\n            if len(self._config[""networks""]) == 1:\n                self._network_reticle_color[network_name] = COLOR_RED\n            else:\n                self._network_reticle_color[network_name] = label_colors[idx]\n\n            if self._dataset is None:\n                self._dataset = dcn.load_training_dataset()\n\n    def load_specific_dataset(self):\n        dataset_config_filename = os.path.join(utils.getDenseCorrespondenceSourceDir(), \'config\', \'dense_correspondence\',\n                                            \'dataset\', \'composite\', \'hats_3_demo_composite.yaml\')\n\n        # dataset_config_filename = os.path.join(utils.getDenseCorrespondenceSourceDir(), \'config\',\n        #                                        \'dense_correspondence\',\n        #                                        \'dataset\', \'composite\', \'4_shoes_all.yaml\')\n\n        dataset_config = utils.getDictFromYamlFilename(dataset_config_filename)\n        self._dataset = SpartanDataset(config=dataset_config)\n\n    def get_random_image_pair(self):\n        """"""\n        Gets a pair of random images for different scenes of the same object\n        """"""\n        object_id = self._dataset.get_random_object_id()\n        # scene_name_a = ""2018-04-10-16-02-59""\n        # scene_name_b = scene_name_a\n\n        scene_name_a = self._dataset.get_random_single_object_scene_name(object_id)\n        scene_name_b = self._dataset.get_different_scene_for_object(object_id, scene_name_a)\n\n        if self._config[""randomize_images""]:\n            image_a_idx = self._dataset.get_random_image_index(scene_name_a)\n            image_b_idx = self._dataset.get_random_image_index(scene_name_b)\n        else:\n            image_a_idx = 0\n            image_b_idx = 0\n\n        return scene_name_a, scene_name_b, image_a_idx, image_b_idx\n\n    def get_random_image_pair_across_object(self):\n        """"""\n        Gets cross object image pairs\n        :param randomize:\n        :type randomize:\n        :return:\n        :rtype:\n        """"""\n\n        object_id_a, object_id_b = self._dataset.get_two_different_object_ids()\n        # object_id_a = ""shoe_red_nike.yaml""\n        # object_id_b = ""shoe_gray_nike""\n        # object_id_b = ""shoe_green_nike""\n        scene_name_a = self._dataset.get_random_single_object_scene_name(object_id_a)\n        scene_name_b = self._dataset.get_random_single_object_scene_name(object_id_b)\n\n        if self._config[""randomize_images""]:\n            image_a_idx = self._dataset.get_random_image_index(scene_name_a)\n            image_b_idx = self._dataset.get_random_image_index(scene_name_b)\n        else:\n            image_a_idx = 0\n            image_b_idx = 0\n\n        return scene_name_a, scene_name_b, image_a_idx, image_b_idx\n\n    def get_random_image_pair_multi_object_scenes(self):\n        """"""\n        Gets cross object image pairs\n        :param randomize:\n        :type randomize:\n        :return:\n        :rtype:\n        """"""\n\n        scene_name_a = self._dataset.get_random_multi_object_scene_name()\n        scene_name_b = self._dataset.get_random_multi_object_scene_name()\n\n        if self._config[""randomize_images""]:\n            image_a_idx = self._dataset.get_random_image_index(scene_name_a)\n            image_b_idx = self._dataset.get_random_image_index(scene_name_b)\n        else:\n            image_a_idx = 0\n            image_b_idx = 0\n\n        return scene_name_a, scene_name_b, image_a_idx, image_b_idx\n\n    def _get_new_images(self):\n        """"""\n        Gets a new pair of images\n        :return:\n        :rtype:\n        """"""\n\n        if random.random() < 0.5:\n            self._dataset.set_train_mode()\n        else:\n            self._dataset.set_test_mode()\n\n        if self._config[""same_object""]:\n            scene_name_1, scene_name_2, image_1_idx, image_2_idx = self.get_random_image_pair()\n        elif self._config[""different_objects""]:\n            scene_name_1, scene_name_2, image_1_idx, image_2_idx = self.get_random_image_pair_across_object()\n        elif self._config[""multiple_object""]:\n            scene_name_1, scene_name_2, image_1_idx, image_2_idx = self.get_random_image_pair_multi_object_scenes()\n        else:\n            raise ValueError(""At least one of the image types must be set tot True"")\n\n\n        # caterpillar\n        # scene_name_1 = ""2018-04-16-14-42-26""\n        # scene_name_2 = ""2018-04-16-14-25-19""\n\n        # hats\n        # scene_name_1 = ""2018-05-15-22-01-44""\n        # scene_name_2 = ""2018-05-15-22-04-17""\n\n        self.img1_pil = self._dataset.get_rgb_image_from_scene_name_and_idx(scene_name_1, image_1_idx)\n        self.img2_pil = self._dataset.get_rgb_image_from_scene_name_and_idx(scene_name_2, image_2_idx)\n\n        self._scene_name_1 = scene_name_1\n        self._scene_name_2 = scene_name_2\n        self._image_1_idx = image_1_idx\n        self._image_2_idx = image_2_idx\n\n        self._compute_descriptors()\n\n        # self.rgb_1_tensor = self._dataset.rgb_image_to_tensor(img1_pil)\n        # self.rgb_2_tensor = self._dataset.rgb_image_to_tensor(img2_pil)\n\n\n    def _compute_descriptors(self):\n        """"""\n        Computes the descriptors for image 1 and image 2 for each network\n        :return:\n        :rtype:\n        """"""\n        self.img1 = pil_image_to_cv2(self.img1_pil)\n        self.img2 = pil_image_to_cv2(self.img2_pil)\n        self.rgb_1_tensor = self._dataset.rgb_image_to_tensor(self.img1_pil)\n        self.rgb_2_tensor = self._dataset.rgb_image_to_tensor(self.img2_pil)\n        self.img1_gray = cv2.cvtColor(self.img1, cv2.COLOR_RGB2GRAY) / 255.0\n        self.img2_gray = cv2.cvtColor(self.img2, cv2.COLOR_RGB2GRAY) / 255.0\n\n        cv2.imshow(\'source\', self.img1)\n        cv2.imshow(\'target\', self.img2)\n\n        self._res_a = dict()\n        self._res_b = dict()\n        for network_name, dcn in self._dcn_dict.iteritems():\n            self._res_a[network_name] = dcn.forward_single_image_tensor(self.rgb_1_tensor).data.cpu().numpy()\n            self._res_b[network_name] = dcn.forward_single_image_tensor(self.rgb_2_tensor).data.cpu().numpy()\n\n\n        self.find_best_match(None, 0, 0, None, None)\n\n    def scale_norm_diffs_to_make_heatmap(self, norm_diffs, threshold):\n        """"""\n        TODO (@manuelli) scale with Gaussian kernel instead of linear\n\n        Scales the norm diffs to make a heatmap. This will be scaled between 0 and 1.\n        0 corresponds to a match, 1 to non-match\n\n        :param norm_diffs: The norm diffs\n        :type norm_diffs: numpy.array [H,W]\n        :return:\n        :rtype:\n        """"""\n\n\n        heatmap = np.copy(norm_diffs)\n        greater_than_threshold = np.where(norm_diffs > threshold)\n        heatmap = heatmap / threshold * self._config[""heatmap_vis_upper_bound""] # linearly scale [0, threshold] to [0, 0.5]\n        heatmap[greater_than_threshold] = 1 # greater than threshold is set to 1\n        heatmap = heatmap.astype(self.img1_gray.dtype)\n        return heatmap\n\n    def find_best_match(self, event,u,v,flags,param):\n\n        """"""\n        For each network, find the best match in the target image to point highlighted\n        with reticle in the source image. Displays the result\n        :return:\n        :rtype:\n        """"""\n\n        if self._paused:\n            return\n\n        img_1_with_reticle = np.copy(self.img1)\n        draw_reticle(img_1_with_reticle, u, v, self._reticle_color)\n        cv2.imshow(""source"", img_1_with_reticle)\n\n        alpha = self._config[""blend_weight_original_image""]\n        beta = 1 - alpha\n\n        img_2_with_reticle = np.copy(self.img2)\n\n\n        print ""\\n\\n""\n\n        self._res_uv = dict()\n\n        # self._res_a_uv = dict()\n        # self._res_b_uv = dict()\n\n        for network_name in self._dcn_dict:\n            res_a = self._res_a[network_name]\n            res_b = self._res_b[network_name]\n            best_match_uv, best_match_diff, norm_diffs = \\\n                DenseCorrespondenceNetwork.find_best_match((u, v), res_a, res_b)\n            print ""\\n\\n""\n            print ""network_name:"", network_name\n            print ""scene_name_1"", self._scene_name_1\n            print ""image_1_idx"", self._image_1_idx\n            print ""scene_name_2"", self._scene_name_2\n            print ""image_2_idx"", self._image_2_idx\n\n            d = dict()\n            d[\'scene_name\'] = self._scene_name_1\n            d[\'image_idx\'] = self._image_1_idx\n            d[\'descriptor\'] = res_a[v, u, :].tolist()\n            d[\'u\'] = u\n            d[\'v\'] = v\n\n            print ""\\n-------keypoint info\\n"", d\n            print ""\\n--------\\n""\n\n            self._res_uv[network_name] = dict()\n            self._res_uv[network_name][\'source\'] = res_a[v, u, :].tolist()\n            self._res_uv[network_name][\'target\'] = res_b[v, u, :].tolist()\n\n            print ""res_a[v, u, :]:"", res_a[v, u, :]\n            print ""res_b[v, u, :]:"", res_b[best_match_uv[1], best_match_uv[0], :]\n\n            print ""%s best match diff: %.3f"" %(network_name, best_match_diff)\n            print ""res_a"", self._res_uv[network_name][\'source\']\n            print ""res_b"", self._res_uv[network_name][\'target\']\n\n            threshold = self._config[""norm_diff_threshold""]\n            if network_name in self._config[""norm_diff_threshold_dict""]:\n                threshold = self._config[""norm_diff_threshold_dict""][network_name]\n\n            heatmap_color = vis_utils.compute_gaussian_kernel_heatmap_from_norm_diffs(norm_diffs, self._config[\'kernel_variance\'])\n\n            reticle_color = self._network_reticle_color[network_name]\n\n            draw_reticle(heatmap_color, best_match_uv[0], best_match_uv[1], reticle_color)\n            draw_reticle(img_2_with_reticle, best_match_uv[0], best_match_uv[1], reticle_color)\n            blended = cv2.addWeighted(self.img2, alpha, heatmap_color, beta, 0)\n            cv2.imshow(network_name, blended)\n\n        cv2.imshow(""target"", img_2_with_reticle)\n        if event == cv2.EVENT_LBUTTONDOWN:\n            utils.saveToYaml(self._res_uv, \'clicked_point.yaml\')\n\n    def run(self):\n        self._get_new_images()\n        cv2.namedWindow(\'target\')\n        cv2.setMouseCallback(\'source\', self.find_best_match)\n\n        self._get_new_images()\n\n        while True:\n            k = cv2.waitKey(20) & 0xFF\n            if k == 27:\n                break\n            elif k == ord(\'n\'):\n                self._get_new_images()\n            elif k == ord(\'s\'):\n                img1_pil = self.img1_pil\n                img2_pil = self.img2_pil\n                self.img1_pil = img2_pil\n                self.img2_pil = img1_pil\n                self._compute_descriptors()\n            elif k == ord(\'p\'):\n                if self._paused:\n                    print ""un pausing""\n                    self._paused = False\n                else:\n                    print ""pausing""\n                    self._paused = True\n\n\nif __name__ == ""__main__"":\n    dc_source_dir = utils.getDenseCorrespondenceSourceDir()\n    config_file = os.path.join(dc_source_dir, \'config\', \'dense_correspondence\', \'heatmap_vis\', \'heatmap.yaml\')\n    config = utils.getDictFromYamlFilename(config_file)\n\n    heatmap_vis = HeatmapVisualization(config)\n    print ""starting heatmap vis""\n    heatmap_vis.run()\n    cv2.destroyAllWindows()\n\ncv2.destroyAllWindows()\n'"
modules/dense_correspondence_manipulation/change_detection/__init__.py,0,b''
modules/dense_correspondence_manipulation/change_detection/change_detection.py,0,"b'import numpy as np\nimport os\nimport time\n\n\nfrom director import imageview\nfrom director import vtkAll as vtk\nfrom director import transformUtils\nfrom director import visualization as vis\nfrom director import viewbehaviors\nfrom director import vtkNumpy as vnp\nfrom director.debugVis import DebugData\nfrom director.timercallback import TimerCallback\nfrom director import ioUtils\nfrom director import transformUtils\nfrom director import filterUtils\nimport PythonQt\n\n\nfrom director import mainwindowapp\nfrom PythonQt import QtCore, QtGui\n\nimport cv2\nfrom dense_correspondence_manipulation.change_detection.depthscanner import DepthScanner\nimport dense_correspondence_manipulation.utils.director_utils as director_utils\nfrom dense_correspondence_manipulation.fusion.fusion_reconstruction import FusionReconstruction, TSDFReconstruction\nfrom dense_correspondence_manipulation.utils.constants import *\n\nimport dense_correspondence_manipulation.utils.utils as utils\nimport dense_correspondence_manipulation.utils.segmentation as segmentation\n\n\nclass DepthImageVisualizer(object):\n    def __init__(self, window_title=\'Depth Image\', scale=1):\n        \n        self.scale = scale\n\n        depthImageColorByRange = [0.0, 4.0]\n\n        lut = vtk.vtkLookupTable()\n        lut.SetNumberOfColors(256)\n        lut.SetHueRange(0, 0.667) # red to blue\n        lut.SetRange(depthImageColorByRange) # map red (near) to blue (far)\n        lut.SetRampToLinear()\n        lut.Build()\n\n        self.depthScaleFilter = vtk.vtkImageShiftScale()\n        self.depthScaleFilter.SetScale(scale)\n        self.depthScaleFilter.SetOutputScalarTypeToUnsignedShort()\n\n        self.depthImageLookupTable = lut\n        self.imageMapToColors = vtk.vtkImageMapToColors()\n        self.imageMapToColors.SetLookupTable(self.depthImageLookupTable)\n        self.imageMapToColors.SetInputConnection(self.depthScaleFilter.GetOutputPort())\n\n        self.imageView = imageview.ImageView()\n        self.imageView.view.setWindowTitle(window_title)\n        self.imageView.setImage(self.imageMapToColors.GetOutput())\n\n    def setImage(self, depthImage):\n        self.depthScaleFilter.SetInputData(depthImage)\n        self.depthScaleFilter.Update()\n\n        self.depthImageLookupTable.SetRange(self.depthScaleFilter.GetOutput().GetScalarRange())\n        self.imageMapToColors.Update()\n        self.imageView.resetCamera()\n\n    def setImageNumpy(self, img):\n        vtkImg = vnp.numpyToImageData(img)\n        self.setImage(vtkImg)\n\n\n\n\nclass ChangeDetection(object):\n\n    def __init__(self, app, view, cameraIntrinsics=None, debug=True,\n        data_directory=None):\n        """"""\n        This shouldn\'t be called directly. Should only be called via the staticmethod from_data_folder\n\n        :param app:\n        :type app:\n        :param view:\n        :type view:\n        :param cameraIntrinsics:\n        :type cameraIntrinsics:\n        :param debug:\n        :type debug:\n        :param data_directory: The \'processed\' subfolder of a top level log folder\n        :type data_directory:\n        """"""\n\n        self.app = app\n        self.views = dict()\n        self.vis = True\n        self.debug = debug\n        self.views[\'foreground\'] = view\n        self.views[\'background\'] = self.createBackgroundView()\n\n        self.threshold = 10\n\n        if cameraIntrinsics is None:\n            self.cameraIntrinsics = makeDefaultCameraIntrinsics()\n        else:\n            self.cameraIntrinsics = cameraIntrinsics\n\n        # set the views to use the camera intrinsics\n        for _, view in self.views.iteritems():\n            director_utils.setCameraIntrinsics(view, self.cameraIntrinsics, lockViewSize=True)\n\n        self.depthScanners = dict()\n\n\n        self.depthScanners[\'foreground\'] = initDepthScanner(self.app, self.views[\'foreground\'],\n                                                            widgetArea=QtCore.Qt.RightDockWidgetArea)\n\n        self.depthScanners[\'background\'] = initDepthScanner(self.app, self.views[\'background\'],\n                                                            widgetArea=QtCore.Qt.LeftDockWidgetArea)\n\n        \n        # self.changeDetectionPointCloudView = PythonQt.dd.ddQVTKWidgetView()\n        # self.changeDetectionPointCloudView.setWindowTitle(\'Change Detection Pointcloud\')\n        # dock = self.app.app.addWidgetToDock(self.changeDetectionPointCloudView, QtCore.Qt.BottomDockWidgetArea)\n        # dock.setMinimumWidth(300)\n        # dock.setMinimumHeight(300)\n\n        # used for visualizing the depth image/mask coming from change detection\n        # self.changeDetectionImageVisualizer = imageview.ImageView()\n        # self.changeDetectionImageVisualizer.view.setWindowTitle(\'Depth Image Change\')\n        # dock = self.app.app.addWidgetToDock(self.changeDetectionImageVisualizer.view, QtCore.Qt.BottomDockWidgetArea)\n        #\n        # dock.setMinimumWidth(300)\n        # dock.setMinimumHeight(300)\n\n    @property\n    def background_reconstruction(self):\n        return self._background_reconstruction\n\n    @background_reconstruction.setter\n    def background_reconstruction(self, value):\n        assert isinstance(value, FusionReconstruction)\n\n        view = self.views[\'background\']\n        self._background_reconstruction = value\n        self._background_reconstruction.name = ""background""\n        self._background_reconstruction.visualize_reconstruction(view)\n\n    @property\n    def foreground_reconstruction(self):\n        return self._foreground_reconstruction\n\n    @foreground_reconstruction.setter\n    def foreground_reconstruction(self, value):\n        assert isinstance(value, FusionReconstruction)\n        self._foreground_reconstruction = value\n        self._foreground_reconstruction.name = ""foreground""\n\n        view = self.views[\'foreground\']\n        self._foreground_reconstruction.visualize_reconstruction(view)\n        \n\n    def createBackgroundView(self):\n        """"""\n        Creates a new view for use in rendering the background scene\n        """"""\n        view = PythonQt.dd.ddQVTKWidgetView()\n        view.orientationMarkerWidget().Off()\n\n        if self.vis:\n            dock = self.app.app.addWidgetToDock(view, QtCore.Qt.LeftDockWidgetArea)\n            dock.setMinimumWidth(300)\n            dock.setMinimumHeight(300)\n\n        return view\n\n    def updateDepthScanners(self):\n        """"""\n        Updates all the DepthScanner objects to make sure we are rendering\n        against the current scene\n        """"""\n        for _, scanner in self.depthScanners.iteritems():\n            scanner.update()\n\n    @staticmethod\n    def drawNumpyImage(title, img):\n        """"""\n        Draws a new image using ImageView\n        """"""\n        imageView = imageview.ImageView()\n        imageView.view.setWindowTitle(title)\n        imageView.showNumpyImage(img)\n        imageView.show()\n\n        return imageView\n\n    @staticmethod\n    def drawDepthImageCV2(title, img):\n        cv2.imshow(title, img.squeeze()/DEPTH_IM_RESCALE)\n\n\n    def setCameraTransform(self, cameraToWorld):\n        """"""\n        Sets the camera location in both foreground and background scenes\n\n        Parameters:\n            cameraToWorld: vtkTransform\n                specifies the transform from camera frame to world frame\n                CameraFrame is encoded as right-down-forward\n        """"""\n\n        for _, view in self.views.iteritems():\n            director_utils.setCameraTransform(view.camera(), cameraToWorld)\n            view.forceRender()\n\n        self.updateDepthScanners()\n\n        \n    def computeForegroundMask(self, visualize=True):\n        """"""\n        Computes the foreground mask. The camera location of the foreground view should\n        already be set correctly. Steps of the pipeline are\n\n        1. Set the background camera to same location as foreground camera\n        2. Render depth images from foreground and background scenes\n        3. Compute change detection based on pair of depth images\n        """"""\n\n        # assume that foreground view is already in the correct place\n        view_f = self.views[\'foreground\']\n        view_b = self.views[\'background\']\n\n        view_f.forceRender()\n        cameraToWorld = director_utils.getCameraTransform(view_f.camera())\n\n        director_utils.setCameraTransform(view_f.camera(), cameraToWorld)\n        view_f.forceRender()\n\n        director_utils.setCameraTransform(view_b.camera(), cameraToWorld)\n        view_b.forceRender()\n\n        self.updateDepthScanners()\n\n        # get the depth images\n        # make sure to convert them to int32 since we want to avoid wrap-around issues when using\n        # uint16 types\n        depth_img_foreground_raw = self.depthScanners[\'foreground\'].getDepthImageAsNumpyArray()\n        depth_img_f = np.array(depth_img_foreground_raw, dtype=np.int32)\n        depth_img_b = np.array(self.depthScanners[\'background\'].getDepthImageAsNumpyArray(), dtype=np.int32)\n\n        # zero values are interpreted as max-range measurements\n        depth_img_b[depth_img_b == 0] = np.iinfo(np.int32).max\n        depth_img_f[depth_img_f == 0] = np.iinfo(np.int32).max\n\n        idx, mask = ChangeDetection.computeForegroundMaskFromDepthImagePair(depth_img_f, depth_img_b, self.threshold)\n\n        depth_img_change = mask * depth_img_f\n\n        returnData = dict()\n        returnData[\'idx\'] = idx\n        returnData[\'mask\'] = mask\n        returnData[\'depth_img_foreground\'] = depth_img_f\n        returnData[\'depth_img_background\'] = depth_img_b\n        returnData[\'depth_img_foreground_raw\'] = depth_img_foreground_raw\n        returnData[\'depth_img_change\'] = depth_img_change\n\n\n        if visualize:\n            # make sure to remap the mask to [0,255], otherwise it will be all black\n            ChangeDetection.drawNumpyImage(\'Change Detection Mask\', mask*255)\n\n        return returnData\n\n    def computeForegroundMaskUsingCropStrategy(self, visualize=True):\n        """"""\n        Computes the foreground mask. The camera location of the foreground view should\n        already be set correctly. Steps of the pipeline are\n\n        1. Render depth image from cropped foreground\n        2. Everything in the ""crop"" should be foreground for now\n        """"""\n\n        # assume that foreground view is already in the correct place\n        view_f = self.views[\'foreground\']\n        view_b = self.views[\'background\']\n\n        view_f.forceRender()\n        cameraToWorld = director_utils.getCameraTransform(view_f.camera())\n        self.updateDepthScanners()\n\n        # get the depth images\n        # make sure to convert them to int32 since we want to avoid wrap-around issues when using\n        # uint16 types\n        depth_img_foreground_raw = self.depthScanners[\'foreground\'].getDepthImageAsNumpyArray()\n        depth_img_f = np.array(depth_img_foreground_raw, dtype=np.int32)\n\n\n        idx = depth_img_f > 0\n        mask = np.zeros(np.shape(depth_img_f))\n        mask[idx] = 1\n\n        returnData = dict()\n        returnData[\'idx\'] = idx\n        returnData[\'mask\'] = mask\n        returnData[\'depth_img_foreground_raw\'] = depth_img_foreground_raw\n        returnData[\'depth_img_foreground\'] = depth_img_f\n\n        if visualize:\n            # make sure to remap the mask to [0,255], otherwise it will be all black\n            ChangeDetection.drawNumpyImage(\'Change Detection Mask\', mask*255)\n\n        return returnData\n\n    @staticmethod\n    def computeForegroundMaskFromDepthImagePair(img_f, img_b, threshold):\n\n        """"""\n        Computes the foreground mask given a pair of depth images. Depths that are closer in\n        img_f compared to img_b, by at least \'threshold\' are classified as foreground pixels\n\n        Be careful with\n        wraparound if you are usint uint16 type.\n        """"""\n\n        \n        idx = (img_b - img_f) > threshold\n        mask = np.zeros(np.shape(img_f))\n        mask[idx] = 1\n        return idx, mask\n\n    def showCameraTransform(self):\n        t = director_utils.getCameraTransform(self.views[\'foreground\'].camera())\n        vis.updateFrame(t, \'camera transform\', scale=0.15)\n\n\n    def run(self, output_dir=None, rendered_images_dir=None):\n        """"""\n        Renders the masks and the depth images for the cropped scene\n\n        Produces 3 types of files\n\n        processed/image_masks/000000_mask.png\n        processed/image_masks/000000_visible_mask.png\n        processed/rendered_images/000000_depth_cropped.png\n        :return:\n        """"""\n\n        if output_dir is None:\n            output_dir = os.path.join(self.foreground_reconstruction.data_dir, \'image_masks\')\n\n        if rendered_images_dir is None:\n            rendered_images_dir = os.path.join(self.foreground_reconstruction.data_dir, \'rendered_images\')\n\n        fusion_mesh_foreground_file = os.path.join(os.path.join(self.foreground_reconstruction.data_dir, \'fusion_mesh_foreground.ply\'))\n\n        if not os.path.isdir(output_dir):\n            os.makedirs(output_dir)\n\n        if not os.path.isdir(rendered_images_dir):\n            os.makedirs(rendered_images_dir)\n\n        start_time = time.time()\n\n        print ""saving cropped mesh""\n        self.foreground_reconstruction.save_poly_data(fusion_mesh_foreground_file)\n\n        # read in each image in the log\n        camera_pose_data = self.foreground_reconstruction.kinematics_pose_data\n        img_file_extension = \'png\'\n\n        num_poses = self.foreground_reconstruction.kinematics_pose_data.num_poses()\n\n        logging_rate = 50\n        counter = 0\n\n        for idx, value in camera_pose_data.pose_dict.iteritems():\n            if (counter % logging_rate) == 0:\n                print ""Rendering mask for pose %d of %d"" %(counter + 1, num_poses)\n\n            mask_image_filename = utils.getPaddedString(idx) + ""_mask"" + ""."" + img_file_extension\n            mask_image_full_filename = os.path.join(output_dir, mask_image_filename)\n\n            camera_to_world = self.foreground_reconstruction.get_camera_to_world(idx)\n            self.setCameraTransform(camera_to_world)\n            d = self.computeForegroundMaskUsingCropStrategy(visualize=False)\n\n            mask = d[\'mask\']\n            visible_mask = mask*255\n\n            visible_mask_filename = os.path.join(output_dir, utils.getPaddedString(idx) + \'_visible_mask\'\n                                                 + ""."" + img_file_extension)\n\n            depth_img_filename = os.path.join(rendered_images_dir, utils.getPaddedString(idx) + \'_depth_cropped\'\n                                                 + ""."" + img_file_extension)\n\n            # save the images\n            cv2.imwrite(mask_image_full_filename, mask)\n            cv2.imwrite(visible_mask_filename, visible_mask)\n\n            # make sure to save this as uint16\n            depth_img = d[\'depth_img_foreground_raw\']\n            cv2.imwrite(depth_img_filename, depth_img)\n\n            counter += 1\n\n        end_time = time.time()\n\n        print ""rendering masks took %d seconds"" %(end_time - start_time)\n\n    def render_depth_images(self, output_dir=None, rendered_images_dir=None):\n        """"""\n        Note: This is actually rendering against whatever is saved as self.foreground_reconstruction\n        Renders the depth images on the entire scene\n        processed/rendered_images/000000_depth.png\n        :return:\n        """"""\n\n        if output_dir is None:\n            output_dir = os.path.join(self.foreground_reconstruction.data_dir, \'image_masks\')\n\n        if rendered_images_dir is None:\n            rendered_images_dir = os.path.join(self.foreground_reconstruction.data_dir, \'rendered_images\')\n\n        start_time = time.time()\n        start_time = time.time()\n\n        # read in each image in the log\n        image_dir = self.foreground_reconstruction.image_dir\n        camera_pose_data = self.foreground_reconstruction.kinematics_pose_data\n        img_file_extension = \'png\'\n\n        num_poses = self.foreground_reconstruction.kinematics_pose_data.num_poses()\n\n        logging_rate = 50\n        counter = 0\n\n        for idx, value in camera_pose_data.pose_dict.iteritems():\n            if (counter % logging_rate) == 0:\n                print ""Rendering depth image for pose %d of %d"" % (counter + 1, num_poses)\n\n\n            camera_to_world = self.foreground_reconstruction.get_camera_to_world(idx)\n            self.setCameraTransform(camera_to_world)\n            depth_img = self.depthScanners[\'foreground\'].getDepthImageAsNumpyArray()\n            depth_img_filename = os.path.join(rendered_images_dir, utils.getPaddedString(idx) + \'_depth\'\n                                              + ""."" + img_file_extension)\n\n            cv2.imwrite(depth_img_filename, depth_img)\n\n            counter += 1\n\n        end_time = time.time()\n\n        print ""rendering depth images took %d seconds"" % (end_time - start_time)\n\n\n    @staticmethod\n    def create_change_detection_app(globalsDict=None):\n        from director import mainwindowapp\n        from PythonQt import QtCore, QtGui\n\n        app = mainwindowapp.construct()\n        app.gridObj.setProperty(\'Visible\', True)\n        app.viewOptions.setProperty(\'Orientation widget\', False)\n        app.viewOptions.setProperty(\'View angle\', 30)\n        app.sceneBrowserDock.setVisible(False)\n        app.propertiesDock.setVisible(False)\n        app.mainWindow.setWindowTitle(\'Depth Scanner\')\n        app.mainWindow.show()\n        app.mainWindow.resize(920, 600)\n        app.mainWindow.move(0, 0)\n\n        view = app.view\n        view.setParent(None)\n        mdiArea = QtGui.QMdiArea()\n        app.mainWindow.setCentralWidget(mdiArea)\n        subWindow = mdiArea.addSubWindow(view)\n        subWindow.setMinimumSize(300, 300)\n        subWindow.setWindowTitle(\'Camera image\')\n        subWindow.resize(640, 480)\n        mdiArea.tileSubWindows()\n\n        globalsDict[\'view\'] = view\n        globalsDict[\'app\'] = app\n\n    @staticmethod\n    def from_data_folder(data_folder, config=None, globalsDict=None, background_data_folder=None):\n        """"""\n        Creates a ChangeDetection object from a data_folder which contains the\n        3D reconstruction and the image files\n\n        :param data_folder: This should point to the `processed` subfolder of a top level log folder\n        :param config:\n        :param globalsDict:\n        :return:\n        """"""\n        foreground_reconstruction = TSDFReconstruction.from_data_folder(data_folder, config=config)\n\n        if background_data_folder is None:\n            background_data_folder = data_folder\n\n        print ""background folder: "", background_data_folder\n\n        background_reconstruction_placeholder = TSDFReconstruction.from_data_folder(background_data_folder, config=config, load_foreground_mesh=False)\n\n\n        if globalsDict is None:\n            globalsDict = dict()\n\n        ChangeDetection.create_change_detection_app(globalsDict)\n        view = globalsDict[\'view\']\n        app = globalsDict[\'app\']\n\n\n        camera_info_file = os.path.join(data_folder, \'images\', \'camera_info.yaml\')\n        camera_intrinsics = utils.CameraIntrinsics.from_yaml_file(camera_info_file)\n        changeDetection = ChangeDetection(app, view, cameraIntrinsics=camera_intrinsics)\n        changeDetection.foreground_reconstruction = foreground_reconstruction\n        changeDetection.background_reconstruction = background_reconstruction_placeholder\n\n        globalsDict[\'changeDetection\'] = changeDetection\n        return changeDetection, globalsDict\n\n\n    @staticmethod\n    def render_depth_images_full_scene(data_folder, config=None, background_data_folder=None):\n        """"""\n        Renders depth images against the entire scene\n        :param data_folder:\n        :type data_folder:\n        :param config:\n        :type config:\n        :param background_data_folder:\n        :type background_data_folder:\n        :return:\n        :rtype:\n        """"""\n\n        change_detection.ChangeDetection.from_data_folder(data_folder, config=config, globalsDict=globalsDict,\n                                                          background_data_folder=background_scene_data_folder)\n\n\n    ##################### DEBUGGING FUNCTIONS #################\n    def testCreateBackground(self):\n        d = DebugData()\n        d.addCube((1,1,0.1), (0,0,0), color=[1,1,1])\n        self.backgroundPolyData = d.getPolyData()\n\n        if self.vis:\n            view = self.views[\'background\']\n            vis.updatePolyData(self.backgroundPolyData, \'background\', view=view, colorByName=\'RGB255\')\n            view.resetCamera()\n\n    def testCreateForeground(self):\n        d = DebugData()\n        dims = np.array([0.2,0.2,0.2])\n        center = (0,0,dims[2]/2.0)\n        d.addCube(dims, center, color=[0,1,0])\n\n        d.addPolyData(self.backgroundPolyData)\n        \n\n        self.foregroundPolyData = d.getPolyData()\n\n        if self.vis:\n            view=self.views[\'foreground\']\n            vis.updatePolyData(self.foregroundPolyData, \'foreground\', view=view, colorByName=\'RGB255\')\n\n            view.resetCamera()\n\n    def drawDebug(self):\n        ChangeDetection.drawDepthImageCV2(\'foreground\', self.data[\'depth_img_foreground\'])\n        ChangeDetection.drawDepthImageCV2(\'background\', self.data[\'depth_img_background\'])\n        ChangeDetection.drawDepthImageCV2(\'change\', self.data[\'depth_img_change\'])\n\n        cv2.imshow(\'mask\', self.data[\'mask\']*255.0)\n\n    def testComputeForegroundMask(self):\n        data = self.computeForegroundMask()\n        depth_img_change = data[\'depth_img_change\']\n        self.img = depth_img_change\n        self.data = data\n\n    def testIdentity(self):\n        view = self.views[\'foreground\']\n        cameraToWorld = director_utils.getCameraTransform(view.camera())\n        self.setCameraTransform(cameraToWorld)\n        self.updateDepthScanners()\n\n    def testGetCameraTransforms(self):\n        vis.updateFrame(vtk.vtkTransform(), ""origin frame"", view=self.views[\'background\'])\n        for viewType, view in self.views.iteritems():\n            print ""\\n\\nviewType: "", viewType\n            cameraTransform = director_utils.getCameraTransform(view.camera())\n            pos, quat = transformUtils.poseFromTransform(cameraTransform)\n            print ""pos: "", pos\n            print ""quat: "", quat\n\n\n            vis.updateFrame(cameraTransform, viewType + ""_frame"")\n\n\n    def testSetCameraPose(self):\n        for viewType, view in self.views.iteritems():\n            camera = view.camera()\n            director_utils.setCameraTransform(camera, CAMERA_TO_WORLD)\n            view.forceRender()\n\n\n    def testMoveCameraAndGetDepthImage(self):\n        self.testSetCameraPose()\n        self.depthScanners[\'foreground\'].update()\n        self.img = self.getDepthImage(visualize=False)\n        # self.changeDetectionImageVisualizer.setImage(self.img)\n\n    def testDepthImageToPointcloud(self):\n        ds = self.depthScanners[\'foreground\']\n        camera = ds.view.camera()\n        depth_img = ds.getDepthImage()\n        depth_img_raw, _, _ = ds.getDepthImageAndPointCloud()\n        depth_img_numpy = ds.getDepthImageAsNumpyArray()\n\n        print ""depth min %.3f, depth max = %.3f"" %(np.min(depth_img_numpy), np.max(depth_img_numpy))\n\n        # self.changeDetectionImageVisualizer.setImage(depth_img)\n\n\n        f = vtk.vtkDepthImageToPointCloud()\n        f.SetCamera(camera)\n        f.SetInputData(depth_img)\n        f.Update()\n        polyData = f.GetOutput()\n\n        print ""polyData.GetNumberOfPoints() = "", polyData.GetNumberOfPoints()\n        view = self.changeDetectionPointCloudView\n        vis.updatePolyData(polyData, \'depth_im_to_pointcloud\')\n\n    def testShowBackground(self):\n        view = self.views[\'foreground\']\n        self.background_reconstruction.visualize_reconstruction(view)\n\n    def testRenderMask(self, idx=0, save_mask=True):\n        camera_to_world = self.foreground_reconstruction.fusion_pose_data.get_camera_to_world_pose(idx)\n        self.setCameraTransform(camera_to_world)\n        d = self.computeForegroundMaskUsingCropStrategy(visualize=True)\n        mask = d[\'mask\']\n\n        mask_filename = ""mask.png""\n        cv2.imwrite(mask_filename, mask)\n\n    def testGetDepthImage(self, type=\'foreground\'):\n        self.testIdentity()\n        self.updateDepthScanners()\n        depth_img = self.depthScanners[type].getDepthImageAsNumpyArray()\n        depth_img_vis = depth_img*255.0/4000.0\n\n\n        # make sure to remap the mask to [0,255], otherwise it will be all black\n        ChangeDetection.drawNumpyImage(\'Depth Image \' + type , depth_img_vis)\n        return depth_img\n\n    def test(self):\n        self.updateDepthScanners()\n        self.testGetDepthImage(\'foreground\')\n        self.testGetDepthImage(\'background\')\n\n\n\n\ndef initDepthScanner(app, view, widgetArea=QtCore.Qt.RightDockWidgetArea):\n    depthScanner = DepthScanner(view)\n    depthScanner.update()\n\n    dock = app.app.addWidgetToDock(depthScanner.imageView.view, widgetArea)\n    dock.setMinimumWidth(300)\n    dock.setMinimumHeight(300)\n\n    dock = app.app.addWidgetToDock(depthScanner.pointCloudView, widgetArea)\n    dock.setMinimumWidth(300)\n    dock.setMinimumHeight(300)\n\n    return depthScanner\n\ndef makeDefaultCameraIntrinsics():\n    fx = 533.6422696034836\n    cx = 319.4091030774892\n    fy = 534.7824445233571\n    cy = 236.4374299691866\n\n    width = 640\n    height = 480\n\n    return utils.CameraIntrinsics(cx, cy, fx, fy, width, height)\\\n\ndef loadDefaultBackground():\n    data_folder = \'/home/manuelli/code/data_volume/sandbox/drill_scenes/00_background\'\n    reconstruction = FusionReconstruction.from_data_folder(data_folder)\n    return reconstruction\n\ndef loadDefaultForeground():\n    data_folder = \'/home/manuelli/code/data_volume/sandbox/drill_scenes/01_drill\'\n    reconstruction = FusionReconstruction.from_data_folder(data_folder)\n    return reconstruction\n\n\ndef main(globalsDict, data_folder=None):\n    if data_folder is None:\n        data_folder = \'/home/manuelli/code/data_volume/sandbox/04_drill_long_downsampled\'\n    changeDetection, globalsDict = ChangeDetection.from_data_folder(data_folder, globalsDict=globalsDict)\n\n    globalsDict[\'cd\'] = changeDetection\n    app = globalsDict[\'app\']\n\n    app.app.start(restoreWindow=False)\n\n\nif __name__ == \'__main__\':\n    main(globals())'"
modules/dense_correspondence_manipulation/change_detection/depthscanner.py,0,"b'from director import imageview\nfrom director import vtkAll as vtk\nfrom director import transformUtils\nfrom director import visualization as vis\nfrom director import viewbehaviors\nfrom director import vtkNumpy as vnp\nfrom director.debugVis import DebugData\nfrom director.timercallback import TimerCallback\nimport PythonQt\nimport numpy as np\n\nimport cv2\n\n    \n\ndef vtk_image_to_numpy_array(vtk_image):\n      w, h, _ = vtk_image.GetDimensions()\n      scalars = vnp.getNumpyFromVtk(vtk_image, vtk_image.GetPointData().GetScalars().GetName())\n      img = scalars.reshape(h, w, -1)\n      img = np.flipud(img) # you might need to flip the image rows, vtk has a difference row ordering convention that numpy/opencv\n      return img\n      \n\n\ndef computeDepthImageAndPointCloud(depthBuffer, colorBuffer, camera):\n    """"""\n    Returns depth image and pointcloud as vtkImageData and vtkPolyData\n\n    :param depthBuffer: OpenGL depth buffer\n    :type depthBuffer:\n    :param colorBuffer: OpenGL color buffer\n    :type colorBuffer:\n    :param camera: vtkCamera instance that was used to render the scene\n    :type camera: vtkCamera instance\n    :return:\n    :rtype: vtkImageData, vtkPolyData, numpy array\n    """"""\n\n    depthImage = vtk.vtkImageData()\n    pts = vtk.vtkPoints()\n    ptColors = vtk.vtkUnsignedCharArray()\n    vtk.vtkDepthImageUtils.DepthBufferToDepthImage(depthBuffer, colorBuffer, camera, depthImage, pts, ptColors)\n\n    pts = vnp.numpy_support.vtk_to_numpy(pts.GetData())\n    polyData = vnp.numpyToPolyData(pts, createVertexCells=True)\n    ptColors.SetName(\'rgb\')\n    polyData.GetPointData().AddArray(ptColors)\n\n    return depthImage, polyData, pts\n\n\nclass DepthScanner(object):\n\n    def __init__(self, view):\n        self.view = view\n\n        self.depthImage = None\n        self.pointCloudObj = None\n        self.renderObserver = None\n\n        self.windowToDepthBuffer = vtk.vtkWindowToImageFilter()\n        self.windowToDepthBuffer.SetInput(self.view.renderWindow())\n        self.windowToDepthBuffer.SetInputBufferTypeToZBuffer()\n        self.windowToDepthBuffer.ShouldRerenderOff()\n\n        self.windowToColorBuffer = vtk.vtkWindowToImageFilter()\n        self.windowToColorBuffer.SetInput(self.view.renderWindow())\n        self.windowToColorBuffer.SetInputBufferTypeToRGB()\n        self.windowToColorBuffer.ShouldRerenderOff()\n\n        useBackBuffer = False\n        if useBackBuffer:\n            self.windowToDepthBuffer.ReadFrontBufferOff()\n            self.windowToColorBuffer.ReadFrontBufferOff()\n\n        self.initDepthImageView()\n        self.initPointCloudView()\n\n        self._block = False\n        self.singleShotTimer = TimerCallback()\n        self.singleShotTimer.callback = self.update\n\n    def getDepthBufferImage(self):\n        return self.windowToDepthBuffer.GetOutput()\n\n    def getDepthImage(self):\n        return self.depthScaleFilter.GetOutput()\n\n    def getDepthImageAsNumpyArray(self):\n        vtk_image = self.getDepthImage()\n        return vtk_image_to_numpy_array(vtk_image)\n\n    def getColorBufferImage(self):\n        return self.windowToColorBuffer.GetOutput()\n\n    def updateBufferImages(self):\n        for f in [self.windowToDepthBuffer, self.windowToColorBuffer]:\n            f.Modified()\n            f.Update()\n\n    def initDepthImageView(self):\n\n        self.depthImageColorByRange = [0.0, 4.0]\n\n        lut = vtk.vtkLookupTable()\n        lut.SetNumberOfColors(256)\n        lut.SetHueRange(0, 0.667) # red to blue\n        lut.SetRange(self.depthImageColorByRange) # map red (near) to blue (far)\n        lut.SetRampToLinear()\n        lut.Build()\n\n        self.depthScaleFilter = vtk.vtkImageShiftScale()\n        self.depthScaleFilter.SetScale(1000)\n        self.depthScaleFilter.SetOutputScalarTypeToUnsignedShort()\n\n        self.depthImageLookupTable = lut\n        self.imageMapToColors = vtk.vtkImageMapToColors()\n        self.imageMapToColors.SetLookupTable(self.depthImageLookupTable)\n        self.imageMapToColors.SetInputConnection(self.depthScaleFilter.GetOutputPort())\n\n        self.imageView = imageview.ImageView()\n        self.imageView.view.setWindowTitle(\'Depth image\')\n        self.imageView.setImage(self.imageMapToColors.GetOutput())\n\n    def initPointCloudView(self):\n        self.pointCloudView = PythonQt.dd.ddQVTKWidgetView()\n        self.pointCloudView.setWindowTitle(\'Pointcloud\')\n        self.pointCloudViewBehaviors = viewbehaviors.ViewBehaviors(self.pointCloudView)\n\n    def update(self):\n\n        if not self.renderObserver:\n            def onEndRender(obj, event):\n                if self._block:\n                    return\n                if not self.singleShotTimer.singleShotTimer.isActive():\n                    self.singleShotTimer.singleShot(0)\n            self.renderObserver = self.view.renderWindow().AddObserver(\'EndEvent\', onEndRender)\n\n        self._block = True\n        self.view.forceRender()\n        self.updateBufferImages()\n        self._block = False\n\n        depthImage, polyData, _ = computeDepthImageAndPointCloud(self.getDepthBufferImage(), self.getColorBufferImage(), self.view.camera())\n\n        self.depthScaleFilter.SetInputData(depthImage)\n        self.depthScaleFilter.Update()\n\n        self.depthImageLookupTable.SetRange(self.depthScaleFilter.GetOutput().GetScalarRange())\n        self.imageMapToColors.Update()\n        self.imageView.resetCamera()\n        #self.imageView.view.render()\n\n        if not self.pointCloudObj:\n            self.pointCloudObj = vis.showPolyData(polyData, \'point cloud\', colorByName=\'rgb\', view=self.pointCloudView)\n        else:\n            self.pointCloudObj.setPolyData(polyData)\n        self.pointCloudView.render()\n\n    def getDepthImageAndPointCloud(self):\n        return computeDepthImageAndPointCloud(self.getDepthBufferImage(), self.getColorBufferImage(), self.view.camera())\n\ndef main(globalsDict=None):\n\n    from director import mainwindowapp\n    from PythonQt import QtCore, QtGui\n\n    app = mainwindowapp.construct()\n    app.gridObj.setProperty(\'Visible\', True)\n    app.viewOptions.setProperty(\'Orientation widget\', False)\n    app.viewOptions.setProperty(\'View angle\', 30)\n    app.sceneBrowserDock.setVisible(False)\n    app.propertiesDock.setVisible(False)\n    app.mainWindow.setWindowTitle(\'Depth Scanner\')\n    app.mainWindow.show()\n    app.mainWindow.resize(920,600)\n    app.mainWindow.move(0,0)\n\n    view = app.view\n    view.setParent(None)\n    mdiArea = QtGui.QMdiArea()\n    app.mainWindow.setCentralWidget(mdiArea)\n    subWindow = mdiArea.addSubWindow(view)\n    subWindow.setMinimumSize(300,300)\n    subWindow.setWindowTitle(\'Camera image\')\n    subWindow.resize(640, 480)\n    mdiArea.tileSubWindows()\n\n    #affordanceManager = affordancemanager.AffordanceObjectModelManager(view)\n\n    depthScanner = DepthScanner(view)\n    depthScanner.update()\n\n    dock = app.app.addWidgetToDock(depthScanner.imageView.view, QtCore.Qt.RightDockWidgetArea)\n    dock.setMinimumWidth(300)\n    dock.setMinimumHeight(300)\n\n    dock = app.app.addWidgetToDock(depthScanner.pointCloudView, QtCore.Qt.RightDockWidgetArea)\n    dock.setMinimumWidth(300)\n    dock.setMinimumHeight(300)\n\n\n    # add some test data\n    def addTestData():\n        d = DebugData()\n        # d.addSphere((0,0,0), radius=0.5)\n        d.addArrow((0,0,0), (0,0,1), color=[1,0,0])\n        d.addArrow((0,0,1), (0,.5,1), color=[0,1,0])\n        vis.showPolyData(d.getPolyData(), \'debug data\', colorByName=\'RGB255\')\n        view.resetCamera()\n\n    addTestData()\n\n    # xvfb command\n    # /usr/bin/Xvfb  :99 -ac -screen 0 1280x1024x16\n\n    if globalsDict is not None:\n        globalsDict.update(dict(app=app, view=view, depthScanner=depthScanner))\n\n    app.app.start(restoreWindow=False)\n\n\n\nif __name__ == \'__main__\':\n    main(globals())\n'"
modules/dense_correspondence_manipulation/change_detection/mesh_processing.py,0,"b""import numpy as np\nimport os\n\n\nfrom director import imageview\nfrom director import vtkAll as vtk\nfrom director import transformUtils\nfrom director import visualization as vis\nfrom director import viewbehaviors\nfrom director import vtkNumpy as vnp\nfrom director.debugVis import DebugData\nfrom director.timercallback import TimerCallback\nfrom director import ioUtils\nimport PythonQt\n\n\nfrom director import mainwindowapp\nfrom PythonQt import QtCore, QtGui\n\nimport cv2\n\nimport dense_correspondence_manipulation.change_detection.change_detection as change_detection\nfrom dense_correspondence_manipulation.utils.constants import *\nimport dense_correspondence_manipulation.utils.utils as utils\nimport dense_correspondence_manipulation.utils.director_utils as director_utils\nfrom dense_correspondence_manipulation.fusion.fusion_reconstruction import FusionReconstruction, TSDFReconstruction\nfrom dense_correspondence_manipulation.mesh_processing.mesh_render import MeshColorizer\n\n\nCONFIG = utils.getDictFromYamlFilename(CHANGE_DETECTION_CONFIG_FILE)\n\nclass ReconstructionProcessing(object):\n\n    def __init__(self):\n        pass\n\n    def spawnCropBox(self, dims=None):\n        if dims is None:\n            dim_x = CONFIG['crop_box']['dimensions']['x']\n            dim_y = CONFIG['crop_box']['dimensions']['y']\n            dim_z = CONFIG['crop_box']['dimensions']['z']\n            dims = [dim_x, dim_y, dim_z]\n\n        transform = director_utils.transformFromPose(CONFIG['crop_box']['transform'])\n        d = DebugData()\n        d.addCube(dims, [0,0,0], color=[0,1,0])\n        self.cube_vis = vis.updatePolyData(d.getPolyData(), 'Crop Cube', colorByName='RGB255')\n        vis.addChildFrame(self.cube_vis)\n        self.cube_vis.getChildFrame().copyFrame(transform)\n        self.cube_vis.setProperty('Alpha', 0.3)\n\n    def getCropBoxFrame(self):\n        transform = self.cube_vis.getChildFrame().transform\n        pos, quat = transformUtils.poseFromTransform(transform)\n        print (pos,quat)\n\n\ndef createApp(globalsDict=None):\n\n    from director import mainwindowapp\n    from PythonQt import QtCore, QtGui\n\n    app = mainwindowapp.construct()\n    app.gridObj.setProperty('Visible', True)\n    app.viewOptions.setProperty('Orientation widget', True)\n    app.viewOptions.setProperty('View angle', 30)\n    app.sceneBrowserDock.setVisible(True)\n    app.propertiesDock.setVisible(False)\n    app.mainWindow.setWindowTitle('Mesh Processing')\n    app.mainWindow.show()\n    app.mainWindow.resize(920,600)\n    app.mainWindow.move(0,0)\n\n    view = app.view\n\n\n    globalsDict['view'] = view\n    globalsDict['app'] = app\n\n\n\ndef main(globalsDict, data_folder):\n    createApp(globalsDict)\n    view = globalsDict['view']\n    app = globalsDict['app']\n\n    reconstruction = TSDFReconstruction.from_data_folder(data_folder, config=CONFIG)\n    reconstruction.visualize_reconstruction(view, vis_uncropped=False)\n    rp = ReconstructionProcessing()\n\n    mesh_colorizer = MeshColorizer(reconstruction.vis_obj)\n\n\n    globalsDict['r'] = reconstruction\n    globalsDict['reconstruction'] = reconstruction\n    globalsDict['rp'] = rp\n\n    globalsDict['mc'] = mesh_colorizer\n    globalsDict['app'] = app\n\n    # rp.spawnCropBox()\n\n    return globalsDict\n\n    \n\n\nif __name__ == '__main__':\n    main(globals())"""
modules/dense_correspondence_manipulation/change_detection/tsdf_converter.py,0,b'\n'
modules/dense_correspondence_manipulation/fusion/__init__.py,0,b''
modules/dense_correspondence_manipulation/fusion/fusion_reconstruction.py,0,"b'\nimport numpy as np\nimport os\n\n\nfrom director import imageview\nfrom director import vtkAll as vtk\nfrom director import transformUtils\nfrom director import visualization as vis\nfrom director import viewbehaviors\nfrom director import vtkNumpy as vnp\nfrom director.debugVis import DebugData\nfrom director.timercallback import TimerCallback\nfrom director import ioUtils\nfrom director import transformUtils\nfrom director import filterUtils\nimport PythonQt\n\nimport dense_correspondence_manipulation.utils.director_utils as director_utils\nimport dense_correspondence_manipulation.utils.utils as utils\nimport dense_correspondence_manipulation.utils.segmentation as segmentation\n\nfrom dense_correspondence_manipulation.utils.constants import *\n\nfrom dense_correspondence.dataset.scene_structure import SceneStructure\n\nclass FusionCameraPoses(object):\n    """"""\n    Abstract class for storing poses coming from a fusion reconstruction\n\n    """"""\n    pass\n\n\nclass ElasticFusionCameraPoses(FusionCameraPoses):\n\n    def __init__(self, posegraph_filename):\n        self.posegraph_filename = posegraph_filename\n        pass\n\n    @property\n    def first_frame_to_world(self):\n        return self._first_frame_to_world\n\n    @first_frame_to_world.setter\n    def first_frame_to_world(self, value):\n        assert isinstance(value, vtk.vtkTransform)\n        self._first_frame_to_world = value\n\n    @property\n    def posegraph_filename(self):\n        return self._posegraph_filename\n\n    @posegraph_filename.setter\n    def posegraph_filename(self, value):\n        self._posegraph_filename = value\n        self.load_camera_poses(self.posegraph_filename)\n\n    def load_camera_poses(self, posegraphFile):\n        data = np.loadtxt(posegraphFile)\n        self.poseTimes = np.array(data[:, 0] * 1e6, dtype=int)\n        self.poses = []\n        for idx, pose in enumerate(data[:, 1:]):\n            pos = pose[:3]\n            quat = pose[6], pose[3], pose[4], pose[5]  # quat data from file is ordered as x, y, z, w\n            self.poses.append((pos, quat))\n\n\n    def get_camera_pose(self, idx):\n        pos, quat = self.poses[idx]\n        transform = transformUtils.transformFromPose(pos, quat)\n        return pos, quat, transform\n\n    def get_camera_to_world_pose(self, idx):\n        _, _, camera_to_first_frame = self.get_camera_pose(idx)\n        camera_to_world = transformUtils.concatenateTransforms([camera_to_first_frame,\n                                                                self.first_frame_to_world])\n\n        return camera_to_world\n\nclass CameraPoses(object):\n    """"""\n    Simple wrapper class for getting camera poses\n\n    """"""\n    def __init__(self, pose_dict):\n        self.pose_dict = pose_dict\n\n    @property\n    def pose_dict(self):\n        return self._pose_dict\n\n    @pose_dict.setter\n    def pose_dict(self, value):\n        self._pose_dict = value\n\n    def get_camera_pose(self, idx):\n        camera_pose_dict = self.pose_dict[idx][\'camera_to_world\']\n        return director_utils.transformFromPose(camera_pose_dict)\n\n    def get_data(self, idx):\n        return self.pose_dict[idx]\n\n    def num_poses(self):\n        return len(self.pose_dict)\n\nclass FusionReconstruction(object):\n    """"""\n    A utility class for storing information about a 3D reconstruction\n\n    e.g. reconstruction produced by ElasticFusion\n    """"""\n\n    def __init__(self):\n        self.poly_data_type = ""points""\n        pass\n\n\n    def setup(self):\n        self.load_poly_data()\n        self.image_dir = os.path.join(self.data_dir, \'images\')\n\n    def save_poly_data(self, filename):\n        """"""\n        Save the poly data to a file\n        :param filename:\n        :type filename:\n        :return:\n        :rtype:\n        """"""\n        ioUtils.writePolyData(self.poly_data, filename)\n\n    @property\n    def poly_data_type(self):\n        return self._poly_data_type\n\n    @poly_data_type.setter\n    def poly_data_type(self, value):\n        self._poly_data_type = value\n\n    @property\n    def name(self):\n        return self._name\n\n    @name.setter\n    def name(self, value):\n        self._name = value\n\n    @property\n    def data_dir(self):\n        return self._data_dir\n\n    @data_dir.setter\n    def data_dir(self, value):\n        self._data_dir = value\n        self._dataset_structure = SceneStructure(self._data_dir)\n\n    @property\n    def dataset_structure(self):\n        return self._dataset_structure\n\n    @property\n    def image_dir(self):\n        return self._image_dir\n\n    @image_dir.setter\n    def image_dir(self, value):\n        self._image_dir = value\n\n    @property\n    def kinematics_pose_data(self):\n        """"""\n        Of type CameraPoses\n        :return:\n        """"""\n        return self._kinematics_pose_data\n\n    @kinematics_pose_data.setter\n    def kinematics_pose_data(self, value):\n        self._kinematics_pose_data = CameraPoses(value)\n        self._reconstruction_to_world = self.kinematics_pose_data.get_camera_pose(0)\n\n    @property\n    def fusion_pose_data(self):\n        return self._fusion_pose_data\n\n    @fusion_pose_data.setter\n    def fusion_pose_data(self, value):\n        self._fusion_pose_data = value\n\n    @property\n    def camera_info(self):\n        return self._camera_info\n\n    @camera_info.setter\n    def camera_info(self, value):\n        self._camera_info = value\n\n    @property\n    def reconstruction_filename(self):\n        return self._reconstruction_filename\n\n    @reconstruction_filename.setter\n    def reconstruction_filename(self, value):\n        self._reconstruction_filename = value\n\n    @property\n    def foreground_reconstruction_filename(self):\n        return self._foreground_reconstruction_filename\n\n    @foreground_reconstruction_filename.setter\n    def foreground_reconstruction_filename(self, value):\n        self._foreground_reconstruction_filename = value\n\n    @property\n    def fusion_posegraph_filename(self):\n        return self._fusion_posegraph_filename\n\n    @fusion_posegraph_filename.setter\n    def fusion_posegraph_filename(self, value):\n        self._fusion_posegraph_filename = value\n        self.fusion_pose_data = ElasticFusionCameraPoses(self._fusion_posegraph_filename)\n\n    @property\n    def config(self):\n        return self._config\n\n    @property\n    def vis_obj(self):\n        """"""\n        The visualization object\n        :return:\n        :rtype:\n        """"""\n        return self.reconstruction_vis_obj\n\n    @config.setter\n    def config(self, value):\n        self._config = value\n\n    def load_poly_data(self):\n        self.poly_data_raw = ioUtils.readPolyData(self.reconstruction_filename)\n        self.poly_data = filterUtils.transformPolyData(self.poly_data_raw, self._reconstruction_to_world)\n        self.crop_poly_data()\n\n    def crop_poly_data(self):\n\n        dim_x = self.config[\'crop_box\'][\'dimensions\'][\'x\']\n        dim_y = self.config[\'crop_box\'][\'dimensions\'][\'y\']\n        dim_z = self.config[\'crop_box\'][\'dimensions\'][\'z\']\n        dimensions = [dim_x, dim_y, dim_z]\n\n        transform = director_utils.transformFromPose(self.config[\'crop_box\'][\'transform\'])\n\n        # store the old poly data\n        self.poly_data_uncropped = self.poly_data\n\n        self.poly_data = director_utils.cropToBox(self.poly_data, transform, dimensions, data_type=self.poly_data_type)\n\n    def visualize_reconstruction(self, view, point_size=None, vis_uncropped=False):\n        if point_size is None:\n            point_size = self.config[\'point_size\']\n\n        self.reconstruction_vis_obj = vis.updatePolyData(self.poly_data, \'Fusion Reconstruction\',\n                                                       view=view, colorByName=\'RGB\')\n        self.reconstruction_vis_obj.setProperty(\'Point Size\', point_size)\n\n        if vis_uncropped:\n            vis_obj = vis.updatePolyData(self.poly_data_uncropped, \'Uncropped Fusion Reconstruction\',\n                               view=view, colorByName=\'RGB\')\n            vis_obj.setProperty(\'Point Size\', point_size)\n\n    def get_camera_to_world(self, idx):\n        return self.fusion_pose_data.get_camera_to_world_pose(idx)\n\n\n    @staticmethod\n    def from_data_folder(data_folder, config=None):\n        fr = FusionReconstruction()\n        fr.data_dir = data_folder\n\n        if config is None:\n            config = FusionReconstruction.load_default_config()\n\n        pose_data_filename = os.path.join(data_folder, \'images\', \'pose_data.yaml\')\n        camera_info_filename = os.path.join(data_folder, \'images\', \'camera_info.yaml\')\n\n        fr.config = config\n        fr.kinematics_pose_data = utils.getDictFromYamlFilename(pose_data_filename)\n        fr.camera_info = utils.getDictFromYamlFilename(camera_info_filename)\n        fr.fusion_posegraph_filename = os.path.join(data_folder, \'images.posegraph\')\n        fr.fusion_pose_data.first_frame_to_world = transformUtils.copyFrame(fr._reconstruction_to_world)\n\n        fr.reconstruction_filename = os.path.join(fr.data_dir, \'images.vtp\')\n        fr.setup()\n        return fr\n\n    @staticmethod\n    def load_default_config():\n        default_config_file = os.path.join(utils.getDenseCorrespondenceSourceDir(), \'config\', \'stations\', \'RLG_iiwa_1\',\n                                           \'change_detection.yaml\')\n        config = utils.getDictFromYamlFilename(default_config_file)\n        return config\n\nclass TSDFReconstruction(FusionReconstruction):\n\n    def __init__(self, load_foreground_mesh):\n        FusionReconstruction.__init__(self)\n        self.poly_data_type = ""cells""\n        self._load_foreground_mesh = load_foreground_mesh\n\n    def setup(self):\n        self.load_poly_data()\n        self.image_dir = os.path.join(self.data_dir, \'images\')\n\n    def load_poly_data(self):\n        reconstruction_filename = self.dataset_structure.fusion_reconstruction_file\n        self.poly_data_raw = ioUtils.readPolyData(reconstruction_filename)\n        self.poly_data = self.poly_data_raw\n\n\n        if self._load_foreground_mesh:\n            foreground_reconstruction_filename =\\\n                self.dataset_structure.foreground_fusion_reconstruction_file\n            if not os.path.isfile(foreground_reconstruction_filename):\n                print ""Foreground mesh file doesn\'t exist, falling back"" \\\n                "" to cropping mesh""\n                self.crop_poly_data()\n            else:\n                self.poly_data = ioUtils.readPolyData(foreground_reconstruction_filename)\n        else:\n            self.crop_poly_data()\n\n    @property\n    def fusion_pose_data(self):\n        raise ValueError(""TSDFReconstruction doesn\'t have fusion_pose_data"")\n\n    def get_image_indices(self):\n        """"""\n        Returns a list of image indices\n        :return: list(int)\n        :rtype:\n        """"""\n        return self.kinematics_pose_data.pose_dict.keys()\n\n    def get_camera_to_world(self, idx):\n        return self.kinematics_pose_data.get_camera_pose(idx)\n\n\n    def visualize_reconstruction(self, view, vis_uncropped=False, name=None, parent=None):\n\n\n        if name is None:\n            vis_name = ""Fusion Reconstruction "" + self.name\n        else:\n            vis_name = name\n\n        self.reconstruction_vis_obj = vis.updatePolyData(self.poly_data, vis_name,\n                                                       view=view, colorByName=\'RGB\', parent=parent)\n\n        if vis_uncropped:\n            vis_obj = vis.updatePolyData(self.poly_data_raw, \'Uncropped Fusion Reconstruction\',\n                               view=view, colorByName=\'RGB\', parent=parent)\n\n    @staticmethod\n    def from_data_folder(data_folder, config=None, name=None, load_foreground_mesh=True):\n        """"""\n\n        :param data_folder: The \'processed\' subfolder of a top level log folder\n        :type data_folder:\n        :param config: YAML file containing parameters. The default file is\n        change_detection.yaml. This file contains the parameters used to crop\n        the fusion reconstruction and extract the foreground.\n        :type config:YAML file\n        :param name:\n        :type name:\n        :return:\n        :rtype:\n        """"""\n        fr = TSDFReconstruction(load_foreground_mesh)\n        fr.data_dir = data_folder\n\n        if name is None:\n            name = """"\n\n        if config is None:\n            print ""no config passed in, loading default""\n            config = FusionReconstruction.load_default_config()\n\n        pose_data_filename = os.path.join(data_folder, \'images\', \'pose_data.yaml\')\n        camera_info_filename = os.path.join(data_folder, \'images\', \'camera_info.yaml\')\n\n        fr.config = config\n        fr.name = name\n        fr.kinematics_pose_data = utils.getDictFromYamlFilename(pose_data_filename)\n        fr.camera_info = utils.getDictFromYamlFilename(camera_info_filename)\n        fr.setup()\n\n        return fr'"
modules/dense_correspondence_manipulation/scripts/__init__.py,0,b''
modules/dense_correspondence_manipulation/scripts/batch_run_change_detection_pipeline.py,0,"b'#!/usr/bin/env directorPython\n\nimport os\nimport shutil\nimport time\n\nimport dense_correspondence_manipulation.scripts.run_change_detection_pipeline as run_change_detection_pipeline\nfrom dense_correspondence_manipulation.utils.constants import *\n\n\n""""""\nScript that runs the change detection pipeline on all subfolders in a directory\n""""""\n\ndef main():\n    parent_folder = os.getcwd()\n    list_of_dirs = sorted(os.listdir(parent_folder))\n    num_dirs = len(list_of_dirs)\n    # list_of_dirs = [""04_drill_long_downsampled"", ""05_drill_long_downsampled""]\n\n    for idx, dir in enumerate(list_of_dirs):\n\n        log_folder = os.path.join(parent_folder, dir)\n        data_folder = os.path.join(log_folder, \'processed\')\n\n        if not os.path.isdir(data_folder):\n            continue\n\n        image_masks_folder = os.path.join(data_folder, \'image_masks\')\n        if os.path.isdir(image_masks_folder):\n            shutil.rmtree(image_masks_folder)\n\n        print ""Processing scene %d of %d"" %(idx, num_dirs)\n        print ""Running change detection for %s"" %(dir)\n\n        cmd = ""run_change_detection.py --data_dir "" + data_folder\n        print ""cmd: "", cmd\n        os.system(cmd)\n        print ""finished running change detection""\n\n        cmd = ""render_depth_images.py --data_dir "" + data_folder\n        print ""\\nrendering depth images""\n        print ""cmd""\n        os.system(cmd)\n        print ""finished rendering depth images\\n\\n""\n\n        # this doesn\'t work, for some reason different directorApp\'s won\'t quit.\n        # run_change_detection_pipeline.run(data_folder, CHANGE_DETECTION_CONFIG_FILE)\n\n        time.sleep(2.0)\n\nif __name__== ""__main__"":\n    main()'"
modules/dense_correspondence_manipulation/scripts/compute_descriptor_images.py,0,"b'""""""\nCompute descriptor images for a single scene\n""""""\n\nimport os\nimport torch\nimport numpy as np\n\n# pdc\nimport dense_correspondence_manipulation.utils.utils as utils\nfrom dense_correspondence.evaluation.evaluation import DenseCorrespondenceEvaluation\nfrom dense_correspondence.dataset.spartan_dataset_masked import SpartanDataset\nfrom dense_correspondence.dataset.scene_structure import SceneStructure\n\n""""""\nComputes descriptor images for a given scene and network. Saves them as \nnpy files. \n\n\nUsage: Modify the global variables in CAPS as needed\n""""""\n\n\nDC_SOURCE_DIR = utils.getDenseCorrespondenceSourceDir()\nNETWORK_NAME = ""caterpillar_M_background_0.500_3""\nEVALUATION_CONFIG_FILENAME = os.path.join(DC_SOURCE_DIR, \'config\', \'dense_correspondence\',\n                                   \'evaluation\', \'lucas_evaluation.yaml\')\nDATASET_CONFIG_FILE = os.path.join(DC_SOURCE_DIR, \'config\', \'dense_correspondence\', \'dataset\', \'composite\',\n                                       \'caterpillar_only_9.yaml\')\n\nSCENE_NAME = ""2018-04-16-14-25-19""\n\nSAVE_DIR = os.path.join(""/home/manuelli/code/data_volume/pdc/logs_test"",\n                            SCENE_NAME, ""processed"",\n                            ""descriptor_images"", NETWORK_NAME)\n\n\ndef compute_descriptor_images_for_single_scene(dataset, scene_name, dcn, save_dir):\n    """"""\n    Computes the descriptor images for a single scene\n    :param dataset:\n    :type dataset:\n    :param scene_name:\n    :type scene_name:\n    :param dcn:\n    :type dcn:\n    :return:\n    :rtype:\n    """"""\n\n    pose_data = dataset.get_pose_data(scene_name)\n\n    if not os.path.isdir(save_dir):\n        os.makedirs(save_dir)\n\n    counter = 1\n    num_images = len(pose_data)\n    for img_idx, data in pose_data.iteritems():\n        rgb, depth, mask, pose = dataset.get_rgbd_mask_pose(scene_name, img_idx)\n\n        # rgb is a PIL image\n        rgb_tensor = dataset.rgb_image_to_tensor(rgb)\n        res = dcn.forward_single_image_tensor(rgb_tensor).data.cpu().numpy()\n\n        # save the file\n\n        descriptor_filename = os.path.join(save_dir, SceneStructure.descriptor_image_filename(img_idx))\n        np.save(descriptor_filename, res)\n\n        print ""descriptor_filename"", descriptor_filename\n        print ""processing image %d of %d"" %(counter, num_images)\n        counter += 1\n\n\n\nif __name__ == ""__main__"":\n    dc_source_dir = utils.getDenseCorrespondenceSourceDir()\n    config_filename = os.path.join(dc_source_dir, \'config\', \'dense_correspondence\',\n                                   \'evaluation\', \'lucas_evaluation.yaml\')\n    eval_config = utils.getDictFromYamlFilename(config_filename)\n    default_config = utils.get_defaults_config()\n    utils.set_cuda_visible_devices(default_config[\'cuda_visible_devices\'])\n\n    dce = DenseCorrespondenceEvaluation(eval_config)\n    network_name = ""caterpillar_M_background_0.500_3""\n    dcn = dce.load_network_from_config(network_name)\n\n    dataset_config_file = os.path.join(dc_source_dir, \'config\', \'dense_correspondence\', \'dataset\', \'composite\',\n                                       \'caterpillar_only_9.yaml\')\n    dataset_config = utils.getDictFromYamlFilename(dataset_config_file)\n    dataset = SpartanDataset(config=dataset_config)\n\n\n    scene_name = SCENE_NAME\n    save_dir = SAVE_DIR\n    compute_descriptor_images_for_single_scene(dataset, scene_name, dcn, save_dir)\n\n\n    print ""finished cleanly""\n\n'"
modules/dense_correspondence_manipulation/scripts/convertPlyToVtp.py,0,"b'#!/usr/bin/env directorPython\n\n\'\'\'\nUsage:\n\ndirectorPython convertPlyToVtp.py <path/to/data.ply>\n\nThis script will read the ply file and save a vtp file.\nThe output vtp file will be given the same name as the\ninput file with the ply extension replaced with vtp.\n\nNote, the ply file needs to be in ascii format and not\nbinary.  The vtk ply reader seems to crash on binary\nply files.  You can use meshlab to open a binary ply\nfile and re-save it as an ascii ply file.\n\'\'\'\n\n\nimport os\nfrom director import ioUtils\nfrom director import vtkNumpy as vnp\n\n\nif __name__ == \'__main__\':\n\n    filename = sys.argv[1]\n    outputFilename = os.path.splitext(filename)[0] + \'.vtp\'\n\n\n    print ""reading poly data""\n    polyData = ioUtils.readPolyData(filename)\n    print ""finished reading poly data""\n\n    # TODO:\n    # This should just be fixed in ioUtils.readPolyData, but for now\n    # there is a workaround for an issue with the ply reader.\n    # The output of the ply reader has points but not vertex cells,\n    # so create a new polydata with vertex cells and copy the cells over.\n    points = vnp.getNumpyFromVtk(polyData, \'Points\')\n    newPolyData = vnp.numpyToPolyData(points, createVertexCells=True)\n    polyData.SetVerts(newPolyData.GetVerts())\n\n    print \'writing:\', outputFilename\n    ioUtils.writePolyData(polyData, outputFilename)\n'"
modules/dense_correspondence_manipulation/scripts/convert_data_to_new_format.py,0,"b'#!/usr/bin/python\nimport os\nimport shutil\n\ndef run_on_single_folder(folder):\n    mesh_filename = os.path.join(folder, \'fusion_mesh.ply\')\n\n    if not os.path.isfile(mesh_filename):\n        print ""this folder is already in new format, skipping""\n        return\n\n    move_to_processed  = []\n    move_to_raw = []\n    for file in os.listdir(folder):\n\n        if file in [""processed"", ""raw""]:\n            continue\n        if file == ""fusion.bag"":\n            move_to_raw.append(file)\n        else:\n            move_to_processed.append(file)\n\n    print ""move_to_processed:"", move_to_processed\n    print ""move_to_raw:"", move_to_raw\n\n\n\n\n    processed = os.path.join(folder, ""processed"")\n    raw = os.path.join(folder, ""raw"")\n\n    if not os.path.isdir(processed):\n        os.makedirs(processed)\n        os.makedirs(raw)\n\n    for file in move_to_raw:\n        src = os.path.join(folder, file)\n        dest = os.path.join(raw, file)\n        shutil.move(src, dest)\n\n    for file in move_to_processed:\n        src = os.path.join(folder, file)\n        dest = os.path.join(processed, file)\n        shutil.move(src, dest)\n\ndef main():\n    logs_proto = ""/home/manuelli/code/data_volume/pdc/logs_proto""\n    folder = ""/home/manuelli/code/data_volume/pdc/logs_proto/00_background""\n    # run_on_single_folder(folder)\n\n    for file in os.listdir(logs_proto):\n        print ""folder name:"", file\n        folder = os.path.join(logs_proto, file)\n        run_on_single_folder(folder)\n\nif __name__ == ""__main__"":\n    main()\n'"
modules/dense_correspondence_manipulation/scripts/convert_ply_to_vtp.py,0,"b'#!/usr/bin/python\nimport os\nimport dense_correspondence_manipulation.utils.utils as utils\n\n\ndef run(data_folder, ply_binary_filename=\'images.ply\'):\n\n    # install ply if do not already have it\n    os.chdir(data_folder)\n    vtp_filename = os.path.join(data_folder, \'images.vtp\')\n    dc_source_dir = utils.getDenseCorrespondenceSourceDir()\n\n    ply_to_ascii_executable = os.path.join(dc_source_dir, \'src\', \'ply\', \'ply2ascii\')\n    path_to_ply = os.path.join(dc_source_dir, ""src"", ""ply"")\n    if not (os.path.isfile(ply_to_ascii_executable)):\n        os.system(""cd "" + path_to_ply + "" && make"")\n\n\n    correct_ply_header_file = os.path.join(dc_source_dir, \'config\', \'correct_ply_header.txt\')\n\n    ply_binary_full_filename = os.path.join(data_folder, ply_binary_filename)\n    converted_ascii_filename = os.path.join(data_folder, ""converted_to_ascii.ply"")\n    converted_ascii_modified_header_filename = os.path.join(data_folder, ""converted_to_ascii_modified_header.ply"")\n\n    # call ply2ascii\n    os.system(ply_to_ascii_executable + ""<./"" + ply_binary_filename + ""> "" + converted_ascii_filename)\n\n    # change header to be compatible with Director\n    # TODO: make so Director accepts other header?\n    line_elements_vertex = """"\n    with open(converted_ascii_modified_header_filename, \'w\') as outfile:\n        with open(converted_ascii_filename) as infile:\n            counter = 0\n            for line in infile:\n                counter += 1\n                if counter == 3:\n                    line_elements_vertex = line\n                    break\n        with open(correct_ply_header_file) as infile:\n            counter = 0\n            for line in infile:\n                counter += 1\n                if counter == 4:\n                    outfile.write(line_elements_vertex)\n                    continue\n                outfile.write(line)\n        with open(converted_ascii_filename) as infile:\n            num_skip = 14\n            counter = 0\n            for line in infile:\n                counter += 1\n                if counter <= 14:\n                    continue\n                outfile.write(line)\n\n    # convert to vtp\n    convert_ply_to_vtp_script = os.path.join(dc_source_dir, \'modules\',\n                                             \'dense_correspondence_manipulation\', \'scripts\', \'convertPlyToVtp.py\')\n\n    print ""converted to ascii ply format""\n\n    os.system(""directorPython "" + convert_ply_to_vtp_script + "" "" + converted_ascii_modified_header_filename)\n\n\n    converted_ascii_modified_header_vtp_filename = os.path.join(data_folder, ""converted_to_ascii_modified_header.vtp"")\n\n    print ""finished convert_ply_to_vtp_script""\n\n    # clean up and rename\n    # os.system(""rm *.ply *.freiburg"")\n    os.rename(converted_ascii_modified_header_vtp_filename, vtp_filename)\n\n\n\nif __name__ == ""__main__"":\n    run(os.getcwd())'"
modules/dense_correspondence_manipulation/scripts/director_dev_app.py,0,"b'#!/usr/bin/env directorPython\n\nimport numpy as np\nimport os\n\n\nfrom director import imageview\nfrom director import vtkAll as vtk\nfrom director import transformUtils\nfrom director import visualization as vis\nfrom director import viewbehaviors\nfrom director import vtkNumpy as vnp\nfrom director.debugVis import DebugData\nfrom director.timercallback import TimerCallback\nfrom director import ioUtils\nimport PythonQt\n\n\nfrom director import mainwindowapp\nfrom PythonQt import QtCore, QtGui\n\ndef createApp(globalsDict=None):\n\n    from director import mainwindowapp\n    from PythonQt import QtCore, QtGui\n\n    app = mainwindowapp.construct()\n    app.gridObj.setProperty(\'Visible\', True)\n    app.viewOptions.setProperty(\'Orientation widget\', True)\n    app.viewOptions.setProperty(\'View angle\', 30)\n    app.sceneBrowserDock.setVisible(False)\n    app.propertiesDock.setVisible(False)\n    app.mainWindow.setWindowTitle(\'Depth Scanner\')\n    app.mainWindow.show()\n    app.mainWindow.resize(920,600)\n    app.mainWindow.move(0,0)\n\n    view = app.view\n\n\n    globalsDict[\'view\'] = view\n    globalsDict[\'app\'] = app\n\n\ndef load_polydata():\n    filename = ""/home/manuelli/code/modules/dense_correspondence_manipulation/scripts/test_reconstruction.ply""\n    poly_data = ioUtils.readPolyData(filename)\n    vis.showPolyData(poly_data, \'test\')\n\ndef main(globalsDict):\n    createApp(globalsDict)\n    view = globalsDict[\'view\']\n    app = globalsDict[\'app\']\n\n\n    load_polydata()\n\n    app.app.start(restoreWindow=True)\n\n\nif __name__ == ""__main__"":\n    main(globals())'"
modules/dense_correspondence_manipulation/scripts/mesh_descriptor_color_app.py,0,"b'#!/usr/bin/env directorPython\n\n# system\nimport os\nimport argparse\nimport numpy as np\n\n# director\nimport director.vtkAll as vtk\nimport director.vtkNumpy as vnp\nimport director.objectmodel as om\nimport director.visualization as vis\n\n# pdc\nfrom dense_correspondence_manipulation.mesh_processing.mesh_render import MeshRender\nfrom dense_correspondence_manipulation.mesh_processing.mesh_render import MeshColorizer\nfrom dense_correspondence_manipulation.mesh_processing.mesh_render import DescriptorMeshColor\n\n\n\n\n""""""\nLaunches a mesh rendering director app.\nThis should be launched from the <path_to_log_folder>/processed location\n""""""\nif __name__ == ""__main__"":\n\n    print ""\\n--------------------------------\\n""\n    print ""Have you disabled anti-aliasing in nvidia-settings? If not this won\'t work correctly""\n    print ""\\n--------------------------------\\n""\n\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(""--data_dir"", type=str, help=""(optional) dataset folder to load"")\n    # parser.add_argument(\'--current_dir\', action=\'store_true\', default=False, help=""uses the current director as the data_foler"")\n\n    args = parser.parse_args()\n    if args.data_dir:\n        data_folder = args.data_dir\n    else:\n        print ""running with data_dir set to current working directory . . . ""\n        data_folder = os.getcwd()\n\n\n    obj_dict = MeshRender.from_data_folder(data_folder)\n    mesh_render = obj_dict[\'mesh_render\']\n    descriptor_mesh_color = DescriptorMeshColor(mesh_render.poly_data_item, mesh_render._data_folder)\n    descriptor_mesh_color.color_mesh_using_descriptors()\n    obj_dict[\'dmc\'] = descriptor_mesh_color\n    app = obj_dict[\'app\']\n\n\n    globalsDict = globals()\n    globalsDict.update(obj_dict)\n\n    app.app.start(restoreWindow=True)'"
modules/dense_correspondence_manipulation/scripts/mesh_processing_app.py,0,"b'#!/usr/bin/env directorPython\n\nimport os\nimport argparse\n\n# pdc\nimport dense_correspondence_manipulation.change_detection.mesh_processing as mesh_processing\nimport dense_correspondence_manipulation.pose_estimation.utils as pose_utils\nimport director.vtkAll as vtk\nimport director.vtkNumpy as vnp\nimport director.objectmodel as om\nimport director.visualization as vis\n\nfrom dense_correspondence_manipulation.mesh_processing.mesh_render import DescriptorMeshColor\nfrom dense_correspondence_manipulation.pose_estimation.descriptor_pose_estimation import DescriptorPoseEstimator\nfrom dense_correspondence.dataset.scene_structure import SceneStructure\n\n\n""""""\nLaunches a mesh processing director app.\nThis should be launched from the <path_to_log_folder>/processed location\n""""""\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(""--data_dir"", type=str, help=""(optional) dataset folder to load"")\n\n    parser.add_argument(""--colorize"", action=\'store_true\', default=False, help=""(optional) colorize mesh with descriptors"")\n\n    parser.add_argument(""--network_name"", type=str, help=""(optional) which network to use when colorizing the mesh, ensure that the descriptor dimension is 3"")\n\n    parser.add_argument(""--pose_estimation"", action=\'store_true\', default=False,\n                        help=""(optional) load the pose estimation module"")\n\n\n    args = parser.parse_args()\n    if args.data_dir:\n        data_folder = args.data_dir\n    else:\n        print ""running with data_dir set to current working directory . . . ""\n        data_folder = os.getcwd()\n\n\n    globalsDict = mesh_processing.main(globals(), data_folder)\n    app = globalsDict[\'app\']\n    reconstruction = globalsDict[\'reconstruction\']\n    poly_data = reconstruction.poly_data\n\n    scene_structure = SceneStructure(data_folder)\n\n    debug = True\n    if debug:\n        globalsDict[\'p\'] = reconstruction.poly_data\n        globalsDict[\'t\'] = poly_data.GetCell(0)\n\n        poly_data_copy = vtk.vtkPolyData()\n        poly_data_copy.CopyStructure(poly_data)\n\n\n    if args.pose_estimation:\n        poly_data_copy = vtk.vtkPolyData()\n        poly_data_copy.CopyStructure(poly_data)\n\n\n        if not args.network_name:\n            raise ValueError(""you must specify the `network_name` arg if you use""\n                             ""the `pose_estimation` flag"")\n        network_name = args.network_name\n\n\n        dpe = DescriptorPoseEstimator(scene_structure.mesh_descriptor_statistics_filename(args.network_name))\n        dpe.poly_data = poly_data_copy\n        dpe.view = globalsDict[\'view\']\n        globalsDict[\'dpe\'] = dpe\n        dpe.initialize_debug()\n\n\n    if args.colorize:\n        if not args.network_name:\n            raise ValueError(""you must specify the `network_name` arg if you use""\n                             ""the `colorize` flag"")\n        network_name = args.network_name\n\n        dmc = DescriptorMeshColor(reconstruction.vis_obj)\n        globalsDict[\'dmc\'] = dmc\n        descriptor_stats_file = scene_structure.mesh_descriptor_statistics_filename(network_name)\n        dmc.color_mesh_using_descriptors(descriptor_stats_file)\n\n    app.app.start(restoreWindow=True)'"
modules/dense_correspondence_manipulation/scripts/render_depth_images.py,0,"b'#!/usr/bin/env directorPython\n\nimport os\nimport argparse\n\nimport dense_correspondence_manipulation.change_detection.change_detection as change_detection\nimport dense_correspondence_manipulation.utils.utils as utils\nfrom dense_correspondence_manipulation.utils.constants import *\nfrom director.timercallback import TimerCallback\n\n\n""""""\nRenders depth images against the entire scene\n""""""\n\nCONFIG_FILE = CHANGE_DETECTION_CONFIG_FILE\n\n\ndef run(data_folder, config_file=CONFIG_FILE, debug=False, globalsDict=None):\n    """"""\n    Runs the change detection pipeline\n    :param data_dir:\n    :param config_file:\n    :return:\n    """"""\n\n    if globalsDict is None:\n        globalsDict = globals()\n\n\n\n\n    config_file = CONFIG_FILE\n    config = utils.getDictFromYamlFilename(config_file)\n\n    # make dimensions large so no cropping\n    for key in config[\'crop_box\'][\'dimensions\']:\n        config[\'crop_box\'][\'dimensions\'][key] = 10.0# set it to 10 meteres\n\n\n    changeDetection, obj_dict = change_detection.ChangeDetection.from_data_folder(data_folder, config=config, globalsDict=globalsDict,\n                                                                                  background_data_folder=data_folder)\n\n    # set foreground mesh to actually be background mesh\n    changeDetection.foreground_reconstruction = changeDetection.background_reconstruction\n\n    app = obj_dict[\'app\']\n    globalsDict[\'cd\'] = changeDetection\n    view = obj_dict[\'view\']\n\n    # if debug:\n    #     changeDetection.background_reconstruction.visualize_reconstruction(view, name=\'background\')\n\n    def single_shot_function():\n        changeDetection.render_depth_images()\n        app.app.quit()\n\n    if not debug:\n        TimerCallback(callback=single_shot_function).singleShot(0)\n\n    app.app.start(restoreWindow=True)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--data_dir"", type=str, default=\'/home/manuelli/code/data_volume/sandbox/drill_scenes/04_drill_long_downsampled\')\n\n    default_config_file = os.path.join(utils.getDenseCorrespondenceSourceDir(), \'config\', \'stations\', \'RLG_iiwa_1\', \'change_detection.yaml\')\n    parser.add_argument(""--config_file"", type=str, default=default_config_file)\n\n    parser.add_argument(\'--current_dir\', action=\'store_true\', default=False, help=""run the script with --data_dir set to the current directory"")\n\n    parser.add_argument(\'--debug\', action=\'store_true\', default=False,\n                        help=""launch the app in debug mode"")\n\n\n    globalsDict = globals()\n    args = parser.parse_args()\n    data_folder = args.data_dir\n\n    if args.current_dir:\n        print ""running with data_dir set to current working directory . . . ""\n        data_folder = os.getcwd()\n\n    run(data_folder, config_file=args.config_file, debug=args.debug, globalsDict=globalsDict)\n'"
modules/dense_correspondence_manipulation/scripts/run_change_detection.py,0,"b'#!/usr/bin/env directorPython\n\nimport os\nimport argparse\n\nimport dense_correspondence_manipulation.change_detection.change_detection as change_detection\nimport dense_correspondence_manipulation.utils.utils as utils\nfrom dense_correspondence_manipulation.utils.constants import *\nfrom director.timercallback import TimerCallback\n\n""""""\nRuns change detection to compute masks for each image\n""""""\n\nCONFIG_FILE = CHANGE_DETECTION_CONFIG_FILE\n# CONFIG_FILE = CHANGE_DETECTION_BACKGROUND_SUBTRACTION_CONFIG_FILE\n\ndef run(data_folder, config_file=CONFIG_FILE, debug=False, globalsDict=None,\n        background_scene_data_folder=None):\n    """"""\n    Runs the change detection pipeline\n    :param data_dir: The \'processed\' subfolder of a top-level log folder\n    :param config_file:\n    :return:\n    """"""\n\n    if globalsDict is None:\n        globalsDict = globals()\n\n    if background_scene_data_folder is None:\n        background_scene_data_folder = data_folder\n\n\n\n\n    config_file = CONFIG_FILE\n    config = utils.getDictFromYamlFilename(config_file)\n\n\n    changeDetection, obj_dict = change_detection.ChangeDetection.from_data_folder(data_folder, config=config, globalsDict=globalsDict,\n                                                                                  background_data_folder=background_scene_data_folder)\n\n    app = obj_dict[\'app\']\n    globalsDict[\'cd\'] = changeDetection\n    view = obj_dict[\'view\']\n\n    # if debug:\n    #     changeDetection.background_reconstruction.visualize_reconstruction(view, name=\'background\')\n\n    def single_shot_function():\n        changeDetection.run()\n        app.app.quit()\n\n    if not debug:\n        TimerCallback(callback=single_shot_function).singleShot(0)\n\n    app.app.start(restoreWindow=True)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--data_dir"", type=str, required=False, help=""full path to the processed/ folder of a top level log folder"")\n\n    default_config_file = os.path.join(utils.getDenseCorrespondenceSourceDir(), \'config\', \'stations\', \'RLG_iiwa_1\', \'change_detection.yaml\')\n    parser.add_argument(""--config_file"", type=str, default=default_config_file)\n\n    parser.add_argument(\'--current_dir\', action=\'store_true\', default=False, help=""run the script with --data_dir set to the current directory. You should be in the processed/ subfolder"")\n\n    parser.add_argument(\'--debug\', action=\'store_true\', default=False,\n                        help=""launch the app in debug mode"")\n\n\n    globalsDict = globals()\n    args = parser.parse_args()\n\n    if (not args.current_dir) and (not args.data_dir):\n        raise ValueError(""You must specify either current_dir or data_dir"")\n    data_folder = args.data_dir\n\n    if args.current_dir:\n        print ""running with data_dir set to current working directory . . . ""\n        data_folder = os.getcwd()\n    elif args.data_dir:\n        data_folder = args.data_dir\n\n    run(data_folder, config_file=args.config_file, debug=args.debug, globalsDict=globalsDict)\n'"
modules/dense_correspondence_manipulation/scripts/run_change_detection_pipeline.py,0,"b'#!/usr/bin/env directorPython\n\nimport os\nimport argparse\n\nimport dense_correspondence_manipulation.change_detection.change_detection as change_detection\nimport dense_correspondence_manipulation.utils.utils as utils\nimport dense_correspondence_manipulation.scripts.convert_ply_to_vtp as convert_ply_to_vtp\nimport dense_correspondence_manipulation.scripts.run_change_detection as run_change_detection\nimport dense_correspondence_manipulation.scripts.render_depth_images as render_depth_images\n\n""""""\nRuns change detection to compute masks for each image\n\n\nRun this from for example pdc/logs_proto\n\n\ncd pdc/logs_proto\nuse_pytorch\nuse_director\nrun...\n\n""""""\n\n\ndef run(data_folder, config_file):\n\n\n    # if not os.path.isfile(os.path.join(data_folder, \'images.vtp\')):\n    #     print ""converting ply to vtp . . . . ""\n    #     convert_ply_to_vtp.run(data_folder)\n    #     print ""finished converting ply to vtp\\n\\n""\n\n    cmd = ""run_change_detection.py --data_dir "" + data_folder\n    print ""cmd: "", cmd\n    os.system(cmd)\n    print ""finished running change detection""\n\n    cmd = ""render_depth_images.py --data_dir "" + data_folder\n    print ""\\nrendering depth images""\n    print ""cmd""\n    os.system(cmd)\n    print ""finished rendering depth images""\n\n\ndef already_ran_change_detection(processed_dir):\n    file_to_check_1 = os.path.join(processed_dir, ""rendered_images"", ""000000_depth.png"")\n    file_to_check_2 = os.path.join(processed_dir, ""image_masks"", ""000000_mask.png"")\n    return (os.path.isfile(file_to_check_1) and os.path.isfile(file_to_check_2))\n\ndef run_on_all_subfolders(directory, config_file):\n    print ""running on all subfolders""\n\n    for dir in sorted(os.listdir(directory)):\n        full_dir = os.path.join(directory, dir)\n\n        if not os.path.isdir(full_dir):\n            continue\n\n        processed_dir = os.path.join(full_dir, \'processed\')\n\n        if not os.path.isdir(processed_dir):\n            raise ValueError(""Need to extract and run fusion on "" + full_dir)\n\n        if already_ran_change_detection(processed_dir):\n            print ""already_ran_change_detection for"", processed_dir\n            continue\n\n        # print ""full_dir"", full_dir\n        #print ""would have run on "", full_dir\n        run(processed_dir, config_file)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--data_dir"", type=str, default=\'/home/manuelli/code/data_volume/sandbox/drill_scenes/01_drill\')\n\n    default_config_file = os.path.join(utils.getDenseCorrespondenceSourceDir(), \'config\', \'stations\', \'RLG_iiwa_1\', \'change_detection.yaml\')\n    parser.add_argument(""--config_file"", type=str, default=default_config_file)\n\n    parser.add_argument(\'--current_dir\', action=\'store_true\', default=True, help=""run the script with --data_dir set to the current directory"")\n\n    args = parser.parse_args()\n    data_folder = args.data_dir\n\n    if args.current_dir:\n        print ""running with data_dir set to current working directory . . . ""\n        data_folder = os.getcwd()\n\n    # run(data_folder, config_file=args.config_file)\n    run_on_all_subfolders(data_folder, args.config_file)'"
modules/dense_correspondence_manipulation/scripts/tsdf_to_mesh.py,0,"b'#!/usr/bin/python\nimport dense_correspondence_manipulation.change_detection.tsdf_converter as tsdf_converter\n\nif __name__ == ""__main__"":\n    tsdf_converter.main()\n\n\n'"
modules/dense_correspondence_manipulation/simple_pixel_correspondence_labeler/__init__.py,0,b''
modules/dense_correspondence_manipulation/simple_pixel_correspondence_labeler/annotate_correspondences.py,0,"b'import sys\nimport os\nimport cv2\nimport numpy as np\nimport copy\n\nimport dense_correspondence_manipulation.utils.utils as utils\ndc_source_dir = utils.getDenseCorrespondenceSourceDir()\nsys.path.append(dc_source_dir)\nsys.path.append(os.path.join(dc_source_dir, ""dense_correspondence"", ""correspondence_tools""))\nfrom dense_correspondence.dataset.spartan_dataset_masked import SpartanDataset, ImageType\n\nconfig_filename = os.path.join(utils.getDenseCorrespondenceSourceDir(), \'config\', \'dense_correspondence\', \n                               \'dataset\', \'composite\', \'star_bot_front_only.yaml\')\nconfig = utils.getDictFromYamlFilename(config_filename)\nsd = SpartanDataset(config=config)\nsd.set_train_mode()\n\n\nUSE_FIRST_IMAGE = False # force using first image in each log\nRANDOMIZE_TEST_TRAIN = False # randomize seletcting\n\ndef numpy_to_cv2(numpy_img):\n    return numpy_img[:, :, ::-1].copy() # open and convert between BGR and RGB\n\ndef pil_image_to_cv2(pil_image):\n    return np.array(pil_image)[:, :, ::-1].copy() # open and convert between BGR and RGB\ndef get_cv2_img_pair_from_spartan():\n    scene_name_a = sd.get_random_scene_name()\n    num_attempts = 50\n    for i in range(num_attempts):\n        if (i % 2) == 0 and RANDOMIZE_TEST_TRAIN:\n            sd.set_train_mode()\n        else:\n            sd.set_test_mode()\n\n        scene_name_b = sd.get_random_scene_name()\n        if scene_name_b != scene_name_a:\n            break    \n        if i == (num_attempts - 1):\n            print ""Failed at randomly getting two different scenes""\n            exit()\n    \n    if USE_FIRST_IMAGE:\n        image_a_idx = 0\n        image_b_idx = 0\n    else:\n        image_a_idx = sd.get_random_image_index(scene_name_a)\n        image_b_idx = sd.get_random_image_index(scene_name_b)\n\n    img_a = sd.get_rgb_image_from_scene_name_and_idx(scene_name_a, image_a_idx)\n    img_b = sd.get_rgb_image_from_scene_name_and_idx(scene_name_b, image_b_idx)\n    img_a, img_b = pil_image_to_cv2(img_a), pil_image_to_cv2(img_b)\n    img_a, img_b = scale_image(img_a, drawing_scale_config), scale_image(img_b, drawing_scale_config)  \n    return [img_a, scene_name_a, image_a_idx], [img_b, scene_name_b, image_b_idx]\n\n####\n\nwhite = (255,255,255)\nblack = (0,0,0)\n\nlabel_colors = [(255,0,0), (0,255,0), (0,0,255), (255,0,255), (0,125,125), (125,125,0), (200,255,50), (255, 125, 220), (10, 125, 255)]\n\ndrawing_scale_config = 2.0\n\n###\n\nannotated_data = []\n\n###\n\ndef draw_reticle(img, x, y, label_color):\n    cv2.circle(img,(x,y),10,label_color,1)\n    cv2.circle(img,(x,y),11,white,1)\n    cv2.circle(img,(x,y),12,label_color,1)\n    cv2.line(img,(x,y+1),(x,y+3),white,1)\n    cv2.line(img,(x+1,y),(x+3,y),white,1)\n    cv2.line(img,(x,y-1),(x,y-3),white,1)\n    cv2.line(img,(x-1,y),(x-3,y),white,1)\n\n# mouse callback function\ndef draw_circle1(event,x,y,flags,param):\n    if event == cv2.EVENT_LBUTTONDOWN:\n        global img1_points_picked\n        this_pair_label_index = len(img1_points_picked)\n        label_color = label_colors[this_pair_label_index%len(label_colors)]\n        img1_points_picked.append((x/drawing_scale_config,y/drawing_scale_config))\n        draw_reticle(img1, x, y, label_color)\n        \ndef draw_circle2(event,x,y,flags,param):\n    if event == cv2.EVENT_LBUTTONDOWN:\n        global img2_points_picked\n        this_pair_label_index = len(img2_points_picked)\n        label_color = label_colors[this_pair_label_index%len(label_colors)]\n        img2_points_picked.append((x/drawing_scale_config,y/drawing_scale_config))\n        draw_reticle(img2, x, y, label_color)\n        \ndef check_same_length(img1_points_picked, img2_points_picked):\n    return (len(img1_points_picked) == len(img2_points_picked))\n\ndef scale_image(img, scale):\n    return cv2.resize(img, (0,0), fx=scale, fy=scale) \n\ndef next_image_pair():\n    global img1_points_picked, img2_points_picked, img1, scene_name_1, image_1_idx, img2, scene_name_2, image_2_idx\n    img1_points_picked = []\n    img2_points_picked = []\n    [img1, scene_name_1, image_1_idx], [img2, scene_name_2, image_2_idx] = get_cv2_img_pair_from_spartan()\n\ndef to_savable_list(points_picked):\n    savable_list = []\n    for u_v_tuple in points_picked:\n        u_v_dict = dict()\n        u_v_dict[""u""] = u_v_tuple[0]\n        u_v_dict[""v""] = u_v_tuple[1]\n        savable_list.append(u_v_dict)\n    return savable_list\n\ndef make_savable_correspondence_pairs():\n    new_dict = dict()\n    new_dict[""image_a""] = dict()\n    new_dict[""image_b""] = dict()\n\n    new_dict[""image_a""][""scene_name""] = scene_name_1\n    new_dict[""image_b""][""scene_name""] = scene_name_2\n\n    new_dict[""image_a""][""image_idx""] = image_1_idx\n    new_dict[""image_b""][""image_idx""] = image_2_idx\n\n    new_dict[""image_a""][""pixels""] = to_savable_list(img1_points_picked)\n    new_dict[""image_b""][""pixels""] = to_savable_list(img2_points_picked)\n\n    return copy.copy(new_dict)\n\nif __name__ == ""__main__"":\n    next_image_pair()\n\n    cv2.namedWindow(\'image1\')\n    cv2.setMouseCallback(\'image1\',draw_circle1)\n\n    cv2.namedWindow(\'image2\')\n    cv2.setMouseCallback(\'image2\',draw_circle2)\n\n    while(1):\n        cv2.imshow(\'image1\',img1)\n        cv2.imshow(\'image2\',img2)\n        k = cv2.waitKey(20) & 0xFF\n        if k == 27:\n            break\n        elif k == ord(\'a\'):\n            print ix,iy\n        elif k == ord(\'s\'):\n            if not check_same_length(img1_points_picked, img2_points_picked):\n                print ""can\'t save when not same length""\n                print ""try choosing a new image pair""\n                print ""or picking more points on the one with less points""\n            else:\n                print ""saving""\n                new_dict = make_savable_correspondence_pairs()\n                annotated_data.append(new_dict)\n                utils.saveToYaml(annotated_data, ""new_annotated_pairs.yaml"")\n        elif k == ord(\'n\'):\n            next_image_pair()\n            \n    cv2.destroyAllWindows()\n'"
modules/dense_correspondence_manipulation/simple_pixel_correspondence_labeler/visualize_saved_correspondences.py,0,"b'import sys\nimport os\nimport cv2\nimport numpy as np\nimport copy\n\nimport dense_correspondence_manipulation.utils.utils as utils\ndc_source_dir = utils.getDenseCorrespondenceSourceDir()\nsys.path.append(dc_source_dir)\nsys.path.append(os.path.join(dc_source_dir, ""dense_correspondence"", ""correspondence_tools""))\nfrom dense_correspondence.dataset.spartan_dataset_masked import SpartanDataset, ImageType\n\nfrom dense_correspondence_manipulation.simple_pixel_correspondence_labeler.annotate_correspondences import label_colors, draw_reticle, pil_image_to_cv2, drawing_scale_config\n\nconfig_filename = os.path.join(utils.getDenseCorrespondenceSourceDir(), \'config\', \'dense_correspondence\', \n                               \'dataset\', \'composite\', \'caterpillar_baymax_starbot_onlymulti_front.yaml\')\nconfig = utils.getDictFromYamlFilename(config_filename)\nsd = SpartanDataset(config=config)\nsd.set_train_mode()\n\nannotated_data_yaml_filename = os.path.join(os.getcwd(), ""new_annotated_pairs.yaml"")\nannotated_data = utils.getDictFromYamlFilename(annotated_data_yaml_filename)\n\nindex_of_pair_to_display = 0\n\ndef draw_points(img, img_points_picked):\n    for index, img_point in enumerate(img_points_picked):\n        color = label_colors[index%len(label_colors)]\n        draw_reticle(img, int(img_point[""u""]), int(img_point[""v""]), color)\n\ndef next_image_pair_from_saved():\n    global img1, img2, index_of_pair_to_display\n    print annotated_data[index_of_pair_to_display]\n    annotated_pair = annotated_data[index_of_pair_to_display]\n    \n    scene_name_1 = annotated_pair[""image_a""][""scene_name""]\n    scene_name_2 = annotated_pair[""image_b""][""scene_name""] \n\n    image_1_idx = annotated_pair[""image_a""][""image_idx""]\n    image_2_idx = annotated_pair[""image_b""][""image_idx""]\n\n    img1_points_picked = annotated_pair[""image_a""][""pixels""]\n    img2_points_picked = annotated_pair[""image_b""][""pixels""]\n\n    print img1_points_picked\n    print img2_points_picked\n\n    img1 = pil_image_to_cv2(sd.get_rgb_image_from_scene_name_and_idx(scene_name_1, image_1_idx))\n    img2 = pil_image_to_cv2(sd.get_rgb_image_from_scene_name_and_idx(scene_name_2, image_2_idx))\n\n    draw_points(img1, img1_points_picked)\n    draw_points(img2, img2_points_picked)\n\n    index_of_pair_to_display += 1\n\n\nnext_image_pair_from_saved()\n\ncv2.namedWindow(\'image1\')\ncv2.namedWindow(\'image2\')\n\nwhile(1):\n    cv2.imshow(\'image1\',img1)\n    cv2.imshow(\'image2\',img2)\n    k = cv2.waitKey(20) & 0xFF\n    if k == 27:\n        break\n    elif k == ord(\'n\'):\n        print ""HEY""\n        next_image_pair_from_saved()\n        \ncv2.destroyAllWindows()'"
modules/dense_correspondence_manipulation/utils/__init__.py,0,b''
modules/dense_correspondence_manipulation/utils/constants.py,0,"b""import os\nimport dense_correspondence_manipulation.utils.utils as utils\n\nCHANGE_DETECTION_CONFIG_FILE = os.path.join(utils.getDenseCorrespondenceSourceDir(), 'config', 'stations', 'RLG_iiwa_1', 'change_detection.yaml')\n\nCHANGE_DETECTION_BACKGROUND_SUBTRACTION_CONFIG_FILE = os.path.join(utils.getDenseCorrespondenceSourceDir(), 'config', 'stations', 'RLG_iiwa_1', 'change_detection_background_subtraction.yaml')\n\nBACKGROUND_SCENE_DATA_FOLDER = os.path.join(utils.getDenseCorrespondenceSourceDir(), 'data_volume', 'pdc','logs_proto', '14_background')\n\nDEPTH_IM_SCALE = 1000.0   # This represents that depth images are saved as uint16, where the integer value\n                          # is depth in millimeters.  So this scale just converts millimeters to meters.\n\nDEPTH_IM_RESCALE = 4000.0 # Only for visualization purposes\n\nIMAGE_NET_MEAN = [0.485, 0.456, 0.406]\nIMAGE_NET_STD_DEV = [0.229, 0.224, 0.225]\n\nDEFAULT_IMAGE_MEAN = [0.5573105812072754, 0.37420374155044556, 0.37020164728164673]\nDEFAULT_IMAGE_STD_DEV = [0.24336038529872894, 0.2987397611141205, 0.31875079870224]\n\nLABEL_COLORS = [(255,0,0), (0,255,0), (0,0,255), (255,0,255), (0,125,125), (125,125,0), (200,255,50), (255, 125, 220), (10, 125, 255)]"""
modules/dense_correspondence_manipulation/utils/director_utils.py,0,"b'import os\nimport numpy as np\nimport scipy.misc\nimport matplotlib.cm as cm\nimport yaml\n\nfrom director import vtkNumpy as vnp\nfrom director import ioUtils\nfrom director import vtkAll as vtk\nfrom director import actionhandlers\nfrom director import screengrabberpanel as sgp\nfrom director import transformUtils\nfrom director import visualization as vis\nfrom director import objectmodel as om\nfrom director.segmentationroutines import *\nimport director.filterUtils as filterUtils\n\n\nimport dense_correspondence_manipulation.utils.utils as utils\n\n\ndef transformFromPose(d):\n    """"""\n    Returns a transform from a standard encoding in dict format\n    :param d:\n    :return:\n    """"""\n    pos = [0]*3\n    pos[0] = d[\'translation\'][\'x\']\n    pos[1] = d[\'translation\'][\'y\']\n    pos[2] = d[\'translation\'][\'z\']\n\n    quatDict = utils.getQuaternionFromDict(d)\n    quat = [0]*4\n    quat[0] = quatDict[\'w\']\n    quat[1] = quatDict[\'x\']\n    quat[2] = quatDict[\'y\']\n    quat[3] = quatDict[\'z\']\n\n    return transformUtils.transformFromPose(pos, quat)\n    \ndef getCameraTransform(camera):\n    """"""\n    Camera transform has X - right, Y - down, Z-forward\n    Note that in VTK GetViewUp and ForwardDir are NOT necessarily orthogonal\n    """"""\n    focal_point = np.array(camera.GetFocalPoint())\n    position = np.array(camera.GetPosition())\n    view_up = np.array(camera.GetViewUp())\n\n\n    forward_dir = focal_point - position\n    if np.linalg.norm(forward_dir) < 1e-8:\n        print ""forward_dir norm was very small, setting to [1,0,0]""\n        forward_dir = [1.0, 0.0, 0.0]\n\n    up_dir = np.array(view_up)\n\n    yaxis = -up_dir\n    zaxis = forward_dir\n    xaxis = np.cross(yaxis, zaxis)\n    yaxis = np.cross(zaxis, xaxis)\n\n\n    # normalize the axes\n    xaxis /= np.linalg.norm(xaxis)\n    yaxis /= np.linalg.norm(yaxis)\n    zaxis /= np.linalg.norm(zaxis)\n\n\n    return transformUtils.getTransformFromAxesAndOrigin(xaxis, yaxis, zaxis, position)\n\ndef setCameraTransform(camera, transform):\n    """"""\n    Camera transform is of the Right-Down-Forward (XYZ) convention.\n    See http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT9/node2.html\n\n    Note you may need to re-render afterwards by calling view.forceRender()\n\n    :param camera: vtkCamera object\n    :type camera:\n    :param transform: vtkTransform\n    :type transform:\n    :return:\n    :rtype:\n    """"""\n\n    origin = np.array(transform.GetPosition())\n    axes = transformUtils.getAxesFromTransform(transform)\n    camera.SetPosition(origin)\n    camera.SetFocalPoint(origin+axes[2])\n    camera.SetViewUp(-axes[1])\n    camera.Modified()\n\ndef focalLengthToViewAngle(focalLength, imageHeight):\n    \'\'\'Returns a view angle in degrees that can be set on a vtkCamera\'\'\'\n    return np.degrees(2.0 * np.arctan2(imageHeight/2.0, focalLength))\n\n\ndef viewAngleToFocalLength(viewAngle, imageHeight):\n    \'\'\'Returns the focal length given a view angle in degrees from a vtkCamera\'\'\'\n    return (imageHeight/2.0)/np.tan(np.radians(viewAngle/2.0))\n\n\ndef setCameraIntrinsics(view, cameraIntrinsics, lockViewSize=True):\n    \'\'\'\n    Note, call this function after setting the view dimensions\n    Parameters:\n        cameraIntrinsics: CameraIntrinsics objects\n\n    \'\'\'\n    cx = cameraIntrinsics.cx\n    cy = cameraIntrinsics.cy\n    fx = cameraIntrinsics.fx\n    fy = cameraIntrinsics.fy\n    imageWidth = cameraIntrinsics.width\n    imageHeight = cameraIntrinsics.height\n\n    if lockViewSize:\n        view.setFixedHeight(imageHeight)\n        view.setFixedWidth(imageWidth)\n\n    # make sure view.height and cameraIntrinsics.height match\n    if (view.height != imageHeight) or (view.width != imageWidth):\n        raise ValueError(\'CameraIntrinsics image dimensions (%d, %d) must match view image dimensions (%d,%d)\' %(imageWidth, imageHeight, view.width, view.height))\n\n    wcx = -2*(cx - float(imageWidth)/2) / imageWidth\n    wcy =  2*(cy - float(imageHeight)/2) / imageHeight\n    viewAngle = focalLengthToViewAngle(fy, imageHeight)\n\n    camera = view.camera()\n    camera.SetWindowCenter(wcx, wcy)\n    camera.SetViewAngle(viewAngle)\n\n    # set the focal length fx\n    # followed the last comment here https://stackoverflow.com/questions/18019732/opengl-vtk-setting-camera-intrinsic-parameters\n    m = np.eye(4)\n    m[0, 0] = 1.0 * fx / fy\n    t = vtk.vtkTransform()\n    t.SetMatrix(m.flatten())\n    camera.SetUserTransform(t)\n\n\ndef setCameraInstrinsicsAsus(view):\n    principalX = 320.0\n    principalY = 240.0\n    focalLength = 528.0\n    setCameraIntrinsics(view, principalX, principalY, focalLength)\n\n\ndef cropToLineSegment(polyData, point1, point2, data_type=\'cells\'):\n\n    line = np.array(point2) - np.array(point1)\n    length = np.linalg.norm(line)\n    axis = line / length\n\n    polyData = labelPointDistanceAlongAxis(polyData, axis, origin=point1, resultArrayName=\'dist_along_line\')\n\n    if data_type == ""cells"":\n        return filterUtils.thresholdCells(polyData, \'dist_along_line\', [0.0, length], arrayType=""points"")\n\n    elif data_type == ""points"":\n        return filterUtils.thresholdPoints(polyData, \'dist_along_line\', [0.0, length])\n    else:\n        raise ValueError(""unknown data_type = %s"" %(data_type))\n\ndef cropToBox(polyData, transform, dimensions, data_type=\'cells\'):\n    \'\'\'\n    dimensions is length 3 describing box dimensions\n    \'\'\'\n    origin = np.array(transform.GetPosition())\n    axes = transformUtils.getAxesFromTransform(transform)\n\n    for axis, length in zip(axes, dimensions):\n        cropAxis = np.array(axis)*(length/2.0)\n        polyData = cropToLineSegment(polyData, origin - cropAxis, origin + cropAxis)\n\n    return polyData\n\n#######################################################################################\n\n\n'"
modules/dense_correspondence_manipulation/utils/image_utils.py,0,"b'""""""\nImage processing utilities\n""""""\n\nimport numpy as np\nimport cv2\n\ndef pil_image_to_cv2(pil_image):\n    """"""\n    This correctly converts between BGR and RGB\n\n    :param pil_image: rgb image in PIL image format\n    :type pil_image: PIL.image\n    :return: cv2 image in bgr format\n    :rtype:\n    """"""\n\n    rgb_image = np.array(pil_image).copy()\n    bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n    return bgr_image\n'"
modules/dense_correspondence_manipulation/utils/segmentation.py,0,"b'import os\nimport sys\nimport math\nimport vtk\nimport time\nimport functools\nimport traceback\nimport PythonQt\nfrom PythonQt import QtCore, QtGui\nimport director.applogic as app\nfrom director import objectmodel as om\nfrom director import transformUtils\nfrom director import visualization as vis\nfrom director.transformUtils import getTransformFromAxes\nfrom director.timercallback import TimerCallback\nfrom director import affordancemanager\nfrom director.affordanceitems import *\nfrom director.visualization import *\nfrom director.filterUtils import *\nfrom director.fieldcontainer import FieldContainer\nfrom director.segmentationroutines import *\nfrom director import cameraview\n\n# from thirdparty import qhull_2d\n# from thirdparty import min_bounding_rect\n\nimport numpy as np\nfrom director import vtkNumpy\nfrom director.debugVis import DebugData\nfrom director.shallowCopy import shallowCopy\nfrom director import ioUtils\nfrom director.uuidutil import newUUID\n\nDRILL_TRIANGLE_BOTTOM_LEFT = \'bottom left\'\nDRILL_TRIANGLE_BOTTOM_RIGHT = \'bottom right\'\nDRILL_TRIANGLE_TOP_LEFT = \'top left\'\nDRILL_TRIANGLE_TOP_RIGHT = \'top right\'\n\n# # prefer drc plane segmentation instead of PCL\n# try:\n#     planeSegmentationFilter = vtk.vtkPlaneSegmentation\n# except AttributeError:\n#     planeSegmentationFilter = vtk.vtkPCLSACSegmentationPlane\n\n_defaultSegmentationView = None\n\n\ndef getSegmentationView():\n    return _defaultSegmentationView or app.getViewManager().findView(\'Segmentation View\')\n\n\ndef getDRCView():\n    return app.getDRCView()\n\n\ndef switchToView(viewName):\n    app.getViewManager().switchToView(viewName)\n\n\ndef getCurrentView():\n    return app.getCurrentRenderView()\n\n\ndef initAffordanceManager(view):\n    \'\'\'\n    Normally the affordance manager is initialized by the application.\n    This function can be called from scripts and tests to initialize the manager.\n    \'\'\'\n    global affordanceManager\n    affordanceManager = affordancemanager.AffordanceObjectModelManager(view)\n\n\ndef cropToLineSegment(polyData, point1, point2):\n    line = np.array(point2) - np.array(point1)\n    length = np.linalg.norm(line)\n    axis = line / length\n\n    polyData = labelPointDistanceAlongAxis(polyData, axis, origin=point1, resultArrayName=\'dist_along_line\')\n    return thresholdPoints(polyData, \'dist_along_line\', [0.0, length])\n\n\n\'\'\'\nicp programmable filter\n\nimport vtkFiltersGeneralPython as filtersGeneral\n\npoints = inputs[0]\nblock = inputs[1]\n\nprint points.GetNumberOfPoints()\nprint block.GetNumberOfPoints()\n\nif points.GetNumberOfPoints() < block.GetNumberOfPoints():\n    block, points = points, block\n\nicp = vtk.vtkIterativeClosestPointTransform()\nicp.SetSource(points.VTKObject)\nicp.SetTarget(block.VTKObject)\nicp.GetLandmarkTransform().SetModeToRigidBody()\nicp.Update()\n\nt = filtersGeneral.vtkTransformPolyDataFilter()\nt.SetInput(points.VTKObject)\nt.SetTransform(icp)\nt.Update()\n\noutput.ShallowCopy(t.GetOutput())\n\'\'\'\n\n\ndef computeAToB(a, b):\n    t = vtk.vtkTransform()\n    t.PostMultiply()\n    t.Concatenate(b)\n    t.Concatenate(a.GetLinearInverse())\n    tt = vtk.vtkTransform()\n    tt.SetMatrix(t.GetMatrix())\n    return tt\n\n\ndef lockAffordanceToHand(aff, hand=\'l_hand\'):\n    linkFrame = getLinkFrame(hand)\n    affT = aff.actor.GetUserTransform()\n\n    if not hasattr(aff, \'handToAffT\') or not aff.handToAffT:\n        aff.handToAffT = computeAToB(linkFrame, affT)\n\n    t = vtk.vtkTransform()\n    t.PostMultiply()\n    t.Concatenate(aff.handToAffT)\n    t.Concatenate(linkFrame)\n    aff.actor.GetUserTransform().SetMatrix(t.GetMatrix())\n\n\nhandAffUpdater = None\n\n\ndef lockToHandOn():\n    aff = getDefaultAffordanceObject()\n    if not aff:\n        return\n\n    global handAffUpdater\n    if handAffUpdater is None:\n        handAffUpdater = TimerCallback()\n        handAffUpdater.targetFps = 30\n\n    handAffUpdater.callback = functools.partial(lockAffordanceToHand, aff)\n    handAffUpdater.start()\n\n\ndef lockToHandOff():\n    aff = getDefaultAffordanceObject()\n    if not aff:\n        return\n\n    handAffUpdater.stop()\n    aff.handToAffT = None\n\n\nclass DisparityPointCloudItem(vis.PolyDataItem):\n    def __init__(self, name, imagesChannel, cameraName, imageManager):\n        vis.PolyDataItem.__init__(self, name, vtk.vtkPolyData(), view=None)\n\n        self.addProperty(\'Channel\', imagesChannel)\n        self.addProperty(\'Camera name\', cameraName)\n\n        self.addProperty(\'Decimation\', 0, attributes=om.PropertyAttributes(enumNames=[\'1\', \'2\', \'4\', \'8\', \'16\']))\n        self.addProperty(\'Remove Size\', 1000,\n                         attributes=om.PropertyAttributes(decimals=0, minimum=0, maximum=100000.0, singleStep=1000))\n        self.addProperty(\'Target FPS\', 1.0,\n                         attributes=om.PropertyAttributes(decimals=1, minimum=0.1, maximum=30.0, singleStep=0.1))\n        self.addProperty(\'Max Range\', 2.0,\n                         attributes=om.PropertyAttributes(decimals=2, minimum=0., maximum=30.0, singleStep=0.25))\n\n        self.timer = TimerCallback()\n        self.timer.callback = self.update\n        self.lastUtime = 0\n        self.imageManager = imageManager\n        self.cameraName = cameraName\n        self.setProperty(\'Visible\', False)\n        self.addProperty(\'Remove Stale Data\', False)\n        self.addProperty(\'Stale Data Timeout\', 5.0,\n                         attributes=om.PropertyAttributes(decimals=1, minimum=0.1, maximum=30.0, singleStep=0.1))\n        self.lastDataReceivedTime = time.time()\n\n    def _onPropertyChanged(self, propertySet, propertyName):\n        vis.PolyDataItem._onPropertyChanged(self, propertySet, propertyName)\n\n        if propertyName == \'Visible\':\n            if self.getProperty(propertyName):\n                self.timer.start()\n            else:\n                self.timer.stop()\n\n        elif propertyName in (\'Decimation\', \'Remove outliers\', \'Max Range\'):\n            self.lastUtime = 0\n\n    def onRemoveFromObjectModel(self):\n        vis.PolyDataItem.onRemoveFromObjectModel(self)\n        self.timer.stop()\n\n    def update(self):\n        utime = self.imageManager.queue.getCurrentImageTime(self.cameraName)\n        if utime == self.lastUtime:\n            if self.getProperty(\'Remove Stale Data\') and (\n                (time.time() - self.lastDataReceivedTime) > self.getProperty(\'Stale Data Timeout\')):\n                if self.polyData.GetNumberOfPoints() > 0:\n                    self.setPolyData(vtk.vtkPolyData())\n            return\n\n        if (utime < self.lastUtime):\n            temp = 0  # dummy\n        elif (utime - self.lastUtime < 1E6 / self.getProperty(\'Target FPS\')):\n            return\n\n        decimation = int(self.properties.getPropertyEnumValue(\'Decimation\'))\n        removeSize = int(self.properties.getProperty(\'Remove Size\'))\n        rangeThreshold = float(self.properties.getProperty(\'Max Range\'))\n        polyData = getDisparityPointCloud(decimation, imagesChannel=self.getProperty(\'Channel\'),\n                                          cameraName=self.getProperty(\'Camera name\'),\n                                          removeOutliers=False, removeSize=removeSize, rangeThreshold=rangeThreshold)\n\n        self.setPolyData(polyData)\n        self.lastDataReceivedTime = time.time()\n\n        if polyData.GetNumberOfPoints() > 0 and not self.lastUtime:\n            self.setProperty(\'Color By\', \'rgb_colors\')\n\n        self.lastUtime = utime\n\n\ndef extractLargestCluster(polyData, **kwargs):\n    \'\'\'\n    Calls applyEuclideanClustering and then extracts the first (largest) cluster.\n    The given keyword arguments are passed into the applyEuclideanClustering function.\n    \'\'\'\n    polyData = applyEuclideanClustering(polyData, **kwargs)\n    return thresholdPoints(polyData, \'cluster_labels\', [1, 1])\n\n\ndef segmentGround(polyData, groundThickness=0.02, sceneHeightFromGround=0.05):\n    \'\'\' A More complex ground removal algorithm. Works when plane isn\'t\n    preceisely flat. First clusters on z to find approx ground height, then fits a plane there\n    \'\'\'\n\n    searchRegionThickness = 0.5\n\n    zvalues = vtkNumpy.getNumpyFromVtk(polyData, \'Points\')[:, 2]\n    groundHeight = np.percentile(zvalues, 5)\n\n    vtkNumpy.addNumpyToVtk(polyData, zvalues.copy(), \'z\')\n    searchRegion = thresholdPoints(polyData, \'z\', [groundHeight - searchRegionThickness / 2.0,\n                                                   groundHeight + searchRegionThickness / 2.0])\n\n    updatePolyData(searchRegion, \'ground search region\', parent=getDebugFolder(), colorByName=\'z\', visible=False)\n\n    _, origin, normal = applyPlaneFit(searchRegion, distanceThreshold=0.02, expectedNormal=[0, 0, 1],\n                                      perpendicularAxis=[0, 0, 1], returnOrigin=True)\n\n    points = vtkNumpy.getNumpyFromVtk(polyData, \'Points\')\n    dist = np.dot(points - origin, normal)\n    vtkNumpy.addNumpyToVtk(polyData, dist, \'dist_to_plane\')\n\n    groundPoints = thresholdPoints(polyData, \'dist_to_plane\', [-groundThickness / 2.0, groundThickness / 2.0])\n    scenePoints = thresholdPoints(polyData, \'dist_to_plane\', [sceneHeightFromGround, 100])\n\n    return origin, normal, groundPoints, scenePoints\n\n\ndef segmentGroundPlane():\n    inputObj = om.findObjectByName(\'pointcloud snapshot\')\n    inputObj.setProperty(\'Visible\', False)\n    polyData = shallowCopy(inputObj.polyData)\n\n    zvalues = vtkNumpy.getNumpyFromVtk(polyData, \'Points\')[:, 2]\n    groundHeight = np.percentile(zvalues, 5)\n    searchRegion = thresholdPoints(polyData, \'z\', [groundHeight - 0.3, groundHeight + 0.3])\n\n    updatePolyData(searchRegion, \'ground search region\', parent=getDebugFolder(), colorByName=\'z\', visible=False)\n\n    _, origin, normal = applyPlaneFit(searchRegion, distanceThreshold=0.02, expectedNormal=[0, 0, 1],\n                                      perpendicularAxis=[0, 0, 1], returnOrigin=True)\n\n    points = vtkNumpy.getNumpyFromVtk(polyData, \'Points\')\n    dist = np.dot(points - origin, normal)\n    vtkNumpy.addNumpyToVtk(polyData, dist, \'dist_to_plane\')\n\n    groundPoints = thresholdPoints(polyData, \'dist_to_plane\', [-0.01, 0.01])\n    scenePoints = thresholdPoints(polyData, \'dist_to_plane\', [0.05, 10])\n\n    updatePolyData(groundPoints, \'ground points\', alpha=0.3)\n    updatePolyData(scenePoints, \'scene points\', alpha=0.3)\n\n    # scenePoints = applyEuclideanClustering(scenePoints, clusterTolerance=0.10, minClusterSize=100, maxClusterSize=1e6)\n    # updatePolyData(scenePoints, \'scene points\', colorByName=\'cluster_labels\')\n\n\ndef applyLocalPlaneFit(polyData, searchPoint, searchRadius, searchRadiusEnd=None, removeGroundFirst=True):\n    useVoxelGrid = True\n    voxelGridSize = 0.03\n    distanceToPlaneThreshold = 0.02\n\n    if useVoxelGrid:\n        polyData = applyVoxelGrid(polyData, leafSize=voxelGridSize)\n\n    if removeGroundFirst:\n        _, polyData = removeGround(polyData, groundThickness=0.02, sceneHeightFromGround=0.04)\n\n    cropped = cropToSphere(polyData, searchPoint, searchRadius)\n    updatePolyData(cropped, \'crop to sphere\', visible=False, colorByName=\'distance_to_point\')\n\n    polyData, normal = applyPlaneFit(polyData, distanceToPlaneThreshold, searchOrigin=searchPoint,\n                                     searchRadius=searchRadius)\n\n    if searchRadiusEnd is not None:\n        polyData, normal = applyPlaneFit(polyData, distanceToPlaneThreshold, perpendicularAxis=normal,\n                                         angleEpsilon=math.radians(30), searchOrigin=searchPoint,\n                                         searchRadius=searchRadiusEnd)\n\n    fitPoints = thresholdPoints(polyData, \'dist_to_plane\', [-distanceToPlaneThreshold, distanceToPlaneThreshold])\n\n    updatePolyData(fitPoints, \'fitPoints\', visible=False)\n\n    fitPoints = labelDistanceToPoint(fitPoints, searchPoint)\n    clusters = extractClusters(fitPoints, clusterTolerance=0.05, minClusterSize=3)\n    clusters.sort(key=lambda x: vtkNumpy.getNumpyFromVtk(x, \'distance_to_point\').min())\n    fitPoints = clusters[0]\n\n    return fitPoints, normal\n\n    normalEstimationSearchRadius = 0.065\n\n    f = vtk.vtkPCLNormalEstimation()\n    f.SetSearchRadius(normalEstimationSearchRadius)\n    f.SetInput(polyData)\n    f.Update()\n    scenePoints = shallowCopy(f.GetOutput())\n\n    normals = vtkNumpy.getNumpyFromVtk(scenePoints, \'normals\')\n    normalsDotPlaneNormal = np.abs(np.dot(normals, normal))\n    vtkNumpy.addNumpyToVtk(scenePoints, normalsDotPlaneNormal, \'normals_dot_plane_normal\')\n\n    showPolyData(scenePoints, \'scene_with_normals\', parent=getDebugFolder(), colorByName=\'normals_dot_plane_normal\')\n\n    surfaces = thresholdPoints(scenePoints, \'normals_dot_plane_normal\', [0.95, 1.0])\n\n    clusters = extractClusters(surfaces, clusterTolerance=0.1, minClusterSize=5)\n    clusters = clusters[:10]\n\n    for i, cluster in enumerate(clusters):\n        showPolyData(cluster, \'plane cluster %i\' % i, parent=getDebugFolder(), visible=False)\n\n    return fitPoints\n\n\ndef orientToMajorPlane(polyData, pickedPoint):\n    \'\'\'\n    Find the largest plane and transform the cloud to align that plane\n    Use the given point as the origin\n    \'\'\'\n    distanceToPlaneThreshold = 0.02\n    searchRadius = 0.5\n\n    planePoints, origin, normal = applyPlaneFit(polyData, distanceToPlaneThreshold, searchOrigin=pickedPoint,\n                                                searchRadius=searchRadius, returnOrigin=True)\n    vis.updatePolyData(planePoints, \'local plane fit\', color=[0, 1, 0], parent=getDebugFolder(), visible=False)\n\n    planeFrame = transformUtils.getTransformFromOriginAndNormal(pickedPoint, normal)\n    vis.updateFrame(planeFrame, \'plane frame\', scale=0.15, parent=getDebugFolder(), visible=False)\n\n    polyData = transformPolyData(polyData, planeFrame.GetLinearInverse())\n\n    # if the mean point is below the horizontal plane, flip the cloud\n    zvalues = vtkNumpy.getNumpyFromVtk(polyData, \'Points\')[:, 2]\n    midCloudHeight = np.mean(zvalues)\n    if (midCloudHeight < 0):\n        flipTransform = transformUtils.frameFromPositionAndRPY([0, 0, 0], [0, 180, 0])\n        polyData = transformPolyData(polyData, flipTransform)\n\n    return polyData, planeFrame\n\n\ndef getMajorPlanes(polyData, useVoxelGrid=True):\n    voxelGridSize = 0.01\n    distanceToPlaneThreshold = 0.02\n\n    if useVoxelGrid:\n        polyData = applyVoxelGrid(polyData, leafSize=voxelGridSize)\n\n    polyDataList = []\n\n    minClusterSize = 100\n\n    while len(polyDataList) < 25:\n\n        f = planeSegmentationFilter()\n        f.SetInput(polyData)\n        f.SetDistanceThreshold(distanceToPlaneThreshold)\n        f.Update()\n        polyData = shallowCopy(f.GetOutput())\n\n        outliers = thresholdPoints(polyData, \'ransac_labels\', [0, 0])\n        inliers = thresholdPoints(polyData, \'ransac_labels\', [1, 1])\n        largestCluster = extractLargestCluster(inliers)\n\n        # i = len(polyDataList)\n        # showPolyData(inliers, \'inliers %d\' % i, color=getRandomColor(), parent=\'major planes\')\n        # showPolyData(outliers, \'outliers %d\' % i, color=getRandomColor(), parent=\'major planes\')\n        # showPolyData(largestCluster, \'cluster %d\' % i, color=getRandomColor(), parent=\'major planes\')\n\n        if largestCluster.GetNumberOfPoints() > minClusterSize:\n            polyDataList.append(largestCluster)\n            polyData = outliers\n        else:\n            break\n\n    return polyDataList\n\n\ndef showMajorPlanes(polyData=None):\n    if not polyData:\n        inputObj = om.findObjectByName(\'pointcloud snapshot\')\n        inputObj.setProperty(\'Visible\', False)\n        polyData = inputObj.polyData\n\n    om.removeFromObjectModel(om.findObjectByName(\'major planes\'))\n    folderObj = om.findObjectByName(\'segmentation\')\n    folderObj = om.getOrCreateContainer(\'major planes\', folderObj)\n\n    origin = SegmentationContext.getGlobalInstance().getViewFrame().GetPosition()\n    polyData = labelDistanceToPoint(polyData, origin)\n    polyData = thresholdPoints(polyData, \'distance_to_point\', [1, 4])\n\n    polyDataList = getMajorPlanes(polyData)\n\n    for i, polyData in enumerate(polyDataList):\n        obj = showPolyData(polyData, \'plane %d\' % i, color=getRandomColor(), visible=True, parent=\'major planes\')\n        obj.setProperty(\'Point Size\', 3)\n\n\ndef cropToBox(polyData, transform, dimensions):\n    \'\'\'\n    dimensions is length 3 describing box dimensions\n    \'\'\'\n    origin = np.array(transform.GetPosition())\n    axes = transformUtils.getAxesFromTransform(transform)\n\n    for axis, length in zip(axes, dimensions):\n        cropAxis = np.array(axis) * (length / 2.0)\n        polyData = cropToLineSegment(polyData, origin - cropAxis, origin + cropAxis)\n\n    return polyData\n\n\ndef cropToBounds(polyData, transform, bounds):\n    \'\'\'\n    bounds is a 2x3 containing the min/max values along the transform axes to use for cropping\n    \'\'\'\n    origin = np.array(transform.GetPosition())\n    axes = transformUtils.getAxesFromTransform(transform)\n\n    for axis, bound in zip(axes, bounds):\n        axis = np.array(axis) / np.linalg.norm(axis)\n        polyData = cropToLineSegment(polyData, origin + axis * bound[0], origin + axis * bound[1])\n\n    return polyData\n\n\ndef cropToSphere(polyData, origin, radius):\n    polyData = labelDistanceToPoint(polyData, origin)\n    return thresholdPoints(polyData, \'distance_to_point\', [0, radius])\n\n\ndef applyPlaneFit(polyData, distanceThreshold=0.02, expectedNormal=None, perpendicularAxis=None, angleEpsilon=0.2,\n                  returnOrigin=False, searchOrigin=None, searchRadius=None):\n    expectedNormal = expectedNormal if expectedNormal is not None else [-1, 0, 0]\n\n    fitInput = polyData\n    if searchOrigin is not None:\n        assert searchRadius\n        fitInput = cropToSphere(fitInput, searchOrigin, searchRadius)\n\n    # perform plane segmentation\n    f = planeSegmentationFilter()\n    f.SetInput(fitInput)\n    f.SetDistanceThreshold(distanceThreshold)\n    if perpendicularAxis is not None:\n        f.SetPerpendicularConstraintEnabled(True)\n        f.SetPerpendicularAxis(perpendicularAxis)\n        f.SetAngleEpsilon(angleEpsilon)\n    f.Update()\n    origin = f.GetPlaneOrigin()\n    normal = np.array(f.GetPlaneNormal())\n\n    # flip the normal if needed\n    if np.dot(normal, expectedNormal) < 0:\n        normal = -normal\n\n    # for each point, compute signed distance to plane\n\n    polyData = shallowCopy(polyData)\n    points = vtkNumpy.getNumpyFromVtk(polyData, \'Points\')\n    dist = np.dot(points - origin, normal)\n    vtkNumpy.addNumpyToVtk(polyData, dist, \'dist_to_plane\')\n\n    if returnOrigin:\n        return polyData, origin, normal\n    else:\n        return polyData, normal\n\n\ndef flipNormalsWithViewDirection(polyData, viewDirection):\n    normals = vnp.getNumpyFromVtk(polyData, \'normals\')\n    normals[np.dot(normals, viewDirection) > 0] *= -1\n\n\ndef normalEstimation(dataObj, searchCloud=None, searchRadius=0.05, useVoxelGrid=False, voxelGridLeafSize=0.05):\n    f = vtk.vtkPCLNormalEstimation()\n    f.SetSearchRadius(searchRadius)\n    f.SetInput(dataObj)\n    if searchCloud:\n        f.SetInput(1, searchCloud)\n    elif useVoxelGrid:\n        f.SetInput(1, applyVoxelGrid(dataObj, voxelGridLeafSize))\n    f.Update()\n    dataObj = shallowCopy(f.GetOutput())\n    dataObj.GetPointData().SetNormals(dataObj.GetPointData().GetArray(\'normals\'))\n\n    return dataObj\n\n\ndef addCoordArraysToPolyData(polyData):\n    polyData = shallowCopy(polyData)\n    points = vtkNumpy.getNumpyFromVtk(polyData, \'Points\')\n    vtkNumpy.addNumpyToVtk(polyData, points[:, 0].copy(), \'x\')\n    vtkNumpy.addNumpyToVtk(polyData, points[:, 1].copy(), \'y\')\n    vtkNumpy.addNumpyToVtk(polyData, points[:, 2].copy(), \'z\')\n\n    viewFrame = SegmentationContext.getGlobalInstance().getViewFrame()\n    viewOrigin = viewFrame.TransformPoint([0.0, 0.0, 0.0])\n    viewX = viewFrame.TransformVector([1.0, 0.0, 0.0])\n    viewY = viewFrame.TransformVector([0.0, 1.0, 0.0])\n    viewZ = viewFrame.TransformVector([0.0, 0.0, 1.0])\n    polyData = labelPointDistanceAlongAxis(polyData, viewX, origin=viewOrigin, resultArrayName=\'distance_along_view_x\')\n    polyData = labelPointDistanceAlongAxis(polyData, viewY, origin=viewOrigin, resultArrayName=\'distance_along_view_y\')\n    polyData = labelPointDistanceAlongAxis(polyData, viewZ, origin=viewOrigin, resultArrayName=\'distance_along_view_z\')\n\n    return polyData\n\n\ndef getDebugRevolutionData():\n    # dataDir = os.path.abspath(os.path.join(os.path.dirname(__file__), \'../../../../drc-data\'))\n    # filename = os.path.join(dataDir, \'valve_wall.vtp\')\n    # filename = os.path.join(dataDir, \'bungie_valve.vtp\')\n    # filename = os.path.join(dataDir, \'cinder-blocks.vtp\')\n    # filename = os.path.join(dataDir, \'cylinder_table.vtp\')\n    # filename = os.path.join(dataDir, \'firehose.vtp\')\n    # filename = os.path.join(dataDir, \'debris.vtp\')\n    # filename = os.path.join(dataDir, \'rev1.vtp\')\n    # filename = os.path.join(dataDir, \'drill-in-hand.vtp\')\n\n    filename = os.path.expanduser(\'~/Desktop/scans/debris-scan.vtp\')\n\n    return addCoordArraysToPolyData(ioUtils.readPolyData(filename))\n\n\ndef getCurrentScanBundle(useVoxelGrid=False):\n    obj = om.findObjectByName(\'SCANS_HALF_SWEEP\')\n    if not obj:\n        return None\n\n    revPolyData = obj.polyData\n    if not revPolyData or not revPolyData.GetNumberOfPoints():\n        return None\n\n    if useVoxelGrid:\n        revPolyData = applyVoxelGrid(revPolyData, leafSize=0.015)\n\n    return addCoordArraysToPolyData(revPolyData)\n\n\ndef getCurrentRevolutionData(useVoxelGrid=False):\n    from director import perception\n    revPolyData = perception._multisenseItem.model.revPolyData\n    if not revPolyData or not revPolyData.GetNumberOfPoints():\n        return getCurrentScanBundle()\n\n    if useVoxelGrid:\n        revPolyData = applyVoxelGrid(revPolyData, leafSize=0.015)\n\n    return addCoordArraysToPolyData(revPolyData)\n\n\ndef getDisparityPointCloud(decimation=4, removeOutliers=True, removeSize=0, rangeThreshold=-1,\n                           imagesChannel=\'MULTISENSE_CAMERA\', cameraName=\'CAMERA_LEFT\'):\n    p = cameraview.getStereoPointCloud(decimation, imagesChannel=imagesChannel, cameraName=cameraName,\n                                       removeSize=removeSize, rangeThreshold=rangeThreshold)\n    if not p:\n        return None\n\n    if removeOutliers:\n        # attempt to scale outlier filtering, best tuned for decimation of 2 or 4\n        scaling = (10 * 16) / (decimation * decimation)\n        p = labelOutliers(p, searchRadius=0.06, neighborsInSearchRadius=scaling)\n        p = thresholdPoints(p, \'is_outlier\', [0.0, 0.0])\n\n    return p\n\n\ndef getCurrentMapServerData():\n    mapServer = om.findObjectByName(\'Map Server\')\n    polyData = None\n    if mapServer and mapServer.getProperty(\'Visible\'):\n        polyData = mapServer.source.polyData\n\n    if not polyData or not polyData.GetNumberOfPoints():\n        return None\n\n    return addCoordArraysToPolyData(polyData)\n\n\ndef segmentGroundPlanes():\n    objs = []\n    for obj in om.getObjects():\n        name = obj.getProperty(\'Name\')\n        if name.startswith(\'pointcloud snapshot\'):\n            objs.append(obj)\n\n    objs = sorted(objs, key=lambda x: x.getProperty(\'Name\'))\n\n    d = DebugData()\n\n    prevHeadAxis = None\n    for obj in objs:\n        name = obj.getProperty(\'Name\')\n        print \'----- %s---------\' % name\n        print  \'head axis:\', obj.headAxis\n        origin, normal, groundPoints, _ = segmentGround(obj.polyData)\n        print \'ground normal:\', normal\n        showPolyData(groundPoints, name + \' ground points\', visible=False)\n        a = np.array([0, 0, 1])\n        b = np.array(normal)\n        diff = math.degrees(math.acos(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))))\n        if diff > 90:\n            print 180 - diff\n        else:\n            print diff\n\n        if prevHeadAxis is not None:\n            a = prevHeadAxis\n            b = np.array(obj.headAxis)\n            diff = math.degrees(math.acos(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))))\n            if diff > 90:\n                print 180 - diff\n            else:\n                print diff\n        prevHeadAxis = np.array(obj.headAxis)\n\n        d.addLine([0, 0, 0], normal)\n\n    updatePolyData(d.getPolyData(), \'normals\')\n\n\ndef extractCircle(polyData, distanceThreshold=0.04, radiusLimit=None):\n    circleFit = vtk.vtkPCLSACSegmentationCircle()\n    circleFit.SetDistanceThreshold(distanceThreshold)\n    circleFit.SetInput(polyData)\n    if radiusLimit is not None:\n        circleFit.SetRadiusLimit(radiusLimit)\n        circleFit.SetRadiusConstraintEnabled(True)\n    circleFit.Update()\n\n    polyData = thresholdPoints(circleFit.GetOutput(), \'ransac_labels\', [1.0, 1.0])\n    return polyData, circleFit\n\n\ndef removeMajorPlane(polyData, distanceThreshold=0.02):\n    # perform plane segmentation\n    f = planeSegmentationFilter()\n    f.SetInput(polyData)\n    f.SetDistanceThreshold(distanceThreshold)\n    f.Update()\n\n    polyData = thresholdPoints(f.GetOutput(), \'ransac_labels\', [0.0, 0.0])\n    return polyData, f\n\n\ndef removeGroundSimple(polyData, groundThickness=0.02, sceneHeightFromGround=0.05):\n    \'\'\' Simple ground plane removal algorithm. Uses ground height\n        and does simple z distance filtering.\n        Suitable for noisy data e.g. kinect/stereo camera\n        (Default args should be relaxed, filtering simplfied)\n    \'\'\'\n    groundHeight = SegmentationContext.getGlobalInstance().getGroundHeight()\n    origin = [0, 0, groundHeight]\n    normal = [0, 0, 1]\n\n    points = vtkNumpy.getNumpyFromVtk(polyData, \'Points\')\n    dist = np.dot(points - origin, normal)\n    vtkNumpy.addNumpyToVtk(polyData, dist, \'dist_to_plane\')\n\n    groundPoints = thresholdPoints(polyData, \'dist_to_plane\', [-groundThickness / 2.0, groundThickness / 2.0])\n    scenePoints = thresholdPoints(polyData, \'dist_to_plane\', [sceneHeightFromGround, 100])\n\n    return groundPoints, scenePoints\n\n\ndef removeGround(polyData, groundThickness=0.02, sceneHeightFromGround=0.05):\n    origin, normal, groundPoints, scenePoints = segmentGround(polyData, groundThickness, sceneHeightFromGround)\n    return groundPoints, scenePoints\n\n\ndef generateFeetForValve():\n    aff = om.findObjectByName(\'valve affordance\')\n    assert aff\n\n    params = aff.params\n\n    origin = np.array(params[\'origin\'])\n    origin[2] = 0.0\n\n    xaxis = -params[\'axis\']\n    zaxis = np.array([0, 0, 1])\n    yaxis = np.cross(zaxis, xaxis)\n    xaxis = np.cross(yaxis, zaxis)\n\n    stanceWidth = 0.2\n    stanceRotation = 25.0\n    stanceOffset = [-1.0, -0.5, 0.0]\n\n    valveFrame = getTransformFromAxes(xaxis, yaxis, zaxis)\n    valveFrame.PostMultiply()\n    valveFrame.Translate(origin)\n\n    stanceFrame, lfootFrame, rfootFrame = getFootFramesFromReferenceFrame(valveFrame, stanceWidth, stanceRotation,\n                                                                          stanceOffset)\n\n    showFrame(boardFrame, \'board ground frame\', parent=aff, scale=0.15, visible=False)\n    showFrame(lfootFrame, \'lfoot frame\', parent=aff, scale=0.15)\n    showFrame(rfootFrame, \'rfoot frame\', parent=aff, scale=0.15)\n\n    # d = DebugData()\n    # d.addLine(valveFrame.GetPosition(), stanceFrame.GetPosition())\n    # updatePolyData(d.getPolyData(), \'stance debug\')\n    # publishSteppingGoal(lfootFrame, rfootFrame)\n\n\ndef generateFeetForDebris():\n    aff = om.findObjectByName(\'board A\')\n    if not aff:\n        return\n\n    params = aff.params\n\n    origin = np.array(params[\'origin\'])\n\n    origin = origin + params[\'zaxis\'] * params[\'zwidth\'] / 2.0 - params[\'xaxis\'] * params[\'xwidth\'] / 2.0\n    origin[2] = 0.0\n\n    yaxis = params[\'zaxis\']\n    zaxis = np.array([0, 0, 1])\n    xaxis = np.cross(yaxis, zaxis)\n\n    stanceWidth = 0.35\n    stanceRotation = 0.0\n    stanceOffset = [-0.48, -0.08, 0]\n\n    boardFrame = getTransformFromAxes(xaxis, yaxis, zaxis)\n    boardFrame.PostMultiply()\n    boardFrame.Translate(origin)\n\n    stanceFrame, lfootFrame, rfootFrame = getFootFramesFromReferenceFrame(boardFrame, stanceWidth, stanceRotation,\n                                                                          stanceOffset)\n\n    showFrame(boardFrame, \'board ground frame\', parent=aff, scale=0.15, visible=False)\n    lfoot = showFrame(lfootFrame, \'lfoot frame\', parent=aff, scale=0.15)\n    rfoot = showFrame(rfootFrame, \'rfoot frame\', parent=aff, scale=0.15)\n\n    for obj in [lfoot, rfoot]:\n        obj.addToView(app.getDRCView())\n\n        # d = DebugData()\n        # d.addLine(valveFrame.GetPosition(), stanceFrame.GetPosition())\n        # updatePolyData(d.getPolyData(), \'stance debug\')\n        # publishSteppingGoal(lfootFrame, rfootFrame)\n\n\ndef generateFeetForWye():\n    aff = om.findObjectByName(\'wye points\')\n    if not aff:\n        return\n\n    params = aff.params\n\n    origin = np.array(params[\'origin\'])\n    origin[2] = 0.0\n\n    yaxis = params[\'xaxis\']\n    xaxis = -params[\'zaxis\']\n    zaxis = np.cross(xaxis, yaxis)\n\n    stanceWidth = 0.20\n    stanceRotation = 0.0\n    stanceOffset = [-0.48, -0.08, 0]\n\n    affGroundFrame = getTransformFromAxes(xaxis, yaxis, zaxis)\n    affGroundFrame.PostMultiply()\n    affGroundFrame.Translate(origin)\n\n    stanceFrame, lfootFrame, rfootFrame = getFootFramesFromReferenceFrame(affGroundFrame, stanceWidth, stanceRotation,\n                                                                          stanceOffset)\n\n    showFrame(affGroundFrame, \'affordance ground frame\', parent=aff, scale=0.15, visible=False)\n    lfoot = showFrame(lfootFrame, \'lfoot frame\', parent=aff, scale=0.15)\n    rfoot = showFrame(rfootFrame, \'rfoot frame\', parent=aff, scale=0.15)\n\n    for obj in [lfoot, rfoot]:\n        obj.addToView(app.getDRCView())\n\n\ndef getFootFramesFromReferenceFrame(referenceFrame, stanceWidth, stanceRotation, stanceOffset):\n    footHeight = 0.0745342\n\n    ref = vtk.vtkTransform()\n    ref.SetMatrix(referenceFrame.GetMatrix())\n\n    stanceFrame = vtk.vtkTransform()\n    stanceFrame.PostMultiply()\n    stanceFrame.RotateZ(stanceRotation)\n    stanceFrame.Translate(stanceOffset)\n    stanceFrame.Concatenate(ref)\n\n    lfootFrame = vtk.vtkTransform()\n    lfootFrame.PostMultiply()\n    lfootFrame.Translate(0, stanceWidth / 2.0, footHeight)\n    lfootFrame.Concatenate(stanceFrame)\n\n    rfootFrame = vtk.vtkTransform()\n    rfootFrame.PostMultiply()\n    rfootFrame.Translate(0, -stanceWidth / 2.0, footHeight)\n    rfootFrame.Concatenate(stanceFrame)\n\n    return stanceFrame, lfootFrame, rfootFrame\n\n\ndef poseFromFrame(frame):\n    import bot_core as lcmbotcore\n\n    pos, quat = transformUtils.poseFromTransform(frame)\n    trans = lcmbotcore.vector_3d_t()\n    trans.x, trans.y, trans.z = pos\n\n    quatMsg = lcmbotcore.quaternion_t()\n    quatMsg.w, quatMsg.x, quatMsg.y, quatMsg.z = quat\n\n    pose = lcmbotcore.position_3d_t()\n    pose.translation = trans\n    pose.rotation = quatMsg\n    return pose\n\n\ndef cropToPlane(polyData, origin, normal, threshold):\n    polyData = shallowCopy(polyData)\n    normal = normal / np.linalg.norm(normal)\n    points = vtkNumpy.getNumpyFromVtk(polyData, \'Points\')\n    dist = np.dot(points - origin, normal)\n    vtkNumpy.addNumpyToVtk(polyData, dist, \'dist_to_plane\')\n    cropped = thresholdPoints(polyData, \'dist_to_plane\', threshold)\n    return cropped, polyData\n\n\ndef createLine(blockDimensions, p1, p2):\n    sliceWidth = np.array(blockDimensions).max() / 2.0 + 0.02\n    sliceThreshold = [-sliceWidth, sliceWidth]\n\n    # require p1 to be point on left\n    if p1[0] > p2[0]:\n        p1, p2 = p2, p1\n\n    _, worldPt1 = getRayFromDisplayPoint(app.getCurrentRenderView(), p1)\n    _, worldPt2 = getRayFromDisplayPoint(app.getCurrentRenderView(), p2)\n\n    cameraPt = np.array(app.getCurrentRenderView().camera().GetPosition())\n\n    leftRay = worldPt1 - cameraPt\n    rightRay = worldPt2 - cameraPt\n    middleRay = (leftRay + rightRay) / 2.0\n\n    d = DebugData()\n    d.addLine(cameraPt, worldPt1)\n    d.addLine(cameraPt, worldPt2)\n    d.addLine(worldPt1, worldPt2)\n    d.addLine(cameraPt, cameraPt + middleRay)\n    updatePolyData(d.getPolyData(), \'line annotation\', parent=getDebugFolder(), visible=False)\n\n    inputObj = om.findObjectByName(\'pointcloud snapshot\')\n    if inputObj:\n        polyData = shallowCopy(inputObj.polyData)\n    else:\n        polyData = getCurrentRevolutionData()\n\n    origin = cameraPt\n\n    normal = np.cross(rightRay, leftRay)\n    leftNormal = np.cross(normal, leftRay)\n    rightNormal = np.cross(rightRay, normal)\n\n    normal /= np.linalg.norm(normal)\n    leftNormal /= np.linalg.norm(leftNormal)\n    rightNormal /= np.linalg.norm(rightNormal)\n    middleRay /= np.linalg.norm(middleRay)\n\n    cropped, polyData = cropToPlane(polyData, origin, normal, sliceThreshold)\n\n    updatePolyData(polyData, \'slice dist\', parent=getDebugFolder(), colorByName=\'dist_to_plane\',\n                   colorByRange=[-0.5, 0.5], visible=False)\n    updatePolyData(cropped, \'slice\', parent=getDebugFolder(), colorByName=\'dist_to_plane\', visible=False)\n\n    cropped, _ = cropToPlane(cropped, origin, leftNormal, [-1e6, 0])\n    cropped, _ = cropToPlane(cropped, origin, rightNormal, [-1e6, 0])\n\n    updatePolyData(cropped, \'slice segment\', parent=getDebugFolder(), colorByName=\'dist_to_plane\', visible=False)\n\n    planePoints, planeNormal = applyPlaneFit(cropped, distanceThreshold=0.005, perpendicularAxis=middleRay,\n                                             angleEpsilon=math.radians(60))\n    planePoints = thresholdPoints(planePoints, \'dist_to_plane\', [-0.005, 0.005])\n    updatePolyData(planePoints, \'board segmentation\', parent=getDebugFolder(), color=getRandomColor(), visible=False)\n\n    \'\'\'\n    names = [\'board A\', \'board B\', \'board C\', \'board D\', \'board E\', \'board F\', \'board G\', \'board H\', \'board I\']\n    for name in names:\n        if not om.findObjectByName(name):\n            break\n    else:\n        name = \'board\'\n    \'\'\'\n    name = \'board\'\n\n    segmentBlockByTopPlane(planePoints, blockDimensions, expectedNormal=-middleRay, expectedXAxis=middleRay,\n                           edgeSign=-1, name=name)\n\n\ndef updateBlockAffordances(polyData=None):\n    for obj in om.getObjects():\n        if isinstance(obj, BoxAffordanceItem):\n            if \'refit\' in obj.getProperty(\'Name\'):\n                om.removeFromObjectModel(obj)\n\n    for obj in om.getObjects():\n        if isinstance(obj, BoxAffordanceItem):\n            updateBlockFit(obj, polyData)\n\n\ndef updateBlockFit(affordanceObj, polyData=None):\n    affordanceObj.updateParamsFromActorTransform()\n\n    name = affordanceObj.getProperty(\'Name\') + \' refit\'\n    origin = affordanceObj.params[\'origin\']\n    normal = affordanceObj.params[\'yaxis\']\n    edgePerpAxis = affordanceObj.params[\'xaxis\']\n    blockDimensions = [affordanceObj.params[\'xwidth\'], affordanceObj.params[\'ywidth\']]\n\n    if polyData is None:\n        inputObj = om.findObjectByName(\'pointcloud snapshot\')\n        polyData = shallowCopy(inputObj.polyData)\n\n    cropThreshold = 0.1\n    cropped = polyData\n    cropped, _ = cropToPlane(cropped, origin, normal, [-cropThreshold, cropThreshold])\n    cropped, _ = cropToPlane(cropped, origin, edgePerpAxis, [-cropThreshold, cropThreshold])\n\n    updatePolyData(cropped, \'refit search region\', parent=getDebugFolder(), visible=False)\n\n    cropped = extractLargestCluster(cropped)\n\n    planePoints, planeNormal = applyPlaneFit(cropped, distanceThreshold=0.005, perpendicularAxis=normal,\n                                             angleEpsilon=math.radians(10))\n    planePoints = thresholdPoints(planePoints, \'dist_to_plane\', [-0.005, 0.005])\n    updatePolyData(planePoints, \'refit board segmentation\', parent=getDebugFolder(), visible=False)\n\n    refitObj = segmentBlockByTopPlane(planePoints, blockDimensions, expectedNormal=normal, expectedXAxis=edgePerpAxis,\n                                      edgeSign=-1, name=name)\n\n    refitOrigin = np.array(refitObj.params[\'origin\'])\n    refitLength = refitObj.params[\'zwidth\']\n    refitZAxis = refitObj.params[\'zaxis\']\n    refitEndPoint1 = refitOrigin + refitZAxis * refitLength / 2.0\n\n    originalLength = affordanceObj.params[\'zwidth\']\n    correctedOrigin = refitEndPoint1 - refitZAxis * originalLength / 2.0\n    originDelta = correctedOrigin - refitOrigin\n\n    refitObj.params[\'zwidth\'] = originalLength\n    refitObj.polyData.DeepCopy(affordanceObj.polyData)\n    refitObj.actor.GetUserTransform().Translate(originDelta)\n    refitObj.updateParamsFromActorTransform()\n\n\ndef startInteractiveLineDraw(blockDimensions):\n    picker = LineDraw(app.getCurrentRenderView())\n    addViewPicker(picker)\n    picker.enabled = True\n    picker.start()\n    picker.annotationFunc = functools.partial(createLine, blockDimensions)\n\n\ndef startLeverValveSegmentation():\n    picker = PointPicker(numberOfPoints=2)\n    addViewPicker(picker)\n    picker.enabled = True\n    picker.drawLines = False\n    picker.start()\n    picker.annotationFunc = functools.partial(segmentLeverValve)\n\n\ndef refitValveAffordance(aff, point1, origin, normal):\n    xaxis = aff.params[\'xaxis\']\n    yaxis = aff.params[\'yaxis\']\n    zaxis = aff.params[\'zaxis\']\n    origin = aff.params[\'origin\']\n\n    zaxis = normal\n    xaxis = [0, 0, 1]\n    yaxis = np.cross(zaxis, xaxis)\n    xaxis = np.cross(yaxis, zaxis)\n    xaxis /= np.linalg.norm(xaxis)\n    yaxis /= np.linalg.norm(yaxis)\n    t = getTransformFromAxes(xaxis, yaxis, zaxis)\n    t.PostMultiply()\n    t.Translate(origin)\n\n    aff.actor.GetUserTransform().SetMatrix(t.GetMatrix())\n    aff.updateParamsFromActorTransform()\n\n\ndef segmentValve(expectedValveRadius, point1, point2):\n    inputObj = om.findObjectByName(\'pointcloud snapshot\')\n    polyData = inputObj.polyData\n\n    viewPlaneNormal = np.array(getSegmentationView().camera().GetViewPlaneNormal())\n\n    polyData, _, wallNormal = applyPlaneFit(polyData, expectedNormal=viewPlaneNormal, searchOrigin=point1,\n                                            searchRadius=0.2, angleEpsilon=0.7, returnOrigin=True)\n\n    wallPoints = thresholdPoints(polyData, \'dist_to_plane\', [-0.01, 0.01])\n    updatePolyData(wallPoints, \'wall points\', parent=getDebugFolder(), visible=False)\n\n    polyData, _, _ = applyPlaneFit(polyData, expectedNormal=wallNormal, searchOrigin=point2,\n                                   searchRadius=expectedValveRadius, angleEpsilon=0.2, returnOrigin=True)\n    valveCluster = thresholdPoints(polyData, \'dist_to_plane\', [-0.01, 0.01])\n    valveCluster = cropToSphere(valveCluster, point2, expectedValveRadius * 2)\n    valveCluster = extractLargestCluster(valveCluster, minClusterSize=1)\n    updatePolyData(valveCluster, \'valve cluster\', parent=getDebugFolder(), visible=False)\n    origin = np.average(vtkNumpy.getNumpyFromVtk(valveCluster, \'Points\'), axis=0)\n\n    zaxis = wallNormal\n    xaxis = [0, 0, 1]\n    yaxis = np.cross(zaxis, xaxis)\n    xaxis = np.cross(yaxis, zaxis)\n    xaxis /= np.linalg.norm(xaxis)\n    yaxis /= np.linalg.norm(yaxis)\n    t = getTransformFromAxes(xaxis, yaxis, zaxis)\n    t.PostMultiply()\n    t.Translate(origin)\n\n    zwidth = 0.03\n    radius = expectedValveRadius\n\n    d = DebugData()\n    d.addLine(np.array([0, 0, -zwidth / 2.0]), np.array([0, 0, zwidth / 2.0]), radius=radius)\n\n    name = \'valve affordance\'\n    obj = showPolyData(d.getPolyData(), name, cls=FrameAffordanceItem, parent=\'affordances\', color=[0, 1, 0])\n    obj.actor.SetUserTransform(t)\n    obj.addToView(app.getDRCView())\n    refitWallCallbacks.append(functools.partial(refitValveAffordance, obj))\n\n    params = dict(axis=zaxis, radius=radius, length=zwidth, origin=origin, xaxis=xaxis, yaxis=yaxis, zaxis=zaxis,\n                  xwidth=radius, ywidth=radius, zwidth=zwidth,\n                  otdf_type=\'steering_cyl\', friendly_name=\'valve\')\n\n    obj.setAffordanceParams(params)\n    obj.updateParamsFromActorTransform()\n\n    frameObj = showFrame(obj.actor.GetUserTransform(), name + \' frame\', parent=obj, scale=radius, visible=False)\n    frameObj.addToView(app.getDRCView())\n\n\ndef segmentValveByBoundingBox(polyData, searchPoint):\n    viewDirection = SegmentationContext.getGlobalInstance().getViewDirection()\n\n    polyData = cropToSphere(polyData, searchPoint, radius=0.6)\n    polyData = applyVoxelGrid(polyData, leafSize=0.015)\n\n    # extract tube search region\n    polyData = labelDistanceToLine(polyData, searchPoint, np.array(searchPoint) + np.array([0, 0, 1]))\n    searchRegion = thresholdPoints(polyData, \'distance_to_line\', [0.0, 0.2])\n    updatePolyData(searchRegion, \'valve tube search region\', parent=getDebugFolder(), color=[1, 0, 0], visible=False)\n\n    # guess valve plane\n    _, origin, normal = applyPlaneFit(searchRegion, distanceThreshold=0.01, perpendicularAxis=viewDirection,\n                                      angleEpsilon=math.radians(30), expectedNormal=-viewDirection, returnOrigin=True)\n\n    # extract plane search region\n    polyData = labelPointDistanceAlongAxis(polyData, normal, origin)\n    searchRegion = thresholdPoints(polyData, \'distance_along_axis\', [-0.05, 0.05])\n    updatePolyData(searchRegion, \'valve plane search region\', parent=getDebugFolder(),\n                   colorByName=\'distance_along_axis\', visible=False)\n\n    valvePoints = extractLargestCluster(searchRegion, minClusterSize=1)\n    updatePolyData(valvePoints, \'valve cluster\', parent=getDebugFolder(), color=[0, 1, 0], visible=False)\n\n    valvePoints, _ = applyPlaneFit(valvePoints, expectedNormal=normal, perpendicularAxis=normal, distanceThreshold=0.01)\n    valveFit = thresholdPoints(valvePoints, \'dist_to_plane\', [-0.01, 0.01])\n\n    updatePolyData(valveFit, \'valve cluster\', parent=getDebugFolder(), color=[0, 1, 0], visible=False)\n\n    points = vtkNumpy.getNumpyFromVtk(valveFit, \'Points\')\n    zvalues = points[:, 2].copy()\n    minZ = np.min(zvalues)\n    maxZ = np.max(zvalues)\n\n    tubeRadius = 0.017\n    radius = float((maxZ - minZ) / 2.0) - tubeRadius\n\n    fields = makePolyDataFields(valveFit)\n    origin = np.array(fields.frame.GetPosition())\n\n    # origin = computeCentroid(valveFit)\n\n    zaxis = [0, 0, 1]\n    xaxis = normal\n    yaxis = np.cross(zaxis, xaxis)\n    yaxis /= np.linalg.norm(yaxis)\n    xaxis = np.cross(yaxis, zaxis)\n    xaxis /= np.linalg.norm(xaxis)\n\n    t = getTransformFromAxes(xaxis, yaxis, zaxis)\n    t.PostMultiply()\n    t.Translate(origin)\n\n    pose = transformUtils.poseFromTransform(t)\n    desc = dict(classname=\'CapsuleRingAffordanceItem\', Name=\'valve\', uuid=newUUID(), pose=pose, Color=[0, 1, 0],\n                Radius=radius, Segments=20)\n    desc[\'Tube Radius\'] = tubeRadius\n\n    obj = affordanceManager.newAffordanceFromDescription(desc)\n    obj.params = dict(radius=radius)\n\n    return obj\n\n\ndef segmentDoorPlane(polyData, doorPoint, stanceFrame):\n    doorPoint = np.array(doorPoint)\n    doorBand = 1.5\n\n    polyData = cropToLineSegment(polyData, doorPoint + [0.0, 0.0, doorBand / 2], doorPoint - [0.0, 0.0, doorBand / 2])\n    fitPoints, normal = applyLocalPlaneFit(polyData, doorPoint, searchRadius=0.2, searchRadiusEnd=1.0,\n                                           removeGroundFirst=False)\n\n    updatePolyData(fitPoints, \'door points\', visible=False, color=[0, 1, 0])\n\n    viewDirection = SegmentationContext.getGlobalInstance().getViewDirection()\n    if np.dot(normal, viewDirection) > 0:\n        normal = -normal\n\n    origin = computeCentroid(fitPoints)\n    groundHeight = stanceFrame.GetPosition()[2]\n    origin = [origin[0], origin[1], groundHeight]\n\n    xaxis = -normal\n    zaxis = [0, 0, 1]\n\n    yaxis = np.cross(zaxis, xaxis)\n    yaxis /= np.linalg.norm(yaxis)\n    xaxis = np.cross(yaxis, zaxis)\n    xaxis /= np.linalg.norm(xaxis)\n\n    t = getTransformFromAxes(xaxis, yaxis, zaxis)\n    t.PostMultiply()\n    t.Translate(origin)\n\n    return t\n\n\ndef segmentValveByRim(polyData, rimPoint1, rimPoint2):\n    viewDirection = SegmentationContext.getGlobalInstance().getViewDirection()\n\n    yaxis = np.array(rimPoint2) - np.array(rimPoint1)\n    zaxis = [0, 0, 1]\n    xaxis = np.cross(yaxis, zaxis)\n    xaxis /= np.linalg.norm(xaxis)\n\n    # flip xaxis to be with view direction\n    if np.dot(xaxis, viewDirection) < 0:\n        xaxis = -xaxis\n\n    yaxis = np.cross(zaxis, xaxis)\n    yaxis /= np.linalg.norm(yaxis)\n\n    origin = (np.array(rimPoint2) + np.array(rimPoint1)) / 2.0\n\n    polyData = labelPointDistanceAlongAxis(polyData, xaxis, origin)\n    polyData = thresholdPoints(polyData, \'distance_along_axis\', [-0.05, 0.05])\n    updatePolyData(polyData, \'valve plane region\', parent=getDebugFolder(), colorByName=\'distance_along_axis\',\n                   visible=False)\n\n    polyData = cropToSphere(polyData, origin, radius=0.4)\n    polyData = applyVoxelGrid(polyData, leafSize=0.015)\n\n    updatePolyData(polyData, \'valve search region\', parent=getDebugFolder(), color=[1, 0, 0], visible=False)\n\n    valveFit = extractLargestCluster(polyData, minClusterSize=1)\n    updatePolyData(valveFit, \'valve cluster\', parent=getDebugFolder(), color=[0, 1, 0], visible=False)\n\n    points = vtkNumpy.getNumpyFromVtk(valveFit, \'Points\')\n    zvalues = points[:, 2].copy()\n    minZ = np.min(zvalues)\n    maxZ = np.max(zvalues)\n\n    tubeRadius = 0.017\n    radius = float((maxZ - minZ) / 2.0) - tubeRadius\n\n    fields = makePolyDataFields(valveFit)\n    origin = np.array(fields.frame.GetPosition())\n    vis.updatePolyData(transformPolyData(fields.box, fields.frame), \'valve cluster bounding box\', visible=False)\n\n    # origin = computeCentroid(valveFit)\n\n    \'\'\'\n    zaxis = [0,0,1]\n    xaxis = normal\n    yaxis = np.cross(zaxis, xaxis)\n    yaxis /= np.linalg.norm(yaxis)\n    xaxis = np.cross(yaxis, zaxis)\n    xaxis /= np.linalg.norm(xaxis)\n    \'\'\'\n\n    radius = np.max(fields.dims) / 2.0 - tubeRadius\n\n    proj = [np.abs(np.dot(xaxis, axis)) for axis in fields.axes]\n    xaxisNew = fields.axes[np.argmax(proj)]\n    if np.dot(xaxisNew, xaxis) < 0:\n        xaxisNew = -xaxisNew\n\n    xaxis = xaxisNew\n\n    yaxis = np.cross(zaxis, xaxis)\n    yaxis /= np.linalg.norm(yaxis)\n    xaxis = np.cross(yaxis, zaxis)\n    xaxis /= np.linalg.norm(xaxis)\n\n    t = getTransformFromAxes(xaxis, yaxis, zaxis)\n    t.PostMultiply()\n    t.Translate(origin)\n\n    pose = transformUtils.poseFromTransform(t)\n    desc = dict(classname=\'CapsuleRingAffordanceItem\', Name=\'valve\', uuid=newUUID(), pose=pose, Color=[0, 1, 0],\n                Radius=float(radius), Segments=20)\n    desc[\'Tube Radius\'] = tubeRadius\n\n    obj = affordanceManager.newAffordanceFromDescription(desc)\n    obj.params = dict(radius=radius)\n\n    return obj\n\n\ndef segmentValveByWallPlane(expectedValveRadius, point1, point2):\n    centerPoint = (point1 + point2) / 2.0\n\n    inputObj = om.findObjectByName(\'pointcloud snapshot\')\n    polyData = inputObj.polyData\n\n    _, polyData = removeGround(polyData)\n\n    viewDirection = SegmentationContext.getGlobalInstance().getViewDirection()\n    polyData, origin, normal = applyPlaneFit(polyData, expectedNormal=-viewDirection, returnOrigin=True)\n\n    perpLine = np.cross(point2 - point1, normal)\n    # perpLine /= np.linalg.norm(perpLine)\n    # perpLine * np.linalg.norm(point2 - point1)/2.0\n    point3, point4 = centerPoint + perpLine / 2.0, centerPoint - perpLine / 2.0\n\n    d = DebugData()\n    d.addLine(point1, point2)\n    d.addLine(point3, point4)\n    updatePolyData(d.getPolyData(), \'crop lines\', parent=getDebugFolder(), visible=False)\n\n    wallPoints = thresholdPoints(polyData, \'dist_to_plane\', [-0.01, 0.01])\n    updatePolyData(wallPoints, \'valve wall\', parent=getDebugFolder(), visible=False)\n\n    searchRegion = thresholdPoints(polyData, \'dist_to_plane\', [0.05, 0.5])\n    searchRegion = cropToLineSegment(searchRegion, point1, point2)\n    searchRegion = cropToLineSegment(searchRegion, point3, point4)\n\n    updatePolyData(searchRegion, \'valve search region\', parent=getDebugFolder(), color=[1, 0, 0], visible=False)\n\n    searchRegionSpokes = shallowCopy(searchRegion)\n\n    searchRegion, origin, _ = applyPlaneFit(searchRegion, expectedNormal=normal, perpendicularAxis=normal,\n                                            returnOrigin=True)\n    searchRegion = thresholdPoints(searchRegion, \'dist_to_plane\', [-0.015, 0.015])\n\n    updatePolyData(searchRegion, \'valve search region 2\', parent=getDebugFolder(), color=[0, 1, 0], visible=False)\n\n    largestCluster = extractLargestCluster(searchRegion, minClusterSize=1)\n\n    updatePolyData(largestCluster, \'valve cluster\', parent=getDebugFolder(), color=[0, 1, 0], visible=False)\n\n    radiusLimit = [expectedValveRadius - 0.01, expectedValveRadius + 0.01] if expectedValveRadius else None\n    # radiusLimit = None\n\n    polyData, circleFit = extractCircle(largestCluster, distanceThreshold=0.01, radiusLimit=radiusLimit)\n    updatePolyData(polyData, \'circle fit\', parent=getDebugFolder(), visible=False)\n\n    # polyData, circleFit = extractCircle(polyData, distanceThreshold=0.01)\n    # showPolyData(polyData, \'circle fit\', colorByName=\'z\')\n\n\n    radius = circleFit.GetCircleRadius()\n    origin = np.array(circleFit.GetCircleOrigin())\n    circleNormal = np.array(circleFit.GetCircleNormal())\n    circleNormal = circleNormal / np.linalg.norm(circleNormal)\n\n    if np.dot(circleNormal, normal) < 0:\n        circleNormal *= -1\n\n    # force use of the plane normal\n    circleNormal = normal\n    radius = expectedValveRadius\n\n    d = DebugData()\n    d.addLine(origin - normal * radius, origin + normal * radius)\n    d.addCircle(origin, circleNormal, radius)\n    updatePolyData(d.getPolyData(), \'valve axes\', parent=getDebugFolder(), visible=False)\n\n    zaxis = -circleNormal\n    xaxis = [0, 0, 1]\n    yaxis = np.cross(zaxis, xaxis)\n    xaxis = np.cross(yaxis, zaxis)\n    xaxis /= np.linalg.norm(xaxis)\n    yaxis /= np.linalg.norm(yaxis)\n    # t = getTransformFromAxes(xaxis, yaxis, zaxis) # this was added to be consistent with segmentValveByRim\n    t = getTransformFromAxes(zaxis, -yaxis, xaxis)  # this was added to be consistent with segmentValveByRim\n    t.PostMultiply()\n    t.Translate(origin)\n\n    # Spoke angle fitting:\n    if (1 == 0):  # disabled jan 2015\n        # extract the relative positon of the points to the valve axis:\n        searchRegionSpokes = labelDistanceToLine(searchRegionSpokes, origin, [origin + circleNormal])\n        searchRegionSpokes = thresholdPoints(searchRegionSpokes, \'distance_to_line\', [0.05, radius - 0.04])\n        updatePolyData(searchRegionSpokes, \'valve spoke search\', parent=getDebugFolder(), visible=False)\n        searchRegionSpokesLocal = transformPolyData(searchRegionSpokes, t.GetLinearInverse())\n        points = vtkNumpy.getNumpyFromVtk(searchRegionSpokesLocal, \'Points\')\n\n        spoke_angle = findValveSpokeAngle(points)\n    else:\n        spoke_angle = 0\n\n    spokeAngleTransform = transformUtils.frameFromPositionAndRPY([0, 0, 0], [0, 0, spoke_angle])\n    spokeTransform = transformUtils.copyFrame(t)\n    spokeAngleTransform.Concatenate(spokeTransform)\n    spokeObj = showFrame(spokeAngleTransform, \'spoke frame\', parent=getDebugFolder(), visible=False, scale=radius)\n    spokeObj.addToView(app.getDRCView())\n    t = spokeAngleTransform\n\n    tubeRadius = 0.017\n\n    pose = transformUtils.poseFromTransform(t)\n    desc = dict(classname=\'CapsuleRingAffordanceItem\', Name=\'valve\', uuid=newUUID(), pose=pose, Color=[0, 1, 0],\n                Radius=float(radius), Segments=20)\n    desc[\'Tube Radius\'] = tubeRadius\n\n    obj = affordanceManager.newAffordanceFromDescription(desc)\n    obj.params = dict(radius=radius)\n\n\ndef showHistogram(polyData, arrayName, numberOfBins=100):\n    import matplotlib.pyplot as plt\n\n    x = vnp.getNumpyFromVtk(polyData, arrayName)\n    hist, bins = np.histogram(x, bins=numberOfBins)\n    width = 0.7 * (bins[1] - bins[0])\n    center = (bins[:-1] + bins[1:]) / 2\n    plt.bar(center, hist, align=\'center\', width=width)\n    plt.show()\n\n    return bins[np.argmax(hist)] + (bins[1] - bins[0]) / 2.0\n\n\ndef showTable(table, parent):\n    \'\'\'\n    explictly draw a table and its frames\n    \'\'\'\n    pose = transformUtils.poseFromTransform(table.frame)\n    desc = dict(classname=\'MeshAffordanceItem\', Name=\'table\', Color=[0, 1, 0], pose=pose)\n    aff = affordanceManager.newAffordanceFromDescription(desc)\n    aff.setPolyData(table.mesh)\n\n    tableBox = vis.showPolyData(table.box, \'table box\', parent=aff, color=[0, 1, 0], visible=False)\n    tableBox.actor.SetUserTransform(table.frame)\n\n\ndef applyKmeansLabel(polyData, arrayName, numberOfClusters, whiten=False):\n    import scipy.cluster\n    ar = vnp.getNumpyFromVtk(polyData, arrayName).copy()\n\n    if whiten:\n        scipy.cluster.vq.whiten(ar)\n\n    codes, disturbances = scipy.cluster.vq.kmeans(ar, numberOfClusters)\n\n    if arrayName == \'normals\' and numberOfClusters == 2:\n        v1 = codes[0]\n        v2 = codes[1]\n        v1 /= np.linalg.norm(v1)\n        v2 /= np.linalg.norm(v2)\n        angle = np.arccos(np.dot(v1, v2))\n        print \'angle between normals:\', np.degrees(angle)\n\n    code, distance = scipy.cluster.vq.vq(ar, codes)\n\n    polyData = shallowCopy(polyData)\n    vnp.addNumpyToVtk(polyData, code, \'%s_kmeans_label\' % arrayName)\n    return polyData\n\n\ndef findValveSpokeAngle(points):\n    \'\'\'\n    Determine the location of the valve spoke angle\n    By binning the spoke returns. returns angle in degrees\n    \'\'\'\n\n    # np.savetxt(""/home/mfallon/Desktop/spoke_points.csv"", points, delimiter="","")\n\n\n    # convert all points to degrees in range [0,120]\n    angle = np.degrees(np.arctan2(points[:, 1], points[:, 0]))\n    qq = np.where(angle < 0)[0]\n    angle[qq] += 360\n    angle = np.mod(angle, 120)\n\n    # find the spoke as the max of a histogram:\n    bins = range(0, 130, 10)  # 0,10,...130\n    freq, bins = np.histogram(angle, bins)\n    amax = np.argmax(freq)\n    spoke_angle = bins[amax] + 5  # correct for 5deg offset\n\n    return spoke_angle\n\n\ndef findWallCenter(polyData, removeGroundMethod=removeGround):\n    \'\'\'\n    Find a frame at the center of the valve wall\n    X&Y: average of points on the wall plane\n    Z: 4 feet off the ground (determined using robot\'s feet\n    Orientation: z-normal into plane, y-axis horizontal\n    \'\'\'\n\n    _, polyData = removeGroundMethod(polyData)\n\n    viewDirection = SegmentationContext.getGlobalInstance().getViewDirection()\n    polyData, origin, normal = applyPlaneFit(polyData, expectedNormal=-viewDirection, returnOrigin=True)\n\n    wallPoints = thresholdPoints(polyData, \'dist_to_plane\', [-0.01, 0.01])\n    wallPoints = applyVoxelGrid(wallPoints, leafSize=0.03)\n    wallPoints = extractLargestCluster(wallPoints, minClusterSize=100)\n\n    updatePolyData(wallPoints, \'auto valve wall\', parent=getDebugFolder(), visible=False)\n\n    xvalues = vtkNumpy.getNumpyFromVtk(wallPoints, \'Points\')[:, 0]\n    yvalues = vtkNumpy.getNumpyFromVtk(wallPoints, \'Points\')[:, 1]\n\n    # median or mid of max or min?\n    # xcenter = np.median(xvalues)\n    # ycenter = np.median(yvalues)\n    xcenter = (np.max(xvalues) + np.min(xvalues)) / 2\n    ycenter = (np.max(yvalues) + np.min(yvalues)) / 2\n\n    # not used, not very reliable\n    # zvalues = vtkNumpy.getNumpyFromVtk(wallPoints, \'Points\')[:,2]\n    # zcenter = np.median(zvalues)\n    zcenter = SegmentationContext.getGlobalInstance().getGroundHeight() + 1.2192  # valves are 4ft from ground\n    point1 = np.array([xcenter, ycenter, zcenter])  # center of the valve wall\n\n    zaxis = -normal\n    xaxis = [0, 0, 1]\n    yaxis = np.cross(zaxis, xaxis)\n    xaxis = np.cross(yaxis, zaxis)\n    xaxis /= np.linalg.norm(xaxis)\n    yaxis /= np.linalg.norm(yaxis)\n    t = getTransformFromAxes(xaxis, yaxis, zaxis)\n    t.PostMultiply()\n    t.Translate(point1)\n\n    normalObj = showFrame(t, \'valve wall frame\', parent=getDebugFolder(), visible=False)  # z direction out of wall\n    normalObj.addToView(app.getDRCView())\n\n    return t\n\n\ndef segmentValveWallAuto(expectedValveRadius=.195, mode=\'both\', removeGroundMethod=removeGround):\n    \'\'\'\n    Automatically segment a valve hanging in front of the wall at the center\n    \'\'\'\n\n    # find the valve wall and its center\n    inputObj = om.findObjectByName(\'pointcloud snapshot\')\n    polyData = inputObj.polyData\n\n    t = findWallCenter(polyData, removeGroundMethod)\n\n    valve_point1 = [0, 0.6, 0]\n    valveTransform1 = transformUtils.frameFromPositionAndRPY(valve_point1, [0, 0, 0])\n    valveTransform1.Concatenate(t)\n    point1 = np.array(valveTransform1.GetPosition())  # left of wall\n\n    valve_point2 = [0, -0.6, 0]\n    valveTransform2 = transformUtils.frameFromPositionAndRPY(valve_point2, [0, 0, 0])\n    valveTransform2.Concatenate(t)\n    point2 = np.array(valveTransform2.GetPosition())  # left of wall\n\n    valve_point3 = [0, 1.0, 0]  # lever can over hang\n    valveTransform3 = transformUtils.frameFromPositionAndRPY(valve_point3, [0, 0, 0])\n    valveTransform3.Concatenate(t)\n    point3 = valveTransform3.GetPosition()  # right of wall\n\n    d = DebugData()\n    d.addSphere(point2, radius=0.01)\n    d.addSphere(point1, radius=0.03)\n    d.addSphere(point3, radius=0.01)\n    updatePolyData(d.getPolyData(), \'auto wall points\', parent=getDebugFolder(), visible=False)\n\n    if (mode == \'valve\'):\n        segmentValveByWallPlane(expectedValveRadius, point1, point2)\n    elif (mode == \'lever\'):\n        segmentLeverByWallPlane(point1, point3)\n    elif (mode == \'both\'):\n        segmentValveByWallPlane(expectedValveRadius, point1, point2)\n        segmentLeverByWallPlane(point1, point3)\n    else:\n        raise Exception(\'unexpected segmentation mode: \' + mode)\n\n\ndef segmentLeverByWallPlane(point1, point2):\n    \'\'\'\n    determine the position (including rotation of a lever near a wall\n    input is as for the valve - to points on the wall either side of the lever\n    \'\'\'\n\n    # 1. determine the wall plane and normal\n    centerPoint = (point1 + point2) / 2.0\n\n    inputObj = om.findObjectByName(\'pointcloud snapshot\')\n    polyData = inputObj.polyData\n\n    viewDirection = SegmentationContext.getGlobalInstance().getViewDirection()\n    polyData, origin, normal = applyPlaneFit(polyData, expectedNormal=-viewDirection, returnOrigin=True)\n\n    # 2. Crop the cloud down to the lever only using the wall plane\n    perpLine = np.cross(point2 - point1, -normal)\n    # perpLine /= np.linalg.norm(perpLine)\n    # perpLine * np.linalg.norm(point2 - point1)/2.0\n    point3, point4 = centerPoint + perpLine / 2.0, centerPoint - perpLine / 2.0\n\n    d = DebugData()\n    d.addLine(point1, point2)\n    d.addLine(point3, point4)\n    updatePolyData(d.getPolyData(), \'lever crop lines\', parent=getDebugFolder(), visible=False)\n\n    wallPoints = thresholdPoints(polyData, \'dist_to_plane\', [-0.01, 0.01])\n    updatePolyData(wallPoints, \'lever valve wall\', parent=getDebugFolder(), visible=False)\n\n    searchRegion = thresholdPoints(polyData, \'dist_to_plane\', [0.12, 0.2])  # very tight threshold\n    searchRegion = cropToLineSegment(searchRegion, point1, point2)\n    searchRegion = cropToLineSegment(searchRegion, point3, point4)\n    updatePolyData(searchRegion, \'lever search region\', parent=getDebugFolder(), color=[1, 0, 0], visible=False)\n\n    # 3. fit line to remaining points - all assumed to be the lever\n    linePoint, lineDirection, _ = applyLineFit(searchRegion, distanceThreshold=0.02)\n    # if np.dot(lineDirection, forwardDirection) < 0:\n    #    lineDirection = -lineDirection\n\n    d = DebugData()\n    d.addSphere(linePoint, radius=0.02)\n    updatePolyData(d.getPolyData(), \'lever point\', parent=getDebugFolder(), visible=False)\n\n    pts = vtkNumpy.getNumpyFromVtk(searchRegion, \'Points\')\n    dists = np.dot(pts - linePoint, lineDirection)\n    lever_center = linePoint + lineDirection * np.min(dists)\n    lever_tip = linePoint + lineDirection * np.max(dists)\n\n    # 4. determine which lever point is closest to the lower left of the wall. That\'s the lever_center point\n    zaxis = -normal\n    xaxis = [0, 0, 1]\n    yaxis = np.cross(zaxis, xaxis)\n    xaxis = np.cross(yaxis, zaxis)\n    xaxis /= np.linalg.norm(xaxis)\n    yaxis /= np.linalg.norm(yaxis)\n    t = getTransformFromAxes(xaxis, yaxis, zaxis)\n    t.PostMultiply()\n    t.Translate(point1)\n\n    # a distant point down and left from wall\n    wall_point_lower_left = [-20, -20.0, 0]\n    wall_point_lower_left_Transform = transformUtils.frameFromPositionAndRPY(wall_point_lower_left, [0, 0, 0])\n    wall_point_lower_left_Transform.Concatenate(t)\n    wall_point_lower_left = wall_point_lower_left_Transform.GetPosition()\n    d1 = np.sqrt(np.sum((wall_point_lower_left - projectPointToPlane(lever_center, origin, normal)) ** 2))\n    d2 = np.sqrt(np.sum((wall_point_lower_left - projectPointToPlane(lever_tip, origin, normal)) ** 2))\n\n    if (d2 < d1):  # flip the points to match variable names\n        p_temp = lever_center\n        lever_center = lever_tip\n        lever_tip = p_temp\n        lineDirection = -lineDirection\n\n    # 5. compute the rotation angle of the lever and, using that, its frame\n    zaxis = -normal\n    xaxis = [0, 0, 1]\n    yaxis = np.cross(zaxis, xaxis)\n    xaxis = np.cross(yaxis, zaxis)\n    xaxis /= np.linalg.norm(xaxis)\n    yaxis /= np.linalg.norm(yaxis)\n    t = getTransformFromAxes(xaxis, yaxis, zaxis)\n    t.PostMultiply()\n    t.Translate(lever_center)  # nominal frame at lever center\n\n    rotationAngle = -computeSignedAngleBetweenVectors(lineDirection, [0, 0, 1], -normal)\n    t_lever = transformUtils.frameFromPositionAndRPY([0, 0, 0], [0, 0, math.degrees(rotationAngle)])\n    t_lever.PostMultiply()\n    t_lever.Concatenate(t)\n\n    d = DebugData()\n    # d.addSphere( point1 , radius=0.1)\n    d.addSphere(wall_point_lower_left, radius=0.1)\n    d.addSphere(lever_center, radius=0.04)\n    d.addSphere(lever_tip, radius=0.01)\n    d.addLine(lever_center, lever_tip)\n    updatePolyData(d.getPolyData(), \'lever end points\', color=[0, 1, 0], parent=getDebugFolder(), visible=False)\n\n    radius = 0.01\n    length = np.sqrt(np.sum((lever_tip - lever_center) ** 2))\n\n    d = DebugData()\n    d.addLine([0, 0, 0], [length, 0, 0], radius=radius)\n    d.addSphere([0, 0, 0], 0.02)\n    geometry = d.getPolyData()\n\n    obj = showPolyData(geometry, \'valve lever\', cls=FrameAffordanceItem, parent=\'affordances\', color=[0, 1, 0],\n                       visible=True)\n    obj.actor.SetUserTransform(t_lever)\n    obj.addToView(app.getDRCView())\n    frameObj = showFrame(t_lever, \'lever frame\', parent=obj, visible=False)\n    frameObj.addToView(app.getDRCView())\n\n    otdfType = \'lever_valve\'\n    params = dict(origin=np.array(t_lever.GetPosition()), xaxis=xaxis, yaxis=yaxis, zaxis=zaxis, xwidth=0.1, ywidth=0.1,\n                  zwidth=0.1, radius=radius, length=length, friendly_name=otdfType, otdf_type=otdfType)\n    obj.setAffordanceParams(params)\n    obj.updateParamsFromActorTransform()\n\n\ndef applyICP(source, target):\n    icp = vtk.vtkIterativeClosestPointTransform()\n    icp.SetSource(source)\n    icp.SetTarget(target)\n    icp.GetLandmarkTransform().SetModeToRigidBody()\n    icp.Update()\n    t = vtk.vtkTransform()\n    t.SetMatrix(icp.GetMatrix())\n    return t\n\n\ndef applyDiskGlyphs(polyData, computeNormals=True):\n    voxelGridLeafSize = 0.03\n    normalEstimationSearchRadius = 0.05\n    diskRadius = 0.015\n    diskResolution = 12\n\n    if computeNormals:\n        scanInput = polyData\n\n        pd = applyVoxelGrid(scanInput, leafSize=voxelGridLeafSize)\n\n        pd = labelOutliers(pd, searchRadius=normalEstimationSearchRadius, neighborsInSearchRadius=3)\n        pd = thresholdPoints(pd, \'is_outlier\', [0, 0])\n\n        pd = normalEstimation(pd, searchRadius=normalEstimationSearchRadius, searchCloud=scanInput)\n    else:\n        pd = polyData\n\n    assert polyData.GetPointData().GetNormals()\n\n    disk = vtk.vtkDiskSource()\n    disk.SetOuterRadius(diskRadius)\n    disk.SetInnerRadius(0.0)\n    disk.SetRadialResolution(0)\n    disk.SetCircumferentialResolution(diskResolution)\n    disk.Update()\n\n    t = vtk.vtkTransform()\n    t.RotateY(90)\n    disk = transformPolyData(disk.GetOutput(), t)\n\n    glyph = vtk.vtkGlyph3D()\n    glyph.ScalingOff()\n    glyph.OrientOn()\n    glyph.SetSource(disk)\n    glyph.SetInput(pd)\n    glyph.SetVectorModeToUseNormal()\n    glyph.Update()\n\n    return shallowCopy(glyph.GetOutput())\n\n\ndef applyArrowGlyphs(polyData, computeNormals=True, voxelGridLeafSize=0.03, normalEstimationSearchRadius=0.05,\n                     arrowSize=0.02):\n    if computeNormals:\n        polyData = applyVoxelGrid(polyData, leafSize=0.02)\n        voxelData = applyVoxelGrid(polyData, leafSize=voxelGridLeafSize)\n        polyData = normalEstimation(polyData, searchRadius=normalEstimationSearchRadius, searchCloud=voxelData)\n        polyData = removeNonFinitePoints(polyData, \'normals\')\n        flipNormalsWithViewDirection(polyData, SegmentationContext.getGlobalInstance().getViewDirection())\n\n    assert polyData.GetPointData().GetNormals()\n\n    arrow = vtk.vtkArrowSource()\n    arrow.Update()\n\n    glyph = vtk.vtkGlyph3D()\n    glyph.SetScaleFactor(arrowSize)\n    glyph.SetSource(arrow.GetOutput())\n    glyph.SetInput(polyData)\n    glyph.SetVectorModeToUseNormal()\n    glyph.Update()\n\n    return shallowCopy(glyph.GetOutput())\n\n\ndef segmentLeverValve(point1, point2):\n    inputObj = om.findObjectByName(\'pointcloud snapshot\')\n    polyData = inputObj.polyData\n\n    viewPlaneNormal = np.array(getSegmentationView().camera().GetViewPlaneNormal())\n    polyData, origin, normal = applyPlaneFit(polyData, expectedNormal=viewPlaneNormal, searchOrigin=point1,\n                                             searchRadius=0.2, angleEpsilon=0.7, returnOrigin=True)\n\n    wallPoints = thresholdPoints(polyData, \'dist_to_plane\', [-0.01, 0.01])\n    updatePolyData(wallPoints, \'wall points\', parent=getDebugFolder(), visible=False)\n\n    radius = 0.01\n    length = 0.33\n\n    normal = -normal  # set z to face into wall\n    zaxis = normal\n    xaxis = [0, 0, 1]\n    yaxis = np.cross(zaxis, xaxis)\n    xaxis = np.cross(yaxis, zaxis)\n    xaxis /= np.linalg.norm(xaxis)\n    yaxis /= np.linalg.norm(yaxis)\n    t = getTransformFromAxes(xaxis, yaxis, zaxis)\n    t.PostMultiply()\n    t.Translate(point2)\n\n    leverP1 = point2\n    leverP2 = point2 + xaxis * length\n    d = DebugData()\n    d.addLine([0, 0, 0], [length, 0, 0], radius=radius)\n    d.addSphere([0, 0, 0], 0.02)\n    geometry = d.getPolyData()\n\n    obj = showPolyData(geometry, \'valve lever\', cls=FrameAffordanceItem, parent=\'affordances\', color=[0, 1, 0],\n                       visible=True)\n    obj.actor.SetUserTransform(t)\n    obj.addToView(app.getDRCView())\n    frameObj = showFrame(t, \'lever frame\', parent=obj, visible=False)\n    frameObj.addToView(app.getDRCView())\n\n    otdfType = \'lever_valve\'\n    params = dict(origin=np.array(t.GetPosition()), xaxis=xaxis, yaxis=yaxis, zaxis=zaxis, xwidth=0.1, ywidth=0.1,\n                  zwidth=0.1, radius=radius, length=length, friendly_name=otdfType, otdf_type=otdfType)\n    obj.setAffordanceParams(params)\n    obj.updateParamsFromActorTransform()\n\n\ndef segmentWye(point1, point2):\n    inputObj = om.findObjectByName(\'pointcloud snapshot\')\n    polyData = inputObj.polyData\n\n    viewPlaneNormal = np.array(getSegmentationView().camera().GetViewPlaneNormal())\n\n    polyData, origin, normal = applyPlaneFit(polyData, expectedNormal=viewPlaneNormal, searchOrigin=point1,\n                                             searchRadius=0.2, angleEpsilon=0.7, returnOrigin=True)\n\n    wallPoints = thresholdPoints(polyData, \'dist_to_plane\', [-0.01, 0.01])\n    updatePolyData(wallPoints, \'wall points\', parent=getDebugFolder(), visible=False)\n\n    wyeMesh = ioUtils.readPolyData(os.path.join(app.getDRCBase(), \'software/models/otdf/wye.obj\'))\n\n    wyeMeshPoint = np.array([0.0, 0.0, 0.005])\n    wyeMeshLeftHandle = np.array([0.032292, 0.02949, 0.068485])\n\n    xaxis = -normal\n    zaxis = [0, 0, 1]\n    yaxis = np.cross(zaxis, xaxis)\n    yaxis /= np.linalg.norm(yaxis)\n    zaxis = np.cross(xaxis, yaxis)\n\n    t = getTransformFromAxes(xaxis, yaxis, zaxis)\n    t.PreMultiply()\n    t.Translate(-wyeMeshPoint)\n    t.PostMultiply()\n    t.Translate(point2)\n\n    d = DebugData()\n    d.addSphere(point2, radius=0.005)\n    updatePolyData(d.getPolyData(), \'wye pick point\', parent=getDebugFolder(), visible=False)\n\n    wyeObj = showPolyData(wyeMesh, \'wye\', cls=FrameAffordanceItem, color=[0, 1, 0], visible=True)\n    wyeObj.actor.SetUserTransform(t)\n    wyeObj.addToView(app.getDRCView())\n    frameObj = showFrame(t, \'wye frame\', parent=wyeObj, visible=False)\n    frameObj.addToView(app.getDRCView())\n\n    params = dict(origin=np.array(t.GetPosition()), xaxis=xaxis, yaxis=yaxis, zaxis=zaxis, xwidth=0.1, ywidth=0.1,\n                  zwidth=0.1, friendly_name=\'wye\', otdf_type=\'wye\')\n    wyeObj.setAffordanceParams(params)\n    wyeObj.updateParamsFromActorTransform()\n\n\ndef segmentDoorHandle(otdfType, point1, point2):\n    inputObj = om.findObjectByName(\'pointcloud snapshot\')\n    polyData = inputObj.polyData\n\n    viewPlaneNormal = np.array(getSegmentationView().camera().GetViewPlaneNormal())\n\n    polyData, origin, normal = applyPlaneFit(polyData, expectedNormal=viewPlaneNormal, searchOrigin=point1,\n                                             searchRadius=0.2, angleEpsilon=0.7, returnOrigin=True)\n\n    wallPoints = thresholdPoints(polyData, \'dist_to_plane\', [-0.01, 0.01])\n    updatePolyData(wallPoints, \'wall points\', parent=getDebugFolder(), visible=False)\n\n    handlePoint = np.array([0.005, 0.065, 0.011])\n\n    xaxis = -normal\n    zaxis = [0, 0, 1]\n    yaxis = np.cross(zaxis, xaxis)\n    yaxis /= np.linalg.norm(yaxis)\n    zaxis = np.cross(xaxis, yaxis)\n\n    xwidth = 0.01\n    ywidth = 0.13\n    zwidth = 0.022\n    cube = vtk.vtkCubeSource()\n    cube.SetXLength(xwidth)\n    cube.SetYLength(ywidth)\n    cube.SetZLength(zwidth)\n    cube.Update()\n    cube = shallowCopy(cube.GetOutput())\n\n    t = getTransformFromAxes(xaxis, yaxis, zaxis)\n    # t.PreMultiply()\n    # t.Translate(-handlePoint)\n    t.PostMultiply()\n    t.Translate(point2)\n\n    name = \'door handle\'\n    obj = showPolyData(cube, name, cls=FrameAffordanceItem, parent=\'affordances\')\n    obj.actor.SetUserTransform(t)\n    obj.addToView(app.getDRCView())\n\n    params = dict(origin=origin, xwidth=xwidth, ywidth=ywidth, zwidth=zwidth, xaxis=xaxis, yaxis=yaxis, zaxis=zaxis,\n                  friendly_name=name, otdf_type=otdfType)\n    obj.setAffordanceParams(params)\n    obj.updateParamsFromActorTransform()\n\n    frameObj = showFrame(obj.actor.GetUserTransform(), name + \' frame\', parent=obj, visible=False)\n    frameObj.addToView(app.getDRCView())\n\n\ndef segmentTruss(point1, point2):\n    edge = point2 - point1\n    edgeLength = np.linalg.norm(edge)\n\n    stanceOffset = [-0.42, 0.0, 0.0]\n    stanceYaw = 0.0\n\n    d = DebugData()\n    p1 = [0.0, 0.0, 0.0]\n    p2 = -np.array([0.0, -1.0, 0.0]) * edgeLength\n    d.addSphere(p1, radius=0.02)\n    d.addSphere(p2, radius=0.02)\n    d.addLine(p1, p2)\n\n    stanceTransform = vtk.vtkTransform()\n    stanceTransform.PostMultiply()\n    stanceTransform.Translate(stanceOffset)\n    # stanceTransform.RotateZ(stanceYaw)\n\n    geometry = transformPolyData(d.getPolyData(), stanceTransform.GetLinearInverse())\n\n    yaxis = edge / edgeLength\n    zaxis = [0.0, 0.0, 1.0]\n    xaxis = np.cross(yaxis, zaxis)\n    xaxis /= np.linalg.norm(xaxis)\n    yaxis = np.cross(zaxis, xaxis)\n    yaxis /= np.linalg.norm(yaxis)\n\n    xwidth = 0.1\n    ywidth = edgeLength\n    zwidth = 0.1\n\n    t = getTransformFromAxes(xaxis, yaxis, zaxis)\n    t.PreMultiply()\n    t.Concatenate(stanceTransform)\n    t.PostMultiply()\n    t.Translate(point1)\n\n    name = \'truss\'\n    otdfType = \'robot_knees\'\n    obj = showPolyData(geometry, name, cls=FrameAffordanceItem, parent=\'affordances\')\n    obj.actor.SetUserTransform(t)\n    obj.addToView(app.getDRCView())\n\n    params = dict(origin=t.GetPosition(), xwidth=xwidth, ywidth=ywidth, zwidth=zwidth, xaxis=xaxis, yaxis=yaxis,\n                  zaxis=zaxis, friendly_name=name, otdf_type=otdfType)\n    obj.setAffordanceParams(params)\n    obj.updateParamsFromActorTransform()\n\n    frameObj = showFrame(obj.actor.GetUserTransform(), name + \' frame\', parent=obj, visible=False)\n    frameObj.addToView(app.getDRCView())\n\n\ndef segmentHoseNozzle(point1):\n    inputObj = om.findObjectByName(\'pointcloud snapshot\')\n    polyData = inputObj.polyData\n\n    searchRegion = cropToSphere(polyData, point1, 0.10)\n    updatePolyData(searchRegion, \'nozzle search region\', parent=getDebugFolder(), visible=False)\n\n    xaxis = [1, 0, 0]\n    yaxis = [0, -1, 0]\n    zaxis = [0, 0, -1]\n    origin = point1\n\n    t = getTransformFromAxes(xaxis, yaxis, zaxis)\n    t.PostMultiply()\n    t.Translate(point1)\n\n    nozzleRadius = 0.0266\n    nozzleLength = 0.042\n    nozzleTipRadius = 0.031\n    nozzleTipLength = 0.024\n\n    d = DebugData()\n    d.addLine(np.array([0, 0, -nozzleLength / 2.0]), np.array([0, 0, nozzleLength / 2.0]), radius=nozzleRadius)\n    d.addLine(np.array([0, 0, nozzleLength / 2.0]), np.array([0, 0, nozzleLength / 2.0 + nozzleTipLength]),\n              radius=nozzleTipRadius)\n\n    obj = showPolyData(d.getPolyData(), \'hose nozzle\', cls=FrameAffordanceItem, color=[0, 1, 0], visible=True)\n    obj.actor.SetUserTransform(t)\n    obj.addToView(app.getDRCView())\n    frameObj = showFrame(t, \'nozzle frame\', parent=obj, visible=False)\n    frameObj.addToView(app.getDRCView())\n\n    params = dict(origin=origin, xaxis=xaxis, yaxis=yaxis, zaxis=zaxis, xwidth=0.1, ywidth=0.1, zwidth=0.1,\n                  friendly_name=\'firehose\', otdf_type=\'firehose\')\n    obj.setAffordanceParams(params)\n    obj.updateParamsFromActorTransform()\n\n\ndef segmentDrillWall(point1, point2, point3):\n    inputObj = om.findObjectByName(\'pointcloud snapshot\')\n    polyData = inputObj.polyData\n\n    points = [point1, point2, point3]\n\n    viewPlaneNormal = np.array(getSegmentationView().camera().GetViewPlaneNormal())\n    expectedNormal = np.cross(point2 - point1, point3 - point1)\n    expectedNormal /= np.linalg.norm(expectedNormal)\n    if np.dot(expectedNormal, viewPlaneNormal) < 0:\n        expectedNormal *= -1.0\n\n    polyData, origin, normal = applyPlaneFit(polyData, expectedNormal=expectedNormal,\n                                             searchOrigin=(point1 + point2 + point3) / 3.0, searchRadius=0.3,\n                                             angleEpsilon=0.3, returnOrigin=True)\n\n    points = [projectPointToPlane(point, origin, normal) for point in points]\n\n    xaxis = -normal\n    zaxis = [0, 0, 1]\n    yaxis = np.cross(zaxis, xaxis)\n    yaxis /= np.linalg.norm(yaxis)\n    zaxis = np.cross(xaxis, yaxis)\n\n    t = getTransformFromAxes(xaxis, yaxis, zaxis)\n    t.PostMultiply()\n    t.Translate(points[0])\n\n    d = DebugData()\n    pointsInWallFrame = []\n    for p in points:\n        pp = np.zeros(3)\n        t.GetLinearInverse().TransformPoint(p, pp)\n        pointsInWallFrame.append(pp)\n        d.addSphere(pp, radius=0.02)\n\n    for a, b in zip(pointsInWallFrame, pointsInWallFrame[1:] + [pointsInWallFrame[0]]):\n        d.addLine(a, b, radius=0.015)\n\n    aff = showPolyData(d.getPolyData(), \'drill target\', cls=FrameAffordanceItem, color=[0, 1, 0], visible=True)\n    aff.actor.SetUserTransform(t)\n    showFrame(t, \'drill target frame\', parent=aff, visible=False)\n    refitWallCallbacks.append(functools.partial(refitDrillWall, aff))\n\n    params = dict(origin=points[0], xaxis=xaxis, yaxis=yaxis, zaxis=zaxis, xwidth=0.1, ywidth=0.1, zwidth=0.1,\n                  p1y=pointsInWallFrame[0][1], p1z=pointsInWallFrame[0][2],\n                  p2y=pointsInWallFrame[1][1], p2z=pointsInWallFrame[1][2],\n                  p3y=pointsInWallFrame[2][1], p3z=pointsInWallFrame[2][2],\n                  friendly_name=\'drill_wall\', otdf_type=\'drill_wall\')\n\n    aff.setAffordanceParams(params)\n    aff.updateParamsFromActorTransform()\n    aff.addToView(app.getDRCView())\n\n\nrefitWallCallbacks = []\n\n\ndef refitWall(point1):\n    inputObj = om.findObjectByName(\'pointcloud snapshot\')\n    polyData = inputObj.polyData\n\n    viewPlaneNormal = np.array(getSegmentationView().camera().GetViewPlaneNormal())\n\n    polyData, origin, normal = applyPlaneFit(polyData, expectedNormal=viewPlaneNormal, searchOrigin=point1,\n                                             searchRadius=0.2, angleEpsilon=0.7, returnOrigin=True)\n\n    wallPoints = thresholdPoints(polyData, \'dist_to_plane\', [-0.01, 0.01])\n    updatePolyData(wallPoints, \'wall points\', parent=getDebugFolder(), visible=False)\n\n    for func in refitWallCallbacks:\n        func(point1, origin, normal)\n\n\ndef refitDrillWall(aff, point1, origin, normal):\n    t = aff.actor.GetUserTransform()\n\n    targetOrigin = np.array(t.GetPosition())\n\n    projectedOrigin = projectPointToPlane(targetOrigin, origin, normal)\n    projectedOrigin[2] = targetOrigin[2]\n\n    xaxis = -normal\n    zaxis = [0, 0, 1]\n    yaxis = np.cross(zaxis, xaxis)\n    yaxis /= np.linalg.norm(yaxis)\n    zaxis = np.cross(xaxis, yaxis)\n\n    t = getTransformFromAxes(xaxis, yaxis, zaxis)\n    t.PostMultiply()\n    t.Translate(projectedOrigin)\n    aff.actor.GetUserTransform().SetMatrix(t.GetMatrix())\n\n\n# this should be depreciated!\ndef getGroundHeightFromFeet():\n    rfoot = getLinkFrame(drcargs.getDirectorConfig()[\'rightFootLink\'])\n    return np.array(rfoot.GetPosition())[2] - 0.0745342\n\n\n# this should be depreciated!\ndef getTranslationRelativeToFoot(t):\n    rfoot = getLinkFrame(drcargs.getDirectorConfig()[\'rightFootLink\'])\n\n\ndef segmentDrillWallConstrained(rightAngleLocation, point1, point2):\n    inputObj = om.findObjectByName(\'pointcloud snapshot\')\n    polyData = inputObj.polyData\n\n    viewPlaneNormal = np.array(getSegmentationView().camera().GetViewPlaneNormal())\n    expectedNormal = np.cross(point2 - point1, [0.0, 0.0, 1.0])\n    expectedNormal /= np.linalg.norm(expectedNormal)\n    if np.dot(expectedNormal, viewPlaneNormal) < 0:\n        expectedNormal *= -1.0\n\n    polyData, origin, normal = applyPlaneFit(polyData, expectedNormal=expectedNormal, searchOrigin=point1,\n                                             searchRadius=0.3, angleEpsilon=0.3, returnOrigin=True)\n\n    triangleOrigin = projectPointToPlane(point2, origin, normal)\n\n    xaxis = -normal\n    zaxis = [0, 0, 1]\n    yaxis = np.cross(zaxis, xaxis)\n    yaxis /= np.linalg.norm(yaxis)\n    zaxis = np.cross(xaxis, yaxis)\n\n    t = getTransformFromAxes(xaxis, yaxis, zaxis)\n    t.PostMultiply()\n    t.Translate(triangleOrigin)\n\n    createDrillWall(rightAngleLocation, t)\n\n\ndef createDrillWall(rightAngleLocation, trianglePose):\n    # recover the origin and axes from the pose:\n    triangleOrigin = trianglePose.GetPosition()\n    xaxis, yaxis, zaxis = transformUtils.getAxesFromTransform(trianglePose)\n\n    # 0.6096 = 24 * .0254 (m = feet)\n    # 0.3048 = 12 * .0254 (m = feet)\n    edgeRight = np.array([0.0, -1.0, 0.0]) * (0.6)\n    edgeUp = np.array([0.0, 0.0, 1.0]) * (0.3)\n\n    pointsInWallFrame = np.zeros((3, 3))\n\n    if rightAngleLocation == DRILL_TRIANGLE_BOTTOM_LEFT:\n        pointsInWallFrame[1] = edgeUp\n        pointsInWallFrame[2] = edgeRight\n\n    elif rightAngleLocation == DRILL_TRIANGLE_BOTTOM_RIGHT:\n        pointsInWallFrame[1] = edgeUp  # edgeRight +edgeUp\n        pointsInWallFrame[2] = -edgeRight  # edgeRight\n\n    elif rightAngleLocation == DRILL_TRIANGLE_TOP_LEFT:\n        pointsInWallFrame[1] = edgeRight\n        pointsInWallFrame[2] = -edgeUp\n\n    elif rightAngleLocation == DRILL_TRIANGLE_TOP_RIGHT:\n        pointsInWallFrame[1] = edgeRight\n        pointsInWallFrame[2] = edgeRight - edgeUp\n    else:\n        raise Exception(\'unexpected value for right angle location: \', + rightAngleLocation)\n\n    center = pointsInWallFrame.sum(axis=0) / 3.0\n    shrinkFactor = 1  # 0.90\n    shrinkPoints = (pointsInWallFrame - center) * shrinkFactor + center\n\n    d = DebugData()\n    for p in pointsInWallFrame:\n        d.addSphere(p, radius=0.015)\n\n    for a, b in zip(pointsInWallFrame, np.vstack((pointsInWallFrame[1:], pointsInWallFrame[0]))):\n        d.addLine(a, b, radius=0.005)  # 01)\n\n    for a, b in zip(shrinkPoints, np.vstack((shrinkPoints[1:], shrinkPoints[0]))):\n        d.addLine(a, b, radius=0.005)  # 0.025\n\n    folder = om.getOrCreateContainer(\'affordances\')\n\n    wall = om.findObjectByName(\'wall\')\n    om.removeFromObjectModel(wall)\n\n    aff = showPolyData(d.getPolyData(), \'wall\', cls=FrameAffordanceItem, color=[0, 1, 0], visible=True, parent=folder)\n    aff.actor.SetUserTransform(trianglePose)\n    aff.addToView(app.getDRCView())\n\n    refitWallCallbacks.append(functools.partial(refitDrillWall, aff))\n\n    frameObj = showFrame(trianglePose, \'wall frame\', parent=aff, scale=0.2, visible=False)\n    frameObj.addToView(app.getDRCView())\n\n    params = dict(origin=triangleOrigin, xaxis=xaxis, yaxis=yaxis, zaxis=zaxis, xwidth=0.1, ywidth=0.1, zwidth=0.1,\n                  p1y=shrinkPoints[0][1], p1z=shrinkPoints[0][2],\n                  p2y=shrinkPoints[1][1], p2z=shrinkPoints[1][2],\n                  p3y=shrinkPoints[2][1], p3z=shrinkPoints[2][2],\n                  friendly_name=\'drill_wall\', otdf_type=\'drill_wall\')\n\n    aff.setAffordanceParams(params)\n    aff.updateParamsFromActorTransform()\n\n    \'\'\'\n    rfoot = getLinkFrame(drcargs.getDirectorConfig()[\'rightFootLink\'])\n    tt = getTransformFromAxes(xaxis, yaxis, zaxis)\n    tt.PostMultiply()\n    tt.Translate(rfoot.GetPosition())\n    showFrame(tt, \'rfoot with wall orientation\')\n    aff.footToAffTransform = computeAToB(tt, trianglePose)\n\n    footToAff = list(aff.footToAffTransform.GetPosition())\n    tt.TransformVector(footToAff, footToAff)\n\n    d = DebugData()\n    d.addSphere(tt.GetPosition(), radius=0.02)\n    d.addLine(tt.GetPosition(), np.array(tt.GetPosition()) + np.array(footToAff))\n    showPolyData(d.getPolyData(), \'rfoot debug\')\n    \'\'\'\n\n\ndef getDrillAffordanceParams(origin, xaxis, yaxis, zaxis, drillType=""dewalt_button""):\n    if (drillType == ""dewalt_button""):\n        params = dict(origin=origin, xaxis=xaxis, yaxis=yaxis, zaxis=zaxis, xwidth=0.1, ywidth=0.1, zwidth=0.1,\n                      button_x=0.007,\n                      button_y=-0.035,\n                      button_z=-0.06,\n                      button_roll=-90.0,\n                      button_pitch=-90.0,\n                      button_yaw=0.0,\n                      bit_x=-0.01,\n                      bit_y=0.0,\n                      bit_z=0.15,\n                      bit_roll=0,\n                      bit_pitch=-90,\n                      bit_yaw=0,\n                      friendly_name=\'dewalt_button\', otdf_type=\'dewalt_button\')\n    else:\n        params = dict(origin=origin, xaxis=xaxis, yaxis=yaxis, zaxis=zaxis, xwidth=0.1, ywidth=0.1, zwidth=0.1,\n                      button_x=0.007,\n                      button_y=-0.035,\n                      button_z=-0.06,\n                      button_roll=0.0,\n                      button_pitch=0.0,\n                      button_yaw=0.0,\n                      bit_x=0.18,\n                      bit_y=0.0,\n                      bit_z=0.13,\n                      bit_roll=0,\n                      bit_pitch=0,\n                      bit_yaw=0,\n                      friendly_name=\'dewalt_barrel\', otdf_type=\'dewalt_barrel\')\n\n    return params\n\n\ndef getDrillMesh(applyBitOffset=False):\n    button = np.array([0.007, -0.035, -0.06])\n    drillMesh = ioUtils.readPolyData(os.path.join(app.getDRCBase(), \'software/models/otdf/dewalt_button.obj\'))\n\n    if applyBitOffset:\n        t = vtk.vtkTransform()\n        t.Translate(0.01, 0.0, 0.0)\n        drillMesh = transformPolyData(drillMesh, t)\n\n    d = DebugData()\n    d.addPolyData(drillMesh)\n    d.addSphere(button, radius=0.005, color=[0, 1, 0])\n    d.addLine([0.0, 0.0, 0.155], [0.0, 0.0, 0.14], radius=0.001, color=[0, 1, 0])\n\n    return shallowCopy(d.getPolyData())\n\n\ndef getDrillBarrelMesh():\n    return ioUtils.readPolyData(os.path.join(app.getDRCBase(), \'software/models/otdf/dewalt.ply\'), computeNormals=True)\n\n\ndef segmentDrill(point1, point2, point3):\n    inputObj = om.findObjectByName(\'pointcloud snapshot\')\n    polyData = inputObj.polyData\n\n    viewPlaneNormal = np.array(getSegmentationView().camera().GetViewPlaneNormal())\n\n    polyData, origin, normal = applyPlaneFit(polyData, expectedNormal=viewPlaneNormal, searchOrigin=point1,\n                                             searchRadius=0.2, angleEpsilon=0.7, returnOrigin=True)\n\n    tablePoints = thresholdPoints(polyData, \'dist_to_plane\', [-0.01, 0.01])\n    updatePolyData(tablePoints, \'table plane points\', parent=getDebugFolder(), visible=False)\n\n    searchRegion = thresholdPoints(polyData, \'dist_to_plane\', [0.03, 0.4])\n    searchRegion = cropToSphere(searchRegion, point2, 0.30)\n    drillPoints = extractLargestCluster(searchRegion)\n\n    drillToTopPoint = np.array([-0.002904, -0.010029, 0.153182])\n\n    zaxis = normal\n    yaxis = point3 - point2\n    yaxis /= np.linalg.norm(yaxis)\n    xaxis = np.cross(yaxis, zaxis)\n    xaxis /= np.linalg.norm(xaxis)\n    yaxis = np.cross(zaxis, xaxis)\n\n    t = getTransformFromAxes(xaxis, yaxis, zaxis)\n    t.PreMultiply()\n    t.Translate(-drillToTopPoint)\n    t.PostMultiply()\n    t.Translate(point2)\n\n    drillMesh = getDrillMesh()\n\n    aff = showPolyData(drillMesh, \'drill\', cls=FrameAffordanceItem, visible=True)\n    aff.actor.SetUserTransform(t)\n    showFrame(t, \'drill frame\', parent=aff, visible=False).addToView(app.getDRCView())\n\n    params = getDrillAffordanceParams(origin, xaxis, yaxis, zaxis)\n    aff.setAffordanceParams(params)\n    aff.updateParamsFromActorTransform()\n    aff.addToView(app.getDRCView())\n\n\ndef makePolyDataFields(pd):\n    mesh = computeDelaunay3D(pd)\n\n    if not mesh.GetNumberOfPoints():\n        return None\n\n    origin, edges, wireframe = getOrientedBoundingBox(mesh)\n\n    edgeLengths = np.array([np.linalg.norm(edge) for edge in edges])\n    axes = [edge / np.linalg.norm(edge) for edge in edges]\n\n    # find axis nearest to the +/- up vector\n    upVector = [0, 0, 1]\n    dotProducts = [np.abs(np.dot(axe, upVector)) for axe in axes]\n    zAxisIndex = np.argmax(dotProducts)\n\n    # re-index axes and edge lengths so that the found axis is the z axis\n    axesInds = [(zAxisIndex + 1) % 3, (zAxisIndex + 2) % 3, zAxisIndex]\n    axes = [axes[i] for i in axesInds]\n    edgeLengths = [edgeLengths[i] for i in axesInds]\n\n    # flip if necessary so that z axis points toward up\n    if np.dot(axes[2], upVector) < 0:\n        axes[1] = -axes[1]\n        axes[2] = -axes[2]\n\n    boxCenter = computeCentroid(wireframe)\n\n    t = getTransformFromAxes(axes[0], axes[1], axes[2])\n    t.PostMultiply()\n    t.Translate(boxCenter)\n\n    pd = transformPolyData(pd, t.GetLinearInverse())\n    wireframe = transformPolyData(wireframe, t.GetLinearInverse())\n    mesh = transformPolyData(mesh, t.GetLinearInverse())\n\n    return FieldContainer(points=pd, box=wireframe, mesh=mesh, frame=t, dims=edgeLengths, axes=axes)\n\n\ndef makeMovable(obj, initialTransform=None):\n    \'\'\'\n    Adds a child frame to the given PolyDataItem.  If initialTransform is not\n    given, then an origin frame is computed for the polydata using the\n    center and orientation of the oriented bounding of the polydata.  The polydata\n    is transformed using the inverse of initialTransform and then a child frame\n    is assigned to the object to reposition it.\n    \'\'\'\n    pd = obj.polyData\n    t = initialTransform\n\n    if t is None:\n        origin, edges, wireframe = getOrientedBoundingBox(pd)\n        edgeLengths = np.array([np.linalg.norm(edge) for edge in edges])\n        axes = [edge / np.linalg.norm(edge) for edge in edges]\n        boxCenter = computeCentroid(wireframe)\n        t = getTransformFromAxes(axes[0], axes[1], axes[2])\n        t.PostMultiply()\n        t.Translate(boxCenter)\n\n    pd = transformPolyData(pd, t.GetLinearInverse())\n    obj.setPolyData(pd)\n\n    frame = obj.getChildFrame()\n    if frame:\n        frame.copyFrame(t)\n    else:\n        frame = vis.showFrame(t, obj.getProperty(\'Name\') + \' frame\', parent=obj, scale=0.2, visible=False)\n        obj.actor.SetUserTransform(t)\n\n\ndef segmentTable(polyData, searchPoint):\n    \'\'\'\n    NB: If you wish to use the table frame use segmentTableAndFrame instead\n    ##################\n    Segment a horizontal table surface (perpendicular to +Z) in the given polyData\n    Input:\n    - polyData\n    - search point on plane\n\n    Output:\n    - polyData, tablePoints, origin, normal\n    - polyData is the input polyData with a new \'dist_to_plane\' attribute.\n    \'\'\'\n    expectedNormal = np.array([0.0, 0.0, 1.0])\n    tableNormalEpsilon = 0.4\n\n    polyData = applyVoxelGrid(polyData, leafSize=0.01)\n\n    polyData, origin, normal = applyPlaneFit(polyData, expectedNormal=expectedNormal, perpendicularAxis=expectedNormal,\n                                             searchOrigin=searchPoint, searchRadius=0.3,\n                                             angleEpsilon=tableNormalEpsilon, returnOrigin=True)\n    tablePoints = thresholdPoints(polyData, \'dist_to_plane\', [-0.01, 0.01])\n\n    tablePoints = labelDistanceToPoint(tablePoints, searchPoint)\n    tablePointsClusters = extractClusters(tablePoints, minClusterSize=10, clusterTolerance=0.1)\n    tablePointsClusters.sort(key=lambda x: vtkNumpy.getNumpyFromVtk(x, \'distance_to_point\').min())\n\n    tablePoints = tablePointsClusters[0]\n\n    updatePolyData(tablePoints, \'table plane points\', parent=getDebugFolder(), visible=False)\n    updatePolyData(tablePoints, \'table points\', parent=getDebugFolder(), visible=False)\n\n    return polyData, tablePoints, origin, normal\n\n\ndef filterClusterObjects(clusters):\n    result = []\n    for cluster in clusters:\n\n        if np.abs(np.dot(cluster.axes[2], [0, 0, 1])) < 0.5:\n            continue\n\n        if cluster.dims[2] < 0.1:\n            continue\n\n        result.append(cluster)\n    return result\n\n\ndef segmentTableScene(polyData, searchPoint, filterClustering=True):\n    objectClusters, tableData = segmentTableSceneClusters(polyData, searchPoint)\n\n    clusters = [makePolyDataFields(cluster) for cluster in objectClusters]\n    clusters = [cluster for cluster in clusters if cluster is not None]\n\n    # Add an additional frame to these objects which has z-axis aligned upwards\n    # but rotated to have the x-axis facing away from the robot\n    table_axes = transformUtils.getAxesFromTransform(tableData.frame)\n    for cluster in clusters:\n        cluster_axes = transformUtils.getAxesFromTransform(cluster.frame)\n\n        zaxis = cluster_axes[2]\n        xaxis = table_axes[0]\n        yaxis = np.cross(zaxis, xaxis)\n        xaxis = np.cross(yaxis, zaxis)\n        xaxis /= np.linalg.norm(xaxis)\n        yaxis /= np.linalg.norm(yaxis)\n        orientedFrame = transformUtils.getTransformFromAxesAndOrigin(xaxis, yaxis, zaxis, cluster.frame.GetPosition())\n        cluster._add_fields(oriented_frame=orientedFrame)\n\n    if (filterClustering):\n        clusters = filterClusterObjects(clusters)\n\n    return FieldContainer(table=tableData, clusters=clusters)\n\n\ndef segmentTableSceneClusters(polyData, searchPoint, clusterInXY=False):\n    \'\'\' Given a point cloud of a table with some objects on it\n        and a point on that table\n        determine the plane of the table and\n        extract clusters above the table\n    \'\'\'\n\n    tableData, polyData = segmentTableAndFrame(polyData, searchPoint)\n\n    searchRegion = thresholdPoints(polyData, \'dist_to_plane\', [0.02, 0.5])\n    # TODO: replace with \'all points above the table\':\n    searchRegion = cropToSphere(searchRegion, tableData.frame.GetPosition(), 0.5)  # was 1.0\n\n    showFrame(tableData.frame, \'tableFrame\', visible=False, parent=getDebugFolder(), scale=0.15)\n    showPolyData(searchRegion, \'searchRegion\', color=[1, 0, 0], visible=False, parent=getDebugFolder())\n\n    objectClusters = extractClusters(searchRegion, clusterInXY, clusterTolerance=0.02, minClusterSize=10)\n\n    # print \'got %d clusters\' % len(objectClusters)\n    for i, c in enumerate(objectClusters):\n        name = ""cluster %d"" % i\n        showPolyData(c, name, color=getRandomColor(), visible=False, parent=getDebugFolder())\n\n    return objectClusters, tableData\n\n\ndef segmentTableAndFrame(polyData, searchPoint):\n    \'\'\'\n    Segment a table using a searchPoint on the table top\n    and then recover its coordinate frame, facing away from the robot\n    Objects/points on the table are ignored\n\n    Input: polyData and searchPoint on the table\n\n    Output: FieldContainer with:\n    - all relevent details about the table (only)\n\n    \'\'\'\n\n    polyData, tablePoints, _, _ = segmentTable(polyData, searchPoint)\n    tableMesh = computeDelaunay3D(tablePoints)\n\n    viewFrame = SegmentationContext.getGlobalInstance().getViewFrame()\n    viewDirection = SegmentationContext.getGlobalInstance().getViewDirection()\n    robotYaw = math.atan2(viewDirection[1], viewDirection[0]) * 180.0 / np.pi\n    linkFrame = transformUtils.frameFromPositionAndRPY(viewFrame.GetPosition(), [0, 0, robotYaw])\n\n    # Function returns corner point that is far right from the robot\n    cornerTransform, rectDepth, rectWidth, _ = findMinimumBoundingRectangle(tablePoints, linkFrame)\n    rectHeight = 0.02  # arbitrary table width\n\n    # recover mid point\n    t = transformUtils.copyFrame(cornerTransform)\n    t.PreMultiply()\n    table_center = [-rectDepth / 2, rectWidth / 2, 0]\n    t3 = transformUtils.frameFromPositionAndRPY(table_center, [0, 0, 0])\n    t.Concatenate(t3)\n\n    # Create required outputs\n    edgeLengths = [rectDepth, rectWidth, rectHeight]\n    tableXAxis, tableYAxis, tableZAxis = transformUtils.getAxesFromTransform(t)\n    axes = tableXAxis, tableYAxis, tableZAxis\n    wf = vtk.vtkOutlineSource()\n    wf.SetBounds([-rectDepth / 2, rectDepth / 2, -rectWidth / 2, rectWidth / 2, -rectHeight / 2, rectHeight / 2])\n    # wf.SetBoxTypeToOriented()\n    # cube =[0,0,0,1,0,0,0,1,0,1,1,0,0,0,1,1,0,1,0,1,1,1,1,1]\n    # wf.SetCorners(cube)\n    wireframe = wf.GetOutput()\n\n    tablePoints = transformPolyData(tablePoints, t.GetLinearInverse())\n    # wireframe = transformPolyData(wireframe, t.GetLinearInverse())\n    tableMesh = transformPolyData(tableMesh, t.GetLinearInverse())\n\n    return FieldContainer(points=tablePoints, box=wireframe, mesh=tableMesh, frame=t, dims=edgeLengths,\n                          axes=axes), polyData\n\n\ndef segmentDrillAuto(point1, polyData=None):\n    if polyData is None:\n        inputObj = om.findObjectByName(\'pointcloud snapshot\')\n        polyData = inputObj.polyData\n\n    expectedNormal = np.array([0.0, 0.0, 1.0])\n\n    polyData, origin, normal = applyPlaneFit(polyData, expectedNormal=expectedNormal, perpendicularAxis=expectedNormal,\n                                             searchOrigin=point1, searchRadius=0.4, angleEpsilon=0.2, returnOrigin=True)\n\n    tablePoints = thresholdPoints(polyData, \'dist_to_plane\', [-0.01, 0.01])\n    updatePolyData(tablePoints, \'table plane points\', parent=getDebugFolder(), visible=False)\n\n    tablePoints = labelDistanceToPoint(tablePoints, point1)\n    tablePointsClusters = extractClusters(tablePoints)\n    tablePointsClusters.sort(key=lambda x: vtkNumpy.getNumpyFromVtk(x, \'distance_to_point\').min())\n\n    tablePoints = tablePointsClusters[0]\n    updatePolyData(tablePoints, \'table points\', parent=getDebugFolder(), visible=False)\n\n    searchRegion = thresholdPoints(polyData, \'dist_to_plane\', [0.03, 0.4])\n    searchRegion = cropToSphere(searchRegion, point1, 0.30)\n    drillPoints = extractLargestCluster(searchRegion, minClusterSize=1)\n\n    # determine drill orientation (rotation about z axis)\n\n    centroids = computeCentroids(drillPoints, axis=normal)\n\n    centroidsPolyData = vtkNumpy.getVtkPolyDataFromNumpyPoints(centroids)\n    d = DebugData()\n    updatePolyData(centroidsPolyData, \'cluster centroids\', parent=getDebugFolder(), visible=False)\n\n    drillToTopPoint = np.array([-0.002904, -0.010029, 0.153182])\n\n    zaxis = normal\n    yaxis = centroids[0] - centroids[-1]\n    yaxis /= np.linalg.norm(yaxis)\n    xaxis = np.cross(yaxis, zaxis)\n    xaxis /= np.linalg.norm(xaxis)\n    yaxis = np.cross(zaxis, xaxis)\n\n    # note this hack to orient the drill correctly:\n    t = getTransformFromAxes(yaxis, -xaxis, zaxis)\n    t.PreMultiply()\n    t.Translate(-drillToTopPoint)\n    t.PostMultiply()\n    t.Translate(centroids[-1])\n\n    drillMesh = getDrillMesh()\n\n    aff = showPolyData(drillMesh, \'drill\', cls=FrameAffordanceItem, visible=True)\n    aff.actor.SetUserTransform(t)\n    showFrame(t, \'drill frame\', parent=aff, visible=False, scale=0.2).addToView(app.getDRCView())\n\n    params = getDrillAffordanceParams(origin, xaxis, yaxis, zaxis)\n    aff.setAffordanceParams(params)\n    aff.updateParamsFromActorTransform()\n    aff.addToView(app.getDRCView())\n\n\ndef segmentDrillButton(point1):\n    d = DebugData()\n    d.addSphere([0, 0, 0], radius=0.005)\n    obj = updatePolyData(d.getPolyData(), \'sensed drill button\', color=[0, 0.5, 0.5], visible=True)\n\n    # there is no orientation, but this allows the XYZ point to be queried\n    pointerTipFrame = transformUtils.frameFromPositionAndRPY(point1, [0, 0, 0])\n    obj.actor.SetUserTransform(pointerTipFrame)\n    obj.addToView(app.getDRCView())\n\n    frameObj = updateFrame(obj.actor.GetUserTransform(), \'sensed drill button frame\', parent=obj, scale=0.2,\n                           visible=False)\n    frameObj.addToView(app.getDRCView())\n\n\ndef segmentPointerTip(point1):\n    d = DebugData()\n    d.addSphere([0, 0, 0], radius=0.005)\n    obj = updatePolyData(d.getPolyData(), \'sensed pointer tip\', color=[0.5, 0.5, 0.0], visible=True)\n\n    # there is no orientation, but this allows the XYZ point to be queried\n    pointerTipFrame = transformUtils.frameFromPositionAndRPY(point1, [0, 0, 0])\n    obj.actor.SetUserTransform(pointerTipFrame)\n    obj.addToView(app.getDRCView())\n\n    frameObj = updateFrame(obj.actor.GetUserTransform(), \'sensed pointer tip frame\', parent=obj, scale=0.2,\n                           visible=False)\n    frameObj.addToView(app.getDRCView())\n\n\ndef fitGroundObject(polyData=None, expectedDimensionsMin=[0.2, 0.02], expectedDimensionsMax=[1.3, 0.1]):\n    removeGroundFunc = removeGroundSimple\n\n    polyData = polyData or getCurrentRevolutionData()\n    groundPoints, scenePoints = removeGroundFunc(polyData, groundThickness=0.02, sceneHeightFromGround=0.035)\n\n    searchRegion = thresholdPoints(scenePoints, \'dist_to_plane\', [0.05, 0.2])\n\n    clusters = extractClusters(searchRegion, clusterTolerance=0.07, minClusterSize=4)\n\n    candidates = []\n    for clusterId, cluster in enumerate(clusters):\n\n        origin, edges, _ = getOrientedBoundingBox(cluster)\n        edgeLengths = [np.linalg.norm(edge) for edge in edges[:2]]\n\n        found = (expectedDimensionsMin[0] <= edgeLengths[0] < expectedDimensionsMax[0]\n                 and expectedDimensionsMin[1] <= edgeLengths[1] < expectedDimensionsMax[1])\n\n        if not found:\n            updatePolyData(cluster, \'candidate cluster %d\' % clusterId, color=[1, 1, 0], parent=getDebugFolder(),\n                           visible=False)\n            continue\n\n        updatePolyData(cluster, \'cluster %d\' % clusterId, color=[0, 1, 0], parent=getDebugFolder(), visible=False)\n        candidates.append(cluster)\n\n    if not candidates:\n        return None\n\n    viewFrame = SegmentationContext.getGlobalInstance().getViewFrame()\n    viewOrigin = np.array(viewFrame.GetPosition())\n\n    dists = [np.linalg.norm(viewOrigin - computeCentroid(cluster)) for cluster in candidates]\n    candidates = [candidates[i] for i in np.argsort(dists)]\n\n    cluster = candidates[0]\n    obj = makePolyDataFields(cluster)\n\n    return vis.showClusterObjects([obj], parent=\'segmentation\')[0]\n\n\ndef findHorizontalSurfaces(polyData, removeGroundFirst=False, normalEstimationSearchRadius=0.05,\n                           clusterTolerance=0.025, minClusterSize=150, distanceToPlaneThreshold=0.0025,\n                           normalsDotUpRange=[0.95, 1.0], showClusters=False):\n    \'\'\'\n    Find the horizontal surfaces, tuned to work with walking terrain\n    \'\'\'\n\n    searchZ = [0.0, 2.0]\n    voxelGridLeafSize = 0.01\n    verboseFlag = False\n\n    if (removeGroundFirst):\n        groundPoints, scenePoints = removeGround(polyData, groundThickness=0.02, sceneHeightFromGround=0.05)\n        scenePoints = thresholdPoints(scenePoints, \'dist_to_plane\', searchZ)\n        updatePolyData(groundPoints, \'ground points\', parent=getDebugFolder(), visible=verboseFlag)\n    else:\n        scenePoints = polyData\n\n    if not scenePoints.GetNumberOfPoints():\n        return\n\n    f = vtk.vtkPCLNormalEstimation()\n    f.SetSearchRadius(normalEstimationSearchRadius)\n    f.SetInput(scenePoints)\n    f.SetInput(1, applyVoxelGrid(scenePoints, voxelGridLeafSize))\n\n    # Duration 0.2 sec for V1 log:\n    f.Update()\n    scenePoints = shallowCopy(f.GetOutput())\n\n    normals = vtkNumpy.getNumpyFromVtk(scenePoints, \'normals\')\n    normalsDotUp = np.abs(np.dot(normals, [0, 0, 1]))\n\n    vtkNumpy.addNumpyToVtk(scenePoints, normalsDotUp, \'normals_dot_up\')\n    surfaces = thresholdPoints(scenePoints, \'normals_dot_up\', normalsDotUpRange)\n\n    updatePolyData(scenePoints, \'scene points\', parent=getDebugFolder(), colorByName=\'normals_dot_up\',\n                   visible=verboseFlag)\n    updatePolyData(surfaces, \'surfaces points\', parent=getDebugFolder(), colorByName=\'normals_dot_up\',\n                   visible=verboseFlag)\n\n    clusters = extractClusters(surfaces, clusterTolerance=clusterTolerance, minClusterSize=minClusterSize)\n    planeClusters = []\n    clustersLarge = []\n\n    om.removeFromObjectModel(om.findObjectByName(\'surface clusters\'))\n    folder = om.getOrCreateContainer(\'surface clusters\', parentObj=getDebugFolder())\n\n    for i, cluster in enumerate(clusters):\n\n        updatePolyData(cluster, \'surface cluster %d\' % i, parent=folder, color=getRandomColor(), visible=verboseFlag)\n        planePoints, _ = applyPlaneFit(cluster, distanceToPlaneThreshold)\n        planePoints = thresholdPoints(planePoints, \'dist_to_plane\',\n                                      [-distanceToPlaneThreshold, distanceToPlaneThreshold])\n\n        if planePoints.GetNumberOfPoints() > minClusterSize:\n            clustersLarge.append(cluster)\n            obj = makePolyDataFields(planePoints)\n            if obj is not None:\n                planeClusters.append(obj)\n\n    folder = om.getOrCreateContainer(\'surface objects\', parentObj=getDebugFolder())\n    if showClusters:\n        vis.showClusterObjects(planeClusters, parent=folder)\n\n    return clustersLarge\n\n\ndef fitVerticalPosts(polyData):\n    groundPoints, scenePoints = removeGround(polyData)\n    scenePoints = thresholdPoints(scenePoints, \'dist_to_plane\', [0.1, 4.0])\n\n    if not scenePoints.GetNumberOfPoints():\n        return\n\n    scenePoints = applyVoxelGrid(scenePoints, leafSize=0.03)\n    clusters = extractClusters(scenePoints, clusterTolerance=0.15, minClusterSize=10)\n\n    def isPostCluster(cluster, lineDirection):\n\n        up = [0, 0, 1]\n        minPostLength = 1.0\n        maxRadius = 0.3\n        angle = math.degrees(\n            math.acos(np.dot(up, lineDirection) / (np.linalg.norm(up) * np.linalg.norm(lineDirection))))\n\n        if angle > 15:\n            return False\n\n        origin, edges, _ = getOrientedBoundingBox(cluster)\n        edgeLengths = [np.linalg.norm(edge) for edge in edges]\n\n        if edgeLengths[0] < minPostLength:\n            return False\n\n        # extract top half\n        zvalues = vtkNumpy.getNumpyFromVtk(cluster, \'Points\')[:, 2].copy()\n        vtkNumpy.addNumpyToVtk(cluster, zvalues, \'z\')\n\n        minZ = np.min(zvalues)\n        maxZ = np.max(zvalues)\n\n        cluster = thresholdPoints(cluster, \'z\', [(minZ + maxZ) / 2.0, maxZ])\n        origin, edges, _ = getOrientedBoundingBox(cluster)\n        edgeLengths = [np.linalg.norm(edge) for edge in edges]\n\n        if edgeLengths[1] > maxRadius or edgeLengths[2] > maxRadius:\n            return False\n\n        return True\n\n    def makeCylinderAffordance(linePoints, lineDirection, lineOrigin, postId):\n\n        pts = vtkNumpy.getNumpyFromVtk(linePoints, \'Points\')\n        dists = np.dot(pts - lineOrigin, lineDirection)\n        p1 = lineOrigin + lineDirection * np.min(dists)\n        p2 = lineOrigin + lineDirection * np.max(dists)\n\n        origin = (p1 + p2) / 2.0\n        lineLength = np.linalg.norm(p2 - p1)\n        t = transformUtils.getTransformFromOriginAndNormal(origin, lineDirection)\n        pose = transformUtils.poseFromTransform(t)\n\n        desc = dict(classname=\'CylinderAffordanceItem\', Name=\'post %d\' % postId,\n                    uuid=newUUID(), pose=pose, Radius=0.05, Length=float(lineLength), Color=[0.0, 1.0, 0.0])\n        desc[\'Collision Enabled\'] = True\n\n        return affordanceManager.newAffordanceFromDescription(desc)\n\n    rejectFolder = om.getOrCreateContainer(\'nonpost clusters\', parentObj=getDebugFolder())\n    keepFolder = om.getOrCreateContainer(\'post clusters\', parentObj=getDebugFolder())\n\n    for i, cluster in enumerate(clusters):\n\n        linePoint, lineDirection, linePoints = applyLineFit(cluster, distanceThreshold=0.1)\n        if isPostCluster(cluster, lineDirection):\n            vis.showPolyData(cluster, \'cluster %d\' % i, visible=False, color=getRandomColor(), alpha=0.5,\n                             parent=keepFolder)\n            makeCylinderAffordance(linePoints, lineDirection, linePoint, i)\n        else:\n            vis.showPolyData(cluster, \'cluster %d\' % i, visible=False, color=getRandomColor(), alpha=0.5,\n                             parent=rejectFolder)\n\n\ndef findAndFitDrillBarrel(polyData=None):\n    \'\'\' Find the horizontal surfaces\n    on the horizontal surfaces, find all the drills\n    \'\'\'\n\n    inputObj = om.findObjectByName(\'pointcloud snapshot\')\n    polyData = polyData or inputObj.polyData\n\n    groundPoints, scenePoints = removeGround(polyData, groundThickness=0.02, sceneHeightFromGround=0.50)\n\n    scenePoints = thresholdPoints(scenePoints, \'dist_to_plane\', [0.5, 1.7])\n\n    if not scenePoints.GetNumberOfPoints():\n        return\n\n    normalEstimationSearchRadius = 0.10\n\n    f = vtk.vtkPCLNormalEstimation()\n    f.SetSearchRadius(normalEstimationSearchRadius)\n    f.SetInput(scenePoints)\n    f.Update()\n    scenePoints = shallowCopy(f.GetOutput())\n\n    normals = vtkNumpy.getNumpyFromVtk(scenePoints, \'normals\')\n    normalsDotUp = np.abs(np.dot(normals, [0, 0, 1]))\n\n    vtkNumpy.addNumpyToVtk(scenePoints, normalsDotUp, \'normals_dot_up\')\n\n    surfaces = thresholdPoints(scenePoints, \'normals_dot_up\', [0.95, 1.0])\n\n    updatePolyData(groundPoints, \'ground points\', parent=getDebugFolder(), visible=False)\n    updatePolyData(scenePoints, \'scene points\', parent=getDebugFolder(), colorByName=\'normals_dot_up\', visible=False)\n    updatePolyData(surfaces, \'surfaces\', parent=getDebugFolder(), visible=False)\n\n    clusters = extractClusters(surfaces, clusterTolerance=0.15, minClusterSize=50)\n\n    fitResults = []\n\n    viewFrame = SegmentationContext.getGlobalInstance().getViewFrame()\n    forwardDirection = np.array([1.0, 0.0, 0.0])\n    viewFrame.TransformVector(forwardDirection, forwardDirection)\n    robotOrigin = viewFrame.GetPosition()\n    robotForward = forwardDirection\n\n    # print \'robot origin:\', robotOrigin\n    # print \'robot forward:\', robotForward\n    centroid = []\n\n    for clusterId, cluster in enumerate(clusters):\n        clusterObj = updatePolyData(cluster, \'surface cluster %d\' % clusterId, color=[1, 1, 0], parent=getDebugFolder(),\n                                    visible=False)\n\n        origin, edges, _ = getOrientedBoundingBox(cluster)\n        edgeLengths = [np.linalg.norm(edge) for edge in edges[:2]]\n\n        skipCluster = False\n        for edgeLength in edgeLengths:\n            # print \'cluster %d edge length: %f\' % (clusterId, edgeLength)\n            if edgeLength < 0.35 or edgeLength > 0.75:\n                skipCluster = True\n\n        if skipCluster:\n            continue\n\n        clusterObj.setSolidColor([0, 0, 1])\n        centroid = np.average(vtkNumpy.getNumpyFromVtk(cluster, \'Points\'), axis=0)\n\n        try:\n            drillFrame = segmentDrillBarrelFrame(centroid, polyData=scenePoints, forwardDirection=robotForward)\n            if drillFrame is not None:\n                fitResults.append((clusterObj, drillFrame))\n        except:\n            print traceback.format_exc()\n            print \'fit drill failed for cluster:\', clusterId\n\n    if not fitResults:\n        return\n\n    sortFittedDrills(fitResults, robotOrigin, robotForward)\n\n    return centroid\n\n\ndef sortFittedDrills(fitResults, robotOrigin, robotForward):\n    angleToFitResults = []\n\n    for fitResult in fitResults:\n        cluster, drillFrame = fitResult\n        drillOrigin = np.array(drillFrame.GetPosition())\n        angleToDrill = np.abs(computeSignedAngleBetweenVectors(robotForward, drillOrigin - robotOrigin, [0, 0, 1]))\n        angleToFitResults.append((angleToDrill, cluster, drillFrame))\n        # print \'angle to candidate drill:\', angleToDrill\n\n    angleToFitResults.sort(key=lambda x: x[0])\n\n    # print \'using drill at angle:\', angleToFitResults[0][0]\n\n    drillMesh = getDrillBarrelMesh()\n\n    for i, fitResult in enumerate(angleToFitResults):\n\n        angleToDrill, cluster, drillFrame = fitResult\n\n        if i == 0:\n\n            drill = om.findObjectByName(\'drill\')\n            drill = updatePolyData(drillMesh, \'drill\', color=[0, 1, 0], cls=FrameAffordanceItem, visible=True)\n            drillFrame = updateFrame(drillFrame, \'drill frame\', parent=drill, visible=False)\n            drill.actor.SetUserTransform(drillFrame.transform)\n\n            drill.setAffordanceParams(dict(otdf_type=\'dewalt_button\', friendly_name=\'dewalt_button\'))\n            drill.updateParamsFromActorTransform()\n\n            drill.setSolidColor([0, 1, 0])\n            # cluster.setProperty(\'Visible\', True)\n\n        else:\n\n            drill = showPolyData(drillMesh, \'drill candidate\', color=[1, 0, 0], visible=False, parent=getDebugFolder())\n            drill.actor.SetUserTransform(drillFrame)\n            om.addToObjectModel(drill, parentObj=getDebugFolder())\n\n\ndef computeSignedAngleBetweenVectors(v1, v2, perpendicularVector):\n    \'\'\'\n    Computes the signed angle between two vectors in 3d, given a perpendicular vector\n    to determine sign.  Result returned is radians.\n    \'\'\'\n    v1 = np.array(v1, dtype=float)\n    v2 = np.array(v2, dtype=float)\n    perpendicularVector = np.array(perpendicularVector, dtype=float)\n    v1 /= np.linalg.norm(v1)\n    v2 /= np.linalg.norm(v2)\n    perpendicularVector /= np.linalg.norm(perpendicularVector)\n    return math.atan2(np.dot(perpendicularVector, np.cross(v1, v2)), np.dot(v1, v2))\n\n\ndef segmentDrillBarrelFrame(point1, polyData, forwardDirection):\n    tableClusterSearchRadius = 0.4\n    drillClusterSearchRadius = 0.5  # 0.3\n\n    expectedNormal = np.array([0.0, 0.0, 1.0])\n\n    if not polyData.GetNumberOfPoints():\n        return\n\n    polyData, plane_origin, plane_normal = applyPlaneFit(polyData, expectedNormal=expectedNormal,\n                                                         perpendicularAxis=expectedNormal, searchOrigin=point1,\n                                                         searchRadius=tableClusterSearchRadius, angleEpsilon=0.2,\n                                                         returnOrigin=True)\n\n    if not polyData.GetNumberOfPoints():\n        return\n\n    tablePoints = thresholdPoints(polyData, \'dist_to_plane\', [-0.01, 0.01])\n    updatePolyData(tablePoints, \'table plane points\', parent=getDebugFolder(), visible=False)\n\n    tablePoints = labelDistanceToPoint(tablePoints, point1)\n    tablePointsClusters = extractClusters(tablePoints)\n    tablePointsClusters.sort(key=lambda x: vtkNumpy.getNumpyFromVtk(x, \'distance_to_point\').min())\n\n    if not tablePointsClusters:\n        return\n\n    tablePoints = tablePointsClusters[0]\n    updatePolyData(tablePoints, \'table points\', parent=getDebugFolder(), visible=False)\n\n    searchRegion = thresholdPoints(polyData, \'dist_to_plane\', [0.02, 0.3])\n    if not searchRegion.GetNumberOfPoints():\n        return\n\n    searchRegion = cropToSphere(searchRegion, point1, drillClusterSearchRadius)\n    # drillPoints = extractLargestCluster(searchRegion, minClusterSize=1)\n\n    t = fitDrillBarrel(searchRegion, forwardDirection, plane_origin, plane_normal)\n    return t\n\n\ndef segmentDrillBarrel(point1):\n    inputObj = om.findObjectByName(\'pointcloud snapshot\')\n    polyData = inputObj.polyData\n\n    forwardDirection = -np.array(getCurrentView().camera().GetViewPlaneNormal())\n\n    t = segmentDrillBarrel(point1, polyData, forwardDirection)\n    assert t is not None\n\n    drillMesh = getDrillBarrelMesh()\n\n    aff = showPolyData(drillMesh, \'drill\', visible=True)\n    aff.addToView(app.getDRCView())\n\n    aff.actor.SetUserTransform(t)\n    drillFrame = showFrame(t, \'drill frame\', parent=aff, visible=False)\n    drillFrame.addToView(app.getDRCView())\n    return aff, drillFrame\n\n\ndef segmentDrillAlignedWithTable(point, polyData=None):\n    \'\'\'\n    Yet Another Drill Fitting Algorithm [tm]\n    This one fits the button drill assuming its on the table\n    and aligned with the table frame (because the button drill orientation is difficult to find)\n    Table must have long side facing robot\n    \'\'\'\n    inputObj = om.findObjectByName(\'pointcloud snapshot\')\n    polyData = polyData or inputObj.polyData\n\n    # segment the table and recover the precise up direction normal:\n    polyDataOut, tablePoints, origin, normal = segmentTable(polyData, point)\n    # print origin # this origin is bunk\n    # tableCentroid = computeCentroid(tablePoints)\n\n    # get the bounding box edges\n    OBBorigin, edges, _ = getOrientedBoundingBox(tablePoints)\n    # print ""OBB out""\n    # print OBBorigin\n    # print edges\n    edgeLengths = np.array([np.linalg.norm(edge) for edge in edges])\n    axes = [edge / np.linalg.norm(edge) for edge in edges]\n    # print edgeLengths\n    # print axes\n\n    # check which direction the robot is facing and flip x-axis of table if necessary\n    viewDirection = SegmentationContext.getGlobalInstance().getViewDirection()\n    # print ""main axes"", axes[1]\n    # print ""viewDirection"", viewDirection\n    # dp = np.dot(axes[1], viewDirection)\n    # print dp\n\n    if np.dot(axes[1], viewDirection) < 0:\n        # print ""flip the x-direction""\n        axes[1] = -axes[1]\n\n    # define the x-axis to be along the 2nd largest edge\n    xaxis = axes[1]\n    xaxis = np.array(xaxis)\n    zaxis = np.array(normal)\n    yaxis = np.cross(zaxis, xaxis)\n    yaxis /= np.linalg.norm(yaxis)\n    xaxis = np.cross(yaxis, zaxis)\n    tableOrientation = transformUtils.getTransformFromAxes(xaxis, yaxis, zaxis)\n\n    # tableTransform = transformUtils.frameFromPositionAndRPY( tableCentroid , tableOrientation.GetOrientation() )\n    # updateFrame(tableTransform, \'table frame [z up, x away face]\', parent=""segmentation"", visible=True).addToView(app.getDRCView())\n\n    data = segmentTableScene(polyData, point)\n    # vis.showClusterObjects(data.clusters + [data.table], parent=\'segmentation\')\n\n    # crude use of the table frame to determine the frame of the drill on the table\n    # t2 = transformUtils.frameFromPositionAndRPY([0,0,0], [180, 0 , 90] )\n    # drillOrientationTransform = transformUtils.copyFrame( om.findObjectByName(\'object 1 frame\').transform )\n    # drillOrientationTransform.PreMultiply()\n    # drillOrientationTransform.Concatenate(t2)\n    # vis.updateFrame(t, \'drillOrientationTransform\',visible=True)\n\n    # table_xaxis, table_yaxis, table_zaxis = transformUtils.getAxesFromTransform( data.table.frame )\n    # drillOrientation = transformUtils.orientationFromAxes( table_yaxis, table_xaxis,  -1*np.array( table_zaxis) )\n    drillTransform = transformUtils.frameFromPositionAndRPY(data.clusters[0].frame.GetPosition(),\n                                                            tableOrientation.GetOrientation())\n\n    drillMesh = getDrillMesh()\n\n    drill = om.findObjectByName(\'drill\')\n    om.removeFromObjectModel(drill)\n\n    aff = showPolyData(drillMesh, \'drill\', color=[0.0, 1.0, 0.0], cls=FrameAffordanceItem, visible=True)\n    aff.actor.SetUserTransform(drillTransform)\n    aff.addToView(app.getDRCView())\n\n    frameObj = updateFrame(drillTransform, \'drill frame\', parent=aff, scale=0.2, visible=False)\n    frameObj.addToView(app.getDRCView())\n\n    params = getDrillAffordanceParams(np.array(drillTransform.GetPosition()), [1, 0, 0], [0, 1, 0], [0, 0, 1],\n                                      drillType=""dewalt_button"")\n    aff.setAffordanceParams(params)\n\n\ndef segmentDrillInHand(p1, p2):\n    inputObj = om.findObjectByName(\'pointcloud snapshot\')\n    polyData = inputObj.polyData\n\n    distanceToLineThreshold = 0.05\n\n    polyData = labelDistanceToLine(polyData, p1, p2)\n    polyData = thresholdPoints(polyData, \'distance_to_line\', [0.0, distanceToLineThreshold])\n\n    lineSegment = p2 - p1\n    lineLength = np.linalg.norm(lineSegment)\n\n    cropped, polyData = cropToPlane(polyData, p1, lineSegment / lineLength, [-0.03, lineLength + 0.03])\n\n    updatePolyData(cropped, \'drill cluster\', parent=getDebugFolder(), visible=False)\n\n    drillPoints = cropped\n    normal = lineSegment / lineLength\n\n    centroids = computeCentroids(drillPoints, axis=normal)\n\n    centroidsPolyData = vtkNumpy.getVtkPolyDataFromNumpyPoints(centroids)\n    d = DebugData()\n    updatePolyData(centroidsPolyData, \'cluster centroids\', parent=getDebugFolder(), visible=False)\n\n    drillToTopPoint = np.array([-0.002904, -0.010029, 0.153182])\n\n    zaxis = normal\n    yaxis = centroids[0] - centroids[-1]\n    yaxis /= np.linalg.norm(yaxis)\n    xaxis = np.cross(yaxis, zaxis)\n    xaxis /= np.linalg.norm(xaxis)\n    yaxis = np.cross(zaxis, xaxis)\n\n    t = getTransformFromAxes(xaxis, yaxis, zaxis)\n    t.PreMultiply()\n    t.Translate(-drillToTopPoint)\n    t.PostMultiply()\n    t.Translate(p2)\n\n    drillMesh = getDrillMesh()\n\n    aff = showPolyData(drillMesh, \'drill\', cls=FrameAffordanceItem, visible=True)\n    aff.actor.SetUserTransform(t)\n    showFrame(t, \'drill frame\', parent=aff, visible=False).addToView(app.getDRCView())\n\n    params = getDrillAffordanceParams(np.array(t.GetPosition()), xaxis, yaxis, zaxis)\n    aff.setAffordanceParams(params)\n    aff.updateParamsFromActorTransform()\n    aff.addToView(app.getDRCView())\n\n\ndef addDrillAffordance():\n    drillMesh = getDrillMesh()\n\n    aff = showPolyData(drillMesh, \'drill\', cls=FrameAffordanceItem, visible=True)\n    t = vtk.vtkTransform()\n    t.PostMultiply()\n    aff.actor.SetUserTransform(t)\n    showFrame(t, \'drill frame\', parent=aff, visible=False).addToView(app.getDRCView())\n\n    params = getDrillAffordanceParams(np.array(t.GetPosition()), [1, 0, 0], [0, 1, 0], [0, 0, 1])\n    aff.setAffordanceParams(params)\n    aff.updateParamsFromActorTransform()\n    aff.addToView(app.getDRCView())\n    return aff\n\n\ndef getLinkFrame(linkName):\n    robotStateModel = om.findObjectByName(\'robot state model\')\n    assert robotStateModel\n    t = vtk.vtkTransform()\n    robotStateModel.model.getLinkToWorld(linkName, t)\n    return t\n\n\ndef getDrillInHandOffset(zRotation=0.0, zTranslation=0.0, xTranslation=0.0, yTranslation=0.0, flip=False):\n    drillOffset = vtk.vtkTransform()\n    drillOffset.PostMultiply()\n    if flip:\n        drillOffset.RotateY(180)\n    drillOffset.RotateZ(zRotation)\n    drillOffset.RotateY(-90)\n    # drillOffset.Translate(0, 0.09, zTranslation - 0.015)\n    # drillOffset.Translate(zTranslation - 0.015, 0.035 + xTranslation, 0.0)\n    drillOffset.Translate(zTranslation, xTranslation, 0.0 + yTranslation)\n    return drillOffset\n\n\ndef moveDrillToHand(drillOffset, hand=\'right\'):\n    drill = om.findObjectByName(\'drill\')\n    if not drill:\n        drill = addDrillAffordance()\n\n    assert hand in (\'right\', \'left\')\n    drillTransform = drill.actor.GetUserTransform()\n    rightBaseLink = getLinkFrame(\'%s_hand_face\' % hand[0])\n    drillTransform.PostMultiply()\n    drillTransform.Identity()\n    drillTransform.Concatenate(drillOffset)\n    drillTransform.Concatenate(rightBaseLink)\n    drill._renderAllViews()\n\n\nclass PointPicker(TimerCallback):\n    def __init__(self, numberOfPoints=3):\n        TimerCallback.__init__(self)\n        self.targetFps = 30\n        self.enabled = False\n        self.numberOfPoints = numberOfPoints\n        self.annotationObj = None\n        self.drawLines = True\n        self.clear()\n\n    def clear(self):\n        self.points = [None for i in xrange(self.numberOfPoints)]\n        self.hoverPos = None\n        self.annotationFunc = None\n        self.lastMovePos = [0, 0]\n\n    def onMouseMove(self, displayPoint, modifiers=None):\n        self.lastMovePos = displayPoint\n\n    def onMousePress(self, displayPoint, modifiers=None):\n\n        # print \'mouse press:\', modifiers\n        # if not modifiers:\n        #    return\n\n        for i in xrange(self.numberOfPoints):\n            if self.points[i] is None:\n                self.points[i] = self.hoverPos\n                break\n\n        if self.points[-1] is not None:\n            self.finish()\n\n    def finish(self):\n\n        self.enabled = False\n        om.removeFromObjectModel(self.annotationObj)\n\n        points = [p.copy() for p in self.points]\n        if self.annotationFunc is not None:\n            self.annotationFunc(*points)\n\n        removeViewPicker(self)\n\n    def handleRelease(self, displayPoint):\n        pass\n\n    def draw(self):\n\n        d = DebugData()\n\n        points = [p if p is not None else self.hoverPos for p in self.points]\n\n        # draw points\n        for p in points:\n            if p is not None:\n                d.addSphere(p, radius=0.01)\n\n        if self.drawLines:\n            # draw lines\n            for a, b in zip(points, points[1:]):\n                if b is not None:\n                    d.addLine(a, b)\n\n            # connect end points\n            if points[-1] is not None:\n                d.addLine(points[0], points[-1])\n\n        self.annotationObj = updatePolyData(d.getPolyData(), \'annotation\', parent=getDebugFolder())\n        self.annotationObj.setProperty(\'Color\', QtGui.QColor(0, 255, 0))\n        self.annotationObj.actor.SetPickable(False)\n\n    def tick(self):\n\n        if not self.enabled:\n            return\n\n        if not om.findObjectByName(\'pointcloud snapshot\'):\n            self.annotationFunc = None\n            self.finish()\n            return\n\n        pickedPointFields = pickPoint(self.lastMovePos, getSegmentationView(), obj=\'pointcloud snapshot\')\n        self.hoverPos = pickedPointFields.pickedPoint\n        self.draw()\n\n\nclass LineDraw(TimerCallback):\n    def __init__(self, view):\n        TimerCallback.__init__(self)\n        self.targetFps = 30\n        self.enabled = False\n        self.view = view\n        self.renderer = view.renderer()\n        self.line = vtk.vtkLeaderActor2D()\n        self.line.SetArrowPlacementToNone()\n        self.line.GetPositionCoordinate().SetCoordinateSystemToViewport()\n        self.line.GetPosition2Coordinate().SetCoordinateSystemToViewport()\n        self.line.GetProperty().SetLineWidth(4)\n        self.line.SetPosition(0, 0)\n        self.line.SetPosition2(0, 0)\n        self.clear()\n\n    def clear(self):\n        self.p1 = None\n        self.p2 = None\n        self.annotationFunc = None\n        self.lastMovePos = [0, 0]\n        self.renderer.RemoveActor2D(self.line)\n\n    def onMouseMove(self, displayPoint, modifiers=None):\n        self.lastMovePos = displayPoint\n\n    def onMousePress(self, displayPoint, modifiers=None):\n\n        if self.p1 is None:\n            self.p1 = list(self.lastMovePos)\n            if self.p1 is not None:\n                self.renderer.AddActor2D(self.line)\n        else:\n            self.p2 = self.lastMovePos\n            self.finish()\n\n    def finish(self):\n\n        self.enabled = False\n        self.renderer.RemoveActor2D(self.line)\n        if self.annotationFunc is not None:\n            self.annotationFunc(self.p1, self.p2)\n\n    def handleRelease(self, displayPoint):\n        pass\n\n    def tick(self):\n\n        if not self.enabled:\n            return\n\n        if self.p1:\n            self.line.SetPosition(self.p1)\n            self.line.SetPosition2(self.lastMovePos)\n            self.view.render()\n\n\nviewPickers = []\n\n\ndef addViewPicker(picker):\n    global viewPickers\n    viewPickers.append(picker)\n\n\ndef removeViewPicker(picker):\n    global viewPickers\n    viewPickers.remove(picker)\n\n\ndef distanceToLine(x0, x1, x2):\n    numerator = np.sqrt(np.sum(np.cross((x0 - x1), (x0 - x2)) ** 2))\n    denom = np.linalg.norm(x2 - x1)\n    return numerator / denom\n\n\ndef labelDistanceToLine(polyData, linePoint1, linePoint2, resultArrayName=\'distance_to_line\'):\n    x0 = vtkNumpy.getNumpyFromVtk(polyData, \'Points\')\n    x1 = np.array(linePoint1)\n    x2 = np.array(linePoint2)\n\n    numerator = np.sqrt(np.sum(np.cross((x0 - x1), (x0 - x2)) ** 2, axis=1))\n    denom = np.linalg.norm(x2 - x1)\n\n    dists = numerator / denom\n\n    polyData = shallowCopy(polyData)\n    vtkNumpy.addNumpyToVtk(polyData, dists, resultArrayName)\n    return polyData\n\n\ndef labelDistanceToPoint(polyData, point, resultArrayName=\'distance_to_point\'):\n    assert polyData.GetNumberOfPoints()\n    points = vtkNumpy.getNumpyFromVtk(polyData, \'Points\')\n    points = points - point\n    dists = np.sqrt(np.sum(points ** 2, axis=1))\n    polyData = shallowCopy(polyData)\n    vtkNumpy.addNumpyToVtk(polyData, dists, resultArrayName)\n    return polyData\n\n\ndef getPlaneEquationFromPolyData(polyData, expectedNormal):\n    _, origin, normal = applyPlaneFit(polyData, expectedNormal=expectedNormal, returnOrigin=True)\n    return origin, normal, np.hstack((normal, [np.dot(origin, normal)]))\n\n\ndef computeEdge(polyData, edgeAxis, perpAxis, binWidth=0.03):\n    polyData = labelPointDistanceAlongAxis(polyData, edgeAxis, resultArrayName=\'dist_along_edge\')\n    polyData = labelPointDistanceAlongAxis(polyData, perpAxis, resultArrayName=\'dist_perp_to_edge\')\n\n    polyData, bins = binByScalar(polyData, \'dist_along_edge\', binWidth)\n    points = vtkNumpy.getNumpyFromVtk(polyData, \'Points\')\n    binLabels = vtkNumpy.getNumpyFromVtk(polyData, \'bin_labels\')\n    distToEdge = vtkNumpy.getNumpyFromVtk(polyData, \'dist_perp_to_edge\')\n\n    numberOfBins = len(bins) - 1\n    edgePoints = []\n    for i in xrange(numberOfBins):\n        binPoints = points[binLabels == i]\n        binDists = distToEdge[binLabels == i]\n        if len(binDists):\n            edgePoints.append(binPoints[binDists.argmax()])\n\n    return np.array(edgePoints)\n\n\ndef computeCentroids(polyData, axis, binWidth=0.025):\n    polyData = labelPointDistanceAlongAxis(polyData, axis, resultArrayName=\'dist_along_axis\')\n\n    polyData, bins = binByScalar(polyData, \'dist_along_axis\', binWidth)\n    points = vtkNumpy.getNumpyFromVtk(polyData, \'Points\')\n    binLabels = vtkNumpy.getNumpyFromVtk(polyData, \'bin_labels\')\n\n    numberOfBins = len(bins) - 1\n    centroids = []\n    for i in xrange(numberOfBins):\n        binPoints = points[binLabels == i]\n\n        if len(binPoints):\n            centroids.append(np.average(binPoints, axis=0))\n\n    return np.array(centroids)\n\n\ndef computePointCountsAlongAxis(polyData, axis, binWidth=0.025):\n    polyData = labelPointDistanceAlongAxis(polyData, axis, resultArrayName=\'dist_along_axis\')\n\n    polyData, bins = binByScalar(polyData, \'dist_along_axis\', binWidth)\n    points = vtkNumpy.getNumpyFromVtk(polyData, \'Points\')\n    binLabels = vtkNumpy.getNumpyFromVtk(polyData, \'bin_labels\')\n\n    numberOfBins = len(bins) - 1\n    binCount = []\n    for i in xrange(numberOfBins):\n        binPoints = points[binLabels == i]\n        binCount.append(len(binPoints))\n\n    return np.array(binCount)\n\n\ndef binByScalar(lidarData, scalarArrayName, binWidth, binLabelsArrayName=\'bin_labels\'):\n    \'\'\'\n    Gets the array with name scalarArrayName from lidarData.\n    Computes bins by dividing the scalar array into bins of size binWidth.\n    Adds a new label array to the lidar points identifying which bin the point belongs to,\n    where the first bin is labeled with 0.\n    Returns the new, labeled lidar data and the bins.\n    The bins are an array where each value represents a bin edge.\n    \'\'\'\n\n    scalars = vtkNumpy.getNumpyFromVtk(lidarData, scalarArrayName)\n    bins = np.arange(scalars.min(), scalars.max() + binWidth, binWidth)\n    binLabels = np.digitize(scalars, bins) - 1\n    assert (len(binLabels) == len(scalars))\n    newData = shallowCopy(lidarData)\n    vtkNumpy.addNumpyToVtk(newData, binLabels, binLabelsArrayName)\n    return newData, bins\n\n\ndef showObbs(polyData):\n    labelsArrayName = \'cluster_labels\'\n    assert polyData.GetPointData().GetArray(labelsArrayName)\n\n    f = vtk.vtkAnnotateOBBs()\n    f.SetInputArrayToProcess(0, 0, 0, vtk.vtkDataObject.FIELD_ASSOCIATION_POINTS, labelsArrayName)\n    f.SetInput(polyData)\n    f.Update()\n    showPolyData(f.GetOutput(), \'bboxes\')\n\n\ndef getOrientedBoundingBox(polyData):\n    \'\'\'\n    returns origin, edges, and outline wireframe\n    \'\'\'\n    nPoints = polyData.GetNumberOfPoints()\n    assert nPoints\n    polyData = shallowCopy(polyData)\n\n    labelsArrayName = \'bbox_labels\'\n    labels = np.ones(nPoints)\n    vtkNumpy.addNumpyToVtk(polyData, labels, labelsArrayName)\n\n    f = vtk.vtkAnnotateOBBs()\n    f.SetInputArrayToProcess(0, 0, 0, vtk.vtkDataObject.FIELD_ASSOCIATION_POINTS, labelsArrayName)\n    f.SetInput(polyData)\n    f.Update()\n\n    assert f.GetNumberOfBoundingBoxes() == 1\n\n    origin = np.zeros(3)\n    edges = [np.zeros(3) for i in xrange(3)]\n\n    f.GetBoundingBoxOrigin(0, origin)\n    for i in xrange(3):\n        f.GetBoundingBoxEdge(0, i, edges[i])\n\n    return origin, edges, shallowCopy(f.GetOutput())\n\n\ndef segmentBlockByAnnotation(blockDimensions, p1, p2, p3):\n    segmentationObj = om.findObjectByName(\'pointcloud snapshot\')\n    segmentationObj.mapper.ScalarVisibilityOff()\n    segmentationObj.setProperty(\'Point Size\', 2)\n    segmentationObj.setProperty(\'Alpha\', 0.8)\n\n    # constraint z to lie in plane\n    # p1[2] = p2[2] = p3[2] = max(p1[2], p2[2], p3[2])\n\n    zedge = p2 - p1\n    zaxis = zedge / np.linalg.norm(zedge)\n\n    # xwidth = distanceToLine(p3, p1, p2)\n\n    # expected dimensions\n    xwidth, ywidth = blockDimensions\n\n    zwidth = np.linalg.norm(zedge)\n\n    yaxis = np.cross(p2 - p1, p3 - p1)\n    yaxis = yaxis / np.linalg.norm(yaxis)\n\n    xaxis = np.cross(yaxis, zaxis)\n\n    # reorient axes\n    viewPlaneNormal = getSegmentationView().camera().GetViewPlaneNormal()\n    if np.dot(yaxis, viewPlaneNormal) < 0:\n        yaxis *= -1\n\n    if np.dot(xaxis, p3 - p1) < 0:\n        xaxis *= -1\n\n    # make right handed\n    zaxis = np.cross(xaxis, yaxis)\n\n    origin = ((p1 + p2) / 2.0) + xaxis * xwidth / 2.0 + yaxis * ywidth / 2.0\n\n    d = DebugData()\n    d.addSphere(origin, radius=0.01)\n    d.addLine(origin - xaxis * xwidth / 2.0, origin + xaxis * xwidth / 2.0)\n    d.addLine(origin - yaxis * ywidth / 2.0, origin + yaxis * ywidth / 2.0)\n    d.addLine(origin - zaxis * zwidth / 2.0, origin + zaxis * zwidth / 2.0)\n    obj = updatePolyData(d.getPolyData(), \'block axes\')\n    obj.setProperty(\'Color\', QtGui.QColor(255, 255, 0))\n    obj.setProperty(\'Visible\', False)\n    om.findObjectByName(\'annotation\').setProperty(\'Visible\', False)\n\n    cube = vtk.vtkCubeSource()\n    cube.SetXLength(xwidth)\n    cube.SetYLength(ywidth)\n    cube.SetZLength(zwidth)\n    cube.Update()\n    cube = shallowCopy(cube.GetOutput())\n\n    t = getTransformFromAxes(xaxis, yaxis, zaxis)\n    t.PostMultiply()\n    t.Translate(origin)\n\n    obj = updatePolyData(cube, \'block affordance\', cls=BlockAffordanceItem, parent=\'affordances\')\n    obj.actor.SetUserTransform(t)\n\n    obj.addToView(app.getDRCView())\n\n    params = dict(origin=origin, xwidth=xwidth, ywidth=ywidth, zwidth=zwidth, xaxis=xaxis, yaxis=yaxis, zaxis=zaxis)\n    obj.setAffordanceParams(params)\n    obj.updateParamsFromActorTransform()\n\n\n####\n# debrs task ground frame\n\ndef getBoardCorners(params):\n    axes = [np.array(params[axis]) for axis in [\'xaxis\', \'yaxis\', \'zaxis\']]\n    widths = [np.array(params[axis]) / 2.0 for axis in [\'xwidth\', \'ywidth\', \'zwidth\']]\n    edges = [axes[i] * widths[i] for i in xrange(3)]\n    origin = np.array(params[\'origin\'])\n    return [\n        origin + edges[0] + edges[1] + edges[2],\n        origin - edges[0] + edges[1] + edges[2],\n        origin - edges[0] - edges[1] + edges[2],\n        origin + edges[0] - edges[1] + edges[2],\n        origin + edges[0] + edges[1] - edges[2],\n        origin - edges[0] + edges[1] - edges[2],\n        origin - edges[0] - edges[1] - edges[2],\n        origin + edges[0] - edges[1] - edges[2],\n    ]\n\n\ndef getPointDistances(target, points):\n    return np.array([np.linalg.norm(target - p) for p in points])\n\n\ndef computeClosestCorner(aff, referenceFrame):\n    corners = getBoardCorners(aff.params)\n    dists = getPointDistances(np.array(referenceFrame.GetPosition()), corners)\n    return corners[dists.argmin()]\n\n\ndef computeGroundFrame(aff, referenceFrame):\n    refAxis = [0.0, -1.0, 0.0]\n    referenceFrame.TransformVector(refAxis, refAxis)\n\n    refAxis = np.array(refAxis)\n\n    axes = [np.array(aff.params[axis]) for axis in [\'xaxis\', \'yaxis\', \'zaxis\']]\n    axisProjections = np.array([np.abs(np.dot(axis, refAxis)) for axis in axes])\n    boardAxis = axes[axisProjections.argmax()]\n    if np.dot(boardAxis, refAxis) < 0:\n        boardAxis = -boardAxis\n\n    xaxis = boardAxis\n    zaxis = np.array([0.0, 0.0, 1.0])\n    yaxis = np.cross(zaxis, xaxis)\n    yaxis /= np.linalg.norm(yaxis)\n    xaxis = np.cross(yaxis, zaxis)\n    closestCorner = computeClosestCorner(aff, referenceFrame)\n    groundFrame = getTransformFromAxes(xaxis, yaxis, zaxis)\n    groundFrame.PostMultiply()\n    groundFrame.Translate(closestCorner[0], closestCorner[1], 0.0)\n    return groundFrame\n\n\ndef computeCornerFrame(aff, referenceFrame):\n    refAxis = [0.0, -1.0, 0.0]\n    referenceFrame.TransformVector(refAxis, refAxis)\n\n    refAxis = np.array(refAxis)\n\n    axes = [np.array(aff.params[axis]) for axis in [\'xaxis\', \'yaxis\', \'zaxis\']]\n    edgeLengths = [edgeLength for edgeLength in [\'xwidth\', \'ywidth\', \'zwidth\']]\n\n    axisProjections = np.array([np.abs(np.dot(axis, refAxis)) for axis in axes])\n    boardAxis = axes[axisProjections.argmax()]\n    if np.dot(boardAxis, refAxis) < 0:\n        boardAxis = -boardAxis\n\n    longAxis = axes[np.argmax(edgeLengths)]\n\n    xaxis = boardAxis\n    yaxis = axes[2]\n    zaxis = np.cross(xaxis, yaxis)\n\n    closestCorner = computeClosestCorner(aff, referenceFrame)\n    cornerFrame = getTransformFromAxes(xaxis, yaxis, zaxis)\n    cornerFrame.PostMultiply()\n    cornerFrame.Translate(closestCorner)\n    return cornerFrame\n\n\ndef createBlockAffordance(origin, xaxis, yaxis, zaxis, xwidth, ywidth, zwidth, name, parent=\'affordances\'):\n    t = getTransformFromAxes(xaxis, yaxis, zaxis)\n    t.PostMultiply()\n    t.Translate(origin)\n\n    obj = BoxAffordanceItem(name, view=app.getCurrentRenderView())\n    obj.setProperty(\'Dimensions\', [float(v) for v in [xwidth, ywidth, zwidth]])\n    obj.actor.SetUserTransform(t)\n\n    om.addToObjectModel(obj, parentObj=om.getOrCreateContainer(parent))\n    frameObj = vis.showFrame(t, name + \' frame\', scale=0.2, visible=False, parent=obj)\n\n    obj.addToView(app.getDRCView())\n    frameObj.addToView(app.getDRCView())\n\n    affordanceManager.registerAffordance(obj)\n    return obj\n\n\ndef segmentBlockByTopPlane(polyData, blockDimensions, expectedNormal, expectedXAxis, edgeSign=1,\n                           name=\'block affordance\'):\n    polyData, planeOrigin, normal = applyPlaneFit(polyData, distanceThreshold=0.05, expectedNormal=expectedNormal,\n                                                  returnOrigin=True)\n\n    _, lineDirection, _ = applyLineFit(polyData)\n\n    zaxis = lineDirection\n    yaxis = normal\n    xaxis = np.cross(yaxis, zaxis)\n\n    if np.dot(xaxis, expectedXAxis) < 0:\n        xaxis *= -1\n\n    # make right handed\n    zaxis = np.cross(xaxis, yaxis)\n    xaxis /= np.linalg.norm(xaxis)\n    yaxis /= np.linalg.norm(yaxis)\n    zaxis /= np.linalg.norm(zaxis)\n\n    expectedXAxis = np.array(xaxis)\n\n    edgePoints = computeEdge(polyData, zaxis, xaxis * edgeSign)\n    edgePoints = vtkNumpy.getVtkPolyDataFromNumpyPoints(edgePoints)\n\n    d = DebugData()\n    obj = updatePolyData(edgePoints, \'edge points\', parent=getDebugFolder(), visible=False)\n\n    linePoint, lineDirection, _ = applyLineFit(edgePoints)\n    zaxis = lineDirection\n    xaxis = np.cross(yaxis, zaxis)\n\n    if np.dot(xaxis, expectedXAxis) < 0:\n        xaxis *= -1\n\n    # make right handed\n    zaxis = np.cross(xaxis, yaxis)\n    xaxis /= np.linalg.norm(xaxis)\n    yaxis /= np.linalg.norm(yaxis)\n    zaxis /= np.linalg.norm(zaxis)\n\n    polyData = labelPointDistanceAlongAxis(polyData, xaxis, resultArrayName=\'dist_along_line\')\n    pts = vtkNumpy.getNumpyFromVtk(polyData, \'Points\')\n\n    dists = np.dot(pts - linePoint, zaxis)\n\n    p1 = linePoint + zaxis * np.min(dists)\n    p2 = linePoint + zaxis * np.max(dists)\n\n    p1 = projectPointToPlane(p1, planeOrigin, normal)\n    p2 = projectPointToPlane(p2, planeOrigin, normal)\n\n    xwidth, ywidth = blockDimensions\n    zwidth = np.linalg.norm(p2 - p1)\n\n    origin = p1 - edgeSign * xaxis * xwidth / 2.0 - yaxis * ywidth / 2.0 + zaxis * zwidth / 2.0\n\n    d = DebugData()\n\n    # d.addSphere(linePoint, radius=0.02)\n    # d.addLine(linePoint, linePoint + yaxis*ywidth)\n    # d.addLine(linePoint, linePoint + xaxis*xwidth)\n    # d.addLine(linePoint, linePoint + zaxis*zwidth)\n\n\n    d.addSphere(p1, radius=0.01)\n    d.addSphere(p2, radius=0.01)\n    d.addLine(p1, p2)\n\n    d.addSphere(origin, radius=0.01)\n    # d.addLine(origin - xaxis*xwidth/2.0, origin + xaxis*xwidth/2.0)\n    # d.addLine(origin - yaxis*ywidth/2.0, origin + yaxis*ywidth/2.0)\n    # d.addLine(origin - zaxis*zwidth/2.0, origin + zaxis*zwidth/2.0)\n\n    d.addLine(origin, origin + xaxis * xwidth / 2.0)\n    d.addLine(origin, origin + yaxis * ywidth / 2.0)\n    d.addLine(origin, origin + zaxis * zwidth / 2.0)\n\n    # obj = updatePolyData(d.getPolyData(), \'block axes\')\n    # obj.setProperty(\'Color\', QtGui.QColor(255, 255, 0))\n    # obj.setProperty(\'Visible\', False)\n\n    obj = createBlockAffordance(origin, xaxis, yaxis, zaxis, xwidth, ywidth, zwidth, name)\n    obj.setProperty(\'Color\', [222 / 255.0, 184 / 255.0, 135 / 255.0])\n\n    computeDebrisGraspSeed(obj)\n    t = computeDebrisStanceFrame(obj)\n    if t:\n        showFrame(t, \'debris stance frame\', parent=obj)\n\n    return obj\n\n\ndef computeDebrisGraspSeed(aff):\n    debrisReferenceFrame = om.findObjectByName(\'debris reference frame\')\n    if debrisReferenceFrame:\n        debrisReferenceFrame = debrisReferenceFrame.transform\n        affCornerFrame = computeCornerFrame(aff, debrisReferenceFrame)\n        showFrame(affCornerFrame, \'board corner frame\', parent=aff, visible=False)\n\n\ndef computeDebrisStanceFrame(aff):\n    debrisReferenceFrame = om.findObjectByName(\'debris reference frame\')\n    debrisWallEdge = om.findObjectByName(\'debris plane edge\')\n\n    if debrisReferenceFrame and debrisWallEdge:\n\n        debrisReferenceFrame = debrisReferenceFrame.transform\n\n        affGroundFrame = computeGroundFrame(aff, debrisReferenceFrame)\n\n        updateFrame(affGroundFrame, \'board ground frame\', parent=getDebugFolder(), visible=False)\n\n        affWallEdge = computeGroundFrame(aff, debrisReferenceFrame)\n\n        framePos = np.array(affGroundFrame.GetPosition())\n        p1, p2 = debrisWallEdge.points\n        edgeAxis = p2 - p1\n        edgeAxis /= np.linalg.norm(edgeAxis)\n        projectedPos = p1 + edgeAxis * np.dot(framePos - p1, edgeAxis)\n\n        affWallFrame = vtk.vtkTransform()\n        affWallFrame.PostMultiply()\n\n        useWallFrameForRotation = True\n\n        if useWallFrameForRotation:\n            affWallFrame.SetMatrix(debrisReferenceFrame.GetMatrix())\n            affWallFrame.Translate(projectedPos - np.array(debrisReferenceFrame.GetPosition()))\n\n            stanceWidth = 0.20\n            stanceOffsetX = -0.35\n            stanceOffsetY = 0.45\n            stanceRotation = 0.0\n\n        else:\n            affWallFrame.SetMatrix(affGroundFrame.GetMatrix())\n            affWallFrame.Translate(projectedPos - framePos)\n\n            stanceWidth = 0.20\n            stanceOffsetX = -0.35\n            stanceOffsetY = -0.45\n            stanceRotation = math.pi / 2.0\n\n        stanceFrame, _, _ = getFootFramesFromReferenceFrame(affWallFrame, stanceWidth, math.degrees(stanceRotation),\n                                                            [stanceOffsetX, stanceOffsetY, 0.0])\n\n        return stanceFrame\n\n\ndef segmentBlockByPlanes(blockDimensions):\n    planes = om.findObjectByName(\'selected planes\').children()[:2]\n\n    viewPlaneNormal = getSegmentationView().camera().GetViewPlaneNormal()\n    origin1, normal1, plane1 = getPlaneEquationFromPolyData(planes[0].polyData, expectedNormal=viewPlaneNormal)\n    origin2, normal2, plane2 = getPlaneEquationFromPolyData(planes[1].polyData, expectedNormal=viewPlaneNormal)\n\n    xaxis = normal2\n    yaxis = normal1\n    zaxis = np.cross(xaxis, yaxis)\n    xaxis = np.cross(yaxis, zaxis)\n\n    pts1 = vtkNumpy.getNumpyFromVtk(planes[0].polyData, \'Points\')\n    pts2 = vtkNumpy.getNumpyFromVtk(planes[1].polyData, \'Points\')\n\n    linePoint = np.zeros(3)\n    centroid2 = np.sum(pts2, axis=0) / len(pts2)\n    vtk.vtkPlane.ProjectPoint(centroid2, origin1, normal1, linePoint)\n\n    dists = np.dot(pts1 - linePoint, zaxis)\n\n    p1 = linePoint + zaxis * np.min(dists)\n    p2 = linePoint + zaxis * np.max(dists)\n\n    xwidth, ywidth = blockDimensions\n    zwidth = np.linalg.norm(p2 - p1)\n\n    origin = p1 + xaxis * xwidth / 2.0 + yaxis * ywidth / 2.0 + zaxis * zwidth / 2.0\n\n    d = DebugData()\n\n    d.addSphere(linePoint, radius=0.02)\n    d.addSphere(p1, radius=0.01)\n    d.addSphere(p2, radius=0.01)\n    d.addLine(p1, p2)\n\n    d.addSphere(origin, radius=0.01)\n    d.addLine(origin - xaxis * xwidth / 2.0, origin + xaxis * xwidth / 2.0)\n    d.addLine(origin - yaxis * ywidth / 2.0, origin + yaxis * ywidth / 2.0)\n    d.addLine(origin - zaxis * zwidth / 2.0, origin + zaxis * zwidth / 2.0)\n    obj = updatePolyData(d.getPolyData(), \'block axes\')\n    obj.setProperty(\'Color\', QtGui.QColor(255, 255, 0))\n    obj.setProperty(\'Visible\', False)\n\n    cube = vtk.vtkCubeSource()\n    cube.SetXLength(xwidth)\n    cube.SetYLength(ywidth)\n    cube.SetZLength(zwidth)\n    cube.Update()\n    cube = shallowCopy(cube.GetOutput())\n\n    t = getTransformFromAxes(xaxis, yaxis, zaxis)\n    t.PostMultiply()\n    t.Translate(origin)\n\n    obj = updatePolyData(cube, \'block affordance\', cls=BlockAffordanceItem, parent=\'affordances\')\n    obj.actor.SetUserTransform(t)\n    obj.addToView(app.getDRCView())\n\n    params = dict(origin=origin, xwidth=xwidth, ywidth=ywidth, zwidth=zwidth, xaxis=xaxis, yaxis=yaxis, zaxis=zaxis)\n    obj.setAffordanceParams(params)\n    obj.updateParamsFromActorTransform()\n\n\ndef estimatePointerTip(robotModel, polyData):\n    \'\'\'\n    Given a robot model, uses forward kinematics to determine a pointer tip\n    search region, then does a ransac line fit in the search region to find\n    points on the pointer, and selects the maximum point along the line fit\n    as the pointer tip.  Returns the pointer tip xyz on success and returns\n    None on failure.\n    \'\'\'\n    palmFrame = robotModel.getLinkFrame(\'r_hand_force_torque\')\n    p1 = [0.0, 0.14, -0.06]\n    p2 = [0.0, 0.24, -0.06]\n\n    palmFrame.TransformPoint(p1, p1)\n    palmFrame.TransformPoint(p2, p2)\n\n    p1 = np.array(p1)\n    p2 = np.array(p2)\n\n    d = DebugData()\n    d.addSphere(p1, radius=0.005)\n    d.addSphere(p2, radius=0.005)\n    d.addLine(p1, p2)\n    vis.updatePolyData(d.getPolyData(), \'pointer line\', color=[1, 0, 0], parent=getDebugFolder(), visible=False)\n\n    polyData = cropToLineSegment(polyData, p1, p2)\n    if not polyData.GetNumberOfPoints():\n        # print \'pointer search region is empty\'\n        return None\n\n    vis.updatePolyData(polyData, \'cropped to pointer line\', parent=getDebugFolder(), visible=False)\n\n    polyData = labelDistanceToLine(polyData, p1, p2)\n\n    polyData = thresholdPoints(polyData, \'distance_to_line\', [0.0, 0.07])\n\n    if polyData.GetNumberOfPoints() < 2:\n        # print \'pointer search region is empty\'\n        return None\n\n    updatePolyData(polyData, \'distance to pointer line\', colorByName=\'distance_to_line\', parent=getDebugFolder(),\n                   visible=False)\n\n    ransacDistanceThreshold = 0.0075\n    lineOrigin, lineDirection, polyData = applyLineFit(polyData, distanceThreshold=ransacDistanceThreshold)\n    updatePolyData(polyData, \'line fit ransac\', colorByName=\'ransac_labels\', parent=getDebugFolder(), visible=False)\n\n    lineDirection = np.array(lineDirection)\n    lineDirection /= np.linalg.norm(lineDirection)\n\n    if np.dot(lineDirection, (p2 - p1)) < 0:\n        lineDirection *= -1\n\n    polyData = thresholdPoints(polyData, \'ransac_labels\', [1.0, 1.0])\n\n    if polyData.GetNumberOfPoints() < 2:\n        # print \'pointer ransac line fit failed to find inliers\'\n        return None\n\n    obj = updatePolyData(polyData, \'line fit points\', colorByName=\'dist_along_line\', parent=getDebugFolder(),\n                         visible=True)\n    obj.setProperty(\'Point Size\', 5)\n\n    pts = vtkNumpy.getNumpyFromVtk(polyData, \'Points\')\n\n    dists = np.dot(pts - lineOrigin, lineDirection)\n\n    p1 = lineOrigin + lineDirection * np.min(dists)\n    p2 = lineOrigin + lineDirection * np.max(dists)\n\n    d = DebugData()\n    # d.addSphere(p1, radius=0.005)\n    d.addSphere(p2, radius=0.005)\n    d.addLine(p1, p2)\n    vis.updatePolyData(d.getPolyData(), \'fit pointer line\', color=[0, 1, 0], parent=getDebugFolder(), visible=True)\n\n    return p2\n\n\ndef startBoundedPlaneSegmentation():\n    picker = PointPicker(numberOfPoints=2)\n    addViewPicker(picker)\n    picker.enabled = True\n    picker.start()\n    picker.annotationFunc = functools.partial(segmentBoundedPlaneByAnnotation)\n\n\ndef startValveSegmentationByWallPlane(expectedValveRadius):\n    picker = PointPicker(numberOfPoints=2)\n    addViewPicker(picker)\n    picker.enabled = True\n    picker.start()\n    picker.annotationFunc = functools.partial(segmentValveByWallPlane, expectedValveRadius)\n\n\ndef startValveSegmentationManual(expectedValveRadius):\n    picker = PointPicker(numberOfPoints=2)\n    addViewPicker(picker)\n    picker.enabled = True\n    picker.drawLines = False\n    picker.start()\n    picker.annotationFunc = functools.partial(segmentValve, expectedValveRadius)\n\n\ndef startRefitWall():\n    picker = PointPicker(numberOfPoints=1)\n    addViewPicker(picker)\n    picker.enabled = True\n    picker.start()\n    picker.annotationFunc = refitWall\n\n\ndef startWyeSegmentation():\n    picker = PointPicker(numberOfPoints=2)\n    addViewPicker(picker)\n    picker.enabled = True\n    picker.drawLines = False\n    picker.start()\n    picker.annotationFunc = functools.partial(segmentWye)\n\n\ndef startDoorHandleSegmentation(otdfType):\n    picker = PointPicker(numberOfPoints=2)\n    addViewPicker(picker)\n    picker.enabled = True\n    picker.drawLines = False\n    picker.start()\n    picker.annotationFunc = functools.partial(segmentDoorHandle, otdfType)\n\n\ndef startTrussSegmentation():\n    picker = PointPicker(numberOfPoints=2)\n    addViewPicker(picker)\n    picker.enabled = True\n    picker.drawLines = True\n    picker.start()\n    picker.annotationFunc = functools.partial(segmentTruss)\n\n\ndef startHoseNozzleSegmentation():\n    picker = PointPicker(numberOfPoints=1)\n    addViewPicker(picker)\n    picker.enabled = True\n    picker.drawLines = False\n    picker.start()\n    picker.annotationFunc = functools.partial(segmentHoseNozzle)\n\n\ndef storePoint(p):\n    global _pickPoint\n    _pickPoint = p\n\n\ndef getPickPoint():\n    global _pickPoint\n    return _pickPoint\n\n\ndef startPickPoint():\n    picker = PointPicker(numberOfPoints=1)\n    addViewPicker(picker)\n    picker.enabled = True\n    picker.drawLines = False\n    picker.start()\n    picker.annotationFunc = storePoint\n\n\ndef startSelectToolTip():\n    picker = PointPicker(numberOfPoints=1)\n    addViewPicker(picker)\n    picker.enabled = True\n    picker.drawLines = False\n    picker.start()\n    picker.annotationFunc = selectToolTip\n\n\ndef startDrillSegmentation():\n    picker = PointPicker(numberOfPoints=3)\n    addViewPicker(picker)\n    picker.enabled = True\n    picker.drawLines = False\n    picker.start()\n    picker.annotationFunc = functools.partial(segmentDrill)\n\n\ndef startDrillAutoSegmentation():\n    picker = PointPicker(numberOfPoints=1)\n    addViewPicker(picker)\n    picker.enabled = True\n    picker.drawLines = False\n    picker.start()\n    picker.annotationFunc = functools.partial(segmentDrillAuto)\n\n\ndef startDrillButtonSegmentation():\n    picker = PointPicker(numberOfPoints=1)\n    addViewPicker(picker)\n    picker.enabled = True\n    picker.drawLines = False\n    picker.start()\n    picker.annotationFunc = functools.partial(segmentDrillButton)\n\n\ndef startPointerTipSegmentation():\n    picker = PointPicker(numberOfPoints=1)\n    addViewPicker(picker)\n    picker.enabled = True\n    picker.drawLines = False\n    picker.start()\n    picker.annotationFunc = functools.partial(segmentPointerTip)\n\n\ndef startDrillAutoSegmentationAlignedWithTable():\n    picker = PointPicker(numberOfPoints=1)\n    addViewPicker(picker)\n    picker.enabled = True\n    picker.drawLines = False\n    picker.start()\n    picker.annotationFunc = functools.partial(segmentDrillAlignedWithTable)\n\n\ndef startDrillBarrelSegmentation():\n    picker = PointPicker(numberOfPoints=1)\n    addViewPicker(picker)\n    picker.enabled = True\n    picker.drawLines = False\n    picker.start()\n    picker.annotationFunc = functools.partial(segmentDrillBarrel)\n\n\ndef startDrillWallSegmentation():\n    picker = PointPicker(numberOfPoints=3)\n    addViewPicker(picker)\n    picker.enabled = True\n    picker.drawLines = True\n    picker.start()\n    picker.annotationFunc = functools.partial(segmentDrillWall)\n\n\ndef startDrillWallSegmentationConstrained(rightAngleLocation):\n    picker = PointPicker(numberOfPoints=2)\n    addViewPicker(picker)\n    picker.enabled = True\n    picker.drawLines = False\n    picker.start()\n    picker.annotationFunc = functools.partial(segmentDrillWallConstrained, rightAngleLocation)\n\n\ndef startDrillInHandSegmentation():\n    picker = PointPicker(numberOfPoints=2)\n    addViewPicker(picker)\n    picker.enabled = True\n    picker.drawLines = True\n    picker.start()\n    picker.annotationFunc = functools.partial(segmentDrillInHand)\n\n\ndef startSegmentDebrisWall():\n    picker = PointPicker(numberOfPoints=1)\n    addViewPicker(picker)\n    picker.enabled = True\n    picker.start()\n    picker.annotationFunc = functools.partial(segmentDebrisWall)\n\n\ndef startSegmentDebrisWallManual():\n    picker = PointPicker(numberOfPoints=2)\n    addViewPicker(picker)\n    picker.enabled = True\n    picker.start()\n    picker.annotationFunc = functools.partial(segmentDebrisWallManual)\n\n\ndef selectToolTip(point1):\n    print point1\n\n\ndef segmentDebrisWallManual(point1, point2):\n    p1, p2 = point1, point2\n\n    d = DebugData()\n    d.addSphere(p1, radius=0.01)\n    d.addSphere(p2, radius=0.01)\n    d.addLine(p1, p2)\n    edgeObj = updatePolyData(d.getPolyData(), \'debris plane edge\', visible=True)\n    edgeObj.points = [p1, p2]\n\n    xaxis = p2 - p1\n    xaxis /= np.linalg.norm(xaxis)\n    zaxis = np.array([0.0, 0.0, 1.0])\n    yaxis = np.cross(zaxis, xaxis)\n\n    t = getTransformFromAxes(xaxis, yaxis, zaxis)\n    t.PostMultiply()\n    t.Translate(p1)\n\n    updateFrame(t, \'debris plane frame\', parent=edgeObj, visible=False)\n\n    refFrame = vtk.vtkTransform()\n    refFrame.PostMultiply()\n    refFrame.SetMatrix(t.GetMatrix())\n    refFrame.Translate(-xaxis + yaxis + zaxis * 20.0)\n    updateFrame(refFrame, \'debris reference frame\', parent=edgeObj, visible=False)\n\n\ndef segmentDebrisWall(point1):\n    inputObj = om.findObjectByName(\'pointcloud snapshot\')\n    polyData = shallowCopy(inputObj.polyData)\n\n    viewPlaneNormal = np.array(getSegmentationView().camera().GetViewPlaneNormal())\n\n    polyData, origin, normal = applyPlaneFit(polyData, distanceThreshold=0.02, expectedNormal=viewPlaneNormal,\n                                             perpendicularAxis=viewPlaneNormal,\n                                             searchOrigin=point1, searchRadius=0.25, angleEpsilon=0.7,\n                                             returnOrigin=True)\n\n    planePoints = thresholdPoints(polyData, \'dist_to_plane\', [-0.02, 0.02])\n    updatePolyData(planePoints, \'unbounded plane points\', parent=getDebugFolder(), visible=False)\n\n    planePoints = applyVoxelGrid(planePoints, leafSize=0.03)\n    planePoints = labelOutliers(planePoints, searchRadius=0.06, neighborsInSearchRadius=10)\n\n    updatePolyData(planePoints, \'voxel plane points\', parent=getDebugFolder(), colorByName=\'is_outlier\', visible=False)\n\n    planePoints = thresholdPoints(planePoints, \'is_outlier\', [0, 0])\n\n    planePoints = labelDistanceToPoint(planePoints, point1)\n    clusters = extractClusters(planePoints, clusterTolerance=0.10)\n    clusters.sort(key=lambda x: vtkNumpy.getNumpyFromVtk(x, \'distance_to_point\').min())\n\n    planePoints = clusters[0]\n    planeObj = updatePolyData(planePoints, \'debris plane points\', parent=getDebugFolder(), visible=False)\n\n    perpAxis = [0, 0, -1]\n    perpAxis /= np.linalg.norm(perpAxis)\n    edgeAxis = np.cross(normal, perpAxis)\n\n    edgePoints = computeEdge(planePoints, edgeAxis, perpAxis)\n    edgePoints = vtkNumpy.getVtkPolyDataFromNumpyPoints(edgePoints)\n    updatePolyData(edgePoints, \'edge points\', parent=getDebugFolder(), visible=False)\n\n    linePoint, lineDirection, _ = applyLineFit(edgePoints)\n\n    # binCounts = computePointCountsAlongAxis(planePoints, lineDirection)\n\n\n    xaxis = lineDirection\n    yaxis = normal\n\n    zaxis = np.cross(xaxis, yaxis)\n\n    if np.dot(zaxis, [0, 0, 1]) < 0:\n        zaxis *= -1\n        xaxis *= -1\n\n    pts = vtkNumpy.getNumpyFromVtk(planePoints, \'Points\')\n\n    dists = np.dot(pts - linePoint, xaxis)\n\n    p1 = linePoint + xaxis * np.min(dists)\n    p2 = linePoint + xaxis * np.max(dists)\n\n    p1 = projectPointToPlane(p1, origin, normal)\n    p2 = projectPointToPlane(p2, origin, normal)\n\n    d = DebugData()\n    d.addSphere(p1, radius=0.01)\n    d.addSphere(p2, radius=0.01)\n    d.addLine(p1, p2)\n    edgeObj = updatePolyData(d.getPolyData(), \'debris plane edge\', parent=planeObj, visible=True)\n    edgeObj.points = [p1, p2]\n\n    t = getTransformFromAxes(xaxis, yaxis, zaxis)\n    t.PostMultiply()\n    t.Translate(p1)\n\n    updateFrame(t, \'debris plane frame\', parent=planeObj, visible=False)\n\n    refFrame = vtk.vtkTransform()\n    refFrame.PostMultiply()\n    refFrame.SetMatrix(t.GetMatrix())\n    refFrame.Translate(-xaxis + yaxis + zaxis * 20.0)\n    updateFrame(refFrame, \'debris reference frame\', parent=planeObj, visible=False)\n\n\ndef segmentBoundedPlaneByAnnotation(point1, point2):\n    inputObj = om.findObjectByName(\'pointcloud snapshot\')\n    polyData = shallowCopy(inputObj.polyData)\n\n    viewPlaneNormal = np.array(getSegmentationView().camera().GetViewPlaneNormal())\n\n    polyData, origin, normal = applyPlaneFit(polyData, distanceThreshold=0.015, expectedNormal=viewPlaneNormal,\n                                             perpendicularAxis=viewPlaneNormal,\n                                             searchOrigin=point1, searchRadius=0.3, angleEpsilon=0.7, returnOrigin=True)\n\n    planePoints = thresholdPoints(polyData, \'dist_to_plane\', [-0.015, 0.015])\n    updatePolyData(planePoints, \'unbounded plane points\', parent=getDebugFolder(), visible=False)\n\n    planePoints = applyVoxelGrid(planePoints, leafSize=0.03)\n    planePoints = labelOutliers(planePoints, searchRadius=0.06, neighborsInSearchRadius=12)\n\n    updatePolyData(planePoints, \'voxel plane points\', parent=getDebugFolder(), colorByName=\'is_outlier\', visible=False)\n\n    planePoints = thresholdPoints(planePoints, \'is_outlier\', [0, 0])\n\n    planePoints = labelDistanceToPoint(planePoints, point1)\n    clusters = extractClusters(planePoints, clusterTolerance=0.10)\n    clusters.sort(key=lambda x: vtkNumpy.getNumpyFromVtk(x, \'distance_to_point\').min())\n\n    planePoints = clusters[0]\n    updatePolyData(planePoints, \'plane points\', parent=getDebugFolder(), visible=False)\n\n    perpAxis = point2 - point1\n    perpAxis /= np.linalg.norm(perpAxis)\n    edgeAxis = np.cross(normal, perpAxis)\n\n    edgePoints = computeEdge(planePoints, edgeAxis, perpAxis)\n    edgePoints = vtkNumpy.getVtkPolyDataFromNumpyPoints(edgePoints)\n    updatePolyData(edgePoints, \'edge points\', parent=getDebugFolder(), visible=False)\n\n    linePoint, lineDirection, _ = applyLineFit(edgePoints)\n\n    zaxis = normal\n    yaxis = lineDirection\n    xaxis = np.cross(yaxis, zaxis)\n\n    if np.dot(xaxis, perpAxis) < 0:\n        xaxis *= -1\n\n    # make right handed\n    yaxis = np.cross(zaxis, xaxis)\n\n    pts = vtkNumpy.getNumpyFromVtk(planePoints, \'Points\')\n\n    dists = np.dot(pts - linePoint, yaxis)\n\n    p1 = linePoint + yaxis * np.min(dists)\n    p2 = linePoint + yaxis * np.max(dists)\n\n    p1 = projectPointToPlane(p1, origin, normal)\n    p2 = projectPointToPlane(p2, origin, normal)\n\n    d = DebugData()\n    d.addSphere(p1, radius=0.01)\n    d.addSphere(p2, radius=0.01)\n    d.addLine(p1, p2)\n    updatePolyData(d.getPolyData(), \'plane edge\', parent=getDebugFolder(), visible=False)\n\n    t = getTransformFromAxes(xaxis, yaxis, zaxis)\n    t.PostMultiply()\n    t.Translate((p1 + p2) / 2.0)\n\n    updateFrame(t, \'plane edge frame\', parent=getDebugFolder(), visible=False)\n\n\nsavedCameraParams = None\n\n\ndef perspective():\n    global savedCameraParams\n    if savedCameraParams is None:\n        return\n\n    aff = getDefaultAffordanceObject()\n    if aff:\n        aff.setProperty(\'Alpha\', 1.0)\n\n    obj = om.findObjectByName(\'pointcloud snapshot\')\n    if obj is not None:\n        obj.actor.SetPickable(1)\n\n    view = getSegmentationView()\n    c = view.camera()\n    c.ParallelProjectionOff()\n    c.SetPosition(savedCameraParams[\'Position\'])\n    c.SetFocalPoint(savedCameraParams[\'FocalPoint\'])\n    c.SetViewUp(savedCameraParams[\'ViewUp\'])\n    view.setCameraManipulationStyle()\n    view.render()\n\n\ndef saveCameraParams(overwrite=False):\n    global savedCameraParams\n    if overwrite or (savedCameraParams is None):\n        view = getSegmentationView()\n        c = view.camera()\n        savedCameraParams = dict(Position=c.GetPosition(), FocalPoint=c.GetFocalPoint(), ViewUp=c.GetViewUp())\n\n\ndef getDefaultAffordanceObject():\n    obj = om.getActiveObject()\n    if isinstance(obj, AffordanceItem):\n        return obj\n\n    for obj in om.getObjects():\n        if isinstance(obj, AffordanceItem):\n            return obj\n\n\ndef orthoX():\n    aff = getDefaultAffordanceObject()\n    if not aff:\n        return\n\n    saveCameraParams()\n\n    aff.updateParamsFromActorTransform()\n    aff.setProperty(\'Alpha\', 0.3)\n    om.findObjectByName(\'pointcloud snapshot\').actor.SetPickable(0)\n\n    view = getSegmentationView()\n    c = view.camera()\n    c.ParallelProjectionOn()\n\n    origin = aff.params[\'origin\']\n    viewDirection = aff.params[\'xaxis\']\n    viewUp = -aff.params[\'yaxis\']\n    viewDistance = aff.params[\'xwidth\'] * 3\n    scale = aff.params[\'zwidth\']\n\n    c.SetFocalPoint(origin)\n    c.SetPosition(origin - viewDirection * viewDistance)\n    c.SetViewUp(viewUp)\n    c.SetParallelScale(scale)\n\n    view.setActorManipulationStyle()\n    view.render()\n\n\ndef orthoY():\n    aff = getDefaultAffordanceObject()\n    if not aff:\n        return\n\n    saveCameraParams()\n\n    aff.updateParamsFromActorTransform()\n    aff.setProperty(\'Alpha\', 0.3)\n    om.findObjectByName(\'pointcloud snapshot\').actor.SetPickable(0)\n\n    view = getSegmentationView()\n    c = view.camera()\n    c.ParallelProjectionOn()\n\n    origin = aff.params[\'origin\']\n    viewDirection = aff.params[\'yaxis\']\n    viewUp = -aff.params[\'xaxis\']\n    viewDistance = aff.params[\'ywidth\'] * 4\n    scale = aff.params[\'zwidth\']\n\n    c.SetFocalPoint(origin)\n    c.SetPosition(origin - viewDirection * viewDistance)\n    c.SetViewUp(viewUp)\n    c.SetParallelScale(scale)\n\n    view.setActorManipulationStyle()\n    view.render()\n\n\ndef orthoZ():\n    aff = getDefaultAffordanceObject()\n    if not aff:\n        return\n\n    saveCameraParams()\n\n    aff.updateParamsFromActorTransform()\n    aff.setProperty(\'Alpha\', 0.3)\n    om.findObjectByName(\'pointcloud snapshot\').actor.SetPickable(0)\n\n    view = getSegmentationView()\n    c = view.camera()\n    c.ParallelProjectionOn()\n\n    origin = aff.params[\'origin\']\n    viewDirection = aff.params[\'zaxis\']\n    viewUp = -aff.params[\'yaxis\']\n    viewDistance = aff.params[\'zwidth\']\n    scale = aff.params[\'ywidth\'] * 6\n\n    c.SetFocalPoint(origin)\n    c.SetPosition(origin - viewDirection * viewDistance)\n    c.SetViewUp(viewUp)\n    c.SetParallelScale(scale)\n\n    view.setActorManipulationStyle()\n    view.render()\n\n\ndef zoomToDisplayPoint(displayPoint, boundsRadius=0.5, view=None):\n    pickedPointFields = pickPoint(displayPoint, getSegmentationView(), obj=\'pointcloud snapshot\')\n    pickedPoint = pickedPointFields.pickedPoint\n    if pickedPoint is None:\n        return\n\n    view = view or app.getCurrentRenderView()\n\n    worldPt1, worldPt2 = getRayFromDisplayPoint(getSegmentationView(), displayPoint)\n\n    diagonal = np.array([boundsRadius, boundsRadius, boundsRadius])\n    bounds = np.hstack([pickedPoint - diagonal, pickedPoint + diagonal])\n    bounds = [bounds[0], bounds[3], bounds[1], bounds[4], bounds[2], bounds[5]]\n    view.renderer().ResetCamera(bounds)\n    view.camera().SetFocalPoint(pickedPoint)\n    view.render()\n\n\ndef extractPointsAlongClickRay(position, ray, polyData=None, distanceToLineThreshold=0.025, nearestToCamera=False):\n    # segmentationObj = om.findObjectByName(\'pointcloud snapshot\')\n    if polyData is None:\n        polyData = getCurrentRevolutionData()\n\n    if not polyData or not polyData.GetNumberOfPoints():\n        return None\n\n    polyData = labelDistanceToLine(polyData, position, position + ray)\n\n    # extract points near line\n    polyData = thresholdPoints(polyData, \'distance_to_line\', [0.0, distanceToLineThreshold])\n    if not polyData.GetNumberOfPoints():\n        return None\n\n    polyData = labelPointDistanceAlongAxis(polyData, ray, origin=position, resultArrayName=\'distance_along_line\')\n    polyData = thresholdPoints(polyData, \'distance_along_line\', [0.20, 1e6])\n    if not polyData.GetNumberOfPoints():\n        return None\n\n    updatePolyData(polyData, \'ray points\', colorByName=\'distance_to_line\', visible=False, parent=getDebugFolder())\n\n    if nearestToCamera:\n        dists = vtkNumpy.getNumpyFromVtk(polyData, \'distance_along_line\')\n    else:\n        dists = vtkNumpy.getNumpyFromVtk(polyData, \'distance_to_line\')\n\n    points = vtkNumpy.getNumpyFromVtk(polyData, \'Points\')\n    intersectionPoint = points[dists.argmin()]\n\n    d = DebugData()\n    d.addSphere(intersectionPoint, radius=0.005)\n    d.addLine(position, intersectionPoint)\n    obj = updatePolyData(d.getPolyData(), \'intersecting ray\', visible=False, color=[0, 1, 0], parent=getDebugFolder())\n    obj.actor.GetProperty().SetLineWidth(2)\n\n    d2 = DebugData()\n    end_of_ray = position + 2 * ray\n    d2.addLine(position, end_of_ray)\n    obj2 = updatePolyData(d2.getPolyData(), \'camera ray\', visible=False, color=[1, 0, 0], parent=getDebugFolder())\n    obj2.actor.GetProperty().SetLineWidth(2)\n\n    return intersectionPoint\n\n\ndef segmentDrillWallFromTag(position, ray):\n    \'\'\'\n    Fix the drill wall relative to a ray intersected with the wall\n    Desc: given a position and a ray (typically derived from a camera pixel)\n    Use that point to determine a position for the Drill Wall\n    This function uses a hard coded offset between the position on the wall\n    to produce the drill cutting origin\n    \'\'\'\n\n    # inputObj = om.findObjectByName(\'pointcloud snapshot\')\n    # polyData = shallowCopy(inputObj.polyData)\n    polyData = getCurrentRevolutionData()\n\n    if (polyData is None):  # no data yet\n        print ""no LIDAR data yet""\n        return False\n\n    point1 = extractPointsAlongClickRay(position, ray, polyData)\n\n    # view direction is out:\n    viewDirection = -1 * SegmentationContext.getGlobalInstance().getViewDirection()\n    polyDataOut, origin, normal = applyPlaneFit(polyData, expectedNormal=viewDirection, searchOrigin=point1,\n                                                searchRadius=0.3, angleEpsilon=0.3, returnOrigin=True)\n\n    # project the lidar point onto the plane (older, variance is >1cm with robot 2m away)\n    # intersection_point = projectPointToPlane(point1, origin, normal)\n    # intersect the ray with the plane (variance was about 4mm with robot 2m away)\n    intersection_point = intersectLineWithPlane(position, ray, origin, normal)\n\n    # Define a frame:\n    xaxis = -normal\n    zaxis = [0, 0, 1]\n    yaxis = np.cross(zaxis, xaxis)\n    yaxis /= np.linalg.norm(yaxis)\n    zaxis = np.cross(xaxis, yaxis)\n    t = transformUtils.getTransformFromAxes(xaxis, yaxis, zaxis)\n    t.PostMultiply()\n    t.Translate(intersection_point)\n\n    t2 = transformUtils.copyFrame(t)\n    t2.PreMultiply()\n    t3 = transformUtils.frameFromPositionAndRPY([0, 0.6, -0.25], [0, 0, 0])\n    t2.Concatenate(t3)\n\n    rightAngleLocation = \'bottom left\'\n    createDrillWall(rightAngleLocation, t2)\n\n    wall = om.findObjectByName(\'wall\')\n    vis.updateFrame(t, \'wall fit tag\', parent=wall, visible=False, scale=0.2)\n\n    d = DebugData()\n    d.addSphere(intersection_point, radius=0.002)\n    obj = updatePolyData(d.getPolyData(), \'intersection\', parent=wall, visible=False, color=[0, 1, 0])  #\n    obj.actor.GetProperty().SetLineWidth(1)\n    return True\n\n\ndef segmentDrillWallFromWallCenter():\n    \'\'\'\n    Get the drill wall target as an offset from the center of\n    the full wall\n    \'\'\'\n\n    # find the valve wall and its center\n    inputObj = om.findObjectByName(\'pointcloud snapshot\')\n    polyData = inputObj.polyData\n\n    # hardcoded position to target frame from center of wall\n    # conincides with the distance from the april tag to this position\n    wallFrame = transformUtils.copyFrame(findWallCenter(polyData))\n    wallFrame.PreMultiply()\n    t3 = transformUtils.frameFromPositionAndRPY([-0.07, -0.3276, 0], [180, -90, 0])\n    wallFrame.Concatenate(t3)\n\n    rightAngleLocation = \'bottom left\'\n    createDrillWall(rightAngleLocation, wallFrame)\n\n    wall = om.findObjectByName(\'wall\')\n    vis.updateFrame(wallFrame, \'wall fit lidar\', parent=wall, visible=False, scale=0.2)\n\n\ndef findFarRightCorner(polyData, linkFrame):\n    \'\'\'\n    Within a point cloud find the point to the far right from the link\n    The input is the 4 corners of a minimum bounding box\n    \'\'\'\n\n    diagonalTransform = transformUtils.copyFrame(linkFrame)\n    diagonalTransform.PreMultiply()\n    diagonalTransform.Concatenate(transformUtils.frameFromPositionAndRPY([0, 0, 0], [0, 0, 45]))\n    vis.updateFrame(diagonalTransform, \'diagonal frame\', parent=getDebugFolder(), visible=False)\n\n    points = vtkNumpy.getNumpyFromVtk(polyData, \'Points\')\n    viewOrigin = diagonalTransform.TransformPoint([0.0, 0.0, 0.0])\n    viewX = diagonalTransform.TransformVector([1.0, 0.0, 0.0])\n    viewY = diagonalTransform.TransformVector([0.0, 1.0, 0.0])\n    viewZ = diagonalTransform.TransformVector([0.0, 0.0, 1.0])\n    polyData = labelPointDistanceAlongAxis(polyData, viewY, origin=viewOrigin, resultArrayName=\'distance_along_foot_y\')\n\n    vis.updatePolyData(polyData, \'cornerPoints\', parent=\'segmentation\', visible=False)\n    farRightIndex = vtkNumpy.getNumpyFromVtk(polyData, \'distance_along_foot_y\').argmin()\n    points = vtkNumpy.getNumpyFromVtk(polyData, \'Points\')\n    return points[farRightIndex, :]\n\n\ndef findMinimumBoundingRectangle(polyData, linkFrame):\n    \'\'\'\n    Find minimum bounding rectangle of a rectangular point cloud\n    The input is assumed to be a rectangular point cloud e.g. the top of a block or table\n    Returns transform of far right corner (pointing away from robot)\n    \'\'\'\n\n    # Originally From: https://github.com/dbworth/minimum-area-bounding-rectangle\n    polyData = applyVoxelGrid(polyData, leafSize=0.02)\n\n    def get2DAsPolyData(xy_points):\n        \'\'\'\n        Convert a 2D numpy array to a 3D polydata by appending z=0\n        \'\'\'\n        d = np.vstack((xy_points.T, np.zeros(xy_points.shape[0]))).T\n        d2 = d.copy()\n        return vtkNumpy.getVtkPolyDataFromNumpyPoints(d2)\n\n    pts = vtkNumpy.getNumpyFromVtk(polyData, \'Points\')\n    xy_points = pts[:, [0, 1]]\n    vis.updatePolyData(get2DAsPolyData(xy_points), \'xy_points\', parent=getDebugFolder(), visible=False)\n    hull_points = qhull_2d.qhull2D(xy_points)\n    vis.updatePolyData(get2DAsPolyData(hull_points), \'hull_points\', parent=getDebugFolder(), visible=False)\n    # Reverse order of points, to match output from other qhull implementations\n    hull_points = hull_points[::-1]\n    # print \'Convex hull points: \\n\', hull_points, ""\\n""\n\n    # Find minimum area bounding rectangle\n    (rot_angle, rectArea, rectDepth, rectWidth, center_point, corner_points_ground) = min_bounding_rect.minBoundingRect(\n        hull_points)\n    vis.updatePolyData(get2DAsPolyData(corner_points_ground), \'corner_points_ground\', parent=getDebugFolder(),\n                       visible=False)\n\n    polyDataCentroid = computeCentroid(polyData)\n    cornerPoints = np.vstack((corner_points_ground.T, polyDataCentroid[2] * np.ones(corner_points_ground.shape[0]))).T\n    cornerPolyData = vtkNumpy.getVtkPolyDataFromNumpyPoints(cornerPoints)\n\n    # Create a frame at the far right point - which points away from the robot\n    farRightCorner = findFarRightCorner(cornerPolyData, linkFrame)\n    viewDirection = SegmentationContext.getGlobalInstance().getViewDirection()\n\n    viewFrame = SegmentationContext.getGlobalInstance().getViewFrame()\n    # vis.showFrame(viewFrame, ""viewFrame"")\n\n    robotYaw = math.atan2(viewDirection[1], viewDirection[0]) * 180.0 / np.pi\n    blockAngle = rot_angle * (180 / math.pi)\n    # print ""robotYaw   "", robotYaw\n    # print ""blockAngle "", blockAngle\n    blockAngleAll = np.array([blockAngle, blockAngle + 90, blockAngle + 180, blockAngle + 270])\n\n    values = blockAngleAll - robotYaw\n    for i in range(0, 4):\n        if (values[i] > 180):\n            values[i] = values[i] - 360\n\n    values = abs(values)\n    min_idx = np.argmin(values)\n    if ((min_idx == 1) or (min_idx == 3)):\n        # print ""flip rectDepth and rectWidth as angle is not away from robot""\n        temp = rectWidth;\n        rectWidth = rectDepth;\n        rectDepth = temp\n\n    # print ""best angle"", blockAngleAll[min_idx]\n    rot_angle = blockAngleAll[min_idx] * math.pi / 180.0\n\n    cornerTransform = transformUtils.frameFromPositionAndRPY(farRightCorner, [0, 0, np.rad2deg(rot_angle)])\n\n    vis.showFrame(cornerTransform, ""cornerTransform"", parent=getDebugFolder(), visible=False)\n\n    # print ""Minimum area bounding box:""\n    # print ""Rotation angle:"", rot_angle, ""rad  ("", rot_angle*(180/math.pi), ""deg )""\n    # print ""rectDepth:"", rectDepth, "" rectWidth:"", rectWidth, ""  Area:"", rectArea\n    # print ""Center point: \\n"", center_point # numpy array\n    # print ""Corner points: \\n"", cornerPoints, ""\\n""  # numpy array\n    return cornerTransform, rectDepth, rectWidth, rectArea\n'"
modules/dense_correspondence_manipulation/utils/transformations.py,0,"b'# -*- coding: utf-8 -*-\n# transformations.py\n\n# Copyright (c) 2006-2018, Christoph Gohlke\n# Copyright (c) 2006-2018, The Regents of the University of California\n# Produced at the Laboratory for Fluorescence Dynamics\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n# * Redistributions of source code must retain the above copyright\n#   notice, this list of conditions and the following disclaimer.\n# * Redistributions in binary form must reproduce the above copyright\n#   notice, this list of conditions and the following disclaimer in the\n#   documentation and/or other materials provided with the distribution.\n# * Neither the name of the copyright holders nor the names of any\n#   contributors may be used to endorse or promote products derived\n#   from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n""""""Homogeneous Transformation Matrices and Quaternions.\n\nA library for calculating 4x4 matrices for translating, rotating, reflecting,\nscaling, shearing, projecting, orthogonalizing, and superimposing arrays of\n3D homogeneous coordinates as well as for converting between rotation matrices,\nEuler angles, and quaternions. Also includes an Arcball control object and\nfunctions to decompose transformation matrices.\n\n:Author:\n  `Christoph Gohlke <https://www.lfd.uci.edu/~gohlke/>`_\n\n:Organization:\n  Laboratory for Fluorescence Dynamics, University of California, Irvine\n\n:Version: 2018.02.08\n\nRequirements\n------------\n* `CPython 2.7 or 3.6 <http://www.python.org>`_\n* `Numpy 1.13 <http://www.numpy.org>`_\n* `Transformations.c 2018.02.08 <https://www.lfd.uci.edu/~gohlke/>`_\n  (recommended for speedup of some functions)\n\nNotes\n-----\nThe API is not stable yet and is expected to change between revisions.\n\nThis Python code is not optimized for speed. Refer to the transformations.c\nmodule for a faster implementation of some functions.\n\nDocumentation in HTML format can be generated with epydoc.\n\nMatrices (M) can be inverted using numpy.linalg.inv(M), be concatenated using\nnumpy.dot(M0, M1), or transform homogeneous coordinate arrays (v) using\nnumpy.dot(M, v) for shape (4, \\*) column vectors, respectively\nnumpy.dot(v, M.T) for shape (\\*, 4) row vectors (""array of points"").\n\nThis module follows the ""column vectors on the right"" and ""row major storage""\n(C contiguous) conventions. The translation components are in the right column\nof the transformation matrix, i.e. M[:3, 3].\nThe transpose of the transformation matrices may have to be used to interface\nwith other graphics systems, e.g. with OpenGL\'s glMultMatrixd(). See also [16].\n\nCalculations are carried out with numpy.float64 precision.\n\nVector, point, quaternion, and matrix function arguments are expected to be\n""array like"", i.e. tuple, list, or numpy arrays.\n\nReturn types are numpy arrays unless specified otherwise.\n\nAngles are in radians unless specified otherwise.\n\nQuaternions w+ix+jy+kz are represented as [w, x, y, z].\n\nA triple of Euler angles can be applied/interpreted in 24 ways, which can\nbe specified using a 4 character string or encoded 4-tuple:\n\n  *Axes 4-string*: e.g. \'sxyz\' or \'ryxy\'\n\n  - first character : rotations are applied to \'s\'tatic or \'r\'otating frame\n  - remaining characters : successive rotation axis \'x\', \'y\', or \'z\'\n\n  *Axes 4-tuple*: e.g. (0, 0, 0, 0) or (1, 1, 1, 1)\n\n  - inner axis: code of axis (\'x\':0, \'y\':1, \'z\':2) of rightmost matrix.\n  - parity : even (0) if inner axis \'x\' is followed by \'y\', \'y\' is followed\n    by \'z\', or \'z\' is followed by \'x\'. Otherwise odd (1).\n  - repetition : first and last axis are same (1) or different (0).\n  - frame : rotations are applied to static (0) or rotating (1) frame.\n\nOther Python packages and modules for 3D transformations and quaternions:\n\n* `Transforms3d <https://pypi.python.org/pypi/transforms3d>`_\n   includes most code of this module.\n* `Blender.mathutils <http://www.blender.org/api/blender_python_api>`_\n* `numpy-dtypes <https://github.com/numpy/numpy-dtypes>`_\n\nReferences\n----------\n(1)  Matrices and transformations. Ronald Goldman.\n     In ""Graphics Gems I"", pp 472-475. Morgan Kaufmann, 1990.\n(2)  More matrices and transformations: shear and pseudo-perspective.\n     Ronald Goldman. In ""Graphics Gems II"", pp 320-323. Morgan Kaufmann, 1991.\n(3)  Decomposing a matrix into simple transformations. Spencer Thomas.\n     In ""Graphics Gems II"", pp 320-323. Morgan Kaufmann, 1991.\n(4)  Recovering the data from the transformation matrix. Ronald Goldman.\n     In ""Graphics Gems II"", pp 324-331. Morgan Kaufmann, 1991.\n(5)  Euler angle conversion. Ken Shoemake.\n     In ""Graphics Gems IV"", pp 222-229. Morgan Kaufmann, 1994.\n(6)  Arcball rotation control. Ken Shoemake.\n     In ""Graphics Gems IV"", pp 175-192. Morgan Kaufmann, 1994.\n(7)  Representing attitude: Euler angles, unit quaternions, and rotation\n     vectors. James Diebel. 2006.\n(8)  A discussion of the solution for the best rotation to relate two sets\n     of vectors. W Kabsch. Acta Cryst. 1978. A34, 827-828.\n(9)  Closed-form solution of absolute orientation using unit quaternions.\n     BKP Horn. J Opt Soc Am A. 1987. 4(4):629-642.\n(10) Quaternions. Ken Shoemake.\n     http://www.sfu.ca/~jwa3/cmpt461/files/quatut.pdf\n(11) From quaternion to matrix and back. JMP van Waveren. 2005.\n     http://www.intel.com/cd/ids/developer/asmo-na/eng/293748.htm\n(12) Uniform random rotations. Ken Shoemake.\n     In ""Graphics Gems III"", pp 124-132. Morgan Kaufmann, 1992.\n(13) Quaternion in molecular modeling. CFF Karney.\n     J Mol Graph Mod, 25(5):595-604\n(14) New method for extracting the quaternion from a rotation matrix.\n     Itzhack Y Bar-Itzhack, J Guid Contr Dynam. 2000. 23(6): 1085-1087.\n(15) Multiple View Geometry in Computer Vision. Hartley and Zissermann.\n     Cambridge University Press; 2nd Ed. 2004. Chapter 4, Algorithm 4.7, p 130.\n(16) Column Vectors vs. Row Vectors.\n     http://steve.hollasch.net/cgindex/math/matrix/column-vec.html\n\nExamples\n--------\n>>> alpha, beta, gamma = 0.123, -1.234, 2.345\n>>> origin, xaxis, yaxis, zaxis = [0, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1]\n>>> I = identity_matrix()\n>>> Rx = rotation_matrix(alpha, xaxis)\n>>> Ry = rotation_matrix(beta, yaxis)\n>>> Rz = rotation_matrix(gamma, zaxis)\n>>> R = concatenate_matrices(Rx, Ry, Rz)\n>>> euler = euler_from_matrix(R, \'rxyz\')\n>>> numpy.allclose([alpha, beta, gamma], euler)\nTrue\n>>> Re = euler_matrix(alpha, beta, gamma, \'rxyz\')\n>>> is_same_transform(R, Re)\nTrue\n>>> al, be, ga = euler_from_matrix(Re, \'rxyz\')\n>>> is_same_transform(Re, euler_matrix(al, be, ga, \'rxyz\'))\nTrue\n>>> qx = quaternion_about_axis(alpha, xaxis)\n>>> qy = quaternion_about_axis(beta, yaxis)\n>>> qz = quaternion_about_axis(gamma, zaxis)\n>>> q = quaternion_multiply(qx, qy)\n>>> q = quaternion_multiply(q, qz)\n>>> Rq = quaternion_matrix(q)\n>>> is_same_transform(R, Rq)\nTrue\n>>> S = scale_matrix(1.23, origin)\n>>> T = translation_matrix([1, 2, 3])\n>>> Z = shear_matrix(beta, xaxis, origin, zaxis)\n>>> R = random_rotation_matrix(numpy.random.rand(3))\n>>> M = concatenate_matrices(T, R, Z, S)\n>>> scale, shear, angles, trans, persp = decompose_matrix(M)\n>>> numpy.allclose(scale, 1.23)\nTrue\n>>> numpy.allclose(trans, [1, 2, 3])\nTrue\n>>> numpy.allclose(shear, [0, math.tan(beta), 0])\nTrue\n>>> is_same_transform(R, euler_matrix(axes=\'sxyz\', *angles))\nTrue\n>>> M1 = compose_matrix(scale, shear, angles, trans, persp)\n>>> is_same_transform(M, M1)\nTrue\n>>> v0, v1 = random_vector(3), random_vector(3)\n>>> M = rotation_matrix(angle_between_vectors(v0, v1), vector_product(v0, v1))\n>>> v2 = numpy.dot(v0, M[:3,:3].T)\n>>> numpy.allclose(unit_vector(v1), unit_vector(v2))\nTrue\n\n""""""\n\nfrom __future__ import division, print_function\n\nimport math\n\nimport numpy\n\n__version__ = \'2018.02.08\'\n__docformat__ = \'restructuredtext en\'\n__all__ = ()\n\n\ndef identity_matrix():\n    """"""Return 4x4 identity/unit matrix.\n\n    >>> I = identity_matrix()\n    >>> numpy.allclose(I, numpy.dot(I, I))\n    True\n    >>> numpy.sum(I), numpy.trace(I)\n    (4.0, 4.0)\n    >>> numpy.allclose(I, numpy.identity(4))\n    True\n\n    """"""\n    return numpy.identity(4)\n\n\ndef translation_matrix(direction):\n    """"""Return matrix to translate by direction vector.\n\n    >>> v = numpy.random.random(3) - 0.5\n    >>> numpy.allclose(v, translation_matrix(v)[:3, 3])\n    True\n\n    """"""\n    M = numpy.identity(4)\n    M[:3, 3] = direction[:3]\n    return M\n\n\ndef translation_from_matrix(matrix):\n    """"""Return translation vector from translation matrix.\n\n    >>> v0 = numpy.random.random(3) - 0.5\n    >>> v1 = translation_from_matrix(translation_matrix(v0))\n    >>> numpy.allclose(v0, v1)\n    True\n\n    """"""\n    return numpy.array(matrix, copy=False)[:3, 3].copy()\n\n\ndef reflection_matrix(point, normal):\n    """"""Return matrix to mirror at plane defined by point and normal vector.\n\n    >>> v0 = numpy.random.random(4) - 0.5\n    >>> v0[3] = 1.\n    >>> v1 = numpy.random.random(3) - 0.5\n    >>> R = reflection_matrix(v0, v1)\n    >>> numpy.allclose(2, numpy.trace(R))\n    True\n    >>> numpy.allclose(v0, numpy.dot(R, v0))\n    True\n    >>> v2 = v0.copy()\n    >>> v2[:3] += v1\n    >>> v3 = v0.copy()\n    >>> v2[:3] -= v1\n    >>> numpy.allclose(v2, numpy.dot(R, v3))\n    True\n\n    """"""\n    normal = unit_vector(normal[:3])\n    M = numpy.identity(4)\n    M[:3, :3] -= 2.0 * numpy.outer(normal, normal)\n    M[:3, 3] = (2.0 * numpy.dot(point[:3], normal)) * normal\n    return M\n\n\ndef reflection_from_matrix(matrix):\n    """"""Return mirror plane point and normal vector from reflection matrix.\n\n    >>> v0 = numpy.random.random(3) - 0.5\n    >>> v1 = numpy.random.random(3) - 0.5\n    >>> M0 = reflection_matrix(v0, v1)\n    >>> point, normal = reflection_from_matrix(M0)\n    >>> M1 = reflection_matrix(point, normal)\n    >>> is_same_transform(M0, M1)\n    True\n\n    """"""\n    M = numpy.array(matrix, dtype=numpy.float64, copy=False)\n    # normal: unit eigenvector corresponding to eigenvalue -1\n    w, V = numpy.linalg.eig(M[:3, :3])\n    i = numpy.where(abs(numpy.real(w) + 1.0) < 1e-8)[0]\n    if not len(i):\n        raise ValueError(\'no unit eigenvector corresponding to eigenvalue -1\')\n    normal = numpy.real(V[:, i[0]]).squeeze()\n    # point: any unit eigenvector corresponding to eigenvalue 1\n    w, V = numpy.linalg.eig(M)\n    i = numpy.where(abs(numpy.real(w) - 1.0) < 1e-8)[0]\n    if not len(i):\n        raise ValueError(\'no unit eigenvector corresponding to eigenvalue 1\')\n    point = numpy.real(V[:, i[-1]]).squeeze()\n    point /= point[3]\n    return point, normal\n\n\ndef rotation_matrix(angle, direction, point=None):\n    """"""Return matrix to rotate about axis defined by point and direction.\n\n    >>> R = rotation_matrix(math.pi/2, [0, 0, 1], [1, 0, 0])\n    >>> numpy.allclose(numpy.dot(R, [0, 0, 0, 1]), [1, -1, 0, 1])\n    True\n    >>> angle = (random.random() - 0.5) * (2*math.pi)\n    >>> direc = numpy.random.random(3) - 0.5\n    >>> point = numpy.random.random(3) - 0.5\n    >>> R0 = rotation_matrix(angle, direc, point)\n    >>> R1 = rotation_matrix(angle-2*math.pi, direc, point)\n    >>> is_same_transform(R0, R1)\n    True\n    >>> R0 = rotation_matrix(angle, direc, point)\n    >>> R1 = rotation_matrix(-angle, -direc, point)\n    >>> is_same_transform(R0, R1)\n    True\n    >>> I = numpy.identity(4, numpy.float64)\n    >>> numpy.allclose(I, rotation_matrix(math.pi*2, direc))\n    True\n    >>> numpy.allclose(2, numpy.trace(rotation_matrix(math.pi/2,\n    ...                                               direc, point)))\n    True\n\n    """"""\n    sina = math.sin(angle)\n    cosa = math.cos(angle)\n    direction = unit_vector(direction[:3])\n    # rotation matrix around unit vector\n    R = numpy.diag([cosa, cosa, cosa])\n    R += numpy.outer(direction, direction) * (1.0 - cosa)\n    direction *= sina\n    R += numpy.array([[ 0.0,         -direction[2],  direction[1]],\n                      [ direction[2], 0.0,          -direction[0]],\n                      [-direction[1], direction[0],  0.0]])\n    M = numpy.identity(4)\n    M[:3, :3] = R\n    if point is not None:\n        # rotation not around origin\n        point = numpy.array(point[:3], dtype=numpy.float64, copy=False)\n        M[:3, 3] = point - numpy.dot(R, point)\n    return M\n\n\ndef rotation_from_matrix(matrix):\n    """"""Return rotation angle and axis from rotation matrix.\n\n    >>> angle = (random.random() - 0.5) * (2*math.pi)\n    >>> direc = numpy.random.random(3) - 0.5\n    >>> point = numpy.random.random(3) - 0.5\n    >>> R0 = rotation_matrix(angle, direc, point)\n    >>> angle, direc, point = rotation_from_matrix(R0)\n    >>> R1 = rotation_matrix(angle, direc, point)\n    >>> is_same_transform(R0, R1)\n    True\n\n    """"""\n    R = numpy.array(matrix, dtype=numpy.float64, copy=False)\n    R33 = R[:3, :3]\n    # direction: unit eigenvector of R33 corresponding to eigenvalue of 1\n    w, W = numpy.linalg.eig(R33.T)\n    i = numpy.where(abs(numpy.real(w) - 1.0) < 1e-8)[0]\n    if not len(i):\n        raise ValueError(\'no unit eigenvector corresponding to eigenvalue 1\')\n    direction = numpy.real(W[:, i[-1]]).squeeze()\n    # point: unit eigenvector of R33 corresponding to eigenvalue of 1\n    w, Q = numpy.linalg.eig(R)\n    i = numpy.where(abs(numpy.real(w) - 1.0) < 1e-8)[0]\n    if not len(i):\n        raise ValueError(\'no unit eigenvector corresponding to eigenvalue 1\')\n    point = numpy.real(Q[:, i[-1]]).squeeze()\n    point /= point[3]\n    # rotation angle depending on direction\n    cosa = (numpy.trace(R33) - 1.0) / 2.0\n    if abs(direction[2]) > 1e-8:\n        sina = (R[1, 0] + (cosa-1.0)*direction[0]*direction[1]) / direction[2]\n    elif abs(direction[1]) > 1e-8:\n        sina = (R[0, 2] + (cosa-1.0)*direction[0]*direction[2]) / direction[1]\n    else:\n        sina = (R[2, 1] + (cosa-1.0)*direction[1]*direction[2]) / direction[0]\n    angle = math.atan2(sina, cosa)\n    return angle, direction, point\n\n\ndef scale_matrix(factor, origin=None, direction=None):\n    """"""Return matrix to scale by factor around origin in direction.\n\n    Use factor -1 for point symmetry.\n\n    >>> v = (numpy.random.rand(4, 5) - 0.5) * 20\n    >>> v[3] = 1\n    >>> S = scale_matrix(-1.234)\n    >>> numpy.allclose(numpy.dot(S, v)[:3], -1.234*v[:3])\n    True\n    >>> factor = random.random() * 10 - 5\n    >>> origin = numpy.random.random(3) - 0.5\n    >>> direct = numpy.random.random(3) - 0.5\n    >>> S = scale_matrix(factor, origin)\n    >>> S = scale_matrix(factor, origin, direct)\n\n    """"""\n    if direction is None:\n        # uniform scaling\n        M = numpy.diag([factor, factor, factor, 1.0])\n        if origin is not None:\n            M[:3, 3] = origin[:3]\n            M[:3, 3] *= 1.0 - factor\n    else:\n        # nonuniform scaling\n        direction = unit_vector(direction[:3])\n        factor = 1.0 - factor\n        M = numpy.identity(4)\n        M[:3, :3] -= factor * numpy.outer(direction, direction)\n        if origin is not None:\n            M[:3, 3] = (factor * numpy.dot(origin[:3], direction)) * direction\n    return M\n\n\ndef scale_from_matrix(matrix):\n    """"""Return scaling factor, origin and direction from scaling matrix.\n\n    >>> factor = random.random() * 10 - 5\n    >>> origin = numpy.random.random(3) - 0.5\n    >>> direct = numpy.random.random(3) - 0.5\n    >>> S0 = scale_matrix(factor, origin)\n    >>> factor, origin, direction = scale_from_matrix(S0)\n    >>> S1 = scale_matrix(factor, origin, direction)\n    >>> is_same_transform(S0, S1)\n    True\n    >>> S0 = scale_matrix(factor, origin, direct)\n    >>> factor, origin, direction = scale_from_matrix(S0)\n    >>> S1 = scale_matrix(factor, origin, direction)\n    >>> is_same_transform(S0, S1)\n    True\n\n    """"""\n    M = numpy.array(matrix, dtype=numpy.float64, copy=False)\n    M33 = M[:3, :3]\n    factor = numpy.trace(M33) - 2.0\n    try:\n        # direction: unit eigenvector corresponding to eigenvalue factor\n        w, V = numpy.linalg.eig(M33)\n        i = numpy.where(abs(numpy.real(w) - factor) < 1e-8)[0][0]\n        direction = numpy.real(V[:, i]).squeeze()\n        direction /= vector_norm(direction)\n    except IndexError:\n        # uniform scaling\n        factor = (factor + 2.0) / 3.0\n        direction = None\n    # origin: any eigenvector corresponding to eigenvalue 1\n    w, V = numpy.linalg.eig(M)\n    i = numpy.where(abs(numpy.real(w) - 1.0) < 1e-8)[0]\n    if not len(i):\n        raise ValueError(\'no eigenvector corresponding to eigenvalue 1\')\n    origin = numpy.real(V[:, i[-1]]).squeeze()\n    origin /= origin[3]\n    return factor, origin, direction\n\n\ndef projection_matrix(point, normal, direction=None,\n                      perspective=None, pseudo=False):\n    """"""Return matrix to project onto plane defined by point and normal.\n\n    Using either perspective point, projection direction, or none of both.\n\n    If pseudo is True, perspective projections will preserve relative depth\n    such that Perspective = dot(Orthogonal, PseudoPerspective).\n\n    >>> P = projection_matrix([0, 0, 0], [1, 0, 0])\n    >>> numpy.allclose(P[1:, 1:], numpy.identity(4)[1:, 1:])\n    True\n    >>> point = numpy.random.random(3) - 0.5\n    >>> normal = numpy.random.random(3) - 0.5\n    >>> direct = numpy.random.random(3) - 0.5\n    >>> persp = numpy.random.random(3) - 0.5\n    >>> P0 = projection_matrix(point, normal)\n    >>> P1 = projection_matrix(point, normal, direction=direct)\n    >>> P2 = projection_matrix(point, normal, perspective=persp)\n    >>> P3 = projection_matrix(point, normal, perspective=persp, pseudo=True)\n    >>> is_same_transform(P2, numpy.dot(P0, P3))\n    True\n    >>> P = projection_matrix([3, 0, 0], [1, 1, 0], [1, 0, 0])\n    >>> v0 = (numpy.random.rand(4, 5) - 0.5) * 20\n    >>> v0[3] = 1\n    >>> v1 = numpy.dot(P, v0)\n    >>> numpy.allclose(v1[1], v0[1])\n    True\n    >>> numpy.allclose(v1[0], 3-v1[1])\n    True\n\n    """"""\n    M = numpy.identity(4)\n    point = numpy.array(point[:3], dtype=numpy.float64, copy=False)\n    normal = unit_vector(normal[:3])\n    if perspective is not None:\n        # perspective projection\n        perspective = numpy.array(perspective[:3], dtype=numpy.float64,\n                                  copy=False)\n        M[0, 0] = M[1, 1] = M[2, 2] = numpy.dot(perspective-point, normal)\n        M[:3, :3] -= numpy.outer(perspective, normal)\n        if pseudo:\n            # preserve relative depth\n            M[:3, :3] -= numpy.outer(normal, normal)\n            M[:3, 3] = numpy.dot(point, normal) * (perspective+normal)\n        else:\n            M[:3, 3] = numpy.dot(point, normal) * perspective\n        M[3, :3] = -normal\n        M[3, 3] = numpy.dot(perspective, normal)\n    elif direction is not None:\n        # parallel projection\n        direction = numpy.array(direction[:3], dtype=numpy.float64, copy=False)\n        scale = numpy.dot(direction, normal)\n        M[:3, :3] -= numpy.outer(direction, normal) / scale\n        M[:3, 3] = direction * (numpy.dot(point, normal) / scale)\n    else:\n        # orthogonal projection\n        M[:3, :3] -= numpy.outer(normal, normal)\n        M[:3, 3] = numpy.dot(point, normal) * normal\n    return M\n\n\ndef projection_from_matrix(matrix, pseudo=False):\n    """"""Return projection plane and perspective point from projection matrix.\n\n    Return values are same as arguments for projection_matrix function:\n    point, normal, direction, perspective, and pseudo.\n\n    >>> point = numpy.random.random(3) - 0.5\n    >>> normal = numpy.random.random(3) - 0.5\n    >>> direct = numpy.random.random(3) - 0.5\n    >>> persp = numpy.random.random(3) - 0.5\n    >>> P0 = projection_matrix(point, normal)\n    >>> result = projection_from_matrix(P0)\n    >>> P1 = projection_matrix(*result)\n    >>> is_same_transform(P0, P1)\n    True\n    >>> P0 = projection_matrix(point, normal, direct)\n    >>> result = projection_from_matrix(P0)\n    >>> P1 = projection_matrix(*result)\n    >>> is_same_transform(P0, P1)\n    True\n    >>> P0 = projection_matrix(point, normal, perspective=persp, pseudo=False)\n    >>> result = projection_from_matrix(P0, pseudo=False)\n    >>> P1 = projection_matrix(*result)\n    >>> is_same_transform(P0, P1)\n    True\n    >>> P0 = projection_matrix(point, normal, perspective=persp, pseudo=True)\n    >>> result = projection_from_matrix(P0, pseudo=True)\n    >>> P1 = projection_matrix(*result)\n    >>> is_same_transform(P0, P1)\n    True\n\n    """"""\n    M = numpy.array(matrix, dtype=numpy.float64, copy=False)\n    M33 = M[:3, :3]\n    w, V = numpy.linalg.eig(M)\n    i = numpy.where(abs(numpy.real(w) - 1.0) < 1e-8)[0]\n    if not pseudo and len(i):\n        # point: any eigenvector corresponding to eigenvalue 1\n        point = numpy.real(V[:, i[-1]]).squeeze()\n        point /= point[3]\n        # direction: unit eigenvector corresponding to eigenvalue 0\n        w, V = numpy.linalg.eig(M33)\n        i = numpy.where(abs(numpy.real(w)) < 1e-8)[0]\n        if not len(i):\n            raise ValueError(\'no eigenvector corresponding to eigenvalue 0\')\n        direction = numpy.real(V[:, i[0]]).squeeze()\n        direction /= vector_norm(direction)\n        # normal: unit eigenvector of M33.T corresponding to eigenvalue 0\n        w, V = numpy.linalg.eig(M33.T)\n        i = numpy.where(abs(numpy.real(w)) < 1e-8)[0]\n        if len(i):\n            # parallel projection\n            normal = numpy.real(V[:, i[0]]).squeeze()\n            normal /= vector_norm(normal)\n            return point, normal, direction, None, False\n        else:\n            # orthogonal projection, where normal equals direction vector\n            return point, direction, None, None, False\n    else:\n        # perspective projection\n        i = numpy.where(abs(numpy.real(w)) > 1e-8)[0]\n        if not len(i):\n            raise ValueError(\n                \'no eigenvector not corresponding to eigenvalue 0\')\n        point = numpy.real(V[:, i[-1]]).squeeze()\n        point /= point[3]\n        normal = - M[3, :3]\n        perspective = M[:3, 3] / numpy.dot(point[:3], normal)\n        if pseudo:\n            perspective -= normal\n        return point, normal, None, perspective, pseudo\n\n\ndef clip_matrix(left, right, bottom, top, near, far, perspective=False):\n    """"""Return matrix to obtain normalized device coordinates from frustum.\n\n    The frustum bounds are axis-aligned along x (left, right),\n    y (bottom, top) and z (near, far).\n\n    Normalized device coordinates are in range [-1, 1] if coordinates are\n    inside the frustum.\n\n    If perspective is True the frustum is a truncated pyramid with the\n    perspective point at origin and direction along z axis, otherwise an\n    orthographic canonical view volume (a box).\n\n    Homogeneous coordinates transformed by the perspective clip matrix\n    need to be dehomogenized (divided by w coordinate).\n\n    >>> frustum = numpy.random.rand(6)\n    >>> frustum[1] += frustum[0]\n    >>> frustum[3] += frustum[2]\n    >>> frustum[5] += frustum[4]\n    >>> M = clip_matrix(perspective=False, *frustum)\n    >>> numpy.dot(M, [frustum[0], frustum[2], frustum[4], 1])\n    array([-1., -1., -1.,  1.])\n    >>> numpy.dot(M, [frustum[1], frustum[3], frustum[5], 1])\n    array([ 1.,  1.,  1.,  1.])\n    >>> M = clip_matrix(perspective=True, *frustum)\n    >>> v = numpy.dot(M, [frustum[0], frustum[2], frustum[4], 1])\n    >>> v / v[3]\n    array([-1., -1., -1.,  1.])\n    >>> v = numpy.dot(M, [frustum[1], frustum[3], frustum[4], 1])\n    >>> v / v[3]\n    array([ 1.,  1., -1.,  1.])\n\n    """"""\n    if left >= right or bottom >= top or near >= far:\n        raise ValueError(\'invalid frustum\')\n    if perspective:\n        if near <= _EPS:\n            raise ValueError(\'invalid frustum: near <= 0\')\n        t = 2.0 * near\n        M = [[t/(left-right), 0.0, (right+left)/(right-left), 0.0],\n             [0.0, t/(bottom-top), (top+bottom)/(top-bottom), 0.0],\n             [0.0, 0.0, (far+near)/(near-far), t*far/(far-near)],\n             [0.0, 0.0, -1.0, 0.0]]\n    else:\n        M = [[2.0/(right-left), 0.0, 0.0, (right+left)/(left-right)],\n             [0.0, 2.0/(top-bottom), 0.0, (top+bottom)/(bottom-top)],\n             [0.0, 0.0, 2.0/(far-near), (far+near)/(near-far)],\n             [0.0, 0.0, 0.0, 1.0]]\n    return numpy.array(M)\n\n\ndef shear_matrix(angle, direction, point, normal):\n    """"""Return matrix to shear by angle along direction vector on shear plane.\n\n    The shear plane is defined by a point and normal vector. The direction\n    vector must be orthogonal to the plane\'s normal vector.\n\n    A point P is transformed by the shear matrix into P"" such that\n    the vector P-P"" is parallel to the direction vector and its extent is\n    given by the angle of P-P\'-P"", where P\' is the orthogonal projection\n    of P onto the shear plane.\n\n    >>> angle = (random.random() - 0.5) * 4*math.pi\n    >>> direct = numpy.random.random(3) - 0.5\n    >>> point = numpy.random.random(3) - 0.5\n    >>> normal = numpy.cross(direct, numpy.random.random(3))\n    >>> S = shear_matrix(angle, direct, point, normal)\n    >>> numpy.allclose(1, numpy.linalg.det(S))\n    True\n\n    """"""\n    normal = unit_vector(normal[:3])\n    direction = unit_vector(direction[:3])\n    if abs(numpy.dot(normal, direction)) > 1e-6:\n        raise ValueError(\'direction and normal vectors are not orthogonal\')\n    angle = math.tan(angle)\n    M = numpy.identity(4)\n    M[:3, :3] += angle * numpy.outer(direction, normal)\n    M[:3, 3] = -angle * numpy.dot(point[:3], normal) * direction\n    return M\n\n\ndef shear_from_matrix(matrix):\n    """"""Return shear angle, direction and plane from shear matrix.\n\n    >>> angle = (random.random() - 0.5) * 4*math.pi\n    >>> direct = numpy.random.random(3) - 0.5\n    >>> point = numpy.random.random(3) - 0.5\n    >>> normal = numpy.cross(direct, numpy.random.random(3))\n    >>> S0 = shear_matrix(angle, direct, point, normal)\n    >>> angle, direct, point, normal = shear_from_matrix(S0)\n    >>> S1 = shear_matrix(angle, direct, point, normal)\n    >>> is_same_transform(S0, S1)\n    True\n\n    """"""\n    M = numpy.array(matrix, dtype=numpy.float64, copy=False)\n    M33 = M[:3, :3]\n    # normal: cross independent eigenvectors corresponding to the eigenvalue 1\n    w, V = numpy.linalg.eig(M33)\n    i = numpy.where(abs(numpy.real(w) - 1.0) < 1e-4)[0]\n    if len(i) < 2:\n        raise ValueError(\'no two linear independent eigenvectors found %s\' % w)\n    V = numpy.real(V[:, i]).squeeze().T\n    lenorm = -1.0\n    for i0, i1 in ((0, 1), (0, 2), (1, 2)):\n        n = numpy.cross(V[i0], V[i1])\n        w = vector_norm(n)\n        if w > lenorm:\n            lenorm = w\n            normal = n\n    normal /= lenorm\n    # direction and angle\n    direction = numpy.dot(M33 - numpy.identity(3), normal)\n    angle = vector_norm(direction)\n    direction /= angle\n    angle = math.atan(angle)\n    # point: eigenvector corresponding to eigenvalue 1\n    w, V = numpy.linalg.eig(M)\n    i = numpy.where(abs(numpy.real(w) - 1.0) < 1e-8)[0]\n    if not len(i):\n        raise ValueError(\'no eigenvector corresponding to eigenvalue 1\')\n    point = numpy.real(V[:, i[-1]]).squeeze()\n    point /= point[3]\n    return angle, direction, point, normal\n\n\ndef decompose_matrix(matrix):\n    """"""Return sequence of transformations from transformation matrix.\n\n    matrix : array_like\n        Non-degenerative homogeneous transformation matrix\n\n    Return tuple of:\n        scale : vector of 3 scaling factors\n        shear : list of shear factors for x-y, x-z, y-z axes\n        angles : list of Euler angles about static x, y, z axes\n        translate : translation vector along x, y, z axes\n        perspective : perspective partition of matrix\n\n    Raise ValueError if matrix is of wrong type or degenerative.\n\n    >>> T0 = translation_matrix([1, 2, 3])\n    >>> scale, shear, angles, trans, persp = decompose_matrix(T0)\n    >>> T1 = translation_matrix(trans)\n    >>> numpy.allclose(T0, T1)\n    True\n    >>> S = scale_matrix(0.123)\n    >>> scale, shear, angles, trans, persp = decompose_matrix(S)\n    >>> scale[0]\n    0.123\n    >>> R0 = euler_matrix(1, 2, 3)\n    >>> scale, shear, angles, trans, persp = decompose_matrix(R0)\n    >>> R1 = euler_matrix(*angles)\n    >>> numpy.allclose(R0, R1)\n    True\n\n    """"""\n    M = numpy.array(matrix, dtype=numpy.float64, copy=True).T\n    if abs(M[3, 3]) < _EPS:\n        raise ValueError(\'M[3, 3] is zero\')\n    M /= M[3, 3]\n    P = M.copy()\n    P[:, 3] = 0.0, 0.0, 0.0, 1.0\n    if not numpy.linalg.det(P):\n        raise ValueError(\'matrix is singular\')\n\n    scale = numpy.zeros((3, ))\n    shear = [0.0, 0.0, 0.0]\n    angles = [0.0, 0.0, 0.0]\n\n    if any(abs(M[:3, 3]) > _EPS):\n        perspective = numpy.dot(M[:, 3], numpy.linalg.inv(P.T))\n        M[:, 3] = 0.0, 0.0, 0.0, 1.0\n    else:\n        perspective = numpy.array([0.0, 0.0, 0.0, 1.0])\n\n    translate = M[3, :3].copy()\n    M[3, :3] = 0.0\n\n    row = M[:3, :3].copy()\n    scale[0] = vector_norm(row[0])\n    row[0] /= scale[0]\n    shear[0] = numpy.dot(row[0], row[1])\n    row[1] -= row[0] * shear[0]\n    scale[1] = vector_norm(row[1])\n    row[1] /= scale[1]\n    shear[0] /= scale[1]\n    shear[1] = numpy.dot(row[0], row[2])\n    row[2] -= row[0] * shear[1]\n    shear[2] = numpy.dot(row[1], row[2])\n    row[2] -= row[1] * shear[2]\n    scale[2] = vector_norm(row[2])\n    row[2] /= scale[2]\n    shear[1:] /= scale[2]\n\n    if numpy.dot(row[0], numpy.cross(row[1], row[2])) < 0:\n        numpy.negative(scale, scale)\n        numpy.negative(row, row)\n\n    angles[1] = math.asin(-row[0, 2])\n    if math.cos(angles[1]):\n        angles[0] = math.atan2(row[1, 2], row[2, 2])\n        angles[2] = math.atan2(row[0, 1], row[0, 0])\n    else:\n        # angles[0] = math.atan2(row[1, 0], row[1, 1])\n        angles[0] = math.atan2(-row[2, 1], row[1, 1])\n        angles[2] = 0.0\n\n    return scale, shear, angles, translate, perspective\n\n\ndef compose_matrix(scale=None, shear=None, angles=None, translate=None,\n                   perspective=None):\n    """"""Return transformation matrix from sequence of transformations.\n\n    This is the inverse of the decompose_matrix function.\n\n    Sequence of transformations:\n        scale : vector of 3 scaling factors\n        shear : list of shear factors for x-y, x-z, y-z axes\n        angles : list of Euler angles about static x, y, z axes\n        translate : translation vector along x, y, z axes\n        perspective : perspective partition of matrix\n\n    >>> scale = numpy.random.random(3) - 0.5\n    >>> shear = numpy.random.random(3) - 0.5\n    >>> angles = (numpy.random.random(3) - 0.5) * (2*math.pi)\n    >>> trans = numpy.random.random(3) - 0.5\n    >>> persp = numpy.random.random(4) - 0.5\n    >>> M0 = compose_matrix(scale, shear, angles, trans, persp)\n    >>> result = decompose_matrix(M0)\n    >>> M1 = compose_matrix(*result)\n    >>> is_same_transform(M0, M1)\n    True\n\n    """"""\n    M = numpy.identity(4)\n    if perspective is not None:\n        P = numpy.identity(4)\n        P[3, :] = perspective[:4]\n        M = numpy.dot(M, P)\n    if translate is not None:\n        T = numpy.identity(4)\n        T[:3, 3] = translate[:3]\n        M = numpy.dot(M, T)\n    if angles is not None:\n        R = euler_matrix(angles[0], angles[1], angles[2], \'sxyz\')\n        M = numpy.dot(M, R)\n    if shear is not None:\n        Z = numpy.identity(4)\n        Z[1, 2] = shear[2]\n        Z[0, 2] = shear[1]\n        Z[0, 1] = shear[0]\n        M = numpy.dot(M, Z)\n    if scale is not None:\n        S = numpy.identity(4)\n        S[0, 0] = scale[0]\n        S[1, 1] = scale[1]\n        S[2, 2] = scale[2]\n        M = numpy.dot(M, S)\n    M /= M[3, 3]\n    return M\n\n\ndef orthogonalization_matrix(lengths, angles):\n    """"""Return orthogonalization matrix for crystallographic cell coordinates.\n\n    Angles are expected in degrees.\n\n    The de-orthogonalization matrix is the inverse.\n\n    >>> O = orthogonalization_matrix([10, 10, 10], [90, 90, 90])\n    >>> numpy.allclose(O[:3, :3], numpy.identity(3, float) * 10)\n    True\n    >>> O = orthogonalization_matrix([9.8, 12.0, 15.5], [87.2, 80.7, 69.7])\n    >>> numpy.allclose(numpy.sum(O), 43.063229)\n    True\n\n    """"""\n    a, b, c = lengths\n    angles = numpy.radians(angles)\n    sina, sinb, _ = numpy.sin(angles)\n    cosa, cosb, cosg = numpy.cos(angles)\n    co = (cosa * cosb - cosg) / (sina * sinb)\n    return numpy.array([\n        [ a*sinb*math.sqrt(1.0-co*co),  0.0,    0.0, 0.0],\n        [-a*sinb*co,                    b*sina, 0.0, 0.0],\n        [ a*cosb,                       b*cosa, c,   0.0],\n        [ 0.0,                          0.0,    0.0, 1.0]])\n\n\ndef affine_matrix_from_points(v0, v1, shear=True, scale=True, usesvd=True):\n    """"""Return affine transform matrix to register two point sets.\n\n    v0 and v1 are shape (ndims, \\*) arrays of at least ndims non-homogeneous\n    coordinates, where ndims is the dimensionality of the coordinate space.\n\n    If shear is False, a similarity transformation matrix is returned.\n    If also scale is False, a rigid/Euclidean transformation matrix\n    is returned.\n\n    By default the algorithm by Hartley and Zissermann [15] is used.\n    If usesvd is True, similarity and Euclidean transformation matrices\n    are calculated by minimizing the weighted sum of squared deviations\n    (RMSD) according to the algorithm by Kabsch [8].\n    Otherwise, and if ndims is 3, the quaternion based algorithm by Horn [9]\n    is used, which is slower when using this Python implementation.\n\n    The returned matrix performs rotation, translation and uniform scaling\n    (if specified).\n\n    >>> v0 = [[0, 1031, 1031, 0], [0, 0, 1600, 1600]]\n    >>> v1 = [[675, 826, 826, 677], [55, 52, 281, 277]]\n    >>> affine_matrix_from_points(v0, v1)\n    array([[   0.14549,    0.00062,  675.50008],\n           [   0.00048,    0.14094,   53.24971],\n           [   0.     ,    0.     ,    1.     ]])\n    >>> T = translation_matrix(numpy.random.random(3)-0.5)\n    >>> R = random_rotation_matrix(numpy.random.random(3))\n    >>> S = scale_matrix(random.random())\n    >>> M = concatenate_matrices(T, R, S)\n    >>> v0 = (numpy.random.rand(4, 100) - 0.5) * 20\n    >>> v0[3] = 1\n    >>> v1 = numpy.dot(M, v0)\n    >>> v0[:3] += numpy.random.normal(0, 1e-8, 300).reshape(3, -1)\n    >>> M = affine_matrix_from_points(v0[:3], v1[:3])\n    >>> numpy.allclose(v1, numpy.dot(M, v0))\n    True\n\n    More examples in superimposition_matrix()\n\n    """"""\n    v0 = numpy.array(v0, dtype=numpy.float64, copy=True)\n    v1 = numpy.array(v1, dtype=numpy.float64, copy=True)\n\n    ndims = v0.shape[0]\n    if ndims < 2 or v0.shape[1] < ndims or v0.shape != v1.shape:\n        raise ValueError(\'input arrays are of wrong shape or type\')\n\n    # move centroids to origin\n    t0 = -numpy.mean(v0, axis=1)\n    M0 = numpy.identity(ndims+1)\n    M0[:ndims, ndims] = t0\n    v0 += t0.reshape(ndims, 1)\n    t1 = -numpy.mean(v1, axis=1)\n    M1 = numpy.identity(ndims+1)\n    M1[:ndims, ndims] = t1\n    v1 += t1.reshape(ndims, 1)\n\n    if shear:\n        # Affine transformation\n        A = numpy.concatenate((v0, v1), axis=0)\n        u, s, vh = numpy.linalg.svd(A.T)\n        vh = vh[:ndims].T\n        B = vh[:ndims]\n        C = vh[ndims:2*ndims]\n        t = numpy.dot(C, numpy.linalg.pinv(B))\n        t = numpy.concatenate((t, numpy.zeros((ndims, 1))), axis=1)\n        M = numpy.vstack((t, ((0.0,)*ndims) + (1.0,)))\n    elif usesvd or ndims != 3:\n        # Rigid transformation via SVD of covariance matrix\n        u, s, vh = numpy.linalg.svd(numpy.dot(v1, v0.T))\n        # rotation matrix from SVD orthonormal bases\n        R = numpy.dot(u, vh)\n        if numpy.linalg.det(R) < 0.0:\n            # R does not constitute right handed system\n            R -= numpy.outer(u[:, ndims-1], vh[ndims-1, :]*2.0)\n            s[-1] *= -1.0\n        # homogeneous transformation matrix\n        M = numpy.identity(ndims+1)\n        M[:ndims, :ndims] = R\n    else:\n        # Rigid transformation matrix via quaternion\n        # compute symmetric matrix N\n        xx, yy, zz = numpy.sum(v0 * v1, axis=1)\n        xy, yz, zx = numpy.sum(v0 * numpy.roll(v1, -1, axis=0), axis=1)\n        xz, yx, zy = numpy.sum(v0 * numpy.roll(v1, -2, axis=0), axis=1)\n        N = [[xx+yy+zz, 0.0,      0.0,      0.0],\n             [yz-zy,    xx-yy-zz, 0.0,      0.0],\n             [zx-xz,    xy+yx,    yy-xx-zz, 0.0],\n             [xy-yx,    zx+xz,    yz+zy,    zz-xx-yy]]\n        # quaternion: eigenvector corresponding to most positive eigenvalue\n        w, V = numpy.linalg.eigh(N)\n        q = V[:, numpy.argmax(w)]\n        q /= vector_norm(q)  # unit quaternion\n        # homogeneous transformation matrix\n        M = quaternion_matrix(q)\n\n    if scale and not shear:\n        # Affine transformation; scale is ratio of RMS deviations from centroid\n        v0 *= v0\n        v1 *= v1\n        M[:ndims, :ndims] *= math.sqrt(numpy.sum(v1) / numpy.sum(v0))\n\n    # move centroids back\n    M = numpy.dot(numpy.linalg.inv(M1), numpy.dot(M, M0))\n    M /= M[ndims, ndims]\n    return M\n\n\ndef superimposition_matrix(v0, v1, scale=False, usesvd=True):\n    """"""Return matrix to transform given 3D point set into second point set.\n\n    v0 and v1 are shape (3, \\*) or (4, \\*) arrays of at least 3 points.\n\n    The parameters scale and usesvd are explained in the more general\n    affine_matrix_from_points function.\n\n    The returned matrix is a similarity or Euclidean transformation matrix.\n    This function has a fast C implementation in transformations.c.\n\n    >>> v0 = numpy.random.rand(3, 10)\n    >>> M = superimposition_matrix(v0, v0)\n    >>> numpy.allclose(M, numpy.identity(4))\n    True\n    >>> R = random_rotation_matrix(numpy.random.random(3))\n    >>> v0 = [[1,0,0], [0,1,0], [0,0,1], [1,1,1]]\n    >>> v1 = numpy.dot(R, v0)\n    >>> M = superimposition_matrix(v0, v1)\n    >>> numpy.allclose(v1, numpy.dot(M, v0))\n    True\n    >>> v0 = (numpy.random.rand(4, 100) - 0.5) * 20\n    >>> v0[3] = 1\n    >>> v1 = numpy.dot(R, v0)\n    >>> M = superimposition_matrix(v0, v1)\n    >>> numpy.allclose(v1, numpy.dot(M, v0))\n    True\n    >>> S = scale_matrix(random.random())\n    >>> T = translation_matrix(numpy.random.random(3)-0.5)\n    >>> M = concatenate_matrices(T, R, S)\n    >>> v1 = numpy.dot(M, v0)\n    >>> v0[:3] += numpy.random.normal(0, 1e-9, 300).reshape(3, -1)\n    >>> M = superimposition_matrix(v0, v1, scale=True)\n    >>> numpy.allclose(v1, numpy.dot(M, v0))\n    True\n    >>> M = superimposition_matrix(v0, v1, scale=True, usesvd=False)\n    >>> numpy.allclose(v1, numpy.dot(M, v0))\n    True\n    >>> v = numpy.empty((4, 100, 3))\n    >>> v[:, :, 0] = v0\n    >>> M = superimposition_matrix(v0, v1, scale=True, usesvd=False)\n    >>> numpy.allclose(v1, numpy.dot(M, v[:, :, 0]))\n    True\n\n    """"""\n    v0 = numpy.array(v0, dtype=numpy.float64, copy=False)[:3]\n    v1 = numpy.array(v1, dtype=numpy.float64, copy=False)[:3]\n    return affine_matrix_from_points(v0, v1, shear=False,\n                                     scale=scale, usesvd=usesvd)\n\n\ndef euler_matrix(ai, aj, ak, axes=\'sxyz\'):\n    """"""Return homogeneous rotation matrix from Euler angles and axis sequence.\n\n    ai, aj, ak : Euler\'s roll, pitch and yaw angles\n    axes : One of 24 axis sequences as string or encoded tuple\n\n    >>> R = euler_matrix(1, 2, 3, \'syxz\')\n    >>> numpy.allclose(numpy.sum(R[0]), -1.34786452)\n    True\n    >>> R = euler_matrix(1, 2, 3, (0, 1, 0, 1))\n    >>> numpy.allclose(numpy.sum(R[0]), -0.383436184)\n    True\n    >>> ai, aj, ak = (4*math.pi) * (numpy.random.random(3) - 0.5)\n    >>> for axes in _AXES2TUPLE.keys():\n    ...    R = euler_matrix(ai, aj, ak, axes)\n    >>> for axes in _TUPLE2AXES.keys():\n    ...    R = euler_matrix(ai, aj, ak, axes)\n\n    """"""\n    try:\n        firstaxis, parity, repetition, frame = _AXES2TUPLE[axes]\n    except (AttributeError, KeyError):\n        _TUPLE2AXES[axes]  # validation\n        firstaxis, parity, repetition, frame = axes\n\n    i = firstaxis\n    j = _NEXT_AXIS[i+parity]\n    k = _NEXT_AXIS[i-parity+1]\n\n    if frame:\n        ai, ak = ak, ai\n    if parity:\n        ai, aj, ak = -ai, -aj, -ak\n\n    si, sj, sk = math.sin(ai), math.sin(aj), math.sin(ak)\n    ci, cj, ck = math.cos(ai), math.cos(aj), math.cos(ak)\n    cc, cs = ci*ck, ci*sk\n    sc, ss = si*ck, si*sk\n\n    M = numpy.identity(4)\n    if repetition:\n        M[i, i] = cj\n        M[i, j] = sj*si\n        M[i, k] = sj*ci\n        M[j, i] = sj*sk\n        M[j, j] = -cj*ss+cc\n        M[j, k] = -cj*cs-sc\n        M[k, i] = -sj*ck\n        M[k, j] = cj*sc+cs\n        M[k, k] = cj*cc-ss\n    else:\n        M[i, i] = cj*ck\n        M[i, j] = sj*sc-cs\n        M[i, k] = sj*cc+ss\n        M[j, i] = cj*sk\n        M[j, j] = sj*ss+cc\n        M[j, k] = sj*cs-sc\n        M[k, i] = -sj\n        M[k, j] = cj*si\n        M[k, k] = cj*ci\n    return M\n\n\ndef euler_from_matrix(matrix, axes=\'sxyz\'):\n    """"""Return Euler angles from rotation matrix for specified axis sequence.\n\n    axes : One of 24 axis sequences as string or encoded tuple\n\n    Note that many Euler angle triplets can describe one matrix.\n\n    >>> R0 = euler_matrix(1, 2, 3, \'syxz\')\n    >>> al, be, ga = euler_from_matrix(R0, \'syxz\')\n    >>> R1 = euler_matrix(al, be, ga, \'syxz\')\n    >>> numpy.allclose(R0, R1)\n    True\n    >>> angles = (4*math.pi) * (numpy.random.random(3) - 0.5)\n    >>> for axes in _AXES2TUPLE.keys():\n    ...    R0 = euler_matrix(axes=axes, *angles)\n    ...    R1 = euler_matrix(axes=axes, *euler_from_matrix(R0, axes))\n    ...    if not numpy.allclose(R0, R1): print(axes, ""failed"")\n\n    """"""\n    try:\n        firstaxis, parity, repetition, frame = _AXES2TUPLE[axes.lower()]\n    except (AttributeError, KeyError):\n        _TUPLE2AXES[axes]  # validation\n        firstaxis, parity, repetition, frame = axes\n\n    i = firstaxis\n    j = _NEXT_AXIS[i+parity]\n    k = _NEXT_AXIS[i-parity+1]\n\n    M = numpy.array(matrix, dtype=numpy.float64, copy=False)[:3, :3]\n    if repetition:\n        sy = math.sqrt(M[i, j]*M[i, j] + M[i, k]*M[i, k])\n        if sy > _EPS:\n            ax = math.atan2( M[i, j],  M[i, k])\n            ay = math.atan2( sy,       M[i, i])\n            az = math.atan2( M[j, i], -M[k, i])\n        else:\n            ax = math.atan2(-M[j, k],  M[j, j])\n            ay = math.atan2( sy,       M[i, i])\n            az = 0.0\n    else:\n        cy = math.sqrt(M[i, i]*M[i, i] + M[j, i]*M[j, i])\n        if cy > _EPS:\n            ax = math.atan2( M[k, j],  M[k, k])\n            ay = math.atan2(-M[k, i],  cy)\n            az = math.atan2( M[j, i],  M[i, i])\n        else:\n            ax = math.atan2(-M[j, k],  M[j, j])\n            ay = math.atan2(-M[k, i],  cy)\n            az = 0.0\n\n    if parity:\n        ax, ay, az = -ax, -ay, -az\n    if frame:\n        ax, az = az, ax\n    return ax, ay, az\n\n\ndef euler_from_quaternion(quaternion, axes=\'sxyz\'):\n    """"""Return Euler angles from quaternion for specified axis sequence.\n\n    >>> angles = euler_from_quaternion([0.99810947, 0.06146124, 0, 0])\n    >>> numpy.allclose(angles, [0.123, 0, 0])\n    True\n\n    """"""\n    return euler_from_matrix(quaternion_matrix(quaternion), axes)\n\n\ndef quaternion_from_euler(ai, aj, ak, axes=\'sxyz\'):\n    """"""Return quaternion from Euler angles and axis sequence.\n\n    ai, aj, ak : Euler\'s roll, pitch and yaw angles\n    axes : One of 24 axis sequences as string or encoded tuple\n\n    >>> q = quaternion_from_euler(1, 2, 3, \'ryxz\')\n    >>> numpy.allclose(q, [0.435953, 0.310622, -0.718287, 0.444435])\n    True\n\n    """"""\n    try:\n        firstaxis, parity, repetition, frame = _AXES2TUPLE[axes.lower()]\n    except (AttributeError, KeyError):\n        _TUPLE2AXES[axes]  # validation\n        firstaxis, parity, repetition, frame = axes\n\n    i = firstaxis + 1\n    j = _NEXT_AXIS[i+parity-1] + 1\n    k = _NEXT_AXIS[i-parity] + 1\n\n    if frame:\n        ai, ak = ak, ai\n    if parity:\n        aj = -aj\n\n    ai /= 2.0\n    aj /= 2.0\n    ak /= 2.0\n    ci = math.cos(ai)\n    si = math.sin(ai)\n    cj = math.cos(aj)\n    sj = math.sin(aj)\n    ck = math.cos(ak)\n    sk = math.sin(ak)\n    cc = ci*ck\n    cs = ci*sk\n    sc = si*ck\n    ss = si*sk\n\n    q = numpy.empty((4, ))\n    if repetition:\n        q[0] = cj*(cc - ss)\n        q[i] = cj*(cs + sc)\n        q[j] = sj*(cc + ss)\n        q[k] = sj*(cs - sc)\n    else:\n        q[0] = cj*cc + sj*ss\n        q[i] = cj*sc - sj*cs\n        q[j] = cj*ss + sj*cc\n        q[k] = cj*cs - sj*sc\n    if parity:\n        q[j] *= -1.0\n\n    return q\n\n\ndef quaternion_about_axis(angle, axis):\n    """"""Return quaternion for rotation about axis.\n\n    >>> q = quaternion_about_axis(0.123, [1, 0, 0])\n    >>> numpy.allclose(q, [0.99810947, 0.06146124, 0, 0])\n    True\n\n    """"""\n    q = numpy.array([0.0, axis[0], axis[1], axis[2]])\n    qlen = vector_norm(q)\n    if qlen > _EPS:\n        q *= math.sin(angle/2.0) / qlen\n    q[0] = math.cos(angle/2.0)\n    return q\n\n\ndef quaternion_matrix(quaternion):\n    """"""Return homogeneous rotation matrix from quaternion.\n\n    >>> M = quaternion_matrix([0.99810947, 0.06146124, 0, 0])\n    >>> numpy.allclose(M, rotation_matrix(0.123, [1, 0, 0]))\n    True\n    >>> M = quaternion_matrix([1, 0, 0, 0])\n    >>> numpy.allclose(M, numpy.identity(4))\n    True\n    >>> M = quaternion_matrix([0, 1, 0, 0])\n    >>> numpy.allclose(M, numpy.diag([1, -1, -1, 1]))\n    True\n\n    """"""\n    q = numpy.array(quaternion, dtype=numpy.float64, copy=True)\n    n = numpy.dot(q, q)\n    if n < _EPS:\n        return numpy.identity(4)\n    q *= math.sqrt(2.0 / n)\n    q = numpy.outer(q, q)\n    return numpy.array([\n        [1.0-q[2, 2]-q[3, 3],     q[1, 2]-q[3, 0],     q[1, 3]+q[2, 0], 0.0],\n        [    q[1, 2]+q[3, 0], 1.0-q[1, 1]-q[3, 3],     q[2, 3]-q[1, 0], 0.0],\n        [    q[1, 3]-q[2, 0],     q[2, 3]+q[1, 0], 1.0-q[1, 1]-q[2, 2], 0.0],\n        [                0.0,                 0.0,                 0.0, 1.0]])\n\n\ndef quaternion_from_matrix(matrix, isprecise=False):\n    """"""Return quaternion from rotation matrix.\n\n    If isprecise is True, the input matrix is assumed to be a precise rotation\n    matrix and a faster algorithm is used.\n\n    >>> q = quaternion_from_matrix(numpy.identity(4), True)\n    >>> numpy.allclose(q, [1, 0, 0, 0])\n    True\n    >>> q = quaternion_from_matrix(numpy.diag([1, -1, -1, 1]))\n    >>> numpy.allclose(q, [0, 1, 0, 0]) or numpy.allclose(q, [0, -1, 0, 0])\n    True\n    >>> R = rotation_matrix(0.123, (1, 2, 3))\n    >>> q = quaternion_from_matrix(R, True)\n    >>> numpy.allclose(q, [0.9981095, 0.0164262, 0.0328524, 0.0492786])\n    True\n    >>> R = [[-0.545, 0.797, 0.260, 0], [0.733, 0.603, -0.313, 0],\n    ...      [-0.407, 0.021, -0.913, 0], [0, 0, 0, 1]]\n    >>> q = quaternion_from_matrix(R)\n    >>> numpy.allclose(q, [0.19069, 0.43736, 0.87485, -0.083611])\n    True\n    >>> R = [[0.395, 0.362, 0.843, 0], [-0.626, 0.796, -0.056, 0],\n    ...      [-0.677, -0.498, 0.529, 0], [0, 0, 0, 1]]\n    >>> q = quaternion_from_matrix(R)\n    >>> numpy.allclose(q, [0.82336615, -0.13610694, 0.46344705, -0.29792603])\n    True\n    >>> R = random_rotation_matrix()\n    >>> q = quaternion_from_matrix(R)\n    >>> is_same_transform(R, quaternion_matrix(q))\n    True\n    >>> is_same_quaternion(quaternion_from_matrix(R, isprecise=False),\n    ...                    quaternion_from_matrix(R, isprecise=True))\n    True\n    >>> R = euler_matrix(0.0, 0.0, numpy.pi/2.0)\n    >>> is_same_quaternion(quaternion_from_matrix(R, isprecise=False),\n    ...                    quaternion_from_matrix(R, isprecise=True))\n    True\n\n    """"""\n    M = numpy.array(matrix, dtype=numpy.float64, copy=False)[:4, :4]\n    if isprecise:\n        q = numpy.empty((4, ))\n        t = numpy.trace(M)\n        if t > M[3, 3]:\n            q[0] = t\n            q[3] = M[1, 0] - M[0, 1]\n            q[2] = M[0, 2] - M[2, 0]\n            q[1] = M[2, 1] - M[1, 2]\n        else:\n            i, j, k = 0, 1, 2\n            if M[1, 1] > M[0, 0]:\n                i, j, k = 1, 2, 0\n            if M[2, 2] > M[i, i]:\n                i, j, k = 2, 0, 1\n            t = M[i, i] - (M[j, j] + M[k, k]) + M[3, 3]\n            q[i] = t\n            q[j] = M[i, j] + M[j, i]\n            q[k] = M[k, i] + M[i, k]\n            q[3] = M[k, j] - M[j, k]\n            q = q[[3, 0, 1, 2]]\n        q *= 0.5 / math.sqrt(t * M[3, 3])\n    else:\n        m00 = M[0, 0]\n        m01 = M[0, 1]\n        m02 = M[0, 2]\n        m10 = M[1, 0]\n        m11 = M[1, 1]\n        m12 = M[1, 2]\n        m20 = M[2, 0]\n        m21 = M[2, 1]\n        m22 = M[2, 2]\n        # symmetric matrix K\n        K = numpy.array([[m00-m11-m22, 0.0,         0.0,         0.0],\n                         [m01+m10,     m11-m00-m22, 0.0,         0.0],\n                         [m02+m20,     m12+m21,     m22-m00-m11, 0.0],\n                         [m21-m12,     m02-m20,     m10-m01,     m00+m11+m22]])\n        K /= 3.0\n        # quaternion is eigenvector of K that corresponds to largest eigenvalue\n        w, V = numpy.linalg.eigh(K)\n        q = V[[3, 0, 1, 2], numpy.argmax(w)]\n    if q[0] < 0.0:\n        numpy.negative(q, q)\n    return q\n\n\ndef quaternion_multiply(quaternion1, quaternion0):\n    """"""Return multiplication of two quaternions.\n\n    >>> q = quaternion_multiply([4, 1, -2, 3], [8, -5, 6, 7])\n    >>> numpy.allclose(q, [28, -44, -14, 48])\n    True\n\n    """"""\n    w0, x0, y0, z0 = quaternion0\n    w1, x1, y1, z1 = quaternion1\n    return numpy.array([\n        -x1*x0 - y1*y0 - z1*z0 + w1*w0,\n        x1*w0 + y1*z0 - z1*y0 + w1*x0,\n        -x1*z0 + y1*w0 + z1*x0 + w1*y0,\n        x1*y0 - y1*x0 + z1*w0 + w1*z0], dtype=numpy.float64)\n\n\ndef quaternion_conjugate(quaternion):\n    """"""Return conjugate of quaternion.\n\n    >>> q0 = random_quaternion()\n    >>> q1 = quaternion_conjugate(q0)\n    >>> q1[0] == q0[0] and all(q1[1:] == -q0[1:])\n    True\n\n    """"""\n    q = numpy.array(quaternion, dtype=numpy.float64, copy=True)\n    numpy.negative(q[1:], q[1:])\n    return q\n\n\ndef quaternion_inverse(quaternion):\n    """"""Return inverse of quaternion.\n\n    >>> q0 = random_quaternion()\n    >>> q1 = quaternion_inverse(q0)\n    >>> numpy.allclose(quaternion_multiply(q0, q1), [1, 0, 0, 0])\n    True\n\n    """"""\n    q = numpy.array(quaternion, dtype=numpy.float64, copy=True)\n    numpy.negative(q[1:], q[1:])\n    return q / numpy.dot(q, q)\n\n\ndef quaternion_real(quaternion):\n    """"""Return real part of quaternion.\n\n    >>> quaternion_real([3, 0, 1, 2])\n    3.0\n\n    """"""\n    return float(quaternion[0])\n\n\ndef quaternion_imag(quaternion):\n    """"""Return imaginary part of quaternion.\n\n    >>> quaternion_imag([3, 0, 1, 2])\n    array([ 0.,  1.,  2.])\n\n    """"""\n    return numpy.array(quaternion[1:4], dtype=numpy.float64, copy=True)\n\n\ndef quaternion_slerp(quat0, quat1, fraction, spin=0, shortestpath=True):\n    """"""Return spherical linear interpolation between two quaternions.\n\n    >>> q0 = random_quaternion()\n    >>> q1 = random_quaternion()\n    >>> q = quaternion_slerp(q0, q1, 0)\n    >>> numpy.allclose(q, q0)\n    True\n    >>> q = quaternion_slerp(q0, q1, 1, 1)\n    >>> numpy.allclose(q, q1)\n    True\n    >>> q = quaternion_slerp(q0, q1, 0.5)\n    >>> angle = math.acos(numpy.dot(q0, q))\n    >>> numpy.allclose(2, math.acos(numpy.dot(q0, q1)) / angle) or \\\n        numpy.allclose(2, math.acos(-numpy.dot(q0, q1)) / angle)\n    True\n\n    """"""\n    q0 = unit_vector(quat0[:4])\n    q1 = unit_vector(quat1[:4])\n    if fraction == 0.0:\n        return q0\n    elif fraction == 1.0:\n        return q1\n    d = numpy.dot(q0, q1)\n    if abs(abs(d) - 1.0) < _EPS:\n        return q0\n    if shortestpath and d < 0.0:\n        # invert rotation\n        d = -d\n        numpy.negative(q1, q1)\n    angle = math.acos(d) + spin * math.pi\n    if abs(angle) < _EPS:\n        return q0\n    isin = 1.0 / math.sin(angle)\n    q0 *= math.sin((1.0 - fraction) * angle) * isin\n    q1 *= math.sin(fraction * angle) * isin\n    q0 += q1\n    return q0\n\n\ndef random_quaternion(rand=None):\n    """"""Return uniform random unit quaternion.\n\n    rand: array like or None\n        Three independent random variables that are uniformly distributed\n        between 0 and 1.\n\n    >>> q = random_quaternion()\n    >>> numpy.allclose(1, vector_norm(q))\n    True\n    >>> q = random_quaternion(numpy.random.random(3))\n    >>> len(q.shape), q.shape[0]==4\n    (1, True)\n\n    """"""\n    if rand is None:\n        rand = numpy.random.rand(3)\n    else:\n        assert len(rand) == 3\n    r1 = numpy.sqrt(1.0 - rand[0])\n    r2 = numpy.sqrt(rand[0])\n    pi2 = math.pi * 2.0\n    t1 = pi2 * rand[1]\n    t2 = pi2 * rand[2]\n    return numpy.array([numpy.cos(t2)*r2, numpy.sin(t1)*r1,\n                        numpy.cos(t1)*r1, numpy.sin(t2)*r2])\n\n\ndef random_rotation_matrix(rand=None):\n    """"""Return uniform random rotation matrix.\n\n    rand: array like\n        Three independent random variables that are uniformly distributed\n        between 0 and 1 for each returned quaternion.\n\n    >>> R = random_rotation_matrix()\n    >>> numpy.allclose(numpy.dot(R.T, R), numpy.identity(4))\n    True\n\n    """"""\n    return quaternion_matrix(random_quaternion(rand))\n\n\nclass Arcball(object):\n    """"""Virtual Trackball Control.\n\n    >>> ball = Arcball()\n    >>> ball = Arcball(initial=numpy.identity(4))\n    >>> ball.place([320, 320], 320)\n    >>> ball.down([500, 250])\n    >>> ball.drag([475, 275])\n    >>> R = ball.matrix()\n    >>> numpy.allclose(numpy.sum(R), 3.90583455)\n    True\n    >>> ball = Arcball(initial=[1, 0, 0, 0])\n    >>> ball.place([320, 320], 320)\n    >>> ball.setaxes([1, 1, 0], [-1, 1, 0])\n    >>> ball.constrain = True\n    >>> ball.down([400, 200])\n    >>> ball.drag([200, 400])\n    >>> R = ball.matrix()\n    >>> numpy.allclose(numpy.sum(R), 0.2055924)\n    True\n    >>> ball.next()\n\n    """"""\n    def __init__(self, initial=None):\n        """"""Initialize virtual trackball control.\n\n        initial : quaternion or rotation matrix\n\n        """"""\n        self._axis = None\n        self._axes = None\n        self._radius = 1.0\n        self._center = [0.0, 0.0]\n        self._vdown = numpy.array([0.0, 0.0, 1.0])\n        self._constrain = False\n        if initial is None:\n            self._qdown = numpy.array([1.0, 0.0, 0.0, 0.0])\n        else:\n            initial = numpy.array(initial, dtype=numpy.float64)\n            if initial.shape == (4, 4):\n                self._qdown = quaternion_from_matrix(initial)\n            elif initial.shape == (4, ):\n                initial /= vector_norm(initial)\n                self._qdown = initial\n            else:\n                raise ValueError(""initial not a quaternion or matrix"")\n        self._qnow = self._qpre = self._qdown\n\n    def place(self, center, radius):\n        """"""Place Arcball, e.g. when window size changes.\n\n        center : sequence[2]\n            Window coordinates of trackball center.\n        radius : float\n            Radius of trackball in window coordinates.\n\n        """"""\n        self._radius = float(radius)\n        self._center[0] = center[0]\n        self._center[1] = center[1]\n\n    def setaxes(self, *axes):\n        """"""Set axes to constrain rotations.""""""\n        if axes is None:\n            self._axes = None\n        else:\n            self._axes = [unit_vector(axis) for axis in axes]\n\n    @property\n    def constrain(self):\n        """"""Return state of constrain to axis mode.""""""\n        return self._constrain\n\n    @constrain.setter\n    def constrain(self, value):\n        """"""Set state of constrain to axis mode.""""""\n        self._constrain = bool(value)\n\n    def down(self, point):\n        """"""Set initial cursor window coordinates and pick constrain-axis.""""""\n        self._vdown = arcball_map_to_sphere(point, self._center, self._radius)\n        self._qdown = self._qpre = self._qnow\n        if self._constrain and self._axes is not None:\n            self._axis = arcball_nearest_axis(self._vdown, self._axes)\n            self._vdown = arcball_constrain_to_axis(self._vdown, self._axis)\n        else:\n            self._axis = None\n\n    def drag(self, point):\n        """"""Update current cursor window coordinates.""""""\n        vnow = arcball_map_to_sphere(point, self._center, self._radius)\n        if self._axis is not None:\n            vnow = arcball_constrain_to_axis(vnow, self._axis)\n        self._qpre = self._qnow\n        t = numpy.cross(self._vdown, vnow)\n        if numpy.dot(t, t) < _EPS:\n            self._qnow = self._qdown\n        else:\n            q = [numpy.dot(self._vdown, vnow), t[0], t[1], t[2]]\n            self._qnow = quaternion_multiply(q, self._qdown)\n\n    def next(self, acceleration=0.0):\n        """"""Continue rotation in direction of last drag.""""""\n        q = quaternion_slerp(self._qpre, self._qnow, 2.0+acceleration, False)\n        self._qpre, self._qnow = self._qnow, q\n\n    def matrix(self):\n        """"""Return homogeneous rotation matrix.""""""\n        return quaternion_matrix(self._qnow)\n\n\ndef arcball_map_to_sphere(point, center, radius):\n    """"""Return unit sphere coordinates from window coordinates.""""""\n    v0 = (point[0] - center[0]) / radius\n    v1 = (center[1] - point[1]) / radius\n    n = v0*v0 + v1*v1\n    if n > 1.0:\n        # position outside of sphere\n        n = math.sqrt(n)\n        return numpy.array([v0/n, v1/n, 0.0])\n    else:\n        return numpy.array([v0, v1, math.sqrt(1.0 - n)])\n\n\ndef arcball_constrain_to_axis(point, axis):\n    """"""Return sphere point perpendicular to axis.""""""\n    v = numpy.array(point, dtype=numpy.float64, copy=True)\n    a = numpy.array(axis, dtype=numpy.float64, copy=True)\n    v -= a * numpy.dot(a, v)  # on plane\n    n = vector_norm(v)\n    if n > _EPS:\n        if v[2] < 0.0:\n            numpy.negative(v, v)\n        v /= n\n        return v\n    if a[2] == 1.0:\n        return numpy.array([1.0, 0.0, 0.0])\n    return unit_vector([-a[1], a[0], 0.0])\n\n\ndef arcball_nearest_axis(point, axes):\n    """"""Return axis, which arc is nearest to point.""""""\n    point = numpy.array(point, dtype=numpy.float64, copy=False)\n    nearest = None\n    mx = -1.0\n    for axis in axes:\n        t = numpy.dot(arcball_constrain_to_axis(point, axis), point)\n        if t > mx:\n            nearest = axis\n            mx = t\n    return nearest\n\n\n# epsilon for testing whether a number is close to zero\n_EPS = numpy.finfo(float).eps * 4.0\n\n# axis sequences for Euler angles\n_NEXT_AXIS = [1, 2, 0, 1]\n\n# map axes strings to/from tuples of inner axis, parity, repetition, frame\n_AXES2TUPLE = {\n    \'sxyz\': (0, 0, 0, 0), \'sxyx\': (0, 0, 1, 0), \'sxzy\': (0, 1, 0, 0),\n    \'sxzx\': (0, 1, 1, 0), \'syzx\': (1, 0, 0, 0), \'syzy\': (1, 0, 1, 0),\n    \'syxz\': (1, 1, 0, 0), \'syxy\': (1, 1, 1, 0), \'szxy\': (2, 0, 0, 0),\n    \'szxz\': (2, 0, 1, 0), \'szyx\': (2, 1, 0, 0), \'szyz\': (2, 1, 1, 0),\n    \'rzyx\': (0, 0, 0, 1), \'rxyx\': (0, 0, 1, 1), \'ryzx\': (0, 1, 0, 1),\n    \'rxzx\': (0, 1, 1, 1), \'rxzy\': (1, 0, 0, 1), \'ryzy\': (1, 0, 1, 1),\n    \'rzxy\': (1, 1, 0, 1), \'ryxy\': (1, 1, 1, 1), \'ryxz\': (2, 0, 0, 1),\n    \'rzxz\': (2, 0, 1, 1), \'rxyz\': (2, 1, 0, 1), \'rzyz\': (2, 1, 1, 1)}\n\n_TUPLE2AXES = dict((v, k) for k, v in _AXES2TUPLE.items())\n\n\ndef vector_norm(data, axis=None, out=None):\n    """"""Return length, i.e. Euclidean norm, of ndarray along axis.\n\n    >>> v = numpy.random.random(3)\n    >>> n = vector_norm(v)\n    >>> numpy.allclose(n, numpy.linalg.norm(v))\n    True\n    >>> v = numpy.random.rand(6, 5, 3)\n    >>> n = vector_norm(v, axis=-1)\n    >>> numpy.allclose(n, numpy.sqrt(numpy.sum(v*v, axis=2)))\n    True\n    >>> n = vector_norm(v, axis=1)\n    >>> numpy.allclose(n, numpy.sqrt(numpy.sum(v*v, axis=1)))\n    True\n    >>> v = numpy.random.rand(5, 4, 3)\n    >>> n = numpy.empty((5, 3))\n    >>> vector_norm(v, axis=1, out=n)\n    >>> numpy.allclose(n, numpy.sqrt(numpy.sum(v*v, axis=1)))\n    True\n    >>> vector_norm([])\n    0.0\n    >>> vector_norm([1])\n    1.0\n\n    """"""\n    data = numpy.array(data, dtype=numpy.float64, copy=True)\n    if out is None:\n        if data.ndim == 1:\n            return math.sqrt(numpy.dot(data, data))\n        data *= data\n        out = numpy.atleast_1d(numpy.sum(data, axis=axis))\n        numpy.sqrt(out, out)\n        return out\n    else:\n        data *= data\n        numpy.sum(data, axis=axis, out=out)\n        numpy.sqrt(out, out)\n\n\ndef unit_vector(data, axis=None, out=None):\n    """"""Return ndarray normalized by length, i.e. Euclidean norm, along axis.\n\n    >>> v0 = numpy.random.random(3)\n    >>> v1 = unit_vector(v0)\n    >>> numpy.allclose(v1, v0 / numpy.linalg.norm(v0))\n    True\n    >>> v0 = numpy.random.rand(5, 4, 3)\n    >>> v1 = unit_vector(v0, axis=-1)\n    >>> v2 = v0 / numpy.expand_dims(numpy.sqrt(numpy.sum(v0*v0, axis=2)), 2)\n    >>> numpy.allclose(v1, v2)\n    True\n    >>> v1 = unit_vector(v0, axis=1)\n    >>> v2 = v0 / numpy.expand_dims(numpy.sqrt(numpy.sum(v0*v0, axis=1)), 1)\n    >>> numpy.allclose(v1, v2)\n    True\n    >>> v1 = numpy.empty((5, 4, 3))\n    >>> unit_vector(v0, axis=1, out=v1)\n    >>> numpy.allclose(v1, v2)\n    True\n    >>> list(unit_vector([]))\n    []\n    >>> list(unit_vector([1]))\n    [1.0]\n\n    """"""\n    if out is None:\n        data = numpy.array(data, dtype=numpy.float64, copy=True)\n        if data.ndim == 1:\n            data /= math.sqrt(numpy.dot(data, data))\n            return data\n    else:\n        if out is not data:\n            out[:] = numpy.array(data, copy=False)\n        data = out\n    length = numpy.atleast_1d(numpy.sum(data*data, axis))\n    numpy.sqrt(length, length)\n    if axis is not None:\n        length = numpy.expand_dims(length, axis)\n    data /= length\n    if out is None:\n        return data\n\n\ndef random_vector(size):\n    """"""Return array of random doubles in the half-open interval [0.0, 1.0).\n\n    >>> v = random_vector(10000)\n    >>> numpy.all(v >= 0) and numpy.all(v < 1)\n    True\n    >>> v0 = random_vector(10)\n    >>> v1 = random_vector(10)\n    >>> numpy.any(v0 == v1)\n    False\n\n    """"""\n    return numpy.random.random(size)\n\n\ndef vector_product(v0, v1, axis=0):\n    """"""Return vector perpendicular to vectors.\n\n    >>> v = vector_product([2, 0, 0], [0, 3, 0])\n    >>> numpy.allclose(v, [0, 0, 6])\n    True\n    >>> v0 = [[2, 0, 0, 2], [0, 2, 0, 2], [0, 0, 2, 2]]\n    >>> v1 = [[3], [0], [0]]\n    >>> v = vector_product(v0, v1)\n    >>> numpy.allclose(v, [[0, 0, 0, 0], [0, 0, 6, 6], [0, -6, 0, -6]])\n    True\n    >>> v0 = [[2, 0, 0], [2, 0, 0], [0, 2, 0], [2, 0, 0]]\n    >>> v1 = [[0, 3, 0], [0, 0, 3], [0, 0, 3], [3, 3, 3]]\n    >>> v = vector_product(v0, v1, axis=1)\n    >>> numpy.allclose(v, [[0, 0, 6], [0, -6, 0], [6, 0, 0], [0, -6, 6]])\n    True\n\n    """"""\n    return numpy.cross(v0, v1, axis=axis)\n\n\ndef angle_between_vectors(v0, v1, directed=True, axis=0):\n    """"""Return angle between vectors.\n\n    If directed is False, the input vectors are interpreted as undirected axes,\n    i.e. the maximum angle is pi/2.\n\n    >>> a = angle_between_vectors([1, -2, 3], [-1, 2, -3])\n    >>> numpy.allclose(a, math.pi)\n    True\n    >>> a = angle_between_vectors([1, -2, 3], [-1, 2, -3], directed=False)\n    >>> numpy.allclose(a, 0)\n    True\n    >>> v0 = [[2, 0, 0, 2], [0, 2, 0, 2], [0, 0, 2, 2]]\n    >>> v1 = [[3], [0], [0]]\n    >>> a = angle_between_vectors(v0, v1)\n    >>> numpy.allclose(a, [0, 1.5708, 1.5708, 0.95532])\n    True\n    >>> v0 = [[2, 0, 0], [2, 0, 0], [0, 2, 0], [2, 0, 0]]\n    >>> v1 = [[0, 3, 0], [0, 0, 3], [0, 0, 3], [3, 3, 3]]\n    >>> a = angle_between_vectors(v0, v1, axis=1)\n    >>> numpy.allclose(a, [1.5708, 1.5708, 1.5708, 0.95532])\n    True\n\n    """"""\n    v0 = numpy.array(v0, dtype=numpy.float64, copy=False)\n    v1 = numpy.array(v1, dtype=numpy.float64, copy=False)\n    dot = numpy.sum(v0 * v1, axis=axis)\n    dot /= vector_norm(v0, axis=axis) * vector_norm(v1, axis=axis)\n    dot = numpy.clip(dot, -1.0, 1.0)\n    return numpy.arccos(dot if directed else numpy.fabs(dot))\n\n\ndef inverse_matrix(matrix):\n    """"""Return inverse of square transformation matrix.\n\n    >>> M0 = random_rotation_matrix()\n    >>> M1 = inverse_matrix(M0.T)\n    >>> numpy.allclose(M1, numpy.linalg.inv(M0.T))\n    True\n    >>> for size in range(1, 7):\n    ...     M0 = numpy.random.rand(size, size)\n    ...     M1 = inverse_matrix(M0)\n    ...     if not numpy.allclose(M1, numpy.linalg.inv(M0)): print(size)\n\n    """"""\n    return numpy.linalg.inv(matrix)\n\n\ndef concatenate_matrices(*matrices):\n    """"""Return concatenation of series of transformation matrices.\n\n    >>> M = numpy.random.rand(16).reshape((4, 4)) - 0.5\n    >>> numpy.allclose(M, concatenate_matrices(M))\n    True\n    >>> numpy.allclose(numpy.dot(M, M.T), concatenate_matrices(M, M.T))\n    True\n\n    """"""\n    M = numpy.identity(4)\n    for i in matrices:\n        M = numpy.dot(M, i)\n    return M\n\n\ndef is_same_transform(matrix0, matrix1):\n    """"""Return True if two matrices perform same transformation.\n\n    >>> is_same_transform(numpy.identity(4), numpy.identity(4))\n    True\n    >>> is_same_transform(numpy.identity(4), random_rotation_matrix())\n    False\n\n    """"""\n    matrix0 = numpy.array(matrix0, dtype=numpy.float64, copy=True)\n    matrix0 /= matrix0[3, 3]\n    matrix1 = numpy.array(matrix1, dtype=numpy.float64, copy=True)\n    matrix1 /= matrix1[3, 3]\n    return numpy.allclose(matrix0, matrix1)\n\n\ndef is_same_quaternion(q0, q1):\n    """"""Return True if two quaternions are equal.""""""\n    q0 = numpy.array(q0)\n    q1 = numpy.array(q1)\n    return numpy.allclose(q0, q1) or numpy.allclose(q0, -q1)\n\n\ndef _import_module(name, package=None, warn=True, prefix=\'_py_\', ignore=\'_\'):\n    """"""Try import all public attributes from module into global namespace.\n\n    Existing attributes with name clashes are renamed with prefix.\n    Attributes starting with underscore are ignored by default.\n\n    Return True on successful import.\n\n    """"""\n    import warnings\n    from importlib import import_module\n    try:\n        if not package:\n            module = import_module(name)\n        else:\n            module = import_module(\'.\' + name, package=package)\n    except ImportError:\n        if warn:\n            warnings.warn(\'failed to import module %s\' % name)\n    else:\n        for attr in dir(module):\n            if ignore and attr.startswith(ignore):\n                continue\n            if prefix:\n                if attr in globals():\n                    globals()[prefix + attr] = globals()[attr]\n                elif warn:\n                    warnings.warn(\'no Python implementation of \' + attr)\n            globals()[attr] = getattr(module, attr)\n        return True\n\n\n_import_module(\'_transformations\', warn=False)\n\nif __name__ == \'__main__\':\n    import doctest\n    import random  # noqa: used in doctests\n    try:\n        numpy.set_printoptions(suppress=True, precision=5, legacy=\'1.13\')\n    except TypeError:\n        numpy.set_printoptions(suppress=True, precision=5)\n    doctest.testmod()'"
modules/dense_correspondence_manipulation/utils/utils.py,4,"b'from __future__ import print_function\n\n# Basic I/O utils\nimport yaml\nfrom yaml import CLoader\nimport numpy as np\nimport os\nimport sys\nimport time\nimport socket\nimport getpass\nimport fnmatch\nimport random\nimport torch\nimport datetime\nfrom PIL import Image\n\n\n\n\nimport dense_correspondence_manipulation.utils.transformations as transformations\n\ndef getDictFromYamlFilename(filename):\n    """"""\n    Read data from a YAML files\n    """"""\n    return yaml.load(open(filename), Loader=CLoader)\n\ndef saveToYaml(data, filename, flush=False):\n    """"""\n\n    :param data:\n    :type data:\n    :param filename:\n    :type filename:\n    :param flush: Forces a flush to disk if true\n    :type flush: bool\n    :return:\n    :rtype:\n    """"""\n    with open(filename, \'w\') as outfile:\n        yaml.dump(data, outfile, default_flow_style=False)\n        if flush:\n            outfile.flush()\n\n\ndef getDenseCorrespondenceSourceDir():\n    return os.getenv(""DC_SOURCE_DIR"")\n\ndef get_data_dir():\n    return os.getenv(""DC_DATA_DIR"")\n\ndef getPdcPath():\n    """"""\n    For backwards compatibility\n    """"""\n    return get_data_dir()\n\ndef dictFromPosQuat(pos, quat):\n    """"""\n    Make a dictionary from position and quaternion vectors\n    """"""\n    d = dict()\n    d[\'translation\'] = dict()\n    d[\'translation\'][\'x\'] = pos[0]\n    d[\'translation\'][\'y\'] = pos[1]\n    d[\'translation\'][\'z\'] = pos[2]\n\n    d[\'quaternion\'] = dict()\n    d[\'quaternion\'][\'w\'] = quat[0]\n    d[\'quaternion\'][\'x\'] = quat[1]\n    d[\'quaternion\'][\'y\'] = quat[2]\n    d[\'quaternion\'][\'z\'] = quat[3]\n\n    return d\n\n\ndef getQuaternionFromDict(d):\n    """"""\n    Get the quaternion from a dict describing a transform. The dict entry could be\n    one of orientation, rotation, quaternion depending on the convention\n    """"""\n    quat = None\n    quatNames = [\'orientation\', \'rotation\', \'quaternion\']\n    for name in quatNames:\n        if name in d:\n            quat = d[name]\n\n\n    if quat is None:\n        raise ValueError(""Error when trying to extract quaternion from dict, your dict doesn\'t contain a key in [\'orientation\', \'rotation\', \'quaternion\']"")\n\n    return quat\n\ndef getPaddedString(idx, width=6):\n    return str(idx).zfill(width)\n\ndef set_cuda_visible_devices(gpu_list):\n    """"""\n    Sets CUDA_VISIBLE_DEVICES environment variable to only show certain gpus\n    If gpu_list is empty does nothing\n    :param gpu_list: list of gpus to set as visible\n    :return: None\n    """"""\n\n    if len(gpu_list) == 0:\n        print(""using all CUDA gpus"")\n        return\n\n    cuda_visible_devices = """"\n    for gpu in gpu_list:\n        cuda_visible_devices += str(gpu) + "",""\n\n    print(""setting CUDA_VISIBLE_DEVICES = "", cuda_visible_devices)\n    os.environ[""CUDA_VISIBLE_DEVICES""] = cuda_visible_devices\n\ndef set_default_cuda_visible_devices():\n    config = get_defaults_config()\n    host_name = socket.gethostname()\n    user_name = getpass.getuser()\n    if host_name in config:\n        if user_name in config[host_name]:\n            gpu_list = config[host_name][user_name][""cuda_visible_devices""]\n            set_cuda_visible_devices(gpu_list)\n\ndef get_defaults_config():\n    dc_source_dir = getDenseCorrespondenceSourceDir()\n    default_config_file = os.path.join(dc_source_dir, \'config\', \'defaults.yaml\')\n\n    return getDictFromYamlFilename(default_config_file)\n\n\ndef add_dense_correspondence_to_python_path():\n    dc_source_dir = getDenseCorrespondenceSourceDir()\n    sys.path.append(dc_source_dir)\n\n    # TODO Pete: potentially only add the pytorch-segmentation-detection stuff \n    # if using this backbone architecture\n    sys.path.append(os.path.join(dc_source_dir, \'external/pytorch-segmentation-detection\'))\n\n    # for some reason it is critical that this be at the beginning . . .\n    sys.path.insert(0, os.path.join(dc_source_dir, \'external/pytorch-segmentation-detection\', \'vision\'))\n\n\ndef convert_to_absolute_path(path):\n    """"""\n    Converts a potentially relative path to an absolute path by pre-pending the home directory\n    :param path: absolute or relative path\n    :type path: str\n    :return: absolute path\n    :rtype: str\n    """"""\n\n    if os.path.isdir(path):\n        return path\n\n\n    home_dir = os.path.expanduser(""~"")\n    return os.path.join(home_dir, path)\n\ndef convert_data_relative_path_to_absolute_path(path, assert_path_exists=False):\n    """"""\n    Expands a path that is relative to the DC_DATA_DIR\n    returned by `get_data_dir()`.\n\n    If the path is already an absolute path then just return the path\n    :param path:\n    :type path:\n    :param assert_path_exists: if you know this path should exist, then try to resolve it using a backwards compatibility check\n    :return:\n    :rtype:\n    """"""\n\n    if os.path.isabs(path):\n        return path\n\n    full_path = os.path.join(get_data_dir(), path)\n\n    if assert_path_exists:\n        if not os.path.exists(full_path):\n            # try a backwards compatibility check for old style\n            # ""code/data_volume/pdc/<path>"" rather than <path>\n            start_path = ""code/data_volume/pdc""\n            rel_path = os.path.relpath(path, start_path)\n            full_path = os.path.join(get_data_dir(), rel_path)\n        \n        if not os.path.exists(full_path):\n            raise ValueError(""full_path %s not found, you asserted that path exists"" %(full_path))\n\n\n    return full_path\n\n\ndef get_current_time_unique_name():\n    """"""\n    Converts current date to a unique name\n    :return:\n    :rtype: str\n    """"""\n\n    unique_name = time.strftime(""%Y%m%d-%H%M%S"")\n    return unique_name\n\ndef homogenous_transform_from_dict(d):\n    """"""\n    Returns a transform from a standard encoding in dict format\n    :param d:\n    :return:\n    """"""\n    pos = [0]*3\n    pos[0] = d[\'translation\'][\'x\']\n    pos[1] = d[\'translation\'][\'y\']\n    pos[2] = d[\'translation\'][\'z\']\n\n    quatDict = getQuaternionFromDict(d)\n    quat = [0]*4\n    quat[0] = quatDict[\'w\']\n    quat[1] = quatDict[\'x\']\n    quat[2] = quatDict[\'y\']\n    quat[3] = quatDict[\'z\']\n\n    transform_matrix = transformations.quaternion_matrix(quat)\n    transform_matrix[0:3,3] = np.array(pos)\n\n    return transform_matrix\n\ndef compute_distance_between_poses(pose_a, pose_b):\n    """"""\n    Computes the linear difference between pose_a and pose_b\n    :param pose_a: 4 x 4 homogeneous transform\n    :type pose_a:\n    :param pose_b:\n    :type pose_b:\n    :return: Distance between translation component of the poses\n    :rtype:\n    """"""\n\n    pos_a = pose_a[0:3,3]\n    pos_b = pose_b[0:3,3]\n\n    return np.linalg.norm(pos_a - pos_b)\n\ndef compute_angle_between_quaternions(q, r):\n    """"""\n    Computes the angle between two quaternions.\n\n    theta = arccos(2 * <q1, q2>^2 - 1)\n\n    See https://math.stackexchange.com/questions/90081/quaternion-distance\n    :param q: numpy array in form [w,x,y,z]. As long as both q,r are consistent it doesn\'t matter\n    :type q:\n    :param r:\n    :type r:\n    :return: angle between the quaternions, in radians\n    :rtype:\n    """"""\n\n    theta = 2*np.arccos(2 * np.dot(q,r)**2 - 1)\n    return theta\n\ndef compute_angle_between_poses(pose_a, pose_b):\n    """"""\n    Computes the angle distance in radians between two homogenous transforms\n    :param pose_a: 4 x 4 homogeneous transform\n    :type pose_a:\n    :param pose_b:\n    :type pose_b:\n    :return: Angle between poses in radians\n    :rtype:\n    """"""\n\n    quat_a = transformations.quaternion_from_matrix(pose_a)\n    quat_b = transformations.quaternion_from_matrix(pose_b)\n\n    return compute_angle_between_quaternions(quat_a, quat_b)\n\n\n\ndef get_model_param_file_from_directory(model_folder, iteration=None):\n    """"""\n    Gets the 003500.pth and 003500.pth.opt files from the specified folder\n\n    :param model_folder: location of the folder containing the param files 001000.pth. Can be absolute or relative path. If relative then it is relative to pdc/trained_models/\n    :type model_folder:\n    :param iteration: which index to use, e.g. 3500, if None it loads the latest one\n    :type iteration:\n    :return: model_param_file, optim_param_file, iteration\n    :rtype: str, str, int\n    """"""\n\n    if not os.path.isdir(model_folder):\n        pdc_path = getPdcPath()\n        model_folder = os.path.join(pdc_path, ""trained_models"", model_folder)\n\n    # find idx.pth and idx.pth.opt files\n    if iteration is None:\n        files = os.listdir(model_folder)\n        model_param_file = sorted(fnmatch.filter(files, \'*.pth\'))[-1]\n        iteration = int(model_param_file.split(""."")[0])\n        optim_param_file = sorted(fnmatch.filter(files, \'*.pth.opt\'))[-1]\n    else:\n        prefix = getPaddedString(iteration, width=6)\n        model_param_file = prefix + "".pth""\n        optim_param_file = prefix + "".pth.opt""\n\n    model_param_file = os.path.join(model_folder, model_param_file)\n    optim_param_file = os.path.join(model_folder, optim_param_file)\n\n    return model_param_file, optim_param_file, iteration\n\n\ndef flattened_pixel_locations_to_u_v(flat_pixel_locations, image_width):\n    """"""\n    :param flat_pixel_locations: A torch.LongTensor of shape torch.Shape([n,1]) where each element\n     is a flattened pixel index, i.e. some integer between 0 and 307,200 for a 640x480 image\n\n    :type flat_pixel_locations: torch.LongTensor\n\n    :return A tuple torch.LongTensor in (u,v) format\n    the pixel and the second column is the v coordinate\n\n    """"""\n    return (flat_pixel_locations%image_width, flat_pixel_locations/image_width)\n\ndef uv_to_flattened_pixel_locations(uv_tuple, image_width):\n    """"""\n    Converts to a flat tensor\n    """"""\n    flat_pixel_locations = uv_tuple[1]*image_width + uv_tuple[0]\n    return flat_pixel_locations\n\ndef reset_random_seed():\n    SEED = 1\n    random.seed(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n\n\ndef load_rgb_image(rgb_filename):\n    """"""\n    Returns PIL.Image.Image\n    :param rgb_filename:\n    :type rgb_filename:\n    :return:\n    :rtype: PIL.Image.Image\n    """"""\n    return Image.open(rgb_filename).convert(\'RGB\')\n\ndef pil_image_to_cv2(pil_image):\n    """"""\n    Converts a PIL image to a cv2 image\n    Need to convert between BGR and RGB\n    :param pil_image:\n    :type pil_image:\n    :return: np.array [H,W,3]\n    :rtype:\n    """"""\n    return np.array(pil_image)[:, :, ::-1].copy() # open and convert between BGR and RGB\n\ndef get_current_YYYY_MM_DD_hh_mm_ss():\n    """"""\n    Returns a string identifying the current:\n    - year, month, day, hour, minute, second\n\n    Using this format:\n\n    YYYY-MM-DD-hh-mm-ss\n\n    For example:\n\n    2018-04-07-19-02-50\n\n    Note: this function will always return strings of the same length.\n\n    :return: current time formatted as a string\n    :rtype: string\n\n    """"""\n\n    now = datetime.datetime.now()\n    string =  ""%0.4d-%0.2d-%0.2d-%0.2d-%0.2d-%0.2d"" % (now.year, now.month, now.day, now.hour, now.minute, now.second)\n    return string\n\n\ndef get_unique_string():\n    """"""\n    Returns a unique string based on current date and a random number\n    :return:\n    :rtype:\n    """"""\n\n    string = get_current_YYYY_MM_DD_hh_mm_ss() + ""_"" + str(random.randint(0,1000))\n    return string\n\nclass CameraIntrinsics(object):\n    """"""\n    Useful class for wrapping camera intrinsics and loading them from a\n    camera_info.yaml file\n    """"""\n    def __init__(self, cx, cy, fx, fy, width, height):\n        self.cx = cx\n        self.cy = cy\n        self.fx = fx\n        self.fy = fy\n        self.width = width\n        self.height = height\n\n        self.K = self.get_camera_matrix()\n\n    def get_camera_matrix(self):\n        return np.array([[self.fx, 0, self.cx], [0, self.fy, self.cy], [0,0,1]])\n\n    @staticmethod\n    def from_yaml_file(filename):\n        config = getDictFromYamlFilename(filename)\n\n        fx = config[\'camera_matrix\'][\'data\'][0]\n        cx = config[\'camera_matrix\'][\'data\'][2]\n\n        fy = config[\'camera_matrix\'][\'data\'][4]\n        cy = config[\'camera_matrix\'][\'data\'][5]\n\n        width = config[\'image_width\']\n        height = config[\'image_height\']\n\n        return CameraIntrinsics(cx, cy, fx, fy, width, height)\n\n'"
modules/dense_correspondence_manipulation/utils/visualization.py,0,"b'import sys\nimport os\nimport cv2\nimport numpy as np\nimport copy\n\n\ndef compute_gaussian_kernel_heatmap_from_norm_diffs(norm_diffs, variance):\n    """"""\n    Computes and RGB heatmap from norm diffs\n    :param norm_diffs: distances in descriptor space to a given keypoint\n    :type norm_diffs: numpy array of shape [H,W]\n    :param variance: the variance of the kernel\n    :type variance:\n    :return: RGB image [H,W,3]\n    :rtype:\n    """"""\n\n    """"""\n    Computes an RGB heatmap from the norm_diffs\n    :param norm_diffs:\n    :type norm_diffs:\n    :return:\n    :rtype:\n    """"""\n\n    heatmap = np.copy(norm_diffs)\n\n    heatmap = np.exp(-heatmap / variance)  # these are now in [0,1]\n    heatmap *= 255\n    heatmap = heatmap.astype(np.uint8)\n    heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    return heatmap_color\n\ndef draw_reticle(img, u, v, label_color):\n    """"""\n    Draws a reticle on the image at the given (u,v) position\n\n    :param img:\n    :type img:\n    :param u:\n    :type u:\n    :param v:\n    :type v:\n    :param label_color:\n    :type label_color:\n    :return:\n    :rtype:\n    """"""\n    white = (255, 255, 255)\n    cv2.circle(img, (u, v), 10, label_color, 1)\n    cv2.circle(img, (u, v), 11, white, 1)\n    cv2.circle(img, (u, v), 12, label_color, 1)\n    cv2.line(img, (u, v + 1), (u, v + 3), white, 1)\n    cv2.line(img, (u + 1, v), (u + 3, v), white, 1)\n    cv2.line(img, (u, v - 1), (u, v - 3), white, 1)\n    cv2.line(img, (u - 1, v), (u - 3, v), white, 1)\n\n'"
