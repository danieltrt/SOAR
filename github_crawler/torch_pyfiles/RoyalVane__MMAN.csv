file_path,api_count,code
test.py,0,"b""import time\nimport os\nfrom options.test_options import TestOptions\nfrom data.data_loader import CreateDataLoader\nfrom models.models import create_model\nfrom util.visualizer import Visualizer\nfrom util import html\n\nopt = TestOptions().parse()\nopt.nThreads = 1   # test code only supports nThreads = 1\nopt.batchSize = 1  # test code only supports batchSize = 1\nopt.serial_batches = True  # no shuffle\nopt.no_flip = True  # no flip\n\ndata_loader = CreateDataLoader(opt)\ndataset = data_loader.load_data()\nmodel = create_model(opt)\nvisualizer = Visualizer(opt)\n# create website\nweb_dir = os.path.join(opt.results_dir, opt.name, '%s_%s' % (opt.phase, opt.which_epoch))\nwebpage = html.HTML(web_dir, 'Experiment = %s, Phase = %s, Epoch = %s' % (opt.name, opt.phase, opt.which_epoch))\n# test\nfor i, data in enumerate(dataset):\n    if i >= opt.how_many:\n        break\n    model.set_input(data)\n    model.test()\n    visuals = model.get_current_visuals()\n    img_path = model.get_image_paths()\n    print('process image... %s' % img_path)\n    visualizer.save_images(webpage, visuals, img_path)\n\nwebpage.save()\n"""
train.py,0,"b""import time\nfrom options.train_options import TrainOptions\nfrom data.data_loader import CreateDataLoader\nfrom models.models import create_model\nfrom util.visualizer import Visualizer\n\nopt = TrainOptions().parse()\ndata_loader = CreateDataLoader(opt)\ndataset = data_loader.load_data()\ndataset_size = len(data_loader)\nprint('#training images = %d' % dataset_size)\n\nmodel = create_model(opt)\nvisualizer = Visualizer(opt)\ntotal_steps = 0\n\nfor epoch in range(opt.epoch_count, opt.niter + opt.niter_decay + 1):\n    epoch_start_time = time.time()\n    epoch_iter = 0\n    \n    for i, data in enumerate(dataset):\n        iter_start_time = time.time()\n        visualizer.reset()\n        total_steps += opt.batchSize\n        epoch_iter += opt.batchSize\n        model.set_input(data)\n        model.optimize_parameters()\n\n        if total_steps % opt.display_freq == 0:\n            save_result = total_steps % opt.update_html_freq == 0\n            visualizer.display_current_results(model.get_current_visuals(), epoch, save_result)\n\n        if total_steps % opt.print_freq == 0:\n            errors = model.get_current_errors()\n            t = (time.time() - iter_start_time) / opt.batchSize\n            visualizer.print_current_errors(epoch, epoch_iter, errors, t)\n            if opt.display_id > 0:\n                visualizer.plot_current_errors(epoch, float(epoch_iter)/dataset_size, opt, errors)\n\n        if total_steps % opt.save_latest_freq == 0:\n            print('saving the latest model (epoch %d, total_steps %d)' %\n                  (epoch, total_steps))\n            model.save('latest')\n\n    if epoch % opt.save_epoch_freq == 0:\n        print('saving the model at the end of epoch %d, iters %d' %\n              (epoch, total_steps))\n        model.save('latest')\n        model.save(epoch)\n\n    print('End of epoch %d / %d \\t Time Taken: %d sec' %\n          (epoch, opt.niter + opt.niter_decay, time.time() - epoch_start_time))\n    model.update_learning_rate()\n"""
data/__init__.py,0,b''
data/aligned_dataset.py,11,"b""import os.path\nimport random\nimport torchvision.transforms as transforms\nimport torch\nimport torch.nn as nn\nimport math\nimport numpy as np\nfrom data.base_dataset import BaseDataset\nfrom data.image_folder import make_dataset\nfrom PIL import Image, ImageOps\n\ndef channel_1toN(img, num_channel):\n    transform1 = transforms.Compose([transforms.ToTensor(),])\n    img = (transform1(img) * 255.0).long()\n    T = torch.LongTensor(num_channel, img.size(1), img.size(2)).zero_()\n    #N = (torch.rand(num_channel, img.size(1), img.size(2)) - 0.5)/random.uniform(1e10, 1e25)#Noise\n    mask = torch.LongTensor(img.size(1), img.size(2)).zero_()\n    for i in range(num_channel):\n        T[i] = T[i] + i\n        layer = T[i] - img\n        T[i] = torch.from_numpy(np.logical_not(np.logical_xor(layer.numpy(), mask.numpy())).astype(int))\n    return T.float()\n\ndef channel_1to1(img):\n    transform1 = transforms.Compose([transforms.ToTensor(),])\n    T = torch.LongTensor(img.height, img.width).zero_()\n    img = (transform1(img) * 255.0).long()\n    T.resize_(img[0].size()).copy_(img[0])\n    return T.long()\n    \ndef swap_1(T, m, n): #Distinguish left & right\n    A = T.numpy()\n    m_mask = np.where(A == m, 1, 0)\n    n_mask = np.where(A == n, 1, 0)\n    A = A + (n - m)*m_mask + (m - n)*n_mask\n    return torch.from_numpy(A)\n\ndef swap_N(T, m, n): #Distinguish left & right\n    A = T.numpy()\n    A[[m, n], :, :] = A[[n, m], :, :]\n    return torch.from_numpy(A)\n    \ndef get_label(T, num_channel):\n    A = T.numpy()\n    R = torch.FloatTensor(num_channel).zero_()\n    for i in range(num_channel):\n        if (A == i).any():\n            R[i] = 1\n    R = R[1:]\n    return R\n\nclass parts_crop():\n    def __init__(self, img, attribute):\n        self.img = img\n        self.attribute = attribute\n        self.parts_bag = []\n    \n    def get_parts(self):\n        array = np.asarray(self.img)\n        for i in range(1, self.attribute.size(0)):\n            w1 = 0\n            w2 = array.shape[1] - 1\n            h1 = 0\n            h2 = array.shape[0] - 1\n            if self.attribute[i]:\n                while w1 < array.shape[1]:\n                    if((array[:,w1] == i).any()):\n                        break\n                    w1  = w1 + 1\n                \n                while w2 > 0:\n                    if((array[:,w2] == i).any()):\n                        break\n                    w2  = w2 - 1\n                        \n                while h1 < array.shape[0]:\n                    if((array[h1,:] == i).any()):\n                        break\n                    h1  = h1 + 1\n                        \n                while h2 > 0:\n                    if((array[h2,:] == i).any()):\n                        break\n                    h2  = h2 - 1\n                    \n                self.parts_bag.append(self.img.crop((w1, h1, w2, h2)))\n\nclass AlignedDataset(BaseDataset):\n    def initialize(self, opt):\n        self.opt = opt\n        self.root = opt.dataroot\n        self.dir_A = os.path.join(opt.dataroot, opt.phase+ '_' + opt.dataset + '_A')\n        self.A_paths = sorted(make_dataset(self.dir_A))\n        self.dir_B = os.path.join(opt.dataroot, opt.phase+ '_' + opt.dataset + '_B')\n        self.B_paths = sorted(make_dataset(self.dir_B))\n\n        assert(len(self.A_paths) == len(self.B_paths))\n        assert(opt.resize_or_crop == 'resize_and_crop')\n        \n        transform_list = [transforms.ToTensor(),\n                          transforms.Normalize((0.485, 0.456, 0.406),\n                                               (0.229, 0.224, 0.225))]\n        self.transform = transforms.Compose(transform_list)\n        \n    def __getitem__(self, index):\n\n        A_path = self.A_paths[index]\n        A = Image.open(A_path)\n        A = A.resize((self.opt.loadSize , self.opt.loadSize), Image.LANCZOS)\n        A_S = A.resize((int(self.opt.loadSize * 0.75), int(self.opt.loadSize * 0.75)), Image.LANCZOS)\n        A_L = A.resize((int(self.opt.loadSize * 1.25), int(self.opt.loadSize * 1.25)), Image.LANCZOS)\n        A_attribute = A.resize((int(self.opt.fineSize/16) , int(self.opt.fineSize/16)), Image.LANCZOS)\n        \n        B_path = self.B_paths[index]\n        B = Image.open(B_path)\n        B = B.resize((self.opt.loadSize , self.opt.loadSize), Image.NEAREST)\n        \n        if self.opt.loadSize > self.opt.fineSize:\n            if random.random() < 0.4:\n                area = A.size[0] * A.size[1]\n                target_area = random.uniform(0.64, 1) * area\n                aspect_ratio = random.uniform(4. / 5, 5. / 4)\n    \n                w = min(int(round(math.sqrt(target_area * aspect_ratio))), self.opt.loadSize)\n                h = min(int(round(math.sqrt(target_area / aspect_ratio))), self.opt.loadSize)\n    \n                if random.random() < 0.5:\n                    w, h = h, w\n    \n                if w <= A.size[0] and h <= A.size[1]:\n                    x1 = random.randint(0, A.size[0] - w)\n                    y1 = random.randint(0, A.size[1] - h)\n    \n                    A = A.crop((x1, y1, x1 + w, y1 + h))\n                    B = B.crop((x1, y1, x1 + w, y1 + h))\n                    assert(A.size == (w, h))\n                \n                A = A.resize((self.opt.fineSize , self.opt.fineSize), Image.LANCZOS)\n                B = B.resize((self.opt.fineSize , self.opt.fineSize), Image.NEAREST)\n            \n            elif  0.4 < random.random() < 0.95:\n                w_offset = random.randint(0, max(0, A.size[1] - self.opt.fineSize - 1))\n                h_offset = random.randint(0, max(0, A.size[0] - self.opt.fineSize - 1))\n                A = A.crop((w_offset, h_offset, w_offset + self.opt.fineSize, h_offset + self.opt.fineSize))\n                B = B.crop((w_offset, h_offset, w_offset + self.opt.fineSize, h_offset + self.opt.fineSize))\n            \n            else:\n                A = A.resize((self.opt.fineSize , self.opt.fineSize), Image.LANCZOS)\n                B = B.resize((self.opt.fineSize , self.opt.fineSize), Image.NEAREST)\n        \n        A = self.transform(A)\n        A_S = self.transform(A_S)\n        A_L = self.transform(A_L)\n        A_attribute = self.transform(A_attribute)\n\n        B_L1 = channel_1to1(B)# single channel long tensor\n        B_attribute_L1 = B.resize((int(self.opt.fineSize/16) , int(self.opt.fineSize/16)), Image.NEAREST)\n        B = channel_1toN(B, self.opt.output_nc) # multi channel float tensor\n        B_attribute_GAN = channel_1toN(B_attribute_L1, self.opt.output_nc) # multi channel float tensor for thumbnail\n        B_attribute_L1 = channel_1to1(B_attribute_L1)\n                \n        if self.opt.which_direction == 'BtoA':\n            input_nc = self.opt.output_nc\n            output_nc = self.opt.input_nc\n        else:\n            input_nc = self.opt.input_nc\n            output_nc = self.opt.output_nc\n\n        if (not self.opt.no_flip) and random.random() < 0.5:\n            idx = [i for i in range(A.size(2) - 1, -1, -1)]\n            idx = torch.LongTensor(idx)\n            idx_2 = [i for i in range(B_attribute_L1.size(1) - 1, -1, -1)]\n            idx_2 = torch.LongTensor(idx_2)\n            A = A.index_select(2, idx)\n            A_attribute = A_attribute.index_select(2, idx_2)\n            B_attribute_GAN = B_attribute_GAN.index_select(2, idx_2)\n            B = B.index_select(2, idx)\n            B_attribute_L1 = B_attribute_L1.index_select(1, idx_2)\n            B_L1 = B_L1.index_select(1, idx)\n            if self.opt.dataset == 'LIP':\n                B = swap_N(B, 14, 15)\n                B = swap_N(B, 16, 17)\n                B = swap_N(B, 18, 19)\n                B_attribute_GAN = swap_N(B_attribute_GAN, 14, 15)\n                B_attribute_GAN = swap_N(B_attribute_GAN, 16, 17)\n                B_attribute_GAN = swap_N(B_attribute_GAN, 18, 19)\n                B_attribute_L1 = swap_1(B_attribute_L1, 14, 15)\n                B_attribute_L1 = swap_1(B_attribute_L1, 16, 17)\n                B_attribute_L1 = swap_1(B_attribute_L1, 18, 19)\n                B_L1 = swap_1(B_L1, 14, 15)\n                B_L1 = swap_1(B_L1, 16, 17)\n                B_L1 = swap_1(B_L1, 18, 19)\n                \n        return {'A': A, 'A_S': A_S, 'A_L': A_L, 'B_L1': B_L1, 'B_GAN': B, \n                'A_Attribute': A_attribute, \n                'B_Attribute_L1': B_attribute_L1, \n                'B_Attribute_GAN': B_attribute_GAN, \n                'A_paths': A_path, 'B_paths': B_path}\n    def __len__(self):\n        return len(self.A_paths)\n\n    def name(self):\n        return 'AlignedDataset'\n"""
data/base_data_loader.py,0,"b'\nclass BaseDataLoader():\n    def __init__(self):\n        pass\n    \n    def initialize(self, opt):\n        self.opt = opt\n        pass\n\n    def load_data():\n        return None\n\n        \n        \n'"
data/base_dataset.py,1,"b""import torch.utils.data as data\nfrom PIL import Image\nimport torchvision.transforms as transforms\n\nclass BaseDataset(data.Dataset):\n    def __init__(self):\n        super(BaseDataset, self).__init__()\n\n    def name(self):\n        return 'BaseDataset'\n\n    def initialize(self, opt):\n        pass\n\ndef get_transform(opt):\n    transform_list = []\n    if opt.resize_or_crop == 'resize_and_crop':\n        osize = [opt.loadSize, opt.loadSize]\n        transform_list.append(transforms.Scale(osize, Image.BICUBIC))\n        transform_list.append(transforms.RandomCrop(opt.fineSize))\n    elif opt.resize_or_crop == 'crop':\n        transform_list.append(transforms.RandomCrop(opt.fineSize))\n    elif opt.resize_or_crop == 'scale_width':\n        transform_list.append(transforms.Lambda(\n            lambda img: __scale_width(img, opt.fineSize)))\n    elif opt.resize_or_crop == 'scale_width_and_crop':\n        transform_list.append(transforms.Lambda(\n            lambda img: __scale_width(img, opt.loadSize)))\n        transform_list.append(transforms.RandomCrop(opt.fineSize))\n\n    if opt.isTrain and not opt.no_flip:\n        transform_list.append(transforms.RandomHorizontalFlip())\n\n    transform_list += [transforms.ToTensor(),\n                       transforms.Normalize((0.5, 0.5, 0.5),\n                                            (0.5, 0.5, 0.5))]\n    return transforms.Compose(transform_list)\n\ndef __scale_width(img, target_width):\n    ow, oh = img.size\n    if (ow == target_width):\n        return img\n    w = target_width\n    h = int(target_width * oh / ow)\n    return img.resize((w, h), Image.BICUBIC)\n"""
data/custom_dataset_data_loader.py,2,"b'import torch.utils.data\nfrom data.base_data_loader import BaseDataLoader\n\n\ndef CreateDataset(opt):\n    dataset = None\n    if opt.dataset_mode == \'aligned\':\n        from data.aligned_dataset import AlignedDataset\n        dataset = AlignedDataset()\n    elif opt.dataset_mode == \'unaligned\':\n        from data.unaligned_dataset import UnalignedDataset\n        dataset = UnalignedDataset()\n    elif opt.dataset_mode == \'single\':\n        from data.single_dataset import SingleDataset\n        dataset = SingleDataset()\n    else:\n        raise ValueError(""Dataset [%s] not recognized."" % opt.dataset_mode)\n\n    print(""dataset [%s] was created"" % (dataset.name()))\n    dataset.initialize(opt)\n    return dataset\n\n\nclass CustomDatasetDataLoader(BaseDataLoader):\n    def name(self):\n        return \'CustomDatasetDataLoader\'\n\n    def initialize(self, opt):\n        BaseDataLoader.initialize(self, opt)\n        self.dataset = CreateDataset(opt)\n        self.dataloader = torch.utils.data.DataLoader(\n            self.dataset,\n            batch_size=opt.batchSize,\n            shuffle=not opt.serial_batches,\n            num_workers=int(opt.nThreads))\n\n    def load_data(self):\n        return self\n\n    def __len__(self):\n        return min(len(self.dataset), self.opt.max_dataset_size)\n\n    def __iter__(self):\n        for i, data in enumerate(self.dataloader):\n            if i >= self.opt.max_dataset_size:\n                break\n            yield data\n'"
data/data_loader.py,0,b'\ndef CreateDataLoader(opt):\n    from data.custom_dataset_data_loader import CustomDatasetDataLoader\n    data_loader = CustomDatasetDataLoader()\n    print(data_loader.name())\n    data_loader.initialize(opt)\n    return data_loader\n'
data/image_folder.py,1,"b'###############################################################################\n# Code from\n# https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py\n# Modified the original code so that it also loads images from the current\n# directory as well as the subdirectories\n###############################################################################\n\nimport torch.utils.data as data\n\nfrom PIL import Image\nimport os\nimport os.path\n\nIMG_EXTENSIONS = [\n    \'.jpg\', \'.JPG\', \'.jpeg\', \'.JPEG\',\n    \'.png\', \'.PNG\', \'.ppm\', \'.PPM\', \'.bmp\', \'.BMP\',\n]\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\n\ndef make_dataset(dir):\n    images = []\n    assert os.path.isdir(dir), \'%s is not a valid directory\' % dir\n\n    for root, _, fnames in sorted(os.walk(dir)):\n        for fname in fnames:\n            if is_image_file(fname):\n                path = os.path.join(root, fname)\n                images.append(path)\n\n    return images\n\n\ndef default_loader(path):\n    return Image.open(path).convert(\'RGB\')\n\n\nclass ImageFolder(data.Dataset):\n\n    def __init__(self, root, transform=None, return_paths=False,\n                 loader=default_loader):\n        imgs = make_dataset(root)\n        if len(imgs) == 0:\n            raise(RuntimeError(""Found 0 images in: "" + root + ""\\n""\n                               ""Supported image extensions are: "" +\n                               "","".join(IMG_EXTENSIONS)))\n\n        self.root = root\n        self.imgs = imgs\n        self.transform = transform\n        self.return_paths = return_paths\n        self.loader = loader\n\n    def __getitem__(self, index):\n        path = self.imgs[index]\n        img = self.loader(path)\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.return_paths:\n            return img, path\n        else:\n            return img\n\n    def __len__(self):\n        return len(self.imgs)\n'"
data/single_dataset.py,0,"b""import os.path\nimport torchvision.transforms as transforms\nfrom data.base_dataset import BaseDataset, get_transform\nfrom data.image_folder import make_dataset\nfrom PIL import Image\n\n\nclass SingleDataset(BaseDataset):\n    def initialize(self, opt):\n        self.opt = opt\n        self.root = opt.dataroot\n        self.dir_A = os.path.join(opt.dataroot)\n\n        self.A_paths = make_dataset(self.dir_A)\n\n        self.A_paths = sorted(self.A_paths)\n\n        self.transform = get_transform(opt)\n\n    def __getitem__(self, index):\n        A_path = self.A_paths[index]\n        A_img = Image.open(A_path).convert('RGB')\n        A = self.transform(A_img)\n        if self.opt.which_direction == 'BtoA':\n            input_nc = self.opt.output_nc\n        else:\n            input_nc = self.opt.input_nc\n\n        if input_nc == 1:  # RGB to gray\n            tmp = A[0, ...] * 0.299 + A[1, ...] * 0.587 + A[2, ...] * 0.114\n            A = tmp.unsqueeze(0)\n\n        return {'A': A, 'A_paths': A_path}\n\n    def __len__(self):\n        return len(self.A_paths)\n\n    def name(self):\n        return 'SingleImageDataset'\n"""
data/unaligned_dataset.py,0,"b""import os.path\nimport torchvision.transforms as transforms\nfrom data.base_dataset import BaseDataset, get_transform\nfrom data.image_folder import make_dataset\nfrom PIL import Image\nimport PIL\nimport random\n\nclass UnalignedDataset(BaseDataset):\n    def initialize(self, opt):\n        self.opt = opt\n        self.root = opt.dataroot\n        self.dir_A = os.path.join(opt.dataroot, opt.phase + 'A')\n        self.dir_B = os.path.join(opt.dataroot, opt.phase + 'B')\n\n        self.A_paths = make_dataset(self.dir_A)\n        self.B_paths = make_dataset(self.dir_B)\n\n        self.A_paths = sorted(self.A_paths)\n        self.B_paths = sorted(self.B_paths)\n        self.A_size = len(self.A_paths)\n        self.B_size = len(self.B_paths)\n        self.transform = get_transform(opt)\n\n    def __getitem__(self, index):\n        A_path = self.A_paths[index % self.A_size]\n        index_A = index % self.A_size\n        index_B = random.randint(0, self.B_size - 1)\n        B_path = self.B_paths[index_B]\n        # print('(A, B) = (%d, %d)' % (index_A, index_B))\n        A_img = Image.open(A_path).convert('RGB')\n        B_img = Image.open(B_path).convert('RGB')\n\n        A = self.transform(A_img)\n        B = self.transform(B_img)\n        if self.opt.which_direction == 'BtoA':\n            input_nc = self.opt.output_nc\n            output_nc = self.opt.input_nc\n        else:\n            input_nc = self.opt.input_nc\n            output_nc = self.opt.output_nc\n\n        if input_nc == 1:  # RGB to gray\n            tmp = A[0, ...] * 0.299 + A[1, ...] * 0.587 + A[2, ...] * 0.114\n            A = tmp.unsqueeze(0)\n\n        if output_nc == 1:  # RGB to gray\n            tmp = B[0, ...] * 0.299 + B[1, ...] * 0.587 + B[2, ...] * 0.114\n            B = tmp.unsqueeze(0)\n        return {'A': A, 'B': B,\n                'A_paths': A_path, 'B_paths': B_path}\n\n    def __len__(self):\n        return max(self.A_size, self.B_size)\n\n    def name(self):\n        return 'UnalignedDataset'\n"""
models/__init__.py,0,b''
models/base_model.py,5,"b""import os\nimport torch\n\n\nclass BaseModel():\n    def name(self):\n        return 'BaseModel'\n\n    def initialize(self, opt):\n        self.opt = opt\n        self.gpu_ids = opt.gpu_ids\n        self.isTrain = opt.isTrain\n        self.Tensor = torch.cuda.FloatTensor if self.gpu_ids else torch.Tensor\n        self.LongTensor = torch.cuda.LongTensor if self.gpu_ids else torch.Tensor\n        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)\n\n    def set_input(self, input):\n        self.input = input\n\n    def forward(self):\n        pass\n\n    # used in test time, no backprop\n    def test(self):\n        pass\n\n    def get_image_paths(self):\n        pass\n\n    def optimize_parameters(self):\n        pass\n\n    def get_current_visuals(self):\n        return self.input\n\n    def get_current_errors(self):\n        return {}\n\n    def save(self, label):\n        pass\n\n    # helper saving function that can be used by subclasses\n    def save_network(self, network, network_label, epoch_label, gpu_ids):\n        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n        save_path = os.path.join(self.save_dir, save_filename)\n        torch.save(network.cpu().state_dict(), save_path)\n        if len(gpu_ids) and torch.cuda.is_available():\n            network.cuda(gpu_ids[0])\n\n    # helper loading function that can be used by subclasses\n    def load_network(self, network, network_label, epoch_label):\n        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n        save_path = os.path.join(self.save_dir, save_filename)\n        network.load_state_dict(torch.load(save_path))\n\n    # update learning rate (called once every epoch)\n    def update_learning_rate(self):\n        for scheduler in self.schedulers:\n            scheduler.step()\n        lr = self.optimizers[0].param_groups[0]['lr']\n        print('learning rate = %.7f' % lr)\n"""
models/cycle_gan_model.py,6,"b""import numpy as np\nimport torch\nimport os\nfrom collections import OrderedDict\nfrom torch.autograd import Variable\nimport itertools\nimport util.util as util\nfrom util.image_pool import ImagePool\nfrom .base_model import BaseModel\nfrom . import networks\nimport sys\n\n\nclass CycleGANModel(BaseModel):\n    def name(self):\n        return 'CycleGANModel'\n\n    def initialize(self, opt):\n        BaseModel.initialize(self, opt)\n\n        nb = opt.batchSize\n        size = opt.fineSize\n        self.input_A = self.Tensor(nb, opt.input_nc, size, size)\n        self.input_B = self.Tensor(nb, opt.output_nc, size, size)\n\n        # load/define networks\n        # The naming conversion is different from those used in the paper\n        # Code (paper): G_A (G), G_B (F), D_A (D_Y), D_B (D_X)\n\n        self.netG_A = networks.define_G(opt.input_nc, opt.output_nc,\n                                        opt.ngf, opt.which_model_netG, opt.norm, not opt.no_dropout, opt.init_type, self.gpu_ids)\n        self.netG_B = networks.define_G(opt.output_nc, opt.input_nc,\n                                        opt.ngf, opt.which_model_netG, opt.norm, not opt.no_dropout, opt.init_type, self.gpu_ids)\n\n        if self.isTrain:\n            use_sigmoid = opt.no_lsgan\n            self.netD_A = networks.define_D(opt.output_nc, opt.ndf,\n                                            opt.which_model_netD,\n                                            opt.n_layers_D, opt.norm, use_sigmoid, opt.init_type, self.gpu_ids)\n            self.netD_B = networks.define_D(opt.input_nc, opt.ndf,\n                                            opt.which_model_netD,\n                                            opt.n_layers_D, opt.norm, use_sigmoid, opt.init_type, self.gpu_ids)\n        if not self.isTrain or opt.continue_train:\n            which_epoch = opt.which_epoch\n            self.load_network(self.netG_A, 'G_A', which_epoch)\n            self.load_network(self.netG_B, 'G_B', which_epoch)\n            if self.isTrain:\n                self.load_network(self.netD_A, 'D_A', which_epoch)\n                self.load_network(self.netD_B, 'D_B', which_epoch)\n\n        if self.isTrain:\n            self.old_lr = opt.lr\n            self.fake_A_pool = ImagePool(opt.pool_size)\n            self.fake_B_pool = ImagePool(opt.pool_size)\n            # define loss functions\n            self.criterionGAN = networks.GANLoss(use_lsgan=not opt.no_lsgan, tensor=self.Tensor)\n            self.criterionCycle = torch.nn.L1Loss()\n            self.criterionIdt = torch.nn.L1Loss()\n            # initialize optimizers\n            self.optimizer_G = torch.optim.Adam(itertools.chain(self.netG_A.parameters(), self.netG_B.parameters()),\n                                                lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizer_D_A = torch.optim.Adam(self.netD_A.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizer_D_B = torch.optim.Adam(self.netD_B.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizers = []\n            self.schedulers = []\n            self.optimizers.append(self.optimizer_G)\n            self.optimizers.append(self.optimizer_D_A)\n            self.optimizers.append(self.optimizer_D_B)\n            for optimizer in self.optimizers:\n                self.schedulers.append(networks.get_scheduler(optimizer, opt))\n\n        print('---------- Networks initialized -------------')\n        networks.print_network(self.netG_A)\n        networks.print_network(self.netG_B)\n        if self.isTrain:\n            networks.print_network(self.netD_A)\n            networks.print_network(self.netD_B)\n        print('-----------------------------------------------')\n\n    def set_input(self, input):\n        AtoB = self.opt.which_direction == 'AtoB'\n        input_A = input['A' if AtoB else 'B']\n        input_B = input['B' if AtoB else 'A']\n        self.input_A.resize_(input_A.size()).copy_(input_A)\n        self.input_B.resize_(input_B.size()).copy_(input_B)\n        self.image_paths = input['A_paths' if AtoB else 'B_paths']\n\n    def forward(self):\n        self.real_A = Variable(self.input_A)\n        self.real_B = Variable(self.input_B)\n\n    def test(self):\n        real_A = Variable(self.input_A, volatile=True)\n        fake_B = self.netG_A(real_A)\n        self.rec_A = self.netG_B(fake_B).data\n        self.fake_B = fake_B.data\n\n        real_B = Variable(self.input_B, volatile=True)\n        fake_A = self.netG_B(real_B)\n        self.rec_B = self.netG_A(fake_A).data\n        self.fake_A = fake_A.data\n\n    # get image paths\n    def get_image_paths(self):\n        return self.image_paths\n\n    def backward_D_basic(self, netD, real, fake):\n        # Real\n        pred_real = netD(real)\n        loss_D_real = self.criterionGAN(pred_real, True)\n        # Fake\n        pred_fake = netD(fake.detach())\n        loss_D_fake = self.criterionGAN(pred_fake, False)\n        # Combined loss\n        loss_D = (loss_D_real + loss_D_fake) * 0.5\n        # backward\n        loss_D.backward()\n        return loss_D\n\n    def backward_D_A(self):\n        fake_B = self.fake_B_pool.query(self.fake_B)\n        loss_D_A = self.backward_D_basic(self.netD_A, self.real_B, fake_B)\n        self.loss_D_A = loss_D_A.data[0]\n\n    def backward_D_B(self):\n        fake_A = self.fake_A_pool.query(self.fake_A)\n        loss_D_B = self.backward_D_basic(self.netD_B, self.real_A, fake_A)\n        self.loss_D_B = loss_D_B.data[0]\n\n    def backward_G(self):\n        lambda_idt = self.opt.identity\n        lambda_A = self.opt.lambda_A\n        lambda_B = self.opt.lambda_B\n        # Identity loss\n        if lambda_idt > 0:\n            # G_A should be identity if real_B is fed.\n            idt_A = self.netG_A(self.real_B)\n            loss_idt_A = self.criterionIdt(idt_A, self.real_B) * lambda_B * lambda_idt\n            # G_B should be identity if real_A is fed.\n            idt_B = self.netG_B(self.real_A)\n            loss_idt_B = self.criterionIdt(idt_B, self.real_A) * lambda_A * lambda_idt\n\n            self.idt_A = idt_A.data\n            self.idt_B = idt_B.data\n            self.loss_idt_A = loss_idt_A.data[0]\n            self.loss_idt_B = loss_idt_B.data[0]\n        else:\n            loss_idt_A = 0\n            loss_idt_B = 0\n            self.loss_idt_A = 0\n            self.loss_idt_B = 0\n\n        # GAN loss D_A(G_A(A))\n        fake_B = self.netG_A(self.real_A)\n        pred_fake = self.netD_A(fake_B)\n        loss_G_A = self.criterionGAN(pred_fake, True)\n\n        # GAN loss D_B(G_B(B))\n        fake_A = self.netG_B(self.real_B)\n        pred_fake = self.netD_B(fake_A)\n        loss_G_B = self.criterionGAN(pred_fake, True)\n\n        # Forward cycle loss\n        rec_A = self.netG_B(fake_B)\n        loss_cycle_A = self.criterionCycle(rec_A, self.real_A) * lambda_A\n\n        # Backward cycle loss\n        rec_B = self.netG_A(fake_A)\n        loss_cycle_B = self.criterionCycle(rec_B, self.real_B) * lambda_B\n        # combined loss\n        loss_G = loss_G_A + loss_G_B + loss_cycle_A + loss_cycle_B + loss_idt_A + loss_idt_B\n        loss_G.backward()\n\n        self.fake_B = fake_B.data\n        self.fake_A = fake_A.data\n        self.rec_A = rec_A.data\n        self.rec_B = rec_B.data\n\n        self.loss_G_A = loss_G_A.data[0]\n        self.loss_G_B = loss_G_B.data[0]\n        self.loss_cycle_A = loss_cycle_A.data[0]\n        self.loss_cycle_B = loss_cycle_B.data[0]\n\n    def optimize_parameters(self):\n        # forward\n        self.forward()\n        # G_A and G_B\n        self.optimizer_G.zero_grad()\n        self.backward_G()\n        self.optimizer_G.step()\n        # D_A\n        self.optimizer_D_A.zero_grad()\n        self.backward_D_A()\n        self.optimizer_D_A.step()\n        # D_B\n        self.optimizer_D_B.zero_grad()\n        self.backward_D_B()\n        self.optimizer_D_B.step()\n\n    def get_current_errors(self):\n        ret_errors = OrderedDict([('D_A', self.loss_D_A), ('G_A', self.loss_G_A), ('Cyc_A', self.loss_cycle_A),\n                                 ('D_B', self.loss_D_B), ('G_B', self.loss_G_B), ('Cyc_B',  self.loss_cycle_B)])\n        if self.opt.identity > 0.0:\n            ret_errors['idt_A'] = self.loss_idt_A\n            ret_errors['idt_B'] = self.loss_idt_B\n        return ret_errors\n\n    def get_current_visuals(self):\n        real_A = util.tensor2im(self.input_A)\n        fake_B = util.tensor2im(self.fake_B)\n        rec_A = util.tensor2im(self.rec_A)\n        real_B = util.tensor2im(self.input_B)\n        fake_A = util.tensor2im(self.fake_A)\n        rec_B = util.tensor2im(self.rec_B)\n        ret_visuals = OrderedDict([('real_A', real_A), ('fake_B', fake_B), ('rec_A', rec_A),\n                                   ('real_B', real_B), ('fake_A', fake_A), ('rec_B', rec_B)])\n        if self.opt.isTrain and self.opt.identity > 0.0:\n            ret_visuals['idt_A'] = util.tensor2im(self.idt_A)\n            ret_visuals['idt_B'] = util.tensor2im(self.idt_B)\n        return ret_visuals\n\n    def save(self, label):\n        self.save_network(self.netG_A, 'G_A', label, self.gpu_ids)\n        self.save_network(self.netD_A, 'D_A', label, self.gpu_ids)\n        self.save_network(self.netG_B, 'G_B', label, self.gpu_ids)\n        self.save_network(self.netD_B, 'D_B', label, self.gpu_ids)\n"""
models/deeplab.py,2,"b'import torch.nn as nn\r\nimport torch\r\nimport numpy as np\r\nfrom torchvision import models\r\n\r\naffine_par = True\r\n\r\ndef get_bn_lr_params(model):\r\n    """"""\r\n    This generator returns all the parameters of the net except for \r\n    the last classification layer. Note that for each batchnorm layer, \r\n    requires_grad is set to False in deeplab_resnet.py, therefore this function does not return \r\n    any batchnorm parameter\r\n    """"""\r\n    b = []\r\n\r\n    b.append(model.bn1.parameters())\r\n    b.append(model.layer1[0].downsample[1].parameters())\r\n    for i in range(3):\r\n        b.append(model.layer1[i].bn1.parameters())\r\n        b.append(model.layer1[i].bn2.parameters())\r\n        b.append(model.layer1[i].bn3.parameters())\r\n    b.append(model.layer2[0].downsample[1].parameters())\r\n    for j in range(4):\r\n        b.append(model.layer2[j].bn1.parameters())\r\n        b.append(model.layer2[j].bn2.parameters())\r\n        b.append(model.layer2[j].bn3.parameters())\r\n    b.append(model.layer3[0].downsample[1].parameters())\r\n    for k in range(23):\r\n        b.append(model.layer3[k].bn1.parameters())\r\n        b.append(model.layer3[k].bn2.parameters())\r\n        b.append(model.layer3[k].bn3.parameters())\r\n    b.append(model.layer4[0].downsample[1].parameters())\r\n    for m in range(3):\r\n        b.append(model.layer4[m].bn1.parameters())\r\n        b.append(model.layer4[m].bn2.parameters())\r\n        b.append(model.layer4[m].bn3.parameters())\r\n\r\n    for ii in range(len(b)):\r\n        for jj in b[ii]:\r\n            yield jj\r\n\r\ndef get_1x_lr_params(model):\r\n    """"""\r\n    This generator returns all the parameters of the net except for \r\n    the last classification layer. Note that for each batchnorm layer, \r\n    requires_grad is set to False in deeplab_resnet.py, therefore this function does not return \r\n    any batchnorm parameter\r\n    """"""\r\n    b = []\r\n    b.append(model.conv1.parameters())\r\n    b.append(model.layer1[0].downsample[0].parameters())\r\n    for i in range(3):\r\n        b.append(model.layer1[i].conv1.parameters())\r\n        b.append(model.layer1[i].conv2.parameters())\r\n        b.append(model.layer1[i].conv3.parameters())\r\n    b.append(model.layer2[0].downsample[0].parameters())\r\n    for j in range(4):\r\n        b.append(model.layer2[j].conv1.parameters())\r\n        b.append(model.layer2[j].conv2.parameters())\r\n        b.append(model.layer2[j].conv3.parameters())\r\n    b.append(model.layer3[0].downsample[0].parameters())\r\n    for k in range(23):\r\n        b.append(model.layer3[k].conv1.parameters())\r\n        b.append(model.layer3[k].conv2.parameters())\r\n        b.append(model.layer3[k].conv3.parameters())\r\n    b.append(model.layer4[0].downsample[0].parameters())\r\n    for m in range(3):\r\n        b.append(model.layer4[m].conv1.parameters())\r\n        b.append(model.layer4[m].conv2.parameters())\r\n        b.append(model.layer4[m].conv3.parameters())\r\n\r\n    for ii in range(len(b)):\r\n        for jj in b[ii]:\r\n            yield jj\r\n\r\ndef get_10x_lr_params(model):\r\n    """"""\r\n    This generator returns all the parameters for the last layer of the net,\r\n    which does the classification of pixel into classes\r\n    """"""\r\n    b = []\r\n    b.append(model.layer5.parameters())\r\n\r\n    for ii in range(len(b)):\r\n        for jj in b[ii]:\r\n            yield jj\r\n\r\nclass D_Classifier_Module(nn.Module):\r\n\r\n    def __init__(self, dilation_series, padding_series, num_classes):\r\n        super(D_Classifier_Module, self).__init__()\r\n        self.conv2d_list = nn.ModuleList()\r\n        self.conv1_1 = nn.Conv2d(num_classes * 4, num_classes, kernel_size = 1)\r\n        self.conv1_1.weight.data.normal_(0, 0.01)\r\n        conv1 = nn.Conv2d(2048, num_classes, kernel_size=1)\r\n        self.conv2d_list.append(conv1)\r\n        for dilation, padding in zip(dilation_series, padding_series):\r\n            self.conv2d_list.append(nn.Conv2d(2048, num_classes, kernel_size=3, stride=1, padding=padding, dilation=dilation, bias = True))\r\n\r\n        for m in self.conv2d_list:\r\n            m.weight.data.normal_(0, 0.01)\r\n\r\n    def forward(self, x):\r\n        out = self.conv2d_list[0](x)\r\n        for i in range(len(self.conv2d_list)-1):\r\n            out = torch.cat([out, self.conv2d_list[i+1](x)], 1)\r\n        out = self.conv1_1(out)\r\n        return out\r\n\r\nclass D_Bottleneck(nn.Module):\r\n    expansion = 4\r\n\r\n    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):\r\n        super(D_Bottleneck, self).__init__()\r\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False) # change\r\n        self.bn1 = nn.BatchNorm2d(planes,affine = affine_par)\r\n        \r\n        padding = dilation\r\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, # change\r\n                               padding=padding, bias=False, dilation = dilation)\r\n        self.bn2 = nn.BatchNorm2d(planes,affine = affine_par)\r\n        \r\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\r\n        self.bn3 = nn.BatchNorm2d(planes * 4, affine = affine_par)\r\n        self.relu = nn.ReLU(inplace=True)\r\n        self.downsample = downsample\r\n        self.stride = stride\r\n\r\n    def forward(self, x):\r\n        residual = x\r\n\r\n        out = self.conv1(x)\r\n        out = self.bn1(out)\r\n        out = self.relu(out)\r\n\r\n        out = self.conv2(out)\r\n        out = self.bn2(out)\r\n        out = self.relu(out)\r\n\r\n        out = self.conv3(out)\r\n        out = self.bn3(out)\r\n\r\n        if self.downsample is not None:\r\n            residual = self.downsample(x)\r\n\r\n        out += residual\r\n        out = self.relu(out)\r\n\r\n        return out\r\n\r\n\r\nclass D_ResNet(nn.Module):\r\n    def __init__(self, block, layers, num_classes, input_size):\r\n        self.inplanes = 64\r\n        super(D_ResNet, self).__init__()\r\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\r\n                               bias=False)\r\n        self.bn1 = nn.BatchNorm2d(64, affine = affine_par)\r\n        self.relu = nn.ReLU(inplace=True)\r\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) # change\r\n        self.layer1 = self._make_layer(block, 64, layers[0], stride=1, dilation = [1])\r\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilation = [1])\r\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilation = [1])\r\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=[1,1,1], rate = 2)\r\n        self.layer5 = self._make_pred_layer(D_Classifier_Module, [2,4,6],[2,4,6], num_classes)\r\n        self.upsample = nn.Upsample(input_size, mode=\'bilinear\')\r\n        \r\n\r\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=[1], rate = 1):\r\n        downsample = None\r\n        downsample = nn.Sequential(\r\n            nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\r\n            nn.BatchNorm2d(planes * block.expansion,affine = affine_par))\r\n        layers = []\r\n        layers.append(block(self.inplanes, planes, stride, dilation=rate * dilation[0], downsample=downsample))\r\n        self.inplanes = planes * block.expansion\r\n        for i in range(1, blocks):\r\n            layers.append(block(self.inplanes, planes, dilation=rate * dilation[i] if len(dilation) > 1 else dilation[0]))\r\n        return nn.Sequential(*layers)\r\n    \r\n    def _make_pred_layer(self,block, dilation_series, padding_series, num_classes):\r\n        return block(dilation_series, padding_series, num_classes)\r\n\r\n    def forward(self, x):\r\n        sm = nn.Softmax2d()\r\n        lsm = nn.LogSoftmax()\r\n        x = self.conv1(x)\r\n        x = self.bn1(x)\r\n        x = self.relu(x)\r\n        x = self.maxpool(x)\r\n        x = self.layer1(x)\r\n        x = self.layer2(x)\r\n        x = self.layer3(x)\r\n        x = self.layer4(x)\r\n        x = self.layer5(x)\r\n        x = self.upsample(x)\r\n        return {\'GAN\':x, \'L1\':lsm(x)}   '"
models/focal_loss.py,8,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nSpyder Editor\r\n\r\nThis is a temporary script file.\r\n""""""\r\n\r\n#!/usr/bin/env python\r\n# -*- coding: utf-8 -*-\r\n# --------------------------------------------------------\r\n# Licensed under The MIT License [see LICENSE for details]\r\n# Written by Chao CHEN (chaochancs@gmail.com)\r\n# Created On: 2017-08-11\r\n# --------------------------------------------------------\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.autograd import Variable\r\n\r\nclass FocalLoss(nn.Module):\r\n    r""""""\r\n        This criterion is a implemenation of Focal Loss, which is proposed in \r\n        Focal Loss for Dense Object Detection.\r\n            \r\n            Loss(x, class) = - \\alpha (1-softmax(x)[class])^gamma \\log(softmax(x)[class])\r\n    \r\n        The losses are averaged across observations for each minibatch.\r\n        Args:\r\n            alpha(1D Tensor, Variable) : the scalar factor for this criterion\r\n            gamma(float, double) : gamma > 0; reduces the relative loss for well-classi?ed examples (p > .5), \r\n                                   putting more focus on hard, misclassi?ed examples\r\n            size_average(bool): size_average(bool): By default, the losses are averaged over observations for each minibatch.\r\n                                However, if the field size_average is set to False, the losses are\r\n                                instead summed for each minibatch.\r\n    """"""\r\n    def __init__(self, class_num, alpha=None, gamma=2, size_average=True):\r\n        super(FocalLoss, self).__init__()\r\n        if alpha is None:\r\n            self.alpha = Variable(torch.ones(class_num, 1))\r\n        else:\r\n            if isinstance(alpha, Variable):\r\n                self.alpha = alpha\r\n            else:\r\n                self.alpha = Variable(alpha)\r\n        self.gamma = gamma\r\n        self.class_num = class_num\r\n        self.size_average = size_average\r\n\r\n\r\n    def forward(self, inputs, targets):\r\n        N = inputs.size(0)\r\n        C = inputs.size(1)\r\n        H = inputs.size(2)\r\n        W = inputs.size(3)\r\n        sm = nn.Softmax2d()\r\n        \r\n        P = sm(inputs)\r\n        \r\n        class_mask = inputs.data.new(N, C, H, W).fill_(0)\r\n        class_mask = Variable(class_mask)\r\n        ids = targets.view(N, 1, H, W)\r\n        class_mask.scatter_(1, ids.data, 1.)\r\n        #print(class_mask)\r\n\r\n        if inputs.is_cuda and not self.alpha.is_cuda:\r\n            self.alpha = self.alpha.cuda()\r\n        alpha = self.alpha[ids.data.view(-1)]\r\n        \r\n        probs = (P*class_mask).sum(1).view(N, H, W)\r\n\r\n        log_p = probs.log()\r\n\r\n        #print(\'probs size= {}\'.format(probs.size()))\r\n        #print(probs)\r\n        #print(\'log_p size= {}\'.format(log_p.size()))\r\n        #print(log_p)\r\n        \r\n\r\n        batch_loss = -alpha*(torch.pow((1-probs), self.gamma))*log_p \r\n        #print(\'-----bacth_loss------\')\r\n        #print(batch_loss)\r\n\r\n        if self.size_average:\r\n            loss = batch_loss.mean()\r\n        else:\r\n            \r\n            loss = batch_loss.sum()\r\n        return loss\r\n\r\n        \r\n# =============================================================================\r\n# if __name__ == ""__main__"":\r\n#     alpha = torch.rand(21, 1)\r\n#     print(alpha)\r\n#     FL = FocalLoss(class_num=7, gamma=2 )\r\n#     CE = nn.CrossEntropyLoss()\r\n#     N = 1\r\n#     C = 3\r\n#     H = 4\r\n#     W = 4\r\n#     inputs = torch.rand(N, C, H, W)\r\n#     targets = torch.LongTensor(N, H, W).random_(C)\r\n#     inputs_fl = Variable(inputs.clone(), requires_grad=True)\r\n#     targets_fl = Variable(targets.clone())\r\n# \r\n#     inputs_ce = Variable(inputs.clone(), requires_grad=True)\r\n#     targets_ce = Variable(targets.clone())\r\n#     print(\'----inputs----\')\r\n#     print(inputs)\r\n#     #print(\'---target-----\')\r\n#     #print(targets)\r\n# \r\n#     fl_loss = FL(inputs_fl, targets_fl)\r\n#     ce_loss = CE(inputs_ce, targets_ce)\r\n#     print(\'ce = {}, fl ={}\'.format(ce_loss.data[0], fl_loss.data[0]))\r\n#     fl_loss.backward()\r\n#     ce_loss.backward()\r\n#     print(inputs_fl.grad.data)\r\n#     print(inputs_ce.grad.data)\r\n# =============================================================================\r\n'"
models/models.py,0,"b'\ndef create_model(opt):\n    model = None\n    print(opt.model)\n    if opt.model == \'cycle_gan\':\n        assert(opt.dataset_mode == \'unaligned\')\n        from .cycle_gan_model import CycleGANModel\n        model = CycleGANModel()\n    elif opt.model == \'pix2pix\':\n        assert(opt.dataset_mode == \'aligned\')\n        from .pix2pix_model import Pix2PixModel\n        model = Pix2PixModel()\n    elif opt.model == \'test\':\n        assert(opt.dataset_mode == \'single\')\n        from .test_model import TestModel\n        model = TestModel()\n    else:\n        raise ValueError(""Model [%s] not recognized."" % opt.model)\n    model.initialize(opt)\n    print(""model [%s] was created"" % (model.name()))\n    return model\n'"
models/networks.py,15,"b""import torch\nimport torch.nn as nn\nfrom torch.nn import init\nimport functools\nfrom torch.autograd import Variable\nfrom torch.optim import lr_scheduler\nfrom torchvision import models\nfrom . import deeplab\n\n###############################################################################\n# Functions\n###############################################################################\ndef weights_init_normal(m):\n    classname = m.__class__.__name__\n    # print(classname)\n    if classname.find('Conv') != -1:\n        init.uniform(m.weight.data, 0.0, 0.02)\n    elif classname.find('Linear') != -1:\n        init.uniform(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm2d') != -1:\n        init.uniform(m.weight.data, 1.0, 0.02)\n        init.constant(m.bias.data, 0.0)\n\n\ndef weights_init_xavier(m):\n    classname = m.__class__.__name__\n    # print(classname)\n    if classname.find('Conv') != -1:\n        init.xavier_normal(m.weight.data, gain=1)\n    elif classname.find('Linear') != -1:\n        init.xavier_normal(m.weight.data, gain=1)\n    elif classname.find('BatchNorm2d') != -1:\n        init.uniform(m.weight.data, 1.0, 0.02)\n        init.constant(m.bias.data, 0.0)\n\ndef weights_init_xavier_U(m):\n    classname = m.__class__.__name__\n    #print(classname)\n    if classname.find('ConvTranspose2d') != -1:\n        init.xavier_normal(m.weight.data, gain=1)\n    elif classname.find('Linear') != -1:\n        init.xavier_normal(m.weight.data, gain=1)\n        \ndef weights_init_xavier_D(m):\n    classname = m.__class__.__name__\n    # print(classname)\n    if classname.find('Conv2d') != -1:\n        init.xavier_normal(m.weight.data, gain=1)\n    elif classname.find('Linear') != -1:\n        init.xavier_normal(m.weight.data, gain=1)\n\ndef weights_init_kaiming(m):\n    classname = m.__class__.__name__\n    # print(classname)\n    if classname.find('Conv') != -1:\n        init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n    elif classname.find('Linear') != -1:\n        init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n    elif classname.find('BatchNorm2d') != -1:\n        init.uniform(m.weight.data, 1.0, 0.02)\n        init.constant(m.bias.data, 0.0)\n\n\ndef weights_init_orthogonal(m):\n    classname = m.__class__.__name__\n    #print(classname)\n    if classname.find('Conv') != -1:\n        init.orthogonal(m.weight.data, gain=1)\n    elif classname.find('Linear') != -1:\n        init.orthogonal(m.weight.data, gain=1)\n    elif classname.find('BatchNorm2d') != -1:\n        init.uniform(m.weight.data, 1.0, 0.02)\n        init.constant(m.bias.data, 0.0)\n\n\ndef init_weights(net, init_type='normal'):\n    print('initialization method [%s]' % init_type)\n    if init_type == 'normal':\n        net.apply(weights_init_normal)\n    elif init_type == 'xavier':\n        net.apply(weights_init_xavier)\n    elif init_type == 'xavier_U':\n        net.apply(weights_init_xavier_U)\n    elif init_type == 'xavier_D':\n        net.apply(weights_init_xavier_D)\n    elif init_type == 'kaiming':\n        net.apply(weights_init_kaiming)\n    elif init_type == 'orthogonal':\n        net.apply(weights_init_orthogonal)\n    else:\n        raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n\ndef set_bn_eval(m):\n    classname = m.__class__.__name__\n    if classname.find('BatchNorm') != -1:\n        m.eval()\n\ndef get_norm_layer(norm_type='instance'):\n    if norm_type == 'batch':\n        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n    elif norm_type == 'instance':\n        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False)\n    elif norm_type == 'none':\n        norm_layer = None\n    else:\n        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n    return norm_layer\n\n\ndef get_scheduler(optimizer, opt):\n    if opt.lr_policy == 'lambda':\n        if opt.which_model_netG == 'deeplab_aspp':\n            def lambda_rule(epoch):\n                lr_l = (max(0.001, 1.0 - epoch/30.0)) ** 0.9\n                return lr_l\n        else:\n            def lambda_rule(epoch):\n                lr_l = 0.1 ** max(0.0, epoch//14.0) #for LIP\n                return lr_l\n        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n    elif opt.lr_policy == 'step':\n        scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.1)\n    elif opt.lr_policy == 'plateau':\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)\n    else:\n        return NotImplementedError('learning rate policy [%s] is not implemented', opt.lr_policy)\n    return scheduler\n\n\ndef define_G(input_nc, output_nc, ngf, which_model_netG, hook, input_size, norm='batch', use_dropout=False, init_type='normal', gpu_ids=[]):\n    netG = None\n    use_gpu = len(gpu_ids) > 0\n    norm_layer = get_norm_layer(norm_type=norm)\n\n    if use_gpu:\n        assert(torch.cuda.is_available())\n\n    if which_model_netG == 'resnet_9blocks':\n        netG = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=9, gpu_ids=gpu_ids)\n    elif which_model_netG == 'resnet_6blocks':\n        netG = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=6, gpu_ids=gpu_ids)\n    elif which_model_netG == 'unet_128':\n        netG = UnetGenerator(input_nc, output_nc, 7, hook, ngf, norm_layer=norm_layer, use_dropout=use_dropout, gpu_ids=gpu_ids)\n    elif which_model_netG == 'unet_256':\n        netG = UnetGenerator(input_nc, output_nc, 8, hook, ngf, norm_layer=norm_layer, use_dropout=use_dropout, gpu_ids=gpu_ids)\n        init_weights(netG, init_type = 'xavier_U')\n    elif which_model_netG == 'deeplab_aspp':\n        netG = deeplab.D_ResNet(deeplab.D_Bottleneck, [3, 4, 23, 3], output_nc, input_size)\n        init_weights(netG, init_type = 'xavier')\n        model_res101 = models.resnet101(pretrained=True)\n        model_res101 = model_res101.cuda()\n        pretrained_dict = model_res101.state_dict()\n        new_params = netG.state_dict().copy()\n        for i in new_params:\n            i_parts = i.split('.')\n            if i_parts[0] != 'layer5':\n                new_params['.'.join(i_parts)] = pretrained_dict[i]\n        netG.load_state_dict(new_params)\n    else:\n        raise NotImplementedError('Generator model name [%s] is not recognized' % which_model_netG)\n    if len(gpu_ids) > 0:\n        netG.cuda(gpu_ids[0])\n    return netG\n\n\ndef define_D(input_nc, ndf, which_model_netD,\n             n_layers_D=3, norm='batch', use_sigmoid=False, init_type='normal', gpu_ids=[]):\n    netD = None\n    use_gpu = len(gpu_ids) > 0\n    norm_layer = get_norm_layer(norm_type=norm)\n\n    if use_gpu:\n        assert(torch.cuda.is_available())\n    if which_model_netD == 'basic':\n        netD = NLayerDiscriminator(input_nc, ndf, n_layers=3, norm_layer=norm_layer, use_sigmoid=use_sigmoid, gpu_ids=gpu_ids)\n    elif which_model_netD == 'n_layers' and n_layers_D < 5:\n        netD = NLayerDiscriminator(input_nc, ndf, n_layers_D, norm_layer=norm_layer, use_sigmoid=use_sigmoid, gpu_ids=gpu_ids)\n    else:\n        raise NotImplementedError('Discriminator model name [%s] is not recognized' %\n                                  which_model_netD)\n    if use_gpu:\n        netD.cuda(gpu_ids[0])\n    init_weights(netD, init_type=init_type)\n    return netD\n\n\ndef print_network(net):\n    num_params = 0\n    for param in net.parameters():\n        num_params += param.numel()\n    print(net)\n    print('Total number of parameters: %d' % num_params)\n\n\n##############################################################################\n# Classes\n##############################################################################\n\n# Flatten the tensor for fc layer\nclass Flatten(nn.Module):\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n    \n# Defines the GAN loss which uses either LSGAN or the regular GAN.\n# When LSGAN is used, it is basically same as MSELoss,\n# but it abstracts away the need to create the target label tensor\n# that has the same size as the input\nclass GANLoss(nn.Module):\n    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0,\n                 tensor=torch.FloatTensor):\n        super(GANLoss, self).__init__()\n        self.real_label = target_real_label\n        self.fake_label = target_fake_label\n        self.real_label_var = None\n        self.fake_label_var = None\n        self.Tensor = tensor\n        if use_lsgan:\n            self.loss = nn.MSELoss()\n        else:\n            self.loss = nn.BCELoss()\n            \n    def get_target_tensor(self, input, target_is_real):\n        target_tensor = None\n        if target_is_real:\n            create_label = ((self.real_label_var is None) or\n                            (self.real_label_var.numel() != input.numel()))\n            if create_label:\n                real_tensor = self.Tensor(input.size()).fill_(self.real_label)\n                self.real_label_var = Variable(real_tensor, requires_grad=False)\n            target_tensor = self.real_label_var\n        else:\n            create_label = ((self.fake_label_var is None) or\n                            (self.fake_label_var.numel() != input.numel()))\n            if create_label:\n                fake_tensor = self.Tensor(input.size()).fill_(self.fake_label)\n                self.fake_label_var = Variable(fake_tensor, requires_grad=False)\n            target_tensor = self.fake_label_var\n        return target_tensor\n\n    def __call__(self, input, target_is_real):\n        target_tensor = self.get_target_tensor(input, target_is_real)\n        return self.loss(input, target_tensor)\n\n\n# Defines the generator that consists of Resnet blocks between a few\n# downsampling/upsampling operations.\n# Code and idea originally from Justin Johnson's architecture.\n# https://github.com/jcjohnson/fast-neural-style/\nclass ResnetGenerator(nn.Module):\n    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, gpu_ids=[], padding_type='reflect'):\n        assert(n_blocks >= 0)\n        super(ResnetGenerator, self).__init__()\n        self.input_nc = input_nc\n        self.output_nc = output_nc\n        self.ngf = ngf\n        self.gpu_ids = gpu_ids\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        model = [nn.ReflectionPad2d(3),\n                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0,\n                           bias=use_bias),\n                 norm_layer(ngf),\n                 nn.ReLU(True)]\n\n        n_downsampling = 2\n        for i in range(n_downsampling):\n            mult = 2**i\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3,\n                                stride=2, padding=1, bias=use_bias),\n                      norm_layer(ngf * mult * 2),\n                      nn.ReLU(True)]\n\n        mult = 2**n_downsampling\n        for i in range(n_blocks):\n            model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n\n        for i in range(n_downsampling):\n            mult = 2**(n_downsampling - i)\n            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n                                         kernel_size=3, stride=2,\n                                         padding=1, output_padding=1,\n                                         bias=use_bias),\n                      norm_layer(int(ngf * mult / 2)),\n                      nn.ReLU(True)]\n        model += [nn.ReflectionPad2d(3)]\n        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n        model += [nn.Tanh()]\n\n    def forward(self, input):\n        if self.gpu_ids and isinstance(input.data, torch.cuda.FloatTensor):\n            return nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n        else:\n            return self.model(input)\n\n# Define a resnet block\nclass ResnetBlock(nn.Module):\n    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n        super(ResnetBlock, self).__init__()\n        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n\n    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n        conv_block = []\n        p = 0\n        if padding_type == 'reflect':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == 'replicate':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == 'zero':\n            p = 1\n        else:\n            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n                       norm_layer(dim),\n                       nn.ReLU(True)]\n        if use_dropout:\n            conv_block += [nn.Dropout(0.5)]\n\n        p = 0\n        if padding_type == 'reflect':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == 'replicate':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == 'zero':\n            p = 1\n        else:\n            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n                       norm_layer(dim)]\n\n        return nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        out = x + self.conv_block(x)\n        return out\n\nclass Bottleneck(nn.Module):\n    def __init__(self, model_cx, model_x):\n        super(Bottleneck, self).__init__()\n        self.ReLU = nn.ReLU()\n        self.model_cx = self.build_block(model_cx)\n        self.model_x = self.build_block(model_x)\n        \n    def build_block(self, model):\n        return nn.Sequential(*model)\n    \n    def forward(self, x):\n        x = self.ReLU(x)\n        if len(self.model_x) == 0:\n            return self.model_cx(x) + x\n        else:\n            return self.model_cx(x) + self.model_x(x)\n\nclass ASPP_Module(nn.Module):\n    def __init__(self, input_nc, conv2d_list):\n        super(ASPP_Module, self).__init__()\n        self.conv2d_list = conv2d_list\n        self.conv1_1 = nn.Conv2d(input_nc * 4, input_nc, kernel_size = 1)\n        self.conv1_1.weight.data.normal_(0, 0.01)\n        for m in self.conv2d_list:\n            m.weight.data.normal_(0, 0.01)\n        \n    def forward(self, x):\n        out = self.conv2d_list[0](x)\n        for i in range(len(self.conv2d_list)-1):\n            out = torch.cat([out, self.conv2d_list[i+1](x)], 1)\n        out = self.conv1_1(out)\n        return out\n\nclass UnetHook():\n    def __init__(self):\n        self.value = 0\n    \n    def hook_out(self, module, input, output):\n        self.value = output\n        \n    def get_value(self):\n        return self.value\n    \n    def print_value(self):\n        print(self.value)\n    \n# Defines the Unet generator.\n# |num_downs|: number of downsamplings in UNet. For example,\n# if |num_downs| == 7, image of size 128x128 will become of size 1x1\n# at the bottleneck\nclass UnetGenerator(nn.Module):\n    def __init__(self, input_nc, output_nc, num_downs, hook, ngf=64, \n                 norm_layer=nn.BatchNorm2d, use_dropout=False, gpu_ids=[]):\n        super(UnetGenerator, self).__init__()\n        self.gpu_ids = gpu_ids\n        \n        #Resnet 101\n        model_res101 = models.resnet101(pretrained=True)\n        model_res101 = model_res101.cuda()\n        \n        # construct unet structure\n        T_block = UnetSkipConnectionBlock(output_nc, ngf * 32, input_nc = ngf * 32, submodule = None, depth = -2, norm_layer = norm_layer, model_ft = model_res101)\n        handle = T_block.register_forward_hook(hook.hook_out)\n        U_block = UnetSkipConnectionBlock(output_nc, ngf * 32, input_nc = None, submodule = T_block, depth = -1, norm_layer = norm_layer, model_ft = model_res101) \n\n        U_block = UnetSkipConnectionBlock(ngf * 16, ngf * 32, input_nc = None, submodule = U_block, depth = 0, norm_layer = norm_layer, model_ft = model_res101)\n        U_block = UnetSkipConnectionBlock(ngf * 8, ngf * 16, input_nc = None, submodule = U_block, depth = 1, norm_layer = norm_layer, model_ft = model_res101)\n        U_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc = None, submodule = U_block, depth = 2, norm_layer = norm_layer, model_ft = model_res101)\n        U_block = UnetSkipConnectionBlock(ngf, ngf * 4, input_nc = None, submodule = U_block, depth = 3, norm_layer = norm_layer, model_ft = model_res101)\n        U_block = UnetSkipConnectionBlock(output_nc, ngf, input_nc = input_nc, submodule = U_block, depth = 4, norm_layer = norm_layer, model_ft = model_res101)\n\n        self.model = U_block\n\n    def forward(self, input):\n        if self.gpu_ids and isinstance(input.data, torch.cuda.FloatTensor):\n            return nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n        else:\n            return self.model(input)\n\n\n# Defines the submodule with skip connection.\n# X -------------------identity---------------------- X\n#   |-- downsampling -- |submodule| -- upsampling --|\nclass UnetSkipConnectionBlock(nn.Module):\n    def __init__(self, outer_nc, inner_nc, depth, input_nc=None,\n                 submodule=None,  norm_layer=nn.BatchNorm2d, use_dropout=False, model_ft=None):\n        super(UnetSkipConnectionBlock, self).__init__()\n        \n        assert(depth <= 4)\n        self.depth = depth\n        \n        #======================== depth 4 ==========================\n        ResBlock0 = [model_ft.conv1, model_ft.bn1]\n    \n        #======================== depth 3 ==========================\n        ResBlock1 = [model_ft.maxpool,]\n        for i in range(3):\n            model_x = []\n            model_cx = []\n            if i == 0:\n                model_x = [model_ft.layer1[i].downsample[0],\n                           model_ft.layer1[i].downsample[1]]\n            model_cx = [model_ft.layer1[i].conv1, \n                        model_ft.layer1[i].bn1,\n                        model_ft.layer1[i].conv2,\n                        model_ft.layer1[i].bn2,\n                        model_ft.layer1[i].conv3,\n                        model_ft.layer1[i].bn3]\n            \n            ResBlock1 += [Bottleneck(model_cx, model_x),]\n        \n        #======================== depth 2 ==========================\n        ResBlock2 = []\n        for j in range(4):\n            model_x = []\n            model_cx = []\n            if j == 0:\n                model_x = [model_ft.layer2[j].downsample[0], \n                           model_ft.layer2[j].downsample[1]]\n            model_cx = [model_ft.layer2[j].conv1, \n                     model_ft.layer2[j].bn1,\n                     model_ft.layer2[j].conv2,\n                     model_ft.layer2[j].bn2,\n                     model_ft.layer2[j].conv3,\n                     model_ft.layer2[j].bn3]\n            ResBlock2 += [Bottleneck(model_cx, model_x),]\n            \n        #======================== depth 1 ==========================\n        ResBlock3 = []\n        for k in range(23):\n            model_x = []\n            model_cx = []\n            if k == 0:\n                model_x = [model_ft.layer3[k].downsample[0], \n                           model_ft.layer3[k].downsample[1]]\n            model_cx = [model_ft.layer3[k].conv1, \n                     model_ft.layer3[k].bn1,\n                     model_ft.layer3[k].conv2,\n                     model_ft.layer3[k].bn2,\n                     model_ft.layer3[k].conv3,\n                     model_ft.layer3[k].bn3]\n            ResBlock3 += [Bottleneck(model_cx, model_x),]\n        \n        #======================== depth 0 ==========================\n        ResBlock4 = []\n        for m in range(3):\n            model_x = []\n            model_cx = []\n            if m == 0:\n                model_x = [model_ft.layer4[m].downsample[0], \n                           model_ft.layer4[m].downsample[1]]\n                model_x[0].stride = (1, 1)\n                           \n            model_cx = [model_ft.layer4[m].conv1, \n                     model_ft.layer4[m].bn1,\n                     model_ft.layer4[m].conv2,\n                     model_ft.layer4[m].bn2,\n                     model_ft.layer4[m].conv3,\n                     model_ft.layer4[m].bn3]\n            model_cx[2].stride = (1, 1)\n            model_cx[2].dilation = (2, 2)\n            model_cx[2].padding = (2, 2)\n            ResBlock4 += [Bottleneck(model_cx, model_x),]\n            \n        #======================== depth -1 ==========================\n        ResBlock5 = []\n        conv_list = nn.ModuleList()\n        conv1 = nn.Conv2d(inner_nc, outer_nc, kernel_size=1)\n        conv_list.append(conv1)\n        \n        for n in range(1, 4):\n            conv3 = nn.Conv2d(inner_nc, outer_nc, kernel_size=3)\n            conv3.stride = (1, 1)\n            conv3.dilation = (2 * n, 2 * n)\n            conv3.padding = (2 * n, 2 * n)\n            conv_list.append(conv3)\n        ResBlock5 += [ASPP_Module(outer_nc, conv_list)]\n         #======================== end =============================\n         \n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n        if input_nc is None:\n            input_nc = outer_nc\n        \n        uprelu = nn.ReLU(False)\n        upnorm = norm_layer(outer_nc)\n        \n        if depth == 4:\n            down = ResBlock0\n            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n                                        kernel_size = 4, stride = 2,\n                                        padding = 1)\n            up = [uprelu, upconv]\n            model = down + [submodule] + up\n            self.U4 = nn.Sequential(*model)\n            \n        if depth == 3:\n            down = ResBlock1\n            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n                                        kernel_size=4, stride=2,\n                                        padding=1, bias=use_bias)\n            up = [uprelu, upconv, upnorm]\n\n            if use_dropout:\n                model = down + [submodule] + up + [nn.Dropout(0.5)]\n            else:\n                model = down + [submodule] + up\n            self.U3 = nn.Sequential(*model)\n            self.con3 = nn.Conv2d(outer_nc, outer_nc, kernel_size=1)\n            self.con3.weight.data.normal_(0, 0.01)\n            \n        if depth == 2:\n            down = ResBlock2\n            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n                                        kernel_size=4, stride=2,\n                                        padding=1, bias=use_bias)\n            up = [uprelu, upconv, upnorm]\n\n            if use_dropout:\n                model = down + [submodule] + up + [nn.Dropout(0.5)]\n            else:\n                model = down + [submodule] + up\n            self.U2 = nn.Sequential(*model)\n            self.con2 = nn.Conv2d(outer_nc, outer_nc, kernel_size=1)\n            self.con2.weight.data.normal_(0, 0.01)\n        \n        if depth == 1:\n            down = ResBlock3\n            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n                                        kernel_size=4, stride=2,\n                                        padding=1, bias=use_bias)\n            up = [uprelu, upconv, upnorm]\n            if use_dropout:\n                model = down + [submodule] + up + [nn.Dropout(0.5)]\n            else:\n                model = down + [submodule] + up\n            self.U1 = nn.Sequential(*model)\n            self.con1 = nn.Conv2d(outer_nc, outer_nc, kernel_size=1)\n            self.con1.weight.data.normal_(0, 0.01)\n            \n        if depth == 0:\n            down = ResBlock4\n            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n                                        kernel_size=3, stride=1,\n                                        padding=1, bias=use_bias)\n            up = [uprelu, upconv, upnorm]\n            if use_dropout:\n                model = down + [submodule] + up + [nn.Dropout(0.5)]\n            else:\n                model = down + [submodule] + up\n            self.U0 = nn.Sequential(*model)\n            self.con0 = nn.Conv2d(outer_nc, outer_nc, kernel_size=1)\n            self.con0.weight.data.normal_(0, 0.01)\n        \n        if depth == -1: #idiot layer, forwards x straightly to next Unet block\n            model = [submodule]\n            self.U_1 = nn.Sequential(*model)\n        \n        if depth == -2: #model(x) forwards to Hook \n            down = ResBlock5\n            #up = [nn.Upsample(256, mode='bilinear'),]\n            lsm = [nn.LogSoftmax(),]\n            model = down + lsm\n            self.U_2 = nn.Sequential(*model)\n                \n    def forward(self, x):\n        if self.depth == 4:\n            sm = nn.Softmax2d()\n            lsm = nn.LogSoftmax()\n            t = self.U4(x)\n            return {'GAN':sm(t * 5.0), 'L1':lsm(t)}\n        elif self.depth == 3:\n            return torch.cat([self.con3(x), self.U3(x)], 1)\n        elif self.depth == 2:\n            return torch.cat([self.con2(x), self.U2(x)], 1)\n        elif self.depth == 1:\n            return torch.cat([self.con1(x), self.U1(x)], 1)\n        elif self.depth == 0:\n            return torch.cat([self.con0(x), self.U0(x)], 1)\n        elif self.depth == -1:\n            _ = self.U_1(x)\n            return x\n        elif self.depth == -2:\n            sm = nn.Softmax2d()\n            lsm = nn.LogSoftmax()\n            t = self.U_2(x)\n            return {'GAN':sm(t * 5.0), 'L1':lsm(t)}\n            #return self.U_2(x)\n\n\n# Defines the PatchGAN discriminator with the specified arguments.\nclass NLayerDiscriminator(nn.Module):\n    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False, gpu_ids=[]):\n        super(NLayerDiscriminator, self).__init__()\n        self.gpu_ids = gpu_ids\n        if type(norm_layer) == functools.partial:\n            #use_bias = norm_layer.func == nn.InstanceNorm2d\n            use_bias = False\n        else:\n            #use_bias = norm_layer == nn.InstanceNorm2d\n            use_bias = False\n\n        kw = 4\n        padw = 1\n        sequence = [\n            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        nf_mult = 1\n        nf_mult_prev = 1\n        for n in range(1, n_layers):\n            nf_mult_prev = nf_mult\n            nf_mult = min(2**n, 8)\n            sequence += [\n                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                          kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n                norm_layer(ndf * nf_mult),\n                nn.LeakyReLU(0.2, True)\n            ]\n\n        nf_mult_prev = nf_mult\n        nf_mult = min(2**n_layers, 8)\n        sequence += [\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                      kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n            norm_layer(ndf * nf_mult),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n\n        if use_sigmoid:\n            sequence += [nn.Sigmoid()]\n\n        self.model = nn.Sequential(*sequence)\n\n    def forward(self, input):\n        if len(self.gpu_ids) and isinstance(input.data, torch.cuda.FloatTensor):\n            return nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n        else:\n            return self.model(input)\n"""
models/pix2pix_model.py,17,"b""import torch\nimport random\nimport torch.nn as nn\nfrom collections import OrderedDict\nfrom torch.autograd import Variable\nimport util.util as util\nfrom util.image_pool import ImagePool\nfrom .base_model import BaseModel\nfrom . import networks\n\ndef swap(T, m, n): #Distinguish left & right\n    A = T.data.cpu().numpy()\n    A[:, [m, n], :, :] = A[:, [n, m], :, :]\n    return Variable(torch.from_numpy(A)).cuda()\n\nclass Pix2PixModel(BaseModel):\n    def name(self):\n        return 'Pix2PixModel'\n\n    def initialize(self, opt):\n        BaseModel.initialize(self, opt)\n        self.epoch = 1\n        self.isTrain = opt.isTrain\n        # define tensors for G1\n        self.input_A = self.Tensor(opt.batchSize, opt.input_nc,\n                                   opt.fineSize, opt.fineSize)\n        self.input_A_S = self.Tensor(opt.batchSize, opt.input_nc,\n                                   int(opt.fineSize * 0.75), int(opt.fineSize * 0.75))\n        self.input_A_L = self.Tensor(opt.batchSize, opt.input_nc,\n                                   int(opt.fineSize * 1.25), int(opt.fineSize * 1.25))\n        \n        self.input_A_Attribute = self.Tensor(opt.batchSize, opt.input_nc, int(opt.fineSize/16), int(opt.fineSize/16))\n        \n        self.input_B_GAN = self.Tensor(opt.batchSize, opt.output_nc,\n                                   opt.fineSize, opt.fineSize)\n        self.input_B_L1 = self.LongTensor(opt.batchSize,\n                                   opt.fineSize, opt.fineSize)\n        \n        self.input_B_Attribute_GAN = self.Tensor(opt.batchSize, opt.output_nc, \n                                    int(opt.fineSize/16), int(opt.fineSize/16))\n        \n        self.input_B_Attribute_L1 = self.LongTensor(opt.batchSize, \n                                    int(opt.fineSize/16), int(opt.fineSize/16))\n        \n        #define hook\n        self.hook = networks.UnetHook()\n        \n        # load/define networks\n        self.netG = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf,\n                                          opt.which_model_netG, self.hook, opt.fineSize, opt.norm, not opt.no_dropout, opt.init_type, self.gpu_ids)\n        \n        if self.isTrain:\n            use_sigmoid = opt.no_lsgan\n            \n            self.netD = networks.define_D(opt.input_nc + opt.output_nc, opt.ndf,\n                                              opt.which_model_netD,\n                                              opt.n_layers_D - 1, opt.norm, use_sigmoid, opt.init_type, self.gpu_ids)\n    \n            self.netD2 = networks.define_D(opt.input_nc + opt.output_nc, opt.ndf,\n                                              opt.which_model_netD,\n                                              opt.n_layers_D - 1, opt.norm, use_sigmoid, opt.init_type, self.gpu_ids)\n            \n        if not self.isTrain or opt.continue_train:\n            self.load_network(self.netG, 'G', opt.which_epoch)\n            if self.isTrain:\n                self.load_network(self.netD, 'D', opt.which_epoch)\n                self.load_network(self.netD2, 'D2', opt.which_epoch)\n                    \n        if self.isTrain:\n            self.fake_AB_pool = ImagePool(opt.pool_size)\n            self.old_lr = opt.lr\n            # define loss functions\n            self.criterionL1 = torch.nn.NLLLoss2d()\n            self.criterionGAN = networks.GANLoss(use_lsgan=not opt.no_lsgan, tensor=self.Tensor)\n            self.criterionAttributeL1 = torch.nn.NLLLoss2d()\n            self.criterionAttributeGAN = networks.GANLoss(use_lsgan=not opt.no_lsgan, tensor=self.Tensor)\n\n            # initialize optimizers\n            self.schedulers = []\n            self.optimizers = []\n            \n            ignored_params = list(map(id, self.netG.model.U4[2].U3[4].U2[4].U1[23].U0[3].U_1[0].U_2.parameters() )) \\\n            + list(map(id, self.netG.model.U4[2].U3[4].U2[4].U1[23].U0[5].parameters() )) + list(map(id, self.netG.model.U4[2].U3[4].U2[4].U1[23].U0[6].parameters() )) + list(map(id, self.netG.model.U4[2].U3[4].U2[4].U1[23].con0.parameters() )) \\\n            + list(map(id, self.netG.model.U4[2].U3[4].U2[4].U1[25].parameters() )) + list(map(id, self.netG.model.U4[2].U3[4].U2[4].U1[26].parameters() )) + list(map(id, self.netG.model.U4[2].U3[4].U2[4].con1.parameters() )) \\\n            + list(map(id, self.netG.model.U4[2].U3[4].U2[6].parameters() )) + list(map(id, self.netG.model.U4[2].U3[4].U2[7].parameters() )) + list(map(id, self.netG.model.U4[2].U3[4].con2.parameters() )) \\\n            + list(map(id, self.netG.model.U4[2].U3[6].parameters() )) + list(map(id, self.netG.model.U4[2].U3[7].parameters() )) + list(map(id, self.netG.model.U4[2].con3.parameters() )) \\\n            + list(map(id, self.netG.model.U4[4].parameters() ))\n            base_params = filter(lambda p: id(p) not in ignored_params, self.netG.parameters())\n            self.optimizer_G = torch.optim.Adam([{'params': base_params, 'lr': 0.1 * opt.lr},\n                                                {'params': self.netG.model.U4[2].U3[4].U2[4].U1[23].U0[3].U_1[0].U_2.parameters(), 'lr': opt.lr},\n                                                {'params': self.netG.model.U4[2].U3[4].U2[4].U1[23].U0[5].parameters(), 'lr': opt.lr},\n                                                {'params': self.netG.model.U4[2].U3[4].U2[4].U1[23].U0[6].parameters(), 'lr': opt.lr},\n                                                {'params': self.netG.model.U4[2].U3[4].U2[4].U1[23].con0.parameters(), 'lr': opt.lr},\n                                                {'params': self.netG.model.U4[2].U3[4].U2[4].U1[25].parameters(), 'lr': opt.lr},\n                                                {'params': self.netG.model.U4[2].U3[4].U2[4].U1[26].parameters(), 'lr': opt.lr},\n                                                {'params': self.netG.model.U4[2].U3[4].U2[4].con1.parameters(), 'lr': opt.lr},\n                                                {'params': self.netG.model.U4[2].U3[4].U2[6].parameters(), 'lr': opt.lr},\n                                                {'params': self.netG.model.U4[2].U3[4].U2[7].parameters(), 'lr': opt.lr},\n                                                {'params': self.netG.model.U4[2].U3[4].con2.parameters(), 'lr': opt.lr},\n                                                {'params': self.netG.model.U4[2].U3[6].parameters(), 'lr': opt.lr},\n                                                {'params': self.netG.model.U4[2].U3[7].parameters(), 'lr': opt.lr},\n                                                {'params': self.netG.model.U4[2].con3.parameters(), 'lr': opt.lr},\n                                                {'params': self.netG.model.U4[4].parameters(), 'lr': opt.lr},\n                                                ], betas=(opt.beta1, 0.999), weight_decay = 1e-4)\n            \n            self.optimizer_D = torch.optim.Adam(self.netD.parameters(),\n                                                lr=opt.lr, betas=(opt.beta1, 0.999), weight_decay = 1e-4)\n            \n            self.optimizer_D2 = torch.optim.Adam(self.netD2.parameters(),\n                                                lr=opt.lr, betas=(opt.beta1, 0.999), weight_decay = 1e-4)\n            \n            self.optimizers.append(self.optimizer_G)\n            self.optimizers.append(self.optimizer_D)\n            self.optimizers.append(self.optimizer_D2)\n            \n            for optimizer in self.optimizers:\n                self.schedulers.append(networks.get_scheduler(optimizer, opt))\n\n        print('---------- Networks initialized -------------')\n        networks.print_network(self.netG)\n        if self.isTrain:\n            networks.print_network(self.netD)\n            networks.print_network(self.netD2)\n        print('-----------------------------------------------')\n\n# =============================================================================\n#     input_A: original color image\n#     input_B_GAN: original label image (multiple channels) for GAN loss calculation\n#     input_B_L1: original label image (single channel) for L1 loss calculation\n#     input_B_Attribute: original thumbnail for Attribute loss calculation\n# =============================================================================\n    def set_input(self, input):\n        AtoB = self.opt.which_direction == 'AtoB'\n        self.image_paths = input['A_paths' if AtoB else 'B_paths']\n        \n        #G1\n        input_A = input['A']\n        input_A_S = input['A_S']\n        input_A_L = input['A_L']\n        input_A_Attribute = input['A_Attribute']\n        \n        input_B_GAN = input['B_GAN']\n        input_B_L1 = input['B_L1']\n        input_B_Attribute_L1 = input['B_Attribute_L1']\n        input_B_Attribute_GAN = input['B_Attribute_GAN']\n        \n        #G1\n        self.input_A.resize_(input_A.size()).copy_(input_A)\n        self.input_A_S.resize_(input_A_S.size()).copy_(input_A_S)\n        self.input_A_L.resize_(input_A_L.size()).copy_(input_A_L)\n        self.input_A_Attribute.resize_(input_A_Attribute.size()).copy_(input_A_Attribute)\n        \n        self.input_B_GAN.resize_(input_B_GAN.size()).copy_(input_B_GAN)\n        self.input_B_L1.resize_(input_B_L1.size()).copy_(input_B_L1)\n        self.input_B_Attribute_GAN.resize_(input_B_Attribute_GAN.size()).copy_(input_B_Attribute_GAN)\n        self.input_B_Attribute_L1.resize_(input_B_Attribute_L1.size()).copy_(input_B_Attribute_L1)\n        \n        \n    def forward(self):\n        self.real_A = Variable(self.input_A)\n        self.real_A_Attribute = Variable(self.input_A_Attribute)\n\n        #Copy from files\n        self.real_B_GAN = Variable(self.input_B_GAN) #multi-channel target for label map\n        self.real_B_L1 = Variable(self.input_B_L1) #single-channel target for label map\n        self.real_B_Attribute_GAN = Variable(self.input_B_Attribute_GAN) #multi-channel target for thumbnail\n        self.real_B_Attribute_L1 = Variable(self.input_B_Attribute_L1) # single-channel target for thumbnail\n\n        #Generate from networks\n        self.fake_B_GAN = self.netG(self.real_A)['GAN'] #multi-channel label map--> target real_B_GAN\n        self.fake_B_L1 = self.netG(self.real_A)['L1'] #multi-channel label map but nagtive --> target real_B_L1\n        self.fake_B_Attribute_GAN = self.hook.get_value()['GAN'] #multi-channel thumbnail --> real_B_Attribute_GAN\n        self.fake_B_Attribute_L1 = self.hook.get_value()['L1'] #multi-channel thumbnail but nagtive --> real_B_Attribute_L1\n        \n\n    def test(self):\n        self.real_A = Variable(self.input_A, volatile=True)\n        self.real_A_S = Variable(self.input_A_S, volatile=True)\n        self.real_A_L = Variable(self.input_A_L, volatile=True)\n\n        M = nn.Upsample(int(self.real_A.size(3)), mode='bilinear')\n\n        self.fake_B_GAN = self.netG(self.real_A)['GAN']\n        self.fake_B_GAN_S = self.netG(self.real_A_S)['GAN']\n        self.fake_B_GAN_L = self.netG(self.real_A_L)['GAN']\n\n        self.fake_B_GAN_S = M(self.fake_B_GAN_S)\n        self.fake_B_GAN_L = M(self.fake_B_GAN_L)\n        \n        idx = [i for i in range(self.real_A.size(3) - 1, -1, -1)]\n        idx = torch.LongTensor(idx).cuda()\n        idx = Variable(idx)\n        idx_S = [i for i in range(self.real_A_S.size(3) - 1, -1, -1)]\n        idx_S = torch.LongTensor(idx_S).cuda()\n        idx_S = Variable(idx_S)\n        idx_L = [i for i in range(self.real_A_L.size(3) - 1, -1, -1)]\n        idx_L = torch.LongTensor(idx_L).cuda()\n        idx_L = Variable(idx_L)\n        \n        self.real_A_flip = self.real_A.index_select(3, idx)\n        self.fake_B_flip = self.netG(self.real_A_flip)['GAN']\n        self.fake_B_flip_flip = self.fake_B_flip.index_select(3, idx)\n        if self.opt.dataset == 'LIP':\n            self.fake_B_flip_flip = swap(self.fake_B_flip_flip, 14, 15)\n            self.fake_B_flip_flip = swap(self.fake_B_flip_flip, 16, 17)\n            self.fake_B_flip_flip = swap(self.fake_B_flip_flip, 18, 19)\n        \n        self.real_A_flip_S = self.real_A_S.index_select(3, idx_S)\n        self.fake_B_flip_S = self.netG(self.real_A_flip_S)['GAN']\n        self.fake_B_flip_flip_S = self.fake_B_flip_S.index_select(3, idx_S)\n        if self.opt.dataset == 'LIP':\n            self.fake_B_flip_flip_S = swap(self.fake_B_flip_flip_S, 14, 15)\n            self.fake_B_flip_flip_S = swap(self.fake_B_flip_flip_S, 16, 17)\n            self.fake_B_flip_flip_S = swap(self.fake_B_flip_flip_S, 18, 19)\n        self.fake_B_flip_flip_S = M(self.fake_B_flip_flip_S)\n        \n        self.real_A_flip_L = self.real_A_L.index_select(3, idx_L)\n        self.fake_B_flip_L = self.netG(self.real_A_flip_L)['GAN']\n        self.fake_B_flip_flip_L = self.fake_B_flip_L.index_select(3, idx_L)\n        if self.opt.dataset == 'LIP':\n            self.fake_B_flip_flip_L = swap(self.fake_B_flip_flip_L, 14, 15)\n            self.fake_B_flip_flip_L = swap(self.fake_B_flip_flip_L, 16, 17)\n            self.fake_B_flip_flip_L = swap(self.fake_B_flip_flip_L, 18, 19)\n        self.fake_B_flip_flip_L = M(self.fake_B_flip_flip_L)\n        \n        self.fake_B_GAN = self.fake_B_GAN + self.fake_B_flip_flip + self.fake_B_GAN_S + self.fake_B_flip_flip_S + self.fake_B_GAN_L + self.fake_B_flip_flip_L\n        self.real_B_GAN = Variable(self.input_B_GAN, volatile=True)\n\n    # get image paths\n    def get_image_paths(self):\n        return self.image_paths\n\n    def backward_D(self):\n        # Fake\n        # stop backprop to the generator by detaching fake_B\n        fake_AB = self.fake_AB_pool.query(torch.cat((self.real_A, self.fake_B_GAN), 1).data)\n        pred_fake = self.netD(fake_AB.detach())\n        self.loss_D_fake = self.criterionGAN(pred_fake, False)\n        \n        # Real\n        real_AB = torch.cat((self.real_A, self.real_B_GAN), 1)\n        pred_real = self.netD(real_AB)\n        self.loss_D_real = self.criterionGAN(pred_real, True)\n        \n        # Combined loss\n        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n        self.loss_D.backward()\n\n    \n    def backward_D2(self):\n        fake_AB_Attribute = torch.cat((self.real_A_Attribute, self.fake_B_Attribute_GAN), 1)\n        pred_fake_Attribute = self.netD2(fake_AB_Attribute.detach())\n        self.loss_D_fake_Attribute = self.criterionAttributeGAN(pred_fake_Attribute, False)\n        \n        real_AB_Attribute = torch.cat((self.real_A_Attribute, self.real_B_Attribute_GAN), 1)\n        pred_real_Attribute = self.netD2(real_AB_Attribute)\n        self.loss_D_real_Attribute = self.criterionAttributeGAN(pred_real_Attribute, True)\n        \n        self.loss_D_Attribute = (self.loss_D_fake_Attribute + self.loss_D_real_Attribute) * 0.5\n        self.loss_D_Attribute.backward()\n        \n        \n    def backward_G(self):\n        #GAN loss: G(x) should fake the discriminator\n        fake_AB = torch.cat((self.real_A, self.fake_B_GAN), 1)\n        pred_fake = self.netD(fake_AB)\n        self.loss_G_GAN = self.criterionGAN(pred_fake, True)\n        \n        #Attribute GAN loss: G(x) should fake the discriminator2\n        fake_AB_Attribute = torch.cat((self.real_A_Attribute, self.fake_B_Attribute_GAN), 1)\n        pred_fake_Attribute = self.netD2(fake_AB_Attribute.detach())\n        self.loss_G_GAN_Attribute = self.criterionAttributeGAN(pred_fake_Attribute, True)\n        \n        #L1 loss: Minimize logSoftmax concats NLL2d between original size\n        self.loss_G_L1 = self.criterionL1(self.fake_B_L1, self.real_B_L1) * self.opt.lambda_A\n        \n        #Attribute L1 loss: Minimize logSoftmax concats NLL2d between thumbnail\n        self.loss_G_L1_Attribute = self.criterionAttributeL1(self.fake_B_Attribute_L1, self.real_B_Attribute_L1) * self.opt.lambda_A\n        \n        self.loss_G = 5.0 * self.loss_G_L1 + 1.0 * self.loss_G_L1_Attribute +  \\\n           1 * self.loss_G_GAN + 1 * self.loss_G_GAN_Attribute #for LIP\n            \n        self.loss_G.backward()\n        \n        \n    def optimize_parameters(self):\n        self.forward()\n\n        self.optimizer_D.zero_grad()\n        self.backward_D()\n        self.optimizer_D2.zero_grad()\n        self.backward_D2()\n        \n        if random.random() < 0.1:\n            self.optimizer_D.step()\n            self.optimizer_D2.step()\n            \n        self.optimizer_G.zero_grad()\n        self.backward_G()\n        self.optimizer_G.step()\n        \n    \n    def get_current_errors(self):\n        return OrderedDict([('G_GAN', self.loss_G_GAN.data[0]),\n                            ('G_L1', self.loss_G_L1.data[0]),\n                            ('D_real', self.loss_D_real.data[0]),\n                            ('D_fake', self.loss_D_fake.data[0]),\n                            ('G_GAN_Attri', self.loss_G_GAN_Attribute.data[0]),\n                            ('G_L1_Attri', self.loss_G_L1_Attribute.data[0]),\n                            ('D_real_Attri', self.loss_D_real_Attribute.data[0]),\n                            ('D_fake_Attri', self.loss_D_fake_Attribute.data[0])\n                            ])\n\n    def get_current_visuals(self):\n        real_A = util.tensor2im(self.real_A.data)\n        fake_B = util.ndim_tensor2im(self.fake_B_GAN.data, dataset = self.opt.dataset)\n        real_B = util.ndim_tensor2im(self.real_B_GAN.data, dataset = self.opt.dataset)\n        return OrderedDict([('real_A', real_A), ('fake_B', fake_B), ('real_B', real_B)])\n\n    def save(self, label):\n        self.epoch = self.epoch + 1\n        print('weight L1:', min(self.epoch / 20.0, 5.0) * self.opt.lambda_A, 'weight L1 Attri:', self.opt.lambda_A, \n              'weight GAN:', min(self.epoch // 20.0, 1.0), 'weight GAN Attri:', 0.5)\n        self.save_network(self.netG, 'G', label, self.gpu_ids)\n        self.save_network(self.netD, 'D', label, self.gpu_ids)\n        self.save_network(self.netD2, 'D2', label, self.gpu_ids)\n"""
models/test_model.py,1,"b""from torch.autograd import Variable\nfrom collections import OrderedDict\nimport util.util as util\nfrom .base_model import BaseModel\nfrom . import networks\n\n\nclass TestModel(BaseModel):\n    def name(self):\n        return 'TestModel'\n\n    def initialize(self, opt):\n        assert(not opt.isTrain)\n        BaseModel.initialize(self, opt)\n        self.input_A = self.Tensor(opt.batchSize, opt.input_nc, opt.fineSize, opt.fineSize)\n\n        self.netG = networks.define_G(opt.input_nc, opt.output_nc,\n                                      opt.ngf, opt.which_model_netG,\n                                      opt.norm, not opt.no_dropout,\n                                      opt.init_type,\n                                      self.gpu_ids)\n        which_epoch = opt.which_epoch\n        self.load_network(self.netG, 'G', which_epoch)\n\n        print('---------- Networks initialized -------------')\n        networks.print_network(self.netG)\n        print('-----------------------------------------------')\n\n    def set_input(self, input):\n        # we need to use single_dataset mode\n        input_A = input['A']\n        self.input_A.resize_(input_A.size()).copy_(input_A)\n        self.image_paths = input['A_paths']\n\n    def test(self):\n        self.real_A = Variable(self.input_A)\n        self.fake_B = self.netG(self.real_A)\n\n    # get image paths\n    def get_image_paths(self):\n        return self.image_paths\n\n    def get_current_visuals(self):\n        real_A = util.tensor2im(self.real_A.data)\n        fake_B = util.tensor2im(self.fake_B.data)\n        return OrderedDict([('real_A', real_A), ('fake_B', fake_B)])\n"""
options/__init__.py,0,b''
options/base_options.py,1,"b'import argparse\nimport os\nfrom util import util\nimport torch\n\n\nclass BaseOptions():\n    def __init__(self):\n        self.parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n        self.initialized = False\n\n    def initialize(self):\n        self.parser.add_argument(\'--dataroot\', required=True, help=\'path to images (should have subfolders trainA, trainB, valA, valB, etc)\')\n        self.parser.add_argument(\'--dataset\', required=True, help=\'PPSS, Pascal or LIP\')\n        self.parser.add_argument(\'--batchSize\', type=int, default=1, help=\'input batch size\')\n        self.parser.add_argument(\'--loadSize\', type=int, default=286, help=\'scale images to this size\')\n        self.parser.add_argument(\'--fineSize\', type=int, default=256, help=\'then crop to this size\')\n        self.parser.add_argument(\'--input_nc\', type=int, default=3, help=\'# of input image channels\')\n        self.parser.add_argument(\'--output_nc\', type=int, default=20, help=\'# of output image channels, LIP = 20\')\n        self.parser.add_argument(\'--ngf\', type=int, default=64, help=\'# of gen filters in first conv layer\')\n        self.parser.add_argument(\'--ndf\', type=int, default=64, help=\'# of discrim filters in first conv layer\')\n        self.parser.add_argument(\'--which_model_netD\', type=str, default=\'n_layers\', help=\'selects model to use for netD\')\n        self.parser.add_argument(\'--which_model_netG\', type=str, default=\'unet_256\', help=\'selects model to use for netG\')\n        self.parser.add_argument(\'--n_layers_D\', type=int, default=3, help=\'only used if which_model_netD==n_layers\')\n        self.parser.add_argument(\'--gpu_ids\', type=str, default=\'0\', help=\'gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU\')\n        self.parser.add_argument(\'--name\', type=str, default=\'experiment_name\', help=\'name of the experiment. It decides where to store samples and models\')\n        self.parser.add_argument(\'--dataset_mode\', type=str, default=\'aligned\', help=\'chooses how datasets are loaded. [unaligned | aligned | single]\')\n        self.parser.add_argument(\'--model\', type=str, default=\'pix2pix\',\n                                 help=\'chooses which model to use. cycle_gan, pix2pix, test\')\n        self.parser.add_argument(\'--which_direction\', type=str, default=\'AtoB\', help=\'AtoB or BtoA\')\n        self.parser.add_argument(\'--nThreads\', default=2, type=int, help=\'# threads for loading data\')\n        self.parser.add_argument(\'--checkpoints_dir\', type=str, default=\'./checkpoints\', help=\'models are saved here\')\n        self.parser.add_argument(\'--norm\', type=str, default=\'instance\', help=\'instance normalization or batch normalization\')\n        self.parser.add_argument(\'--serial_batches\', action=\'store_true\', help=\'if true, takes images in order to make batches, otherwise takes them randomly\')\n        self.parser.add_argument(\'--display_winsize\', type=int, default=256,  help=\'display window size\')\n        self.parser.add_argument(\'--display_id\', type=int, default=1, help=\'window id of the web display\')\n        self.parser.add_argument(\'--display_port\', type=int, default=8097, help=\'visdom port of the web display\')\n        self.parser.add_argument(\'--no_dropout\', action=\'store_true\', help=\'no dropout for the generator\')\n        self.parser.add_argument(\'--max_dataset_size\', type=int, default=float(""inf""), help=\'Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.\')\n        self.parser.add_argument(\'--resize_or_crop\', type=str, default=\'resize_and_crop\', help=\'scaling and cropping of images at load time [resize_and_crop|crop|scale_width|scale_width_and_crop]\')\n        self.parser.add_argument(\'--no_flip\', action=\'store_true\', help=\'if specified, do not flip the images for data augmentation\')\n        self.parser.add_argument(\'--init_type\', type=str, default=\'xavier\', help=\'network initialization [normal|xavier|kaiming|orthogonal]\')\n\n        self.initialized = True\n\n    def parse(self):\n        if not self.initialized:\n            self.initialize()\n        self.opt = self.parser.parse_args()\n        self.opt.isTrain = self.isTrain   # train or test\n\n        str_ids = self.opt.gpu_ids.split(\',\')\n        self.opt.gpu_ids = []\n        for str_id in str_ids:\n            id = int(str_id)\n            if id >= 0:\n                self.opt.gpu_ids.append(id)\n\n        # set gpu ids\n        if len(self.opt.gpu_ids) > 0:\n            torch.cuda.set_device(self.opt.gpu_ids[0])\n\n        args = vars(self.opt)\n\n        print(\'------------ Options -------------\')\n        for k, v in sorted(args.items()):\n            print(\'%s: %s\' % (str(k), str(v)))\n        print(\'-------------- End ----------------\')\n\n        # save to the disk\n        expr_dir = os.path.join(self.opt.checkpoints_dir, self.opt.name)\n        util.mkdirs(expr_dir)\n        file_name = os.path.join(expr_dir, \'opt.txt\')\n        with open(file_name, \'wt\') as opt_file:\n            opt_file.write(\'------------ Options -------------\\n\')\n            for k, v in sorted(args.items()):\n                opt_file.write(\'%s: %s\\n\' % (str(k), str(v)))\n            opt_file.write(\'-------------- End ----------------\\n\')\n        return self.opt\n'"
options/test_options.py,0,"b'from .base_options import BaseOptions\n\n\nclass TestOptions(BaseOptions):\n    def initialize(self):\n        BaseOptions.initialize(self)\n        self.parser.add_argument(\'--ntest\', type=int, default=float(""inf""), help=\'# of test examples.\')\n        self.parser.add_argument(\'--results_dir\', type=str, default=\'./results/\', help=\'saves results here.\')\n        self.parser.add_argument(\'--aspect_ratio\', type=float, default=1.0, help=\'aspect ratio of result images\')\n        self.parser.add_argument(\'--phase\', type=str, default=\'test\', help=\'train, val, test, etc\')\n        self.parser.add_argument(\'--which_epoch\', type=str, default=\'latest\', help=\'which epoch to load? set to latest to use latest cached model\')\n        self.parser.add_argument(\'--how_many\', type=int, default=50, help=\'how many test images to run\')\n        #self.parser.add_argument(\'--identity\', type=float, default=0.0, help=\'use identity mapping. Setting identity other than 1 has an effect of scaling the weight of the identity mapping loss. For example, if the weight of the identity loss should be 10 times smaller than the weight of the reconstruction loss, please set optidentity = 0.1\')\n        self.isTrain = False\n'"
options/train_options.py,0,"b""from .base_options import BaseOptions\n\n\nclass TrainOptions(BaseOptions):\n    def initialize(self):\n        BaseOptions.initialize(self)\n        self.parser.add_argument('--display_freq', type=int, default=100, help='frequency of showing training results on screen')\n        self.parser.add_argument('--display_single_pane_ncols', type=int, default=0, help='if positive, display all images in a single visdom web panel with certain number of images per row.')\n        self.parser.add_argument('--update_html_freq', type=int, default=1000, help='frequency of saving training results to html')\n        self.parser.add_argument('--print_freq', type=int, default=100, help='frequency of showing training results on console')\n        self.parser.add_argument('--save_latest_freq', type=int, default=5000, help='frequency of saving the latest results')\n        self.parser.add_argument('--save_epoch_freq', type=int, default=5, help='frequency of saving checkpoints at the end of epochs')\n        self.parser.add_argument('--continue_train', action='store_true', help='continue training: load the latest model')\n        self.parser.add_argument('--pre_trained', action='store_true', help='load pretrained resnet101 model on imagenet')\n        self.parser.add_argument('--epoch_count', type=int, default=1, help='the starting epoch count, we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>, ...')\n        self.parser.add_argument('--phase', type=str, default='train', help='train, val, test, etc')\n        self.parser.add_argument('--which_epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n        self.parser.add_argument('--niter', type=int, default=100, help='# of iter at starting learning rate')\n        self.parser.add_argument('--niter_decay', type=int, default=100, help='# of iter to linearly decay learning rate to zero')\n        self.parser.add_argument('--beta1', type=float, default=0.5, help='momentum term of adam')\n        self.parser.add_argument('--lr', type=float, default=0.0002, help='initial learning rate for adam')\n        self.parser.add_argument('--no_lsgan', action='store_true', help='do *not* use least square GAN, if false, use vanilla GAN')\n        self.parser.add_argument('--lambda_A', type=float, default=50.0, help='weight for cycle loss (A -> B -> A)')\n        self.parser.add_argument('--lambda_B', type=float, default=10.0, help='weight for cycle loss (B -> A -> B)')\n        self.parser.add_argument('--pool_size', type=int, default=0, help='the size of image buffer that stores previously generated images')\n        self.parser.add_argument('--no_html', action='store_true', help='do not save intermediate training results to [opt.checkpoints_dir]/[opt.name]/web/')\n        self.parser.add_argument('--lr_policy', type=str, default='lambda', help='learning rate policy: lambda|step|plateau')\n        self.parser.add_argument('--lr_decay_iters', type=int, default=50, help='multiply by a gamma every lr_decay_iters iterations')\n        self.parser.add_argument('--identity', type=float, default=0.5, help='use identity mapping. Setting identity other than 1 has an effect of scaling the weight of the identity mapping loss. For example, if the weight of the identity loss should be 10 times smaller than the weight of the reconstruction loss, please set optidentity = 0.1')\n\n        self.isTrain = True\n"""
util/__init__.py,0,b''
util/get_data.py,0,"b'from __future__ import print_function\nimport os\nimport tarfile\nimport requests\nfrom warnings import warn\nfrom zipfile import ZipFile\nfrom bs4 import BeautifulSoup\nfrom os.path import abspath, isdir, join, basename\n\n\nclass GetData(object):\n    """"""\n\n    Download CycleGAN or Pix2Pix Data.\n\n    Args:\n        technique : str\n            One of: \'cyclegan\' or \'pix2pix\'.\n        verbose : bool\n            If True, print additional information.\n\n    Examples:\n        >>> from util.get_data import GetData\n        >>> gd = GetData(technique=\'cyclegan\')\n        >>> new_data_path = gd.get(save_path=\'./datasets\')  # options will be displayed.\n\n    """"""\n\n    def __init__(self, technique=\'cyclegan\', verbose=True):\n        url_dict = {\n            \'pix2pix\': \'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets\',\n            \'cyclegan\': \'https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets\'\n        }\n        self.url = url_dict.get(technique.lower())\n        self._verbose = verbose\n\n    def _print(self, text):\n        if self._verbose:\n            print(text)\n\n    @staticmethod\n    def _get_options(r):\n        soup = BeautifulSoup(r.text, \'lxml\')\n        options = [h.text for h in soup.find_all(\'a\', href=True)\n                   if h.text.endswith((\'.zip\', \'tar.gz\'))]\n        return options\n\n    def _present_options(self):\n        r = requests.get(self.url)\n        options = self._get_options(r)\n        print(\'Options:\\n\')\n        for i, o in enumerate(options):\n            print(""{0}: {1}"".format(i, o))\n        choice = input(""\\nPlease enter the number of the ""\n                       ""dataset above you wish to download:"")\n        return options[int(choice)]\n\n    def _download_data(self, dataset_url, save_path):\n        if not isdir(save_path):\n            os.makedirs(save_path)\n\n        base = basename(dataset_url)\n        temp_save_path = join(save_path, base)\n\n        with open(temp_save_path, ""wb"") as f:\n            r = requests.get(dataset_url)\n            f.write(r.content)\n\n        if base.endswith(\'.tar.gz\'):\n            obj = tarfile.open(temp_save_path)\n        elif base.endswith(\'.zip\'):\n            obj = ZipFile(temp_save_path, \'r\')\n        else:\n            raise ValueError(""Unknown File Type: {0}."".format(base))\n\n        self._print(""Unpacking Data..."")\n        obj.extractall(save_path)\n        obj.close()\n        os.remove(temp_save_path)\n\n    def get(self, save_path, dataset=None):\n        """"""\n\n        Download a dataset.\n\n        Args:\n            save_path : str\n                A directory to save the data to.\n            dataset : str, optional\n                A specific dataset to download.\n                Note: this must include the file extension.\n                If None, options will be presented for you\n                to choose from.\n\n        Returns:\n            save_path_full : str\n                The absolute path to the downloaded data.\n\n        """"""\n        if dataset is None:\n            selected_dataset = self._present_options()\n        else:\n            selected_dataset = dataset\n\n        save_path_full = join(save_path, selected_dataset.split(\'.\')[0])\n\n        if isdir(save_path_full):\n            warn(""\\n\'{0}\' already exists. Voiding Download."".format(\n                save_path_full))\n        else:\n            self._print(\'Downloading Data...\')\n            url = ""{0}/{1}"".format(self.url, selected_dataset)\n            self._download_data(url, save_path=save_path)\n\n        return abspath(save_path_full)\n'"
util/html.py,0,"b'import dominate\nfrom dominate.tags import *\nimport os\n\n\nclass HTML:\n    def __init__(self, web_dir, title, reflesh=0):\n        self.title = title\n        self.web_dir = web_dir\n        self.img_dir = os.path.join(self.web_dir, \'images\')\n        if not os.path.exists(self.web_dir):\n            os.makedirs(self.web_dir)\n        if not os.path.exists(self.img_dir):\n            os.makedirs(self.img_dir)\n        # print(self.img_dir)\n\n        self.doc = dominate.document(title=title)\n        if reflesh > 0:\n            with self.doc.head:\n                meta(http_equiv=""reflesh"", content=str(reflesh))\n\n    def get_image_dir(self):\n        return self.img_dir\n\n    def add_header(self, str):\n        with self.doc:\n            h3(str)\n\n    def add_table(self, border=1):\n        self.t = table(border=border, style=""table-layout: fixed;"")\n        self.doc.add(self.t)\n\n    def add_images(self, ims, txts, links, width=400):\n        self.add_table()\n        with self.t:\n            with tr():\n                for im, txt, link in zip(ims, txts, links):\n                    with td(style=""word-wrap: break-word;"", halign=""center"", valign=""top""):\n                        with p():\n                            with a(href=os.path.join(\'images\', link)):\n                                img(style=""width:%dpx"" % width, src=os.path.join(\'images\', im))\n                            br()\n                            p(txt)\n\n    def save(self):\n        html_file = \'%s/index.html\' % self.web_dir\n        f = open(html_file, \'wt\')\n        f.write(self.doc.render())\n        f.close()\n\n\nif __name__ == \'__main__\':\n    html = HTML(\'web/\', \'test_html\')\n    html.add_header(\'hello world\')\n\n    ims = []\n    txts = []\n    links = []\n    for n in range(4):\n        ims.append(\'image_%d.png\' % n)\n        txts.append(\'text_%d\' % n)\n        links.append(\'image_%d.png\' % n)\n    html.add_images(ims, txts, links)\n    html.save()\n'"
util/image_pool.py,3,"b'import random\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\n\n\nclass ImagePool():\n    def __init__(self, pool_size):\n        self.pool_size = pool_size\n        if self.pool_size > 0:\n            self.num_imgs = 0\n            self.images = []\n\n    def query(self, images):\n        if self.pool_size == 0:\n            return Variable(images)\n        return_images = []\n        for image in images:\n            image = torch.unsqueeze(image, 0)\n            if self.num_imgs < self.pool_size:\n                self.num_imgs = self.num_imgs + 1\n                self.images.append(image)\n                return_images.append(image)\n            else:\n                p = random.uniform(0, 1)\n                if p > 0.5:\n                    random_id = random.randint(0, self.pool_size-1)\n                    tmp = self.images[random_id].clone()\n                    self.images[random_id] = image\n                    return_images.append(tmp)\n                else:\n                    return_images.append(image)\n        return_images = Variable(torch.cat(return_images, 0))\n        return return_images\n'"
util/png.py,0,"b'import struct\nimport zlib\n\ndef encode(buf, width, height):\n  """""" buf: must be bytes or a bytearray in py3, a regular string in py2. formatted RGBRGB... """"""\n  assert (width * height * 3 == len(buf))\n  bpp = 3\n\n  def raw_data():\n    # reverse the vertical line order and add null bytes at the start\n    row_bytes = width * bpp\n    for row_start in range((height - 1) * width * bpp, -1, -row_bytes):\n      yield b\'\\x00\'\n      yield buf[row_start:row_start + row_bytes]\n\n  def chunk(tag, data):\n    return [\n        struct.pack(""!I"", len(data)),\n        tag,\n        data,\n        struct.pack(""!I"", 0xFFFFFFFF & zlib.crc32(data, zlib.crc32(tag)))\n      ]\n\n  SIGNATURE = b\'\\x89PNG\\r\\n\\x1a\\n\'\n  COLOR_TYPE_RGB = 2\n  COLOR_TYPE_RGBA = 6\n  bit_depth = 8\n  return b\'\'.join(\n      [ SIGNATURE ] +\n      chunk(b\'IHDR\', struct.pack(""!2I5B"", width, height, bit_depth, COLOR_TYPE_RGB, 0, 0, 0)) +\n      chunk(b\'IDAT\', zlib.compress(b\'\'.join(raw_data()), 9)) +\n      chunk(b\'IEND\', b\'\')\n    )\n'"
util/util.py,1,"b'from __future__ import print_function\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport inspect, re\nimport os\nimport collections\n\n# Converts a Tensor into a Numpy array\n# |imtype|: the desired type of the converted numpy array\n\ndef tensor2im(image_tensor, imtype=np.uint8):\n    #print(image_tensor)\n    image_numpy = image_tensor[0].cpu().float().numpy()\n    if image_numpy.shape[0] == 1:\n        image_numpy = np.tile(image_numpy, (3, 1, 1))\n    image_numpy = (np.transpose(image_numpy, (1, 2, 0)))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    image_numpy = (std * image_numpy + mean) * 255\n    return image_numpy.astype(imtype)\n\ndef ndim_tensor2im(image_tensor, imtype=np.uint8, dataset = \'PPSS\', dim = \'L2\'):\n    if dataset == \'Horse\' or dataset == \'Cow\' or dataset == \'PPSS\':\n        #palette_idx = np.array([[0, 0, 0], [0, 32, 255], [0, 191, 255], [96, 255, 159], [255, 80, 0], [255, 255, 0], [175, 0, 0], [143, 0, 0]]) #PPSS\n         palette_idx = np.array([[0, 0, 143], [0, 32, 255], [0, 32, 255], [255, 80, 0], [0, 191, 255], [96, 255, 159], [96, 255, 159], [96, 255, 159], [143, 0, 0], [255, 255, 0],\n              [96, 255, 159], [96, 255, 159], [255, 255, 0], [0, 191, 255], [255, 80, 0], [255, 80, 0], [175, 0, 0], [175, 0, 0], [143, 0, 0], [143, 0, 0]])#LIP\n    if dataset == \'LIP\' or dataset == \'Market\':\n        palette_idx = np.array([[0, 0, 0], [0, 255, 255], [255, 0, 255], [255, 255, 0], [255, 170, 255], [255, 255, 170], [170, 255, 255], [85, 255, 255], [85, 170, 255], [105, 45, 190],\n              [170, 105, 255], [170, 255, 85], [255, 170, 85], [255, 85, 170],  [85, 255, 85], [0, 255, 85], [255, 0, 85], [255, 85, 85], [0, 85, 255], [85, 85, 255]])#LIP\n    if dataset == \'Pascal\':\n        palette_idx = np.array([[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0], [0, 0, 128], [128, 0, 128], [0, 128, 128], [85, 255, 255], [85, 170, 255], [105, 45, 190],\n              [170, 105, 255], [170, 255, 85], [255, 170, 85], [255, 85, 170],  [85, 255, 85], [0, 255, 85], [255, 0, 85], [255, 85, 85], [0, 85, 255], [85, 85, 255]])#Pascal\n    if dataset == \'Market2\':\n        palette_idx = np.array([[0, 0, 0], [255, 0, 255], [255, 0, 255], [0, 255, 85], [255, 85, 170], [255, 255, 170], [255, 255, 170], [255, 255, 170], [255, 85, 85], [255, 170, 85],\n              [255, 255, 170], [255, 255, 170], [255, 170, 85], [255, 85, 170],  [0, 255, 85], [0, 255, 85], [255, 85, 85], [255, 85, 85], [85, 85, 255], [85, 85, 255]])#LIP\n    result = np.zeros(shape = (image_tensor.size(2), image_tensor.size(3), 3))\n    image_numpy = image_tensor[0].cpu().float().numpy()\n    #image_numpy = image_tensor[0].data.numpy()\n    for i in range(image_numpy.shape[1]):\n        for j in range(image_numpy.shape[2]):\n            if dim == \'L2\':\n                result[i][j] = palette_idx[np.argmax(image_numpy[:,i,j])]\n            elif dim == \'pose\':\n                result[i][j] = palette_idx[np.argmax(image_numpy[:,i,j]) + 1]\n    return result.astype(imtype)\n    \ndef ndim_tensor2im2(image_tensor, imtype=np.uint8, dataset = \'PPSS\', dim = \'L2\'):\n    if dataset == \'Horse\' or dataset == \'Cow\' or dataset == \'PPSS\':\n        palette_idx = np.array([[0, 0, 0], [0, 32, 255], [0, 191, 255], [96, 255, 159], [255, 80, 0], [255, 255, 0], [175, 0, 0], [143, 0, 0]]) #PPSS\n# =============================================================================\n#          palette_idx = np.array([[0, 0, 143], [0, 32, 255], [0, 32, 255], [255, 80, 0], [0, 191, 255], [96, 255, 159], [96, 255, 159], [96, 255, 159], [143, 0, 0], [255, 255, 0],\n#               [96, 255, 159], [96, 255, 159], [255, 255, 0], [0, 191, 255], [255, 80, 0], [255, 80, 0], [175, 0, 0], [175, 0, 0], [143, 0, 0], [143, 0, 0]])#LIP\n# =============================================================================\n    if dataset == \'LIP\':\n        palette_idx = np.array([[0, 0, 0], [0, 255, 255], [255, 0, 255], [255, 255, 0], [255, 170, 255], [255, 255, 170], [170, 255, 255], [85, 255, 255], [85, 170, 255], [105, 45, 190],\n              [170, 105, 255], [170, 255, 85], [255, 170, 85], [255, 85, 170],  [85, 255, 85], [0, 255, 85], [255, 0, 85], [255, 85, 85], [0, 85, 255], [85, 85, 255]])#LIP\n    if dataset == \'Pascal\':\n        palette_idx = np.array([[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0], [0, 0, 128], [128, 0, 128], [0, 128, 128], [85, 255, 255], [85, 170, 255], [105, 45, 190],\n              [170, 105, 255], [170, 255, 85], [255, 170, 85], [255, 85, 170],  [85, 255, 85], [0, 255, 85], [255, 0, 85], [255, 85, 85], [0, 85, 255], [85, 85, 255]])#Pascal\n    \n    result = np.zeros(shape = (image_tensor.size(2), image_tensor.size(3), 3))\n    image_numpy = image_tensor[0].cpu().float().numpy()\n    #image_numpy = image_tensor[0].data.numpy()\n    for i in range(image_numpy.shape[1]):\n        for j in range(image_numpy.shape[2]):\n            if dim == \'L2\':\n                result[i][j] = palette_idx[np.argmax(image_numpy[:,i,j])]\n            elif dim == \'pose\':\n                result[i][j] = palette_idx[np.argmax(image_numpy[:,i,j]) + 1]\n    return result.astype(imtype)\n\ndef onedim_tensor2im(image_tensor, imtype=np.uint8, dataset = \'PPSS\'):\n    if dataset == \'PPSS\':\n        palette_idx = np.array([[0, 0, 143], [0, 32, 255], [0, 191, 255], [96, 255, 159], [255, 80, 0], [255, 255, 0], [175, 0, 0], [143, 0, 0]]) #PPSS\n    if dataset == \'LIP\':\n        palette_idx = np.array([[0, 0, 0], [0, 255, 255], [255, 0, 255], [255, 255, 0], [255, 170, 255], [255, 255, 170], [170, 255, 255], [85, 255, 255], [85, 170, 255], [105, 45, 190],\n              [170, 105, 255], [170, 255, 85], [255, 170, 85], [255, 85, 170],  [85, 255, 85], [0, 255, 85], [255, 0, 85], [255, 85, 85], [0, 85, 255], [85, 85, 255]])#LIP\n    if dataset == \'Pascal\':\n        palette_idx = np.array([[0, 0, 0], [255, 0, 0], [155, 100, 0], [128, 128, 0], [0, 128, 128], [0, 100, 155], [0, 0, 255]])#Pascal\n    result = np.zeros(shape = (image_tensor.size(1), image_tensor.size(2), 3))\n    #image_numpy = image_tensor[0].cpu().float().numpy()\n    for i in range(image_tensor.size(1)):\n        for j in range(image_tensor.size(2)):\n            #result[i][j] = palette_idx[np.argmax(image_numpy[:,i,j]) + 1]\n            if image_tensor.data[0][i][j] > 0.8:\n                result[i][j] = palette_idx[1]\n            elif 0.65 < image_tensor.data[0][i][j] < 0.8:\n                result[i][j] = palette_idx[2]\n            elif 0.5 < image_tensor.data[0][i][j] < 0.65:\n                result[i][j] = palette_idx[3]\n            elif 0.35 < image_tensor.data[0][i][j] < 0.5:\n                result[i][j] = palette_idx[4]\n            elif 0.2 < image_tensor.data[0][i][j] < 0.35:\n                result[i][j] = palette_idx[5]\n            else:\n                result[i][j] = palette_idx[6]\n    return result.astype(imtype)\n\ndef pose_tensor2im(image_tensor, imtype=np.uint8, dataset = \'Pascal\'):\n    \n    result = np.zeros(shape = (image_tensor.size(2), image_tensor.size(3), 3))\n    image_numpy = image_tensor[0].cpu().float().numpy()\n    image_numpy = ([0.5,] * image_numpy + [0.5,]) * 255\n    for i in range(image_numpy.shape[1]):\n        for j in range(image_numpy.shape[2]):\n            result[i][j] = [image_numpy[0][i][j], image_numpy[0][i][j], image_numpy[0][i][j]]\n    \n    return result.astype(imtype)\n\ndef diagnose_network(net, name=\'network\'):\n    mean = 0.0\n    count = 0\n    for param in net.parameters():\n        if param.grad is not None:\n            mean += torch.mean(torch.abs(param.grad.data))\n            count += 1\n    if count > 0:\n        mean = mean / count\n    print(name)\n    print(mean)\n\n\ndef save_image(image_numpy, image_path):\n    image_pil = Image.fromarray(image_numpy)\n    image_pil.save(image_path)\n\ndef info(object, spacing=10, collapse=1):\n    """"""Print methods and doc strings.\n    Takes module, class, list, dictionary, or string.""""""\n    methodList = [e for e in dir(object) if isinstance(getattr(object, e), collections.Callable)]\n    processFunc = collapse and (lambda s: "" "".join(s.split())) or (lambda s: s)\n    print( ""\\n"".join([""%s %s"" %\n                     (method.ljust(spacing),\n                      processFunc(str(getattr(object, method).__doc__)))\n                     for method in methodList]) )\n\ndef varname(p):\n    for line in inspect.getframeinfo(inspect.currentframe().f_back)[3]:\n        m = re.search(r\'\\bvarname\\s*\\(\\s*([A-Za-z_][A-Za-z0-9_]*)\\s*\\)\', line)\n        if m:\n            return m.group(1)\n\ndef print_numpy(x, val=True, shp=False):\n    x = x.astype(np.float64)\n    if shp:\n        print(\'shape,\', x.shape)\n    if val:\n        x = x.flatten()\n        print(\'mean = %3.3f, min = %3.3f, max = %3.3f, median = %3.3f, std=%3.3f\' % (\n            np.mean(x), np.min(x), np.max(x), np.median(x), np.std(x)))\n\n\ndef mkdirs(paths):\n    if isinstance(paths, list) and not isinstance(paths, str):\n        for path in paths:\n            mkdir(path)\n    else:\n        mkdir(paths)\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n'"
util/visualizer.py,0,"b'import numpy as np\nimport os\nimport ntpath\nimport time\nfrom . import util\nfrom . import html\n\n\nclass Visualizer():\n    def __init__(self, opt):\n        # self.opt = opt\n        self.display_id = opt.display_id\n        self.use_html = opt.isTrain and not opt.no_html\n        self.win_size = opt.display_winsize\n        self.name = opt.name\n        self.opt = opt\n        self.saved = False\n        if self.display_id > 0:\n            import visdom\n            self.vis = visdom.Visdom(port=opt.display_port)\n\n        if self.use_html:\n            self.web_dir = os.path.join(opt.checkpoints_dir, opt.name, \'web\')\n            self.img_dir = os.path.join(self.web_dir, \'images\')\n            print(\'create web directory %s...\' % self.web_dir)\n            util.mkdirs([self.web_dir, self.img_dir])\n        self.log_name = os.path.join(opt.checkpoints_dir, opt.name, \'loss_log.txt\')\n        with open(self.log_name, ""a"") as log_file:\n            now = time.strftime(""%c"")\n            log_file.write(\'================ Training Loss (%s) ================\\n\' % now)\n\n    def reset(self):\n        self.saved = False\n\n    # |visuals|: dictionary of images to display or save\n    def display_current_results(self, visuals, epoch, save_result):\n        if self.display_id > 0:  # show images in the browser\n            ncols = self.opt.display_single_pane_ncols\n            if ncols > 0:\n                h, w = next(iter(visuals.values())).shape[:2]\n                table_css = """"""<style>\n                        table {border-collapse: separate; border-spacing:4px; white-space:nowrap; text-align:center}\n                        table td {width: %dpx; height: %dpx; padding: 4px; outline: 4px solid black}\n                        </style>"""""" % (w, h)\n                title = self.name\n                label_html = \'\'\n                label_html_row = \'\'\n                nrows = int(np.ceil(len(visuals.items()) / ncols))\n                images = []\n                idx = 0\n                for label, image_numpy in visuals.items():\n                    label_html_row += \'<td>%s</td>\' % label\n                    images.append(image_numpy.transpose([2, 0, 1]))\n                    idx += 1\n                    if idx % ncols == 0:\n                        label_html += \'<tr>%s</tr>\' % label_html_row\n                        label_html_row = \'\'\n                white_image = np.ones_like(image_numpy.transpose([2, 0, 1]))*255\n                while idx % ncols != 0:\n                    images.append(white_image)\n                    label_html_row += \'<td></td>\'\n                    idx += 1\n                if label_html_row != \'\':\n                    label_html += \'<tr>%s</tr>\' % label_html_row\n                # pane col = image row\n                self.vis.images(images, nrow=ncols, win=self.display_id + 1,\n                                padding=2, opts=dict(title=title + \' images\'))\n                label_html = \'<table>%s</table>\' % label_html\n                self.vis.text(table_css + label_html, win=self.display_id + 2,\n                              opts=dict(title=title + \' labels\'))\n            else:\n                idx = 1\n                for label, image_numpy in visuals.items():\n                    self.vis.image(image_numpy.transpose([2, 0, 1]), opts=dict(title=label),\n                                   win=self.display_id + idx)\n                    idx += 1\n\n        if self.use_html and (save_result or not self.saved):  # save images to a html file\n            self.saved = True\n            for label, image_numpy in visuals.items():\n                img_path = os.path.join(self.img_dir, \'epoch%.3d_%s.png\' % (epoch, label))\n                util.save_image(image_numpy, img_path)\n            # update website\n            webpage = html.HTML(self.web_dir, \'Experiment name = %s\' % self.name, reflesh=1)\n            for n in range(epoch, 0, -1):\n                webpage.add_header(\'epoch [%d]\' % n)\n                ims = []\n                txts = []\n                links = []\n\n                for label, image_numpy in visuals.items():\n                    img_path = \'epoch%.3d_%s.png\' % (n, label)\n                    ims.append(img_path)\n                    txts.append(label)\n                    links.append(img_path)\n                webpage.add_images(ims, txts, links, width=self.win_size)\n            webpage.save()\n\n    # errors: dictionary of error labels and values\n    def plot_current_errors(self, epoch, counter_ratio, opt, errors):\n        if not hasattr(self, \'plot_data\'):\n            self.plot_data = {\'X\': [], \'Y\': [], \'legend\': list(errors.keys())}\n        self.plot_data[\'X\'].append(epoch + counter_ratio)\n        self.plot_data[\'Y\'].append([errors[k] for k in self.plot_data[\'legend\']])\n        self.vis.line(\n            X=np.stack([np.array(self.plot_data[\'X\'])] * len(self.plot_data[\'legend\']), 1),\n            Y=np.array(self.plot_data[\'Y\']),\n            opts={\n                \'title\': self.name + \' loss over time\',\n                \'legend\': self.plot_data[\'legend\'],\n                \'xlabel\': \'epoch\',\n                \'ylabel\': \'loss\'},\n            win=self.display_id)\n\n    # errors: same format as |errors| of plotCurrentErrors\n    def print_current_errors(self, epoch, i, errors, t):\n        message = \'(epoch: %d, iters: %d, time: %.3f) \' % (epoch, i, t)\n        for k, v in errors.items():\n            message += \'%s: %.3f \' % (k, v)\n\n        print(message)\n        with open(self.log_name, ""a"") as log_file:\n            log_file.write(\'%s\\n\' % message)\n\n    # save image to the disk\n    def save_images(self, webpage, visuals, image_path):\n        image_dir = webpage.get_image_dir()\n        short_path = ntpath.basename(image_path[0])\n        name = os.path.splitext(short_path)[0]\n\n        webpage.add_header(name)\n        ims = []\n        txts = []\n        links = []\n\n        for label, image_numpy in visuals.items():\n            image_name = \'%s_%s.png\' % (name, label)\n            save_path = os.path.join(image_dir, image_name)\n            util.save_image(image_numpy, save_path)\n\n            ims.append(image_name)\n            txts.append(label)\n            links.append(image_name)\n        webpage.add_images(ims, txts, links, width=self.win_size)\n'"
