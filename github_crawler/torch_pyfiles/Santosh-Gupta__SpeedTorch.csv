file_path,api_count,code
__init__.py,0,b'\n'
setup.py,0,"b'import codecs\nfrom setuptools import setup, find_packages\n\nwith codecs.open(\'README.md\', \'r\', \'utf8\') as reader:\n    long_description = reader.read()\n\nwith codecs.open(\'requirements.txt\', \'r\', \'utf8\') as reader:\n    install_requires = list(map(lambda x: x.strip(), reader.readlines()))\n    \ntry:\n    import cupy\nexcept:\n    print(\'CuPy is not available. Please install it manually: https://docs-cupy.chainer.org/en/stable/install.html#install-cupy\')\n#     print(\'Cupy not installed. Package will install cupy, which will take several minutes\')\n#     print(\'If you would like to install Cupy yourself, check here https://docs-cupy.chainer.org/en/stable/install.html#install-cupy\')\n#     install_requires.append(\'cupy\')\n\nsetup(\n    name=\'SpeedTorch\',\n    version=\'0.1.6\',\n    packages=find_packages(),\n    url=\'https://github.com/Santosh-Gupta/SpeedTorch\',\n    license=\'MIT\',\n    author=\'Santosh Gupta\',\n    author_email=\'SanGupta.ML@gmail.com\',\n    description=\'Fast Pinned CPU -> GPU transfer\',\n    long_description=\'Fast Pinned CPU -> GPU transfer\',\n    long_description_content_taype=\'text/markdown\',\n    python_requires=\'>=3.5.0\',\n    install_requires=install_requires,\n    classifiers=(\n        ""Programming Language :: Python :: 2.7"",\n        ""Programming Language :: Python :: 3.6"",\n        ""License :: OSI Approved :: MIT License"",\n        ""Operating System :: OS Independent"",\n    ),\n)\n'"
SpeedTorch/CPUCupyPinned.py,19,"b""import numpy as np\nimport cupy\n# cupy.cuda.set_allocator(cupy.cuda.MemoryPool(cupy.cuda.memory.malloc_managed).malloc)\nimport torch\nimport os\n\nfrom torch.utils.dlpack import to_dlpack\nfrom torch.utils.dlpack import from_dlpack\n\nclass PMemoryMM(cupy.cuda.memory.BaseMemory):\n    def __init__(self, size):\n        self.size = size\n        self.device_id = cupy.cuda.device.get_device_id()\n        self.ptr = 0\n        if size > 0:\n            self.ptr = cupy.cuda.runtime.hostAlloc(size, 0)\n    def __del__(self):\n        if self.ptr:\n            cupy.cuda.runtime.freeHost(self.ptr)\n\ndef my_pinned_allocatorMM(bsize):\n    return cupy.cuda.memory.MemoryPointer(PMemory(bsize),0)\n\nclass _CommonMM():\n    def _preInit(self):\n        fileNumber = 0\n        while os.path.isfile( self.diskname + str(fileNumber) + '.memmap.cpy.npy'  ) == True:\n            fileNumber = fileNumber + 1\n        else:\n            self.fileName = self.diskname + str(fileNumber) + '.memmap' \n        numpyMemmap = np.memmap( self.fileName, dtype='float32', mode='w+', shape=(self.total_classes ,self.embed_dimension ))\n        np.save( self.fileName + '.cpy' , numpyMemmap, allow_pickle=True)\n        del numpyMemmap\n        os.remove(self.fileName)\n\n    def getCupyMM(self):\n        return self.CUPYmemmap\n\n    def saveCupy(self, saveFileName):\n        cupy.save( saveFileName, self.CUPYmemmap)\n\n    def getNumpyVersion(self):\n        return cupy.asnumpy(self.CUPYmemmap)\n\n    def _getReshapedRetrieval( self, retrievedPosIndexes , retrievedNegIndexes = None):\n        if not retrievedNegIndexes is None:\n            reshapedRetrieval =  np.concatenate( [ retrievedPosIndexes.reshape(-1) , retrievedNegIndexes.reshape(-1) ] )\n        else:\n            reshapedRetrieval = retrievedPosIndexes.reshape(-1)\n        return reshapedRetrieval\n\nclass ModelFactoryMM(_CommonMM):\n\n    def __init__(self, model_variable,  total_classes,  embed_dimension, diskname = 'variable', datatype = 'float32', CPUPinn = False):\n        self.model_variable = model_variable\n        self.total_classes = total_classes\n        self.embed_dimension = embed_dimension\n        self.diskname = diskname\n        self.datatype = datatype\n        self.CPUPinn = CPUPinn\n\n    def zerosInit(self ):\n        #Initialize the memmap with just zeros\n        if self.CPUPinn == True:\n            cupy.cuda.set_allocator(my_pinned_allocator)\n        self._preInit()\n        self.CUPYmemmap = cupy.load( self.fileName+'.cpy.npy' , mmap_mode = 'r+' )\n        if self.CPUPinn == True:\n            cupy.cuda.set_allocator(None)\n\n    def uniformDistributionInit(self, low, high):\n        #Initialize the memmap with a uniform distribution \n        if self.CPUPinn == True:\n            cupy.cuda.set_allocator(my_pinned_allocator)\n\n        self._preInit()\n        self.CUPYmemmap = cupy.load( self.fileName+'.cpy.npy' , mmap_mode = 'r+' )\n\n        if self.total_classes  > 100000:    \n            for i in range( int( self.total_classes/100000) ):\n                j=i*100000   \n                self.CUPYmemmap[j:j+100000] = cupy.random.uniform(low=low, high=high, size=(100000, self.embed_dimension), dtype=self.datatype)\n            \n            for i in range( int( self.total_classes/100000)*100000,  int( self.total_classes/10000) ):\n                j=i*10000   \n                self.CUPYmemmap[j:j+10000] = cupy.random.uniform(low=low, high=high, size=(10000, self.embed_dimension), dtype=self.datatype)\n            \n            for i in range( int( self.total_classes /10000)*10000 , self.total_classes ):\n                self.CUPYmemmap[i] = cupy.random.uniform(low=low, high=high, size=(self.embed_dimension), dtype=self.datatype)\n        \n        elif self.total_classes  > 10000:    \n            for i in range( int(self.total_classes/10000) ):\n                j=i*10000\n                self.CUPYmemmap[j:j+10000] = cupy.random.uniform(low=low, high=high, size=(10000, self.embed_dimension), dtype=self.datatype)\n            \n            for i in range( int( self.total_classes/10000)*10000 , self.total_classes ):\n                self.CUPYmemmap[i] = cupy.random.uniform(low=low, high=high, size=(self.embed_dimension), dtype=self.datatype)\n\n        else:\n            for i in range( self.total_classes  ):\n                self.CUPYmemmap[i] = cupy.random.uniform(low=low, high=high, size=(self.embed_dimension), dtype=self.datatype)\n\n        if self.CPUPinn == True:\n            cupy.cuda.set_allocator(None)\n                \n    def normalDistributionInit(self, mean, stdDev):\n        #Initialize the memmap with a normal distribution \n        if self.CPUPinn == True:\n            cupy.cuda.set_allocator(my_pinned_allocator)\n\n        self._preInit()\n        self.CUPYmemmap = cupy.load( self.fileName+'.cpy.npy' , mmap_mode = 'r+' )\n\n        if self.total_classes > 100000: \n            for i in range( int(self.total_classes/100000) ):\n                j=i*100000\n                self.CUPYmemmap[j:j+100000] = cupy.random.normal(loc=mean, scale=stdDev, size=(100000, self.embed_dimension), dtype=self.datatype )\n                \n            for i in range( int(self.total_classes/100000)*100000, int(self.total_classes/10000) ):\n                j=i*10000\n                self.CUPYmemmap[j:j+10000] = cupy.random.normal(loc=mean, scale=stdDev, size=(10000, self.embed_dimension), dtype=self.datatype )\n\n            for i in range( int(self.total_classes/10000)*10000 , self.total_classes ):\n                self.CUPYmemmap[i] = cupy.random.normal(loc=mean, scale=stdDev, size=(self.embed_dimension), dtype=self.datatype )\n\n        elif self.total_classes > 10000:\n            for i in range( int(self.total_classes/10000) ):\n                j=i*10000\n                self.CUPYmemmap[j:j+10000] = cupy.random.normal(loc=mean, scale=stdDev, size=(10000, self.embed_dimension), dtype=self.datatype )\n\n            for i in range( int(self.total_classes/10000)*10000 , self.total_classes ):\n                self.CUPYmemmap[i] = cupy.random.normal(loc=mean, scale=stdDev, size=(self.embed_dimension), dtype=self.datatype )\n\n        else:\n            for i in range( self.total_classes ):\n                self.CUPYmemmap[i] = cupy.random.normal(loc=mean, scale=stdDev, size=(self.embed_dimension), dtype=self.datatype )\n\n        if self.CPUPinn == True:\n            cupy.cuda.set_allocator(None)\n                \n    def variableTransformer(self, batchSize, posPerBatch,  negPerBatch = None ):\n        if not negPerBatch == None:\n            return (np.arange( batchSize*posPerBatch ).reshape( batchSize , posPerBatch), \n                np.arange(start = batchSize*posPerBatch, \n                    stop = batchSize*posPerBatch + batchSize*negPerBatch ).reshape(batchSize, negPerBatch) )\n        else:\n            return np.arange( batchSize*posPerBatch ).reshape( batchSize, posPerBatch )\n\n    def beforeForwardPass(self, retrievedPosIndexes , retrievedNegIndexes = None):\n        reshapedRetrieval = self._getReshapedRetrieval( retrievedPosIndexes, retrievedNegIndexes )\n\n        self.model_variable.weight.data = (\n            from_dlpack(self.CUPYmemmap[ reshapedRetrieval ].toDlpack() ) )\n\n    def afterOptimizerStep(self,retrievedPosIndexes , retrievedNegIndexes = None):\n        reshapedRetrieval = self._getReshapedRetrieval( retrievedPosIndexes, retrievedNegIndexes )\n\n        self.CUPYmemmap[ reshapedRetrieval ] = (\n            cupy.fromDlpack( to_dlpack( self.model_variable.weight.data ) ) )\n        \n    \nclass OptimizerFactoryMM(_CommonMM): #to do later, able to load matrixes to continue training\n#take into account different size embedding matrices \n\n    def __init__(self, given_optimizer,  total_classes,  embed_dimension, model, variable_name, dtype='float32' , CPUPinn = False):\n        self.given_optimizer = given_optimizer\n        self.total_classes = total_classes\n        self.embed_dimension = embed_dimension\n        self.model = model\n        self.variable_name = variable_name\n        self.dtype = dtype\n        optimizer_index = None\n        self.CPUPinn = CPUPinn\n\n        #Some optiizers do not initialize its state until after first step\n        #So they need to initialized here\n        for group in given_optimizer.param_groups:\n            for p in group['params']:\n                state = given_optimizer.state[p]\n                # State initialization\n\n                if given_optimizer.__str__().split(' ', 1)[0] == 'SparseAdam':\n                    # State initialization\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['exp_avg'] = torch.zeros_like(p.data)\n                        state['exp_avg_sq'] = torch.zeros_like(p.data)\n                    self.optVarList = [ 'exp_avg', 'exp_avg_sq']\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'Adagrad':\n                    self.optVarList = [ 'sum' ]\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'Adadelta':\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['square_avg'] = torch.zeros_like(p.data)\n                        state['acc_delta'] = torch.zeros_like(p.data)\n                    self.optVarList = [ 'square_avg', 'acc_delta']\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'Adamax':\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['exp_avg'] = torch.zeros_like(p.data)\n                        state['exp_inf'] = torch.zeros_like(p.data)\n                    self.optVarList = [ 'exp_avg', 'exp_inf']\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'RMSprop':\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['square_avg'] = torch.zeros_like(p.data)\n                        if group['momentum'] > 0:\n                            state['momentum_buffer'] = torch.zeros_like(p.data)\n                        if group['centered']:\n                            state['grad_avg'] = torch.zeros_like(p.data)\n                    self.optVarList = [ 'square_avg']\n                    if group['momentum'] > 0:\n                         self.optVarList.append( 'momentum_buffer' )\n                    if group['centered']:\n                        self.optVarList.append( 'grad_avg' )\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'Rprop':\n                    if p.grad is None:\n                        print('Error, gradients are empty')\n                        print('For Rprop, need to first run at least 1 training step that has gradients')\n                        return\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['prev'] = torch.zeros_like(p.data)\n                        #For now, do now know how to Not initialize this due to len(state)==0 in optimizer\n                        state['step_size'] = grad.new().resize_as_(grad).fill_(group['lr'])\n                    self.optVarList = [ 'prev']\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'ASGD': \n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['eta'] = group['lr']\n                        state['mu'] = 1\n                        state['ax'] = torch.zeros_like(p.data)\n                        self.optVarList = [ 'ax']\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'AdamW': \n                    amsgrad = group['amsgrad']\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['exp_avg'] = torch.zeros_like(p.data)\n                        state['exp_avg_sq'] = torch.zeros_like(p.data)\n                        if amsgrad:\n                            state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n                    self.optVarList = [ 'exp_avg', 'exp_avg_sq']\n                    if amsgrad:\n                        self.optVarList.append('max_exp_avg_sq')\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'Adam': \n                    amsgrad = group['amsgrad']\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['exp_avg'] = torch.zeros_like(p.data)\n                        state['exp_avg_sq'] = torch.zeros_like(p.data)\n                        if amsgrad:\n                            state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n                    self.optVarList = [ 'exp_avg', 'exp_avg_sq']\n                    if amsgrad:\n                        self.optVarList.append( 'max_exp_avg_sq' )\n                else:\n                    print('This optimizer is not currently supported. Please choose a different optimizer')\n                    return\n\n        #Figure out which index for given variable \n        for i, item in enumerate( self.model.named_parameters() ):\n            if item[0][:-7] == self.variable_name:\n                optimizer_index = i\n                self.diskname = item[0][:-7] + given_optimizer.__str__().split(' ', 1)[0]\n        if optimizer_index == None:\n            print( 'Error: No variable with that name is in Model. Please initialize again with correct name' ) \n            return\n\n        optimizerKeyList = list(self.given_optimizer.state_dict()['state'].keys())\n        self.optimizerKey = optimizerKeyList[ optimizer_index ]\n\n    def _preInit(self):\n        for optVar in self.optVarList:\n            fileNumber = 0\n            while os.path.isfile( self.diskname + str(fileNumber) + '.memmap.cpy.npy'  ) == True:\n                fileNumber = fileNumber + 1\n            else:\n                self.fileName = self.diskname + str(fileNumber) + '.memmap' \n            numpyMemmap = np.memmap( self.fileName+optVar, dtype='float32', mode='w+', shape=(self.total_classes ,self.embed_dimension ))\n            np.save( self.fileName + optVar + '.cpy' , numpyMemmap, allow_pickle=True)\n            del numpyMemmap\n            os.remove(self.fileName+optVar)\n\n    def optInit(self):\n        if self.CPUPinn == True:\n            cupy.cuda.set_allocator(my_pinned_allocator)\n\n        self._preInit()\n        self.CUPYmemmap = []\n        for optVar in self.optVarList:\n            self.CUPYmemmap.append( cupy.load( self.fileName+optVar+'.cpy.npy' , mmap_mode = 'r+' )  )\n\n        if self.CPUPinn == True:\n            cupy.cuda.set_allocator(None)\n            \n    def beforeForwardPass(self, retrievedPosIndexes , retrievedNegIndexes = None):\n        reshapedRetrieval = self._getReshapedRetrieval( retrievedPosIndexes, retrievedNegIndexes )\n\n        for idx, optVar in enumerate(self.optVarList):\n            self.given_optimizer.state_dict()['state'][ self.optimizerKey ][optVar] = (\n                from_dlpack( self.CUPYmemmap[idx][ reshapedRetrieval ].toDlpack() )   )\n\n    def afterOptimizerStep(self, retrievedPosIndexes , retrievedNegIndexes = None):\n        reshapedRetrieval = self._getReshapedRetrieval( retrievedPosIndexes, retrievedNegIndexes )\n\n        for idx, optVar in enumerate(self.optVarList):\n            self.CUPYmemmap[idx][ reshapedRetrieval ] = (\n                cupy.fromDlpack( to_dlpack( self.given_optimizer.state_dict()['state'][ self.optimizerKey ][optVar] ) )  )\n    \nclass COMMM(_CommonMM):\n\n    def __init__(self, total_classes, diskname = 'COM', datatype = 'uint32', CPUPinn = False  ):\n        self.total_classes = total_classes\n        self.datatype = datatype\n        self.diskname = diskname\n        self.CPUPinn = CPUPinn\n\n    def _preInit(self): #Can't depend on inherited since the shape is different \n\n        fileNumber = 0\n        while os.path.isfile(diskself.disknamename+str(fileNumber) + 'memmap' ) == false:\n            fileNumber = fileNumber + 1\n        else:\n            self.fileName = self.diskname+str(fileNumber)\n        numpyMemmap = np.memmap(self.fileName, dtype=datatype, mode='w+', shape=(total_classes , total_classes ))\n        np.save( self.fileName+'.cpy' , numpyMemmap, allow_pickle=True)\n        del numpyMemmap\n        os.remove(self.fileName)\n\n    def comInit(self, CPUPinn=False):\n        if self.CPUPinn == True:\n            cupy.cuda.set_allocator(my_pinned_allocator)\n\n        self._preInit()\n        self.CUPYmemmap = cupy.load( fileName+'.cpy.npy'  , mmap_mode = 'r+' )\n\n        if self.CPUPinn == True:\n            cupy.cuda.set_allocator(None)\n        \nclass DataGadgetMM(_CommonMM):\n    def __init__(self, fileName, CPUPinn=False):\n        self.Numpyfilename = Numpyfilename\n        self.CPUPinn = CPUPinn\n\n    def gadgetInit(self):\n        if self.CPUPinn == True:\n            cupy.cuda.set_allocator(my_pinned_allocator)\n\n        self.CUPYmemmap = cupy.load( self.fileName , mmap_mode = 'r+' )\n\n        if self.CPUPinn == True:\n            cupy.cuda.set_allocator(None)\n\n    def getData(self, retrievedPosIndexes , retrievedNegIndexes = None):\n        return from_dlpack( self.CUPYmemmap[ reshapedRetrieval ].toDlpack() )\n"""
SpeedTorch/CPUTorchPinned.py,29,"b""import torch\nimport os\nimport numpy as np\n\nclass _CPUPytorchCommon():\n    def getTorchVar(self):\n        return self.TorchFSet\n\n    def saveTorch(self, saveFileName):\n        torch.save(self.pytorchCPUVar, saveFileName)\n\n    def getNumpyVersion(self):\n        return self.pytorchCPUVar\n\n    def _getReshapedRetrieval( self, retrievedPosIndexes , retrievedNegIndexes = None):\n        if not retrievedNegIndexes is None:\n            reshapedRetrieval =  np.concatenate( [ retrievedPosIndexes.reshape(-1) , retrievedNegIndexes.reshape(-1) ] )\n        else:\n            reshapedRetrieval = retrievedPosIndexes.reshape(-1)\n        return reshapedRetrieval\n\nclass CPUPytorchModelFactory(_CPUPytorchCommon):\n\n    def __init__(self, model_variable,  total_classes,  embed_dimension, datatype = torch.float ):\n        self.model_variable = model_variable\n        self.total_classes = total_classes\n        self.embed_dimension = embed_dimension\n        self.dtype = datatype\n\n    def zerosInit(self):\n        self.pytorchCPUVar = torch.zeros( size=(self.total_classes, self.embed_dimension), dtype=self.dtype, device = 'cpu', pin_memory = True)\n\n    def customInit(self, initFunction, *args):\n        self.pytorchCPUVar = torch.empty(size=(self.total_classes, self.embed_dimension), dtype=self.dtype, device='cpu' , pin_memory = True )\n        initFunction( self.pytorchCPUVar, *args )\n\n    def uniformDistributionInit(self, low, high):\n        self.pytorchCPUVar = torch.empty(size=(self.total_classes, self.embed_dimension), dtype=self.dtype, device='cpu' ,  pin_memory = True)\n        torch.nn.init.uniform_(self.pytorchCPUVar, a=low, b=high)\n                \n    def normalDistributionInit(self, mean, stdDev ):\n        self.pytorchCPUVar = torch.empty(size=(self.total_classes, self.embed_dimension), dtype=self.dtype, device='cpu' ,  pin_memory = True)\n        torch.nn.init.normal_(self.pytorchCPUVar, mean=mean, std=stdDev)\n                \n    def variableTransformer(self, batchSize, posPerBatch,  negPerBatch = None ):\n        if not negPerBatch == None:\n            return (np.arange( batchSize*posPerBatch ).reshape( batchSize , posPerBatch), \n                np.arange(start = batchSize*posPerBatch, \n                    stop = batchSize*posPerBatch + batchSize*negPerBatch ).reshape(batchSize, negPerBatch) )\n        else:\n            return np.arange( batchSize*posPerBatch ).reshape( batchSize, posPerBatch )\n\n    def beforeForwardPass(self, retrievedPosIndexes , retrievedNegIndexes = None):\n        reshapedRetrieval = self._getReshapedRetrieval( retrievedPosIndexes, retrievedNegIndexes )\n\n        self.model_variable.weight.data = self.pytorchCPUVar[ reshapedRetrieval ].cuda()\n\n    def afterOptimizerStep(self,retrievedPosIndexes , retrievedNegIndexes = None):\n        reshapedRetrieval = self._getReshapedRetrieval( retrievedPosIndexes, retrievedNegIndexes )\n\n        self.pytorchCPUVar[ reshapedRetrieval ] = self.model_variable.weight.data.detach().cpu().pin_memory() \n        \nclass CPUPytorchOptimizerFactory(_CPUPytorchCommon): #to do later, able to load matrixes to continue training\n#take into account different size embedding matrices \n\n    def __init__(self, given_optimizer,  total_classes,  embed_dimension, model, variable_name, dtype=torch.float ):\n        self.given_optimizer = given_optimizer\n        self.total_classes = total_classes\n        self.embed_dimension = embed_dimension\n        self.model = model\n        self.variable_name = variable_name\n        self.dtype = dtype\n        optimizer_index = None\n\n        #Some optiizers do not initialize its state until after first step\n        #So they need to initialized here\n        for group in given_optimizer.param_groups:\n            for p in group['params']:\n                state = given_optimizer.state[p]\n                # State initialization\n\n                if given_optimizer.__str__().split(' ', 1)[0] == 'SparseAdam':\n                    # State initialization\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['exp_avg'] = torch.zeros_like(p.data)\n                        state['exp_avg_sq'] = torch.zeros_like(p.data)\n                    self.optVarList = [ 'exp_avg', 'exp_avg_sq']\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'Adagrad':\n                    self.optVarList = [ 'sum' ]\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'Adadelta':\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['square_avg'] = torch.zeros_like(p.data)\n                        state['acc_delta'] = torch.zeros_like(p.data)\n                    self.optVarList = [ 'square_avg', 'acc_delta']\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'Adamax':\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['exp_avg'] = torch.zeros_like(p.data)\n                        state['exp_inf'] = torch.zeros_like(p.data)\n                    self.optVarList = [ 'exp_avg', 'exp_inf']\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'RMSprop':\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['square_avg'] = torch.zeros_like(p.data)\n                        if group['momentum'] > 0:\n                            state['momentum_buffer'] = torch.zeros_like(p.data)\n                        if group['centered']:\n                            state['grad_avg'] = torch.zeros_like(p.data)\n                    self.optVarList = [ 'square_avg']\n                    if group['momentum'] > 0:\n                         self.optVarList.append( 'momentum_buffer' )\n                    if group['centered']:\n                        self.optVarList.append( 'grad_avg' )\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'Rprop':\n                    if p.grad is None:\n                        print('Error, gradients are empty')\n                        print('For Rprop, need to first run at least 1 training step that has gradients')\n                        return\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['prev'] = torch.zeros_like(p.data)\n                        #For now, do now know how to Not initialize this due to len(state)==0 in optimizer\n                        state['step_size'] = grad.new().resize_as_(grad).fill_(group['lr'])\n                    self.optVarList = [ 'prev']\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'ASGD': \n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['eta'] = group['lr']\n                        state['mu'] = 1\n                        state['ax'] = torch.zeros_like(p.data)\n                        self.optVarList = [ 'ax']\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'AdamW': \n                    amsgrad = group['amsgrad']\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['exp_avg'] = torch.zeros_like(p.data)\n                        state['exp_avg_sq'] = torch.zeros_like(p.data)\n                        if amsgrad:\n                            state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n                    self.optVarList = [ 'exp_avg', 'exp_avg_sq']\n                    if amsgrad:\n                        self.optVarList.append('max_exp_avg_sq')\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'Adam': \n                    amsgrad = group['amsgrad']\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['exp_avg'] = torch.zeros_like(p.data)\n                        state['exp_avg_sq'] = torch.zeros_like(p.data)\n                        if amsgrad:\n                            state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n                    self.optVarList = [ 'exp_avg', 'exp_avg_sq']\n                    if amsgrad:\n                        self.optVarList.append( 'max_exp_avg_sq' )\n                else:\n                    print('This optimizer is not currently supported. Please choose a different optimizer')\n                    return\n\n        #Figure out which index for given variable \n        for i, item in enumerate( self.model.named_parameters() ):\n            if item[0][:-7] == self.variable_name:\n                optimizer_index = i\n        if optimizer_index == None:\n            print( 'Error: No variable with that name is in Model. Please initialize again with correct name' ) \n            return\n\n        optimizerKeyList = list(self.given_optimizer.state_dict()['state'].keys())\n        self.optimizerKey = optimizerKeyList[ optimizer_index ]\n\n    def optInit(self):\n        self.pytorchCPUVar = []\n        for optVar in self.optVarList:\n            self.pytorchCPUVar.append( torch.zeros( size=(self.total_classes, self.embed_dimension), dtype=self.dtype, device = 'cpu' , pin_memory = True) )\n            \n    def beforeForwardPass(self, retrievedPosIndexes , retrievedNegIndexes = None):\n        reshapedRetrieval = self._getReshapedRetrieval( retrievedPosIndexes, retrievedNegIndexes )\n\n        for idx, optVar in enumerate(self.optVarList):\n            self.given_optimizer.state_dict()['state'][ self.optimizerKey ][optVar] = self.pytorchCPUVar[idx][ reshapedRetrieval ].cuda()\n\n    def afterOptimizerStep(self, retrievedPosIndexes , retrievedNegIndexes = None):\n        reshapedRetrieval = self._getReshapedRetrieval( retrievedPosIndexes, retrievedNegIndexes )\n\n        for idx, optVar in enumerate(self.optVarList):\n            self.pytorchCPUVar[idx][ reshapedRetrieval ] =  self.given_optimizer.state_dict()['state'][ self.optimizerKey ][optVar].detach().cpu().pin_memory()  \n            \nclass CPUPytorchCOM(_CPUPytorchCommon):\n\n    def __init__(self, total_classes, datatype = torch.int32 ):\n        self.total_classes = total_classes\n        self.dtype = datatype\n\n    def comInit(self):\n        self.pytorchCPUVar = torch.zeros( size=(self.total_classes, self.embed_dimension), dtype=self.dtype, device = 'cpu' , pin_memory = True )\n\n"""
SpeedTorch/CUPYLive.py,25,"b""\nimport numpy as np\nimport cupy\nimport torch\nimport os\n\nfrom torch.utils.dlpack import to_dlpack\nfrom torch.utils.dlpack import from_dlpack\n\nclass PMemory(cupy.cuda.memory.BaseMemory):\n    def __init__(self, size):\n        self.size = size\n        self.device_id = cupy.cuda.device.get_device_id()\n        self.ptr = 0\n        if size > 0:\n            self.ptr = cupy.cuda.runtime.hostAlloc(size, 0)\n    def __del__(self):\n        if self.ptr:\n            cupy.cuda.runtime.freeHost(self.ptr)\n\ndef my_pinned_allocator(bsize):\n    return cupy.cuda.memory.MemoryPointer(PMemory(bsize),0)\n\nclass _Common():\n\n    def getCupyVar(self):\n        return self.CUPYcorpus\n\n    def saveCupy(self, saveFileName):\n        cupy.save( saveFileName, self.CUPYcorpus)\n        \n    def loadCupy(self, loadFileName):\n        if self.CPUPinn == True:\n            cupy.cuda.set_allocator(my_pinned_allocator)\n        self.CUPYcorpus = cupy.load( loadFileName )\n        if self.CPUPinn == True:\n            cupy.cuda.set_allocator(None)\n\n    def getNumpyVersion(self):\n        return cupy.asnumpy(self.CUPYcorpus)\n\n    def _getReshapedRetrieval( self, retrievedPosIndexes , retrievedNegIndexes = None):\n        if not retrievedNegIndexes is None:\n            reshapedRetrieval =  np.concatenate( [ retrievedPosIndexes.reshape(-1) , retrievedNegIndexes.reshape(-1) ] )\n        else:\n            reshapedRetrieval = retrievedPosIndexes.reshape(-1)\n        return reshapedRetrieval\n\nclass ModelFactory(_Common):\n\n    def __init__(self, model_variable,  total_classes,  embed_dimension, datatype = 'float32', CPUPinn = False):\n        self.model_variable = model_variable\n        self.total_classes = total_classes\n        self.embed_dimension = embed_dimension\n        self.datatype = datatype\n        self.CPUPinn = CPUPinn\n\n    def zerosInit(self):\n        if self.CPUPinn == True:\n            cupy.cuda.set_allocator(my_pinned_allocator)\n        self.CUPYcorpus = cupy.zeros(shape=(self.total_classes, self.embed_dimension), dtype=self.datatype)\n        if self.CPUPinn == True:\n            cupy.cuda.set_allocator(None)\n\n    def uniformDistributionInit(self, low, high):\n        if self.CPUPinn == True:\n            cupy.cuda.set_allocator(my_pinned_allocator)\n        self.CUPYcorpus = cupy.random.uniform(low=low, high=high, size=(self.total_classes, self.embed_dimension), dtype=self.datatype )\n        if self.CPUPinn == True:\n            cupy.cuda.set_allocator(None)\n                \n    def normalDistributionInit(self, mean, stdDev):\n        if self.CPUPinn == True:\n            cupy.cuda.set_allocator(my_pinned_allocator)\n        self.CUPYcorpus = cupy.random.normal(loc=mean, scale=stdDev, size=(self.total_classes, self.embed_dimension), dtype=self.datatype )\n        if self.CPUPinn == True:\n            cupy.cuda.set_allocator(None)\n                \n    def variableTransformer(self, batchSize, posPerBatch,  negPerBatch = None ):\n        if not negPerBatch == None:\n            return (np.arange( batchSize*posPerBatch ).reshape( batchSize , posPerBatch), \n                np.arange(start = batchSize*posPerBatch, \n                    stop = batchSize*posPerBatch + batchSize*negPerBatch ).reshape(batchSize, negPerBatch) )\n        else:\n            return np.arange( batchSize*posPerBatch ).reshape( batchSize, posPerBatch )\n\n    def beforeForwardPass(self, retrievedPosIndexes , retrievedNegIndexes = None):\n        torch.cuda.synchronize()\n        cupy.cuda.Device().synchronize()\n        \n        reshapedRetrieval = self._getReshapedRetrieval( retrievedPosIndexes, retrievedNegIndexes )\n\n        self.model_variable.weight.data = (\n            from_dlpack(self.CUPYcorpus[ reshapedRetrieval ].toDlpack() ) )\n\n    def afterOptimizerStep(self,retrievedPosIndexes , retrievedNegIndexes = None):\n        torch.cuda.synchronize()\n        cupy.cuda.Device().synchronize()\n        \n        reshapedRetrieval = self._getReshapedRetrieval( retrievedPosIndexes, retrievedNegIndexes )\n\n        self.CUPYcorpus[ reshapedRetrieval ] = (\n            cupy.fromDlpack( to_dlpack( self.model_variable.weight.data ) ) )\n        \n    \nclass OptimizerFactory(_Common): #to do later, able to load matrixes to continue training\n#take into account different size embedding matrices \n\n    def __init__(self, given_optimizer,  total_classes,  embed_dimension, model, variable_name, dtype='float32', CPUPinn = False):\n        self.given_optimizer = given_optimizer\n        self.total_classes = total_classes\n        self.embed_dimension = embed_dimension\n        self.model = model\n        self.variable_name = variable_name\n        self.datatype = dtype\n        optimizer_index = None\n        self.CPUPinn = CPUPinn\n\n        #Some optiizers do not initialize its state until after first step\n        #So they need to initialized here\n        for group in given_optimizer.param_groups:\n            for p in group['params']:\n                state = given_optimizer.state[p]\n                # State initialization\n\n                if given_optimizer.__str__().split(' ', 1)[0] == 'SparseAdam':\n                    # State initialization\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['exp_avg'] = torch.zeros_like(p.data)\n                        state['exp_avg_sq'] = torch.zeros_like(p.data)\n                    self.optVarList = [ 'exp_avg', 'exp_avg_sq']\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'Adagrad':\n                    self.optVarList = [ 'sum' ]\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'Adadelta':\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['square_avg'] = torch.zeros_like(p.data)\n                        state['acc_delta'] = torch.zeros_like(p.data)\n                    self.optVarList = [ 'square_avg', 'acc_delta']\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'Adamax':\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['exp_avg'] = torch.zeros_like(p.data)\n                        state['exp_inf'] = torch.zeros_like(p.data)\n                    self.optVarList = [ 'exp_avg', 'exp_inf']\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'RMSprop':\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['square_avg'] = torch.zeros_like(p.data)\n                        if group['momentum'] > 0:\n                            state['momentum_buffer'] = torch.zeros_like(p.data)\n                        if group['centered']:\n                            state['grad_avg'] = torch.zeros_like(p.data)\n                    self.optVarList = [ 'square_avg']\n                    if group['momentum'] > 0:\n                         self.optVarList.append( 'momentum_buffer' )\n                    if group['centered']:\n                        self.optVarList.append( 'grad_avg' )\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'Rprop':\n                    if p.grad is None:\n                        print('Error, gradients are empty')\n                        print('For Rprop, need to first run at least 1 training step that has gradients')\n                        return\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['prev'] = torch.zeros_like(p.data)\n                        #For now, do now know how to Not initialize this due to len(state)==0 in optimizer\n                        state['step_size'] = grad.new().resize_as_(grad).fill_(group['lr'])\n                    self.optVarList = [ 'prev']\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'ASGD': \n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['eta'] = group['lr']\n                        state['mu'] = 1\n                        state['ax'] = torch.zeros_like(p.data)\n                        self.optVarList = [ 'ax']\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'AdamW': \n                    amsgrad = group['amsgrad']\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['exp_avg'] = torch.zeros_like(p.data)\n                        state['exp_avg_sq'] = torch.zeros_like(p.data)\n                        if amsgrad:\n                            state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n                    self.optVarList = [ 'exp_avg', 'exp_avg_sq']\n                    if amsgrad:\n                        self.optVarList.append('max_exp_avg_sq')\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'Adam': \n                    amsgrad = group['amsgrad']\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['exp_avg'] = torch.zeros_like(p.data)\n                        state['exp_avg_sq'] = torch.zeros_like(p.data)\n                        if amsgrad:\n                            state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n                    self.optVarList = [ 'exp_avg', 'exp_avg_sq']\n                    if amsgrad:\n                        self.optVarList.append( 'max_exp_avg_sq' )\n                else:\n                    print('This optimizer is not currently supported. Please choose a different optimizer')\n                    return\n\n        #Figure out which index for given variable \n        for i, item in enumerate( self.model.named_parameters() ):\n            if item[0][:-7] == self.variable_name:\n                optimizer_index = i\n        if optimizer_index == None:\n            print( 'Error: No variable with that name is in Model. Please initialize again with correct name' ) \n            return\n\n        optimizerKeyList = list(self.given_optimizer.state_dict()['state'].keys())\n        self.optimizerKey = optimizerKeyList[ optimizer_index ]\n\n    def optInit(self):\n        if self.CPUPinn == True:\n            cupy.cuda.set_allocator(my_pinned_allocator)\n\n        self.CUPYcorpi = []\n        for optVar in self.optVarList:\n            self.CUPYcorpi.append( cupy.zeros( shape =( self.total_classes, self.embed_dimension), dtype=self.datatype) )\n\n        if self.CPUPinn == True:\n            cupy.cuda.set_allocator(None)\n            \n    def beforeForwardPass(self, retrievedPosIndexes , retrievedNegIndexes = None):\n        torch.cuda.synchronize()\n        cupy.cuda.Device().synchronize()\n        \n        reshapedRetrieval = self._getReshapedRetrieval( retrievedPosIndexes, retrievedNegIndexes )\n\n        for idx, optVar in enumerate(self.optVarList):\n            self.given_optimizer.state_dict()['state'][ self.optimizerKey ][optVar] = (\n                from_dlpack( self.CUPYcorpi[idx][ reshapedRetrieval ].toDlpack() )   )\n\n    def afterOptimizerStep(self, retrievedPosIndexes , retrievedNegIndexes = None):\n        torch.cuda.synchronize()\n        cupy.cuda.Device().synchronize()\n        \n        reshapedRetrieval = self._getReshapedRetrieval( retrievedPosIndexes, retrievedNegIndexes )\n\n        for idx, optVar in enumerate(self.optVarList):\n            self.CUPYcorpi[idx][ reshapedRetrieval ] = (\n                cupy.fromDlpack( to_dlpack( self.given_optimizer.state_dict()['state'][ self.optimizerKey ][optVar] ) )  )\n    \nclass COM(_Common):\n\n    def __init__(self, total_classes, datatype = 'uint32', CPUPinn = False  ):\n        self.total_classes = total_classes\n        self.datatype = datatype\n        self.CPUPinn = CPUPinn\n\n    def comInit(self, CPUPinn=False):\n        if self.CPUPinn == True:\n            cupy.cuda.set_allocator(my_pinned_allocator)\n        self.CUPYCom = cupy.zeros( shape =( self.total_classes, self.total_classes), dtype=self.datatype)\n        if self.CPUPinn == True:\n            cupy.cuda.set_allocator(None)\n    \nclass DataGadget(_Common):\n    def __init__(self, fileName, CPUPinn=False):\n        self.fileName = fileName\n        self.CPUPinn = CPUPinn\n\n    def gadgetInit(self):\n        if self.CPUPinn == True:\n            cupy.cuda.set_allocator(my_pinned_allocator)\n\n        self.CUPYcorpus = cupy.load( self.fileName)\n\n        if self.CPUPinn == True:\n            cupy.cuda.set_allocator(None)\n\n    def getData(self, indexes):\n        torch.cuda.synchronize()\n        cupy.cuda.Device().synchronize()\n        \n        return from_dlpack( self.CUPYcorpus[indexes].toDlpack() )\n\n    def insertData(self, dataObject, indexes):\n        torch.cuda.synchronize()\n        cupy.cuda.Device().synchronize()\n        \n        self.CUPYcorpus[indexes] =  cupy.fromDlpack( to_dlpack( dataObject ) ) \n    \n    \n    \n"""
SpeedTorch/GPUTorch.py,34,"b""import torch\nimport os\nimport numpy as np\n\nclass _GPUPytorchCommon():\n    def getTorchVar(self):\n        return self.TorchFSet\n\n    def saveTorch(self, saveFileName):\n        torch.save(self.pytorchGPUVar, saveFileName)\n        \n    def loadTorch(self, loadFileName):\n        self.pytorchGPUVar = torch.load(loadFileName)\n\n    def getNumpyVersion(self):\n        return self.pytorchGPUVar.cpu().numpy()\n\n    def _getReshapedRetrieval( self, retrievedPosIndexes , retrievedNegIndexes = None):\n        if not retrievedNegIndexes is None:\n            reshapedRetrieval =  np.concatenate( [ retrievedPosIndexes.reshape(-1) , retrievedNegIndexes.reshape(-1)] )\n        else:\n            reshapedRetrieval = retrievedPosIndexes.reshape(-1)\n        return reshapedRetrieval\n\nclass GPUPytorchModelFactory(_GPUPytorchCommon):\n\n    def __init__(self, model_variable,  total_classes,  embed_dimension, datatype = torch.float ):\n        self.model_variable = model_variable\n        self.total_classes = total_classes\n        self.embed_dimension = embed_dimension\n        self.dtype = datatype\n\n    def zerosInit(self):\n        self.pytorchGPUVar = torch.zeros( size=(self.total_classes, self.embed_dimension), dtype=self.dtype, device = 'cuda')\n\n    def customInit(self, initFunction, *args):\n        self.pytorchGPUVar = torch.empty(size=(self.total_classes, self.embed_dimension), dtype=self.dtype, device='cuda')\n        initFunction( self.pytorchGPUVar, *args )\n\n    def uniformDistributionInit(self, low, high):\n        self.pytorchGPUVar = torch.empty(size=(self.total_classes, self.embed_dimension), dtype=self.dtype, device='cuda')\n        torch.nn.init.uniform_(self.pytorchGPUVar, a=low, b=high)\n                \n    def normalDistributionInit(self, mean, stdDev ):\n        self.pytorchGPUVar = torch.empty(size=(self.total_classes, self.embed_dimension), dtype=self.dtype, device='cuda')\n        torch.nn.init.normal_(self.pytorchGPUVar, mean=mean, std=stdDev)\n                \n    def variableTransformer(self, batchSize, posPerBatch,  negPerBatch = None ):\n        if not negPerBatch == None:\n            return (np.arange( batchSize*posPerBatch ).reshape( batchSize , posPerBatch), \n                np.arange(start = batchSize*posPerBatch, \n                    stop = batchSize*posPerBatch + batchSize*negPerBatch ).reshape(batchSize, negPerBatch) )\n        else:\n            return np.arange( batchSize*posPerBatch ).reshape( batchSize, posPerBatch )\n\n    def beforeForwardPass(self, retrievedPosIndexes , retrievedNegIndexes = None):\n        torch.cuda.synchronize()\n        \n        reshapedRetrieval = self._getReshapedRetrieval( retrievedPosIndexes, retrievedNegIndexes )\n\n        self.model_variable.weight.data = self.pytorchGPUVar[ reshapedRetrieval ]\n\n    def afterOptimizerStep(self,retrievedPosIndexes , retrievedNegIndexes = None):\n        torch.cuda.synchronize()\n        \n        reshapedRetrieval = self._getReshapedRetrieval( retrievedPosIndexes, retrievedNegIndexes )\n\n        self.pytorchGPUVar[ reshapedRetrieval ] = self.model_variable.weight.data\n        \nclass GPUPytorchOptimizerFactory(_GPUPytorchCommon): #to do later, able to load matrixes to continue training\n#take into account different size embedding matrices \n\n    def __init__(self, given_optimizer,  total_classes,  embed_dimension, model, variable_name, dtype=torch.float ):\n        self.given_optimizer = given_optimizer\n        self.total_classes = total_classes\n        self.embed_dimension = embed_dimension\n        self.model = model\n        self.variable_name = variable_name\n        self.dtype = dtype\n        optimizer_index = None\n\n        #Some optiizers do not initialize its state until after first step\n        #So they need to initialized here\n        for group in given_optimizer.param_groups:\n            for p in group['params']:\n                state = given_optimizer.state[p]\n                # State initialization\n\n                if given_optimizer.__str__().split(' ', 1)[0] == 'SparseAdam':\n                    # State initialization\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['exp_avg'] = torch.zeros_like(p.data)\n                        state['exp_avg_sq'] = torch.zeros_like(p.data)\n                    self.optVarList = [ 'exp_avg', 'exp_avg_sq']\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'Adagrad':\n                    self.optVarList = [ 'sum' ]\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'Adadelta':\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['square_avg'] = torch.zeros_like(p.data)\n                        state['acc_delta'] = torch.zeros_like(p.data)\n                    self.optVarList = [ 'square_avg', 'acc_delta']\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'Adamax':\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['exp_avg'] = torch.zeros_like(p.data)\n                        state['exp_inf'] = torch.zeros_like(p.data)\n                    self.optVarList = [ 'exp_avg', 'exp_inf']\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'RMSprop':\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['square_avg'] = torch.zeros_like(p.data)\n                        if group['momentum'] > 0:\n                            state['momentum_buffer'] = torch.zeros_like(p.data)\n                        if group['centered']:\n                            state['grad_avg'] = torch.zeros_like(p.data)\n                    self.optVarList = [ 'square_avg']\n                    if group['momentum'] > 0:\n                         self.optVarList.append( 'momentum_buffer' )\n                    if group['centered']:\n                        self.optVarList.append( 'grad_avg' )\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'Rprop':\n                    if p.grad is None:\n                        print('Error, gradients are empty')\n                        print('For Rprop, need to first run at least 1 training step that has gradients')\n                        return\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['prev'] = torch.zeros_like(p.data)\n                        #For now, do now know how to Not initialize this due to len(state)==0 in optimizer\n                        state['step_size'] = grad.new().resize_as_(grad).fill_(group['lr'])\n                    self.optVarList = [ 'prev']\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'ASGD': \n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['eta'] = group['lr']\n                        state['mu'] = 1\n                        state['ax'] = torch.zeros_like(p.data)\n                        self.optVarList = [ 'ax']\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'AdamW': \n                    amsgrad = group['amsgrad']\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['exp_avg'] = torch.zeros_like(p.data)\n                        state['exp_avg_sq'] = torch.zeros_like(p.data)\n                        if amsgrad:\n                            state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n                    self.optVarList = [ 'exp_avg', 'exp_avg_sq']\n                    if amsgrad:\n                        self.optVarList.append('max_exp_avg_sq')\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'Adam': \n                    amsgrad = group['amsgrad']\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['exp_avg'] = torch.zeros_like(p.data)\n                        state['exp_avg_sq'] = torch.zeros_like(p.data)\n                        if amsgrad:\n                            state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n                    self.optVarList = [ 'exp_avg', 'exp_avg_sq']\n                    if amsgrad:\n                        self.optVarList.append( 'max_exp_avg_sq' )\n                else:\n                    print('This optimizer is not currently supported. Please choose a different optimizer')\n                    return\n\n        #Figure out which index for given variable \n        for i, item in enumerate( self.model.named_parameters() ):\n            if item[0][:-7] == self.variable_name:\n                optimizer_index = i\n        if optimizer_index == None:\n            print( 'Error: No variable with that name is in Model. Please initialize again with correct name' ) \n            return\n\n        optimizerKeyList = list(self.given_optimizer.state_dict()['state'].keys())\n        self.optimizerKey = optimizerKeyList[ optimizer_index ]\n\n    def optInit(self):\n        self.pytorchGPUVar = []\n        for optVar in self.optVarList:\n            self.pytorchGPUVar.append( torch.zeros( size=(self.total_classes, self.embed_dimension), dtype=self.dtype, device = 'cuda') )\n            \n    def beforeForwardPass(self, retrievedPosIndexes , retrievedNegIndexes = None):\n        torch.cuda.synchronize()\n        \n        reshapedRetrieval = self._getReshapedRetrieval( retrievedPosIndexes, retrievedNegIndexes )\n\n        for idx, optVar in enumerate(self.optVarList):\n            self.given_optimizer.state_dict()['state'][ self.optimizerKey ][optVar] = self.pytorchGPUVar[idx][ reshapedRetrieval ]\n\n    def afterOptimizerStep(self, retrievedPosIndexes , retrievedNegIndexes = None):\n        torch.cuda.synchronize()\n        \n        reshapedRetrieval = self._getReshapedRetrieval( retrievedPosIndexes, retrievedNegIndexes )\n\n        for idx, optVar in enumerate(self.optVarList):\n            self.pytorchGPUVar[idx][ reshapedRetrieval ] =  self.given_optimizer.state_dict()['state'][ self.optimizerKey ][optVar] \n            \nclass GPUPytorchCOM(_GPUPytorchCommon):\n\n    def __init__(self, total_classes, datatype = torch.int32 ):\n        self.total_classes = total_classes\n        self.dtype = datatype\n\n    def comInit(self):\n        self.pytorchGPUVar = torch.zeros( size=(self.total_classes, self.embed_dimension), dtype=self.dtype, device = 'cuda')\n\n"""
SpeedTorch/GenTorch.py,34,"b""import torch\nimport os\nimport numpy as np\n\nclass _PytorchCommon():\n    def getTorchVar(self):\n        return self.TorchFSet\n\n    def saveTorch(self, saveFileName):\n        torch.save(self.pytorchGPUVar, saveFileName)\n        \n    def loadTorch(self, loadFileName):\n        self.pytorchGPUVar = torch.load(loadFileName)\n\n    def getNumpyVersion(self):\n        return self.pytorchGPUVar.cpu().numpy()\n\n    def _getReshapedRetrieval( self, retrievedPosIndexes , retrievedNegIndexes = None):\n        if not retrievedNegIndexes is None:\n            reshapedRetrieval =  np.concatenate( [ retrievedPosIndexes.reshape(-1) , retrievedNegIndexes.reshape(-1)] )\n        else:\n            reshapedRetrieval = retrievedPosIndexes.reshape(-1)\n        return reshapedRetrieval\n\nclass PytorchModelFactory(_PytorchCommon):\n\n    def __init__(self, model_variable, total_classes, embed_dimension, datatype = torch.float, deviceType = 'cuda', pinType = False):\n        self.model_variable = model_variable\n        self.total_classes = total_classes\n        self.embed_dimension = embed_dimension\n        self.dtype = datatype\n        self.deviceType = deviceType\n        self.pinType = pinType\n\n    def zerosInit(self):\n        self.pytorchGPUVar = torch.zeros( size=(self.total_classes, self.embed_dimension), dtype=self.dtype, device=self.deviceType, pin_memory=self.pinType)\n\n    def customInit(self, initFunction, *args):\n        self.pytorchGPUVar = torch.empty(size=(self.total_classes, self.embed_dimension), dtype=self.dtype, device=self.deviceType, pin_memory=self.pinType)\n        initFunction( self.pytorchGPUVar, *args )\n\n    def uniformDistributionInit(self, low, high):\n        self.pytorchGPUVar = torch.empty(size=(self.total_classes, self.embed_dimension), dtype=self.dtype, device=self.deviceType, pin_memory=self.pinType)\n        torch.nn.init.uniform_(self.pytorchGPUVar, a=low, b=high)\n                \n    def normalDistributionInit(self, mean, stdDev ):\n        self.pytorchGPUVar = torch.empty(size=(self.total_classes, self.embed_dimension), dtype=self.dtype, device=self.deviceType, pin_memory=self.pinType)\n        torch.nn.init.normal_(self.pytorchGPUVar, mean=mean, std=stdDev)\n                \n    def variableTransformer(self, batchSize, posPerBatch,  negPerBatch = None ):\n        if not negPerBatch == None:\n            return (np.arange( batchSize*posPerBatch ).reshape( batchSize , posPerBatch), \n                np.arange(start = batchSize*posPerBatch, \n                    stop = batchSize*posPerBatch + batchSize*negPerBatch ).reshape(batchSize, negPerBatch) )\n        else:\n            return np.arange( batchSize*posPerBatch ).reshape( batchSize, posPerBatch )\n\n    def beforeForwardPass(self, retrievedPosIndexes , retrievedNegIndexes = None):\n        torch.cuda.synchronize()\n        \n        reshapedRetrieval = self._getReshapedRetrieval( retrievedPosIndexes, retrievedNegIndexes )\n\n        self.model_variable.weight.data = self.pytorchGPUVar[ reshapedRetrieval ]\n\n    def afterOptimizerStep(self,retrievedPosIndexes , retrievedNegIndexes = None):\n        torch.cuda.synchronize()\n        \n        reshapedRetrieval = self._getReshapedRetrieval( retrievedPosIndexes, retrievedNegIndexes )\n\n        self.pytorchGPUVar[ reshapedRetrieval ] = self.model_variable.weight.data\n        \nclass PytorchOptimizerFactory(_PytorchCommon): #to do later, able to load matrixes to continue training\n#take into account different size embedding matrices \n\n    def __init__(self, given_optimizer,  total_classes,  embed_dimension, model, variable_name, dtype=torch.float, deviceType = 'cuda', pinType = False):\n        self.given_optimizer = given_optimizer\n        self.total_classes = total_classes\n        self.embed_dimension = embed_dimension\n        self.model = model\n        self.variable_name = variable_name\n        self.dtype = dtype\n        self.deviceType = deviceType\n        self.pinType = pinType\n        optimizer_index = None\n\n        #Some optiizers do not initialize its state until after first step\n        #So they need to initialized here\n        for group in given_optimizer.param_groups:\n            for p in group['params']:\n                state = given_optimizer.state[p]\n                # State initialization\n\n                if given_optimizer.__str__().split(' ', 1)[0] == 'SparseAdam':\n                    # State initialization\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['exp_avg'] = torch.zeros_like(p.data)\n                        state['exp_avg_sq'] = torch.zeros_like(p.data)\n                    self.optVarList = [ 'exp_avg', 'exp_avg_sq']\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'Adagrad':\n                    self.optVarList = [ 'sum' ]\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'Adadelta':\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['square_avg'] = torch.zeros_like(p.data)\n                        state['acc_delta'] = torch.zeros_like(p.data)\n                    self.optVarList = [ 'square_avg', 'acc_delta']\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'Adamax':\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['exp_avg'] = torch.zeros_like(p.data)\n                        state['exp_inf'] = torch.zeros_like(p.data)\n                    self.optVarList = [ 'exp_avg', 'exp_inf']\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'RMSprop':\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['square_avg'] = torch.zeros_like(p.data)\n                        if group['momentum'] > 0:\n                            state['momentum_buffer'] = torch.zeros_like(p.data)\n                        if group['centered']:\n                            state['grad_avg'] = torch.zeros_like(p.data)\n                    self.optVarList = [ 'square_avg']\n                    if group['momentum'] > 0:\n                         self.optVarList.append( 'momentum_buffer' )\n                    if group['centered']:\n                        self.optVarList.append( 'grad_avg' )\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'Rprop':\n                    if p.grad is None:\n                        print('Error, gradients are empty')\n                        print('For Rprop, need to first run at least 1 training step that has gradients')\n                        return\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['prev'] = torch.zeros_like(p.data)\n                        #For now, do now know how to Not initialize this due to len(state)==0 in optimizer\n                        state['step_size'] = grad.new().resize_as_(grad).fill_(group['lr'])\n                    self.optVarList = [ 'prev']\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'ASGD': \n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['eta'] = group['lr']\n                        state['mu'] = 1\n                        state['ax'] = torch.zeros_like(p.data)\n                        self.optVarList = [ 'ax']\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'AdamW': \n                    amsgrad = group['amsgrad']\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['exp_avg'] = torch.zeros_like(p.data)\n                        state['exp_avg_sq'] = torch.zeros_like(p.data)\n                        if amsgrad:\n                            state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n                    self.optVarList = [ 'exp_avg', 'exp_avg_sq']\n                    if amsgrad:\n                        self.optVarList.append('max_exp_avg_sq')\n                elif given_optimizer.__str__().split(' ', 1)[0] == 'Adam': \n                    amsgrad = group['amsgrad']\n                    if len(state) == 0:\n                        state['step'] = 0\n                        state['exp_avg'] = torch.zeros_like(p.data)\n                        state['exp_avg_sq'] = torch.zeros_like(p.data)\n                        if amsgrad:\n                            state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n                    self.optVarList = [ 'exp_avg', 'exp_avg_sq']\n                    if amsgrad:\n                        self.optVarList.append( 'max_exp_avg_sq' )\n                else:\n                    print('This optimizer is not currently supported. Please choose a different optimizer')\n                    return\n\n        #Figure out which index for given variable \n        for i, item in enumerate( self.model.named_parameters() ):\n            if item[0][:-7] == self.variable_name:\n                optimizer_index = i\n        if optimizer_index == None:\n            print( 'Error: No variable with that name is in Model. Please initialize again with correct name' ) \n            return\n\n        optimizerKeyList = list(self.given_optimizer.state_dict()['state'].keys())\n        self.optimizerKey = optimizerKeyList[ optimizer_index ]\n\n    def optInit(self):\n        self.pytorchGPUVar = []\n        for optVar in self.optVarList:\n            self.pytorchGPUVar.append( torch.zeros( size=(self.total_classes, self.embed_dimension), dtype=self.dtype, device=self.deviceType, pin_memory=self.pinType) )\n            \n    def beforeForwardPass(self, retrievedPosIndexes , retrievedNegIndexes = None):\n        torch.cuda.synchronize()\n        \n        reshapedRetrieval = self._getReshapedRetrieval( retrievedPosIndexes, retrievedNegIndexes )\n\n        for idx, optVar in enumerate(self.optVarList):\n            self.given_optimizer.state_dict()['state'][ self.optimizerKey ][optVar] = self.pytorchGPUVar[idx][ reshapedRetrieval ]\n\n    def afterOptimizerStep(self, retrievedPosIndexes , retrievedNegIndexes = None):\n        torch.cuda.synchronize()\n        \n        reshapedRetrieval = self._getReshapedRetrieval( retrievedPosIndexes, retrievedNegIndexes )\n\n        for idx, optVar in enumerate(self.optVarList):\n            self.pytorchGPUVar[idx][ reshapedRetrieval ] = self.given_optimizer.state_dict()['state'][ self.optimizerKey ][optVar] \n            \nclass PytorchCOM(_PytorchCommon):\n\n    def __init__(self, total_classes, datatype = torch.int32 ):\n        self.total_classes = total_classes\n        self.dtype = datatype\n\n    def comInit(self):\n        self.pytorchGPUVar = torch.zeros( size=(self.total_classes, self.embed_dimension), dtype=self.dtype, device=self.deviceType, pin_memory=self.pinType)\n\n"""
SpeedTorch/__init__.py,0,b'\nfrom SpeedTorch.CUPYLive import PMemory\nfrom SpeedTorch.CUPYLive import my_pinned_allocator\nfrom SpeedTorch.CUPYLive import _Common\nfrom SpeedTorch.CUPYLive import ModelFactory\nfrom SpeedTorch.CUPYLive import OptimizerFactory\nfrom SpeedTorch.CUPYLive import COM\nfrom SpeedTorch.CUPYLive import DataGadget\n\nfrom SpeedTorch.CPUCupyPinned import PMemoryMM\nfrom SpeedTorch.CPUCupyPinned import my_pinned_allocatorMM\nfrom SpeedTorch.CPUCupyPinned import _CommonMM\nfrom SpeedTorch.CPUCupyPinned import ModelFactoryMM\nfrom SpeedTorch.CPUCupyPinned import OptimizerFactoryMM\nfrom SpeedTorch.CPUCupyPinned import COMMM\nfrom SpeedTorch.CPUCupyPinned import DataGadgetMM\n\nfrom SpeedTorch.GPUTorch import _GPUPytorchCommon\nfrom SpeedTorch.GPUTorch import GPUPytorchModelFactory\nfrom SpeedTorch.GPUTorch import GPUPytorchOptimizerFactory\nfrom SpeedTorch.GPUTorch import GPUPytorchCOM\n\nfrom SpeedTorch.CPUTorchPinned import _CPUPytorchCommon\nfrom SpeedTorch.CPUTorchPinned import CPUPytorchModelFactory\nfrom SpeedTorch.CPUTorchPinned import CPUPytorchOptimizerFactory\nfrom SpeedTorch.CPUTorchPinned import CPUPytorchCOM\n\nfrom SpeedTorch.GenTorch import _PytorchCommon\nfrom SpeedTorch.GenTorch import PytorchModelFactory\nfrom SpeedTorch.GenTorch import PytorchOptimizerFactory\nfrom SpeedTorch.GenTorch import PytorchCOM\n'
