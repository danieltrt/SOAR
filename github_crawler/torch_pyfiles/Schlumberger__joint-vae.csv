file_path,api_count,code
main.py,2,"b""import torch\nfrom jointvae.models import VAE\nfrom jointvae.training import Trainer\nfrom utils.dataloaders import get_mnist_dataloaders\nfrom torch import optim\n\n\nbatch_size = 64\nlr = 5e-4\nepochs = 100\n\n# Check for cuda\nuse_cuda = torch.cuda.is_available()\n\n# Load data\ndata_loader, _ = get_mnist_dataloaders(batch_size=batch_size)\nimg_size = (1, 32, 32)\n\n# Define latent spec and model\nlatent_spec = {'cont': 10, 'disc': [10]}\nmodel = VAE(img_size=img_size, latent_spec=latent_spec,\n            use_cuda=use_cuda)\nif use_cuda:\n    model.cuda()\n\n# Define optimizer\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\n# Define trainer\ntrainer = Trainer(model, optimizer,\n                  cont_capacity=[0.0, 5.0, 25000, 30],\n                  disc_capacity=[0.0, 5.0, 25000, 30],\n                  use_cuda=use_cuda)\n\n# Train model for 100 epochs\ntrainer.train(data_loader, epochs)\n\n# Save trained model\ntorch.save(trainer.model.state_dict(), 'example-model.pt')\n"""
jointvae/__init__.py,0,b''
jointvae/models.py,16,"b'import torch\nfrom torch import nn, optim\nfrom torch.nn import functional as F\n\nEPS = 1e-12\n\n\nclass VAE(nn.Module):\n    def __init__(self, img_size, latent_spec, temperature=.67, use_cuda=False):\n        """"""\n        Class which defines model and forward pass.\n\n        Parameters\n        ----------\n        img_size : tuple of ints\n            Size of images. E.g. (1, 32, 32) or (3, 64, 64).\n\n        latent_spec : dict\n            Specifies latent distribution. For example:\n            {\'cont\': 10, \'disc\': [10, 4, 3]} encodes 10 normal variables and\n            3 gumbel softmax variables of dimension 10, 4 and 3. A latent spec\n            can include both \'cont\' and \'disc\' or only \'cont\' or only \'disc\'.\n\n        temperature : float\n            Temperature for gumbel softmax distribution.\n\n        use_cuda : bool\n            If True moves model to GPU\n        """"""\n        super(VAE, self).__init__()\n        self.use_cuda = use_cuda\n\n        # Parameters\n        self.img_size = img_size\n        self.is_continuous = \'cont\' in latent_spec\n        self.is_discrete = \'disc\' in latent_spec\n        self.latent_spec = latent_spec\n        self.num_pixels = img_size[1] * img_size[2]\n        self.temperature = temperature\n        self.hidden_dim = 256  # Hidden dimension of linear layer\n        self.reshape = (64, 4, 4)  # Shape required to start transpose convs\n\n        # Calculate dimensions of latent distribution\n        self.latent_cont_dim = 0\n        self.latent_disc_dim = 0\n        self.num_disc_latents = 0\n        if self.is_continuous:\n            self.latent_cont_dim = self.latent_spec[\'cont\']\n        if self.is_discrete:\n            self.latent_disc_dim += sum([dim for dim in self.latent_spec[\'disc\']])\n            self.num_disc_latents = len(self.latent_spec[\'disc\'])\n        self.latent_dim = self.latent_cont_dim + self.latent_disc_dim\n\n        # Define encoder layers\n        # Intial layer\n        encoder_layers = [\n            nn.Conv2d(self.img_size[0], 32, (4, 4), stride=2, padding=1),\n            nn.ReLU()\n        ]\n        # Add additional layer if (64, 64) images\n        if self.img_size[1:] == (64, 64):\n            encoder_layers += [\n                nn.Conv2d(32, 32, (4, 4), stride=2, padding=1),\n                nn.ReLU()\n            ]\n        elif self.img_size[1:] == (32, 32):\n            # (32, 32) images are supported but do not require an extra layer\n            pass\n        else:\n            raise RuntimeError(""{} sized images not supported. Only (None, 32, 32) and (None, 64, 64) supported. Build your own architecture or reshape images!"".format(img_size))\n        # Add final layers\n        encoder_layers += [\n            nn.Conv2d(32, 64, (4, 4), stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, (4, 4), stride=2, padding=1),\n            nn.ReLU()\n        ]\n\n        # Define encoder\n        self.img_to_features = nn.Sequential(*encoder_layers)\n\n        # Map encoded features into a hidden vector which will be used to\n        # encode parameters of the latent distribution\n        self.features_to_hidden = nn.Sequential(\n            nn.Linear(64 * 4 * 4, self.hidden_dim),\n            nn.ReLU()\n        )\n\n        # Encode parameters of latent distribution\n        if self.is_continuous:\n            self.fc_mean = nn.Linear(self.hidden_dim, self.latent_cont_dim)\n            self.fc_log_var = nn.Linear(self.hidden_dim, self.latent_cont_dim)\n        if self.is_discrete:\n            # Linear layer for each of the categorical distributions\n            fc_alphas = []\n            for disc_dim in self.latent_spec[\'disc\']:\n                fc_alphas.append(nn.Linear(self.hidden_dim, disc_dim))\n            self.fc_alphas = nn.ModuleList(fc_alphas)\n\n        # Map latent samples to features to be used by generative model\n        self.latent_to_features = nn.Sequential(\n            nn.Linear(self.latent_dim, self.hidden_dim),\n            nn.ReLU(),\n            nn.Linear(self.hidden_dim, 64 * 4 * 4),\n            nn.ReLU()\n        )\n\n        # Define decoder\n        decoder_layers = []\n\n        # Additional decoding layer for (64, 64) images\n        if self.img_size[1:] == (64, 64):\n            decoder_layers += [\n                nn.ConvTranspose2d(64, 64, (4, 4), stride=2, padding=1),\n                nn.ReLU()\n            ]\n\n        decoder_layers += [\n            nn.ConvTranspose2d(64, 32, (4, 4), stride=2, padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(32, 32, (4, 4), stride=2, padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(32, self.img_size[0], (4, 4), stride=2, padding=1),\n            nn.Sigmoid()\n        ]\n\n        # Define decoder\n        self.features_to_img = nn.Sequential(*decoder_layers)\n\n    def encode(self, x):\n        """"""\n        Encodes an image into parameters of a latent distribution defined in\n        self.latent_spec.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Batch of data, shape (N, C, H, W)\n        """"""\n        batch_size = x.size()[0]\n\n        # Encode image to hidden features\n        features = self.img_to_features(x)\n        hidden = self.features_to_hidden(features.view(batch_size, -1))\n\n        # Output parameters of latent distribution from hidden representation\n        latent_dist = {}\n\n        if self.is_continuous:\n            latent_dist[\'cont\'] = [self.fc_mean(hidden), self.fc_log_var(hidden)]\n\n        if self.is_discrete:\n            latent_dist[\'disc\'] = []\n            for fc_alpha in self.fc_alphas:\n                latent_dist[\'disc\'].append(F.softmax(fc_alpha(hidden), dim=1))\n\n        return latent_dist\n\n    def reparameterize(self, latent_dist):\n        """"""\n        Samples from latent distribution using the reparameterization trick.\n\n        Parameters\n        ----------\n        latent_dist : dict\n            Dict with keys \'cont\' or \'disc\' or both, containing the parameters\n            of the latent distributions as torch.Tensor instances.\n        """"""\n        latent_sample = []\n\n        if self.is_continuous:\n            mean, logvar = latent_dist[\'cont\']\n            cont_sample = self.sample_normal(mean, logvar)\n            latent_sample.append(cont_sample)\n\n        if self.is_discrete:\n            for alpha in latent_dist[\'disc\']:\n                disc_sample = self.sample_gumbel_softmax(alpha)\n                latent_sample.append(disc_sample)\n\n        # Concatenate continuous and discrete samples into one large sample\n        return torch.cat(latent_sample, dim=1)\n\n    def sample_normal(self, mean, logvar):\n        """"""\n        Samples from a normal distribution using the reparameterization trick.\n\n        Parameters\n        ----------\n        mean : torch.Tensor\n            Mean of the normal distribution. Shape (N, D) where D is dimension\n            of distribution.\n\n        logvar : torch.Tensor\n            Diagonal log variance of the normal distribution. Shape (N, D)\n        """"""\n        if self.training:\n            std = torch.exp(0.5 * logvar)\n            eps = torch.zeros(std.size()).normal_()\n            if self.use_cuda:\n                eps = eps.cuda()\n            return mean + std * eps\n        else:\n            # Reconstruction mode\n            return mean\n\n    def sample_gumbel_softmax(self, alpha):\n        """"""\n        Samples from a gumbel-softmax distribution using the reparameterization\n        trick.\n\n        Parameters\n        ----------\n        alpha : torch.Tensor\n            Parameters of the gumbel-softmax distribution. Shape (N, D)\n        """"""\n        if self.training:\n            # Sample from gumbel distribution\n            unif = torch.rand(alpha.size())\n            if self.use_cuda:\n                unif = unif.cuda()\n            gumbel = -torch.log(-torch.log(unif + EPS) + EPS)\n            # Reparameterize to create gumbel softmax sample\n            log_alpha = torch.log(alpha + EPS)\n            logit = (log_alpha + gumbel) / self.temperature\n            return F.softmax(logit, dim=1)\n        else:\n            # In reconstruction mode, pick most likely sample\n            _, max_alpha = torch.max(alpha, dim=1)\n            one_hot_samples = torch.zeros(alpha.size())\n            # On axis 1 of one_hot_samples, scatter the value 1 at indices\n            # max_alpha. Note the view is because scatter_ only accepts 2D\n            # tensors.\n            one_hot_samples.scatter_(1, max_alpha.view(-1, 1).data.cpu(), 1)\n            if self.use_cuda:\n                one_hot_samples = one_hot_samples.cuda()\n            return one_hot_samples\n\n    def decode(self, latent_sample):\n        """"""\n        Decodes sample from latent distribution into an image.\n\n        Parameters\n        ----------\n        latent_sample : torch.Tensor\n            Sample from latent distribution. Shape (N, L) where L is dimension\n            of latent distribution.\n        """"""\n        features = self.latent_to_features(latent_sample)\n        return self.features_to_img(features.view(-1, *self.reshape))\n\n    def forward(self, x):\n        """"""\n        Forward pass of model.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Batch of data. Shape (N, C, H, W)\n        """"""\n        latent_dist = self.encode(x)\n        latent_sample = self.reparameterize(latent_dist)\n        return self.decode(latent_sample), latent_dist\n'"
jointvae/training.py,19,"b'import imageio\nimport numpy as np\nimport torch\nfrom torch.nn import functional as F\nfrom torchvision.utils import make_grid\n\nEPS = 1e-12\n\n\nclass Trainer():\n    def __init__(self, model, optimizer, cont_capacity=None,\n                 disc_capacity=None, print_loss_every=50, record_loss_every=5,\n                 use_cuda=False):\n        """"""\n        Class to handle training of model.\n\n        Parameters\n        ----------\n        model : jointvae.models.VAE instance\n\n        optimizer : torch.optim.Optimizer instance\n\n        cont_capacity : tuple (float, float, int, float) or None\n            Tuple containing (min_capacity, max_capacity, num_iters, gamma_z).\n            Parameters to control the capacity of the continuous latent\n            channels. Cannot be None if model.is_continuous is True.\n\n        disc_capacity : tuple (float, float, int, float) or None\n            Tuple containing (min_capacity, max_capacity, num_iters, gamma_c).\n            Parameters to control the capacity of the discrete latent channels.\n            Cannot be None if model.is_discrete is True.\n\n        print_loss_every : int\n            Frequency with which loss is printed during training.\n\n        record_loss_every : int\n            Frequency with which loss is recorded during training.\n\n        use_cuda : bool\n            If True moves model and training to GPU.\n        """"""\n        self.model = model\n        self.optimizer = optimizer\n        self.cont_capacity = cont_capacity\n        self.disc_capacity = disc_capacity\n        self.print_loss_every = print_loss_every\n        self.record_loss_every = record_loss_every\n        self.use_cuda = use_cuda\n\n        if self.model.is_continuous and self.cont_capacity is None:\n            raise RuntimeError(""Model is continuous but cont_capacity not provided."")\n\n        if self.model.is_discrete and self.disc_capacity is None:\n            raise RuntimeError(""Model is discrete but disc_capacity not provided."")\n\n        if self.use_cuda:\n            self.model.cuda()\n\n        # Initialize attributes\n        self.num_steps = 0\n        self.batch_size = None\n        self.losses = {\'loss\': [],\n                       \'recon_loss\': [],\n                       \'kl_loss\': []}\n\n        # Keep track of divergence values for each latent variable\n        if self.model.is_continuous:\n            self.losses[\'kl_loss_cont\'] = []\n            # For every dimension of continuous latent variables\n            for i in range(self.model.latent_spec[\'cont\']):\n                self.losses[\'kl_loss_cont_\' + str(i)] = []\n\n        if self.model.is_discrete:\n            self.losses[\'kl_loss_disc\'] = []\n            # For every discrete latent variable\n            for i in range(len(self.model.latent_spec[\'disc\'])):\n                self.losses[\'kl_loss_disc_\' + str(i)] = []\n\n    def train(self, data_loader, epochs=10, save_training_gif=None):\n        """"""\n        Trains the model.\n\n        Parameters\n        ----------\n        data_loader : torch.utils.data.DataLoader\n\n        epochs : int\n            Number of epochs to train the model for.\n\n        save_training_gif : None or tuple (string, Visualizer instance)\n            If not None, will use visualizer object to create image of samples\n            after every epoch and will save gif of these at location specified\n            by string. Note that string should end with \'.gif\'.\n        """"""\n        if save_training_gif is not None:\n            training_progress_images = []\n\n        self.batch_size = data_loader.batch_size\n        self.model.train()\n        for epoch in range(epochs):\n            mean_epoch_loss = self._train_epoch(data_loader)\n            print(\'Epoch: {} Average loss: {:.2f}\'.format(epoch + 1,\n                                                          self.batch_size * self.model.num_pixels * mean_epoch_loss))\n\n            if save_training_gif is not None:\n                # Generate batch of images and convert to grid\n                viz = save_training_gif[1]\n                viz.save_images = False\n                img_grid = viz.all_latent_traversals(size=10)\n                # Convert to numpy and transpose axes to fit imageio convention\n                # i.e. (width, height, channels)\n                img_grid = np.transpose(img_grid.numpy(), (1, 2, 0))\n                # Add image grid to training progress\n                training_progress_images.append(img_grid)\n\n        if save_training_gif is not None:\n            imageio.mimsave(save_training_gif[0], training_progress_images,\n                            fps=24)\n\n    def _train_epoch(self, data_loader):\n        """"""\n        Trains the model for one epoch.\n\n        Parameters\n        ----------\n        data_loader : torch.utils.data.DataLoader\n        """"""\n        epoch_loss = 0.\n        print_every_loss = 0.  # Keeps track of loss to print every\n                               # self.print_loss_every\n        for batch_idx, (data, label) in enumerate(data_loader):\n            iter_loss = self._train_iteration(data)\n            epoch_loss += iter_loss\n            print_every_loss += iter_loss\n            # Print loss info every self.print_loss_every iteration\n            if batch_idx % self.print_loss_every == 0:\n                if batch_idx == 0:\n                    mean_loss = print_every_loss\n                else:\n                    mean_loss = print_every_loss / self.print_loss_every\n                print(\'{}/{}\\tLoss: {:.3f}\'.format(batch_idx * len(data),\n                                                  len(data_loader.dataset),\n                                                  self.model.num_pixels * mean_loss))\n                print_every_loss = 0.\n        # Return mean epoch loss\n        return epoch_loss / len(data_loader.dataset)\n\n    def _train_iteration(self, data):\n        """"""\n        Trains the model for one iteration on a batch of data.\n\n        Parameters\n        ----------\n        data : torch.Tensor\n            A batch of data. Shape (N, C, H, W)\n        """"""\n        self.num_steps += 1\n        if self.use_cuda:\n            data = data.cuda()\n\n        self.optimizer.zero_grad()\n        recon_batch, latent_dist = self.model(data)\n        loss = self._loss_function(data, recon_batch, latent_dist)\n        loss.backward()\n        self.optimizer.step()\n\n        train_loss = loss.item()\n        return train_loss\n\n    def _loss_function(self, data, recon_data, latent_dist):\n        """"""\n        Calculates loss for a batch of data.\n\n        Parameters\n        ----------\n        data : torch.Tensor\n            Input data (e.g. batch of images). Should have shape (N, C, H, W)\n\n        recon_data : torch.Tensor\n            Reconstructed data. Should have shape (N, C, H, W)\n\n        latent_dist : dict\n            Dict with keys \'cont\' or \'disc\' or both containing the parameters\n            of the latent distributions as values.\n        """"""\n        # Reconstruction loss is pixel wise cross-entropy\n        recon_loss = F.binary_cross_entropy(recon_data.view(-1, self.model.num_pixels),\n                                            data.view(-1, self.model.num_pixels))\n        # F.binary_cross_entropy takes mean over pixels, so unnormalise this\n        recon_loss *= self.model.num_pixels\n\n        # Calculate KL divergences\n        kl_cont_loss = 0  # Used to compute capacity loss (but not a loss in itself)\n        kl_disc_loss = 0  # Used to compute capacity loss (but not a loss in itself)\n        cont_capacity_loss = 0\n        disc_capacity_loss = 0\n\n        if self.model.is_continuous:\n            # Calculate KL divergence\n            mean, logvar = latent_dist[\'cont\']\n            kl_cont_loss = self._kl_normal_loss(mean, logvar)\n            # Linearly increase capacity of continuous channels\n            cont_min, cont_max, cont_num_iters, cont_gamma = \\\n                self.cont_capacity\n            # Increase continuous capacity without exceeding cont_max\n            cont_cap_current = (cont_max - cont_min) * self.num_steps / float(cont_num_iters) + cont_min\n            cont_cap_current = min(cont_cap_current, cont_max)\n            # Calculate continuous capacity loss\n            cont_capacity_loss = cont_gamma * torch.abs(cont_cap_current - kl_cont_loss)\n\n        if self.model.is_discrete:\n            # Calculate KL divergence\n            kl_disc_loss = self._kl_multiple_discrete_loss(latent_dist[\'disc\'])\n            # Linearly increase capacity of discrete channels\n            disc_min, disc_max, disc_num_iters, disc_gamma = \\\n                self.disc_capacity\n            # Increase discrete capacity without exceeding disc_max or theoretical\n            # maximum (i.e. sum of log of dimension of each discrete variable)\n            disc_cap_current = (disc_max - disc_min) * self.num_steps / float(disc_num_iters) + disc_min\n            disc_cap_current = min(disc_cap_current, disc_max)\n            # Require float conversion here to not end up with numpy float\n            disc_theoretical_max = sum([float(np.log(disc_dim)) for disc_dim in self.model.latent_spec[\'disc\']])\n            disc_cap_current = min(disc_cap_current, disc_theoretical_max)\n            # Calculate discrete capacity loss\n            disc_capacity_loss = disc_gamma * torch.abs(disc_cap_current - kl_disc_loss)\n\n        # Calculate total kl value to record it\n        kl_loss = kl_cont_loss + kl_disc_loss\n\n        # Calculate total loss\n        total_loss = recon_loss + cont_capacity_loss + disc_capacity_loss\n\n        # Record losses\n        if self.model.training and self.num_steps % self.record_loss_every == 1:\n            self.losses[\'recon_loss\'].append(recon_loss.item())\n            self.losses[\'kl_loss\'].append(kl_loss.item())\n            self.losses[\'loss\'].append(total_loss.item())\n\n        # To avoid large losses normalise by number of pixels\n        return total_loss / self.model.num_pixels\n\n    def _kl_normal_loss(self, mean, logvar):\n        """"""\n        Calculates the KL divergence between a normal distribution with\n        diagonal covariance and a unit normal distribution.\n\n        Parameters\n        ----------\n        mean : torch.Tensor\n            Mean of the normal distribution. Shape (N, D) where D is dimension\n            of distribution.\n\n        logvar : torch.Tensor\n            Diagonal log variance of the normal distribution. Shape (N, D)\n        """"""\n        # Calculate KL divergence\n        kl_values = -0.5 * (1 + logvar - mean.pow(2) - logvar.exp())\n        # Mean KL divergence across batch for each latent variable\n        kl_means = torch.mean(kl_values, dim=0)\n        # KL loss is sum of mean KL of each latent variable\n        kl_loss = torch.sum(kl_means)\n\n        # Record losses\n        if self.model.training and self.num_steps % self.record_loss_every == 1:\n            self.losses[\'kl_loss_cont\'].append(kl_loss.item())\n            for i in range(self.model.latent_spec[\'cont\']):\n                self.losses[\'kl_loss_cont_\' + str(i)].append(kl_means[i].item())\n\n        return kl_loss\n\n    def _kl_multiple_discrete_loss(self, alphas):\n        """"""\n        Calculates the KL divergence between a set of categorical distributions\n        and a set of uniform categorical distributions.\n\n        Parameters\n        ----------\n        alphas : list\n            List of the alpha parameters of a categorical (or gumbel-softmax)\n            distribution. For example, if the categorical atent distribution of\n            the model has dimensions [2, 5, 10] then alphas will contain 3\n            torch.Tensor instances with the parameters for each of\n            the distributions. Each of these will have shape (N, D).\n        """"""\n        # Calculate kl losses for each discrete latent\n        kl_losses = [self._kl_discrete_loss(alpha) for alpha in alphas]\n\n        # Total loss is sum of kl loss for each discrete latent\n        kl_loss = torch.sum(torch.cat(kl_losses))\n\n        # Record losses\n        if self.model.training and self.num_steps % self.record_loss_every == 1:\n            self.losses[\'kl_loss_disc\'].append(kl_loss.item())\n            for i in range(len(alphas)):\n                self.losses[\'kl_loss_disc_\' + str(i)].append(kl_losses[i].item())\n\n        return kl_loss\n\n    def _kl_discrete_loss(self, alpha):\n        """"""\n        Calculates the KL divergence between a categorical distribution and a\n        uniform categorical distribution.\n\n        Parameters\n        ----------\n        alpha : torch.Tensor\n            Parameters of the categorical or gumbel-softmax distribution.\n            Shape (N, D)\n        """"""\n        disc_dim = int(alpha.size()[-1])\n        log_dim = torch.Tensor([np.log(disc_dim)])\n        if self.use_cuda:\n            log_dim = log_dim.cuda()\n        # Calculate negative entropy of each row\n        neg_entropy = torch.sum(alpha * torch.log(alpha + EPS), dim=1)\n        # Take mean of negative entropy across batch\n        mean_neg_entropy = torch.mean(neg_entropy, dim=0)\n        # KL loss of alpha with uniform categorical variable\n        kl_loss = log_dim + mean_neg_entropy\n        return kl_loss\n'"
utils/__init__.py,0,b''
utils/dataloaders.py,1,"b'import glob\nimport numpy as np\nfrom skimage.io import imread\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets, transforms\n\n\ndef get_mnist_dataloaders(batch_size=128, path_to_data=\'../data\'):\n    """"""MNIST dataloader with (32, 32) images.""""""\n    all_transforms = transforms.Compose([\n        transforms.Resize(32),\n        transforms.ToTensor()\n    ])\n    train_data = datasets.MNIST(path_to_data, train=True, download=True,\n                                transform=all_transforms)\n    test_data = datasets.MNIST(path_to_data, train=False,\n                               transform=all_transforms)\n    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n    return train_loader, test_loader\n\n\ndef get_fashion_mnist_dataloaders(batch_size=128,\n                                  path_to_data=\'../fashion_data\'):\n    """"""FashionMNIST dataloader with (32, 32) images.""""""\n    all_transforms = transforms.Compose([\n        transforms.Resize(32),\n        transforms.ToTensor()\n    ])\n    train_data = datasets.FashionMNIST(path_to_data, train=True, download=True,\n                                       transform=all_transforms)\n    test_data = datasets.FashionMNIST(path_to_data, train=False,\n                                      transform=all_transforms)\n    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n    return train_loader, test_loader\n\n\ndef get_dsprites_dataloader(batch_size=128,\n                            path_to_data=\'../dsprites-data/dsprites_data.npz\'):\n    """"""DSprites dataloader.""""""\n    dsprites_data = DSpritesDataset(path_to_data,\n                                    transform=transforms.ToTensor())\n    dsprites_loader = DataLoader(dsprites_data, batch_size=batch_size,\n                                 shuffle=True)\n    return dsprites_loader\n\n\ndef get_chairs_dataloader(batch_size=128,\n                          path_to_data=\'../rendered_chairs_64\'):\n    """"""Chairs dataloader. Chairs are center cropped and resized to (64, 64).""""""\n    all_transforms = transforms.Compose([\n        transforms.Grayscale(),\n        transforms.ToTensor()\n    ])\n    chairs_data = datasets.ImageFolder(root=path_to_data,\n                                       transform=all_transforms)\n    chairs_loader = DataLoader(chairs_data, batch_size=batch_size,\n                               shuffle=True)\n    return chairs_loader\n\n\ndef get_chairs_test_dataloader(batch_size=62,\n                               path_to_data=\'../rendered_chairs_64_test\'):\n    """"""There are 62 pictures of each chair, so get batches of data containing\n    one chair per batch.""""""\n    all_transforms = transforms.Compose([\n        transforms.Grayscale(),\n        transforms.ToTensor()\n    ])\n    chairs_data = datasets.ImageFolder(root=path_to_data,\n                                       transform=all_transforms)\n    chairs_loader = DataLoader(chairs_data, batch_size=batch_size,\n                               shuffle=False)\n    return chairs_loader\n\n\ndef get_celeba_dataloader(batch_size=128, path_to_data=\'../celeba_64\'):\n    """"""CelebA dataloader with (64, 64) images.""""""\n    celeba_data = CelebADataset(path_to_data,\n                                transform=transforms.ToTensor())\n    celeba_loader = DataLoader(celeba_data, batch_size=batch_size,\n                               shuffle=True)\n    return celeba_loader\n\n\nclass DSpritesDataset(Dataset):\n    """"""D Sprites dataset.""""""\n    def __init__(self, path_to_data, subsample=1, transform=None):\n        """"""\n        Parameters\n        ----------\n        subsample : int\n            Only load every |subsample| number of images.\n        """"""\n        self.imgs = np.load(path_to_data)[\'imgs\'][::subsample]\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.imgs)\n\n    def __getitem__(self, idx):\n        # Each image in the dataset has binary values so multiply by 255 to get\n        # pixel values\n        sample = self.imgs[idx] * 255\n        # Add extra dimension to turn shape into (H, W) -> (H, W, C)\n        sample = sample.reshape(sample.shape + (1,))\n\n        if self.transform:\n            sample = self.transform(sample)\n        # Since there are no labels, we just return 0 for the ""label"" here\n        return sample, 0\n\n\nclass CelebADataset(Dataset):\n    """"""CelebA dataset with 64 by 64 images.""""""\n    def __init__(self, path_to_data, subsample=1, transform=None):\n        """"""\n        Parameters\n        ----------\n        subsample : int\n            Only load every |subsample| number of images.\n        """"""\n        self.img_paths = glob.glob(path_to_data + \'/*\')[::subsample]\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def __getitem__(self, idx):\n        sample_path = self.img_paths[idx]\n        sample = imread(sample_path)\n\n        if self.transform:\n            sample = self.transform(sample)\n        # Since there are no labels, we just return 0 for the ""label"" here\n        return sample, 0'"
utils/load_model.py,1,"b'import json\nimport torch\nfrom jointvae.models import VAE\nfrom utils.dataloaders import (get_mnist_dataloaders, get_dsprites_dataloader,\n                               get_chairs_dataloader, get_fashion_mnist_dataloaders)\n\n\ndef load(path):\n    """"""\n    Loads a trained model.\n\n    Parameters\n    ----------\n    path : string\n        Path to folder where model is saved. For example\n        \'./trained_models/mnist/\'. Note the path MUST end with a \'/\'\n    """"""\n    path_to_specs = path + \'specs.json\'\n    path_to_model = path + \'model.pt\'\n\n    # Open specs file\n    with open(path_to_specs) as specs_file:\n        specs = json.load(specs_file)\n\n    # Unpack specs\n    dataset = specs[""dataset""]\n    latent_spec = specs[""latent_spec""]\n\n    # Get image size\n    if dataset == \'mnist\' or dataset == \'fashion_mnist\':\n        img_size = (1, 32, 32)\n    if dataset == \'chairs\' or dataset == \'dsprites\':\n        img_size = (1, 64, 64)\n    if dataset == \'celeba\':\n        img_size = (3, 64, 64)\n\n    # Get model\n    model = VAE(img_size=img_size, latent_spec=latent_spec)\n    model.load_state_dict(torch.load(path_to_model,\n                                     map_location=lambda storage, loc: storage))\n\n    return model\n'"
viz/__init__.py,0,b''
viz/latent_traversals.py,6,"b'import numpy as np\nimport torch\nfrom scipy import stats\n\n\nclass LatentTraverser():\n    def __init__(self, latent_spec):\n        """"""\n        LatentTraverser is used to generate traversals of the latent space.\n\n        Parameters\n        ----------\n        latent_spec : dict\n            See jointvae.models.VAE for parameter definition.\n        """"""\n        self.latent_spec = latent_spec\n        self.sample_prior = False  # If False fixes samples in untraversed\n                                   # latent dimensions. If True samples\n                                   # untraversed latent dimensions from prior.\n        self.is_continuous = \'cont\' in latent_spec\n        self.is_discrete = \'disc\' in latent_spec\n        self.cont_dim = latent_spec[\'cont\'] if self.is_continuous else None\n        self.disc_dims = latent_spec[\'disc\'] if self.is_discrete else None\n\n    def traverse_line(self, cont_idx=None, disc_idx=None, size=5):\n        """"""\n        Returns a (size, D) latent sample, corresponding to a traversal of the\n        latent variable indicated by cont_idx or disc_idx.\n\n        Parameters\n        ----------\n        cont_idx : int or None\n            Index of continuous dimension to traverse. If the continuous latent\n            vector is 10 dimensional and cont_idx = 7, then the 7th dimension\n            will be traversed while all others will either be fixed or randomly\n            sampled. If None, no latent is traversed and all latent\n            dimensions are randomly sampled or kept fixed.\n\n        disc_idx : int or None\n            Index of discrete latent dimension to traverse. If there are 5\n            discrete latent variables and disc_idx = 3, then only the 3rd\n            discrete latent will be traversed while others will be fixed or\n            randomly sampled. If None, no latent is traversed and all latent\n            dimensions are randomly sampled or kept fixed.\n\n        size : int\n            Number of samples to generate.\n        """"""\n        samples = []\n\n        if self.is_continuous:\n            samples.append(self._traverse_continuous_line(idx=cont_idx,\n                                                          size=size))\n        if self.is_discrete:\n            for i, disc_dim in enumerate(self.disc_dims):\n                if i == disc_idx:\n                    samples.append(self._traverse_discrete_line(dim=disc_dim,\n                                                                traverse=True,\n                                                                size=size))\n                else:\n                    samples.append(self._traverse_discrete_line(dim=disc_dim,\n                                                                traverse=False,\n                                                                size=size))\n\n        return torch.cat(samples, dim=1)\n\n    def _traverse_continuous_line(self, idx, size):\n        """"""\n        Returns a (size, cont_dim) latent sample, corresponding to a traversal\n        of a continuous latent variable indicated by idx.\n\n        Parameters\n        ----------\n        idx : int or None\n            Index of continuous latent dimension to traverse. If None, no\n            latent is traversed and all latent dimensions are randomly sampled\n            or kept fixed.\n\n        size : int\n            Number of samples to generate.\n        """"""\n        if self.sample_prior:\n            samples = np.random.normal(size=(size, self.cont_dim))\n        else:\n            samples = np.zeros(shape=(size, self.cont_dim))\n\n        if idx is not None:\n            # Sweep over linearly spaced coordinates transformed through the\n            # inverse CDF (ppf) of a gaussian since the prior of the latent\n            # space is gaussian\n            cdf_traversal = np.linspace(0.05, 0.95, size)\n            cont_traversal = stats.norm.ppf(cdf_traversal)\n\n            for i in range(size):\n                samples[i, idx] = cont_traversal[i]\n\n        return torch.Tensor(samples)\n\n    def _traverse_discrete_line(self, dim, traverse, size):\n        """"""\n        Returns a (size, dim) latent sample, corresponding to a traversal of a\n        discrete latent variable.\n\n        Parameters\n        ----------\n        dim : int\n            Number of categories of discrete latent variable.\n\n        traverse : bool\n            If True, traverse the categorical variable otherwise keep it fixed\n            or randomly sample.\n\n        size : int\n            Number of samples to generate.\n        """"""\n        samples = np.zeros((size, dim))\n\n        if traverse:\n            for i in range(size):\n                samples[i, i % dim] = 1.\n        else:\n            # Randomly select discrete variable (i.e. sample from uniform prior)\n            if self.sample_prior:\n                samples[np.arange(size), np.random.randint(0, dim, size)] = 1.\n            else:\n                samples[:, 0] = 1.\n\n        return torch.Tensor(samples)\n\n    def traverse_grid(self, cont_idx=None, cont_axis=None, disc_idx=None,\n                      disc_axis=None, size=(5, 5)):\n        """"""\n        Returns a (size[0] * size[1], D) latent sample, corresponding to a\n        two dimensional traversal of the latent space.\n\n        Parameters\n        ----------\n        cont_idx : int or None\n            Index of continuous dimension to traverse. If the continuous latent\n            vector is 10 dimensional and cont_idx = 7, then the 7th dimension\n            will be traversed while all others will either be fixed or randomly\n            sampled. If None, no latent is traversed and all latent\n            dimensions are randomly sampled or kept fixed.\n\n        cont_axis : int or None\n            Either 0 for traversal across the rows or 1 for traversal across\n            the columns. If None and disc_axis not None will default to axis\n            which disc_axis is not. Otherwise will default to 0.\n\n        disc_idx : int or None\n            Index of discrete latent dimension to traverse. If there are 5\n            discrete latent variables and disc_idx = 3, then only the 3rd\n            discrete latent will be traversed while others will be fixed or\n            randomly sampled. If None, no latent is traversed and all latent\n            dimensions are randomly sampled or kept fixed.\n\n        disc_axis : int or None\n            Either 0 for traversal across the rows or 1 for traversal across\n            the columns. If None and cont_axis not None will default to axis\n            which cont_axis is not. Otherwise will default to 1.\n\n        size : tuple of ints\n            Shape of grid to generate. E.g. (6, 4).\n        """"""\n        if cont_axis is None and disc_axis is None:\n            cont_axis = 0\n            disc_axis = 0\n        elif cont_axis is None:\n            cont_axis = int(not disc_axis)\n        elif disc_axis is None:\n            disc_axis = int(not cont_axis)\n\n        samples = []\n\n        if self.is_continuous:\n            samples.append(self._traverse_continuous_grid(idx=cont_idx,\n                                                          axis=cont_axis,\n                                                          size=size))\n        if self.is_discrete:\n            for i, disc_dim in enumerate(self.disc_dims):\n                if i == disc_idx:\n                    samples.append(self._traverse_discrete_grid(dim=disc_dim,\n                                                                axis=disc_axis,\n                                                                traverse=True,\n                                                                size=size))\n                else:\n                    samples.append(self._traverse_discrete_grid(dim=disc_dim,\n                                                                axis=disc_axis,\n                                                                traverse=False,\n                                                                size=size))\n\n        return torch.cat(samples, dim=1)\n\n    def _traverse_continuous_grid(self, idx, axis, size):\n        """"""\n        Returns a (size[0] * size[1], cont_dim) latent sample, corresponding to\n        a two dimensional traversal of the continuous latent space.\n\n        Parameters\n        ----------\n        idx : int or None\n            Index of continuous latent dimension to traverse. If None, no\n            latent is traversed and all latent dimensions are randomly sampled\n            or kept fixed.\n\n        axis : int\n            Either 0 for traversal across the rows or 1 for traversal across\n            the columns.\n\n        size : tuple of ints\n            Shape of grid to generate. E.g. (6, 4).\n        """"""\n        num_samples = size[0] * size[1]\n\n        if self.sample_prior:\n            samples = np.random.normal(size=(num_samples, self.cont_dim))\n        else:\n            samples = np.zeros(shape=(num_samples, self.cont_dim))\n\n        if idx is not None:\n            # Sweep over linearly spaced coordinates transformed through the\n            # inverse CDF (ppf) of a gaussian since the prior of the latent\n            # space is gaussian\n            cdf_traversal = np.linspace(0.05, 0.95, size[axis])\n            cont_traversal = stats.norm.ppf(cdf_traversal)\n\n            for i in range(size[0]):\n                for j in range(size[1]):\n                    if axis == 0:\n                        samples[i * size[1] + j, idx] = cont_traversal[i]\n                    else:\n                        samples[i * size[1] + j, idx] = cont_traversal[j]\n\n        return torch.Tensor(samples)\n\n    def _traverse_discrete_grid(self, dim, axis, traverse, size):\n        """"""\n        Returns a (size[0] * size[1], dim) latent sample, corresponding to a\n        two dimensional traversal of a discrete latent variable, where the\n        dimension of the traversal is determined by axis.\n\n        Parameters\n        ----------\n        idx : int or None\n            Index of continuous latent dimension to traverse. If None, no\n            latent is traversed and all latent dimensions are randomly sampled\n            or kept fixed.\n\n        axis : int\n            Either 0 for traversal across the rows or 1 for traversal across\n            the columns.\n\n        traverse : bool\n            If True, traverse the categorical variable otherwise keep it fixed\n            or randomly sample.\n\n        size : tuple of ints\n            Shape of grid to generate. E.g. (6, 4).\n        """"""\n        num_samples = size[0] * size[1]\n        samples = np.zeros((num_samples, dim))\n\n        if traverse:\n            disc_traversal = [i % dim for i in range(size[axis])]\n            for i in range(size[0]):\n                for j in range(size[1]):\n                    if axis == 0:\n                        samples[i * size[1] + j, disc_traversal[i]] = 1.\n                    else:\n                        samples[i * size[1] + j, disc_traversal[j]] = 1.\n        else:\n            # Randomly select discrete variable (i.e. sample from uniform prior)\n            if self.sample_prior:\n                samples[np.arange(num_samples), np.random.randint(0, dim, num_samples)] = 1.\n            else:\n                samples[:, 0] = 1.\n\n        return torch.Tensor(samples)'"
viz/visualize.py,10,"b'import numpy as np\nimport torch\nfrom latent_traversals import LatentTraverser\nfrom scipy import stats\nfrom torch.autograd import Variable\nfrom torchvision.utils import make_grid, save_image\n\n\nclass Visualizer():\n    def __init__(self, model):\n        """"""\n        Visualizer is used to generate images of samples, reconstructions,\n        latent traversals and so on of the trained model.\n\n        Parameters\n        ----------\n        model : jointvae.models.VAE instance\n        """"""\n        self.model = model\n        self.latent_traverser = LatentTraverser(self.model.latent_spec)\n        self.save_images = True  # If false, each method returns a tensor\n                                 # instead of saving image.\n\n    def reconstructions(self, data, size=(8, 8), filename=\'recon.png\'):\n        """"""\n        Generates reconstructions of data through the model.\n\n        Parameters\n        ----------\n        data : torch.Tensor\n            Data to be reconstructed. Shape (N, C, H, W)\n\n        size : tuple of ints\n            Size of grid on which reconstructions will be plotted. The number\n            of rows should be even, so that upper half contains true data and\n            bottom half contains reconstructions\n        """"""\n        # Plot reconstructions in test mode, i.e. without sampling from latent\n        self.model.eval()\n        # Pass data through VAE to obtain reconstruction\n        input_data = Variable(data, volatile=True)\n        if self.model.use_cuda:\n            input_data = input_data.cuda()\n        recon_data, _ = self.model(input_data)\n        self.model.train()\n\n        # Upper half of plot will contain data, bottom half will contain\n        # reconstructions\n        num_images = size[0] * size[1] / 2\n        originals = input_data[:num_images].cpu()\n        reconstructions = recon_data.view(-1, *self.model.img_size)[:num_images].cpu()\n        # If there are fewer examples given than spaces available in grid,\n        # augment with blank images\n        num_examples = originals.size()[0]\n        if num_images > num_examples:\n            blank_images = torch.zeros((num_images - num_examples,) + originals.size()[1:])\n            originals = torch.cat([originals, blank_images])\n            reconstructions = torch.cat([reconstructions, blank_images])\n\n        # Concatenate images and reconstructions\n        comparison = torch.cat([originals, reconstructions])\n\n        if self.save_images:\n            save_image(comparison.data, filename, nrow=size[0])\n        else:\n            return make_grid(comparison.data, nrow=size[0])\n\n    def samples(self, size=(8, 8), filename=\'samples.png\'):\n        """"""\n        Generates samples from learned distribution by sampling prior and\n        decoding.\n\n        size : tuple of ints\n        """"""\n        # Get prior samples from latent distribution\n        cached_sample_prior = self.latent_traverser.sample_prior\n        self.latent_traverser.sample_prior = True\n        prior_samples = self.latent_traverser.traverse_grid(size=size)\n        self.latent_traverser.sample_prior = cached_sample_prior\n\n        # Map samples through decoder\n        generated = self._decode_latents(prior_samples)\n\n        if self.save_images:\n            save_image(generated.data, filename, nrow=size[1])\n        else:\n            return make_grid(generated.data, nrow=size[1])\n\n    def latent_traversal_line(self, cont_idx=None, disc_idx=None, size=8,\n                              filename=\'traversal_line.png\'):\n        """"""\n        Generates an image traversal through a latent dimension.\n\n        Parameters\n        ----------\n        See viz.latent_traversals.LatentTraverser.traverse_line for parameter\n        documentation.\n        """"""\n        # Generate latent traversal\n        latent_samples = self.latent_traverser.traverse_line(cont_idx=cont_idx,\n                                                             disc_idx=disc_idx,\n                                                             size=size)\n\n        # Map samples through decoder\n        generated = self._decode_latents(latent_samples)\n\n        if self.save_images:\n            save_image(generated.data, filename, nrow=size)\n        else:\n            return make_grid(generated.data, nrow=size)\n\n    def latent_traversal_grid(self, cont_idx=None, cont_axis=None,\n                              disc_idx=None, disc_axis=None, size=(5, 5),\n                              filename=\'traversal_grid.png\'):\n        """"""\n        Generates a grid of image traversals through two latent dimensions.\n\n        Parameters\n        ----------\n        See viz.latent_traversals.LatentTraverser.traverse_grid for parameter\n        documentation.\n        """"""\n        # Generate latent traversal\n        latent_samples = self.latent_traverser.traverse_grid(cont_idx=cont_idx,\n                                                             cont_axis=cont_axis,\n                                                             disc_idx=disc_idx,\n                                                             disc_axis=disc_axis,\n                                                             size=size)\n\n        # Map samples through decoder\n        generated = self._decode_latents(latent_samples)\n\n        if self.save_images:\n            save_image(generated.data, filename, nrow=size[1])\n        else:\n            return make_grid(generated.data, nrow=size[1])\n\n    def all_latent_traversals(self, size=8, filename=\'all_traversals.png\'):\n        """"""\n        Traverses all latent dimensions one by one and plots a grid of images\n        where each row corresponds to a latent traversal of one latent\n        dimension.\n\n        Parameters\n        ----------\n        size : int\n            Number of samples for each latent traversal.\n        """"""\n        latent_samples = []\n\n        # Perform line traversal of every continuous and discrete latent\n        for cont_idx in range(self.model.latent_cont_dim):\n            latent_samples.append(self.latent_traverser.traverse_line(cont_idx=cont_idx,\n                                                                      disc_idx=None,\n                                                                      size=size))\n\n        for disc_idx in range(self.model.num_disc_latents):\n            latent_samples.append(self.latent_traverser.traverse_line(cont_idx=None,\n                                                                      disc_idx=disc_idx,\n                                                                      size=size))\n\n        # Decode samples\n        generated = self._decode_latents(torch.cat(latent_samples, dim=0))\n\n        if self.save_images:\n            save_image(generated.data, filename, nrow=size)\n        else:\n            return make_grid(generated.data, nrow=size)\n\n    def _decode_latents(self, latent_samples):\n        """"""\n        Decodes latent samples into images.\n\n        Parameters\n        ----------\n        latent_samples : torch.autograd.Variable\n            Samples from latent distribution. Shape (N, L) where L is dimension\n            of latent distribution.\n        """"""\n        latent_samples = Variable(latent_samples)\n        if self.model.use_cuda:\n            latent_samples = latent_samples.cuda()\n        return self.model.decode(latent_samples).cpu()\n\n\ndef reorder_img(orig_img, reorder, by_row=True, img_size=(3, 32, 32), padding=2):\n    """"""\n    Reorders rows or columns of an image grid.\n\n    Parameters\n    ----------\n    orig_img : torch.Tensor\n        Original image. Shape (channels, width, height)\n\n    reorder : list of ints\n        List corresponding to desired permutation of rows or columns\n\n    by_row : bool\n        If True reorders rows, otherwise reorders columns\n\n    img_size : tuple of ints\n        Image size following pytorch convention\n\n    padding : int\n        Number of pixels used to pad in torchvision.utils.make_grid\n    """"""\n    reordered_img = torch.zeros(orig_img.size())\n    _, height, width = img_size\n\n    for new_idx, old_idx in enumerate(reorder):\n        if by_row:\n            start_pix_new = new_idx * (padding + height) + padding\n            start_pix_old = old_idx * (padding + height) + padding\n            reordered_img[:, start_pix_new:start_pix_new + height, :] = orig_img[:, start_pix_old:start_pix_old + height, :]\n        else:\n            start_pix_new = new_idx * (padding + width) + padding\n            start_pix_old = old_idx * (padding + width) + padding\n            reordered_img[:, :, start_pix_new:start_pix_new + width] = orig_img[:, :, start_pix_old:start_pix_old + width]\n\n    return reordered_img'"
