file_path,api_count,code
CMN/train_iemocap.py,0,"b'\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom IEMOCAP.utils_cmn import *\nfrom IEMOCAP.cmn import *\nimport os\nfrom sklearn import model_selection, metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n# Desired graphics card selection\nos.environ[""CUDA_DEVICE_ORDER""]=""PCI_BUS_ID""\nos.environ[""CUDA_VISIBLE_DEVICES""]=""0""\n\n# Desired graphics card config\nsession_conf = tf.ConfigProto(\n      allow_soft_placement=True,\n      log_device_placement=False,\n      gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.7))\n\n# Select appropriate config from the file config.py\ntf.flags.DEFINE_string(""mode"", ""text"", ""which modality"")\ntf.flags.DEFINE_boolean(""context"", False, ""which kind of features to choose"")\ntf.flags.DEFINE_string(""nonlin_func"", ""tf.nn.tanh"", ""type of nonlinearity"")\ntf.flags.DEFINE_float(""learning_rate"", 0.001, ""Learning rate for SGD."")\ntf.flags.DEFINE_float(""anneal_rate"", 60, ""Number of epochs between halving the learnign rate."")\ntf.flags.DEFINE_float(""anneal_stop_epoch"", 100, ""Epoch number to end annealed lr schedule."")\ntf.flags.DEFINE_float(""max_grad_norm"", 40.0, ""Clip gradients to this norm."")\ntf.flags.DEFINE_integer(""evaluation_interval"", 1, ""Evaluate and print results every x epochs"")\ntf.flags.DEFINE_integer(""batch_size"", 512, ""Batch size for training."")\ntf.flags.DEFINE_integer(""hops"", 3, ""Number of hops in the Memory Network."")\ntf.flags.DEFINE_integer(""epochs"", 10, ""Number of epochs to train for."")\ntf.flags.DEFINE_integer(""embedding_size"", 100, ""Embedding size for embedding matrices."")\ntf.flags.DEFINE_integer(""input_dims"", None, ""Number of timesteps of the RNN"")\ntf.flags.DEFINE_integer(""timesteps"", 40, ""Number of timesteps of the RNN"")\ntf.flags.DEFINE_integer(""class_size"", None, ""No. of output classes"")\ntf.flags.DEFINE_boolean(""nonlin"", True, ""Use non linearity"")\ntf.flags.DEFINE_float(""dropout_keep_prob"", 0.3, ""Percentage of input to keep in dropout"")\n\n# Misc Parameters\ntf.flags.DEFINE_integer(""checkpoint_every"", 10, ""Save model after this many steps (default: 100)"")\ntf.flags.DEFINE_integer(""num_checkpoints"", 10, ""Number of checkpoints to store (default: 5)"")\ntf.flags.DEFINE_boolean(""allow_soft_placement"", True, ""Allow device soft device placement"")\ntf.flags.DEFINE_boolean(""log_device_placement"", False, ""Log placement of ops on devices"")\nFLAGS = tf.flags.FLAGS\n\n\ndef main():\n\n\t## Loading the train and test data\n\ttrainQueries, trainOwnHistory, trainOtherHistory, trainOwnHistoryMask, trainOtherHistoryMask, trainLabels, \\\n\t\t\tvalQueries, valOwnHistory, valOtherHistory, valOwnHistoryMask, valOtherHistoryMask, valLabels, \\\n\t        testQueries, testOwnHistory, testOtherHistory, testOwnHistoryMask, testOtherHistoryMask, testLabels = loadData(FLAGS)\n\n\t## Update FLAG parameters\n\tFLAGS.class_size = trainLabels.shape[1]\n\tFLAGS.input_dims = trainQueries.shape[1]\n\n\t## Total instances\n\tn_train = trainQueries.shape[0]  \n\tn_test = testQueries.shape[0]\n\tn_val = valQueries.shape[0]\n\tprint(""Training/Validation/Testing Size: "", n_train, n_val, n_test)\n\n\t## Calculating training batch sizes\n\tbatches = zip(range(0, n_train-FLAGS.batch_size, FLAGS.batch_size), range(FLAGS.batch_size, n_train, FLAGS.batch_size))\n\tbatches = [(start, end) for start, end in batches]\n\tbatches.append( (batches[-1][1], n_train) )\n\tevalTrainBatches = batches[:]\n\n\t## Training of the model\n\twith tf.Graph().as_default():\n\t\ttf.set_random_seed(1234) # Graph level random seed\n\n\t\tsess = tf.Session(config=session_conf) # Defining the session of the Graph\n\t\twith sess.as_default():\n\n\t\t\tmodel = CMN(FLAGS, sess)\n\n\t\t\tmax_val_accuracy = 0\n\t\t\tmax_test_acc= 0\n\t\t\tmin_val_loss = 100000\n\t\t\ttmax_val_test_accuracy = 0\n\t\t\tmax_val_test_preds = None\n\n\t\t\tfor t in range(1, FLAGS.epochs+1):\n\n\t\t\t\t# Annealing of the learning rate\n\t\t\t\tif t - 1 <= FLAGS.anneal_stop_epoch:\n\t\t\t\t\tanneal = 2.0 ** ((t - 1) // FLAGS.anneal_rate)\n\t\t\t\telse:\n\t\t\t\t\tanneal = 2.0 ** (FLAGS.anneal_stop_epoch // FLAGS.anneal_rate)\n\t\t\t\tlr = FLAGS.learning_rate / anneal\n\n\t\t\t\t# Shuffling the batches in each epoch\n\t\t\t\t# np.random.shuffle(batches)\n\n\t\t\t\ttotal_cost = 0.0\n\t\t\t\tfor start, end in batches:\n\t\t\t\t\thistOwn = trainOwnHistory[start:end]\n\t\t\t\t\thistOther = trainOtherHistory[start:end]\n\t\t\t\t\thistOwnMask = trainOwnHistoryMask[start:end]\n\t\t\t\t\thistOtherMask = trainOtherHistoryMask[start:end]\n\t\t\t\t\tmask = (histOwnMask + histOtherMask).astype(np.bool)\n\t\t\t\t\tquery = trainQueries[start:end]\n\t\t\t\t\tanswers = trainLabels[start:end]\n\t\t\t\t\t\n\t\t\t\t\tif answers.shape[0] < FLAGS.batch_size:\n\t\t\t\t\t\thistOwn = np.concatenate( (histOwn, np.zeros( (FLAGS.batch_size-histOwn.shape[0],histOwn.shape[1], histOwn.shape[2]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\thistOther = np.concatenate( (histOther, np.zeros( (FLAGS.batch_size-histOther.shape[0],histOther.shape[1], histOther.shape[2]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\thistOwnMask = np.concatenate( (histOwnMask, np.zeros( (FLAGS.batch_size-histOwnMask.shape[0],histOwn.shape[1]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\thistOtherMask = np.concatenate( (histOtherMask, np.zeros( (FLAGS.batch_size-histOtherMask.shape[0],histOwn.shape[1]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\tmask = np.concatenate( (mask, np.zeros((FLAGS.batch_size-mask.shape[0],histOwn.shape[1]) , dtype=np.bool))  , axis=0)\n\t\t\t\t\t\tquery = np.concatenate( (query, np.zeros( (FLAGS.batch_size-query.shape[0],query.shape[1]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\tanswers = np.concatenate( (answers, np.zeros( (FLAGS.batch_size-answers.shape[0],answers.shape[1]) , dtype=np.float32))  , axis=0)\n\n\t\t\t\t\tcost_t = model.batch_fit(histOwn, histOther, histOwnMask, histOtherMask, mask, query, answers, lr, FLAGS.dropout_keep_prob, training_mode=True)\n\t\t\t\t\ttotal_cost += cost_t\n\t\t\t\t\t\n\n\t\t\t\tif t % FLAGS.evaluation_interval == 0:\n\n\t\t\t\t\t## Training evaluation\n\n\t\t\t\t\ttrain_preds = []\n\t\t\t\t\tfor start, end in evalTrainBatches:\n\t\t\t\t\t\thistOwn = trainOwnHistory[start:end]\n\t\t\t\t\t\thistOther = trainOtherHistory[start:end]\n\t\t\t\t\t\thistOwnMask = trainOwnHistoryMask[start:end]\n\t\t\t\t\t\thistOtherMask = trainOtherHistoryMask[start:end]\n\t\t\t\t\t\tmask = (histOwnMask + histOtherMask).astype(np.bool)\n\t\t\t\t\t\tquery = trainQueries[start:end]\n\t\t\t\t\t\tanswers = trainLabels[start:end]\n\t\t\t\t\t\t\n\t\t\t\t\t\tif answers.shape[0] < FLAGS.batch_size:\n\t\t\t\t\t\t\thistOwn = np.concatenate( (histOwn, np.zeros( (FLAGS.batch_size-histOwn.shape[0],histOwn.shape[1], histOwn.shape[2]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\t\thistOther = np.concatenate( (histOther, np.zeros( (FLAGS.batch_size-histOther.shape[0],histOther.shape[1], histOther.shape[2]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\t\thistOwnMask = np.concatenate( (histOwnMask, np.zeros( (FLAGS.batch_size-histOwnMask.shape[0],histOwn.shape[1]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\t\thistOtherMask = np.concatenate( (histOtherMask, np.zeros( (FLAGS.batch_size-histOtherMask.shape[0],histOwn.shape[1]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\t\tmask = np.concatenate( (mask, np.zeros((FLAGS.batch_size-mask.shape[0],histOwn.shape[1]) , dtype=np.bool))  , axis=0)\n\t\t\t\t\t\t\tquery = np.concatenate( (query, np.zeros( (FLAGS.batch_size-query.shape[0],query.shape[1]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\t\tanswers = np.concatenate( (answers, np.zeros( (FLAGS.batch_size-answers.shape[0],answers.shape[1]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\tloss, pred= model.predict(histOwn, histOther, histOwnMask, histOtherMask, mask, query, FLAGS.dropout_keep_prob, answers, training_mode=False)\n\t\t\t\t\t\ttrain_preds += list(pred)\n\t\t\t\t\t\t\n\t\t\t\t\t\n\t\t\t\t\ttrain_preds = train_preds[:n_train]\n\t\t\t\t\ttrain_acc = metrics.accuracy_score(np.argmax(trainLabels, axis=1), np.array(train_preds))\n\n\t\t\t\t\tprint(total_cost, train_acc)\n\n\t\t\t\t\t\n\t\t\t\t\t## Validation evaluation\n\n\t\t\t\t\t# Creating batches for validation\n\t\t\t\t\tval_batches = zip(range(0, n_val, FLAGS.batch_size), range(FLAGS.batch_size, n_val+FLAGS.batch_size, FLAGS.batch_size))\n\t\t\t\t\tval_batches = [(start, end) for start, end in val_batches]\n\t\t\t\t\tval_preds=[]\n\t\t\t\t\tval_loss = 0.0\n\n\t\t\t\t\tfor start, end in val_batches:\n\t\t\t\t\t\thistOwn = valOwnHistory[start:end]\n\t\t\t\t\t\thistOther = valOtherHistory[start:end]\n\t\t\t\t\t\thistOwnMask = valOwnHistoryMask[start:end]\n\t\t\t\t\t\thistOtherMask = valOtherHistoryMask[start:end]\n\t\t\t\t\t\tmask = (histOwnMask + histOtherMask).astype(np.bool)\n\t\t\t\t\t\tquery = valQueries[start:end]\n\t\t\t\t\t\tanswers = valLabels[start:end]\n\t\t\t\t\t\t\n\t\t\t\t\t\tif histOwn.shape[0] < FLAGS.batch_size:\n\t\t\t\t\t\t\thistOwn = np.concatenate( (histOwn, np.zeros( (FLAGS.batch_size-histOwn.shape[0],histOwn.shape[1], histOwn.shape[2]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\t\thistOther = np.concatenate( (histOther, np.zeros( (FLAGS.batch_size-histOther.shape[0],histOther.shape[1], histOther.shape[2]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\t\thistOwnMask = np.concatenate( (histOwnMask, np.zeros( (FLAGS.batch_size-histOwnMask.shape[0],histOwn.shape[1]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\t\thistOtherMask = np.concatenate( (histOtherMask, np.zeros( (FLAGS.batch_size-histOtherMask.shape[0],histOwn.shape[1]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\t\tmask = np.concatenate( (mask, np.zeros((FLAGS.batch_size-mask.shape[0],histOwn.shape[1]) , dtype=np.bool))  , axis=0)\n\t\t\t\t\t\t\tquery = np.concatenate( (query, np.zeros( (FLAGS.batch_size-query.shape[0],query.shape[1]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\t\tanswers = np.concatenate( (answers, np.zeros( (FLAGS.batch_size-answers.shape[0],answers.shape[1]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\tloss, pred = model.predict(histOwn, histOther, histOwnMask, histOtherMask, mask, query, 1.0, answers, training_mode=False)\n\t\t\t\t\t\tval_preds += list(pred)\n\t\t\t\t\t\tval_loss += loss\n\n\t\t\t\t\tval_preds = val_preds[:n_val]\n\t\t\t\t\tval_acc = metrics.accuracy_score(np.argmax(valLabels, axis=1), val_preds)\n\n\t\t\t\t\t## Testing evaluation\n\n\t\t\t\t\t# Creating batches for testing\n\t\t\t\t\ttest_batches = zip(range(0, n_test, FLAGS.batch_size), range(FLAGS.batch_size, n_test+FLAGS.batch_size, FLAGS.batch_size))\n\t\t\t\t\ttest_batches = [(start, end) for start, end in test_batches]\n\t\t\t\t\ttest_preds=[]\n\t\t\t\t\tfor start, end in test_batches:\n\t\t\t\t\t\thistOwn = testOwnHistory[start:end]\n\t\t\t\t\t\thistOther = testOtherHistory[start:end]\n\t\t\t\t\t\thistOwnMask = testOwnHistoryMask[start:end]\n\t\t\t\t\t\thistOtherMask = testOtherHistoryMask[start:end]\n\t\t\t\t\t\tmask = (histOwnMask + histOtherMask).astype(np.bool)\n\t\t\t\t\t\tquery = testQueries[start:end]\n\t\t\t\t\t\tanswers = testLabels[start:end]\n\t\t\t\t\t\t\n\t\t\t\t\t\tif histOwn.shape[0] < FLAGS.batch_size:\n\t\t\t\t\t\t\thistOwn = np.concatenate( (histOwn, np.zeros( (FLAGS.batch_size-histOwn.shape[0],histOwn.shape[1], histOwn.shape[2]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\t\thistOther = np.concatenate( (histOther, np.zeros( (FLAGS.batch_size-histOther.shape[0],histOther.shape[1], histOther.shape[2]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\t\thistOwnMask = np.concatenate( (histOwnMask, np.zeros( (FLAGS.batch_size-histOwnMask.shape[0],histOwn.shape[1]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\t\thistOtherMask = np.concatenate( (histOtherMask, np.zeros( (FLAGS.batch_size-histOtherMask.shape[0],histOwn.shape[1]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\t\tmask = np.concatenate( (mask, np.zeros((FLAGS.batch_size-mask.shape[0],histOwn.shape[1]) , dtype=np.bool))  , axis=0)\n\t\t\t\t\t\t\tquery = np.concatenate( (query, np.zeros( (FLAGS.batch_size-query.shape[0],query.shape[1]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\t\tanswers = np.concatenate( (answers, np.zeros( (FLAGS.batch_size-answers.shape[0],answers.shape[1]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\tloss, pred = model.predict(histOwn, histOther, histOwnMask, histOtherMask, mask, query, 1.0, answers, training_mode=False)\n\t\t\t\t\t\ttest_preds += list(pred)\n\n\t\t\t\t\ttest_preds = test_preds[:n_test]\n\t\t\t\t\ttest_acc = metrics.accuracy_score(np.argmax(testLabels, axis=1), test_preds)\n\t\t\t\t\t\n\t\t\t\t\ttest_cmat = confusion_matrix(np.argmax(testLabels, axis=1), test_preds)\n\t\t\t\t\ttest_fscore = metrics.classification_report(np.argmax(testLabels, axis=1), test_preds, digits=3)\n\n\n\t\t\t\t\tprint(\'-----------------------\')\n\t\t\t\t\tprint(\'Epoch\', t)\n\t\t\t\t\tprint(\'Total Cost:\', total_cost, \', Training Accuracy:\', train_acc, \', Validation Accuracy:\', val_acc,\\\n\t\t\t\t\t\'Validation Loss:\', val_loss)\n\t\t\t\t\tprint(\'-----------------------\')\n\n\t\t\t\t\tif val_loss < min_val_loss:\n\t\t\t\t\t\tmax_val_acc = val_acc\n\t\t\t\t\t\tmax_test_acc = test_acc\n\t\t\t\t\t\tmax_test_preds = test_preds\n\t\t\t\t\t\tmax_fscore = test_fscore\n\t\t\t\t\t\tmax_test_cmat = test_cmat\n\t\t\t\t\t\tmin_val_loss = val_loss\n\n\tprint(""Final metrics:"")\n\tprint(""confusion_matrix (test): "")\n\tprint(max_test_cmat)\n\tprint(""val accuracy: "", max_val_acc, "" test accuracy: "", max_test_acc)\n\tprint(""classification report: "")\n\tprint(max_fscore)\n\treturn max_test_acc\n\nif __name__ == ""__main__"":\n\tmain()'"
DialogueGCN/dataloader.py,27,"b""import torch\nfrom torch.utils.data import Dataset\nfrom torch.nn.utils.rnn import pad_sequence\nimport pickle, pandas as pd\n\nclass IEMOCAPDataset(Dataset):\n\n    def __init__(self, train=True):\n        self.videoIDs, self.videoSpeakers, self.videoLabels, self.videoText,\\\n        self.videoAudio, self.videoVisual, self.videoSentence, self.trainVid,\\\n        self.testVid = pickle.load(open('./IEMOCAP_features/IEMOCAP_features.pkl', 'rb'), encoding='latin1')\n        '''\n        label index mapping = {'hap':0, 'sad':1, 'neu':2, 'ang':3, 'exc':4, 'fru':5}\n        '''\n        self.keys = [x for x in (self.trainVid if train else self.testVid)]\n\n        self.len = len(self.keys)\n\n    def __getitem__(self, index):\n        vid = self.keys[index]\n        return torch.FloatTensor(self.videoText[vid]),\\\n               torch.FloatTensor(self.videoVisual[vid]),\\\n               torch.FloatTensor(self.videoAudio[vid]),\\\n               torch.FloatTensor([[1,0] if x=='M' else [0,1] for x in\\\n                                  self.videoSpeakers[vid]]),\\\n               torch.FloatTensor([1]*len(self.videoLabels[vid])),\\\n               torch.LongTensor(self.videoLabels[vid]),\\\n               vid\n\n    def __len__(self):\n        return self.len\n\n    def collate_fn(self, data):\n        dat = pd.DataFrame(data)\n        return [pad_sequence(dat[i]) if i<4 else pad_sequence(dat[i], True) if i<6 else dat[i].tolist() for i in dat]\n\n\nclass AVECDataset(Dataset):\n\n    def __init__(self, path, train=True):\n        self.videoIDs, self.videoSpeakers, self.videoLabels, self.videoText,\\\n            self.videoAudio, self.videoVisual, self.videoSentence,\\\n            self.trainVid, self.testVid = pickle.load(open(path, 'rb'),encoding='latin1')\n\n        self.keys = [x for x in (self.trainVid if train else self.testVid)]\n\n        self.len = len(self.keys)\n\n    def __getitem__(self, index):\n        vid = self.keys[index]\n        return torch.FloatTensor(self.videoText[vid]),\\\n               torch.FloatTensor(self.videoVisual[vid]),\\\n               torch.FloatTensor(self.videoAudio[vid]),\\\n               torch.FloatTensor([[1,0] if x=='user' else [0,1] for x in\\\n                                  self.videoSpeakers[vid]]),\\\n               torch.FloatTensor([1]*len(self.videoLabels[vid])),\\\n               torch.FloatTensor(self.videoLabels[vid])\n\n    def __len__(self):\n        return self.len\n\n    def collate_fn(self, data):\n        dat = pd.DataFrame(data)\n        return [pad_sequence(dat[i]) if i<4 else pad_sequence(dat[i], True) for i in dat]\n\n\nclass MELDDataset(Dataset):\n\n    def __init__(self, path, classify, train=True):\n        self.videoIDs, self.videoSpeakers, self.videoLabelsEmotion, self.videoText,\\\n        self.videoAudio, self.videoSentence, self.trainVid,\\\n        self.testVid, self.videoLabelsSentiment = pickle.load(open(path, 'rb'))\n\n        if classify == 'emotion':\n            self.videoLabels = self.videoLabelsEmotion\n        else:\n            self.videoLabels = self.videoLabelsSentiment\n        '''\n        label index mapping = {'neutral': 0, 'surprise': 1, 'fear': 2, 'sadness': 3, 'joy': 4, 'disgust': 5, 'anger':6}\n        '''\n        self.keys = [x for x in (self.trainVid if train else self.testVid)]\n\n        self.len = len(self.keys)\n\n    def __getitem__(self, index):\n        vid = self.keys[index]\n        return torch.FloatTensor(self.videoText[vid]),\\\n               torch.FloatTensor(self.videoAudio[vid]),\\\n               torch.FloatTensor(self.videoSpeakers[vid]),\\\n               torch.FloatTensor([1]*len(self.videoLabels[vid])),\\\n               torch.LongTensor(self.videoLabels[vid]),\\\n               vid\n\n    def __len__(self):\n        return self.len\n\n    def collate_fn(self, data):\n        dat = pd.DataFrame(data)\n        return [pad_sequence(dat[i]) if i<3 else pad_sequence(dat[i], True) if i<5 else dat[i].tolist() for i in dat]\n\n\nclass DailyDialogueDataset(Dataset):\n\n    def __init__(self, split, path):\n        \n        self.Speakers, self.Features, \\\n        self.ActLabels, self.EmotionLabels, self.trainId, self.testId, self.validId = pickle.load(open(path, 'rb'))\n        \n        if split == 'train':\n            self.keys = [x for x in self.trainId]\n        elif split == 'test':\n            self.keys = [x for x in self.testId]\n        elif split == 'valid':\n            self.keys = [x for x in self.validId]\n\n        self.len = len(self.keys)\n\n    def __getitem__(self, index):\n        conv = self.keys[index]\n        \n        return  torch.FloatTensor(self.Features[conv]), \\\n                torch.FloatTensor([[1,0] if x=='0' else [0,1] for x in self.Speakers[conv]]),\\\n                torch.FloatTensor([1]*len(self.EmotionLabels[conv])), \\\n                torch.LongTensor(self.EmotionLabels[conv]), \\\n                conv\n\n    def __len__(self):\n        return self.len\n    \n    def collate_fn(self, data):\n        dat = pd.DataFrame(data)\n        return [pad_sequence(dat[i]) if i<2 else pad_sequence(dat[i], True) if i<4 else dat[i].tolist() for i in dat]\n\n\nclass DailyDialogueDataset2(Dataset):\n\n    def __init__(self, split, path):\n\n        self.Speakers, self.Features, _, \\\n        self.ActLabels, self.EmotionLabels, self.trainId, self.testId, self.validId = pickle.load(open(path, 'rb'))\n\n        if split == 'train':\n            self.keys = [x for x in self.trainId]\n        elif split == 'test':\n            self.keys = [x for x in self.testId]\n        elif split == 'valid':\n            self.keys = [x for x in self.validId]\n\n        self.len = len(self.keys)\n\n    def __getitem__(self, index):\n        conv = self.keys[index]\n\n        return torch.FloatTensor(self.Features[conv]), \\\n               torch.FloatTensor([[1, 0] if x == '0' else [0, 1] for x in self.Speakers[conv]]), \\\n               torch.FloatTensor([1] * len(self.EmotionLabels[conv])), \\\n               torch.LongTensor(self.EmotionLabels[conv]), \\\n               conv\n\n    def __len__(self):\n        return self.len\n\n    def collate_fn(self, data):\n        dat = pd.DataFrame(data)\n\n        return [pad_sequence(dat[i]) if i < 2 else pad_sequence(dat[i], True) if i < 4 else dat[i].tolist() for i in\n                dat]\n"""
DialogueGCN/model.py,95,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch_geometric.nn import RGCNConv, GraphConv\nimport numpy as np, itertools, random, copy, math\n\n# For methods and models related to DialogueGCN jump to line 516\n\nclass MaskedNLLLoss(nn.Module):\n\n    def __init__(self, weight=None):\n        super(MaskedNLLLoss, self).__init__()\n        self.weight = weight\n        self.loss = nn.NLLLoss(weight=weight,\n                               reduction=\'sum\')\n\n    def forward(self, pred, target, mask):\n        """"""\n        pred -> batch*seq_len, n_classes\n        target -> batch*seq_len\n        mask -> batch, seq_len\n        """"""\n        mask_ = mask.view(-1,1) # batch*seq_len, 1\n        if type(self.weight)==type(None):\n            loss = self.loss(pred*mask_, target)/torch.sum(mask)\n        else:\n            loss = self.loss(pred*mask_, target)\\\n                            /torch.sum(self.weight[target]*mask_.squeeze())\n        return loss\n\n\nclass MaskedMSELoss(nn.Module):\n\n    def __init__(self):\n        super(MaskedMSELoss, self).__init__()\n        self.loss = nn.MSELoss(reduction=\'sum\')\n\n    def forward(self, pred, target, mask):\n        """"""\n        pred -> batch*seq_len\n        target -> batch*seq_len\n        mask -> batch*seq_len\n        """"""\n        loss = self.loss(pred*mask, target)/torch.sum(mask)\n        return loss\n\n\nclass UnMaskedWeightedNLLLoss(nn.Module):\n\n    def __init__(self, weight=None):\n        super(UnMaskedWeightedNLLLoss, self).__init__()\n        self.weight = weight\n        self.loss = nn.NLLLoss(weight=weight,\n                               reduction=\'sum\')\n\n    def forward(self, pred, target):\n        """"""\n        pred -> batch*seq_len, n_classes\n        target -> batch*seq_len\n        """"""\n        if type(self.weight)==type(None):\n            loss = self.loss(pred, target)\n        else:\n            loss = self.loss(pred, target)\\\n                            /torch.sum(self.weight[target])\n        return loss\n\n\nclass SimpleAttention(nn.Module):\n\n    def __init__(self, input_dim):\n        super(SimpleAttention, self).__init__()\n        self.input_dim = input_dim\n        self.scalar = nn.Linear(self.input_dim,1,bias=False)\n\n    def forward(self, M, x=None):\n        """"""\n        M -> (seq_len, batch, vector)\n        x -> dummy argument for the compatibility with MatchingAttention\n        """"""\n        scale = self.scalar(M) # seq_len, batch, 1\n        alpha = F.softmax(scale, dim=0).permute(1,2,0) # batch, 1, seq_len\n        attn_pool = torch.bmm(alpha, M.transpose(0,1))[:,0,:] # batch, vector\n        return attn_pool, alpha\n\n\nclass MatchingAttention(nn.Module):\n\n    def __init__(self, mem_dim, cand_dim, alpha_dim=None, att_type=\'general\'):\n        super(MatchingAttention, self).__init__()\n        assert att_type!=\'concat\' or alpha_dim!=None\n        assert att_type!=\'dot\' or mem_dim==cand_dim\n        self.mem_dim = mem_dim\n        self.cand_dim = cand_dim\n        self.att_type = att_type\n        if att_type==\'general\':\n            self.transform = nn.Linear(cand_dim, mem_dim, bias=False)\n        if att_type==\'general2\':\n            self.transform = nn.Linear(cand_dim, mem_dim, bias=True)\n            #torch.nn.init.normal_(self.transform.weight,std=0.01)\n        elif att_type==\'concat\':\n            self.transform = nn.Linear(cand_dim+mem_dim, alpha_dim, bias=False)\n            self.vector_prod = nn.Linear(alpha_dim, 1, bias=False)\n\n    def forward(self, M, x, mask=None):\n        """"""\n        M -> (seq_len, batch, mem_dim)\n        x -> (batch, cand_dim)\n        mask -> (batch, seq_len)\n        """"""\n        if type(mask)==type(None):\n            mask = torch.ones(M.size(1), M.size(0)).type(M.type())\n\n        if self.att_type==\'dot\':\n            # vector = cand_dim = mem_dim\n            M_ = M.permute(1,2,0) # batch, vector, seqlen\n            x_ = x.unsqueeze(1) # batch, 1, vector\n            alpha = F.softmax(torch.bmm(x_, M_), dim=2) # batch, 1, seqlen\n        elif self.att_type==\'general\':\n            M_ = M.permute(1,2,0) # batch, mem_dim, seqlen\n            x_ = self.transform(x).unsqueeze(1) # batch, 1, mem_dim\n            alpha = F.softmax(torch.bmm(x_, M_), dim=2) # batch, 1, seqlen\n        elif self.att_type==\'general2\':\n            M_ = M.permute(1,2,0) # batch, mem_dim, seqlen\n            x_ = self.transform(x).unsqueeze(1) # batch, 1, mem_dim\n            mask_ = mask.unsqueeze(2).repeat(1, 1, self.mem_dim).transpose(1, 2) # batch, seq_len, mem_dim\n            M_ = M_ * mask_\n            alpha_ = torch.bmm(x_, M_)*mask.unsqueeze(1)\n            alpha_ = torch.tanh(alpha_)\n            alpha_ = F.softmax(alpha_, dim=2)\n            # alpha_ = F.softmax((torch.bmm(x_, M_))*mask.unsqueeze(1), dim=2) # batch, 1, seqlen\n            alpha_masked = alpha_*mask.unsqueeze(1) # batch, 1, seqlen\n            alpha_sum = torch.sum(alpha_masked, dim=2, keepdim=True) # batch, 1, 1\n            alpha = alpha_masked/alpha_sum # batch, 1, 1 ; normalized\n            #import ipdb;ipdb.set_trace()\n        else:\n            M_ = M.transpose(0,1) # batch, seqlen, mem_dim\n            x_ = x.unsqueeze(1).expand(-1,M.size()[0],-1) # batch, seqlen, cand_dim\n            M_x_ = torch.cat([M_,x_],2) # batch, seqlen, mem_dim+cand_dim\n            mx_a = F.tanh(self.transform(M_x_)) # batch, seqlen, alpha_dim\n            alpha = F.softmax(self.vector_prod(mx_a),1).transpose(1,2) # batch, 1, seqlen\n\n        attn_pool = torch.bmm(alpha, M.transpose(0,1))[:,0,:] # batch, mem_dim\n        return attn_pool, alpha\n\n\nclass Attention(nn.Module):\n    def __init__(self, embed_dim, hidden_dim=None, out_dim=None, n_head=1, score_function=\'dot_product\', dropout=0):\n        \'\'\' Attention Mechanism\n        :param embed_dim:\n        :param hidden_dim:\n        :param out_dim:\n        :param n_head: num of head (Multi-Head Attention)\n        :param score_function: scaled_dot_product / mlp (concat) / bi_linear (general dot)\n        :return (?, q_len, out_dim,)\n        \'\'\'\n        super(Attention, self).__init__()\n        if hidden_dim is None:\n            hidden_dim = embed_dim // n_head\n        if out_dim is None:\n            out_dim = embed_dim\n        self.embed_dim = embed_dim\n        self.hidden_dim = hidden_dim\n        self.n_head = n_head\n        self.score_function = score_function\n        self.w_k = nn.Linear(embed_dim, n_head * hidden_dim)\n        self.w_q = nn.Linear(embed_dim, n_head * hidden_dim)\n        self.proj = nn.Linear(n_head * hidden_dim, out_dim)\n        self.dropout = nn.Dropout(dropout)\n        if score_function == \'mlp\':\n            self.weight = nn.Parameter(torch.Tensor(hidden_dim*2))\n        elif self.score_function == \'bi_linear\':\n            self.weight = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n        else:  # dot_product / scaled_dot_product\n            self.register_parameter(\'weight\', None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.hidden_dim)\n        if self.weight is not None:\n            self.weight.data.uniform_(-stdv, stdv)\n\n    def forward(self, k, q):\n        if len(q.shape) == 2:  # q_len missing\n            q = torch.unsqueeze(q, dim=1)\n        if len(k.shape) == 2:  # k_len missing\n            k = torch.unsqueeze(k, dim=1)\n        mb_size = k.shape[0]  # ?\n        k_len = k.shape[1]\n        q_len = q.shape[1]\n        # k: (?, k_len, embed_dim,)\n        # q: (?, q_len, embed_dim,)\n        # kx: (n_head*?, k_len, hidden_dim)\n        # qx: (n_head*?, q_len, hidden_dim)\n        # score: (n_head*?, q_len, k_len,)\n        # output: (?, q_len, out_dim,)\n        kx = self.w_k(k).view(mb_size, k_len, self.n_head, self.hidden_dim)\n        kx = kx.permute(2, 0, 1, 3).contiguous().view(-1, k_len, self.hidden_dim)\n        qx = self.w_q(q).view(mb_size, q_len, self.n_head, self.hidden_dim)\n        qx = qx.permute(2, 0, 1, 3).contiguous().view(-1, q_len, self.hidden_dim)\n        if self.score_function == \'dot_product\':\n            kt = kx.permute(0, 2, 1)\n            score = torch.bmm(qx, kt)\n        elif self.score_function == \'scaled_dot_product\':\n            kt = kx.permute(0, 2, 1)\n            qkt = torch.bmm(qx, kt)\n            score = torch.div(qkt, math.sqrt(self.hidden_dim))\n        elif self.score_function == \'mlp\':\n            kxx = torch.unsqueeze(kx, dim=1).expand(-1, q_len, -1, -1)\n            qxx = torch.unsqueeze(qx, dim=2).expand(-1, -1, k_len, -1)\n            kq = torch.cat((kxx, qxx), dim=-1)  # (n_head*?, q_len, k_len, hidden_dim*2)\n            # kq = torch.unsqueeze(kx, dim=1) + torch.unsqueeze(qx, dim=2)\n            score = torch.tanh(torch.matmul(kq, self.weight))\n        elif self.score_function == \'bi_linear\':\n            qw = torch.matmul(qx, self.weight)\n            kt = kx.permute(0, 2, 1)\n            score = torch.bmm(qw, kt)\n        else:\n            raise RuntimeError(\'invalid score_function\')\n        #score = F.softmax(score, dim=-1)\n        score = F.softmax(score, dim=0)\n        # print (score)\n        # print (sum(score))\n        output = torch.bmm(score, kx)  # (n_head*?, q_len, hidden_dim)\n        output = torch.cat(torch.split(output, mb_size, dim=0), dim=-1)  # (?, q_len, n_head*hidden_dim)\n        output = self.proj(output)  # (?, q_len, out_dim)\n        output = self.dropout(output)\n        return output, score\n\n\nclass DialogueRNNCell(nn.Module):\n\n    def __init__(self, D_m, D_g, D_p, D_e, listener_state=False,\n                            context_attention=\'simple\', D_a=100, dropout=0.5):\n        super(DialogueRNNCell, self).__init__()\n\n        self.D_m = D_m\n        self.D_g = D_g\n        self.D_p = D_p\n        self.D_e = D_e\n\n        self.listener_state = listener_state\n        self.g_cell = nn.GRUCell(D_m+D_p,D_g)\n        self.p_cell = nn.GRUCell(D_m+D_g,D_p)\n        self.e_cell = nn.GRUCell(D_p,D_e)\n        if listener_state:\n            self.l_cell = nn.GRUCell(D_m+D_p,D_p)\n\n        self.dropout = nn.Dropout(dropout)\n\n        if context_attention==\'simple\':\n            self.attention = SimpleAttention(D_g)\n        else:\n            self.attention = MatchingAttention(D_g, D_m, D_a, context_attention)\n\n    def _select_parties(self, X, indices):\n        q0_sel = []\n        for idx, j in zip(indices, X):\n            q0_sel.append(j[idx].unsqueeze(0))\n        q0_sel = torch.cat(q0_sel,0)\n        return q0_sel\n\n    def forward(self, U, qmask, g_hist, q0, e0):\n        """"""\n        U -> batch, D_m\n        qmask -> batch, party\n        g_hist -> t-1, batch, D_g\n        q0 -> batch, party, D_p\n        e0 -> batch, self.D_e\n        """"""\n        qm_idx = torch.argmax(qmask, 1)\n        q0_sel = self._select_parties(q0, qm_idx)\n\n        g_ = self.g_cell(torch.cat([U,q0_sel], dim=1),\n                torch.zeros(U.size()[0],self.D_g).type(U.type()) if g_hist.size()[0]==0 else\n                g_hist[-1])\n        g_ = self.dropout(g_)\n        if g_hist.size()[0]==0:\n            c_ = torch.zeros(U.size()[0],self.D_g).type(U.type())\n            alpha = None\n        else:\n            c_, alpha = self.attention(g_hist,U)\n        # c_ = torch.zeros(U.size()[0],self.D_g).type(U.type()) if g_hist.size()[0]==0\\\n        #         else self.attention(g_hist,U)[0] # batch, D_g\n        U_c_ = torch.cat([U,c_], dim=1).unsqueeze(1).expand(-1,qmask.size()[1],-1)\n        qs_ = self.p_cell(U_c_.contiguous().view(-1,self.D_m+self.D_g),\n                q0.view(-1, self.D_p)).view(U.size()[0],-1,self.D_p)\n        qs_ = self.dropout(qs_)\n\n        if self.listener_state:\n            U_ = U.unsqueeze(1).expand(-1,qmask.size()[1],-1).contiguous().view(-1,self.D_m)\n            ss_ = self._select_parties(qs_, qm_idx).unsqueeze(1).\\\n                    expand(-1,qmask.size()[1],-1).contiguous().view(-1,self.D_p)\n            U_ss_ = torch.cat([U_,ss_],1)\n            ql_ = self.l_cell(U_ss_,q0.view(-1, self.D_p)).view(U.size()[0],-1,self.D_p)\n            ql_ = self.dropout(ql_)\n        else:\n            ql_ = q0\n        qmask_ = qmask.unsqueeze(2)\n        q_ = ql_*(1-qmask_) + qs_*qmask_\n        e0 = torch.zeros(qmask.size()[0], self.D_e).type(U.type()) if e0.size()[0]==0\\\n                else e0\n        e_ = self.e_cell(self._select_parties(q_,qm_idx), e0)\n        e_ = self.dropout(e_)\n        return g_,q_,e_,alpha\n\n\nclass DialogueRNN(nn.Module):\n\n    def __init__(self, D_m, D_g, D_p, D_e, listener_state=False,\n                            context_attention=\'simple\', D_a=100, dropout=0.5):\n        super(DialogueRNN, self).__init__()\n\n        self.D_m = D_m\n        self.D_g = D_g\n        self.D_p = D_p\n        self.D_e = D_e\n        self.dropout = nn.Dropout(dropout)\n\n        self.dialogue_cell = DialogueRNNCell(D_m, D_g, D_p, D_e,\n                            listener_state, context_attention, D_a, dropout)\n\n    def forward(self, U, qmask):\n        """"""\n        U -> seq_len, batch, D_m\n        qmask -> seq_len, batch, party\n        """"""\n\n        g_hist = torch.zeros(0).type(U.type()) # 0-dimensional tensor\n        q_ = torch.zeros(qmask.size()[1], qmask.size()[2],\n                                    self.D_p).type(U.type()) # batch, party, D_p\n        e_ = torch.zeros(0).type(U.type()) # batch, D_e\n        e = e_\n\n        alpha = []\n        for u_,qmask_ in zip(U, qmask):\n            g_, q_, e_, alpha_ = self.dialogue_cell(u_, qmask_, g_hist, q_, e_)\n            g_hist = torch.cat([g_hist, g_.unsqueeze(0)],0)\n            e = torch.cat([e, e_.unsqueeze(0)],0)\n            if type(alpha_)!=type(None):\n                alpha.append(alpha_[:,0,:])\n\n        return e,alpha # seq_len, batch, D_e\n\n\nclass GRUModel(nn.Module):\n\n    def __init__(self, D_m, D_e, D_h, n_classes=7, dropout=0.5):\n        \n        super(GRUModel, self).__init__()\n        \n        self.n_classes = n_classes\n        self.dropout   = nn.Dropout(dropout)\n        self.gru = nn.GRU(input_size=D_m, hidden_size=D_e, num_layers=2, bidirectional=True, dropout=dropout)\n        self.matchatt = MatchingAttention(2*D_e, 2*D_e, att_type=\'general2\')\n        self.linear = nn.Linear(2*D_e, D_h)\n        self.smax_fc = nn.Linear(D_h, n_classes)\n        \n    def forward(self, U, qmask, umask, att2=True):\n        """"""\n        U -> seq_len, batch, D_m\n        qmask -> seq_len, batch, party\n        """"""\n        emotions, hidden = self.gru(U)\n        alpha, alpha_f, alpha_b = [], [], []\n        \n        if att2:\n            att_emotions = []\n            alpha = []\n            for t in emotions:\n                att_em, alpha_ = self.matchatt(emotions,t,mask=umask)\n                att_emotions.append(att_em.unsqueeze(0))\n                alpha.append(alpha_[:,0,:])\n            att_emotions = torch.cat(att_emotions,dim=0)\n            hidden = F.relu(self.linear(att_emotions))\n        else:\n            hidden = F.relu(self.linear(emotions))\n        \n        # hidden = F.relu(self.linear(emotions))\n        hidden = self.dropout(hidden)\n        log_prob = F.log_softmax(self.smax_fc(hidden), 2)\n        return log_prob, alpha, alpha_f, alpha_b, emotions\n\n\nclass LSTMModel(nn.Module):\n\n    def __init__(self, D_m, D_e, D_h, n_classes=7, dropout=0.5):\n        \n        super(LSTMModel, self).__init__()\n        \n        self.n_classes = n_classes\n        self.dropout   = nn.Dropout(dropout)\n        self.lstm = nn.LSTM(input_size=D_m, hidden_size=D_e, num_layers=2, bidirectional=True, dropout=dropout)\n        self.matchatt = MatchingAttention(2*D_e, 2*D_e, att_type=\'general2\')\n        self.linear = nn.Linear(2*D_e, D_h)\n        self.smax_fc = nn.Linear(D_h, n_classes)\n\n    def forward(self, U, qmask, umask, att2=True):\n        """"""\n        U -> seq_len, batch, D_m\n        qmask -> seq_len, batch, party\n        """"""\n        emotions, hidden = self.lstm(U)\n        alpha, alpha_f, alpha_b = [], [], []\n        \n        if att2:\n            att_emotions = []\n            alpha = []\n            for t in emotions:\n                att_em, alpha_ = self.matchatt(emotions,t,mask=umask)\n                att_emotions.append(att_em.unsqueeze(0))\n                alpha.append(alpha_[:,0,:])\n            att_emotions = torch.cat(att_emotions,dim=0)\n            hidden = F.relu(self.linear(att_emotions))\n        else:\n            hidden = F.relu(self.linear(emotions))\n        \n        # hidden = F.relu(self.linear(emotions))\n        hidden = self.dropout(hidden)\n        log_prob = F.log_softmax(self.smax_fc(hidden), 2)\n        return log_prob, alpha, alpha_f, alpha_b, emotions\n\n\nclass DialogRNNModel(nn.Module):\n\n    def __init__(self, D_m, D_g, D_p, D_e, D_h, D_a=100, n_classes=7, listener_state=False, \n        context_attention=\'simple\', dropout_rec=0.5, dropout=0.5):\n\n        super(DialogRNNModel, self).__init__()\n\n        self.dropout   = nn.Dropout(dropout)\n        self.dropout_rec = nn.Dropout(dropout+0.15)\n        self.dialog_rnn_f = DialogueRNN(D_m, D_g, D_p, D_e,listener_state,\n                                    context_attention, D_a, dropout_rec)\n        self.dialog_rnn_r = DialogueRNN(D_m, D_g, D_p, D_e,listener_state,\n                                    context_attention, D_a, dropout_rec)\n        self.matchatt = MatchingAttention(2*D_e,2*D_e,att_type=\'general2\')\n        self.linear     = nn.Linear(2*D_e, D_h)\n        self.smax_fc    = nn.Linear(D_h, n_classes)\n\n    def _reverse_seq(self, X, mask):\n        """"""\n        X -> seq_len, batch, dim\n        mask -> batch, seq_len\n        """"""\n        X_ = X.transpose(0,1)\n        mask_sum = torch.sum(mask, 1).int()\n\n        xfs = []\n        for x, c in zip(X_, mask_sum):\n            xf = torch.flip(x[:c], [0])\n            xfs.append(xf)\n        return pad_sequence(xfs)\n\n    def forward(self, U, qmask, umask,att2=True):\n        """"""\n        U -> seq_len, batch, D_m\n        qmask -> seq_len, batch, party\n        """"""\n        emotions_f, alpha_f = self.dialog_rnn_f(U, qmask) # seq_len, batch, D_e\n        emotions_f = self.dropout_rec(emotions_f)\n        rev_U = self._reverse_seq(U, umask)\n        rev_qmask = self._reverse_seq(qmask, umask)\n        emotions_b, alpha_b = self.dialog_rnn_r(rev_U, rev_qmask)\n        emotions_b = self._reverse_seq(emotions_b, umask)\n        emotions_b = self.dropout_rec(emotions_b)\n        emotions = torch.cat([emotions_f,emotions_b],dim=-1)\n        # alpha, alpha_f, alpha_b = [], [], []\n        if att2:\n            att_emotions = []\n            alpha = []\n            for t in emotions:\n                att_em, alpha_ = self.matchatt(emotions,t,mask=umask)\n                att_emotions.append(att_em.unsqueeze(0))\n                alpha.append(alpha_[:,0,:])\n            att_emotions = torch.cat(att_emotions,dim=0)\n            hidden = F.relu(self.linear(att_emotions))\n        else:\n            hidden = F.relu(self.linear(emotions))\n        # hidden = F.relu(self.linear(emotions))\n        hidden = self.dropout(hidden)\n        log_prob = F.log_softmax(self.smax_fc(hidden), 2) # seq_len, batch, n_classes\n        return log_prob, alpha, alpha_f, alpha_b, emotions\n\n\nclass AVECDialogRNNModel(nn.Module):\n\n    def __init__(self, D_m, D_g, D_p, D_e, D_h, D_a=100, n_classes=7, listener_state=False, \n        context_attention=\'simple\', dropout_rec=0.5, dropout=0.5):\n\n        super(AVECDialogRNNModel, self).__init__()\n\n        self.dropout   = nn.Dropout(dropout)\n        self.dropout_rec = nn.Dropout(dropout)\n        self.dialog_rnn = DialogueRNN(D_m, D_g, D_p, D_e,listener_state,\n                                    context_attention, D_a, dropout_rec)\n\n        self.linear     = nn.Linear(D_e, D_h)\n        self.smax_fc    = nn.Linear(D_h, n_classes)\n\n    def forward(self, U, qmask, umask, att2=True):\n        """"""\n        U -> seq_len, batch, D_m\n        qmask -> seq_len, batch, party\n        """"""\n        emotions, _ = self.dialog_rnn(U, qmask) # seq_len, batch, D_e\n        emotions = self.dropout_rec(emotions)\n        hidden = torch.tanh(self.linear(emotions))\n        hidden = self.dropout(hidden)\n        pred = (self.smax_fc(hidden).squeeze()) # seq_len, batch\n        return pred.transpose(0,1).contiguous().view(-1), [], [], [], emotions\n\n\nclass MaskedEdgeAttention(nn.Module):\n\n    def __init__(self, input_dim, max_seq_len, no_cuda):\n        """"""\n        Method to compute the edge weights, as in Equation 1. in the paper. \n        attn_type = \'attn1\' refers to the equation in the paper.\n        For slightly different attention mechanisms refer to attn_type = \'attn2\' or attn_type = \'attn3\'\n        """"""\n\n        super(MaskedEdgeAttention, self).__init__()\n        \n        self.input_dim = input_dim\n        self.max_seq_len = max_seq_len\n        self.scalar = nn.Linear(self.input_dim, self.max_seq_len, bias=False)\n        self.matchatt = MatchingAttention(self.input_dim, self.input_dim, att_type=\'general2\')\n        self.simpleatt = SimpleAttention(self.input_dim)\n        self.att = Attention(self.input_dim, score_function=\'mlp\')\n        self.no_cuda = no_cuda\n\n    def forward(self, M, lengths, edge_ind):\n        """"""\n        M -> (seq_len, batch, vector)\n        lengths -> length of the sequences in the batch\n        """"""\n        attn_type = \'attn1\'\n\n        if attn_type == \'attn1\':\n\n            scale = self.scalar(M)\n            # scale = torch.tanh(scale)\n            alpha = F.softmax(scale, dim=0).permute(1, 2, 0)\n            \n            #if torch.cuda.is_available():\n            if not self.no_cuda:\n                mask = Variable(torch.ones(alpha.size()) * 1e-10).detach().cuda()\n                mask_copy = Variable(torch.zeros(alpha.size())).detach().cuda()\n                \n            else:\n                mask = Variable(torch.ones(alpha.size()) * 1e-10).detach()\n                mask_copy = Variable(torch.zeros(alpha.size())).detach()\n            \n            edge_ind_ = []\n            for i, j in enumerate(edge_ind):\n                for x in j:\n                    edge_ind_.append([i, x[0], x[1]])\n            \n            edge_ind_ = np.array(edge_ind_).transpose()\n            mask[edge_ind_] = 1\n            mask_copy[edge_ind_] = 1\n            masked_alpha = alpha * mask\n            _sums = masked_alpha.sum(-1, keepdim=True)\n            scores = masked_alpha.div(_sums) * mask_copy\n            return scores\n\n        elif attn_type == \'attn2\':\n            scores = torch.zeros(M.size(1), self.max_seq_len, self.max_seq_len, requires_grad=True)\n\n            # if torch.cuda.is_available():\n            if not self.no_cuda:\n                scores = scores.cuda()\n\n\n            for j in range(M.size(1)):\n            \n                ei = np.array(edge_ind[j])\n\n                for node in range(lengths[j]):\n                \n                    neighbour = ei[ei[:, 0] == node, 1]\n\n                    M_ = M[neighbour, j, :].unsqueeze(1)\n                    t = M[node, j, :].unsqueeze(0)\n                    _, alpha_ = self.simpleatt(M_, t)\n                    scores[j, node, neighbour] = alpha_\n\n        elif attn_type == \'attn3\':\n            scores = torch.zeros(M.size(1), self.max_seq_len, self.max_seq_len, requires_grad=True)\n\n            #if torch.cuda.is_available():\n            if not self.no_cuda:\n                scores = scores.cuda()\n\n            for j in range(M.size(1)):\n\n                ei = np.array(edge_ind[j])\n\n                for node in range(lengths[j]):\n\n                    neighbour = ei[ei[:, 0] == node, 1]\n\n                    M_ = M[neighbour, j, :].unsqueeze(1).transpose(0, 1)\n                    t = M[node, j, :].unsqueeze(0).unsqueeze(0).repeat(len(neighbour), 1, 1).transpose(0, 1)\n                    _, alpha_ = self.att(M_, t)\n                    scores[j, node, neighbour] = alpha_[0, :, 0]\n\n        return scores\n\n\ndef pad(tensor, length, no_cuda):\n    if isinstance(tensor, Variable):\n        var = tensor\n        if length > var.size(0):\n            #if torch.cuda.is_available():\n            if not no_cuda:\n                return torch.cat([var, torch.zeros(length - var.size(0), *var.size()[1:]).cuda()])\n            else:\n                return torch.cat([var, torch.zeros(length - var.size(0), *var.size()[1:])])\n        else:\n            return var\n    else:\n        if length > tensor.size(0):\n            #if torch.cuda.is_available():\n            if not no_cuda:\n                return torch.cat([tensor, torch.zeros(length - tensor.size(0), *tensor.size()[1:]).cuda()])\n            else:\n                return torch.cat([tensor, torch.zeros(length - tensor.size(0), *tensor.size()[1:])])\n        else:\n            return tensor\n\n\ndef edge_perms(l, window_past, window_future):\n    """"""\n    Method to construct the edges considering the past and future window.\n    """"""\n\n    all_perms = set()\n    array = np.arange(l)\n    for j in range(l):\n        perms = set()\n        \n        if window_past == -1 and window_future == -1:\n            eff_array = array\n        elif window_past == -1:\n            eff_array = array[:min(l, j+window_future+1)]\n        elif window_future == -1:\n            eff_array = array[max(0, j-window_past):]\n        else:\n            eff_array = array[max(0, j-window_past):min(l, j+window_future+1)]\n        \n        for item in eff_array:\n            perms.add((j, item))\n        all_perms = all_perms.union(perms)\n    return list(all_perms)\n    \n        \ndef batch_graphify(features, qmask, lengths, window_past, window_future, edge_type_mapping, att_model, no_cuda):\n    """"""\n    Method to prepare the data format required for the GCN network. Pytorch geometric puts all nodes for classification \n    in one single graph. Following this, we create a single graph for a mini-batch of dialogue instances. This method \n    ensures that the various graph indexing is properly carried out so as to make sure that, utterances (nodes) from \n    each dialogue instance will have edges with utterances in that same dialogue instance, but not with utternaces \n    from any other dialogue instances in that mini-batch.\n    """"""\n    \n    edge_index, edge_norm, edge_type, node_features = [], [], [], []\n    batch_size = features.size(1)\n    length_sum = 0\n    edge_ind = []\n    edge_index_lengths = []\n    \n    for j in range(batch_size):\n        edge_ind.append(edge_perms(lengths[j], window_past, window_future))\n    \n    # scores are the edge weights\n    scores = att_model(features, lengths, edge_ind)\n\n    for j in range(batch_size):\n        node_features.append(features[:lengths[j], j, :])\n    \n        perms1 = edge_perms(lengths[j], window_past, window_future)\n        perms2 = [(item[0]+length_sum, item[1]+length_sum) for item in perms1]\n        length_sum += lengths[j]\n\n        edge_index_lengths.append(len(perms1))\n    \n        for item1, item2 in zip(perms1, perms2):\n            edge_index.append(torch.tensor([item2[0], item2[1]]))\n            edge_norm.append(scores[j, item1[0], item1[1]])\n        \n            speaker0 = (qmask[item1[0], j, :] == 1).nonzero()[0][0].tolist()\n            speaker1 = (qmask[item1[1], j, :] == 1).nonzero()[0][0].tolist()\n        \n            if item1[0] < item1[1]:\n                # edge_type.append(0) # ablation by removing speaker dependency: only 2 relation types\n                # edge_type.append(edge_type_mapping[str(speaker0) + str(speaker1) + \'0\']) # ablation by removing temporal dependency: M^2 relation types\n                edge_type.append(edge_type_mapping[str(speaker0) + str(speaker1) + \'0\'])\n            else:\n                # edge_type.append(1) # ablation by removing speaker dependency: only 2 relation types\n                # edge_type.append(edge_type_mapping[str(speaker0) + str(speaker1) + \'0\']) # ablation by removing temporal dependency: M^2 relation types\n                edge_type.append(edge_type_mapping[str(speaker0) + str(speaker1) + \'1\'])\n    \n    node_features = torch.cat(node_features, dim=0)\n    edge_index = torch.stack(edge_index).transpose(0, 1)\n    edge_norm = torch.stack(edge_norm)\n    edge_type = torch.tensor(edge_type)\n\n    #if torch.cuda.is_available():\n    if not no_cuda:\n        node_features = node_features.cuda()\n        edge_index = edge_index.cuda()\n        edge_norm = edge_norm.cuda()\n        edge_type = edge_type.cuda()\n    \n    return node_features, edge_index, edge_norm, edge_type, edge_index_lengths \n\n\ndef attentive_node_features(emotions, seq_lengths, umask, matchatt_layer, no_cuda):\n    """"""\n    Method to obtain attentive node features over the graph convoluted features, as in Equation 4, 5, 6. in the paper.\n    """"""\n    \n    input_conversation_length = torch.tensor(seq_lengths)\n    start_zero = input_conversation_length.data.new(1).zero_()\n    \n    #if torch.cuda.is_available():\n    if not no_cuda:\n        input_conversation_length = input_conversation_length.cuda()\n        start_zero = start_zero.cuda()\n\n    max_len = max(seq_lengths)\n\n    start = torch.cumsum(torch.cat((start_zero, input_conversation_length[:-1])), 0)\n\n    emotions = torch.stack([pad(emotions.narrow(0, s, l), max_len, no_cuda) \n                                for s, l in zip(start.data.tolist(),\n                                input_conversation_length.data.tolist())], 0).transpose(0, 1)\n\n\n    alpha, alpha_f, alpha_b = [], [], []\n    att_emotions = []\n\n    for t in emotions:\n        att_em, alpha_ = matchatt_layer(emotions, t, mask=umask)\n        att_emotions.append(att_em.unsqueeze(0))\n        alpha.append(alpha_[:,0,:])\n\n    att_emotions = torch.cat(att_emotions, dim=0)\n\n    return att_emotions\n\n\ndef classify_node_features(emotions, seq_lengths, umask, matchatt_layer, linear_layer, dropout_layer, smax_fc_layer, nodal_attn, avec, no_cuda):\n    """"""\n    Function for the final classification, as in Equation 7, 8, 9. in the paper.\n    """"""\n\n    if nodal_attn:\n\n        emotions = attentive_node_features(emotions, seq_lengths, umask, matchatt_layer, no_cuda)\n        hidden = F.relu(linear_layer(emotions))\n        hidden = dropout_layer(hidden)\n        hidden = smax_fc_layer(hidden)\n\n        if avec:\n            return torch.cat([hidden[:, j, :][:seq_lengths[j]] for j in range(len(seq_lengths))])\n\n        log_prob = F.log_softmax(hidden, 2)\n        log_prob = torch.cat([log_prob[:, j, :][:seq_lengths[j]] for j in range(len(seq_lengths))])\n        return log_prob\n\n    else:\n\n        hidden = F.relu(linear_layer(emotions))\n        hidden = dropout_layer(hidden)\n        hidden = smax_fc_layer(hidden)\n\n        if avec:\n            return hidden\n\n        log_prob = F.log_softmax(hidden, 1)\n        return log_prob\n\n\nclass GraphNetwork(torch.nn.Module):\n    def __init__(self, num_features, num_classes, num_relations, max_seq_len, hidden_size=64, dropout=0.5, no_cuda=False):\n        """"""\n        The Speaker-level context encoder in the form of a 2 layer GCN.\n        """"""\n        super(GraphNetwork, self).__init__()\n        \n        self.conv1 = RGCNConv(num_features, hidden_size, num_relations, num_bases=30)\n        self.conv2 = GraphConv(hidden_size, hidden_size)\n        self.matchatt = MatchingAttention(num_features+hidden_size, num_features+hidden_size, att_type=\'general2\')\n        self.linear   = nn.Linear(num_features+hidden_size, hidden_size)\n        self.dropout = nn.Dropout(dropout)\n        self.smax_fc  = nn.Linear(hidden_size, num_classes)\n        self.no_cuda = no_cuda \n\n    def forward(self, x, edge_index, edge_norm, edge_type, seq_lengths, umask, nodal_attn, avec):\n        \n        out = self.conv1(x, edge_index, edge_type, edge_norm)\n        out = self.conv2(out, edge_index)\n        emotions = torch.cat([x, out], dim=-1)\n        log_prob = classify_node_features(emotions, seq_lengths, umask, self.matchatt, self.linear, self.dropout, self.smax_fc, nodal_attn, avec, self.no_cuda)\n        return log_prob\n\n\n\nclass DialogueGCNModel(nn.Module):\n\n    def __init__(self, base_model, D_m, D_g, D_p, D_e, D_h, D_a, graph_hidden_size, n_speakers, max_seq_len, window_past, window_future,\n                 n_classes=7, listener_state=False, context_attention=\'simple\', dropout_rec=0.5, dropout=0.5, nodal_attention=True, avec=False, no_cuda=False):\n        \n        super(DialogueGCNModel, self).__init__()\n\n        self.base_model = base_model\n        self.avec = avec\n        self.no_cuda = no_cuda\n\n        # The base model is the sequential context encoder.\n        if self.base_model == \'DialogRNN\':\n            self.dialog_rnn_f = DialogueRNN(D_m, D_g, D_p, D_e, listener_state, context_attention, D_a, dropout_rec)\n            self.dialog_rnn_r = DialogueRNN(D_m, D_g, D_p, D_e, listener_state, context_attention, D_a, dropout_rec)\n\n        elif self.base_model == \'LSTM\':\n            self.lstm = nn.LSTM(input_size=D_m, hidden_size=D_e, num_layers=2, bidirectional=True, dropout=dropout)\n\n        elif self.base_model == \'GRU\':\n            self.gru = nn.GRU(input_size=D_m, hidden_size=D_e, num_layers=2, bidirectional=True, dropout=dropout)\n\n\n        elif self.base_model == \'None\':\n            self.base_linear = nn.Linear(D_m, 2*D_e)\n\n        else:\n            print (\'Base model must be one of DialogRNN/LSTM/GRU\')\n            raise NotImplementedError \n\n        n_relations = 2 * n_speakers ** 2\n        self.window_past = window_past\n        self.window_future = window_future\n\n        self.att_model = MaskedEdgeAttention(2*D_e, max_seq_len, self.no_cuda)\n        self.nodal_attention = nodal_attention\n\n        self.graph_net = GraphNetwork(2*D_e, n_classes, n_relations, max_seq_len, graph_hidden_size, dropout, self.no_cuda)\n\n        edge_type_mapping = {}\n        for j in range(n_speakers):\n            for k in range(n_speakers):\n                edge_type_mapping[str(j) + str(k) + \'0\'] = len(edge_type_mapping)\n                edge_type_mapping[str(j) + str(k) + \'1\'] = len(edge_type_mapping)\n\n        self.edge_type_mapping = edge_type_mapping\n\n\n    def _reverse_seq(self, X, mask):\n        """"""\n        X -> seq_len, batch, dim\n        mask -> batch, seq_len\n        """"""\n        X_ = X.transpose(0,1)\n        mask_sum = torch.sum(mask, 1).int()\n\n        xfs = []\n        for x, c in zip(X_, mask_sum):\n            xf = torch.flip(x[:c], [0])\n            xfs.append(xf)\n\n        return pad_sequence(xfs)\n\n\n    def forward(self, U, qmask, umask, seq_lengths):\n        """"""\n        U -> seq_len, batch, D_m\n        qmask -> seq_len, batch, party\n        """"""\n        if self.base_model == ""DialogRNN"":\n\n            if self.avec:\n                emotions, _ = self.dialog_rnn_f(U, qmask)\n\n            else:\n                emotions_f, alpha_f = self.dialog_rnn_f(U, qmask) # seq_len, batch, D_e\n                rev_U = self._reverse_seq(U, umask)\n                rev_qmask = self._reverse_seq(qmask, umask)\n                emotions_b, alpha_b = self.dialog_rnn_r(rev_U, rev_qmask)\n                emotions_b = self._reverse_seq(emotions_b, umask)\n                emotions = torch.cat([emotions_f,emotions_b],dim=-1)\n\n        elif self.base_model == \'LSTM\':\n            emotions, hidden = self.lstm(U)\n\n        elif self.base_model == \'GRU\':\n            emotions, hidden = self.gru(U)\n\n        elif self.base_model == \'None\':\n            emotions = self.base_linear(U)\n\n        features, edge_index, edge_norm, edge_type, edge_index_lengths = batch_graphify(emotions, qmask, seq_lengths, self.window_past, self.window_future, self.edge_type_mapping, self.att_model, self.no_cuda)\n        log_prob = self.graph_net(features, edge_index, edge_norm, edge_type, seq_lengths, umask, self.nodal_attention, self.avec)\n\n        return log_prob, edge_index, edge_norm, edge_type, edge_index_lengths\n\n\nclass CNNFeatureExtractor(nn.Module):\n    """"""\n    Module from DialogueRNN\n    """"""\n    def __init__(self, vocab_size, embedding_dim, output_size, filters, kernel_sizes, dropout):\n        super(CNNFeatureExtractor, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.convs = nn.ModuleList(\n            [nn.Conv1d(in_channels=embedding_dim, out_channels=filters, kernel_size=K) for K in kernel_sizes])\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(len(kernel_sizes) * filters, output_size)\n        self.feature_dim = output_size\n\n    def init_pretrained_embeddings_from_numpy(self, pretrained_word_vectors):\n        self.embedding.weight = nn.Parameter(torch.from_numpy(pretrained_word_vectors).float())\n        # if is_static:\n        self.embedding.weight.requires_grad = False\n\n    def forward(self, x, umask):\n        num_utt, batch, num_words = x.size()\n\n        x = x.type(LongTensor)  # (num_utt, batch, num_words)\n        x = x.view(-1, num_words)  # (num_utt, batch, num_words) -> (num_utt * batch, num_words)\n        emb = self.embedding(x)  # (num_utt * batch, num_words) -> (num_utt * batch, num_words, 300)\n        emb = emb.transpose(-2,\n                            -1).contiguous()  # (num_utt * batch, num_words, 300)  -> (num_utt * batch, 300, num_words)\n\n        convoluted = [F.relu(conv(emb)) for conv in self.convs]\n        pooled = [F.max_pool1d(c, c.size(2)).squeeze() for c in convoluted]\n        concated = torch.cat(pooled, 1)\n        features = F.relu(self.fc(self.dropout(concated)))  # (num_utt * batch, 150) -> (num_utt * batch, 100)\n        features = features.view(num_utt, batch, -1)  # (num_utt * batch, 100) -> (num_utt, batch, 100)\n        mask = umask.unsqueeze(-1).type(FloatTensor)  # (batch, num_utt) -> (batch, num_utt, 1)\n        mask = mask.transpose(0, 1)  # (batch, num_utt, 1) -> (num_utt, batch, 1)\n        mask = mask.repeat(1, 1, self.feature_dim)  # (num_utt, batch, 1) -> (num_utt, batch, 100)\n        features = (features * mask)  # (num_utt, batch, 100) -> (num_utt, batch, 100)\n\n        return features\n\n\nclass DialogueGCN_DailyModel(nn.Module):\n    """"""\n    Use CNN to extract features from 300-dimension vector to 100-dimension vector.\n    """"""\n    def __init__(self, base_model, D_m, D_g, D_p, D_e, D_h, D_a, graph_hidden_size, n_speakers, max_seq_len,\n                 window_past, window_future,\n                 vocab_size, embedding_dim=300,\n                 cnn_output_size=100, cnn_filters=50, cnn_kernel_sizes=(3, 4, 5), cnn_dropout=0.5,\n                 n_classes=7, listener_state=False, context_attention=\'simple\', dropout_rec=0.5, dropout=0.5,\n                 nodal_attention=True, avec=False, no_cuda=False):\n\n        super(DialogueGCN_DailyModel, self).__init__()\n        self.cnn_feat_extractor = CNNFeatureExtractor(vocab_size, embedding_dim, cnn_output_size, cnn_filters,\n                                                      cnn_kernel_sizes, cnn_dropout)\n        self.base_model = base_model\n        self.avec = avec\n        self.no_cuda = no_cuda\n\n        # The base model is the sequential context encoder.\n        if self.base_model == \'DialogRNN\':\n            self.dialog_rnn_f = DialogueRNN(D_m, D_g, D_p, D_e, listener_state, context_attention, D_a, dropout_rec)\n            self.dialog_rnn_r = DialogueRNN(D_m, D_g, D_p, D_e, listener_state, context_attention, D_a, dropout_rec)\n\n        elif self.base_model == \'LSTM\':\n            self.lstm = nn.LSTM(input_size=D_m, hidden_size=D_e, num_layers=2, bidirectional=True, dropout=dropout)\n\n        elif self.base_model == \'GRU\':\n            self.gru = nn.GRU(input_size=D_m, hidden_size=D_e, num_layers=2, bidirectional=True, dropout=dropout)\n\n\n        elif self.base_model == \'None\':\n            self.base_linear = nn.Linear(D_m, 2 * D_e)\n\n        else:\n            print(\'Base model must be one of DialogRNN/LSTM/GRU\')\n            raise NotImplementedError\n\n        n_relations = 2 * n_speakers ** 2\n        self.window_past = window_past\n        self.window_future = window_future\n\n        self.att_model = MaskedEdgeAttention(2 * D_e, max_seq_len, self.no_cuda)\n        self.nodal_attention = nodal_attention\n\n        self.graph_net = GraphNetwork(2 * D_e, n_classes, n_relations, max_seq_len, graph_hidden_size, dropout,\n                                      self.no_cuda)\n\n        edge_type_mapping = {}\n        for j in range(n_speakers):\n            for k in range(n_speakers):\n                edge_type_mapping[str(j) + str(k) + \'0\'] = len(edge_type_mapping)\n                edge_type_mapping[str(j) + str(k) + \'1\'] = len(edge_type_mapping)\n\n        self.edge_type_mapping = edge_type_mapping\n\n    def init_pretrained_embeddings(self, pretrained_word_vectors):\n        self.cnn_feat_extractor.init_pretrained_embeddings_from_numpy(pretrained_word_vectors)\n\n    def _reverse_seq(self, X, mask):\n        """"""\n        X -> seq_len, batch, dim\n        mask -> batch, seq_len\n        """"""\n        X_ = X.transpose(0, 1)\n        mask_sum = torch.sum(mask, 1).int()\n\n        xfs = []\n        for x, c in zip(X_, mask_sum):\n            xf = torch.flip(x[:c], [0])\n            xfs.append(xf)\n\n        return pad_sequence(xfs)\n\n    def forward(self, input_seq, qmask, umask, seq_lengths):\n        """"""\n        U -> seq_len, batch, D_m\n        qmask -> seq_len, batch, party\n        """"""\n        U = self.cnn_feat_extractor(input_seq, umask)\n\n        if self.base_model == ""DialogRNN"":\n\n            if self.avec:\n                emotions, _ = self.dialog_rnn_f(U, qmask)\n\n            else:\n                emotions_f, alpha_f = self.dialog_rnn_f(U, qmask)  # seq_len, batch, D_e\n                rev_U = self._reverse_seq(U, umask)\n                rev_qmask = self._reverse_seq(qmask, umask)\n                emotions_b, alpha_b = self.dialog_rnn_r(rev_U, rev_qmask)\n                emotions_b = self._reverse_seq(emotions_b, umask)\n                emotions = torch.cat([emotions_f, emotions_b], dim=-1)\n\n        elif self.base_model == \'LSTM\':\n            emotions, hidden = self.lstm(U)\n\n        elif self.base_model == \'GRU\':\n            emotions, hidden = self.gru(U)\n\n        elif self.base_model == \'None\':\n            emotions = self.base_linear(U)\n\n        features, edge_index, edge_norm, edge_type, edge_index_lengths = batch_graphify(emotions, qmask, seq_lengths,\n                                                                                        self.window_past,\n                                                                                        self.window_future,\n                                                                                        self.edge_type_mapping,\n                                                                                        self.att_model, self.no_cuda)\n        log_prob = self.graph_net(features, edge_index, edge_norm, edge_type, seq_lengths, umask, self.nodal_attention,\n                                  self.avec)\n\n        return log_prob, edge_index, edge_norm, edge_type, edge_index_lengths\n'"
DialogueGCN/preprocess_dailydialog.py,0,"b'import pandas as pd, numpy as np, pickle\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n\ndef preprocess_text(x):\n    for punct in \'""!&?.,}-/<>#$%\\()*+:;=?@[\\\\]^_`|\\~\':\n        x = x.replace(punct, \' \')\n\n    x = \' \'.join(x.split())\n    x = x.lower()\n\n    return x\n\n\ndef create_utterances(filename, split):\n    sentences, act_labels, emotion_labels, speakers, conv_id, utt_id = [], [], [], [], [], []\n\n    lengths = []\n    with open(filename, \'r\') as f:\n        for c_id, line in enumerate(f):\n            s = eval(line)\n            for u_id, item in enumerate(s[\'dialogue\']):\n                sentences.append(item[\'text\'])\n                act_labels.append(item[\'act\'])\n                emotion_labels.append(item[\'emotion\'])\n                conv_id.append(split[:2] + \'_c\' + str(c_id))\n                utt_id.append(split[:2] + \'_c\' + str(c_id) + \'_u\' + str(u_id))\n                speakers.append(str(u_id % 2))\n\n                # u_id += 1\n\n    data = pd.DataFrame(sentences, columns=[\'sentence\'])\n    data[\'sentence\'] = data[\'sentence\'].apply(lambda x: preprocess_text(x))\n    data[\'act_label\'] = act_labels\n    data[\'emotion_label\'] = emotion_labels\n    data[\'speaker\'] = speakers\n    data[\'conv_id\'] = conv_id\n    data[\'utt_id\'] = utt_id\n\n    return data\n\n\ndef load_pretrained_glove():\n    print(""Loading GloVe model, this can take some time..."")\n    glv_vector = {}\n    # Put your glove embedding path here\n    f = open(\'/your/path/glove.840B.300d.txt\', encoding=\'utf-8\')\n\n    for line in f:\n        values = line.split()\n        word = values[0]\n        try:\n            coefs = np.asarray(values[1:], dtype=\'float\')\n            glv_vector[word] = coefs\n        except ValueError:\n            continue\n    f.close()\n    print(""Completed loading pretrained GloVe model."")\n    return glv_vector\n\n\ndef encode_labels(encoder, l):\n    return encoder[l]\n\n\nif __name__ == \'__main__\':\n    # Your training data path\n    # Data format is consistent with DialogueRNN\n    train_data = create_utterances(\'dailydialog/train.json\', \'train\')\n    valid_data = create_utterances(\'dailydialog/dev.json\', \'valid\')\n    test_data = create_utterances(\'dailydialog/test.json\', \'test\')\n\n    ## encode the emotion and dialog act labels ##\n    all_act_labels, all_emotion_labels = set(train_data[\'act_label\']), set(train_data[\'emotion_label\'])\n    act_label_encoder, emotion_label_encoder, act_label_decoder, emotion_label_decoder = {}, {}, {}, {}\n\n    for i, label in enumerate(all_act_labels):\n        act_label_encoder[label] = i\n        act_label_decoder[i] = label\n\n    # Here To print the label mapping\n    # This is very import for your own dataset and also for reproduce the paper result\n    for i, label in enumerate(all_emotion_labels):\n        emotion_label_encoder[label] = i\n        print(str(i) + "" "" + str(label))\n        emotion_label_decoder[i] = label\n        print(str(emotion_label_encoder[label]) + "" "" + str(emotion_label_decoder[i]))\n\n    pickle.dump(act_label_encoder, open(\'dailydialog/act_label_encoder.pkl\', \'wb\'))\n    pickle.dump(act_label_decoder, open(\'dailydialog/act_label_decoder.pkl\', \'wb\'))\n    pickle.dump(emotion_label_encoder, open(\'dailydialog/emotion_label_encoder.pkl\', \'wb\'))\n    pickle.dump(emotion_label_decoder, open(\'dailydialog/emotion_label_decoder.pkl\', \'wb\'))\n\n    train_data[\'encoded_act_label\'] = train_data[\'act_label\'].map(lambda x: encode_labels(act_label_encoder, x))\n    test_data[\'encoded_act_label\'] = test_data[\'act_label\'].map(lambda x: encode_labels(act_label_encoder, x))\n    valid_data[\'encoded_act_label\'] = valid_data[\'act_label\'].map(lambda x: encode_labels(act_label_encoder, x))\n\n    train_data[\'encoded_emotion_label\'] = train_data[\'emotion_label\'].map(\n        lambda x: encode_labels(emotion_label_encoder, x))\n    test_data[\'encoded_emotion_label\'] = test_data[\'emotion_label\'].map(\n        lambda x: encode_labels(emotion_label_encoder, x))\n    valid_data[\'encoded_emotion_label\'] = valid_data[\'emotion_label\'].map(\n        lambda x: encode_labels(emotion_label_encoder, x))\n\n    ## tokenize all sentences ##\n    all_text = list(train_data[\'sentence\'])\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(all_text)\n    pickle.dump(tokenizer, open(\'dailydialog/tokenizer.pkl\', \'wb\'))\n\n    ## convert the sentences into sequences ##\n    train_sequence = tokenizer.texts_to_sequences(list(train_data[\'sentence\']))\n    valid_sequence = tokenizer.texts_to_sequences(list(valid_data[\'sentence\']))\n    test_sequence = tokenizer.texts_to_sequences(list(test_data[\'sentence\']))\n\n    train_data[\'sentence_length\'] = [len(item) for item in train_sequence]\n    valid_data[\'sentence_length\'] = [len(item) for item in valid_sequence]\n    test_data[\'sentence_length\'] = [len(item) for item in test_sequence]\n\n    max_num_tokens = 250\n\n    train_sequence = pad_sequences(train_sequence, maxlen=max_num_tokens, padding=\'post\')\n    valid_sequence = pad_sequences(valid_sequence, maxlen=max_num_tokens, padding=\'post\')\n    test_sequence = pad_sequences(test_sequence, maxlen=max_num_tokens, padding=\'post\')\n\n    train_data[\'sequence\'] = list(train_sequence)\n    valid_data[\'sequence\'] = list(valid_sequence)\n    test_data[\'sequence\'] = list(test_sequence)\n\n    ## save the data in pickle format ##\n    convSpeakers, convInputSequence, convInputMaxSequenceLength, convActLabels, convEmotionLabels = {}, {}, {}, {}, {}\n    train_conv_ids, test_conv_ids, valid_conv_ids = set(train_data[\'conv_id\']), set(test_data[\'conv_id\']), set(\n        valid_data[\'conv_id\'])\n    all_data = train_data.append(test_data, ignore_index=True).append(valid_data, ignore_index=True)\n\n    print(\'Preparing dataset. Hang on...\')\n    for item in list(train_conv_ids) + list(test_conv_ids) + list(valid_conv_ids):\n        df = all_data[all_data[\'conv_id\'] == item]\n\n        convSpeakers[item] = list(df[\'speaker\'])\n        convInputSequence[item] = list(df[\'sequence\'])\n        convInputMaxSequenceLength[item] = max(list(df[\'sentence_length\']))\n        convActLabels[item] = list(df[\'encoded_act_label\'])\n        convEmotionLabels[item] = list(df[\'encoded_emotion_label\'])\n\n    pickle.dump([convSpeakers, convInputSequence, convInputMaxSequenceLength, convActLabels, convEmotionLabels,\n                 train_conv_ids, test_conv_ids, valid_conv_ids], open(\'dailydialog/daily_dialogue2.pkl\', \'wb\'))\n\n    ## save pretrained embedding matrix ##\n    glv_vector = load_pretrained_glove()\n    word_vector_length = len(glv_vector[\'the\'])\n    word_index = tokenizer.word_index\n    inv_word_index = {v: k for k, v in word_index.items()}\n    num_unique_words = len(word_index)\n    glv_embedding_matrix = np.zeros((num_unique_words + 1, word_vector_length))\n\n    for j in range(1, num_unique_words + 1):\n        try:\n            glv_embedding_matrix[j] = glv_vector[inv_word_index[j]]\n        except KeyError:\n            glv_embedding_matrix[j] = np.random.randn(word_vector_length) / 200\n\n    np.ndarray.dump(glv_embedding_matrix, open(\'dailydialog/glv_embedding_matrix2\', \'wb\'))\n    print(\'Done. Completed preprocessing.\')\n'"
DialogueGCN/train_DailyDialog.py,21,"b""import numpy as np, argparse, time, pickle, random\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom dataloader import DailyDialogueDataset2\nfrom model import MaskedNLLLoss, LSTMModel, GRUModel, DialogRNNModel, DialogueGCNModel, DialogueGCN_DailyModel\nfrom sklearn.metrics import f1_score, confusion_matrix, accuracy_score, classification_report, \\\n    precision_recall_fscore_support, precision_score, recall_score\n\n# We use seed = 100 for reproduction of the results reported in the paper.\nseed = 100\n\n\ndef seed_everything(seed=seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\n\ndef get_train_valid_sampler(trainset, valid=0.1):\n    size = len(trainset)\n    idx = list(range(size))\n    split = int(valid * size)\n    return SubsetRandomSampler(idx[split:]), SubsetRandomSampler(idx[:split])\n\n\ndef get_DailyDialogue_loaders(path, batch_size=32, num_workers=0, pin_memory=False):\n    trainset = DailyDialogueDataset2('train', path)\n    testset = DailyDialogueDataset2('test', path)\n    validset = DailyDialogueDataset2('valid', path)\n\n    train_loader = DataLoader(trainset,\n                              batch_size=batch_size,\n                              collate_fn=trainset.collate_fn,\n                              num_workers=num_workers,\n                              pin_memory=pin_memory)\n\n    valid_loader = DataLoader(validset,\n                              batch_size=batch_size,\n                              collate_fn=validset.collate_fn,\n                              num_workers=num_workers,\n                              pin_memory=pin_memory)\n\n    test_loader = DataLoader(testset,\n                             batch_size=batch_size,\n                             collate_fn=testset.collate_fn,\n                             num_workers=num_workers,\n                             pin_memory=pin_memory)\n\n    return train_loader, valid_loader, test_loader\n\n\ndef train_or_eval_model(model, loss_function, dataloader, epoch, optimizer=None, train=False):\n    losses, preds, labels, masks = [], [], [], []\n    alphas, alphas_f, alphas_b, vids = [], [], [], []\n    max_sequence_len = []\n\n    assert not train or optimizer != None\n    if train:\n        model.train()\n    else:\n        model.eval()\n\n    seed_everything()\n    for data in dataloader:\n        if train:\n            optimizer.zero_grad()\n\n        # import ipdb;ipdb.set_trace()\n        textf, qmask, umask, label = [d.cuda() for d in data[:-1]] if cuda else data[:-1]\n        max_sequence_len.append(textf.size(0))\n\n        log_prob, alpha, alpha_f, alpha_b, _ = model(textf, qmask, umask)  # seq_len, batch, n_classes\n        lp_ = log_prob.transpose(0, 1).contiguous().view(-1, log_prob.size()[2])  # batch*seq_len, n_classes\n        labels_ = label.view(-1)  # batch*seq_len\n        loss = loss_function(lp_, labels_, umask)\n\n        pred_ = torch.argmax(lp_, 1)  # batch*seq_len\n        preds.append(pred_.data.cpu().numpy())\n        labels.append(labels_.data.cpu().numpy())\n        masks.append(umask.view(-1).cpu().numpy())\n\n        losses.append(loss.item() * masks[-1].sum())\n        if train:\n            loss.backward()\n            if args.tensorboard:\n                for param in model.named_parameters():\n                    writer.add_histogram(param[0], param[1].grad, epoch)\n            optimizer.step()\n        else:\n            alphas += alpha\n            alphas_f += alpha_f\n            alphas_b += alpha_b\n            vids += data[-1]\n\n    if preds != []:\n        preds = np.concatenate(preds)\n        labels = np.concatenate(labels)\n        masks = np.concatenate(masks)\n    else:\n        return float('nan'), float('nan'), [], [], [], float('nan'), []\n\n    avg_loss = round(np.sum(losses) / np.sum(masks), 4)\n    avg_accuracy = round(accuracy_score(labels, preds, sample_weight=masks) * 100, 2)\n    avg_fscore = round(f1_score(labels, preds, sample_weight=masks, average='weighted') * 100, 2)\n\n    return avg_loss, avg_accuracy, labels, preds, masks, avg_fscore, [alphas, alphas_f, alphas_b, vids]\n\n# Only modified graph model\ndef train_or_eval_graph_model(model, loss_function, dataloader, epoch, cuda, optimizer=None, train=False):\n    losses, preds, labels = [], [], []\n    scores, vids = [], []\n\n    ei, et, en, el = torch.empty(0).type(torch.LongTensor), torch.empty(0).type(torch.LongTensor), torch.empty(0), []\n\n    # if torch.cuda.is_available():\n    if cuda:\n        ei, et, en = ei.cuda(), et.cuda(), en.cuda()\n\n    assert not train or optimizer != None\n    if train:\n        model.train()\n    else:\n        model.eval()\n\n    seed_everything()\n    for data in dataloader:\n        if train:\n            optimizer.zero_grad()\n\n        # textf, visuf, acouf, qmask, umask, label = [d.cuda() for d in data[:-1]] if cuda else data[:-1]\n        textf, qmask, umask, label = [d.cuda() for d in data[:-1]] if cuda else data[:-1]\n        lengths = [(umask[j] == 1).nonzero().tolist()[-1][0] + 1 for j in range(len(umask))]\n\n        log_prob, e_i, e_n, e_t, e_l = model(textf, qmask, umask, lengths)\n        label = torch.cat([label[j][:lengths[j]] for j in range(len(label))])\n        loss = loss_function(log_prob, label)\n\n        ei = torch.cat([ei, e_i], dim=1)\n        et = torch.cat([et, e_t])\n        en = torch.cat([en, e_n])\n        el += e_l\n\n        preds.append(torch.argmax(log_prob, 1).cpu().numpy())\n        labels.append(label.cpu().numpy())\n        losses.append(loss.item())\n\n        if train:\n            loss.backward()\n            if args.tensorboard:\n                for param in model.named_parameters():\n                    writer.add_histogram(param[0], param[1].grad, epoch)\n            optimizer.step()\n\n    if preds != []:\n        preds = np.concatenate(preds)\n        labels = np.concatenate(labels)\n    else:\n        return float('nan'), float('nan'), [], [], float('nan'), [], [], [], [], []\n\n    vids += data[-1]\n    ei = ei.data.cpu().numpy()\n    et = et.data.cpu().numpy()\n    en = en.data.cpu().numpy()\n    el = np.array(el)\n    labels = np.array(labels)\n    preds = np.array(preds)\n    vids = np.array(vids)\n\n    avg_loss = round(np.sum(losses) / len(losses), 4)\n    avg_accuracy = round(accuracy_score(labels, preds) * 100, 2)\n    # Here, You should mask the label 'no_emotion'\n    # So you should remember the pre-processing label mapping\n    # My 'no_emotion' label matched to '3'\n    # Follow the same metric settings in DialogueRNN\n    avg_fscore = round(f1_score(labels, preds, average='micro', labels=[0, 1, 2, 4, 5, 6]) * 100, 2)\n    # Add precision and recall\n    precision = round(precision_score(labels, preds, average='micro', labels=[0, 1, 2, 4, 5, 6]) * 100, 2)\n    recall = round(recall_score(labels, preds, average='micro', labels=[0, 1, 2, 4, 5, 6]) * 100, 2)\n\n    return avg_loss, avg_accuracy, labels, preds, avg_fscore, vids, ei, et, en, el, precision, recall\n\n\nif __name__ == '__main__':\n\n    path = './saved/DailyDialog/'\n\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--no-cuda', action='store_true', default=False, help='does not use GPU')\n\n    parser.add_argument('--base-model', default='LSTM', help='base recurrent model, must be one of DialogRNN/LSTM/GRU')\n\n    parser.add_argument('--graph-model', action='store_true', default=False,\n                        help='whether to use graph model after recurrent encoding')\n\n    parser.add_argument('--nodal-attention', action='store_true', default=False,\n                        help='whether to use nodal attention in graph model: Equation 4,5,6 in Paper')\n\n    parser.add_argument('--windowp', type=int, default=10,\n                        help='context window size for constructing edges in graph model for past utterances')\n\n    parser.add_argument('--windowf', type=int, default=10,\n                        help='context window size for constructing edges in graph model for future utterances')\n\n    parser.add_argument('--lr', type=float, default=0.0001, metavar='LR', help='learning rate')\n\n    parser.add_argument('--l2', type=float, default=0.00001, metavar='L2', help='L2 regularization weight')\n\n    parser.add_argument('--rec-dropout', type=float, default=0.1, metavar='rec_dropout', help='rec_dropout rate')\n\n    parser.add_argument('--dropout', type=float, default=0.5, metavar='dropout', help='dropout rate')\n\n    parser.add_argument('--batch-size', type=int, default=32, metavar='BS', help='batch size')\n\n    parser.add_argument('--epochs', type=int, default=60, metavar='E', help='number of epochs')\n\n    parser.add_argument('--class-weight', action='store_true', default=False, help='use class weights')\n\n    parser.add_argument('--active-listener', action='store_true', default=False, help='active listener')\n\n    parser.add_argument('--attention', default='general', help='Attention type in DialogRNN model')\n\n    parser.add_argument('--tensorboard', action='store_true', default=False, help='Enables tensorboard log')\n\n    args = parser.parse_args()\n    print(args)\n\n    args.cuda = torch.cuda.is_available() and not args.no_cuda\n    if args.cuda:\n        print('Running on GPU')\n    else:\n        print('Running on CPU')\n\n    if args.tensorboard:\n        from tensorboardX import SummaryWriter\n\n        writer = SummaryWriter()\n\n    n_classes = 7\n    cuda = args.cuda\n    n_epochs = args.epochs\n    batch_size = args.batch_size\n    # change D_m into\n    D_m = 100\n    D_g = 150\n    D_p = 150\n    D_e = 100\n    D_h = 100\n    D_a = 100\n    graph_h = 100\n    kernel_sizes = [3, 4, 5]\n    glv_pretrained = np.load(open('dailydialog/glv_embedding_matrix2', 'rb'))\n    vocab_size, embedding_dim = glv_pretrained.shape\n    if args.graph_model:\n        seed_everything()\n\n        model = DialogueGCN_DailyModel(args.base_model,\n                                       D_m, D_g, D_p, D_e, D_h, D_a, graph_h,\n                                       n_speakers=2,\n                                       max_seq_len=110,\n                                       window_past=args.windowp,\n                                       window_future=args.windowf,\n                                       vocab_size=vocab_size,\n                                       n_classes=n_classes,\n                                       listener_state=args.active_listener,\n                                       context_attention=args.attention,\n                                       dropout=args.dropout,\n                                       nodal_attention=args.nodal_attention,\n                                       no_cuda=args.no_cuda\n                                       )\n        model.init_pretrained_embeddings(glv_pretrained)\n\n        print('Graph NN with', args.base_model, 'as base model.')\n        name = 'Graph'\n\n    else:\n        if args.base_model == 'DialogRNN':\n            model = DialogRNNModel(D_m, D_g, D_p, D_e, D_h, D_a,\n                                   n_classes=n_classes,\n                                   listener_state=args.active_listener,\n                                   context_attention=args.attention,\n                                   dropout_rec=args.rec_dropout,\n                                   dropout=args.dropout)\n\n            print('Basic Dialog RNN Model.')\n\n\n        elif args.base_model == 'GRU':\n            model = GRUModel(D_m, D_e, D_h,\n                             n_classes=n_classes,\n                             dropout=args.dropout)\n\n            print('Basic GRU Model.')\n\n\n        elif args.base_model == 'LSTM':\n            model = LSTMModel(D_m, D_e, D_h,\n                              n_classes=n_classes,\n                              dropout=args.dropout)\n\n            print('Basic LSTM Model.')\n\n        else:\n            print('Base model must be one of DialogRNN/LSTM/GRU/Transformer')\n            raise NotImplementedError\n\n        name = 'Base'\n\n    if cuda:\n        model.cuda()\n\n    # for daily_dialog_bert2.pkl\n    # In the pre-processing, you should remember the label mapping.\n    # Here is my label mapping and weights for the dailydialogue\n    loss_weights = torch.FloatTensor([1 / 0.0017,\n                                      1 / 0.0034,\n                                      1 / 0.1251,\n                                      1 / 0.831,\n                                      1 / 0.0099,\n                                      1 / 0.0177,\n                                      1 / 0.0112])\n    if args.class_weight:\n        if args.graph_model:\n            loss_function = nn.NLLLoss(loss_weights.cuda() if cuda else loss_weights)\n        else:\n            loss_function = MaskedNLLLoss(loss_weights.cuda() if cuda else loss_weights)\n    else:\n        if args.graph_model:\n            loss_function = nn.NLLLoss()\n        else:\n            loss_function = MaskedNLLLoss()\n\n    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.l2)\n\n    if args.class_weight:\n        train_loader, valid_loader, test_loader = get_DailyDialogue_loaders('dailydialog/daily_dialogue2.pkl',\n                                                                            batch_size=batch_size, num_workers=0)\n    else:\n        train_loader, valid_loader, test_loader = get_DailyDialogue_loaders('dailydialog/daily_dialogue2.pkl',\n                                                                            batch_size=batch_size, num_workers=0)\n    best_fscore, best_loss, best_label, best_pred, best_mask = None, None, None, None, None\n    all_fscore, all_acc, all_loss = [], [], []\n    all_precision, all_recall = [], []\n\n    for e in range(n_epochs):\n        start_time = time.time()\n\n        if args.graph_model:\n            train_loss, train_acc, _, _, train_fscore, _, _, _, _, _, train_precision, train_recall = train_or_eval_graph_model(\n                model, loss_function, train_loader, e, cuda, optimizer, True)\n            valid_loss, valid_acc, _, _, valid_fscore, _, _, _, _, _, valid_precision, valid_recall = train_or_eval_graph_model(\n                model, loss_function, valid_loader, e, cuda)\n            test_loss, test_acc, test_label, test_pred, test_fscore, _, _, _, _, _, test_precision, test_recall = train_or_eval_graph_model(\n                model, loss_function, test_loader, e, cuda)\n            all_fscore.append(test_fscore)\n            all_precision.append(test_precision)\n            all_recall.append(test_recall)\n            # torch.save({'model_state_dict': model.state_dict()}, path + name + args.base_model + '_' + str(e) + '.pkl')\n\n\n        else:\n            train_loss, train_acc, _, _, _, train_fscore, _ = train_or_eval_model(model, loss_function, train_loader, e,\n                                                                                  optimizer, True)\n            valid_loss, valid_acc, _, _, _, valid_fscore, _ = train_or_eval_model(model, loss_function, valid_loader, e)\n            test_loss, test_acc, test_label, test_pred, test_mask, test_fscore, attentions = train_or_eval_model(model,\n                                                                                                                 loss_function,\n                                                                                                                 test_loader,\n                                                                                                                 e)\n            all_fscore.append(test_fscore)\n            # torch.save({'model_state_dict': model.state_dict()}, path + name + args.base_model + '_' + str(e) + '.pkl')\n\n        if args.tensorboard:\n            writer.add_scalar('test: accuracy/loss', test_acc / test_loss, e)\n            writer.add_scalar('train: accuracy/loss', train_acc / train_loss, e)\n\n        print(\n            'epoch: {}, train_loss: {}, train_acc: {}, train_fscore: {}, train_precision: {}, train_recall: {}, valid_loss: {}, valid_acc: {}, valid_fscore: {}, valid_precision: {}, valid_recall: {}, test_loss: {}, test_acc: {}, test_fscore: {}, test_precision: {}, test_recall: {}, time: {} sec'. \\\n            format(e + 1, train_loss, train_acc, train_fscore, train_precision, train_recall, valid_loss, valid_acc,\n                   valid_fscore, valid_precision, valid_recall, test_loss, test_acc, test_fscore, test_precision,\n                   test_recall, round(time.time() - start_time, 2)))\n\n    if args.tensorboard:\n        writer.close()\n\n    print('Test performance..')\n    index_max = all_fscore.index(max(all_fscore))\n    print('F-Score:', all_fscore[index_max])\n    print('Precision:', all_precision[index_max])\n    print('Recall:', all_recall[index_max])\n"""
DialogueGCN/train_IEMOCAP.py,22,"b""import numpy as np, argparse, time, pickle, random\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom dataloader import IEMOCAPDataset\nfrom model import MaskedNLLLoss, LSTMModel, GRUModel, DialogRNNModel, DialogueGCNModel\nfrom sklearn.metrics import f1_score, confusion_matrix, accuracy_score, classification_report, precision_recall_fscore_support\n\n# We use seed = 100 for reproduction of the results reported in the paper.\nseed = 100\n\ndef seed_everything(seed=seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\n\ndef get_train_valid_sampler(trainset, valid=0.1):\n    size = len(trainset)\n    idx = list(range(size))\n    split = int(valid*size)\n    return SubsetRandomSampler(idx[split:]), SubsetRandomSampler(idx[:split])\n\n\ndef get_IEMOCAP_loaders(batch_size=32, valid=0.1, num_workers=0, pin_memory=False):\n    trainset = IEMOCAPDataset()\n    train_sampler, valid_sampler = get_train_valid_sampler(trainset, valid)\n\n    train_loader = DataLoader(trainset,\n                              batch_size=batch_size,\n                              sampler=train_sampler,\n                              collate_fn=trainset.collate_fn,\n                              num_workers=num_workers,\n                              pin_memory=pin_memory)\n\n    valid_loader = DataLoader(trainset,\n                              batch_size=batch_size,\n                              sampler=valid_sampler,\n                              collate_fn=trainset.collate_fn,\n                              num_workers=num_workers,\n                              pin_memory=pin_memory)\n\n    testset = IEMOCAPDataset(train=False)\n    test_loader = DataLoader(testset,\n                             batch_size=batch_size,\n                             collate_fn=testset.collate_fn,\n                             num_workers=num_workers,\n                             pin_memory=pin_memory)\n\n    return train_loader, valid_loader, test_loader\n\n\ndef train_or_eval_model(model, loss_function, dataloader, epoch, optimizer=None, train=False):\n    losses, preds, labels, masks = [], [], [], []\n    alphas, alphas_f, alphas_b, vids = [], [], [], []\n    max_sequence_len = []\n\n    assert not train or optimizer!=None\n    if train:\n        model.train()\n    else:\n        model.eval()\n\n    seed_everything()\n    for data in dataloader:\n        if train:\n            optimizer.zero_grad()\n        \n        # import ipdb;ipdb.set_trace()\n        textf, visuf, acouf, qmask, umask, label = [d.cuda() for d in data[:-1]] if cuda else data[:-1]        \n        max_sequence_len.append(textf.size(0))\n        \n        # log_prob, alpha, alpha_f, alpha_b = model(torch.cat((textf, acouf, visuf), dim=-1), qmask, umask)\n        log_prob, alpha, alpha_f, alpha_b, _ = model(textf, qmask, umask) # seq_len, batch, n_classes\n        lp_ = log_prob.transpose(0,1).contiguous().view(-1, log_prob.size()[2]) # batch*seq_len, n_classes\n        labels_ = label.view(-1) # batch*seq_len\n        loss = loss_function(lp_, labels_, umask)\n\n        pred_ = torch.argmax(lp_,1) # batch*seq_len\n        preds.append(pred_.data.cpu().numpy())\n        labels.append(labels_.data.cpu().numpy())\n        masks.append(umask.view(-1).cpu().numpy())\n\n        losses.append(loss.item()*masks[-1].sum())\n        if train:\n            loss.backward()\n            if args.tensorboard:\n                for param in model.named_parameters():\n                    writer.add_histogram(param[0], param[1].grad, epoch)\n            optimizer.step()\n        else:\n            alphas += alpha\n            alphas_f += alpha_f\n            alphas_b += alpha_b\n            vids += data[-1]\n\n    if preds!=[]:\n        preds  = np.concatenate(preds)\n        labels = np.concatenate(labels)\n        masks  = np.concatenate(masks)\n    else:\n        return float('nan'), float('nan'), [], [], [], float('nan'),[]\n\n    avg_loss = round(np.sum(losses)/np.sum(masks), 4)\n    avg_accuracy = round(accuracy_score(labels,preds, sample_weight=masks)*100, 2)\n    avg_fscore = round(f1_score(labels,preds, sample_weight=masks, average='weighted')*100, 2)\n    \n    return avg_loss, avg_accuracy, labels, preds, masks, avg_fscore, [alphas, alphas_f, alphas_b, vids]\n\n\ndef train_or_eval_graph_model(model, loss_function, dataloader, epoch, cuda, optimizer=None, train=False):\n    losses, preds, labels = [], [], []\n    scores, vids = [], []\n\n    ei, et, en, el = torch.empty(0).type(torch.LongTensor), torch.empty(0).type(torch.LongTensor), torch.empty(0), []\n\n    #if torch.cuda.is_available():\n    if cuda:\n        ei, et, en = ei.cuda(), et.cuda(), en.cuda()\n\n    assert not train or optimizer!=None\n    if train:\n        model.train()\n    else:\n        model.eval()\n\n    seed_everything()\n    for data in dataloader:\n        if train:\n            optimizer.zero_grad()\n        \n        textf, visuf, acouf, qmask, umask, label = [d.cuda() for d in data[:-1]] if cuda else data[:-1]\n\n        lengths = [(umask[j] == 1).nonzero().tolist()[-1][0] + 1 for j in range(len(umask))]\n\n        log_prob, e_i, e_n, e_t, e_l = model(textf, qmask, umask, lengths)\n        label = torch.cat([label[j][:lengths[j]] for j in range(len(label))])\n        loss = loss_function(log_prob, label)\n\n        ei = torch.cat([ei, e_i], dim=1)\n        et = torch.cat([et, e_t])\n        en = torch.cat([en, e_n])\n        el += e_l\n\n        preds.append(torch.argmax(log_prob, 1).cpu().numpy())\n        labels.append(label.cpu().numpy())\n        losses.append(loss.item())\n\n        if train:\n            loss.backward()\n            if args.tensorboard:\n                for param in model.named_parameters():\n                    writer.add_histogram(param[0], param[1].grad, epoch)\n            optimizer.step()\n\n    if preds!=[]:\n        preds  = np.concatenate(preds)\n        labels = np.concatenate(labels)\n    else:\n        return float('nan'), float('nan'), [], [], float('nan'), [], [], [], [], []\n\n    vids += data[-1]\n    ei = ei.data.cpu().numpy()\n    et = et.data.cpu().numpy()\n    en = en.data.cpu().numpy()\n    el = np.array(el)\n    labels = np.array(labels)\n    preds = np.array(preds)\n    vids = np.array(vids)\n\n    avg_loss = round(np.sum(losses)/len(losses), 4)\n    avg_accuracy = round(accuracy_score(labels, preds)*100, 2)\n    avg_fscore = round(f1_score(labels,preds, average='weighted')*100, 2)\n\n    return avg_loss, avg_accuracy, labels, preds, avg_fscore, vids, ei, et, en, el\n\n\nif __name__ == '__main__':\n\n    path = './saved/IEMOCAP/'\n\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--no-cuda', action='store_true', default=False, help='does not use GPU')\n\n    parser.add_argument('--base-model', default='LSTM', help='base recurrent model, must be one of DialogRNN/LSTM/GRU')\n\n    parser.add_argument('--graph-model', action='store_true', default=False, help='whether to use graph model after recurrent encoding')\n\n    parser.add_argument('--nodal-attention', action='store_true', default=False, help='whether to use nodal attention in graph model: Equation 4,5,6 in Paper')\n\n    parser.add_argument('--windowp', type=int, default=10, help='context window size for constructing edges in graph model for past utterances')\n\n    parser.add_argument('--windowf', type=int, default=10, help='context window size for constructing edges in graph model for future utterances')\n\n    parser.add_argument('--lr', type=float, default=0.0001, metavar='LR', help='learning rate')\n    \n    parser.add_argument('--l2', type=float, default=0.00001, metavar='L2', help='L2 regularization weight')\n    \n    parser.add_argument('--rec-dropout', type=float, default=0.1, metavar='rec_dropout', help='rec_dropout rate')\n    \n    parser.add_argument('--dropout', type=float, default=0.5, metavar='dropout', help='dropout rate')\n    \n    parser.add_argument('--batch-size', type=int, default=32, metavar='BS', help='batch size')\n    \n    parser.add_argument('--epochs', type=int, default=60, metavar='E', help='number of epochs')\n    \n    parser.add_argument('--class-weight', action='store_true', default=False, help='use class weights')\n    \n    parser.add_argument('--active-listener', action='store_true', default=False, help='active listener')\n    \n    parser.add_argument('--attention', default='general', help='Attention type in DialogRNN model')\n    \n    parser.add_argument('--tensorboard', action='store_true', default=False, help='Enables tensorboard log')\n\n    args = parser.parse_args()\n    print(args)\n\n    args.cuda = torch.cuda.is_available() and not args.no_cuda\n    if args.cuda:\n        print('Running on GPU')\n    else:\n        print('Running on CPU')\n\n    if args.tensorboard:\n        from tensorboardX import SummaryWriter\n        writer = SummaryWriter()\n\n    n_classes  = 6\n    cuda       = args.cuda\n    n_epochs   = args.epochs\n    batch_size = args.batch_size\n\n    D_m = 100\n    D_g = 150\n    D_p = 150\n    D_e = 100\n    D_h = 100\n    D_a = 100\n    graph_h = 100\n\n    if args.graph_model:\n        seed_everything()\n        model = DialogueGCNModel(args.base_model,\n                                 D_m, D_g, D_p, D_e, D_h, D_a, graph_h,\n                                 n_speakers=2,\n                                 max_seq_len=110,\n                                 window_past=args.windowp,\n                                 window_future=args.windowf,\n                                 n_classes=n_classes,\n                                 listener_state=args.active_listener,\n                                 context_attention=args.attention,\n                                 dropout=args.dropout,\n                                 nodal_attention=args.nodal_attention,\n                                 no_cuda=args.no_cuda)\n\n        print ('Graph NN with', args.base_model, 'as base model.')\n        name = 'Graph'\n\n    else:\n        if args.base_model == 'DialogRNN':\n            model = DialogRNNModel(D_m, D_g, D_p, D_e, D_h, D_a, \n                                   n_classes=n_classes,\n                                   listener_state=args.active_listener,\n                                   context_attention=args.attention,\n                                   dropout_rec=args.rec_dropout,\n                                   dropout=args.dropout)\n\n            print ('Basic Dialog RNN Model.')\n\n\n        elif args.base_model == 'GRU':\n            model = GRUModel(D_m, D_e, D_h, \n                              n_classes=n_classes, \n                              dropout=args.dropout)\n\n            print ('Basic GRU Model.')\n\n\n        elif args.base_model == 'LSTM':\n            model = LSTMModel(D_m, D_e, D_h, \n                              n_classes=n_classes, \n                              dropout=args.dropout)\n\n            print ('Basic LSTM Model.')\n\n        else:\n            print ('Base model must be one of DialogRNN/LSTM/GRU/Transformer')\n            raise NotImplementedError\n\n        name = 'Base'\n\n    if cuda:\n        model.cuda()\n\n    loss_weights = torch.FloatTensor([1/0.086747,\n                                      1/0.144406,\n                                      1/0.227883,\n                                      1/0.160585,\n                                      1/0.127711,\n                                      1/0.252668])\n    \n    if args.class_weight:\n        if args.graph_model:\n            loss_function  = nn.NLLLoss(loss_weights.cuda() if cuda else loss_weights)\n        else:\n            loss_function  = MaskedNLLLoss(loss_weights.cuda() if cuda else loss_weights)\n    else:\n        if args.graph_model:\n            loss_function = nn.NLLLoss()\n        else:\n            loss_function = MaskedNLLLoss()\n\n    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.l2)\n\n    train_loader, valid_loader, test_loader = get_IEMOCAP_loaders(valid=0.0,\n                                                                  batch_size=batch_size,\n                                                                  num_workers=0)\n\n    best_fscore, best_loss, best_label, best_pred, best_mask = None, None, None, None, None\n    all_fscore, all_acc, all_loss = [], [], []\n\n    for e in range(n_epochs):\n        start_time = time.time()\n\n        if args.graph_model:\n            train_loss, train_acc, _, _, train_fscore, _, _, _, _, _ = train_or_eval_graph_model(model, loss_function, train_loader, e, cuda, optimizer, True)\n            valid_loss, valid_acc, _, _, valid_fscore, _, _, _, _, _ = train_or_eval_graph_model(model, loss_function, valid_loader, e, cuda)\n            test_loss, test_acc, test_label, test_pred, test_fscore, _, _, _, _, _ = train_or_eval_graph_model(model, loss_function, test_loader, e, cuda)\n            all_fscore.append(test_fscore)\n            # torch.save({'model_state_dict': model.state_dict()}, path + name + args.base_model + '_' + str(e) + '.pkl')\n\n\n        else:\n            train_loss, train_acc, _, _, _, train_fscore, _ = train_or_eval_model(model, loss_function, train_loader, e, optimizer, True)\n            valid_loss, valid_acc, _, _, _, valid_fscore, _ = train_or_eval_model(model, loss_function, valid_loader, e)\n            test_loss, test_acc, test_label, test_pred, test_mask, test_fscore, attentions = train_or_eval_model(model, loss_function, test_loader, e)\n            all_fscore.append(test_fscore)\n            # torch.save({'model_state_dict': model.state_dict()}, path + name + args.base_model + '_' + str(e) + '.pkl')\n\n        if args.tensorboard:\n            writer.add_scalar('test: accuracy/loss', test_acc/test_loss, e)\n            writer.add_scalar('train: accuracy/loss', train_acc/train_loss, e)\n\n        print('epoch: {}, train_loss: {}, train_acc: {}, train_fscore: {}, valid_loss: {}, valid_acc: {}, valid_fscore: {}, test_loss: {}, test_acc: {}, test_fscore: {}, time: {} sec'.\\\n                format(e+1, train_loss, train_acc, train_fscore, valid_loss, valid_acc, valid_fscore, test_loss, test_acc, test_fscore, round(time.time()-start_time, 2)))\n    \n\n    if args.tensorboard:\n        writer.close()\n\n    print('Test performance..')\n    print ('F-Score:', max(all_fscore))\n"""
DialogueRNN/dataloader.py,26,"b""import torch\nfrom torch.utils.data import Dataset\nfrom torch.nn.utils.rnn import pad_sequence\nimport pickle\nimport pandas as pd\n\nclass IEMOCAPDataset(Dataset):\n\n    def __init__(self, path, train=True):\n        self.videoIDs, self.videoSpeakers, self.videoLabels, self.videoText,\\\n        self.videoAudio, self.videoVisual, self.videoSentence, self.trainVid,\\\n        self.testVid = pickle.load(open(path, 'rb'), encoding='latin1')\n        '''\n        label index mapping = {'hap':0, 'sad':1, 'neu':2, 'ang':3, 'exc':4, 'fru':5}\n        '''\n        self.keys = [x for x in (self.trainVid if train else self.testVid)]\n\n        self.len = len(self.keys)\n\n    def __getitem__(self, index):\n        vid = self.keys[index]\n        return torch.FloatTensor(self.videoText[vid]),\\\n               torch.FloatTensor(self.videoVisual[vid]),\\\n               torch.FloatTensor(self.videoAudio[vid]),\\\n               torch.FloatTensor([[1,0] if x=='M' else [0,1] for x in\\\n                                  self.videoSpeakers[vid]]),\\\n               torch.FloatTensor([1]*len(self.videoLabels[vid])),\\\n               torch.LongTensor(self.videoLabels[vid]),\\\n               vid\n\n    def __len__(self):\n        return self.len\n\n    def collate_fn(self, data):\n        dat = pd.DataFrame(data)\n        return [pad_sequence(dat[i]) if i<4 else pad_sequence(dat[i], True) if i<6 else dat[i].tolist() for i in dat]\n\nclass AVECDataset(Dataset):\n\n    def __init__(self, path, train=True):\n        self.videoIDs, self.videoSpeakers, self.videoLabels, self.videoText,\\\n            self.videoAudio, self.videoVisual, self.videoSentence,\\\n            self.trainVid, self.testVid = pickle.load(open(path, 'rb'),encoding='latin1')\n\n        self.keys = [x for x in (self.trainVid if train else self.testVid)]\n\n        self.len = len(self.keys)\n\n    def __getitem__(self, index):\n        vid = self.keys[index]\n        return torch.FloatTensor(self.videoText[vid]),\\\n               torch.FloatTensor(self.videoVisual[vid]),\\\n               torch.FloatTensor(self.videoAudio[vid]),\\\n               torch.FloatTensor([[1,0] if x=='user' else [0,1] for x in\\\n                                  self.videoSpeakers[vid]]),\\\n               torch.FloatTensor([1]*len(self.videoLabels[vid])),\\\n               torch.FloatTensor(self.videoLabels[vid])\n\n    def __len__(self):\n        return self.len\n\n    def collate_fn(self, data):\n        dat = pd.DataFrame(data)\n        return [pad_sequence(dat[i]) if i<4 else pad_sequence(dat[i], True) for i in dat]\nclass MELDDataset(Dataset):\n\n    def __init__(self, path, n_classes, train=True):\n        if n_classes == 3:\n            self.videoIDs, self.videoSpeakers, _, self.videoText,\\\n            self.videoAudio, self.videoSentence, self.trainVid,\\\n            self.testVid, self.videoLabels = pickle.load(open(path, 'rb'))\n        elif n_classes == 7:\n            self.videoIDs, self.videoSpeakers, self.videoLabels, self.videoText,\\\n            self.videoAudio, self.videoSentence, self.trainVid,\\\n            self.testVid, _ = pickle.load(open(path, 'rb'))\n        '''\n        label index mapping = {'neutral': 0, 'surprise': 1, 'fear': 2, 'sadness': 3, 'joy': 4, 'disgust': 5, 'anger':6}\n        '''\n        self.keys = [x for x in (self.trainVid if train else self.testVid)]\n\n        self.len = len(self.keys)\n\n    def __getitem__(self, index):\n        vid = self.keys[index]\n        return torch.FloatTensor(self.videoText[vid]),\\\n               torch.FloatTensor(self.videoAudio[vid]),\\\n               torch.FloatTensor(self.videoSpeakers[vid]),\\\n               torch.FloatTensor([1]*len(self.videoLabels[vid])),\\\n               torch.LongTensor(self.videoLabels[vid]),\\\n               vid\n\n    def __len__(self):\n        return self.len\n\n    def collate_fn(self, data):\n        dat = pd.DataFrame(data)\n        return [pad_sequence(dat[i]) if i<3 else pad_sequence(dat[i], True) if i<5 else dat[i].tolist() for i in dat]\n\n\nclass DailyDialogueDataset(Dataset):\n\n    def __init__(self, split, path):\n        \n        self.Speakers, self.InputSequence, self.InputMaxSequenceLength, \\\n        self.ActLabels, self.EmotionLabels, self.trainId, self.testId, self.validId = pickle.load(open(path, 'rb'))\n        \n        if split == 'train':\n            self.keys = [x for x in self.trainId]\n        elif split == 'test':\n            self.keys = [x for x in self.testId]\n        elif split == 'valid':\n            self.keys = [x for x in self.validId]\n\n        self.len = len(self.keys)\n\n    def __getitem__(self, index):\n        conv = self.keys[index]\n        \n        return torch.LongTensor(self.InputSequence[conv]), \\\n                torch.FloatTensor([[1,0] if x=='0' else [0,1] for x in self.Speakers[conv]]),\\\n                torch.FloatTensor([1]*len(self.ActLabels[conv])), \\\n                torch.LongTensor(self.ActLabels[conv]), \\\n                torch.LongTensor(self.EmotionLabels[conv]), \\\n                self.InputMaxSequenceLength[conv], \\\n                conv\n\n    def __len__(self):\n        return self.len\n    \n\n\nclass DailyDialoguePadCollate:\n\n    def __init__(self, dim=0):\n        self.dim = dim\n\n    def pad_tensor(self, vec, pad, dim):\n\n        pad_size = list(vec.shape)\n        pad_size[dim] = pad - vec.size(dim)\n        return torch.cat([vec, torch.zeros(*pad_size).type(torch.LongTensor)], dim=dim)\n\n    def pad_collate(self, batch):\n        \n        # find longest sequence\n        max_len = max(map(lambda x: x.shape[self.dim], batch))\n        \n        # pad according to max_len\n        batch = [self.pad_tensor(x, pad=max_len, dim=self.dim) for x in batch]\n        \n        # stack all\n        return torch.stack(batch, dim=0)\n    \n    def __call__(self, batch):\n        dat = pd.DataFrame(batch)\n        \n        return [self.pad_collate(dat[i]).transpose(1, 0).contiguous() if i==0 else \\\n                pad_sequence(dat[i]) if i == 1 else \\\n                pad_sequence(dat[i], True) if i < 5 else \\\n                dat[i].tolist() for i in dat]\n"""
DialogueRNN/model.py,63,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pad_sequence\n\nclass SimpleAttention(nn.Module):\n\n    def __init__(self, input_dim):\n        super(SimpleAttention, self).__init__()\n        self.input_dim = input_dim\n        self.scalar = nn.Linear(self.input_dim,1,bias=False)\n\n    def forward(self, M, x=None):\n        """"""\n        M -> (seq_len, batch, vector)\n        x -> dummy argument for the compatibility with MatchingAttention\n        """"""\n        scale = self.scalar(M) # seq_len, batch, 1\n        alpha = F.softmax(scale, dim=0).permute(1,2,0) # batch, 1, seq_len\n        attn_pool = torch.bmm(alpha, M.transpose(0,1))[:,0,:] # batch, vector\n\n        return attn_pool, alpha\n\nclass MatchingAttention(nn.Module):\n\n    def __init__(self, mem_dim, cand_dim, alpha_dim=None, att_type=\'general\'):\n        super(MatchingAttention, self).__init__()\n        assert att_type!=\'concat\' or alpha_dim!=None\n        assert att_type!=\'dot\' or mem_dim==cand_dim\n        self.mem_dim = mem_dim\n        self.cand_dim = cand_dim\n        self.att_type = att_type\n        if att_type==\'general\':\n            self.transform = nn.Linear(cand_dim, mem_dim, bias=False)\n        if att_type==\'general2\':\n            self.transform = nn.Linear(cand_dim, mem_dim, bias=True)\n            #torch.nn.init.normal_(self.transform.weight,std=0.01)\n        elif att_type==\'concat\':\n            self.transform = nn.Linear(cand_dim+mem_dim, alpha_dim, bias=False)\n            self.vector_prod = nn.Linear(alpha_dim, 1, bias=False)\n\n    def forward(self, M, x, mask=None):\n        """"""\n        M -> (seq_len, batch, mem_dim)\n        x -> (batch, cand_dim)\n        mask -> (batch, seq_len)\n        """"""\n        if type(mask)==type(None):\n            mask = torch.ones(M.size(1), M.size(0)).type(M.type())\n\n        if self.att_type==\'dot\':\n            # vector = cand_dim = mem_dim\n            M_ = M.permute(1,2,0) # batch, vector, seqlen\n            x_ = x.unsqueeze(1) # batch, 1, vector\n            alpha = F.softmax(torch.bmm(x_, M_), dim=2) # batch, 1, seqlen\n        elif self.att_type==\'general\':\n            M_ = M.permute(1,2,0) # batch, mem_dim, seqlen\n            x_ = self.transform(x).unsqueeze(1) # batch, 1, mem_dim\n            alpha = F.softmax(torch.bmm(x_, M_), dim=2) # batch, 1, seqlen\n        elif self.att_type==\'general2\':\n            M_ = M.permute(1,2,0) # batch, mem_dim, seqlen\n            x_ = self.transform(x).unsqueeze(1) # batch, 1, mem_dim\n            alpha_ = F.softmax((torch.bmm(x_, M_))*mask.unsqueeze(1), dim=2) # batch, 1, seqlen\n            alpha_masked = alpha_*mask.unsqueeze(1) # batch, 1, seqlen\n            alpha_sum = torch.sum(alpha_masked, dim=2, keepdim=True) # batch, 1, 1\n            alpha = alpha_masked/alpha_sum # batch, 1, 1 ; normalized\n            #import ipdb;ipdb.set_trace()\n        else:\n            M_ = M.transpose(0,1) # batch, seqlen, mem_dim\n            x_ = x.unsqueeze(1).expand(-1,M.size()[0],-1) # batch, seqlen, cand_dim\n            M_x_ = torch.cat([M_,x_],2) # batch, seqlen, mem_dim+cand_dim\n            mx_a = F.tanh(self.transform(M_x_)) # batch, seqlen, alpha_dim\n            alpha = F.softmax(self.vector_prod(mx_a),1).transpose(1,2) # batch, 1, seqlen\n\n        attn_pool = torch.bmm(alpha, M.transpose(0,1))[:,0,:] # batch, mem_dim\n\n        return attn_pool, alpha\n\n\nclass DialogueRNNCell(nn.Module):\n\n    def __init__(self, D_m, D_g, D_p, D_e, listener_state=False,\n                            context_attention=\'simple\', D_a=100, dropout=0.5):\n        super(DialogueRNNCell, self).__init__()\n\n        self.D_m = D_m\n        self.D_g = D_g\n        self.D_p = D_p\n        self.D_e = D_e\n\n        self.listener_state = listener_state\n        self.g_cell = nn.GRUCell(D_m+D_p,D_g)\n        self.p_cell = nn.GRUCell(D_m+D_g,D_p)\n        self.e_cell = nn.GRUCell(D_p,D_e)\n        if listener_state:\n            self.l_cell = nn.GRUCell(D_m+D_p,D_p)\n\n        self.dropout = nn.Dropout(dropout)\n\n        if context_attention==\'simple\':\n            self.attention = SimpleAttention(D_g)\n        else:\n            self.attention = MatchingAttention(D_g, D_m, D_a, context_attention)\n\n    def _select_parties(self, X, indices):\n        q0_sel = []\n        for idx, j in zip(indices, X):\n            q0_sel.append(j[idx].unsqueeze(0))\n        q0_sel = torch.cat(q0_sel,0)\n        return q0_sel\n\n    def forward(self, U, qmask, g_hist, q0, e0):\n        """"""\n        U -> batch, D_m\n        qmask -> batch, party\n        g_hist -> t-1, batch, D_g\n        q0 -> batch, party, D_p\n        e0 -> batch, self.D_e\n        """"""\n        qm_idx = torch.argmax(qmask, 1)\n        q0_sel = self._select_parties(q0, qm_idx)\n\n        g_ = self.g_cell(torch.cat([U,q0_sel], dim=1),\n                torch.zeros(U.size()[0],self.D_g).type(U.type()) if g_hist.size()[0]==0 else\n                g_hist[-1])\n        g_ = self.dropout(g_)\n        if g_hist.size()[0]==0:\n            c_ = torch.zeros(U.size()[0],self.D_g).type(U.type())\n            alpha = None\n        else:\n            c_, alpha = self.attention(g_hist,U)\n        # c_ = torch.zeros(U.size()[0],self.D_g).type(U.type()) if g_hist.size()[0]==0\\\n        #         else self.attention(g_hist,U)[0] # batch, D_g\n        U_c_ = torch.cat([U,c_], dim=1).unsqueeze(1).expand(-1,qmask.size()[1],-1)\n        qs_ = self.p_cell(U_c_.contiguous().view(-1,self.D_m+self.D_g),\n                q0.view(-1, self.D_p)).view(U.size()[0],-1,self.D_p)\n        qs_ = self.dropout(qs_)\n\n        if self.listener_state:\n            U_ = U.unsqueeze(1).expand(-1,qmask.size()[1],-1).contiguous().view(-1,self.D_m)\n            ss_ = self._select_parties(qs_, qm_idx).unsqueeze(1).\\\n                    expand(-1,qmask.size()[1],-1).contiguous().view(-1,self.D_p)\n            U_ss_ = torch.cat([U_,ss_],1)\n            ql_ = self.l_cell(U_ss_,q0.view(-1, self.D_p)).view(U.size()[0],-1,self.D_p)\n            ql_ = self.dropout(ql_)\n        else:\n            ql_ = q0\n        qmask_ = qmask.unsqueeze(2)\n        q_ = ql_*(1-qmask_) + qs_*qmask_\n        e0 = torch.zeros(qmask.size()[0], self.D_e).type(U.type()) if e0.size()[0]==0\\\n                else e0\n        e_ = self.e_cell(self._select_parties(q_,qm_idx), e0)\n        e_ = self.dropout(e_)\n\n        return g_,q_,e_,alpha\n\nclass DialogueRNN(nn.Module):\n\n    def __init__(self, D_m, D_g, D_p, D_e, listener_state=False,\n                            context_attention=\'simple\', D_a=100, dropout=0.5):\n        super(DialogueRNN, self).__init__()\n\n        self.D_m = D_m\n        self.D_g = D_g\n        self.D_p = D_p\n        self.D_e = D_e\n        self.dropout = nn.Dropout(dropout)\n\n        self.dialogue_cell = DialogueRNNCell(D_m, D_g, D_p, D_e,\n                            listener_state, context_attention, D_a, dropout)\n\n    def forward(self, U, qmask):\n        """"""\n        U -> seq_len, batch, D_m\n        qmask -> seq_len, batch, party\n        """"""\n\n        g_hist = torch.zeros(0).type(U.type()) # 0-dimensional tensor\n        q_ = torch.zeros(qmask.size()[1], qmask.size()[2],\n                                    self.D_p).type(U.type()) # batch, party, D_p\n        e_ = torch.zeros(0).type(U.type()) # batch, D_e\n        e = e_\n\n        alpha = []\n        for u_,qmask_ in zip(U, qmask):\n            g_, q_, e_, alpha_ = self.dialogue_cell(u_, qmask_, g_hist, q_, e_)\n            g_hist = torch.cat([g_hist, g_.unsqueeze(0)],0)\n            e = torch.cat([e, e_.unsqueeze(0)],0)\n            if type(alpha_)!=type(None):\n                alpha.append(alpha_[:,0,:])\n\n        return e,alpha # seq_len, batch, D_e\nclass BiModel(nn.Module):\n\n    def __init__(self, D_m, D_g, D_p, D_e, D_h,\n                 n_classes=7, listener_state=False, context_attention=\'simple\', D_a=100, dropout_rec=0.5,\n                 dropout=0.5):\n        super(BiModel, self).__init__()\n\n        self.D_m       = D_m\n        self.D_g       = D_g\n        self.D_p       = D_p\n        self.D_e       = D_e\n        self.D_h       = D_h\n        self.n_classes = n_classes\n        self.dropout   = nn.Dropout(dropout)\n        self.dropout_rec = nn.Dropout(dropout+0.15)\n        self.dialog_rnn_f = DialogueRNN(D_m, D_g, D_p, D_e,listener_state,\n                                    context_attention, D_a, dropout_rec)\n        self.dialog_rnn_r = DialogueRNN(D_m, D_g, D_p, D_e,listener_state,\n                                    context_attention, D_a, dropout_rec)\n        self.linear     = nn.Linear(2*D_e, 2*D_h)\n        self.smax_fc    = nn.Linear(2*D_h, n_classes)\n        self.matchatt = MatchingAttention(2*D_e,2*D_e,att_type=\'general2\')\n\n    def _reverse_seq(self, X, mask):\n        """"""\n        X -> seq_len, batch, dim\n        mask -> batch, seq_len\n        """"""\n        X_ = X.transpose(0,1)\n        mask_sum = torch.sum(mask, 1).int()\n\n        xfs = []\n        for x, c in zip(X_, mask_sum):\n            xf = torch.flip(x[:c], [0])\n            xfs.append(xf)\n\n        return pad_sequence(xfs)\n\n\n    def forward(self, U, qmask, umask,att2=True):\n        """"""\n        U -> seq_len, batch, D_m\n        qmask -> seq_len, batch, party\n        """"""\n\n        emotions_f, alpha_f = self.dialog_rnn_f(U, qmask) # seq_len, batch, D_e\n        emotions_f = self.dropout_rec(emotions_f)\n        rev_U = self._reverse_seq(U, umask)\n        rev_qmask = self._reverse_seq(qmask, umask)\n        emotions_b, alpha_b = self.dialog_rnn_r(rev_U, rev_qmask)\n        emotions_b = self._reverse_seq(emotions_b, umask)\n        emotions_b = self.dropout_rec(emotions_b)\n        emotions = torch.cat([emotions_f,emotions_b],dim=-1)\n        if att2:\n            att_emotions = []\n            alpha = []\n            for t in emotions:\n                att_em, alpha_ = self.matchatt(emotions,t,mask=umask)\n                att_emotions.append(att_em.unsqueeze(0))\n                alpha.append(alpha_[:,0,:])\n            att_emotions = torch.cat(att_emotions,dim=0)\n            hidden = F.relu(self.linear(att_emotions))\n        else:\n            hidden = F.relu(self.linear(emotions))\n        #hidden = F.relu(self.linear(emotions))\n        hidden = self.dropout(hidden)\n        log_prob = F.log_softmax(self.smax_fc(hidden), 2) # seq_len, batch, n_classes\n        return log_prob, alpha, alpha_f, alpha_b\n\nclass BiE2EModel(nn.Module):\n\n    def __init__(self, D_emb, D_m, D_g, D_p, D_e, D_h, word_embeddings,\n                 n_classes=7, listener_state=False, context_attention=\'simple\', D_a=100, dropout_rec=0.5,\n                 dropout=0.5):\n        super(BiE2EModel, self).__init__()\n\n        self.D_emb     = D_emb\n        self.D_m       = D_m\n        self.D_g       = D_g\n        self.D_p       = D_p\n        self.D_e       = D_e\n        self.D_h       = D_h\n        self.n_classes = n_classes\n        self.dropout   = nn.Dropout(dropout)\n        #self.dropout_rec = nn.Dropout(0.2)\n        self.dropout_rec = nn.Dropout(dropout)\n        self.turn_rnn = nn.GRU(D_emb, D_m)\n        self.dialog_rnn_f = DialogueRNN(D_m, D_g, D_p, D_e,listener_state,\n                                    context_attention, D_a, dropout_rec)\n        self.dialog_rnn_r = DialogueRNN(D_m, D_g, D_p, D_e,listener_state,\n                                    context_attention, D_a, dropout_rec)\n        self.linear1     = nn.Linear(2*D_e, D_h)\n        #self.linear2     = nn.Linear(D_h, D_h)\n        #self.linear3     = nn.Linear(D_h, D_h)\n        self.smax_fc    = nn.Linear(D_h, n_classes)\n        self.embedding = nn.Embedding(word_embeddings.shape[0],word_embeddings.shape[1])\n        self.embedding.weight.data.copy_(word_embeddings)\n        self.embedding.weight.requires_grad = True\n        self.matchatt = MatchingAttention(2*D_e,2*D_e,att_type=\'general2\')\n    def _reverse_seq(self, X, mask):\n        """"""\n        X -> seq_len, batch, dim\n        mask -> batch, seq_len\n        """"""\n        X_ = X.transpose(0,1)\n        mask_sum = torch.sum(mask, 1).int()\n\n        xfs = []\n        for x, c in zip(X_, mask_sum):\n            xf = torch.flip(x[:c], [0])\n            xfs.append(xf)\n\n        return pad_sequence(xfs)\n\n    def forward(self, data, att2=False):\n\n        #T1 = word_embeddings[data.turn1] # seq_len, batch, D_emb\n        #T2 = word_embeddings[data.turn2] # seq_len, batch, D_emb\n        #T3 = word_embeddings[data.turn3] # seq_len, batch, D_emb\n\n        T1 = (self.embedding(data.turn1))\n        T2 = (self.embedding(data.turn2))\n        T3 = (self.embedding(data.turn3))\n\n        T1_, h_out1 = self.turn_rnn(T1,\n                                    torch.zeros(1, T1.size(1), self.D_m).type(T1.type()))\n        T2_, h_out2 = self.turn_rnn(T2,\n                                    torch.zeros(1, T1.size(1), self.D_m).type(T1.type()))\n        T3_, h_out3 = self.turn_rnn(T3,\n                                    torch.zeros(1, T1.size(1), self.D_m).type(T1.type()))\n\n        U = torch.cat([h_out1, h_out2, h_out3], 0) # 3, batch, D_m\n\n        qmask = torch.FloatTensor([[1,0],[0,1],[1,0]]).type(T1.type())\n        qmask = qmask.unsqueeze(1).expand(-1, T1.size(1), -1)\n\n        umask = torch.FloatTensor([[1,1,1]]).type(T1.type())\n        umask = umask.expand( T1.size(1),-1)\n\n        emotions_f, alpha_f = self.dialog_rnn_f(U, qmask) # seq_len, batch, D_e\n        emotions_f = self.dropout_rec(emotions_f)\n        rev_U = self._reverse_seq(U, umask)\n        rev_qmask = self._reverse_seq(qmask, umask)\n        emotions_b, alpha_b = self.dialog_rnn_r(rev_U, rev_qmask)\n        emotions_b = self._reverse_seq(emotions_b, umask)\n        #emotions_b = self.dropout_rec(emotions_b)\n        emotions = torch.cat([emotions_f,emotions_b],dim=-1)\n        #print(emotions)\n        emotions = self.dropout_rec(emotions)\n\n        #emotions = emotions.unsqueeze(1)\n        if att2:\n            att_emotion, _ = self.matchatt(emotions, emotions[-1])\n            hidden = F.relu(self.linear1(att_emotion))\n        else:\n            hidden = F.relu(self.linear1(emotions[-1]))\n        #hidden = F.relu(self.linear2(hidden))\n        #hidden = F.relu(self.linear3(hidden))\n       # hidden = self.dropout(hidden)\n        log_prob = F.log_softmax(self.smax_fc(hidden), -1) # batch, n_classes\n        return log_prob\n\nclass E2EModel(nn.Module):\n\n    def __init__(self, D_emb, D_m, D_g, D_p, D_e, D_h,\n                 n_classes=7, listener_state=False, context_attention=\'simple\', D_a=100, dropout_rec=0.5,\n                 dropout=0.5):\n        super(E2EModel, self).__init__()\n\n        self.D_emb     = D_emb\n        self.D_m       = D_m\n        self.D_g       = D_g\n        self.D_p       = D_p\n        self.D_e       = D_e\n        self.D_h       = D_h\n        self.n_classes = n_classes\n        self.dropout   = nn.Dropout(dropout)\n        #self.dropout_rec = nn.Dropout(0.2)\n        self.dropout_rec = nn.Dropout(dropout+0.15)\n        self.turn_rnn = nn.GRU(D_emb, D_m)\n        self.dialog_rnn = DialogueRNN(D_m, D_g, D_p, D_e,listener_state,\n                                    context_attention, D_a, dropout_rec)\n        self.linear1     = nn.Linear(D_e, D_h)\n        #self.linear2     = nn.Linear(D_h, D_h)\n        #self.linear3     = nn.Linear(D_h, D_h)\n        self.smax_fc    = nn.Linear(D_h, n_classes)\n\n        self.matchatt = MatchingAttention(D_e,D_e,att_type=\'general2\')\n\n    def forward(self, data, word_embeddings, att2=False):\n\n        T1 = word_embeddings[data.turn1] # seq_len, batch, D_emb\n        T2 = word_embeddings[data.turn2] # seq_len, batch, D_emb\n        T3 = word_embeddings[data.turn3] # seq_len, batch, D_emb\n\n        T1_, h_out1 = self.turn_rnn(T1,\n                                    torch.zeros(1, T1.size(1), self.D_m).type(T1.type()))\n        T2_, h_out2 = self.turn_rnn(T2,\n                                    torch.zeros(1, T1.size(1), self.D_m).type(T1.type()))\n        T3_, h_out3 = self.turn_rnn(T3,\n                                    torch.zeros(1, T1.size(1), self.D_m).type(T1.type()))\n\n        U = torch.cat([h_out1, h_out2, h_out3], 0) # 3, batch, D_m\n\n        qmask = torch.FloatTensor([[1,0],[0,1],[1,0]]).type(T1.type())\n        qmask = qmask.unsqueeze(1).expand(-1, T1.size(1), -1)\n\n        emotions, _ = self.dialog_rnn(U, qmask) # seq_len, batch, D_e\n        #print(emotions)\n        emotions = self.dropout_rec(emotions)\n\n        #emotions = emotions.unsqueeze(1)\n        if att2:\n            att_emotion, _ = self.matchatt(emotions,emotions[-1])\n            hidden = F.relu(self.linear1(att_emotion))\n        else:\n            hidden = F.relu(self.linear1(emotions[-1]))\n        #hidden = F.relu(self.linear2(hidden))\n        #hidden = F.relu(self.linear3(hidden))\n        hidden = self.dropout(hidden)\n        log_prob = F.log_softmax(self.smax_fc(hidden), -1) # batch, n_classes\n        return log_prob\nclass Model(nn.Module):\n\n    def __init__(self, D_m, D_g, D_p, D_e, D_h,\n                 n_classes=7, listener_state=False, context_attention=\'simple\', D_a=100, dropout_rec=0.5,\n                 dropout=0.5):\n        super(Model, self).__init__()\n\n        self.D_m       = D_m\n        self.D_g       = D_g\n        self.D_p       = D_p\n        self.D_e       = D_e\n        self.D_h       = D_h\n        self.n_classes = n_classes\n        self.dropout   = nn.Dropout(dropout)\n        #self.dropout_rec = nn.Dropout(0.2)\n        self.dropout_rec = nn.Dropout(dropout+0.15)\n        self.dialog_rnn = DialogueRNN(D_m, D_g, D_p, D_e,listener_state,\n                                    context_attention, D_a, dropout_rec)\n        self.linear1     = nn.Linear(D_e, D_h)\n        #self.linear2     = nn.Linear(D_h, D_h)\n        #self.linear3     = nn.Linear(D_h, D_h)\n        self.smax_fc    = nn.Linear(D_h, n_classes)\n\n        self.matchatt = MatchingAttention(D_e,D_e,att_type=\'general2\')\n\n    def forward(self, U, qmask, umask=None, att2=False):\n        """"""\n        U -> seq_len, batch, D_m\n        qmask -> seq_len, batch, party\n        """"""\n\n        emotions = self.dialog_rnn(U, qmask) # seq_len, batch, D_e\n        #print(emotions)\n        emotions = self.dropout_rec(emotions)\n\n        #emotions = emotions.unsqueeze(1)\n        if att2:\n            att_emotions = []\n            for t in emotions:\n                att_emotions.append(self.matchatt(emotions,t,mask=umask)[0].unsqueeze(0))\n            att_emotions = torch.cat(att_emotions,dim=0)\n            hidden = F.relu(self.linear1(att_emotions))\n        else:\n            hidden = F.relu(self.linear1(emotions))\n        #hidden = F.relu(self.linear2(hidden))\n        #hidden = F.relu(self.linear3(hidden))\n        hidden = self.dropout(hidden)\n        log_prob = F.log_softmax(self.smax_fc(hidden), 2) # seq_len, batch, n_classes\n        return log_prob\n\nclass AVECModel(nn.Module):\n\n    def __init__(self, D_m, D_g, D_p, D_e, D_h, attr, listener_state=False,\n            context_attention=\'simple\', D_a=100, dropout_rec=0.5, dropout=0.5):\n        super(AVECModel, self).__init__()\n\n        self.D_m         = D_m\n        self.D_g         = D_g\n        self.D_p         = D_p\n        self.D_e         = D_e\n        self.D_h         = D_h\n        self.attr        = attr\n        self.dropout     = nn.Dropout(dropout)\n        self.dropout_rec = nn.Dropout(dropout)\n        self.dialog_rnn  = DialogueRNN(D_m, D_g, D_p, D_e,listener_state,\n                                    context_attention, D_a, dropout_rec)\n        self.linear      = nn.Linear(D_e, D_h)\n        self.smax_fc     = nn.Linear(D_h, 1)\n\n    def forward(self, U, qmask):\n        """"""\n        U -> seq_len, batch, D_m\n        qmask -> seq_len, batch, party\n        """"""\n\n        emotions,_ = self.dialog_rnn(U, qmask) # seq_len, batch, D_e\n        emotions = self.dropout_rec(emotions)\n        hidden = torch.tanh(self.linear(emotions))\n        hidden = self.dropout(hidden)\n        if self.attr!=4:\n            pred = (self.smax_fc(hidden).squeeze()) # seq_len, batch\n        else:\n            pred = (self.smax_fc(hidden).squeeze()) # seq_len, batch\n        return pred.transpose(0,1).contiguous().view(-1)\n\nclass MaskedNLLLoss(nn.Module):\n\n    def __init__(self, weight=None):\n        super(MaskedNLLLoss, self).__init__()\n        self.weight = weight\n        self.loss = nn.NLLLoss(weight=weight,\n                               reduction=\'sum\')\n\n    def forward(self, pred, target, mask):\n        """"""\n        pred -> batch*seq_len, n_classes\n        target -> batch*seq_len\n        mask -> batch, seq_len\n        """"""\n        mask_ = mask.view(-1,1) # batch*seq_len, 1\n        if type(self.weight)==type(None):\n            loss = self.loss(pred*mask_, target)/torch.sum(mask)\n        else:\n            loss = self.loss(pred*mask_, target)\\\n                            /torch.sum(self.weight[target]*mask_.squeeze())\n        return loss\n\nclass MaskedMSELoss(nn.Module):\n\n    def __init__(self):\n        super(MaskedMSELoss, self).__init__()\n        self.loss = nn.MSELoss(reduction=\'sum\')\n\n    def forward(self, pred, target, mask):\n        """"""\n        pred -> batch*seq_len\n        target -> batch*seq_len\n        mask -> batch*seq_len\n        """"""\n        loss = self.loss(pred*mask, target)/torch.sum(mask)\n        return loss\n\nif torch.cuda.is_available():\n    FloatTensor = torch.cuda.FloatTensor\n    LongTensor = torch.cuda.LongTensor\n    ByteTensor = torch.cuda.ByteTensor\n\nelse:\n    FloatTensor = torch.FloatTensor\n    LongTensor = torch.LongTensor\n    ByteTensor = torch.ByteTensor\n\nclass CNNFeatureExtractor(nn.Module):\n    \n    def __init__(self, vocab_size, embedding_dim, output_size, filters, kernel_sizes, dropout):\n        super(CNNFeatureExtractor, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.convs = nn.ModuleList([nn.Conv1d(in_channels=embedding_dim, out_channels=filters, kernel_size=K) for K in kernel_sizes])\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(len(kernel_sizes) * filters, output_size)\n        self.feature_dim = output_size\n\n\n    def init_pretrained_embeddings_from_numpy(self, pretrained_word_vectors):\n        self.embedding.weight = nn.Parameter(torch.from_numpy(pretrained_word_vectors).float())\n        # if is_static:\n        self.embedding.weight.requires_grad = False\n\n\n    def forward(self, x, umask):\n        \n        num_utt, batch, num_words = x.size()\n        \n        x = x.type(LongTensor)  # (num_utt, batch, num_words)\n        x = x.view(-1, num_words) # (num_utt, batch, num_words) -> (num_utt * batch, num_words)\n        emb = self.embedding(x) # (num_utt * batch, num_words) -> (num_utt * batch, num_words, 300) \n        emb = emb.transpose(-2, -1).contiguous() # (num_utt * batch, num_words, 300)  -> (num_utt * batch, 300, num_words) \n        \n        convoluted = [F.relu(conv(emb)) for conv in self.convs] \n        pooled = [F.max_pool1d(c, c.size(2)).squeeze() for c in convoluted] \n        concated = torch.cat(pooled, 1)\n        features = F.relu(self.fc(self.dropout(concated))) # (num_utt * batch, 150) -> (num_utt * batch, 100)\n        features = features.view(num_utt, batch, -1) # (num_utt * batch, 100) -> (num_utt, batch, 100)\n        mask = umask.unsqueeze(-1).type(FloatTensor) # (batch, num_utt) -> (batch, num_utt, 1)\n        mask = mask.transpose(0, 1) # (batch, num_utt, 1) -> (num_utt, batch, 1)\n        mask = mask.repeat(1, 1, self.feature_dim) #  (num_utt, batch, 1) -> (num_utt, batch, 100)\n        features = (features * mask) # (num_utt, batch, 100) -> (num_utt, batch, 100)\n\n        return features\n\nclass DailyDialogueModel(nn.Module):\n\n    def __init__(self, D_m, D_g, D_p, D_e, D_h,\n                 vocab_size, n_classes=7, embedding_dim=300, \n                 cnn_output_size=100, cnn_filters=50, cnn_kernel_sizes=(3,4,5), cnn_dropout=0.5,\n                 listener_state=False, context_attention=\'simple\', D_a=100, dropout_rec=0.5,\n                 dropout=0.5, att2=True):\n        \n        super(DailyDialogueModel, self).__init__()\n\n        self.cnn_feat_extractor = CNNFeatureExtractor(vocab_size, embedding_dim, cnn_output_size, cnn_filters, cnn_kernel_sizes, cnn_dropout)\n                \n        self.D_m       = D_m\n        self.D_g       = D_g\n        self.D_p       = D_p\n        self.D_e       = D_e\n        self.D_h       = D_h\n        self.dropout   = nn.Dropout(dropout)\n        self.dropout_rec = nn.Dropout(dropout_rec)\n        self.dialog_rnn_f = DialogueRNN(D_m, D_g, D_p, D_e, listener_state,\n                                    context_attention, D_a, dropout_rec)\n        self.dialog_rnn_r = DialogueRNN(D_m, D_g, D_p, D_e, listener_state,\n                                    context_attention, D_a, dropout_rec)\n        self.linear     = nn.Linear(2*D_e, 2*D_h)\n        self.matchatt = MatchingAttention(2*D_e,2*D_e,att_type=\'general2\')\n\n        self.n_classes = n_classes\n        self.smax_fc    = nn.Linear(2*D_h, n_classes)\n        self.att2 = att2\n\n        \n    \n    def init_pretrained_embeddings(self, pretrained_word_vectors):\n        self.cnn_feat_extractor.init_pretrained_embeddings_from_numpy(pretrained_word_vectors)\n\n\n    def _reverse_seq(self, X, mask):\n        """"""\n        X -> seq_len, batch, dim\n        mask -> batch, seq_len\n        """"""\n        X_ = X.transpose(0,1)\n        mask_sum = torch.sum(mask, 1).int()\n\n        xfs = []\n        for x, c in zip(X_, mask_sum):\n            xf = torch.flip(x[:c], [0])\n            xfs.append(xf)\n\n        return pad_sequence(xfs)\n\n\n    def forward(self, input_seq, qmask, umask):\n        """"""\n        U -> seq_len, batch, D_m\n        qmask -> seq_len, batch, party\n        """"""\n\n        U = self.cnn_feat_extractor(input_seq, umask)\n\n        emotions_f, alpha_f = self.dialog_rnn_f(U, qmask) # seq_len, batch, D_e\n        emotions_f = self.dropout_rec(emotions_f)\n        rev_U = self._reverse_seq(U, umask)\n        rev_qmask = self._reverse_seq(qmask, umask)\n        emotions_b, alpha_b = self.dialog_rnn_r(rev_U, rev_qmask)\n        emotions_b = self._reverse_seq(emotions_b, umask)\n        emotions_b = self.dropout_rec(emotions_b)\n        emotions = torch.cat([emotions_f, emotions_b], dim=-1)\n        if self.att2:\n            att_emotions = []\n            alpha = []\n            for t in emotions:\n                att_em, alpha_ = self.matchatt(emotions,t,mask=umask)\n                att_emotions.append(att_em.unsqueeze(0))\n                alpha.append(alpha_[:,0,:])\n            att_emotions = torch.cat(att_emotions,dim=0)\n            hidden = F.relu(self.linear(att_emotions))\n        else:\n            hidden = F.relu(self.linear(emotions))\n        # hidden = F.relu(self.linear(emotions))\n        hidden = self.dropout(hidden)\n        log_prob = F.log_softmax(self.smax_fc(hidden), 2) # seq_len, batch, n_classes\n        return log_prob, alpha, alpha_f, alpha_b\n\nclass UnMaskedWeightedNLLLoss(nn.Module):\n\n    def __init__(self, weight=None):\n        super(UnMaskedWeightedNLLLoss, self).__init__()\n        self.weight = weight\n        self.loss = nn.NLLLoss(weight=weight,\n                               reduction=\'sum\')\n\n    def forward(self, pred, target):\n        """"""\n        pred -> batch*seq_len, n_classes\n        target -> batch*seq_len\n        """"""\n        if type(self.weight)==type(None):\n            loss = self.loss(pred, target)\n        else:\n            loss = self.loss(pred, target)\\\n                            /torch.sum(self.weight[target])\n        return loss\n\n'"
DialogueRNN/preprocess_dailydialog.py,0,"b'import nltk, json, pandas as pd, numpy as np, pickle\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n\ndef preprocess_text(x):\n    for punct in \'""!&?.,}-/<>#$%\\()*+:;=?@[\\\\]^_`|\\~\':\n        x = x.replace(punct, \' \')\n    \n    x = \' \'.join(x.split())\n    x = x.lower()\n    \n    return x\n\n\ndef create_utterances(filename, split):\n    sentences, act_labels, emotion_labels, speakers, conv_id, utt_id = [], [], [], [], [], []\n    \n    lengths = []\n    with open(filename, \'r\') as f:\n        for c_id, line in enumerate(f):\n            s = eval(line)\n            for u_id, item in enumerate(s[\'dialogue\']):\n                sentences.append(item[\'text\'])\n                act_labels.append(item[\'act\'])\n                emotion_labels.append(item[\'emotion\'])\n                conv_id.append(split[:2] + \'_c\' + str(c_id))\n                utt_id.append(split[:2] + \'_c\' + str(c_id) + \'_u\' + str(u_id))\n                speakers.append(str(u_id%2))\n                \n                # u_id += 1\n                \n    data = pd.DataFrame(sentences, columns=[\'sentence\'])\n    data[\'sentence\'] = data[\'sentence\'].apply(lambda x: preprocess_text(x))\n    data[\'act_label\'] = act_labels\n    data[\'emotion_label\'] = emotion_labels\n    data[\'speaker\'] = speakers\n    data[\'conv_id\'] = conv_id\n    data[\'utt_id\'] = utt_id\n    \n    return data\n\n\ndef load_pretrained_glove():\n    print(""Loading GloVe model, this can take some time..."")\n    glv_vector = {}\n    f = open(\'/media/backup/nlp-cic/glove.840B.300d.txt\', encoding=\'utf-8\')\n\n    for line in f:\n        values = line.split()\n        word = values[0]\n        try:\n            coefs = np.asarray(values[1:], dtype=\'float\')\n            glv_vector[word] = coefs\n        except ValueError:\n            continue\n    f.close()\n    print(""Completed loading pretrained GloVe model."")\n    return glv_vector\n\ndef encode_labels(encoder, l):\n    return encoder[l]\n\n\nif __name__ == \'__main__\':\n\n    train_data = create_utterances(\'data/dailydialog/train.json\', \'train\')\n    valid_data = create_utterances(\'data/dailydialog/valid.json\', \'valid\')\n    test_data = create_utterances(\'data/dailydialog/test.json\', \'test\')\n    \n    ## encode the emotion and dialog act labels ##\n    all_act_labels, all_emotion_labels = set(train_data[\'act_label\']), set(train_data[\'emotion_label\'])\n    act_label_encoder, emotion_label_encoder, act_label_decoder, emotion_label_decoder = {}, {}, {}, {}\n\n    for i, label in enumerate(all_act_labels):\n        act_label_encoder[label] = i\n        act_label_decoder[i] = label\n    \n    for i, label in enumerate(all_emotion_labels):\n        emotion_label_encoder[label] = i\n        emotion_label_decoder[i] = label\n\n    pickle.dump(act_label_encoder, open(\'data/dailydialog/act_label_encoder.pkl\', \'wb\'))\n    pickle.dump(act_label_decoder, open(\'data/dailydialog/act_label_decoder.pkl\', \'wb\'))\n    pickle.dump(emotion_label_encoder, open(\'data/dailydialog/emotion_label_encoder.pkl\', \'wb\'))\n    pickle.dump(emotion_label_decoder, open(\'data/dailydialog/emotion_label_decoder.pkl\', \'wb\'))\n\n    train_data[\'encoded_act_label\'] = train_data[\'act_label\'].map(lambda x: encode_labels(act_label_encoder, x))\n    test_data[\'encoded_act_label\'] = test_data[\'act_label\'].map(lambda x: encode_labels(act_label_encoder, x))\n    valid_data[\'encoded_act_label\'] = valid_data[\'act_label\'].map(lambda x: encode_labels(act_label_encoder, x))\n\n    train_data[\'encoded_emotion_label\'] = train_data[\'emotion_label\'].map(lambda x: encode_labels(emotion_label_encoder, x))\n    test_data[\'encoded_emotion_label\'] = test_data[\'emotion_label\'].map(lambda x: encode_labels(emotion_label_encoder, x))\n    valid_data[\'encoded_emotion_label\'] = valid_data[\'emotion_label\'].map(lambda x: encode_labels(emotion_label_encoder, x))\n    \n    \n    ## tokenize all sentences ##\n    all_text = list(train_data[\'sentence\'])\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(all_text)\n    pickle.dump(tokenizer, open(\'data/dailydialog/tokenizer.pkl\', \'wb\'))\n\n    ## convert the sentences into sequences ##\n    train_sequence = tokenizer.texts_to_sequences(list(train_data[\'sentence\']))\n    valid_sequence = tokenizer.texts_to_sequences(list(valid_data[\'sentence\']))\n    test_sequence = tokenizer.texts_to_sequences(list(test_data[\'sentence\']))\n    \n    train_data[\'sentence_length\'] = [len(item) for item in train_sequence]\n    valid_data[\'sentence_length\'] = [len(item) for item in valid_sequence]\n    test_data[\'sentence_length\'] = [len(item) for item in test_sequence]\n    \n    max_num_tokens = 250\n\n    train_sequence = pad_sequences(train_sequence, maxlen=max_num_tokens, padding=\'post\')\n    valid_sequence = pad_sequences(valid_sequence, maxlen=max_num_tokens, padding=\'post\')\n    test_sequence = pad_sequences(test_sequence, maxlen=max_num_tokens, padding=\'post\')\n\n    train_data[\'sequence\'] = list(train_sequence)\n    valid_data[\'sequence\'] = list(valid_sequence)\n    test_data[\'sequence\'] = list(test_sequence)\n    \n    ## save the data in pickle format ##\n    convSpeakers, convInputSequence, convInputMaxSequenceLength, convActLabels, convEmotionLabels = {}, {}, {}, {}, {}\n    train_conv_ids, test_conv_ids, valid_conv_ids = set(train_data[\'conv_id\']), set(test_data[\'conv_id\']), set(valid_data[\'conv_id\'])\n    all_data = train_data.append(test_data, ignore_index=True).append(valid_data, ignore_index=True)\n    \n    print (\'Preparing dataset. Hang on...\')\n    for item in list(train_conv_ids) + list(test_conv_ids) + list(valid_conv_ids):\n\n        df = all_data[all_data[\'conv_id\'] == item]\n        \n        convSpeakers[item] = list(df[\'speaker\'])\n        convInputSequence[item] = list(df[\'sequence\'])\n        convInputMaxSequenceLength[item] = max(list(df[\'sentence_length\']))\n        convActLabels[item] = list(df[\'encoded_act_label\'])\n        convEmotionLabels[item] = list(df[\'encoded_emotion_label\'])\n        \n    pickle.dump([convSpeakers, convInputSequence, convInputMaxSequenceLength, convActLabels, convEmotionLabels,\n                 train_conv_ids, test_conv_ids, valid_conv_ids], open(\'data/dailydialog/daily_dialogue.pkl\', \'wb\'))\n    \n    \n    ## save pretrained embedding matrix ##\n    glv_vector = load_pretrained_glove()\n    word_vector_length = len(glv_vector[\'the\'])\n    word_index = tokenizer.word_index\n    inv_word_index = {v: k for k, v in word_index.items()}\n    num_unique_words = len(word_index)\n    glv_embedding_matrix = np.zeros((num_unique_words+1, word_vector_length))\n\n    for j in range(1, num_unique_words+1):\n        try:\n            glv_embedding_matrix[j] = glv_vector[inv_word_index[j]]\n        except KeyError:\n            glv_embedding_matrix[j] = np.random.randn(word_vector_length)/200\n\n    np.ndarray.dump(glv_embedding_matrix, open(\'data/dailydialog/glv_embedding_matrix\', \'wb\'))\n    print (\'Done. Completed preprocessing.\')\n'"
DialogueRNN/train_AVEC.py,5,"b""import numpy as np\nnp.random.seed(1234)\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport torch.optim as optim\n\nimport argparse\nimport time\n\nfrom sklearn.metrics import mean_absolute_error\nfrom scipy.stats import pearsonr\n\nimport pandas as pd\n\nfrom model import AVECModel, MaskedMSELoss\nfrom dataloader import AVECDataset\n\n\ndef get_train_valid_sampler(trainset, valid=0.1):\n    size = len(trainset)\n    idx = range(size)\n    split = int(valid*size)\n    return SubsetRandomSampler(idx[split:]), SubsetRandomSampler(idx[:split])\n\ndef get_AVEC_loaders(path, batch_size=32, valid=0.1, num_workers=0, pin_memory=False):\n    trainset = AVECDataset(path=path)\n    train_sampler, valid_sampler = get_train_valid_sampler(trainset, valid)\n    train_loader = DataLoader(trainset,\n                              batch_size=batch_size,\n                              sampler=train_sampler,\n                              collate_fn=trainset.collate_fn,\n                              num_workers=num_workers,\n                              pin_memory=pin_memory)\n    valid_loader = DataLoader(trainset,\n                              batch_size=batch_size,\n                              sampler=valid_sampler,\n                              collate_fn=trainset.collate_fn,\n                              num_workers=num_workers,\n                              pin_memory=pin_memory)\n\n    testset = AVECDataset(path=path, train=False)\n    test_loader = DataLoader(testset,\n                             batch_size=batch_size,\n                             collate_fn=testset.collate_fn,\n                             num_workers=num_workers,\n                             pin_memory=pin_memory)\n\n    return train_loader, valid_loader, test_loader\n\ndef train_or_eval_model(model, loss_function, dataloader, epoch, optimizer=None, train=False):\n    losses = []\n    preds = []\n    labels = []\n    masks = []\n    assert not train or optimizer!=None\n    if train:\n        model.train()\n    else:\n        model.eval()\n    for data in dataloader:\n        if train:\n            optimizer.zero_grad()\n        textf, visuf, acouf, qmask, umask, label =\\\n                                [d.cuda() for d in data] if cuda else data\n        pred = model(textf, qmask) # batch*seq_len\n        labels_ = label.view(-1) # batch*seq_len\n        umask_ = umask.view(-1) # batch*seq_len\n        loss = loss_function(pred, labels_, umask_)\n\n        preds.append(pred.data.cpu().numpy())\n        labels.append(labels_.data.cpu().numpy())\n        masks.append(umask_.cpu().numpy())\n\n        losses.append(loss.item()*masks[-1].sum())\n        if train:\n            loss.backward()\n            if args.tensorboard:\n                for param in model.named_parameters():\n                    writer.add_histogram(param[0], param[1].grad, epoch)\n            optimizer.step()\n\n    if preds!=[]:\n        preds  = np.concatenate(preds)\n        labels = np.concatenate(labels)\n        masks  = np.concatenate(masks)\n    else:\n        return float('nan'), float('nan'), float('nan'), [], [], []\n\n    avg_loss = round(np.sum(losses)/np.sum(masks),4)\n    mae = round(mean_absolute_error(labels,preds,sample_weight=masks),4)\n    pred_lab = pd.DataFrame(list(filter(lambda x: x[2]==1, zip(labels, preds, masks))))\n    pear = round(pearsonr(pred_lab[0], pred_lab[1])[0], 4)\n    return avg_loss, mae, pear, labels, preds, masks\n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--no-cuda', action='store_true', default=False,\n                        help='does not use GPU')\n    parser.add_argument('--lr', type=float, default=0.0001, metavar='LR',\n                        help='learning rate')\n    parser.add_argument('--l2', type=float, default=0.0001, metavar='L2',\n                        help='L2 regularization weight')\n    parser.add_argument('--rec-dropout', type=float, default=0.0,\n                        metavar='rec_dropout', help='rec_dropout rate')\n    parser.add_argument('--dropout', type=float, default=0.0, metavar='dropout',\n                        help='dropout rate')\n    parser.add_argument('--batch-size', type=int, default=30, metavar='BS',\n                        help='batch size')\n    parser.add_argument('--epochs', type=int, default=100, metavar='E',\n                        help='number of epochs')\n    parser.add_argument('--active-listener', action='store_true', default=False,\n                        help='active listener')\n    parser.add_argument('--attention', default='simple', help='Attention type')\n    parser.add_argument('--tensorboard', action='store_true', default=False,\n                        help='Enables tensorboard log')\n    parser.add_argument('--attribute', type=int, default=1, help='AVEC attribute')\n    args = parser.parse_args()\n\n    print(args)\n\n    args.cuda = torch.cuda.is_available() and not args.no_cuda\n    if args.cuda:\n        print('Running on GPU')\n    else:\n        print('Running on CPU')\n\n    if args.tensorboard:\n        from tensorboardX import SummaryWriter\n        writer = SummaryWriter()\n\n    batch_size = args.batch_size\n    n_classes  = 6\n    cuda       = args.cuda\n    n_epochs   = args.epochs\n\n    D_m = 100\n    D_g = 100\n    D_p = 100\n    D_e = 100\n    D_h = 100\n\n    D_a = 100 # concat attention\n\n    model = AVECModel(D_m, D_g, D_p, D_e, D_h,\n                    attr=args.attribute,\n                    listener_state=args.active_listener,\n                    context_attention=args.attention,\n                    dropout_rec=args.rec_dropout,\n                    dropout=args.dropout)\n    if cuda:\n        model.cuda()\n    loss_function = MaskedMSELoss()\n    optimizer = optim.Adam(model.parameters(),\n                           lr=args.lr,\n                           weight_decay=args.l2)\n\n    train_loader, valid_loader, test_loader =\\\n            get_AVEC_loaders('./AVEC_features/AVEC_features_{}.pkl'.format(args.attribute),\n                                valid=0.0,\n                                batch_size=batch_size,\n                                num_workers=2)\n\n    best_loss, best_label, best_pred, best_mask, best_pear = None, None, None, None, None\n\n    for e in range(n_epochs):\n        start_time = time.time()\n        train_loss, train_mae, train_pear,_,_,_ = train_or_eval_model(model, loss_function,\n                                               train_loader, e, optimizer, True)\n        valid_loss, valid_mae, valid_pear,_,_,_ = train_or_eval_model(model, loss_function, valid_loader, e)\n        test_loss, test_mae, test_pear, test_label, test_pred, test_mask = train_or_eval_model(model, loss_function, test_loader, e)\n\n        if best_loss == None or best_loss > test_loss:\n            best_loss, best_label, best_pred, best_mask, best_pear =\\\n                    test_loss, test_label, test_pred, test_mask, test_pear\n\n        if args.tensorboard:\n            writer.add_scalar('test: loss',test_loss,e)\n            writer.add_scalar('train: loss',train_loss,e)\n            writer.add_scalar('test: mae',test_mae,e)\n            writer.add_scalar('train: mae',train_mae,e)\n            writer.add_scalar('test: pear',test_pear,e)\n            writer.add_scalar('train: pear',train_pear,e)\n        print('epoch {} train_loss {} train_mae {} train_pear {} valid_loss {} valid_mae {} valid_pear {} test_loss {} test_mae {} test_pear {} time {}'.\\\n                format(e+1, train_loss, train_mae, train_pear, valid_loss, valid_mae,\\\n                        valid_pear, test_loss, test_mae, test_pear, round(time.time()-start_time,2)))\n    if args.tensorboard:\n        writer.close()\n\n    print('Test performance..')\n    print('Loss {} MAE {} r {}'.format(best_loss,\n                                 round(mean_absolute_error(best_label,best_pred,sample_weight=best_mask),4),\n                                 best_pear))\n"""
DialogueRNN/train_E2E.py,11,"b'import numpy as np\nnp.random.seed(1234)\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport torch.optim as optim\n\nimport argparse\nimport time\nimport pickle\n\nfrom sklearn.metrics import f1_score, confusion_matrix, accuracy_score,\\\n                        classification_report, precision_recall_fscore_support\n\nfrom model import BiE2EModel,UnMaskedWeightedNLLLoss\n\nfrom torchtext import data, vocab\nfrom torchtext.data import TabularDataset\nfrom torchtext.data import BucketIterator, Pipeline\n\nfrom keras.utils import to_categorical\n\nimport spacy\nspacy_en = spacy.load(\'en\')\n\nlabel2emotion = {0:""others"", 1:""happy"", 2: ""sad"", 3:""angry""}\nemotion2label = {""others"":0, ""happy"":1, ""sad"":2, ""angry"":3}\n\ndef tokenizer(text):\n    return [token.text for token in spacy_en.tokenizer(text)]\n\ndef convert_token(token, *args):\n    return token-1\n\ndef get_E2E_loaders(path, valid=0.1, batch_size=32):\n    utterance = data.Field(tokenize=tokenizer, lower=True)\n    label     = data.Field(sequential=False, postprocessing=Pipeline(convert_token=convert_token))\n    id        = data.Field(use_vocab=False,sequential=False)\n    fields = [(\'id\', id),\n              (\'turn1\', utterance),\n              (\'turn2\', utterance),\n              (\'turn3\', utterance),\n              (\'label\', label)]\n\n    train = data.TabularDataset(\'{}/train.txt\'.format(path),\n                                format=\'tsv\',\n                                fields=fields,\n                                skip_header=True)\n    valid = data.TabularDataset(\'{}/valid.txt\'.format(path),\n                                format=\'tsv\',\n                                fields=fields,\n                                skip_header=True)\n\n    test = data.TabularDataset(\'{}/test.txt\'.format(path),\n                                format=\'tsv\',\n                                fields=fields,\n                                skip_header=True)\n    vectors = vocab.Vectors(name=\'emojiplusglove.txt\', cache=\'/media/backup/nlp-cic/DialogueRNN/\')\n    utterance.build_vocab(train, valid, test, vectors=vectors)\n    #utterance.build_vocab(train, valid, test, vectors=\'glove.840B.300d\')\n    label.build_vocab(train)\n    train_iter = BucketIterator(train,\n                                  train=True,\n                                  batch_size=batch_size,\n                                  sort_key=lambda x: len(x.turn3),\n                                  device=torch.device(0))\n    valid_iter = BucketIterator(valid,\n                                  batch_size=batch_size,\n                                  sort_key=lambda x: len(x.turn3),\n                                  device=torch.device(0))\n    test_iter = BucketIterator(test,\n                                  batch_size=batch_size,\n                                  sort_key=lambda x: len(x.turn3),\n                                  device=torch.device(0))\n    return train_iter, valid_iter, test_iter,\\\n            utterance.vocab.vectors if not args.cuda else utterance.vocab.vectors.cuda(),\\\n            label.vocab.itos\n\ndef train_or_eval_model(model, embeddings, dataloader, epoch, loss_function=None, optimizer=None, train=False, valid=False, test=False):\n    losses = []\n    preds = []\n    labels = []\n    masks = []\n    alphas, alphas_f, alphas_b, vids = [], [], [], []\n    # assert not train or optimizer!=None\n    #umask = torch.FloatTensor([[1,1,1]]).type(T1.type())\n    #umask = umask.expand( T1.size(1),-1)\n    if train:\n        model.train()\n    else:\n        model.eval()\n    for data in dataloader:\n        if train:\n            optimizer.zero_grad()\n\n        log_prob = model(data,True) # batch, n_classes\n        lp_ = log_prob # batch, n_classes\n        # import ipdb;ipdb.set_trace()\n        if train or valid or test:\n            labels_ = data.label # batch\n            loss = loss_function(lp_, labels_)\n            losses.append(loss.item())\n\n        pred_ = torch.argmax(lp_,1) # batch\n        preds.append(pred_.data.cpu().numpy())\n        if train or valid or test:\n            labels.append(labels_.data.cpu().numpy())\n            masks.append(data.turn1.size(1))\n        else:\n            masks.append(data.id.data.cpu().numpy())\n\n        if train:\n            loss.backward()\n            # if args.tensorboard:\n            #     for param in model.named_parameters():\n            #         writer.add_histogram(param[0], param[1].grad, epoch)\n            optimizer.step()\n\n    if train or valid or test:\n        if preds!=[]:\n            # import ipdb;ipdb.set_trace()\n            preds  = np.concatenate(preds)\n            labels = np.concatenate(labels)\n        else:\n            return float(\'nan\'), float(\'nan\'), [], [], float(\'nan\')\n\n        avg_loss = round(np.sum(losses)/np.sum(masks),4)\n        avg_accuracy = round(accuracy_score(labels,preds)*100,2)\n        _,_,_,avg_fscore = get_metrics(labels,preds)\n        return avg_loss, avg_accuracy, labels, preds, avg_fscore\n    else:\n        preds  = np.concatenate(preds)\n        masks  = np.concatenate(masks)\n        return masks, preds\ndef get_metrics(discretePredictions, ground,n_classes=4):\n    \n    discretePredictions = to_categorical(discretePredictions,4)\n    ground = to_categorical(ground,4)\n    truePositives = np.sum(discretePredictions*ground, axis=0)\n    falsePositives = np.sum(np.clip(discretePredictions - ground, 0, 1), axis=0)\n    falseNegatives = np.sum(np.clip(ground-discretePredictions, 0, 1), axis=0)\n    \n    print(""True Positives per class : "", truePositives)\n    print(""False Positives per class : "", falsePositives)\n    print(""False Negatives per class : "", falseNegatives)\n    \n    # ------------- Macro level calculation ---------------\n    macroPrecision = 0\n    macroRecall = 0\n    accuracy = np.mean(discretePredictions==ground)\n    # We ignore the ""Others"" class during the calculation of Precision, Recall and F1\n    for c in range(1, n_classes):\n        precision = truePositives[c] / (truePositives[c] + falsePositives[c])\n        macroPrecision += precision\n        recall = truePositives[c] / (truePositives[c] + falseNegatives[c])\n        macroRecall += recall\n        f1 = ( 2 * recall * precision ) / (precision + recall) if (precision+recall) > 0 else 0\n        print(""Class %s : Precision : %.3f, Recall : %.3f, F1 : %.3f"" % (label2emotion[c], precision, recall, f1))\n    \n    macroPrecision /= 3\n    macroRecall /= 3\n    macroF1 = (2 * macroRecall * macroPrecision ) / (macroPrecision + macroRecall) if (macroPrecision+macroRecall) > 0 else 0\n    print(""Ignoring the Others class, Macro Precision : %.4f, Macro Recall : %.4f, Macro F1 : %.4f"" % (macroPrecision, macroRecall, macroF1))   \n    \n    # ------------- Micro level calculation ---------------\n    truePositives = truePositives[1:].sum()\n    falsePositives = falsePositives[1:].sum()\n    falseNegatives = falseNegatives[1:].sum()    \n    \n    print(""Ignoring the Others class, Micro TP : %d, FP : %d, FN : %d"" % (truePositives, falsePositives, falseNegatives))\n    \n    microPrecision = truePositives / (truePositives + falsePositives)\n    microRecall = truePositives / (truePositives + falseNegatives)\n    \n    microF1 = ( 2 * microRecall * microPrecision ) / (microPrecision + microRecall) if (microPrecision+microRecall) > 0 else 0\n    # -----------------------------------------------------\n    \n    \n    print(""Accuracy : %.4f, Micro Precision : %.4f, Micro Recall : %.4f, Micro F1 : %.4f"" % (accuracy, microPrecision, microRecall, microF1))\n    return accuracy, microPrecision, microRecall, microF1\n\nif __name__ == \'__main__\':\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                        help=\'does not use GPU\')\n    parser.add_argument(\'--lr\', type=float, default=0.001, metavar=\'LR\',\n                        help=\'learning rate\')\n    parser.add_argument(\'--l2\', type=float, default=0.00001, metavar=\'L2\',\n                        help=\'L2 regularization weight\')\n    parser.add_argument(\'--rec-dropout\', type=float, default=0.1,\n                        metavar=\'rec_dropout\', help=\'rec_dropout rate\')\n    parser.add_argument(\'--dropout\', type=float, default=0.1, metavar=\'dropout\',\n                        help=\'dropout rate\')\n    parser.add_argument(\'--batch-size\', type=int, default=30, metavar=\'BS\',\n                        help=\'batch size\')\n    parser.add_argument(\'--epochs\', type=int, default=15, metavar=\'E\',\n                        help=\'number of epochs\')\n    parser.add_argument(\'--class-weight\', action=\'store_true\', default=False,\n                        help=\'class weight\')\n    parser.add_argument(\'--active-listener\', action=\'store_true\', default=False,\n                        help=\'active listener\')\n    parser.add_argument(\'--attention\', default=\'general\', help=\'Attention type\')\n    parser.add_argument(\'--tensorboard\', action=\'store_true\', default=False,\n                        help=\'Enables tensorboard log\')\n    args = parser.parse_args()\n\n    print(args)\n\n    args.cuda = torch.cuda.is_available() and not args.no_cuda\n    if args.cuda:\n        print(\'Running on GPU\')\n    else:\n        print(\'Running on CPU\')\n\n    if args.tensorboard:\n        from tensorboardX import SummaryWriter\n        writer = SummaryWriter()\n\n    batch_size = args.batch_size\n    n_classes  = 4\n    cuda       = args.cuda\n    n_epochs   = args.epochs\n\n    D_emb = 300\n    D_m   = 200\n    D_g   = 150\n    D_p   = 150\n    D_e   = 100\n    D_h   = 100\n\n    D_a = 100 # concat attention\n\n    #model = BiE2EModel(D_emb, D_m, D_g, D_p, D_e, D_h,\n    #                 n_classes=n_classes,\n    #                 listener_state=args.active_listener,\n    #                 context_attention=args.attention,\n    #                 dropout_rec=args.rec_dropout,\n    #                 dropout=args.dropout)\n    #if cuda:\n    #    model.cuda()\n    loss_weights = torch.FloatTensor([\n                                        2, 1, 1 , 1\n                                        ])\n    if args.class_weight:\n        loss_function  = UnMaskedWeightedNLLLoss(loss_weights.cuda() if cuda else loss_weights)\n    else:\n        loss_function = UnMaskedWeightedNLLLoss()\n    #optimizer = optim.Adam(model.parameters(),\n    #                       lr=args.lr,\n    #                       weight_decay=args.l2)\n\n    train_loader, valid_loader, test_loader, embeddings, id2label =\\\n            get_E2E_loaders(\'./semeval19_emocon\',\n                            valid=0.1,\n                            batch_size=batch_size)\n    #optimizer = optim.Adam(model.parameters(),\n    #                       lr=args.lr,\n    #                       weight_decay=args.l2)\n\n    model = BiE2EModel(D_emb, D_m, D_g, D_p, D_e, D_h, embeddings,\n                     n_classes=n_classes,\n                     listener_state=args.active_listener,\n                     context_attention=args.attention,\n                     dropout_rec=args.rec_dropout,\n                     dropout=args.dropout)\n    optimizer = optim.Adam(model.parameters(),\n                           lr=args.lr,\n                           weight_decay=args.l2)\n    if cuda:\n        model.cuda()\n\n    best_loss, best_f1, best_pred, best_val_pred, best_val_label, best_ids =\\\n            None, None, None, None, None, None\n\n    for e in range(n_epochs):\n        start_time = time.time()\n        train_loss, train_acc, _,_,train_fscore = train_or_eval_model(model, embeddings,\n                                               train_loader, e, loss_function, optimizer, train=True)\n        valid_loss, valid_acc, valid_label, valid_pred, val_fscore = train_or_eval_model(model, embeddings, valid_loader, e, loss_function, valid=True)\n        test_loss, test_acc, test_label, test_pred, test_fscore = train_or_eval_model(model, embeddings, test_loader, e, loss_function, test=True)\n\n        if best_loss == None or best_loss > valid_loss:\n            best_loss, best_f1, best_pred, best_test_pred, best_test_label =\\\n                    valid_loss, test_fscore, test_pred+1, test_pred, test_label\n            best_valid_f1, best_pred, best_valid_pred, best_valid_label =\\\n                    val_fscore, valid_pred+1, valid_pred, valid_label\n\n        if args.tensorboard:\n            writer.add_scalar(\'test: accuracy/loss\',test_acc/test_loss,e)\n            writer.add_scalar(\'train: accuracy/loss\',train_acc/train_loss,e)\n        print(\'epoch {} train_loss {} train_acc {} train_fscore {} valid_loss {} valid_acc {} val_fscore {} test_loss {} test_acc {} test_fscore {} time {}\'.\\\n                format(e+1, train_loss, train_acc, train_fscore, valid_loss, valid_acc, val_fscore, test_loss, test_acc, test_fscore, \\\n                        round(time.time()-start_time,2)))\n    if args.tensorboard:\n        writer.close()\n\n    print(\'Test performance..\')\n    print(\'Loss {} fscore {}\'.format(best_loss, round(best_f1,2)))\n    print(classification_report(best_test_label,best_test_pred,digits=4))\n    print(confusion_matrix(best_test_label,best_test_pred))\n\n    print(\'Valid performance..\')\n    print(\'Loss {} fscore {}\'.format(best_loss, round(best_valid_f1,2)))\n    print(classification_report(best_valid_label,best_valid_pred,digits=4))\n    print(confusion_matrix(best_valid_label,best_valid_pred))\n    #with open(\'./semeval19_emocon/test.txt\',\'w\') as f:\n    #    f.write(\'id\\tturn1\\tturn2\\tturn3\\tlabel\\n\')\n    #    for id, label in zip(best_ids, best_pred):\n    #        f.write(\'{}\\tdummy1\\tdummy2\\tdummy3\\t{}\\n\'.format(id, id2label[label]))\n    # with open(\'best_attention.p\',\'wb\') as f:\n    #     pickle.dump(best_attn+[best_label,best_pred,best_mask],f)\n'"
DialogueRNN/train_IEMOCAP.py,8,"b""import numpy as np\nnp.random.seed(1234)\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport torch.optim as optim\n\nimport argparse\nimport time\nimport pickle\n\nfrom sklearn.metrics import f1_score, confusion_matrix, accuracy_score,\\\n                        classification_report, precision_recall_fscore_support\n\nfrom model import BiModel, Model, MaskedNLLLoss\nfrom dataloader import IEMOCAPDataset\n\ndef get_train_valid_sampler(trainset, valid=0.1):\n    size = len(trainset)\n    idx = list(range(size))\n    split = int(valid*size)\n    return SubsetRandomSampler(idx[split:]), SubsetRandomSampler(idx[:split])\n\ndef get_IEMOCAP_loaders(path, batch_size=32, valid=0.1, num_workers=0, pin_memory=False):\n    trainset = IEMOCAPDataset(path=path)\n    train_sampler, valid_sampler = get_train_valid_sampler(trainset, valid)\n    train_loader = DataLoader(trainset,\n                              batch_size=batch_size,\n                              sampler=train_sampler,\n                              collate_fn=trainset.collate_fn,\n                              num_workers=num_workers,\n                              pin_memory=pin_memory)\n    valid_loader = DataLoader(trainset,\n                              batch_size=batch_size,\n                              sampler=valid_sampler,\n                              collate_fn=trainset.collate_fn,\n                              num_workers=num_workers,\n                              pin_memory=pin_memory)\n\n    testset = IEMOCAPDataset(path=path, train=False)\n    test_loader = DataLoader(testset,\n                             batch_size=batch_size,\n                             collate_fn=testset.collate_fn,\n                             num_workers=num_workers,\n                             pin_memory=pin_memory)\n\n    return train_loader, valid_loader, test_loader\n\ndef train_or_eval_model(model, loss_function, dataloader, epoch, optimizer=None, train=False):\n    losses = []\n    preds = []\n    labels = []\n    masks = []\n    alphas, alphas_f, alphas_b, vids = [], [], [], []\n    assert not train or optimizer!=None\n    if train:\n        model.train()\n    else:\n        model.eval()\n    for data in dataloader:\n        if train:\n            optimizer.zero_grad()\n        # import ipdb;ipdb.set_trace()\n        textf, visuf, acouf, qmask, umask, label =\\\n                [d.cuda() for d in data[:-1]] if cuda else data[:-1]\n        #log_prob = model(torch.cat((textf,acouf,visuf),dim=-1), qmask,umask) # seq_len, batch, n_classes\n        log_prob, alpha, alpha_f, alpha_b = model(textf, qmask,umask) # seq_len, batch, n_classes\n        lp_ = log_prob.transpose(0,1).contiguous().view(-1,log_prob.size()[2]) # batch*seq_len, n_classes\n        labels_ = label.view(-1) # batch*seq_len\n        loss = loss_function(lp_, labels_, umask)\n\n        pred_ = torch.argmax(lp_,1) # batch*seq_len\n        preds.append(pred_.data.cpu().numpy())\n        labels.append(labels_.data.cpu().numpy())\n        masks.append(umask.view(-1).cpu().numpy())\n\n        losses.append(loss.item()*masks[-1].sum())\n        if train:\n            loss.backward()\n            if args.tensorboard:\n                for param in model.named_parameters():\n                    writer.add_histogram(param[0], param[1].grad, epoch)\n            optimizer.step()\n        else:\n            alphas += alpha\n            alphas_f += alpha_f\n            alphas_b += alpha_b\n            vids += data[-1]\n\n    if preds!=[]:\n        preds  = np.concatenate(preds)\n        labels = np.concatenate(labels)\n        masks  = np.concatenate(masks)\n    else:\n        return float('nan'), float('nan'), [], [], [], float('nan'),[]\n\n    avg_loss = round(np.sum(losses)/np.sum(masks),4)\n    avg_accuracy = round(accuracy_score(labels,preds,sample_weight=masks)*100,2)\n    avg_fscore = round(f1_score(labels,preds,sample_weight=masks,average='weighted')*100,2)\n    return avg_loss, avg_accuracy, labels, preds, masks,avg_fscore, [alphas, alphas_f, alphas_b, vids]\n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--no-cuda', action='store_true', default=False,\n                        help='does not use GPU')\n    parser.add_argument('--lr', type=float, default=0.0001, metavar='LR',\n                        help='learning rate')\n    parser.add_argument('--l2', type=float, default=0.00001, metavar='L2',\n                        help='L2 regularization weight')\n    parser.add_argument('--rec-dropout', type=float, default=0.1,\n                        metavar='rec_dropout', help='rec_dropout rate')\n    parser.add_argument('--dropout', type=float, default=0.1, metavar='dropout',\n                        help='dropout rate')\n    parser.add_argument('--batch-size', type=int, default=30, metavar='BS',\n                        help='batch size')\n    parser.add_argument('--epochs', type=int, default=60, metavar='E',\n                        help='number of epochs')\n    parser.add_argument('--class-weight', action='store_true', default=True,\n                        help='class weight')\n    parser.add_argument('--active-listener', action='store_true', default=False,\n                        help='active listener')\n    parser.add_argument('--attention', default='general', help='Attention type')\n    parser.add_argument('--tensorboard', action='store_true', default=False,\n                        help='Enables tensorboard log')\n    args = parser.parse_args()\n\n    print(args)\n\n    args.cuda = torch.cuda.is_available() and not args.no_cuda\n    if args.cuda:\n        print('Running on GPU')\n    else:\n        print('Running on CPU')\n\n    if args.tensorboard:\n        from tensorboardX import SummaryWriter\n        writer = SummaryWriter()\n\n    batch_size = args.batch_size\n    n_classes  = 6\n    cuda       = args.cuda\n    n_epochs   = args.epochs\n\n    D_m = 100\n    D_g = 150\n    D_p = 150\n    D_e = 100\n    D_h = 100\n\n    D_a = 100 # concat attention\n\n    model = BiModel(D_m, D_g, D_p, D_e, D_h,\n                    n_classes=n_classes,\n                    listener_state=args.active_listener,\n                    context_attention=args.attention,\n                    dropout_rec=args.rec_dropout,\n                    dropout=args.dropout)\n    if cuda:\n        model.cuda()\n    loss_weights = torch.FloatTensor([\n                                        1/0.086747,\n                                        1/0.144406,\n                                        1/0.227883,\n                                        1/0.160585,\n                                        1/0.127711,\n                                        1/0.252668,\n                                        ])\n    if args.class_weight:\n        loss_function  = MaskedNLLLoss(loss_weights.cuda() if cuda else loss_weights)\n    else:\n        loss_function = MaskedNLLLoss()\n    optimizer = optim.Adam(model.parameters(),\n                           lr=args.lr,\n                           weight_decay=args.l2)\n\n    train_loader, valid_loader, test_loader =\\\n            get_IEMOCAP_loaders('./IEMOCAP_features/IEMOCAP_features_raw.pkl',\n                                valid=0.0,\n                                batch_size=batch_size,\n                                num_workers=2)\n\n    best_loss, best_label, best_pred, best_mask = None, None, None, None\n\n    for e in range(n_epochs):\n        start_time = time.time()\n        train_loss, train_acc, _,_,_,train_fscore,_= train_or_eval_model(model, loss_function,\n                                               train_loader, e, optimizer, True)\n        valid_loss, valid_acc, _,_,_,val_fscore,_= train_or_eval_model(model, loss_function, valid_loader, e)\n        test_loss, test_acc, test_label, test_pred, test_mask, test_fscore, attentions = train_or_eval_model(model, loss_function, test_loader, e)\n\n        if best_loss == None or best_loss > test_loss:\n            best_loss, best_label, best_pred, best_mask, best_attn =\\\n                    test_loss, test_label, test_pred, test_mask, attentions\n\n        if args.tensorboard:\n            writer.add_scalar('test: accuracy/loss',test_acc/test_loss,e)\n            writer.add_scalar('train: accuracy/loss',train_acc/train_loss,e)\n        print('epoch {} train_loss {} train_acc {} train_fscore{} valid_loss {} valid_acc {} val_fscore{} test_loss {} test_acc {} test_fscore {} time {}'.\\\n                format(e+1, train_loss, train_acc, train_fscore, valid_loss, valid_acc, val_fscore,\\\n                        test_loss, test_acc, test_fscore, round(time.time()-start_time,2)))\n    if args.tensorboard:\n        writer.close()\n\n    print('Test performance..')\n    print('Loss {} accuracy {}'.format(best_loss,\n                                     round(accuracy_score(best_label,best_pred,sample_weight=best_mask)*100,2)))\n    print(classification_report(best_label,best_pred,sample_weight=best_mask,digits=4))\n    print(confusion_matrix(best_label,best_pred,sample_weight=best_mask))\n    # with open('best_attention.p','wb') as f:\n    #     pickle.dump(best_attn+[best_label,best_pred,best_mask],f)\n"""
DialogueRNN/train_MELD.py,11,"b'import torch\r\nfrom torch.utils.data import Dataset\r\nfrom torch.nn.utils.rnn import pad_sequence\r\nimport pickle\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\n\r\nimport torch.nn as nn\r\nfrom torch.utils.data import DataLoader\r\nfrom torch.utils.data.sampler import SubsetRandomSampler\r\nimport torch.optim as optim\r\n\r\nimport argparse\r\nimport time\r\nimport pickle\r\n\r\nfrom sklearn.metrics import f1_score, confusion_matrix, accuracy_score,\\\r\n                        classification_report, precision_recall_fscore_support\r\n\r\nfrom model import BiModel, Model, MaskedNLLLoss\r\nfrom dataloader import MELDDataset\r\nnp.random.seed(1234)\r\n\r\ndef get_train_valid_sampler(trainset, valid=0.1):\r\n    size = len(trainset)\r\n    idx = list(range(size))\r\n    split = int(valid*size)\r\n    return SubsetRandomSampler(idx[split:]), SubsetRandomSampler(idx[:split])\r\n\r\ndef get_MELD_loaders(path, n_classes, batch_size=32, valid=0.1, num_workers=0, pin_memory=False):\r\n    trainset = MELDDataset(path=path, n_classes=n_classes)\r\n    train_sampler, valid_sampler = get_train_valid_sampler(trainset, valid)\r\n    train_loader = DataLoader(trainset,\r\n                              batch_size=batch_size,\r\n                              sampler=train_sampler,\r\n                              collate_fn=trainset.collate_fn,\r\n                              num_workers=num_workers,\r\n                              pin_memory=pin_memory)\r\n    valid_loader = DataLoader(trainset,\r\n                              batch_size=batch_size,\r\n                              sampler=valid_sampler,\r\n                              collate_fn=trainset.collate_fn,\r\n                              num_workers=num_workers,\r\n                              pin_memory=pin_memory)\r\n\r\n    testset = MELDDataset(path=path, n_classes=n_classes, train=False)\r\n    test_loader = DataLoader(testset,\r\n                             batch_size=batch_size,\r\n                             collate_fn=testset.collate_fn,\r\n                             num_workers=num_workers,\r\n                             pin_memory=pin_memory)\r\n\r\n    return train_loader, valid_loader, test_loader\r\n\r\ndef train_or_eval_model(model, loss_function, dataloader, epoch, optimizer=None, train=False):\r\n    losses = []\r\n    preds = []\r\n    labels = []\r\n    masks = []\r\n    alphas, alphas_f, alphas_b, vids = [], [], [], []\r\n    assert not train or optimizer!=None\r\n    if train:\r\n        model.train()\r\n    else:\r\n        model.eval()\r\n    for data in dataloader:\r\n        if train:\r\n            optimizer.zero_grad()\r\n        # import ipdb;ipdb.set_trace()\r\n        textf, acouf, qmask, umask, label =\\\r\n                [d.cuda() for d in data[:-1]] if cuda else data[:-1]\r\n        if feature_type == ""audio"":\r\n            log_prob, alpha, alpha_f, alpha_b = model(acouf, qmask,umask) # seq_len, batch, n_classes\r\n        elif feature_type == ""text"":\r\n            log_prob, alpha, alpha_f, alpha_b = model(textf, qmask,umask) # seq_len, batch, n_classes\r\n        else:\r\n            log_prob, alpha, alpha_f, alpha_b = model(torch.cat((textf,acouf),dim=-1), qmask,umask) # seq_len, batch, n_classes\r\n        lp_ = log_prob.transpose(0,1).contiguous().view(-1,log_prob.size()[2]) # batch*seq_len, n_classes\r\n        labels_ = label.view(-1) # batch*seq_len\r\n        loss = loss_function(lp_, labels_, umask)\r\n\r\n        pred_ = torch.argmax(lp_,1) # batch*seq_len\r\n        preds.append(pred_.data.cpu().numpy())\r\n        labels.append(labels_.data.cpu().numpy())\r\n        masks.append(umask.view(-1).cpu().numpy())\r\n\r\n        losses.append(loss.item()*masks[-1].sum())\r\n        if train:\r\n            loss.backward()\r\n#             if args.tensorboard:\r\n#                 for param in model.named_parameters():\r\n#                     writer.add_histogram(param[0], param[1].grad, epoch)\r\n            optimizer.step()\r\n        else:\r\n            alphas += alpha\r\n            alphas_f += alpha_f\r\n            alphas_b += alpha_b\r\n            vids += data[-1]\r\n\r\n    if preds!=[]:\r\n        preds  = np.concatenate(preds)\r\n        labels = np.concatenate(labels)\r\n        masks  = np.concatenate(masks)\r\n    else:\r\n        return float(\'nan\'), float(\'nan\'), [], [], [], float(\'nan\'),[]\r\n\r\n    avg_loss = round(np.sum(losses)/np.sum(masks),4)\r\n    avg_accuracy = round(accuracy_score(labels,preds,sample_weight=masks)*100,2)\r\n    avg_fscore = round(f1_score(labels,preds,sample_weight=masks,average=\'weighted\')*100,2)\r\n    class_report = classification_report(labels,preds,sample_weight=masks,digits=4)\r\n    return avg_loss, avg_accuracy, labels, preds, masks,avg_fscore, [alphas, alphas_f, alphas_b, vids], class_report\r\n\r\ncuda = torch.cuda.is_available()\r\nif cuda:\r\n    print(\'Running on GPU\')\r\nelse:\r\n    print(\'Running on CPU\')\r\n    \r\ntensorboard = True    \r\nif tensorboard:\r\n    from tensorboardX import SummaryWriter\r\nwriter = SummaryWriter()\r\n\r\n# choose between \'sentiment\' or \'emotion\'\r\nclassification_type = \'emotion\'\r\nfeature_type = \'multimodal\'\r\n\r\ndata_path = \'DialogueRNN_features/MELD_features/\'\r\nbatch_size = 30\r\nn_classes = 3\r\nn_epochs = 100\r\nactive_listener = False\r\nattention = \'general\'\r\nclass_weight = False\r\ndropout = 0.1\r\nrec_dropout = 0.1\r\nl2 = 0.00001\r\nlr = 0.0005\r\n\r\nif feature_type == \'text\':\r\n    print(""Running on the text features........"")\r\n    D_m = 600\r\nelif feature_type == \'audio\':\r\n    print(""Running on the audio features........"")\r\n    D_m = 300\r\nelse:\r\n    print(""Running on the multimodal features........"")\r\n    D_m = 900\r\nD_g = 150\r\nD_p = 150\r\nD_e = 100\r\nD_h = 100\r\n\r\nD_a = 100 # concat attention\r\n\r\nloss_weights = torch.FloatTensor([1.0,1.0,1.0])\r\n\r\nif classification_type.strip().lower() == \'emotion\':\r\n    n_classes = 7\r\n    loss_weights = torch.FloatTensor([1.0,1.0,1.0,1.0,1.0,1.0,1.0])\r\n\r\nmodel = BiModel(D_m, D_g, D_p, D_e, D_h,\r\n                n_classes=n_classes,\r\n                listener_state=active_listener,\r\n                context_attention=attention,\r\n                dropout_rec=rec_dropout,\r\n                dropout=dropout)\r\n\r\nif cuda:\r\n    model.cuda()\r\nif class_weight:\r\n    loss_function  = MaskedNLLLoss(loss_weights.cuda() if cuda else loss_weights)\r\nelse:\r\n    loss_function = MaskedNLLLoss()\r\noptimizer = optim.Adam(model.parameters(),\r\n                       lr=lr,\r\n                       weight_decay=l2)\r\n\r\ntrain_loader, valid_loader, test_loader =\\\r\n        get_MELD_loaders(data_path + \'MELD_features_raw.pkl\', n_classes,\r\n                            valid=0.0,\r\n                            batch_size=batch_size,\r\n                            num_workers=0)\r\n\r\nbest_fscore, best_loss, best_label, best_pred, best_mask = None, None, None, None, None\r\n\r\n\r\nfor e in range(n_epochs):\r\n    start_time = time.time()\r\n    train_loss, train_acc, _,_,_,train_fscore,_,_= train_or_eval_model(model, loss_function,\r\n                                           train_loader, e, optimizer, True)\r\n    valid_loss, valid_acc, _,_,_,val_fscore,_= train_or_eval_model(model, loss_function, valid_loader, e)\r\n    test_loss, test_acc, test_label, test_pred, test_mask, test_fscore, attentions, test_class_report = train_or_eval_model(model, loss_function, test_loader, e)\r\n\r\n    if best_fscore == None or best_fscore < test_fscore:\r\n        best_fscore, best_loss, best_label, best_pred, best_mask, best_attn =\\\r\n                test_fscore, test_loss, test_label, test_pred, test_mask, attentions\r\n\r\n#     if args.tensorboard:\r\n#         writer.add_scalar(\'test: accuracy/loss\',test_acc/test_loss,e)\r\n#         writer.add_scalar(\'train: accuracy/loss\',train_acc/train_loss,e)\r\n    print(\'epoch {} train_loss {} train_acc {} train_fscore {} valid_loss {} valid_acc {} val_fscore {} test_loss {} test_acc {} test_fscore {} time {}\'.\\\r\n            format(e+1, train_loss, train_acc, train_fscore, valid_loss, valid_acc, val_fscore,\\\r\n                    test_loss, test_acc, test_fscore, round(time.time()-start_time,2)))\r\n    print (test_class_report)\r\nif tensorboard:\r\n    writer.close()\r\n\r\nprint(\'Test performance..\')\r\nprint(\'Fscore {} accuracy {}\'.format(best_fscore,\r\n                                 round(accuracy_score(best_label,best_pred,sample_weight=best_mask)*100,2)))\r\nprint(classification_report(best_label,best_pred,sample_weight=best_mask,digits=4))\r\nprint(confusion_matrix(best_label,best_pred,sample_weight=best_mask))\r\n'"
DialogueRNN/train_dailydialog.py,6,"b""import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport numpy as np, pickle, time, argparse\nfrom model import DailyDialogueModel, MaskedNLLLoss\nfrom dataloader import DailyDialoguePadCollate, DailyDialogueDataset\nfrom sklearn.metrics import f1_score, confusion_matrix, accuracy_score, classification_report, precision_recall_fscore_support\n\n\ndef get_DailyDialogue_loaders(path, batch_size=32, num_workers=0, pin_memory=False):\n    \n    trainset = DailyDialogueDataset('train', path)\n    testset = DailyDialogueDataset('test', path)\n    validset = DailyDialogueDataset('valid', path)\n\n    train_loader = DataLoader(trainset,\n                              batch_size=batch_size,\n                              collate_fn = DailyDialoguePadCollate(dim=0),\n                              num_workers=num_workers,\n                              pin_memory=pin_memory)\n    \n    valid_loader = DataLoader(validset,\n                              batch_size=batch_size,\n                              collate_fn = DailyDialoguePadCollate(dim=0),\n                              num_workers=num_workers,\n                              pin_memory=pin_memory)\n\n    test_loader = DataLoader(testset,\n                             batch_size=batch_size,\n                             collate_fn = DailyDialoguePadCollate(dim=0),\n                             num_workers=num_workers,\n                             pin_memory=pin_memory)\n\n    return train_loader, valid_loader, test_loader\n\n\ndef process_data_loader(data):\n    \n    input_sequence, qmask, umask, act_labels, emotion_labels, max_sequence_lengths, _ = data\n    input_sequence = input_sequence[:, :, :max(max_sequence_lengths)]\n    \n    input_sequence, qmask, umask = input_sequence.cuda(), qmask.cuda(), umask.cuda()\n    # act_labels = act_labels.cuda()\n    emotion_labels = emotion_labels.cuda()\n    \n    return [input_sequence, qmask, umask, emotion_labels]\n\n\n\ndef train_or_eval_model(model, loss_function, dataloader, epoch, optimizer=None, train=False):\n    losses = []\n    preds = []\n    labels = []\n    masks = []\n    alphas, alphas_f, alphas_b, vids = [], [], [], []\n    assert not train or optimizer!=None\n    if train:\n        model.train()\n    else:\n        model.eval()\n        \n    for data in dataloader:\n        if train:\n            optimizer.zero_grad()\n\n        input_sequence, qmask, umask, label = process_data_loader(data)\n        log_prob, alpha, alpha_f, alpha_b = model(input_sequence, qmask, umask)\n        \n        lp_ = log_prob.transpose(0,1).contiguous().view(-1,log_prob.size()[2]) # batch*seq_len, n_classes\n        labels_ = label.view(-1) # batch*seq_len\n        loss = loss_function(lp_, labels_, umask)\n\n        pred_ = torch.argmax(lp_,1) # batch*seq_len\n        preds.append(pred_.data.cpu().numpy())\n        labels.append(labels_.data.cpu().numpy())\n        masks.append(umask.view(-1).cpu().numpy())\n\n        losses.append(loss.item()*masks[-1].sum())\n        if train:\n            loss.backward()\n            if args.tensorboard:\n                for param in model.named_parameters():\n                    writer.add_histogram(param[0], param[1].grad, epoch)\n            optimizer.step()\n        else:\n            alphas += alpha\n            alphas_f += alpha_f\n            alphas_b += alpha_b\n            vids += data[-1]\n\n    if preds!=[]:\n        preds  = np.concatenate(preds)\n        labels = np.concatenate(labels)\n        masks  = np.concatenate(masks)\n    else:\n        return float('nan'), float('nan'), [], [], [], float('nan'),[]\n\n    avg_loss = round(np.sum(losses)/np.sum(masks),4)\n    avg_accuracy = round(accuracy_score(labels,preds,sample_weight=masks)*100,2)\n    avg_fscore = round(f1_score(labels,preds,sample_weight=masks,average='micro', labels=[0,2,3,4,5,6])*100,2)\n    return avg_loss, avg_accuracy, labels, preds, masks,avg_fscore, [alphas, alphas_f, alphas_b, vids]\n\n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--lr', type=float, default=0.0001, metavar='LR',\n                        help='learning rate')\n    parser.add_argument('--l2', type=float, default=0.00001, metavar='L2',\n                        help='L2 regularization weight')\n    parser.add_argument('--rec-dropout', type=float, default=0.1,\n                        metavar='rec_dropout', help='rec_dropout rate')\n    parser.add_argument('--dropout', type=float, default=0.5, metavar='dropout',\n                        help='dropout rate')\n    parser.add_argument('--batch-size', type=int, default=10, metavar='BS',\n                        help='batch size')\n    parser.add_argument('--epochs', type=int, default=60, metavar='E',\n                        help='number of epochs')\n    parser.add_argument('--class-weight', action='store_true', default=False,\n                        help='class weight')\n    parser.add_argument('--active-listener', action='store_true', default=False,\n                        help='active listener')\n    parser.add_argument('--attention', default='general', help='Attention type')\n    parser.add_argument('--tensorboard', action='store_true', default=False,\n                        help='Enables tensorboard log')\n    parser.add_argument('--cnn_filters', type=int, default=50,\n                        help='Number of cnn filters for cnn feature extraction')\n    parser.add_argument('--cnn_output_size', type=int, default=100,\n                        help='feature size from cnn layer')\n    parser.add_argument('--cnn_dropout', type=float, default=0.5, metavar='cnn_dropout',\n                        help='cnn dropout rate')\n    args = parser.parse_args()\n\n    print(args)\n    \n    args.cuda = torch.cuda.is_available()\n    if args.cuda:\n        print('Running on GPU')\n    else:\n        print('Running on CPU')\n\n    if args.tensorboard:\n        from tensorboardX import SummaryWriter\n        writer = SummaryWriter()\n\n    batch_size = args.batch_size\n    n_classes  = 7\n    cuda       = args.cuda\n    n_epochs   = args.epochs\n    \n    D_m = 100\n    D_g = 150\n    D_p = 150\n    D_e = 100\n    D_h = 100\n    D_a = 100\n    \n    kernel_sizes = [3,4,5]\n    \n    glv_pretrained = np.load(open('dailydialog/glv_embedding_matrix', 'rb'))\n    vocab_size, embedding_dim = glv_pretrained.shape\n    # glv_pretrained[0, :] = np.random.rand(embedding_dim)\n    model = DailyDialogueModel(D_m, D_g, D_p, D_e, D_h, vocab_size=vocab_size, n_classes=7, \n                               embedding_dim=embedding_dim,\n                               cnn_output_size=args.cnn_output_size,\n                               cnn_filters=args.cnn_filters, \n                               cnn_kernel_sizes=kernel_sizes,\n                               cnn_dropout=args.cnn_dropout,\n                               listener_state=args.active_listener,\n                               context_attention=args.attention,\n                               dropout_rec=args.rec_dropout,\n                               dropout=args.dropout)\n    model.init_pretrained_embeddings(glv_pretrained)    \n    if cuda:\n        model.cuda()\n        \n        \n    loss_weights = torch.FloatTensor([1,1,1,1,1,1,1])\n    if args.class_weight:\n        loss_function  = MaskedNLLLoss(loss_weights.cuda() if cuda else loss_weights)\n    else:\n        loss_function = MaskedNLLLoss()\n        \n    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n                           lr=args.lr,\n                           weight_decay=args.l2)\n    \n\n    train_loader, valid_loader, test_loader = get_DailyDialogue_loaders('dailydialog/daily_dialogue.pkl', \n                                                                        batch_size=batch_size, num_workers=0)\n    \n    best_loss, best_label, best_pred, best_mask = None, None, None, None\n\n    for e in range(n_epochs):\n        start_time = time.time()\n        train_loss, train_acc, _,_,_,train_fscore,_= train_or_eval_model(model, loss_function,\n                                               train_loader, e, optimizer, True)\n        valid_loss, valid_acc, _,_,_,val_fscore,_= train_or_eval_model(model, loss_function, valid_loader, e)\n        test_loss, test_acc, test_label, test_pred, test_mask, test_fscore, attentions = train_or_eval_model(model, loss_function, test_loader, e)\n\n        if best_loss == None or best_loss > test_loss:\n            best_loss, best_label, best_pred, best_mask, best_attn =                    test_loss, test_label, test_pred, test_mask, attentions\n\n        if args.tensorboard:\n            writer.add_scalar('test: accuracy/loss',test_acc/test_loss,e)\n            writer.add_scalar('train: accuracy/loss',train_acc/train_loss,e)\n        print('epoch {} train_loss {} train_acc {} train_fscore {} valid_loss {} valid_acc {} valid_fscore {} test_loss {} test_acc {} test_fscore {} time {}s'.format(e+1, train_loss, train_acc, train_fscore, valid_loss, valid_acc, val_fscore, test_loss, test_acc, test_fscore, round(time.time()-start_time,2)))\n    if args.tensorboard:\n        writer.close()\n\n    print('Test performance..')\n    print('Loss {} F1-score {}'.format(best_loss,\n                                     round(f1_score(best_label,best_pred,sample_weight=best_mask,labels=[0,2,3,4,5,6])*100,2)))\n    print(classification_report(best_label,best_pred,sample_weight=best_mask,labels=[0,2,3,4,5,6],digits=4))\n    print(confusion_matrix(best_label,best_pred,sample_weight=best_mask))\n"""
ICON-end-to-end/data_helper.py,0,"b'import numpy as np\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM\nfrom keras import optimizers\nfrom keras.models import load_model\nfrom pathlib import Path\nimport json, argparse, os\nimport re, pickle\nimport io\nimport sys\nimport os\n\n\n\n\n\nclass DataHelper:\n\n    def __init__(self, config):\n        self.config = config\n\n        # Label mappings\n        self.label2emotion = {0:""others"", 1:""happy"", 2: ""sad"", 3:""angry""}\n        self.emotion2label = {""others"":0, ""happy"":1, ""sad"":2, ""angry"":3}\n\n    def preprocessData(self, dataFilePath, mode):\n        """"""Load data from a file, process and return indices, conversations and labels in separate lists\n        Input:\n            dataFilePath : Path to train/test file to be processed\n            mode : ""train"" mode returns labels. ""test"" mode doesn\'t return labels.\n        Output:\n            indices : Unique conversation ID list\n            queries : List of turn 3 sentences\n            ownHistories : List of turn 1 sentences\n            otherHistories: List of turn 2 sentences\n            conversations : List of 3 turn conversations, processed and each turn separated by the <eos> tag\n            labels : [Only available in ""train"" mode] List of labels\n        """"""\n        indices = []\n        conversations = []\n        queries, ownHistories, otherHistories = [], [], []\n        conversations = []\n        labels = []\n        with io.open(dataFilePath, encoding=""utf8"") as finput:\n            finput.readline()\n            for line in finput:\n                # Convert multiple instances of . ? ! , to single instance\n                # okay...sure -> okay . sure\n                # okay???sure -> okay ? sure\n                # Add whitespace around such punctuation\n                # okay!sure -> okay ! sure\n                repeatedChars = [\'.\', \'?\', \'!\', \',\']\n                for c in repeatedChars:\n                    lineSplit = line.split(c)\n                    while True:\n                        try:\n                            lineSplit.remove(\'\')\n                        except:\n                            break\n                    cSpace = \' \' + c + \' \'    \n                    line = cSpace.join(lineSplit)\n                \n                line = line.strip().split(\'\\t\')\n                if mode == ""train"":\n                    # Train data contains id, 3 turns and label\n                    label = self.emotion2label[line[4]]\n                    labels.append(label)\n\n                ownHistory = line[1]\n                otherHistory = line[2]\n                query = line[3]\n                conv = \' <eos> \'.join(line[1:4])\n\n                # Remove any duplicate spaces\n                duplicateSpacePattern = re.compile(r\'\\ +\')\n                ownHistory = re.sub(duplicateSpacePattern, \' \', ownHistory)\n                otherHistory = re.sub(duplicateSpacePattern, \' \', otherHistory)\n                query = re.sub(duplicateSpacePattern, \' \', query)\n                conv = re.sub(duplicateSpacePattern, \' \', conv)\n                \n                indices.append(int(line[0]))\n                queries.append(query.lower())\n                ownHistories.append(ownHistory.lower())\n                otherHistories.append(otherHistory.lower())\n                conversations.append(conv.lower())\n        \n        if mode == ""train"":\n            return indices, queries, ownHistories, otherHistories, conversations, labels\n        else:\n            return indices, queries, ownHistories, otherHistories, conversations\n\n\n    def getMetrics(self, predictions, ground):\n        """"""Given predicted labels and the respective ground truth labels, display some metrics\n        Input: shape [# of samples, NUM_CLASSES]\n            predictions : Model output. Every row has 4 decimal values, with the highest belonging to the predicted class\n            ground : Ground truth labels, converted to one-hot encodings. A sample belonging to Happy class will be [0, 1, 0, 0]\n        Output:\n            accuracy : Average accuracy\n            microPrecision : Precision calculated on a micro level. Ref - https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin/16001\n            microRecall : Recall calculated on a micro level\n            microF1 : Harmonic mean of microPrecision and microRecall. Higher value implies better classification  \n        """"""\n        # [0.1, 0.3 , 0.2, 0.1] -> [0, 1, 0, 0]\n        discretePredictions = to_categorical(predictions.argmax(axis=1))\n        \n        truePositives = np.sum(discretePredictions*ground, axis=0)\n        falsePositives = np.sum(np.clip(discretePredictions - ground, 0, 1), axis=0)\n        falseNegatives = np.sum(np.clip(ground-discretePredictions, 0, 1), axis=0)\n        \n        print(""True Positives per class : "", truePositives)\n        print(""False Positives per class : "", falsePositives)\n        print(""False Negatives per class : "", falseNegatives)\n        \n        # ------------- Macro level calculation ---------------\n        macroPrecision = 0\n        macroRecall = 0\n        # We ignore the ""Others"" class during the calculation of Precision, Recall and F1\n        for c in range(1, NUM_CLASSES):\n            precision = truePositives[c] / (truePositives[c] + falsePositives[c])\n            macroPrecision += precision\n            recall = truePositives[c] / (truePositives[c] + falseNegatives[c])\n            macroRecall += recall\n            f1 = ( 2 * recall * precision ) / (precision + recall) if (precision+recall) > 0 else 0\n            print(""Class %s : Precision : %.3f, Recall : %.3f, F1 : %.3f"" % (label2emotion[c], precision, recall, f1))\n        \n        macroPrecision /= 3\n        macroRecall /= 3\n        macroF1 = (2 * macroRecall * macroPrecision ) / (macroPrecision + macroRecall) if (macroPrecision+macroRecall) > 0 else 0\n        print(""Ignoring the Others class, Macro Precision : %.4f, Macro Recall : %.4f, Macro F1 : %.4f"" % (macroPrecision, macroRecall, macroF1))   \n        \n        # ------------- Micro level calculation ---------------\n        truePositives = truePositives[1:].sum()\n        falsePositives = falsePositives[1:].sum()\n        falseNegatives = falseNegatives[1:].sum()    \n        \n        print(""Ignoring the Others class, Micro TP : %d, FP : %d, FN : %d"" % (truePositives, falsePositives, falseNegatives))\n        \n        microPrecision = truePositives / (truePositives + falsePositives)\n        microRecall = truePositives / (truePositives + falseNegatives)\n        \n        microF1 = ( 2 * microRecall * microPrecision ) / (microPrecision + microRecall) if (microPrecision+microRecall) > 0 else 0\n        # -----------------------------------------------------\n        \n        predictions = predictions.argmax(axis=1)\n        ground = ground.argmax(axis=1)\n        accuracy = np.mean(predictions==ground)\n        \n        print(""Accuracy : %.4f, Micro Precision : %.4f, Micro Recall : %.4f, Micro F1 : %.4f"" % (accuracy, microPrecision, microRecall, microF1))\n        return accuracy, microPrecision, microRecall, microF1\n\n\n    def getEmbeddingMatrix(self, wordIndex):\n        """"""Populate an embedding matrix using a word-index. If the word ""happy"" has an index 19,\n           the 19th row in the embedding matrix should contain the embedding vector for the word ""happy"".\n        Input:\n            wordIndex : A dictionary of (word : index) pairs, extracted using a tokeniser\n        Output:\n            embeddingMatrix : A matrix where every row has 100 dimensional GloVe embedding\n        """"""\n\n        embeddings_file = Path(""./tmp/embeddings.p"")\n\n        if not embeddings_file.is_file():\n            embeddingsIndex = {}\n            # Load the embedding vectors from ther GloVe file\n            with io.open(os.path.join(self.config[""glove_dir""], \'glove.840B.300d.txt\'), encoding=""utf8"") as f:\n                for line in f:\n                    try:\n                        values = line.split()\n                        word = values[0]\n                        embeddingVector = np.asarray(values[1:], dtype=\'float32\')\n                        embeddingsIndex[word] = embeddingVector\n                    except:\n                        continue\n\n            print(\'Glove word vectors : %s\' % len(embeddingsIndex))\n            \n            # Minimum word index of any word is 1. \n            embeddingMatrix = np.zeros((len(wordIndex) + 1, self.config[""embedding_dim""]))\n            tokens_found=0\n            for word, i in wordIndex.items():\n                embeddingVector = embeddingsIndex.get(word)\n                if embeddingVector is not None:\n                    tokens_found+=1\n                    # words not found in embedding index will be all-zeros.\n                    embeddingMatrix[i] = embeddingVector\n\n            print(\'Matching word vectors : %s\' % len(embeddingsIndex))\n            print(\'Tokens matched: {}/{}\'.format(tokens_found, len(wordIndex)))\n\n            pickle.dump( [wordIndex, embeddingsIndex, embeddingMatrix], open(""./tmp/embeddings.p"", ""wb""))\n        else:\n            wordIndex, embeddingsIndex, embeddingMatrix = pickle.load(open(""./tmp/embeddings.p"", ""rb""))\n        \n        return embeddingMatrix\n\n\n    def prepare_history(self, data, mode, maxlen):\n        data = pad_sequences(data, maxlen) # (batch, maxlen)\n        pads = np.zeros(data.shape, dtype=np.float32) # (batch, maxlen)\n        if mode == ""own"":\n            data = np.stack((data, pads), axis=1)\n        else:\n            data = np.stack((pads, data), axis=1)\n        return data # (batch, 2, maxlen)\n'"
ICON-end-to-end/model.py,0,"b'import tensorflow as tf\n\nclass ICON:\n\n    def __init__(self, config, embeddingMatrix, session = None):\n        \n        # CNN related\n        self._embedding_matrix = embeddingMatrix\n        self._sequence_length = config[""max_sequence_length""]\n        self._embedding_size = config[""embedding_dim""]\n        self._filter_sizes = config[""filter_sizes""]\n        self._num_filters = config[""num_filters""]\n        self._num_filters_total = self._num_filters * len(self._filter_sizes)\n\n        # Memory Network hops\n        self._timesteps = config[""timesteps""]\n        self._hops = config[""hops""]\n        \n        # Training related\n        self._lr = config[""learning_rate""]\n        self._batch_size = config[""batch_size""]\n        self._class_size = config[""num_classes""]\n        self._max_grad_norm = config[""max_grad_norm""]\n        self._init = tf.random_normal_initializer(stddev=0.01, seed=1227)\n        self._name = ""ICON""\n\n\n        ## inputs to receive from the dataset\n        self._build_inputs()\n\n        ## tensor variables of the tensorflow graph\n        self._build_vars()\n\n        ## optimizer choices for training\n        # self._opt = tf.train.GradientDescentOptimizer(learning_rate=self._lr)\n        self._opt = tf.train.AdamOptimizer(learning_rate=self._lr)\n\n        ## cross entropy loss\n        logits = self._inference() # (batch_size, class size)\n        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf.cast(self._labels, tf.float32), name=""cross_entropy"")\n        cross_entropy_sum = tf.reduce_mean(cross_entropy, keepdims=False, name=""cross_entropy_sum"")\n\n        print(\'\\n ---- TRAINABLE VARIABLES ---- \\n\')\n        tvars = tf.trainable_variables()\n        reg_loss=[]\n        for tvar in tvars:\n            print(tvar.name)\n            if ""bias"" not in tvar.name:\n                reg_loss.append(tf.nn.l2_loss(tvar))\n        print(\'----------- \\n\')\n\n        # loss op\n        self.regularization_loss = tf.reduce_mean(reg_loss)\n        self.loss_op = loss_op = tf.reduce_mean(cross_entropy_sum + 0.001*self.regularization_loss)\n\n        # gradient pipeline\n        grads_and_vars = self._opt.compute_gradients(loss_op)\n        grads_and_vars = [(tf.clip_by_norm(g, self._max_grad_norm), v) for g,v in grads_and_vars]\n        grads_and_vars = [(g, v) for g,v in grads_and_vars]\n        self.train_op = train_op = self._opt.apply_gradients(grads_and_vars, name=""train_op"")\n\n        # predict ops\n        self.predict_op = predict_op = tf.argmax(logits, 1, name=""predict_op"")\n\n        self._sess = session\n        self._sess.run(tf.global_variables_initializer())\n\n\n    def _build_inputs(self):\n\n        # input queries\n        self._input_queries = tf.placeholder(tf.int32, [self._batch_size, self._sequence_length], name=""input_queries"")\n\n        # histories\n        self._own_histories = tf.placeholder(tf.int32, [self._batch_size, self._timesteps, self._sequence_length], name=""own_histories"")\n        self._other_histories = tf.placeholder(tf.int32, [self._batch_size, self._timesteps, self._sequence_length], name=""other_histories"")\n\n        # True Labels\n        self._labels = tf.placeholder(tf.int32, [self._batch_size, self._class_size], name=""labels"")\n\n\n    def _build_vars(self):\n        with tf.variable_scope(self._name):\n\n            with tf.variable_scope(""CNN""):\n                self.embedding_matrix = tf.Variable(self._embedding_matrix, name=""embedding_matrix"", dtype=tf.float32)\n                self.conv_final_W = tf.get_variable(\n                        ""conv_final_W"", \n                        shape=[self._num_filters_total, self._embedding_size],\n                        initializer=tf.contrib.layers.xavier_initializer())\n                self.conv_final_b = tf.Variable(tf.constant(0.1, shape=[self._embedding_size]), name=""conv_final_b"")\n\n            with tf.variable_scope(""output""):\n                self.output_W = tf.get_variable(\n                    ""output_W"",\n                    shape=[self._embedding_size, self._class_size],\n                    initializer=tf.contrib.layers.xavier_initializer())\n                self.output_b = tf.Variable(tf.constant(0.1, shape=[self._class_size]), name=""output_b"")\n\n            with tf.variable_scope(""SIM""):\n                self.rnn_own_history= tf.contrib.rnn.GRUCell(num_units=self._embedding_size, reuse = tf.AUTO_REUSE, name=\'rnn_own_history\')\n                self.rnn_other_history= tf.contrib.rnn.GRUCell(num_units=self._embedding_size, reuse = tf.AUTO_REUSE, name=\'rnn_other_history\')\n\n            with tf.variable_scope(""DGIM""):\n                self.rnn_dgim= tf.contrib.rnn.GRUCell(num_units=self._embedding_size, reuse = tf.AUTO_REUSE, name=\'rnn_dgim\')\n\n            with tf.variable_scope(""MemoryNet""):\n                self.rnn_memory= tf.contrib.rnn.GRUCell(num_units=self._embedding_size, reuse = tf.AUTO_REUSE, name=\'rnn_memory\')\n\n    def _convolution(self, input_to_conv):\n\n        # Create a convolution + maxpool layer for each filter size\n        pooled_outputs = []\n        for idx, filter_size in enumerate(self._filter_sizes):\n            with tf.variable_scope(""conv-maxpool-%s"" % filter_size):\n                \n                # Convolution Layer\n                filter_shape = [filter_size, self._embedding_size, 1, self._num_filters]\n                W = tf.get_variable(""W"", initializer=tf.truncated_normal(filter_shape, stddev=0.1))\n                b = tf.get_variable(""b"", initializer=tf.constant(0.1, shape=[self._num_filters]))\n                conv = tf.nn.conv2d(\n                    input_to_conv,\n                    W,\n                    strides=[1, 1, 1, 1],\n                    padding=""VALID"",\n                    name=""conv"")\n\n                # Apply nonlinearity\n                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=""relu"")\n                \n                # Maxpooling over the outputs\n                pooled = tf.nn.max_pool(\n                    h,\n                    ksize=[1, self._sequence_length - filter_size + 1, 1, 1],\n                    strides=[1, 1, 1, 1],\n                    padding=\'VALID\',\n                    name=""pool"")\n                pooled_outputs.append(pooled)\n\n        # Combine all the pooled features\n        self.h_pool = tf.concat(pooled_outputs, 3)\n        self.h_pool_flat = tf.reshape(self.h_pool, [-1, self._num_filters_total], name=""h_pool_flat"")\n\n        return tf.nn.xw_plus_b(self.h_pool_flat, self.conv_final_W, self.conv_final_b, name=""conv_dense"")\n\n\n    def _inference(self):\n\n        with tf.variable_scope(self._name, reuse=tf.AUTO_REUSE):\n\n            with tf.variable_scope(""CNN"", reuse=tf.AUTO_REUSE):\n\n                # feature extraction for queries\n                embedded_words_queries = tf.expand_dims(tf.nn.embedding_lookup(self.embedding_matrix, self._input_queries), -1) # (batch, sequence_length, embedding_dim, 1)\n                queries = queries_conv_output = self._convolution(embedded_words_queries) # (batch, _num_filters_total)\n\n                # feature extraction for ownHistory\n                own_history_conv_output=[]\n                for i in range(self._timesteps):\n                    local_history = tf.squeeze(self._own_histories[:,tf.constant(i)]) # (batch, sequence_length)\n                    embedded_local_history = tf.expand_dims(tf.nn.embedding_lookup(self.embedding_matrix, local_history), -1) # (batch, sequence_length, embedding_dim, 1)\n                    own_history_conv_output.append(self._convolution(embedded_local_history)[:,tf.newaxis,:]) # (batch, 1, _num_filters_total)\n                own_history_conv_output = tf.concat(own_history_conv_output, axis=1) # (batch, timesteps, _num_filters_total)\n\n                # feature extraction for otherHistory\n                other_history_conv_output=[]\n                for i in range(self._timesteps):\n                    local_history = tf.squeeze(self._other_histories[:,tf.constant(i)]) # (batch, sequence_length)\n                    embedded_local_history = tf.expand_dims(tf.nn.embedding_lookup(self.embedding_matrix, local_history), -1) # (batch, sequence_length, embedding_dim, 1)\n                    other_history_conv_output.append(self._convolution(embedded_local_history)[:,tf.newaxis,:]) # (batch, 1, _num_filters_total)\n                other_history_conv_output = tf.concat(other_history_conv_output, axis=1) # (batch, timesteps, _num_filters_total)\n\n\n                # SIM on histories\n                rnn_own_history, _ = tf.nn.dynamic_rnn(self.rnn_own_history, own_history_conv_output, dtype=tf.float32)\n                rnn_other_history, _ = tf.nn.dynamic_rnn(self.rnn_other_history, other_history_conv_output, dtype=tf.float32)\n\n                print(rnn_own_history.get_shape())\n\n                # DGIM on histories\n                dgim_input = (rnn_own_history + rnn_other_history)\n\n\n            with tf.variable_scope(""DGIM""):\n\n                for hop in range(self._hops):\n\n                    # Memory Update\n                    if hop == 0:\n                        rnn_input = dgim_input\n                        rnn_cell = self.rnn_dgim\n                    else:\n                        rnn_input = rnn_outputs\n                        rnn_cell = self.rnn_memory\n\n                    # Memory write of previous hop == memory input of current hop\n                    rnn_outputs, _ = tf.nn.dynamic_rnn(rnn_cell, rnn_input, dtype=tf.float32)\n\n\n                    # Attentional Read operation from rnn_output memories\n                    attScore = tf.nn.tanh(tf.squeeze(tf.matmul(queries[:,tf.newaxis,:], tf.transpose(rnn_outputs,[0,2,1]))))  # (batch, 1, _num_filters_total)  X (batch, _num_filters_total, timesteps) == (batch, 1, timesteps) -> (batch, time)\n                    attScore = tf.nn.softmax(attScore) # (batch, time)\n                    weighted = tf.squeeze(tf.matmul(attScore[:,tf.newaxis,:], rnn_outputs)) # (batch, 1, timesteps)  X (batch, timesteps, _num_filters_total) == (batch, _num_filters_total)\n                    queries = tf.nn.tanh(queries + weighted)\n\n                \n\n            with tf.variable_scope(""output""):\n                \n                return tf.nn.xw_plus_b(queries, self.output_W, self.output_b, name=""output_scores"")\n\n\n    def batch_fit(self, queries, ownHistory, otherHistory, labels):\n\n        feed_dict = {self._input_queries: queries, self._own_histories: ownHistory, self._other_histories: otherHistory, self._labels: labels}\n        loss, _ = self._sess.run([self.loss_op, self.train_op], feed_dict=feed_dict)\n        return loss\n\n    def predict(self, queries, ownHistory, otherHistory,):\n\n        feed_dict = {self._input_queries: queries, self._own_histories: ownHistory, self._other_histories: otherHistory}\n        return self._sess.run(self.predict_op, feed_dict=feed_dict)\n\n'"
ICON-end-to-end/train.py,0,"b'#Please use python 3.5 or above\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom keras.preprocessing.text import Tokenizer\r\nfrom keras.preprocessing.sequence import pad_sequences\r\nfrom keras.utils import to_categorical\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Embedding, LSTM\r\nfrom keras import optimizers\r\nfrom keras.models import load_model\r\nimport json, argparse, os\r\nimport re\r\nimport io\r\nimport sys\r\nimport os\r\n\r\nfrom data_helper import DataHelper\r\nfrom model import ICON\r\n\r\n# Selecting the GPU to work on\r\nos.environ[""CUDA_DEVICE_ORDER""]=""PCI_BUS_ID""\r\nos.environ[""CUDA_VISIBLE_DEVICES""]=""0""\r\n\r\n# Desired graphics card config\r\nsession_conf = tf.ConfigProto(\r\n      allow_soft_placement=True,\r\n      log_device_placement=False,\r\n      gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.7))\r\n\r\n\r\ndef pad_batch(data, remainder_length, dtype):\r\n    new_shape = list(data.shape)\r\n    new_shape[0]=remainder_length\r\n    new_shape = tuple(new_shape)\r\n    return np.concatenate((data, np.zeros(new_shape, dtype=dtype)), axis=0)\r\n\r\n\r\ndef train_model(config, dataQueries, dataOwnHistories, dataOtherHistories, labels, embeddingMatrix, batches):\r\n\r\n    print(""Training model on entire data"")\r\n\r\n    with tf.Graph().as_default():\r\n        tf.set_random_seed(1234) # Graph level random seed\r\n        sess = tf.Session(config=session_conf) # Defining the session of the Graph\r\n        with sess.as_default():\r\n\r\n            model = ICON(config, embeddingMatrix, session=sess)\r\n\r\n            for t in range(1, config[""num_epochs""]+1):\r\n\r\n                # Annealing of the learning rate\r\n                if t - 1 <= config[""anneal_stop_epoch""]:\r\n                    anneal = 2.0 ** ((t-1) // config[""anneal_rate""])\r\n                else:\r\n                    anneal = 2.0 ** (config[""anneal_stop_epoch""] // config[""anneal_rate""])\r\n                lr = config[""learning_rate""] / anneal\r\n\r\n                # Shuffling the batches in each epoch\r\n                np.random.shuffle(batches)\r\n\r\n                total_cost = 0.0\r\n                for start, end in batches:\r\n                    query = dataQueries[start:end]\r\n                    ownHistory = dataOwnHistories[start:end]\r\n                    otherHistory = dataOtherHistories[start:end]\r\n                    answers = labels[start:end]\r\n                    \r\n                    if query.shape[0] < config[""batch_size""]:\r\n                        remainder_length = config[""batch_size""]-query.shape[0]\r\n                        query = pad_batch(query, remainder_length, np.float32)\r\n                        ownHistory = pad_batch(ownHistory, remainder_length, np.float32)\r\n                        otherHistory = pad_batch(otherHistory, remainder_length, np.float32)\r\n                        answers = pad_batch(answers, remainder_length, np.float32)\r\n\r\n                    cost_t = model.batch_fit(query, ownHistory, otherHistory, answers)\r\n                    total_cost += cost_t\r\n                print(total_cost)\r\n            return model\r\n\r\ndef predict_model(config, model, dataQueries, dataOwnHistories, dataOtherHistories, batches):\r\n\r\n    preds=[]\r\n    for start, end in batches:\r\n        query = dataQueries[start:end]\r\n        ownHistory = dataOwnHistories[start:end]\r\n        otherHistory = dataOtherHistories[start:end]\r\n\r\n        \r\n        if query.shape[0] < config[""batch_size""]:\r\n            remainder_length = config[""batch_size""]-query.shape[0]\r\n            query = pad_batch(query, remainder_length, np.float32)\r\n            ownHistory = pad_batch(ownHistory, remainder_length, np.float32)\r\n            otherHistory = pad_batch(otherHistory, remainder_length, np.float32)\r\n\r\n        preds += list(model.predict(query, ownHistory, otherHistory))\r\n\r\n    return preds[:len(dataQueries)]\r\n\r\n\r\ndef main():\r\n    parser = argparse.ArgumentParser(description=""Baseline Script for SemEval"")\r\n    parser.add_argument(\'-config\', help=\'Config to read details\', required=True)\r\n    args = parser.parse_args()\r\n\r\n    with open(args.config) as configfile:\r\n        config = json.load(configfile)\r\n    \r\n\r\n    ####################  Pre-processing  #############################\r\n    # Data Helper object\r\n    datahelper = DataHelper(config)\r\n\r\n\r\n    # Loading the data        \r\n    print(""Processing training data..."")\r\n    trainIndices, trainQueries, trainOwnHistories, trainOtherHistories, trainTexts, labels = datahelper.preprocessData(config[""train_data_path""], mode=""train"")\r\n\r\n    print(""Processing test data..."")\r\n    testIndices, testQueries, testOwnHistories, testOtherHistories, testTexts = datahelper.preprocessData(config[""test_data_path""], mode=""test"")\r\n\r\n    # Size of data\r\n    n_train = len(trainIndices)\r\n    n_test = len(testIndices)\r\n\r\n    print(""Extracting tokens..."")\r\n    tokenizer = Tokenizer(num_words=config[""max_nb_words""])\r\n    tokenizer.fit_on_texts(trainTexts)\r\n    trainQueriesSequences = tokenizer.texts_to_sequences(trainQueries)\r\n    testQueriesSequences = tokenizer.texts_to_sequences(testQueries)\r\n    trainOwnHistoriesSequences = tokenizer.texts_to_sequences(trainOwnHistories)\r\n    testOwnHistoriesSequences = tokenizer.texts_to_sequences(testOwnHistories)\r\n    trainOtherHistoriesSequences = tokenizer.texts_to_sequences(trainOtherHistories)\r\n    testOtherHistoriesSequences = tokenizer.texts_to_sequences(testOtherHistories)\r\n\r\n    wordIndex = tokenizer.word_index\r\n    print(""Found %s unique tokens."" % len(wordIndex))\r\n\r\n    print(""Populating embedding matrix..."")\r\n    embeddingMatrix = datahelper.getEmbeddingMatrix(wordIndex)\r\n\r\n    ####################  Training  #############################\r\n\r\n\r\n    # Prepare training data\r\n    dataQueries = pad_sequences(trainQueriesSequences, maxlen=config[""max_sequence_length""])\r\n\r\n    dataOwnHistories = datahelper.prepare_history(trainOwnHistoriesSequences, mode=""own"", maxlen=config[""max_sequence_length""])\r\n    dataOtherHistories = datahelper.prepare_history(trainOtherHistoriesSequences, mode=""other"", maxlen=config[""max_sequence_length""])\r\n    labels = to_categorical(np.asarray(labels))\r\n    print(""Shape of training data tensor: "", dataQueries.shape, dataOwnHistories.shape, dataOtherHistories.shape)\r\n    print(""Shape of label tensor: "", labels.shape)\r\n        \r\n    # Randomize data\r\n    np.random.shuffle(trainIndices)\r\n    dataQueries = dataQueries[trainIndices]\r\n    dataOwnHistories = dataOwnHistories[trainIndices]\r\n    dataOtherHistories = dataOtherHistories[trainIndices]\r\n    labels = labels[trainIndices]\r\n    \r\n\r\n    ## Calculating training batch sizes\r\n    batches = zip(range(0, n_train, config[""batch_size""]), range(config[""batch_size""], n_train+config[""batch_size""], config[""batch_size""]))\r\n    batches = [(start, end) for start, end in batches]\r\n\r\n    model = train_model(config, dataQueries, dataOwnHistories, dataOtherHistories, labels, embeddingMatrix, batches)\r\n\r\n\r\n\r\n    ####################  Test file generation  #############################\r\n\r\n\r\n    print(""Creating solution file..."")\r\n\r\n    # Preparing test data\r\n    testQueries = pad_sequences(testQueriesSequences, maxlen=config[""max_sequence_length""])\r\n    testOwnHistories = datahelper.prepare_history(testOwnHistoriesSequences, mode=""own"", maxlen=config[""max_sequence_length""])\r\n    testOtherHistories = datahelper.prepare_history(testOtherHistoriesSequences, mode=""other"", maxlen=config[""max_sequence_length""])\r\n\r\n    ## Calculating testing batch sizes\r\n    batches = zip(range(0, n_test, config[""batch_size""]), range(config[""batch_size""], n_test+config[""batch_size""], config[""batch_size""]))\r\n    batches = [(start, end) for start, end in batches]\r\n\r\n    predictions = predict_model(config, model, testQueries, testOwnHistories, testOtherHistories, batches)\r\n\r\n    with io.open(config[""solution_path""], ""w"", encoding=""utf8"") as fout:\r\n        fout.write(\'\\t\'.join([""id"", ""turn1"", ""turn2"", ""turn3"", ""label""]) + \'\\n\')        \r\n        with io.open(config[""test_data_path""], encoding=""utf8"") as fin:\r\n            fin.readline()\r\n            for lineNum, line in enumerate(fin):\r\n                fout.write(\'\\t\'.join(line.strip().split(\'\\t\')[:4]) + \'\\t\')\r\n                fout.write(datahelper.label2emotion[predictions[lineNum]] + \'\\n\')\r\n\r\n\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()'"
ICON/train_iemocap.py,0,"b'import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom IEMOCAP.utils import *\nfrom IEMOCAP.model import *\nimport os\nfrom sklearn import model_selection, metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n# Desired graphics card selection\nos.environ[""CUDA_DEVICE_ORDER""]=""PCI_BUS_ID""\nos.environ[""CUDA_VISIBLE_DEVICES""]=""0""\n\n# Desired graphics card config\nsession_conf = tf.ConfigProto(\n      allow_soft_placement=True,\n      log_device_placement=False,\n      gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.7))\n\n# Select appropriate config from the file config.py\ntf.flags.DEFINE_string(""mode"", ""all"", ""which modality"")\ntf.flags.DEFINE_boolean(""context"", True, ""which kind of features to choose"")\ntf.flags.DEFINE_string(""nonlin_func"", ""tf.nn.tanh"", ""type of nonlinearity"")\ntf.flags.DEFINE_float(""learning_rate"", 0.001, ""Learning rate for SGD."")\ntf.flags.DEFINE_float(""anneal_rate"", 60, ""Number of epochs between halving the learnign rate."")\ntf.flags.DEFINE_float(""anneal_stop_epoch"", 100, ""Epoch number to end annealed lr schedule."")\ntf.flags.DEFINE_float(""max_grad_norm"", 40.0, ""Clip gradients to this norm."")\ntf.flags.DEFINE_integer(""evaluation_interval"", 1, ""Evaluate and print results every x epochs"")\ntf.flags.DEFINE_integer(""batch_size"", 512, ""Batch size for training."")\ntf.flags.DEFINE_integer(""hops"", 3, ""Number of hops in the Memory Network."")\ntf.flags.DEFINE_integer(""epochs"", 10, ""Number of epochs to train for."")\ntf.flags.DEFINE_integer(""embedding_size"", 100, ""Embedding size for embedding matrices."")\ntf.flags.DEFINE_integer(""input_dims"", None, ""Number of timesteps of the RNN"")\ntf.flags.DEFINE_integer(""timesteps"", 40, ""Number of timesteps of the RNN"")\ntf.flags.DEFINE_integer(""class_size"", None, ""No. of output classes"")\ntf.flags.DEFINE_boolean(""nonlin"", True, ""Use non linearity"")\ntf.flags.DEFINE_float(""dropout_keep_prob"", 0.3, ""Percentage of input to keep in dropout"")\n\n# Misc Parameters\ntf.flags.DEFINE_integer(""checkpoint_every"", 10, ""Save model after this many steps (default: 100)"")\ntf.flags.DEFINE_integer(""num_checkpoints"", 10, ""Number of checkpoints to store (default: 5)"")\ntf.flags.DEFINE_boolean(""allow_soft_placement"", True, ""Allow device soft device placement"")\ntf.flags.DEFINE_boolean(""log_device_placement"", False, ""Log placement of ops on devices"")\nFLAGS = tf.flags.FLAGS\n\n\ndef main():\n\n\t## Loading the train and test data\n\ttrainQueries, trainOwnHistory, trainOtherHistory, trainOwnHistoryMask, trainOtherHistoryMask, trainLabels, \\\n\t\t\tvalQueries, valOwnHistory, valOtherHistory, valOwnHistoryMask, valOtherHistoryMask, valLabels, \\\n\t        testQueries, testOwnHistory, testOtherHistory, testOwnHistoryMask, testOtherHistoryMask, testLabels = loadData(FLAGS)\n\n\t## Update FLAG parameters\n\tFLAGS.class_size = trainLabels.shape[1]\n\tFLAGS.input_dims = trainQueries.shape[1]\n\n\t## Total instances\n\tn_train = trainQueries.shape[0]  \n\tn_test = testQueries.shape[0]\n\tn_val = valQueries.shape[0]\n\tprint(""Training/Validation/Testing Size: "", n_train, n_val, n_test)\n\n\t## Calculating training batch sizes\n\tbatches = zip(range(0, n_train-FLAGS.batch_size, FLAGS.batch_size), range(FLAGS.batch_size, n_train, FLAGS.batch_size))\n\tbatches = [(start, end) for start, end in batches]\n\tbatches.append( (batches[-1][1], n_train) )\n\tevalTrainBatches = batches[:]\n\n\t## Training of the model\n\twith tf.Graph().as_default():\n\t\ttf.set_random_seed(1234) # Graph level random seed\n\n\t\tsess = tf.Session(config=session_conf) # Defining the session of the Graph\n\t\twith sess.as_default():\n\n\t\t\tmodel = ICON(FLAGS, sess)\n\n\t\t\tmax_val_accuracy = 0\n\t\t\tmax_test_acc= 0\n\t\t\tmin_val_loss = 100000\n\t\t\ttmax_val_test_accuracy = 0\n\t\t\tmax_val_test_preds = None\n\n\t\t\tfor t in range(1, FLAGS.epochs+1):\n\n\t\t\t\t# Annealing of the learning rate\n\t\t\t\tif t - 1 <= FLAGS.anneal_stop_epoch:\n\t\t\t\t\tanneal = 2.0 ** ((t - 1) // FLAGS.anneal_rate)\n\t\t\t\telse:\n\t\t\t\t\tanneal = 2.0 ** (FLAGS.anneal_stop_epoch // FLAGS.anneal_rate)\n\t\t\t\tlr = FLAGS.learning_rate / anneal\n\n\t\t\t\t# Shuffling the batches in each epoch\n\t\t\t\t# np.random.shuffle(batches)\n\n\t\t\t\ttotal_cost = 0.0\n\t\t\t\tfor start, end in batches:\n\t\t\t\t\thistOwn = trainOwnHistory[start:end]\n\t\t\t\t\thistOther = trainOtherHistory[start:end]\n\t\t\t\t\thistOwnMask = trainOwnHistoryMask[start:end]\n\t\t\t\t\thistOtherMask = trainOtherHistoryMask[start:end]\n\t\t\t\t\tmask = (histOwnMask + histOtherMask).astype(np.bool)\n\t\t\t\t\tquery = trainQueries[start:end]\n\t\t\t\t\tanswers = trainLabels[start:end]\n\t\t\t\t\t\n\t\t\t\t\tif answers.shape[0] < FLAGS.batch_size:\n\t\t\t\t\t\thistOwn = np.concatenate( (histOwn, np.zeros( (FLAGS.batch_size-histOwn.shape[0],histOwn.shape[1], histOwn.shape[2]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\thistOther = np.concatenate( (histOther, np.zeros( (FLAGS.batch_size-histOther.shape[0],histOther.shape[1], histOther.shape[2]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\thistOwnMask = np.concatenate( (histOwnMask, np.zeros( (FLAGS.batch_size-histOwnMask.shape[0],histOwn.shape[1]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\thistOtherMask = np.concatenate( (histOtherMask, np.zeros( (FLAGS.batch_size-histOtherMask.shape[0],histOwn.shape[1]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\tmask = np.concatenate( (mask, np.zeros((FLAGS.batch_size-mask.shape[0],histOwn.shape[1]) , dtype=np.bool))  , axis=0)\n\t\t\t\t\t\tquery = np.concatenate( (query, np.zeros( (FLAGS.batch_size-query.shape[0],query.shape[1]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\tanswers = np.concatenate( (answers, np.zeros( (FLAGS.batch_size-answers.shape[0],answers.shape[1]) , dtype=np.float32))  , axis=0)\n\n\t\t\t\t\tcost_t = model.batch_fit(histOwn, histOther, histOwnMask, histOtherMask, mask, query, answers, lr, FLAGS.dropout_keep_prob, training_mode=True)\n\t\t\t\t\ttotal_cost += cost_t\n\t\t\t\t\t\n\n\t\t\t\tif t % FLAGS.evaluation_interval == 0:\n\n\t\t\t\t\t## Training evaluation\n\n\t\t\t\t\ttrain_preds = []\n\t\t\t\t\tfor start, end in evalTrainBatches:\n\t\t\t\t\t\thistOwn = trainOwnHistory[start:end]\n\t\t\t\t\t\thistOther = trainOtherHistory[start:end]\n\t\t\t\t\t\thistOwnMask = trainOwnHistoryMask[start:end]\n\t\t\t\t\t\thistOtherMask = trainOtherHistoryMask[start:end]\n\t\t\t\t\t\tmask = (histOwnMask + histOtherMask).astype(np.bool)\n\t\t\t\t\t\tquery = trainQueries[start:end]\n\t\t\t\t\t\tanswers = trainLabels[start:end]\n\t\t\t\t\t\t\n\t\t\t\t\t\tif answers.shape[0] < FLAGS.batch_size:\n\t\t\t\t\t\t\thistOwn = np.concatenate( (histOwn, np.zeros( (FLAGS.batch_size-histOwn.shape[0],histOwn.shape[1], histOwn.shape[2]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\t\thistOther = np.concatenate( (histOther, np.zeros( (FLAGS.batch_size-histOther.shape[0],histOther.shape[1], histOther.shape[2]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\t\thistOwnMask = np.concatenate( (histOwnMask, np.zeros( (FLAGS.batch_size-histOwnMask.shape[0],histOwn.shape[1]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\t\thistOtherMask = np.concatenate( (histOtherMask, np.zeros( (FLAGS.batch_size-histOtherMask.shape[0],histOwn.shape[1]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\t\tmask = np.concatenate( (mask, np.zeros((FLAGS.batch_size-mask.shape[0],histOwn.shape[1]) , dtype=np.bool))  , axis=0)\n\t\t\t\t\t\t\tquery = np.concatenate( (query, np.zeros( (FLAGS.batch_size-query.shape[0],query.shape[1]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\t\tanswers = np.concatenate( (answers, np.zeros( (FLAGS.batch_size-answers.shape[0],answers.shape[1]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\tloss, pred= model.predict(histOwn, histOther, histOwnMask, histOtherMask, mask, query, FLAGS.dropout_keep_prob, answers, training_mode=False)\n\t\t\t\t\t\ttrain_preds += list(pred)\n\t\t\t\t\t\t\n\t\t\t\t\t\n\t\t\t\t\ttrain_preds = train_preds[:n_train]\n\t\t\t\t\ttrain_acc = metrics.accuracy_score(np.argmax(trainLabels, axis=1), np.array(train_preds))\n\n\t\t\t\t\tprint(total_cost, train_acc)\n\n\t\t\t\t\t\n\n\t\t\t\t\t# def evaluation(n_data, ownHistory, otherHistory, ownHistoryMask, otherHistoryMask, queries, labels):\n\t\t\t\t\t# \tbatches = zip(range(0, n_data, FLAGS.batch_size), range(FLAGS.batch_size, n_data+FLAGS.batch_size, FLAGS.batch_size))\n\t\t\t\t\t# \tbatches = [(start, end) for start, end in batches]\n\t\t\t\t\t# \tpreds=[]\n\t\t\t\t\t# \tloss=0.0\n\n\t\t\t\t\t# \tfor start, end in batches:\n\t\t\t\t\t# \t\thistOwn, histOther = ownHistory[start:end], otherHistory[start:end]\n\t\t\t\t\t# \t\thistOwnMask, histOtherMask = ownHistoryMask[start:end], otherHistoryMask[start:end]\n\t\t\t\t\t# \t\tmask = (histOwnMask + histOtherMask).astype(np.bool)\n\t\t\t\t\t# \t\tquery, answers = queries[start:end], labels[start:end]\n\n\t\t\t\t\t# \t\tprint(histOwn.shape, histOther.shape, histOwnMask.shape, histOtherMask.shape, mask.shape, query.shape, answers.shape)\n\t\t\t\t\t# \t\texit()\n\n\t\t\t\t\t# evaluation(n_val, valOwnHistory,  valOtherHistory, valOwnHistoryMask, valOtherHistoryMask, valQueries, valLabels)\n\n\t\t\t\t\t## Validation evaluation\n\n\t\t\t\t\t# Creating batches for validation\n\t\t\t\t\tval_batches = zip(range(0, n_val, FLAGS.batch_size), range(FLAGS.batch_size, n_val+FLAGS.batch_size, FLAGS.batch_size))\n\t\t\t\t\tval_batches = [(start, end) for start, end in val_batches]\n\t\t\t\t\tval_preds=[]\n\t\t\t\t\tval_loss = 0.0\n\n\t\t\t\t\tfor start, end in val_batches:\n\t\t\t\t\t\thistOwn = valOwnHistory[start:end]\n\t\t\t\t\t\thistOther = valOtherHistory[start:end]\n\t\t\t\t\t\thistOwnMask = valOwnHistoryMask[start:end]\n\t\t\t\t\t\thistOtherMask = valOtherHistoryMask[start:end]\n\t\t\t\t\t\tmask = (histOwnMask + histOtherMask).astype(np.bool)\n\t\t\t\t\t\tquery = valQueries[start:end]\n\t\t\t\t\t\tanswers = valLabels[start:end]\n\t\t\t\t\t\t\n\t\t\t\t\t\tif histOwn.shape[0] < FLAGS.batch_size:\n\t\t\t\t\t\t\thistOwn = np.concatenate( (histOwn, np.zeros( (FLAGS.batch_size-histOwn.shape[0],histOwn.shape[1], histOwn.shape[2]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\t\thistOther = np.concatenate( (histOther, np.zeros( (FLAGS.batch_size-histOther.shape[0],histOther.shape[1], histOther.shape[2]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\t\thistOwnMask = np.concatenate( (histOwnMask, np.zeros( (FLAGS.batch_size-histOwnMask.shape[0],histOwn.shape[1]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\t\thistOtherMask = np.concatenate( (histOtherMask, np.zeros( (FLAGS.batch_size-histOtherMask.shape[0],histOwn.shape[1]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\t\tmask = np.concatenate( (mask, np.zeros((FLAGS.batch_size-mask.shape[0],histOwn.shape[1]) , dtype=np.bool))  , axis=0)\n\t\t\t\t\t\t\tquery = np.concatenate( (query, np.zeros( (FLAGS.batch_size-query.shape[0],query.shape[1]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\t\tanswers = np.concatenate( (answers, np.zeros( (FLAGS.batch_size-answers.shape[0],answers.shape[1]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\tloss, pred = model.predict(histOwn, histOther, histOwnMask, histOtherMask, mask, query, 1.0, answers, training_mode=False)\n\t\t\t\t\t\tval_preds += list(pred)\n\t\t\t\t\t\tval_loss += loss\n\n\t\t\t\t\tval_preds = val_preds[:n_val]\n\t\t\t\t\tval_acc = metrics.accuracy_score(np.argmax(valLabels, axis=1), val_preds)\n\n\t\t\t\t\t## Testing evaluation\n\n\t\t\t\t\t# Creating batches for testing\n\t\t\t\t\ttest_batches = zip(range(0, n_test, FLAGS.batch_size), range(FLAGS.batch_size, n_test+FLAGS.batch_size, FLAGS.batch_size))\n\t\t\t\t\ttest_batches = [(start, end) for start, end in test_batches]\n\t\t\t\t\ttest_preds=[]\n\t\t\t\t\tfor start, end in test_batches:\n\t\t\t\t\t\thistOwn = testOwnHistory[start:end]\n\t\t\t\t\t\thistOther = testOtherHistory[start:end]\n\t\t\t\t\t\thistOwnMask = testOwnHistoryMask[start:end]\n\t\t\t\t\t\thistOtherMask = testOtherHistoryMask[start:end]\n\t\t\t\t\t\tmask = (histOwnMask + histOtherMask).astype(np.bool)\n\t\t\t\t\t\tquery = testQueries[start:end]\n\t\t\t\t\t\tanswers = testLabels[start:end]\n\t\t\t\t\t\t\n\t\t\t\t\t\tif histOwn.shape[0] < FLAGS.batch_size:\n\t\t\t\t\t\t\thistOwn = np.concatenate( (histOwn, np.zeros( (FLAGS.batch_size-histOwn.shape[0],histOwn.shape[1], histOwn.shape[2]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\t\thistOther = np.concatenate( (histOther, np.zeros( (FLAGS.batch_size-histOther.shape[0],histOther.shape[1], histOther.shape[2]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\t\thistOwnMask = np.concatenate( (histOwnMask, np.zeros( (FLAGS.batch_size-histOwnMask.shape[0],histOwn.shape[1]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\t\thistOtherMask = np.concatenate( (histOtherMask, np.zeros( (FLAGS.batch_size-histOtherMask.shape[0],histOwn.shape[1]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\t\tmask = np.concatenate( (mask, np.zeros((FLAGS.batch_size-mask.shape[0],histOwn.shape[1]) , dtype=np.bool))  , axis=0)\n\t\t\t\t\t\t\tquery = np.concatenate( (query, np.zeros( (FLAGS.batch_size-query.shape[0],query.shape[1]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\t\tanswers = np.concatenate( (answers, np.zeros( (FLAGS.batch_size-answers.shape[0],answers.shape[1]) , dtype=np.float32))  , axis=0)\n\t\t\t\t\t\tloss, pred = model.predict(histOwn, histOther, histOwnMask, histOtherMask, mask, query, 1.0, answers, training_mode=False)\n\t\t\t\t\t\ttest_preds += list(pred)\n\n\t\t\t\t\ttest_preds = test_preds[:n_test]\n\t\t\t\t\ttest_acc = metrics.accuracy_score(np.argmax(testLabels, axis=1), test_preds)\n\t\t\t\t\t\n\t\t\t\t\ttest_cmat = confusion_matrix(np.argmax(testLabels, axis=1), test_preds)\n\t\t\t\t\ttest_fscore = metrics.classification_report(np.argmax(testLabels, axis=1), test_preds, digits=3)\n\n\n\t\t\t\t\tprint(\'-----------------------\')\n\t\t\t\t\tprint(\'Epoch\', t)\n\t\t\t\t\tprint(\'Total Cost:\', total_cost, \', Training Accuracy:\', train_acc, \', Validation Accuracy:\', val_acc,\\\n\t\t\t\t\t\'Validation Loss:\', val_loss, "", Testing Accuracy:"", test_acc)\n\n\t\t\t\t\tprint(\'-----------------------\')\n\t\t\t\t\tif val_loss < min_val_loss:\n\t\t\t\t\t\tmax_val_acc = val_acc\n\t\t\t\t\t\tmax_test_acc = test_acc\n\t\t\t\t\t\tmax_test_preds = test_preds\n\t\t\t\t\t\tmax_fscore = test_fscore\n\t\t\t\t\t\tmax_test_cmat = test_cmat\n\t\t\t\t\t\tmin_val_loss = val_loss\n\n\tprint(""Final metrics:"")\n\tprint(""confusion_matrix (test): "")\n\tprint(max_test_cmat)\n\tprint(""val accuracy: "", max_val_acc, "" test accuracy: "", max_test_acc)\n\tprint(""classification report: "")\n\tprint(max_fscore)\n\treturn max_test_acc\n\nif __name__ == ""__main__"":\n\tmain()\n\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n'"
TL-ERC/dailydialog_preprocess.py,0,"b'from multiprocessing import Pool\nfrom pathlib import Path\nfrom collections import OrderedDict\nfrom urllib.request import urlretrieve\nimport os\nimport argparse\nimport tarfile\nimport pickle\n\nfrom tqdm import tqdm\nimport pandas as pd, numpy as np\n\nfrom utils import Tokenizer, Vocab, PAD_TOKEN, SOS_TOKEN, EOS_TOKEN\n\n\nproject_dir = Path(__file__).resolve().parent\ndatasets_dir = project_dir.joinpath(\'datasets/\')\ndailydialog_dir = datasets_dir.joinpath(\'dailydialog/\')\n\n# dailydialog_meta_dir = dailydialog_dir.joinpath(\'meta/\')\n# dialogs_dir = dailydialog_dir.joinpath(\'dialogs/\')\nGLOVE_DIR = """"\n\ntokenizer = Tokenizer(\'spacy\')\n\nemo_classes = {\'no_emotion\': 0, \'happiness\': 1, \'sadness\': 2, \'surprise\': 3, \n                \'anger\': 4, \'fear\': 5, \'disgust\':6}\n\n\ndef read_and_tokenize(dialog_path):\n    """"""\n    Read conversation\n    Args:\n        dialog_path (str): path of dialog (tsv format)\n    Return:\n        dialogs: (list of list of str) [dialog_length, sentence_length]\n        users: (list of str); [2]\n    """"""\n    \n    all_dialogs = []\n    all_emotion_classes = []\n    with open(dialog_path, \'r\') as f:\n        \n        for line in tqdm(f):\n            dialog = []\n            emotions = []\n            \n            s = eval(line)\n            for item in s[\'dialogue\']:\n                # print (item[\'text\'])\n                dialog.append(item[\'text\'])\n                emotions.append(emo_classes[item[\'emotion\']])\n                \n            # print (\'-\'*30)\n            dialog = [tokenizer(sentence) for sentence in dialog]\n            \n            #for k in range(1, len(dialog)):\n            all_dialogs.append(dialog)\n            all_emotion_classes.append(emotions)\n\n    return all_dialogs, all_emotion_classes #, users\n\n\ndef pad_sentences(conversations, max_sentence_length=30, max_conversation_length=10):\n\n    def pad_tokens(tokens, max_sentence_length=max_sentence_length):\n        n_valid_tokens = len(tokens)\n        if n_valid_tokens > max_sentence_length - 1:\n            tokens = tokens[:max_sentence_length - 1]\n        n_pad = max_sentence_length - n_valid_tokens - 1\n        tokens = tokens + [EOS_TOKEN] + [PAD_TOKEN] * n_pad\n        return tokens\n\n    def pad_conversation(conversation):\n        conversation = [pad_tokens(sentence) for sentence in conversation]\n        return conversation\n\n    all_padded_sentences = []\n    all_sentence_length = []\n\n    for conversation in conversations:\n        if len(conversation) > max_conversation_length:\n            conversation = conversation[:max_conversation_length]\n        sentence_length = [min(len(sentence) + 1, max_sentence_length) # +1 for EOS token\n                           for sentence in conversation]\n        all_sentence_length.append(sentence_length)\n\n        sentences = pad_conversation(conversation)\n        all_padded_sentences.append(sentences)\n\n    # [n_conversations, n_sentence (various), max_sentence_length]\n    sentences = all_padded_sentences\n    # [n_conversations, n_sentence (various)]\n    sentence_length = all_sentence_length\n    return sentences, sentence_length\n\n\ndef load_pretrained_glove(path):\n    glv_vector = {}\n    f = open(path, encoding=\'utf-8\')\n\n    for line in f:\n        values = line.split()\n        word = values[0]\n        try:\n            coefs = np.asarray(values[1:], dtype=\'float\')\n            glv_vector[word] = coefs\n        except ValueError:\n            continue\n    f.close()\n    return glv_vector\n\n\nif __name__ == \'__main__\':\n    \n    parser = argparse.ArgumentParser()\n\n    # Maximum valid length of sentence\n    # => SOS/EOS will surround sentence (EOS for source / SOS for target)\n    # => maximum length of tensor = max_sentence_length + 1\n    parser.add_argument(\'-s\', \'--max_sentence_length\', type=int, default=30)\n    parser.add_argument(\'-c\', \'--max_conversation_length\', type=int, default=38) # Dont change this\n\n    # Vocabulary\n    parser.add_argument(\'--max_vocab_size\', type=int, default=20000)\n    parser.add_argument(\'--min_vocab_frequency\', type=int, default=5)\n\n    # Multiprocess\n    parser.add_argument(\'--n_workers\', type=int, default=1)\n    parser.add_argument(\'--glv_path_100\', type=str, default=\'\')\n    parser.add_argument(\'--glv_path_300\', type=str, default=\'\')\n\n    args = parser.parse_args()\n\n    max_sent_len = args.max_sentence_length\n    max_conv_len = args.max_conversation_length\n    max_vocab_size = args.max_vocab_size\n    min_freq = args.min_vocab_frequency\n    n_workers = args.n_workers\n    \n    glv_path_100 = args.glv_path_100\n    glv_path_300 = args.glv_path_300\n    \n\n\n    def to_pickle(obj, path):\n        with open(str(path), \'wb\') as f:\n            pickle.dump(obj, f)\n\n    for split_type in [\'train\', \'test\', \'valid\']:\n        \n        print(\'Processing {a} dataset.\'.format(a=split_type))\n        split_data_dir = dailydialog_dir.joinpath(split_type)\n        split_data_dir.mkdir(exist_ok=True)\n        \n        dialog_path = \'datasets/dailydialog/\' + split_type + \'.json\'\n        \n        conversations, emotions = read_and_tokenize(dialog_path)\n        \n        shuffled_indices = np.arange(len(conversations))\n        \n        for j in range(10):\n            np.random.shuffle(shuffled_indices)\n            \n        conversations = list(np.array(conversations)[shuffled_indices])\n        emotions = list(np.array(emotions)[shuffled_indices])\n\n\n\n        print (\'Number of instances in {a} data: {b}.\'.format(a=split_type, b=len(conversations)))\n\n        conversation_length = [len(conversation) for conversation in conversations]\n\n\n        sentences, sentence_length = pad_sentences(\n            conversations,\n            max_sentence_length=max_sent_len,\n            max_conversation_length=max_conv_len)\n\n\n        print(\'Saving preprocessed data at\', split_data_dir)\n        to_pickle(conversation_length, split_data_dir.joinpath(\'conversation_length.pkl\'))\n        to_pickle(sentences, split_data_dir.joinpath(\'sentences.pkl\'))\n        to_pickle(sentence_length, split_data_dir.joinpath(\'sentence_length.pkl\'))\n        to_pickle(emotions, split_data_dir.joinpath(\'labels.pkl\'))\n\n        if split_type == \'train\':\n\n\n            print(\'Save Vocabulary...\')\n            vocab = Vocab(tokenizer)\n            vocab.add_dataframe(conversations)\n            assert(GLOVE_DIR != """")\n            vocab.update(GLOVE_DIR, max_size=max_vocab_size, min_freq=min_freq)\n\n            print(\'Vocabulary size: \', len(vocab))\n            vocab.pickle(dailydialog_dir.joinpath(\'word2id.pkl\'),\n                         dailydialog_dir.joinpath(\'id2word.pkl\'),\n                         dailydialog_dir.joinpath(\'word_emb.pkl\'))\n            \n\n        print(\'Done!\')\n\n'"
TL-ERC/iemocap_preprocess.py,0,"b'# Preprocess iemocap conversation emotion dataset\n\nimport argparse\nimport pickle\nimport random\nimport os\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom utils import Vocab, Tokenizer, PAD_TOKEN, SOS_TOKEN, EOS_TOKEN\n\nproject_dir = Path(__file__).resolve().parent\ndatasets_dir = project_dir.joinpath(\'datasets/\')\niemocap_dir = datasets_dir.joinpath(\'iemocap/\')\niemocap_pickle = iemocap_dir.joinpath(""IEMOCAP_features_raw.pkl"")\nGLOVE_DIR = """"\n\n# Tokenizer\ntokenizer = Tokenizer(\'spacy\')\n\nclass IEMOCAP:\n    \'\'\'\n    label index mapping = {\'hap\':0, \'sad\':1, \'neu\':2, \'ang\':3, \'exc\':4, \'fru\':5}\n    \'\'\'\n    def load_iemocap_data(self):\n        _, self.videoSpeakers, self.videoLabels, _, _, _, self.videoSentence, trainVid, self.testVid = pickle.load(\n            open(iemocap_pickle, ""rb""), encoding=""latin1"")\n\n        self.trainVid, self.valVid = train_test_split(\n            list(trainVid), test_size=.2, random_state=1227)\n        \n        self.vids = {""train"":self.trainVid, ""valid"":self.valVid, ""test"":self.testVid}\n        \n        # Calculating maximum sentence length\n        self.max_conv_length = max([len(self.videoSentence[vid]) for vid in self.trainVid])\n        \n\ndef tokenize_conversation(lines):\n    sentence_list = [tokenizer(line) for line in lines]\n    return sentence_list\n\n\ndef pad_sentences(conversations, max_sentence_length=30, max_conversation_length=10):\n    def pad_tokens(tokens, max_sentence_length=max_sentence_length):\n        n_valid_tokens = len(tokens)\n        if n_valid_tokens > max_sentence_length - 1:\n            tokens = tokens[:max_sentence_length - 1]\n        n_pad = max_sentence_length - n_valid_tokens - 1\n        tokens = tokens + [EOS_TOKEN] + [PAD_TOKEN] * n_pad\n        return tokens\n\n    def pad_conversation(conversation):\n        conversation = [pad_tokens(sentence) for sentence in conversation]\n        return conversation\n\n    all_padded_sentences = []\n    all_sentence_length = []\n\n    for conversation in conversations:\n        if len(conversation) > max_conversation_length:\n            conversation = conversation[:max_conversation_length]\n        sentence_length = [min(len(sentence) + 1, max_sentence_length)  # +1 for EOS token\n                           for sentence in conversation]\n        all_sentence_length.append(sentence_length)\n\n        sentences = pad_conversation(conversation)\n        all_padded_sentences.append(sentences)\n\n    sentences = all_padded_sentences\n    sentence_length = all_sentence_length\n    return sentences, sentence_length \n\n\nif __name__ == \'__main__\':\n\n    parser = argparse.ArgumentParser()\n    \n    # Load the dataset\n    iemocap = IEMOCAP()\n    iemocap.load_iemocap_data()\n\n    # Maximum valid length of sentence\n    # => SOS/EOS will surround sentence (EOS for source / SOS for target)\n    # => maximum length of tensor = max_sentence_length + 1\n    parser.add_argument(\'-s\', \'--max_sentence_length\', type=int, default=30)\n\n    # Vocabulary\n    parser.add_argument(\'--max_vocab_size\', type=int, default=20000)\n    parser.add_argument(\'--min_vocab_frequency\', type=int, default=5)\n\n    args = parser.parse_args()\n\n    max_sent_len = args.max_sentence_length\n    max_conv_len = iemocap.max_conv_length\n    max_vocab_size = args.max_vocab_size\n    min_freq = args.min_vocab_frequency\n\n    \n\n    def to_pickle(obj, path):\n        with open(path, \'wb\') as f:\n            pickle.dump(obj, f)\n    \n    for split_type in [\'train\', \'valid\', \'test\']:\n        conv_sentences = [iemocap.videoSentence[vid] for vid in iemocap.vids[split_type]]\n        conv_labels = [iemocap.videoLabels[vid]\n                       for vid in iemocap.vids[split_type]]\n\n\n        print(f\'Processing {split_type} dataset...\')\n        split_data_dir = iemocap_dir.joinpath(split_type)\n        split_data_dir.mkdir(exist_ok=True)\n        \n        conv_sentences = list([tokenize_conversation(conv) for conv in conv_sentences])\n        conversation_length = [min(len(conv), max_conv_len)\n                               for conv in conv_sentences]\n\n        # fix labels as per conversation_length\n        for idx, conv_len in enumerate(conversation_length):\n            conv_labels[idx]=conv_labels[idx][:conv_len]\n\n        \n        sentences, sentence_length = pad_sentences(\n            conv_sentences,\n            max_sentence_length=max_sent_len,\n            max_conversation_length=max_conv_len)\n\n        for sentence_len, label in zip(conversation_length, conv_labels):\n            assert(sentence_len ==len(label))\n\n        \n        print(\'Saving preprocessed data at\', split_data_dir)\n        to_pickle(conversation_length, split_data_dir.joinpath(\n            \'conversation_length.pkl\'))\n        to_pickle(sentences, split_data_dir.joinpath(\'sentences.pkl\'))\n        to_pickle(conv_labels, split_data_dir.joinpath(\'labels.pkl\'))\n        to_pickle(sentence_length, split_data_dir.joinpath(\n            \'sentence_length.pkl\'))\n        to_pickle(iemocap.vids[split_type], split_data_dir.joinpath(\'video_id.pkl\'))\n\n        if split_type == \'train\':\n\n            print(\'Save Vocabulary...\')\n            vocab = Vocab(tokenizer)\n            vocab.add_dataframe(conv_sentences)\n\n            assert(GLOVE_DIR != """")\n            vocab.update(GLOVE_DIR, max_size=max_vocab_size, min_freq=min_freq)\n\n            print(\'Vocabulary size: \', len(vocab))\n            vocab.pickle(iemocap_dir.joinpath(\'word2id.pkl\'),\n                         iemocap_dir.joinpath(\'id2word.pkl\'),\n                         iemocap_dir.joinpath(\'word_emb.pkl\'))'"
TL-ERC/setup.py,0,"b'import os\n\nDATASET_DIRECTORY_PATH = ""./datasets/""\nGENERATIVE_WEIGHTS_DIRECTORY_PATH = ""./generative_weights/""\n\nif not os.path.exists(DATASET_DIRECTORY_PATH):\n    os.makedirs(DATASET_DIRECTORY_PATH)\n\n\nif not os.path.exists(GENERATIVE_WEIGHTS_DIRECTORY_PATH):\n    os.makedirs(GENERATIVE_WEIGHTS_DIRECTORY_PATH)'"
bc-LSTM/baseline.py,0,"b'#Please use python 3.5 or above\r\nimport numpy as np\r\nfrom keras.preprocessing.text import Tokenizer\r\nfrom keras.preprocessing.sequence import pad_sequences\r\nfrom keras.utils import to_categorical\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Input, Dense, Embedding, LSTM, Concatenate, Reshape, GRU, Bidirectional\r\nfrom keras import optimizers\r\nfrom keras.models import load_model\r\nimport json, argparse, os\r\nimport re\r\nimport io\r\nimport sys\r\nfrom keras.models import Model\r\n\r\n# Path to training and testing data file. This data can be downloaded from a link, details of which will be provided.\r\ntrainDataPath = """"\r\ntestDataPath = """"\r\n# Output file that will be generated. This file can be directly submitted.\r\nsolutionPath = """"\r\n# Path to directory where GloVe file is saved.\r\ngloveDir = """"\r\n\r\nNUM_FOLDS = None                   # Value of K in K-fold Cross Validation\r\nNUM_CLASSES = None                 # Number of classes - Happy, Sad, Angry, Others\r\nMAX_NB_WORDS = None                # To set the upper limit on the number of tokens extracted using keras.preprocessing.text.Tokenizer \r\nMAX_SEQUENCE_LENGTH = None         # All sentences having lesser number of words than this will be padded\r\nEMBEDDING_DIM = None               # The dimension of the word embeddings\r\nBATCH_SIZE = None                  # The batch size to be chosen for training the model.\r\nLSTM_DIM = None                    # The dimension of the representations learnt by the LSTM model\r\nDROPOUT = None                     # Fraction of the units to drop for the linear transformation of the inputs. Ref - https://keras.io/layers/recurrent/\r\nNUM_EPOCHS = None                  # Number of epochs to train a model for\r\n\r\n\r\nlabel2emotion = {0:""others"", 1:""happy"", 2: ""sad"", 3:""angry""}\r\nemotion2label = {""others"":0, ""happy"":1, ""sad"":2, ""angry"":3}\r\n\r\n\r\ndef preprocessData(dataFilePath, mode):\r\n    """"""Load data from a file, process and return indices, conversations and labels in separate lists\r\n    Input:\r\n        dataFilePath : Path to train/test file to be processed\r\n        mode : ""train"" mode returns labels. ""test"" mode doesn\'t return labels.\r\n    Output:\r\n        indices : Unique conversation ID list\r\n        conversations : List of 3 turn conversations, processed and each turn separated by the <eos> tag\r\n        labels : [Only available in ""train"" mode] List of labels\r\n    """"""\r\n    indices = []\r\n    conversations = []\r\n    labels = []\r\n    u1 = []\r\n    u2 = []\r\n    u3 = []\r\n    with io.open(dataFilePath, encoding=""utf8"") as finput:\r\n        finput.readline()\r\n        for line in finput:\r\n            # Convert multiple instances of . ? ! , to single instance\r\n            # okay...sure -> okay . sure\r\n            # okay???sure -> okay ? sure\r\n            # Add whitespace around such punctuation\r\n            # okay!sure -> okay ! sure\r\n            repeatedChars = [\'.\', \'?\', \'!\', \',\']\r\n            for c in repeatedChars:\r\n                lineSplit = line.split(c)\r\n                while True:\r\n                    try:\r\n                        lineSplit.remove(\'\')\r\n                    except:\r\n                        break\r\n                cSpace = \' \' + c + \' \'    \r\n                line = cSpace.join(lineSplit)\r\n            \r\n            line = line.strip().split(\'\\t\')\r\n            if mode == ""train"":\r\n                # Train data contains id, 3 turns and label\r\n                label = emotion2label[line[4]]\r\n                labels.append(label)\r\n            \r\n            conv = \' <eos> \'.join(line[1:4])\r\n\r\n            u1.append(line[1])\r\n            u2.append(line[2])\r\n            u3.append(line[3])\r\n            \r\n            # Remove any duplicate spaces\r\n            duplicateSpacePattern = re.compile(r\'\\ +\')\r\n            conv = re.sub(duplicateSpacePattern, \' \', conv)\r\n            \r\n            indices.append(int(line[0]))\r\n            conversations.append(conv.lower())\r\n    \r\n    if mode == ""train"":\r\n        return indices, conversations, labels, u1, u2, u3\r\n    else:\r\n        return indices, conversations, u1, u2, u3\r\n\r\n\r\ndef getMetrics(predictions, ground):\r\n    """"""Given predicted labels and the respective ground truth labels, display some metrics\r\n    Input: shape [# of samples, NUM_CLASSES]\r\n        predictions : Model output. Every row has 4 decimal values, with the highest belonging to the predicted class\r\n        ground : Ground truth labels, converted to one-hot encodings. A sample belonging to Happy class will be [0, 1, 0, 0]\r\n    Output:\r\n        accuracy : Average accuracy\r\n        microPrecision : Precision calculated on a micro level. Ref - https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin/16001\r\n        microRecall : Recall calculated on a micro level\r\n        microF1 : Harmonic mean of microPrecision and microRecall. Higher value implies better classification  \r\n    """"""\r\n    # [0.1, 0.3 , 0.2, 0.1] -> [0, 1, 0, 0]\r\n    discretePredictions = to_categorical(predictions.argmax(axis=1))\r\n    \r\n    truePositives = np.sum(discretePredictions*ground, axis=0)\r\n    falsePositives = np.sum(np.clip(discretePredictions - ground, 0, 1), axis=0)\r\n    falseNegatives = np.sum(np.clip(ground-discretePredictions, 0, 1), axis=0)\r\n    \r\n    print(""True Positives per class : "", truePositives)\r\n    print(""False Positives per class : "", falsePositives)\r\n    print(""False Negatives per class : "", falseNegatives)\r\n    \r\n    # ------------- Macro level calculation ---------------\r\n    macroPrecision = 0\r\n    macroRecall = 0\r\n    # We ignore the ""Others"" class during the calculation of Precision, Recall and F1\r\n    for c in range(1, NUM_CLASSES):\r\n        precision = truePositives[c] / (truePositives[c] + falsePositives[c])\r\n        macroPrecision += precision\r\n        recall = truePositives[c] / (truePositives[c] + falseNegatives[c])\r\n        macroRecall += recall\r\n        f1 = ( 2 * recall * precision ) / (precision + recall) if (precision+recall) > 0 else 0\r\n        print(""Class %s : Precision : %.3f, Recall : %.3f, F1 : %.3f"" % (label2emotion[c], precision, recall, f1))\r\n    \r\n    macroPrecision /= 3\r\n    macroRecall /= 3\r\n    macroF1 = (2 * macroRecall * macroPrecision ) / (macroPrecision + macroRecall) if (macroPrecision+macroRecall) > 0 else 0\r\n    print(""Ignoring the Others class, Macro Precision : %.4f, Macro Recall : %.4f, Macro F1 : %.4f"" % (macroPrecision, macroRecall, macroF1))   \r\n    \r\n    # ------------- Micro level calculation ---------------\r\n    truePositives = truePositives[1:].sum()\r\n    falsePositives = falsePositives[1:].sum()\r\n    falseNegatives = falseNegatives[1:].sum()    \r\n    \r\n    print(""Ignoring the Others class, Micro TP : %d, FP : %d, FN : %d"" % (truePositives, falsePositives, falseNegatives))\r\n    \r\n    microPrecision = truePositives / (truePositives + falsePositives)\r\n    microRecall = truePositives / (truePositives + falseNegatives)\r\n    \r\n    microF1 = ( 2 * microRecall * microPrecision ) / (microPrecision + microRecall) if (microPrecision+microRecall) > 0 else 0\r\n    # -----------------------------------------------------\r\n    \r\n    predictions = predictions.argmax(axis=1)\r\n    ground = ground.argmax(axis=1)\r\n    accuracy = np.mean(predictions==ground)\r\n    \r\n    print(""Accuracy : %.4f, Micro Precision : %.4f, Micro Recall : %.4f, Micro F1 : %.4f"" % (accuracy, microPrecision, microRecall, microF1))\r\n    return accuracy, microPrecision, microRecall, microF1\r\n\r\n\r\ndef writeNormalisedData(dataFilePath, texts):\r\n    """"""Write normalised data to a file\r\n    Input:\r\n        dataFilePath : Path to original train/test file that has been processed\r\n        texts : List containing the normalised 3 turn conversations, separated by the <eos> tag.\r\n    """"""\r\n    normalisedDataFilePath = dataFilePath.replace("".txt"", ""_normalised.txt"")\r\n    with io.open(normalisedDataFilePath, \'w\', encoding=\'utf8\') as fout:\r\n        with io.open(dataFilePath, encoding=\'utf8\') as fin:\r\n            fin.readline()\r\n            for lineNum, line in enumerate(fin):\r\n                line = line.strip().split(\'\\t\')\r\n                normalisedLine = texts[lineNum].strip().split(\'<eos>\')\r\n                fout.write(line[0] + \'\\t\')\r\n                # Write the original turn, followed by the normalised version of the same turn\r\n                fout.write(line[1] + \'\\t\' + normalisedLine[0] + \'\\t\')\r\n                fout.write(line[2] + \'\\t\' + normalisedLine[1] + \'\\t\')\r\n                fout.write(line[3] + \'\\t\' + normalisedLine[2] + \'\\t\')\r\n                try:\r\n                    # If label information available (train time)\r\n                    fout.write(line[4] + \'\\n\')    \r\n                except:\r\n                    # If label information not available (test time)\r\n                    fout.write(\'\\n\')\r\n\r\n\r\ndef getEmbeddingMatrix(wordIndex):\r\n    """"""Populate an embedding matrix using a word-index. If the word ""happy"" has an index 19,\r\n       the 19th row in the embedding matrix should contain the embedding vector for the word ""happy"".\r\n    Input:\r\n        wordIndex : A dictionary of (word : index) pairs, extracted using a tokeniser\r\n    Output:\r\n        embeddingMatrix : A matrix where every row has 100 dimensional GloVe embedding\r\n    """"""\r\n    embeddingsIndex = {}\r\n    # Load the embedding vectors from ther GloVe file\r\n    with io.open(os.path.join(gloveDir, \'glove.840B.300d.txt\'), encoding=""utf8"") as f:\r\n        for line in f:\r\n            values = line.split(\' \')\r\n           # print(values)\r\n            word = values[0]\r\n            embeddingVector = np.array([float(val) for val in values[1:]])\r\n            embeddingsIndex[word] = embeddingVector\r\n    \r\n    print(\'Found %s word vectors.\' % len(embeddingsIndex))\r\n    \r\n    # Minimum word index of any word is 1. \r\n    embeddingMatrix = np.zeros((len(wordIndex) + 1, EMBEDDING_DIM))\r\n    for word, i in wordIndex.items():\r\n        embeddingVector = embeddingsIndex.get(word)\r\n        if embeddingVector is not None:\r\n            # words not found in embedding index will be all-zeros.\r\n            embeddingMatrix[i] = embeddingVector\r\n    \r\n    return embeddingMatrix\r\n            \r\n\r\ndef buildModel(embeddingMatrix):\r\n    """"""Constructs the architecture of the model\r\n    Input:\r\n        embeddingMatrix : The embedding matrix to be loaded in the embedding layer.\r\n    Output:\r\n        model : A basic LSTM model\r\n    """"""\r\n    x1 = Input(shape=(100,), dtype=\'int32\', name=\'main_input1\')\r\n    x2 = Input(shape=(100,), dtype=\'int32\', name=\'main_input2\')\r\n    x3 = Input(shape=(100,), dtype=\'int32\', name=\'main_input3\')\r\n\r\n    embeddingLayer = Embedding(embeddingMatrix.shape[0],\r\n                                EMBEDDING_DIM,\r\n                                weights=[embeddingMatrix],\r\n                                input_length=MAX_SEQUENCE_LENGTH,\r\n                                trainable=False)\r\n    emb1 = embeddingLayer(x1)\r\n    emb2 = embeddingLayer(x2)\r\n    emb3 = embeddingLayer(x3)\r\n\r\n    lstm = Bidirectional(LSTM(LSTM_DIM, dropout=DROPOUT))\r\n\r\n    lstm1 = lstm(emb1)\r\n    lstm2 = lstm(emb2)\r\n    lstm3 = lstm(emb3)\r\n\r\n    inp = Concatenate(axis=-1)([lstm1, lstm2, lstm3])\r\n\r\n    inp = Reshape((3, 2*LSTM_DIM, )) (inp)\r\n\r\n    lstm_up = LSTM(LSTM_DIM, dropout=DROPOUT)\r\n\r\n    out = lstm_up(inp)\r\n\r\n    out = Dense(NUM_CLASSES, activation=\'softmax\')(out)\r\n    \r\n    adam = optimizers.adam(lr=LEARNING_RATE)\r\n    model = Model([x1,x2,x3],out)\r\n    model.compile(loss=\'categorical_crossentropy\',\r\n                  optimizer=adam,\r\n                  metrics=[\'acc\'])\r\n    print(model.summary())\r\n    return model\r\n    \r\n\r\ndef main():\r\n    parser = argparse.ArgumentParser(description=""Baseline Script for SemEval"")\r\n    parser.add_argument(\'-config\', help=\'Config to read details\', required=True)\r\n    args = parser.parse_args()\r\n\r\n    with open(args.config) as configfile:\r\n        config = json.load(configfile)\r\n        \r\n    global trainDataPath, testDataPath, solutionPath, gloveDir\r\n    global NUM_FOLDS, NUM_CLASSES, MAX_NB_WORDS, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM\r\n    global BATCH_SIZE, LSTM_DIM, DROPOUT, NUM_EPOCHS, LEARNING_RATE    \r\n    \r\n    trainDataPath = config[""train_data_path""]\r\n    testDataPath = config[""test_data_path""]\r\n    solutionPath = config[""solution_path""]\r\n    gloveDir = config[""glove_dir""]\r\n    \r\n    NUM_FOLDS = config[""num_folds""]\r\n    NUM_CLASSES = config[""num_classes""]\r\n    MAX_NB_WORDS = config[""max_nb_words""]\r\n    MAX_SEQUENCE_LENGTH = config[""max_sequence_length""]\r\n    EMBEDDING_DIM = config[""embedding_dim""]\r\n    BATCH_SIZE = config[""batch_size""]\r\n    LSTM_DIM = config[""lstm_dim""]\r\n    DROPOUT = config[""dropout""]\r\n    LEARNING_RATE = config[""learning_rate""]\r\n    NUM_EPOCHS = config[""num_epochs""]\r\n        \r\n    print(""Processing training data..."")\r\n    trainIndices, trainTexts, labels, u1_train, u2_train, u3_train = preprocessData(trainDataPath, mode=""train"")\r\n    # Write normalised text to file to check if normalisation works. Disabled now. Uncomment following line to enable   \r\n    # writeNormalisedData(trainDataPath, trainTexts)\r\n    print(""Processing test data..."")\r\n    testIndices, testTexts, u1_test, u2_test, u3_test = preprocessData(testDataPath, mode=""test"")\r\n    # writeNormalisedData(testDataPath, testTexts)\r\n\r\n    print(""Extracting tokens..."")\r\n    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\r\n    tokenizer.fit_on_texts(u1_train+u2_train+u3_train)\r\n    u1_trainSequences, u2_trainSequences, u3_trainSequences = tokenizer.texts_to_sequences(u1_train), tokenizer.texts_to_sequences(u2_train), tokenizer.texts_to_sequences(u3_train)\r\n    u1_testSequences, u2_testSequences, u3_testSequences = tokenizer.texts_to_sequences(u1_test), tokenizer.texts_to_sequences(u2_test), tokenizer.texts_to_sequences(u3_test)\r\n    \r\n\r\n    wordIndex = tokenizer.word_index\r\n    print(""Found %s unique tokens."" % len(wordIndex))\r\n\r\n    print(""Populating embedding matrix..."")\r\n    embeddingMatrix = getEmbeddingMatrix(wordIndex)\r\n\r\n    u1_data = pad_sequences(u1_trainSequences, maxlen=MAX_SEQUENCE_LENGTH)\r\n    u2_data = pad_sequences(u2_trainSequences, maxlen=MAX_SEQUENCE_LENGTH)\r\n    u3_data = pad_sequences(u3_trainSequences, maxlen=MAX_SEQUENCE_LENGTH)\r\n    labels = to_categorical(np.asarray(labels))\r\n    print(""Shape of training data tensor: "", u1_data.shape)\r\n    print(""Shape of label tensor: "", labels.shape)\r\n        \r\n    # Randomize data\r\n    np.random.shuffle(trainIndices)\r\n\r\n    u1_data = u1_data[trainIndices]\r\n    u2_data = u2_data[trainIndices]\r\n    u3_data = u3_data[trainIndices]\r\n\r\n    labels = labels[trainIndices]\r\n      \r\n    # Perform k-fold cross validation\r\n    metrics = {""accuracy"" : [],\r\n               ""microPrecision"" : [],\r\n               ""microRecall"" : [],\r\n               ""microF1"" : []}\r\n    \r\n    print(""Starting k-fold cross validation..."")\r\n    print(\'-\'*40)\r\n    print(""Building model..."")\r\n    model = buildModel(embeddingMatrix)\r\n    #model.fit([u1_data,u2_data,u3_data], labels, \r\n    #              validation_split=0.1,\r\n    #              epochs=NUM_EPOCHS, batch_size=BATCH_SIZE)\r\n\r\n        #predictions = model.predict(xVal, batch_size=BATCH_SIZE)\r\n        #accuracy, microPrecision, microRecall, microF1 = getMetrics(predictions, yVal)\r\n        #metrics[""accuracy""].append(accuracy)\r\n        #metrics[""microPrecision""].append(microPrecision)\r\n        #metrics[""microRecall""].append(microRecall)\r\n        #metrics[""microF1""].append(microF1)\r\n        \r\n    #print(""\\n============= Metrics ================="")\r\n    #print(""Average Cross-Validation Accuracy : %.4f"" % (sum(metrics[""accuracy""])/len(metrics[""accuracy""])))\r\n    #print(""Average Cross-Validation Micro Precision : %.4f"" % (sum(metrics[""microPrecision""])/len(metrics[""microPrecision""])))\r\n    #print(""Average Cross-Validation Micro Recall : %.4f"" % (sum(metrics[""microRecall""])/len(metrics[""microRecall""])))\r\n    #print(""Average Cross-Validation Micro F1 : %.4f"" % (sum(metrics[""microF1""])/len(metrics[""microF1""])))\r\n    \r\n    print(""\\n======================================"")\r\n    \r\n    print(""Retraining model on entire data to create solution file"")\r\n    model = buildModel(embeddingMatrix)\r\n    model.fit([u1_data,u2_data,u3_data], labels, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE)\r\n    model.save(\'EP%d_LR%de-5_LDim%d_BS%d.h5\'%(NUM_EPOCHS, int(LEARNING_RATE*(10**5)), LSTM_DIM, BATCH_SIZE))\r\n    # model = load_model(\'EP%d_LR%de-5_LDim%d_BS%d.h5\'%(NUM_EPOCHS, int(LEARNING_RATE*(10**5)), LSTM_DIM, BATCH_SIZE))\r\n\r\n    print(""Creating solution file..."")\r\n    u1_testData, u2_testData, u3_testData = pad_sequences(u1_testSequences, maxlen=MAX_SEQUENCE_LENGTH), pad_sequences(u2_testSequences, maxlen=MAX_SEQUENCE_LENGTH), pad_sequences(u3_testSequences, maxlen=MAX_SEQUENCE_LENGTH)\r\n    predictions = model.predict([u1_testData, u2_testData, u3_testData], batch_size=BATCH_SIZE)\r\n    predictions = predictions.argmax(axis=1)\r\n\r\n    with io.open(solutionPath, ""w"", encoding=""utf8"") as fout:\r\n        fout.write(\'\\t\'.join([""id"", ""turn1"", ""turn2"", ""turn3"", ""label""]) + \'\\n\')        \r\n        with io.open(testDataPath, encoding=""utf8"") as fin:\r\n            fin.readline()\r\n            for lineNum, line in enumerate(fin):\r\n                fout.write(\'\\t\'.join(line.strip().split(\'\\t\')[:4]) + \'\\t\')\r\n                fout.write(label2emotion[predictions[lineNum]] + \'\\n\')\r\n    print(""Completed. Model parameters: "")\r\n    print(""Learning rate : %.3f, LSTM Dim : %d, Dropout : %.3f, Batch_size : %d"" \r\n          % (LEARNING_RATE, LSTM_DIM, DROPOUT, BATCH_SIZE))\r\n    \r\n               \r\nif __name__ == \'__main__\':\r\n    main()\r\n'"
CMN/IEMOCAP/cmn.py,0,"b'import tensorflow as tf\nimport numpy as np\nimport sys\nimport tensorflow.contrib.rnn as rnn_cell\nfrom sklearn.model_selection import train_test_split\n\n\n\n\nclass CMN:\n\n    def __init__(self, CONFIG, session = None):\n        \n        self._batch_size = CONFIG.batch_size\n        self._input_dim = CONFIG.input_dims\n        self._timesteps = CONFIG.timesteps\n        self._class_size = CONFIG.class_size\n        self._embedding_size = CONFIG.embedding_size\n        self._hops = CONFIG.hops\n        self._max_grad_norm = CONFIG.max_grad_norm\n        self._nonlin = CONFIG.nonlin\n        self._nonlin_func = CONFIG.nonlin_func\n        self._init = tf.random_normal_initializer(stddev=0.01, seed=1227)\n        self._name = ""cmn_shared""\n\n        ## inputs to receive from the dataset\n        self._build_inputs()\n\n        ## tensor variables of the tensorflow graph\n        self._build_vars()\n\n        \n\n        ## optimizer for training\n        # self._opt = tf.train.GradientDescentOptimizer(learning_rate=self._lr)\n        self._opt = tf.train.AdamOptimizer(learning_rate=self._lr)\n\n        ## cross entropy loss\n        logits = self._inference(self._histories_own, self._histories_other, self._queries) # (batch_size, class size)\n        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf.cast(self._labels, tf.float32), name=""cross_entropy"")\n        cross_entropy_sum = tf.reduce_mean(cross_entropy, keepdims=False, name=""cross_entropy_sum"")\n\n        print(\'\\n ---- TRAINABLE VARIABLES ---- \\n\')\n        tvars = tf.trainable_variables()\n        reg_loss=[]\n        for tvar in tvars:\n            print(tvar.name)\n            if ""bias"" not in tvar.name:\n                reg_loss.append(tf.nn.l2_loss(tvar))\n        print(\'----------- \\n\')\n\n        # loss op\n        self.regularization_loss = tf.reduce_mean(reg_loss)\n        self.loss_op = loss_op = tf.reduce_mean(cross_entropy_sum + 0.001*self.regularization_loss)\n\n        # gradient pipeline\n        grads_and_vars = self._opt.compute_gradients(loss_op)\n        grads_and_vars = [(tf.clip_by_norm(g, self._max_grad_norm), v) for g,v in grads_and_vars]\n        grads_and_vars = [(g, v) for g,v in grads_and_vars]\n        self.train_op = train_op = self._opt.apply_gradients(grads_and_vars, name=""train_op"")\n\n        # predict ops\n        self.predict_op = predict_op = tf.argmax(logits, 1, name=""predict_op"")\n        # self.predict_proba_op = predict_proba_op = tf.nn.softmax(logits, name=""predict_proba_op"")\n        # self.predict_log_proba_op = predict_log_proba_op = tf.log(predict_proba_op, name=""predict_log_proba_op"")\n\n        self._sess = session\n        self._sess.run(tf.global_variables_initializer())\n\n\n    def _build_inputs(self):\n\n        self._queries = tf.placeholder(tf.float32, [self._batch_size, self._input_dim], name=""queries"") # utterances to be classified\n\n        # Histories of the utterances\n        self._histories_own = tf.placeholder(tf.float32, [self._batch_size, self._timesteps, self._input_dim], name=""histories_own"") \n        self._histories_other = tf.placeholder(tf.float32, [self._batch_size, self._timesteps, self._input_dim], name=""histories_other"")\n        self._histories_own_mask = tf.cast(tf.placeholder(tf.float32, [self._batch_size, self._timesteps], name=""histories_own_mask""), dtype=tf.bool)\n        self._histories_other_mask = tf.cast(tf.placeholder(tf.float32, [self._batch_size, self._timesteps], name=""histories_other_mask""), dtype=tf.bool)\n        self._mask = tf.cast(tf.placeholder(tf.float32, [self._batch_size, self._timesteps], name=""global_mask""), dtype=tf.bool)\n\n        # True Labels\n        self._labels = tf.placeholder(tf.int32, [self._batch_size, self._class_size], name=""labels"")\n        \n\n        # Learning Rate\n        self._lr = tf.placeholder(tf.float32, [], name=""learning_rate"")\n\n        # Dropout Probability\n        self._dropout = tf.placeholder(tf.float32, [], name=""dropout_keep_rate"")\n\n        # \n        self._training = tf.placeholder(tf.bool, [], name=""training_testing_mode"")\n\n    def _build_vars(self):\n\n        with tf.variable_scope(self._name):\n\n            # with tf.variable_scope(""input""):\n\n            #     # Input Projection Matrix\n            #     self.inputProj = tf.get_variable(""inputProj"", shape=([self._input_dim, self._embedding_size]), trainable=True, initializer=self._init)\n            #     self.inputProjBias = tf.get_variable(""inputProjBias"", shape=([1, self._embedding_size]), trainable=True, initializer=self._init)\n\n            with tf.variable_scope(""output""):\n\n                # Output Projection Matrix\n                self.outputProj = tf.get_variable(""outProj"", shape=([self._embedding_size, self._class_size]), trainable=True, initializer=self._init)\n                self.outputProjBias = tf.get_variable(""outputProjBias"", shape=([1, self._class_size]), trainable=True, initializer=self._init)\n            \n            with tf.variable_scope(""localGRU""):\n\n                # GRUs for per-person local input modeling\n                self.localGRUOwn = tf.contrib.rnn.GRUCell(num_units=self._embedding_size, reuse = tf.AUTO_REUSE, name=\'localGRUOwn\')\n                self.localGRUOther = tf.contrib.rnn.GRUCell(num_units=self._embedding_size, reuse = tf.AUTO_REUSE, name=\'localGRUOther\')\n\n\n            with tf.variable_scope(""memoryGRU""):\n\n                self.memoryGRUown = tf.contrib.rnn.GRUCell(num_units=self._embedding_size, reuse = tf.AUTO_REUSE, name=\'memoryGRUown\')\n                self.memoryGRUother = tf.contrib.rnn.GRUCell(num_units=self._embedding_size, reuse = tf.AUTO_REUSE, name=\'memoryGRUother\')\n            \n\n    def _inference(self, histories_own, histories_other, queries):\n\n        with tf.variable_scope(self._name):\n\n            with tf.variable_scope(""input""):\n\n                # q = tf.add(tf.matmul(queries, self.inputProj), self.inputProjBias) # (batch_size, embedding_size) \n\n                q = tf.contrib.layers.fully_connected(\n                    queries,\n                    self._embedding_size,\n                    activation_fn=tf.nn.tanh,\n                    normalizer_fn=None,\n                    normalizer_params=None,\n                    weights_initializer=tf.contrib.layers.xavier_initializer(uniform=True, seed=1227),\n                    weights_regularizer=tf.contrib.layers.l2_regularizer(0.001),\n                    biases_initializer=tf.zeros_initializer(),\n                    trainable=True,\n                    scope=""input""\n                )\n\n            with tf.variable_scope(""localGRU""):\n\n                \n                ## Input GRU Own\n                hidden_vector = self.localGRUOwn.zero_state(self._batch_size, tf.float32)\n                ownRNNOutput=[]\n                for i in range(self._timesteps):\n\n                    localMask = tf.squeeze(self._histories_own_mask[:,tf.constant(i)]) # batch_size\n                    localInput = tf.squeeze(histories_own[:,tf.constant(i),:]) # batch_size * dim\n                    prev_hidden_vector = hidden_vector\n                    hidden_vector,_= self.localGRUOwn(localInput, hidden_vector) # batch_size * dim\n                    # mask hidden vector (mask 0 places should not apply GRU)\n                    hidden_vector = tf.where(localMask, hidden_vector, prev_hidden_vector) # batch_size * dim\n\n                    # masked output_vector\n                    output_vector = tf.where(localMask, hidden_vector, tf.zeros( (self._batch_size,self._embedding_size), dtype=np.float32)) # batch_size * dim\n                    ownRNNOutput.append(output_vector[:,tf.newaxis,:])\n                    \n                ownHistoryRNNOutput = tf.concat(ownRNNOutput, axis=1) # batch_size * timesteps * dim\n                ownHistoryRNNOutput = tf.nn.dropout(ownHistoryRNNOutput, keep_prob = self._dropout, name = ""own_rnn_dropout"")\n\n                # Input GRU Other\n                hidden_vector = self.localGRUOther.zero_state(self._batch_size, tf.float32)\n                otherRNNOutput=[]\n                for i in range(self._timesteps):\n\n                    localMask = tf.squeeze(self._histories_other_mask[:,tf.constant(i)]) # batch_size\n                    localInput = tf.squeeze(histories_other[:,tf.constant(i),:]) # batch_size * dim\n                    prev_hidden_vector = hidden_vector\n                    hidden_vector,_= self.localGRUOther(localInput, hidden_vector) # batch_size * dim\n                    # mask hidden vector (mask 0 places should not apply GRU)\n                    hidden_vector = tf.where(localMask, hidden_vector, prev_hidden_vector) # batch_size * dim\n\n                    # masked output_vector\n                    output_vector = tf.where(localMask, hidden_vector, tf.zeros( (self._batch_size,self._embedding_size), dtype=np.float32)) # batch_size * dim\n                    otherRNNOutput.append(output_vector[:,tf.newaxis,:])\n                    \n                otherHistoryRNNOutput = tf.concat(otherRNNOutput, axis=1)\n                otherHistoryRNNOutput = tf.nn.dropout(otherHistoryRNNOutput, keep_prob = self._dropout, name = ""own_rnn_dropout"")\n\n\n\n            \n            ownHistoryRNNOutput = tf.nn.tanh(ownHistoryRNNOutput)\n            otherHistoryRNNOutput = tf.nn.tanh(otherHistoryRNNOutput)\n\n\n            with tf.variable_scope(""globalGRU""):                 \n\n                for hop in range(self._hops):\n\n                    # Memory Update\n                    if hop == 0:\n                        input_rnn_outputs_own, final_state = tf.nn.dynamic_rnn(self.memoryGRUown, ownHistoryRNNOutput, dtype=tf.float32)\n                        output_rnn_outputs_own, final_state = tf.nn.dynamic_rnn(self.memoryGRUown, ownHistoryRNNOutput, dtype=tf.float32)\n                    else:\n                        input_rnn_outputs_own = output_rnn_outputs_own\n                        output_rnn_outputs_own, final_state = tf.nn.dynamic_rnn(self.memoryGRUown, ownHistoryRNNOutput, dtype=tf.float32)\n\n                    rnnOutputs = []\n                    for j in range(self._timesteps):\n                        localMask = tf.squeeze(self._histories_own_mask[:,tf.constant(j)]) # batch_size\n                        localOutput = tf.squeeze(output_rnn_outputs_own[:,tf.constant(j),:]) # batch_size * dim\n                        outputVector = tf.where(localMask, localOutput, tf.zeros((self._batch_size,self._embedding_size), dtype=np.float32))\n                        rnnOutputs.append(outputVector[:,tf.newaxis,:])\n                    output_rnn_outputs_own = tf.concat(rnnOutputs, axis=1)\n\n                    # Attentional Read operation from rnn_output memories\n                    attScore = tf.nn.tanh(tf.squeeze(tf.matmul(q[:,tf.newaxis,:], tf.transpose(input_rnn_outputs_own,[0,2,1]))))  # (batch, 1, dim)  X (batch, dim, time) == (batch, 1, time) -> (batch, time)\n                    attScore = tf.where( self._mask, attScore, tf.constant( -10000 , shape= [self._batch_size, self._timesteps], dtype= tf.float32))\n                    softmax_output = attScore = tf.nn.softmax(attScore) # (batch, time)\n                    attScore = tf.nn.dropout(attScore, keep_prob = self._dropout, name=\'ttScore_dropout\')\n                    attScore = tf.where( self._mask, attScore, tf.zeros(tf.shape(attScore), dtype= tf.float32))\n                    weighted_own = tf.squeeze(tf.matmul(attScore[:,tf.newaxis,:], output_rnn_outputs_own)) # (batch, 1, time)  X (batch, time, dim) == (batch, dim)\n\n                    if hop == 0:\n                        input_rnn_outputs_other, final_state = tf.nn.dynamic_rnn(self.memoryGRUother, otherHistoryRNNOutput, dtype=tf.float32)\n                        output_rnn_outputs_other, final_state = tf.nn.dynamic_rnn(self.memoryGRUother, otherHistoryRNNOutput, dtype=tf.float32)\n                    else:\n                        input_rnn_outputs_other = output_rnn_outputs_other\n                        output_rnn_outputs_other, final_state = tf.nn.dynamic_rnn(self.memoryGRUother, otherHistoryRNNOutput, dtype=tf.float32)\n\n                    rnnOutputs = []\n                    for j in range(self._timesteps):\n                        localMask = tf.squeeze(self._histories_other_mask[:,tf.constant(j)]) # batch_size\n                        localOutput = tf.squeeze(output_rnn_outputs_other[:,tf.constant(j),:]) # batch_size * dim\n                        outputVector = tf.where(localMask, localOutput, tf.zeros((self._batch_size,self._embedding_size), dtype=np.float32))\n                        rnnOutputs.append(outputVector[:,tf.newaxis,:])\n                    output_rnn_outputs_other = tf.concat(rnnOutputs, axis=1)\n\n                    # Attentional Read operation from rnn_output memories\n                    attScore = tf.nn.tanh(tf.squeeze(tf.matmul(q[:,tf.newaxis,:], tf.transpose(input_rnn_outputs_other,[0,2,1]))))  # (batch, 1, dim)  X (batch, dim, time) == (batch, 1, time) -> (batch, time)\n                    attScore = tf.where( self._mask, attScore, tf.constant( -10000 , shape= [self._batch_size, self._timesteps], dtype= tf.float32))\n                    softmax_output = attScore = tf.nn.softmax(attScore) # (batch, time)\n                    attScore = tf.nn.dropout(attScore, keep_prob = self._dropout, name=\'ttScore_dropout\')\n                    attScore = tf.where( self._mask, attScore, tf.zeros(tf.shape(attScore), dtype= tf.float32))\n                    weighted_other = tf.squeeze(tf.matmul(attScore[:,tf.newaxis,:], output_rnn_outputs_other)) # (batch, 1, time)  X (batch, time, dim) == (batch, dim)\n\n                    q = tf.nn.tanh(q + weighted_own + weighted_other)\n\n\n            with tf.variable_scope(""output""):\n\n                return tf.add(tf.matmul(q, self.outputProj), self.outputProjBias)\n\n\n    def batch_fit(self, histories_own, histories_other, histories_own_mask, histories_other_mask, global_mask, queries, labels, learning_rate, dropout_keep_rate, training_mode=None):\n        \'\'\'\n        Runs the training algorithm over the passed batch\n        Returns:\n            loss: floating-point number, the loss computed for the batch\n        \'\'\'\n        feed_dict = {self._histories_own: histories_own, self._histories_other: histories_other, self._queries: queries, self._labels: labels,\\\n         self._lr: learning_rate, self._histories_own_mask: histories_own_mask, self._histories_other_mask: histories_other_mask, self._dropout: dropout_keep_rate, self._mask: global_mask, self._training:training_mode}\n        loss, _ = self._sess.run([self.loss_op, self.train_op], feed_dict=feed_dict)\n        return loss\n\n        \n    def predict(self, histories_own, histories_other, histories_own_mask, histories_other_mask, global_mask, queries, dropout_keep_rate, labels, training_mode=None):\n        \'\'\'\n        Predicts answers as one-hot encoding.\n        Returns:\n            loss: floating-point number, the loss computed for the batch\n            answers: Tensor (None, class size)\n        \'\'\'\n        feed_dict = {self._histories_own: histories_own, self._histories_other: histories_other, self._queries: queries, self._labels: labels,\\\n        self._histories_own_mask: histories_own_mask, self._histories_other_mask: histories_other_mask, self._dropout: dropout_keep_rate, self._mask: global_mask, self._training:training_mode}\n        return self._sess.run([self.loss_op, self.predict_op], feed_dict=feed_dict)'"
CMN/IEMOCAP/utils_cmn.py,0,"b'import numpy as np\nimport pandas as pd\nimport pickle\nfrom sklearn import model_selection, metrics\n\nTEXT_EMBEDDINGS = ""./IEMOCAP/data/text/IEMOCAP_text_embeddings.pickle""\nVIDEO_EMBEDDINGS = ""./IEMOCAP/data/video/IEMOCAP_video_features.pickle""\nAUDIO_EMBEDDINGS = ""./IEMOCAP/data/audio/IEMOCAP_audio_features.pickle""\n\ntrainID = pickle.load(open(""./IEMOCAP/data/trainID.pkl"",\'rb\'), encoding=""latin1"")\ntestID = pickle.load(open(""./IEMOCAP/data/testID.pkl"",\'rb\'), encoding=""latin1"")\nvalID,_ = model_selection.train_test_split(testID, test_size=.4, random_state=1227)\n# valID = testID\n\ntranscripts, labels, own_historyID, other_historyID, own_historyID_rank, other_historyID_rank = pickle.load(open(""./IEMOCAP/data/dataset.pkl"",\'rb\'), encoding=""latin1"")\nlabel_idx = {\'hap\':0, \'sad\':1, \'neu\':2, \'ang\':3, \'exc\':4, \'fru\':5}\n\ndef oneHot(trainLabels, valLabels, testLabels):\n\t\n\t# Calculate the total number of classes\n\tnumOfClasses = np.max(trainLabels)+1\n\t\n\ttrainLabelOneHot = np.zeros((len(trainLabels),numOfClasses), dtype=np.float32)\n\tvalLabelOneHot = np.zeros((len(valLabels),numOfClasses), dtype=np.float32)\n\ttestLabelOneHot = np.zeros((len(testLabels),numOfClasses), dtype=np.float32)\n\n\tfor idx, label in enumerate(trainLabels):\n\t\ttrainLabelOneHot[idx, int(label)]=1.0\n\tfor idx, label in enumerate(valLabels):\n\t\tvalLabelOneHot[idx, int(label)]=1.0\n\tfor idx, label in enumerate(testLabels):\n\t\ttestLabelOneHot[idx, int(label)]=1.0\n\n\treturn trainLabelOneHot, valLabelOneHot, testLabelOneHot\n\ndef updateDictText(text_transcripts_emb, text_own_history_emb, text_other_history_emb, text_emb):\n\n\tfor ID, value in text_transcripts_emb.items():\n\t\tif ID in text_emb.keys():\n\t\t\ttext_transcripts_emb[ID] = text_emb[ID]\n\t# updating the context faeturs\n\tfor ID, value in text_own_history_emb.items():\n\t\tids = own_historyID[ID]\n\t\tfor idx, iD in enumerate(ids):\n\t\t\tif iD in text_emb.keys():\n\t\t\t\ttext_own_history_emb[ID][idx]= text_emb[iD]\n\n\t# updating the context faeturs\n\tfor ID, value in text_other_history_emb.items():\n\t\tids = other_historyID[ID]\n\t\tfor idx, iD in enumerate(ids):\n\t\t\tif iD in text_emb.keys():\n\t\t\t\ttext_other_history_emb[ID][idx]= text_emb[iD]\n\n\treturn text_transcripts_emb, text_own_history_emb, text_other_history_emb\n\n\ndef loadData(FLAGS):\n\n\t## Load Labels\n\ttrainLabels = np.asarray([label_idx[labels[ID]] for ID in trainID])\n\tvalLabels = np.asarray([label_idx[labels[ID]] for ID in valID])\n\ttestLabels = np.asarray([label_idx[labels[ID]] for ID in testID])\n\ttrainLabels, valLabels, testLabels = oneHot(trainLabels, valLabels, testLabels)\n\n\t## Loading Text features\n\ttext_transcripts_emb, text_own_history_emb, text_other_history_emb = pickle.load( open(TEXT_EMBEDDINGS, \'rb\'), encoding=""latin1"")\n\tif FLAGS.context:\n\t\tprint(""loading contextual features"")\n\t\ttext_emb = pickle.load(open(""./IEMOCAP/data/text/IEMOCAP_text_context.pickle"", \'rb\'), encoding=""latin1"")\n\t\ttext_transcripts_emb, text_own_history_emb, text_other_history_emb = updateDictText(text_transcripts_emb, text_own_history_emb, text_other_history_emb, text_emb)\n\n\t## Loading Audio features\n\taudio_emb = pickle.load(open(AUDIO_EMBEDDINGS, \'rb\'), encoding=""latin1"")\n\tif FLAGS.context:\n\t\taudio_emb_context = pickle.load(open(""./IEMOCAP/data/audio/IEMOCAP_audio_context.pickle"", \'rb\'), encoding=""latin1"")\n\t\tfor ID in audio_emb.keys():\n\t\t\tif ID in audio_emb_context.keys():\n\t\t\t\taudio_emb[ID] = audio_emb_context[ID]\n\t\n\t## Loading Video features \n\tvideo_emb = pickle.load(open(VIDEO_EMBEDDINGS, \'rb\'), encoding=""latin1"")\n\t# video_emb_context = pickle.load(open(""./IEMOCAP/data/video/IEMOCAP_video_context.pickle"", \'rb\'), encoding=""latin1"")\n\t# for ID in video_emb.keys():\n\t# \tif ID in video_emb_context.keys():\n\t# \t\tvideo_emb[ID] = video_emb_context[ID]\n\t\n\t## Text Embeddings for the queries\n\ttext_trainQueries = np.asarray([text_transcripts_emb[ID] for ID in trainID])\n\ttext_valQueries = np.asarray([text_transcripts_emb[ID] for ID in valID])\n\ttext_testQueries = np.asarray([text_transcripts_emb[ID] for ID in testID])\n\n\t## Audio Embeddings for the queries\n\taudio_trainQueries = np.asarray([audio_emb[ID] for ID in trainID])\n\taudio_valQueries = np.asarray([audio_emb[ID] for ID in valID])\n\taudio_testQueries = np.asarray([audio_emb[ID] for ID in testID])\n\n\t## Video Embeddings for the queries\n\tvideo_trainQueries = np.asarray([video_emb[ID] for ID in trainID])\n\tvideo_valQueries = np.asarray([video_emb[ID] for ID in valID])\n\tvideo_testQueries = np.asarray([video_emb[ID] for ID in testID])\n\n\n\n\t\n\tif FLAGS.mode == ""text"":\n\t\ttrainQueries = text_trainQueries\n\t\tvalQueries = text_valQueries\n\t\ttestQueries = text_testQueries\n\tif FLAGS.mode == ""video"":\n\t\ttrainQueries = video_trainQueries \n\t\tvalQueries = video_valQueries\n\t\ttestQueries = video_testQueries\n\tif FLAGS.mode == ""audio"":\n\t\ttrainQueries = audio_trainQueries \n\t\tvalQueries = audio_valQueries \n\t\ttestQueries = audio_testQueries\n\tif FLAGS.mode == ""textvideo"":\n\t\ttrainQueries = np.concatenate((text_trainQueries, video_trainQueries), axis=1)\n\t\tvalQueries = np.concatenate((text_valQueries, video_valQueries), axis=1)\n\t\ttestQueries = np.concatenate((text_testQueries, video_testQueries), axis=1)\n\tif FLAGS.mode == ""audiovideo"":\n\t\ttrainQueries = np.concatenate((audio_trainQueries, video_trainQueries), axis=1)\n\t\tvalQueries = np.concatenate((audio_valQueries, video_valQueries), axis=1)\n\t\ttestQueries = np.concatenate((audio_testQueries, video_testQueries), axis=1)\n\tif FLAGS.mode == ""textaudio"":\n\t\ttrainQueries = np.concatenate((text_trainQueries, audio_trainQueries), axis=1)\n\t\tvalQueries = np.concatenate((text_valQueries, audio_valQueries), axis=1)\n\t\ttestQueries = np.concatenate((text_testQueries, audio_testQueries), axis=1)\n\tif FLAGS.mode == ""all"":\n\t\ttrainQueries = np.concatenate((text_trainQueries, audio_trainQueries, video_trainQueries), axis=1)\n\t\tvalQueries = np.concatenate((text_valQueries, audio_valQueries, video_valQueries), axis=1)\n\t\ttestQueries = np.concatenate((text_testQueries, audio_testQueries, video_testQueries), axis=1)\n\n\t## Pad the histories upto maximum length\n\n\t#Train queries\' histories\n\t#(older to newer)\n\n\ttrainOwnHistory = np.zeros((len(trainID), FLAGS.timesteps, trainQueries.shape[1]), dtype = np.float32)\n\ttrainOtherHistory = np.zeros((len(trainID), FLAGS.timesteps, trainQueries.shape[1]), dtype = np.float32)\n\ttrainOwnHistoryMask = np.zeros((len(trainID), FLAGS.timesteps), dtype = np.float32)\n\ttrainOtherHistoryMask = np.zeros((len(trainID), FLAGS.timesteps), dtype = np.float32)\n\n\tfor iddx, ID in enumerate(trainID):\n\n\t\tcombined_historyID_rank = own_historyID_rank[ID][:] + other_historyID_rank[ID][:]\n\n\t\tif len(combined_historyID_rank) > 0:\n\t\t\n\t\t\tmaxRank = np.max(combined_historyID_rank)\n\t\t\town_history_rank = [maxRank - currRank for currRank in own_historyID_rank[ID]]\n\t\t\tother_history_rank = [maxRank - currRank for currRank in other_historyID_rank[ID]] \n\t\t\t\n\t\t\ttextOwnHistoryEmb = np.asarray(text_own_history_emb[ID])\n\t\t\ttextOtherHistoryEmb = np.asarray(text_other_history_emb[ID])\n\n\t\t\taudioOwnHistoryEmb = np.asarray( [audio_emb[own_historyID[ID][idx]] for idx in range(len(own_historyID[ID]))]  )\n\t\t\taudioOtherHistoryEmb = np.asarray( [audio_emb[other_historyID[ID][idx]] for idx in range(len(other_historyID[ID]))]  )\n\n\t\t\tvideoOwnHistoryEmb = np.asarray( [video_emb[own_historyID[ID][idx]] for idx in range(len(own_historyID[ID]))]  )\n\t\t\tvideoOtherHistoryEmb = np.asarray( [video_emb[other_historyID[ID][idx]] for idx in range(len(other_historyID[ID]))]  )\n\n\n\t\t\tfor idx, rank in enumerate(own_history_rank):\n\t\t\t\tif rank < FLAGS.timesteps:\n\t\t\t\t\tif FLAGS.mode == ""text"":\n\t\t\t\t\t\ttrainOwnHistory[iddx,rank] = textOwnHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""video"":\n\t\t\t\t\t\ttrainOwnHistory[iddx,rank] = videoOwnHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""audio"":\n\t\t\t\t\t\ttrainOwnHistory[iddx,rank] = audioOwnHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""textvideo"":\n\t\t\t\t\t\ttrainOwnHistory[iddx,rank] = np.concatenate((textOwnHistoryEmb[idx], videoOwnHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""audiovideo"":\n\t\t\t\t\t\ttrainOwnHistory[iddx,rank] = np.concatenate((audioOwnHistoryEmb[idx], videoOwnHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""textaudio"":\n\t\t\t\t\t\ttrainOwnHistory[iddx,rank] = np.concatenate((textOwnHistoryEmb[idx], audioOwnHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""all"":\n\t\t\t\t\t\ttrainOwnHistory[iddx,rank] = np.concatenate((textOwnHistoryEmb[idx], audioOwnHistoryEmb[idx], videoOwnHistoryEmb[idx]))\n\n\t\t\t\t\t\t\n\t\t\t\t\ttrainOwnHistoryMask[iddx,rank] = 1.0\n\t\t\ttrainOwnHistory[iddx] = trainOwnHistory[iddx,::-1,:]\n\t\t\ttrainOwnHistoryMask[iddx] = trainOwnHistoryMask[iddx,::-1]\n\n\t\t\tfor idx, rank in enumerate(other_history_rank):\n\t\t\t\tif rank < FLAGS.timesteps:\n\t\t\t\t\tif FLAGS.mode == ""text"":\n\t\t\t\t\t\ttrainOtherHistory[iddx,rank] = textOtherHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""video"":\n\t\t\t\t\t\ttrainOtherHistory[iddx,rank] = videoOtherHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""audio"":\n\t\t\t\t\t\ttrainOtherHistory[iddx,rank] = audioOtherHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""textvideo"":\n\t\t\t\t\t\ttrainOtherHistory[iddx,rank] = np.concatenate((textOtherHistoryEmb[idx], videoOtherHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""audiovideo"":\n\t\t\t\t\t\ttrainOtherHistory[iddx,rank] = np.concatenate((audioOtherHistoryEmb[idx], videoOtherHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""textaudio"":\n\t\t\t\t\t\ttrainOtherHistory[iddx,rank] = np.concatenate((textOtherHistoryEmb[idx], audioOtherHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""all"":\n\t\t\t\t\t\ttrainOtherHistory[iddx,rank] = np.concatenate((textOtherHistoryEmb[idx], audioOtherHistoryEmb[idx], videoOtherHistoryEmb[idx]))\n\n\t\t\t\t\ttrainOtherHistoryMask[iddx,rank] = 1.0\n\t\t\ttrainOtherHistory[iddx] = trainOtherHistory[iddx,::-1,:]\n\t\t\ttrainOtherHistoryMask[iddx] = trainOtherHistoryMask[iddx,::-1]\n\n\n\tvalOwnHistory = np.zeros((len(valID), FLAGS.timesteps, valQueries.shape[1]), dtype = np.float32)\n\tvalOtherHistory = np.zeros((len(valID), FLAGS.timesteps, valQueries.shape[1]), dtype = np.float32)\n\tvalOwnHistoryMask = np.zeros((len(valID), FLAGS.timesteps), dtype = np.float32)\n\tvalOtherHistoryMask = np.zeros((len(valID), FLAGS.timesteps), dtype = np.float32)\n\n\tfor iddx, ID in enumerate(valID):\n\n\t\tcombined_historyID_rank = own_historyID_rank[ID][:] + other_historyID_rank[ID][:]\n\n\t\tif len(combined_historyID_rank) > 0:\n\t\t\n\t\t\tmaxRank = np.max(combined_historyID_rank)\n\t\t\town_history_rank = [maxRank - currRank for currRank in own_historyID_rank[ID]]\n\t\t\tother_history_rank = [maxRank - currRank for currRank in other_historyID_rank[ID]] \n\t\t\t\n\t\t\ttextOwnHistoryEmb = np.asarray(text_own_history_emb[ID])\n\t\t\ttextOtherHistoryEmb = np.asarray(text_other_history_emb[ID])\n\n\t\t\taudioOwnHistoryEmb = np.asarray( [audio_emb[own_historyID[ID][idx]] for idx in range(len(own_historyID[ID]))]  )\n\t\t\taudioOtherHistoryEmb = np.asarray( [audio_emb[other_historyID[ID][idx]] for idx in range(len(other_historyID[ID]))]  )\n\n\t\t\tvideoOwnHistoryEmb = np.asarray( [video_emb[own_historyID[ID][idx]] for idx in range(len(own_historyID[ID]))]  )\n\t\t\tvideoOtherHistoryEmb = np.asarray( [video_emb[other_historyID[ID][idx]] for idx in range(len(other_historyID[ID]))]  )\n\n\n\t\t\tfor idx, rank in enumerate(own_history_rank):\n\t\t\t\tif rank < FLAGS.timesteps:\n\t\t\t\t\tif FLAGS.mode == ""text"":\n\t\t\t\t\t\tvalOwnHistory[iddx,rank] = textOwnHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""video"":\n\t\t\t\t\t\tvalOwnHistory[iddx,rank] = videoOwnHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""audio"":\n\t\t\t\t\t\tvalOwnHistory[iddx,rank] = audioOwnHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""textvideo"":\n\t\t\t\t\t\tvalOwnHistory[iddx,rank] = np.concatenate((textOwnHistoryEmb[idx], videoOwnHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""audiovideo"":\n\t\t\t\t\t\tvalOwnHistory[iddx,rank] = np.concatenate((audioOwnHistoryEmb[idx], videoOwnHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""textaudio"":\n\t\t\t\t\t\tvalOwnHistory[iddx,rank] = np.concatenate((textOwnHistoryEmb[idx], audioOwnHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""all"":\n\t\t\t\t\t\tvalOwnHistory[iddx,rank] = np.concatenate((textOwnHistoryEmb[idx], audioOwnHistoryEmb[idx], videoOwnHistoryEmb[idx]))\n\n\t\t\t\t\t\t\n\t\t\t\t\tvalOwnHistoryMask[iddx,rank] = 1.0\n\t\t\tvalOwnHistory[iddx] = valOwnHistory[iddx,::-1,:]\n\t\t\tvalOwnHistoryMask[iddx] = valOwnHistoryMask[iddx,::-1]\n\n\t\t\tfor idx, rank in enumerate(other_history_rank):\n\t\t\t\tif rank < FLAGS.timesteps:\n\t\t\t\t\tif FLAGS.mode == ""text"":\n\t\t\t\t\t\tvalOtherHistory[iddx,rank] = textOtherHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""video"":\n\t\t\t\t\t\tvalOtherHistory[iddx,rank] = videoOtherHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""audio"":\n\t\t\t\t\t\tvalOtherHistory[iddx,rank] = audioOtherHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""textvideo"":\n\t\t\t\t\t\tvalOtherHistory[iddx,rank] = np.concatenate((textOtherHistoryEmb[idx], videoOtherHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""audiovideo"":\n\t\t\t\t\t\tvalOtherHistory[iddx,rank] = np.concatenate((audioOtherHistoryEmb[idx], videoOtherHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""textaudio"":\n\t\t\t\t\t\tvalOtherHistory[iddx,rank] = np.concatenate((textOtherHistoryEmb[idx], audioOtherHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""all"":\n\t\t\t\t\t\tvalOtherHistory[iddx,rank] = np.concatenate((textOtherHistoryEmb[idx], audioOtherHistoryEmb[idx], videoOtherHistoryEmb[idx]))\n\n\t\t\t\t\tvalOtherHistoryMask[iddx,rank] = 1.0\n\t\t\tvalOtherHistory[iddx] = valOtherHistory[iddx,::-1,:]\n\t\t\tvalOtherHistoryMask[iddx] = valOtherHistoryMask[iddx,::-1]\n\n\n\t#Test queries\' histories\n\ttestOwnHistory = np.zeros((len(testID), FLAGS.timesteps, testQueries.shape[1]), dtype = np.float32)\n\ttestOtherHistory = np.zeros((len(testID), FLAGS.timesteps, testQueries.shape[1]), dtype = np.float32)\n\ttestOwnHistoryMask = np.zeros((len(testID), FLAGS.timesteps), dtype = np.float32)\n\ttestOtherHistoryMask = np.zeros((len(testID), FLAGS.timesteps), dtype = np.float32)\n\n\tfor iddx, ID in enumerate(testID):\n\n\t\tcombined_historyID_rank = own_historyID_rank[ID][:] + other_historyID_rank[ID][:]\n\n\t\tif len(combined_historyID_rank) > 0:\n\t\t\n\t\t\tmaxRank = np.max(combined_historyID_rank)\n\t\t\town_history_rank = [maxRank - currRank for currRank in own_historyID_rank[ID]]\n\t\t\tother_history_rank = [maxRank - currRank for currRank in other_historyID_rank[ID]] \n\t\t\t\n\t\t\ttextOwnHistoryEmb = np.asarray(text_own_history_emb[ID])\n\t\t\ttextOtherHistoryEmb = np.asarray(text_other_history_emb[ID])\n\n\t\t\taudioOwnHistoryEmb = np.asarray( [audio_emb[own_historyID[ID][idx]] for idx in range(len(own_historyID[ID]))]  )\n\t\t\taudioOtherHistoryEmb = np.asarray( [audio_emb[other_historyID[ID][idx]] for idx in range(len(other_historyID[ID]))]  )\n\n\t\t\tvideoOwnHistoryEmb = np.asarray( [video_emb[own_historyID[ID][idx]] for idx in range(len(own_historyID[ID]))]  )\n\t\t\tvideoOtherHistoryEmb = np.asarray( [video_emb[other_historyID[ID][idx]] for idx in range(len(other_historyID[ID]))]  )\n\t\t\t\n\n\t\t\tfor idx, rank in enumerate(own_history_rank):\n\t\t\t\tif rank < FLAGS.timesteps:\n\t\t\t\t\tif FLAGS.mode == ""text"":\n\t\t\t\t\t\ttestOwnHistory[iddx,rank] = textOwnHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""video"":\n\t\t\t\t\t\ttestOwnHistory[iddx,rank] = videoOwnHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""audio"":\n\t\t\t\t\t\ttestOwnHistory[iddx,rank] = audioOwnHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""textvideo"":\n\t\t\t\t\t\ttestOwnHistory[iddx,rank] = np.concatenate((textOwnHistoryEmb[idx], videoOwnHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""audiovideo"":\n\t\t\t\t\t\ttestOwnHistory[iddx,rank] = np.concatenate((audioOwnHistoryEmb[idx], videoOwnHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""textaudio"":\n\t\t\t\t\t\ttestOwnHistory[iddx,rank] = np.concatenate((textOwnHistoryEmb[idx], audioOwnHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""all"":\n\t\t\t\t\t\ttestOwnHistory[iddx,rank] = np.concatenate((textOwnHistoryEmb[idx], audioOwnHistoryEmb[idx], videoOwnHistoryEmb[idx]))\n\n\t\t\t\t\ttestOwnHistoryMask[iddx,rank] = 1.0\n\t\t\ttestOwnHistory[iddx] = testOwnHistory[iddx,::-1,:]\n\t\t\ttestOwnHistoryMask[iddx] = testOwnHistoryMask[iddx,::-1]\n\n\t\t\tfor idx, rank in enumerate(other_history_rank):\n\t\t\t\tif rank < FLAGS.timesteps:\n\t\t\t\t\tif FLAGS.mode == ""text"":\n\t\t\t\t\t\ttestOtherHistory[iddx,rank] = textOtherHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""video"":\n\t\t\t\t\t\ttestOtherHistory[iddx,rank] = videoOtherHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""audio"":\n\t\t\t\t\t\ttestOtherHistory[iddx,rank] = audioOtherHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""textvideo"":\n\t\t\t\t\t\ttestOtherHistory[iddx,rank] = np.concatenate((textOtherHistoryEmb[idx], videoOtherHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""audiovideo"":\n\t\t\t\t\t\ttestOtherHistory[iddx,rank] = np.concatenate((audioOtherHistoryEmb[idx], videoOtherHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""textaudio"":\n\t\t\t\t\t\ttestOtherHistory[iddx,rank] = np.concatenate((textOtherHistoryEmb[idx], audioOtherHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""all"":\n\t\t\t\t\t\ttestOtherHistory[iddx,rank] = np.concatenate((textOtherHistoryEmb[idx], audioOtherHistoryEmb[idx], videoOtherHistoryEmb[idx]))\n\n\t\t\t\t\ttestOtherHistoryMask[iddx,rank] = 1.0\n\t\t\ttestOtherHistory[iddx] = testOtherHistory[iddx,::-1,:]\n\t\t\ttestOtherHistoryMask[iddx] = testOtherHistoryMask[iddx,::-1]\n\n\treturn trainQueries, trainOwnHistory, trainOtherHistory, trainOwnHistoryMask, trainOtherHistoryMask, trainLabels, \\\n\t\t\tvalQueries, valOwnHistory, valOtherHistory, valOwnHistoryMask, valOtherHistoryMask, valLabels, \\\n\t\t\ttestQueries, testOwnHistory, testOtherHistory, testOwnHistoryMask, testOtherHistoryMask, testLabels\n\n\nif __name__ == ""__main__"":\n\tloadData()'"
ICON/IEMOCAP/model.py,0,"b'import tensorflow as tf\nimport numpy as np\nimport sys\nimport tensorflow.contrib.rnn as rnn_cell\nfrom sklearn.model_selection import train_test_split\n\n\n\n\nclass ICON:\n\n    def __init__(self, CONFIG, session = None):\n        \n        self._batch_size = CONFIG.batch_size\n        self._input_dim = CONFIG.input_dims\n        self._timesteps = CONFIG.timesteps\n        self._class_size = CONFIG.class_size\n        self._embedding_size = CONFIG.embedding_size\n        self._hops = CONFIG.hops\n        self._max_grad_norm = CONFIG.max_grad_norm\n        self._nonlin = CONFIG.nonlin\n        self._nonlin_func = CONFIG.nonlin_func\n        self._init = tf.random_normal_initializer(stddev=0.01, seed=1227)\n        self._name = ""ICON""\n\n        ## inputs to receive from the dataset\n        self._build_inputs()\n\n        ## tensor variables of the tensorflow graph\n        self._build_vars()\n\n        \n\n        ## optimizer choices for training\n        # self._opt = tf.train.GradientDescentOptimizer(learning_rate=self._lr)\n        self._opt = tf.train.AdamOptimizer(learning_rate=self._lr)\n\n        ## cross entropy loss\n        logits = self._inference(self._histories_own, self._histories_other, self._queries) # (batch_size, class size)\n        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf.cast(self._labels, tf.float32), name=""cross_entropy"")\n        cross_entropy_sum = tf.reduce_mean(cross_entropy, keepdims=False, name=""cross_entropy_sum"")\n\n        print(\'\\n ---- TRAINABLE VARIABLES ---- \\n\')\n        tvars = tf.trainable_variables()\n        reg_loss=[]\n        for tvar in tvars:\n            print(tvar.name)\n            if ""bias"" not in tvar.name:\n                reg_loss.append(tf.nn.l2_loss(tvar))\n        print(\'----------- \\n\')\n\n        # loss op\n        self.regularization_loss = tf.reduce_mean(reg_loss)\n        self.loss_op = loss_op = tf.reduce_mean(cross_entropy_sum + 0.001*self.regularization_loss)\n\n        # gradient pipeline\n        grads_and_vars = self._opt.compute_gradients(loss_op)\n        grads_and_vars = [(tf.clip_by_norm(g, self._max_grad_norm), v) for g,v in grads_and_vars]\n        grads_and_vars = [(g, v) for g,v in grads_and_vars]\n        self.train_op = train_op = self._opt.apply_gradients(grads_and_vars, name=""train_op"")\n\n        # predict ops\n        self.predict_op = predict_op = tf.argmax(logits, 1, name=""predict_op"")\n\n        self._sess = session\n        self._sess.run(tf.global_variables_initializer())\n\n\n    def _build_inputs(self):\n\n        self._queries = tf.placeholder(tf.float32, [self._batch_size, self._input_dim], name=""queries"") # utterances to be classified\n\n        # Histories of the utterances\n        self._histories_own = tf.placeholder(tf.float32, [self._batch_size, self._timesteps, self._input_dim], name=""histories_own"") \n        self._histories_other = tf.placeholder(tf.float32, [self._batch_size, self._timesteps, self._input_dim], name=""histories_other"")\n        self._histories_own_mask = tf.cast(tf.placeholder(tf.float32, [self._batch_size, self._timesteps], name=""histories_own_mask""), dtype=tf.bool)\n        self._histories_other_mask = tf.cast(tf.placeholder(tf.float32, [self._batch_size, self._timesteps], name=""histories_other_mask""), dtype=tf.bool)\n        self._mask = tf.cast(tf.placeholder(tf.float32, [self._batch_size, self._timesteps], name=""global_mask""), dtype=tf.bool)\n\n        # True Labels\n        self._labels = tf.placeholder(tf.int32, [self._batch_size, self._class_size], name=""labels"")\n        \n\n        # Learning Rate\n        self._lr = tf.placeholder(tf.float32, [], name=""learning_rate"")\n\n        # Dropout Probability\n        self._dropout = tf.placeholder(tf.float32, [], name=""dropout_keep_rate"")\n\n        # Training mode\n        self._training = tf.placeholder(tf.bool, [], name=""training_testing_mode"")\n\n    def _build_vars(self):\n\n        with tf.variable_scope(self._name):\n\n            with tf.variable_scope(""localGRU""):\n                # GRUs for per-person local input modeling\n                self.localGRUOwn = tf.contrib.rnn.GRUCell(num_units=self._embedding_size, reuse = tf.AUTO_REUSE, name=\'localGRUOwn\')\n                self.localGRUOther = tf.contrib.rnn.GRUCell(num_units=self._embedding_size, reuse = tf.AUTO_REUSE, name=\'localGRUOther\')\n\n            with tf.variable_scope(""globalGRU""):\n                self.globalGRU = tf.contrib.rnn.GRUCell(num_units=self._embedding_size, reuse = tf.AUTO_REUSE, name=\'globalGRU\')\n\n            with tf.variable_scope(""memoryGRU""):\n                self.memoryGRU = tf.contrib.rnn.GRUCell(num_units=self._embedding_size, reuse = tf.AUTO_REUSE, name=\'memoryGRU\')\n\n            with tf.variable_scope(""output""):\n\n                # Output Projection Matrix\n                self.outputProj = tf.get_variable(""outProj"", shape=([self._embedding_size, self._class_size]), trainable=True, initializer=self._init)\n                self.outputProjBias = tf.get_variable(""outputProjBias"", shape=([1, self._class_size]), trainable=True, initializer=self._init)\n            \n\n    def _inference(self, histories_own, histories_other, queries):\n\n        with tf.variable_scope(self._name):\n\n            with tf.variable_scope(""input""):\n                \n                q = tf.contrib.layers.fully_connected(\n                    queries,\n                    self._embedding_size,\n                    activation_fn=tf.nn.tanh,\n                    normalizer_fn=None,\n                    normalizer_params=None,\n                    weights_initializer=tf.contrib.layers.xavier_initializer(uniform=True, seed=1227),\n                    weights_regularizer=tf.contrib.layers.l2_regularizer(0.001),\n                    biases_initializer=tf.zeros_initializer(),\n                    trainable=True,\n                    scope=""input""\n                )\n\n            # SIM Module\n            with tf.variable_scope(""localGRU""):\n\n                \n                ## Input GRU Own\n                hidden_vector = self.localGRUOwn.zero_state(self._batch_size, tf.float32)\n                ownRNNOutput=[]\n                for i in range(self._timesteps):\n\n                    localMask = tf.squeeze(self._histories_own_mask[:,tf.constant(i)]) # batch_size\n                    localInput = tf.squeeze(histories_own[:,tf.constant(i),:]) # batch_size * dim\n                    prev_hidden_vector = hidden_vector\n                    hidden_vector,_= self.localGRUOwn(localInput, hidden_vector) # batch_size * dim\n                    # mask hidden vector (mask 0 places should not apply GRU)\n                    hidden_vector = tf.where(localMask, hidden_vector, prev_hidden_vector) # batch_size * dim\n\n                    # masked output_vector\n                    output_vector = tf.where(localMask, hidden_vector, tf.zeros( (self._batch_size,self._embedding_size), dtype=np.float32)) # batch_size * dim\n                    ownRNNOutput.append(output_vector[:,tf.newaxis,:])\n                    \n                ownHistoryRNNOutput = tf.concat(ownRNNOutput, axis=1) # batch_size * timesteps * dim\n                ownHistoryRNNOutput = tf.nn.dropout(ownHistoryRNNOutput, keep_prob = self._dropout, name = ""own_rnn_dropout"")\n\n                ## Input GRU Other\n                hidden_vector = self.localGRUOther.zero_state(self._batch_size, tf.float32)\n                otherRNNOutput=[]\n                for i in range(self._timesteps):\n\n                    localMask = tf.squeeze(self._histories_other_mask[:,tf.constant(i)]) # batch_size\n                    localInput = tf.squeeze(histories_other[:,tf.constant(i),:]) # batch_size * dim\n                    prev_hidden_vector = hidden_vector\n                    hidden_vector,_= self.localGRUOther(localInput, hidden_vector) # batch_size * dim\n                    # mask hidden vector (mask 0 places should not apply GRU)\n                    hidden_vector = tf.where(localMask, hidden_vector, prev_hidden_vector) # batch_size * dim\n\n                    # masked output_vector\n                    output_vector = tf.where(localMask, hidden_vector, tf.zeros( (self._batch_size,self._embedding_size), dtype=np.float32)) # batch_size * dim\n                    otherRNNOutput.append(output_vector[:,tf.newaxis,:])\n                    \n                otherHistoryRNNOutput = tf.concat(otherRNNOutput, axis=1)\n                otherHistoryRNNOutput = tf.nn.dropout(otherHistoryRNNOutput, keep_prob = self._dropout, name = ""other_rnn_dropout"")\n\n\n            # DGIM Module\n            globalGRUInput = ownHistoryRNNOutput + otherHistoryRNNOutput\n            globalGRUInput = tf.nn.tanh(globalGRUInput)\n\n            with tf.variable_scope(""globalGRU""):                 \n\n                for hop in range(self._hops):\n\n                    # Memory Update\n                    if hop == 0:\n                        rnn_input = globalGRUInput\n                        rnn_cell = self.globalGRU\n                    else:\n                        rnn_input = rnn_outputs\n                        rnn_cell = self.memoryGRU\n\n                    rnn_outputs, final_state = tf.nn.dynamic_rnn(rnn_cell, rnn_input, dtype=tf.float32)\n\n                    # looping for masking as tf.where doesnt support 2d masking\n                    rnnOutputs = []\n                    for j in range(self._timesteps):\n                        localMask = tf.squeeze(self._mask[:,tf.constant(j)]) # batch_size\n                        localOutput = tf.squeeze(rnn_outputs[:,tf.constant(j),:]) # batch_size * dim\n                        outputVector = tf.where(localMask, localOutput, tf.zeros((self._batch_size,self._embedding_size), dtype=np.float32))\n                        rnnOutputs.append(outputVector[:,tf.newaxis,:])\n                    rnn_outputs = tf.concat(rnnOutputs, axis=1)\n\n                    \n                    # Attentional Read operation from rnn_output memories\n                    attScore = tf.nn.tanh(tf.squeeze(tf.matmul(q[:,tf.newaxis,:], tf.transpose(rnn_outputs,[0,2,1]))))  # (batch, 1, dim)  X (batch, dim, time) == (batch, 1, time) -> (batch, time)\n                    attScore = tf.where( self._mask, attScore, tf.constant( -10000 , shape= [self._batch_size, self._timesteps], dtype= tf.float32))\n                    softmax_output = attScore = tf.nn.softmax(attScore) # (batch, time)\n                    attScore = tf.nn.dropout(attScore, keep_prob = self._dropout, name=\'ttScore_dropout\')\n                    attScore = tf.where( self._mask, attScore, tf.zeros(tf.shape(attScore), dtype= tf.float32))\n                    weighted = tf.squeeze(tf.matmul(attScore[:,tf.newaxis,:], rnn_outputs)) # (batch, 1, time)  X (batch, time, dim) == (batch, dim)\n                    q = tf.nn.tanh(q + weighted)\n\n\n            with tf.variable_scope(""output""):\n\n                return tf.add(tf.matmul(q, self.outputProj), self.outputProjBias)\n\n\n    def batch_fit(self, histories_own, histories_other, histories_own_mask, histories_other_mask, global_mask, queries, labels, learning_rate, dropout_keep_rate, training_mode=None):\n        \'\'\'\n        Runs the training algorithm over the passed batch\n        Returns:\n            loss: floating-point number, the loss computed for the batch\n        \'\'\'\n        feed_dict = {self._histories_own: histories_own, self._histories_other: histories_other, self._queries: queries, self._labels: labels,\\\n         self._lr: learning_rate, self._histories_own_mask: histories_own_mask, self._histories_other_mask: histories_other_mask, self._dropout: dropout_keep_rate, self._mask: global_mask, self._training:training_mode}\n        loss, _ = self._sess.run([self.loss_op, self.train_op], feed_dict=feed_dict)\n        return loss\n\n        \n    def predict(self, histories_own, histories_other, histories_own_mask, histories_other_mask, global_mask, queries, dropout_keep_rate, labels, training_mode=None):\n        \'\'\'\n        Predicts answers as one-hot encoding.\n        Returns:\n            loss: floating-point number, the loss computed for the batch\n            answers: Tensor (None, class size)\n        \'\'\'\n        feed_dict = {self._histories_own: histories_own, self._histories_other: histories_other, self._queries: queries, self._labels: labels,\\\n        self._histories_own_mask: histories_own_mask, self._histories_other_mask: histories_other_mask, self._dropout: dropout_keep_rate, self._mask: global_mask, self._training:training_mode}\n        return self._sess.run([self.loss_op, self.predict_op], feed_dict=feed_dict)'"
ICON/IEMOCAP/utils.py,0,"b'import numpy as np\nimport pandas as pd\nimport pickle\nfrom sklearn import model_selection, metrics\n\nTEXT_EMBEDDINGS = ""./IEMOCAP/data/text/IEMOCAP_text_embeddings.pickle""\nVIDEO_EMBEDDINGS = ""./IEMOCAP/data/video/IEMOCAP_video_features.pickle""\nAUDIO_EMBEDDINGS = ""./IEMOCAP/data/audio/IEMOCAP_audio_features.pickle""\n\ntrainID = pickle.load(open(""./IEMOCAP/data/trainID.pkl"",\'rb\'), encoding=""latin1"")\ntestID = pickle.load(open(""./IEMOCAP/data/testID.pkl"",\'rb\'), encoding=""latin1"")\nvalID,_ = model_selection.train_test_split(testID, test_size=.4, random_state=1227)\n# valID = testID\n\ntranscripts, labels, own_historyID, other_historyID, own_historyID_rank, other_historyID_rank = pickle.load(open(""./IEMOCAP/data/dataset.pkl"",\'rb\'), encoding=""latin1"")\n\nlabel_idx = {\'hap\':0, \'sad\':1, \'neu\':2, \'ang\':3, \'exc\':4, \'fru\':5}\n\n\ndef oneHot(trainLabels, valLabels, testLabels):\n\t\n\t# Calculate the total number of classes\n\tnumOfClasses = np.max(trainLabels)+1\n\t\n\ttrainLabelOneHot = np.zeros((len(trainLabels),numOfClasses), dtype=np.float32)\n\tvalLabelOneHot = np.zeros((len(valLabels),numOfClasses), dtype=np.float32)\n\ttestLabelOneHot = np.zeros((len(testLabels),numOfClasses), dtype=np.float32)\n\n\tfor idx, label in enumerate(trainLabels):\n\t\ttrainLabelOneHot[idx, int(label)]=1.0\n\tfor idx, label in enumerate(valLabels):\n\t\tvalLabelOneHot[idx, int(label)]=1.0\n\tfor idx, label in enumerate(testLabels):\n\t\ttestLabelOneHot[idx, int(label)]=1.0\n\n\treturn trainLabelOneHot, valLabelOneHot, testLabelOneHot\n\ndef updateDictText(text_transcripts_emb, text_own_history_emb, text_other_history_emb, text_emb):\n\n\tfor ID, value in text_transcripts_emb.items():\n\t\tif ID in text_emb.keys():\n\t\t\ttext_transcripts_emb[ID] = text_emb[ID]\n\t# updating the context faeturs\n\tfor ID, value in text_own_history_emb.items():\n\t\tids = own_historyID[ID]\n\t\tfor idx, iD in enumerate(ids):\n\t\t\tif iD in text_emb.keys():\n\t\t\t\ttext_own_history_emb[ID][idx]= text_emb[iD]\n\n\t# updating the context faeturs\n\tfor ID, value in text_other_history_emb.items():\n\t\tids = other_historyID[ID]\n\t\tfor idx, iD in enumerate(ids):\n\t\t\tif iD in text_emb.keys():\n\t\t\t\ttext_other_history_emb[ID][idx]= text_emb[iD]\n\n\treturn text_transcripts_emb, text_own_history_emb, text_other_history_emb\n\n\ndef loadData(FLAGS):\n\n\t## Load Labels\n\ttrainLabels = np.asarray([label_idx[labels[ID]] for ID in trainID])\n\tvalLabels = np.asarray([label_idx[labels[ID]] for ID in valID])\n\ttestLabels = np.asarray([label_idx[labels[ID]] for ID in testID])\n\ttrainLabels, valLabels, testLabels = oneHot(trainLabels, valLabels, testLabels)\n\n\t## Loading Text features\n\ttext_transcripts_emb, text_own_history_emb, text_other_history_emb = pickle.load( open(TEXT_EMBEDDINGS, \'rb\'), encoding=""latin1"")\n\tif FLAGS.context:\n\t\tprint(""loading contextual features"")\n\t\ttext_emb = pickle.load(open(""./IEMOCAP/data/text/IEMOCAP_text_context.pickle"", \'rb\'), encoding=""latin1"")\n\t\ttext_transcripts_emb, text_own_history_emb, text_other_history_emb = updateDictText(text_transcripts_emb, text_own_history_emb, text_other_history_emb, text_emb)\n\n\t## Loading Audio features\n\taudio_emb = pickle.load(open(AUDIO_EMBEDDINGS, \'rb\'), encoding=""latin1"")\n\tif FLAGS.context:\n\t\taudio_emb_context = pickle.load(open(""./IEMOCAP/data/audio/IEMOCAP_audio_context.pickle"", \'rb\'), encoding=""latin1"")\n\t\tfor ID in audio_emb.keys():\n\t\t\tif ID in audio_emb_context.keys():\n\t\t\t\taudio_emb[ID] = audio_emb_context[ID]\n\t\n\t## Loading Video features \n\tvideo_emb = pickle.load(open(VIDEO_EMBEDDINGS, \'rb\'), encoding=""latin1"")\n\t# video_emb_context = pickle.load(open(""./IEMOCAP/data/video/IEMOCAP_video_context.pickle"", \'rb\'), encoding=""latin1"")\n\t# for ID in video_emb.keys():\n\t# \tif ID in video_emb_context.keys():\n\t# \t\tvideo_emb[ID] = video_emb_context[ID]\n\t\n\t## Text Embeddings for the queries\n\ttext_trainQueries = np.asarray([text_transcripts_emb[ID] for ID in trainID])\n\ttext_valQueries = np.asarray([text_transcripts_emb[ID] for ID in valID])\n\ttext_testQueries = np.asarray([text_transcripts_emb[ID] for ID in testID])\n\n\t## Audio Embeddings for the queries\n\taudio_trainQueries = np.asarray([audio_emb[ID] for ID in trainID])\n\taudio_valQueries = np.asarray([audio_emb[ID] for ID in valID])\n\taudio_testQueries = np.asarray([audio_emb[ID] for ID in testID])\n\n\t## Video Embeddings for the queries\n\tvideo_trainQueries = np.asarray([video_emb[ID] for ID in trainID])\n\tvideo_valQueries = np.asarray([video_emb[ID] for ID in valID])\n\tvideo_testQueries = np.asarray([video_emb[ID] for ID in testID])\n\n\n\n\t\n\tif FLAGS.mode == ""text"":\n\t\ttrainQueries = text_trainQueries\n\t\tvalQueries = text_valQueries\n\t\ttestQueries = text_testQueries\n\tif FLAGS.mode == ""video"":\n\t\ttrainQueries = video_trainQueries \n\t\tvalQueries = video_valQueries\n\t\ttestQueries = video_testQueries\n\tif FLAGS.mode == ""audio"":\n\t\ttrainQueries = audio_trainQueries \n\t\tvalQueries = audio_valQueries \n\t\ttestQueries = audio_testQueries\n\tif FLAGS.mode == ""textvideo"":\n\t\ttrainQueries = np.concatenate((text_trainQueries, video_trainQueries), axis=1)\n\t\tvalQueries = np.concatenate((text_valQueries, video_valQueries), axis=1)\n\t\ttestQueries = np.concatenate((text_testQueries, video_testQueries), axis=1)\n\tif FLAGS.mode == ""audiovideo"":\n\t\ttrainQueries = np.concatenate((audio_trainQueries, video_trainQueries), axis=1)\n\t\tvalQueries = np.concatenate((audio_valQueries, video_valQueries), axis=1)\n\t\ttestQueries = np.concatenate((audio_testQueries, video_testQueries), axis=1)\n\tif FLAGS.mode == ""textaudio"":\n\t\ttrainQueries = np.concatenate((text_trainQueries, audio_trainQueries), axis=1)\n\t\tvalQueries = np.concatenate((text_valQueries, audio_valQueries), axis=1)\n\t\ttestQueries = np.concatenate((text_testQueries, audio_testQueries), axis=1)\n\tif FLAGS.mode == ""all"":\n\t\ttrainQueries = np.concatenate((text_trainQueries, audio_trainQueries, video_trainQueries), axis=1)\n\t\tvalQueries = np.concatenate((text_valQueries, audio_valQueries, video_valQueries), axis=1)\n\t\ttestQueries = np.concatenate((text_testQueries, audio_testQueries, video_testQueries), axis=1)\n\n\t## Pad the histories upto maximum length\n\n\t#Train queries\' histories\n\t#(older to newer)\n\n\ttrainOwnHistory = np.zeros((len(trainID), FLAGS.timesteps, trainQueries.shape[1]), dtype = np.float32)\n\ttrainOtherHistory = np.zeros((len(trainID), FLAGS.timesteps, trainQueries.shape[1]), dtype = np.float32)\n\ttrainOwnHistoryMask = np.zeros((len(trainID), FLAGS.timesteps), dtype = np.float32)\n\ttrainOtherHistoryMask = np.zeros((len(trainID), FLAGS.timesteps), dtype = np.float32)\n\n\tfor iddx, ID in enumerate(trainID):\n\n\t\tcombined_historyID_rank = own_historyID_rank[ID][:] + other_historyID_rank[ID][:]\n\n\t\tif len(combined_historyID_rank) > 0:\n\t\t\n\t\t\tmaxRank = np.max(combined_historyID_rank)\n\t\t\town_history_rank = [maxRank - currRank for currRank in own_historyID_rank[ID]]\n\t\t\tother_history_rank = [maxRank - currRank for currRank in other_historyID_rank[ID]] \n\t\t\t\n\t\t\ttextOwnHistoryEmb = np.asarray(text_own_history_emb[ID])\n\t\t\ttextOtherHistoryEmb = np.asarray(text_other_history_emb[ID])\n\n\t\t\taudioOwnHistoryEmb = np.asarray( [audio_emb[own_historyID[ID][idx]] for idx in range(len(own_historyID[ID]))]  )\n\t\t\taudioOtherHistoryEmb = np.asarray( [audio_emb[other_historyID[ID][idx]] for idx in range(len(other_historyID[ID]))]  )\n\n\t\t\tvideoOwnHistoryEmb = np.asarray( [video_emb[own_historyID[ID][idx]] for idx in range(len(own_historyID[ID]))]  )\n\t\t\tvideoOtherHistoryEmb = np.asarray( [video_emb[other_historyID[ID][idx]] for idx in range(len(other_historyID[ID]))]  )\n\n\n\t\t\tfor idx, rank in enumerate(own_history_rank):\n\t\t\t\tif rank < FLAGS.timesteps:\n\t\t\t\t\tif FLAGS.mode == ""text"":\n\t\t\t\t\t\ttrainOwnHistory[iddx,rank] = textOwnHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""video"":\n\t\t\t\t\t\ttrainOwnHistory[iddx,rank] = videoOwnHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""audio"":\n\t\t\t\t\t\ttrainOwnHistory[iddx,rank] = audioOwnHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""textvideo"":\n\t\t\t\t\t\ttrainOwnHistory[iddx,rank] = np.concatenate((textOwnHistoryEmb[idx], videoOwnHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""audiovideo"":\n\t\t\t\t\t\ttrainOwnHistory[iddx,rank] = np.concatenate((audioOwnHistoryEmb[idx], videoOwnHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""textaudio"":\n\t\t\t\t\t\ttrainOwnHistory[iddx,rank] = np.concatenate((textOwnHistoryEmb[idx], audioOwnHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""all"":\n\t\t\t\t\t\ttrainOwnHistory[iddx,rank] = np.concatenate((textOwnHistoryEmb[idx], audioOwnHistoryEmb[idx], videoOwnHistoryEmb[idx]))\n\n\t\t\t\t\t\t\n\t\t\t\t\ttrainOwnHistoryMask[iddx,rank] = 1.0\n\t\t\ttrainOwnHistory[iddx] = trainOwnHistory[iddx,::-1,:]\n\t\t\ttrainOwnHistoryMask[iddx] = trainOwnHistoryMask[iddx,::-1]\n\n\t\t\tfor idx, rank in enumerate(other_history_rank):\n\t\t\t\tif rank < FLAGS.timesteps:\n\t\t\t\t\tif FLAGS.mode == ""text"":\n\t\t\t\t\t\ttrainOtherHistory[iddx,rank] = textOtherHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""video"":\n\t\t\t\t\t\ttrainOtherHistory[iddx,rank] = videoOtherHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""audio"":\n\t\t\t\t\t\ttrainOtherHistory[iddx,rank] = audioOtherHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""textvideo"":\n\t\t\t\t\t\ttrainOtherHistory[iddx,rank] = np.concatenate((textOtherHistoryEmb[idx], videoOtherHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""audiovideo"":\n\t\t\t\t\t\ttrainOtherHistory[iddx,rank] = np.concatenate((audioOtherHistoryEmb[idx], videoOtherHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""textaudio"":\n\t\t\t\t\t\ttrainOtherHistory[iddx,rank] = np.concatenate((textOtherHistoryEmb[idx], audioOtherHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""all"":\n\t\t\t\t\t\ttrainOtherHistory[iddx,rank] = np.concatenate((textOtherHistoryEmb[idx], audioOtherHistoryEmb[idx], videoOtherHistoryEmb[idx]))\n\n\t\t\t\t\ttrainOtherHistoryMask[iddx,rank] = 1.0\n\t\t\ttrainOtherHistory[iddx] = trainOtherHistory[iddx,::-1,:]\n\t\t\ttrainOtherHistoryMask[iddx] = trainOtherHistoryMask[iddx,::-1]\n\n\n\tvalOwnHistory = np.zeros((len(valID), FLAGS.timesteps, valQueries.shape[1]), dtype = np.float32)\n\tvalOtherHistory = np.zeros((len(valID), FLAGS.timesteps, valQueries.shape[1]), dtype = np.float32)\n\tvalOwnHistoryMask = np.zeros((len(valID), FLAGS.timesteps), dtype = np.float32)\n\tvalOtherHistoryMask = np.zeros((len(valID), FLAGS.timesteps), dtype = np.float32)\n\n\tfor iddx, ID in enumerate(valID):\n\n\t\tcombined_historyID_rank = own_historyID_rank[ID][:] + other_historyID_rank[ID][:]\n\n\t\tif len(combined_historyID_rank) > 0:\n\t\t\n\t\t\tmaxRank = np.max(combined_historyID_rank)\n\t\t\town_history_rank = [maxRank - currRank for currRank in own_historyID_rank[ID]]\n\t\t\tother_history_rank = [maxRank - currRank for currRank in other_historyID_rank[ID]] \n\t\t\t\n\t\t\ttextOwnHistoryEmb = np.asarray(text_own_history_emb[ID])\n\t\t\ttextOtherHistoryEmb = np.asarray(text_other_history_emb[ID])\n\n\t\t\taudioOwnHistoryEmb = np.asarray( [audio_emb[own_historyID[ID][idx]] for idx in range(len(own_historyID[ID]))]  )\n\t\t\taudioOtherHistoryEmb = np.asarray( [audio_emb[other_historyID[ID][idx]] for idx in range(len(other_historyID[ID]))]  )\n\n\t\t\tvideoOwnHistoryEmb = np.asarray( [video_emb[own_historyID[ID][idx]] for idx in range(len(own_historyID[ID]))]  )\n\t\t\tvideoOtherHistoryEmb = np.asarray( [video_emb[other_historyID[ID][idx]] for idx in range(len(other_historyID[ID]))]  )\n\n\n\t\t\tfor idx, rank in enumerate(own_history_rank):\n\t\t\t\tif rank < FLAGS.timesteps:\n\t\t\t\t\tif FLAGS.mode == ""text"":\n\t\t\t\t\t\tvalOwnHistory[iddx,rank] = textOwnHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""video"":\n\t\t\t\t\t\tvalOwnHistory[iddx,rank] = videoOwnHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""audio"":\n\t\t\t\t\t\tvalOwnHistory[iddx,rank] = audioOwnHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""textvideo"":\n\t\t\t\t\t\tvalOwnHistory[iddx,rank] = np.concatenate((textOwnHistoryEmb[idx], videoOwnHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""audiovideo"":\n\t\t\t\t\t\tvalOwnHistory[iddx,rank] = np.concatenate((audioOwnHistoryEmb[idx], videoOwnHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""textaudio"":\n\t\t\t\t\t\tvalOwnHistory[iddx,rank] = np.concatenate((textOwnHistoryEmb[idx], audioOwnHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""all"":\n\t\t\t\t\t\tvalOwnHistory[iddx,rank] = np.concatenate((textOwnHistoryEmb[idx], audioOwnHistoryEmb[idx], videoOwnHistoryEmb[idx]))\n\n\t\t\t\t\t\t\n\t\t\t\t\tvalOwnHistoryMask[iddx,rank] = 1.0\n\t\t\tvalOwnHistory[iddx] = valOwnHistory[iddx,::-1,:]\n\t\t\tvalOwnHistoryMask[iddx] = valOwnHistoryMask[iddx,::-1]\n\n\t\t\tfor idx, rank in enumerate(other_history_rank):\n\t\t\t\tif rank < FLAGS.timesteps:\n\t\t\t\t\tif FLAGS.mode == ""text"":\n\t\t\t\t\t\tvalOtherHistory[iddx,rank] = textOtherHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""video"":\n\t\t\t\t\t\tvalOtherHistory[iddx,rank] = videoOtherHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""audio"":\n\t\t\t\t\t\tvalOtherHistory[iddx,rank] = audioOtherHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""textvideo"":\n\t\t\t\t\t\tvalOtherHistory[iddx,rank] = np.concatenate((textOtherHistoryEmb[idx], videoOtherHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""audiovideo"":\n\t\t\t\t\t\tvalOtherHistory[iddx,rank] = np.concatenate((audioOtherHistoryEmb[idx], videoOtherHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""textaudio"":\n\t\t\t\t\t\tvalOtherHistory[iddx,rank] = np.concatenate((textOtherHistoryEmb[idx], audioOtherHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""all"":\n\t\t\t\t\t\tvalOtherHistory[iddx,rank] = np.concatenate((textOtherHistoryEmb[idx], audioOtherHistoryEmb[idx], videoOtherHistoryEmb[idx]))\n\n\t\t\t\t\tvalOtherHistoryMask[iddx,rank] = 1.0\n\t\t\tvalOtherHistory[iddx] = valOtherHistory[iddx,::-1,:]\n\t\t\tvalOtherHistoryMask[iddx] = valOtherHistoryMask[iddx,::-1]\n\n\n\t#Test queries\' histories\n\ttestOwnHistory = np.zeros((len(testID), FLAGS.timesteps, testQueries.shape[1]), dtype = np.float32)\n\ttestOtherHistory = np.zeros((len(testID), FLAGS.timesteps, testQueries.shape[1]), dtype = np.float32)\n\ttestOwnHistoryMask = np.zeros((len(testID), FLAGS.timesteps), dtype = np.float32)\n\ttestOtherHistoryMask = np.zeros((len(testID), FLAGS.timesteps), dtype = np.float32)\n\n\tfor iddx, ID in enumerate(testID):\n\n\t\tcombined_historyID_rank = own_historyID_rank[ID][:] + other_historyID_rank[ID][:]\n\n\t\tif len(combined_historyID_rank) > 0:\n\t\t\n\t\t\tmaxRank = np.max(combined_historyID_rank)\n\t\t\town_history_rank = [maxRank - currRank for currRank in own_historyID_rank[ID]]\n\t\t\tother_history_rank = [maxRank - currRank for currRank in other_historyID_rank[ID]] \n\t\t\t\n\t\t\ttextOwnHistoryEmb = np.asarray(text_own_history_emb[ID])\n\t\t\ttextOtherHistoryEmb = np.asarray(text_other_history_emb[ID])\n\n\t\t\taudioOwnHistoryEmb = np.asarray( [audio_emb[own_historyID[ID][idx]] for idx in range(len(own_historyID[ID]))]  )\n\t\t\taudioOtherHistoryEmb = np.asarray( [audio_emb[other_historyID[ID][idx]] for idx in range(len(other_historyID[ID]))]  )\n\n\t\t\tvideoOwnHistoryEmb = np.asarray( [video_emb[own_historyID[ID][idx]] for idx in range(len(own_historyID[ID]))]  )\n\t\t\tvideoOtherHistoryEmb = np.asarray( [video_emb[other_historyID[ID][idx]] for idx in range(len(other_historyID[ID]))]  )\n\t\t\t\n\n\t\t\tfor idx, rank in enumerate(own_history_rank):\n\t\t\t\tif rank < FLAGS.timesteps:\n\t\t\t\t\tif FLAGS.mode == ""text"":\n\t\t\t\t\t\ttestOwnHistory[iddx,rank] = textOwnHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""video"":\n\t\t\t\t\t\ttestOwnHistory[iddx,rank] = videoOwnHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""audio"":\n\t\t\t\t\t\ttestOwnHistory[iddx,rank] = audioOwnHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""textvideo"":\n\t\t\t\t\t\ttestOwnHistory[iddx,rank] = np.concatenate((textOwnHistoryEmb[idx], videoOwnHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""audiovideo"":\n\t\t\t\t\t\ttestOwnHistory[iddx,rank] = np.concatenate((audioOwnHistoryEmb[idx], videoOwnHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""textaudio"":\n\t\t\t\t\t\ttestOwnHistory[iddx,rank] = np.concatenate((textOwnHistoryEmb[idx], audioOwnHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""all"":\n\t\t\t\t\t\ttestOwnHistory[iddx,rank] = np.concatenate((textOwnHistoryEmb[idx], audioOwnHistoryEmb[idx], videoOwnHistoryEmb[idx]))\n\n\t\t\t\t\ttestOwnHistoryMask[iddx,rank] = 1.0\n\t\t\ttestOwnHistory[iddx] = testOwnHistory[iddx,::-1,:]\n\t\t\ttestOwnHistoryMask[iddx] = testOwnHistoryMask[iddx,::-1]\n\n\t\t\tfor idx, rank in enumerate(other_history_rank):\n\t\t\t\tif rank < FLAGS.timesteps:\n\t\t\t\t\tif FLAGS.mode == ""text"":\n\t\t\t\t\t\ttestOtherHistory[iddx,rank] = textOtherHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""video"":\n\t\t\t\t\t\ttestOtherHistory[iddx,rank] = videoOtherHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""audio"":\n\t\t\t\t\t\ttestOtherHistory[iddx,rank] = audioOtherHistoryEmb[idx]\n\t\t\t\t\telif FLAGS.mode == ""textvideo"":\n\t\t\t\t\t\ttestOtherHistory[iddx,rank] = np.concatenate((textOtherHistoryEmb[idx], videoOtherHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""audiovideo"":\n\t\t\t\t\t\ttestOtherHistory[iddx,rank] = np.concatenate((audioOtherHistoryEmb[idx], videoOtherHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""textaudio"":\n\t\t\t\t\t\ttestOtherHistory[iddx,rank] = np.concatenate((textOtherHistoryEmb[idx], audioOtherHistoryEmb[idx]))\n\t\t\t\t\telif FLAGS.mode == ""all"":\n\t\t\t\t\t\ttestOtherHistory[iddx,rank] = np.concatenate((textOtherHistoryEmb[idx], audioOtherHistoryEmb[idx], videoOtherHistoryEmb[idx]))\n\n\t\t\t\t\ttestOtherHistoryMask[iddx,rank] = 1.0\n\t\t\ttestOtherHistory[iddx] = testOtherHistory[iddx,::-1,:]\n\t\t\ttestOtherHistoryMask[iddx] = testOtherHistoryMask[iddx,::-1]\n\n\treturn trainQueries, trainOwnHistory, trainOtherHistory, trainOwnHistoryMask, trainOtherHistoryMask, trainLabels, \\\n\t\t\tvalQueries, valOwnHistory, valOtherHistory, valOwnHistoryMask, valOtherHistoryMask, valLabels, \\\n\t\t\ttestQueries, testOwnHistory, testOtherHistory, testOwnHistoryMask, testOtherHistoryMask, testLabels\n\n\nif __name__ == ""__main__"":\n\tloadData()'"
TL-ERC/bert_model/configs.py,1,"b'import os\nimport argparse\nfrom datetime import datetime\nfrom collections import defaultdict\nfrom pathlib import Path\nimport pprint\nfrom torch import optim\nimport torch.nn as nn\nfrom layer.rnncells import StackedLSTMCell, StackedGRUCell\n\nproject_dir = Path(__file__).resolve().parent.parent\ndata_dir = project_dir.joinpath(\'datasets\')\ndata_dict = {\'iemocap\': data_dir.joinpath(\'iemocap\') , \'dailydialog\': data_dir.joinpath(\'dailydialog\')}\noptimizer_dict = {\'RMSprop\': optim.RMSprop, \'Adam\': optim.Adam}\nrnn_dict = {\'lstm\': nn.LSTM, \'gru\': nn.GRU}\nrnncell_dict = {\'lstm\': StackedLSTMCell, \'gru\': StackedGRUCell}\nusername = Path.home().name\n\ndef str2bool(v):\n    """"""string to boolean""""""\n    if v.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n        return True\n    elif v.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\'Boolean value expected.\')\n\n\nclass Config(object):\n    def __init__(self, **kwargs):\n        """"""Configuration Class: set kwargs as class attributes with setattr""""""\n        if kwargs is not None:\n            for key, value in kwargs.items():\n                if key == \'optimizer\':\n                    value = optimizer_dict[value]\n                if key == \'rnn\':\n                    value = rnn_dict[value]\n                if key == \'rnncell\':\n                    value = rnncell_dict[value]\n                setattr(self, key, value)\n\n        # Dataset directory: ex) ./datasets/cornell/\n        self.dataset_dir = data_dict[self.data.lower()]\n\n        # Data Split ex) \'train\', \'valid\', \'test\'\n        self.data_dir = self.dataset_dir.joinpath(self.mode)\n        # Pickled Vocabulary\n        self.word2id_path = self.dataset_dir.joinpath(\'word2id.pkl\')\n        self.id2word_path = self.dataset_dir.joinpath(\'id2word.pkl\')\n        self.word_emb_path = self.dataset_dir.joinpath(\'word_emb.pkl\')\n\n        # Pickled Dataframes\n        self.sentences_path = self.data_dir.joinpath(\'sentences.pkl\')\n        self.label_path = self.data_dir.joinpath(\'labels.pkl\')\n        self.sentence_length_path = self.data_dir.joinpath(\'sentence_length.pkl\')\n        self.conversation_length_path = self.data_dir.joinpath(\'conversation_length.pkl\')\n\n    def __str__(self):\n        """"""Pretty-print configurations in alphabetical order""""""\n        config_str = \'Configurations\\n\'\n        config_str += pprint.pformat(self.__dict__)\n        return config_str\n\n\ndef get_config(parse=True, **optional_kwargs):\n\n    """"""\n    Get configurations as attributes of class\n    1. Parse configurations with argparse.\n    2. Create Config class initilized with parsed kwargs.\n    3. Return Config class.\n    """"""\n    parser = argparse.ArgumentParser()\n\n    # Mode\n    parser.add_argument(\'--mode\', type=str, default=\'train\')\n    parser.add_argument(\'--runs\', type=int, default=5)\n\n    # Train\n    parser.add_argument(\'--num_classes\', type=int, default=0) \n    parser.add_argument(\'--batch_size\', type=int, default=2)\n    parser.add_argument(\'--eval_batch_size\', type=int, default=2)\n    parser.add_argument(\'--n_epoch\', type=int, default=500)\n    parser.add_argument(\'--patience\', type=int, default=10)\n    parser.add_argument(\'--minimum_improvement\', type=int, default=0.001)\n    parser.add_argument(\'--learning_rate\', type=float, default=1e-4)\n    parser.add_argument(\'--optimizer\', type=str, default=\'Adam\')\n    parser.add_argument(\'--clip\', type=float, default=1.0)\n    parser.add_argument(\'--checkpoint\', type=str, default=None)\n    parser.add_argument(\'--load_checkpoint\', type=str, default=None)\n    parser.add_argument(\'--num_bert_layers\', type=int, default=4)\n    parser.add_argument(\'--training_percentage\', type=float, default=1.0)\n\n    # Currently does not support lstm\n    parser.add_argument(\'--rnn\', type=str, default=\'gru\')\n    parser.add_argument(\'--rnncell\', type=str, default=\'gru\')\n    parser.add_argument(\'--num_layers\', type=int, default=1)\n    parser.add_argument(\'--embedding_size\', type=int, default=300)\n    parser.add_argument(\'--encoder_hidden_size\', type=int, default=768)\n    parser.add_argument(\'--bidirectional\', type=str2bool, default=True)\n    parser.add_argument(\'--train_emb\', type=str2bool, default=True)\n    parser.add_argument(\'--dropout\', type=float, default=0.0)\n    parser.add_argument(\'--context_size\', type=int, default=256)\n    parser.add_argument(\'--feedforward\', type=str, default=\'FeedForward\')\n    parser.add_argument(\'--activation\', type=str, default=\'Tanh\')\n\n    # Model\n    parser.add_argument(\'--model\', type=str, default=\'bc_RNN\')\n\n\n    # Utility\n    parser.add_argument(\'--print_every\', type=int, default=100)\n    parser.add_argument(\'--plot_every_epoch\', type=int, default=1)\n    parser.add_argument(\'--save_every_epoch\', type=int, default=1)\n\n    # Data\n    parser.add_argument(\'--data\', type=str, default=\'iemocap\')\n\n    # Parse arguments\n    if parse:\n        kwargs = parser.parse_args()\n    else:\n        kwargs = parser.parse_known_args()[0]\n\n    print(kwargs.data)\n    if kwargs.data == ""iemocap"":\n        kwargs.num_classes = 6\n    elif kwargs.data == ""dailydialog"":\n        kwargs.num_classes = 7\n    else:\n        print(""No dataset mentioned"")\n        exit()\n\n    # Namespace => Dictionary\n    kwargs = vars(kwargs)\n    kwargs.update(optional_kwargs)\n\n    return Config(**kwargs)\n'"
TL-ERC/bert_model/data_loader.py,1,"b'import random\nfrom collections import defaultdict\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\nfrom pytorch_pretrained_bert.tokenization import BertTokenizer\n\nSEQ_LEN = 30\n\nclass DialogDataset(Dataset):\n    def __init__(self, conversations, labels, conversation_length, sentence_length, data=None):\n\n        # [total_data_size, max_conversation_length, max_sentence_length]\n        # tokenized raw text of sentences\n        self.conversations = conversations\n        self.labels = labels\n\n        # conversation length of each batch\n        # [total_data_size]\n        self.conversation_length = conversation_length\n\n        # list of length of sentences\n        # [total_data_size, max_conversation_length]\n        self.sentence_length = sentence_length\n        self.data = data\n        self.len = len(conversations)\n\n        # Prepare for BERT\n        self.tokenizer = BertTokenizer.from_pretrained(""bert-base-uncased"", do_lower_case=True)\n        self.prepare_BERT()\n\n    def prepare_BERT(self,):\n\n        new_conversations=[]\n        new_sentence_lengths=[]\n        new_type_id = []\n        new_masks = []\n\n        for idx in range(len(self.conversations)):\n            conversation = self.conversations[idx]\n            sentence_lengths = self.sentence_length[idx]\n            assert(len(conversation)==len(sentence_lengths))\n\n            local_sentences, local_sentence_length, local_type_id, local_masks = [],[],[],[] \n            for sentence, length in zip(conversation, sentence_lengths):\n                line = "" "".join(sentence[:sentence.index(""<eos>"")])\n                tokens_a = self.tokenizer.tokenize(line)\n                \n                tokens = []\n                input_type_ids = []\n                tokens.append(""[CLS]"")\n                input_type_ids.append(0)\n                for token in tokens_a:\n                    tokens.append(token)\n                    input_type_ids.append(0)\n                tokens.append(""[SEP]"")\n                input_type_ids.append(0)\n\n                input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n\n                # The mask has 1 for real tokens and 0 for padding tokens. Only real\n                # tokens are attended to.\n                input_mask = [1] * len(input_ids)\n\n                # Zero-pad up to the sequence length.\n                while len(input_ids) < SEQ_LEN:\n                    input_ids.append(0)\n                    input_mask.append(0)\n                    input_type_ids.append(0)\n\n                input_ids = input_ids[:SEQ_LEN]\n                input_mask = input_mask[:SEQ_LEN]\n                input_type_ids = input_type_ids[:SEQ_LEN]\n\n                assert len(input_ids) == SEQ_LEN\n                assert len(input_mask) == SEQ_LEN\n                assert len(input_type_ids) == SEQ_LEN\n\n\n                local_sentences.append(input_ids)\n                local_sentence_length.append(len(input_ids))\n                local_type_id.append(input_type_ids)\n                local_masks.append(input_mask)\n            new_conversations.append(local_sentences.copy())\n            new_sentence_lengths.append(local_sentence_length.copy())\n            new_type_id.append(local_type_id.copy())\n            new_masks.append(local_masks.copy())\n\n        self.conversations = new_conversations\n        self.sentence_length = new_sentence_lengths\n        self.type_ids = new_type_id\n        self.masks = new_masks\n\n\n    def __getitem__(self, index):\n        """"""Return Single data sentence""""""\n        # [max_conversation_length, max_sentence_length]\n        conversation = self.conversations[index]\n        labels = self.labels[index]\n        conversation_length = self.conversation_length[index]\n        sentence_length = self.sentence_length[index]\n        type_id = self.type_ids[index]\n        masks = self.masks[index]\n\n\n        return conversation, labels, conversation_length, sentence_length, type_id, masks\n\n    def __len__(self):\n        return self.len\n\n\n\n\ndef get_loader(sentences, labels, conversation_length, sentence_length, batch_size=100, data=None, shuffle=True):\n    """"""Load DataLoader of given DialogDataset""""""\n\n\n    dataset = DialogDataset(sentences, labels, conversation_length,\n                            sentence_length, data=data)\n\n    for sentence, label in zip(sentences, labels):\n        assert(np.array(sentence).shape[0] == np.array(label).shape[0])\n\n\n\n\n\n    def collate_fn(data):\n        """"""\n        Collate list of data in to batch\n\n        Args:\n            data: list of tuple(source, target, conversation_length, source_length, target_length)\n        Return:\n            Batch of each feature\n            - source (LongTensor): [batch_size, max_conversation_length, max_source_length]\n            - target (LongTensor): [batch_size, max_conversation_length, max_source_length]\n            - conversation_length (np.array): [batch_size]\n            - source_length (LongTensor): [batch_size, max_conversation_length]\n        """"""\n        # Sort by conversation length (descending order) to use \'pack_padded_sequence\'\n        data.sort(key=lambda x: x[2], reverse=True)\n\n        # Separate\n        sentences, labels, conversation_length, sentence_length, type_id, mask = zip(*data)\n\n        # return sentences, conversation_length, sentence_length.tolist()\n        return sentences, labels, conversation_length, sentence_length, type_id, mask\n\n\n    data_loader = DataLoader(\n        dataset=dataset,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        collate_fn=collate_fn)\n\n    return data_loader\n'"
TL-ERC/bert_model/models.py,6,"b'import torch\nimport torch.nn as nn\nfrom util import to_var, pad, normal_kl_div, normal_logpdf, bag_of_words_loss, to_bow, EOS_ID\nimport layer\nimport numpy as np\nimport random\n\nfrom pytorch_pretrained_bert.modeling import BertModel\n\n\n# nn.Sequential(*list(BertModel.from_pretrained(""bert-base-uncased"").modules())[:-10])\n\nclass bc_RNN(nn.Module):\n    def __init__(self, config):\n        super(bc_RNN, self).__init__()\n\n        self.config = config\n        self.encoder = BertModel.from_pretrained(""bert-base-uncased"")\n\n        context_input_size = (config.num_layers\n                              * config.encoder_hidden_size)\n\n        self.context_encoder = layer.ContextRNN(context_input_size,\n                                                 config.context_size,\n                                                 config.rnn,\n                                                 config.num_layers,\n                                                 config.dropout)\n\n        self.context2decoder = layer.FeedForward(config.context_size,\n                                                  config.num_layers * config.context_size,\n                                                  num_layers=1,\n                                                  activation=config.activation,\n                                                  isActivation=True)\n        \n        self.decoder2output = layer.FeedForward(config.num_layers * config.context_size,\n                                                 config.num_classes,\n                                                 num_layers=1,\n                                                 isActivation=False)\n        self.dropoutLayer = nn.Dropout(p=config.dropout)\n\n    def forward(self, input_sentences, input_sentence_length, input_conversation_length, input_masks):\n        """"""\n        Args:\n            input_sentences: (Variable, LongTensor) [num_sentences, seq_len]\n            target_sentences: (Variable, LongTensor) [num_sentences, seq_len]\n        Return:\n            decoder_outputs: (Variable, FloatTensor)\n                - train: [batch_size, seq_len, vocab_size]\n                - eval: [batch_size, seq_len]\n        """"""\n        num_sentences = input_sentences.size(0)\n        max_len = input_conversation_length.max().item()\n\n\n        # encoder_outputs: [num_sentences, max_source_length, hidden_size * direction]\n        # encoder_hidden: [num_layers * direction, num_sentences, hidden_size]\n        # encoder_outputs, encoder_hidden = self.encoder(input_sentences,\n        #                                                input_sentence_length)\n        all_encoder_layers, _ = self.encoder(input_sentences, token_type_ids=None, attention_mask=input_masks)\n\n\n        bert_output = []\n        for idx in range(self.config.num_bert_layers):\n          layer = all_encoder_layers[idx]\n          bert_output.append(layer[:,0,:])\n        bert_output = torch.stack(bert_output, dim=1)\n        bert_output = torch.mean(bert_output, dim=1, keepdim=False)\n\n        \n\n        # encoder_hidden: [num_sentences, num_layers * direction * hidden_size]\n        encoder_hidden = bert_output\n\n        # pad and pack encoder_hidden\n        start = torch.cumsum(torch.cat((to_var(input_conversation_length.data.new(1).zero_()),\n                                        input_conversation_length[:-1])), 0)\n\n        # encoder_hidden: [batch_size, max_len, num_layers * direction * hidden_size]\n        encoder_hidden = torch.stack([pad(encoder_hidden.narrow(0, s, l), max_len)\n                                      for s, l in zip(start.data.tolist(),\n                                                      input_conversation_length.data.tolist())], 0)\n\n        # context_outputs: [batch_size, max_len, context_size]\n        context_outputs, context_last_hidden = self.context_encoder(encoder_hidden,\n                                                                    input_conversation_length)\n\n\n        # flatten outputs\n        # context_outputs: [num_sentences, context_size]\n        context_outputs = torch.cat([context_outputs[i, :l, :]\n                                     for i, l in enumerate(input_conversation_length.data)])\n\n        context_outputs = self.dropoutLayer(context_outputs)\n\n        # project context_outputs to decoder init state\n        decoder_init = self.context2decoder(context_outputs)\n\n        output = self.decoder2output(decoder_init)\n\n\n        return output\n'"
TL-ERC/bert_model/solver.py,16,"b'from itertools import cycle\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport models\nfrom util import to_var, time_desc_decorator\nimport os\nfrom tqdm import tqdm\nfrom math import isnan\nimport re\nimport math\nimport pickle\nimport gensim\nfrom sklearn.metrics import classification_report\n\n\nclass Solver(object):\n    def __init__(self, config, train_data_loader, valid_data_loader, test_data_loader, is_train=True, model=None):\n\n        self.config = config\n        self.epoch_i = 0\n        self.train_data_loader = train_data_loader\n        self.valid_data_loader = valid_data_loader\n        self.test_data_loader = test_data_loader\n        self.is_train = is_train\n        self.model = model\n\n    @time_desc_decorator(\'Build Graph\')\n    def build(self, cuda=True):\n\n        if self.model is None:\n\n            self.model = getattr(models, self.config.model)(self.config)\n\n            # orthogonal initialiation for hidden weights\n            # input gate bias for GRUs\n            if self.config.mode == \'train\' and self.config.checkpoint is None:\n\n                # Make later layers require_grad = False\n                for name, param in self.model.named_parameters():\n                    if ""encoder.encoder.layer"" in name:\n                        layer_num = int(name.split(""encoder.encoder.layer."")[-1].split(""."")[0])\n                        if layer_num >= (self.config.num_bert_layers):\n                            param.requires_grad = False\n\n\n                print(\'Parameter initiailization\')\n                for name, param in self.model.named_parameters(): \n                    if (\'weight_hh\' in name) and (""encoder.encoder"" not in name):\n                        print(\'\\t\' + name)\n                        nn.init.orthogonal_(param)\n\n                # Final list\n                for name, param in self.model.named_parameters():\n                    print(\'\\t\' + name, param.requires_grad)\n\n\n        if torch.cuda.is_available() and cuda:\n            self.model.cuda()\n\n        # Overview Parameters\n        print(\'Model Parameters\')\n        for name, param in self.model.named_parameters():\n            print(\'\\t\' + name + \'\\t\', list(param.size()))\n\n        if self.config.load_checkpoint:\n            self.load_model(self.config.load_checkpoint)\n\n        if self.is_train:\n            self.optimizer = self.config.optimizer(\n                filter(lambda p: p.requires_grad, self.model.parameters()),\n                lr=self.config.learning_rate)\n\n\n    def load_model(self, checkpoint):\n        """"""Load parameters from checkpoint""""""\n        print(f\'Load parameters from {checkpoint}\')\n\n        pretrained_dict = torch.load(checkpoint)\n        model_dict =self.model.state_dict()\n\n        # 1. filter out unnecessary keys\n        filtered_pretrained_dict = {}\n        for k, v in pretrained_dict.items():\n            if (k in model_dict) and (""embedding"" not in k) and (""context"" in k) and (""ih"" not in k):\n                filtered_pretrained_dict[k]=v\n\n        print(f""Filtered pretrained dict: {filtered_pretrained_dict.keys()}"")\n\n        # 2. overwrite entries in the existing state dict\n        model_dict.update(filtered_pretrained_dict)\n\n        # 3. load the new state dict\n        self.model.load_state_dict(model_dict)\n\n\n    @time_desc_decorator(\'Training Start!\')\n    def train(self):\n        min_val_loss = np.inf\n        patience_counter=0\n        best_epoch = -1\n        \n        for epoch_i in range(self.epoch_i, self.config.n_epoch):\n            self.epoch_i = epoch_i\n\n            batch_loss_history = []\n            predictions, ground_truth = [], []\n            self.model.train()\n            n_total_words = 0\n            before_gradient = None\n\n            for batch_i, (conversations, labels, conversation_length, sentence_length, type_ids, masks) in enumerate(tqdm(self.train_data_loader, ncols=80)):\n                # conversations: (batch_size) list of conversations\n                #   conversation: list of sentences\n                #   sentence: list of tokens\n                # conversation_length: list of int\n                # sentence_length: (batch_size) list of conversation list of sentence_lengths\n\n                input_conversations = conversations\n\n                # flatten input and target conversations\n                input_sentences = [sent for conv in input_conversations for sent in conv]\n                input_labels = [label for utt in labels for label in utt]\n                input_sentence_length = [l for len_list in sentence_length for l in len_list]\n                input_conversation_length = [l for l in conversation_length]\n                input_masks = [mask for conv in masks for mask in conv]\n                orig_input_labels = input_labels\n\n                # transfering the input to cuda\n                input_sentences = to_var(torch.LongTensor(input_sentences))\n                input_labels = to_var(torch.LongTensor(input_labels))\n                input_sentence_length = to_var(torch.LongTensor(input_sentence_length))\n                input_conversation_length = to_var(torch.LongTensor(input_conversation_length))\n                input_masks = to_var(torch.LongTensor(input_masks))\n\n                # reset gradient\n                self.optimizer.zero_grad()\n\n                sentence_logits = self.model(\n                    input_sentences,\n                    input_sentence_length,\n                    input_conversation_length,\n                    input_masks)\n\n                present_predictions = list(np.argmax(sentence_logits.detach().cpu().numpy(), axis=1))\n\n                loss_function = nn.CrossEntropyLoss()\n                batch_loss = loss_function(sentence_logits, input_labels)\n\n                predictions += present_predictions\n                ground_truth += orig_input_labels\n\n                assert not isnan(batch_loss.item())\n                batch_loss_history.append(batch_loss.item())\n\n                if batch_i % self.config.print_every == 0:\n                    tqdm.write(\n                        f\'Epoch: {epoch_i+1}, iter {batch_i}: loss = {batch_loss.item()}\')\n\n                # Back-propagation\n                batch_loss.backward()\n\n                # Gradient cliping\n                torch.nn.utils.clip_grad_norm_(\n                    self.model.parameters(), self.config.clip)\n\n                # Run optimizer\n                self.optimizer.step()\n\n\n            epoch_loss = np.mean(batch_loss_history)\n            self.epoch_loss = epoch_loss\n\n            print_str = f\'Epoch {epoch_i+1} loss average: {epoch_loss:.3f}\'\n            print(print_str)\n            \n            self.w_train_f1 = self.print_metric(ground_truth, predictions, ""train"")\n\n\n            self.validation_loss, self.w_valid_f1, valid_predictions = self.evaluate(self.valid_data_loader, mode=""valid"")\n            self.test_loss, self.w_test_f1, test_predictions = self.evaluate(self.test_data_loader, mode=""test"")\n\n            print(self.epoch_loss, self.w_train_f1, self.w_valid_f1, self.w_test_f1)\n\n            IMPROVED = False\n            if self.validation_loss < min_val_loss:\n                IMPROVED = True\n                min_val_loss = self.validation_loss\n                best_test_loss = self.test_loss\n                best_test_f1_w = self.w_test_f1\n                best_epoch = (self.epoch_i+1)\n\n            if (not IMPROVED):\n                patience_counter+=1\n            else:\n                patience_counter = 0\n            print(f\'Patience counter: {patience_counter}\')\n            if (patience_counter > self.config.patience):\n                break\n\n\n        return best_test_loss, best_test_f1_w, best_epoch\n\n\n\n    def evaluate(self, data_loader, mode=None):\n        assert(mode is not None)\n\n        self.model.eval()\n        batch_loss_history, predictions, ground_truth = [], [], []\n        for batch_i, (conversations, labels, conversation_length, sentence_length, type_ids, masks) in enumerate(data_loader):\n            # conversations: (batch_size) list of conversations\n            #   conversation: list of sentences\n            #   sentence: list of tokens\n            # conversation_length: list of int\n            # sentence_length: (batch_size) list of conversation list of sentence_lengths\n\n            input_conversations = conversations\n\n            # flatten input and target conversations\n            input_sentences = [sent for conv in input_conversations for sent in conv]\n            input_labels = [label for conv in labels for label in conv]\n            input_sentence_length = [l for len_list in sentence_length for l in len_list]\n            input_conversation_length = [l for l in conversation_length]\n            input_masks = [mask for conv in masks for mask in conv]\n            orig_input_labels = input_labels\n\n            with torch.no_grad():\n                # transfering the input to cuda\n                input_sentences = to_var(torch.LongTensor(input_sentences))\n                input_labels = to_var(torch.LongTensor(input_labels))\n                input_sentence_length = to_var(torch.LongTensor(input_sentence_length))\n                input_conversation_length = to_var(torch.LongTensor(input_conversation_length))\n                input_masks = to_var(torch.LongTensor(input_masks))\n\n            sentence_logits = self.model(\n                input_sentences,\n                input_sentence_length,\n                input_conversation_length,\n                input_masks)\n\n            present_predictions = list(np.argmax(sentence_logits.detach().cpu().numpy(), axis=1))\n            \n            loss_function = nn.CrossEntropyLoss()\n            batch_loss = loss_function(sentence_logits, input_labels)\n\n            predictions += present_predictions\n            ground_truth += orig_input_labels\n\n            assert not isnan(batch_loss.item())\n            batch_loss_history.append(batch_loss.item())\n\n        epoch_loss = np.mean(batch_loss_history)\n\n        print_str = f\'{mode} loss: {epoch_loss:.3f}\\n\'\n\n        w_f1_score = self.print_metric(ground_truth, predictions, mode)\n        return epoch_loss, w_f1_score, predictions\n    \n\n    \n    def print_metric(self, y_true, y_pred, mode):\n\n        if mode in [""train"", ""test""]:\n            print(mode)\n            if (self.config.data == ""dailydialog""):\n                print(classification_report(y_true, y_pred, labels=[1,2,3,4,5,6], digits=4))\n            else:\n                print(classification_report(y_true, y_pred, digits=4))\n        \n\n        if (self.config.data == ""dailydialog""):\n            weighted_fscore = classification_report(y_true, y_pred, labels=[1,2,3,4,5,6], output_dict=True, digits=4)[""weighted avg""][""f1-score""]\n        else:\n            weighted_fscore = classification_report(y_true, y_pred, output_dict=True, digits=4)[""weighted avg""][""f1-score""]\n\n        return weighted_fscore\n'"
TL-ERC/bert_model/train.py,0,"b'from solver import *\nfrom data_loader import get_loader\nfrom configs import get_config\nfrom util import Vocab\nimport os\nimport pickle\n\n\ndef load_pickle(path):\n    with open(path, \'rb\') as f:\n        return pickle.load(f)\n\nif __name__ == \'__main__\':\n    config = get_config(mode=\'train\')\n    val_config = get_config(mode=\'valid\')\n    test_config = get_config(mode=\'test\')\n\n    _RUNS = config.runs\n\n    _best_test_loss, _best_test_f1_w, _best_test_f1_m, _best_epoch = [], [], [], []\n\n    for run in range(_RUNS):\n\n        print(config)\n\n        # No. of videos to consider\n        training_data_len = int(config.training_percentage * \\\n            len(load_pickle(config.sentences_path)))\n\n\n        train_data_loader = get_loader(\n            sentences=load_pickle(config.sentences_path)[:training_data_len],\n            labels=load_pickle(config.label_path)[:training_data_len],\n            conversation_length=load_pickle(config.conversation_length_path)[:training_data_len],\n            sentence_length=load_pickle(config.sentence_length_path)[:training_data_len],\n            batch_size=config.batch_size)\n\n        eval_data_loader = get_loader(\n            sentences=load_pickle(val_config.sentences_path),\n            labels=load_pickle(val_config.label_path),\n            conversation_length=load_pickle(val_config.conversation_length_path),\n            sentence_length=load_pickle(val_config.sentence_length_path),\n            batch_size=val_config.eval_batch_size,\n            shuffle=False)\n        \n        test_data_loader = get_loader(\n            sentences=load_pickle(test_config.sentences_path),\n            labels=load_pickle(test_config.label_path),\n            conversation_length=load_pickle(test_config.conversation_length_path),\n            sentence_length=load_pickle(test_config.sentence_length_path),\n            batch_size=test_config.eval_batch_size,\n            shuffle=False)\n\n\n\n\n        # for testing\n        solver = Solver\n\n        solver = solver(config, train_data_loader,\n                        eval_data_loader, test_data_loader, is_train=True)\n\n        solver.build()\n\n        best_test_loss, best_test_f1_w, best_epoch = solver.train()\n\n        print(f""Current RUN: {run+1}"")\n\n        print(""\\n\\nBest test loss"")\n        print(best_test_loss)\n        print(""Best test f1 weighted"")\n        print(best_test_f1_w)\n        print(""Best epoch"")\n        print(best_epoch)\n\n        _best_test_loss.append(best_test_loss)\n        _best_test_f1_w.append(best_test_f1_w)\n        _best_epoch.append(best_epoch)\n\n\n    # Print final\n    print(f""\\n\\nAverage across runs:"")\n\n    print(""Best epoch"")\n    print(_best_epoch)\n\n    print(""\\n\\nBest test loss"")\n    print(np.mean(np.array(_best_test_loss), axis=0))\n\n    print(""Overall test f1 weighted"")\n    print(np.array(_best_test_f1_w))\n    \n    print(""Best test f1 weighted"")\n    print(np.mean(np.array(_best_test_f1_w), axis=0))'"
TL-ERC/utils/__init__.py,0,b'from .convert import *\nfrom .time_track import time_desc_decorator\nfrom .tensorboard import TensorboardWriter\nfrom .vocab import *\nfrom .mask import *\nfrom .tokenizer import *\nfrom .probability import *\nfrom .pad import *\nfrom .bow import *\nfrom .embedding_metric import *\n'
TL-ERC/utils/bow.py,3,"b""import numpy as np\nfrom collections import Counter\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torch\nfrom math import isnan\nfrom .vocab import PAD_ID, EOS_ID\n\n\ndef to_bow(sentence, vocab_size):\n    '''  Convert a sentence into a bag of words representation\n    Args\n        - sentence: a list of token ids\n        - vocab_size: V\n    Returns\n        - bow: a integer vector of size V\n    '''\n    bow = Counter(sentence)\n    # Remove EOS tokens\n    bow[PAD_ID] = 0\n    bow[EOS_ID] = 0\n\n    x = np.zeros(vocab_size, dtype=np.int64)\n    x[list(bow.keys())] = list(bow.values())\n\n    return x\n\n\ndef bag_of_words_loss(bow_logits, target_bow, weight=None):\n    ''' Calculate bag of words representation loss\n    Args\n        - bow_logits: [num_sentences, vocab_size]\n        - target_bow: [num_sentences]\n    '''\n    log_probs = F.log_softmax(bow_logits, dim=1)\n    target_distribution = target_bow / (target_bow.sum(1).view(-1, 1) + 1e-23) + 1e-23\n    entropy = -(torch.log(target_distribution) * target_bow).sum()\n    loss = -(log_probs * target_bow).sum() - entropy\n\n    return loss\n"""
TL-ERC/utils/convert.py,7,"b'import torch\nfrom torch.autograd import Variable\n\n\ndef to_var(x, on_cpu=False, gpu_id=None, async=False):\n    """"""Tensor => Variable""""""\n    if torch.cuda.is_available() and not on_cpu:\n        x = x.cuda(gpu_id, async)\n        #x = Variable(x)\n    return x\n\n\ndef to_tensor(x):\n    """"""Variable => Tensor""""""\n    if torch.cuda.is_available():\n        x = x.cpu()\n    return x.data\n\ndef reverse_order(tensor, dim=0):\n    """"""Reverse Tensor or Variable""""""\n    if isinstance(tensor, torch.Tensor) or isinstance(tensor, torch.LongTensor):\n        idx = [i for i in range(tensor.size(dim)-1, -1, -1)]\n        idx = torch.LongTensor(idx)\n        inverted_tensor = tensor.index_select(dim, idx)\n    if isinstance(tensor, torch.cuda.FloatTensor) or isinstance(tensor, torch.cuda.LongTensor):\n        idx = [i for i in range(tensor.size(dim)-1, -1, -1)]\n        idx = torch.cuda.LongTensor(idx)\n        inverted_tensor = tensor.index_select(dim, idx)\n        return inverted_tensor\n    elif isinstance(tensor, Variable):\n        variable = tensor\n        variable.data = reverse_order(variable.data, dim)\n        return variable\n\ndef reverse_order_valid(tensor, length_list, dim=0):\n    """"""\n    Reverse Tensor of Variable only in given length\n    Ex)\n    Args:\n        - tensor (Tensor or Variable)\n         1   2   3   4   5   6\n         6   7   8   9   0   0\n        11  12  13   0   0   0\n        16  17   0   0   0   0\n        21  22  23  24  25  26\n\n        - length_list (list)\n        [6, 4, 3, 2, 6]\n \n    Return:\n        tensor (Tensor or Variable; in-place)\n         6   5   4   3   2   1\n         0   0   9   8   7   6\n         0   0   0  13  12  11\n         0   0   0   0  17  16\n        26  25  24  23  22  21\n    """"""\n    for row, length in zip(tensor, length_list):\n        valid_row = row[:length]\n        reversed_valid_row = reverse_order(valid_row, dim=dim)\n        row[:length] = reversed_valid_row\n    return tensor\n'"
TL-ERC/utils/embedding_metric.py,0,"b""import numpy as np\n\n\ndef cosine_similarity(s, g):\n    similarity = np.sum(s * g, axis=1) / np.sqrt((np.sum(s * s, axis=1) * np.sum(g * g, axis=1)))\n\n    # return np.sum(similarity)\n    return similarity\n\n\ndef embedding_metric(samples, ground_truth, word2vec, method='average'):\n\n    if method == 'average':\n        # s, g: [n_samples, word_dim]\n        s = [np.mean(sample, axis=0) for sample in samples]\n        g = [np.mean(gt, axis=0) for gt in ground_truth]\n        return cosine_similarity(np.array(s), np.array(g))\n    elif method == 'extrema':\n        s_list = []\n        g_list = []\n        for sample, gt in zip(samples, ground_truth):\n            s_max = np.max(sample, axis=0)\n            s_min = np.min(sample, axis=0)\n            s_plus = np.absolute(s_min) <= s_max\n            s_abs = np.max(np.absolute(sample), axis=0)\n            s = s_max * s_plus + s_min * np.logical_not(s_plus)\n            s_list.append(s)\n\n            g_max = np.max(gt, axis=0)\n            g_min = np.min(gt, axis=0)\n            g_plus = np.absolute(g_min) <= g_max\n            g_abs = np.max(np.absolute(gt), axis=0)\n            g = g_max * g_plus + g_min * np.logical_not(g_plus)\n            g_list.append(g)\n\n        return cosine_similarity(np.array(s_list), np.array(g_list))\n    elif method == 'greedy':\n        sim_list = []\n        for s, g in zip(samples, ground_truth):\n            s = np.array(s)\n            g = np.array(g).T\n            sim = (np.matmul(s, g)\n                   / np.sqrt(np.matmul(np.sum(s * s, axis=1, keepdims=True), np.sum(g * g, axis=0, keepdims=True))))\n            sim = np.max(sim, axis=0)\n            sim_list.append(np.mean(sim))\n\n        # return np.sum(sim_list)\n        return np.array(sim_list)\n    else:\n        raise NotImplementedError\n"""
TL-ERC/utils/mask.py,1,"b'import torch\nfrom .convert import to_var\n\n\ndef sequence_mask(sequence_length, max_len=None):\n    """"""\n    Args:\n        sequence_length (Variable, LongTensor) [batch_size]\n            - list of sequence length of each batch\n        max_len (int)\n    Return:\n        masks (bool): [batch_size, max_len]\n            - True if current sequence is valid (not padded), False otherwise\n\n    Ex.\n    sequence length: [3, 2, 1]\n\n    seq_length_expand\n    [[3, 3, 3],\n     [2, 2, 2]\n     [1, 1, 1]]\n\n    seq_range_expand\n    [[0, 1, 2]\n     [0, 1, 2],\n     [0, 1, 2]]\n\n    masks\n    [[True, True, True],\n     [True, True, False],\n     [True, False, False]]\n    """"""\n    if max_len is None:\n        max_len = sequence_length.max()\n    batch_size = sequence_length.size(0)\n\n    # [max_len]\n    seq_range = torch.arange(0, max_len).long()  # [0, 1, ... max_len-1]\n\n    # [batch_size, max_len]\n    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n    seq_range_expand = to_var(seq_range_expand)\n\n    # [batch_size, max_len]\n    seq_length_expand = sequence_length.unsqueeze(1).expand_as(seq_range_expand)\n\n    # [batch_size, max_len]\n    masks = seq_range_expand < seq_length_expand\n\n    return masks\n'"
TL-ERC/utils/pad.py,6,"b'import torch\nfrom torch.autograd import Variable\nfrom .convert import to_var\n\n\ndef pad(tensor, length):\n\n    if isinstance(tensor, Variable):\n        var = tensor\n        if length > var.size(0):\n            return torch.cat([var,\n                              torch.zeros(length - var.size(0), *var.size()[1:]).cuda()])\n        else:\n            return var\n    else:\n        if length > tensor.size(0):\n            return torch.cat([tensor,\n                              torch.zeros(length - tensor.size(0), *tensor.size()[1:]).cuda()])\n        else:\n            return tensor\n\n\ndef pad_and_pack(tensor_list):\n    length_list = ([t.size(0) for t in tensor_list])\n    max_len = max(length_list)\n    padded = [pad(t, max_len) for t in tensor_list]\n    packed = torch.stack(padded, 0)\n    return packed, length_list\n'"
TL-ERC/utils/probability.py,6,"b'import torch\nimport numpy as np\nfrom .convert import to_var\n\n\ndef normal_logpdf(x, mean, var):\n    """"""\n    Args:\n        x: (Variable, FloatTensor) [batch_size, dim]\n        mean: (Variable, FloatTensor) [batch_size, dim] or [batch_size] or [1]\n        var: (Variable, FloatTensor) [batch_size, dim]: positive value\n    Return:\n        log_p: (Variable, FloatTensor) [batch_size]\n    """"""\n\n    pi = to_var(torch.FloatTensor([np.pi]))\n    return 0.5 * torch.sum(-torch.log(2.0 * pi) - torch.log(var) - ((x - mean).pow(2) / var), dim=1)\n\n\ndef normal_kl_div(mu1, var1,\n                  mu2=to_var(torch.FloatTensor([0.0])),\n                  var2=to_var(torch.FloatTensor([1.0]))):\n    one = to_var(torch.FloatTensor([1.0]))\n    return torch.sum(0.5 * (torch.log(var2) - torch.log(var1)\n                            + (var1 + (mu1 - mu2).pow(2)) / var2 - one), 1)\n'"
TL-ERC/utils/tensorboard.py,0,"b'from tensorboardX import SummaryWriter\n\nclass TensorboardWriter(SummaryWriter):\n    def __init__(self, logdir):\n        """"""\n        Extended SummaryWriter Class from tensorboard-pytorch (tensorbaordX)\n        https://github.com/lanpa/tensorboard-pytorch/blob/master/tensorboardX/writer.py\n\n        Internally calls self.file_writer\n        """"""\n        super(TensorboardWriter, self).__init__(logdir)\n        self.logdir = self.file_writer.get_logdir()\n\n    def update_parameters(self, module, step_i):\n        """"""\n        module: nn.Module\n        """"""\n        for name, param in module.named_parameters():\n            self.add_histogram(name, param.clone().cpu().data.numpy(), step_i)\n\n    def update_loss(self, loss, step_i, name=\'loss\'):\n        self.add_scalar(name, loss, step_i)\n\n    def update_histogram(self, values, step_i, name=\'hist\'):\n        self.add_histogram(name, values, step_i)\n'"
TL-ERC/utils/time_track.py,0,"b""import time\nfrom functools import partial\n\n\ndef base_time_desc_decorator(method, desc='test_description'):\n    def timed(*args, **kwargs):\n\n        # Print Description\n        # print('#' * 50)\n        print(desc)\n        # print('#' * 50 + '\\n')\n\n        # Calculation Runtime\n        start = time.time()\n\n        # Run Method\n        try:\n            result = method(*args, **kwargs)\n        except TypeError:\n            result = method(**kwargs)\n\n        # Print Runtime\n        print('Done! It took {:.2} secs\\n'.format(time.time() - start))\n\n        if result is not None:\n            return result\n\n    return timed\n\n\ndef time_desc_decorator(desc): return partial(base_time_desc_decorator, desc=desc)\n\n\n@time_desc_decorator('this is description')\ndef time_test(arg, kwarg='this is kwarg'):\n    time.sleep(3)\n    print('Inside of time_test')\n    print('printing arg: ', arg)\n    print('printing kwarg: ',  kwarg)\n\n\n@time_desc_decorator('this is second description')\ndef no_arg_method():\n    print('this method has no argument')\n\n\nif __name__ == '__main__':\n    time_test('hello', kwarg=3)\n    time_test(3)\n    no_arg_method()\n"""
TL-ERC/utils/tokenizer.py,0,"b'import re\n\n\ndef clean_str(string):\n    """"""\n    Tokenization/string cleaning for all datasets except for SST.\n    Every dataset is lower cased except for TREC\n    """"""\n    string = re.sub(r""[^A-Za-z0-9,!?\\\'\\`\\.]"", "" "", string)\n    string = re.sub(r""\\.{3}"", "" ..."", string)\n    string = re.sub(r""\\\'s"", "" \\\'s"", string)\n    string = re.sub(r""\\\'ve"", "" \\\'ve"", string)\n    string = re.sub(r""n\\\'t"", "" n\\\'t"", string)\n    string = re.sub(r""\\\'re"", "" \\\'re"", string)\n    string = re.sub(r""\\\'d"", "" \\\'d"", string)\n    string = re.sub(r""\\\'ll"", "" \\\'ll"", string)\n    string = re.sub(r"","", "" , "", string)\n    string = re.sub(r""!"", "" ! "", string)\n    string = re.sub(r""\\?"", "" ? "", string)\n    string = re.sub(r""\\s{2,}"", "" "", string)\n    return string.strip().lower()\n\n\nclass Tokenizer():\n    def __init__(self, tokenizer=\'whitespace\', clean_string=True):\n        self.clean_string = clean_string\n        tokenizer = tokenizer.lower()\n\n        # Tokenize with whitespace\n        if tokenizer == \'whitespace\':\n            print(\'Loading whitespace tokenizer\')\n            self.tokenize = lambda string: string.strip().split()\n\n        if tokenizer == \'regex\':\n            print(\'Loading regex tokenizer\')\n            import re\n            pattern = r""[A-Z]{2,}(?![a-z])|[A-Z][a-z]+(?=[A-Z])|[\\\'\\w\\-]+""\n            self.tokenize = lambda string: re.findall(pattern, string)\n\n        if tokenizer == \'spacy\':\n            print(\'Loading SpaCy\')\n            import spacy\n            nlp = spacy.load(\'en\')\n            self.tokenize = lambda string: [token.text for token in nlp(string)]\n\n        # Tokenize with punctuations other than periods\n        if tokenizer == \'nltk\':\n            print(\'Loading NLTK word tokenizer\')\n            from nltk import word_tokenize\n\n            self.tokenize = word_tokenize\n\n    def __call__(self, string):\n        if self.clean_string:\n            string = clean_str(string)\n        return self.tokenize(string)\n\n\nif __name__ == \'__main__\':\n    tokenizer = Tokenizer()\n    print(tokenizer(""Hello, how are you doin\'?""))\n\n    tokenizer = Tokenizer(\'spacy\')\n    print(tokenizer(""Hello, how are you doin\'?""))\n'"
TL-ERC/utils/vocab.py,2,"b'from collections import defaultdict\nimport pickle\nimport torch\nimport numpy as np\nimport os\nfrom torch import Tensor\nfrom torch.autograd import Variable\nfrom nltk import FreqDist\nfrom .convert import to_tensor, to_var\n\n\nPAD_TOKEN = \'<pad>\'\nUNK_TOKEN = \'<unk>\'\nSOS_TOKEN = \'<sos>\'\nEOS_TOKEN = \'<eos>\'\n\nPAD_ID, UNK_ID, SOS_ID, EOS_ID = [0, 1, 2, 3]\n\n\nclass Vocab(object):\n    def __init__(self, tokenizer=None, max_size=None, min_freq=1):\n        """"""Basic Vocabulary object""""""\n\n        self.vocab_size = 0\n        self.freqdist = FreqDist()\n        self.tokenizer = tokenizer\n\n    def update(self, glove_dir, max_size=None, min_freq=1):\n        """"""\n        Initialize id2word & word2id based on self.freqdist\n        max_size include 4 special tokens\n        """"""\n\n        # {0: \'<pad>\', 1: \'<unk>\', 2: \'<sos>\', 3: \'<eos>\'}\n        self.id2word = {\n            PAD_ID: PAD_TOKEN, UNK_ID: UNK_TOKEN,\n            SOS_ID: SOS_TOKEN, EOS_ID: EOS_TOKEN\n        }\n        # {\'<pad>\': 0, \'<unk>\': 1, \'<sos>\': 2, \'<eos>\': 3}\n        self.word2id = defaultdict(lambda: UNK_ID)  # Not in vocab => return UNK\n        self.word2id.update({\n            PAD_TOKEN: PAD_ID, UNK_TOKEN: UNK_ID,\n            SOS_TOKEN: SOS_ID, EOS_TOKEN: EOS_ID\n        })\n        # self.word2id = {\n        #     PAD_TOKEN: PAD_ID, UNK_TOKEN: UNK_ID,\n        #     SOS_TOKEN: SOS_ID, EOS_TOKEN: EOS_ID\n        # }\n\n        vocab_size = 4\n        min_freq = max(min_freq, 1)\n\n        # Reset frequencies of special tokens\n        # [...(\'<eos>\', 0), (\'<pad>\', 0), (\'<sos>\', 0), (\'<unk>\', 0)]\n        freqdist = self.freqdist.copy()\n        special_freqdist = {token: freqdist[token]\n                            for token in [PAD_TOKEN, UNK_TOKEN, SOS_TOKEN, EOS_TOKEN]}\n        freqdist.subtract(special_freqdist)\n\n        # Sort: by frequency, then alphabetically\n        # Ex) freqdist = { \'a\': 4,   \'b\': 5,   \'c\': 3 }\n        #  =>   sorted = [(\'b\', 5), (\'a\', 4), (\'c\', 3)]\n        sorted_frequency_counter = sorted(freqdist.items(), key=lambda k_v: k_v[0])\n        sorted_frequency_counter.sort(key=lambda k_v: k_v[1], reverse=True)\n\n        # Load glove vector\n        word_emb_dict = self.get_glove_emb(glove_dir)\n\n        for word, freq in sorted_frequency_counter:\n\n            if freq < min_freq or vocab_size == max_size:\n                break\n            self.id2word[vocab_size] = word\n            self.word2id[word] = vocab_size\n            vocab_size += 1\n\n        self.vocab_size = vocab_size\n\n\n        # Create embedding matrix\n        self.embedding_matrix = embedding_matrix = np.zeros((self.vocab_size, 300))\n\n        for word, ind in self.word2id.items():\n            if word.lower() in word_emb_dict:\n                embedding_matrix[self.word2id[word]] = word_emb_dict[word.lower()]\n            else:\n                embedding_matrix[self.word2id[word]] = np.random.uniform(-0.25, 0.25, 300)\n\n    def get_glove_emb(self, GLOVE_DIR):\n        embeddings_index = {}\n        f = open(os.path.join(GLOVE_DIR, \'glove.840B.300d.txt\'), \'rb\')\n        for line in f:\n            values = line.split()\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype=\'float32\')\n            embeddings_index[word.decode().lower()] = coefs\n        f.close()\n        return embeddings_index\n\n\n    def __len__(self):\n        return len(self.id2word)\n\n\n    def load(self, word2id_path=None, id2word_path=None, word_emb_path=None):\n        if word2id_path:\n            with open(word2id_path, \'rb\') as f:\n                word2id = pickle.load(f)\n            # Can\'t pickle lambda function\n            self.word2id = defaultdict(lambda: UNK_ID)\n            self.word2id.update(word2id)\n            self.vocab_size = len(self.word2id)\n\n        if id2word_path:\n            with open(id2word_path, \'rb\') as f:\n                id2word = pickle.load(f)\n            self.id2word = id2word\n        \n        if word_emb_path:\n            with open(word_emb_path, \'rb\') as f:\n                embedding_matrix = pickle.load(f)\n            self.embedding_matrix = embedding_matrix\n\n    def add_word(self, word):\n        assert isinstance(word, str), \'Input should be str\'\n        self.freqdist.update([word])\n\n    def add_sentence(self, sentence, tokenized=False):\n        if not tokenized:\n            sentence = self.tokenizer(sentence)\n        for word in sentence:\n            self.add_word(word)\n\n    def add_dataframe(self, conversation_df, tokenized=True):\n        for conversation in conversation_df:\n            for sentence in conversation:\n                self.add_sentence(sentence, tokenized=tokenized)\n\n    def pickle(self, word2id_path, id2word_path, word_emb_path):\n        with open(word2id_path, \'wb\') as f:\n            pickle.dump(dict(self.word2id), f)\n\n        with open(id2word_path, \'wb\') as f:\n            pickle.dump(self.id2word, f)\n\n        with open(word_emb_path, \'wb\') as f:\n            pickle.dump(self.embedding_matrix, f)\n\n    def to_list(self, list_like):\n        """"""Convert list-like containers to list""""""\n        if isinstance(list_like, list):\n            return list_like\n\n        if isinstance(list_like, Variable):\n            return list(to_tensor(list_like).numpy())\n        elif isinstance(list_like, Tensor):\n            return list(list_like.numpy())\n\n    def id2sent(self, id_list):\n        """"""list of id => list of tokens (Single sentence)""""""\n        id_list = self.to_list(id_list)\n        sentence = []\n        for id in id_list:\n            word = self.id2word[id]\n            if word not in [EOS_TOKEN, SOS_TOKEN, PAD_TOKEN]:\n                sentence.append(word)\n            if word == EOS_TOKEN:\n                break\n        return sentence\n\n    def sent2id(self, sentence, var=False):\n        """"""list of tokens => list of id (Single sentence)""""""\n        id_list = [self.word2id[word] for word in sentence]\n        if var:\n            id_list = to_var(torch.LongTensor(id_list), eval=True)\n        return id_list\n\n    def decode(self, id_list):\n        sentence = self.id2sent(id_list)\n        return \' \'.join(sentence)\n'"
TL-ERC/bert_model/layer/__init__.py,0,"b'from .encoder import *\nfrom .rnncells import StackedLSTMCell, StackedGRUCell\nfrom .loss import *\nfrom .feedforward import *\n'"
TL-ERC/bert_model/layer/encoder.py,8,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, PackedSequence\nfrom util import to_var, reverse_order_valid, PAD_ID\nfrom .rnncells import StackedGRUCell, StackedLSTMCell\n\nimport copy\n\n\nclass BaseRNNEncoder(nn.Module):\n    def __init__(self):\n        """"""Base RNN Encoder Class""""""\n        super(BaseRNNEncoder, self).__init__()\n\n    @property\n    def use_lstm(self):\n        if hasattr(self, \'rnn\'):\n            return isinstance(self.rnn, nn.LSTM)\n        else:\n            raise AttributeError(\'no rnn selected\')\n\n    def init_h(self, batch_size=None, hidden=None):\n        """"""Return RNN initial state""""""\n        if hidden is not None:\n            return hidden\n\n        if self.use_lstm:\n            return (to_var(torch.zeros(self.num_layers*self.num_directions,\n                                       batch_size,\n                                       self.hidden_size)),\n                    to_var(torch.zeros(self.num_layers*self.num_directions,\n                                       batch_size,\n                                       self.hidden_size)))\n        else:\n            return to_var(torch.zeros(self.num_layers*self.num_directions,\n                                      batch_size,\n                                      self.hidden_size))\n\n    def batch_size(self, inputs=None, h=None):\n        """"""\n        inputs: [batch_size, seq_len]\n        h: [num_layers, batch_size, hidden_size] (RNN/GRU)\n        h_c: [2, num_layers, batch_size, hidden_size] (LSTM)\n        """"""\n        if inputs is not None:\n            batch_size = inputs.size(0)\n            return batch_size\n\n        else:\n            if self.use_lstm:\n                batch_size = h[0].size(1)\n            else:\n                batch_size = h.size(1)\n            return batch_size\n\n    def forward(self):\n        raise NotImplementedError\n\n\nclass EncoderRNN(BaseRNNEncoder):\n    def __init__(self, vocab_size, embedding_size,\n                 hidden_size, rnn=nn.GRU, num_layers=1, bidirectional=False,\n                 dropout=0.0, bias=True, batch_first=True, train_emb = False, emb_weights_matrix=None):\n        """"""Sentence-level Encoder""""""\n        super(EncoderRNN, self).__init__()\n\n        self.vocab_size = vocab_size\n        self.embedding_size = embedding_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.batch_first = batch_first\n        self.bidirectional = bidirectional\n\n        if bidirectional:\n            self.num_directions = 2\n        else:\n            self.num_directions = 1\n\n        # word embedding\n        self.embedding = nn.Embedding(\n            vocab_size, embedding_size, padding_idx=PAD_ID)\n        # self.embedding.load_state_dict({\'weight\': to_var(torch.LongTensor(emb_weights_matrix))})\n        # if (not train_emb):\n        #     self.embedding.weight.requires_grad = False \n\n        self.rnn = rnn(input_size=embedding_size,\n                       hidden_size=hidden_size,\n                       num_layers=num_layers,\n                       bias=bias,\n                       batch_first=batch_first,\n                       dropout=dropout,\n                       bidirectional=bidirectional)\n\n    def forward(self, inputs, input_length, hidden=None):\n        """"""\n        Args:\n            inputs (Variable, LongTensor): [num_setences, max_seq_len]\n            input_length (Variable, LongTensor): [num_sentences]\n        Return:\n            outputs (Variable): [max_source_length, batch_size, hidden_size]\n                - list of all hidden states\n            hidden ((tuple of) Variable): [num_layers*num_directions, batch_size, hidden_size]\n                - last hidden state\n                - (h, c) or h\n        """"""\n        batch_size, seq_len = inputs.size()\n\n        # Sort in decreasing order of length for pack_padded_sequence()\n        input_length_sorted, indices = input_length.sort(descending=True)\n\n        input_length_sorted = input_length_sorted.data.tolist()\n\n        # [num_sentences, max_source_length]\n        inputs_sorted = inputs.index_select(0, indices)\n\n        # [num_sentences, max_source_length, embedding_dim]\n        embedded = self.embedding(inputs_sorted)\n\n        # batch_first=True\n        rnn_input = pack_padded_sequence(embedded, input_length_sorted,\n                                         batch_first=self.batch_first)\n\n        hidden = self.init_h(batch_size, hidden=hidden)\n        \n\n        # outputs: [batch, seq_len, hidden_size * num_directions]\n        # hidden: [num_layers * num_directions, batch, hidden_size]\n        self.rnn.flatten_parameters()\n        outputs, hidden = self.rnn(rnn_input, hidden)\n\n        outputs, outputs_lengths = pad_packed_sequence(\n            outputs, batch_first=self.batch_first)\n\n        # Reorder outputs and hidden\n        _, inverse_indices = indices.sort()\n        outputs = outputs.index_select(0, inverse_indices)\n\n        if self.use_lstm:\n            hidden = (hidden[0].index_select(1, inverse_indices),\n                      hidden[1].index_select(1, inverse_indices))\n        else:\n            hidden = hidden.index_select(1, inverse_indices)\n\n        return outputs, hidden\n\n\nclass ContextRNN(BaseRNNEncoder):\n    def __init__(self, input_size, context_size, rnn=nn.GRU, num_layers=1, dropout=0.0,\n                 bidirectional=False, bias=True, batch_first=True):\n        """"""Context-level Encoder""""""\n        super(ContextRNN, self).__init__()\n\n        self.input_size = input_size\n        self.context_size = context_size\n        self.hidden_size = self.context_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.bidirectional = bidirectional\n        self.batch_first = batch_first\n\n        if bidirectional:\n            self.num_directions = 2\n        else:\n            self.num_directions = 1\n\n        self.rnn = rnn(input_size=input_size,\n                       hidden_size=context_size,\n                       num_layers=num_layers,\n                       bias=bias,\n                       batch_first=batch_first,\n                       dropout=dropout,\n                       bidirectional=bidirectional)\n\n    def forward(self, encoder_hidden, conversation_length, hidden=None):\n        """"""\n        Args:\n            encoder_hidden (Variable, FloatTensor): [batch_size, max_len, num_layers * direction * hidden_size]\n            conversation_length (Variable, LongTensor): [batch_size]\n        Return:\n            outputs (Variable): [batch_size, max_seq_len, hidden_size]\n                - list of all hidden states\n            hidden ((tuple of) Variable): [num_layers*num_directions, batch_size, hidden_size]\n                - last hidden state\n                - (h, c) or h\n        """"""\n        batch_size, seq_len, _ = encoder_hidden.size()\n\n        # Sort for PackedSequence\n        conv_length_sorted, indices = conversation_length.sort(descending=True)\n        \n        conv_length_sorted = conv_length_sorted.data.tolist()\n        encoder_hidden_sorted = encoder_hidden.index_select(0, indices)\n\n        \n        rnn_input = pack_padded_sequence(\n            encoder_hidden_sorted, conv_length_sorted, batch_first=True)\n\n        hidden = self.init_h(batch_size, hidden=hidden)\n\n        self.rnn.flatten_parameters()\n        outputs, hidden = self.rnn(rnn_input, hidden)\n\n        # outputs: [batch_size, max_conversation_length, context_size]\n        outputs, outputs_length = pad_packed_sequence(\n            outputs, batch_first=True)\n\n        # reorder outputs and hidden\n        _, inverse_indices = indices.sort()\n        outputs = outputs.index_select(0, inverse_indices)\n\n        if self.use_lstm:\n            hidden = (hidden[0].index_select(1, inverse_indices),\n                      hidden[1].index_select(1, inverse_indices))\n        else:\n            hidden = hidden.index_select(1, inverse_indices)\n\n        # outputs: [batch, seq_len, hidden_size * num_directions]\n        # hidden: [num_layers * num_directions, batch, hidden_size]\n        return outputs, hidden\n\n    def step(self, encoder_hidden, hidden):\n\n        batch_size = encoder_hidden.size(0)\n        # encoder_hidden: [1, batch_size, hidden_size]\n        encoder_hidden = torch.unsqueeze(encoder_hidden, 1)\n\n        if hidden is None:\n            hidden = self.init_h(batch_size, hidden=None)\n\n        outputs, hidden = self.rnn(encoder_hidden, hidden)\n        return outputs, hidden\n'"
TL-ERC/bert_model/layer/feedforward.py,1,"b'import torch\nimport torch.nn as nn\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, input_size, output_size, num_layers=1, hidden_size=None,\n                 activation=""Tanh"", bias=True, isActivation=True):\n        super(FeedForward, self).__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.activation = getattr(nn, activation, None)\n        if self.activation is not None:\n            self.activation = self.activation()\n        else:\n            self.activation = getattr(nn.functional, activation)()\n        self.isActivation = isActivation\n        \n        n_inputs = [input_size] + [hidden_size] * (num_layers - 1)\n        n_outputs = [hidden_size] * (num_layers - 1) + [output_size]\n        self.linears = nn.ModuleList([nn.Linear(n_in, n_out, bias=bias)\n                                      for n_in, n_out in zip(n_inputs, n_outputs)])\n\n    def forward(self, input):\n        x = input\n        for linear in self.linears:\n            x = linear(x)\n            if self.isActivation:\n                x = self.activation(x)\n\n        return x\n'"
TL-ERC/bert_model/layer/loss.py,3,"b'import torch\nfrom torch.nn import functional as F\nimport torch.nn as nn\nfrom util import to_var, sequence_mask\n\n\n# https://gist.github.com/jihunchoi/f1434a77df9db1bb337417854b398df1\ndef masked_cross_entropy(logits, target, length, per_example=False):\n    """"""\n    Args:\n        logits (Variable, FloatTensor): [batch, max_len, num_classes]\n            - unnormalized probability for each class\n        target (Variable, LongTensor): [batch, max_len]\n            - index of true class for each corresponding step\n        length (Variable, LongTensor): [batch]\n            - length of each data in a batch\n    Returns:\n        loss (Variable): []\n            - An average loss value masked by the length\n    """"""\n    batch_size, max_len, num_classes = logits.size()\n\n    # [batch_size * max_len, num_classes]\n    logits_flat = logits.view(-1, num_classes)\n\n    # [batch_size * max_len, num_classes]\n    log_probs_flat = F.log_softmax(logits_flat, dim=1)\n\n    # [batch_size * max_len, 1]\n    target_flat = target.view(-1, 1)\n\n    # Negative Log-likelihood: -sum {  1* log P(target)  + 0 log P(non-target)} = -sum( log P(target) )\n    # [batch_size * max_len, 1]\n    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n\n    # [batch_size, max_len]\n    losses = losses_flat.view(batch_size, max_len)\n\n    # [batch_size, max_len]\n    mask = sequence_mask(sequence_length=length, max_len=max_len)\n\n    # Apply masking on loss\n    losses = losses * mask.float()\n\n    # word-wise cross entropy\n    # loss = losses.sum() / length.float().sum()\n\n    if per_example:\n        # loss: [batch_size]\n        return losses.sum(1)\n    else:\n        loss = losses.sum()\n        return loss, length.float().sum()\n'"
TL-ERC/bert_model/layer/rnncells.py,6,"b'# Modified from OpenNMT.py, Z-forcing\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n# from torch.nn._functions.thnn.rnnFusedPointwise import LSTMFused, GRUFused\n\n\nclass StackedLSTMCell(nn.Module):\n\n    def __init__(self, num_layers, input_size, rnn_size, dropout):\n        super(StackedLSTMCell, self).__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.num_layers = num_layers\n\n        self.layers = nn.ModuleList()\n        for i in range(num_layers):\n            self.layers.append(nn.LSTMCell(input_size, rnn_size))\n            input_size = rnn_size\n\n    def forward(self, x, h_c):\n        """"""\n        Args:\n            x: [batch_size, input_size]\n            h_c: [2, num_layers, batch_size, hidden_size]\n        Return:\n            last_h_c: [2, batch_size, hidden_size] (h from last layer)\n            h_c_list: [2, num_layers, batch_size, hidden_size] (h and c from all layers)\n        """"""\n        h_0, c_0 = h_c\n        h_list, c_list = [], []\n        for i, layer in enumerate(self.layers):\n            # h of i-th layer\n            h_i, c_i = layer(x, (h_0[i], c_0[i]))\n\n            # x for next layer\n            x = h_i\n            if i + 1 != self.num_layers:\n                x = self.dropout(x)\n            h_list += [h_i]\n            c_list += [c_i]\n\n        last_h_c = (h_list[-1], c_list[-1])\n        h_list = torch.stack(h_list)\n        c_list = torch.stack(c_list)\n        h_c_list = (h_list, c_list)\n\n        return last_h_c, h_c_list\n\n\nclass StackedGRUCell(nn.Module):\n\n    def __init__(self, num_layers, input_size, rnn_size, dropout):\n        super(StackedGRUCell, self).__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.num_layers = num_layers\n\n        self.layers = nn.ModuleList()\n        for i in range(num_layers):\n            self.layers.append(nn.GRUCell(input_size, rnn_size))\n            input_size = rnn_size\n\n    def forward(self, x, h):\n        """"""\n        Args:\n            x: [batch_size, input_size]\n            h: [num_layers, batch_size, hidden_size]\n        Return:\n            last_h: [batch_size, hidden_size] (h from last layer)\n            h_list: [num_layers, batch_size, hidden_size] (h from all layers)\n        """"""\n        # h of all layers\n        h_list = []\n        for i, layer in enumerate(self.layers):\n            # h of i-th layer\n            h_i = layer(x, h[i])\n\n            # x for next layer\n            x = h_i\n            if i + 1 is not self.num_layers:\n                x = self.dropout(x)\n            h_list.append(h_i)\n\n        last_h = h_list[-1]\n        h_list = torch.stack(h_list)\n\n        return last_h, h_list\n'"
TL-ERC/bert_model/util/__init__.py,0,b'from .convert import *\nfrom .time_track import time_desc_decorator\nfrom .tensorboard import TensorboardWriter\nfrom .vocab import *\nfrom .mask import *\nfrom .tokenizer import *\nfrom .probability import *\nfrom .pad import *\nfrom .bow import *\nfrom .embedding_metric import *\n'
TL-ERC/bert_model/util/bow.py,3,"b""import numpy as np\nfrom collections import Counter\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torch\nfrom math import isnan\nfrom .vocab import PAD_ID, EOS_ID\n\n\ndef to_bow(sentence, vocab_size):\n    '''  Convert a sentence into a bag of words representation\n    Args\n        - sentence: a list of token ids\n        - vocab_size: V\n    Returns\n        - bow: a integer vector of size V\n    '''\n    bow = Counter(sentence)\n    # Remove EOS tokens\n    bow[PAD_ID] = 0\n    bow[EOS_ID] = 0\n\n    x = np.zeros(vocab_size, dtype=np.int64)\n    x[list(bow.keys())] = list(bow.values())\n\n    return x\n\n\ndef bag_of_words_loss(bow_logits, target_bow, weight=None):\n    ''' Calculate bag of words representation loss\n    Args\n        - bow_logits: [num_sentences, vocab_size]\n        - target_bow: [num_sentences]\n    '''\n    log_probs = F.log_softmax(bow_logits, dim=1)\n    target_distribution = target_bow / (target_bow.sum(1).view(-1, 1) + 1e-23) + 1e-23\n    entropy = -(torch.log(target_distribution) * target_bow).sum()\n    loss = -(log_probs * target_bow).sum() - entropy\n\n    return loss\n"""
TL-ERC/bert_model/util/convert.py,7,"b'import torch\nfrom torch.autograd import Variable\n\n\ndef to_var(x, on_cpu=False, gpu_id=None, async=False):\n    """"""Tensor => Variable""""""\n    if torch.cuda.is_available() and not on_cpu:\n        x = x.cuda(gpu_id, async)\n        #x = Variable(x)\n    return x\n\n\ndef to_tensor(x):\n    """"""Variable => Tensor""""""\n    if torch.cuda.is_available():\n        x = x.cpu()\n    return x.data\n\ndef reverse_order(tensor, dim=0):\n    """"""Reverse Tensor or Variable""""""\n    if isinstance(tensor, torch.Tensor) or isinstance(tensor, torch.LongTensor):\n        idx = [i for i in range(tensor.size(dim)-1, -1, -1)]\n        idx = torch.LongTensor(idx)\n        inverted_tensor = tensor.index_select(dim, idx)\n    if isinstance(tensor, torch.cuda.FloatTensor) or isinstance(tensor, torch.cuda.LongTensor):\n        idx = [i for i in range(tensor.size(dim)-1, -1, -1)]\n        idx = torch.cuda.LongTensor(idx)\n        inverted_tensor = tensor.index_select(dim, idx)\n        return inverted_tensor\n    elif isinstance(tensor, Variable):\n        variable = tensor\n        variable.data = reverse_order(variable.data, dim)\n        return variable\n\ndef reverse_order_valid(tensor, length_list, dim=0):\n    """"""\n    Reverse Tensor of Variable only in given length\n    Ex)\n    Args:\n        - tensor (Tensor or Variable)\n         1   2   3   4   5   6\n         6   7   8   9   0   0\n        11  12  13   0   0   0\n        16  17   0   0   0   0\n        21  22  23  24  25  26\n\n        - length_list (list)\n        [6, 4, 3, 2, 6]\n \n    Return:\n        tensor (Tensor or Variable; in-place)\n         6   5   4   3   2   1\n         0   0   9   8   7   6\n         0   0   0  13  12  11\n         0   0   0   0  17  16\n        26  25  24  23  22  21\n    """"""\n    for row, length in zip(tensor, length_list):\n        valid_row = row[:length]\n        reversed_valid_row = reverse_order(valid_row, dim=dim)\n        row[:length] = reversed_valid_row\n    return tensor\n'"
TL-ERC/bert_model/util/embedding_metric.py,0,"b""import numpy as np\n\n\ndef cosine_similarity(s, g):\n    similarity = np.sum(s * g, axis=1) / np.sqrt((np.sum(s * s, axis=1) * np.sum(g * g, axis=1)))\n\n    # return np.sum(similarity)\n    return similarity\n\n\ndef embedding_metric(samples, ground_truth, word2vec, method='average'):\n\n    if method == 'average':\n        # s, g: [n_samples, word_dim]\n        s = [np.mean(sample, axis=0) for sample in samples]\n        g = [np.mean(gt, axis=0) for gt in ground_truth]\n        return cosine_similarity(np.array(s), np.array(g))\n    elif method == 'extrema':\n        s_list = []\n        g_list = []\n        for sample, gt in zip(samples, ground_truth):\n            s_max = np.max(sample, axis=0)\n            s_min = np.min(sample, axis=0)\n            s_plus = np.absolute(s_min) <= s_max\n            s_abs = np.max(np.absolute(sample), axis=0)\n            s = s_max * s_plus + s_min * np.logical_not(s_plus)\n            s_list.append(s)\n\n            g_max = np.max(gt, axis=0)\n            g_min = np.min(gt, axis=0)\n            g_plus = np.absolute(g_min) <= g_max\n            g_abs = np.max(np.absolute(gt), axis=0)\n            g = g_max * g_plus + g_min * np.logical_not(g_plus)\n            g_list.append(g)\n\n        return cosine_similarity(np.array(s_list), np.array(g_list))\n    elif method == 'greedy':\n        sim_list = []\n        for s, g in zip(samples, ground_truth):\n            s = np.array(s)\n            g = np.array(g).T\n            sim = (np.matmul(s, g)\n                   / np.sqrt(np.matmul(np.sum(s * s, axis=1, keepdims=True), np.sum(g * g, axis=0, keepdims=True))))\n            sim = np.max(sim, axis=0)\n            sim_list.append(np.mean(sim))\n\n        # return np.sum(sim_list)\n        return np.array(sim_list)\n    else:\n        raise NotImplementedError\n"""
TL-ERC/bert_model/util/mask.py,1,"b'import torch\nfrom .convert import to_var\n\n\ndef sequence_mask(sequence_length, max_len=None):\n    """"""\n    Args:\n        sequence_length (Variable, LongTensor) [batch_size]\n            - list of sequence length of each batch\n        max_len (int)\n    Return:\n        masks (bool): [batch_size, max_len]\n            - True if current sequence is valid (not padded), False otherwise\n\n    Ex.\n    sequence length: [3, 2, 1]\n\n    seq_length_expand\n    [[3, 3, 3],\n     [2, 2, 2]\n     [1, 1, 1]]\n\n    seq_range_expand\n    [[0, 1, 2]\n     [0, 1, 2],\n     [0, 1, 2]]\n\n    masks\n    [[True, True, True],\n     [True, True, False],\n     [True, False, False]]\n    """"""\n    if max_len is None:\n        max_len = sequence_length.max()\n    batch_size = sequence_length.size(0)\n\n    # [max_len]\n    seq_range = torch.arange(0, max_len).long()  # [0, 1, ... max_len-1]\n\n    # [batch_size, max_len]\n    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n    seq_range_expand = to_var(seq_range_expand)\n\n    # [batch_size, max_len]\n    seq_length_expand = sequence_length.unsqueeze(1).expand_as(seq_range_expand)\n\n    # [batch_size, max_len]\n    masks = seq_range_expand < seq_length_expand\n\n    return masks\n'"
TL-ERC/bert_model/util/pad.py,6,"b'import torch\nfrom torch.autograd import Variable\nfrom .convert import to_var\n\n\ndef pad(tensor, length):\n\n    if isinstance(tensor, Variable):\n        var = tensor\n        if length > var.size(0):\n            return torch.cat([var,\n                              torch.zeros(length - var.size(0), *var.size()[1:]).cuda()])\n        else:\n            return var\n    else:\n        if length > tensor.size(0):\n            return torch.cat([tensor,\n                              torch.zeros(length - tensor.size(0), *tensor.size()[1:]).cuda()])\n        else:\n            return tensor\n\n\ndef pad_and_pack(tensor_list):\n    length_list = ([t.size(0) for t in tensor_list])\n    max_len = max(length_list)\n    padded = [pad(t, max_len) for t in tensor_list]\n    packed = torch.stack(padded, 0)\n    return packed, length_list\n'"
TL-ERC/bert_model/util/probability.py,6,"b'import torch\nimport numpy as np\nfrom .convert import to_var\n\n\ndef normal_logpdf(x, mean, var):\n    """"""\n    Args:\n        x: (Variable, FloatTensor) [batch_size, dim]\n        mean: (Variable, FloatTensor) [batch_size, dim] or [batch_size] or [1]\n        var: (Variable, FloatTensor) [batch_size, dim]: positive value\n    Return:\n        log_p: (Variable, FloatTensor) [batch_size]\n    """"""\n\n    pi = to_var(torch.FloatTensor([np.pi]))\n    return 0.5 * torch.sum(-torch.log(2.0 * pi) - torch.log(var) - ((x - mean).pow(2) / var), dim=1)\n\n\ndef normal_kl_div(mu1, var1,\n                  mu2=to_var(torch.FloatTensor([0.0])),\n                  var2=to_var(torch.FloatTensor([1.0]))):\n    one = to_var(torch.FloatTensor([1.0]))\n    return torch.sum(0.5 * (torch.log(var2) - torch.log(var1)\n                            + (var1 + (mu1 - mu2).pow(2)) / var2 - one), 1)\n'"
TL-ERC/bert_model/util/tensorboard.py,0,"b'from tensorboardX import SummaryWriter\n\nclass TensorboardWriter(SummaryWriter):\n    def __init__(self, logdir):\n        """"""\n        Extended SummaryWriter Class from tensorboard-pytorch (tensorbaordX)\n        https://github.com/lanpa/tensorboard-pytorch/blob/master/tensorboardX/writer.py\n\n        Internally calls self.file_writer\n        """"""\n        super(TensorboardWriter, self).__init__(logdir)\n        self.logdir = self.file_writer.get_logdir()\n\n    def update_parameters(self, module, step_i):\n        """"""\n        module: nn.Module\n        """"""\n        for name, param in module.named_parameters():\n            self.add_histogram(name, param.clone().cpu().data.numpy(), step_i)\n\n    def update_loss(self, loss, step_i, name=\'loss\'):\n        self.add_scalar(name, loss, step_i)\n\n    def update_histogram(self, values, step_i, name=\'hist\'):\n        self.add_histogram(name, values, step_i)\n'"
TL-ERC/bert_model/util/time_track.py,0,"b""import time\nfrom functools import partial\n\n\ndef base_time_desc_decorator(method, desc='test_description'):\n    def timed(*args, **kwargs):\n\n        # Print Description\n        # print('#' * 50)\n        print(desc)\n        # print('#' * 50 + '\\n')\n\n        # Calculation Runtime\n        start = time.time()\n\n        # Run Method\n        try:\n            result = method(*args, **kwargs)\n        except TypeError:\n            result = method(**kwargs)\n\n        # Print Runtime\n        print('Done! It took {:.2} secs\\n'.format(time.time() - start))\n\n        if result is not None:\n            return result\n\n    return timed\n\n\ndef time_desc_decorator(desc): return partial(base_time_desc_decorator, desc=desc)\n\n\n@time_desc_decorator('this is description')\ndef time_test(arg, kwarg='this is kwarg'):\n    time.sleep(3)\n    print('Inside of time_test')\n    print('printing arg: ', arg)\n    print('printing kwarg: ',  kwarg)\n\n\n@time_desc_decorator('this is second description')\ndef no_arg_method():\n    print('this method has no argument')\n\n\nif __name__ == '__main__':\n    time_test('hello', kwarg=3)\n    time_test(3)\n    no_arg_method()\n"""
TL-ERC/bert_model/util/tokenizer.py,0,"b'import re\n\n\ndef clean_str(string):\n    """"""\n    Tokenization/string cleaning for all datasets except for SST.\n    Every dataset is lower cased except for TREC\n    """"""\n    string = re.sub(r""[^A-Za-z0-9,!?\\\'\\`\\.]"", "" "", string)\n    string = re.sub(r""\\.{3}"", "" ..."", string)\n    string = re.sub(r""\\\'s"", "" \\\'s"", string)\n    string = re.sub(r""\\\'ve"", "" \\\'ve"", string)\n    string = re.sub(r""n\\\'t"", "" n\\\'t"", string)\n    string = re.sub(r""\\\'re"", "" \\\'re"", string)\n    string = re.sub(r""\\\'d"", "" \\\'d"", string)\n    string = re.sub(r""\\\'ll"", "" \\\'ll"", string)\n    string = re.sub(r"","", "" , "", string)\n    string = re.sub(r""!"", "" ! "", string)\n    string = re.sub(r""\\?"", "" ? "", string)\n    string = re.sub(r""\\s{2,}"", "" "", string)\n    return string.strip().lower()\n\n\nclass Tokenizer():\n    def __init__(self, tokenizer=\'whitespace\', clean_string=True):\n        self.clean_string = clean_string\n        tokenizer = tokenizer.lower()\n\n        # Tokenize with whitespace\n        if tokenizer == \'whitespace\':\n            print(\'Loading whitespace tokenizer\')\n            self.tokenize = lambda string: string.strip().split()\n\n        if tokenizer == \'regex\':\n            print(\'Loading regex tokenizer\')\n            import re\n            pattern = r""[A-Z]{2,}(?![a-z])|[A-Z][a-z]+(?=[A-Z])|[\\\'\\w\\-]+""\n            self.tokenize = lambda string: re.findall(pattern, string)\n\n        if tokenizer == \'spacy\':\n            print(\'Loading SpaCy\')\n            import spacy\n            nlp = spacy.load(\'en\')\n            self.tokenize = lambda string: [token.text for token in nlp(string)]\n\n        # Tokenize with punctuations other than periods\n        if tokenizer == \'nltk\':\n            print(\'Loading NLTK word tokenizer\')\n            from nltk import word_tokenize\n\n            self.tokenize = word_tokenize\n\n    def __call__(self, string):\n        if self.clean_string:\n            string = clean_str(string)\n        return self.tokenize(string)\n\n\nif __name__ == \'__main__\':\n    tokenizer = Tokenizer()\n    print(tokenizer(""Hello, how are you doin\'?""))\n\n    tokenizer = Tokenizer(\'spacy\')\n    print(tokenizer(""Hello, how are you doin\'?""))\n'"
TL-ERC/bert_model/util/vocab.py,2,"b'from collections import defaultdict\nimport pickle\nimport torch\nimport numpy as np\nimport os\nfrom torch import Tensor\nfrom torch.autograd import Variable\nfrom nltk import FreqDist\nfrom .convert import to_tensor, to_var\n\n\nPAD_TOKEN = \'<pad>\'\nUNK_TOKEN = \'<unk>\'\nSOS_TOKEN = \'<sos>\'\nEOS_TOKEN = \'<eos>\'\n\nPAD_ID, UNK_ID, SOS_ID, EOS_ID = [0, 1, 2, 3]\n\n\nclass Vocab(object):\n    def __init__(self, tokenizer=None, max_size=None, min_freq=1):\n        """"""Basic Vocabulary object""""""\n\n        self.vocab_size = 0\n        self.freqdist = FreqDist()\n        self.tokenizer = tokenizer\n\n    def update(self, glove_dir, max_size=None, min_freq=1):\n        """"""\n        Initialize id2word & word2id based on self.freqdist\n        max_size include 4 special tokens\n        """"""\n\n        # {0: \'<pad>\', 1: \'<unk>\', 2: \'<sos>\', 3: \'<eos>\'}\n        self.id2word = {\n            PAD_ID: PAD_TOKEN, UNK_ID: UNK_TOKEN,\n            SOS_ID: SOS_TOKEN, EOS_ID: EOS_TOKEN\n        }\n        # {\'<pad>\': 0, \'<unk>\': 1, \'<sos>\': 2, \'<eos>\': 3}\n        self.word2id = defaultdict(lambda: UNK_ID)  # Not in vocab => return UNK\n        self.word2id.update({\n            PAD_TOKEN: PAD_ID, UNK_TOKEN: UNK_ID,\n            SOS_TOKEN: SOS_ID, EOS_TOKEN: EOS_ID\n        })\n        # self.word2id = {\n        #     PAD_TOKEN: PAD_ID, UNK_TOKEN: UNK_ID,\n        #     SOS_TOKEN: SOS_ID, EOS_TOKEN: EOS_ID\n        # }\n\n        vocab_size = 4\n        min_freq = max(min_freq, 1)\n\n        # Reset frequencies of special tokens\n        # [...(\'<eos>\', 0), (\'<pad>\', 0), (\'<sos>\', 0), (\'<unk>\', 0)]\n        freqdist = self.freqdist.copy()\n        special_freqdist = {token: freqdist[token]\n                            for token in [PAD_TOKEN, UNK_TOKEN, SOS_TOKEN, EOS_TOKEN]}\n        freqdist.subtract(special_freqdist)\n\n        # Sort: by frequency, then alphabetically\n        # Ex) freqdist = { \'a\': 4,   \'b\': 5,   \'c\': 3 }\n        #  =>   sorted = [(\'b\', 5), (\'a\', 4), (\'c\', 3)]\n        sorted_frequency_counter = sorted(freqdist.items(), key=lambda k_v: k_v[0])\n        sorted_frequency_counter.sort(key=lambda k_v: k_v[1], reverse=True)\n\n        # Load glove vector\n        word_emb_dict = self.get_glove_emb(glove_dir)\n\n        for word, freq in sorted_frequency_counter:\n\n            if freq < min_freq or vocab_size == max_size:\n                break\n            self.id2word[vocab_size] = word\n            self.word2id[word] = vocab_size\n            vocab_size += 1\n\n        self.vocab_size = vocab_size\n\n\n        # Create embedding matrix\n        self.embedding_matrix = embedding_matrix = np.zeros((self.vocab_size, 300))\n\n        for word, ind in self.word2id.items():\n            if word.lower() in word_emb_dict:\n                embedding_matrix[self.word2id[word]] = word_emb_dict[word.lower()]\n            else:\n                embedding_matrix[self.word2id[word]] = np.random.uniform(-0.25, 0.25, 300)\n\n    def get_glove_emb(self, GLOVE_DIR):\n        embeddings_index = {}\n        f = open(os.path.join(GLOVE_DIR, \'glove.840B.300d.txt\'), \'rb\')\n        for line in f:\n            values = line.split()\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype=\'float32\')\n            embeddings_index[word.decode().lower()] = coefs\n        f.close()\n        return embeddings_index\n\n\n    def __len__(self):\n        return len(self.id2word)\n\n\n    def load(self, word2id_path=None, id2word_path=None, word_emb_path=None):\n        if word2id_path:\n            with open(word2id_path, \'rb\') as f:\n                word2id = pickle.load(f)\n            # Can\'t pickle lambda function\n            self.word2id = defaultdict(lambda: UNK_ID)\n            self.word2id.update(word2id)\n            self.vocab_size = len(self.word2id)\n\n        if id2word_path:\n            with open(id2word_path, \'rb\') as f:\n                id2word = pickle.load(f)\n            self.id2word = id2word\n        \n        if word_emb_path:\n            with open(word_emb_path, \'rb\') as f:\n                embedding_matrix = pickle.load(f)\n            self.embedding_matrix = embedding_matrix\n\n    def add_word(self, word):\n        assert isinstance(word, str), \'Input should be str\'\n        self.freqdist.update([word])\n\n    def add_sentence(self, sentence, tokenized=False):\n        if not tokenized:\n            sentence = self.tokenizer(sentence)\n        for word in sentence:\n            self.add_word(word)\n\n    def add_dataframe(self, conversation_df, tokenized=True):\n        for conversation in conversation_df:\n            for sentence in conversation:\n                self.add_sentence(sentence, tokenized=tokenized)\n\n    def pickle(self, word2id_path, id2word_path, word_emb_path):\n        with open(word2id_path, \'wb\') as f:\n            pickle.dump(dict(self.word2id), f)\n\n        with open(id2word_path, \'wb\') as f:\n            pickle.dump(self.id2word, f)\n\n        with open(word_emb_path, \'wb\') as f:\n            pickle.dump(self.embedding_matrix, f)\n\n    def to_list(self, list_like):\n        """"""Convert list-like containers to list""""""\n        if isinstance(list_like, list):\n            return list_like\n\n        if isinstance(list_like, Variable):\n            return list(to_tensor(list_like).numpy())\n        elif isinstance(list_like, Tensor):\n            return list(list_like.numpy())\n\n    def id2sent(self, id_list):\n        """"""list of id => list of tokens (Single sentence)""""""\n        id_list = self.to_list(id_list)\n        sentence = []\n        for id in id_list:\n            word = self.id2word[id]\n            if word not in [EOS_TOKEN, SOS_TOKEN, PAD_TOKEN]:\n                sentence.append(word)\n            if word == EOS_TOKEN:\n                break\n        return sentence\n\n    def sent2id(self, sentence, var=False):\n        """"""list of tokens => list of id (Single sentence)""""""\n        id_list = [self.word2id[word] for word in sentence]\n        if var:\n            id_list = to_var(torch.LongTensor(id_list), eval=True)\n        return id_list\n\n    def decode(self, id_list):\n        sentence = self.id2sent(id_list)\n        return \' \'.join(sentence)\n'"
