file_path,api_count,code
predict.py,0,"b'import argparse\nfrom data_process.data_loader import *\nfrom data_process.transform import *\nfrom loss.bce_losses import *\nfrom loss.cyclic_lr import *\nfrom loss.lovasz_losses import *\nfrom utils import create_submission\nfrom loss.metric import do_kaggle_metric\nimport time\nimport datetime\n\nfrom train import SingleModelSolver\nfrom utils import *\n\ndef votingAllCycle(root_dir , model_list, save_name):\n    print(\'voting ensemble\')\n\n    def get_predict_dict(csv_list):\n        # vote_thres = int(len(csv_list) / 2) + 1\n        vote_dict = {}\n        for csv in csv_list:\n            csv_dict = decode_csv(csv_name=csv)\n            for id in csv_dict:\n                if id in vote_dict:\n                    vote_dict[id] += csv_dict[id]\n                else:\n                    vote_dict[id] = csv_dict[id]\n            csv_dict.clear()\n\n        for id in vote_dict:\n            vote_dict[id] = vote_dict[id] / (len(csv_list)*1.0)\n        return vote_dict\n\n    model_list = [os.path.join(root_dir, tmp) for tmp in model_list]\n\n    vote_dict_list = []\n    for dir_tmp in model_list:\n        print(os.path.split(dir_tmp)[1])\n        csv_list = create_csv_lists_recursive(dir_tmp)\n        vote_dict_tmp = get_predict_dict(csv_list)\n        vote_dict_list.append(vote_dict_tmp)\n\n\n    vote_dict = {}\n    for dict in vote_dict_list:\n        for id in dict:\n            if id in vote_dict:\n                vote_dict[id] += dict[id] / len(vote_dict_list)\n            else:\n                vote_dict[id] = dict[id] / len(vote_dict_list)\n        dict.clear()\n\n    out = []\n    for id in vote_dict:\n        vote_dict[id][vote_dict[id] >= 0.5] = 1\n        vote_dict[id][vote_dict[id] < 0.5] = 0\n        vote_dict[id] = vote_dict[id].astype(np.uint8)\n        out.append([id, vote_dict[id]])\n\n    submission = create_submission(out)\n    submission.to_csv(os.path.join(save_name), index=None)\n\n    print(\'done\')\n\ndef ensemble_models(root_dir ,model_list , save_name):\n    votingAllCycle(root_dir, model_list, save_name)\n    submission_apply_jigsaw_postprocessing(save_name)\n\ndef main(config):\n    if config.mode == \'InferModel10Fold\':\n        solver = SingleModelSolver(config)\n        for i in range(10):\n            solver.infer_fold_all_Cycle(i)\n\n    if config.mode == \'EnsembleModels\':\n        print(config.model_name_list)\n        model_name_list = config.model_name_list.split(\',\')\n        print(model_name_list)\n\n\n        ensemble_models(config.model_save_path, model_name_list, config.save_sub_name)\n\n    if config.mode == \'SolveJigsawPuzzles\':\n        from jigsaw.jigsaw_puzzles import solve_jigsaw_puzzles\n        solve_jigsaw_puzzles(config.jigsaw_dir)\n\nif __name__ == \'__main__\':\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""0,1,2,3""\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--mode\', type=str, default=\'InferModel10Fold\', choices=[\'InferModel10Fold\',\n                                                                                 \'EnsembleModels\',\n                                                                                 \'SolveJigsawPuzzles\'])\n\n    parser.add_argument(\'--model_name_list\', type=str, default= r\'model_34\', required = True)\n    parser.add_argument(\'--save_sub_name\', type=str, default= \'model_34_fold0.csv\')\n    parser.add_argument(\'--train_fold_index\', type=int, default=0)\n    parser.add_argument(\'--model\', type=str, default=\'model_34\')\n    parser.add_argument(\'--model_name\', type=str, default=\'model_34\')\n\n    parser.add_argument(\'--image_size\', type=int, default=128)\n    parser.add_argument(\'--batch_size\', type=int, default=16*4)\n\n    # Test settings\n    parser.add_argument(\'--log_path\', type=str, default=\'logs\')\n    parser.add_argument(\'--model_save_path\', type=str, default=\'models\')\n    parser.add_argument(\'--sample_path\', type=str, default=\'samples\')\n    parser.add_argument(\'--result_path\', type=str, default=\'results\')\n    parser.add_argument(\'--jigsaw_dir\', type=str, default=\'./jigsaw\')\n\n    # no use\n    parser.add_argument(\'--pseudo_csv\', type=str, default = None)\n    parser.add_argument(\'--pseudo_split\', type=int, default = -1)\n    parser.add_argument(\'--log_step\', type=int, default=10)\n    parser.add_argument(\'--sample_step\', type=int, default=10)\n    parser.add_argument(\'--model_save_step\', type=int, default=20000)\n    parser.add_argument(\'--pretrained_model\', type=str, default=None)\n    parser.add_argument(\'--lr\', type=float, default=0.01)\n    parser.add_argument(\'--cycle_num\', type=int, default=7)\n    parser.add_argument(\'--cycle_inter\', type=int, default=50)\n    parser.add_argument(\'--dice_bce_pretrain_epochs\', type=int, default=10)\n    parser.add_argument(\'--dice_weight\', type=float, default=0.5)\n    parser.add_argument(\'--bce_weight\', type=float, default=0.9)\n    parser.add_argument(\'--num_workers\', type=int, default=16)\n\n    config = parser.parse_args()\n    print(config)\n    main(config)\n\n\n\n'"
prepare_data.py,0,"b""from utils import *\n\ndef save_train_mask(train_csv, save_dir):\n    print(train_csv)\n    print(save_dir)\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    dict = decode_csv(train_csv)\n    for item in dict:\n        image = dict[item]*255\n        cv2.imwrite(os.path.join(save_dir, item+'.png'), image)\n    print('done')\n\nif __name__=='__main__':\n    parser = argparse.ArgumentParser(description='data prepare')\n    parser.add_argument('--train_csv_path', default = r'/data1/shentao/DATA/Kaggle/Salt/Kaggle_salt/train.csv',required = True, help='train.csv path')\n    parser.add_argument('--train_mask_save_dir', default = r'/data1/shentao/DATA/Kaggle/Salt/Kaggle_salt/train_mask_try', required = True,help='train mask save_dir')\n    args = parser.parse_args()\n    save_train_mask(args.train_csv_path, args.train_mask_save_dir)\n\n"""
train.py,22,"b'import argparse\nfrom data_process.data_loader import *\nfrom data_process.transform import *\nfrom loss.bce_losses import *\nfrom loss.cyclic_lr import *\nfrom loss.lovasz_losses import *\n\nfrom model.model import model34_DeepSupervion,\\\n                        model50A_DeepSupervion,\\\n                        model50A_slim_DeepSupervion,\\\n                        model101A_DeepSupervion,\\\n                        model101B_DeepSupervion,\\\n                        model152_DeepSupervion,\\\n                        model154_DeepSupervion\n\nfrom utils import create_submission\nfrom loss.metric import do_kaggle_metric\nimport time\nimport datetime\n\nclass SingleModelSolver(object):\n    def __init__(self, config):\n        self.model_name = config.model_name\n        self.model = config.model\n\n        self.dice_weight = config.dice_weight\n        self.bce_weight = config.bce_weight\n\n        # Model hyper-parameters\n        self.image_size = config.image_size\n\n        # Hyper-parameteres\n        self.g_lr = config.lr\n        self.cycle_num = config.cycle_num\n        self.cycle_inter = config.cycle_inter\n        self.dice_bce_pretrain_epochs = config.dice_bce_pretrain_epochs\n\n        self.batch_size = config.batch_size\n        self.pretrained_model = config.pretrained_model\n\n        #pseudo label\n        self.pseudo_csv = config.pseudo_csv\n        self.pseudo_split = config.pseudo_split\n\n        # Path\n        self.log_path = os.path.join(\'./models\', self.model_name, config.log_path)\n        self.sample_path = os.path.join(\'./models\', self.model_name, config.sample_path)\n        self.model_save_path = os.path.join(\'./models\', self.model_name, config.model_save_path)\n        self.result_path = os.path.join(\'./models\', self.model_name, config.result_path)\n        # Create directories if not exist\n        if not os.path.exists(self.log_path):\n            os.makedirs(self.log_path)\n        if not os.path.exists(self.model_save_path):\n            os.makedirs(self.model_save_path)\n        if not os.path.exists(self.sample_path):\n            os.makedirs(self.sample_path)\n        if not os.path.exists(self.result_path):\n            os.makedirs(self.result_path)\n\n        # Step size\n        self.log_step = config.log_step\n        self.sample_step = config.sample_step\n        self.model_save_step = config.model_save_step\n\n        # Build tensorboard if use\n        self.build_model()\n        self.load_pretrained_model(config.train_fold_index)\n\n    def build_model(self):\n\n        if self.model == \'model_34\':\n            self.G = model34_DeepSupervion()\n\n        elif self.model == \'model_50A\':\n            self.G = model50A_DeepSupervion()\n\n        elif self.model == \'model_50A_slim\':\n            self.G = model50A_slim_DeepSupervion()\n\n        elif self.model == \'model_101A\':\n            self.G = model101A_DeepSupervion()\n\n        elif self.model == \'model_101B\':\n            self.G = model101B_DeepSupervion()\n\n        elif self.model == \'model_152\':\n            self.G = model152_DeepSupervion()\n\n        elif self.model == \'model_154\':\n            self.G = model154_DeepSupervion()\n\n        self.g_optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, self.G.parameters()),\n                                           self.g_lr, weight_decay=0.0002, momentum=0.9)\n        self.print_network(self.G, \'G\')\n        if torch.cuda.is_available():\n            self.G = torch.nn.DataParallel(self.G)\n            self.G.cuda()\n\n    def print_network(self, model, name):\n        num_params = 0\n        for p in model.parameters():\n            num_params += p.numel()\n        print(name)\n        print(model)\n        print(""The number of parameters: {}"".format(num_params))\n\n    def load_pretrained_model(self, fold_index, mode = None, Cycle=None):\n\n            if mode == None:\n                if os.path.exists(os.path.join(self.model_save_path, \'fold_\' + str(fold_index),\n                                               \'{}_G.pth\'.format(self.pretrained_model))):\n                    self.G.load_state_dict(torch.load(os.path.join(self.model_save_path,\'fold_\' + str(fold_index),\n                                                                   \'{}_G.pth\'.format(self.pretrained_model))))\n                    print(\'loaded trained G models fold: {} (step: {})..!\'.format(fold_index, self.pretrained_model))\n\n            elif mode == \'max_map\':\n                if Cycle is None:\n                    pth = os.path.join(self.model_save_path,\'fold_\' + str(fold_index),\n                                      \'Lsoftmax_maxMap_G.pth\')\n                else:\n                    pth = os.path.join(self.model_save_path,\'fold_\' + str(fold_index),\n                                      \'Cycle_\'+str(Cycle)+\'_Lsoftmax_maxMap_G.pth\')\n\n                # print(pth)\n\n                if os.path.exists(pth):\n                    self.G.load_state_dict(torch.load(pth))\n                    print(\'loaded trained G models fold: {} (step: {})..!\'.format(fold_index,pth))\n\n            elif mode == \'min_loss\':\n                if Cycle is None:\n                    pth = os.path.join(self.model_save_path, \'fold_\' + str(fold_index),\n                                       \'Lsoftmax_minValidLoss_G.pth\')\n                else:\n                    pth = os.path.join(self.model_save_path, \'fold_\' + str(fold_index),\n                                       \'Cycle_\' + str(Cycle) + \'_Lsoftmax_minValidLoss_G.pth\')\n\n                if os.path.exists(pth):\n                    self.G.load_state_dict(torch.load(pth))\n                    print(\'loaded trained G models fold: {} (step: {})..!\'.format(fold_index,pth))\n\n    def update_lr(self, g_lr):\n        for param_group in self.g_optimizer.param_groups:\n            param_group[\'lr\'] = g_lr\n\n    def to_var(self, x, volatile=False):\n        if torch.cuda.is_available():\n            x = x.cuda()\n        return Variable(x, volatile=volatile)\n\n    def criterion(self, logits, label):\n        logits = logits.squeeze(1)\n        label = label.squeeze(1)\n        loss = lovasz_hinge(logits, label, per_image=True, ignore=None)\n        return loss\n\n    def train_fold(self, fold_index, aug_list):\n        CE = torch.nn.CrossEntropyLoss()\n\n        if not os.path.exists(os.path.join(self.model_save_path,\'fold_\'+str(fold_index))):\n            os.makedirs(os.path.join(self.model_save_path,\'fold_\'+str(fold_index)))\n\n        print(\'train loader!!!\')\n        data_loader = get_foldloader(self.image_size,\n                                     self.batch_size,\n                                     fold_index, aug_list,\n                                     mode= \'train\',\n                                     pseudo_csv=self.pseudo_csv,\n                                     pseudo_index=self.pseudo_split)\n        print(\'val loader!!!\')\n        val_loader = get_foldloader(self.image_size, 1, fold_index, mode=\'val\')\n\n        iters_per_epoch = len(data_loader)\n\n        for init_index in range(self.dice_bce_pretrain_epochs):\n            for i, (images, labels, is_empty) in enumerate(data_loader):\n                inputs = self.to_var(images)\n                labels = self.to_var(labels)\n                class_lbls = self.to_var(torch.LongTensor(is_empty))\n                binary_logits, no_empty_logits, final_logits = self.G(inputs)\n                bce_loss_final = mixed_dice_bce_loss(final_logits, labels, dice_weight=self.dice_weight, bce_weight=self.bce_weight)\n                class_loss = CE(binary_logits, class_lbls)\n\n                non_empty = []\n                for c in range(len(is_empty)):\n                    if is_empty[c] == 0:\n                        non_empty.append(c)\n\n                has_empty_nonempty = False\n                if len(non_empty) * len(is_empty) > 0:\n                    has_empty_nonempty = True\n\n                all_loss = bce_loss_final + 0.05 * class_loss\n\n                loss = {}\n                loss[\'loss_seg\'] = bce_loss_final.data[0]\n                loss[\'loss_classifier\'] = class_loss.data[0]\n\n                if has_empty_nonempty:\n                    indices = self.to_var(torch.LongTensor(non_empty))\n                    y_non_empty = torch.index_select(no_empty_logits, 0, indices)\n                    mask_non_empty = torch.index_select(labels, 0, indices)\n                    loss_no_empty = mixed_dice_bce_loss(y_non_empty, mask_non_empty, dice_weight=self.dice_weight, bce_weight=self.bce_weight)\n                    all_loss += 0.50 * loss_no_empty\n                    loss[\'loss_seg_noempty\'] = loss_no_empty.data[0]\n\n                self.g_optimizer.zero_grad()\n                all_loss.backward()\n                self.g_optimizer.step()\n\n                # Print out log info\n                if (i+1) % 10 == 0:\n                    lr = self.g_optimizer.param_groups[0][\'lr\']\n                    log = ""{} FOLD: {}, BCE+DICE pretrain Epoch [{}/{}], lr {:.4f}"".format(\n                        self.model_name, fold_index, init_index, 10, lr)\n                    for tag, value in loss.items():\n                        log += "", {}: {:.4f}"".format(tag, value)\n                    print(log)\n\n\n        sgdr = CosineAnnealingLR_with_Restart(self.g_optimizer,\n                                              T_max=self.cycle_inter,\n                                              T_mult=1,\n                                              model=self.G,\n                                              out_dir=\'../input/\',\n                                              take_snapshot=False,\n                                              eta_min=1e-3)\n\n        # Start training\n        start_time = time.time()\n        for cycle_index in range(self.cycle_num):\n            print(\'cycle index: \'+ str(cycle_index))\n            valid_loss_plot = []\n            max_map_plot = []\n\n            for e in range(0, self.cycle_inter):\n                sgdr.step()\n                lr = self.g_optimizer.param_groups[0][\'lr\']\n                print(\'change learning rate into: {:.4f}\'.format(lr))\n\n                for i, (images, labels, is_empty) in enumerate(data_loader):\n                    # all images\n                    inputs = self.to_var(images)\n                    labels = self.to_var(labels)\n                    class_lbls = self.to_var(torch.LongTensor(is_empty))\n                    binary_logits, no_empty_logits, final_logits = self.G(inputs)\n                    loss_final = self.criterion(final_logits, labels)\n                    class_loss = CE(binary_logits, class_lbls)\n\n                    non_empty = []\n                    for c in range(len(is_empty)):\n                        if is_empty[c] == 0:\n                            non_empty.append(c)\n\n                    has_empty_nonempty = False\n                    if len(non_empty) * len(is_empty) > 0:\n                        has_empty_nonempty = True\n\n                    all_loss = loss_final +  0.05 * class_loss\n\n                    loss = {}\n                    loss[\'loss_seg\'] = loss_final.data[0]\n                    loss[\'loss_classifier\'] = class_loss.data[0]\n\n                    if has_empty_nonempty:\n                        indices = self.to_var(torch.LongTensor(non_empty))\n                        y_non_empty = torch.index_select(no_empty_logits, 0, indices)\n                        mask_non_empty = torch.index_select(labels, 0, indices)\n                        loss_no_empty = self.criterion(y_non_empty, mask_non_empty)\n                        all_loss +=  0.50 * loss_no_empty\n                        loss[\'loss_seg_noempty\'] = loss_no_empty.data[0]\n\n                    self.g_optimizer.zero_grad()\n                    all_loss.backward()\n                    self.g_optimizer.step()\n\n                    # Print out log info\n                    if (i+1) % self.log_step == 0:\n                        elapsed = time.time() - start_time\n                        elapsed = str(datetime.timedelta(seconds=elapsed))\n                        lr = self.g_optimizer.param_groups[0][\'lr\']\n                        log = ""{} FOLD: {}, Cycle: {}, Elapsed [{}], Epoch [{}/{}], Iter [{}/{}], lr {:.4f}"".format(\n                            self.model_name, fold_index, cycle_index, elapsed, e+1, self.cycle_inter, i+1, iters_per_epoch, lr)\n                        for tag, value in loss.items():\n                            log += "", {}: {:.4f}"".format(tag, value)\n                        print(log)\n\n                if e + 1 >= 20 and (e+1) % 5 == 0:\n                    valid_loss, max_map, max_thres = self.val_TTA(fold_index, val_loader, is_load=False)\n\n                    if len(valid_loss_plot) == 0 or valid_loss < min(valid_loss_plot):\n                        print(\'save min valid loss model\')\n                        torch.save(self.G.state_dict(), os.path.join(self.model_save_path,\n                                                                     \'fold_\' + str(fold_index),\n                                                                     \'Cycle_\'+str(cycle_index)+\'_Lsoftmax_minValidLoss_G.pth\'))\n\n                        f = open(os.path.join(self.model_save_path, \'fold_\' + str(fold_index), \'Cycle_\'+str(cycle_index)+\'_Lsoftmax_min_valid_loss.txt\'), \'w\')\n                        f.write(str(max_map) + \' \' + str(max_thres) + \' \' + str(valid_loss) + \' epoch: \' + str(e))\n                        f.close()\n\n                    if len(valid_loss_plot) == 0 or max_map > max(max_map_plot):\n                        print(\'save max map model\')\n                        torch.save(self.G.state_dict(), os.path.join(self.model_save_path,\n                                                                     \'fold_\' + str(fold_index),\n                                                                     \'Cycle_\'+str(cycle_index)+\'_Lsoftmax_maxMap_G.pth\'))\n\n                        f = open(os.path.join(self.model_save_path, \'fold_\' + str(fold_index), \'Cycle_\'+str(cycle_index)+\'_Lsoftmax_max_map.txt\'), \'w\')\n                        f.write(str(max_map) + \' \' + str(max_thres) + \' \' + str(valid_loss) + \' epoch: \' + str(e))\n                        f.close()\n\n                    valid_loss_plot.append(valid_loss)\n                    max_map_plot.append(max_map)\n\n    def val_TTA(self, fold_index, val_loader, is_load = False, mode=None, Cycle = None):\n        if fold_index<0:\n            return\n\n        if is_load:\n            self.load_pretrained_model(fold_index, mode=mode,Cycle=Cycle)\n        self.G.eval()\n\n        loss = 0\n        t = 0\n\n        output_list = []\n        labels_list = []\n        for i, (images, labels, _) in enumerate(val_loader):\n                img1 = images.numpy()\n                img2 = img1[:, :, :, ::-1]\n                batch_size = img1.shape[0]\n                img_all = np.concatenate([img1, img2])\n                images = torch.FloatTensor(img_all)\n\n                inputs = self.to_var(images)\n                _,_, output = self.G(inputs)\n\n                output = output.data.cpu().numpy()\n                mask = output[0:batch_size*2]\n                output = mask[0:batch_size] + mask[batch_size:batch_size*2][:, :, :, ::-1]\n\n                output = output / 2.0\n                labels = labels.numpy()\n\n                output = output.transpose(2, 3, 0, 1).reshape([self.image_size,self.image_size,-1])\n                labels = labels.transpose(2, 3, 0, 1).reshape([self.image_size,self.image_size,-1])\n\n                if self.image_size == 128:\n                    output = center_corp(output,self.image_size, crop_size=101)\n                    labels = center_corp(labels, self.image_size, crop_size=101)\n                elif self.image_size == 160:\n                    output = center_corp(output,self.image_size, crop_size=101)\n                    labels = center_corp(labels, self.image_size, crop_size=101)\n                elif self.image_size == 256:\n                    output = center_corp(output,self.image_size, crop_size=202)\n                    labels = center_corp(labels, self.image_size, crop_size=202)\n\n                output = cv2.resize(output, (101, 101)).reshape([101, 101, -1])\n                labels = cv2.resize(labels, (101, 101)).reshape([101, 101, -1])\n\n                output_list.append(output)\n                labels_list.append(labels)\n\n                output = output.transpose(2, 0, 1).reshape([batch_size, 1,101, 101])\n                labels = labels.transpose(2, 0, 1).reshape([batch_size, 1,101, 101])\n\n                output = torch.FloatTensor(output)\n                labels = torch.FloatTensor(labels)\n\n                labels = self.to_var(labels)\n                output = self.to_var(output)\n\n                bce_loss = self.criterion(output, labels)\n                loss += bce_loss.data[0]\n                t += 1.0\n\n        valid_loss = loss / t\n        output = np.concatenate(output_list, axis=2)\n        labels = np.concatenate(labels_list, axis=2)\n\n        output = output.transpose(2, 0, 1)\n        labels = labels.transpose(2, 0, 1)\n\n        threshold = np.arange(-0.5, 0.5, 0.0125)\n        def get_max_map(output_, labels_):\n            precision_list = []\n            for thres in threshold:\n                precision, result, _ = do_kaggle_metric(output_, labels_, threshold=thres)\n                precision = precision.mean()\n                precision_list.append(precision)\n\n            max_map = max(precision_list)\n            max_index = np.argmax(np.asarray(precision_list))\n            print(""max map: {:.4f} at thres:{:.4f}"".format(max_map, threshold[max_index]))\n            return max_map, max_index\n\n        max_map, max_index = get_max_map(output, labels)\n\n        log = ""{} FOLD: {} valid loss: {:.4f}"".format(self.model_name, fold_index, valid_loss)\n        print(log)\n\n        self.G.train()\n        return valid_loss, max_map, threshold[max_index]\n\n    def get_infer_TTA(self, fold_index, thres):\n        self.G.eval()\n        test_loader = get_foldloader(self.image_size, self.batch_size/2, 0, mode=\'test\')\n\n        out = []\n        for i, (id , images) in enumerate(test_loader):\n            img1 = images.numpy()\n            img2 = img1[:, :, :, ::-1]\n            batch_size = img1.shape[0]\n            img_all = np.concatenate([img1, img2])\n            images = torch.FloatTensor(img_all)\n\n            inputs = self.to_var(images)\n            _, _, output = self.G(inputs)\n\n            output = output.data.cpu().numpy()\n            mask = output[0:batch_size * 2]\n            output = mask[0:batch_size] + mask[batch_size:batch_size * 2][:, :, :, ::-1]\n            output = output / 2.0\n\n            output = output.transpose(2, 3, 0, 1).reshape([self.image_size, self.image_size, batch_size])\n\n            if self.image_size == 128:\n                output = center_corp(output, self.image_size, crop_size=101)\n\n            output = cv2.resize(output, (101, 101), cv2.INTER_CUBIC)\n            output[output >= thres] = 1.0\n            output[output < thres] = 0.0\n\n            output = output.transpose(2, 0, 1)\n            output = output.reshape([batch_size, 101, 101]).astype(np.uint8)\n\n            for id_index in range(batch_size):\n                out.append([id[id_index], output[id_index].reshape([101,101])])\n\n            if i%1000 == 0 and i>0:\n                print(self.model_name + \' fold index: \'+str(fold_index) +\' \'+str(i))\n\n        return out\n\n    def infer_fold_TTA(self, fold_index, mode = \'max_map\', Cycle = None):\n\n        print(mode)\n        val_loader = get_foldloader(self.image_size, self.batch_size/2, fold_index, mode=\'val\')\n        _, max_map, thres = self.val_TTA(fold_index, val_loader, is_load = True, mode = mode, Cycle = Cycle)\n\n        if fold_index<0:\n            return\n\n        infer = self.get_infer_TTA(fold_index, thres)\n\n        if Cycle is None:\n            name_tmp = \'fold_{}_TTA_{}{:.3f}at{:.3f}.csv\'.format(fold_index,mode,max_map,thres)\n        else:\n            name_tmp = \'fold_{}_Cycle_{}_TTA_{}{:.3f}at{:.3f}.csv\'.format(fold_index, Cycle, mode, max_map, thres)\n\n        if not os.path.exists(os.path.join(self.result_path, \'fold_\' + str(fold_index))):\n            os.makedirs(os.path.join(self.result_path, \'fold_\' + str(fold_index)))\n\n        output_name = os.path.join(self.result_path, \'fold_\' + str(fold_index), name_tmp)\n        submission = create_submission(infer)\n        submission.to_csv(output_name, index=None)\n\n    def infer_fold_all_Cycle(self, fold_index, mode = \'max_map\'):\n        for i in range(self.cycle_num):\n            self.infer_fold_TTA(fold_index, mode, Cycle=i)\n\ndef main(config, aug_list):\n    # cudnn.benchmark = True\n    if config.mode == \'train\':\n        solver = SingleModelSolver(config)\n        solver.train_fold(config.train_fold_index, aug_list)\n    if config.mode == \'test\':\n        solver = SingleModelSolver(config)\n        solver.infer_fold_all_Cycle(config.train_fold_index)\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""\n    parser.add_argument(\'--train_fold_index\', type=int, default=0)\n    parser.add_argument(\'--model\', type=str, default=\'model_34\')\n    parser.add_argument(\'--model_name\', type=str, default=\'model_34\')\n    parser.add_argument(\'--image_size\', type=int, default=128)\n    parser.add_argument(\'--batch_size\', type=int, default=16)\n\n\n    aug_list = [\'flip_lr\']\n    parser.add_argument(\'--mode\', type=str, default=\'train\', choices=[\'train\', \'test\'])\n    parser.add_argument(\'--pretrained_model\', type=str, default=None)\n\n    # pseudo label\n    parser.add_argument(\'--pseudo_csv\', type=str, default=None)\n    parser.add_argument(\'--pseudo_split\', type=int, default=0)\n\n    parser.add_argument(\'--lr\', type=float, default=0.01)\n    parser.add_argument(\'--cycle_num\', type=int, default=7)\n    parser.add_argument(\'--cycle_inter\', type=int, default=50)\n\n    parser.add_argument(\'--dice_bce_pretrain_epochs\', type=int, default=10)\n    parser.add_argument(\'--dice_weight\', type=float, default=0.5)\n    parser.add_argument(\'--bce_weight\', type=float, default=0.9)\n    parser.add_argument(\'--num_workers\', type=int, default=16)\n\n    # Test settings\n    parser.add_argument(\'--log_path\', type=str, default=\'logs\')\n    parser.add_argument(\'--model_save_path\', type=str, default=\'models\')\n    parser.add_argument(\'--sample_path\', type=str, default=\'samples\')\n    parser.add_argument(\'--result_path\', type=str, default=\'results\')\n\n    # Step size\n    parser.add_argument(\'--log_step\', type=int, default=10)\n    parser.add_argument(\'--sample_step\', type=int, default=10)\n    parser.add_argument(\'--model_save_step\', type=int, default=20000)\n\n    config = parser.parse_args()\n    print(config)\n    main(config, aug_list)\n'"
utils.py,3,"b'import logging\nimport os\n# import pathlib\nimport random\nimport sys\nimport time\nfrom itertools import chain\nfrom collections import Iterable\n\n# from deepsense import neptune\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom PIL import Image\nimport yaml\nfrom imgaug import augmenters as iaa\nimport imgaug as ia\n\n# NEPTUNE_CONFIG_PATH = str(pathlib.Path(__file__).resolve().parents[1] / \'configs\' / \'neptune.yaml\')\n\nimport argparse\nfrom utils import *\nimport numpy as np\nimport os\nimport glob\nimport cv2\nfrom jigsaw.handcraft_ruls_postprocessing import submission_apply_jigsaw_postprocessing\n\ndef save_csv_images(csv_path, save_path):\n    dict = decode_csv(csv_name=csv_path)\n\n    for id in dict:\n        id_img = dict[id]*255\n        cv2.imwrite(os.path.join(save_path,id+\'.png\'),id_img)\n\ndef create_csv_lists(image_dir, printable = True):\n    if not os.path.exists(image_dir):\n        print(""Image directory \'"" + image_dir + ""\' not found."")\n        return None\n\n    file_list = []\n    file_glob = os.path.join(image_dir,\'*.\' + \'csv\')\n    file_list.extend(glob.glob(file_glob))\n    if printable:\n        print(len(file_list))\n    return file_list\n\ndef create_csv_lists_recursive(image_dir):\n    total_list = []\n    for i in os.walk(image_dir):\n        cur_path = i[0]\n        list = create_csv_lists(cur_path,printable=False)\n        total_list.extend(list)\n\n    print(len(total_list))\n    return total_list\n\ndef do_length_encode(x):\n    bs = np.where(x.T.flatten())[0]\n\n    rle = []\n    prev = -2\n    for b in bs:\n        if (b>prev+1): rle.extend((b + 1, 0))\n        rle[-1] += 1\n        prev = b\n\n    #https://www.kaggle.com/c/data-science-bowl-2018/discussion/48561#\n    #if len(rle)!=0 and rle[-1]+rle[-2] == x.size:\n    #    rle[-2] = rle[-2] -1\n\n    rle = \' \'.join([str(r) for r in rle])\n    return rle\n\nfrom math import isnan\ndef do_length_decode(rle, H, W, fill_value=255):\n    mask = np.zeros((H,W), np.uint8)\n    if type(rle).__name__ == \'float\': return mask\n\n    mask = mask.reshape(-1)\n    rle = np.array([int(s) for s in rle.split(\' \')]).reshape(-1, 2)\n    for r in rle:\n        start = r[0]-1\n        end = start + r[1]\n        mask[start : end] = fill_value\n    mask = mask.reshape(W, H).T   # H, W need to swap as transposing.\n    return mask\n\ndef decode_csv(csv_name):\n    import pandas as pd\n    data = pd.read_csv(csv_name)\n    id = data[\'id\']\n    rle_mask = data[\'rle_mask\']\n\n    dict = {}\n    for id, rle in zip(id,rle_mask):\n        tmp = do_length_decode(rle, 101, 101, fill_value=1)\n        dict[id] = tmp\n\n    return dict\n\ndef save_id_fea(predict_dict, save_dir):\n    for id in predict_dict:\n        output_mat = predict_dict[id].astype(np.float32)\n        np.save(os.path.join(save_dir,id), output_mat)\n\ndef state_dict_remove_moudle(moudle_state_dict, model):\n    state_dict = model.state_dict()\n    keys = list(moudle_state_dict.keys())\n    for key in keys:\n        print(key + \' loaded\')\n        new_key = key.replace(r\'module.\', r\'\')\n        print(new_key)\n        state_dict[new_key] = moudle_state_dict[key]\n\n    return state_dict\n\ndef decompose(labeled):\n    nr_true = labeled.max()\n    masks = []\n    for i in range(1, nr_true + 1):\n        msk = labeled.copy()\n        msk[msk != i] = 0.\n        msk[msk == i] = 255.\n        masks.append(msk)\n\n    if not masks:\n        return [labeled]\n    else:\n        return masks\n\ndef encode_rle(predictions):\n    return [run_length_encoding(mask) for mask in predictions]\n\ndef create_submission(predictions):\n    output = []\n    for image_id, mask in predictions:\n        # print(image_id)\n\n        rle_encoded = \' \'.join(str(rle) for rle in run_length_encoding(mask))\n        output.append([image_id, rle_encoded])\n\n    submission = pd.DataFrame(output, columns=[\'id\', \'rle_mask\']).astype(str)\n    return submission\n\ndef run_length_encoding(x):\n    # https://www.kaggle.com/c/data-science-bowl-2018/discussion/48561#\n    bs = np.where(x.T.flatten())[0]\n\n    rle = []\n    prev = -2\n    for b in bs:\n        if (b > prev + 1): rle.extend((b + 1, 0))\n        rle[-1] += 1\n        prev = b\n\n    if len(rle) != 0 and rle[-1] + rle[-2] > (x.size+1):\n        rle[-2] = rle[-2] - 1\n\n    return rle\n\ndef run_length_decoding(mask_rle, shape):\n    """"""\n    Based on https://www.kaggle.com/msl23518/visualize-the-stage1-test-solution and modified\n    Args:\n        mask_rle: run-length as string formatted (start length)\n        shape: (height, width) of array to return\n\n    Returns:\n        numpy array, 1 - mask, 0 - background\n\n    """"""\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[1] * shape[0], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape((shape[1], shape[0])).T\n\ndef get_crop_pad_sequence(vertical, horizontal):\n    top = int(vertical / 2)\n    bottom = vertical - top\n    right = int(horizontal / 2)\n    left = horizontal - right\n    return (top, right, bottom, left)\n\ndef get_list_of_image_predictions(batch_predictions):\n    image_predictions = []\n    for batch_pred in batch_predictions:\n        image_predictions.extend(list(batch_pred))\n    return image_predictions\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\ndef get_seed():\n    seed = int(time.time()) + int(os.getpid())\n    return seed\n\ndef reseed(augmenter, deterministic=True):\n    augmenter.random_state = ia.new_random_state(get_seed())\n    if deterministic:\n        augmenter.deterministic = True\n\n    for lists in augmenter.get_children_lists():\n        for aug in lists:\n            aug = reseed(aug, deterministic=True)\n    return augmenter\n'"
data_process/__init__.py,0,b''
data_process/data_loader.py,4,"b'import torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom data_process.transform import *\nimport random\nimport pandas as pd\n\ndef read_txt(txt):\n    f = open(txt, \'r\')\n    lines = f.readlines()\n    f.close()\n    return [tmp.strip() for tmp in lines]\n\nclass SaltDataset(Dataset):\n    def __init__(self, transform, mode, image_size, fold_index, aug_list, pseudo_csv = None, pseudo_index = -1):\n\n        self.transform = transform\n        self.mode = mode\n        self.image_size = image_size\n        self.aug_list = aug_list\n\n        if pseudo_csv is None:\n            self.is_pseudo = False\n            self.pseudo_index = -1\n        else:\n            self.is_pseudo = True\n            self.pseudo_mask_path = r\'./data_process/pseudo_mask_path\'\n            csv_name = os.path.split(pseudo_csv)[1].replace(\'.csv\',\'\')\n            self.pseudo_mask_path = os.path.join(self.pseudo_mask_path, csv_name)\n            print(self.pseudo_mask_path)\n\n            if not os.path.exists(self.pseudo_mask_path):\n                os.makedirs(self.pseudo_mask_path)\n\n            print(\'save the csv images to disk\')\n            save_csv_images(pseudo_csv, self.pseudo_mask_path)\n            self.pseudo_index = pseudo_index\n\n        print(\'AugList: \')\n        print(self.aug_list)\n\n        # change to your path\n        self.train_image_path = r\'/data1/shentao/DATA/Kaggle/Salt/Kaggle_salt/train/images\'\n        self.train_mask_path = r\'/data1/shentao/DATA/Kaggle/Salt/Kaggle_salt/train/masks\'\n        self.test_image_path = r\'/data1/shentao/DATA/Kaggle/Salt/Kaggle_salt/test/images\'\n\n        self.fold_index = None\n        self.set_mode(mode, fold_index)\n\n\n    def set_mode(self, mode, fold_index):\n        self.mode = mode\n        self.fold_index = fold_index\n        print(\'fold index set: \' + str(fold_index))\n\n        if self.mode == \'train\':\n            data = pd.read_csv(\'./data_process/10fold/fold\' + str(fold_index) + \'_train.csv\')\n            self.train_list = data[\'fold\']\n            self.train_list = [tmp + \'.png\' for tmp in self.train_list]\n            self.num_data = len(self.train_list)\n\n            if self.is_pseudo:\n                print(\'pseudo labeling part: \' + str(self.pseudo_index))\n                self.pseudo_list = read_txt(\'./data_process/pseudo_split/pseudo_split\'+str(self.pseudo_index)+\'.txt\')\n                print(len(self.pseudo_list))\n\n        elif self.mode == \'val\':\n            data = pd.read_csv(\'./data_process/10fold/fold\' + str(fold_index) + \'_valid.csv\')\n            self.val_list = data[\'fold\']\n            self.val_list = [tmp + \'.png\' for tmp in self.val_list]\n            self.num_data = len(self.val_list)\n\n        elif self.mode == \'test\':\n            self.test_list = read_txt(\'./data_process/10fold/test.txt\')\n            self.num_data = len(self.test_list)\n            print(\'set dataset mode: test\')\n\n    def __getitem__(self, index):\n        if self.fold_index is None:\n            print(\'WRONG!!!!!!! fold index is NONE!!!!!!!!!!!!!!!!!\')\n            return\n\n        if self.mode == \'train\':\n            switch = 0\n            # random select pseudo label images\n            if self.is_pseudo:\n                switch = random.randint(0, 1)\n\n            if switch == 0:\n                image = cv2.imread(os.path.join(self.train_image_path, self.train_list[index]), 1)\n                label = cv2.imread(os.path.join(self.train_mask_path, self.train_list[index]), 0)\n            else:\n                index = random.randint(0,len(self.pseudo_list) - 1)\n                image = cv2.imread(os.path.join(self.test_image_path, self.pseudo_list[index]), 1)\n                label = cv2.imread(os.path.join(self.pseudo_mask_path, self.pseudo_list[index]), 0)\n\n        if self.mode == \'val\':\n            image = cv2.imread(os.path.join(self.train_image_path, self.val_list[index]), 1)\n            label = cv2.imread(os.path.join(self.train_mask_path, self.val_list[index]), 0)\n\n        if self.mode == \'test\':\n            image = cv2.imread(os.path.join(self.test_image_path, self.test_list[index]), 1)\n            image_id = self.test_list[index].replace(\'.png\', \'\')\n\n            if self.image_size == 128:\n                image = resize_and_pad(image, resize_size=101, factor=64)\n\n            image = image.reshape([self.image_size, self.image_size, 3])\n            image = np.transpose(image, (2, 0, 1))\n            image = image.astype(np.float32)\n            image = image.reshape([3, self.image_size, self.image_size])\n            image = (image.astype(np.float32) - 127.5) / 127.5\n            return image_id, torch.FloatTensor(image)\n\n        is_empty = False\n        if np.sum(label) == 0:\n            is_empty = True\n\n        if self.mode == \'train\':\n            image, label = resize_and_random_pad(image, label, resize_size=101, factor=128, limit=(-13, 13))\n        else:\n            image = resize_and_pad(image, resize_size=101, factor=128)\n            label = resize_and_pad(label, resize_size=101, factor=128)\n\n        image = cv2.resize(image, (self.image_size, self.image_size))\n        label = cv2.resize(label, (self.image_size, self.image_size))\n\n        if self.mode == \'train\':\n            if \'flip_lr\' in self.aug_list:\n                if random.randint(0, 1) == 0:\n                    image = cv2.flip(image, 1)\n                    label = cv2.flip(label, 1)\n\n        image = image.reshape([self.image_size, self.image_size, 3])\n        label = label.reshape([self.image_size, self.image_size, 1])\n        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n        image = (image.astype(np.float32) - 127.5) / 127.5\n\n        label = label.reshape([1, self.image_size, self.image_size])\n        label = np.asarray(label).astype(np.float32) / 255.0\n        label[label >= 0.5] = 1.0\n        label[label < 0.5] = 0.0\n\n        return torch.FloatTensor(image), torch.FloatTensor(label), is_empty\n\n    def __len__(self):\n        return self.num_data\n\n\ndef get_foldloader(image_size, batch_size, fold_index, aug_list = None, mode=\'train\', pseudo_csv = None, pseudo_index = -1):\n    """"""Build and return data loader.""""""\n    dataset = SaltDataset(None, mode, image_size, fold_index, aug_list, pseudo_csv = pseudo_csv, pseudo_index = pseudo_index)\n\n    shuffle = False\n    if mode == \'train\':\n        shuffle = True\n\n    data_loader = DataLoader(dataset=dataset, batch_size=batch_size, num_workers=4, shuffle=shuffle)\n    return data_loader\n\n\n\n\n'"
data_process/transform.py,0,"b""import cv2\nimport numpy as np\nimport os\n\ndef do_resize2(image, mask, H, W):\n    image = cv2.resize(image,dsize=(W,H))\n    mask = cv2.resize(mask,dsize=(W,H))\n    mask  = (mask>0.5).astype(np.float32)\n\n    return image,mask\n\n\n#################################################################\ndef compute_center_pad(H,W, factor=32):\n    if H%factor==0:\n        dy0,dy1=0,0\n    else:\n        dy  = factor - H%factor\n        dy0 = dy//2\n        dy1 = dy - dy0\n\n    if W%factor==0:\n        dx0,dx1=0,0\n    else:\n        dx  = factor - W%factor\n        dx0 = dx//2\n        dx1 = dx - dx0\n\n    return dy0, dy1, dx0, dx1\n\n\ndef do_center_pad_to_factor(image, factor=32):\n    H,W = image.shape[:2]\n    dy0, dy1, dx0, dx1 = compute_center_pad(H,W, factor)\n\n    image = cv2.copyMakeBorder(image, dy0, dy1, dx0, dx1, cv2.BORDER_REPLICATE)\n    return image\n\ndef do_center_pad_to_factor_edgeYreflectX(image, factor=32):\n    H,W = image.shape[:2]\n    dy0, dy1, dx0, dx1 = compute_center_pad(H,W, factor)\n\n    image = cv2.copyMakeBorder(image, 0, 0, dx0, dx1, cv2.BORDER_REFLECT101)\n    image = cv2.copyMakeBorder(image, dy0, dy1, 0, 0, cv2.BORDER_REPLICATE)\n    return image\n\n\ndef do_center_pad_to_factor2(image, mask, factor=32):\n    image = do_center_pad_to_factor(image, factor)\n    mask  = do_center_pad_to_factor(mask, factor)\n    return image, mask\n\n#---\ndef do_horizontal_flip(image):\n    #flip left-right\n    image = cv2.flip(image,1)\n    return image\n\ndef do_horizontal_flip2(image,mask):\n    image = do_horizontal_flip(image)\n    mask  = do_horizontal_flip(mask )\n    return image, mask\n\n#---\n\ndef compute_random_pad(H,W, limit=(-4,4), factor=32):\n    if H%factor==0:\n        dy0,dy1=0,0\n    else:\n        dy  = factor - H%factor\n        dy0 = dy//2 + np.random.randint(limit[0],limit[1]) # np.random.choice(dy)\n        dy1 = dy - dy0\n\n    if W%factor==0:\n        dx0,dx1=0,0\n    else:\n        dx  = factor - W%factor\n        dx0 = dx//2 + np.random.randint(limit[0],limit[1]) # np.random.choice(dx)\n        dx1 = dx - dx0\n\n    return dy0, dy1, dx0, dx1\n\n\ndef do_random_pad_to_factor2(image, mask, limit=(-4,4), factor=32):\n    H,W = image.shape[:2]\n    dy0, dy1, dx0, dx1 = compute_random_pad(H,W, limit, factor)\n\n    image = cv2.copyMakeBorder(image, dy0, dy1, dx0, dx1, cv2.BORDER_REPLICATE)\n    mask  = cv2.copyMakeBorder(mask,  dy0, dy1, dx0, dx1, cv2.BORDER_REPLICATE)\n\n    return image, mask\n\ndef do_random_pad_to_factor2_edgeYreflectX(image, mask, limit=(-4,4), factor=32):\n    H,W = image.shape[:2]\n    dy0, dy1, dx0, dx1 = compute_random_pad(H,W, limit, factor)\n\n    # image = cv2.copyMakeBorder(image, dy0, dy1, dx0, dx1, cv2.BORDER_REPLICATE)\n    image = cv2.copyMakeBorder(image, 0, 0, dx0, dx1, cv2.BORDER_REFLECT101)\n    image = cv2.copyMakeBorder(image, dy0, dy1, 0, 0, cv2.BORDER_REPLICATE)\n\n    # mask  = cv2.copyMakeBorder(mask,  dy0, dy1, dx0, dx1, cv2.BORDER_REPLICATE)\n    mask = cv2.copyMakeBorder(mask, 0, 0, dx0, dx1, cv2.BORDER_REFLECT101)\n    mask = cv2.copyMakeBorder(mask, dy0, dy1, 0, 0, cv2.BORDER_REPLICATE)\n\n    return image, mask\n\n#----\ndef do_invert_intensity(image):\n    #flip left-right\n    image = np.clip(1-image,0,1)\n    return image\n\n\ndef do_brightness_shift(image, alpha=0.125):\n    image = image + alpha\n    image = np.clip(image, 0, 1)\n    return image\n\n\ndef do_brightness_multiply(image, alpha=1):\n    image = alpha*image\n    image = np.clip(image, 0, 1)\n    return image\n\n\n#https://www.pyimagesearch.com/2015/10/05/opencv-gamma-correction/\ndef do_gamma(image, gamma=1.0):\n\n    image = image ** (1.0 / gamma)\n    image = np.clip(image, 0, 1)\n    return image\n\n\ndef do_flip_transpose2(image, mask, type=0):\n    #choose one of the 8 cases\n\n    if type==1: #rotate90\n        image = image.transpose(1,0)\n        image = cv2.flip(image,1)\n\n        mask = mask.transpose(1,0)\n        mask = cv2.flip(mask,1)\n\n\n    if type==2: #rotate180\n        image = cv2.flip(image,-1)\n        mask  = cv2.flip(mask,-1)\n\n\n    if type==3: #rotate270\n        image = image.transpose(1,0)\n        image = cv2.flip(image,0)\n\n        mask = mask.transpose(1,0)\n        mask = cv2.flip(mask,0)\n\n\n    if type==4: #flip left-right\n        image = cv2.flip(image,1)\n        mask  = cv2.flip(mask,1)\n\n\n    if type==5: #flip up-down\n        image = cv2.flip(image,0)\n        mask  = cv2.flip(mask,0)\n\n    if type==6:\n        image = cv2.flip(image,1)\n        image = image.transpose(1,0)\n        image = cv2.flip(image,1)\n\n        mask = cv2.flip(mask,1)\n        mask = mask.transpose(1,0)\n        mask = cv2.flip(mask,1)\n\n    if type==7:\n        image = cv2.flip(image,0)\n        image = image.transpose(1,0)\n        image = cv2.flip(image,1)\n\n        mask = cv2.flip(mask,0)\n        mask = mask.transpose(1,0)\n        mask = cv2.flip(mask,1)\n\n\n    return image, mask\n\n##================================\ndef do_shift_scale_crop( image, mask, x0=0, y0=0, x1=1, y1=1 ):\n    #cv2.BORDER_REFLECT_101\n    #cv2.BORDER_CONSTANT\n    height, width = image.shape[:2]\n    image = image[y0:y1,x0:x1]\n    mask  = mask [y0:y1,x0:x1]\n\n    image = cv2.resize(image,dsize=(width,height))\n    mask  = cv2.resize(mask,dsize=(width,height))\n    mask  = (mask>0.5).astype(np.float32)\n    return image, mask\n\n\ndef do_random_shift_scale_crop_pad2(image, mask, limit=0.10):\n    H, W = image.shape[:2]\n\n    dy = int(H*limit)\n    y0 =   np.random.randint(0,dy)\n    y1 = H-np.random.randint(0,dy)\n\n    dx = int(W*limit)\n    x0 =   np.random.randint(0,dx)\n    x1 = W-np.random.randint(0,dx)\n\n    #y0, y1, x0, x1\n    image, mask = do_shift_scale_crop( image, mask, x0, y0, x1, y1 )\n    return image, mask\n\n#===========================================================================\n\ndef resize_and_pad(image, resize_size, factor):\n    image = cv2.resize(image, (resize_size,resize_size))\n    image = do_center_pad_to_factor(image, factor)\n    return image\n\ndef resize_and_pad_edgeYreflectX(image, resize_size, factor):\n    image = cv2.resize(image, (resize_size,resize_size))\n    image = do_center_pad_to_factor_edgeYreflectX(image, factor)\n    return image\n\ndef resize_and_random_pad(image, mask, resize_size, factor,limit=(-13, 13)):\n    image = cv2.resize(image, (resize_size,resize_size))\n    mask = cv2.resize(mask, (resize_size, resize_size))\n    image, mask = do_random_pad_to_factor2(image, mask, limit = limit, factor = factor)\n    return image, mask\n\ndef resize_and_random_pad_edgeYreflectX(image, mask, resize_size, factor):\n    image = cv2.resize(image, (resize_size,resize_size))\n    mask = cv2.resize(mask, (resize_size, resize_size))\n\n    image, mask = do_random_pad_to_factor2_edgeYreflectX(image, mask, limit=(-13, 13), factor = factor)\n    return image, mask\n\ndef center_corp(image, image_size, crop_size):\n    image = cv2.resize(image, (image_size, image_size))\n    radius = (image_size - crop_size)/2\n    image = image[radius:radius+crop_size,radius:radius+crop_size]\n    return image\n\n\nfrom math import isnan\ndef do_length_decode(rle, H, W, fill_value=255):\n    mask = np.zeros((H,W), np.uint8)\n    if type(rle).__name__ == 'float': return mask\n\n    mask = mask.reshape(-1)\n    rle = np.array([int(s) for s in rle.split(' ')]).reshape(-1, 2)\n    for r in rle:\n        start = r[0]-1\n        end = start + r[1]\n        mask[start : end] = fill_value\n    mask = mask.reshape(W, H).T   # H, W need to swap as transposing.\n    return mask\n\ndef decode_csv(csv_name):\n    import pandas as pd\n    data = pd.read_csv(csv_name)\n    id = data['id']\n    rle_mask = data['rle_mask']\n\n    dict = {}\n    for id, rle in zip(id,rle_mask):\n        tmp = do_length_decode(rle, 101, 101, fill_value=1)\n        dict[id] = tmp\n\n    return dict\n\ndef save_csv_images(csv_path, save_path):\n    dict = decode_csv(csv_name=csv_path)\n\n    for id in dict:\n        id_img = dict[id]*255\n        cv2.imwrite(os.path.join(save_path,id+'.png'),id_img)\n"""
jigsaw/__init__.py,0,b''
jigsaw/handcraft_ruls_postprocessing.py,0,"b'from include import *\n\ntrain_mask_path = os.path.join(root_dir, r\'train/masks\')\ntrain_data_dir = os.path.join(root_dir, r\'train/images\')\ntest_data_dir = os.path.join(root_dir, r\'test/images\')\n\nvertical_y_ratio = 0.8\n# build train img set, test img set\ntrain_set = set()\ntest_set = set()\nfor subdir in os.listdir(train_data_dir):\n    train_set.add(subdir.split(\'.\')[0])\nfor subdir in os.listdir(test_data_dir):\n    test_set.add(subdir.split(\'.\')[0])\n\ndef create_submission(predictions):\n    output = []\n    for image_id, mask in predictions:\n        rle_encoded = \' \'.join(str(rle) for rle in run_length_encoding(mask))\n        output.append([image_id, rle_encoded])\n    submission = pd.DataFrame(output, columns=[\'id\', \'rle_mask\']).astype(str)\n    return submission\n\ndef run_length_encoding(x):\n    # https://www.kaggle.com/c/data-science-bowl-2018/discussion/48561#\n    bs = np.where(x.T.flatten())[0]\n    rle = []\n    prev = -2\n    for b in bs:\n        if (b > prev + 1): rle.extend((b + 1, 0))\n        rle[-1] += 1\n        prev = b\n    if len(rle) != 0 and rle[-1] + rle[-2] > (x.size+1):\n        rle[-2] = rle[-2] - 1\n    return rle\n\n# detect semi vertical mask\ndef detect_semi_vertical_mask(img):\n    # semi vertical threshlod\n    h_offset = int(101 * vertical_y_ratio)\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n    if np.mean(img[100]) == 255 or np.mean(img[100]) == 0:\n        is_semi_vertical = False\n        return is_semi_vertical\n    is_semi_vertical = np.all(img[h_offset:] == img[100])\n    if np.all(img == img[100]):\n        is_semi_vertical = False\n    return is_semi_vertical\n\n# detect full vertical mask\ndef detect_full_vertical_mask(img):\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n    if np.mean(img[100]) == 255 or np.mean(img[100]) == 0:\n        is_full_vertical = False\n        return is_full_vertical\n    is_full_vertical = np.all(img == img[100])\n    return is_full_vertical\n\n################  load train mask ##################\ndef load_train_mask(train_mask_path):\n    train_mask = {}\n    for subdir in os.listdir(train_mask_path):\n        imgpath = os.path.join(train_mask_path, subdir)\n        mask = cv2.imread(imgpath)\n        train_mask.setdefault(subdir.split(\'.\')[0], mask)\n    return train_mask\n\n##############  make test mask dict  ###############\ndef do_length_decode(rle, H, W, fill_value=255):\n    mask = np.zeros((H,W), np.uint8)\n    if type(rle).__name__ == \'float\': return mask\n    mask = mask.reshape(-1)\n    rle = np.array([int(s) for s in rle.split(\' \')]).reshape(-1, 2)\n    for r in rle:\n        start = r[0]-1\n        end = start + r[1]\n        mask[start : end] = fill_value\n    mask = mask.reshape(W, H).T   # H, W need to swap as transposing.\n    return mask\n\ndef decode_csv(csv_name):\n    import pandas as pd\n    data = pd.read_csv(csv_name)\n    id = data[\'id\']\n    rle_mask = data[\'rle_mask\']\n    dict = {}\n    for id, rle in zip(id,rle_mask):\n        tmp = do_length_decode(rle, 101, 101, fill_value=1)\n        tmp *= 255\n        tmp = cv2.cvtColor(tmp, cv2.COLOR_GRAY2RGB)\n        dict[id] = tmp\n    return dict\n\ndef load_test_mask(test_mask_path):\n    mask_test = decode_csv(test_mask_path)\n    return mask_test\n\n#############  find all full vertical mask ##############\ndef find_all_vertical_mask():\n    train_vertical_dict = {}\n    train_semi_vertical_dict = {}\n    train_mask = load_train_mask(train_mask_path)\n    for k, v in train_mask.items():\n        is_semi_v = detect_semi_vertical_mask(v)\n        if is_semi_v:\n            train_semi_vertical_dict.setdefault(k, \'semi\')\n        is_full_v = detect_full_vertical_mask(v)\n        if is_full_v:\n            train_vertical_dict.setdefault(k, \'full\')\n    with open(r\'./jigsaw/json_files/train_vertical_mask.json\', \'w\') as f:\n        json.dump(train_vertical_dict, f)\n    with open(r\'./jigsaw/json_files/train_semi_vertical_mask.json\', \'w\') as f:\n        json.dump(train_semi_vertical_dict, f)\n\n#################### vertical rules ####################\ndef vertical_rule1(img_map, map_img, vertical_mask):\n    test_vertical_mask = {}\n    for k, v in vertical_mask.items():\n        if k in img_map.keys():\n            mapid, x, y = img_map[k][\'mapid\'], img_map[k][\'x\'], img_map[k][\'y\']\n            try:\n                up_imgid = map_img[mapid][x][str(int(y) - 1)]\n            except:\n                up_imgid = \'None\'\n            if up_imgid in test_set:\n                if up_imgid not in test_vertical_mask.keys():\n                    test_vertical_mask.setdefault(up_imgid, k)\n\n    with open(r\'./jigsaw/json_files/vertical_test_rule1.json\', \'w\') as f:\n        json.dump(test_vertical_mask, f)\n    print(\'Rule1 detect test vertical : \', len(test_vertical_mask.keys()))\n\ndef vertical_rule2(vertical_mask, img_map, map_img):\n    test_vertical_mask = {}\n\n    masks = vertical_mask\n    json_file = r\'./jigsaw/json_files/vertical_test_rule2.json\'\n\n    for k, v in masks.items():\n        if k in img_map.keys():\n            mapid, x, y = img_map[k][\'mapid\'], img_map[k][\'x\'], img_map[k][\'y\']\n            for key, value in map_img[mapid][x].items():\n                if value in test_set and (int(key) > int(y)):\n                    test_vertical_mask.setdefault(value, k)\n\n    with open(json_file, \'w\') as f:\n        json.dump(test_vertical_mask, f)\n    print(\'Rule4 detect test vertical : \', len(test_vertical_mask.keys()))\n\ndef semi_vertical_rule(semi_vertical_mask, img_map, map_img):\n    test_vertical_mask = {}\n\n    masks = semi_vertical_mask\n    json_file = r\'./jigsaw/json_files/semi_vertical.json\'\n\n    for k, v in masks.items():\n        if k in img_map.keys():\n            mapid, x, y = img_map[k][\'mapid\'], img_map[k][\'x\'], img_map[k][\'y\']\n            for key, value in map_img[mapid][x].items():\n                if value in test_set and (int(key) > int(y)):\n                    test_vertical_mask.setdefault(value, k)\n\n    with open(json_file, \'w\') as f:\n        json.dump(test_vertical_mask, f)\n    print(\'Rule4 detect test vertical : \', len(test_vertical_mask.keys()))\n\ndef vertical_rule3(vertical_mask, test_mask_path, img_map, map_img):\n    test_vertical_mask1 = {}\n    test_vertical_mask3_2 = {}\n    test_vertical_mask4 = {}\n    test_vertical_mask5 = {}\n    test_mask = load_test_mask(test_mask_path)\n    #########################################\n    for k, v in vertical_mask.items():\n        if k not in img_map.keys(): continue\n        mapid, x, y = img_map[k][\'mapid\'], img_map[k][\'x\'], img_map[k][\'y\']\n        for key, value in map_img[mapid][x].items():\n            if value not in test_set: continue\n            # find max train image, min test image, min big test image, in same columns\n            max_y, min_y_test, min_y_test_m = -1, 100, 100\n            for kk, vv in map_img[mapid][x].items():\n                if vv in train_set and (vv != k) and (vv not in vertical_mask.keys()) and (int(kk) < int(y)) and (\n                        int(kk) > max_y):\n                    max_y = int(kk)\n                if vv in test_set:\n                    mask_test = test_mask[vv]\n                    if np.sum(mask_test) != 0 and (int(kk) < min_y_test):\n                        min_y_test = int(kk)\n                    if np.sum(mask_test) > 50 * 50 * 255 and (int(kk) < min_y_test_m):\n                        min_y_test_m = int(kk)\n\n            # apply rule5-1, 5-5\n            if int(key) < int(y) and (int(key) > max_y and max_y != -1):\n                if map_img[mapid][x][str(max_y)] in train_set:\n                    imgpath = os.path.join(train_mask_path, map_img[mapid][x][str(max_y)] + \'.png\')\n                    mask = cv2.imread(imgpath)\n                    # rule5-1\n                    if np.sum(mask) == 0:\n                        test_vertical_mask1.setdefault(value, k)\n                    # rule5-5\n                    else:\n                        test_vertical_mask5.setdefault(value, k)\n\n            # apply rule5-3-2\n            if int(key) < int(y) and (int(key) > max_y and max_y == -1) and min_y_test_m != 100 and (\n                    int(key) > min_y_test_m):\n                test_vertical_mask3_2.setdefault(value, k)\n            if int(key) < int(y) and (int(key) > max_y and max_y == -1) and min_y_test != 100:\n                if np.sum(test_mask[map_img[mapid][x][str(min_y_test)]]) <= 50 * 50 * 255 and np.sum(\n                        test_mask[value]) <= 10 * 255 and (int(key) > min_y_test):\n                    test_vertical_mask3_2.setdefault(value, k)\n            # apply rule 5-4\n            if int(key) < int(y) and (int(key) > max_y and max_y == -1) and (min_y_test == 100) and (\n                    int(key) >= int(int(y) / 2)):\n                test_vertical_mask4.setdefault(value, k)\n\n    with open(r\'./jigsaw/json_files/vertical_test_rule3_1.json\', \'w\') as f:\n        json.dump(test_vertical_mask1, f)\n    with open(r\'./jigsaw/json_files/vertical_test_rule3_2.json\', \'w\') as f:\n        json.dump(test_vertical_mask3_2, f)\n    with open(r\'./jigsaw/json_files/vertical_test_rule3_3.json\', \'w\') as f:\n        json.dump(test_vertical_mask4, f)\n    with open(r\'./jigsaw/json_files/vertical_test_rule3_4.json\', \'w\') as f:\n        json.dump(test_vertical_mask5, f)\n\n    print(\'test_vertical_mask1_num : \'+str( len(test_vertical_mask1.keys())))\n    print(\'test_vertical_mask3_num : \'+str( len(test_vertical_mask3_2.keys())))\n    print(\'test_vertical_mask4_num : \'+str( len(test_vertical_mask4.keys())))\n    print(\'test_vertical_mask5_num : \'+str( len(test_vertical_mask5.keys())))\n\ndef vertical_rules_combine(rule):\n    vertical_test_rule = {}\n    rule_name = \'\'\n    for i in rule:\n        json_file_path = r\'./jigsaw/json_files/vertical_test_rule\' + i + \'.json\'\n        print(json_file_path)\n\n        rule_name += \'-\'+ i\n        with open(json_file_path, \'r\') as f:\n            vertical_test_rule_tmp = json.load(f)\n            for k, v in vertical_test_rule_tmp.items():\n                if k in vertical_test_rule.keys(): continue\n                vertical_test_rule.setdefault(k, v)\n\n        os.remove(json_file_path)\n\n    print(\'Vertical test rule number : \', len(vertical_test_rule.keys()))\n    with open(r\'./jigsaw/json_files/vertical.json\', \'w\') as f:\n        json.dump(vertical_test_rule, f)\n\n################## empty rules ###################\ndef build_full_train_mask(train_mask_path, img_map, img_map_set):\n    mask = img_map.copy()\n    train_mask = load_train_mask(train_mask_path)\n    train_full_set = train_set.copy()\n    for i in (train_set & img_map_set):\n        train_mask_tmp = cv2.cvtColor(train_mask[i], cv2.COLOR_BGR2GRAY)\n        if np.sum(train_mask_tmp) == 0: continue\n        mapid, x, y = img_map[i][\'mapid\'], img_map[i][\'x\'], img_map[i][\'y\']\n        # left\n        if np.sum(train_mask_tmp[:, 0]) > 255:  # 0~\n            mask.setdefault(\'L-\' + i, {\'x\': str(int(x) - 1), \'y\': y, \'mapid\': mapid})\n            train_full_set.add(\'L-\' + i)\n        elif np.mean(train_mask_tmp[100]) == 255:\n            mask.setdefault(\'L-\' + i, {\'x\': str(int(x) - 1), \'y\': str(int(y) + 1), \'mapid\': mapid})\n            train_full_set.add(\'L-\' + i)\n\n        # right\n        if np.sum(train_mask_tmp[:, 100]) > 255:  # 0~\n            mask.setdefault(\'R-\' + i, {\'x\': str(int(x) + 1), \'y\': y, \'mapid\': mapid})\n            train_full_set.add(\'R-\' + i)\n        elif np.mean(train_mask_tmp[100]) == 255:\n            mask.setdefault(\'R-\' + i, {\'x\': str(int(x) + 1), \'y\': str(int(y) + 1), \'mapid\': mapid})\n            train_full_set.add(\'R-\' + i)\n\n    with open(r\'./jigsaw/json_files/jigsaw_map_virtual.json\', \'w\') as f:\n        json.dump(mask, f)\n\ndef empty_rule1(train_mask_path, img_map, img_map_set, map_img):\n    mask = {}\n    train_mask = load_train_mask(train_mask_path)\n    for i in (train_set & img_map_set):\n        if np.mean(train_mask[i][100, :, :]) == 255:\n            mapid, x, y = img_map[i][\'mapid\'], img_map[i][\'x\'], img_map[i][\'y\']\n            for y_, imgid in map_img[mapid][x].items():\n                if imgid in test_set and (int(y_) > int(y)) and int(mapid) <= 158: mask.setdefault(imgid, \'Empty\')\n\n    with open(r\'./jigsaw/json_files/empty_rule1.json\', \'w\') as f:\n        json.dump(mask, f)\n\ndef empty_rule2(train_mask_path, test_mask_path, img_map, img_map_set, map_img, map_img_virtual):\n    mask = {}\n    train_mask = load_train_mask(train_mask_path)\n    test_mask = load_test_mask(test_mask_path)\n    for i in (train_set & img_map_set):\n        if np.sum(train_mask[i]) == 0:\n            mapid, x, y = img_map[i][\'mapid\'], img_map[i][\'x\'], img_map[i][\'y\']\n            flag = 0\n            for y_, imgid in map_img[mapid][x].items():\n                if imgid in train_set: img_mask = train_mask[imgid]\n                else: img_mask = test_mask[imgid]\n                if (int(y_) < int(y)) and np.sum(img_mask) != 0 and int(mapid) <= 158: flag = 1\n            for y_idx in range(-1, int(y)):\n                if mapid in map_img_virtual.keys() and x in map_img_virtual[mapid].keys() and str(y_idx) in map_img_virtual[mapid][x].keys() and \\\n                        (\'-\' in  map_img_virtual[mapid][x][str(y_idx)]): flag = 1\n            if flag == 0: continue\n            for y_, imgid in map_img[mapid][x].items():\n                if imgid in test_set and (int(y_) > int(y)) and int(mapid) <= 158: mask.setdefault(imgid, \'Empty\')\n\n    with open(r\'./jigsaw/json_files/empty_rule2.json\', \'w\') as f:\n        json.dump(mask, f)\n\ndef make_sub_rule_empty(test_mask_path):\n    empty_mask = {}\n    test_mask = load_test_mask(test_mask_path)\n    output_test_mask_path = test_mask_path.replace(\'.csv\', \'\') + \'-empty.csv\'\n    out = []\n    with open(r\'./jigsaw/json_files/empty_rule1.json\') as f:\n        rule1_dict = json.load(f)\n        empty_mask.update(rule1_dict)\n    with open(r\'./jigsaw/json_files/empty_rule2.json\') as f:\n        rule2_dict = json.load(f)\n        empty_mask.update(rule2_dict)\n\n    for id in test_mask:\n        sum_tmp = test_mask[id][:, :, 0] / 255\n        if id in empty_mask.keys():\n            sum_tmp = np.zeros([101, 101]).astype(np.uint8)\n        out.append([id, sum_tmp])\n    submission = create_submission(out)\n    submission.to_csv(output_test_mask_path, index=None)\n\n    os.remove(r\'./jigsaw/json_files/empty_rule1.json\')\n    os.remove(r\'./jigsaw/json_files/empty_rule2.json\')\n\n################## smooth rules #################\ndef padding(input, output, img_map, map_img, test_mask):\n    with open(input) as f:\n        rule6_dict = json.load(f)\n    rule6_test_dict = rule6_dict.copy()\n    for k, v in rule6_dict.items():\n        mapid, x, y = img_map[k][\'mapid\'], img_map[k][\'x\'], img_map[k][\'y\']\n        if str(int(y) - 1) in map_img[mapid][x].keys():\n            up = map_img[mapid][x][str(int(y) - 1)]\n            if up in test_set:\n                if \'up\' not in v.keys() and (np.mean(test_mask[up][100, :]) > 0):\n                    rule6_test_dict.setdefault(k, {}).setdefault(\'up\', test_mask[up][100, :, 0].tolist())\n        if str(int(y) + 1) in map_img[mapid][x].keys():\n            down = map_img[mapid][x][str(int(y) + 1)]\n            if down in test_set:\n                if \'down\' not in v.keys() and (np.mean(test_mask[down][0, :]) > 0):\n                    rule6_test_dict.setdefault(k, {}).setdefault(\'down\', test_mask[down][0, :, 0].tolist())\n        if str(int(x) - 1) in map_img[mapid].keys() and y in map_img[mapid][str(int(x) - 1)].keys():\n            left = map_img[mapid][str(int(x) - 1)][y]\n            if left in test_set:\n                if \'left\' not in v.keys() and (np.mean(test_mask[left][:, 100]) > 0):\n                    rule6_test_dict.setdefault(k, {}).setdefault(\'left\', test_mask[left][:, 100, 0].tolist())\n        if str(int(x) + 1) in map_img[mapid].keys() and y in map_img[mapid][str(int(x) + 1)].keys():\n            right = map_img[mapid][str(int(x) + 1)][y]\n            if right in test_set:\n                if \'right\' not in v.keys() and (np.mean(test_mask[right][:, 0]) > 0):\n                    rule6_test_dict.setdefault(k, {}).setdefault(\'right\', test_mask[right][:, 0, 0].tolist())\n    with open(output, \'w\') as f:\n        json.dump(rule6_test_dict, f)\n\ndef smooth_rule(train_mask_path, test_mask_path, vertical_mask, img_map, img_map_set, map_img):\n    mask = {}\n    train_mask = load_train_mask(train_mask_path)\n    test_mask = load_test_mask(test_mask_path)\n    for i in (train_set & img_map_set):\n        if i in vertical_mask.keys(): continue\n        train_mask_tmp = cv2.cvtColor(train_mask[i], cv2.COLOR_BGR2GRAY)\n        if np.sum(train_mask_tmp) == 0: continue\n        # up\n        if np.mean(train_mask_tmp[0]) < 255 and (np.sum(train_mask_tmp[0]) > 255):\n            mapid, x, y = img_map[i][\'mapid\'], img_map[i][\'x\'], img_map[i][\'y\']\n            if str(int(y) - 1) in map_img[mapid][x].keys() and int(mapid) < 159:\n                imgid = map_img[mapid][x][str(int(y) - 1)]\n                if imgid in test_set and np.sum(test_mask[imgid]) == 0: mask.setdefault(imgid, {}).setdefault(""down"", train_mask_tmp[0].tolist())\n        # down\n        if np.mean(train_mask_tmp[100]) < 255 and (np.sum(train_mask_tmp[100]) > 255):\n            mapid, x, y = img_map[i][\'mapid\'], img_map[i][\'x\'], img_map[i][\'y\']\n            if str(int(y) + 1) in map_img[mapid][x].keys() and int(mapid) < 159:\n                imgid = map_img[mapid][x][str(int(y) + 1)]\n                if imgid in test_set and np.sum(test_mask[imgid]) == 0: mask.setdefault(imgid, {}).setdefault(""up"", train_mask_tmp[100].tolist())\n        # left\n        if np.mean(train_mask_tmp[:, 0]) < 255 and (np.sum(train_mask_tmp[:, 0]) > 255):\n            mapid, x, y = img_map[i][\'mapid\'], img_map[i][\'x\'], img_map[i][\'y\']\n            if str(int(x) - 1) in map_img[mapid].keys() and y in map_img[mapid][str(int(x) - 1)].keys() and int(mapid) < 159:\n                imgid = map_img[mapid][str(int(x) - 1)][y]\n                if imgid in test_set and np.sum(test_mask[imgid]) == 0: mask.setdefault(imgid, {}).setdefault(""right"", train_mask_tmp[:, 0].tolist())\n        # right\n        if np.mean(train_mask_tmp[:, 100]) < 255 and (np.sum(train_mask_tmp[:, 100]) > 255):\n            mapid, x, y = img_map[i][\'mapid\'], img_map[i][\'x\'], img_map[i][\'y\']\n            if str(int(x) + 1) in map_img[mapid].keys() and y in map_img[mapid][str(int(x) + 1)].keys() and int(mapid) < 159:\n                imgid = map_img[mapid][str(int(x) + 1)][y]\n                if imgid in test_set and np.sum(test_mask[imgid]) == 0: mask.setdefault(imgid, {}).setdefault(""left"", train_mask_tmp[:, 100].tolist())\n    with open(r\'./jigsaw/json_files/smooth_rule.json\', \'w\') as f:\n        json.dump(mask, f)\n    padding(r\'./jigsaw/json_files/smooth_rule.json\',\n            r\'./jigsaw/json_files/smooth_rule_test.json\', img_map, map_img, test_mask)\n\ndef liner_line(x1, y1, x2, y2):\n    if y2 - y1 == 0: return 10000000000\n    k = float(x2 - x1) / float(y2 - y1)\n    return k\n\ndef for_smooth_rule():\n    test_mask_dict = {}\n    with open(r\'./jigsaw/json_files/smooth_rule_test.json\') as f:\n        rule6_dict = json.load(f)\n    for k, v in rule6_dict.items():\n        new_mask = np.zeros((101, 101))\n        if len(v.keys()) == 1:\n            if \'up\' in v.keys():\n                new_mask += np.array([v[\'up\'] for i in range(101)])\n            if \'down\' in v.keys():\n                new_mask += np.array([v[\'down\'] for i in range(101)])\n            if \'left\' in v.keys():\n                new_mask_tmp = np.array([v[\'left\'] for i in range(101)])\n                new_mask_tmp = new_mask_tmp.transpose((1, 0))\n                new_mask += new_mask_tmp\n            if \'right\' in v.keys():\n                new_mask_tmp = np.array([v[\'right\'] for i in range(101)])\n                new_mask_tmp = new_mask_tmp.transpose((1, 0))\n                new_mask += new_mask_tmp\n        if len(v.keys()) == 2:\n            # up - down\n            if \'up\' in v.keys() and \'down\' in v.keys():\n                up_first, up_last = np.where(np.array(v[\'up\'])==255)[0][0], \\\n                                    np.where(np.array(v[\'up\'])==255)[0][-1]\n                down_first, down_last = np.where(np.array(v[\'down\']) == 255)[0][0], \\\n                                        np.where(np.array(v[\'down\']) == 255)[0][-1]\n                k1, k2 = liner_line(up_first, 0, down_first, 100), liner_line(up_last, 0, down_last, 100)\n                for i in range(new_mask.shape[0]):\n                    new_mask[i, max(0, int(k1 * i + up_first)) : min(101, int(k2 * i + up_last))] = 255\n            if \'left\' in v.keys() and \'right\' in v.keys():\n                left_first, left_last = np.where(np.array(v[\'left\']) == 255)[0][0], \\\n                                        np.where(np.array(v[\'left\']) == 255)[0][-1]\n                right_first, right_last = np.where(np.array(v[\'right\']) == 255)[0][0], \\\n                                          np.where(np.array(v[\'right\']) == 255)[0][-1]\n                k1 = liner_line(left_first, 0, right_first, 100)\n                for i in range(new_mask.shape[1]):\n                    new_mask[min(100, max(0, int(k1 * i + left_first))):, i] = 255\n\n            if \'up\' in v.keys() and \'left\' in v.keys():\n                up_first, up_last = np.where(np.array(v[\'up\']) == 255)[0][0], \\\n                                    np.where(np.array(v[\'up\']) == 255)[0][-1]\n                left_first, left_last = np.where(np.array(v[\'left\']) == 255)[0][0], \\\n                                        np.where(np.array(v[\'left\']) == 255)[0][-1]\n                if left_first == 0: left_first += 1\n                k1 = liner_line(up_first, 0, 0, left_first)\n                for i in range(new_mask.shape[0]):\n                    new_mask[i, max(0, int(k1 * i + up_first)) : up_last] = 255\n            if \'up\' in v.keys() and \'right\' in v.keys():\n                up_first, up_last = np.where(np.array(v[\'up\']) == 255)[0][0], \\\n                                    np.where(np.array(v[\'up\']) == 255)[0][-1]\n                right_first, right_last = np.where(np.array(v[\'right\']) == 255)[0][0], \\\n                                          np.where(np.array(v[\'right\']) == 255)[0][-1]\n                if right_first == 0: right_first += 1\n                k2 = liner_line(up_last, 0, 100, right_first)\n                for i in range(new_mask.shape[0]):\n                    new_mask[i, up_first: min(101, int(k2 * i + up_last))] = 255\n            if \'down\' in v.keys() and \'left\' in v.keys():\n                down_first, down_last = np.where(np.array(v[\'down\']) == 255)[0][0], \\\n                                        np.where(np.array(v[\'down\']) == 255)[0][-1]\n                left_first, left_last = np.where(np.array(v[\'left\']) == 255)[0][0], \\\n                                        np.where(np.array(v[\'left\']) == 255)[0][-1]\n                k1 = liner_line(0, left_first, down_last, 100)\n                for i in range(left_first, 101):\n                    new_mask[i, 0 : min(101, int(k1 * (i - left_first)))] = 255\n            if \'down\' in v.keys() and \'right\' in v.keys():\n                down_first, down_last = np.where(np.array(v[\'down\']) == 255)[0][0], \\\n                                        np.where(np.array(v[\'down\']) == 255)[0][-1]\n                right_first, right_last = np.where(np.array(v[\'right\']) == 255)[0][0], \\\n                                          np.where(np.array(v[\'right\']) == 255)[0][-1]\n                k1 = liner_line(100, right_first, down_first, 100)\n                for i in range(right_first, 101):\n                    new_mask[i, max(0, int(k1 * (i - right_first) + 100)): 101] = 255\n        if len(v.keys()) == 3:\n            if \'down\' in v.keys() and \'left\' in v.keys() and \'right\' in v.keys():\n                down_first, down_last = np.where(np.array(v[\'down\']) == 0)[0][0], \\\n                                        np.where(np.array(v[\'down\']) == 0)[0][-1]\n                left_first, left_last = np.where(np.array(v[\'left\']) == 255)[0][0], \\\n                                        np.where(np.array(v[\'left\']) == 255)[0][-1]\n                right_first, right_last = np.where(np.array(v[\'right\']) == 255)[0][0], \\\n                                          np.where(np.array(v[\'right\']) == 255)[0][-1]\n                k1, k2 = liner_line(0, left_first, down_first, 100), liner_line(100, right_first, down_last, 100)\n                for i in range(left_first, 101):\n                    new_mask[i, 0 : min(101, int(k1 * (i - left_first)))] = 255\n                for i in range(right_first, 101):\n                    new_mask[i, max(0, int(k2 * (i - right_first) + 100)): 101] = 255\n        new_mask = new_mask[:, :, np.newaxis]\n        if np.sum(new_mask) != 0:\n            test_mask_dict[k] = np.reshape(new_mask, (101, 101))\n    print(""Smooth Rule1 for train detect : "", len(test_mask_dict.keys()))\n\n    os.remove(r\'./jigsaw/json_files/smooth_rule_test.json\')\n    return test_mask_dict\n\ndef smooth_rule_on_test(test_mask_path, vertical_mask, semi_vertical_mask, img_map, img_map_set, map_img):\n    mask = {}\n    test_mask = load_test_mask(test_mask_path)\n    for i in (test_set & img_map_set):\n        if i in vertical_mask.keys(): continue\n        train_mask_tmp = cv2.cvtColor(test_mask[i], cv2.COLOR_BGR2GRAY)\n        if np.sum(train_mask_tmp) == 0: continue\n        # up\n        if np.mean(train_mask_tmp[0]) < 255 and (np.sum(train_mask_tmp[0]) > 255):\n            mapid, x, y = img_map[i][\'mapid\'], img_map[i][\'x\'], img_map[i][\'y\']\n            if str(int(y) - 1) in map_img[mapid][x].keys() and int(mapid) < 159:\n                imgid = map_img[mapid][x][str(int(y) - 1)]\n                if imgid in test_set and np.sum(test_mask[imgid]) == 0: mask.setdefault(imgid, {}).setdefault(""down"", train_mask_tmp[0].tolist())\n        # down\n        if np.mean(train_mask_tmp[100]) < 255 and (np.sum(train_mask_tmp[100]) > 255):\n            # tmp rule\n            if i in semi_vertical_mask.keys(): continue\n            mapid, x, y = img_map[i][\'mapid\'], img_map[i][\'x\'], img_map[i][\'y\']\n            if str(int(y) + 1) in map_img[mapid][x].keys() and int(mapid) < 159:\n                imgid = map_img[mapid][x][str(int(y) + 1)]\n                if imgid in test_set and np.sum(test_mask[imgid]) == 0: mask.setdefault(imgid, {}).setdefault(""up"", train_mask_tmp[100].tolist())\n        # left\n        if np.mean(train_mask_tmp[:, 0]) < 255 and (np.sum(train_mask_tmp[:, 0]) > 255):\n            mapid, x, y = img_map[i][\'mapid\'], img_map[i][\'x\'], img_map[i][\'y\']\n            if str(int(x) - 1) in map_img[mapid].keys() and y in map_img[mapid][str(int(x) - 1)].keys() and int(mapid) < 159:\n                imgid = map_img[mapid][str(int(x) - 1)][y]\n                if imgid in test_set and np.sum(test_mask[imgid]) == 0: mask.setdefault(imgid, {}).setdefault(""right"", train_mask_tmp[:, 0].tolist())\n        # right\n        if np.mean(train_mask_tmp[:, 100]) < 255 and (np.sum(train_mask_tmp[:, 100]) > 255):\n            mapid, x, y = img_map[i][\'mapid\'], img_map[i][\'x\'], img_map[i][\'y\']\n            if str(int(x) + 1) in map_img[mapid].keys() and y in map_img[mapid][str(int(x) + 1)].keys() and int(mapid) < 159:\n                imgid = map_img[mapid][str(int(x) + 1)][y]\n                if imgid in test_set and np.sum(test_mask[imgid]) == 0: mask.setdefault(imgid, {}).setdefault(""left"", train_mask_tmp[:, 100].tolist())\n\n    with open(r\'./jigsaw/json_files/smooth_rule_on_test.json\', \'w\') as f:\n        json.dump(mask, f)\n\n    padding(r\'./jigsaw/json_files/smooth_rule_on_test.json\',\n            r\'./jigsaw/json_files/smooth_rule_on_test_test.json\', img_map, map_img, test_mask)\n\ndef for_smooth_rule_on_test():\n    test_mask_dict = {}\n    with open(r\'./jigsaw/json_files/smooth_rule_on_test_test.json\') as f:\n        rule6_dict = json.load(f)\n    for k, v in rule6_dict.items():\n        new_mask = np.zeros((101, 101))\n        if len(v.keys()) == 1:\n            continue\n        if len(v.keys()) == 2:\n            # up - down\n            if \'up\' in v.keys() and \'down\' in v.keys():\n                up_first, up_last = np.where(np.array(v[\'up\'])==255)[0][0], \\\n                                    np.where(np.array(v[\'up\'])==255)[0][-1]\n                down_first, down_last = np.where(np.array(v[\'down\']) == 255)[0][0], \\\n                                        np.where(np.array(v[\'down\']) == 255)[0][-1]\n                k1, k2 = liner_line(up_first, 0, down_first, 100), liner_line(up_last, 0, down_last, 100)\n                for i in range(new_mask.shape[0]):\n                    new_mask[i, max(0, int(k1 * i + up_first)) : min(101, int(k2 * i + up_last))] = 255\n            if \'left\' in v.keys() and \'right\' in v.keys():\n                left_first, left_last = np.where(np.array(v[\'left\']) == 255)[0][0], \\\n                                        np.where(np.array(v[\'left\']) == 255)[0][-1]\n                right_first, right_last = np.where(np.array(v[\'right\']) == 255)[0][0], \\\n                                          np.where(np.array(v[\'right\']) == 255)[0][-1]\n                k1 = liner_line(left_first, 0, right_first, 100)\n                for i in range(new_mask.shape[1]):\n                    new_mask[min(100, max(0, int(k1 * i + left_first))):, i] = 255\n\n            if \'up\' in v.keys() and \'left\' in v.keys():\n                continue\n            if \'up\' in v.keys() and \'right\' in v.keys():\n                continue\n            if \'down\' in v.keys() and \'left\' in v.keys():\n                if np.mean(v[\'down\']) != 255:\n                    down_first, down_last = np.where(np.array(v[\'down\']) == 255)[0][0], \\\n                                            np.where(np.array(v[\'down\']) == 0)[0][0]\n                else:\n                    down_first, down_last = np.where(np.array(v[\'down\']) == 255)[0][0], \\\n                                            np.where(np.array(v[\'down\']) == 255)[0][-1]\n                left_first, left_last = np.where(np.array(v[\'left\']) == 255)[0][0], \\\n                                        np.where(np.array(v[\'left\']) == 255)[0][-1]\n                k1 = liner_line(0, left_first, down_last, 100)\n                for i in range(left_first, 101):\n                    new_mask[i, 0 : min(101, int(k1 * (i - left_first)))] = 255\n            if \'down\' in v.keys() and \'right\' in v.keys():\n                if np.mean(v[\'down\']) != 255:\n                    down_first, down_last = np.where(np.array(v[\'down\']) == 0)[0][-1], \\\n                                            np.where(np.array(v[\'down\']) == 255)[0][-1]\n                else:\n                    down_first, down_last = np.where(np.array(v[\'down\']) == 255)[0][0], \\\n                                            np.where(np.array(v[\'down\']) == 255)[0][-1]\n                right_first, right_last = np.where(np.array(v[\'right\']) == 255)[0][0], \\\n                                          np.where(np.array(v[\'right\']) == 255)[0][-1]\n                k1 = liner_line(100, right_first, down_first, 100)\n                for i in range(right_first, 101):\n                    new_mask[i, max(0, int(k1 * (i - right_first) + 100)): 101] = 255\n        if len(v.keys()) == 3:\n            continue\n        new_mask = new_mask[:, :, np.newaxis]\n        if np.sum(new_mask) != 0:\n            test_mask_dict[k] = np.reshape(new_mask, (101, 101))\n    print(""Smooth Rule1 for test detect : "", len(test_mask_dict.keys()))\n\n    os.remove(r\'./jigsaw/json_files/smooth_rule_on_test_test.json\')\n    return test_mask_dict\n\ndef make_sub_rule_smoth(test_mask_path):\n    save_csv_path = test_mask_path.replace(\'.csv\', \'\') + \'-smooth.csv\'\n\n    test_mask_dict = load_test_mask(test_mask_path)\n    test_mask_dict1 = for_smooth_rule()\n    test_mask_dict2 = for_smooth_rule_on_test()\n    test_mask_dict.update(test_mask_dict2)\n    test_mask_dict.update(test_mask_dict1)\n    out = []\n    count = 0\n    for id in test_mask_dict:\n        if len(test_mask_dict[id].shape) == 3:\n            sum_tmp = test_mask_dict[id][:, :, 0]\n        else:\n            sum_tmp = test_mask_dict[id]\n        out.append([id, sum_tmp])\n        count += 1\n    submission = create_submission(out)\n    submission.to_csv(save_csv_path, index=None)\n    print(count)\n\ndef jigsaw_folder_dict(path):\n    import json\n    with open(path, \'r\') as load_f:\n        dict = json.load(load_f)\n\n    print(len(dict))\n    return dict\n\ndef make_vertical_rule_sub(jason_type_v1, jason_type_v2, train_mask_path, csv_path):\n    save_csv_path = csv_path.replace(\'.csv\', \'\') + \'-vertical.csv\'\n    print(jason_type_v1)\n    dict_jigsaw_vertical = jigsaw_folder_dict(jason_type_v1)\n    print(jason_type_v2)\n    dict_jigsaw_semi_vertical = jigsaw_folder_dict(jason_type_v2)\n\n    dict = decode_csv(csv_name=csv_path)\n\n    for id in dict_jigsaw_vertical:\n        train_id = dict_jigsaw_vertical[id]\n        train_mask_tmp = os.path.join(train_mask_path,train_id+\'.png\')\n        train_mask = cv2.imread(train_mask_tmp,0) / 255\n        dict[id] = train_mask\n\n    for id in dict_jigsaw_semi_vertical:\n        train_id = dict_jigsaw_semi_vertical[id]\n        train_mask_tmp = os.path.join(train_mask_path,train_id+\'.png\')\n        train_mask = cv2.imread(train_mask_tmp, 0)\n        down_bound = train_mask[100,:]\n        for i in range(101):\n            train_mask[i,:] = down_bound\n        train_mask = train_mask/ 255\n        dict[id] = train_mask\n\n    # print(count)\n    out = []\n    for id in dict:\n        sum_tmp = dict[id]\n        out.append([id, sum_tmp])\n    submission = create_submission(out)\n    submission.to_csv(save_csv_path, index=None)\n\n\ndef vertical_rule_sub(img_map, map_img, vertical_mask, semi_vertical_mask, submission_path):\n    vertical_rule1(img_map, map_img, vertical_mask)\n    vertical_rule2(vertical_mask, img_map, map_img)\n    vertical_rule3(vertical_mask, submission_path, img_map, map_img)\n    semi_vertical_rule(semi_vertical_mask, img_map, map_img)\n    ### resolve rule conflict\n    vertical_rules_combine(rule=[\'1\', \'2\', \'3_1\', \'3_2\', \'3_3\', \'3_4\'])\n    ### make submission on vertical rules\n    make_vertical_rule_sub(\'./jigsaw/json_files/vertical.json\',\n                           \'./jigsaw/json_files/semi_vertical.json\',\n                           train_mask_path, submission_path)\n\n\ndef empty_rule_sub(img_map, map_img, img_map_set, submission_path):\n\n    build_full_train_mask(train_mask_path, img_map, img_map_set)\n    with open(r\'./jigsaw/json_files/jigsaw_map_virtual.json\') as f:\n        img_map_virtual = json.load(f)\n        map_img_virtual = {}\n        for k, v in img_map_virtual.items():\n            map_img_virtual.setdefault(v[\'mapid\'], {}).setdefault(v[\'x\'], {}).setdefault(v[\'y\'], k)\n\n    ### apply empty rules\n    empty_rule1(train_mask_path, img_map, img_map_set, map_img)\n    empty_rule2(train_mask_path, submission_path, img_map, img_map_set, map_img, map_img_virtual)\n    ### make submission on empty rules\n    make_sub_rule_empty(submission_path)\n\ndef smoth_rule_sub(train_mask_path,img_map, map_img, img_map_set, vertical_mask, semi_vertical_mask, submission_path):\n\n    smooth_rule(train_mask_path, submission_path, vertical_mask, img_map, img_map_set, map_img)\n    smooth_rule_on_test(submission_path, vertical_mask, semi_vertical_mask, img_map, img_map_set, map_img)\n    ###make submission on smooth rules\n    make_sub_rule_smoth(submission_path)\n\n\ndef submission_apply_jigsaw_postprocessing(submission_path):\n    # step 1: find all vertical masks in train set and save to json\n    find_all_vertical_mask()\n\n    # step2: load all vertical masks in train set from json\n    with open(r\'./jigsaw/json_files/train_vertical_mask.json\', \'r\') as mf:\n        vertical_mask = json.load(mf)\n        print(\'num_full_vertical_masks: \', len(vertical_mask.keys()))\n\n    with open(r\'./jigsaw/json_files/train_semi_vertical_mask.json\', \'r\') as mf:\n        semi_vertical_mask = json.load(mf)\n        print(\'num_semi_vertical_masks: \', len(semi_vertical_mask.keys()))\n\n    # step3: load jigsaw puzzles\n    with open(r\'./jigsaw/json_files/jigsaw_maps.json\', \'r\') as f:\n        img_map = json.load(f)\n        img_map_set = set(img_map.keys())\n        map_img = {}\n        for k, v in img_map.items():\n            map_img.setdefault(v[\'mapid\'], {}).setdefault(v[\'x\'], {}).setdefault(v[\'y\'], k)\n\n    # apply vertical rules\n    vertical_rule_sub(img_map, map_img, vertical_mask, semi_vertical_mask, submission_path)\n\n    #apply empty rules\n    submission_vertical_path = submission_path.replace(\'.csv\', \'\') + \'-vertical.csv\'\n    empty_rule_sub(img_map, map_img, img_map_set, submission_vertical_path)\n\n    # apply smooth rules\n    submission_empty_path = submission_vertical_path.replace(\'.csv\', \'\') + \'-empty.csv\'\n    smoth_rule_sub(train_mask_path, img_map, map_img, img_map_set, vertical_mask, semi_vertical_mask, submission_empty_path)\n\n    os.remove(submission_vertical_path)\n    os.remove(submission_empty_path)\n\n\n#################### apply rules ####################\nif __name__==\'__main__\':\n    parser = argparse.ArgumentParser(description=\'JIGSAW PUZZLE RULES\')\n    parser.add_argument(\'--submission_path\', required=True, help=\'Path for predicted test masks\')\n    args = parser.parse_args()\n    submission_path = args.submission_path\n    submission_apply_jigsaw_postprocessing(submission_path)\n\n'"
jigsaw/include.py,0,"b""import argparse\nimport cv2\nimport os\nimport numpy as np\nimport json\nimport pandas as pd\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc\nfrom sklearn.neighbors import KDTree\nimport networkx as nx\n\n# add your path here\nroot_dir = r'/data1/shentao/DATA/Kaggle/Salt/Kaggle_salt'"""
jigsaw/jigsaw_puzzles.py,0,"b""from include import *\n\nDisimi_Threshold = 10\nCompa_Threshold = 0.2\n\ntrain_img_path = os.path.join(root_dir,'train/images/')\ntrain_mask_path = os.path.join(root_dir,'train/masks/')\ntest_path = os.path.join(root_dir,'test/images/')\ntrain_file_path = os.path.join(root_dir,'train.csv')\ntest_file_path = os.path.join(root_dir,'test.csv')\n\ndef make_test_list():\n    outfile = open(test_file_path, 'w')\n    print(test_path)\n    for subdir in os.listdir(test_path):\n        outfile.write(subdir + '\\n')\n    outfile.close()\n\ndef make_train_files(train_file_path):\n    train_files = []\n    infile = open(train_file_path, 'r')\n    for line in infile:\n        line = line.strip()\n        row = line.split(',')\n        if row[0] == 'id':\n            continue\n        train_files.append(row[0] + '.png')\n    return np.asarray(train_files)\n\ndef make_test_files(test_file_path):\n    test_files = []\n    infile = open(test_file_path, 'r')\n    for line in infile:\n        line = line.strip()\n        row = line.split(',')\n        if row[0] == 'id':\n            continue\n        test_files.append(row[0])\n    return np.asarray(test_files)\n\nmake_test_list()\n\ntrain_files = make_train_files(train_file_path)\ntest_files = make_test_files(test_file_path)\nall_files = np.append(train_files,test_files, axis=0)\n\ndef load_img(img_path):\n    img = cv2.imread(img_path)\n    return img\n\nimg_train = np.zeros((4000, 101, 101))\nmask_train = np.zeros((4000, 101, 101))\nfor i, file in enumerate(train_files):\n    img_train[i] = np.expand_dims(np.array(load_img(train_img_path + file))[:, :, 0], axis=0)\n    mask_train[i] = np.expand_dims(np.array(load_img(train_mask_path + file))[:, :, 0], axis=0)\n\nimg_test = np.zeros((18000, 101, 101))\nfor i, file in enumerate(test_files):\n    img_test[i] = np.expand_dims(np.array(load_img(test_path + file))[:, :, 0], axis=0)\n\nall_arr = np.append(img_train, img_test, axis=0)\nall_arr = all_arr / 255.0\ndel img_train, img_test\ngc.collect()\n\nall_u_ex = 2*all_arr[:, :, 0] - all_arr[:, :, 1]\nall_u_ex = (all_u_ex - np.mean(all_u_ex, axis=0)) / np.std(all_u_ex, axis=0, ddof=1)\n\nall_d_ex = 2*all_arr[:, :, 100]-all_arr[:, :, 99]\nall_d_ex = (all_d_ex - np.mean(all_d_ex, axis=0)) / np.std(all_d_ex, axis=0, ddof=1)\n\nall_l_ex = 2*all_arr[:, 0, :]-all_arr[:, 1, :]\nall_l_ex = (all_l_ex - np.mean(all_l_ex, axis=0)) / np.std(all_l_ex, axis=0, ddof=1)\n\nall_r_ex = 2*all_arr[:, 100, :]-all_arr[:, 99, :]\nall_r_ex = (all_r_ex - np.mean(all_r_ex, axis=0)) / np.std(all_r_ex, axis=0, ddof=1)\n\nall_u_ex = np.transpose((np.transpose(all_u_ex) - np.mean(all_u_ex, axis=1)) / np.std(all_u_ex, axis=1, ddof=1))\nall_d_ex = np.transpose((np.transpose(all_d_ex) - np.mean(all_d_ex, axis=1)) / np.std(all_d_ex, axis=1, ddof=1))\nall_l_ex = np.transpose((np.transpose(all_l_ex) - np.mean(all_l_ex, axis=1)) / np.std(all_l_ex, axis=1, ddof=1))\nall_r_ex = np.transpose((np.transpose(all_r_ex) - np.mean(all_r_ex, axis=1)) / np.std(all_r_ex, axis=1, ddof=1))\n\ntree = KDTree(all_u_ex)\ndist_ud, ind_ud = tree.query(all_d_ex, k=2)\n\ntree = KDTree(all_d_ex)\ndist_du, ind_du = tree.query(all_u_ex, k=2)\n\ntree = KDTree(all_l_ex)\ndist_lr, ind_lr = tree.query(all_r_ex, k=2)\n\ntree = KDTree(all_r_ex)\ndist_rl, ind_rl = tree.query(all_l_ex, k=2)\n\nfrom pandas import Series,DataFrame\n\ndef gen_mosaic(ad, ac):\n    # generate candidates left-right\n    length = dist_lr.shape[0]\n    dlr_dict = {'i1': ind_lr[:, 0],\n                'i2': ind_lr[:, 1],\n                'd1': dist_lr[:, 0],\n                'd2': dist_lr[:, 1],\n                'i0': np.asarray([i for i in range(length)]),\n                'c': 1 - dist_lr[:, 0] / dist_lr[:, 1]}\n    dlr = DataFrame(dlr_dict)\n\n    drl_dict = {'i1': ind_rl[:, 0],\n                'i2': ind_rl[:, 1],\n                'd1': dist_rl[:, 0],\n                'd2': dist_rl[:, 1],\n                'i0': np.asarray([i for i in range(length)]),\n                'c': 1 - dist_rl[:, 0] / dist_rl[:, 1]}\n    drl = DataFrame(drl_dict)\n\n    bb = pd.merge(dlr, drl, how='inner', left_on='i0', right_on='i1')\n    # filter by disimilarity and compatibility\n    bb2 = bb.loc[(bb.i0_x != bb.i1_x) & (bb.d1_x < ad) & (bb.c_x > ac) & (bb.c_y > ac)]\n    # find left-right strips\n    nt = length\n    lcols = []\n    eval = np.zeros(nt, dtype=int) > 0\n    for i in range(length):\n        if not eval[i]:\n            lt = np.array([i])\n            eval[i] = False\n            cond = True\n            i0 = i\n            while (cond):\n                i1 = np.array(bb2[bb.i0_x == i0]['i1_x'])\n                if len(i1) == 1 and np.where(lt == i1[0])[0].shape[0] == 0:\n                    lt = np.concatenate((i1, lt))\n                    i0 = i1[0]\n                    eval[i1[0]] = True\n                else:\n                    cond = False\n            cond = True\n            i0 = i\n            while (cond):\n                i1 = np.array(bb2[bb.i1_x == i0]['i0_x'])\n                if len(i1) == 1 and np.where(lt == i1[0])[0].shape[0] == 0:\n                    lt = np.concatenate((lt, i1))\n                    i0 = i1[0]\n                    eval[i1[0]] = True\n                else:\n                    cond = False\n            if lt.shape[0] > 1:\n                lcols.append(lt.tolist())\n\n    # same for up-down\n    length = dist_du.shape[0]\n    ddu_dict = {'i1': ind_du[:, 0],\n                'i2': ind_du[:, 1],\n                'd1': dist_du[:, 0],\n                'd2': dist_du[:, 1],\n                'i0': np.asarray([i for i in range(length)]),\n                'c': 1 - dist_du[:, 0] / dist_du[:, 1]}\n    ddu = DataFrame(ddu_dict)\n\n    dud_dict = {'i1': ind_ud[:, 0],\n                'i2': ind_ud[:, 1],\n                'd1': dist_ud[:, 0],\n                'd2': dist_ud[:, 1],\n                'i0': np.asarray([i for i in range(length)]),\n                'c': 1 - dist_ud[:, 0] / dist_ud[:, 1]}\n    dud = DataFrame(dud_dict)\n\n    bb = pd.merge(ddu, dud, how='inner', left_on='i0', right_on='i1')\n    # filter by disimilarity and compatibility\n    bb = bb.loc[(bb.i0_x != bb.i1_x) & (bb.d1_x < ad) & (bb.c_x > ac) & (bb.c_y > ac)]\n\n    # Generate up-down strips\n    nt = length\n    lrows = []\n    eval = np.zeros(nt, dtype=int) > 0\n    for i in range(length):\n        if not eval[i]:\n            lt = np.array([i])\n            eval[i] = False\n            cond = True\n            i0 = i\n            while (cond):\n                i1 = np.array(bb[bb.i0_x == i0]['i1_x'])\n                if len(i1) == 1 and np.where(lt == i1[0])[0].shape[0] == 0:\n                    lt = np.concatenate((i1, lt))\n                    i0 = i1[0]\n                    eval[i1[0]] = True\n                else:\n                    cond = False\n            cond = True\n            i0 = i\n            while (cond):\n                i1 = np.array(bb[bb.i1_x == i0]['i0_x'])\n                if len(i1) == 1 and np.where(lt == i1[0])[0].shape[0] == 0:\n                    lt = np.concatenate((lt, i1))\n                    i0 = i1[0]\n                    eval[i1[0]] = True\n                else:\n                    cond = False\n            if lt.shape[0] > 1:\n                lrows.append(lt.tolist())\n    # Finally combine rows and colums\n    rc = - np.ones(shape=(nt, 2), dtype=int)\n    for i in range(len(lrows)):\n        for j in lrows[i]:\n            rc[j, 0] = i\n    for i in range(len(lcols)):\n        for j in lcols[i]:\n            rc[j, 1] = i\n\n    bt = pd.concat([bb, bb2], ignore_index=True)\n\n    nodes_set = set()\n    nodes_list = []\n    edge_set = set()\n    edge_list = []\n    for i in range(bt.shape[0]):\n        nodes_set.add(bt['i0_x'][i])\n        nodes_set.add(bt['i1_x'][i])\n        edge_list.append((bt['i0_x'][i], bt['i1_x'][i]))\n    #     edge_set.add((bt['i0_x'][i], bt['i1_x'][i]))\n    for i in nodes_set:\n        nodes_list.append(i)\n\n    G = nx.Graph(edge_list)\n    clu = nx.connected_components(G)\n\n    ls_tmp = {}\n    count = 0\n    for i in clu:\n        count += 1\n        tmp = sorted(i)\n        key = tmp[0]\n        value = tmp\n        ls_tmp.setdefault(key, value)\n    print(count)\n\n    ls = []\n    for i in range(length):\n        if i in ls_tmp.keys():\n            ls.append(ls_tmp[i])\n        else:\n            ls.append([i])\n\n    lls = []\n    for i in ls:\n        lls.append(len(i))\n\n    dd = {}\n    for i in range(len(lls)):\n        dd.setdefault(i, lls[i])\n\n    dd = sorted(dd.items(), key=lambda item: item[1], reverse=True)\n    return dd, ls, rc, lrows, lcols\n\ndd, ls, rc, lrows, lcols = gen_mosaic(Disimi_Threshold, Compa_Threshold)\n# Complete iteratively a mosaic from a seed image\ndef complete(se, mat):\n    ir0 = rc[se, 0]\n    ic0 = rc[se, 1]\n\n    if len(np.where(mat[:, 0] == se)[0]) > 0:\n        x0 = mat[np.where(mat[:, 0] == se)[0], 1]\n        y0 = mat[np.where(mat[:, 0] == se)[0], 2]\n    else:\n        x0 = mat[0, 1]\n        y0 = mat[0, 2]\n\n    if ir0 > -1:\n        r0 = lrows[ir0]\n        # r0 = r0[::-1]\n        for i in range(len(r0)):\n            if len(np.where(mat[:, 0] == r0[i])[0]) == 0:\n                mat_tmp = np.array([[r0[i], x0 - np.where(r0 == se)[0] + i, y0, -1]])\n                mat = np.concatenate((mat, mat_tmp), axis=0)\n\n    if ic0 > -1:\n        c0 = lcols[ic0]\n        c0 = c0[::-1]\n        for i in range(len(c0)):\n            if len(np.where(mat[:, 0] == c0[i])[0]) == 0:\n                mat_tmp = np.array([[c0[i], x0, y0 - np.where(c0 == se)[0] + i, -1]])\n                mat = np.concatenate((mat, mat_tmp), axis=0)\n    mat[np.where(mat[:, 0] == se)[0][0], 3] = 1\n    # mat = np.unique(mat, axis=0)\n    return mat\n\ndef gen_mos(se):\n    mat = -np.ones(shape=(1, 4), dtype=int)\n    mat[0, 0] = se\n    while np.sum(mat[:, 3]) < mat.shape[0]:\n        for i in range(mat.shape[0]):\n            if mat[i, 3] == -1:\n                mat = complete(mat[i, 0], mat)\n    mat[:, 1] = mat[:, 1] - np.min(mat[:, 1])\n    mat[:, 2] = mat[:, 2] - np.min(mat[:, 2])\n    return mat\n\n####### generate jigsaw puzzles and save to files #######\ndef solve_jigsaw_puzzles(dir):\n    jigsaw_puzzles = {}\n    if not os.path.exists(os.path.join(dir,'jigsaw_file')):\n        os.mkdir(os.path.join(dir,'jigsaw_file'))\n    if not os.path.exists(os.path.join(dir,'json_files')):\n        os.mkdir(os.path.join(dir,'json_files'))\n    for ii in range(0, 514):\n        mat = gen_mos(ls[dd[ii][0]][0])\n        var1 = mat[:, 0]\n        var1_name = []\n        for i in var1:\n            var1_name.append(all_files[i].split('.')[0])\n        var1_name = np.asarray(var1_name)\n        var2 = mat[:, 1]\n        var3 = mat[:, 2]\n        df1 = DataFrame({'id': var1_name, 'x': var2, 'y': var3})\n        df1.to_csv(os.path.join(dir, r'jigsaw_file/jigsaw-' + str(ii) + '.csv'), index=False, index_label=False, header=False)\n        for i in range(len(var1_name)):\n            jigsaw_puzzles.setdefault(var1_name[i], {'x': str(mat[i, 1]), 'y': str(mat[i, 2]), 'mapid': str(ii)})\n        with open(os.path.join(dir,r'json_files/jigsaw_maps.json'), 'w') as f:\n            json.dump(jigsaw_puzzles, f)\n\n\n"""
loss/__init__.py,0,b''
loss/bce_losses.py,10,"b'import numpy as np\nimport torch\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torch.nn as nn\nfrom functools import partial\n\n\nclass DiceLoss(nn.Module):\n    def __init__(self, smooth=0, eps=1e-7):\n        super(DiceLoss, self).__init__()\n        self.smooth = smooth\n        self.eps = eps\n\n    def forward(self, output, target):\n        return 1 - (2 * torch.sum(output * target) + self.smooth) / (\n                torch.sum(output) + torch.sum(target) + self.smooth + self.eps)\n\n\ndef mixed_dice_bce_loss(output, target, dice_weight=0.2, dice_loss=None,\n                        bce_weight=0.9, bce_loss=None,\n                        smooth=0, dice_activation=\'sigmoid\'):\n\n    num_classes = output.size(1)\n    target = target[:, :num_classes, :, :].long()\n    if bce_loss is None:\n        bce_loss = nn.BCEWithLogitsLoss()\n    if dice_loss is None:\n        dice_loss = multiclass_dice_loss\n    return dice_weight * dice_loss(output, target, smooth, dice_activation) + bce_weight * bce_loss(output, target)\n\n\ndef multiclass_dice_loss(output, target, smooth=0, activation=\'softmax\'):\n    """"""Calculate Dice Loss for multiple class output.\n\n    Args:\n        output (torch.Tensor): Model output of shape (N x C x H x W).\n        target (torch.Tensor): Target of shape (N x H x W).\n        smooth (float, optional): Smoothing factor. Defaults to 0.\n        activation (string, optional): Name of the activation function, softmax or sigmoid. Defaults to \'softmax\'.\n\n    Returns:\n        torch.Tensor: Loss value.\n\n    """"""\n    if activation == \'softmax\':\n        activation_nn = torch.nn.Softmax2d()\n    elif activation == \'sigmoid\':\n        activation_nn = torch.nn.Sigmoid()\n    else:\n        raise NotImplementedError(\'only sigmoid and softmax are implemented\')\n\n    loss = 0\n    dice = DiceLoss(smooth=smooth)\n    output = activation_nn(output)\n    num_classes = output.size(1)\n    target.data = target.data.float()\n    for class_nr in range(num_classes):\n        loss += dice(output[:, class_nr, :, :], target[:, class_nr, :, :])\n    return loss / num_classes\n\ndef where(cond, x_1, x_2):\n    cond = cond.long()\n    return (cond * x_1) + ((1 - cond) * x_2)\n'"
loss/cyclic_lr.py,2,"b'import torch\nimport math\nfrom torch.optim.lr_scheduler import _LRScheduler\n\nclass CosineAnnealingLR_with_Restart(_LRScheduler):\n    """"""Set the learning rate of each parameter group using a cosine annealing\n    schedule, where :math:`\\eta_{max}` is set to the initial lr and\n    :math:`T_{cur}` is the number of epochs since the last restart in SGDR:\n\n    .. math::\n\n        \\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})(1 +\n        \\cos(\\frac{T_{cur}}{T_{max}}\\pi))\n\n    When last_epoch=-1, sets initial lr as lr.\n\n    It has been proposed in\n    `SGDR: Stochastic Gradient Descent with Warm Restarts`_. The original pytorch\n    implementation only implements the cosine annealing part of SGDR,\n    I added my own implementation of the restarts part.\n\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        T_max (int): Maximum number of iterations.\n        T_mult (float): Increase T_max by a factor of T_mult\n        eta_min (float): Minimum learning rate. Default: 0.\n        last_epoch (int): The index of last epoch. Default: -1.\n        model (pytorch model): The model to save.\n        out_dir (str): Directory to save snapshots\n        take_snapshot (bool): Whether to save snapshots at every restart\n\n    .. _SGDR\\: Stochastic Gradient Descent with Warm Restarts:\n        https://arxiv.org/abs/1608.03983\n    """"""\n\n    def __init__(self, optimizer, T_max, T_mult, model, out_dir, take_snapshot, eta_min=0, last_epoch=-1):\n        self.T_max = T_max\n        self.T_mult = T_mult\n        self.Te = self.T_max\n        self.eta_min = eta_min\n        self.current_epoch = last_epoch\n\n        self.model = model\n        self.out_dir = out_dir\n        self.take_snapshot = take_snapshot\n\n        self.lr_history = []\n\n        super(CosineAnnealingLR_with_Restart, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        new_lrs = [self.eta_min + (base_lr - self.eta_min) *\n                   (1 + math.cos(math.pi * self.current_epoch / self.Te)) / 2\n                   for base_lr in self.base_lrs]\n\n        self.lr_history.append(new_lrs)\n        return new_lrs\n\n    def step(self, epoch=None):\n        if epoch is None:\n            epoch = self.last_epoch + 1\n        self.last_epoch = epoch\n        self.current_epoch += 1\n\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n            param_group[\'lr\'] = lr\n\n        ## restart\n        if self.current_epoch == self.Te:\n            print(""restart at epoch {:03d}"".format(self.last_epoch + 1))\n\n            if self.take_snapshot:\n                torch.save({\n                    \'epoch\': self.T_max,\n                    \'state_dict\': self.model.state_dict()\n                }, self.out_dir + ""Weight/"" + \'snapshot_e_{:03d}.pth.tar\'.format(self.T_max))\n\n            ## reset epochs since the last reset\n            self.current_epoch = 0\n\n            ## reset the next goal\n            self.Te = int(self.Te * self.T_mult)\n            self.T_max = self.T_max + self.Te'"
loss/lovasz_losses.py,9,"b'""""""\nLovasz-Softmax and Jaccard hinge loss in PyTorch\nMaxim Berman 2018 ESAT-PSI KU Leuven (MIT License)\n""""""\n\nfrom __future__ import print_function, division\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport numpy as np\ntry:\n    from itertools import  ifilterfalse\nexcept ImportError: # py3k\n    from itertools import  filterfalse\n\n\ndef lovasz_grad(gt_sorted):\n    """"""\n    Computes gradient of the Lovasz extension w.r.t sorted errors\n    See Alg. 1 in paper\n    """"""\n    p = len(gt_sorted)\n    gts = gt_sorted.sum()\n    intersection = gts - gt_sorted.float().cumsum(0)\n    union = gts + (1 - gt_sorted).float().cumsum(0)\n    jaccard = 1. - intersection / union\n    if p > 1: # cover 1-pixel case\n        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n    return jaccard\n\n\ndef iou_binary(preds, labels, EMPTY=1., ignore=None, per_image=True):\n    """"""\n    IoU for foreground class\n    binary: 1 foreground, 0 background\n    """"""\n    if not per_image:\n        preds, labels = (preds,), (labels,)\n    ious = []\n    for pred, label in zip(preds, labels):\n        intersection = ((label == 1) & (pred == 1)).sum()\n        union = ((label == 1) | ((pred == 1) & (label != ignore))).sum()\n        if not union:\n            iou = EMPTY\n        else:\n            iou = float(intersection) / union\n        ious.append(iou)\n    iou = mean(ious)    # mean accross images if per_image\n    return 100 * iou\n\n\ndef iou(preds, labels, C, EMPTY=1., ignore=None, per_image=False):\n    """"""\n    Array of IoU for each (non ignored) class\n    """"""\n    if not per_image:\n        preds, labels = (preds,), (labels,)\n    ious = []\n    for pred, label in zip(preds, labels):\n        iou = []    \n        for i in range(C):\n            if i != ignore: # The ignored label is sometimes among predicted classes (ENet - CityScapes)\n                intersection = ((label == i) & (pred == i)).sum()\n                union = ((label == i) | ((pred == i) & (label != ignore))).sum()\n                if not union:\n                    iou.append(EMPTY)\n                else:\n                    iou.append(float(intersection) / union)\n        ious.append(iou)\n    ious = map(mean, zip(*ious)) # mean accross images if per_image\n    return 100 * np.array(ious)\n\n\n# --------------------------- BINARY LOSSES ---------------------------\n\n\ndef lovasz_hinge(logits, labels, per_image=True, ignore=None):\n    """"""\n    Binary Lovasz hinge loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      per_image: compute the loss per image instead of per batch\n      ignore: void class id\n    """"""\n    if per_image:\n        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n                          for log, lab in zip(logits, labels))\n    else:\n        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n    return loss\n\n\ndef lovasz_hinge_flat(logits, labels):\n    """"""\n    Binary Lovasz hinge loss\n      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n      labels: [P] Tensor, binary ground truth labels (0 or 1)\n      ignore: label to ignore\n    """"""\n    if len(labels) == 0:\n        # only void pixels, the gradients should be 0\n        return logits.sum() * 0.\n    signs = 2. * labels.float() - 1.\n    errors = (1. - logits * signs)\n    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n    perm = perm.data\n    gt_sorted = labels[perm]\n    grad = lovasz_grad(gt_sorted)\n    # loss = torch.dot(F.relu(errors_sorted), grad)\n    # print(\'elu!!!!!!\')\n    loss = torch.dot(F.elu(errors_sorted)+1, grad)\n    # loss = torch.dot(F.leaky_relu(errors_sorted)+1, grad)\n    return loss\n\n\ndef flatten_binary_scores(scores, labels, ignore=None):\n    """"""\n    Flattens predictions in the batch (binary case)\n    Remove labels equal to \'ignore\'\n    """"""\n    scores = scores.view(-1)\n    labels = labels.view(-1)\n    if ignore is None:\n        return scores, labels\n    valid = (labels != ignore)\n    vscores = scores[valid]\n    vlabels = labels[valid]\n    return vscores, vlabels\n\n\nclass StableBCELoss(torch.nn.modules.Module):\n    def __init__(self):\n         super(StableBCELoss, self).__init__()\n    def forward(self, input, target):\n         neg_abs = - input.abs()\n         loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\n         return loss.mean()\n\n\ndef binary_xloss(logits, labels, ignore=None):\n    """"""\n    Binary Cross entropy loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      ignore: void class id\n    """"""\n    logits, labels = flatten_binary_scores(logits, labels, ignore)\n    loss = StableBCELoss()(logits, Variable(labels.float()))\n    return loss\n\n\n# --------------------------- MULTICLASS LOSSES ---------------------------\n\n\ndef lovasz_softmax(probas, labels, only_present=False, per_image=False, ignore=None):\n    """"""\n    Multi-class Lovasz-Softmax loss\n      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1)\n      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n      only_present: average only on classes present in ground truth\n      per_image: compute the loss per image instead of per batch\n      ignore: void class labels\n    """"""\n    if per_image:\n        loss = mean(lovasz_softmax_flat(*flatten_probas(prob.unsqueeze(0), lab.unsqueeze(0), ignore), only_present=only_present)\n                          for prob, lab in zip(probas, labels))\n    else:\n        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore), only_present=only_present)\n    return loss\n\n\ndef lovasz_softmax_flat(probas, labels, only_present=False):\n    """"""\n    Multi-class Lovasz-Softmax loss\n      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n      only_present: average only on classes present in ground truth\n    """"""\n    C = probas.size(1)\n    losses = []\n    for c in range(C):\n        fg = (labels == c).float() # foreground for class c\n        if only_present and fg.sum() == 0:\n            continue\n        errors = (Variable(fg) - probas[:, c]).abs()\n        errors_sorted, perm = torch.sort(errors, 0, descending=True)\n        perm = perm.data\n        fg_sorted = fg[perm]\n        losses.append(torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted))))\n    return mean(losses)\n\n\ndef flatten_probas(probas, labels, ignore=None):\n    """"""\n    Flattens predictions in the batch\n    """"""\n    B, C, H, W = probas.size()\n    probas = probas.permute(0, 2, 3, 1).contiguous().view(-1, C)  # B * H * W, C = P, C\n    labels = labels.view(-1)\n    if ignore is None:\n        return probas, labels\n    valid = (labels != ignore)\n    vprobas = probas[valid.nonzero().squeeze()]\n    vlabels = labels[valid]\n    return vprobas, vlabels\n\ndef xloss(logits, labels, ignore=None):\n    """"""\n    Cross entropy loss\n    """"""\n    return F.cross_entropy(logits, Variable(labels), ignore_index=255)\n\n\n# --------------------------- HELPER FUNCTIONS ---------------------------\n\ndef mean(l, ignore_nan=False, empty=0):\n    """"""\n    nanmean compatible with generators.\n    """"""\n    l = iter(l)\n    if ignore_nan:\n        l = ifilterfalse(np.isnan, l)\n    try:\n        n = 1\n        acc = next(l)\n    except StopIteration:\n        if empty == \'raise\':\n            raise ValueError(\'Empty mean\')\n        return empty\n    for n, v in enumerate(l, 2):\n        acc += v\n    if n == 1:\n        return acc\n    return acc / n\n'"
loss/metric.py,0,"b'import numpy as np\n\n### metric #################################################################################\n# https://github.com/neptune-ml/open-solution-salt-detection/blob/master/src/metrics.py\n# https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/61550\nEPS = 0.00001\n\ndef do_kaggle_metric(predict, truth, threshold=0.5):\n    N = len(predict)\n    predict = predict.reshape(N,-1)\n    truth   = truth.reshape(N,-1)\n\n    predict = predict>threshold\n    truth   = truth>0.5\n    intersection = truth & predict\n    union        = truth | predict\n\n    iou = intersection.sum(1)/(union.sum(1)+EPS)\n    #-------------------------------------------\n    result = []\n    precision = []\n    is_empty_truth   = (truth.sum(1)==0)\n    is_empty_predict = (predict.sum(1)==0)\n\n    threshold = np.array([0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95])\n    for t in threshold:\n        p = iou>=t\n\n        tp  = (~is_empty_truth)  & (~is_empty_predict) & (iou> t)\n        fp  = (~is_empty_truth)  & (~is_empty_predict) & (iou<=t)\n        fn  = (~is_empty_truth)  & ( is_empty_predict)\n        fp_empty = ( is_empty_truth)  & (~is_empty_predict)\n        tn_empty = ( is_empty_truth)  & ( is_empty_predict)\n\n        p = (tp + tn_empty) / (tp + tn_empty + fp + fp_empty + fn)\n\n        result.append( np.column_stack((tp,fp,fn,tn_empty,fp_empty)) )\n        precision.append(p)\n\n    result = np.array(result).transpose(1,2,0)\n    precision = np.column_stack(precision)\n    precision = precision.mean(1)\n\n    return precision, result, threshold\n\n\n \n'"
model/__init__.py,0,b''
model/ibnnet.py,6,"b'from __future__ import division\n"""""" \nCreates a ResNeXt Model as defined in:\nXie, S., Girshick, R., Dollar, P., Tu, Z., & He, K. (2016). \nAggregated residual transformations for deep neural networks. \narXiv preprint arXiv:1611.05431.\nimport from https://github.com/facebookresearch/ResNeXt/blob/master/models/resnext.lua\n""""""\nimport math\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import init\nimport torch\n\n__all__ = [\'resnext50_ibn_a\', \'resnext101_ibn_a\', \'resnext152_ibn_a\']\n\nclass IBN(nn.Module):\n    def __init__(self, planes):\n        super(IBN, self).__init__()\n        half1 = int(planes/2)\n        self.half = half1\n        half2 = planes - half1\n        self.IN = nn.InstanceNorm2d(half1, affine=True)\n        self.BN = nn.BatchNorm2d(half2)\n    \n    def forward(self, x):\n        split = torch.split(x, self.half, 1)\n        out1 = self.IN(split[0].contiguous())\n        out2 = self.BN(split[1].contiguous())\n        out = torch.cat((out1, out2), 1)\n        return out\n\nclass Bottleneck(nn.Module):\n    """"""\n    RexNeXt bottleneck type C\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, baseWidth, cardinality, stride=1, downsample=None, ibn=False):\n        """""" Constructor\n        Args:\n            inplanes: input channel dimensionality\n            planes: output channel dimensionality\n            baseWidth: base width.\n            cardinality: num of convolution groups.\n            stride: conv stride. Replaces pooling layer.\n        """"""\n        super(Bottleneck, self).__init__()\n\n        D = int(math.floor(planes * (baseWidth / 64)))\n        C = cardinality\n        self.conv1 = nn.Conv2d(inplanes, D*C, kernel_size=1, stride=1, padding=0, bias=False)\n        if ibn:\n            self.bn1 = IBN(D*C)\n        else:\n            self.bn1 = nn.BatchNorm2d(D*C)\n        self.conv2 = nn.Conv2d(D*C, D*C, kernel_size=3, stride=stride, padding=1, groups=C, bias=False)\n        self.bn2 = nn.BatchNorm2d(D*C)\n        self.conv3 = nn.Conv2d(D*C, planes * 4, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.downsample = downsample\n        \n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNeXt(nn.Module):\n    """"""\n    ResNext optimized for the ImageNet dataset, as specified in\n    https://arxiv.org/pdf/1611.05431.pdf\n    """"""\n    def __init__(self, baseWidth, cardinality, layers, num_classes):\n        """""" Constructor\n        Args:\n            baseWidth: baseWidth for ResNeXt.\n            cardinality: number of convolution groups.\n            layers: config of layers, e.g., [3, 4, 6, 3]\n            num_classes: number of classes\n        """"""\n        super(ResNeXt, self).__init__()\n        block = Bottleneck\n\n        self.cardinality = cardinality\n        self.baseWidth = baseWidth\n        self.num_classes = num_classes\n        self.inplanes = 64\n        self.output_size = 64\n\n        self.conv1 = nn.Conv2d(3, 64, 7, 2, 3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(7)      \n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        self.conv1.weight.data.normal_(0, math.sqrt(2. / (7 * 7 * 64)))\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.InstanceNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        """""" Stack n bottleneck modules where n is inferred from the depth of the network.\n        Args:\n            block: block type used to construct ResNext\n            planes: number of output channels (need to multiply by block.expansion)\n            blocks: number of blocks to be built\n            stride: factor to reduce the spatial dimensionality in the first bottleneck of the block.\n        Returns: a Module consisting of n sequential bottlenecks.\n        """"""\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        ibn = True\n        if planes == 512:\n            ibn = False\n        layers.append(block(self.inplanes, planes, self.baseWidth, self.cardinality, stride, downsample, ibn))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, self.baseWidth, self.cardinality, 1, None, ibn))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool1(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef resnext50_ibn_a(baseWidth, cardinality):\n    """"""\n    Construct ResNeXt-50.\n    """"""\n    model = ResNeXt(baseWidth, cardinality, [3, 4, 6, 3], 1000)\n    return model\n\nfrom utils import state_dict_remove_moudle\ndef resnext101_ibn_a(baseWidth, cardinality, pretrained = True):\n    """"""\n    Construct ResNeXt-101.\n    """"""\n    model = ResNeXt(baseWidth, cardinality, [3, 4, 23, 3], 1000)\n    if pretrained:\n        state_dict = torch.load(r\'/data/shentao/Airbus/code/pretrained_model/resnext101_ibn_a.pth.tar\')[\'state_dict\']\n        state_dict = state_dict_remove_moudle(state_dict, model)\n        model.load_state_dict(state_dict)\n\n    return model\n\n\ndef resnext152_ibn_a(baseWidth, cardinality):\n    """"""\n    Construct ResNeXt-152.\n    """"""\n    model = ResNeXt(baseWidth, cardinality, [3, 8, 36, 3], 1000)\n    return model\n'"
model/model.py,27,"b""import torch\nimport torch.nn as nn\nimport torchvision\nimport torch.nn.functional as F\nfrom ibnnet import resnext101_ibn_a,IBN\nfrom senet import se_resnext50_32x4d\n\ndef conv3x3(in_, out, bias=True):\n    return nn.Conv2d(in_, out, 3, padding=1, bias=bias)\n\ndef conv7x7(in_, out, bias=True):\n    return nn.Conv2d(in_, out, 7, padding=3, bias=bias)\n\ndef conv5x5(in_, out, bias=True):\n    return nn.Conv2d(in_, out, 5, padding=2, bias=bias)\n\ndef conv1x1(in_, out, bias=True):\n    return nn.Conv2d(in_, out, 1, padding=0, bias=bias)\n\nclass ConvRelu(nn.Module):\n    def __init__(self, in_, out, kernel_size, norm_type = None):\n        super(ConvRelu,self).__init__()\n\n        is_bias = True\n        self.norm_type = norm_type\n        if norm_type == 'batch_norm':\n            self.norm = nn.BatchNorm2d(out)\n            is_bias = False\n\n        elif norm_type == 'instance_norm':\n            self.norm = nn.InstanceNorm2d(out)\n            is_bias = True\n\n        if kernel_size == 3:\n            self.conv = conv3x3(in_, out, is_bias)\n        elif kernel_size == 7:\n            self.conv = conv7x7(in_, out, is_bias)\n        elif kernel_size == 5:\n            self.conv = conv5x5(in_, out, is_bias)\n        elif kernel_size == 1:\n            self.conv = conv1x1(in_, out, is_bias)\n\n        self.activation = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        if self.norm_type is not None:\n            x = self.conv(x)\n            x = self.norm(x)\n            x = self.activation(x)\n        else:\n            x = self.conv(x)\n            x = self.activation(x)\n        return x\n\nclass ImprovedIBNaDecoderBlock(nn.Module):\n    def __init__(self, in_channels, n_filters):\n        super(ImprovedIBNaDecoderBlock, self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels, in_channels // 4, 1)\n        self.norm1 = IBN(in_channels // 4)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.deconv2 = nn.ConvTranspose2d(in_channels // 4, in_channels // 4, 4, stride=2, padding=1)\n        self.norm2 = nn.BatchNorm2d(in_channels // 4)\n\n        self.conv3 = nn.Conv2d(in_channels // 4, n_filters, 1)\n        self.norm3 = nn.BatchNorm2d(n_filters)\n\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.norm1(x)\n        x = self.relu(x)\n        x = self.deconv2(x)\n        x = self.norm2(x)\n        x = self.relu(x)\n        x = self.conv3(x)\n        x = self.norm3(x)\n        x = self.relu(x)\n        return x\n\nclass SELayer(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n                nn.Linear(channel, int(channel/reduction), bias=False),\n                nn.ReLU(inplace=True),\n                nn.Linear(int(channel/reduction), channel, bias=False),\n                nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y.expand_as(x)\n\nclass SCSEBlock(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SCSEBlock, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n\n        self.channel_excitation = nn.Sequential(nn.Linear(channel, int(channel//reduction)),\n                                                nn.ReLU(inplace=True),\n                                                nn.Linear(int(channel//reduction), channel),\n                                                nn.Sigmoid())\n\n        self.spatial_se = nn.Sequential(nn.Conv2d(channel, 1, kernel_size=1,\n                                                  stride=1, padding=0, bias=False),\n                                        nn.Sigmoid())\n\n    def forward(self, x):\n        bahs, chs, _, _ = x.size()\n\n        # Returns a new tensor with the same data as the self tensor but of a different size.\n        chn_se = self.avg_pool(x).view(bahs, chs)\n        chn_se = self.channel_excitation(chn_se).view(bahs, chs, 1, 1)\n        chn_se = torch.mul(x, chn_se)\n\n        spa_se = self.spatial_se(x)\n        spa_se = torch.mul(x, spa_se)\n        return torch.add(chn_se, 1, spa_se)\n\nclass Decoder(nn.Module):\n    def __init__(self,in_channels, channels, out_channels):\n        super(Decoder, self).__init__()\n        self.conv1 = nn.Sequential(nn.Conv2d(in_channels, channels, kernel_size=3,padding=1),\n                                   nn.BatchNorm2d(channels),\n                                   nn.ReLU(inplace=True))\n        self.conv2 = nn.Sequential(nn.Conv2d(channels, out_channels, kernel_size=3,padding=1),\n                                   nn.BatchNorm2d(out_channels),\n                                   nn.ReLU(inplace=True))\n        self.SCSE = SCSEBlock(out_channels)\n\n    def forward(self, x, e = None):\n        x = F.upsample(x, scale_factor=2, mode='bilinear')\n        if e is not None:\n            x = torch.cat([x,e],1)\n            x = F.dropout2d(x, p = 0.50)\n\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.SCSE(x)\n        return x\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, outplanes):\n        super(Bottleneck, self).__init__()\n        planes = inplanes // 4\n\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.conv3 = nn.Conv2d(planes, outplanes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(outplanes)\n\n        self.relu = nn.ReLU(inplace=True)\n\n        self.is_skip = True\n        if inplanes != outplanes:\n            self.is_skip = False\n\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.is_skip:\n            out += residual\n        out = self.relu(out)\n\n        return out\n\nclass Decoder_bottleneck(nn.Module):\n    def __init__(self,in_channels, channels, out_channels):\n        super(Decoder_bottleneck, self).__init__()\n\n        self.block1 = Bottleneck(in_channels, channels)\n        self.block2 = Bottleneck(channels, out_channels)\n        self.SCSE = SCSEBlock(out_channels)\n\n    def forward(self, x, e = None):\n        x = F.upsample(x, scale_factor=2, mode='bilinear')\n        if e is not None:\n            x = torch.cat([x,e],1)\n\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.SCSE(x)\n        return x\n\nclass model34_DeepSupervion(nn.Module):\n    def __init__(self, num_classes=1, mask_class = 2):\n        super(model34_DeepSupervion, self).__init__()\n\n        self.num_classes = num_classes\n\n        self.encoder = torchvision.models.resnet34(pretrained=True)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv1 = nn.Sequential(self.encoder.conv1,\n                                   self.encoder.bn1,\n                                   self.encoder.relu)\n\n        self.conv2 = self.encoder.layer1\n        self.conv3 = self.encoder.layer2\n        self.conv4 = self.encoder.layer3\n        self.conv5 = self.encoder.layer4\n\n        self.center_global_pool = nn.AdaptiveAvgPool2d([1,1])\n        self.center_conv1x1 = nn.Conv2d(512, 64, kernel_size=1)\n        self.center_fc = nn.Linear(64, mask_class)\n\n        self.center = nn.Sequential(nn.Conv2d(512, 512, kernel_size=3,padding=1),\n                                    nn.BatchNorm2d(512),\n                                    nn.ReLU(inplace=True),\n                                    nn.Conv2d(512, 256, kernel_size=3, padding=1),\n                                    nn.BatchNorm2d(256),\n                                    nn.ReLU(inplace=True),\n                                    nn.MaxPool2d(kernel_size=2,stride=2))\n\n        self.decoder5 = Decoder(256 + 512, 512, 64)\n        self.decoder4 = Decoder(64 + 256, 256, 64)\n        self.decoder3 = Decoder(64 + 128, 128, 64)\n        self.decoder2 = Decoder(64 + 64, 64, 64)\n        self.decoder1 = Decoder(64, 32, 64)\n\n        self.logits_no_empty = nn.Sequential(nn.Conv2d(320, 64, kernel_size=3, padding=1),\n                                    nn.ReLU(inplace=True),\n                                    nn.Conv2d(64, 1, kernel_size=1, padding=0))\n\n        self.logits_final = nn.Sequential(nn.Conv2d(320+64, 64, kernel_size=3, padding=1),\n                                         nn.ReLU(inplace=True),\n                                         nn.Conv2d(64, 1, kernel_size=1, padding=0))\n\n    def forward(self, x):\n        conv1 = self.conv1(x)     #1/4\n        conv2 = self.conv2(conv1) #1/4\n        conv3 = self.conv3(conv2) #1/8\n        conv4 = self.conv4(conv3) #1/16\n        conv5 = self.conv5(conv4) #1/32\n\n        center_512 = self.center_global_pool(conv5)\n        center_64 = self.center_conv1x1(center_512)\n        center_64_flatten = center_64.view(center_64.size(0), -1)\n        center_fc = self.center_fc(center_64_flatten)\n\n        f = self.center(conv5)\n        d5 = self.decoder5(f, conv5)\n        d4 = self.decoder4(d5, conv4)\n        d3 = self.decoder3(d4, conv3)\n        d2 = self.decoder2(d3, conv2)\n        d1 = self.decoder1(d2)\n\n        hypercol = torch.cat((\n            d1,\n            F.upsample(d2, scale_factor=2,mode='bilinear'),\n            F.upsample(d3, scale_factor=4, mode='bilinear'),\n            F.upsample(d4, scale_factor=8, mode='bilinear'),\n            F.upsample(d5, scale_factor=16, mode='bilinear')),1)\n        hypercol = F.dropout2d(hypercol, p = 0.50)\n\n        x_no_empty = self.logits_no_empty(hypercol)\n        hypercol_add_center = torch.cat((\n            hypercol,\n            F.upsample(center_64, scale_factor=128,mode='bilinear')),1)\n\n        x_final = self.logits_final( hypercol_add_center)\n        return center_fc, x_no_empty, x_final\n\nclass model50A_DeepSupervion(nn.Module):\n    def __init__(self, num_classes=1):\n        super(model50A_DeepSupervion, self).__init__()\n\n        self.num_classes = num_classes\n        self.encoder = se_resnext50_32x4d()\n\n        self.relu = nn.ReLU(inplace=True)\n        self.conv1 = nn.Sequential(self.encoder.layer0.conv1,\n                                   self.encoder.layer0.bn1,\n                                   self.encoder.layer0.relu1)\n\n        self.conv2 = self.encoder.layer1\n        self.conv3 = self.encoder.layer2\n        self.conv4 = self.encoder.layer3\n        self.conv5 = self.encoder.layer4\n\n        self.center_global_pool = nn.AdaptiveAvgPool2d([1,1])\n        self.center_conv1x1 = nn.Conv2d(512*4, 64, kernel_size=1)\n        self.center_fc = nn.Linear(64, 2)\n\n        self.center = nn.Sequential(nn.Conv2d(512*4, 512, kernel_size=3,padding=1),\n                                    nn.BatchNorm2d(512),\n                                    nn.ReLU(inplace=True),\n                                    nn.Conv2d(512, 256, kernel_size=3, padding=1),\n                                    nn.BatchNorm2d(256),\n                                    nn.ReLU(inplace=True),\n                                    nn.MaxPool2d(kernel_size=2,stride=2))\n\n        self.decoder5 = Decoder(256 + 512*4, 512, 64)\n        self.decoder4 = Decoder(64 + 256*4, 256, 64)\n        self.decoder3 = Decoder(64 + 128*4, 128, 64)\n        self.decoder2 = Decoder(64 + 64*4, 64, 64)\n        self.decoder1 = Decoder(64, 32, 64)\n\n        self.logits_no_empty = nn.Sequential(nn.Conv2d(320, 64, kernel_size=3, padding=1),\n                                    nn.ReLU(inplace=True),\n                                    nn.Conv2d(64, 1, kernel_size=1, padding=0))\n\n        self.logits_final = nn.Sequential(nn.Conv2d(320+64, 64, kernel_size=3, padding=1),\n                                         nn.ReLU(inplace=True),\n                                         nn.Conv2d(64, 1, kernel_size=1, padding=0))\n\n    def forward(self, x):\n        conv1 = self.conv1(x)     #1/4\n        conv2 = self.conv2(conv1) #1/4\n        conv3 = self.conv3(conv2) #1/8\n        conv4 = self.conv4(conv3) #1/16\n        conv5 = self.conv5(conv4) #1/32\n\n        center_512 = self.center_global_pool(conv5)\n        center_64 = self.center_conv1x1(center_512)\n        center_64_flatten = center_64.view(center_64.size(0), -1)\n        center_fc = self.center_fc(center_64_flatten)\n\n        f = self.center(conv5)\n        d5 = self.decoder5(f, conv5)\n        d4 = self.decoder4(d5, conv4)\n        d3 = self.decoder3(d4, conv3)\n        d2 = self.decoder2(d3, conv2)\n        d1 = self.decoder1(d2)\n\n        hypercol = torch.cat((\n            d1,\n            F.upsample(d2, scale_factor=2,mode='bilinear'),\n            F.upsample(d3, scale_factor=4, mode='bilinear'),\n            F.upsample(d4, scale_factor=8, mode='bilinear'),\n            F.upsample(d5, scale_factor=16, mode='bilinear')),1)\n        hypercol = F.dropout2d(hypercol, p = 0.50)\n\n        x_no_empty = self.logits_no_empty(hypercol)\n        hypercol_add_center = torch.cat((\n            hypercol,\n            F.upsample(center_64, scale_factor=128,mode='bilinear')),1)\n\n        x_final = self.logits_final( hypercol_add_center)\n        return center_fc, x_no_empty, x_final\n\nclass model50A_slim_DeepSupervion(nn.Module):\n    def __init__(self, num_classes=1):\n        super(model50A_slim_DeepSupervion, self).__init__()\n\n        self.num_classes = num_classes\n        self.encoder = se_resnext50_32x4d()\n\n        self.relu = nn.ReLU(inplace=True)\n        self.conv1 = nn.Sequential(self.encoder.layer0.conv1,\n                                   self.encoder.layer0.bn1,\n                                   self.encoder.layer0.relu1)\n\n        self.conv2 = self.encoder.layer1\n        self.conv3 = self.encoder.layer2\n        self.conv4 = self.encoder.layer3\n        self.conv5 = self.encoder.layer4\n\n        self.center_global_pool = nn.AdaptiveAvgPool2d([1,1])\n        self.center_conv1x1 = nn.Conv2d(512*4, 64, kernel_size=1)\n        self.center_fc = nn.Linear(64, 2)\n\n        self.center = nn.Sequential(nn.Conv2d(512*4, 512, kernel_size=3,padding=1),\n                                    nn.BatchNorm2d(512),\n                                    nn.ReLU(inplace=True),\n                                    nn.Conv2d(512, 256, kernel_size=3, padding=1),\n                                    nn.BatchNorm2d(256),\n                                    nn.ReLU(inplace=True),\n                                    nn.MaxPool2d(kernel_size=2,stride=2))\n\n        self.dec5_1x1 = nn.Sequential(nn.Conv2d(512 * 4, 512, kernel_size=1), nn.BatchNorm2d(512),nn.ReLU(inplace=True))\n        self.decoder5 = Decoder_bottleneck(256 + 512, 512, 64)\n\n        self.dec4_1x1 = nn.Sequential(nn.Conv2d(256 * 4, 256, kernel_size=1), nn.BatchNorm2d(256),nn.ReLU(inplace=True))\n        self.decoder4 = Decoder_bottleneck(64 + 256, 256, 64)\n\n        self.dec3_1x1 = nn.Sequential(nn.Conv2d(128 * 4, 128, kernel_size=1), nn.BatchNorm2d(128),nn.ReLU(inplace=True))\n        self.decoder3 = Decoder_bottleneck(64 + 128, 128, 64)\n\n        self.dec2_1x1 = nn.Sequential(nn.Conv2d(64 * 4, 64, kernel_size=1), nn.BatchNorm2d(64),nn.ReLU(inplace=True))\n        self.decoder2 = Decoder_bottleneck(64 + 64, 64, 64)\n\n        self.decoder1 = Decoder_bottleneck(64, 32, 64)\n\n        self.logits_no_empty = nn.Sequential(nn.Conv2d(320, 64, kernel_size=3, padding=1),\n                                    nn.ReLU(inplace=True),\n                                    nn.Conv2d(64, 1, kernel_size=1, padding=0))\n\n        self.logits_final = nn.Sequential(nn.Conv2d(320+64, 64, kernel_size=3, padding=1),\n                                         nn.ReLU(inplace=True),\n                                         nn.Conv2d(64, 1, kernel_size=1, padding=0))\n\n    def forward(self, x):\n        conv1 = self.conv1(x)     #1/4\n        conv2 = self.conv2(conv1) #1/4\n        conv3 = self.conv3(conv2) #1/8\n        conv4 = self.conv4(conv3) #1/16\n        conv5 = self.conv5(conv4) #1/32\n\n        center_512 = self.center_global_pool(conv5)\n        center_64 = self.center_conv1x1(center_512)\n        center_64_flatten = center_64.view(center_64.size(0), -1)\n        center_fc = self.center_fc(center_64_flatten)\n\n        f = self.center(conv5)\n\n        conv5 = self.dec5_1x1(conv5)\n        d5 = self.decoder5(f, conv5)\n\n        conv4 = self.dec4_1x1(conv4)\n        d4 = self.decoder4(d5, conv4)\n\n        conv3 = self.dec3_1x1(conv3)\n        d3 = self.decoder3(d4, conv3)\n\n        conv2 = self.dec2_1x1(conv2)\n        d2 = self.decoder2(d3, conv2)\n\n        d1 = self.decoder1(d2)\n\n        hypercol = torch.cat((\n            d1,\n            F.upsample(d2, scale_factor=2,mode='bilinear'),\n            F.upsample(d3, scale_factor=4, mode='bilinear'),\n            F.upsample(d4, scale_factor=8, mode='bilinear'),\n            F.upsample(d5, scale_factor=16, mode='bilinear')),1)\n        hypercol = F.dropout2d(hypercol, p = 0.50)\n\n        x_no_empty = self.logits_no_empty(hypercol)\n        x_no_empty_sig = F.sigmoid(x_no_empty)\n\n        hypercol_add_center = torch.cat((\n            hypercol,\n            F.upsample(center_64, scale_factor=hypercol.shape[2],mode='bilinear')),1)\n\n        x_final = self.logits_final(hypercol_add_center)\n        x_final_sig = F.sigmoid(x_final)\n\n        return center_fc, x_no_empty, x_final\n\nclass model101A_DeepSupervion(nn.Module):\n    def __init__(self, num_classes=1):\n        super(model101A_DeepSupervion, self).__init__()\n\n        self.num_classes = num_classes\n        num_filters = 32\n        baseWidth = 4\n        cardinality = 32\n        self.encoder = resnext101_ibn_a(baseWidth, cardinality, pretrained = True)\n\n\n        self.relu = nn.ReLU(inplace=True)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv1 = nn.Sequential(self.encoder.conv1,\n                                   self.encoder.bn1,\n                                   self.encoder.relu)\n\n        self.conv2 = self.encoder.layer1\n        self.conv3 = self.encoder.layer2\n        self.conv4 = self.encoder.layer3\n        self.conv5 = self.encoder.layer4\n\n        self.center_global_pool = nn.AdaptiveAvgPool2d([1,1])\n        self.center_conv1x1 = nn.Conv2d(2048, 64, kernel_size=1)\n        self.center_fc = nn.Linear(64, 2)\n\n        self.center_se = SELayer(512*4)\n        self.center = ImprovedIBNaDecoderBlock(512*4,  num_filters * 8)\n\n        self.dec5_se = SELayer(512*4 + num_filters * 8)\n        self.dec5 = ImprovedIBNaDecoderBlock(512*4 + num_filters * 8, num_filters * 8)\n\n        self.dec4_se = SELayer(256*4 + num_filters * 8)\n        self.dec4 = ImprovedIBNaDecoderBlock(256*4 + num_filters * 8, num_filters * 8)\n\n        self.dec3_se = SELayer(128*4 + num_filters * 8)\n        self.dec3 = ImprovedIBNaDecoderBlock(128*4 + num_filters * 8, num_filters * 4)\n\n        self.dec2_se = SELayer(64*4 + num_filters * 4)\n        self.dec2 = ImprovedIBNaDecoderBlock(64*4 + num_filters * 4, num_filters * 4)\n\n        self.logits_no_empty = nn.Sequential(StConvRelu(num_filters * 4, num_filters, 3),\n                                             nn.Dropout2d(0.5),\n                                             nn.Conv2d(num_filters, 1, kernel_size=1, padding=0))\n\n\n        self.logits_final = nn.Sequential(StConvRelu(num_filters * 4 + 64, num_filters, 3),\n                                          nn.Dropout2d(0.5),\n                                          nn.Conv2d(num_filters, 1, kernel_size=1, padding=0))\n\n    def forward(self, x):\n        conv1 = self.conv1(x)     #1/2\n        conv2 = self.conv2(conv1) #1/2\n        conv3 = self.conv3(conv2) #1/4\n        conv4 = self.conv4(conv3) #1/8\n        conv5 = self.conv5(conv4) #1/16\n\n        center_2048 = self.center_global_pool(conv5)\n        center_64 = self.center_conv1x1(center_2048)\n        center_64_flatten = center_64.view(center_64.size(0), -1)\n        center_fc = self.center_fc(center_64_flatten)\n\n        center = self.center(self.center_se(self.pool(conv5)))#1/16\n\n        dec5 = self.dec5(self.dec5_se(torch.cat([center, conv5], 1)))#1/8\n        dec4 = self.dec4(self.dec4_se(torch.cat([dec5, conv4], 1)))  #1/4\n        dec3 = self.dec3(self.dec3_se(torch.cat([dec4, conv3], 1)))  #1/2\n        dec2 = self.dec2(self.dec2_se(torch.cat([dec3, conv2], 1)))  #1\n\n        x_no_empty = self.logits_no_empty(dec2)\n        dec0_add_center = torch.cat((\n            dec2,\n            F.upsample(center_64, scale_factor=128, mode='bilinear')), 1)\n        x_final = self.logits_final(dec0_add_center)\n\n        return center_fc, x_no_empty, x_final\n\nfrom senet import se_resnext101_32x4d\nclass model101B_DeepSupervion(nn.Module):\n    def __init__(self, num_classes=1):\n        super(model101B_DeepSupervion, self).__init__()\n\n        self.num_classes = num_classes\n\n        num_filters = 32\n\n        self.encoder = se_resnext101_32x4d()\n\n        self.relu = nn.ReLU(inplace=True)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv1 = nn.Sequential(self.encoder.layer0.conv1,\n                                   self.encoder.layer0.bn1,\n                                   self.encoder.layer0.relu1)\n\n        self.conv2 = self.encoder.layer1\n        self.conv3 = self.encoder.layer2\n        self.conv4 = self.encoder.layer3\n        self.conv5 = self.encoder.layer4\n\n        self.center_global_pool = nn.AdaptiveAvgPool2d([1,1])\n        self.center_conv1x1 = nn.Conv2d(2048, 64, kernel_size=1)\n        self.center_fc = nn.Linear(64, 2)\n\n        self.center_se = SELayer(512*4)\n        self.center = ImprovedIBNaDecoderBlock(512*4,  num_filters * 8)\n\n        self.dec5_se = SELayer(512*4 + num_filters * 8)\n        self.dec5 = ImprovedIBNaDecoderBlock(512*4 + num_filters * 8, num_filters * 8)\n\n        self.dec4_se = SELayer(256*4 + num_filters * 8)\n        self.dec4 = ImprovedIBNaDecoderBlock(256*4 + num_filters * 8, num_filters * 8)\n\n        self.dec3_se = SELayer(128*4 + num_filters * 8)\n        self.dec3 = ImprovedIBNaDecoderBlock(128*4 + num_filters * 8, num_filters * 4)\n\n        self.dec2_se = SELayer(64*4 + num_filters * 4)\n        self.dec2 = ImprovedIBNaDecoderBlock(64*4 + num_filters * 4, num_filters * 4)\n\n        self.logits_no_empty = nn.Sequential(ConvRelu(num_filters * 4, num_filters, 3),\n                                             nn.Dropout2d(0.5),\n                                             nn.Conv2d(num_filters, 1, kernel_size=1, padding=0))\n\n\n        self.logits_final = nn.Sequential(ConvRelu(num_filters * 4 + 64, num_filters, 3),\n                                          nn.Dropout2d(0.5),\n                                          nn.Conv2d(num_filters, 1, kernel_size=1, padding=0))\n\n\n    def forward(self, x):\n        conv1 = self.conv1(x)     #1/2\n        conv2 = self.conv2(conv1) #1/2\n        conv3 = self.conv3(conv2) #1/4\n        conv4 = self.conv4(conv3) #1/8\n        conv5 = self.conv5(conv4) #1/16\n\n        center_2048 = self.center_global_pool(conv5)\n        center_64 = self.center_conv1x1(center_2048)\n        center_64_flatten = center_64.view(center_64.size(0), -1)\n        center_fc = self.center_fc(center_64_flatten)\n\n        center = self.center(self.center_se(self.pool(conv5)))#1/16\n\n        dec5 = self.dec5(self.dec5_se(torch.cat([center, conv5], 1)))#1/8\n        dec4 = self.dec4(self.dec4_se(torch.cat([dec5, conv4], 1)))  #1/4\n        dec3 = self.dec3(self.dec3_se(torch.cat([dec4, conv3], 1)))  #1/2\n        dec2 = self.dec2(self.dec2_se(torch.cat([dec3, conv2], 1)))  #1\n\n        x_no_empty = self.logits_no_empty(dec2)\n\n        dec0_add_center = torch.cat((\n            dec2,\n            F.upsample(center_64, scale_factor=128, mode='bilinear')), 1)\n\n        x_final = self.logits_final(dec0_add_center)\n\n        return center_fc, x_no_empty, x_final\n\nfrom senet import se_resnet152\nclass model152_DeepSupervion(nn.Module):\n    def __init__(self, num_classes=1):\n        super(model152_DeepSupervion, self).__init__()\n\n        self.num_classes = num_classes\n        self.encoder = se_resnet152()\n\n        self.relu = nn.ReLU(inplace=True)\n        self.conv1 = nn.Sequential(self.encoder.layer0.conv1,\n                                   self.encoder.layer0.bn1,\n                                   self.encoder.layer0.relu1)\n\n        self.conv2 = self.encoder.layer1\n        self.conv3 = self.encoder.layer2\n        self.conv4 = self.encoder.layer3\n        self.conv5 = self.encoder.layer4\n\n        self.center_global_pool = nn.AdaptiveAvgPool2d([1,1])\n        self.center_conv1x1 = nn.Conv2d(512*4, 64, kernel_size=1)\n        self.center_fc = nn.Linear(64, 2)\n\n        self.center = nn.Sequential(nn.Conv2d(512*4, 512, kernel_size=3,padding=1),\n                                    nn.BatchNorm2d(512),\n                                    nn.ReLU(inplace=True),\n                                    nn.Conv2d(512, 256, kernel_size=3, padding=1),\n                                    nn.BatchNorm2d(256),\n                                    nn.ReLU(inplace=True),\n                                    nn.MaxPool2d(kernel_size=2,stride=2))\n\n        self.dec5_1x1 = nn.Sequential(nn.Conv2d(512 * 4, 512, kernel_size=1), nn.BatchNorm2d(512),nn.ReLU(inplace=True))\n        self.decoder5 = Decoder_bottleneck(256 + 512, 512, 64)\n\n        self.dec4_1x1 = nn.Sequential(nn.Conv2d(256 * 4, 256, kernel_size=1), nn.BatchNorm2d(256),nn.ReLU(inplace=True))\n        self.decoder4 = Decoder_bottleneck(64 + 256, 256, 64)\n\n        self.dec3_1x1 = nn.Sequential(nn.Conv2d(128 * 4, 128, kernel_size=1), nn.BatchNorm2d(128),nn.ReLU(inplace=True))\n        self.decoder3 = Decoder_bottleneck(64 + 128, 128, 64)\n\n        self.dec2_1x1 = nn.Sequential(nn.Conv2d(64 * 4, 64, kernel_size=1), nn.BatchNorm2d(64),nn.ReLU(inplace=True))\n        self.decoder2 = Decoder_bottleneck(64 + 64, 64, 64)\n\n        self.decoder1 = Decoder_bottleneck(64, 32, 64)\n\n        self.logits_no_empty = nn.Sequential(nn.Conv2d(320, 64, kernel_size=3, padding=1),\n                                    nn.ReLU(inplace=True),\n                                    nn.Conv2d(64, 1, kernel_size=1, padding=0))\n\n        self.logits_final = nn.Sequential(nn.Conv2d(320+64, 64, kernel_size=3, padding=1),\n                                         nn.ReLU(inplace=True),\n                                         nn.Conv2d(64, 1, kernel_size=1, padding=0))\n\n    def forward(self, x):\n        conv1 = self.conv1(x)     #1/4\n        conv2 = self.conv2(conv1) #1/4\n        conv3 = self.conv3(conv2) #1/8\n        conv4 = self.conv4(conv3) #1/16\n        conv5 = self.conv5(conv4) #1/32\n\n        center_512 = self.center_global_pool(conv5)\n        center_64 = self.center_conv1x1(center_512)\n        center_64_flatten = center_64.view(center_64.size(0), -1)\n        center_fc = self.center_fc(center_64_flatten)\n\n        f = self.center(conv5)\n\n        conv5 = self.dec5_1x1(conv5)\n        d5 = self.decoder5(f, conv5)\n\n        conv4 = self.dec4_1x1(conv4)\n        d4 = self.decoder4(d5, conv4)\n\n        conv3 = self.dec3_1x1(conv3)\n        d3 = self.decoder3(d4, conv3)\n\n        conv2 = self.dec2_1x1(conv2)\n        d2 = self.decoder2(d3, conv2)\n\n        d1 = self.decoder1(d2)\n\n        hypercol = torch.cat((\n            d1,\n            F.upsample(d2, scale_factor=2,mode='bilinear'),\n            F.upsample(d3, scale_factor=4, mode='bilinear'),\n            F.upsample(d4, scale_factor=8, mode='bilinear'),\n            F.upsample(d5, scale_factor=16, mode='bilinear')),1)\n\n        hypercol = F.dropout2d(hypercol, p = 0.50)\n\n        x_no_empty = self.logits_no_empty(hypercol)\n\n        hypercol_add_center = torch.cat((\n            hypercol,\n            F.upsample(center_64, scale_factor=hypercol.shape[2],mode='bilinear')),1)\n\n        x_final = self.logits_final(hypercol_add_center)\n\n        return center_fc, x_no_empty, x_final\n\nfrom senet import senet154\nclass model154_DeepSupervion(nn.Module):\n    def __init__(self, num_classes=1):\n        super(model154_DeepSupervion, self).__init__()\n\n        self.num_classes = num_classes\n        self.encoder = senet154()\n\n        self.relu = nn.ReLU(inplace=True)\n        self.conv1 = nn.Sequential(self.encoder.layer0.conv1,\n                                   self.encoder.layer0.bn1,\n                                   self.encoder.layer0.relu1,\n                                   self.encoder.layer0.conv2,\n                                   self.encoder.layer0.bn2,\n                                   self.encoder.layer0.relu2,\n                                   self.encoder.layer0.conv3,\n                                   self.encoder.layer0.bn3,\n                                   self.encoder.layer0.relu3\n                                   )\n\n        self.conv2 = self.encoder.layer1\n        self.conv3 = self.encoder.layer2\n        self.conv4 = self.encoder.layer3\n        self.conv5 = self.encoder.layer4\n\n        self.center_global_pool = nn.AdaptiveAvgPool2d([1, 1])\n        self.center_conv1x1 = nn.Conv2d(512 * 4, 64, kernel_size=1)\n        self.center_fc = nn.Linear(64, 2)\n\n        self.center = nn.Sequential(nn.Conv2d(512 * 4, 512, kernel_size=3, padding=1),\n                                    nn.BatchNorm2d(512),\n                                    nn.ReLU(inplace=True),\n                                    nn.Conv2d(512, 256, kernel_size=3, padding=1),\n                                    nn.BatchNorm2d(256),\n                                    nn.ReLU(inplace=True),\n                                    nn.MaxPool2d(kernel_size=2, stride=2))\n\n        self.dec5_1x1 = nn.Sequential(nn.Conv2d(512 * 4, 512, kernel_size=1), nn.BatchNorm2d(512),\n                                      nn.ReLU(inplace=True))\n        self.decoder5 = Decoder_bottleneck(256 + 512, 512, 64)\n\n        self.dec4_1x1 = nn.Sequential(nn.Conv2d(256 * 4, 256, kernel_size=1), nn.BatchNorm2d(256),\n                                      nn.ReLU(inplace=True))\n        self.decoder4 = Decoder_bottleneck(64 + 256, 256, 64)\n\n        self.dec3_1x1 = nn.Sequential(nn.Conv2d(128 * 4, 128, kernel_size=1), nn.BatchNorm2d(128),\n                                      nn.ReLU(inplace=True))\n        self.decoder3 = Decoder_bottleneck(64 + 128, 128, 64)\n\n        self.dec2_1x1 = nn.Sequential(nn.Conv2d(64 * 4, 64, kernel_size=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True))\n        self.decoder2 = Decoder_bottleneck(64 + 64, 64, 64)\n\n        self.decoder1 = Decoder_bottleneck(64, 32, 64)\n\n        self.logits_no_empty = nn.Sequential(nn.Conv2d(320, 64, kernel_size=3, padding=1),\n                                             nn.ReLU(inplace=True),\n                                             nn.Conv2d(64, 1, kernel_size=1, padding=0))\n\n        self.logits_final = nn.Sequential(nn.Conv2d(320 + 64, 64, kernel_size=3, padding=1),\n                                          nn.ReLU(inplace=True),\n                                          nn.Conv2d(64, 1, kernel_size=1, padding=0))\n\n    def forward(self, x):\n        conv1 = self.conv1(x)  # 1/4\n        conv2 = self.conv2(conv1)  # 1/4\n        conv3 = self.conv3(conv2)  # 1/8\n        conv4 = self.conv4(conv3)  # 1/16\n        conv5 = self.conv5(conv4)  # 1/32\n\n        center_512 = self.center_global_pool(conv5)\n        center_64 = self.center_conv1x1(center_512)\n        center_64_flatten = center_64.view(center_64.size(0), -1)\n        center_fc = self.center_fc(center_64_flatten)\n\n        f = self.center(conv5)\n\n        conv5 = self.dec5_1x1(conv5)\n        d5 = self.decoder5(f, conv5)\n\n        conv4 = self.dec4_1x1(conv4)\n        d4 = self.decoder4(d5, conv4)\n\n        conv3 = self.dec3_1x1(conv3)\n        d3 = self.decoder3(d4, conv3)\n\n        conv2 = self.dec2_1x1(conv2)\n        d2 = self.decoder2(d3, conv2)\n\n        d1 = self.decoder1(d2)\n\n        hypercol = torch.cat((\n            d1,\n            F.upsample(d2, scale_factor=2, mode='bilinear'),\n            F.upsample(d3, scale_factor=4, mode='bilinear'),\n            F.upsample(d4, scale_factor=8, mode='bilinear'),\n            F.upsample(d5, scale_factor=16, mode='bilinear')), 1)\n\n        hypercol = F.dropout2d(hypercol, p=0.50)\n        x_no_empty = self.logits_no_empty(hypercol)\n\n        hypercol_add_center = torch.cat((\n            hypercol,\n            F.upsample(center_64, scale_factor=hypercol.shape[2], mode='bilinear')), 1)\n\n        x_final = self.logits_final(hypercol_add_center)\n\n        return center_fc, x_no_empty,  x_final"""
model/senet.py,2,"b'""""""\nResNet code gently borrowed from\nhttps://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n""""""\nfrom __future__ import print_function, division, absolute_import\nfrom collections import OrderedDict\nimport math\n\nimport torch.nn as nn\nfrom torch.utils import model_zoo\n\n__all__ = [\'SENet\', \'senet154\', \'se_resnet50\', \'se_resnet101\', \'se_resnet152\',\n           \'se_resnext50_32x4d\', \'se_resnext101_32x4d\']\n\npretrained_settings = {\n    \'senet154\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/senet154-c7b49a05.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnet50\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet50-ce0d4300.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnet101\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet101-7e38fcc6.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnet152\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet152-d17c99b7.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnext50_32x4d\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext50_32x4d-a260b3a4.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnext101_32x4d\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext101_32x4d-3b2fe3d8.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n}\n\n\nclass SEModule(nn.Module):\n\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1,\n                             padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1,\n                             padding=0)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        module_input = x\n        x = self.avg_pool(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return module_input * x\n\n\nclass Bottleneck(nn.Module):\n    """"""\n    Base class for bottlenecks that implements `forward()` method.\n    """"""\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = self.se_module(out) + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBottleneck(Bottleneck):\n    """"""\n    Bottleneck for SENet154.\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes * 2)\n        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n                               stride=stride, padding=1, groups=groups,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(planes * 4)\n        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNetBottleneck(Bottleneck):\n    """"""\n    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n    (the latter is used in the torchvision implementation of ResNet).\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEResNetBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False,\n                               stride=stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1,\n                               groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNeXtBottleneck(Bottleneck):\n    """"""\n    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None, base_width=4):\n        super(SEResNeXtBottleneck, self).__init__()\n        width = int(math.floor(planes * (base_width / 64)) * groups)\n\n\n        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, stride=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(width)\n        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n                               padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(width)\n        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SENet(nn.Module):\n\n    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n                 downsample_padding=1, num_classes=1000):\n        """"""\n        Parameters\n        ----------\n        block (nn.Module): Bottleneck class.\n            - For SENet154: SEBottleneck\n            - For SE-ResNet models: SEResNetBottleneck\n            - For SE-ResNeXt models:  SEResNeXtBottleneck\n        layers (list of ints): Number of residual blocks for 4 layers of the\n            network (layer1...layer4).\n        groups (int): Number of groups for the 3x3 convolution in each\n            bottleneck block.\n            - For SENet154: 64\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models:  32\n        reduction (int): Reduction ratio for Squeeze-and-Excitation modules.\n            - For all models: 16\n        dropout_p (float or None): Drop probability for the Dropout layer.\n            If `None` the Dropout layer is not used.\n            - For SENet154: 0.2\n            - For SE-ResNet models: None\n            - For SE-ResNeXt models: None\n        inplanes (int):  Number of input channels for layer1.\n            - For SENet154: 128\n            - For SE-ResNet models: 64\n            - For SE-ResNeXt models: 64\n        input_3x3 (bool): If `True`, use three 3x3 convolutions instead of\n            a single 7x7 convolution in layer0.\n            - For SENet154: True\n            - For SE-ResNet models: False\n            - For SE-ResNeXt models: False\n        downsample_kernel_size (int): Kernel size for downsampling convolutions\n            in layer2, layer3 and layer4.\n            - For SENet154: 3\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models: 1\n        downsample_padding (int): Padding for downsampling convolutions in\n            layer2, layer3 and layer4.\n            - For SENet154: 1\n            - For SE-ResNet models: 0\n            - For SE-ResNeXt models: 0\n        num_classes (int): Number of outputs in `last_linear` layer.\n            - For all models: 1000\n        """"""\n        super(SENet, self).__init__()\n        self.inplanes = inplanes\n        if input_3x3:\n            layer0_modules = [\n                (\'conv1\', nn.Conv2d(3, 64, 3, stride=2, padding=1,\n                                    bias=False)),\n                (\'bn1\', nn.BatchNorm2d(64)),\n                (\'relu1\', nn.ReLU(inplace=True)),\n                (\'conv2\', nn.Conv2d(64, 64, 3, stride=1, padding=1,\n                                    bias=False)),\n                (\'bn2\', nn.BatchNorm2d(64)),\n                (\'relu2\', nn.ReLU(inplace=True)),\n                (\'conv3\', nn.Conv2d(64, inplanes, 3, stride=1, padding=1,\n                                    bias=False)),\n                (\'bn3\', nn.BatchNorm2d(inplanes)),\n                (\'relu3\', nn.ReLU(inplace=True)),\n            ]\n        else:\n            layer0_modules = [\n                (\'conv1\', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n                                    padding=3, bias=False)),\n                (\'bn1\', nn.BatchNorm2d(inplanes)),\n                (\'relu1\', nn.ReLU(inplace=True)),\n            ]\n        # To preserve compatibility with Caffe weights `ceil_mode=True`\n        # is used instead of `padding=1`.\n        layer0_modules.append((\'pool\', nn.MaxPool2d(3, stride=2,\n                                                    ceil_mode=True)))\n        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n        self.layer1 = self._make_layer(\n            block,\n            planes=64,\n            blocks=layers[0],\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=1,\n            downsample_padding=0\n        )\n        self.layer2 = self._make_layer(\n            block,\n            planes=128,\n            blocks=layers[1],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer3 = self._make_layer(\n            block,\n            planes=256,\n            blocks=layers[2],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer4 = self._make_layer(\n            block,\n            planes=512,\n            blocks=layers[3],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.avg_pool = nn.AvgPool2d(7, stride=1)\n        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n                    downsample_kernel_size=1, downsample_padding=0):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=downsample_kernel_size, stride=stride,\n                          padding=downsample_padding, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n                            downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups, reduction))\n\n        return nn.Sequential(*layers)\n\n    def features(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def logits(self, x):\n        x = self.avg_pool(x)\n        if self.dropout is not None:\n            x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.logits(x)\n        return x\n\n\ndef initialize_pretrained_model(model, num_classes, settings):\n    assert num_classes == settings[\'num_classes\'], \\\n        \'num_classes should be {}, but is {}\'.format(\n            settings[\'num_classes\'], num_classes)\n    model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n    model.input_space = settings[\'input_space\']\n    model.input_size = settings[\'input_size\']\n    model.input_range = settings[\'input_range\']\n    model.mean = settings[\'mean\']\n    model.std = settings[\'std\']\n\n\ndef senet154(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEBottleneck, [3, 8, 36, 3], groups=64, reduction=16,\n                  dropout_p=0.2, num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'senet154\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet50(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEResNetBottleneck, [3, 4, 6, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnet50\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet101(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEResNetBottleneck, [3, 4, 23, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnet101\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet152(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEResNetBottleneck, [3, 8, 36, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnet152\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnext50_32x4d(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnext50_32x4d\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnext101_32x4d(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 23, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnext101_32x4d\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n'"
