file_path,api_count,code
examples.py,21,"b""#######################################################################\n# Copyright (C) 2017 Shangtong Zhang(zhangshangtong.cpp@gmail.com)    #\n# Permission given to modify the code as long as you keep this        #\n# declaration at the top                                              #\n#######################################################################\n\nfrom deep_rl import *\n\n\n# DQN\ndef dqn_feature(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    config = Config()\n    config.merge(kwargs)\n\n    config.task_fn = lambda: Task(config.game)\n    config.eval_env = config.task_fn()\n\n    config.optimizer_fn = lambda params: torch.optim.RMSprop(params, 0.001)\n    config.network_fn = lambda: VanillaNet(config.action_dim, FCBody(config.state_dim))\n    # config.network_fn = lambda: DuelingNet(config.action_dim, FCBody(config.state_dim))\n    # config.replay_fn = lambda: Replay(memory_size=int(1e4), batch_size=10)\n    config.replay_fn = lambda: AsyncReplay(memory_size=int(1e4), batch_size=10)\n\n    config.random_action_prob = LinearSchedule(1.0, 0.1, 1e4)\n    config.discount = 0.99\n    config.target_network_update_freq = 200\n    config.exploration_steps = 1000\n    # config.double_q = True\n    config.double_q = False\n    config.sgd_update_frequency = 4\n    config.gradient_clip = 5\n    config.eval_interval = int(5e3)\n    config.max_steps = 1e5\n    config.async_actor = False\n    run_steps(DQNAgent(config))\n\n\ndef dqn_pixel(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    config = Config()\n    config.merge(kwargs)\n\n    config.task_fn = lambda: Task(config.game)\n    config.eval_env = config.task_fn()\n\n    config.optimizer_fn = lambda params: torch.optim.RMSprop(\n        params, lr=0.00025, alpha=0.95, eps=0.01, centered=True)\n    config.network_fn = lambda: VanillaNet(config.action_dim, NatureConvBody(in_channels=config.history_length))\n    # config.network_fn = lambda: DuelingNet(config.action_dim, NatureConvBody(in_channels=config.history_length))\n    config.random_action_prob = LinearSchedule(1.0, 0.01, 1e6)\n\n    # config.replay_fn = lambda: Replay(memory_size=int(1e6), batch_size=32)\n    config.replay_fn = lambda: AsyncReplay(memory_size=int(1e6), batch_size=32)\n\n    config.batch_size = 32\n    config.state_normalizer = ImageNormalizer()\n    config.reward_normalizer = SignNormalizer()\n    config.discount = 0.99\n    config.target_network_update_freq = 10000\n    config.exploration_steps = 50000\n    config.sgd_update_frequency = 4\n    config.gradient_clip = 5\n    config.history_length = 4\n    # config.double_q = True\n    config.double_q = False\n    config.max_steps = int(2e7)\n    run_steps(DQNAgent(config))\n\n\n# QR DQN\ndef quantile_regression_dqn_feature(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    config = Config()\n    config.merge(kwargs)\n\n    config.task_fn = lambda: Task(config.game)\n    config.eval_env = config.task_fn()\n    config.optimizer_fn = lambda params: torch.optim.RMSprop(params, 0.001)\n    config.network_fn = lambda: QuantileNet(config.action_dim, config.num_quantiles, FCBody(config.state_dim))\n\n    # config.replay_fn = lambda: Replay(memory_size=int(1e4), batch_size=10)\n    config.replay_fn = lambda: AsyncReplay(memory_size=int(1e4), batch_size=10)\n\n    config.random_action_prob = LinearSchedule(1.0, 0.1, 1e4)\n    config.discount = 0.99\n    config.target_network_update_freq = 200\n    config.exploration_steps = 100\n    config.num_quantiles = 20\n    config.gradient_clip = 5\n    config.sgd_update_frequency = 4\n    config.eval_interval = int(5e3)\n    config.max_steps = 1e5\n    run_steps(QuantileRegressionDQNAgent(config))\n\n\ndef quantile_regression_dqn_pixel(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    config = Config()\n    config.merge(kwargs)\n\n    config.task_fn = lambda: Task(config.game)\n    config.eval_env = config.task_fn()\n\n    config.optimizer_fn = lambda params: torch.optim.Adam(params, lr=0.00005, eps=0.01 / 32)\n    config.network_fn = lambda: QuantileNet(config.action_dim, config.num_quantiles, NatureConvBody())\n    config.random_action_prob = LinearSchedule(1.0, 0.01, 1e6)\n\n    # config.replay_fn = lambda: Replay(memory_size=int(1e6), batch_size=32)\n    config.replay_fn = lambda: AsyncReplay(memory_size=int(1e6), batch_size=32)\n\n    config.state_normalizer = ImageNormalizer()\n    config.reward_normalizer = SignNormalizer()\n    config.discount = 0.99\n    config.target_network_update_freq = 10000\n    config.exploration_steps = 50000\n    config.sgd_update_frequency = 4\n    config.gradient_clip = 5\n    config.num_quantiles = 200\n    config.max_steps = int(2e7)\n    run_steps(QuantileRegressionDQNAgent(config))\n\n\n# C51\ndef categorical_dqn_feature(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    config = Config()\n    config.merge(kwargs)\n\n    config.task_fn = lambda: Task(config.game)\n    config.eval_env = config.task_fn()\n    config.optimizer_fn = lambda params: torch.optim.RMSprop(params, 0.001)\n    config.network_fn = lambda: CategoricalNet(config.action_dim, config.categorical_n_atoms, FCBody(config.state_dim))\n    config.random_action_prob = LinearSchedule(1.0, 0.1, 1e4)\n\n    # config.replay_fn = lambda: Replay(memory_size=10000, batch_size=10)\n    config.replay_fn = lambda: AsyncReplay(memory_size=10000, batch_size=10)\n\n    config.discount = 0.99\n    config.target_network_update_freq = 200\n    config.exploration_steps = 100\n    config.categorical_v_max = 100\n    config.categorical_v_min = -100\n    config.categorical_n_atoms = 50\n    config.gradient_clip = 5\n    config.sgd_update_frequency = 4\n\n    config.eval_interval = int(5e3)\n    config.max_steps = 1e5\n    run_steps(CategoricalDQNAgent(config))\n\n\ndef categorical_dqn_pixel(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    config = Config()\n    config.merge(kwargs)\n\n    config.task_fn = lambda: Task(config.game)\n    config.eval_env = config.task_fn()\n    config.optimizer_fn = lambda params: torch.optim.Adam(params, lr=0.00025, eps=0.01 / 32)\n    config.network_fn = lambda: CategoricalNet(config.action_dim, config.categorical_n_atoms, NatureConvBody())\n    config.random_action_prob = LinearSchedule(1.0, 0.01, 1e6)\n\n    # config.replay_fn = lambda: Replay(memory_size=int(1e6), batch_size=32)\n    config.replay_fn = lambda: AsyncReplay(memory_size=int(1e6), batch_size=32)\n\n    config.discount = 0.99\n    config.state_normalizer = ImageNormalizer()\n    config.reward_normalizer = SignNormalizer()\n    config.target_network_update_freq = 10000\n    config.exploration_steps = 50000\n    config.categorical_v_max = 10\n    config.categorical_v_min = -10\n    config.categorical_n_atoms = 51\n    config.sgd_update_frequency = 4\n    config.gradient_clip = 0.5\n    config.max_steps = int(2e7)\n    run_steps(CategoricalDQNAgent(config))\n\n\n# A2C\ndef a2c_feature(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    config = Config()\n    config.merge(kwargs)\n\n    config.num_workers = 5\n    config.task_fn = lambda: Task(config.game, num_envs=config.num_workers)\n    config.eval_env = Task(config.game)\n    config.optimizer_fn = lambda params: torch.optim.RMSprop(params, 0.001)\n    config.network_fn = lambda: CategoricalActorCriticNet(\n        config.state_dim, config.action_dim, FCBody(config.state_dim, gate=F.tanh))\n    config.discount = 0.99\n    config.use_gae = True\n    config.gae_tau = 0.95\n    config.entropy_weight = 0.01\n    config.rollout_length = 5\n    config.gradient_clip = 0.5\n    run_steps(A2CAgent(config))\n\n\ndef a2c_pixel(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    config = Config()\n    config.merge(kwargs)\n\n    config.num_workers = 16\n    config.task_fn = lambda: Task(config.game, num_envs=config.num_workers)\n    config.eval_env = Task(config.game)\n    config.optimizer_fn = lambda params: torch.optim.RMSprop(params, lr=1e-4, alpha=0.99, eps=1e-5)\n    config.network_fn = lambda: CategoricalActorCriticNet(config.state_dim, config.action_dim, NatureConvBody())\n    config.state_normalizer = ImageNormalizer()\n    config.reward_normalizer = SignNormalizer()\n    config.discount = 0.99\n    config.use_gae = True\n    config.gae_tau = 1.0\n    config.entropy_weight = 0.01\n    config.rollout_length = 5\n    config.gradient_clip = 5\n    config.max_steps = int(2e7)\n    run_steps(A2CAgent(config))\n\n\ndef a2c_continuous(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    config = Config()\n    config.merge(kwargs)\n\n    config.num_workers = 16\n    config.task_fn = lambda: Task(config.game, num_envs=config.num_workers)\n    config.eval_env = Task(config.game)\n    config.optimizer_fn = lambda params: torch.optim.RMSprop(params, lr=0.0007)\n    config.network_fn = lambda: GaussianActorCriticNet(\n        config.state_dim, config.action_dim,\n        actor_body=FCBody(config.state_dim), critic_body=FCBody(config.state_dim))\n    config.discount = 0.99\n    config.use_gae = True\n    config.gae_tau = 1.0\n    config.entropy_weight = 0.01\n    config.rollout_length = 5\n    config.gradient_clip = 5\n    config.max_steps = int(2e7)\n    run_steps(A2CAgent(config))\n\n\n# N-Step DQN\ndef n_step_dqn_feature(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    config = Config()\n    config.merge(kwargs)\n\n    config.task_fn = lambda: Task(config.game, num_envs=config.num_workers)\n    config.eval_env = Task(config.game)\n    config.num_workers = 5\n    config.optimizer_fn = lambda params: torch.optim.RMSprop(params, 0.001)\n    config.network_fn = lambda: VanillaNet(config.action_dim, FCBody(config.state_dim))\n    config.random_action_prob = LinearSchedule(1.0, 0.1, 1e4)\n    config.discount = 0.99\n    config.target_network_update_freq = 200\n    config.rollout_length = 5\n    config.gradient_clip = 5\n    run_steps(NStepDQNAgent(config))\n\n\ndef n_step_dqn_pixel(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    config = Config()\n    config.merge(kwargs)\n\n    config.task_fn = lambda: Task(config.game, num_envs=config.num_workers)\n    config.eval_env = Task(config.game)\n    config.num_workers = 16\n    config.optimizer_fn = lambda params: torch.optim.RMSprop(params, lr=1e-4, alpha=0.99, eps=1e-5)\n    config.network_fn = lambda: VanillaNet(config.action_dim, NatureConvBody())\n    config.random_action_prob = LinearSchedule(1.0, 0.05, 1e6)\n    config.state_normalizer = ImageNormalizer()\n    config.reward_normalizer = SignNormalizer()\n    config.discount = 0.99\n    config.target_network_update_freq = 10000\n    config.rollout_length = 5\n    config.gradient_clip = 5\n    config.max_steps = int(2e7)\n    run_steps(NStepDQNAgent(config))\n\n\n# Option-Critic\ndef option_critic_feature(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    config = Config()\n    config.merge(kwargs)\n\n    config.num_workers = 5\n    config.task_fn = lambda: Task(config.game, num_envs=config.num_workers)\n    config.eval_env = Task(config.game)\n    config.optimizer_fn = lambda params: torch.optim.RMSprop(params, 0.001)\n    config.network_fn = lambda: OptionCriticNet(FCBody(config.state_dim), config.action_dim, num_options=2)\n    config.random_option_prob = LinearSchedule(1.0, 0.1, 1e4)\n    config.discount = 0.99\n    config.target_network_update_freq = 200\n    config.rollout_length = 5\n    config.termination_regularizer = 0.01\n    config.entropy_weight = 0.01\n    config.gradient_clip = 5\n    run_steps(OptionCriticAgent(config))\n\n\ndef option_critic_pixel(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    config = Config()\n    config.merge(kwargs)\n\n    config.task_fn = lambda: Task(config.game, num_envs=config.num_workers)\n    config.eval_env = Task(config.game)\n    config.num_workers = 16\n    config.optimizer_fn = lambda params: torch.optim.RMSprop(params, lr=1e-4, alpha=0.99, eps=1e-5)\n    config.network_fn = lambda: OptionCriticNet(NatureConvBody(), config.action_dim, num_options=4)\n    config.random_option_prob = LinearSchedule(0.1)\n    config.state_normalizer = ImageNormalizer()\n    config.reward_normalizer = SignNormalizer()\n    config.discount = 0.99\n    config.target_network_update_freq = 10000\n    config.rollout_length = 5\n    config.gradient_clip = 5\n    config.max_steps = int(2e7)\n    config.entropy_weight = 0.01\n    config.termination_regularizer = 0.01\n    run_steps(OptionCriticAgent(config))\n\n\n# PPO\ndef ppo_continuous(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    config = Config()\n    config.merge(kwargs)\n\n    config.task_fn = lambda: Task(config.game)\n    config.eval_env = config.task_fn()\n\n    config.network_fn = lambda: GaussianActorCriticNet(\n        config.state_dim, config.action_dim, actor_body=FCBody(config.state_dim, gate=torch.tanh),\n        critic_body=FCBody(config.state_dim, gate=torch.tanh))\n    config.actor_opt_fn = lambda params: torch.optim.Adam(params, 3e-4)\n    config.critic_opt_fn = lambda params: torch.optim.Adam(params, 1e-3)\n    config.discount = 0.99\n    config.use_gae = True\n    config.gae_tau = 0.95\n    config.gradient_clip = 0.5\n    config.rollout_length = 2048\n    config.optimization_epochs = 10\n    config.mini_batch_size = 64\n    config.ppo_ratio_clip = 0.2\n    config.log_interval = 2048\n    config.max_steps = 3e6\n    config.target_kl = 0.01\n    config.state_normalizer = MeanStdNormalizer()\n    run_steps(PPOAgent(config))\n\n\n# DDPG\ndef ddpg_continuous(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    config = Config()\n    config.merge(kwargs)\n\n    config.task_fn = lambda: Task(config.game)\n    config.eval_env = config.task_fn()\n    config.max_steps = int(1e6)\n    config.eval_interval = int(1e4)\n    config.eval_episodes = 20\n\n    config.network_fn = lambda: DeterministicActorCriticNet(\n        config.state_dim, config.action_dim,\n        actor_body=FCBody(config.state_dim, (400, 300), gate=F.relu),\n        critic_body=TwoLayerFCBodyWithAction(\n            config.state_dim, config.action_dim, (400, 300), gate=F.relu),\n        actor_opt_fn=lambda params: torch.optim.Adam(params, lr=1e-4),\n        critic_opt_fn=lambda params: torch.optim.Adam(params, lr=1e-3))\n\n    config.replay_fn = lambda: Replay(memory_size=int(1e6), batch_size=64)\n    config.discount = 0.99\n    config.random_process_fn = lambda: OrnsteinUhlenbeckProcess(\n        size=(config.action_dim,), std=LinearSchedule(0.2))\n    config.warm_up = int(1e4)\n    config.target_network_mix = 1e-3\n    run_steps(DDPGAgent(config))\n\n\n# TD3\ndef td3_continuous(**kwargs):\n    generate_tag(kwargs)\n    kwargs.setdefault('log_level', 0)\n    config = Config()\n    config.merge(kwargs)\n\n    config.task_fn = lambda: Task(config.game)\n    config.eval_env = config.task_fn()\n    config.max_steps = int(1e6)\n    config.eval_interval = int(1e4)\n    config.eval_episodes = 20\n\n    config.network_fn = lambda: TD3Net(\n        config.action_dim,\n        actor_body_fn=lambda: FCBody(config.state_dim, (400, 300), gate=F.relu),\n        critic_body_fn=lambda: FCBody(\n            config.state_dim+config.action_dim, (400, 300), gate=F.relu),\n        actor_opt_fn=lambda params: torch.optim.Adam(params, lr=1e-3),\n        critic_opt_fn=lambda params: torch.optim.Adam(params, lr=1e-3))\n\n    config.replay_fn = lambda: Replay(memory_size=int(1e6), batch_size=100)\n    config.discount = 0.99\n    config.random_process_fn = lambda: GaussianProcess(\n        size=(config.action_dim,), std=LinearSchedule(0.1))\n    config.td3_noise = 0.2\n    config.td3_noise_clip = 0.5\n    config.td3_delay = 2\n    config.warm_up = int(1e4)\n    config.target_network_mix = 5e-3\n    run_steps(TD3Agent(config))\n\n\nif __name__ == '__main__':\n    mkdir('log')\n    mkdir('tf_log')\n    set_one_thread()\n    random_seed()\n    select_device(-1)\n    # select_device(0)\n\n    game = 'CartPole-v0'\n    # dqn_feature(game=game)\n    # quantile_regression_dqn_feature(game=game)\n    # categorical_dqn_feature(game=game)\n    # a2c_feature(game=game)\n    # n_step_dqn_feature(game=game)\n    # option_critic_feature(game=game)\n\n    game = 'HalfCheetah-v2'\n    # game = 'Hopper-v2'\n    # a2c_continuous(game=game)\n    # ppo_continuous(game=game)\n    # ddpg_continuous(game=game)\n    # td3_continuous(game=game)\n\n    game = 'BreakoutNoFrameskip-v4'\n    # dqn_pixel(game=game)\n    # quantile_regression_dqn_pixel(game=game)\n    # categorical_dqn_pixel(game=game)\n    # a2c_pixel(game=game)\n    # n_step_dqn_pixel(game=game)\n    # option_critic_pixel(game=game)\n"""
setup.py,0,"b'from setuptools import setup, find_packages\nimport sys\n\nprint(\'Please install OpenAI Baselines (commit 8e56dd) and requirement.txt\')\nif not (sys.version.startswith(\'3.5\') or sys.version.startswith(\'3.6\')):\n    raise Exception(\'Only Python 3.5 and 3.6 are supported\')\n\nsetup(name=\'deep_rl\',\n      packages=[package for package in find_packages()\n                if package.startswith(\'deep_rl\')],\n      install_requires=[],\n      description=""Modularized Implementation of Deep RL Algorithms"",\n      author=""Shangtong Zhang"",\n      url=\'https://github.com/ShangtongZhang/DeepRL\',\n      author_email=""zhangshangtong.cpp@gmail.com"",\n      version=""1.4"")'"
template_jobs.py,0,"b""from examples import *\n\n\ndef batch_atari():\n    cf = Config()\n    cf.add_argument('--i', type=int, default=0)\n    cf.add_argument('--j', type=int, default=0)\n    cf.merge()\n\n    games = [\n        'BreakoutNoFrameskip-v4',\n        # 'AlienNoFrameskip-v4',\n        # 'DemonAttackNoFrameskip-v4',\n        # 'SeaquestNoFrameskip-v4',\n        # 'MsPacmanNoFrameskip-v4'\n    ]\n\n    algos = [\n        dqn_pixel,\n        quantile_regression_dqn_pixel,\n        categorical_dqn_pixel,\n        a2c_pixel,\n        n_step_dqn_pixel,\n        option_critic_pixel,\n    ]\n\n    algo = algos[cf.i]\n\n    for game in games:\n        for r in range(1):\n            algo(game=game, run=r, remark=algo.__name__)\n\n    exit()\n\n\ndef batch_mujoco():\n    cf = Config()\n    cf.add_argument('--i', type=int, default=0)\n    cf.add_argument('--j', type=int, default=0)\n    cf.merge()\n\n    games = [\n        'dm-acrobot-swingup',\n        'dm-acrobot-swingup_sparse',\n        'dm-ball_in_cup-catch',\n        'dm-cartpole-swingup',\n        'dm-cartpole-swingup_sparse',\n        'dm-cartpole-balance',\n        'dm-cartpole-balance_sparse',\n        'dm-cheetah-run',\n        'dm-finger-turn_hard',\n        'dm-finger-spin',\n        'dm-finger-turn_easy',\n        'dm-fish-upright',\n        'dm-fish-swim',\n        'dm-hopper-stand',\n        'dm-hopper-hop',\n        'dm-humanoid-stand',\n        'dm-humanoid-walk',\n        'dm-humanoid-run',\n        'dm-manipulator-bring_ball',\n        'dm-pendulum-swingup',\n        'dm-point_mass-easy',\n        'dm-reacher-easy',\n        'dm-reacher-hard',\n        'dm-swimmer-swimmer15',\n        'dm-swimmer-swimmer6',\n        'dm-walker-stand',\n        'dm-walker-walk',\n        'dm-walker-run',\n    ]\n\n    games = [\n        'HalfCheetah-v2',\n        'Walker2d-v2',\n        'Swimmer-v2',\n        'Hopper-v2',\n        'Reacher-v2',\n        'Ant-v2',\n        'Humanoid-v2',\n        'HumanoidStandup-v2',\n    ]\n\n    params = []\n\n    for game in games:\n        for algo in [ppo_continuous, ddpg_continuous, td3_continuous]:\n            for r in range(5):\n                params.append([algo, dict(game=game, run=r)])\n\n    algo, param = params[cf.i]\n    algo(**param, remark=algo.__name__)\n\n    exit()\n\n\nif __name__ == '__main__':\n    mkdir('log')\n    mkdir('data')\n    random_seed()\n\n    # select_device(0)\n    # batch_atari()\n\n    select_device(-1)\n    batch_mujoco()\n"""
template_plot.py,0,"b""import matplotlib\n# matplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n# plt.rc('text', usetex=True)\nfrom deep_rl import *\n\n\ndef plot_ppo():\n    plotter = Plotter()\n    games = [\n        'HalfCheetah-v2',\n        'Walker2d-v2',\n        'Hopper-v2',\n        'Swimmer-v2',\n        'Reacher-v2',\n        'Ant-v2',\n        'Humanoid-v2',\n        'HumanoidStandup-v2',\n    ]\n\n    patterns = [\n        'remark_ppo',\n    ]\n\n    labels = [\n        'PPO'\n    ]\n\n    plotter.plot_games(games=games,\n                       patterns=patterns,\n                       agg='mean',\n                       downsample=0,\n                       labels=labels,\n                       right_align=False,\n                       tag=plotter.RETURN_TRAIN,\n                       root='./data/benchmark/ppo',\n                       interpolation=100,\n                       window=10,\n                       )\n\n    # plt.show()\n    plt.tight_layout()\n    plt.savefig('images/PPO.png', bbox_inches='tight')\n\n\ndef plot_ddpg_td3():\n    plotter = Plotter()\n    games = [\n        'HalfCheetah-v2',\n        'Walker2d-v2',\n        'Hopper-v2',\n        'Swimmer-v2',\n        'Reacher-v2',\n        'Ant-v2',\n        # 'Humanoid-v2',\n        # 'HumanoidStandup-v2',\n    ]\n\n    patterns = [\n        'remark_ddpg',\n        'remark_td3',\n    ]\n\n    labels = [\n        'DDPG',\n        'TD3',\n    ]\n\n    plotter.plot_games(games=games,\n                       patterns=patterns,\n                       agg='mean',\n                       downsample=0,\n                       labels=labels,\n                       right_align=False,\n                       tag=plotter.RETURN_TEST,\n                       root='./data/benchmark',\n                       interpolation=0,\n                       window=0,\n                       )\n\n    # plt.show()\n    plt.tight_layout()\n    plt.savefig('images/mujoco_eval.png', bbox_inches='tight')\n\n\ndef plot_atari():\n    plotter = Plotter()\n    games = [\n        'BreakoutNoFrameskip-v4',\n    ]\n\n    patterns = [\n        'remark_a2c',\n        'remark_categorical',\n        'remark_dqn',\n        'remark_n_step_dqn',\n        'remark_option_critic',\n        'remark_quantile',\n    ]\n\n    labels = [\n        'A2C',\n        'C51',\n        'DQN',\n        'N-Step DQN',\n        'OC',\n        'QR-DQN',\n    ]\n\n    plotter.plot_games(games=games,\n                       patterns=patterns,\n                       agg='mean',\n                       downsample=100,\n                       labels=labels,\n                       right_align=False,\n                       tag=plotter.RETURN_TRAIN,\n                       root='./data/benchmark/atari',\n                       interpolation=0,\n                       window=100,\n                       )\n\n    # plt.show()\n    plt.tight_layout()\n    plt.savefig('images/Breakout.png', bbox_inches='tight')\n\n\nif __name__ == '__main__':\n    mkdir('images')\n    plot_ppo()\n    plot_ddpg_td3()\n    # plot_atari()"""
deep_rl/__init__.py,0,b'from .agent import *\nfrom .component import *\nfrom .network import *\nfrom .utils import *'
deep_rl/agent/A2C_agent.py,0,"b""#######################################################################\n# Copyright (C) 2017 Shangtong Zhang(zhangshangtong.cpp@gmail.com)    #\n# Permission given to modify the code as long as you keep this        #\n# declaration at the top                                              #\n#######################################################################\n\nfrom ..network import *\nfrom ..component import *\nfrom .BaseAgent import *\n\n\nclass A2CAgent(BaseAgent):\n    def __init__(self, config):\n        BaseAgent.__init__(self, config)\n        self.config = config\n        self.task = config.task_fn()\n        self.network = config.network_fn()\n        self.optimizer = config.optimizer_fn(self.network.parameters())\n        self.total_steps = 0\n        self.states = self.task.reset()\n\n    def step(self):\n        config = self.config\n        storage = Storage(config.rollout_length)\n        states = self.states\n        for _ in range(config.rollout_length):\n            prediction = self.network(config.state_normalizer(states))\n            next_states, rewards, terminals, info = self.task.step(to_np(prediction['a']))\n            self.record_online_return(info)\n            rewards = config.reward_normalizer(rewards)\n            storage.add(prediction)\n            storage.add({'r': tensor(rewards).unsqueeze(-1),\n                         'm': tensor(1 - terminals).unsqueeze(-1)})\n\n            states = next_states\n            self.total_steps += config.num_workers\n\n        self.states = states\n        prediction = self.network(config.state_normalizer(states))\n        storage.add(prediction)\n        storage.placeholder()\n\n        advantages = tensor(np.zeros((config.num_workers, 1)))\n        returns = prediction['v'].detach()\n        for i in reversed(range(config.rollout_length)):\n            returns = storage.r[i] + config.discount * storage.m[i] * returns\n            if not config.use_gae:\n                advantages = returns - storage.v[i].detach()\n            else:\n                td_error = storage.r[i] + config.discount * storage.m[i] * storage.v[i + 1] - storage.v[i]\n                advantages = advantages * config.gae_tau * config.discount * storage.m[i] + td_error\n            storage.adv[i] = advantages.detach()\n            storage.ret[i] = returns.detach()\n\n        log_prob, value, returns, advantages, entropy = storage.cat(['log_pi_a', 'v', 'ret', 'adv', 'ent'])\n        policy_loss = -(log_prob * advantages).mean()\n        value_loss = 0.5 * (returns - value).pow(2).mean()\n        entropy_loss = entropy.mean()\n\n        self.optimizer.zero_grad()\n        (policy_loss - config.entropy_weight * entropy_loss +\n         config.value_loss_weight * value_loss).backward()\n        nn.utils.clip_grad_norm_(self.network.parameters(), config.gradient_clip)\n        self.optimizer.step()\n"""
deep_rl/agent/BaseAgent.py,3,"b""#######################################################################\n# Copyright (C) 2017 Shangtong Zhang(zhangshangtong.cpp@gmail.com)    #\n# Permission given to modify the code as long as you keep this        #\n# declaration at the top                                              #\n#######################################################################\n\nimport torch\nimport numpy as np\nfrom ..utils import *\nimport torch.multiprocessing as mp\nfrom collections import deque\nfrom skimage.io import imsave\n\n\nclass BaseAgent:\n    def __init__(self, config):\n        self.config = config\n        self.logger = get_logger(tag=config.tag, log_level=config.log_level)\n        self.task_ind = 0\n\n    def close(self):\n        close_obj(self.task)\n\n    def save(self, filename):\n        torch.save(self.network.state_dict(), '%s.model' % (filename))\n        with open('%s.stats' % (filename), 'wb') as f:\n            pickle.dump(self.config.state_normalizer.state_dict(), f)\n\n    def load(self, filename):\n        state_dict = torch.load('%s.model' % filename, map_location=lambda storage, loc: storage)\n        self.network.load_state_dict(state_dict)\n        with open('%s.stats' % (filename), 'rb') as f:\n            self.config.state_normalizer.load_state_dict(pickle.load(f))\n\n    def eval_step(self, state):\n        raise NotImplementedError\n\n    def eval_episode(self):\n        env = self.config.eval_env\n        state = env.reset()\n        while True:\n            action = self.eval_step(state)\n            state, reward, done, info = env.step(action)\n            ret = info[0]['episodic_return']\n            if ret is not None:\n                break\n        return ret\n\n    def eval_episodes(self):\n        episodic_returns = []\n        for ep in range(self.config.eval_episodes):\n            total_rewards = self.eval_episode()\n            episodic_returns.append(np.sum(total_rewards))\n        self.logger.info('steps %d, episodic_return_test %.2f(%.2f)' % (\n            self.total_steps, np.mean(episodic_returns), np.std(episodic_returns) / np.sqrt(len(episodic_returns))\n        ))\n        self.logger.add_scalar('episodic_return_test', np.mean(episodic_returns), self.total_steps)\n        return {\n            'episodic_return_test': np.mean(episodic_returns),\n        }\n\n    def record_online_return(self, info, offset=0):\n        if isinstance(info, dict):\n            ret = info['episodic_return']\n            if ret is not None:\n                self.logger.add_scalar('episodic_return_train', ret, self.total_steps + offset)\n                self.logger.info('steps %d, episodic_return_train %s' % (self.total_steps + offset, ret))\n        elif isinstance(info, tuple):\n            for i, info_ in enumerate(info):\n                self.record_online_return(info_, i)\n        else:\n            raise NotImplementedError\n\n    def switch_task(self):\n        config = self.config\n        if not config.tasks:\n            return\n        segs = np.linspace(0, config.max_steps, len(config.tasks) + 1)\n        if self.total_steps > segs[self.task_ind + 1]:\n            self.task_ind += 1\n            self.task = config.tasks[self.task_ind]\n            self.states = self.task.reset()\n            self.states = config.state_normalizer(self.states)\n\n    def record_episode(self, dir, env):\n        mkdir(dir)\n        steps = 0\n        state = env.reset()\n        while True:\n            self.record_obs(env, dir, steps)\n            action = self.record_step(state)\n            state, reward, done, info = env.step(action)\n            ret = info[0]['episodic_return']\n            steps += 1\n            if ret is not None:\n                break\n\n    def record_step(self, state):\n        raise NotImplementedError\n\n    # For DMControl\n    def record_obs(self, env, dir, steps):\n        env = env.env.envs[0]\n        obs = env.render(mode='rgb_array')\n        imsave('%s/%04d.png' % (dir, steps), obs)\n\n\nclass BaseActor(mp.Process):\n    STEP = 0\n    RESET = 1\n    EXIT = 2\n    SPECS = 3\n    NETWORK = 4\n    CACHE = 5\n\n    def __init__(self, config):\n        mp.Process.__init__(self)\n        self.config = config\n        self.__pipe, self.__worker_pipe = mp.Pipe()\n\n        self._state = None\n        self._task = None\n        self._network = None\n        self._total_steps = 0\n        self.__cache_len = 2\n\n        if not config.async_actor:\n            self.start = lambda: None\n            self.step = self._sample\n            self.close = lambda: None\n            self._set_up()\n            self._task = config.task_fn()\n\n    def _sample(self):\n        transitions = []\n        for _ in range(self.config.sgd_update_frequency):\n            transitions.append(self._transition())\n        return transitions\n\n    def run(self):\n        self._set_up()\n        config = self.config\n        self._task = config.task_fn()\n\n        cache = deque([], maxlen=2)\n        while True:\n            op, data = self.__worker_pipe.recv()\n            if op == self.STEP:\n                if not len(cache):\n                    cache.append(self._sample())\n                    cache.append(self._sample())\n                self.__worker_pipe.send(cache.popleft())\n                cache.append(self._sample())\n            elif op == self.EXIT:\n                self.__worker_pipe.close()\n                return\n            elif op == self.NETWORK:\n                self._network = data\n            else:\n                raise NotImplementedError\n\n    def _transition(self):\n        raise NotImplementedError\n\n    def _set_up(self):\n        pass\n\n    def step(self):\n        self.__pipe.send([self.STEP, None])\n        return self.__pipe.recv()\n\n    def close(self):\n        self.__pipe.send([self.EXIT, None])\n        self.__pipe.close()\n\n    def set_network(self, net):\n        if not self.config.async_actor:\n            self._network = net\n        else:\n            self.__pipe.send([self.NETWORK, net])\n"""
deep_rl/agent/CategoricalDQN_agent.py,1,"b'#######################################################################\n# Copyright (C) 2017 Shangtong Zhang(zhangshangtong.cpp@gmail.com)    #\n# Permission given to modify the code as long as you keep this        #\n# declaration at the top                                              #\n#######################################################################\n\nfrom ..network import *\nfrom ..component import *\nfrom ..utils import *\nimport time\nfrom .BaseAgent import *\n\n\nclass CategoricalDQNActor(BaseActor):\n    def __init__(self, config):\n        BaseActor.__init__(self, config)\n        self.config = config\n        self.start()\n\n    def _set_up(self):\n        self.config.atoms = tensor(self.config.atoms)\n\n    def _transition(self):\n        if self._state is None:\n            self._state = self._task.reset()\n        config = self.config\n        with config.lock:\n            probs, _ = self._network(config.state_normalizer(self._state))\n        q_values = (probs * self.config.atoms).sum(-1)\n        q_values = to_np(q_values).flatten()\n        if self._total_steps < config.exploration_steps \\\n                or np.random.rand() < config.random_action_prob():\n            action = np.random.randint(0, len(q_values))\n        else:\n            action = np.argmax(q_values)\n        next_state, reward, done, info = self._task.step([action])\n        entry = [self._state[0], action, reward[0], next_state[0], int(done[0]), info]\n        self._total_steps += 1\n        self._state = next_state\n        return entry\n\n\nclass CategoricalDQNAgent(BaseAgent):\n    def __init__(self, config):\n        BaseAgent.__init__(self, config)\n        self.config = config\n        config.lock = mp.Lock()\n        config.atoms = np.linspace(config.categorical_v_min,\n                                   config.categorical_v_max, config.categorical_n_atoms)\n\n        self.replay = config.replay_fn()\n        self.actor = CategoricalDQNActor(config)\n\n        self.network = config.network_fn()\n        self.network.share_memory()\n        self.target_network = config.network_fn()\n        self.target_network.load_state_dict(self.network.state_dict())\n        self.optimizer = config.optimizer_fn(self.network.parameters())\n\n        self.actor.set_network(self.network)\n\n        self.total_steps = 0\n        self.batch_indices = range_tensor(self.replay.batch_size)\n        self.atoms = tensor(config.atoms)\n        self.delta_atom = (config.categorical_v_max - config.categorical_v_min) / float(config.categorical_n_atoms - 1)\n\n    def close(self):\n        close_obj(self.replay)\n        close_obj(self.actor)\n\n    def eval_step(self, state):\n        self.config.state_normalizer.set_read_only()\n        state = self.config.state_normalizer(state)\n        prob, _ = self.network(state)\n        q = (prob * self.atoms).sum(-1)\n        action = np.argmax(to_np(q).flatten())\n        self.config.state_normalizer.unset_read_only()\n        return [action]\n\n    def step(self):\n        config = self.config\n        transitions = self.actor.step()\n        experiences = []\n        for state, action, reward, next_state, done, info in transitions:\n            self.record_online_return(info)\n            self.total_steps += 1\n            reward = config.reward_normalizer(reward)\n            experiences.append([state, action, reward, next_state, done])\n        self.replay.feed_batch(experiences)\n\n        if self.total_steps > self.config.exploration_steps:\n            experiences = self.replay.sample()\n            states, actions, rewards, next_states, terminals = experiences\n            states = self.config.state_normalizer(states)\n            next_states = self.config.state_normalizer(next_states)\n\n            prob_next, _ = self.target_network(next_states)\n            prob_next = prob_next.detach()\n            q_next = (prob_next * self.atoms).sum(-1)\n            a_next = torch.argmax(q_next, dim=-1)\n            prob_next = prob_next[self.batch_indices, a_next, :]\n\n            rewards = tensor(rewards).unsqueeze(-1)\n            terminals = tensor(terminals).unsqueeze(-1)\n            atoms_next = rewards + self.config.discount * (1 - terminals) * self.atoms.view(1, -1)\n\n            atoms_next.clamp_(self.config.categorical_v_min, self.config.categorical_v_max)\n            b = (atoms_next - self.config.categorical_v_min) / self.delta_atom\n            l = b.floor()\n            u = b.ceil()\n            d_m_l = (u + (l == u).float() - b) * prob_next\n            d_m_u = (b - l) * prob_next\n            target_prob = tensor(np.zeros(prob_next.size()))\n            for i in range(target_prob.size(0)):\n                target_prob[i].index_add_(0, l[i].long(), d_m_l[i])\n                target_prob[i].index_add_(0, u[i].long(), d_m_u[i])\n\n            _, log_prob = self.network(states)\n            actions = tensor(actions).long()\n            log_prob = log_prob[self.batch_indices, actions, :]\n            loss = -(target_prob * log_prob).sum(-1).mean()\n\n            self.optimizer.zero_grad()\n            loss.backward()\n            nn.utils.clip_grad_norm_(self.network.parameters(), self.config.gradient_clip)\n            with config.lock:\n                self.optimizer.step()\n\n        if self.total_steps / self.config.sgd_update_frequency % \\\n                self.config.target_network_update_freq == 0:\n            self.target_network.load_state_dict(self.network.state_dict())\n'"
deep_rl/agent/DDPG_agent.py,0,"b'#######################################################################\n# Copyright (C) 2017 Shangtong Zhang(zhangshangtong.cpp@gmail.com)    #\n# Permission given to modify the code as long as you keep this        #\n# declaration at the top                                              #\n#######################################################################\n\nfrom ..network import *\nfrom ..component import *\nfrom .BaseAgent import *\nimport torchvision\n\n\nclass DDPGAgent(BaseAgent):\n    def __init__(self, config):\n        BaseAgent.__init__(self, config)\n        self.config = config\n        self.task = config.task_fn()\n        self.network = config.network_fn()\n        self.target_network = config.network_fn()\n        self.target_network.load_state_dict(self.network.state_dict())\n        self.replay = config.replay_fn()\n        self.random_process = config.random_process_fn()\n        self.total_steps = 0\n        self.state = None\n\n    def soft_update(self, target, src):\n        for target_param, param in zip(target.parameters(), src.parameters()):\n            target_param.detach_()\n            target_param.copy_(target_param * (1.0 - self.config.target_network_mix) +\n                               param * self.config.target_network_mix)\n\n    def eval_step(self, state):\n        self.config.state_normalizer.set_read_only()\n        state = self.config.state_normalizer(state)\n        action = self.network(state)\n        self.config.state_normalizer.unset_read_only()\n        return to_np(action)\n\n    def step(self):\n        config = self.config\n        if self.state is None:\n            self.random_process.reset_states()\n            self.state = self.task.reset()\n            self.state = config.state_normalizer(self.state)\n\n        if self.total_steps < config.warm_up:\n            action = [self.task.action_space.sample()]\n        else:\n            action = self.network(self.state)\n            action = to_np(action)\n            action += self.random_process.sample()\n        action = np.clip(action, self.task.action_space.low, self.task.action_space.high)\n        next_state, reward, done, info = self.task.step(action)\n        next_state = self.config.state_normalizer(next_state)\n        self.record_online_return(info)\n        reward = self.config.reward_normalizer(reward)\n\n        experiences = list(zip(self.state, action, reward, next_state, done))\n        self.replay.feed_batch(experiences)\n        if done[0]:\n            self.random_process.reset_states()\n        self.state = next_state\n        self.total_steps += 1\n\n        if self.replay.size() >= config.warm_up:\n            experiences = self.replay.sample()\n            states, actions, rewards, next_states, terminals = experiences\n            states = tensor(states)\n            actions = tensor(actions)\n            rewards = tensor(rewards).unsqueeze(-1)\n            next_states = tensor(next_states)\n            mask = tensor(1 - terminals).unsqueeze(-1)\n\n            phi_next = self.target_network.feature(next_states)\n            a_next = self.target_network.actor(phi_next)\n            q_next = self.target_network.critic(phi_next, a_next)\n            q_next = config.discount * mask * q_next\n            q_next.add_(rewards)\n            q_next = q_next.detach()\n            phi = self.network.feature(states)\n            q = self.network.critic(phi, actions)\n            critic_loss = (q - q_next).pow(2).mul(0.5).sum(-1).mean()\n\n            self.network.zero_grad()\n            critic_loss.backward()\n            self.network.critic_opt.step()\n\n            phi = self.network.feature(states)\n            action = self.network.actor(phi)\n            policy_loss = -self.network.critic(phi.detach(), action).mean()\n\n            self.network.zero_grad()\n            policy_loss.backward()\n            self.network.actor_opt.step()\n\n            self.soft_update(self.target_network, self.network)\n'"
deep_rl/agent/DQN_agent.py,1,"b'#######################################################################\n# Copyright (C) 2017 Shangtong Zhang(zhangshangtong.cpp@gmail.com)    #\n# Permission given to modify the code as long as you keep this        #\n# declaration at the top                                              #\n#######################################################################\n\nfrom ..network import *\nfrom ..component import *\nfrom ..utils import *\nimport time\nfrom .BaseAgent import *\n\n\nclass DQNActor(BaseActor):\n    def __init__(self, config):\n        BaseActor.__init__(self, config)\n        self.config = config\n        self.start()\n\n    def _transition(self):\n        if self._state is None:\n            self._state = self._task.reset()\n        config = self.config\n        with config.lock:\n            q_values = self._network(config.state_normalizer(self._state))\n        q_values = to_np(q_values).flatten()\n        if self._total_steps < config.exploration_steps \\\n                or np.random.rand() < config.random_action_prob():\n            action = np.random.randint(0, len(q_values))\n        else:\n            action = np.argmax(q_values)\n        next_state, reward, done, info = self._task.step([action])\n        entry = [self._state[0], action, reward[0], next_state[0], int(done[0]), info]\n        self._total_steps += 1\n        self._state = next_state\n        return entry\n\n\nclass DQNAgent(BaseAgent):\n    def __init__(self, config):\n        BaseAgent.__init__(self, config)\n        self.config = config\n        config.lock = mp.Lock()\n\n        self.replay = config.replay_fn()\n        self.actor = DQNActor(config)\n\n        self.network = config.network_fn()\n        self.network.share_memory()\n        self.target_network = config.network_fn()\n        self.target_network.load_state_dict(self.network.state_dict())\n        self.optimizer = config.optimizer_fn(self.network.parameters())\n\n        self.actor.set_network(self.network)\n\n        self.total_steps = 0\n        self.batch_indices = range_tensor(self.replay.batch_size)\n\n    def close(self):\n        close_obj(self.replay)\n        close_obj(self.actor)\n\n    def eval_step(self, state):\n        self.config.state_normalizer.set_read_only()\n        state = self.config.state_normalizer(state)\n        q = self.network(state)\n        action = to_np(q.argmax(-1))\n        self.config.state_normalizer.unset_read_only()\n        return action\n\n    def step(self):\n        config = self.config\n        transitions = self.actor.step()\n        experiences = []\n        for state, action, reward, next_state, done, info in transitions:\n            self.record_online_return(info)\n            self.total_steps += 1\n            reward = config.reward_normalizer(reward)\n            experiences.append([state, action, reward, next_state, done])\n        self.replay.feed_batch(experiences)\n\n        if self.total_steps > self.config.exploration_steps:\n            experiences = self.replay.sample()\n            states, actions, rewards, next_states, terminals = experiences\n            states = self.config.state_normalizer(states)\n            next_states = self.config.state_normalizer(next_states)\n            q_next = self.target_network(next_states).detach()\n            if self.config.double_q:\n                best_actions = torch.argmax(self.network(next_states), dim=-1)\n                q_next = q_next[self.batch_indices, best_actions]\n            else:\n                q_next = q_next.max(1)[0]\n            terminals = tensor(terminals)\n            rewards = tensor(rewards)\n            q_next = self.config.discount * q_next * (1 - terminals)\n            q_next.add_(rewards)\n            actions = tensor(actions).long()\n            q = self.network(states)\n            q = q[self.batch_indices, actions]\n            loss = (q_next - q).pow(2).mul(0.5).mean()\n            self.optimizer.zero_grad()\n            loss.backward()\n            nn.utils.clip_grad_norm_(self.network.parameters(), self.config.gradient_clip)\n            with config.lock:\n                self.optimizer.step()\n\n        if self.total_steps / self.config.sgd_update_frequency % \\\n                self.config.target_network_update_freq == 0:\n            self.target_network.load_state_dict(self.network.state_dict())\n'"
deep_rl/agent/NStepDQN_agent.py,1,"b""#######################################################################\n# Copyright (C) 2017 Shangtong Zhang(zhangshangtong.cpp@gmail.com)    #\n# Permission given to modify the code as long as you keep this        #\n# declaration at the top                                              #\n#######################################################################\n\nfrom ..network import *\nfrom ..component import *\nfrom ..utils import *\nfrom .BaseAgent import *\n\n\nclass NStepDQNAgent(BaseAgent):\n    def __init__(self, config):\n        BaseAgent.__init__(self, config)\n        self.config = config\n        self.task = config.task_fn()\n        self.network = config.network_fn()\n        self.target_network = config.network_fn()\n        self.optimizer = config.optimizer_fn(self.network.parameters())\n        self.target_network.load_state_dict(self.network.state_dict())\n\n        self.total_steps = 0\n        self.states = self.task.reset()\n\n    def step(self):\n        config = self.config\n        storage = Storage(config.rollout_length)\n\n        states = self.states\n        for _ in range(config.rollout_length):\n            q = self.network(self.config.state_normalizer(states))\n\n            epsilon = config.random_action_prob(config.num_workers)\n            actions = epsilon_greedy(epsilon, to_np(q))\n\n            next_states, rewards, terminals, info = self.task.step(actions)\n            self.record_online_return(info)\n            rewards = config.reward_normalizer(rewards)\n\n            storage.add({'q': q,\n                         'a': tensor(actions).unsqueeze(-1).long(),\n                         'r': tensor(rewards).unsqueeze(-1),\n                         'm': tensor(1 - terminals).unsqueeze(-1)})\n\n            states = next_states\n\n            self.total_steps += config.num_workers\n            if self.total_steps // config.num_workers % config.target_network_update_freq == 0:\n                self.target_network.load_state_dict(self.network.state_dict())\n\n        self.states = states\n\n        storage.placeholder()\n\n        ret = self.target_network(config.state_normalizer(states)).detach()\n        ret = torch.max(ret, dim=1, keepdim=True)[0]\n        for i in reversed(range(config.rollout_length)):\n            ret = storage.r[i] + config.discount * storage.m[i] * ret\n            storage.ret[i] = ret\n\n        q, action, ret = storage.cat(['q', 'a', 'ret'])\n        loss = 0.5 * (q.gather(1, action) - ret).pow(2).mean()\n        self.optimizer.zero_grad()\n        loss.backward()\n        nn.utils.clip_grad_norm_(self.network.parameters(), config.gradient_clip)\n        self.optimizer.step()\n"""
deep_rl/agent/OptionCritic_agent.py,10,"b""#######################################################################\n# Copyright (C) 2017 Shangtong Zhang(zhangshangtong.cpp@gmail.com)    #\n# Permission given to modify the code as long as you keep this        #\n# declaration at the top                                              #\n#######################################################################\n\nfrom ..network import *\nfrom ..component import *\nfrom .BaseAgent import *\n\n\nclass OptionCriticAgent(BaseAgent):\n    def __init__(self, config):\n        BaseAgent.__init__(self, config)\n        self.config = config\n        self.task = config.task_fn()\n        self.network = config.network_fn()\n        self.target_network = config.network_fn()\n        self.optimizer = config.optimizer_fn(self.network.parameters())\n        self.target_network.load_state_dict(self.network.state_dict())\n\n        self.total_steps = 0\n        self.worker_index = tensor(np.arange(config.num_workers)).long()\n\n        self.states = self.config.state_normalizer(self.task.reset())\n        self.is_initial_states = tensor(np.ones((config.num_workers))).byte()\n        self.prev_options = self.is_initial_states.clone().long()\n\n    def sample_option(self, prediction, epsilon, prev_option, is_intial_states):\n        with torch.no_grad():\n            q_option = prediction['q']\n            pi_option = torch.zeros_like(q_option).add(epsilon / q_option.size(1))\n            greedy_option = q_option.argmax(dim=-1, keepdim=True)\n            prob = 1 - epsilon + epsilon / q_option.size(1)\n            prob = torch.zeros_like(pi_option).add(prob)\n            pi_option.scatter_(1, greedy_option, prob)\n\n            mask = torch.zeros_like(q_option)\n            mask[:, prev_option] = 1\n            beta = prediction['beta']\n            pi_hat_option = (1 - beta) * mask + beta * pi_option\n\n            dist = torch.distributions.Categorical(probs=pi_option)\n            options = dist.sample()\n            dist = torch.distributions.Categorical(probs=pi_hat_option)\n            options_hat = dist.sample()\n\n            options = torch.where(is_intial_states, options, options_hat)\n        return options\n\n    def step(self):\n        config = self.config\n        storage = Storage(config.rollout_length, ['beta', 'o', 'beta_adv', 'prev_o', 'init', 'eps'])\n\n        for _ in range(config.rollout_length):\n            prediction = self.network(self.states)\n            epsilon = config.random_option_prob(config.num_workers)\n            options = self.sample_option(prediction, epsilon, self.prev_options, self.is_initial_states)\n            prediction['pi'] = prediction['pi'][self.worker_index, options]\n            prediction['log_pi'] = prediction['log_pi'][self.worker_index, options]\n            dist = torch.distributions.Categorical(probs=prediction['pi'])\n            actions = dist.sample()\n            entropy = dist.entropy()\n\n            next_states, rewards, terminals, info = self.task.step(to_np(actions))\n            self.record_online_return(info)\n            next_states = config.state_normalizer(next_states)\n            rewards = config.reward_normalizer(rewards)\n            storage.add(prediction)\n            storage.add({'r': tensor(rewards).unsqueeze(-1),\n                         'm': tensor(1 - terminals).unsqueeze(-1),\n                         'o': options.unsqueeze(-1),\n                         'prev_o': self.prev_options.unsqueeze(-1),\n                         'ent': entropy.unsqueeze(-1),\n                         'a': actions.unsqueeze(-1),\n                         'init': self.is_initial_states.unsqueeze(-1).float(),\n                         'eps': epsilon})\n\n            self.is_initial_states = tensor(terminals).byte()\n            self.prev_options = options\n            self.states = next_states\n\n            self.total_steps += config.num_workers\n            if self.total_steps // config.num_workers % config.target_network_update_freq == 0:\n                self.target_network.load_state_dict(self.network.state_dict())\n\n        with torch.no_grad():\n            prediction = self.target_network(self.states)\n            storage.placeholder()\n            betas = prediction['beta'][self.worker_index, self.prev_options]\n            ret = (1 - betas) * prediction['q'][self.worker_index, self.prev_options] + \\\n                  betas * torch.max(prediction['q'], dim=-1)[0]\n            ret = ret.unsqueeze(-1)\n\n        for i in reversed(range(config.rollout_length)):\n            ret = storage.r[i] + config.discount * storage.m[i] * ret\n            adv = ret - storage.q[i].gather(1, storage.o[i])\n            storage.ret[i] = ret\n            storage.adv[i] = adv\n\n            v = storage.q[i].max(dim=-1, keepdim=True)[0] * (1 - storage.eps[i]) + storage.q[i].mean(-1).unsqueeze(-1) * storage.eps[i]\n            q = storage.q[i].gather(1, storage.prev_o[i])\n            storage.beta_adv[i] = q - v + config.termination_regularizer\n\n        q, beta, log_pi, ret, adv, beta_adv, ent, option, action, initial_states, prev_o = \\\n            storage.cat(['q', 'beta', 'log_pi', 'ret', 'adv', 'beta_adv', 'ent', 'o', 'a', 'init', 'prev_o'])\n\n        q_loss = (q.gather(1, option) - ret.detach()).pow(2).mul(0.5).mean()\n        pi_loss = -(log_pi.gather(1, action) * adv.detach()) - config.entropy_weight * ent\n        pi_loss = pi_loss.mean()\n        beta_loss = (beta.gather(1, prev_o) * beta_adv.detach() * (1 - initial_states)).mean()\n\n        self.optimizer.zero_grad()\n        (pi_loss + q_loss + beta_loss).backward()\n        nn.utils.clip_grad_norm_(self.network.parameters(), config.gradient_clip)\n        self.optimizer.step()\n"""
deep_rl/agent/PPO_agent.py,1,"b""#######################################################################\n# Copyright (C) 2017 Shangtong Zhang(zhangshangtong.cpp@gmail.com)    #\n# Permission given to modify the code as long as you keep this        #\n# declaration at the top                                              #\n#######################################################################\n\nfrom ..network import *\nfrom ..component import *\nfrom .BaseAgent import *\n\n\nclass PPOAgent(BaseAgent):\n    def __init__(self, config):\n        BaseAgent.__init__(self, config)\n        self.config = config\n        self.task = config.task_fn()\n        self.network = config.network_fn()\n        self.actor_opt = config.actor_opt_fn(self.network.actor_params)\n        self.critic_opt = config.critic_opt_fn(self.network.critic_params)\n        self.total_steps = 0\n        self.states = self.task.reset()\n        self.states = config.state_normalizer(self.states)\n\n    def step(self):\n        config = self.config\n        storage = Storage(config.rollout_length)\n        states = self.states\n        for _ in range(config.rollout_length):\n            prediction = self.network(states)\n            next_states, rewards, terminals, info = self.task.step(to_np(prediction['a']))\n            self.record_online_return(info)\n            rewards = config.reward_normalizer(rewards)\n            next_states = config.state_normalizer(next_states)\n            storage.add(prediction)\n            storage.add({'r': tensor(rewards).unsqueeze(-1),\n                         'm': tensor(1 - terminals).unsqueeze(-1),\n                         's': tensor(states)})\n            states = next_states\n            self.total_steps += config.num_workers\n\n        self.states = states\n        prediction = self.network(states)\n        storage.add(prediction)\n        storage.placeholder()\n\n        advantages = tensor(np.zeros((config.num_workers, 1)))\n        returns = prediction['v'].detach()\n        for i in reversed(range(config.rollout_length)):\n            returns = storage.r[i] + config.discount * storage.m[i] * returns\n            if not config.use_gae:\n                advantages = returns - storage.v[i].detach()\n            else:\n                td_error = storage.r[i] + config.discount * storage.m[i] * storage.v[i + 1] - storage.v[i]\n                advantages = advantages * config.gae_tau * config.discount * storage.m[i] + td_error\n            storage.adv[i] = advantages.detach()\n            storage.ret[i] = returns.detach()\n\n        states, actions, log_probs_old, returns, advantages = storage.cat(['s', 'a', 'log_pi_a', 'ret', 'adv'])\n        actions = actions.detach()\n        log_probs_old = log_probs_old.detach()\n        advantages = (advantages - advantages.mean()) / advantages.std()\n\n        for _ in range(config.optimization_epochs):\n            sampler = random_sample(np.arange(states.size(0)), config.mini_batch_size)\n            for batch_indices in sampler:\n                batch_indices = tensor(batch_indices).long()\n                sampled_states = states[batch_indices]\n                sampled_actions = actions[batch_indices]\n                sampled_log_probs_old = log_probs_old[batch_indices]\n                sampled_returns = returns[batch_indices]\n                sampled_advantages = advantages[batch_indices]\n\n                prediction = self.network(sampled_states, sampled_actions)\n                ratio = (prediction['log_pi_a'] - sampled_log_probs_old).exp()\n                obj = ratio * sampled_advantages\n                obj_clipped = ratio.clamp(1.0 - self.config.ppo_ratio_clip,\n                                          1.0 + self.config.ppo_ratio_clip) * sampled_advantages\n                policy_loss = -torch.min(obj, obj_clipped).mean() - config.entropy_weight * prediction['ent'].mean()\n\n                value_loss = 0.5 * (sampled_returns - prediction['v']).pow(2).mean()\n\n                approx_kl = (sampled_log_probs_old - prediction['log_pi_a']).mean()\n                if approx_kl <= 1.5 * config.target_kl:\n                    self.actor_opt.zero_grad()\n                    policy_loss.backward()\n                    self.actor_opt.step()\n\n                self.critic_opt.zero_grad()\n                value_loss.backward()\n                self.critic_opt.step()\n\n"""
deep_rl/agent/QuantileRegressionDQN_agent.py,1,"b'#######################################################################\n# Copyright (C) 2017 Shangtong Zhang(zhangshangtong.cpp@gmail.com)    #\n# Permission given to modify the code as long as you keep this        #\n# declaration at the top                                              #\n#######################################################################\n\nfrom ..network import *\nfrom ..component import *\nfrom ..utils import *\nfrom .BaseAgent import *\n\n\nclass QuantileRegressionDQNActor(BaseActor):\n    def __init__(self, config):\n        BaseActor.__init__(self, config)\n        self.config = config\n        self.start()\n\n    def _transition(self):\n        if self._state is None:\n            self._state = self._task.reset()\n        config = self.config\n        with config.lock:\n            q_values = self._network(config.state_normalizer(self._state)).mean(-1)\n        q_values = to_np(q_values).flatten()\n        if self._total_steps < config.exploration_steps \\\n                or np.random.rand() < config.random_action_prob():\n            action = np.random.randint(0, len(q_values))\n        else:\n            action = np.argmax(q_values)\n        next_state, reward, done, info = self._task.step([action])\n        entry = [self._state[0], action, reward[0], next_state[0], int(done[0]), info]\n        self._total_steps += 1\n        self._state = next_state\n        return entry\n\n\nclass QuantileRegressionDQNAgent(BaseAgent):\n    def __init__(self, config):\n        BaseAgent.__init__(self, config)\n        self.config = config\n        config.lock = mp.Lock()\n\n        self.replay = config.replay_fn()\n        self.actor = QuantileRegressionDQNActor(config)\n\n        self.network = config.network_fn()\n        self.network.share_memory()\n        self.target_network = config.network_fn()\n        self.target_network.load_state_dict(self.network.state_dict())\n        self.optimizer = config.optimizer_fn(self.network.parameters())\n\n        self.actor.set_network(self.network)\n\n        self.total_steps = 0\n        self.batch_indices = range_tensor(self.replay.batch_size)\n\n        self.quantile_weight = 1.0 / self.config.num_quantiles\n        self.cumulative_density = tensor(\n            (2 * np.arange(self.config.num_quantiles) + 1) / (2.0 * self.config.num_quantiles)).view(1, -1)\n\n    def close(self):\n        close_obj(self.replay)\n        close_obj(self.actor)\n\n    def eval_step(self, state):\n        self.config.state_normalizer.set_read_only()\n        state = self.config.state_normalizer(state)\n        q = self.network(state).mean(-1)\n        action = np.argmax(to_np(q).flatten())\n        self.config.state_normalizer.unset_read_only()\n        return [action]\n\n    def step(self):\n        config = self.config\n        transitions = self.actor.step()\n        experiences = []\n        for state, action, reward, next_state, done, info in transitions:\n            self.record_online_return(info)\n            self.total_steps += 1\n            reward = config.reward_normalizer(reward)\n            experiences.append([state, action, reward, next_state, done])\n        self.replay.feed_batch(experiences)\n\n        if self.total_steps > self.config.exploration_steps:\n            experiences = self.replay.sample()\n            states, actions, rewards, next_states, terminals = experiences\n            states = self.config.state_normalizer(states)\n            next_states = self.config.state_normalizer(next_states)\n\n            quantiles_next = self.target_network(next_states).detach()\n            a_next = torch.argmax(quantiles_next.sum(-1), dim=-1)\n            quantiles_next = quantiles_next[self.batch_indices, a_next, :]\n\n            rewards = tensor(rewards).unsqueeze(-1)\n            terminals = tensor(terminals).unsqueeze(-1)\n            quantiles_next = rewards + self.config.discount * (1 - terminals) * quantiles_next\n\n            quantiles = self.network(states)\n            actions = tensor(actions).long()\n            quantiles = quantiles[self.batch_indices, actions, :]\n\n            quantiles_next = quantiles_next.t().unsqueeze(-1)\n            diff = quantiles_next - quantiles\n            loss = huber(diff) * (self.cumulative_density - (diff.detach() < 0).float()).abs()\n\n            self.optimizer.zero_grad()\n            loss.mean(0).mean(1).sum().backward()\n            nn.utils.clip_grad_norm_(self.network.parameters(), self.config.gradient_clip)\n            with config.lock:\n                self.optimizer.step()\n\n        if self.total_steps / self.config.sgd_update_frequency % \\\n                self.config.target_network_update_freq == 0:\n            self.target_network.load_state_dict(self.network.state_dict())\n'"
deep_rl/agent/TD3_agent.py,2,"b'#######################################################################\n# Copyright (C) 2017 Shangtong Zhang(zhangshangtong.cpp@gmail.com)    #\n# Permission given to modify the code as long as you keep this        #\n# declaration at the top                                              #\n#######################################################################\n\nfrom ..network import *\nfrom ..component import *\nfrom .BaseAgent import *\nimport torchvision\n\n\nclass TD3Agent(BaseAgent):\n    def __init__(self, config):\n        BaseAgent.__init__(self, config)\n        self.config = config\n        self.task = config.task_fn()\n        self.network = config.network_fn()\n        self.target_network = config.network_fn()\n        self.target_network.load_state_dict(self.network.state_dict())\n        self.replay = config.replay_fn()\n        self.random_process = config.random_process_fn()\n        self.total_steps = 0\n        self.state = None\n\n    def soft_update(self, target, src):\n        for target_param, param in zip(target.parameters(), src.parameters()):\n            target_param.detach_()\n            target_param.copy_(target_param * (1.0 - self.config.target_network_mix) +\n                               param * self.config.target_network_mix)\n\n    def eval_step(self, state):\n        self.config.state_normalizer.set_read_only()\n        state = self.config.state_normalizer(state)\n        action = self.network(state)\n        self.config.state_normalizer.unset_read_only()\n        return to_np(action)\n\n    def step(self):\n        config = self.config\n        if self.state is None:\n            self.random_process.reset_states()\n            self.state = self.task.reset()\n            self.state = config.state_normalizer(self.state)\n\n        if self.total_steps < config.warm_up:\n            action = [self.task.action_space.sample()]\n        else:\n            action = self.network(self.state)\n            action = to_np(action)\n            action += self.random_process.sample()\n        action = np.clip(action, self.task.action_space.low, self.task.action_space.high)\n        next_state, reward, done, info = self.task.step(action)\n        next_state = self.config.state_normalizer(next_state)\n        self.record_online_return(info)\n        reward = self.config.reward_normalizer(reward)\n\n        experiences = list(zip(self.state, action, reward, next_state, done))\n        self.replay.feed_batch(experiences)\n        if done[0]:\n            self.random_process.reset_states()\n        self.state = next_state\n        self.total_steps += 1\n\n        if self.replay.size() >= config.warm_up:\n            experiences = self.replay.sample()\n            states, actions, rewards, next_states, terminals = experiences\n            states = tensor(states)\n            actions = tensor(actions)\n            rewards = tensor(rewards).unsqueeze(-1)\n            next_states = tensor(next_states)\n            mask = tensor(1 - terminals).unsqueeze(-1)\n\n            a_next = self.target_network(next_states)\n            noise = torch.randn_like(a_next).mul(config.td3_noise)\n            noise = noise.clamp(-config.td3_noise_clip, config.td3_noise_clip)\n\n            min_a = float(self.task.action_space.low[0])\n            max_a = float(self.task.action_space.high[0])\n            a_next = (a_next + noise).clamp(min_a, max_a)\n\n            q_1, q_2 = self.target_network.q(next_states, a_next)\n            target = rewards + config.discount * mask * torch.min(q_1, q_2)\n            target = target.detach()\n\n            q_1, q_2 = self.network.q(states, actions)\n            critic_loss = F.mse_loss(q_1, target) + F.mse_loss(q_2, target)\n\n            self.network.zero_grad()\n            critic_loss.backward()\n            self.network.critic_opt.step()\n\n            if self.total_steps % config.td3_delay:\n                action = self.network(states)\n                policy_loss = -self.network.q(states, action)[0].mean()\n\n                self.network.zero_grad()\n                policy_loss.backward()\n                self.network.actor_opt.step()\n\n                self.soft_update(self.target_network, self.network)\n'"
deep_rl/agent/__init__.py,0,b'from .DQN_agent import *\nfrom .DDPG_agent import *\nfrom .A2C_agent import *\nfrom .CategoricalDQN_agent import *\nfrom .NStepDQN_agent import *\nfrom .QuantileRegressionDQN_agent import *\nfrom .PPO_agent import *\nfrom .OptionCritic_agent import *\nfrom .TD3_agent import *\n'
deep_rl/component/__init__.py,0,b'from .replay import *\nfrom .random_process import *\nfrom .envs import Task\n'
deep_rl/component/envs.py,0,"b'#######################################################################\n# Copyright (C) 2017 Shangtong Zhang(zhangshangtong.cpp@gmail.com)    #\n# Permission given to modify the code as long as you keep this        #\n# declaration at the top                                              #\n#######################################################################\n\nimport os\nimport gym\nimport numpy as np\nimport torch\nfrom gym.spaces.box import Box\nfrom gym.spaces.discrete import Discrete\n\nfrom baselines.common.atari_wrappers import make_atari, wrap_deepmind\nfrom baselines.common.atari_wrappers import FrameStack as FrameStack_\nfrom baselines.common.vec_env.subproc_vec_env import SubprocVecEnv, VecEnv\n\nfrom ..utils import *\n\ntry:\n    import roboschool\nexcept ImportError:\n    pass\n\n\n# adapted from https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/blob/master/envs.py\ndef make_env(env_id, seed, rank, episode_life=True):\n    def _thunk():\n        random_seed(seed)\n        if env_id.startswith(""dm""):\n            import dm_control2gym\n            _, domain, task = env_id.split(\'-\')\n            env = dm_control2gym.make(domain_name=domain, task_name=task)\n        else:\n            env = gym.make(env_id)\n        is_atari = hasattr(gym.envs, \'atari\') and isinstance(\n            env.unwrapped, gym.envs.atari.atari_env.AtariEnv)\n        if is_atari:\n            env = make_atari(env_id)\n        env.seed(seed + rank)\n        env = OriginalReturnWrapper(env)\n        if is_atari:\n            env = wrap_deepmind(env,\n                                episode_life=episode_life,\n                                clip_rewards=False,\n                                frame_stack=False,\n                                scale=False)\n            obs_shape = env.observation_space.shape\n            if len(obs_shape) == 3:\n                env = TransposeImage(env)\n            env = FrameStack(env, 4)\n\n        return env\n\n    return _thunk\n\n\nclass OriginalReturnWrapper(gym.Wrapper):\n    def __init__(self, env):\n        gym.Wrapper.__init__(self, env)\n        self.total_rewards = 0\n\n    def step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        self.total_rewards += reward\n        if done:\n            info[\'episodic_return\'] = self.total_rewards\n            self.total_rewards = 0\n        else:\n            info[\'episodic_return\'] = None\n        return obs, reward, done, info\n\n    def reset(self):\n        return self.env.reset()\n\n\nclass TransposeImage(gym.ObservationWrapper):\n    def __init__(self, env=None):\n        super(TransposeImage, self).__init__(env)\n        obs_shape = self.observation_space.shape\n        self.observation_space = Box(\n            self.observation_space.low[0, 0, 0],\n            self.observation_space.high[0, 0, 0],\n            [obs_shape[2], obs_shape[1], obs_shape[0]],\n            dtype=self.observation_space.dtype)\n\n    def observation(self, observation):\n        return observation.transpose(2, 0, 1)\n\n\n# The original LayzeFrames doesn\'t work well\nclass LazyFrames(object):\n    def __init__(self, frames):\n        """"""This object ensures that common frames between the observations are only stored once.\n        It exists purely to optimize memory usage which can be huge for DQN\'s 1M frames replay\n        buffers.\n\n        This object should only be converted to numpy array before being passed to the model.\n\n        You\'d not believe how complex the previous solution was.""""""\n        self._frames = frames\n\n    def __array__(self, dtype=None):\n        out = np.concatenate(self._frames, axis=0)\n        if dtype is not None:\n            out = out.astype(dtype)\n        return out\n\n    def __len__(self):\n        return len(self.__array__())\n\n    def __getitem__(self, i):\n        return self.__array__()[i]\n\n\nclass FrameStack(FrameStack_):\n    def __init__(self, env, k):\n        FrameStack_.__init__(self, env, k)\n\n    def _get_ob(self):\n        assert len(self.frames) == self.k\n        return LazyFrames(list(self.frames))\n\n\n# The original one in baselines is really bad\nclass DummyVecEnv(VecEnv):\n    def __init__(self, env_fns):\n        self.envs = [fn() for fn in env_fns]\n        env = self.envs[0]\n        VecEnv.__init__(self, len(env_fns), env.observation_space, env.action_space)\n        self.actions = None\n\n    def step_async(self, actions):\n        self.actions = actions\n\n    def step_wait(self):\n        data = []\n        for i in range(self.num_envs):\n            obs, rew, done, info = self.envs[i].step(self.actions[i])\n            if done:\n                obs = self.envs[i].reset()\n            data.append([obs, rew, done, info])\n        obs, rew, done, info = zip(*data)\n        return obs, np.asarray(rew), np.asarray(done), info\n\n    def reset(self):\n        return [env.reset() for env in self.envs]\n\n    def close(self):\n        return\n\n\nclass Task:\n    def __init__(self,\n                 name,\n                 num_envs=1,\n                 single_process=True,\n                 log_dir=None,\n                 episode_life=True,\n                 seed=None):\n        if seed is None:\n            seed = np.random.randint(int(1e9))\n        if log_dir is not None:\n            mkdir(log_dir)\n        envs = [make_env(name, seed, i, episode_life) for i in range(num_envs)]\n        if single_process:\n            Wrapper = DummyVecEnv\n        else:\n            Wrapper = SubprocVecEnv\n        self.env = Wrapper(envs)\n        self.name = name\n        self.observation_space = self.env.observation_space\n        self.state_dim = int(np.prod(self.env.observation_space.shape))\n\n        self.action_space = self.env.action_space\n        if isinstance(self.action_space, Discrete):\n            self.action_dim = self.action_space.n\n        elif isinstance(self.action_space, Box):\n            self.action_dim = self.action_space.shape[0]\n        else:\n            assert \'unknown action space\'\n\n    def reset(self):\n        return self.env.reset()\n\n    def step(self, actions):\n        if isinstance(self.action_space, Box):\n            actions = np.clip(actions, self.action_space.low, self.action_space.high)\n        return self.env.step(actions)\n\n\nif __name__ == \'__main__\':\n    task = Task(\'Hopper-v2\', 5, single_process=False)\n    state = task.reset()\n    while True:\n        action = np.random.rand(task.observation_space.shape[0])\n        next_state, reward, done, _ = task.step(action)\n        print(done)\n'"
deep_rl/component/random_process.py,0,"b'#######################################################################\n# Copyright (C) 2017 Shangtong Zhang(zhangshangtong.cpp@gmail.com)    #\n# Permission given to modify the code as long as you keep this        #\n# declaration at the top                                              #\n#######################################################################\n\nimport numpy as np\n\n\nclass RandomProcess(object):\n    def reset_states(self):\n        pass\n\n\nclass GaussianProcess(RandomProcess):\n    def __init__(self, size, std):\n        self.size = size\n        self.std = std\n\n    def sample(self):\n        return np.random.randn(*self.size) * self.std()\n\n\nclass OrnsteinUhlenbeckProcess(RandomProcess):\n    def __init__(self, size, std, theta=.15, dt=1e-2, x0=None):\n        self.theta = theta\n        self.mu = 0\n        self.std = std\n        self.dt = dt\n        self.x0 = x0\n        self.size = size\n        self.reset_states()\n\n    def sample(self):\n        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.std() * np.sqrt(\n            self.dt) * np.random.randn(*self.size)\n        self.x_prev = x\n        return x\n\n    def reset_states(self):\n        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)\n'"
deep_rl/component/replay.py,2,"b""#######################################################################\n# Copyright (C) 2017 Shangtong Zhang(zhangshangtong.cpp@gmail.com)    #\n# Permission given to modify the code as long as you keep this        #\n# declaration at the top                                              #\n#######################################################################\n\nimport torch\nimport numpy as np\nimport torch.multiprocessing as mp\nfrom collections import deque\nfrom ..utils import *\n\n\nclass Replay:\n    def __init__(self, memory_size, batch_size, drop_prob=0, to_np=True):\n        self.memory_size = memory_size\n        self.batch_size = batch_size\n        self.data = []\n        self.pos = 0\n        self.drop_prob = drop_prob\n        self.to_np = to_np\n\n    def feed(self, experience):\n        if np.random.rand() < self.drop_prob:\n            return\n        if self.pos >= len(self.data):\n            self.data.append(experience)\n        else:\n            self.data[self.pos] = experience\n        self.pos = (self.pos + 1) % self.memory_size\n\n    def feed_batch(self, experience):\n        for exp in experience:\n            self.feed(exp)\n\n    def sample(self, batch_size=None):\n        if self.empty():\n            return None\n        if batch_size is None:\n            batch_size = self.batch_size\n\n        sampled_indices = [np.random.randint(0, len(self.data)) for _ in range(batch_size)]\n        sampled_data = [self.data[ind] for ind in sampled_indices]\n        sampled_data = zip(*sampled_data)\n        if self.to_np:\n            sampled_data = list(map(lambda x: np.asarray(x), sampled_data))\n        return sampled_data\n\n    def size(self):\n        return len(self.data)\n\n    def empty(self):\n        return not len(self.data)\n\n    def shuffle(self):\n        np.random.shuffle(self.data)\n\n    def clear(self):\n        self.data = []\n        self.pos = 0\n\n\nclass SkewedReplay:\n    def __init__(self, memory_size, batch_size, criterion):\n        self.replay1 = Replay(memory_size // 2, batch_size // 2)\n        self.replay2 = Replay(memory_size // 2, batch_size // 2)\n        self.criterion = criterion\n\n    def feed(self, experience):\n        if self.criterion(experience):\n            self.replay1.feed(experience)\n        else:\n            self.replay2.feed(experience)\n\n    def feed_batch(self, experience):\n        for exp in experience:\n            self.feed(exp)\n\n    def sample(self):\n        data1 = self.replay1.sample()\n        data2 = self.replay2.sample()\n        if data2 is not None:\n            data = list(map(lambda x: np.concatenate(x, axis=0), zip(data1, data2)))\n        else:\n            data = data1\n        return data\n\n\nclass AsyncReplay(mp.Process):\n    FEED = 0\n    SAMPLE = 1\n    EXIT = 2\n    FEED_BATCH = 3\n\n    def __init__(self, memory_size, batch_size):\n        mp.Process.__init__(self)\n        self.pipe, self.worker_pipe = mp.Pipe()\n        self.memory_size = memory_size\n        self.batch_size = batch_size\n        self.cache_len = 2\n        self.start()\n\n    def run(self):\n        replay = Replay(self.memory_size, self.batch_size)\n        cache = []\n        pending_batch = None\n\n        first = True\n        cur_cache = 0\n\n        def set_up_cache():\n            batch_data = replay.sample()\n            batch_data = [tensor(x) for x in batch_data]\n            for i in range(self.cache_len):\n                cache.append([x.clone() for x in batch_data])\n                for x in cache[i]: x.share_memory_()\n            sample(0)\n            sample(1)\n\n        def sample(cur_cache):\n            batch_data = replay.sample()\n            batch_data = [tensor(x) for x in batch_data]\n            for cache_x, x in zip(cache[cur_cache], batch_data):\n                cache_x.copy_(x)\n\n        while True:\n            op, data = self.worker_pipe.recv()\n            if op == self.FEED:\n                replay.feed(data)\n            elif op == self.FEED_BATCH:\n                if not first:\n                    pending_batch = data\n                else:\n                    for transition in data:\n                        replay.feed(transition)\n            elif op == self.SAMPLE:\n                if first:\n                    set_up_cache()\n                    first = False\n                    self.worker_pipe.send([cur_cache, cache])\n                else:\n                    self.worker_pipe.send([cur_cache, None])\n                cur_cache = (cur_cache + 1) % 2\n                sample(cur_cache)\n                if pending_batch is not None:\n                    for transition in pending_batch:\n                        replay.feed(transition)\n                    pending_batch = None\n            elif op == self.EXIT:\n                self.worker_pipe.close()\n                return\n            else:\n                raise Exception('Unknown command')\n\n    def feed(self, exp):\n        self.pipe.send([self.FEED, exp])\n\n    def feed_batch(self, exps):\n        self.pipe.send([self.FEED_BATCH, exps])\n\n    def sample(self):\n        self.pipe.send([self.SAMPLE, None])\n        cache_id, data = self.pipe.recv()\n        if data is not None:\n            self.cache = data\n        return self.cache[cache_id]\n\n    def close(self):\n        self.pipe.send([self.EXIT, None])\n        self.pipe.close()\n\n\nclass Storage:\n    def __init__(self, size, keys=None):\n        if keys is None:\n            keys = []\n        keys = keys + ['s', 'a', 'r', 'm',\n                       'v', 'q', 'pi', 'log_pi', 'ent',\n                       'adv', 'ret', 'q_a', 'log_pi_a',\n                       'mean']\n        self.keys = keys\n        self.size = size\n        self.reset()\n\n    def add(self, data):\n        for k, v in data.items():\n            if k not in self.keys:\n                self.keys.append(k)\n                setattr(self, k, [])\n            getattr(self, k).append(v)\n\n    def placeholder(self):\n        for k in self.keys:\n            v = getattr(self, k)\n            if len(v) == 0:\n                setattr(self, k, [None] * self.size)\n\n    def reset(self):\n        for key in self.keys:\n            setattr(self, key, [])\n\n    def cat(self, keys):\n        data = [getattr(self, k)[:self.size] for k in keys]\n        return map(lambda x: torch.cat(x, dim=0), data)\n"""
deep_rl/network/__init__.py,0,b'from .network_utils import *\nfrom .network_bodies import *\nfrom .network_heads import *\n'
deep_rl/network/network_bodies.py,2,"b'#######################################################################\n# Copyright (C) 2017 Shangtong Zhang(zhangshangtong.cpp@gmail.com)    #\n# Permission given to modify the code as long as you keep this        #\n# declaration at the top                                              #\n#######################################################################\n\nfrom .network_utils import *\n\n\nclass NatureConvBody(nn.Module):\n    def __init__(self, in_channels=4):\n        super(NatureConvBody, self).__init__()\n        self.feature_dim = 512\n        self.conv1 = layer_init(nn.Conv2d(in_channels, 32, kernel_size=8, stride=4))\n        self.conv2 = layer_init(nn.Conv2d(32, 64, kernel_size=4, stride=2))\n        self.conv3 = layer_init(nn.Conv2d(64, 64, kernel_size=3, stride=1))\n        self.fc4 = layer_init(nn.Linear(7 * 7 * 64, self.feature_dim))\n\n    def forward(self, x):\n        y = F.relu(self.conv1(x))\n        y = F.relu(self.conv2(y))\n        y = F.relu(self.conv3(y))\n        y = y.view(y.size(0), -1)\n        y = F.relu(self.fc4(y))\n        return y\n\n\nclass DDPGConvBody(nn.Module):\n    def __init__(self, in_channels=4):\n        super(DDPGConvBody, self).__init__()\n        self.feature_dim = 39 * 39 * 32\n        self.conv1 = layer_init(nn.Conv2d(in_channels, 32, kernel_size=3, stride=2))\n        self.conv2 = layer_init(nn.Conv2d(32, 32, kernel_size=3))\n\n    def forward(self, x):\n        y = F.elu(self.conv1(x))\n        y = F.elu(self.conv2(y))\n        y = y.view(y.size(0), -1)\n        return y\n\n\nclass FCBody(nn.Module):\n    def __init__(self, state_dim, hidden_units=(64, 64), gate=F.relu):\n        super(FCBody, self).__init__()\n        dims = (state_dim,) + hidden_units\n        self.layers = nn.ModuleList(\n            [layer_init(nn.Linear(dim_in, dim_out)) for dim_in, dim_out in zip(dims[:-1], dims[1:])])\n        self.gate = gate\n        self.feature_dim = dims[-1]\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = self.gate(layer(x))\n        return x\n\n\nclass TwoLayerFCBodyWithAction(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_units=(64, 64), gate=F.relu):\n        super(TwoLayerFCBodyWithAction, self).__init__()\n        hidden_size1, hidden_size2 = hidden_units\n        self.fc1 = layer_init(nn.Linear(state_dim, hidden_size1))\n        self.fc2 = layer_init(nn.Linear(hidden_size1 + action_dim, hidden_size2))\n        self.gate = gate\n        self.feature_dim = hidden_size2\n\n    def forward(self, x, action):\n        x = self.gate(self.fc1(x))\n        phi = self.gate(self.fc2(torch.cat([x, action], dim=1)))\n        return phi\n\n\nclass OneLayerFCBodyWithAction(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_units, gate=F.relu):\n        super(OneLayerFCBodyWithAction, self).__init__()\n        self.fc_s = layer_init(nn.Linear(state_dim, hidden_units))\n        self.fc_a = layer_init(nn.Linear(action_dim, hidden_units))\n        self.gate = gate\n        self.feature_dim = hidden_units * 2\n\n    def forward(self, x, action):\n        phi = self.gate(torch.cat([self.fc_s(x), self.fc_a(action)], dim=1))\n        return phi\n\n\nclass DummyBody(nn.Module):\n    def __init__(self, state_dim):\n        super(DummyBody, self).__init__()\n        self.feature_dim = state_dim\n\n    def forward(self, x):\n        return x\n'"
deep_rl/network/network_heads.py,7,"b""#######################################################################\n# Copyright (C) 2017 Shangtong Zhang(zhangshangtong.cpp@gmail.com)    #\n# Permission given to modify the code as long as you keep this        #\n# declaration at the top                                              #\n#######################################################################\n\nfrom .network_utils import *\nfrom .network_bodies import *\n\n\nclass VanillaNet(nn.Module, BaseNet):\n    def __init__(self, output_dim, body):\n        super(VanillaNet, self).__init__()\n        self.fc_head = layer_init(nn.Linear(body.feature_dim, output_dim))\n        self.body = body\n        self.to(Config.DEVICE)\n\n    def forward(self, x):\n        phi = self.body(tensor(x))\n        y = self.fc_head(phi)\n        return y\n\n\nclass DuelingNet(nn.Module, BaseNet):\n    def __init__(self, action_dim, body):\n        super(DuelingNet, self).__init__()\n        self.fc_value = layer_init(nn.Linear(body.feature_dim, 1))\n        self.fc_advantage = layer_init(nn.Linear(body.feature_dim, action_dim))\n        self.body = body\n        self.to(Config.DEVICE)\n\n    def forward(self, x, to_numpy=False):\n        phi = self.body(tensor(x))\n        value = self.fc_value(phi)\n        advantange = self.fc_advantage(phi)\n        q = value.expand_as(advantange) + (advantange - advantange.mean(1, keepdim=True).expand_as(advantange))\n        return q\n\n\nclass CategoricalNet(nn.Module, BaseNet):\n    def __init__(self, action_dim, num_atoms, body):\n        super(CategoricalNet, self).__init__()\n        self.fc_categorical = layer_init(nn.Linear(body.feature_dim, action_dim * num_atoms))\n        self.action_dim = action_dim\n        self.num_atoms = num_atoms\n        self.body = body\n        self.to(Config.DEVICE)\n\n    def forward(self, x):\n        phi = self.body(tensor(x))\n        pre_prob = self.fc_categorical(phi).view((-1, self.action_dim, self.num_atoms))\n        prob = F.softmax(pre_prob, dim=-1)\n        log_prob = F.log_softmax(pre_prob, dim=-1)\n        return prob, log_prob\n\n\nclass QuantileNet(nn.Module, BaseNet):\n    def __init__(self, action_dim, num_quantiles, body):\n        super(QuantileNet, self).__init__()\n        self.fc_quantiles = layer_init(nn.Linear(body.feature_dim, action_dim * num_quantiles))\n        self.action_dim = action_dim\n        self.num_quantiles = num_quantiles\n        self.body = body\n        self.to(Config.DEVICE)\n\n    def forward(self, x):\n        phi = self.body(tensor(x))\n        quantiles = self.fc_quantiles(phi)\n        quantiles = quantiles.view((-1, self.action_dim, self.num_quantiles))\n        return quantiles\n\n\nclass OptionCriticNet(nn.Module, BaseNet):\n    def __init__(self, body, action_dim, num_options):\n        super(OptionCriticNet, self).__init__()\n        self.fc_q = layer_init(nn.Linear(body.feature_dim, num_options))\n        self.fc_pi = layer_init(nn.Linear(body.feature_dim, num_options * action_dim))\n        self.fc_beta = layer_init(nn.Linear(body.feature_dim, num_options))\n        self.num_options = num_options\n        self.action_dim = action_dim\n        self.body = body\n        self.to(Config.DEVICE)\n\n    def forward(self, x):\n        phi = self.body(tensor(x))\n        q = self.fc_q(phi)\n        beta = F.sigmoid(self.fc_beta(phi))\n        pi = self.fc_pi(phi)\n        pi = pi.view(-1, self.num_options, self.action_dim)\n        log_pi = F.log_softmax(pi, dim=-1)\n        pi = F.softmax(pi, dim=-1)\n        return {'q': q,\n                'beta': beta,\n                'log_pi': log_pi,\n                'pi': pi}\n\n\nclass DeterministicActorCriticNet(nn.Module, BaseNet):\n    def __init__(self,\n                 state_dim,\n                 action_dim,\n                 actor_opt_fn,\n                 critic_opt_fn,\n                 phi_body=None,\n                 actor_body=None,\n                 critic_body=None):\n        super(DeterministicActorCriticNet, self).__init__()\n        if phi_body is None: phi_body = DummyBody(state_dim)\n        if actor_body is None: actor_body = DummyBody(phi_body.feature_dim)\n        if critic_body is None: critic_body = DummyBody(phi_body.feature_dim)\n        self.phi_body = phi_body\n        self.actor_body = actor_body\n        self.critic_body = critic_body\n        self.fc_action = layer_init(nn.Linear(actor_body.feature_dim, action_dim), 1e-3)\n        self.fc_critic = layer_init(nn.Linear(critic_body.feature_dim, 1), 1e-3)\n\n        self.actor_params = list(self.actor_body.parameters()) + list(self.fc_action.parameters())\n        self.critic_params = list(self.critic_body.parameters()) + list(self.fc_critic.parameters())\n        self.phi_params = list(self.phi_body.parameters())\n        \n        self.actor_opt = actor_opt_fn(self.actor_params + self.phi_params)\n        self.critic_opt = critic_opt_fn(self.critic_params + self.phi_params)\n        self.to(Config.DEVICE)\n\n    def forward(self, obs):\n        phi = self.feature(obs)\n        action = self.actor(phi)\n        return action\n\n    def feature(self, obs):\n        obs = tensor(obs)\n        return self.phi_body(obs)\n\n    def actor(self, phi):\n        return torch.tanh(self.fc_action(self.actor_body(phi)))\n\n    def critic(self, phi, a):\n        return self.fc_critic(self.critic_body(phi, a))\n\n\nclass GaussianActorCriticNet(nn.Module, BaseNet):\n    def __init__(self,\n                 state_dim,\n                 action_dim,\n                 phi_body=None,\n                 actor_body=None,\n                 critic_body=None):\n        super(GaussianActorCriticNet, self).__init__()\n        if phi_body is None: phi_body = DummyBody(state_dim)\n        if actor_body is None: actor_body = DummyBody(phi_body.feature_dim)\n        if critic_body is None: critic_body = DummyBody(phi_body.feature_dim)\n        self.phi_body = phi_body\n        self.actor_body = actor_body\n        self.critic_body = critic_body\n        self.fc_action = layer_init(nn.Linear(actor_body.feature_dim, action_dim), 1e-3)\n        self.fc_critic = layer_init(nn.Linear(critic_body.feature_dim, 1), 1e-3)\n        self.std = nn.Parameter(torch.zeros(action_dim))\n        self.phi_params = list(self.phi_body.parameters())\n\n        self.actor_params = list(self.actor_body.parameters()) + list(self.fc_action.parameters()) + self.phi_params\n        self.actor_params.append(self.std)\n        self.critic_params = list(self.critic_body.parameters()) + list(self.fc_critic.parameters()) + self.phi_params\n\n        self.to(Config.DEVICE)\n\n    def forward(self, obs, action=None):\n        obs = tensor(obs)\n        phi = self.phi_body(obs)\n        phi_a = self.actor_body(phi)\n        phi_v = self.critic_body(phi)\n        mean = torch.tanh(self.fc_action(phi_a))\n        v = self.fc_critic(phi_v)\n        dist = torch.distributions.Normal(mean, F.softplus(self.std))\n        if action is None:\n            action = dist.sample()\n        log_prob = dist.log_prob(action).sum(-1).unsqueeze(-1)\n        entropy = dist.entropy().sum(-1).unsqueeze(-1)\n        return {'a': action,\n                'log_pi_a': log_prob,\n                'ent': entropy,\n                'mean': mean,\n                'v': v}\n\n\nclass CategoricalActorCriticNet(nn.Module, BaseNet):\n    def __init__(self,\n                 state_dim,\n                 action_dim,\n                 phi_body=None,\n                 actor_body=None,\n                 critic_body=None):\n        super(CategoricalActorCriticNet, self).__init__()\n        if phi_body is None: phi_body = DummyBody(state_dim)\n        if actor_body is None: actor_body = DummyBody(phi_body.feature_dim)\n        if critic_body is None: critic_body = DummyBody(phi_body.feature_dim)\n        self.phi_body = phi_body\n        self.actor_body = actor_body\n        self.critic_body = critic_body\n        self.fc_action = layer_init(nn.Linear(actor_body.feature_dim, action_dim), 1e-3)\n        self.fc_critic = layer_init(nn.Linear(critic_body.feature_dim, 1), 1e-3)\n\n        self.actor_params = list(self.actor_body.parameters()) + list(self.fc_action.parameters())\n        self.critic_params = list(self.critic_body.parameters()) + list(self.fc_critic.parameters())\n        self.phi_params = list(self.phi_body.parameters())\n        \n        self.to(Config.DEVICE)\n\n    def forward(self, obs, action=None):\n        obs = tensor(obs)\n        phi = self.phi_body(obs)\n        phi_a = self.actor_body(phi)\n        phi_v = self.critic_body(phi)\n        logits = self.fc_action(phi_a)\n        v = self.fc_critic(phi_v)\n        dist = torch.distributions.Categorical(logits=logits)\n        if action is None:\n            action = dist.sample()\n        log_prob = dist.log_prob(action).unsqueeze(-1)\n        entropy = dist.entropy().unsqueeze(-1)\n        return {'a': action,\n                'log_pi_a': log_prob,\n                'ent': entropy,\n                'v': v}\n\n\nclass TD3Net(nn.Module, BaseNet):\n    def __init__(self,\n                 action_dim,\n                 actor_body_fn,\n                 critic_body_fn,\n                 actor_opt_fn,\n                 critic_opt_fn,\n                 ):\n        super(TD3Net, self).__init__()\n        self.actor_body = actor_body_fn()\n        self.critic_body_1 = critic_body_fn()\n        self.critic_body_2 = critic_body_fn()\n\n        self.fc_action = layer_init(nn.Linear(self.actor_body.feature_dim, action_dim), 1e-3)\n        self.fc_critic_1 = layer_init(nn.Linear(self.critic_body_1.feature_dim, 1), 1e-3)\n        self.fc_critic_2 = layer_init(nn.Linear(self.critic_body_2.feature_dim, 1), 1e-3)\n\n        self.actor_params = list(self.actor_body.parameters()) + list(self.fc_action.parameters())\n        self.critic_params = list(self.critic_body_1.parameters()) + list(self.fc_critic_1.parameters()) +\\\n                             list(self.critic_body_2.parameters()) + list(self.fc_critic_2.parameters())\n\n        self.actor_opt = actor_opt_fn(self.actor_params)\n        self.critic_opt = critic_opt_fn(self.critic_params)\n        self.to(Config.DEVICE)\n\n    def forward(self, obs):\n        obs = tensor(obs)\n        return torch.tanh(self.fc_action(self.actor_body(obs)))\n\n    def q(self, obs, a):\n        obs = tensor(obs)\n        a = tensor(a)\n        x = torch.cat([obs, a], dim=1)\n        q_1 = self.fc_critic_1(self.critic_body_1(x))\n        q_2 = self.fc_critic_2(self.critic_body_2(x))\n        return q_1, q_2\n"""
deep_rl/network/network_utils.py,2,"b'#######################################################################\n# Copyright (C) 2017 Shangtong Zhang(zhangshangtong.cpp@gmail.com)    #\n# Permission given to modify the code as long as you keep this        #\n# declaration at the top                                              #\n#######################################################################\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom ..utils import *\n\n\nclass BaseNet:\n    def __init__(self):\n        pass\n\n\ndef layer_init(layer, w_scale=1.0):\n    nn.init.orthogonal_(layer.weight.data)\n    layer.weight.data.mul_(w_scale)\n    nn.init.constant_(layer.bias.data, 0)\n    return layer\n'"
deep_rl/utils/__init__.py,0,b'from .config import *\nfrom .normalizer import *\nfrom .misc import *\nfrom .logger import *\nfrom .plot import Plotter\nfrom .schedule import *\nfrom .torch_utils import *\n'
deep_rl/utils/config.py,1,"b""#######################################################################\n# Copyright (C) 2017 Shangtong Zhang(zhangshangtong.cpp@gmail.com)    #\n# Permission given to modify the code as long as you keep this        #\n# declaration at the top                                              #\n#######################################################################\nfrom .normalizer import *\nimport argparse\nimport torch\n\n\nclass Config:\n    DEVICE = torch.device('cpu')\n\n    def __init__(self):\n        self.parser = argparse.ArgumentParser()\n        self.task_fn = None\n        self.optimizer_fn = None\n        self.actor_optimizer_fn = None\n        self.critic_optimizer_fn = None\n        self.network_fn = None\n        self.actor_network_fn = None\n        self.critic_network_fn = None\n        self.replay_fn = None\n        self.random_process_fn = None\n        self.discount = None\n        self.target_network_update_freq = None\n        self.exploration_steps = None\n        self.log_level = 0\n        self.history_length = None\n        self.double_q = False\n        self.tag = 'vanilla'\n        self.num_workers = 1\n        self.gradient_clip = None\n        self.entropy_weight = 0\n        self.use_gae = False\n        self.gae_tau = 1.0\n        self.target_network_mix = 0.001\n        self.state_normalizer = RescaleNormalizer()\n        self.reward_normalizer = RescaleNormalizer()\n        self.min_memory_size = None\n        self.max_steps = 0\n        self.rollout_length = None\n        self.value_loss_weight = 1.0\n        self.iteration_log_interval = 30\n        self.categorical_v_min = None\n        self.categorical_v_max = None\n        self.categorical_n_atoms = 51\n        self.num_quantiles = None\n        self.optimization_epochs = 4\n        self.mini_batch_size = 64\n        self.termination_regularizer = 0\n        self.sgd_update_frequency = None\n        self.random_action_prob = None\n        self.__eval_env = None\n        self.log_interval = int(1e3)\n        self.save_interval = 0\n        self.eval_interval = 0\n        self.eval_episodes = 10\n        self.async_actor = True\n        self.tasks = False\n\n    @property\n    def eval_env(self):\n        return self.__eval_env\n\n    @eval_env.setter\n    def eval_env(self, env):\n        self.__eval_env = env\n        self.state_dim = env.state_dim\n        self.action_dim = env.action_dim\n        self.task_name = env.name\n\n    def add_argument(self, *args, **kwargs):\n        self.parser.add_argument(*args, **kwargs)\n\n    def merge(self, config_dict=None):\n        if config_dict is None:\n            args = self.parser.parse_args()\n            config_dict = args.__dict__\n        for key in config_dict.keys():\n            setattr(self, key, config_dict[key])\n"""
deep_rl/utils/logger.py,1,"b""#######################################################################\n# Copyright (C) 2017 Shangtong Zhang(zhangshangtong.cpp@gmail.com)    #\n# Permission given to modify the code as long as you keep this        #\n# declaration at the top                                              #\n#######################################################################\n\nfrom tensorboardX import SummaryWriter\nimport os\nimport numpy as np\nimport torch\nimport logging\n\nlogging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s: %(message)s')\nfrom .misc import *\n\n\ndef get_logger(tag='default', log_level=0):\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n    if tag is not None:\n        fh = logging.FileHandler('./log/%s-%s.txt' % (tag, get_time_str()))\n        fh.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s: %(message)s'))\n        fh.setLevel(logging.INFO)\n        logger.addHandler(fh)\n    return Logger(logger, './tf_log/logger-%s-%s' % (tag, get_time_str()), log_level)\n\n\nclass Logger(object):\n    def __init__(self, vanilla_logger, log_dir, log_level=0):\n        self.log_level = log_level\n        self.writer = None\n        if vanilla_logger is not None:\n            self.info = vanilla_logger.info\n            self.debug = vanilla_logger.debug\n            self.warning = vanilla_logger.warning\n        self.all_steps = {}\n        self.log_dir = log_dir\n\n    def lazy_init_writer(self):\n        if self.writer is None:\n            self.writer = SummaryWriter(self.log_dir)\n\n    def to_numpy(self, v):\n        if isinstance(v, torch.Tensor):\n            v = v.cpu().detach().numpy()\n        return v\n\n    def get_step(self, tag):\n        if tag not in self.all_steps:\n            self.all_steps[tag] = 0\n        step = self.all_steps[tag]\n        self.all_steps[tag] += 1\n        return step\n\n    def add_scalar(self, tag, value, step=None, log_level=0):\n        self.lazy_init_writer()\n        if log_level > self.log_level:\n            return\n        value = self.to_numpy(value)\n        if step is None:\n            step = self.get_step(tag)\n        if np.isscalar(value):\n            value = np.asarray([value])\n        self.writer.add_scalar(tag, value, step)\n\n    def add_histogram(self, tag, values, step=None, log_level=0):\n        self.lazy_init_writer()\n        if log_level > self.log_level:\n            return\n        values = self.to_numpy(values)\n        if step is None:\n            step = self.get_step(tag)\n        self.writer.add_histogram(tag, values, step)\n"""
deep_rl/utils/misc.py,0,"b'#######################################################################\n# Copyright (C) 2017 Shangtong Zhang(zhangshangtong.cpp@gmail.com)    #\n# Permission given to modify the code as long as you keep this        #\n# declaration at the top                                              #\n#######################################################################\n\nimport numpy as np\nimport pickle\nimport os\nimport datetime\nimport torch\nimport time\nfrom .torch_utils import *\nfrom pathlib import Path\n\n\ndef run_steps(agent):\n    config = agent.config\n    agent_name = agent.__class__.__name__\n    t0 = time.time()\n    while True:\n        if config.save_interval and not agent.total_steps % config.save_interval:\n            agent.save(\'data/%s-%s-%d\' % (agent_name, config.tag, agent.total_steps))\n        if config.log_interval and not agent.total_steps % config.log_interval:\n            agent.logger.info(\'steps %d, %.2f steps/s\' % (agent.total_steps, config.log_interval / (time.time() - t0)))\n            t0 = time.time()\n        if config.eval_interval and not agent.total_steps % config.eval_interval:\n            agent.eval_episodes()\n        if config.max_steps and agent.total_steps >= config.max_steps:\n            agent.close()\n            break\n        agent.step()\n        agent.switch_task()\n\n\ndef get_time_str():\n    return datetime.datetime.now().strftime(""%y%m%d-%H%M%S"")\n\n\ndef get_default_log_dir(name):\n    return \'./log/%s-%s\' % (name, get_time_str())\n\n\ndef mkdir(path):\n    Path(path).mkdir(parents=True, exist_ok=True)\n\n\ndef close_obj(obj):\n    if hasattr(obj, \'close\'):\n        obj.close()\n\n\ndef random_sample(indices, batch_size):\n    indices = np.asarray(np.random.permutation(indices))\n    batches = indices[:len(indices) // batch_size * batch_size].reshape(-1, batch_size)\n    for batch in batches:\n        yield batch\n    r = len(indices) % batch_size\n    if r:\n        yield indices[-r:]\n\n\ndef generate_tag(params):\n    if \'tag\' in params.keys():\n        return\n    game = params[\'game\']\n    params.setdefault(\'run\', 0)\n    run = params[\'run\']\n    del params[\'game\']\n    del params[\'run\']\n    str = [\'%s_%s\' % (k, v) for k, v in sorted(params.items())]\n    tag = \'%s-%s-run-%d\' % (game, \'-\'.join(str), run)\n    params[\'tag\'] = tag\n    params[\'game\'] = game\n    params[\'run\'] = run\n\n\ndef translate(pattern):\n    groups = pattern.split(\'.\')\n    pattern = (\'\\.\').join(groups)\n    return pattern\n\n\ndef split(a, n):\n    k, m = divmod(len(a), n)\n    return (a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))\n'"
deep_rl/utils/normalizer.py,1,"b""#######################################################################\n# Copyright (C) 2017 Shangtong Zhang(zhangshangtong.cpp@gmail.com)    #\n# Permission given to modify the code as long as you keep this        #\n# declaration at the top                                              #\n#######################################################################\nimport numpy as np\nimport torch\nfrom baselines.common.running_mean_std import RunningMeanStd\n\n\nclass BaseNormalizer:\n    def __init__(self, read_only=False):\n        self.read_only = read_only\n\n    def set_read_only(self):\n        self.read_only = True\n\n    def unset_read_only(self):\n        self.read_only = False\n\n    def state_dict(self):\n        return None\n\n    def load_state_dict(self, _):\n        return\n\n\nclass MeanStdNormalizer(BaseNormalizer):\n    def __init__(self, read_only=False, clip=10.0, epsilon=1e-8):\n        BaseNormalizer.__init__(self, read_only)\n        self.read_only = read_only\n        self.rms = None\n        self.clip = clip\n        self.epsilon = epsilon\n\n    def __call__(self, x):\n        x = np.asarray(x)\n        if self.rms is None:\n            self.rms = RunningMeanStd(shape=(1,) + x.shape[1:])\n        if not self.read_only:\n            self.rms.update(x)\n        return np.clip((x - self.rms.mean) / np.sqrt(self.rms.var + self.epsilon),\n                       -self.clip, self.clip)\n\n    def state_dict(self):\n        return {'mean': self.rms.mean,\n                'var': self.rms.var}\n\n    def load_state_dict(self, saved):\n        self.rms.mean = saved['mean']\n        self.rms.var = saved['var']\n\nclass RescaleNormalizer(BaseNormalizer):\n    def __init__(self, coef=1.0):\n        BaseNormalizer.__init__(self)\n        self.coef = coef\n\n    def __call__(self, x):\n        if not isinstance(x, torch.Tensor):\n            x = np.asarray(x)\n        return self.coef * x\n\n\nclass ImageNormalizer(RescaleNormalizer):\n    def __init__(self):\n        RescaleNormalizer.__init__(self, 1.0 / 255)\n\n\nclass SignNormalizer(BaseNormalizer):\n    def __call__(self, x):\n        return np.sign(x)\n"""
deep_rl/utils/plot.py,0,"b""#######################################################################\n# Copyright (C) 2017 Shangtong Zhang(zhangshangtong.cpp@gmail.com)    #\n# Permission given to modify the code as long as you keep this        #\n# declaration at the top                                              #\n#######################################################################\n\nimport numpy as np\nimport os\nimport re\n\n\nclass Plotter:\n    COLORS = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black', 'purple', 'pink',\n              'brown', 'orange', 'teal', 'coral', 'lightblue', 'lime', 'lavender', 'turquoise',\n              'darkgreen', 'tan', 'salmon', 'gold', 'lightpurple', 'darkred', 'darkblue']\n\n    RETURN_TRAIN = 'episodic_return_train'\n    RETURN_TEST = 'episodic_return_test'\n\n    def __init__(self):\n        pass\n\n    def _rolling_window(self, a, window):\n        shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n        strides = a.strides + (a.strides[-1],)\n        return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n\n    def _window_func(self, x, y, window, func):\n        yw = self._rolling_window(y, window)\n        yw_func = func(yw, axis=-1)\n        return x[window - 1:], yw_func\n\n    def load_results(self, dirs, **kwargs):\n        kwargs.setdefault('tag', self.RETURN_TRAIN)\n        kwargs.setdefault('right_align', False)\n        kwargs.setdefault('window', 0)\n        kwargs.setdefault('top_k', 0)\n        kwargs.setdefault('top_k_measure', None)\n        kwargs.setdefault('interpolation', 100)\n        xy_list = self.load_log_dirs(dirs, **kwargs)\n\n        if kwargs['top_k']:\n            perf = [kwargs['top_k_measure'](y) for _, y in xy_list]\n            top_k_runs = np.argsort(perf)[-kwargs['top_k']:]\n            new_xy_list = []\n            for r, (x, y) in enumerate(xy_list):\n                if r in top_k_runs:\n                    new_xy_list.append((x, y))\n            xy_list = new_xy_list\n\n        if kwargs['interpolation']:\n            x_right = float('inf')\n            for x, y in xy_list:\n                x_right = min(x_right, x[-1])\n            x = np.arange(0, x_right, kwargs['interpolation'])\n            y = []\n            for x_, y_ in xy_list:\n                y.append(np.interp(x, x_, y_))\n            y = np.asarray(y)\n        else:\n            x = xy_list[0][0]\n            y = [y for _, y in xy_list]\n            x = np.asarray(x)\n            y = np.asarray(y)\n\n        return x, y\n\n    def filter_log_dirs(self, pattern, negative_pattern=' ', root='./log', **kwargs):\n        dirs = [item[0] for item in os.walk(root)]\n        leaf_dirs = []\n        for i in range(len(dirs)):\n            if i + 1 < len(dirs) and dirs[i + 1].startswith(dirs[i]):\n                continue\n            leaf_dirs.append(dirs[i])\n        names = []\n        p = re.compile(pattern)\n        np = re.compile(negative_pattern)\n        for dir in leaf_dirs:\n            if p.match(dir) and not np.match(dir):\n                names.append(dir)\n                print(dir)\n        print('')\n        return sorted(names)\n\n    def load_log_dirs(self, dirs, **kwargs):\n        kwargs.setdefault('right_align', False)\n        kwargs.setdefault('window', 0)\n        xy_list = []\n        from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n        for dir in dirs:\n            event_acc = EventAccumulator(dir)\n            event_acc.Reload()\n            _, x, y = zip(*event_acc.Scalars(kwargs['tag']))\n            xy_list.append([x, y])\n        if kwargs['right_align']:\n            x_max = float('inf')\n            for x, y in xy_list:\n                x_max = min(x_max, len(y))\n            xy_list = [[x[:x_max], y[:x_max]] for x, y in xy_list]\n        if kwargs['window']:\n            xy_list = [self._window_func(np.asarray(x), np.asarray(y), kwargs['window'], np.mean) for x, y in xy_list]\n        return xy_list\n\n    def plot_mean(self, data, x=None, **kwargs):\n        import matplotlib.pyplot as plt\n        if x is None:\n            x = np.arange(data.shape[1])\n        if kwargs['error'] == 'se':\n            e_x = np.std(data, axis=0) / np.sqrt(data.shape[0])\n        elif kwargs['error'] == 'std':\n            e_x = np.std(data, axis=0)\n        else:\n            raise NotImplementedError\n        m_x = np.mean(data, axis=0)\n        del kwargs['error']\n        plt.plot(x, m_x, **kwargs)\n        del kwargs['label']\n        plt.fill_between(x, m_x + e_x, m_x - e_x, alpha=0.3, **kwargs)\n\n    def plot_median_std(self, data, x=None, **kwargs):\n        import matplotlib.pyplot as plt\n        if x is None:\n            x = np.arange(data.shape[1])\n        e_x = np.std(data, axis=0)\n        m_x = np.median(data, axis=0)\n        plt.plot(x, m_x, **kwargs)\n        del kwargs['label']\n        plt.fill_between(x, m_x + e_x, m_x - e_x, alpha=0.3, **kwargs)\n\n    def plot_games(self, games, **kwargs):\n        kwargs.setdefault('agg', 'mean')\n        import matplotlib.pyplot as plt\n        l = len(games)\n        plt.figure(figsize=(l * 5, 5))\n        for i, game in enumerate(games):\n            plt.subplot(1, l, i + 1)\n            for j, p in enumerate(kwargs['patterns']):\n                label = kwargs['labels'][j]\n                color = self.COLORS[j]\n                log_dirs = self.filter_log_dirs(pattern='.*%s.*%s' % (game, p), **kwargs)\n                x, y = self.load_results(log_dirs, **kwargs)\n                if kwargs['downsample']:\n                    indices = np.linspace(0, len(x) - 1, kwargs['downsample']).astype(np.int)\n                    x = x[indices]\n                    y = y[:, indices]\n                if kwargs['agg'] == 'mean':\n                    self.plot_mean(y, x, label=label, color=color, error='se')\n                elif kwargs['agg'] == 'mean_std':\n                    self.plot_mean(y, x, label=label, color=color, error='std')\n                elif kwargs['agg'] == 'median':\n                    self.plot_median_std(y, x, label=label, color=color)\n                else:\n                    for k in range(y.shape[0]):\n                        plt.plot(x, y[i], label=label, color=color)\n                        label = None\n            plt.xlabel('steps')\n            if not i:\n                plt.ylabel(kwargs['tag'])\n            plt.title(game)\n            plt.legend()\n\n    def select_best_parameters(self, patterns, **kwargs):\n        scores = []\n        for pattern in patterns:\n            log_dirs = self.filter_log_dirs(pattern, **kwargs)\n            xy_list = self.load_log_dirs(log_dirs, **kwargs)\n            y = np.asarray([xy[1] for xy in xy_list])\n            scores.append(kwargs['score'](y))\n        indices = np.argsort(-np.asarray(scores))\n        return indices\n"""
deep_rl/utils/schedule.py,0,"b'#######################################################################\n# Copyright (C) 2017 Shangtong Zhang(zhangshangtong.cpp@gmail.com)    #\n# Permission given to modify the code as long as you keep this        #\n# declaration at the top                                              #\n#######################################################################\n\nclass ConstantSchedule:\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, steps=1):\n        return self.val\n\n\nclass LinearSchedule:\n    def __init__(self, start, end=None, steps=None):\n        if end is None:\n            end = start\n            steps = 1\n        self.inc = (end - start) / float(steps)\n        self.current = start\n        self.end = end\n        if end > start:\n            self.bound = min\n        else:\n            self.bound = max\n\n    def __call__(self, steps=1):\n        val = self.current\n        self.current = self.bound(self.current + self.inc * steps, self.end)\n        return val\n'"
deep_rl/utils/torch_utils.py,18,"b""#######################################################################\n# Copyright (C) 2017 Shangtong Zhang(zhangshangtong.cpp@gmail.com)    #\n# Permission given to modify the code as long as you keep this        #\n# declaration at the top                                              #\n#######################################################################\n\nfrom .config import *\nimport torch\nimport os\n\n\ndef select_device(gpu_id):\n    # if torch.cuda.is_available() and gpu_id >= 0:\n    if gpu_id >= 0:\n        Config.DEVICE = torch.device('cuda:%d' % (gpu_id))\n    else:\n        Config.DEVICE = torch.device('cpu')\n\n\ndef tensor(x):\n    if isinstance(x, torch.Tensor):\n        return x\n    x = np.asarray(x, dtype=np.float)\n    x = torch.tensor(x, device=Config.DEVICE, dtype=torch.float32)\n    return x\n\n\ndef range_tensor(end):\n    return torch.arange(end).long().to(Config.DEVICE)\n\n\ndef to_np(t):\n    return t.cpu().detach().numpy()\n\n\ndef random_seed(seed=None):\n    np.random.seed(seed)\n    torch.manual_seed(np.random.randint(int(1e6)))\n\n\ndef set_one_thread():\n    os.environ['OMP_NUM_THREADS'] = '1'\n    os.environ['MKL_NUM_THREADS'] = '1'\n    torch.set_num_threads(1)\n\n\ndef huber(x, k=1.0):\n    return torch.where(x.abs() < k, 0.5 * x.pow(2), k * (x.abs() - 0.5 * k))\n\n\ndef epsilon_greedy(epsilon, x):\n    if len(x.shape) == 1:\n        return np.random.randint(len(x)) if np.random.rand() < epsilon else np.argmax(x)\n    elif len(x.shape) == 2:\n        random_actions = np.random.randint(x.shape[1], size=x.shape[0])\n        greedy_actions = np.argmax(x, axis=-1)\n        dice = np.random.rand(x.shape[0])\n        return np.where(dice < epsilon, random_actions, greedy_actions)\n\n\ndef sync_grad(target_network, src_network):\n    for param, src_param in zip(target_network.parameters(), src_network.parameters()):\n        if src_param.grad is not None:\n            param._grad = src_param.grad.clone()\n\n\n# adapted from https://github.com/pytorch/pytorch/issues/12160\ndef batch_diagonal(input):\n    # idea from here: https://discuss.pytorch.org/t/batch-of-diagonal-matrix/13560\n    # batches a stack of vectors (batch x N) -> a stack of diagonal matrices (batch x N x N)\n    # works in  2D -> 3D, should also work in higher dimensions\n    # make a zero matrix, which duplicates the last dim of input\n    dims = input.size()\n    dims = dims + dims[-1:]\n    output = torch.zeros(dims, device=input.device)\n    # stride across the first dimensions, add one to get the diagonal of the last dimension\n    strides = [output.stride(i) for i in range(input.dim() - 1)]\n    strides.append(output.size(-1) + 1)\n    # stride and copy the input to the diagonal\n    output.as_strided(input.size(), strides).copy_(input)\n    return output\n\n\ndef batch_trace(input):\n    i = range_tensor(input.size(-1))\n    t = input[:, i, i].sum(-1).unsqueeze(-1).unsqueeze(-1)\n    return t\n\n\nclass DiagonalNormal:\n    def __init__(self, mean, std):\n        self.dist = torch.distributions.Normal(mean, std)\n        self.sample = self.dist.sample\n\n    def log_prob(self, action):\n        return self.dist.log_prob(action).sum(-1).unsqueeze(-1)\n\n    def entropy(self):\n        return self.dist.entropy().sum(-1).unsqueeze(-1)\n\n    def cdf(self, action):\n        return self.dist.cdf(action).prod(-1).unsqueeze(-1)\n\n\nclass BatchCategorical:\n    def __init__(self, logits):\n        self.pre_shape = logits.size()[:-1]\n        logits = logits.view(-1, logits.size(-1))\n        self.dist = torch.distributions.Categorical(logits=logits)\n\n    def log_prob(self, action):\n        log_pi = self.dist.log_prob(action.view(-1))\n        log_pi = log_pi.view(action.size()[:-1] + (-1,))\n        return log_pi\n\n    def entropy(self):\n        ent = self.dist.entropy()\n        ent = ent.view(self.pre_shape + (-1,))\n        return ent\n\n    def sample(self, sample_shape=torch.Size([])):\n        ret = self.dist.sample(sample_shape)\n        ret = ret.view(sample_shape + self.pre_shape + (-1,))\n        return ret\n\n\nclass Grad:\n    def __init__(self, network=None, grads=None):\n        if grads is not None:\n            self.grads = grads\n        else:\n            self.grads = []\n            for param in network.parameters():\n                self.grads.append(torch.zeros(param.data.size(), device=Config.DEVICE))\n\n    def add(self, op):\n        if isinstance(op, Grad):\n            for grad, op_grad in zip(self.grads, op.grads):\n                grad.add_(op_grad)\n        elif isinstance(op, torch.nn.Module):\n            for grad, param in zip(self.grads, op.parameters()):\n                if param.grad is not None:\n                    grad.add_(param.grad)\n        return self\n\n    def mul(self, coef):\n        for grad in self.grads:\n            grad.mul_(coef)\n        return self\n\n    def assign(self, network):\n        for grad, param in zip(self.grads, network.parameters()):\n            param._grad = grad.clone()\n\n    def zero(self):\n        for grad in self.grads:\n            grad.zero_()\n\n    def clone(self):\n        return Grad(grads=[grad.clone() for grad in self.grads])\n\n\nclass Grads:\n    def __init__(self, network=None, n=0, grads=None):\n        if grads is not None:\n            self.grads = grads\n        else:\n            self.grads = [Grad(network) for _ in range(n)]\n\n    def clone(self):\n        return Grads(grads=[grad.clone() for grad in self.grads])\n\n    def mul(self, op):\n        if np.isscalar(op):\n            for grad in self.grads:\n                grad.mul(op)\n        elif isinstance(op, torch.Tensor):\n            op = op.view(-1)\n            for i, grad in enumerate(self.grads):\n                grad.mul(op[i])\n        else:\n            raise NotImplementedError\n        return self\n\n    def add(self, op):\n        if np.isscalar(op):\n            for grad in self.grads:\n                grad.mul(op)\n        elif isinstance(op, Grads):\n            for grad, op_grad in zip(self.grads, op.grads):\n                grad.add(op_grad)\n        elif isinstance(op, torch.Tensor):\n            op = op.view(-1)\n            for i, grad in enumerate(self.grads):\n                grad.mul(op[i])\n        else:\n            raise NotImplementedError\n        return self\n\n    def mean(self):\n        grad = self.grads[0].clone()\n        grad.zero()\n        for g in self.grads:\n            grad.add(g)\n        grad.mul(1 / len(self.grads))\n        return grad\n\n\ndef escape_float(x):\n    return ('%s' % x).replace('.', '\\.')"""
