file_path,api_count,code
setup.py,0,"b'# encoding: utf-8\nfrom setuptools import setup, find_packages\n\nwith open(""README.md"", ""r"", encoding=""utf-8"") as fh:\n    long_description = fh.read()\n\nsetup(name=\'service_streamer\',\n      version=""0.1.2"",\n      description=\'Boosting your web service of deep learning applications\',\n      long_description=long_description,\n      long_description_content_type=""text/markdown"",\n      classifiers=[\n          \'Development Status :: 3 - Alpha\',\n          \'Programming Language :: Python :: 3.5\',\n          \'Programming Language :: Python :: 3.6\',\n          \'Programming Language :: Python :: 3.7\',\n          \'Programming Language :: Python :: Implementation :: CPython\',\n          \'Operating System :: OS Independent\',\n      ],      \n      keywords=\'service_streamer\',\n      url=\'https://github.com/shannonAI\',\n      packages=find_packages(exclude=[""example""]),\n      install_requires=[\n          \'redis\',\n          \'tqdm\',\n      ],\n      include_package_data=True,\n      python_requires=\'>=3.5\',\n      zip_safe=False\n)\n'"
example/bert_model.py,6,"b'# coding=utf-8\n# Created by Meteorix at 2019/7/30\nimport logging\nimport torch\nfrom typing import List\nfrom pytorch_transformers import *\nfrom service_streamer import ManagedModel\n\nlogging.basicConfig(level=logging.ERROR)\n\nSEED = 0\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\n\n\nclass TextInfillingModel(object):\n    def __init__(self, max_sent_len=16, model_path=""bert-base-uncased""):\n        self.model_path = model_path\n        self.tokenizer = BertTokenizer.from_pretrained(self.model_path)\n        self.bert = BertForMaskedLM.from_pretrained(self.model_path)\n        self.bert.eval()\n        self.bert.to(""cuda"")\n        self.max_sent_len = max_sent_len\n\n    def predict(self, batch: List[str]) -> List[str]:\n        """"""predict masked word""""""\n        batch_inputs = []\n        masked_indexes = []\n\n        for text in batch:\n            tokenized_text = self.tokenizer.tokenize(text)\n            if len(tokenized_text) > self.max_sent_len - 2:\n                tokenized_text = tokenized_text[: self.max_sent_len - 2]\n            tokenized_text = [\'[CLS]\'] + tokenized_text + [\'[SEP]\']\n            tokenized_text += [\'[PAD]\'] * (self.max_sent_len - len(tokenized_text))\n            indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n            batch_inputs.append(indexed_tokens)\n            masked_indexes.append(tokenized_text.index(\'[MASK]\'))\n        tokens_tensor = torch.tensor(batch_inputs).to(""cuda"")\n\n        with torch.no_grad():\n            # prediction_scores: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.vocab_size)``\n            prediction_scores = self.bert(tokens_tensor)[0]\n\n        batch_outputs = []\n        for i in range(len(batch_inputs)):\n            predicted_index = torch.argmax(prediction_scores[i, masked_indexes[i]]).item()\n            predicted_token = self.tokenizer.convert_ids_to_tokens(predicted_index)\n            batch_outputs.append(predicted_token)\n\n        return batch_outputs\n\n\nclass ManagedBertModel(ManagedModel):\n\n    def init_model(self):\n        self.model = TextInfillingModel()\n\n    def predict(self, batch):\n        return self.model.predict(batch)\n\n\nif __name__ == ""__main__"":\n    batch = [""twinkle twinkle [MASK] star."",\n             ""Happy birthday to [MASK]."",\n             \'the answer to life, the [MASK], and everything.\']\n    model = TextInfillingModel()\n    outputs = model.predict(batch)\n    print(outputs)\n'"
example/flask_example.py,0,"b'# coding=utf-8\n# Created by Meteorix at 2019/7/30\n\nfrom bert_model import TextInfillingModel as Model\nfrom flask import Flask, request, jsonify\n\nfrom service_streamer import ThreadedStreamer\n\napp = Flask(__name__)\nmodel = None\nstreamer = None\n\n\n@app.route(""/naive"", methods=[""POST""])\ndef naive_predict():\n    inputs = request.form.getlist(""s"")\n    outputs = model.predict(inputs)\n    return jsonify(outputs)\n\n\n@app.route(""/stream"", methods=[""POST""])\ndef stream_predict():\n    inputs = request.form.getlist(""s"")\n    outputs = streamer.predict(inputs)\n    return jsonify(outputs)\n\n\nif __name__ == ""__main__"":\n    model = Model()\n    # start child thread as worker\n    streamer = ThreadedStreamer(model.predict, batch_size=64, max_latency=0.1)\n\n    # spawn child process as worker\n    # streamer = Streamer(model.predict, batch_size=64, max_latency=0.1)\n\n    app.run(port=5005, debug=False)\n'"
example/flask_multigpu_example.py,0,"b'# coding=utf-8\n# Created by Meteorix at 2019/7/30\nfrom flask import Flask, request, jsonify\nfrom gevent import monkey; monkey.patch_all()\nfrom gevent.pywsgi import WSGIServer\n\nfrom bert_model import ManagedBertModel, TextInfillingModel as Model\nfrom service_streamer import Streamer\n\napp = Flask(__name__)\nmodel = None\nstreamer = None\n\n\n@app.route(""/naive"", methods=[""POST""])\ndef naive_predict():\n    inputs = request.form.getlist(""s"")\n    outputs = model.predict(inputs)\n    return jsonify(outputs)\n\n\n@app.route(""/stream"", methods=[""POST""])\ndef stream_predict():\n    inputs = request.form.getlist(""s"")\n    outputs = streamer.predict(inputs)\n    return jsonify(outputs)\n\n\nif __name__ == ""__main__"":\n    streamer = Streamer(ManagedBertModel, batch_size=64, max_latency=0.1, worker_num=4, cuda_devices=(0, 1, 2, 3))\n    model = Model()\n    WSGIServer((""0.0.0.0"", 5005), app).serve_forever()\n'"
example/future_example.py,0,"b'# coding=utf-8\n# Created by Meteorix at 2019/7/22\n\nimport time\nfrom tqdm import tqdm\nfrom service_streamer import ThreadedStreamer, Streamer, RedisStreamer\nfrom example.bert_model import TextInfillingModel, ManagedBertModel\n\n\ndef main():\n    batch_size = 64\n    model = TextInfillingModel()\n    # streamer = ThreadedStreamer(model.predict, batch_size=batch_size, max_latency=0.1)\n    streamer = Streamer(ManagedBertModel, batch_size=batch_size, max_latency=0.1, worker_num=4, cuda_devices=(0, 1, 2, 3))\n    streamer._wait_for_worker_ready()\n    # streamer = RedisStreamer()\n\n    text = ""Happy birthday to [MASK]""\n    num_epochs = 100\n    total_steps = batch_size * num_epochs\n\n    t_start = time.time()\n    for i in tqdm(range(num_epochs)):\n        output = model.predict([text])\n    t_end = time.time()\n    print(\'model prediction time\', t_end - t_start)\n\n    t_start = time.time()\n    for i in tqdm(range(num_epochs)):\n        output = model.predict([text] * batch_size)\n    t_end = time.time()\n    print(\'[batched]sentences per second\', total_steps / (t_end - t_start))\n\n    t_start = time.time()\n    xs = []\n    for i in range(total_steps):\n        future = streamer.submit([text])\n        xs.append(future)\n\n    for future in tqdm(xs):  # \xe5\x85\x88\xe6\x8b\xbf\xe5\x88\xb0\xe6\x89\x80\xe6\x9c\x89future\xe5\xaf\xb9\xe8\xb1\xa1\xef\xbc\x8c\xe5\x86\x8d\xe7\xad\x89\xe5\xbe\x85\xe5\xbc\x82\xe6\xad\xa5\xe8\xbf\x94\xe5\x9b\x9e\n        output = future.result(timeout=20)\n    t_end = time.time()\n    print(\'[streamed]sentences per second\', total_steps / (t_end - t_start))\n\n    streamer.destroy_workers()\n    time.sleep(10)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
example/redis_streamer_gunicorn.py,0,"b'# coding=utf-8\n# Created by Meteorix at 2019/7/31\nfrom gevent import monkey; monkey.patch_all()\n\n\ndef post_fork(server, worker):\n    from service_streamer import RedisStreamer\n    import flask_example\n    flask_example.streamer = RedisStreamer()\n\n\nbind = \'0.0.0.0:5005\'\nworkers = 4\nworker_class = \'gunicorn.workers.ggevent.GeventWorker\'\nproc_name = ""redis_streamer""\n'"
example/redis_worker_example.py,0,"b'# coding=utf-8\n# Created by Meteorix at 2019/7/30\nfrom service_streamer import run_redis_workers_forever\nfrom bert_model import ManagedBertModel\n\n\nif __name__ == ""__main__"":\n    run_redis_workers_forever(ManagedBertModel, 64, 0.1, worker_num=4, cuda_devices=(0, 1, 2, 3))\n'"
example_vision/app.py,0,"b'# coding=utf-8\n# Created by Meteorix at 2019/8/9\nfrom flask import Flask, jsonify, request\nfrom model import get_prediction, batch_prediction\nfrom service_streamer import ThreadedStreamer\n\n\napp = Flask(__name__)\nstreamer = ThreadedStreamer(batch_prediction, batch_size=64)\n\n\n@app.route(\'/predict\', methods=[\'POST\'])\ndef predict():\n    if request.method == \'POST\':\n        file = request.files[\'file\']\n        img_bytes = file.read()\n        class_id, class_name = get_prediction(img_bytes)\n        return jsonify({\'class_id\': class_id, \'class_name\': class_name})\n\n\n@app.route(\'/stream_predict\', methods=[\'POST\'])\ndef stream_predict():\n    if request.method == \'POST\':\n        file = request.files[\'file\']\n        img_bytes = file.read()\n        class_id, class_name = streamer.predict([img_bytes])[0]\n        return jsonify({\'class_id\': class_id, \'class_name\': class_name})\n\n\nif __name__ == ""__main__"":\n    app.run(host=""0.0.0.0"", port=5005)\n'"
example_vision/model.py,1,"b'# coding=utf-8\n# Created by Meteorix at 2019/8/9\nimport io\nimport json\nimport torch\nfrom torchvision import models\nfrom torchvision import transforms\nfrom PIL import Image\n\n\nimagenet_class_index = json.load(open(\'imagenet_class_index.json\'))\ndevice = ""cuda""\n# Make sure to pass `pretrained` as `True` to use the pretrained weights:\nmodel = models.densenet121(pretrained=True)\nmodel.to(device)\n# Since we are using our model only for inference, switch to `eval` mode:\nmodel.eval()\n\n\ndef transform_image(image_bytes):\n    my_transforms = transforms.Compose([transforms.Resize(255),\n                                        transforms.CenterCrop(224),\n                                        transforms.ToTensor(),\n                                        transforms.Normalize(\n                                            [0.485, 0.456, 0.406],\n                                            [0.229, 0.224, 0.225])])\n    image = Image.open(io.BytesIO(image_bytes))\n    return my_transforms(image).unsqueeze(0)\n\n\ndef get_prediction(image_bytes):\n    tensor = transform_image(image_bytes=image_bytes).to(device)\n    outputs = model.forward(tensor)\n    _, y_hat = outputs.max(1)\n    predicted_idx = str(y_hat.item())\n    return imagenet_class_index[predicted_idx]\n\n\ndef batch_prediction(image_bytes_batch):\n    image_tensors = [transform_image(image_bytes=image_bytes) for image_bytes in image_bytes_batch]\n    tensor = torch.cat(image_tensors).to(device)\n    outputs = model.forward(tensor)\n    _, y_hat = outputs.max(1)\n    predicted_ids = y_hat.tolist()\n    return [imagenet_class_index[str(i)] for i in predicted_ids]\n\n\nif __name__ == ""__main__"":\n    with open(r""cat.jpg"", \'rb\') as f:\n        image_bytes = f.read()\n\n    result = get_prediction(image_bytes)\n    print(result)\n    batch_result = batch_prediction([image_bytes] * 64)\n    assert batch_result == [result] * 64\n'"
service_streamer/__init__.py,0,"b'# coding=utf-8\n# Created by Meteorix at 2019/7/22\n\nfrom .service_streamer import ThreadedStreamer, Streamer, RedisStreamer, RedisWorker, run_redis_workers_forever\nfrom .managed_model import ManagedModel\n'"
service_streamer/managed_model.py,0,"b'# coding=utf-8\n# Created by Meteorix at 2019/7/22\nfrom typing import List\nimport os\n\n\nclass ManagedModel(object):\n    def __init__(self, gpu_id=None):\n        self.model = None\n        self.gpu_id = gpu_id\n        self.set_gpu_id(self.gpu_id)\n\n    @staticmethod\n    def set_gpu_id(gpu_id=None):\n        if gpu_id is not None:\n            os.environ[""CUDA_VISIBLE_DEVICES""] = str(gpu_id)\n\n    def init_model(self, *args, **kwargs):\n        raise NotImplementedError\n\n    def predict(self, batch: List) -> List:\n        raise NotImplementedError\n'"
service_streamer/service_streamer.py,0,"b'# coding=utf-8\n# Created by Meteorix at 2019/7/13\nimport logging\nimport multiprocessing\nimport os\nimport threading\nimport time\nimport uuid\nimport weakref\nimport pickle\nfrom queue import Queue, Empty\nfrom typing import List\n\nfrom redis import Redis\n\nfrom .managed_model import ManagedModel\n\nTIMEOUT = 1\nTIME_SLEEP = 0.001\nWORKER_TIMEOUT = 20\nlogger = logging.getLogger(__name__)\nlogger.setLevel(""INFO"")\n\n\nclass Future(object):\n    def __init__(self, task_id, task_size, future_cache_ref):\n        self._id = task_id\n        self._size = task_size\n        self._future_cache_ref = future_cache_ref\n        self._outputs = []\n        self._finish_event = threading.Event()\n\n    def result(self, timeout=None):\n        if self._size == 0:\n            self._finish_event.set()\n            return []\n        finished = self._finish_event.wait(timeout)\n\n        if not finished:\n            raise TimeoutError(""Task: %d Timeout"" % self._id)\n\n        # remove from future_cache\n        future_cache = self._future_cache_ref()\n        if future_cache is not None:\n            del future_cache[self._id]\n\n        # [(request_id, output), ...] sorted by request_id\n        self._outputs.sort(key=lambda i: i[0])\n        # restore batch result from outputs\n        batch_result = [i[1] for i in self._outputs]\n\n        return batch_result\n\n    def done(self):\n        if self._finish_event.is_set():\n            return True\n\n    def _append_result(self, it_id, it_output):\n        self._outputs.append((it_id, it_output))\n        if len(self._outputs) >= self._size:\n            self._finish_event.set()\n\n\nclass _FutureCache(dict):\n    ""Dict for weakref only""\n    pass\n\n\nclass _BaseStreamer(object):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        self._client_id = str(uuid.uuid4())\n        self._task_id = 0\n        self._future_cache = _FutureCache()  # {task_id: future}\n\n        self.back_thread = threading.Thread(target=self._loop_collect_result, name=""thread_collect_result"")\n        self.back_thread.daemon = True\n        self.lock = threading.Lock()\n\n    def _delay_setup(self):\n        self.back_thread.start()\n\n    def _send_request(self, task_id, request_id, model_input):\n        raise NotImplementedError\n\n    def _recv_response(self, timeout=TIMEOUT):\n        raise NotImplementedError\n\n    def _input(self, batch: List) -> int:\n        """"""\n        input a batch, distribute each item to mq, return task_id\n        """"""\n        # task id in one client\n        self.lock.acquire()\n        task_id = self._task_id\n        self._task_id += 1\n        self.lock.release()\n        # request id in one task\n        request_id = 0\n\n        future = Future(task_id, len(batch), weakref.ref(self._future_cache))\n        self._future_cache[task_id] = future\n\n        for model_input in batch:\n            self._send_request(task_id, request_id, model_input)\n            request_id += 1\n\n        return task_id\n\n    def _loop_collect_result(self):\n        logger.info(""start _loop_collect_result"")\n        while True:\n            message = self._recv_response(timeout=TIMEOUT)\n            if message:\n                (task_id, request_id, item) = message\n                future = self._future_cache[task_id]\n                future._append_result(request_id, item)\n            else:\n                # todo\n                time.sleep(TIME_SLEEP)\n\n    def _output(self, task_id: int) -> List:\n        future = self._future_cache[task_id]\n        batch_result = future.result(WORKER_TIMEOUT)\n        return batch_result\n\n    def submit(self, batch):\n        task_id = self._input(batch)\n        future = self._future_cache[task_id]\n        return future\n\n    def predict(self, batch):\n        task_id = self._input(batch)\n        ret = self._output(task_id)\n        assert len(batch) == len(ret), ""input batch size {} and output batch size {} must be equal."".format(len(batch), len(ret))\n        return ret\n\n    def destroy_workers(self):\n        raise NotImplementedError\n\n\nclass _BaseStreamWorker(object):\n    def __init__(self, predict_function, batch_size, max_latency, *args, **kwargs):\n        super().__init__()\n        assert callable(predict_function)\n        self._pid = os.getpid()\n        self._predict = predict_function\n        self._batch_size = batch_size\n        self._max_latency = max_latency\n        self._destroy_event = kwargs.get(""destroy_event"", None)\n\n    def run_forever(self, *args, **kwargs):\n        self._pid = os.getpid()  # overwrite the pid\n        logger.info(""[gpu worker %d] %s start working"" % (self._pid, self))\n\n        while True:\n            handled = self._run_once()\n            if self._destroy_event and self._destroy_event.is_set():\n                break\n            if not handled:\n                # sleep if no data handled last time\n                time.sleep(TIME_SLEEP)\n        logger.info(""[gpu worker %d] %s shutdown"" % (self._pid, self))\n\n    def model_predict(self, batch_input):\n        batch_result = self._predict(batch_input)\n        assert len(batch_input) == len(batch_result), ""input batch size {} and output batch size {} must be equal."".format(len(batch_input), len(batch_result))\n        return batch_result\n\n    def _run_once(self):\n        batch = []\n        start_time = time.time()\n        for i in range(self._batch_size):\n            try:\n                item = self._recv_request(timeout=self._max_latency)\n            except TimeoutError:\n                # each item timeout exceed the max latency\n                break\n            else:\n                batch.append(item)\n            if (time.time() - start_time) > self._max_latency:\n                # total batch time exceeds the max latency\n                break\n        if not batch:\n            return 0\n\n        model_inputs = [i[3] for i in batch]\n        model_outputs = self.model_predict(model_inputs)\n\n        # publish results to redis\n        for i, item in enumerate(batch):\n            client_id, task_id, request_id, _ = item\n            self._send_response(client_id, task_id, request_id, model_outputs[i])\n\n        batch_size = len(batch)\n        logger.info(""[gpu worker %d] run_once batch_size: %d start_at: %s spend: %s"" % (\n            self._pid, batch_size, start_time, time.time() - start_time))\n        return batch_size\n\n    def _recv_request(self, timeout=TIMEOUT):\n        raise NotImplementedError\n\n    def _send_response(self, client_id, task_id, request_id, model_input):\n        raise NotImplementedError\n\n\nclass ThreadedStreamer(_BaseStreamer):\n    def __init__(self, predict_function, batch_size, max_latency=0.1):\n        super().__init__()\n        self._input_queue = Queue()\n        self._output_queue = Queue()\n        self._worker_destroy_event=threading.Event()\n        self._worker = ThreadedWorker(predict_function, batch_size, max_latency,\n                                      self._input_queue, self._output_queue,\n                                      destroy_event=self._worker_destroy_event)\n        self._worker_thread = threading.Thread(target=self._worker.run_forever, name=""thread_worker"")\n        self._worker_thread.daemon = True\n        self._worker_thread.start()\n        self._delay_setup()\n\n    def _send_request(self, task_id, request_id, model_input):\n        self._input_queue.put((0, task_id, request_id, model_input))\n\n    def _recv_response(self, timeout=TIMEOUT):\n        try:\n            message = self._output_queue.get(timeout=timeout)\n        except Empty:\n            message = None\n        return message\n\n    def destroy_workers(self):\n        self._worker_destroy_event.set()\n        self._worker_thread.join(timeout=WORKER_TIMEOUT)\n        if self._worker_thread.is_alive():\n            raise TimeoutError(""worker_thread destroy timeout"")\n        logger.info(""workers destroyed"")\n\n\nclass ThreadedWorker(_BaseStreamWorker):\n    def __init__(self, predict_function, batch_size, max_latency, request_queue, response_queue, *args, **kwargs):\n        super().__init__(predict_function, batch_size, max_latency, *args, **kwargs)\n        self._request_queue = request_queue\n        self._response_queue = response_queue\n\n    def _recv_request(self, timeout=TIMEOUT):\n        try:\n            item = self._request_queue.get(timeout=timeout)\n        except Empty:\n            raise TimeoutError\n        else:\n            return item\n\n    def _send_response(self, client_id, task_id, request_id, model_output):\n        self._response_queue.put((task_id, request_id, model_output))\n\n\nclass Streamer(_BaseStreamer):\n    def __init__(self, predict_function_or_model, batch_size, max_latency=0.1, worker_num=1,\n                 cuda_devices=None, model_init_args=None, model_init_kwargs=None, wait_for_worker_ready=False,\n                 mp_start_method=\'spawn\'):\n        super().__init__()\n        self.worker_num = worker_num\n        self.cuda_devices = cuda_devices\n        self.mp = multiprocessing.get_context(mp_start_method)\n        self._input_queue = self.mp.Queue()\n        self._output_queue = self.mp.Queue()\n        self._worker = StreamWorker(predict_function_or_model, batch_size, max_latency,\n                                    self._input_queue, self._output_queue,\n                                    model_init_args, model_init_kwargs)\n        self._worker_ps = []\n        self._worker_ready_events = []\n        self._worker_destroy_events = []\n        self._setup_gpu_worker()\n        if wait_for_worker_ready:\n            self._wait_for_worker_ready()\n        self._delay_setup()\n\n    def _setup_gpu_worker(self):\n        for i in range(self.worker_num):\n            ready_event = self.mp.Event()\n            destroy_event = self.mp.Event()\n            if self.cuda_devices is not None:\n                gpu_id = self.cuda_devices[i % len(self.cuda_devices)]\n                args = (gpu_id, ready_event, destroy_event)\n            else:\n                args = (None, ready_event, destroy_event)\n            p = self.mp.Process(target=self._worker.run_forever, args=args, name=""stream_worker"", daemon=True)\n            p.start()\n            self._worker_ps.append(p)\n            self._worker_ready_events.append(ready_event)\n            self._worker_destroy_events.append(destroy_event)\n\n    def _wait_for_worker_ready(self, timeout=WORKER_TIMEOUT):\n        # wait for all workers finishing init\n        for (i, e) in enumerate(self._worker_ready_events):\n            # todo: select all events with timeout\n            is_ready = e.wait(timeout)\n            logger.info(""gpu worker:%d ready state: %s"" % (i, is_ready))\n\n    def _send_request(self, task_id, request_id, model_input):\n        self._input_queue.put((0, task_id, request_id, model_input))\n\n    def _recv_response(self, timeout=TIMEOUT):\n        try:\n            message = self._output_queue.get(timeout=timeout)\n        except Empty:\n            message = None\n        return message\n\n    def destroy_workers(self):\n        for e in self._worker_destroy_events:\n            e.set()\n        for p in self._worker_ps:\n            p.join(timeout=WORKER_TIMEOUT)\n            if p.is_alive():\n                raise TimeoutError(""worker_process destroy timeout"")\n        logger.info(""workers destroyed"")\n\n\nclass StreamWorker(_BaseStreamWorker):\n    def __init__(self, predict_function_or_model, batch_size, max_latency, request_queue, response_queue,\n                 model_init_args, model_init_kwargs, *args, **kwargs):\n        super().__init__(predict_function_or_model, batch_size, max_latency, *args, **kwargs)\n        self._request_queue = request_queue\n        self._response_queue = response_queue\n        self._model_init_args = model_init_args or []\n        self._model_init_kwargs = model_init_kwargs or {}\n\n    def run_forever(self, gpu_id=None, ready_event=None, destroy_event=None):\n        # if it is a managed model, lazy init model after forked & set CUDA_VISIBLE_DEVICES\n        if isinstance(self._predict, type) and issubclass(self._predict, ManagedModel):\n            model_class = self._predict\n            logger.info(""[gpu worker %d] init model on gpu:%s"" % (os.getpid(), gpu_id))\n            self._model = model_class(gpu_id)\n            self._model.init_model(*self._model_init_args, **self._model_init_kwargs)\n            logger.info(""[gpu worker %d] init model on gpu:%s"" % (os.getpid(), gpu_id))\n            self._predict = self._model.predict\n            if ready_event:\n                ready_event.set()  # tell father process that init is finished\n        if destroy_event:\n            self._destroy_event = destroy_event\n        super().run_forever()\n\n    def _recv_request(self, timeout=TIMEOUT):\n        try:\n            item = self._request_queue.get(timeout=timeout)\n        except Empty:\n            raise TimeoutError\n        else:\n            return item\n\n    def _send_response(self, client_id, task_id, request_id, model_output):\n        self._response_queue.put((task_id, request_id, model_output))\n\n\nclass RedisStreamer(_BaseStreamer):\n    """"""\n    1. input batch as a task\n    2. distribute every single item in batch to redis\n    3. backend loop collecting results\n    3. output batch result for a task when every single item is returned\n    """"""\n\n    def __init__(self, redis_broker=""localhost:6379"", prefix=\'\'):\n        super().__init__()\n        self.prefix = prefix\n        self._redis_broker = redis_broker\n        self._redis = _RedisClient(self._client_id, self._redis_broker, self.prefix)\n        self._delay_setup()\n\n    def _send_request(self, task_id, request_id, model_input):\n        self._redis.send_request(task_id, request_id, model_input)\n\n    def _recv_response(self, timeout=TIMEOUT):\n        return self._redis.recv_response(timeout)\n\n\nclass RedisWorker(_BaseStreamWorker):\n    def __init__(self, model_class, batch_size, max_latency=0.1,\n                 redis_broker=""localhost:6379"", prefix=\'\',\n                 model_init_args=None, model_init_kwargs=None, *args, **kwargs):\n        # assert issubclass(model_class, ManagedModel)\n        super().__init__(model_class, batch_size, max_latency, *args, **kwargs)\n\n        self.prefix = prefix\n        self._model_init_args = model_init_args or []\n        self._model_init_kwargs = model_init_kwargs or {}\n        self._redis_broker = redis_broker\n        self._redis = _RedisServer(0, self._redis_broker, self.prefix)\n        self._requests_queue = Queue()\n\n        self.back_thread = threading.Thread(target=self._loop_recv_request, name=""thread_recv_request"")\n        self.back_thread.daemon = True\n        self.back_thread.start()\n\n    def run_forever(self, gpu_id=None):\n        logger.info(""[gpu worker %d] init model on gpu:%s"" % (os.getpid(), gpu_id))\n        model_class = self._predict\n        self._model = model_class(gpu_id)\n        self._model.init_model(*self._model_init_args, **self._model_init_kwargs)\n        self._predict = self._model.predict\n\n        super().run_forever()\n\n    def _loop_recv_request(self):\n        logger.info(""[gpu worker %d] start loop_recv_request"" % (os.getpid()))\n        while True:\n            message = self._redis.recv_request(timeout=TIMEOUT)\n            if message:\n                (client_id, task_id, request_id, request_item) = pickle.loads(message)\n                self._requests_queue.put((client_id, task_id, request_id, request_item))\n            else:\n                # sleep if recv timeout\n                time.sleep(TIME_SLEEP)\n\n    def _recv_request(self, timeout=TIMEOUT):\n        try:\n            item = self._requests_queue.get(timeout=timeout)\n        except Empty:\n            raise TimeoutError\n        else:\n            return item\n\n    def _send_response(self, client_id, task_id, request_id, model_output):\n        self._redis.send_response(client_id, task_id, request_id, model_output)\n\n\ndef _setup_redis_worker_and_runforever(model_class, batch_size, max_latency, gpu_id,\n                                       redis_broker, prefix=\'\', model_init_args=None, model_init_kwargs=None):\n    redis_worker = RedisWorker(model_class, batch_size, max_latency, redis_broker=redis_broker, prefix=prefix,\n                               model_init_args=model_init_args, model_init_kwargs=model_init_kwargs)\n    redis_worker.run_forever(gpu_id)\n\n\ndef run_redis_workers_forever(model_class, batch_size, max_latency=0.1,\n                              worker_num=1, cuda_devices=None, redis_broker=""localhost:6379"",\n                              prefix=\'\', mp_start_method=\'spawn\', model_init_args=None, model_init_kwargs=None):\n    procs = []\n    mp = multiprocessing.get_context(mp_start_method)\n    for i in range(worker_num):\n        if cuda_devices is not None:\n            gpu_id = cuda_devices[i % len(cuda_devices)]\n        else:\n            gpu_id = None\n        args = [model_class, batch_size, max_latency, gpu_id, redis_broker, prefix, model_init_args, model_init_kwargs]\n        p = mp.Process(target=_setup_redis_worker_and_runforever, args=args, name=""stream_worker"", daemon=True)\n        p.start()\n        procs.append(p)\n\n    for p in procs:\n        p.join()\n\n\nclass _RedisAgent(object):\n    def __init__(self, redis_id, redis_broker=\'localhost:6379\', prefix=\'\'):\n        self._redis_id = redis_id\n        self._redis_host = redis_broker.split("":"")[0]\n        self._redis_port = int(redis_broker.split("":"")[1])\n        self._redis_request_queue_name = ""request_queue"" +  prefix\n        self._redis_response_pb_prefix = ""response_pb_""  + prefix\n        self._redis = Redis(host=self._redis_host, port=self._redis_port)\n        self._response_pb = self._redis.pubsub(ignore_subscribe_messages=True)\n        self._setup()\n\n    def _setup(self):\n        raise NotImplementedError\n\n    def _response_pb_name(self, redis_id):\n        return self._redis_response_pb_prefix + redis_id\n\n\nclass _RedisClient(_RedisAgent):\n    def _setup(self):\n        self._response_pb.subscribe(self._response_pb_name(self._redis_id))\n\n    def send_request(self, task_id, request_id, model_input):\n        message = (self._redis_id, task_id, request_id, model_input)\n        self._redis.lpush(self._redis_request_queue_name, pickle.dumps(message))\n\n    def recv_response(self, timeout):\n        message = self._response_pb.get_message(timeout=timeout)\n        if message:\n            return pickle.loads(message[""data""])\n\n\nclass _RedisServer(_RedisAgent):\n    def _setup(self):\n        # server subscribe all pubsub\n        self._response_pb.psubscribe(self._redis_response_pb_prefix + ""*"")\n\n    def recv_request(self, timeout):\n        message = self._redis.blpop(self._redis_request_queue_name, timeout=timeout)\n        # (queue_name, data)\n        if message:\n            return message[1]\n\n    def send_response(self, client_id, task_id, request_id, model_output):\n        message = (task_id, request_id, model_output)\n        channel_name = self._response_pb_name(client_id)\n        self._redis.publish(channel_name, pickle.dumps(message))\n'"
tests/test_service_streamer.py,1,"b'# coding=utf-8\n# Created by Meteorix at 2019/8/16\nimport os\nimport threading\n\nfrom vision_case.model import VisionDensenetModel, VisionResNetModel, DIR_PATH\n\nfrom service_streamer import ThreadedStreamer, ManagedModel, Streamer, RedisStreamer, RedisWorker, \\\n    run_redis_workers_forever\nimport torch\nimport pytest\n\nBATCH_SIZE = 2\n\nif torch.cuda.is_available():\n    device = ""cuda""\nelse:\n    device = ""cpu""  # in case ci environment do not have gpu\n\n\nclass ManagedVisionDensenetModel(ManagedModel):\n    def init_model(self):\n        self.model = VisionDensenetModel(device=device)\n\n    def predict(self, batch):\n        return self.model.batch_prediction(batch)\n\n\nclass ManagedVisionResNetModel(ManagedModel):\n    def init_model(self):\n        self.model = VisionResNetModel(device=device)\n\n    def predict(self, batch):\n        return self.model.batch_prediction(batch)\n\n\nclass TestClass(object):\n\n    def setup_class(self):\n        with open(os.path.join(DIR_PATH, ""cat.jpg""), \'rb\') as f:\n            image_bytes = f.read()\n        self.input_batch = [image_bytes]\n        self.vision_model = VisionDensenetModel(device=device)\n        self.single_output = self.vision_model.batch_prediction(self.input_batch)\n        self.batch_output = self.vision_model.batch_prediction(self.input_batch * BATCH_SIZE)\n\n        with open(os.path.join(DIR_PATH, ""dog.jpg""), \'rb\') as f:\n            image_bytes2 = f.read()\n        self.input_batch2 = [image_bytes2]\n        self.vision_model2 = VisionResNetModel(device=device)\n        self.single_output2 = self.vision_model2.batch_prediction(self.input_batch2)\n        self.batch_output2 = self.vision_model2.batch_prediction(self.input_batch2 * BATCH_SIZE)\n\n        self.managed_model = ManagedVisionDensenetModel()\n        self.managed_model.init_model()\n\n    def test_init_redis_workers(self):\n        thread = threading.Thread(target=run_redis_workers_forever, args=(\n            ManagedVisionDensenetModel, 8, 0.1, 2, (0, 1, 2, 3), ""localhost:6379"", \'\'), daemon=True)\n        thread1 = threading.Thread(target=run_redis_workers_forever, args=(\n            ManagedVisionDensenetModel, 8, 0.1, 2, (0, 1, 2, 3), ""localhost:6379"", \'channel_for_densenet\'), daemon=True)\n        thread2 = threading.Thread(target=run_redis_workers_forever, args=(\n            ManagedVisionResNetModel, 8, 0.1, 2, (0, 1, 2, 3), ""localhost:6379"", \'channel_for_resnet\'), daemon=True)\n\n        thread.start()\n        thread1.start()\n        thread2.start()\n\n    def test_threaded_streamer(self):\n        streamer = ThreadedStreamer(self.vision_model.batch_prediction, batch_size=8)\n        single_predict = streamer.predict(self.input_batch)\n        assert single_predict == self.single_output\n\n        batch_predict = streamer.predict(self.input_batch * BATCH_SIZE)\n        assert batch_predict == self.batch_output\n\n        streamer.destroy_workers()\n\n    def test_managed_model(self):\n        single_predict = self.managed_model.predict(self.input_batch)\n        assert single_predict == self.single_output\n\n        batch_predict = self.managed_model.predict(self.input_batch * BATCH_SIZE)\n        assert batch_predict == self.batch_output\n\n    def test_spawned_streamer(self):\n        # Spawn releases 4 gpu worker processes\n        streamer = Streamer(self.vision_model.batch_prediction, batch_size=8, worker_num=4, cuda_devices=(0, 1, 2, 3))\n        single_predict = streamer.predict(self.input_batch)\n        assert single_predict == self.single_output\n\n        batch_predict = streamer.predict(self.input_batch * BATCH_SIZE)\n        assert batch_predict == self.batch_output\n\n        streamer.destroy_workers()\n\n    def test_future_api(self):\n        streamer = ThreadedStreamer(self.vision_model.batch_prediction, batch_size=8)\n\n        xs = []\n        for i in range(BATCH_SIZE):\n            future = streamer.submit(self.input_batch)\n            xs.append(future)\n        batch_predict = []\n        # Get all instances of future object and wait for asynchronous responses.\n        for future in xs:\n            batch_predict.extend(future.result())\n        assert batch_predict == self.batch_output\n\n        streamer.destroy_workers()\n\n    def test_redis_streamer(self):\n        # Spawn releases 4 gpu worker processes\n        streamer = RedisStreamer()\n        single_predict = streamer.predict(self.input_batch)\n        assert single_predict == self.single_output\n\n        batch_predict = streamer.predict(self.input_batch * BATCH_SIZE)\n        assert batch_predict == self.batch_output\n\n        with pytest.raises(NotImplementedError):\n            streamer.destroy_workers()\n\n    def test_multi_channel_streamer(self):\n        streamer_1 = RedisStreamer(prefix=\'channel_for_densenet\')\n        streamer_2 = RedisStreamer(prefix=\'channel_for_resnet\')\n\n        single_predict = streamer_1.predict(self.input_batch)\n        assert single_predict == self.single_output\n\n        batch_predict = streamer_1.predict(self.input_batch * BATCH_SIZE)\n        assert batch_predict == self.batch_output\n\n        single_predict2 = streamer_2.predict(self.input_batch2)\n        assert single_predict2 == self.single_output2\n\n        batch_predict2 = streamer_2.predict(self.input_batch2 * BATCH_SIZE)\n        assert batch_predict2 == self.batch_output2\n'"
tests/vision_case/__init__.py,0,b''
tests/vision_case/model.py,1,"b'# coding=utf-8\n# Created by Meteorix at 2019/8/9\nimport io\nimport os\nimport json\nimport torch\nfrom torchvision import models\nfrom torchvision import transforms\nfrom PIL import Image\n\nDIR_PATH = os.path.dirname(os.path.abspath(__file__))\n\n\nclass VisionModel(object):\n    def __init__(self, device=""cpu""):\n        self.imagenet_class_index = json.load(open(os.path.join(DIR_PATH, \'imagenet_class_index.json\')))\n        self.device = device\n\n    @staticmethod\n    def transform_image(image_bytes):\n        my_transforms = transforms.Compose([transforms.Resize(255),\n                                            transforms.CenterCrop(224),\n                                            transforms.ToTensor(),\n                                            transforms.Normalize(\n                                                [0.485, 0.456, 0.406],\n                                                [0.229, 0.224, 0.225])])\n        image = Image.open(io.BytesIO(image_bytes))\n        return my_transforms(image).unsqueeze(0)\n\n    def batch_prediction(self, image_bytes_batch):\n        image_tensors = [self.transform_image(image_bytes=image_bytes) for image_bytes in image_bytes_batch]\n        tensor = torch.cat(image_tensors).to(self.device)\n        outputs = self.model.forward(tensor)\n        _, y_hat = outputs.max(1)\n        predicted_ids = y_hat.tolist()\n        return [self.imagenet_class_index[str(i)] for i in predicted_ids]\n\n\nclass VisionDensenetModel(VisionModel):\n    def __init__(self, device=""cpu""):\n        super().__init__(device=device)\n        self.model = models.densenet121(pretrained=True)\n        self.model.to(self.device)\n        self.model.eval()\n\n\nclass VisionResNetModel(VisionModel):\n    def __init__(self, device=""cpu""):\n        super().__init__(device=device)\n        self.model = models.resnet101(pretrained=True)\n        self.model.to(self.device)\n        self.model.eval()\n'"
