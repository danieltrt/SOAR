file_path,api_count,code
main.py,3,"b'#! /usr/bin/env python\nimport os\nimport argparse\nimport datetime\nimport torch\nimport torchtext.data as data\nimport torchtext.datasets as datasets\nimport model\nimport train\nimport mydatasets\n\n\nparser = argparse.ArgumentParser(description=\'CNN text classificer\')\n# learning\nparser.add_argument(\'-lr\', type=float, default=0.001, help=\'initial learning rate [default: 0.001]\')\nparser.add_argument(\'-epochs\', type=int, default=256, help=\'number of epochs for train [default: 256]\')\nparser.add_argument(\'-batch-size\', type=int, default=64, help=\'batch size for training [default: 64]\')\nparser.add_argument(\'-log-interval\',  type=int, default=1,   help=\'how many steps to wait before logging training status [default: 1]\')\nparser.add_argument(\'-test-interval\', type=int, default=100, help=\'how many steps to wait before testing [default: 100]\')\nparser.add_argument(\'-save-interval\', type=int, default=500, help=\'how many steps to wait before saving [default:500]\')\nparser.add_argument(\'-save-dir\', type=str, default=\'snapshot\', help=\'where to save the snapshot\')\nparser.add_argument(\'-early-stop\', type=int, default=1000, help=\'iteration numbers to stop without performance increasing\')\nparser.add_argument(\'-save-best\', type=bool, default=True, help=\'whether to save when get best performance\')\n# data \nparser.add_argument(\'-shuffle\', action=\'store_true\', default=False, help=\'shuffle the data every epoch\')\n# model\nparser.add_argument(\'-dropout\', type=float, default=0.5, help=\'the probability for dropout [default: 0.5]\')\nparser.add_argument(\'-max-norm\', type=float, default=3.0, help=\'l2 constraint of parameters [default: 3.0]\')\nparser.add_argument(\'-embed-dim\', type=int, default=128, help=\'number of embedding dimension [default: 128]\')\nparser.add_argument(\'-kernel-num\', type=int, default=100, help=\'number of each kind of kernel\')\nparser.add_argument(\'-kernel-sizes\', type=str, default=\'3,4,5\', help=\'comma-separated kernel size to use for convolution\')\nparser.add_argument(\'-static\', action=\'store_true\', default=False, help=\'fix the embedding\')\n# device\nparser.add_argument(\'-device\', type=int, default=-1, help=\'device to use for iterate data, -1 mean cpu [default: -1]\')\nparser.add_argument(\'-no-cuda\', action=\'store_true\', default=False, help=\'disable the gpu\')\n# option\nparser.add_argument(\'-snapshot\', type=str, default=None, help=\'filename of model snapshot [default: None]\')\nparser.add_argument(\'-predict\', type=str, default=None, help=\'predict the sentence given\')\nparser.add_argument(\'-test\', action=\'store_true\', default=False, help=\'train or test\')\nargs = parser.parse_args()\n\n\n# load SST dataset\ndef sst(text_field, label_field,  **kargs):\n    train_data, dev_data, test_data = datasets.SST.splits(text_field, label_field, fine_grained=True)\n    text_field.build_vocab(train_data, dev_data, test_data)\n    label_field.build_vocab(train_data, dev_data, test_data)\n    train_iter, dev_iter, test_iter = data.BucketIterator.splits(\n                                        (train_data, dev_data, test_data), \n                                        batch_sizes=(args.batch_size, \n                                                     len(dev_data), \n                                                     len(test_data)),\n                                        **kargs)\n    return train_iter, dev_iter, test_iter \n\n\n# load MR dataset\ndef mr(text_field, label_field, **kargs):\n    train_data, dev_data = mydatasets.MR.splits(text_field, label_field)\n    text_field.build_vocab(train_data, dev_data)\n    label_field.build_vocab(train_data, dev_data)\n    train_iter, dev_iter = data.Iterator.splits(\n                                (train_data, dev_data), \n                                batch_sizes=(args.batch_size, len(dev_data)),\n                                **kargs)\n    return train_iter, dev_iter\n\n\n# load data\nprint(""\\nLoading data..."")\ntext_field = data.Field(lower=True)\nlabel_field = data.Field(sequential=False)\ntrain_iter, dev_iter = mr(text_field, label_field, device=-1, repeat=False)\n# train_iter, dev_iter, test_iter = sst(text_field, label_field, device=-1, repeat=False)\n\n\n# update args and print\nargs.embed_num = len(text_field.vocab)\nargs.class_num = len(label_field.vocab) - 1\nargs.cuda = (not args.no_cuda) and torch.cuda.is_available(); del args.no_cuda\nargs.kernel_sizes = [int(k) for k in args.kernel_sizes.split(\',\')]\nargs.save_dir = os.path.join(args.save_dir, datetime.datetime.now().strftime(\'%Y-%m-%d_%H-%M-%S\'))\n\nprint(""\\nParameters:"")\nfor attr, value in sorted(args.__dict__.items()):\n    print(""\\t{}={}"".format(attr.upper(), value))\n\n\n# model\ncnn = model.CNN_Text(args)\nif args.snapshot is not None:\n    print(\'\\nLoading model from {}...\'.format(args.snapshot))\n    cnn.load_state_dict(torch.load(args.snapshot))\n\nif args.cuda:\n    torch.cuda.set_device(args.device)\n    cnn = cnn.cuda()\n        \n\n# train or predict\nif args.predict is not None:\n    label = train.predict(args.predict, cnn, text_field, label_field, args.cuda)\n    print(\'\\n[Text]  {}\\n[Label] {}\\n\'.format(args.predict, label))\nelif args.test:\n    try:\n        train.eval(test_iter, cnn, args) \n    except Exception as e:\n        print(""\\nSorry. The test dataset doesn\'t  exist.\\n"")\nelse:\n    print()\n    try:\n        train.train(train_iter, dev_iter, cnn, args)\n    except KeyboardInterrupt:\n        print(\'\\n\' + \'-\' * 89)\n        print(\'Exiting from training early\')\n\n'"
model.py,5,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\nclass CNN_Text(nn.Module):\n    \n    def __init__(self, args):\n        super(CNN_Text, self).__init__()\n        self.args = args\n        \n        V = args.embed_num\n        D = args.embed_dim\n        C = args.class_num\n        Ci = 1\n        Co = args.kernel_num\n        Ks = args.kernel_sizes\n\n        self.embed = nn.Embedding(V, D)\n        # self.convs1 = [nn.Conv2d(Ci, Co, (K, D)) for K in Ks]\n        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n        '''\n        self.conv13 = nn.Conv2d(Ci, Co, (3, D))\n        self.conv14 = nn.Conv2d(Ci, Co, (4, D))\n        self.conv15 = nn.Conv2d(Ci, Co, (5, D))\n        '''\n        self.dropout = nn.Dropout(args.dropout)\n        self.fc1 = nn.Linear(len(Ks)*Co, C)\n\n    def conv_and_pool(self, x, conv):\n        x = F.relu(conv(x)).squeeze(3)  # (N, Co, W)\n        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n        return x\n\n    def forward(self, x):\n        x = self.embed(x)  # (N, W, D)\n        \n        if self.args.static:\n            x = Variable(x)\n\n        x = x.unsqueeze(1)  # (N, Ci, W, D)\n\n        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n\n        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n\n        x = torch.cat(x, 1)\n\n        '''\n        x1 = self.conv_and_pool(x,self.conv13) #(N,Co)\n        x2 = self.conv_and_pool(x,self.conv14) #(N,Co)\n        x3 = self.conv_and_pool(x,self.conv15) #(N,Co)\n        x = torch.cat((x1, x2, x3), 1) # (N,len(Ks)*Co)\n        '''\n        x = self.dropout(x)  # (N, len(Ks)*Co)\n        logit = self.fc1(x)  # (N, C)\n        return logit\n"""
mydatasets.py,0,"b'import re\nimport os\nimport random\nimport tarfile\nimport urllib\nfrom torchtext import data\n\n\nclass TarDataset(data.Dataset):\n    """"""Defines a Dataset loaded from a downloadable tar archive.\n\n    Attributes:\n        url: URL where the tar archive can be downloaded.\n        filename: Filename of the downloaded tar archive.\n        dirname: Name of the top-level directory within the zip archive that\n            contains the data files.\n    """"""\n\n    @classmethod\n    def download_or_unzip(cls, root):\n        path = os.path.join(root, cls.dirname)\n        if not os.path.isdir(path):\n            tpath = os.path.join(root, cls.filename)\n            if not os.path.isfile(tpath):\n                print(\'downloading\')\n                urllib.request.urlretrieve(cls.url, tpath)\n            with tarfile.open(tpath, \'r\') as tfile:\n                print(\'extracting\')\n                tfile.extractall(root)\n        return os.path.join(path, \'\')\n\n\nclass MR(TarDataset):\n\n    url = \'https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz\'\n    filename = \'rt-polaritydata.tar\'\n    dirname = \'rt-polaritydata\'\n\n    @staticmethod\n    def sort_key(ex):\n        return len(ex.text)\n\n    def __init__(self, text_field, label_field, path=None, examples=None, **kwargs):\n        """"""Create an MR dataset instance given a path and fields.\n\n        Arguments:\n            text_field: The field that will be used for text data.\n            label_field: The field that will be used for label data.\n            path: Path to the data file.\n            examples: The examples contain all the data.\n            Remaining keyword arguments: Passed to the constructor of\n                data.Dataset.\n        """"""\n        def clean_str(string):\n            """"""\n            Tokenization/string cleaning for all datasets except for SST.\n            Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n            """"""\n            string = re.sub(r""[^A-Za-z0-9(),!?\\\'\\`]"", "" "", string)\n            string = re.sub(r""\\\'s"", "" \\\'s"", string)\n            string = re.sub(r""\\\'ve"", "" \\\'ve"", string)\n            string = re.sub(r""n\\\'t"", "" n\\\'t"", string)\n            string = re.sub(r""\\\'re"", "" \\\'re"", string)\n            string = re.sub(r""\\\'d"", "" \\\'d"", string)\n            string = re.sub(r""\\\'ll"", "" \\\'ll"", string)\n            string = re.sub(r"","", "" , "", string)\n            string = re.sub(r""!"", "" ! "", string)\n            string = re.sub(r""\\("", "" \\( "", string)\n            string = re.sub(r""\\)"", "" \\) "", string)\n            string = re.sub(r""\\?"", "" \\? "", string)\n            string = re.sub(r""\\s{2,}"", "" "", string)\n            return string.strip()\n\n        text_field.preprocessing = data.Pipeline(clean_str)\n        fields = [(\'text\', text_field), (\'label\', label_field)]\n\n        if examples is None:\n            path = self.dirname if path is None else path\n            examples = []\n            with open(os.path.join(path, \'rt-polarity.neg\'), errors=\'ignore\') as f:\n                examples += [\n                    data.Example.fromlist([line, \'negative\'], fields) for line in f]\n            with open(os.path.join(path, \'rt-polarity.pos\'), errors=\'ignore\') as f:\n                examples += [\n                    data.Example.fromlist([line, \'positive\'], fields) for line in f]\n        super(MR, self).__init__(examples, fields, **kwargs)\n\n    @classmethod\n    def splits(cls, text_field, label_field, dev_ratio=.1, shuffle=True, root=\'.\', **kwargs):\n        """"""Create dataset objects for splits of the MR dataset.\n\n        Arguments:\n            text_field: The field that will be used for the sentence.\n            label_field: The field that will be used for label data.\n            dev_ratio: The ratio that will be used to get split validation dataset.\n            shuffle: Whether to shuffle the data before split.\n            root: The root directory that the dataset\'s zip archive will be\n                expanded into; therefore the directory in whose trees\n                subdirectory the data files will be stored.\n            train: The filename of the train data. Default: \'train.txt\'.\n            Remaining keyword arguments: Passed to the splits method of\n                Dataset.\n        """"""\n        path = cls.download_or_unzip(root)\n        examples = cls(text_field, label_field, path=path, **kwargs).examples\n        if shuffle: random.shuffle(examples)\n        dev_index = -1 * int(dev_ratio*len(examples))\n\n        return (cls(text_field, label_field, examples=examples[:dev_index]),\n                cls(text_field, label_field, examples=examples[dev_index:]))\n'"
train.py,8,"b""import os\nimport sys\nimport torch\nimport torch.autograd as autograd\nimport torch.nn.functional as F\n\n\ndef train(train_iter, dev_iter, model, args):\n    if args.cuda:\n        model.cuda()\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n\n    steps = 0\n    best_acc = 0\n    last_step = 0\n    model.train()\n    for epoch in range(1, args.epochs+1):\n        for batch in train_iter:\n            feature, target = batch.text, batch.label\n            feature.data.t_(), target.data.sub_(1)  # batch first, index align\n            if args.cuda:\n                feature, target = feature.cuda(), target.cuda()\n\n            optimizer.zero_grad()\n            logit = model(feature)\n\n            #print('logit vector', logit.size())\n            #print('target vector', target.size())\n            loss = F.cross_entropy(logit, target)\n            loss.backward()\n            optimizer.step()\n\n            steps += 1\n            if steps % args.log_interval == 0:\n                corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n                accuracy = 100.0 * corrects/batch.batch_size\n                sys.stdout.write(\n                    '\\rBatch[{}] - loss: {:.6f}  acc: {:.4f}%({}/{})'.format(steps, \n                                                                             loss.data[0], \n                                                                             accuracy,\n                                                                             corrects,\n                                                                             batch.batch_size))\n            if steps % args.test_interval == 0:\n                dev_acc = eval(dev_iter, model, args)\n                if dev_acc > best_acc:\n                    best_acc = dev_acc\n                    last_step = steps\n                    if args.save_best:\n                        save(model, args.save_dir, 'best', steps)\n                else:\n                    if steps - last_step >= args.early_stop:\n                        print('early stop by {} steps.'.format(args.early_stop))\n            elif steps % args.save_interval == 0:\n                save(model, args.save_dir, 'snapshot', steps)\n\n\ndef eval(data_iter, model, args):\n    model.eval()\n    corrects, avg_loss = 0, 0\n    for batch in data_iter:\n        feature, target = batch.text, batch.label\n        feature.data.t_(), target.data.sub_(1)  # batch first, index align\n        if args.cuda:\n            feature, target = feature.cuda(), target.cuda()\n\n        logit = model(feature)\n        loss = F.cross_entropy(logit, target, size_average=False)\n\n        avg_loss += loss.data[0]\n        corrects += (torch.max(logit, 1)\n                     [1].view(target.size()).data == target.data).sum()\n\n    size = len(data_iter.dataset)\n    avg_loss /= size\n    accuracy = 100.0 * corrects/size\n    print('\\nEvaluation - loss: {:.6f}  acc: {:.4f}%({}/{}) \\n'.format(avg_loss, \n                                                                       accuracy, \n                                                                       corrects, \n                                                                       size))\n    return accuracy\n\n\ndef predict(text, model, text_field, label_feild, cuda_flag):\n    assert isinstance(text, str)\n    model.eval()\n    # text = text_field.tokenize(text)\n    text = text_field.preprocess(text)\n    text = [[text_field.vocab.stoi[x] for x in text]]\n    x = torch.tensor(text)\n    x = autograd.Variable(x)\n    if cuda_flag:\n        x = x.cuda()\n    print(x)\n    output = model(x)\n    _, predicted = torch.max(output, 1)\n    #return label_feild.vocab.itos[predicted.data[0][0]+1]\n    return label_feild.vocab.itos[predicted.data[0]+1]\n\n\ndef save(model, save_dir, save_prefix, steps):\n    if not os.path.isdir(save_dir):\n        os.makedirs(save_dir)\n    save_prefix = os.path.join(save_dir, save_prefix)\n    save_path = '{}_steps_{}.pt'.format(save_prefix, steps)\n    torch.save(model.state_dict(), save_path)\n"""
