file_path,api_count,code
layers.py,2,"b""import torch\nimport torch.nn as nn\nfrom utils import init_weights\n\nclass unetConv2(nn.Module):\n    def __init__(self, in_size, out_size, is_batchnorm, n=2, ks=3, stride=1, padding=1):\n        super(unetConv2, self).__init__()\n        self.n = n\n        self.ks = ks\n        self.stride = stride\n        self.padding = padding\n        s = stride\n        p = padding\n        if is_batchnorm:\n            for i in range(1, n+1):\n                conv = nn.Sequential(nn.Conv2d(in_size, out_size, ks, s, p),\n                                     nn.BatchNorm2d(out_size),\n                                     nn.ReLU(inplace=True),)\n                setattr(self, 'conv%d'%i, conv)\n                in_size = out_size\n\n        else:\n            for i in range(1, n+1):\n                conv = nn.Sequential(nn.Conv2d(in_size, out_size, ks, s, p),\n                                     nn.ReLU(inplace=True),)\n                setattr(self, 'conv%d'%i, conv)\n                in_size = out_size\n\n        # initialise the blocks\n        for m in self.children():\n            init_weights(m, init_type='kaiming')\n\n    def forward(self, inputs):\n        x = inputs\n        for i in range(1, self.n+1):\n            conv = getattr(self, 'conv%d'%i)\n            x = conv(x)\n\n        return x\n\nclass unetUp(nn.Module):\n    def __init__(self, in_size, out_size, is_deconv, n_concat=2):\n        super(unetUp, self).__init__()\n        self.conv = unetConv2(in_size+(n_concat-2)*out_size, out_size, False)\n        if is_deconv:\n            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2, padding=0)\n        else:\n            self.up = nn.Sequential(\n                 nn.UpsamplingBilinear2d(scale_factor=2),\n                 nn.Conv2d(in_size, out_size, 1))\n           \n        # initialise the blocks\n        for m in self.children():\n            if m.__class__.__name__.find('unetConv2') != -1: continue\n            init_weights(m, init_type='kaiming')\n\n    def forward(self, high_feature, *low_feature):\n        outputs0 = self.up(high_feature)\n        for feature in low_feature:\n            outputs0 = torch.cat([outputs0, feature], 1)\n        return self.conv(outputs0)\n\n"""
utils.py,1,"b""from torch.nn import init\n\n\n### initalize the module\ndef init_weights(net, init_type='normal'):\n    #print('initialization method [%s]' % init_type)\n    if init_type == 'kaiming':\n        net.apply(weights_init_kaiming)\n    else:\n        raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n\ndef weights_init_kaiming(m):\n    classname = m.__class__.__name__\n    #print(classname)\n    if classname.find('Conv') != -1:\n        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n    elif classname.find('Linear') != -1:\n        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n    elif classname.find('BatchNorm') != -1:\n        init.normal_(m.weight.data, 1.0, 0.02)\n        init.constant_(m.bias.data, 0.0)\n\n### compute model params\ndef count_param(model):\n    param_count = 0\n    for param in model.parameters():\n        param_count += param.view(-1).size()[0]\n    return param_count"""
networks/UNet.py,3,"b""import _init_paths\nimport torch\nimport torch.nn as nn\nfrom layers import unetConv2, unetUp\nfrom utils import init_weights, count_param\n\nclass UNet(nn.Module):\n\n    def __init__(self, in_channels=1, n_classes=2, feature_scale=2, is_deconv=True, is_batchnorm=True):\n        super(UNet, self).__init__()\n        self.in_channels = in_channels\n        self.feature_scale = feature_scale\n        self.is_deconv = is_deconv\n        self.is_batchnorm = is_batchnorm\n        \n\n        filters = [64, 128, 256, 512, 1024]\n        filters = [int(x / self.feature_scale) for x in filters]\n\n        # downsampling\n        self.maxpool = nn.MaxPool2d(kernel_size=2)\n        self.conv1 = unetConv2(self.in_channels, filters[0], self.is_batchnorm)\n        self.conv2 = unetConv2(filters[0], filters[1], self.is_batchnorm)\n        self.conv3 = unetConv2(filters[1], filters[2], self.is_batchnorm)\n        self.conv4 = unetConv2(filters[2], filters[3], self.is_batchnorm)\n        self.center = unetConv2(filters[3], filters[4], self.is_batchnorm)\n        # upsampling\n        self.up_concat4 = unetUp(filters[4], filters[3], self.is_deconv)\n        self.up_concat3 = unetUp(filters[3], filters[2], self.is_deconv)\n        self.up_concat2 = unetUp(filters[2], filters[1], self.is_deconv)\n        self.up_concat1 = unetUp(filters[1], filters[0], self.is_deconv)\n        # final conv (without any concat)\n        self.final = nn.Conv2d(filters[0], n_classes, 1)\n\n        # initialise weights\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                init_weights(m, init_type='kaiming')\n            elif isinstance(m, nn.BatchNorm2d):\n                init_weights(m, init_type='kaiming')\n\n    def forward(self, inputs):\n        conv1 = self.conv1(inputs)           # 16*512*512\n        maxpool1 = self.maxpool(conv1)       # 16*256*256\n        \n        conv2 = self.conv2(maxpool1)         # 32*256*256\n        maxpool2 = self.maxpool(conv2)       # 32*128*128\n\n        conv3 = self.conv3(maxpool2)         # 64*128*128\n        maxpool3 = self.maxpool(conv3)       # 64*64*64\n\n        conv4 = self.conv4(maxpool3)         # 128*64*64\n        maxpool4 = self.maxpool(conv4)       # 128*32*32\n\n        center = self.center(maxpool4)       # 256*32*32\n        up4 = self.up_concat4(center,conv4)  # 128*64*64\n        up3 = self.up_concat3(up4,conv3)     # 64*128*128\n        up2 = self.up_concat2(up3,conv2)     # 32*256*256\n        up1 = self.up_concat1(up2,conv1)     # 16*512*512\n\n        final = self.final(up1)\n\n        return final\n\nif __name__ == '__main__':\n    print('#### Test Case ###')\n    from torch.autograd import Variable\n    x = Variable(torch.rand(2,1,64,64)).cuda()\n    model = UNet().cuda()\n    param = count_param(model)\n    y = model(x)\n    print('Output shape:',y.shape)\n    print('UNet totoal parameters: %.2fM (%d)'%(param/1e6,param))\n\n\n      \n"""
networks/UNet_Nested.py,3,"b""import _init_paths\nimport torch\nimport torch.nn as nn\nfrom layers import unetConv2, unetUp\nfrom utils import init_weights, count_param\n\nclass UNet_Nested(nn.Module):\n\n    def __init__(self, in_channels=1, n_classes=2, feature_scale=2, is_deconv=True, is_batchnorm=True, is_ds=True):\n        super(UNet_Nested, self).__init__()\n        self.in_channels = in_channels\n        self.feature_scale = feature_scale\n        self.is_deconv = is_deconv\n        self.is_batchnorm = is_batchnorm\n        self.is_ds = is_ds\n\n        filters = [64, 128, 256, 512, 1024]\n        filters = [int(x / self.feature_scale) for x in filters]\n\n        # downsampling\n        self.maxpool = nn.MaxPool2d(kernel_size=2)\n        self.conv00 = unetConv2(self.in_channels, filters[0], self.is_batchnorm)\n        self.conv10 = unetConv2(filters[0], filters[1], self.is_batchnorm)\n        self.conv20 = unetConv2(filters[1], filters[2], self.is_batchnorm)\n        self.conv30 = unetConv2(filters[2], filters[3], self.is_batchnorm)\n        self.conv40 = unetConv2(filters[3], filters[4], self.is_batchnorm)\n\n        # upsampling\n        self.up_concat01 = unetUp(filters[1], filters[0], self.is_deconv)\n        self.up_concat11 = unetUp(filters[2], filters[1], self.is_deconv)\n        self.up_concat21 = unetUp(filters[3], filters[2], self.is_deconv)\n        self.up_concat31 = unetUp(filters[4], filters[3], self.is_deconv)\n\n        self.up_concat02 = unetUp(filters[1], filters[0], self.is_deconv, 3)\n        self.up_concat12 = unetUp(filters[2], filters[1], self.is_deconv, 3)\n        self.up_concat22 = unetUp(filters[3], filters[2], self.is_deconv, 3)\n\n        self.up_concat03 = unetUp(filters[1], filters[0], self.is_deconv, 4)\n        self.up_concat13 = unetUp(filters[2], filters[1], self.is_deconv, 4)\n        \n        self.up_concat04 = unetUp(filters[1], filters[0], self.is_deconv, 5)\n        \n        # final conv (without any concat)\n        self.final_1 = nn.Conv2d(filters[0], n_classes, 1)\n        self.final_2 = nn.Conv2d(filters[0], n_classes, 1)\n        self.final_3 = nn.Conv2d(filters[0], n_classes, 1)\n        self.final_4 = nn.Conv2d(filters[0], n_classes, 1)\n\n        # initialise weights\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                init_weights(m, init_type='kaiming')\n            elif isinstance(m, nn.BatchNorm2d):\n                init_weights(m, init_type='kaiming')\n\n    def forward(self, inputs):\n        # column : 0\n        X_00 = self.conv00(inputs)       # 16*512*512\n        maxpool0 = self.maxpool(X_00)    # 16*256*256\n        X_10= self.conv10(maxpool0)      # 32*256*256\n        maxpool1 = self.maxpool(X_10)    # 32*128*128\n        X_20 = self.conv20(maxpool1)     # 64*128*128\n        maxpool2 = self.maxpool(X_20)    # 64*64*64\n        X_30 = self.conv30(maxpool2)     # 128*64*64\n        maxpool3 = self.maxpool(X_30)    # 128*32*32\n        X_40 = self.conv40(maxpool3)     # 256*32*32\n        # column : 1\n        X_01 = self.up_concat01(X_10,X_00)\n        X_11 = self.up_concat11(X_20,X_10)\n        X_21 = self.up_concat21(X_30,X_20)\n        X_31 = self.up_concat31(X_40,X_30)\n        # column : 2\n        X_02 = self.up_concat02(X_11,X_00,X_01)\n        X_12 = self.up_concat12(X_21,X_10,X_11)\n        X_22 = self.up_concat22(X_31,X_20,X_21)\n        # column : 3\n        X_03 = self.up_concat03(X_12,X_00,X_01,X_02)\n        X_13 = self.up_concat13(X_22,X_10,X_11,X_12)\n        # column : 4\n        X_04 = self.up_concat04(X_13,X_00,X_01,X_02,X_03)\n\n        # final layer\n        final_1 = self.final_1(X_01)\n        final_2 = self.final_2(X_02)\n        final_3 = self.final_3(X_03)\n        final_4 = self.final_4(X_04)\n\n        final = (final_1+final_2+final_3+final_4)/4\n\n        if self.is_ds:\n            return final\n        else:\n            return final_4\n\nif __name__ == '__main__':\n    print('#### Test Case ###')\n    from torch.autograd import Variable\n    x = Variable(torch.rand(2,1,64,64)).cuda()\n    model = UNet_Nested().cuda()\n    param = count_param(model)\n    y = model(x)\n    print('Output shape:',y.shape)\n    print('UNet++ totoal parameters: %.2fM (%d)'%(param/1e6,param))\n\n\n      \n"""
networks/_init_paths.py,0,"b""import sys\nimport os.path as osp\n\n\ndef add_path(path):\n    if path not in sys.path:\n        sys.path.insert(0, path)\n\nthis_dir = osp.dirname(__file__)\nadd_path(osp.join(this_dir, '..'))\n# print(this_dir)\n# print(osp.join(this_dir, '..'))\n\n"""
