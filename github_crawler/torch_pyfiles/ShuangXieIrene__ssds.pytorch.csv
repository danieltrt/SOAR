file_path,api_count,code
demo.py,0,"b'from __future__ import print_function\nimport sys\nimport os\nimport argparse\nimport numpy as np\nif \'/data/software/opencv-3.4.0/lib/python2.7/dist-packages\' in sys.path:\n    sys.path.remove(\'/data/software/opencv-3.4.0/lib/python2.7/dist-packages\')\nif \'/data/software/opencv-3.3.1/lib/python2.7/dist-packages\' in sys.path:\n    sys.path.remove(\'/data/software/opencv-3.3.1/lib/python2.7/dist-packages\')\nimport cv2\n\nfrom lib.ssds import ObjectDetector\nfrom lib.utils.config_parse import cfg_from_file\n\nVOC_CLASSES = ( \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n    \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n    \'cow\', \'diningtable\', \'dog\', \'horse\',\n    \'motorbike\', \'person\', \'pottedplant\',\n    \'sheep\', \'sofa\', \'train\', \'tvmonitor\')\n\ndef parse_args():\n    """"""\n    Parse input arguments\n    """"""\n    parser = argparse.ArgumentParser(description=\'Demo a ssds.pytorch network\')\n    parser.add_argument(\'--cfg\', dest=\'confg_file\',\n            help=\'the address of optional config file\', default=None, type=str, required=True)\n    parser.add_argument(\'--demo\', dest=\'demo_file\',\n            help=\'the address of the demo file\', default=None, type=str, required=True)\n    parser.add_argument(\'-t\', \'--type\', dest=\'type\',\n            help=\'the type of the demo file, could be ""image"", ""video"", ""camera"" or ""time"", default is ""image""\', default=\'image\', type=str)\n    parser.add_argument(\'-d\', \'--display\', dest=\'display\',\n            help=\'whether display the detection result, default is True\', default=True, type=bool)\n    parser.add_argument(\'-s\', \'--save\', dest=\'save\',\n            help=\'whether write the detection result, default is False\', default=False, type=bool)  \n\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(1)\n\n    args = parser.parse_args()\n    return args\n\n\nCOLORS = [(255, 0, 0), (0, 255, 0), (0, 0, 255)]\nFONT = cv2.FONT_HERSHEY_SIMPLEX\n\ndef demo(args, image_path):\n    # 1. load the configure file\n    cfg_from_file(args.confg_file)\n\n    # 2. load detector based on the configure file\n    object_detector = ObjectDetector()\n\n    # 3. load image\n    image = cv2.imread(image_path)\n\n    # 4. detect\n    _labels, _scores, _coords = object_detector.predict(image)\n\n    # 5. draw bounding box on the image\n    for labels, scores, coords in zip(_labels, _scores, _coords):\n        cv2.rectangle(image, (int(coords[0]), int(coords[1])), (int(coords[2]), int(coords[3])), COLORS[labels % 3], 2)\n        cv2.putText(image, \'{label}: {score:.3f}\'.format(label=VOC_CLASSES[labels], score=scores), (int(coords[0]), int(coords[1])), FONT, 0.5, COLORS[labels % 3], 2)\n    \n    # 6. visualize result\n    if args.display is True:\n        cv2.imshow(\'result\', image)\n        cv2.waitKey(0)\n\n    # 7. write result\n    if args.save is True:\n        path, _ = os.path.splitext(image_path)\n        cv2.imwrite(path + \'_result.jpg\', image)\n    \n\ndef demo_live(args, video_path):\n    # 1. load the configure file\n    cfg_from_file(args.confg_file)\n\n    # 2. load detector based on the configure file\n    object_detector = ObjectDetector()\n\n    # 3. load video\n    video = cv2.VideoCapture(video_path)\n\n    index = -1\n    while(video.isOpened()):\n        index = index + 1\n        sys.stdout.write(\'Process image: {} \\r\'.format(index))\n        sys.stdout.flush()\n\n        # 4. read image\n        flag, image = video.read()\n        if flag == False:\n            print(""Can not read image in Frame : {}"".format(index))\n            break\n\n        # 5. detect\n        _labels, _scores, _coords = object_detector.predict(image)\n\n        # 6. draw bounding box on the image\n        for labels, scores, coords in zip(_labels, _scores, _coords):\n            cv2.rectangle(image, (int(coords[0]), int(coords[1])), (int(coords[2]), int(coords[3])), COLORS[labels % 3], 2)\n            cv2.putText(image, \'{label}: {score:.3f}\'.format(label=VOC_CLASSES[labels], score=scores), (int(coords[0]), int(coords[1])), FONT, 0.5, COLORS[labels % 3], 2)\n    \n        # 7. visualize result\n        if args.display is True:\n            cv2.imshow(\'result\', image)\n            cv2.waitKey(33)\n\n        # 8. write result\n        if args.save is True:\n            path, _ = os.path.splitext(video_path)\n            path = path + \'_result\'\n            if not os.path.exists(path):\n                os.mkdir(path)\n            cv2.imwrite(path + \'/{}.jpg\'.format(index), image)        \n\n\ndef time_benchmark(args, image_path):\n    # 1. load the configure file\n    cfg_from_file(args.confg_file)\n\n    # 2. load detector based on the configure file\n    object_detector = ObjectDetector()\n\n    # 3. load image\n    image = cv2.imread(image_path)\n\n    # 4. time test\n    warmup = 20\n    time_iter = 100\n    print(\'Warmup the detector...\')\n    _t = list()\n    for i in range(warmup+time_iter):\n        _, _, _, (total_time, preprocess_time, net_forward_time, detect_time, output_time) \\\n            = object_detector.predict(image, check_time=True)\n        if i > warmup:\n            _t.append([total_time, preprocess_time, net_forward_time, detect_time, output_time])\n            if i % 20 == 0: \n                print(\'In {}\\{}, total time: {} \\n preprocess: {} \\n net_forward: {} \\n detect: {} \\n output: {}\'.format(\n                    i-warmup, time_iter, total_time, preprocess_time, net_forward_time, detect_time, output_time\n                ))\n    total_time, preprocess_time, net_forward_time, detect_time, output_time = np.sum(_t, axis=0)/time_iter * 1000 # 1000ms to 1s\n    print(\'In average, total time: {}ms \\n preprocess: {}ms \\n net_forward: {}ms \\n detect: {}ms \\n output: {}ms\'.format(\n        total_time, preprocess_time, net_forward_time, detect_time, output_time\n    ))\n    with open(\'./time_benchmark.csv\', \'a\') as f:\n        f.write(""{:s},{:.2f}ms,{:.2f}ms,{:.2f}ms,{:.2f}ms,{:.2f}ms\\n"".format(args.confg_file, total_time, preprocess_time, net_forward_time, detect_time, output_time))\n\n\n    \nif __name__ == \'__main__\':\n    args = parse_args()\n    if args.type == \'image\':\n        demo(args, args.demo_file)\n    elif args.type == \'video\':\n        demo_live(args, args.demo_file)\n    elif args.type == \'camera\':\n        demo_live(args, int(args.demo_file))\n    elif args.type == \'time\':\n        time_benchmark(args, args.demo_file)\n    else:\n        AssertionError(\'type is not correct\')\n'"
setup.py,0,b''
test.py,3,"b'from __future__ import print_function\n\nimport sys\nimport os\nimport argparse\nimport numpy as np\nif \'/data/software/opencv-3.4.0/lib/python2.7/dist-packages\' in sys.path:\n    sys.path.remove(\'/data/software/opencv-3.4.0/lib/python2.7/dist-packages\')\nif \'/data/software/opencv-3.3.1/lib/python2.7/dist-packages\' in sys.path:\n    sys.path.remove(\'/data/software/opencv-3.3.1/lib/python2.7/dist-packages\')\nimport cv2\nfrom datetime import datetime\n\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nfrom torch.autograd import Variable\n\nfrom lib.utils.config_parse import cfg_from_file\nfrom lib.ssds_train import test_model\n\ndef parse_args():\n    """"""\n    Parse input arguments\n    """"""\n    parser = argparse.ArgumentParser(description=\'Train a ssds.pytorch network\')\n    parser.add_argument(\'--cfg\', dest=\'config_file\',\n            help=\'optional config file\', default=None, type=str)\n\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(1)\n\n    args = parser.parse_args()\n    return args\n\ndef test():\n    args = parse_args()\n    if args.config_file is not None:\n        cfg_from_file(args.config_file)\n    test_model()\n\nif __name__ == \'__main__\':\n    test()\n'"
train.py,3,"b'from __future__ import print_function\n\nimport sys\nimport os\nimport argparse\nimport numpy as np\nif \'/data/software/opencv-3.4.0/lib/python2.7/dist-packages\' in sys.path:\n    sys.path.remove(\'/data/software/opencv-3.4.0/lib/python2.7/dist-packages\')\nif \'/data/software/opencv-3.3.1/lib/python2.7/dist-packages\' in sys.path:\n    sys.path.remove(\'/data/software/opencv-3.3.1/lib/python2.7/dist-packages\')\nimport cv2\nfrom datetime import datetime\n\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nfrom torch.autograd import Variable\n\nfrom lib.utils.config_parse import cfg_from_file\nfrom lib.ssds_train import train_model\n\ndef parse_args():\n    """"""\n    Parse input arguments\n    """"""\n    parser = argparse.ArgumentParser(description=\'Train a ssds.pytorch network\')\n    parser.add_argument(\'--cfg\', dest=\'config_file\',\n            help=\'optional config file\', default=None, type=str)\n\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(1)\n\n    args = parser.parse_args()\n    return args\n\ndef train():\n    args = parse_args()\n    if args.config_file is not None:\n        cfg_from_file(args.config_file)\n    train_model()\n\nif __name__ == \'__main__\':\n    train()\n'"
lib/__init__.py,0,b''
lib/ssds.py,8,"b""from __future__ import print_function\nimport numpy as np\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\n\nfrom lib.layers import *\nfrom lib.utils.timer import Timer\nfrom lib.utils.data_augment import preproc\nfrom lib.modeling.model_builder import create_model\nfrom lib.utils.config_parse import cfg\n\nclass ObjectDetector:\n    def __init__(self, viz_arch=False):\n        self.cfg = cfg\n\n        # Build model\n        print('===> Building model')\n        self.model, self.priorbox = create_model(cfg.MODEL)\n        self.priors = Variable(self.priorbox.forward(), volatile=True)\n\n        # Print the model architecture and parameters\n        if viz_arch is True:\n            print('Model architectures:\\n{}\\n'.format(self.model))\n\n        # Utilize GPUs for computation\n        self.use_gpu = torch.cuda.is_available()\n        self.half = False\n        if self.use_gpu:\n            print('Utilize GPUs for computation')\n            print('Number of GPU available', torch.cuda.device_count())\n            self.model.cuda()\n            self.priors.cuda()\n            cudnn.benchmark = True\n            # self.model = torch.nn.DataParallel(self.model).module\n            # Utilize half precision\n            self.half = cfg.MODEL.HALF_PRECISION\n            if self.half:\n                self.model = self.model.half()\n                self.priors = self.priors.half()\n        \n        # Build preprocessor and detector\n        self.preprocessor = preproc(cfg.MODEL.IMAGE_SIZE, cfg.DATASET.PIXEL_MEANS, -2)\n        self.detector = Detect(cfg.POST_PROCESS, self.priors)\n\n        # Load weight:\n        if cfg.RESUME_CHECKPOINT == '':\n            AssertionError('RESUME_CHECKPOINT can not be empty')\n        print('=> loading checkpoint {:s}'.format(cfg.RESUME_CHECKPOINT))\n        checkpoint = torch.load(cfg.RESUME_CHECKPOINT)\n        # checkpoint = torch.load(cfg.RESUME_CHECKPOINT, map_location='gpu' if self.use_gpu else 'cpu')\n        self.model.load_state_dict(checkpoint)\n\n        # test only\n        self.model.eval()\n\n\n    def predict(self, img, threshold=0.6, check_time=False):\n        # make sure the input channel is 3 \n        assert img.shape[2] == 3\n        scale = torch.Tensor([img.shape[1::-1], img.shape[1::-1]])\n        \n        _t = {'preprocess': Timer(), 'net_forward': Timer(), 'detect': Timer(), 'output': Timer()}\n        \n        # preprocess image\n        _t['preprocess'].tic()\n        x = Variable(self.preprocessor(img)[0].unsqueeze(0),volatile=True)\n        if self.use_gpu:\n            x = x.cuda()\n        if self.half:\n            x = x.half()\n        preprocess_time = _t['preprocess'].toc()\n\n        # forward\n        _t['net_forward'].tic()\n        out = self.model(x)  # forward pass\n        net_forward_time = _t['net_forward'].toc()\n\n        # detect\n        _t['detect'].tic()\n        detections = self.detector.forward(out)\n        detect_time = _t['detect'].toc()\n        \n        # output\n        _t['output'].tic()\n        labels, scores, coords = [list() for _ in range(3)]\n        # for batch in range(detections.size(0)):\n        #     print('Batch:', batch)\n        batch=0\n        for classes in range(detections.size(1)):\n            num = 0\n            while detections[batch,classes,num,0] >= threshold:\n                scores.append(detections[batch,classes,num,0])\n                labels.append(classes-1)\n                coords.append(detections[batch,classes,num,1:]*scale)\n                num+=1\n        output_time = _t['output'].toc()\n        total_time = preprocess_time + net_forward_time + detect_time + output_time\n        \n        if check_time is True:\n            return labels, scores, coords, (total_time, preprocess_time, net_forward_time, detect_time, output_time)\n            # total_time = preprocess_time + net_forward_time + detect_time + output_time\n            # print('total time: {} \\n preprocess: {} \\n net_forward: {} \\n detect: {} \\n output: {}'.format(\n            #     total_time, preprocess_time, net_forward_time, detect_time, output_time\n            # ))\n        return labels, scores, coords"""
lib/ssds_train.py,14,"b'from __future__ import print_function\nimport numpy as np\nimport os\nimport sys\nimport cv2\nimport random\nimport pickle\n\nimport torch\nimport torch.backends.cudnn as cudnn\nfrom torch.autograd import Variable\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torch.utils.data as data\nimport torch.nn.init as init\n\nfrom tensorboardX import SummaryWriter\n\nfrom lib.layers import *\nfrom lib.utils.timer import Timer\nfrom lib.utils.data_augment import preproc\nfrom lib.modeling.model_builder import create_model\nfrom lib.dataset.dataset_factory import load_data\nfrom lib.utils.config_parse import cfg\nfrom lib.utils.eval_utils import *\nfrom lib.utils.visualize_utils import *\n\nclass Solver(object):\n    """"""\n    A wrapper class for the training process\n    """"""\n    def __init__(self):\n        self.cfg = cfg\n\n        # Load data\n        print(\'===> Loading data\')\n        self.train_loader = load_data(cfg.DATASET, \'train\') if \'train\' in cfg.PHASE else None\n        self.eval_loader = load_data(cfg.DATASET, \'eval\') if \'eval\' in cfg.PHASE else None\n        self.test_loader = load_data(cfg.DATASET, \'test\') if \'test\' in cfg.PHASE else None\n        self.visualize_loader = load_data(cfg.DATASET, \'visualize\') if \'visualize\' in cfg.PHASE else None\n\n        # Build model\n        print(\'===> Building model\')\n        self.model, self.priorbox = create_model(cfg.MODEL)\n        self.priors = Variable(self.priorbox.forward(), volatile=True)\n        self.detector = Detect(cfg.POST_PROCESS, self.priors)\n\n        # Utilize GPUs for computation\n        self.use_gpu = torch.cuda.is_available()\n        if self.use_gpu:\n            print(\'Utilize GPUs for computation\')\n            print(\'Number of GPU available\', torch.cuda.device_count())\n            self.model.cuda()\n            self.priors.cuda()\n            cudnn.benchmark = True\n            # if torch.cuda.device_count() > 1:\n                # self.model = torch.nn.DataParallel(self.model).module\n\n        # Print the model architecture and parameters\n        print(\'Model architectures:\\n{}\\n\'.format(self.model))\n\n        # print(\'Parameters and size:\')\n        # for name, param in self.model.named_parameters():\n        #     print(\'{}: {}\'.format(name, list(param.size())))\n\n        # print trainable scope\n        print(\'Trainable scope: {}\'.format(cfg.TRAIN.TRAINABLE_SCOPE))\n        trainable_param = self.trainable_param(cfg.TRAIN.TRAINABLE_SCOPE)\n        self.optimizer = self.configure_optimizer(trainable_param, cfg.TRAIN.OPTIMIZER)\n        self.exp_lr_scheduler = self.configure_lr_scheduler(self.optimizer, cfg.TRAIN.LR_SCHEDULER)\n        self.max_epochs = cfg.TRAIN.MAX_EPOCHS\n\n        # metric\n        self.criterion = MultiBoxLoss(cfg.MATCHER, self.priors, self.use_gpu)\n\n        # Set the logger\n        self.writer = SummaryWriter(log_dir=cfg.LOG_DIR)\n        self.output_dir = cfg.EXP_DIR\n        self.checkpoint = cfg.RESUME_CHECKPOINT\n        self.checkpoint_prefix = cfg.CHECKPOINTS_PREFIX\n\n\n    def save_checkpoints(self, epochs, iters=None):\n        if not os.path.exists(self.output_dir):\n            os.makedirs(self.output_dir)\n        if iters:\n            filename = self.checkpoint_prefix + \'_epoch_{:d}_iter_{:d}\'.format(epochs, iters) + \'.pth\'\n        else:\n            filename = self.checkpoint_prefix + \'_epoch_{:d}\'.format(epochs) + \'.pth\'\n        filename = os.path.join(self.output_dir, filename)\n        torch.save(self.model.state_dict(), filename)\n        with open(os.path.join(self.output_dir, \'checkpoint_list.txt\'), \'a\') as f:\n            f.write(\'epoch {epoch:d}: {filename}\\n\'.format(epoch=epochs, filename=filename))\n        print(\'Wrote snapshot to: {:s}\'.format(filename))\n\n        # TODO: write relative cfg under the same page\n\n    def resume_checkpoint(self, resume_checkpoint):\n        if resume_checkpoint == \'\' or not os.path.isfile(resume_checkpoint):\n            print((""=> no checkpoint found at \'{}\'"".format(resume_checkpoint)))\n            return False\n        print((""=> loading checkpoint \'{:s}\'"".format(resume_checkpoint)))\n        checkpoint = torch.load(resume_checkpoint)\n\n        # print(""=> Weigths in the checkpoints:"")\n        # print([k for k, v in list(checkpoint.items())])\n\n        # remove the module in the parrallel model\n        if \'module.\' in list(checkpoint.items())[0][0]:\n            pretrained_dict = {\'.\'.join(k.split(\'.\')[1:]): v for k, v in list(checkpoint.items())}\n            checkpoint = pretrained_dict\n\n        # change the name of the weights which exists in other model\n        # change_dict = {\n        #         \'conv1.weight\':\'base.0.weight\',\n        #         \'bn1.running_mean\':\'base.1.running_mean\',\n        #         \'bn1.running_var\':\'base.1.running_var\',\n        #         \'bn1.bias\':\'base.1.bias\',\n        #         \'bn1.weight\':\'base.1.weight\',\n        #         }\n        # for k, v in list(checkpoint.items()):\n        #     for _k, _v in list(change_dict.items()):\n        #         if _k == k:\n        #             new_key = k.replace(_k, _v)\n        #             checkpoint[new_key] = checkpoint.pop(k)\n        # change_dict = {\'layer1.{:d}.\'.format(i):\'base.{:d}.\'.format(i+4) for i in range(20)}\n        # change_dict.update({\'layer2.{:d}.\'.format(i):\'base.{:d}.\'.format(i+7) for i in range(20)})\n        # change_dict.update({\'layer3.{:d}.\'.format(i):\'base.{:d}.\'.format(i+11) for i in range(30)})\n        # for k, v in list(checkpoint.items()):\n        #     for _k, _v in list(change_dict.items()):\n        #         if _k in k:\n        #             new_key = k.replace(_k, _v)\n        #             checkpoint[new_key] = checkpoint.pop(k)\n\n        resume_scope = self.cfg.TRAIN.RESUME_SCOPE\n        # extract the weights based on the resume scope\n        if resume_scope != \'\':\n            pretrained_dict = {}\n            for k, v in list(checkpoint.items()):\n                for resume_key in resume_scope.split(\',\'):\n                    if resume_key in k:\n                        pretrained_dict[k] = v\n                        break\n            checkpoint = pretrained_dict\n\n        pretrained_dict = {k: v for k, v in checkpoint.items() if k in self.model.state_dict()}\n        # print(""=> Resume weigths:"")\n        # print([k for k, v in list(pretrained_dict.items())])\n\n        checkpoint = self.model.state_dict()\n\n        unresume_dict = set(checkpoint)-set(pretrained_dict)\n        if len(unresume_dict) != 0:\n            print(""=> UNResume weigths:"")\n            print(unresume_dict)\n\n        checkpoint.update(pretrained_dict)\n\n        return self.model.load_state_dict(checkpoint)\n\n\n    def find_previous(self):\n        if not os.path.exists(os.path.join(self.output_dir, \'checkpoint_list.txt\')):\n            return False\n        with open(os.path.join(self.output_dir, \'checkpoint_list.txt\'), \'r\') as f:\n            lineList = f.readlines()\n        epoches, resume_checkpoints = [list() for _ in range(2)]\n        for line in lineList:\n            epoch = int(line[line.find(\'epoch \') + len(\'epoch \'): line.find(\':\')])\n            checkpoint = line[line.find(\':\') + 2:-1]\n            epoches.append(epoch)\n            resume_checkpoints.append(checkpoint)\n        return epoches, resume_checkpoints\n\n    def weights_init(self, m):\n        for key in m.state_dict():\n            if key.split(\'.\')[-1] == \'weight\':\n                if \'conv\' in key:\n                    init.kaiming_normal(m.state_dict()[key], mode=\'fan_out\')\n                if \'bn\' in key:\n                    m.state_dict()[key][...] = 1\n            elif key.split(\'.\')[-1] == \'bias\':\n                m.state_dict()[key][...] = 0\n\n\n    def initialize(self):\n        # TODO: ADD INIT ways\n        # raise ValueError(""Fan in and fan out can not be computed for tensor with less than 2 dimensions"")\n        # for module in self.cfg.TRAIN.TRAINABLE_SCOPE.split(\',\'):\n        #     if hasattr(self.model, module):\n        #         getattr(self.model, module).apply(self.weights_init)\n        if self.checkpoint:\n            print(\'Loading initial model weights from {:s}\'.format(self.checkpoint))\n            self.resume_checkpoint(self.checkpoint)\n\n        start_epoch = 0\n        return start_epoch\n\n    def trainable_param(self, trainable_scope):\n        for param in self.model.parameters():\n            param.requires_grad = False\n\n        trainable_param = []\n        for module in trainable_scope.split(\',\'):\n            if hasattr(self.model, module):\n                # print(getattr(self.model, module))\n                for param in getattr(self.model, module).parameters():\n                    param.requires_grad = True\n                trainable_param.extend(getattr(self.model, module).parameters())\n\n        return trainable_param\n\n    def train_model(self):\n        previous = self.find_previous()\n        if previous:\n            start_epoch = previous[0][-1]\n            self.resume_checkpoint(previous[1][-1])\n        else:\n            start_epoch = self.initialize()\n\n        # export graph for the model, onnx always not works\n        # self.export_graph()\n\n        # warm_up epoch\n        warm_up = self.cfg.TRAIN.LR_SCHEDULER.WARM_UP_EPOCHS\n        for epoch in iter(range(start_epoch+1, self.max_epochs+1)):\n            #learning rate\n            sys.stdout.write(\'\\rEpoch {epoch:d}/{max_epochs:d}:\\n\'.format(epoch=epoch, max_epochs=self.max_epochs))\n            if epoch > warm_up:\n                self.exp_lr_scheduler.step(epoch-warm_up)\n            if \'train\' in cfg.PHASE:\n                self.train_epoch(self.model, self.train_loader, self.optimizer, self.criterion, self.writer, epoch, self.use_gpu)\n            if \'eval\' in cfg.PHASE:\n                self.eval_epoch(self.model, self.eval_loader, self.detector, self.criterion, self.writer, epoch, self.use_gpu)\n            if \'test\' in cfg.PHASE:\n                self.test_epoch(self.model, self.test_loader, self.detector, self.output_dir, self.use_gpu)\n            if \'visualize\' in cfg.PHASE:\n                self.visualize_epoch(self.model, self.visualize_loader, self.priorbox, self.writer, epoch,  self.use_gpu)\n\n            if epoch % cfg.TRAIN.CHECKPOINTS_EPOCHS == 0:\n                self.save_checkpoints(epoch)\n\n    def test_model(self):\n        previous = self.find_previous()\n        if previous:\n            for epoch, resume_checkpoint in zip(previous[0], previous[1]):\n                if self.cfg.TEST.TEST_SCOPE[0] <= epoch <= self.cfg.TEST.TEST_SCOPE[1]:\n                    sys.stdout.write(\'\\rEpoch {epoch:d}/{max_epochs:d}:\\n\'.format(epoch=epoch, max_epochs=self.cfg.TEST.TEST_SCOPE[1]))\n                    self.resume_checkpoint(resume_checkpoint)\n                    if \'eval\' in cfg.PHASE:\n                        self.eval_epoch(self.model, self.eval_loader, self.detector, self.criterion, self.writer, epoch, self.use_gpu)\n                    if \'test\' in cfg.PHASE:\n                        self.test_epoch(self.model, self.test_loader, self.detector, self.output_dir , self.use_gpu)\n                    if \'visualize\' in cfg.PHASE:\n                        self.visualize_epoch(self.model, self.visualize_loader, self.priorbox, self.writer, epoch,  self.use_gpu)\n        else:\n            sys.stdout.write(\'\\rCheckpoint {}:\\n\'.format(self.checkpoint))\n            self.resume_checkpoint(self.checkpoint)\n            if \'eval\' in cfg.PHASE:\n                self.eval_epoch(self.model, self.eval_loader, self.detector, self.criterion, self.writer, 0, self.use_gpu)\n            if \'test\' in cfg.PHASE:\n                self.test_epoch(self.model, self.test_loader, self.detector, self.output_dir , self.use_gpu)\n            if \'visualize\' in cfg.PHASE:\n                self.visualize_epoch(self.model, self.visualize_loader, self.priorbox, self.writer, 0,  self.use_gpu)\n\n\n    def train_epoch(self, model, data_loader, optimizer, criterion, writer, epoch, use_gpu):\n        model.train()\n\n        epoch_size = len(data_loader)\n        batch_iterator = iter(data_loader)\n\n        loc_loss = 0\n        conf_loss = 0\n        _t = Timer()\n\n        for iteration in iter(range((epoch_size))):\n            images, targets = next(batch_iterator)\n            if use_gpu:\n                images = Variable(images.cuda())\n                targets = [Variable(anno.cuda(), volatile=True) for anno in targets]\n            else:\n                images = Variable(images)\n                targets = [Variable(anno, volatile=True) for anno in targets]\n            _t.tic()\n            # forward\n            out = model(images, phase=\'train\')\n\n            # backprop\n            optimizer.zero_grad()\n            loss_l, loss_c = criterion(out, targets)\n\n            # some bugs in coco train2017. maybe the annonation bug.\n            if loss_l.data[0] == float(""Inf""):\n                continue\n\n            loss = loss_l + loss_c\n            loss.backward()\n            optimizer.step()\n\n            time = _t.toc()\n            loc_loss += loss_l.data[0]\n            conf_loss += loss_c.data[0]\n\n            # log per iter\n            log = \'\\r==>Train: || {iters:d}/{epoch_size:d} in {time:.3f}s [{prograss}] || loc_loss: {loc_loss:.4f} cls_loss: {cls_loss:.4f}\\r\'.format(\n                    prograss=\'#\'*int(round(10*iteration/epoch_size)) + \'-\'*int(round(10*(1-iteration/epoch_size))), iters=iteration, epoch_size=epoch_size,\n                    time=time, loc_loss=loss_l.data[0], cls_loss=loss_c.data[0])\n\n            sys.stdout.write(log)\n            sys.stdout.flush()\n\n        # log per epoch\n        sys.stdout.write(\'\\r\')\n        sys.stdout.flush()\n        lr = optimizer.param_groups[0][\'lr\']\n        log = \'\\r==>Train: || Total_time: {time:.3f}s || loc_loss: {loc_loss:.4f} conf_loss: {conf_loss:.4f} || lr: {lr:.6f}\\n\'.format(lr=lr,\n                time=_t.total_time, loc_loss=loc_loss/epoch_size, conf_loss=conf_loss/epoch_size)\n        sys.stdout.write(log)\n        sys.stdout.flush()\n\n        # log for tensorboard\n        writer.add_scalar(\'Train/loc_loss\', loc_loss/epoch_size, epoch)\n        writer.add_scalar(\'Train/conf_loss\', conf_loss/epoch_size, epoch)\n        writer.add_scalar(\'Train/lr\', lr, epoch)\n\n\n    def eval_epoch(self, model, data_loader, detector, criterion, writer, epoch, use_gpu):\n        model.eval()\n\n        epoch_size = len(data_loader)\n        batch_iterator = iter(data_loader)\n\n        loc_loss = 0\n        conf_loss = 0\n        _t = Timer()\n\n        label = [list() for _ in range(model.num_classes)]\n        gt_label = [list() for _ in range(model.num_classes)]\n        score = [list() for _ in range(model.num_classes)]\n        size = [list() for _ in range(model.num_classes)]\n        npos = [0] * model.num_classes\n\n        for iteration in iter(range((epoch_size))):\n        # for iteration in iter(range((10))):\n            images, targets = next(batch_iterator)\n            if use_gpu:\n                images = Variable(images.cuda())\n                targets = [Variable(anno.cuda(), volatile=True) for anno in targets]\n            else:\n                images = Variable(images)\n                targets = [Variable(anno, volatile=True) for anno in targets]\n\n            _t.tic()\n            # forward\n            out = model(images, phase=\'train\')\n\n            # loss\n            loss_l, loss_c = criterion(out, targets)\n\n            out = (out[0], model.softmax(out[1].view(-1, model.num_classes)))\n\n            # detect\n            detections = detector.forward(out)\n\n            time = _t.toc()\n\n            # evals\n            label, score, npos, gt_label = cal_tp_fp(detections, targets, label, score, npos, gt_label)\n            size = cal_size(detections, targets, size)\n            loc_loss += loss_l.data[0]\n            conf_loss += loss_c.data[0]\n\n            # log per iter\n            log = \'\\r==>Eval: || {iters:d}/{epoch_size:d} in {time:.3f}s [{prograss}] || loc_loss: {loc_loss:.4f} cls_loss: {cls_loss:.4f}\\r\'.format(\n                    prograss=\'#\'*int(round(10*iteration/epoch_size)) + \'-\'*int(round(10*(1-iteration/epoch_size))), iters=iteration, epoch_size=epoch_size,\n                    time=time, loc_loss=loss_l.data[0], cls_loss=loss_c.data[0])\n\n            sys.stdout.write(log)\n            sys.stdout.flush()\n\n        # eval mAP\n        prec, rec, ap = cal_pr(label, score, npos)\n\n        # log per epoch\n        sys.stdout.write(\'\\r\')\n        sys.stdout.flush()\n        log = \'\\r==>Eval: || Total_time: {time:.3f}s || loc_loss: {loc_loss:.4f} conf_loss: {conf_loss:.4f} || mAP: {mAP:.6f}\\n\'.format(mAP=ap,\n                time=_t.total_time, loc_loss=loc_loss/epoch_size, conf_loss=conf_loss/epoch_size)\n        sys.stdout.write(log)\n        sys.stdout.flush()\n\n        # log for tensorboard\n        writer.add_scalar(\'Eval/loc_loss\', loc_loss/epoch_size, epoch)\n        writer.add_scalar(\'Eval/conf_loss\', conf_loss/epoch_size, epoch)\n        writer.add_scalar(\'Eval/mAP\', ap, epoch)\n        viz_pr_curve(writer, prec, rec, epoch)\n        viz_archor_strategy(writer, size, gt_label, epoch)\n\n    # TODO: HOW TO MAKE THE DATALOADER WITHOUT SHUFFLE\n    # def test_epoch(self, model, data_loader, detector, output_dir, use_gpu):\n    #     # sys.stdout.write(\'\\r===> Eval mode\\n\')\n\n    #     model.eval()\n\n    #     num_images = len(data_loader.dataset)\n    #     num_classes = detector.num_classes\n    #     batch_size = data_loader.batch_size\n    #     all_boxes = [[[] for _ in range(num_images)] for _ in range(num_classes)]\n    #     empty_array = np.transpose(np.array([[],[],[],[],[]]),(1,0))\n\n    #     epoch_size = len(data_loader)\n    #     batch_iterator = iter(data_loader)\n\n    #     _t = Timer()\n\n    #     for iteration in iter(range((epoch_size))):\n    #         images, targets = next(batch_iterator)\n    #         targets = [[anno[0][1], anno[0][0], anno[0][1], anno[0][0]] for anno in targets] # contains the image size\n    #         if use_gpu:\n    #             images = Variable(images.cuda())\n    #         else:\n    #             images = Variable(images)\n\n    #         _t.tic()\n    #         # forward\n    #         out = model(images, is_train=False)\n\n    #         # detect\n    #         detections = detector.forward(out)\n\n    #         time = _t.toc()\n\n    #         # TODO: make it smart:\n    #         for i, (dets, scale) in enumerate(zip(detections, targets)):\n    #             for j in range(1, num_classes):\n    #                 cls_dets = list()\n    #                 for det in dets[j]:\n    #                     if det[0] > 0:\n    #                         d = det.cpu().numpy()\n    #                         score, box = d[0], d[1:]\n    #                         box *= scale\n    #                         box = np.append(box, score)\n    #                         cls_dets.append(box)\n    #                 if len(cls_dets) == 0:\n    #                     cls_dets = empty_array\n    #                 all_boxes[j][iteration*batch_size+i] = np.array(cls_dets)\n\n    #         # log per iter\n    #         log = \'\\r==>Test: || {iters:d}/{epoch_size:d} in {time:.3f}s [{prograss}]\\r\'.format(\n    #                 prograss=\'#\'*int(round(10*iteration/epoch_size)) + \'-\'*int(round(10*(1-iteration/epoch_size))), iters=iteration, epoch_size=epoch_size,\n    #                 time=time)\n    #         sys.stdout.write(log)\n    #         sys.stdout.flush()\n\n    #     # write result to pkl\n    #     with open(os.path.join(output_dir, \'detections.pkl\'), \'wb\') as f:\n    #         pickle.dump(all_boxes, f, pickle.HIGHEST_PROTOCOL)\n\n    #     print(\'Evaluating detections\')\n    #     data_loader.dataset.evaluate_detections(all_boxes, output_dir)\n\n\n    def test_epoch(self, model, data_loader, detector, output_dir, use_gpu):\n        model.eval()\n\n        dataset = data_loader.dataset\n        num_images = len(dataset)\n        num_classes = detector.num_classes\n        all_boxes = [[[] for _ in range(num_images)] for _ in range(num_classes)]\n        empty_array = np.transpose(np.array([[],[],[],[],[]]),(1,0))\n\n        _t = Timer()\n\n        for i in iter(range((num_images))):\n            img = dataset.pull_image(i)\n            scale = [img.shape[1], img.shape[0], img.shape[1], img.shape[0]]\n            if use_gpu:\n                images = Variable(dataset.preproc(img)[0].unsqueeze(0).cuda(), volatile=True)\n            else:\n                images = Variable(dataset.preproc(img)[0].unsqueeze(0), volatile=True)\n\n            _t.tic()\n            # forward\n            out = model(images, phase=\'eval\')\n\n            # detect\n            detections = detector.forward(out)\n\n            time = _t.toc()\n\n            # TODO: make it smart:\n            for j in range(1, num_classes):\n                cls_dets = list()\n                for det in detections[0][j]:\n                    if det[0] > 0:\n                        d = det.cpu().numpy()\n                        score, box = d[0], d[1:]\n                        box *= scale\n                        box = np.append(box, score)\n                        cls_dets.append(box)\n                if len(cls_dets) == 0:\n                    cls_dets = empty_array\n                all_boxes[j][i] = np.array(cls_dets)\n\n            # log per iter\n            log = \'\\r==>Test: || {iters:d}/{epoch_size:d} in {time:.3f}s [{prograss}]\\r\'.format(\n                    prograss=\'#\'*int(round(10*i/num_images)) + \'-\'*int(round(10*(1-i/num_images))), iters=i, epoch_size=num_images,\n                    time=time)\n            sys.stdout.write(log)\n            sys.stdout.flush()\n\n        # write result to pkl\n        with open(os.path.join(output_dir, \'detections.pkl\'), \'wb\') as f:\n            pickle.dump(all_boxes, f, pickle.HIGHEST_PROTOCOL)\n\n        # currently the COCO dataset do not return the mean ap or ap 0.5:0.95 values\n        print(\'Evaluating detections\')\n        data_loader.dataset.evaluate_detections(all_boxes, output_dir)\n\n\n    def visualize_epoch(self, model, data_loader, priorbox, writer, epoch, use_gpu):\n        model.eval()\n\n        img_index = random.randint(0, len(data_loader.dataset)-1)\n\n        # get img\n        image = data_loader.dataset.pull_image(img_index)\n        anno = data_loader.dataset.pull_anno(img_index)\n\n        # visualize archor box\n        viz_prior_box(writer, priorbox, image, epoch)\n\n        # get preproc\n        preproc = data_loader.dataset.preproc\n        preproc.add_writer(writer, epoch)\n        # preproc.p = 0.6\n\n        # preproc image & visualize preprocess prograss\n        images = Variable(preproc(image, anno)[0].unsqueeze(0), volatile=True)\n        if use_gpu:\n            images = images.cuda()\n\n        # visualize feature map in base and extras\n        base_out = viz_module_feature_maps(writer, model.base, images, module_name=\'base\', epoch=epoch)\n        extras_out = viz_module_feature_maps(writer, model.extras, base_out, module_name=\'extras\', epoch=epoch)\n        # visualize feature map in feature_extractors\n        viz_feature_maps(writer, model(images, \'feature\'), module_name=\'feature_extractors\', epoch=epoch)\n\n        model.train()\n        images.requires_grad = True\n        images.volatile=False\n        base_out = viz_module_grads(writer, model, model.base, images, images, preproc.means, module_name=\'base\', epoch=epoch)\n\n        # TODO: add more...\n\n\n    def configure_optimizer(self, trainable_param, cfg):\n        if cfg.OPTIMIZER == \'sgd\':\n            optimizer = optim.SGD(trainable_param, lr=cfg.LEARNING_RATE,\n                        momentum=cfg.MOMENTUM, weight_decay=cfg.WEIGHT_DECAY)\n        elif cfg.OPTIMIZER == \'rmsprop\':\n            optimizer = optim.RMSprop(trainable_param, lr=cfg.LEARNING_RATE,\n                        momentum=cfg.MOMENTUM, alpha=cfg.MOMENTUM_2, eps=cfg.EPS, weight_decay=cfg.WEIGHT_DECAY)\n        elif cfg.OPTIMIZER == \'adam\':\n            optimizer = optim.Adam(trainable_param, lr=cfg.LEARNING_RATE,\n                        betas=(cfg.MOMENTUM, cfg.MOMENTUM_2), eps=cfg.EPS, weight_decay=cfg.WEIGHT_DECAY)\n        else:\n            AssertionError(\'optimizer can not be recognized.\')\n        return optimizer\n\n\n    def configure_lr_scheduler(self, optimizer, cfg):\n        if cfg.SCHEDULER == \'step\':\n            scheduler = lr_scheduler.StepLR(optimizer, step_size=cfg.STEPS[0], gamma=cfg.GAMMA)\n        elif cfg.SCHEDULER == \'multi_step\':\n            scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=cfg.STEPS, gamma=cfg.GAMMA)\n        elif cfg.SCHEDULER == \'exponential\':\n            scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=cfg.GAMMA)\n        elif cfg.SCHEDULER == \'SGDR\':\n            scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.MAX_EPOCHS)\n        else:\n            AssertionError(\'scheduler can not be recognized.\')\n        return scheduler\n\n\n    def export_graph(self):\n        self.model.train(False)\n        dummy_input = Variable(torch.randn(1, 3, cfg.MODEL.IMAGE_SIZE[0], cfg.MODEL.IMAGE_SIZE[1])).cuda()\n        # Export the model\n        torch_out = torch.onnx._export(self.model,             # model being run\n                                       dummy_input,            # model input (or a tuple for multiple inputs)\n                                       ""graph.onnx"",           # where to save the model (can be a file or file-like object)\n                                       export_params=True)     # store the trained parameter weights inside the model file\n        # if not os.path.exists(cfg.EXP_DIR):\n        #     os.makedirs(cfg.EXP_DIR)\n        # self.writer.add_graph(self.model, (dummy_input, ))\n\n\ndef train_model():\n    s = Solver()\n    s.train_model()\n    return True\n\ndef test_model():\n    s = Solver()\n    s.test_model()\n    return True\n'"
lib/dataset/__init__.py,0,b''
lib/dataset/coco.py,2,"b'import os\nimport pickle\nimport os.path\nimport sys\nimport torch\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nimport cv2\nimport numpy as np\nimport json\nimport uuid\n\nfrom lib.utils.pycocotools.coco import COCO\nfrom lib.utils.pycocotools.cocoeval import COCOeval\nfrom lib.utils.pycocotools import mask as COCOmask\n\n\nclass COCODetection(data.Dataset):\n\n    """"""VOC Detection Dataset Object\n\n    input is image, target is annotation\n\n    Arguments:\n        root (string): filepath to VOCdevkit folder.\n        image_set (string): imageset to use (eg. \'train\', \'val\', \'test\')\n        transform (callable, optional): transformation to perform on the\n            input image\n        target_transform (callable, optional): transformation to perform on the\n            target `annotation`\n            (eg: take in caption string, return tensor of word indices)\n        dataset_name (string, optional): which dataset to load\n            (default: \'VOC2007\')\n    """"""\n\n    def __init__(self, root, image_sets, preproc=None, target_transform=None,\n                 dataset_name=\'COCO\'):\n        self.root = root\n        self.cache_path = os.path.join(self.root, \'cache\')\n        self.image_set = image_sets\n        self.preproc = preproc\n        self.target_transform = target_transform\n        self.name = dataset_name\n        self.ids = list()\n        self.annotations = list()\n        self._view_map = {\n            \'minival2014\' : \'val2014\',          # 5k val2014 subset\n            \'valminusminival2014\' : \'val2014\',  # val2014 \\setminus minival2014\n            \'test-dev2015\' : \'test2015\',\n        }\n\n        # self.data_name = list()\n        # self.data_len = list()\n        for (year, image_set) in image_sets:\n            coco_name = image_set+year\n            data_name = (self._view_map[coco_name]\n                        if coco_name in self._view_map\n                        else coco_name)\n            annofile = self._get_ann_file(coco_name)\n            _COCO = COCO(annofile)\n            self._COCO = _COCO\n            self.coco_name = coco_name\n            cats = _COCO.loadCats(_COCO.getCatIds())\n            self._classes = tuple([\'__background__\'] + [c[\'name\'] for c in cats])\n            self.num_classes = len(self._classes)\n            self._class_to_ind = dict(zip(self._classes, range(self.num_classes)))\n            self._class_to_coco_cat_id = dict(zip([c[\'name\'] for c in cats],\n                                                  _COCO.getCatIds()))\n            indexes = _COCO.getImgIds()\n            self.image_indexes = indexes\n            # seems it will reduce the speed during the training.\n            # self.ids.extend(indexes)\n            # self.data_len.append(len(indexes))\n            # self.data_name.append(data_name)\n            self.ids.extend(self._load_coco_img_path(coco_name, indexes))\n            if image_set.find(\'test\') != -1:\n                print(\'test set will not load annotations!\')\n            else:\n                self.annotations.extend(self._load_coco_annotations(coco_name, indexes,_COCO))\n\n\n    def image_path_from_index(self, name, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n        # Example image path for index=119993:\n        #   images/train2014/COCO_train2014_000000119993.jpg\n        # file_name = (\'COCO_\' + name + \'_\' +\n        #              str(index).zfill(12) + \'.jpg\')\n        # change Example image path for index=119993 to images/train2017/000000119993.jpg\n        file_name = (str(index).zfill(12) + \'.jpg\')\n        image_path = os.path.join(self.root, \'images\',\n                              name, file_name)\n        assert os.path.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n\n    def _get_ann_file(self, name):\n        prefix = \'instances\' if name.find(\'test\') == -1 \\\n                else \'image_info\'\n        return os.path.join(self.root, \'annotations\',\n                        prefix + \'_\' + name + \'.json\')\n\n\n    def _load_coco_annotations(self, coco_name, indexes, _COCO):\n        cache_file=os.path.join(self.cache_path,coco_name+\'_gt_roidb.pkl\')\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = pickle.load(fid)\n            print(\'{} gt roidb loaded from {}\'.format(coco_name,cache_file))\n            return roidb\n\n        print(\'parsing gt roidb for {}\'.format(coco_name))\n        gt_roidb = [self._annotation_from_index(index, _COCO)\n                    for index in indexes]\n        with open(cache_file, \'wb\') as fid:\n            pickle.dump(gt_roidb,fid,pickle.HIGHEST_PROTOCOL)\n        print(\'wrote gt roidb to {}\'.format(cache_file))\n        return gt_roidb\n\n    def _load_coco_img_path(self, coco_name, indexes):\n        cache_file=os.path.join(self.cache_path,coco_name+\'_img_path.pkl\')\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                img_path = pickle.load(fid)\n            print(\'{} img path loaded from {}\'.format(coco_name,cache_file))\n            return img_path\n\n        print(\'parsing img path for {}\'.format(coco_name))\n        img_path = [self.image_path_from_index(coco_name, index)\n                    for index in indexes]\n        with open(cache_file, \'wb\') as fid:\n            pickle.dump(img_path,fid,pickle.HIGHEST_PROTOCOL)\n        print(\'wrote img path to {}\'.format(cache_file))\n        return img_path\n\n    def _annotation_from_index(self, index, _COCO):\n        """"""\n        Loads COCO bounding-box instance annotations. Crowd instances are\n        handled by marking their overlaps (with all categories) to -1. This\n        overlap value means that crowd ""instances"" are excluded from training.\n        """"""\n        im_ann = _COCO.loadImgs(index)[0]\n        width = im_ann[\'width\']\n        height = im_ann[\'height\']\n\n        annIds = _COCO.getAnnIds(imgIds=index, iscrowd=None)\n        objs = _COCO.loadAnns(annIds)\n        # Sanitize bboxes -- some are invalid\n        valid_objs = []\n        for obj in objs:\n            x1 = np.max((0, obj[\'bbox\'][0]))\n            y1 = np.max((0, obj[\'bbox\'][1]))\n            x2 = np.min((width - 1, x1 + np.max((0, obj[\'bbox\'][2] - 1))))\n            y2 = np.min((height - 1, y1 + np.max((0, obj[\'bbox\'][3] - 1))))\n            if obj[\'area\'] > 0 and x2 >= x1 and y2 >= y1:\n                obj[\'clean_bbox\'] = [x1, y1, x2, y2]\n                valid_objs.append(obj)\n        objs = valid_objs\n        num_objs = len(objs)\n\n        res = np.zeros((num_objs, 5))\n\n        # Lookup table to map from COCO category ids to our internal class\n        # indices\n        coco_cat_id_to_class_ind = dict([(self._class_to_coco_cat_id[cls],\n                                          self._class_to_ind[cls])\n                                         for cls in self._classes[1:]])\n\n        for ix, obj in enumerate(objs):\n            cls = coco_cat_id_to_class_ind[obj[\'category_id\']]\n            res[ix, 0:4] = obj[\'clean_bbox\']\n            res[ix, 4] = cls\n\n        return res\n\n\n\n    def __getitem__(self, index):\n        # lens = 0\n        # name = \'\'\n        # for n, l in zip(self.data_name, self.data_len):\n        #     lens += l\n        #     name = n\n        #     if index < lens:\n        #         break\n        # img_id = self.image_path_from_index(name, self.ids[index])\n        img_id = self.ids[index]\n        target = self.annotations[index]\n\n        img = cv2.imread(img_id, cv2.IMREAD_COLOR)\n        height, width, _ = img.shape\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n\n        if self.preproc is not None:\n            img, target = self.preproc(img, target)\n\n                    # target = self.target_transform(target, width, height)\n        #print(target.shape)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.ids)\n\n    def pull_image(self, index):\n        \'\'\'Returns the original image object at index in PIL form\n\n        Note: not using self.__getitem__(), as any transformations passed in\n        could mess up this functionality.\n\n        Argument:\n            index (int): index of img to show\n        Return:\n            PIL img\n        \'\'\'\n        img_id = self.ids[index]\n        return cv2.imread(img_id, cv2.IMREAD_COLOR)\n\n\n    def pull_anno(self, index):\n        \'\'\'Returns the original annotation of image at index\n\n        Note: not using self.__getitem__(), as any transformations passed in\n        could mess up this functionality.\n\n        Argument:\n            index (int): index of img to get annotation of\n        Return:\n            list:  [img_id, [(label, bbox coords),...]]\n                eg: (\'001718\', [(\'dog\', (96, 13, 438, 332))])\n        \'\'\'\n        anno = self.annotations[index]\n        if self.target_transform is not None:\n            anno = self.target_transform(anno)\n        return anno\n\n\n    def pull_tensor(self, index):\n        \'\'\'Returns the original image at an index in tensor form\n\n        Note: not using self.__getitem__(), as any transformations passed in\n        could mess up this functionality.\n\n        Argument:\n            index (int): index of img to show\n        Return:\n            tensorized version of img, squeezed\n        \'\'\'\n        to_tensor = transforms.ToTensor()\n        return torch.Tensor(self.pull_image(index)).unsqueeze_(0)\n\n    def _print_detection_eval_metrics(self, coco_eval):\n        IoU_lo_thresh = 0.5\n        IoU_hi_thresh = 0.95\n        def _get_thr_ind(coco_eval, thr):\n            ind = np.where((coco_eval.params.iouThrs > thr - 1e-5) &\n                           (coco_eval.params.iouThrs < thr + 1e-5))[0][0]\n            iou_thr = coco_eval.params.iouThrs[ind]\n            assert np.isclose(iou_thr, thr)\n            return ind\n\n        ind_lo = _get_thr_ind(coco_eval, IoU_lo_thresh)\n        ind_hi = _get_thr_ind(coco_eval, IoU_hi_thresh)\n        # precision has dims (iou, recall, cls, area range, max dets)\n        # area range index 0: all area ranges\n        # max dets index 2: 100 per image\n        precision = \\\n            coco_eval.eval[\'precision\'][ind_lo:(ind_hi + 1), :, :, 0, 2]\n        ap_default = np.mean(precision[precision > -1])\n        print(\'~~~~ Mean and per-category AP @ IoU=[{:.2f},{:.2f}] \'\n               \'~~~~\'.format(IoU_lo_thresh, IoU_hi_thresh))\n        print(\'{:.1f}\'.format(100 * ap_default))\n        for cls_ind, cls in enumerate(self._classes):\n            if cls == \'__background__\':\n                continue\n            # minus 1 because of __background__\n            precision = coco_eval.eval[\'precision\'][ind_lo:(ind_hi + 1), :, cls_ind - 1, 0, 2]\n            ap = np.mean(precision[precision > -1])\n            print(\'{:.1f}\'.format(100 * ap))\n\n        print(\'~~~~ Summary metrics ~~~~\')\n        coco_eval.summarize()\n\n    def _do_detection_eval(self, res_file, output_dir):\n        ann_type = \'bbox\'\n        coco_dt = self._COCO.loadRes(res_file)\n        coco_eval = COCOeval(self._COCO, coco_dt)\n        coco_eval.params.useSegm = (ann_type == \'segm\')\n        coco_eval.evaluate()\n        coco_eval.accumulate()\n        self._print_detection_eval_metrics(coco_eval)\n        eval_file = os.path.join(output_dir, \'detection_results.pkl\')\n        with open(eval_file, \'wb\') as fid:\n            pickle.dump(coco_eval, fid, pickle.HIGHEST_PROTOCOL)\n        print(\'Wrote COCO eval results to: {}\'.format(eval_file))\n\n    def _coco_results_one_category(self, boxes, cat_id):\n        results = []\n        for im_ind, index in enumerate(self.image_indexes):\n            dets = boxes[im_ind].astype(np.float)\n            if dets == []:\n                continue\n            scores = dets[:, -1]\n            xs = dets[:, 0]\n            ys = dets[:, 1]\n            ws = dets[:, 2] - xs + 1\n            hs = dets[:, 3] - ys + 1\n            results.extend(\n              [{\'image_id\' : index,\n                \'category_id\' : cat_id,\n                \'bbox\' : [xs[k], ys[k], ws[k], hs[k]],\n                \'score\' : scores[k]} for k in range(dets.shape[0])])\n        return results\n\n    def _write_coco_results_file(self, all_boxes, res_file):\n        # [{""image_id"": 42,\n        #   ""category_id"": 18,\n        #   ""bbox"": [258.15,41.29,348.26,243.78],\n        #   ""score"": 0.236}, ...]\n        results = []\n        for cls_ind, cls in enumerate(self._classes):\n            if cls == \'__background__\':\n                continue\n            print(\'Collecting {} results ({:d}/{:d})\'.format(cls, cls_ind,\n                                                          self.num_classes ))\n            coco_cat_id = self._class_to_coco_cat_id[cls]\n            results.extend(self._coco_results_one_category(all_boxes[cls_ind],\n                                                           coco_cat_id))\n            \'\'\'\n            if cls_ind ==30:\n                res_f = res_file+ \'_1.json\'\n                print(\'Writing results json to {}\'.format(res_f))\n                with open(res_f, \'w\') as fid:\n                    json.dump(results, fid)\n                results = []\n            \'\'\'\n        #res_f2 = res_file+\'_2.json\'\n        print(\'Writing results json to {}\'.format(res_file))\n        with open(res_file, \'w\') as fid:\n            json.dump(results, fid)\n\n    def evaluate_detections(self, all_boxes, output_dir):\n        res_file = os.path.join(output_dir, (\'detections_\' +\n                                         self.coco_name +\n                                         \'_results\'))\n        res_file += \'.json\'\n        self._write_coco_results_file(all_boxes, res_file)\n        # Only do evaluation on non-test sets\n        if self.coco_name.find(\'test\') == -1:\n            self._do_detection_eval(res_file, output_dir)\n        # Optionally cleanup results json file\n'"
lib/dataset/dataset_factory.py,4,"b'from lib.dataset import voc\nfrom lib.dataset import coco\n\ndataset_map = {\n                \'voc\': voc.VOCDetection,\n                \'coco\': coco.COCODetection,\n            }\n\ndef gen_dataset_fn(name):\n    """"""Returns a dataset func.\n\n    Args:\n    name: The name of the dataset.\n\n    Returns:\n    func: dataset_fn\n\n    Raises:\n    ValueError: If network `name` is not recognized.\n    """"""\n    if name not in dataset_map:\n        raise ValueError(\'The dataset unknown %s\' % name)\n    func = dataset_map[name]\n    return func\n\n\nimport torch\nimport numpy as np\n\ndef detection_collate(batch):\n    """"""Custom collate fn for dealing with batches of images that have a different\n    number of associated object annotations (bounding boxes).\n\n    Arguments:\n        batch: (tuple) A tuple of tensor images and lists of annotations\n\n    Return:\n        A tuple containing:\n            1) (tensor) batch of images stacked on their 0 dim\n            2) (list of tensors) annotations for a given image are stacked on 0 dim\n    """"""\n    targets = []\n    imgs = []\n    for _, sample in enumerate(batch):\n        for _, tup in enumerate(sample):\n            if torch.is_tensor(tup):\n                imgs.append(tup)\n            elif isinstance(tup, type(np.empty(0))):\n                annos = torch.from_numpy(tup).float()\n                targets.append(annos)\n\n    return (torch.stack(imgs, 0), targets)\n\nfrom lib.utils.data_augment import preproc\nimport torch.utils.data as data\n\ndef load_data(cfg, phase):\n    if phase == \'train\':\n        dataset = dataset_map[cfg.DATASET](cfg.DATASET_DIR, cfg.TRAIN_SETS, preproc(cfg.IMAGE_SIZE, cfg.PIXEL_MEANS, cfg.PROB))\n        data_loader = data.DataLoader(dataset, cfg.TRAIN_BATCH_SIZE, num_workers=cfg.NUM_WORKERS,\n                                  shuffle=True, collate_fn=detection_collate, pin_memory=True)\n    if phase == \'eval\':\n        dataset = dataset_map[cfg.DATASET](cfg.DATASET_DIR, cfg.TEST_SETS, preproc(cfg.IMAGE_SIZE, cfg.PIXEL_MEANS, -1))\n        data_loader = data.DataLoader(dataset, cfg.TEST_BATCH_SIZE, num_workers=cfg.NUM_WORKERS,\n                                  shuffle=False, collate_fn=detection_collate, pin_memory=True)\n    if phase == \'test\':\n        dataset = dataset_map[cfg.DATASET](cfg.DATASET_DIR, cfg.TEST_SETS, preproc(cfg.IMAGE_SIZE, cfg.PIXEL_MEANS, -2))\n        data_loader = data.DataLoader(dataset, cfg.TEST_BATCH_SIZE, num_workers=cfg.NUM_WORKERS,\n                                  shuffle=False, collate_fn=detection_collate, pin_memory=True)\n    if phase == \'visualize\':\n        dataset = dataset_map[cfg.DATASET](cfg.DATASET_DIR, cfg.TEST_SETS, preproc(cfg.IMAGE_SIZE, cfg.PIXEL_MEANS, 1))\n        data_loader = data.DataLoader(dataset, cfg.TEST_BATCH_SIZE, num_workers=cfg.NUM_WORKERS,\n                                  shuffle=False, collate_fn=detection_collate, pin_memory=True)\n    return data_loader\n'"
lib/dataset/voc.py,2,"b'import os\nimport pickle\nimport os.path\nimport sys\nimport torch\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nfrom PIL import Image, ImageDraw, ImageFont\nimport cv2\nimport numpy as np\nfrom .voc_eval import voc_eval\nif sys.version_info[0] == 2:\n    import xml.etree.cElementTree as ET\nelse:\n    import xml.etree.ElementTree as ET\n\n\nVOC_CLASSES = ( \'__background__\', # always index 0\n    \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n    \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n    \'cow\', \'diningtable\', \'dog\', \'horse\',\n    \'motorbike\', \'person\', \'pottedplant\',\n    \'sheep\', \'sofa\', \'train\', \'tvmonitor\')\n\n# for making bounding boxes pretty\nCOLORS = ((255, 0, 0, 128), (0, 255, 0, 128), (0, 0, 255, 128),\n          (0, 255, 255, 128), (255, 0, 255, 128), (255, 255, 0, 128))\n\n\nclass VOCSegmentation(data.Dataset):\n\n    """"""VOC Segmentation Dataset Object\n    input and target are both images\n\n    NOTE: need to address https://github.com/pytorch/vision/issues/9\n\n    Arguments:\n        root (string): filepath to VOCdevkit folder.\n        image_set (string): imageset to use (eg: \'train\', \'val\', \'test\').\n        transform (callable, optional): transformation to perform on the\n            input image\n        target_transform (callable, optional): transformation to perform on the\n            target image\n        dataset_name (string, optional): which dataset to load\n            (default: \'VOC2007\')\n    """"""\n\n    def __init__(self, root, image_set, transform=None, target_transform=None,\n                 dataset_name=\'VOC2007\'):\n        self.root = root\n        self.image_set = image_set\n        self.transform = transform\n        self.target_transform = target_transform\n\n        self._annopath = os.path.join(\n            self.root, dataset_name, \'SegmentationClass\', \'%s.png\')\n        self._imgpath = os.path.join(\n            self.root, dataset_name, \'JPEGImages\', \'%s.jpg\')\n        self._imgsetpath = os.path.join(\n            self.root, dataset_name, \'ImageSets\', \'Segmentation\', \'%s.txt\')\n\n        with open(self._imgsetpath % self.image_set) as f:\n            self.ids = f.readlines()\n        self.ids = [x.strip(\'\\n\') for x in self.ids]\n\n    def __getitem__(self, index):\n        img_id = self.ids[index]\n\n        target = Image.open(self._annopath % img_id).convert(\'RGB\')\n        img = Image.open(self._imgpath % img_id).convert(\'RGB\')\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.ids)\n\n\nclass AnnotationTransform(object):\n\n    """"""Transforms a VOC annotation into a Tensor of bbox coords and label index\n    Initilized with a dictionary lookup of classnames to indexes\n\n    Arguments:\n        class_to_ind (dict, optional): dictionary lookup of classnames -> indexes\n            (default: alphabetic indexing of VOC\'s 20 classes)\n        keep_difficult (bool, optional): keep difficult instances or not\n            (default: False)\n        height (int): height\n        width (int): width\n    """"""\n\n    def __init__(self, class_to_ind=None, keep_difficult=True):\n        self.class_to_ind = class_to_ind or dict(\n            zip(VOC_CLASSES, range(len(VOC_CLASSES))))\n        self.keep_difficult = keep_difficult\n\n    def __call__(self, target):\n        """"""\n        Arguments:\n            target (annotation) : the target annotation to be made usable\n                will be an ET.Element\n        Returns:\n            a list containing lists of bounding boxes  [bbox coords, class name]\n        """"""\n        res = np.empty((0,5)) \n        for obj in target.iter(\'object\'):\n            difficult = int(obj.find(\'difficult\').text) == 1\n            if not self.keep_difficult and difficult:\n                continue\n            name = obj.find(\'name\').text.lower().strip()\n            bbox = obj.find(\'bndbox\')\n\n            pts = [\'xmin\', \'ymin\', \'xmax\', \'ymax\']\n            bndbox = []\n            for i, pt in enumerate(pts):\n                cur_pt = int(bbox.find(pt).text) - 1\n                # scale height or width\n                #cur_pt = cur_pt / width if i % 2 == 0 else cur_pt / height\n                bndbox.append(cur_pt)\n            label_idx = self.class_to_ind[name]\n            bndbox.append(label_idx)\n            res = np.vstack((res,bndbox))  # [xmin, ymin, xmax, ymax, label_ind]\n            # img_id = target.find(\'filename\').text[:-4]\n\n        return res  # [[xmin, ymin, xmax, ymax, label_ind], ... ]\n\n\nclass VOCDetection(data.Dataset):\n\n    """"""VOC Detection Dataset Object\n\n    input is image, target is annotation\n\n    Arguments:\n        root (string): filepath to VOCdevkit folder.\n        image_set (string): imageset to use (eg. \'train\', \'val\', \'test\')\n        transform (callable, optional): transformation to perform on the\n            input image\n        target_transform (callable, optional): transformation to perform on the\n            target `annotation`\n            (eg: take in caption string, return tensor of word indices)\n        dataset_name (string, optional): which dataset to load\n            (default: \'VOC2007\')\n    """"""\n\n    def __init__(self, root, image_sets, preproc=None, target_transform=AnnotationTransform(),\n                 dataset_name=\'VOC0712\'):\n        self.root = root\n        self.image_set = image_sets\n        self.preproc = preproc\n        self.target_transform = target_transform\n        self.name = dataset_name\n        self._annopath = os.path.join(\'%s\', \'Annotations\', \'%s.xml\')\n        self._imgpath = os.path.join(\'%s\', \'JPEGImages\', \'%s.jpg\')\n        self.ids = list()\n        for (year, name) in image_sets:\n            self._year = year\n            rootpath = os.path.join(self.root, \'VOC\' + year)\n            for line in open(os.path.join(rootpath, \'ImageSets\', \'Main\', name + \'.txt\')):\n                self.ids.append((rootpath, line.strip()))\n\n    def __getitem__(self, index):\n        img_id = self.ids[index]\n        target = ET.parse(self._annopath % img_id).getroot()\n        img = cv2.imread(self._imgpath % img_id, cv2.IMREAD_COLOR)\n        height, width, _ = img.shape\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n\n        if self.preproc is not None:\n            img, target = self.preproc(img, target)\n            #print(img.size())\n\n                    # target = self.target_transform(target, width, height)\n        #print(target.shape)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.ids)\n\n    def pull_image(self, index):\n        \'\'\'Returns the original image object at index in PIL form\n\n        Note: not using self.__getitem__(), as any transformations passed in\n        could mess up this functionality.\n\n        Argument:\n            index (int): index of img to show\n        Return:\n            PIL img\n        \'\'\'\n        img_id = self.ids[index]\n        return cv2.imread(self._imgpath % img_id, cv2.IMREAD_COLOR)\n\n    def pull_anno(self, index):\n        \'\'\'Returns the original annotation of image at index\n\n        Note: not using self.__getitem__(), as any transformations passed in\n        could mess up this functionality.\n\n        Argument:\n            index (int): index of img to get annotation of\n        Return:\n            list:  [img_id, [(label, bbox coords),...]]\n                eg: (\'001718\', [(\'dog\', (96, 13, 438, 332))])\n        \'\'\'\n        img_id = self.ids[index]\n        anno = ET.parse(self._annopath % img_id).getroot()\n        # gt = self.target_transform(anno, 1, 1)\n        # gt = self.target_transform(anno)\n        # return img_id[1], gt\n        if self.target_transform is not None:\n            anno = self.target_transform(anno)\n        return anno\n        \n\n    def pull_img_anno(self, index):\n        \'\'\'Returns the original annotation of image at index\n\n        Note: not using self.__getitem__(), as any transformations passed in\n        could mess up this functionality.\n\n        Argument:\n            index (int): index of img to get annotation of\n        Return:\n            list:  [img_id, [(label, bbox coords),...]]\n                eg: (\'001718\', [(\'dog\', (96, 13, 438, 332))])\n        \'\'\'\n        img_id = self.ids[index]\n        img = cv2.imread(self._imgpath % img_id, cv2.IMREAD_COLOR)\n        anno = ET.parse(self._annopath % img_id).getroot()\n        gt = self.target_transform(anno)\n        height, width, _ = img.shape\n        boxes = gt[:,:-1]\n        labels = gt[:,-1]\n        boxes[:, 0::2] /= width\n        boxes[:, 1::2] /= height\n        labels = np.expand_dims(labels,1)\n        targets = np.hstack((boxes,labels))\n        \n        return img, targets\n\n    def pull_tensor(self, index):\n        \'\'\'Returns the original image at an index in tensor form\n\n        Note: not using self.__getitem__(), as any transformations passed in\n        could mess up this functionality.\n\n        Argument:\n            index (int): index of img to show\n        Return:\n            tensorized version of img, squeezed\n        \'\'\'\n        to_tensor = transforms.ToTensor()\n        return torch.Tensor(self.pull_image(index)).unsqueeze_(0)\n\n    def evaluate_detections(self, all_boxes, output_dir=None):\n        """"""\n        all_boxes is a list of length number-of-classes.\n        Each list element is a list of length number-of-images.\n        Each of those list elements is either an empty list []\n        or a numpy array of detection.\n\n        all_boxes[class][image] = [] or np.array of shape #dets x 5\n        """"""\n        self._write_voc_results_file(all_boxes)\n        aps,map = self._do_python_eval(output_dir)\n        return aps,map\n\n    def _get_voc_results_file_template(self):\n        filename = \'comp4_det_test\' + \'_{:s}.txt\'\n        filedir = os.path.join(\n            self.root, \'results\', \'VOC\' + self._year, \'Main\')\n        if not os.path.exists(filedir):\n            os.makedirs(filedir)\n        path = os.path.join(filedir, filename)\n        return path\n\n    def _write_voc_results_file(self, all_boxes):\n        for cls_ind, cls in enumerate(VOC_CLASSES):\n            cls_ind = cls_ind \n            if cls == \'__background__\':\n                continue\n            print(\'Writing {} VOC results file\'.format(cls))\n            filename = self._get_voc_results_file_template().format(cls)\n            with open(filename, \'wt\') as f:\n                for im_ind, index in enumerate(self.ids):\n                    index = index[1]\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in range(dets.shape[0]):\n                        f.write(\'{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n\'.\n                                format(index, dets[k, -1],\n                                dets[k, 0] + 1, dets[k, 1] + 1,\n                                dets[k, 2] + 1, dets[k, 3] + 1))\n\n    def _do_python_eval(self, output_dir=\'output\'):\n        rootpath = os.path.join(self.root, \'VOC\' + self._year)\n        name = self.image_set[0][1]\n        annopath = os.path.join(\n                                rootpath,\n                                \'Annotations\',\n                                \'{:s}.xml\')\n        imagesetfile = os.path.join(\n                                rootpath,\n                                \'ImageSets\',\n                                \'Main\',\n                                name+\'.txt\')\n        cachedir = os.path.join(self.root, \'annotations_cache\')\n        aps = []\n        # The PASCAL VOC metric changed in 2010\n        use_07_metric = True if int(self._year) < 2010 else False\n        print(\'VOC07 metric? \' + (\'Yes\' if use_07_metric else \'No\'))\n        if output_dir is not None and not os.path.isdir(output_dir):\n            os.mkdir(output_dir)\n        for i, cls in enumerate(VOC_CLASSES):\n\n            if cls == \'__background__\':\n                continue\n\n            filename = self._get_voc_results_file_template().format(cls)\n            rec, prec, ap = voc_eval(\n                                    filename, annopath, imagesetfile, cls, cachedir, ovthresh=0.5,\n                                    use_07_metric=use_07_metric)\n            aps += [ap]\n            print(\'AP for {} = {:.4f}\'.format(cls, ap))\n            if output_dir is not None:\n                with open(os.path.join(output_dir, cls + \'_pr.pkl\'), \'wb\') as f:\n                    pickle.dump({\'rec\': rec, \'prec\': prec, \'ap\': ap}, f)\n        print(\'Mean AP = {:.4f}\'.format(np.mean(aps)))\n        print(\'~~~~~~~~\')\n        print(\'Results:\')\n        for ap in aps:\n            print(\'{:.3f}\'.format(ap))\n        print(\'{:.3f}\'.format(np.mean(aps)))\n        print(\'~~~~~~~~\')\n        print(\'\')\n        print(\'--------------------------------------------------------------\')\n        print(\'Results computed with the **unofficial** Python eval code.\')\n        print(\'Results should be very close to the official MATLAB eval code.\')\n        print(\'Recompute with `./tools/reval.py --matlab ...` for your paper.\')\n        print(\'-- Thanks, The Management\')\n        print(\'--------------------------------------------------------------\')\n        return aps,np.mean(aps)\n\n    def show(self, index):\n        img, target = self.__getitem__(index)\n        for obj in target:\n            obj = obj.astype(np.int)\n            cv2.rectangle(img, (obj[0], obj[1]), (obj[2], obj[3]), (255,0,0), 3)\n        cv2.imwrite(\'./image.jpg\', img)\n\n\n\n\n## test\n# if __name__ == \'__main__\':\n#     ds = VOCDetection(\'../../../../../dataset/VOCdevkit/\', [(\'2012\', \'train\')],\n#             None, AnnotationTransform())\n#     print(len(ds))\n#     img, target = ds[0]\n#     print(target)\n#     ds.show(1)'"
lib/dataset/voc_eval.py,0,"b'# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Bharath Hariharan\n# --------------------------------------------------------\n\nimport xml.etree.ElementTree as ET\nimport os\nimport pickle\nimport numpy as np\nimport pdb\n\n\ndef parse_rec(filename):\n    """""" Parse a PASCAL VOC xml file """"""\n    tree = ET.parse(filename)\n    objects = []\n    for obj in tree.findall(\'object\'):\n        obj_struct = {}\n        obj_struct[\'name\'] = obj.find(\'name\').text\n        obj_struct[\'pose\'] = obj.find(\'pose\').text\n        obj_struct[\'truncated\'] = int(obj.find(\'truncated\').text)\n        obj_struct[\'difficult\'] = int(obj.find(\'difficult\').text)\n        bbox = obj.find(\'bndbox\')\n        obj_struct[\'bbox\'] = [int(bbox.find(\'xmin\').text),\n                              int(bbox.find(\'ymin\').text),\n                              int(bbox.find(\'xmax\').text),\n                              int(bbox.find(\'ymax\').text)]\n        objects.append(obj_struct)\n\n    return objects\n\n\n\ndef voc_ap(rec, prec, use_07_metric=False):\n    """""" ap = voc_ap(rec, prec, [use_07_metric])\n    Compute VOC AP given precision and recall.\n    If use_07_metric is true, uses the\n    VOC 07 11 point method (default:False).\n    """"""\n    if use_07_metric:\n        # 11 point metric\n        ap = 0.\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(rec >= t) == 0:\n                p = 0\n            else:\n                p = np.max(prec[rec >= t])\n            ap = ap + p / 11.\n    else:\n        # correct AP calculation\n        # first append sentinel values at the end\n        mrec = np.concatenate(([0.], rec, [1.]))\n        mpre = np.concatenate(([0.], prec, [0.]))\n\n        # compute the precision envelope\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # to calculate area under PR curve, look for points\n        # where X axis (recall) changes value\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # and sum (\\Delta recall) * prec\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n\ndef voc_eval(detpath,\n             annopath,\n             imagesetfile,\n             classname,\n             cachedir,\n             ovthresh=0.5,\n             use_07_metric=False):\n    """"""rec, prec, ap = voc_eval(detpath,\n                                annopath,\n                                imagesetfile,\n                                classname,\n                                [ovthresh],\n                                [use_07_metric])\n\n    Top level function that does the PASCAL VOC evaluation.\n\n    detpath: Path to detections\n        detpath.format(classname) should produce the detection results file.\n    annopath: Path to annotations\n        annopath.format(imagename) should be the xml annotations file.\n    imagesetfile: Text file containing the list of images, one image per line.\n    classname: Category name (duh)\n    cachedir: Directory for caching the annotations\n    [ovthresh]: Overlap threshold (default = 0.5)\n    [use_07_metric]: Whether to use VOC07\'s 11 point AP computation\n        (default False)\n    """"""\n    # assumes detections are in detpath.format(classname)\n    # assumes annotations are in annopath.format(imagename)\n    # assumes imagesetfile is a text file with each line an image name\n    # cachedir caches the annotations in a pickle file\n\n    # first load gt\n    if not os.path.isdir(cachedir):\n        os.mkdir(cachedir)\n    cachefile = os.path.join(cachedir, \'annots.pkl\')\n    # read list of images\n    with open(imagesetfile, \'r\') as f:\n        lines = f.readlines()\n    imagenames = [x.strip() for x in lines]\n\n    if not os.path.isfile(cachefile):\n        # load annots\n        recs = {}\n        for i, imagename in enumerate(imagenames):\n            recs[imagename] = parse_rec(annopath.format(imagename))\n            if i % 100 == 0:\n                print(\'Reading annotation for {:d}/{:d}\'.format(\n                    i + 1, len(imagenames)))\n        # save\n        print(\'Saving cached annotations to {:s}\'.format(cachefile))\n        with open(cachefile, \'wb\') as f:\n            pickle.dump(recs, f)\n    else:\n        # load\n        with open(cachefile, \'rb\') as f:\n            recs = pickle.load(f)\n\n    # extract gt objects for this class\n    class_recs = {}\n    npos = 0\n    for imagename in imagenames:\n        R = [obj for obj in recs[imagename] if obj[\'name\'] == classname]\n        bbox = np.array([x[\'bbox\'] for x in R])\n        difficult = np.array([x[\'difficult\'] for x in R]).astype(np.bool)\n        det = [False] * len(R)\n        npos = npos + sum(~difficult)\n        class_recs[imagename] = {\'bbox\': bbox,\n                                 \'difficult\': difficult,\n                                 \'det\': det}\n\n    # read dets\n    detfile = detpath.format(classname)\n    with open(detfile, \'r\') as f:\n        lines = f.readlines()\n\n    splitlines = [x.strip().split(\' \') for x in lines]\n    image_ids = [x[0] for x in splitlines]\n    confidence = np.array([float(x[1]) for x in splitlines])\n    BB = np.array([[float(z) for z in x[2:]] for x in splitlines])\n\n        # sort by confidence\n    sorted_ind = np.argsort(-confidence)\n    sorted_scores = np.sort(-confidence)\n    BB = BB[sorted_ind, :]\n    image_ids = [image_ids[x] for x in sorted_ind]\n\n        # go down dets and mark TPs and FPs\n    nd = len(image_ids)\n    tp = np.zeros(nd)\n    fp = np.zeros(nd)\n    for d in range(nd):\n        R = class_recs[image_ids[d]]\n        bb = BB[d, :].astype(float)\n        ovmax = -np.inf\n        BBGT = R[\'bbox\'].astype(float)\n\n        if BBGT.size > 0:\n            # compute overlaps\n            # intersection\n            ixmin = np.maximum(BBGT[:, 0], bb[0])\n            iymin = np.maximum(BBGT[:, 1], bb[1])\n            ixmax = np.minimum(BBGT[:, 2], bb[2])\n            iymax = np.minimum(BBGT[:, 3], bb[3])\n            iw = np.maximum(ixmax - ixmin + 1., 0.)\n            ih = np.maximum(iymax - iymin + 1., 0.)\n            inters = iw * ih\n\n                # union\n            uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +\n                   (BBGT[:, 2] - BBGT[:, 0] + 1.) *\n                   (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)\n\n            overlaps = inters / uni\n            ovmax = np.max(overlaps)\n            jmax = np.argmax(overlaps)\n\n        if ovmax > ovthresh:\n            if not R[\'difficult\'][jmax]:\n                if not R[\'det\'][jmax]:\n                    tp[d] = 1.\n                    R[\'det\'][jmax] = 1\n                else:\n                    fp[d] = 1.\n        else:\n            fp[d] = 1.\n\n        # compute precision recall\n    fp = np.cumsum(fp)\n    tp = np.cumsum(tp)\n    rec = tp / float(npos)\n        # avoid divide by zero in case the first detection matches a difficult\n        # ground truth\n    prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n    ap = voc_ap(rec, prec, use_07_metric)\n\n    return rec, prec, ap\n'"
lib/layers/__init__.py,0,b'from .functions import *\nfrom .modules import *\n'
lib/modeling/__init__.py,0,b''
lib/modeling/model_builder.py,2,"b""\n# ssds part\nfrom lib.modeling.ssds import ssd\nfrom lib.modeling.ssds import ssd_lite\nfrom lib.modeling.ssds import rfb\nfrom lib.modeling.ssds import rfb_lite\nfrom lib.modeling.ssds import fssd\nfrom lib.modeling.ssds import fssd_lite\nfrom lib.modeling.ssds import yolo\n\nssds_map = {\n                'ssd': ssd.build_ssd,\n                'ssd_lite': ssd_lite.build_ssd_lite,\n                'rfb': rfb.build_rfb,\n                'rfb_lite': rfb_lite.build_rfb_lite,\n                'fssd': fssd.build_fssd,\n                'fssd_lite': fssd_lite.build_fssd_lite,\n                'yolo_v2': yolo.build_yolo_v2,\n                'yolo_v3': yolo.build_yolo_v3,\n            }\n\n# nets part\nfrom lib.modeling.nets import vgg\nfrom lib.modeling.nets import resnet\nfrom lib.modeling.nets import mobilenet\nfrom lib.modeling.nets import darknet\nnetworks_map = {\n                    'vgg16': vgg.vgg16,\n                    'resnet_18': resnet.resnet_18,\n                    'resnet_34': resnet.resnet_34,\n                    'resnet_50': resnet.resnet_50,\n                    'resnet_101': resnet.resnet_101,\n                    'mobilenet_v1': mobilenet.mobilenet_v1,\n                    'mobilenet_v1_075': mobilenet.mobilenet_v1_075,\n                    'mobilenet_v1_050': mobilenet.mobilenet_v1_050,\n                    'mobilenet_v1_025': mobilenet.mobilenet_v1_025,\n                    'mobilenet_v2': mobilenet.mobilenet_v2,\n                    'mobilenet_v2_075': mobilenet.mobilenet_v2_075,\n                    'mobilenet_v2_050': mobilenet.mobilenet_v2_050,\n                    'mobilenet_v2_025': mobilenet.mobilenet_v2_025,\n                    'darknet_19': darknet.darknet_19,\n                    'darknet_53': darknet.darknet_53,\n               }\n\nfrom lib.layers.functions.prior_box import PriorBox\nimport torch\n\ndef _forward_features_size(model, img_size):\n    model.eval()\n    x = torch.rand(1, 3, img_size[0], img_size[1])\n    x = torch.autograd.Variable(x, volatile=True) #.cuda()\n    feature_maps = model(x, phase='feature')\n    return [(o.size()[2], o.size()[3]) for o in feature_maps]\n\n\ndef create_model(cfg):\n    '''\n    '''\n    #\n    base = networks_map[cfg.NETS]\n    number_box= [2*len(aspect_ratios) if isinstance(aspect_ratios[0], int) else len(aspect_ratios) for aspect_ratios in cfg.ASPECT_RATIOS]  \n        \n    model = ssds_map[cfg.SSDS](base=base, feature_layer=cfg.FEATURE_LAYER, mbox=number_box, num_classes=cfg.NUM_CLASSES)\n    #\n    feature_maps = _forward_features_size(model, cfg.IMAGE_SIZE)\n    print('==>Feature map size:')\n    print(feature_maps)\n    # \n    priorbox = PriorBox(image_size=cfg.IMAGE_SIZE, feature_maps=feature_maps, aspect_ratios=cfg.ASPECT_RATIOS, \n                    scale=cfg.SIZES, archor_stride=cfg.STEPS, clip=cfg.CLIP)\n    # priors = Variable(priorbox.forward(), volatile=True)\n\n    return model, priorbox"""
lib/utils/__init__.py,0,b''
lib/utils/box_utils.py,33,"b'import torch\nimport torch.nn as nn\nimport math\nimport numpy as np\nif torch.cuda.is_available():\n    import torch.backends.cudnn as cudnn\n    torch.set_default_tensor_type(\'torch.cuda.FloatTensor\')\n\n\ndef point_form(boxes):\n    """""" Convert prior_boxes to (xmin, ymin, xmax, ymax)\n    representation for comparison to point form ground truth data.\n    Args:\n        boxes: (tensor) center-size default boxes from priorbox layers.\n    Return:\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n    """"""\n    return torch.cat((boxes[:, :2] - boxes[:, 2:]/2,     # xmin, ymin\n                     boxes[:, :2] + boxes[:, 2:]/2), 1)  # xmax, ymax\n\n\ndef center_size(boxes):\n    """""" Convert prior_boxes to (cx, cy, w, h)\n    representation for comparison to center-size form ground truth data.\n    Args:\n        boxes: (tensor) point_form boxes\n    Return:\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n    """"""\n    return torch.cat((boxes[:, 2:] + boxes[:, :2])/2,  # cx, cy\n                     boxes[:, 2:] - boxes[:, :2], 1)  # w, h\n\n\ndef intersect(box_a, box_b):\n    """""" We resize both tensors to [A,B,2] without new malloc:\n    [A,2] -> [A,1,2] -> [A,B,2]\n    [B,2] -> [1,B,2] -> [A,B,2]\n    Then we compute the area of intersect between box_a and box_b.\n    Args:\n      box_a: (tensor) bounding boxes, Shape: [A,4].\n      box_b: (tensor) bounding boxes, Shape: [B,4].\n    Return:\n      (tensor) intersection area, Shape: [A,B].\n    """"""\n    A = box_a.size(0)\n    B = box_b.size(0)\n    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n    inter = torch.clamp((max_xy - min_xy), min=0)\n    return inter[:, :, 0] * inter[:, :, 1]\n\n\ndef jaccard(box_a, box_b):\n    """"""Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.  Here we operate on\n    ground truth boxes and default boxes.\n    E.g.:\n        A \xe2\x88\xa9 B / A \xe2\x88\xaa B = A \xe2\x88\xa9 B / (area(A) + area(B) - A \xe2\x88\xa9 B)\n    Args:\n        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n    Return:\n        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n    """"""\n    inter = intersect(box_a, box_b)\n    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n              (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n    area_b = ((box_b[:, 2]-box_b[:, 0]) *\n              (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n    union = area_a + area_b - inter\n    return inter / union  # [A,B]\n\ndef matrix_iou(a,b):\n    """"""\n    return iou of a and b, numpy version for data augenmentation\n    """"""\n    lt = np.maximum(a[:, np.newaxis, :2], b[:, :2])\n    rb = np.minimum(a[:, np.newaxis, 2:], b[:, 2:])\n\n    area_i = np.prod(rb - lt, axis=2) * (lt < rb).all(axis=2)\n    area_a = np.prod(a[:, 2:] - a[:, :2], axis=1)\n    area_b = np.prod(b[:, 2:] - b[:, :2], axis=1)\n    return area_i / (area_a[:, np.newaxis] + area_b - area_i)\n\n\ndef match(threshold, truths, priors, variances, labels, loc_t, conf_t, idx):\n    """"""Match each prior box with the ground truth box of the highest jaccard\n    overlap, encode the bounding boxes, then return the matched indices\n    corresponding to both confidence and location preds.\n    Args:\n        threshold: (float) The overlap threshold used when mathing boxes.\n        truths: (tensor) Ground truth boxes, Shape: [num_obj, num_priors].\n        priors: (tensor) Prior boxes from priorbox layers, Shape: [n_priors,4].\n        variances: (tensor) Variances corresponding to each prior coord,\n            Shape: [num_priors, 4].\n        labels: (tensor) All the class labels for the image, Shape: [num_obj].\n        loc_t: (tensor) Tensor to be filled w/ endcoded location targets.\n        conf_t: (tensor) Tensor to be filled w/ matched indices for conf preds.\n        idx: (int) current batch index\n    Return:\n        The matched indices corresponding to 1)location and 2)confidence preds.\n    """"""\n    # jaccard index\n    overlaps = jaccard(\n        truths,\n        point_form(priors)\n    )\n    # (Bipartite Matching)\n    # [1,num_objects] best prior for each ground truth\n    best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True)\n    # [1,num_priors] best ground truth for each prior\n    best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True)\n    best_truth_idx.squeeze_(0)\n    best_truth_overlap.squeeze_(0)\n    best_prior_idx.squeeze_(1)\n    best_prior_overlap.squeeze_(1)\n    best_truth_overlap.index_fill_(0, best_prior_idx, 2)  # ensure best prior\n    # TODO refactor: index  best_prior_idx with long tensor\n    # ensure every gt matches with its prior of max overlap\n    for j in range(best_prior_idx.size(0)):\n        best_truth_idx[best_prior_idx[j]] = j\n    matches = truths[best_truth_idx]          # Shape: [num_priors,4]\n    conf = labels[best_truth_idx]          # Shape: [num_priors]\n    conf[best_truth_overlap < threshold] = 0  # label as background\n    loc = encode(matches, priors, variances)\n    loc_t[idx] = loc    # [num_priors,4] encoded offsets to learn\n    conf_t[idx] = conf  # [num_priors] top class label for each prior\n\ndef encode(matched, priors, variances):\n    """"""Encode the variances from the priorbox layers into the ground truth boxes\n    we have matched (based on jaccard overlap) with the prior boxes.\n    Args:\n        matched: (tensor) Coords of ground truth for each prior in point-form\n            Shape: [num_priors, 4].\n        priors: (tensor) Prior boxes in center-offset form\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        encoded boxes (tensor), Shape: [num_priors, 4]\n    """"""\n\n    # dist b/t match center and prior\'s center\n    g_cxcy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2]\n    # encode variance\n    g_cxcy /= (variances[0] * priors[:, 2:])\n    # match wh / prior wh\n    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n    g_wh = torch.log(g_wh) / variances[1]\n    # return target for smooth_l1_loss\n    return torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]\n\n\ndef encode_multi(matched, priors, offsets, variances):\n    """"""Encode the variances from the priorbox layers into the ground truth boxes\n    we have matched (based on jaccard overlap) with the prior boxes.\n    Args:\n        matched: (tensor) Coords of ground truth for each prior in point-form\n            Shape: [num_priors, 4].\n        priors: (tensor) Prior boxes in center-offset form\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        encoded boxes (tensor), Shape: [num_priors, 4]\n    """"""\n\n    # dist b/t match center and prior\'s center\n    g_cxcy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2] - offsets[:,:2]\n    # encode variance\n    #g_cxcy /= (variances[0] * priors[:, 2:])\n    g_cxcy.div_(variances[0] * offsets[:, 2:])\n    # match wh / prior wh\n    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n    g_wh = torch.log(g_wh) / variances[1]\n    # return target for smooth_l1_loss\n    return torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]\n\n# Adapted from https://github.com/Hakuyume/chainer-ssd\ndef decode(loc, priors, variances):\n    """"""Decode locations from predictions using priors to undo\n    the encoding we did for offset regression at train time.\n    Args:\n        loc (tensor): location predictions for loc layers,\n            Shape: [num_priors,4]\n        priors (tensor): Prior boxes in center-offset form.\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        decoded bounding box predictions\n    """"""\n\n    boxes = torch.cat((\n        priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n        priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)\n    boxes[:, :2] -= boxes[:, 2:] / 2\n    boxes[:, 2:] += boxes[:, :2]\n    return boxes\n\ndef decode_multi(loc, priors, offsets, variances):\n    """"""Decode locations from predictions using priors to undo\n    the encoding we did for offset regression at train time.\n    Args:\n        loc (tensor): location predictions for loc layers,\n            Shape: [num_priors,4]\n        priors (tensor): Prior boxes in center-offset form.\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        decoded bounding box predictions\n    """"""\n\n    boxes = torch.cat((\n        priors[:, :2] + offsets[:,:2]+ loc[:, :2] * variances[0] * offsets[:, 2:],\n        priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)\n    boxes[:, :2] -= boxes[:, 2:] / 2\n    boxes[:, 2:] += boxes[:, :2]\n    return boxes\n\ndef log_sum_exp(x):\n    """"""Utility function for computing log_sum_exp while determining\n    This will be used to determine unaveraged confidence loss across\n    all examples in a batch.\n    Args:\n        x (Variable(tensor)): conf_preds from conf layers\n    """"""\n    x_max = x.data.max()\n    return torch.log(torch.sum(torch.exp(x-x_max), 1, keepdim=True)) + x_max\n\n\ndef one_hot_embedding(labels, num_classes):\n    \'\'\'Embedding labels to one-hot form.\n    Args:\n      labels: (LongTensor) class labels, sized [N,].\n      num_classes: (int) number of classes.\n    Returns:\n      (tensor) encoded labels, sized [N,#classes].\n    \'\'\'\n    y = torch.eye(num_classes)  # [D,D]\n    return y[labels]            # [N,D]\n\n# Original author: Francisco Massa:\n# https://github.com/fmassa/object-detection.torch\n# Ported to PyTorch by Max deGroot (02/01/2017)\ndef nms(boxes, scores, overlap=0.5, top_k=200):\n    """"""Apply non-maximum suppression at test time to avoid detecting too many\n    overlapping bounding boxes for a given object.\n    Args:\n        boxes: (tensor) The location preds for the img, Shape: [num_priors,4].\n        scores: (tensor) The class predscores for the img, Shape:[num_priors].\n        overlap: (float) The overlap thresh for suppressing unnecessary boxes.\n        top_k: (int) The Maximum number of box preds to consider.\n    Return:\n        The indices of the kept boxes with respect to num_priors.\n    """"""\n\n    keep = torch.Tensor(scores.size(0)).fill_(0).long()\n    if boxes.numel() == 0:\n        return keep\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    area = torch.mul(x2 - x1, y2 - y1)\n    v, idx = scores.sort(0)  # sort in ascending order\n    # I = I[v >= 0.01]\n    idx = idx[-top_k:]  # indices of the top-k largest vals\n    xx1 = boxes.new()\n    yy1 = boxes.new()\n    xx2 = boxes.new()\n    yy2 = boxes.new()\n    w = boxes.new()\n    h = boxes.new()\n\n    # keep = torch.Tensor()\n    count = 0\n    while idx.numel() > 0:\n        i = idx[-1]  # index of current largest val\n        # keep.append(i)\n        keep[count] = i\n        count += 1\n        if idx.size(0) == 1:\n            break\n        idx = idx[:-1]  # remove kept element from view\n        # load bboxes of next highest vals\n        torch.index_select(x1, 0, idx, out=xx1)\n        torch.index_select(y1, 0, idx, out=yy1)\n        torch.index_select(x2, 0, idx, out=xx2)\n        torch.index_select(y2, 0, idx, out=yy2)\n        # store element-wise max with next highest score\n        xx1 = torch.clamp(xx1, min=x1[i])\n        yy1 = torch.clamp(yy1, min=y1[i])\n        xx2 = torch.clamp(xx2, max=x2[i])\n        yy2 = torch.clamp(yy2, max=y2[i])\n        w.resize_as_(xx2)\n        h.resize_as_(yy2)\n        w = xx2 - xx1\n        h = yy2 - yy1\n        # check sizes of xx1 and xx2.. after each iteration\n        w = torch.clamp(w, min=0.0)\n        h = torch.clamp(h, min=0.0)\n        inter = w*h\n        # IoU = i / (area(a) + area(b) - i)\n        rem_areas = torch.index_select(area, 0, idx)  # load remaining areas)\n        union = (rem_areas - inter) + area[i]\n        IoU = inter/union  # store result in iou\n        # keep only elements with an IoU <= overlap\n        idx = idx[IoU.le(overlap)]\n    return keep, count\n'"
lib/utils/config_parse.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom ast import literal_eval\nimport os\nimport os.path as osp\nimport numpy as np\n\n""""""config system.\nThis file specifies default config options. You should not\nchange values in this file. Instead, you should write a config file (in yaml)\nand use merge_cfg_from_file(yaml_file) to load it and override the default\noptions.\n""""""\n\nclass AttrDict(dict):\n\n    def __getattr__(self, name):\n        if name in self.__dict__:\n            return self.__dict__[name]\n        elif name in self:\n            return self[name]\n        else:\n            raise AttributeError(name)\n\n    def __setattr__(self, name, value):\n        if name in self.__dict__:\n            self.__dict__[name] = value\n        else:\n            self[name] = value\n\n__C = AttrDict()\n\ncfg = __C\n\n# ---------------------------------------------------------------------------- #\n# Model options\n# ---------------------------------------------------------------------------- #\n__C.MODEL = AttrDict()\n\n# Name of the base net used to extract the features\n__C.MODEL.NETS = \'vgg16\'\n\n# Name of the model used to detect boundingbox\n__C.MODEL.SSDS = \'ssd\'\n\n# Whether use half precision for the model. currently only inference support.\n__C.MODEL.HALF_PRECISION = True\n\n# image size for ssd\n__C.MODEL.IMAGE_SIZE = [300, 300]\n\n# number of the class for the model\n__C.MODEL.NUM_CLASSES = 21\n\n# FEATURE_LAYER to extract the proposed bounding box, \n# the first dimension is the feature layer/type, \n# while the second dimension is feature map channel. \n__C.MODEL.FEATURE_LAYER = [[22, 34, \'S\', \'S\', \'\', \'\'], [512, 1024, 512, 256, 256, 256]]\n\n# STEPS for the proposed bounding box, if empty the STEPS = image_size / feature_map_size\n__C.MODEL.STEPS = []\n\n# STEPS for the proposed bounding box, a list from min value to max value\n__C.MODEL.SIZES = [0.2, 0.95]\n\n# ASPECT_RATIOS for the proposed bounding box, 1 is default contains\n__C.MODEL.ASPECT_RATIOS = [[2,3], [2, 3], [2, 3], [2, 3], [2], [2]]\n\n# \n__C.MODEL.CLIP = True\n\n# FSSD setting, NUM_FUSED for fssd\n__C.MODEL.NUM_FUSED = 3\n\n\n# ---------------------------------------------------------------------------- #\n# Train options\n# ---------------------------------------------------------------------------- #\n__C.TRAIN = AttrDict()\n# The number of checkpoints kept, older ones are deleted to save space\n__C.TRAIN.CHECKPOINTS_KEPT = 10\n__C.TRAIN.CHECKPOINTS_EPOCHS = 5\n# The number of max iters\n__C.TRAIN.MAX_EPOCHS = 300\n# Minibatch size\n__C.TRAIN.BATCH_SIZE = 128\n# trainable scope and resuming scope\n__C.TRAIN.TRAINABLE_SCOPE = \'base,extras,norm,loc,conf\'\n__C.TRAIN.RESUME_SCOPE = \'\'\n\n# ---------------------------------------------------------------------------- #\n# optimizer options\n# ---------------------------------------------------------------------------- #\n__C.TRAIN.OPTIMIZER = AttrDict()\n# type of the optimizer\n__C.TRAIN.OPTIMIZER.OPTIMIZER = \'sgd\'\n# Initial learning rate\n__C.TRAIN.OPTIMIZER.LEARNING_RATE = 0.001\n# Momentum\n__C.TRAIN.OPTIMIZER.MOMENTUM = 0.9\n# Momentum_2\n__C.TRAIN.OPTIMIZER.MOMENTUM_2 = 0.99\n# epsilon\n__C.TRAIN.OPTIMIZER.EPS = 1e-8\n# Weight decay, for regularization\n__C.TRAIN.OPTIMIZER.WEIGHT_DECAY = 0.0001\n\n# ---------------------------------------------------------------------------- #\n# lr_scheduler options\n# ---------------------------------------------------------------------------- #\n__C.TRAIN.LR_SCHEDULER = AttrDict()\n# type of the LR_SCHEDULER\n__C.TRAIN.LR_SCHEDULER.SCHEDULER = \'step\'\n# Step size for reducing the learning rate\n__C.TRAIN.LR_SCHEDULER.STEPS = [1]\n# Factor for reducing the learning rate\n__C.TRAIN.LR_SCHEDULER.GAMMA = 0.98\n# warm_up epochs\n__C.TRAIN.LR_SCHEDULER.WARM_UP_EPOCHS = 0\n# The number of max iters\n__C.TRAIN.LR_SCHEDULER.MAX_EPOCHS = __C.TRAIN.MAX_EPOCHS - __C.TRAIN.LR_SCHEDULER.WARM_UP_EPOCHS\n\n# ---------------------------------------------------------------------------- #\n# Test options\n# ---------------------------------------------------------------------------- #\n__C.TEST = AttrDict()\n__C.TEST.BATCH_SIZE = __C.TRAIN.BATCH_SIZE\n__C.TEST.TEST_SCOPE = [0, 300]\n\n\n# ---------------------------------------------------------------------------- #\n# Matcher options\n# ---------------------------------------------------------------------------- #\n# matcher\n__C.MATCHER = AttrDict()\n__C.MATCHER.NUM_CLASSES = __C.MODEL.NUM_CLASSES\n__C.MATCHER.BACKGROUND_LABEL = 0\n__C.MATCHER.MATCHED_THRESHOLD = 0.5\n__C.MATCHER.UNMATCHED_THRESHOLD = 0.5\n__C.MATCHER.NEGPOS_RATIO = 3\n__C.MATCHER.VARIANCE = [0.1, 0.2]\n\n\n# ---------------------------------------------------------------------------- #\n# Post process options\n# ---------------------------------------------------------------------------- #\n# post process\n__C.POST_PROCESS = AttrDict()\n__C.POST_PROCESS.NUM_CLASSES = __C.MODEL.NUM_CLASSES\n__C.POST_PROCESS.BACKGROUND_LABEL = __C.MATCHER.BACKGROUND_LABEL\n__C.POST_PROCESS.SCORE_THRESHOLD = 0.01\n__C.POST_PROCESS.IOU_THRESHOLD = 0.6\n__C.POST_PROCESS.MAX_DETECTIONS = 100\n__C.POST_PROCESS.VARIANCE = __C.MATCHER.VARIANCE \n\n\n# ---------------------------------------------------------------------------- #\n# Dataset options\n# ---------------------------------------------------------------------------- #\n# Root directory of project\n__C.ROOT_DIR = osp.abspath(osp.join(osp.dirname(__file__), \'..\', \'..\'))\n\n__C.DATASET = AttrDict()\n# name of the dataset\n__C.DATASET.DATASET = \'voc\'\n# path of the dataset\n__C.DATASET.DATASET_DIR = osp.abspath(osp.join(__C.ROOT_DIR, \'data\'))\n# train set scope\n__C.DATASET.TRAIN_SETS = [(\'2007\', \'trainval\'), (\'2012\', \'trainval\')]\n# test set scope\n__C.DATASET.TEST_SETS = [(\'2007\', \'test\')]\n# image expand probability during train\n__C.DATASET.PROB = 0.6\n# image size\n__C.DATASET.IMAGE_SIZE = __C.MODEL.IMAGE_SIZE\n# image mean\n__C.DATASET.PIXEL_MEANS = (103.94, 116.78, 123.68)\n# train batch size\n__C.DATASET.TRAIN_BATCH_SIZE = __C.TRAIN.BATCH_SIZE\n# test batch size\n__C.DATASET.TEST_BATCH_SIZE = __C.TEST.BATCH_SIZE\n# number of workers to extract datas\n__C.DATASET.NUM_WORKERS = 8\n\n\n# ---------------------------------------------------------------------------- #\n# Export options\n# ---------------------------------------------------------------------------- #\n# Place outputs model under an experiments directory\n__C.EXP_DIR = osp.abspath(osp.join(__C.ROOT_DIR, \'experiments/models/\'))\n__C.LOG_DIR = __C.EXP_DIR\n__C.RESUME_CHECKPOINT = \'\'\n__C.CHECKPOINTS_PREFIX = \'{}_{}_{}\'.format(__C.MODEL.SSDS, __C.MODEL.NETS, __C.DATASET.DATASET)\n__C.PHASE = [\'train\', \'eval\', \'test\']\n\n# def _merge_a_into_b(a, b):\n#   """"""Merge config dictionary a into config dictionary b, clobbering the\n#   options in b whenever they are also specified in a.\n#   """"""\n#   if type(a) is not AttrDict:\n#     return\n\n#   for k, v in a.items():\n#     # a must specify keys that are in b\n#     if k not in b:\n#       raise KeyError(\'{} is not a valid config key\'.format(k))\n\n#     # the types must match, too\n#     old_type = type(b[k])\n#     if old_type is not type(v):\n#       if isinstance(b[k], np.ndarray):\n#         v = np.array(v, dtype=b[k].dtype)\n#       else:\n#         raise ValueError((\'Type mismatch ({} vs. {}) \'\n#                           \'for config key: {}\').format(type(b[k]),\n#                                                        type(v), k))\n#     # recursively merge dicts\n#     if type(v) is AttrDict:\n#       try:\n#         _merge_a_into_b(a[k], b[k])\n#       except:\n#         print((\'Error under config key: {}\'.format(k)))\n#         raise\n#     else:\n#       b[k] = v\n\ndef _merge_a_into_b(a, b, stack=None):\n    """"""Merge config dictionary a into config dictionary b, clobbering the\n    options in b whenever they are also specified in a.\n    """"""\n    assert isinstance(a, AttrDict), \'Argument `a` must be an AttrDict\'\n    assert isinstance(b, AttrDict), \'Argument `b` must be an AttrDict\'\n\n    for k, v_ in a.items():\n        full_key = \'.\'.join(stack) + \'.\' + k if stack is not None else k\n        # a must specify keys that are in b\n        if k not in b:\n          raise KeyError(\'Non-existent config key: {}\'.format(full_key))\n\n        v = _decode_cfg_value(v_)\n        v = _check_and_coerce_cfg_value_type(v, b[k], k, full_key)\n\n        # Recursively merge dicts\n        if isinstance(v, AttrDict):\n            try:\n                stack_push = [k] if stack is None else stack + [k]\n                _merge_a_into_b(v, b[k], stack=stack_push)\n            except BaseException:\n                raise\n        else:\n            b[k] = v\n\n\ndef update_cfg():\n    __C.TRAIN.LR_SCHEDULER.MAX_EPOCHS = __C.TRAIN.MAX_EPOCHS - __C.TRAIN.LR_SCHEDULER.WARM_UP_EPOCHS\n    __C.DATASET.IMAGE_SIZE = __C.MODEL.IMAGE_SIZE\n    __C.DATASET.TRAIN_BATCH_SIZE = __C.TRAIN.BATCH_SIZE\n    __C.DATASET.TEST_BATCH_SIZE = __C.TEST.BATCH_SIZE\n    __C.MATCHER.NUM_CLASSES = __C.MODEL.NUM_CLASSES\n    __C.POST_PROCESS.NUM_CLASSES = __C.MODEL.NUM_CLASSES\n    __C.POST_PROCESS.BACKGROUND_LABEL = __C.MATCHER.BACKGROUND_LABEL\n    __C.POST_PROCESS.VARIANCE = __C.MATCHER.VARIANCE \n    __C.CHECKPOINTS_PREFIX = \'{}_{}_{}\'.format(__C.MODEL.SSDS, __C.MODEL.NETS, __C.DATASET.DATASET)\n\n\ndef cfg_from_file(filename):\n    """"""Load a config file and merge it into the default options.""""""\n    import yaml\n    with open(filename, \'r\') as f:\n        yaml_cfg = AttrDict(yaml.load(f))\n\n    _merge_a_into_b(yaml_cfg, __C)\n    update_cfg()\n\ndef _decode_cfg_value(v):\n    """"""Decodes a raw config value (e.g., from a yaml config files or command\n    line argument) into a Python object.\n    """"""\n    # Configs parsed from raw yaml will contain dictionary keys that need to be\n    # converted to AttrDict objects\n    if isinstance(v, dict):\n        return AttrDict(v)\n    # All remaining processing is only applied to strings\n    if not isinstance(v, str):\n        return v\n    # Try to interpret `v` as a:\n    #   string, number, tuple, list, dict, boolean, or None\n    try:\n        v = literal_eval(v)\n    # The following two excepts allow v to pass through when it represents a\n    # string.\n    #\n    # Longer explanation:\n    # The type of v is always a string (before calling literal_eval), but\n    # sometimes it *represents* a string and other times a data structure, like\n    # a list. In the case that v represents a string, what we got back from the\n    # yaml parser is \'foo\' *without quotes* (so, not \'""foo""\'). literal_eval is\n    # ok with \'""foo""\', but will raise a ValueError if given \'foo\'. In other\n    # cases, like paths (v = \'foo/bar\' and not v = \'""foo/bar""\'), literal_eval\n    # will raise a SyntaxError.\n    except ValueError:\n        pass\n    except SyntaxError:\n        pass\n    return v\n\ndef _check_and_coerce_cfg_value_type(value_a, value_b, key, full_key):\n    """"""Checks that `value_a`, which is intended to replace `value_b` is of the\n    right type. The type is correct if it matches exactly or is one of a few\n    cases in which the type can be easily coerced.\n    """"""\n    # The types must match (with some exceptions)\n    type_b = type(value_b)\n    type_a = type(value_a)\n    if type_a is type_b:\n        return value_a\n\n    # Exceptions: numpy arrays, strings, tuple<->list\n    if isinstance(value_b, np.ndarray):\n        value_a = np.array(value_a, dtype=value_b.dtype)\n    elif isinstance(value_b, str):\n        value_a = str(value_a)\n    elif isinstance(value_a, tuple) and isinstance(value_b, list):\n        value_a = list(value_a)\n    elif isinstance(value_a, list) and isinstance(value_b, tuple):\n        value_a = tuple(value_a)\n    else:\n        raise ValueError(\n            \'Type mismatch ({} vs. {}) with values ({} vs. {}) for config \'\n            \'key: {}\'.format(type_b, type_a, value_b, value_a, full_key)\n        )\n    return value_a'"
lib/utils/dark2pth.py,9,"b""import numpy as np\n\n\ndef parse_cfg(cfgfile):\n    blocks = []\n    fp = open(cfgfile, 'r')\n    block =  None\n    line = fp.readline()\n    while line != '':\n        line = line.rstrip()\n        if line == '' or line[0] == '#':\n            line = fp.readline()\n            continue        \n        elif line[0] == '[':\n            if block:\n                blocks.append(block)\n            block = dict()\n            block['type'] = line.lstrip('[').rstrip(']')\n            # set default value\n            if block['type'] == 'convolutional':\n                block['batch_normalize'] = 0\n        else:\n            key,value = line.split('=')\n            key = key.strip()\n            if key == 'type':\n                key = '_type'\n            value = value.strip()\n            block[key] = value\n        line = fp.readline()\n\n    if block:\n        blocks.append(block)\n    fp.close()\n    return blocks\n\n\ndef print_cfg(blocks):\n    print('layer     filters    size              input                output');\n    prev_width = 416\n    prev_height = 416\n    prev_filters = 3\n    out_filters =[]\n    out_widths =[]\n    out_heights =[]\n    ind = -2\n    for block in blocks:\n        ind = ind + 1\n        if block['type'] == 'net':\n            prev_width = int(block['width'])\n            prev_height = int(block['height'])\n            continue\n        elif block['type'] == 'convolutional':\n            filters = int(block['filters'])\n            kernel_size = int(block['size'])\n            stride = int(block['stride'])\n            is_pad = int(block['pad'])\n            pad = (kernel_size-1)/2 if is_pad else 0\n            width = (prev_width + 2*pad - kernel_size)/stride + 1\n            height = (prev_height + 2*pad - kernel_size)/stride + 1\n            print('%5d %-6s %4d  %d x %d / %d   %3d x %3d x%4d   ->   %3d x %3d x%4d' % (ind, 'conv', filters, kernel_size, kernel_size, stride, prev_width, prev_height, prev_filters, width, height, filters))\n            prev_width = int(width)\n            prev_height = int(height)\n            prev_filters = filters\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block['type'] == 'maxpool':\n            pool_size = int(block['size'])\n            stride = int(block['stride'])\n            width = prev_width/stride\n            height = prev_height/stride\n            print('%5d %-6s       %d x %d / %d   %3d x %3d x%4d   ->   %3d x %3d x%4d' % (ind, 'max', pool_size, pool_size, stride, prev_width, prev_height, prev_filters, width, height, filters))\n            prev_width = int(width)\n            prev_height = int(height)\n            prev_filters = filters\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block['type'] == 'avgpool':\n            width = 1\n            height = 1\n            print('%5d %-6s                   %3d x %3d x%4d   ->  %3d' % (ind, 'avg', prev_width, prev_height, prev_filters,  prev_filters))\n            prev_width = int(width)\n            prev_height = int(height)\n            prev_filters = filters\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block['type'] == 'softmax':\n            print('%5d %-6s                                    ->  %3d' % (ind, 'softmax', prev_filters))\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block['type'] == 'cost':\n            print('%5d %-6s                                     ->  %3d' % (ind, 'cost', prev_filters))\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block['type'] == 'reorg':\n            stride = int(block['stride'])\n            filters = stride * stride * prev_filters\n            width = prev_width/stride\n            height = prev_height/stride\n            print('%5d %-6s             / %d   %3d x %3d x%4d   ->   %3d x %3d x%4d' % (ind, 'reorg', stride, prev_width, prev_height, prev_filters, width, height, filters))\n            prev_width = int(width)\n            prev_height = int(height)\n            prev_filters = filters\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block['type'] == 'route':\n            layers = block['layers'].split(',')\n            layers = [int(i) if int(i) > 0 else int(i)+ind for i in layers]\n            if len(layers) == 1:\n                print('%5d %-6s %d' % (ind, 'route', layers[0]))\n                prev_width = out_widths[layers[0]]\n                prev_height = out_heights[layers[0]]\n                prev_filters = out_filters[layers[0]]\n            elif len(layers) == 2:\n                print('%5d %-6s %d %d' % (ind, 'route', layers[0], layers[1]))\n                prev_width = out_widths[layers[0]]\n                prev_height = out_heights[layers[0]]\n                assert(prev_width == out_widths[layers[1]])\n                assert(prev_height == out_heights[layers[1]])\n                prev_filters = out_filters[layers[0]] + out_filters[layers[1]]\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block['type'] == 'region':\n            print('%5d %-6s' % (ind, 'detection'))\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block['type'] == 'yolo':\n            print('%5d %-6s' % (ind, 'yolo'))\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block['type'] == 'upsample':\n            stride = int(block['stride'])\n            filters = prev_filters\n            width = prev_width*stride\n            height = prev_height*stride\n            print('%5d %-6s           / %d   %3d x %3d x%4d   ->   %3d x %3d x%4d' % (ind, 'upsample', stride, prev_width, prev_height, prev_filters, width, height, filters))\n            prev_width = int(width)\n            prev_height = int(height)\n            prev_filters = filters\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block['type'] == 'shortcut':\n            from_id = int(block['from'])\n            from_id = from_id if from_id > 0 else from_id+ind\n            print('%5d %-6s %d' % (ind, 'shortcut', from_id))\n            prev_width = out_widths[from_id]\n            prev_height = out_heights[from_id]\n            prev_filters = out_filters[from_id]\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block['type'] == 'connected':\n            filters = int(block['output'])\n            print('%5d %-6s                            %d  ->  %3d' % (ind, 'connected', prev_filters,  filters))\n            prev_filters = filters\n            out_widths.append(1)\n            out_heights.append(1)\n            out_filters.append(prev_filters)\n        else:\n            print('unknown type %s' % (block['type']))\n\n\ndef load_conv(buf, start):\n    # start = start + 425 * 2\n    start = start + 225 * 2\n    # num_w = conv_model.weight.numel()\n    # num_b = conv_model.bias.numel()\n    # conv_model.bias.data.copy_(torch.from_numpy(buf[start:start+num_b]));   start = start + num_b\n    # conv_model.weight.data.copy_(torch.from_numpy(buf[start:start+num_w])); start = start + num_w\n    return start\n\ndef load_conv_bn(buf, start, conv_model, bn_model):\n    num_w = conv_model.weight.numel()\n    num_b = bn_model.bias.numel()\n    bn_model.bias.data.copy_(torch.from_numpy(buf[start:start+num_b]));     start = start + num_b\n    bn_model.weight.data.copy_(torch.from_numpy(buf[start:start+num_b]));   start = start + num_b\n    bn_model.running_mean.copy_(torch.from_numpy(buf[start:start+num_b]));  start = start + num_b\n    bn_model.running_var.copy_(torch.from_numpy(buf[start:start+num_b]));   start = start + num_b\n    conv_model.weight.data.copy_(torch.from_numpy(buf[start:start+num_w])); start = start + num_w \n    return start\n\ndef yolo2pth(yolo, cfgfile, weightsfile):\n    blocks = parse_cfg(cfgfile)\n    # print_cfg(blocks)\n\n    fp = open(weightsfile, 'rb')\n    header = np.fromfile(fp, count=4, dtype=np.int32)\n    buf = np.fromfile(fp, dtype = np.float32)\n    fp.close()\n\n\n    print('layer     filters    size              input                output')\n    prev_width = 416\n    prev_height = 416\n    prev_filters = 3\n    out_filters =[]\n    out_widths =[]\n    out_heights =[]\n    start = 0\n    ind = -2\n    model_index = 0\n    model_sub = 0\n    model = yolo.base\n    for block in blocks:\n        if model_index == len(yolo.base):\n            yolo.base = model\n            model = yolo.extras\n            model_index = 0\n\n        ind = ind + 1\n        if start >= buf.size:\n            break\n        if block['type'] == 'net':\n            prev_width = int(block['width'])\n            prev_height = int(block['height'])\n            continue\n        elif block['type'] == 'convolutional':\n            filters = int(block['filters'])\n            kernel_size = int(block['size'])\n            stride = int(block['stride'])\n            is_pad = int(block['pad'])\n            pad = (kernel_size-1)/2 if is_pad else 0\n            width = (prev_width + 2*pad - kernel_size)/stride + 1\n            height = (prev_height + 2*pad - kernel_size)/stride + 1\n            print('%5d %-6s %4d  %d x %d / %d   %3d x %3d x%4d   ->   %3d x %3d x%4d' % (ind, 'conv', filters, kernel_size, kernel_size, stride, prev_width, prev_height, prev_filters, width, height, filters))\n            prev_width = int(width)\n            prev_height = int(height)\n            prev_filters = filters\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n\n            batch_normalize = int(block['batch_normalize'])\n            if batch_normalize:\n                print(model_index, model[model_index].conv[model_sub*3])\n                # load weights\n                start = load_conv_bn(buf, start, model[model_index].conv[model_sub*3], model[model_index].conv[model_sub*3+1])\n\n                if len(model[model_index].conv) == 3:\n                    model_index = model_index+1\n                elif len(model[model_index].conv) == 6:\n                    model_sub = model_sub + 1\n                    if model_sub == 2:\n                        model_index = model_index+1\n                        model_sub = 0\n            else:\n                #pass classification\n                start = load_conv(buf, start)\n                # start = load_conv(buf, start, model[0])\n        elif block['type'] == 'maxpool':\n            model_index = model_index+1\n            pool_size = int(block['size'])\n            stride = int(block['stride'])\n            width = prev_width/stride\n            height = prev_height/stride\n            print('%5d %-6s       %d x %d / %d   %3d x %3d x%4d   ->   %3d x %3d x%4d' % (ind, 'max', pool_size, pool_size, stride, prev_width, prev_height, prev_filters, width, height, filters))\n            prev_width = int(width)\n            prev_height = int(height)\n            prev_filters = filters\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block['type'] == 'avgpool':\n            width = 1\n            height = 1\n            print('%5d %-6s                   %3d x %3d x%4d   ->  %3d' % (ind, 'avg', prev_width, prev_height, prev_filters,  prev_filters))\n            prev_width = int(width)\n            prev_height = int(height)\n            prev_filters = filters\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block['type'] == 'softmax':\n            print('%5d %-6s                                    ->  %3d' % (ind, 'softmax', prev_filters))\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block['type'] == 'cost':\n            print('%5d %-6s                                     ->  %3d' % (ind, 'cost', prev_filters))\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block['type'] == 'reorg':\n            stride = int(block['stride'])\n            filters = stride * stride * prev_filters\n            width = prev_width/stride\n            height = prev_height/stride\n            print('%5d %-6s             / %d   %3d x %3d x%4d   ->   %3d x %3d x%4d' % (ind, 'reorg', stride, prev_width, prev_height, prev_filters, width, height, filters))\n            prev_width = int(width)\n            prev_height = int(height)\n            prev_filters = filters\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block['type'] == 'route':\n            layers = block['layers'].split(',')\n            layers = [int(i) if int(i) > 0 else int(i)+ind for i in layers]\n            if len(layers) == 1:\n                print('%5d %-6s %d' % (ind, 'route', layers[0]))\n                prev_width = out_widths[layers[0]]\n                prev_height = out_heights[layers[0]]\n                prev_filters = out_filters[layers[0]]\n            elif len(layers) == 2:\n                print('%5d %-6s %d %d' % (ind, 'route', layers[0], layers[1]))\n                prev_width = out_widths[layers[0]]\n                prev_height = out_heights[layers[0]]\n                assert(prev_width == out_widths[layers[1]])\n                assert(prev_height == out_heights[layers[1]])\n                prev_filters = out_filters[layers[0]] + out_filters[layers[1]]\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block['type'] == 'region':\n            print('%5d %-6s' % (ind, 'detection'))\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block['type'] == 'yolo':\n            print('%5d %-6s' % (ind, 'yolo'))\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block['type'] == 'upsample':\n            stride = int(block['stride'])\n            filters = prev_filters\n            width = prev_width*stride\n            height = prev_height*stride\n            print('%5d %-6s           / %d   %3d x %3d x%4d   ->   %3d x %3d x%4d' % (ind, 'upsample', stride, prev_width, prev_height, prev_filters, width, height, filters))\n            prev_width = int(width)\n            prev_height = int(height)\n            prev_filters = filters\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block['type'] == 'shortcut':\n            from_id = int(block['from'])\n            from_id = from_id if from_id > 0 else from_id+ind\n            print('%5d %-6s %d' % (ind, 'shortcut', from_id))\n            prev_width = out_widths[from_id]\n            prev_height = out_heights[from_id]\n            prev_filters = out_filters[from_id]\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block['type'] == 'connected':\n            filters = int(block['output'])\n            print('%5d %-6s                            %d  ->  %3d' % (ind, 'connected', prev_filters,  filters))\n            prev_filters = filters\n            out_widths.append(1)\n            out_heights.append(1)\n            out_filters.append(prev_filters)\n        else:\n            print('unknown type %s' % (block['type']))\n    yolo.extras = model\n    return yolo\n\nif __name__ == '__main__':\n    from lib.modeling.ssds.yolo import *\n    from lib.modeling.nets.darknet import *\n\n    #convert yolo2\n    print('convert yolov2...')\n    feature_layer_v2 = [[['', '',12, '']], [[1024, 1024, 64, 1024]]]\n    mbox_v2 = [5]\n\n    yolo_v2 = build_yolo_v2(darknet_19, feature_layer_v2, mbox_v2, 81)\n    # print(yolo_v2.base[0].conv[1].bias)\n    # print(yolo_v2.loc[0].weight)\n    yolo_v2 = yolo2pth(yolo_v2, './weights/dark/yolov2.cfg', './weights/dark/yolov2.weights')\n    # print(yolo_v2.base[0].conv[1].bias)\n    # print(yolo_v2.loc[0].weight)\n    torch.save(yolo_v2.state_dict(), './weights/dark/yolov2.pth')\n\n\n    print('convert yolov3...')\n    feature_layer_v3 = [[['B','B','B'], [23,'B','B','B'], [14,'B','B','B']],\n                        [[1024,1024,1024], [256, 512, 512, 512], [128, 256, 256, 256]]]\n    mbox_v3 = [3, 3, 3]\n\n    yolo_v3 = build_yolo_v3(darknet_53, feature_layer_v3, mbox_v3, 81)\n    yolo_v3 = yolo2pth(yolo_v3, './weights/dark/yolov3.cfg', './weights/dark/yolov3.weights')\n    torch.save(yolo_v3.state_dict(), './weights/dark/yolov3.pth')"""
lib/utils/data_augment.py,5,"b'""""""Data augmentation functionality. Passed as callable transformations to\nDataset classes.\n\nThe data augmentation procedures were interpreted from @weiliu89\'s SSD paper\nhttp://arxiv.org/abs/1512.02325\n\nEllis Brown, Max deGroot\n""""""\n\nimport torch\nimport cv2\nimport numpy as np\nimport random\nimport math\nfrom lib.utils.box_utils import matrix_iou\n\ndef _crop(image, boxes, labels):\n    height, width, _ = image.shape\n\n    if len(boxes)== 0:\n        return image, boxes, labels\n\n    while True:\n        mode = random.choice((\n            None,\n            (0.1, None),\n            (0.3, None),\n            (0.5, None),\n            (0.7, None),\n            (0.9, None),\n            (None, None),\n        ))\n\n        if mode is None:\n            return image, boxes, labels\n\n        min_iou, max_iou = mode\n        if min_iou is None:\n            min_iou = float(\'-inf\')\n        if max_iou is None:\n            max_iou = float(\'inf\')\n\n        for _ in range(50):\n            scale = random.uniform(0.3,1.)\n            min_ratio = max(0.5, scale*scale)\n            max_ratio = min(2, 1. / scale / scale)\n            ratio = math.sqrt(random.uniform(min_ratio, max_ratio))\n            w = int(scale * ratio * width)\n            h = int((scale / ratio) * height)\n\n\n            l = random.randrange(width - w)\n            t = random.randrange(height - h)\n            roi = np.array((l, t, l + w, t + h))\n\n            iou = matrix_iou(boxes, roi[np.newaxis])\n            \n            if not (min_iou <= iou.min() and iou.max() <= max_iou):\n                continue\n\n            image_t = image[roi[1]:roi[3], roi[0]:roi[2]]\n\n            centers = (boxes[:, :2] + boxes[:, 2:]) / 2\n            mask = np.logical_and(roi[:2] < centers, centers < roi[2:]) \\\n                     .all(axis=1)\n            boxes_t = boxes[mask].copy()\n            labels_t = labels[mask].copy()\n            if len(boxes_t) == 0:\n                continue\n\n            boxes_t[:, :2] = np.maximum(boxes_t[:, :2], roi[:2])\n            boxes_t[:, :2] -= roi[:2]\n            boxes_t[:, 2:] = np.minimum(boxes_t[:, 2:], roi[2:])\n            boxes_t[:, 2:] -= roi[:2]\n\n            return image_t, boxes_t,labels_t\n\n\ndef _distort(image):\n    def _convert(image, alpha=1, beta=0):\n        tmp = image.astype(float) * alpha + beta\n        tmp[tmp < 0] = 0\n        tmp[tmp > 255] = 255\n        image[:] = tmp\n\n    image = image.copy()\n\n    if random.randrange(2):\n        _convert(image, beta=random.uniform(-32, 32))\n\n    if random.randrange(2):\n        _convert(image, alpha=random.uniform(0.5, 1.5))\n\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n    if random.randrange(2):\n        tmp = image[:, :, 0].astype(int) + random.randint(-18, 18)\n        tmp %= 180\n        image[:, :, 0] = tmp\n\n    if random.randrange(2):\n        _convert(image[:, :, 1], alpha=random.uniform(0.5, 1.5))\n\n    image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n\n    return image\n\n\ndef _expand(image, boxes, fill, p):\n    if random.random() > p:\n        return image, boxes\n\n    height, width, depth = image.shape\n    for _ in range(50):\n        scale = random.uniform(1,4)\n\n        min_ratio = max(0.5, 1./scale/scale)\n        max_ratio = min(2, scale*scale)\n        ratio = math.sqrt(random.uniform(min_ratio, max_ratio))\n        ws = scale*ratio\n        hs = scale/ratio\n        if ws < 1 or hs < 1:\n            continue\n        w = int(ws * width)\n        h = int(hs * height)\n\n        left = random.randint(0, w - width)\n        top = random.randint(0, h - height)\n\n        boxes_t = boxes.copy()\n        boxes_t[:, :2] += (left, top)\n        boxes_t[:, 2:] += (left, top)\n\n\n        expand_image = np.empty(\n            (h, w, depth),\n            dtype=image.dtype)\n        expand_image[:, :] = fill\n        expand_image[top:top + height, left:left + width] = image\n        image = expand_image\n\n        return image, boxes_t\n\n\ndef _mirror(image, boxes):\n    _, width, _ = image.shape\n    if random.randrange(2):\n        image = image[:, ::-1]\n        boxes = boxes.copy()\n        boxes[:, 0::2] = width - boxes[:, 2::-2]\n    return image, boxes\n\n\ndef _elastic(image, p, alpha=None, sigma=None, random_state=None):\n    """"""Elastic deformation of images as described in [Simard2003]_ (with modifications).\n    .. [Simard2003] Simard, Steinkraus and Platt, ""Best Practices for\n         Convolutional Neural Networks applied to Visual Document Analysis"", in\n         Proc. of the International Conference on Document Analysis and\n         Recognition, 2003.\n     Based on https://gist.github.com/erniejunior/601cdf56d2b424757de5\n     From: \n     https://www.kaggle.com/bguberfain/elastic-transform-for-data-augmentation\n    """"""\n    if random.random() > p:\n        return image\n    if alpha == None:\n        alpha = image.shape[0] * random.uniform(0.5,2)\n    if sigma == None:\n        sigma = int(image.shape[0] * random.uniform(0.5,1))\n    if random_state is None:\n        random_state = np.random.RandomState(None)\n\n    shape = image.shape[:2]\n    \n    dx, dy = [cv2.GaussianBlur((random_state.rand(*shape) * 2 - 1) * alpha, (sigma|1, sigma|1), 0) for _ in range(2)]\n    x, y = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]))\n    x, y = np.clip(x+dx, 0, shape[1]-1).astype(np.float32), np.clip(y+dy, 0, shape[0]-1).astype(np.float32)\n    return cv2.remap(image, x, y, interpolation=cv2.INTER_LINEAR, borderValue= 0, borderMode=cv2.BORDER_REFLECT)\n\n\ndef preproc_for_test(image, insize, mean):\n    interp_methods = [cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_NEAREST, cv2.INTER_LANCZOS4]\n    interp_method = interp_methods[random.randrange(5)]\n    image = cv2.resize(image, (insize[0], insize[1]),interpolation=interp_method)\n    image = image.astype(np.float32)\n    image -= mean\n    return image.transpose(2, 0, 1)\n\ndef draw_bbox(image, bbxs, color=(0, 255, 0)):\n    img = image.copy()\n    bbxs = np.array(bbxs).astype(np.int32)\n    for bbx in bbxs:\n        cv2.rectangle(img, (bbx[0], bbx[1]), (bbx[2], bbx[3]), color, 5)\n    return img\n\nclass preproc(object):\n\n    def __init__(self, resize, rgb_means, p, writer=None):\n        self.means = rgb_means\n        self.resize = resize\n        self.p = p\n        self.writer = writer # writer used for tensorboard visualization\n        self.epoch = 0\n\n    def __call__(self, image, targets=None):\n        # some bugs \n        if self.p == -2: # abs_test\n            targets = np.zeros((1,5))\n            targets[0] = image.shape[0]\n            targets[0] = image.shape[1]\n            image = preproc_for_test(image, self.resize, self.means)\n            return torch.from_numpy(image), targets\n\n        boxes = targets[:,:-1].copy()\n        labels = targets[:,-1].copy()\n        if len(boxes) == 0:\n            targets = np.zeros((1,5))\n            image = preproc_for_test(image, self.resize, self.means) # some ground truth in coco do not have bounding box! weird!\n            return torch.from_numpy(image), targets\n        if self.p == -1: # eval\n            height, width, _ = image.shape\n            boxes[:, 0::2] /= width\n            boxes[:, 1::2] /= height\n            labels = np.expand_dims(labels,1)\n            targets = np.hstack((boxes,labels))\n            image = preproc_for_test(image, self.resize, self.means)\n            return torch.from_numpy(image), targets\n\n        image_o = image.copy()\n        targets_o = targets.copy()\n        height_o, width_o, _ = image_o.shape\n        boxes_o = targets_o[:,:-1]\n        labels_o = targets_o[:,-1]\n        boxes_o[:, 0::2] /= width_o\n        boxes_o[:, 1::2] /= height_o\n        labels_o = np.expand_dims(labels_o,1)\n        targets_o = np.hstack((boxes_o,labels_o))\n\n        if self.writer is not None:\n            image_show = draw_bbox(image, boxes)\n            self.writer.add_image(\'preprocess/input_image\', image_show, self.epoch)\n\n        image_t, boxes, labels = _crop(image, boxes, labels)\n        if self.writer is not None:\n            image_show = draw_bbox(image_t, boxes)\n            self.writer.add_image(\'preprocess/crop_image\', image_show, self.epoch)\n\n        image_t = _distort(image_t)\n        if self.writer is not None:\n            image_show = draw_bbox(image_t, boxes)\n            self.writer.add_image(\'preprocess/distort_image\', image_show, self.epoch)\n        \n        # image_t = _elastic(image_t, self.p)\n        # if self.writer is not None:\n        #     image_show = draw_bbox(image_t, boxes)\n        #     self.writer.add_image(\'preprocess/elastic_image\', image_show, self.epoch)\n\n        image_t, boxes = _expand(image_t, boxes, self.means, self.p)\n        if self.writer is not None:\n            image_show = draw_bbox(image_t, boxes)\n            self.writer.add_image(\'preprocess/expand_image\', image_show, self.epoch)\n\n        image_t, boxes = _mirror(image_t, boxes)\n        if self.writer is not None:\n            image_show = draw_bbox(image_t, boxes)\n            self.writer.add_image(\'preprocess/mirror_image\', image_show, self.epoch)\n\n        # only write the preprocess step for the first image\n        if self.writer is not None:\n            # print(\'image adding\')\n            self.release_writer()\n\n        height, width, _ = image_t.shape\n        image_t = preproc_for_test(image_t, self.resize, self.means)\n        boxes = boxes.copy()\n        boxes[:, 0::2] /= width\n        boxes[:, 1::2] /= height\n        b_w = (boxes[:, 2] - boxes[:, 0])*1.\n        b_h = (boxes[:, 3] - boxes[:, 1])*1.\n        mask_b= np.minimum(b_w, b_h) > 0.01\n        boxes_t = boxes[mask_b]\n        labels_t = labels[mask_b].copy()\n\n        if len(boxes_t)==0:\n            image = preproc_for_test(image_o, self.resize, self.means)\n            return torch.from_numpy(image),targets_o\n\n        labels_t = np.expand_dims(labels_t,1)\n        targets_t = np.hstack((boxes_t,labels_t))\n\n        return torch.from_numpy(image_t), targets_t\n    \n    def add_writer(self, writer, epoch=None):\n        self.writer = writer\n        self.epoch = epoch if epoch is not None else self.epoch + 1\n    \n    def release_writer(self):\n        self.writer = None\n'"
lib/utils/data_augment_test.py,0,"b'""""""Data augmentation functionality. Passed as callable transformations to\nDataset classes.\n\nThe data augmentation procedures were interpreted from @weiliu89\'s SSD paper\nhttp://arxiv.org/abs/1512.02325\n\nEllis Brown, Max deGroot\n""""""\n\nimport cv2\nimport numpy as np\nfrom data_augment import draw_bbox,_crop,_distort,_elastic,_expand,_mirror\n\nif __name__ == \'__main__\':\n    image = cv2.imread(\'./experiments/2011_001100.jpg\')\n    boxes = np.array([np.array([124, 150, 322, 351])]) # ymin, xmin, ymax, xmax\n    labels = np.array([[1]])\n    p = 1\n\n    image_show = draw_bbox(image, boxes)\n    cv2.imshow(\'input_image\', image_show)\n\n    image_t, boxes, labels = _crop(image, boxes, labels)\n    image_show = draw_bbox(image_t, boxes)\n    cv2.imshow(\'crop_image\', image_show)\n    \n    image_t = _distort(image_t)\n    image_show = draw_bbox(image_t, boxes)\n    cv2.imshow(\'distort_image\', image_show)\n\n    image_t = _elastic(image_t, p)\n    image_show = draw_bbox(image_t, boxes)\n    cv2.imshow(\'elastic_image\', image_show)\n\n    image_t, boxes = _expand(image_t, boxes, (103.94, 116.78, 123.68), p)\n    image_show = draw_bbox(image_t, boxes)\n    cv2.imshow(\'expand_image\', image_show)\n\n    image_t, boxes = _mirror(image_t, boxes)\n    image_show = draw_bbox(image_t, boxes)\n    cv2.imshow(\'mirror_image\', image_show)\n\n    cv2.waitKey(0)'"
lib/utils/eval_utils.py,2,"b'import torch\nimport numpy as np\n\ndef iou_gt(detect, ground_turths):\n    det_size = (detect[2] - detect[0])*(detect[3] - detect[1])\n    detect = detect.resize_(1,4)\n    iou = []\n    ioa = []\n\n    for gt in ground_turths:\n        # print(ground_turths)\n        # print(\'gt\', gt)\n        # print(detect)\n        \n        gt = gt.resize_(1,4)\n        # print(\'gt\', gt)\n        gt_size = (gt[0][2] - gt[0][0])*(gt[0][3] - gt[0][1])\n\n        inter_max = torch.max(detect, gt)\n        inter_min = torch.min(detect, gt)\n        # print(inter_max)\n        # print(inter_min)\n        inter_size = max(inter_min[0][2] - inter_max[0][0], 0.) * max(inter_min[0][3] - inter_max[0][1], 0.)\n\n        _iou = inter_size / (det_size + gt_size - inter_size)\n        _ioa = inter_size / gt_size\n\n        iou.append(_iou)\n        ioa.append(_ioa)\n\n        # print(inter_size)\n        # print(det_size, gt_size)\n        # print(iou)\n    return iou, ioa\n    \n\n## TODO: currently, it is super time cost. any ideas to improve it?\n# def cal_tp_fp(detects, ground_turths, label, score, npos, iou_threshold=0.5, conf_threshold=0.01):\n#     \'\'\'\n#     \'\'\'\n#     for det, gt in zip(detects, ground_turths):\n#         for i, det_c in enumerate(det):            \n#             gt_c = [_gt[:4].data.resize_(1,4) for _gt in gt if int(_gt[4]) == i+1]  # only 20 in det\n#             if len(det_c) == 0:cls_dets\n#                 npos[i] += len(gt_c)\n#                 continue\n#             # print(det_c)\n#             # assert(False)\n#             iou_c = []\n#             ioa_c = []\n#             score_c = []\n#             num=0\n#             for det_c_n in det_c:\n#                 if len(gt_c) > 0:\n#                     _iou, _ioa = iou_gt(det_c_n[:4], gt_c)\n#                     iou_c.append(_iou)\n#                     ioa_c.append(_ioa)\n#                 score_c.append(det_c_n[4])\n#                 num+=1\n            \n#             # No detection \n#             if len(iou_c) == 0:\n#                 npos[i] += len(gt_c)\n#                 continue\n\n#             labels_c = [0] * len(score_c)\n#             # TODO: currently ignore the difficulty & ignore the group of boxes.\n#             # Tp-fp evaluation for non-group of boxes (if any).\n#             if len(gt_c) > 0:\n#                 # groundtruth_nongroup_of_is_difficult_list = groundtruth_is_difficult_list[\n#                 #     ~groundtruth_is_group_of_list]\n#                 max_overlap_gt_ids = np.argmax(np.array(iou_c), axis=1)\n#                 is_gt_box_detected = np.zeros(len(gt_c), dtype=bool)\n#                 for iters in range(len(labels_c)):\n#                     gt_id = max_overlap_gt_ids[iters]\n#                     if iou_c[iters][gt_id] >= iou_threshold:\n#                         # if not groundtruth_nongroup_of_is_difficult_list[gt_id]:\n#                         if not is_gt_box_detected[gt_id]:\n#                             labels_c[iters] = 1\n#                             is_gt_box_detected[gt_id] = True\n#                         # else:\n#                         #     is_matched_to_difficult_box[i] = True\n\n#             # append to the global label, score\n#             npos[i] += len(gt_c)\n#             label[i].extend(labels_c)\n#             score[i].extend(score_c)\n        \n#     return label, score, npos\n\ndef cal_tp_fp(detects, ground_turths, label, score, npos, gt_label, iou_threshold=0.5, conf_threshold=0.01):\n    \'\'\'\n    \'\'\'\n    for det, gt in zip(detects, ground_turths):\n        for i, det_c in enumerate(det):            \n            gt_c = [_gt[:4].data.resize_(1,4) for _gt in gt if int(_gt[4]) == i] \n            iou_c = []\n            ioa_c = []\n            score_c = []\n            # num=0\n            for det_c_n in det_c:\n                if det_c_n[0] < conf_threshold:\n                    break\n                if len(gt_c) > 0:\n                    _iou, _ioa = iou_gt(det_c_n[1:], gt_c)\n                    iou_c.append(_iou)\n                    ioa_c.append(_ioa)\n                score_c.append(det_c_n[0])\n            # while det_c[num,0] > conf_threshold:\n            #     if len(gt_c) > 0:\n            #         _iou, _ioa = iou_gt(det_c[num,1:], gt_c)\n            #         iou_c.append(_iou)\n            #         ioa_c.append(_ioa)\n            #     score_c.append(det_c[num, 0])\n            #     num+=1\n            \n            # No detection \n            if len(iou_c) == 0:\n                npos[i] += len(gt_c)\n                if len(gt_c) > 0:\n                    is_gt_box_detected = np.zeros(len(gt_c), dtype=bool)\n                    gt_label[i] += is_gt_box_detected.tolist()\n                continue\n\n            labels_c = [0] * len(score_c)\n            # TODO: currently ignore the difficulty & ignore the group of boxes.\n            # Tp-fp evaluation for non-group of boxes (if any).\n            if len(gt_c) > 0:\n                # groundtruth_nongroup_of_is_difficult_list = groundtruth_is_difficult_list[\n                #     ~groundtruth_is_group_of_list]\n                max_overlap_gt_ids = np.argmax(np.array(iou_c), axis=1)\n                is_gt_box_detected = np.zeros(len(gt_c), dtype=bool)\n                for iters in range(len(labels_c)):\n                    gt_id = max_overlap_gt_ids[iters]\n                    if iou_c[iters][gt_id] >= iou_threshold:\n                        # if not groundtruth_nongroup_of_is_difficult_list[gt_id]:\n                        if not is_gt_box_detected[gt_id]:\n                            labels_c[iters] = 1\n                            is_gt_box_detected[gt_id] = True\n                        # else:\n                        #     is_matched_to_difficult_box[i] = True\n\n            # append to the global label, score\n            npos[i] += len(gt_c)\n            label[i] += labels_c\n            score[i] += score_c\n            gt_label[i] += is_gt_box_detected.tolist()\n        \n    return label, score, npos, gt_label\n\n\ndef cal_size(detects, ground_turths, size):\n    for det, gt in zip(detects, ground_turths):\n        for i, det_c in enumerate(det):  \n            gt_c = [_gt[:4].data.resize_(1,4) for _gt in gt if int(_gt[4]) == i] \n            if len(gt_c) == 0:\n                continue\n            gt_size_c = [ [(_gt[0][2] - _gt[0][0]), (_gt[0][3] - _gt[0][1])] for _gt in gt_c ]\n            # scale_c = [ min(_size) for _size in gt_size_c ]\n            size[i] += gt_size_c\n    return size\n\n# def get_correct_detection(detects, ground_turths, iou_threshold=0.5, conf_threshold=0.01):\n#     detected = list()\n#     for det, gt in zip(detects, ground_turths):\n#         for i, det_c in enumerate(det):            \n#             gt_c = [_gt[:4].data.resize_(1,4) for _gt in gt if int(_gt[4]) == i] \n#             iou_c = []\n#             ioa_c = []\n#             detected_c = []\n#             # num=0\n#             for det_c_n in det_c:\n#                 if det_c_n[0] < conf_threshold:\n#                     break\n#                 if len(gt_c) > 0:\n#                     _iou, _ioa = iou_gt(det_c_n[1:], gt_c)\n#                     if _iou > iou_threshold:\n#                         iou_c.append(_iou)\n#                         ioa_c.append(_ioa)\n#                 detected_c.append(det_c_n[1:])\n\n#             if len(iou_c) == 0:\n#                 npos[i] += len(gt_c)\n#                 continue\n\n#             labels_c = [0] * len(detected_c)\n#             # TODO: currently ignore the difficulty & ignore the group of boxes.\n#             # Tp-fp evaluation for non-group of boxes (if any).\n#             if len(gt_c) > 0:\n#                 # groundtruth_nongroup_of_is_difficult_list = groundtruth_is_difficult_list[\n#                 #     ~groundtruth_is_group_of_list]\n#                 max_overlap_gt_ids = np.argmax(np.array(iou_c), axis=1)\n#                 is_gt_box_detected = np.zeros(len(gt_c), dtype=bool)\n#                 for iters in range(len(labels_c)):\n#                     gt_id = max_overlap_gt_ids[iters]\n#                     if iou_c[iters][gt_id] >= iou_threshold:\n#                         # if not groundtruth_nongroup_of_is_difficult_list[gt_id]:\n#                         if not is_gt_box_detected[gt_id]:\n#                             labels_c[iters] = 1\n#                             is_gt_box_detected[gt_id] = True\n#             detected_c = detected_c[labels_c]\n#             detected.append(detected_c)\n        \n#     return detected\n\n\ndef cal_pr(_label, _score, _npos):\n    recall = []\n    precision = []\n    ap = []\n    for labels, scores, npos in zip(_label[1:], _score[1:], _npos[1:]):\n        sorted_indices = np.argsort(scores)\n        sorted_indices = sorted_indices[::-1]\n        labels = np.array(labels).astype(int)\n        true_positive_labels = labels[sorted_indices]\n        false_positive_labels = 1 - true_positive_labels\n        tp = np.cumsum(true_positive_labels)\n        fp = np.cumsum(false_positive_labels)\n\n        rec = tp.astype(float) / float(npos)\n        prec = tp.astype(float) / np.maximum(tp + fp, np.finfo(np.float64).eps)\n        ap += [compute_average_precision(prec, rec)]\n        recall+=[rec]\n        precision+=[prec]\n    mAP = np.nanmean(ap)\n    return precision, recall, mAP\n\n\n# def voc_ap(rec, prec, use_07_metric=False):\n#     """""" ap = voc_ap(rec, prec, [use_07_metric])\n#     Compute VOC AP given precision and recall.\n#     If use_07_metric is true, uses the\n#     VOC 07 11 point method (default:False).\n#     """"""\n#     if use_07_metric:\n#         # 11 point metric\n#         ap = 0.\n#         for t in np.arange(0., 1.1, 0.1):\n#             if np.sum(rec >= t) == 0:\n#                 p = 0\n#             else:\n#                 p = np.max(prec[rec >= t])\n#             ap = ap + p / 11.\n#     else:\n#         # correct AP calculation\n#         # first append sentinel values at the end\n#         mrec = np.concatenate(([0.], rec, [1.]))\n#         mpre = np.concatenate(([0.], prec, [0.]))\n\n#         # compute the precision envelope\n#         for i in range(mpre.size - 1, 0, -1):\n#             mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n#         # to calculate area under PR curve, look for points\n#         # where X axis (recall) changes value\n#         i = np.where(mrec[1:] != mrec[:-1])[0]\n\n#         # and sum (\\Delta recall) * prec\n#         ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n#     return ap\n\ndef compute_average_precision(precision, recall):\n  """"""Compute Average Precision according to the definition in VOCdevkit.\n\n  Precision is modified to ensure that it does not decrease as recall\n  decrease.\n\n  Args:\n    precision: A float [N, 1] numpy array of precisions\n    recall: A float [N, 1] numpy array of recalls\n\n  Raises:\n    ValueError: if the input is not of the correct format\n\n  Returns:\n    average_precison: The area under the precision recall curve. NaN if\n      precision and recall are None.\n\n  """"""\n  if precision is None:\n    if recall is not None:\n      raise ValueError(""If precision is None, recall must also be None"")\n    return np.NAN\n\n  if not isinstance(precision, np.ndarray) or not isinstance(recall,\n                                                             np.ndarray):\n    raise ValueError(""precision and recall must be numpy array"")\n  if precision.dtype != np.float or recall.dtype != np.float:\n    raise ValueError(""input must be float numpy array."")\n  if len(precision) != len(recall):\n    raise ValueError(""precision and recall must be of the same size."")\n  if not precision.size:\n    return 0.0\n  if np.amin(precision) < 0 or np.amax(precision) > 1:\n    raise ValueError(""Precision must be in the range of [0, 1]."")\n  if np.amin(recall) < 0 or np.amax(recall) > 1:\n    raise ValueError(""recall must be in the range of [0, 1]."")\n  if not all(recall[i] <= recall[i + 1] for i in range(len(recall) - 1)):\n    raise ValueError(""recall must be a non-decreasing array"")\n\n  recall = np.concatenate([[0], recall, [1]])\n  precision = np.concatenate([[0], precision, [0]])\n\n  # Preprocess precision to be a non-decreasing array\n  for i in range(len(precision) - 2, -1, -1):\n    precision[i] = np.maximum(precision[i], precision[i + 1])\n\n  indices = np.where(recall[1:] != recall[:-1])[0] + 1\n  average_precision = np.sum(\n      (recall[indices] - recall[indices - 1]) * precision[indices])\n  return average_precision'"
lib/utils/fp16_utils.py,3,"b""import torch\nimport torch.nn as nn\n\n\nclass tofp16(nn.Module):\n    def __init__(self):\n        super(tofp16, self).__init__()\n\n    def forward(self, input):\n        return input.half()\n\n\ndef copy_in_params(net, params):\n    net_params = list(net.parameters())\n    for i in range(len(params)):\n        net_params[i].data.copy_(params[i].data)\n\n\ndef set_grad(params, params_with_grad):\n\n    for param, param_w_grad in zip(params, params_with_grad):\n        if param.grad is None:\n            param.grad = torch.nn.Parameter(param.data.new().resize_(*param.data.size()))\n        param.grad.data.copy_(param_w_grad.grad.data)\n\n\ndef BN_convert_float(module):\n    '''\n    BatchNorm layers to have parameters in single precision.\n    Find all layers and convert them back to float. This can't\n    be done with built in .apply as that function will apply\n    fn to all modules, parameters, and buffers. Thus we wouldn't\n    be able to guard the float conversion based on the module type.\n    '''\n    if isinstance(module, torch.nn.modules.batchnorm._BatchNorm):\n        module.float()\n    for child in module.children():\n        BN_convert_float(child)\n    return module\n\n\ndef network_to_half(network):\n    return nn.Sequential(tofp16(), BN_convert_float(network.half()))\n"""
lib/utils/timer.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport time\n\n\nclass Timer(object):\n    """"""A simple timer.""""""\n    def __init__(self):\n        self.total_time = 0.\n        self.calls = 0\n        self.start_time = 0.\n        self.diff = 0.\n        self.average_time = 0.\n\n    def tic(self):\n        # using time.time instead of time.clock because time time.clock\n        # does not normalize for multithreading\n        self.start_time = time.time()\n\n    def toc(self, average=True):\n        self.diff = time.time() - self.start_time\n        self.total_time += self.diff\n        self.calls += 1\n        self.average_time = self.total_time / self.calls\n        if average:\n            return self.average_time\n        else:\n            return self.diff\n\n    def clear(self):\n        self.total_time = 0.\n        self.calls = 0\n        self.start_time = 0.\n        self.diff = 0.\n        self.average_time = 0.\n'"
lib/utils/visualize_utils.py,3,"b'import torch\nimport cv2\nimport numpy as np\nimport math\nfrom itertools import product as product\n\ndef images_to_writer(writer, images, prefix=\'image\', names=\'image\', epoch=0):\n    if isinstance(names, str):\n        names = [names+\'_{}\'.format(i) for i in range(len(images))]\n\n    for image, name in zip(images, names):\n        writer.add_image(\'{}/{}\'.format(prefix, name), image, epoch)\n\n\ndef to_grayscale(image):\n    """"""\n    input is (d,w,h)\n    converts 3D image tensor to grayscale images corresponding to each channel\n    """"""\n    # print(image.shape)\n    channel = image.shape[0]\n    image = torch.sum(image, dim=0)\n    # print(image.shape)\n    image = torch.div(image, channel)\n    # print(image.shape)\n    # assert False\n    return image\n\n\ndef to_image_size(feature, target_img):\n    height, width, _ = target_img.shape\n    resized_feature = cv2.resize(feature, (width, height)) \n    return resized_feature\n\n\ndef features_to_grid(features):\n    num, height, width, channel = (len(features), len(features[0]), len(features[0][0]), len(features[0][0]))\n    rows = math.ceil(np.sqrt(num))\n    output = np.zeros([rows*(height+2),rows*(width+2), 3],dtype=np.float32)\n\n    for i, feature in enumerate(features):\n        row = i % rows\n        col = math.floor(i / rows)\n        output[row*(2+height)+1:(row+1)*(2+height)-1, col*(2+width)+1:(col+1)*(2+width)-1] = feature\n\n    return output\n    \n\ndef viz_feature_maps(writer, feature_maps, module_name=\'base\', epoch=0, prefix=\'module_feature_maps\'):\n    feature_map_visualization = []\n    for i in feature_maps:\n        i = i.squeeze(0)\n        temp = to_grayscale(i)\n        feature_map_visualization.append(temp.data.cpu().numpy())\n        \n    names, feature_map_heatmap = [], []\n    for i, feature_map in enumerate(feature_map_visualization):\n        feature_map = (feature_map * 255)\n        heatmap = cv2.applyColorMap(feature_map.astype(np.uint8), cv2.COLORMAP_JET)\n        feature_map_heatmap.append(heatmap[..., ::-1])\n        names.append(\'{}.{}\'.format(module_name, i))\n\n    images_to_writer(writer, feature_map_heatmap, prefix, names, epoch)\n\ndef viz_grads(writer, model, feature_maps, target_image, target_mean, module_name=\'base\', epoch=0, prefix=\'module_grads\'):\n    grads_visualization = []\n    names = []\n    for i, feature_map in enumerate(feature_maps):\n        model.zero_grad()\n        \n        # print()\n        feature_map.backward(torch.Tensor(np.ones(feature_map.size())), retain_graph=True)\n        # print(target_image.grad)\n        grads = target_image.grad.data.clamp(min=0).squeeze(0).permute(1,2,0)\n        # print(grads)\n        # assert False\n        grads_visualization.append(grads.cpu().numpy()+target_mean)\n        names.append(\'{}.{}\'.format(module_name, i))\n    \n    images_to_writer(writer, grads_visualization, prefix, names, epoch)\n\ndef viz_module_feature_maps(writer, module, input_image, module_name=\'base\', epoch=0, mode=\'one\', prefix=\'module_feature_maps\'):\n    output_image = input_image\n    feature_maps = []\n\n    for i, layer in enumerate(module):\n        output_image = layer(output_image)\n        feature_maps.append(output_image)\n\n    if mode is \'grid\':\n        pass\n    elif mode is \'one\':\n        viz_feature_maps(writer, feature_maps, module_name, epoch, prefix)\n\n    return output_image\n\n\ndef viz_module_grads(writer, model, module, input_image, target_image, target_mean, module_name=\'base\', epoch=0, mode=\'one\', prefix=\'module_grads\'):\n    output_image = input_image\n    feature_maps = []\n\n    for i, layer in enumerate(module):\n        output_image = layer(output_image)\n        feature_maps.append(output_image)\n\n    if mode is \'grid\':\n        pass\n    elif mode is \'one\':\n        viz_grads(writer, model, feature_maps, target_image, target_mean, module_name, epoch, prefix)\n    \n    return output_image\n\ndef viz_prior_box(writer, prior_box, image=None, epoch=0):  \n    if isinstance(image, type(None)):\n        image = np.random.random((prior_box.image_size[0], prior_box.image_size[1], 3))\n    elif isinstance(image, str):\n        image = cv2.imread(image, -1)\n    image = cv2.resize(image, (prior_box.image_size[0], prior_box.image_size[1]))\n    \n    for k, f in enumerate(prior_box.feature_maps):\n        bbxs = []\n        image_show = image.copy()\n        for i, j in product(range(f[0]), range(f[1])):\n            cx = j * prior_box.steps[k][1] + prior_box.offset[k][1]\n            cy = i * prior_box.steps[k][0] + prior_box.offset[k][0]\n\n            # aspect_ratio: 1 Min size\n            s_k = prior_box.scales[k]\n            bbxs += [cx, cy, s_k, s_k]\n\n            # # aspect_ratio: 1 Max size\n            # # rel size: sqrt(s_k * s_(k+1))\n            # s_k_prime = sqrt(s_k * self.scales[k+1])\n            # bbxs += [cx, cy, s_k_prime, s_k_prime]\n\n            # # rest of aspect ratios\n            # for ar in self.aspect_ratios[k]:\n            #     ar_sqrt = sqrt(ar)\n            #     bbxs += [cx, cy, s_k*ar_sqrt, s_k/ar_sqrt]\n            #     bbxs += [cx, cy, s_k/ar_sqrt, s_k*ar_sqrt]\n\n        scale = [prior_box.image_size[1], prior_box.image_size[0], prior_box.image_size[1], prior_box.image_size[0]]\n        bbxs = np.array(bbxs).reshape((-1, 4))\n        archors = bbxs[:, :2] * scale[:2]\n        bbxs = np.hstack((bbxs[:, :2] - bbxs[:, 2:4]/2, bbxs[:, :2] + bbxs[:, 2:4]/2)) * scale\n        archors = archors.astype(np.int32)\n        bbxs = bbxs.astype(np.int32)\n\n        for archor, bbx in zip(archors, bbxs):\n            cv2.circle(image_show,(archor[0],archor[1]), 2, (0,0,255), -1)\n            if archor[0] == archor[1]:\n                cv2.rectangle(image_show, (bbx[0], bbx[1]), (bbx[2], bbx[3]), (0, 255, 0), 1)\n\n        writer.add_image(\'example_prior_boxs/feature_map_{}\'.format(k), image_show, epoch)\n\n\ndef add_pr_curve_raw(writer, tag, precision, recall, epoch=0):\n    num_thresholds = len(precision)\n    writer.add_pr_curve_raw(\n        tag=tag,\n        true_positive_counts = -np.ones(num_thresholds),\n        false_positive_counts = -np.ones(num_thresholds),\n        true_negative_counts = -np.ones(num_thresholds),\n        false_negative_counts = -np.ones(num_thresholds),\n        precision = precision,\n        recall = recall,\n        global_step = epoch,\n        num_thresholds = num_thresholds\n    )\n\n\ndef viz_pr_curve(writer, precision, recall, epoch=0):\n    for i, (_prec, _rec) in enumerate(zip(precision, recall)):\n        # _prec, _rec = prec, rec\n        num_thresholds = min(500, len(_prec))\n        if num_thresholds != len(_prec):\n            gap = int(len(_prec) / num_thresholds)\n            _prec = np.append(_prec[::gap], _prec[-1])\n            _rec  = np.append(_rec[::gap], _rec[-1])\n            num_thresholds = len(_prec)\n        # the pr_curve_raw_data_pb() needs the a ascending precisions array and a descending recalls array\n        _prec.sort()\n        _rec[::-1].sort()\n        # TODO: need to change i to the name of the class\n        # 0 is the background class as default\n        add_pr_curve_raw(\n            writer=writer, tag=\'pr_curve/class_{}\'.format(i+1), precision = _prec, recall = _rec, epoch = epoch )\n\n\ndef viz_archor_strategy(writer, sizes, labels, epoch=0):\n    \'\'\' generate archor strategy for all classes\n    \'\'\'\n\n    # merge all data into one \n    height, width, max_size, min_size, aspect_ratio, label = [list() for _ in range(6)]\n    for _size, _label in zip(sizes[1:], labels[1:]):\n        _height, _width, _max_size, _min_size, _aspect_ratio = [list() for _ in range(5)]\n        for size in _size:\n            _height += [size[0]]\n            _width  += [size[1]]\n            _max_size += [max(size)]\n            _min_size += [min(size)]\n            _aspect_ratio += [size[0]/size[1] if size[0] < size[1] else size[1]/size[0]]\n        height += _height\n        width += _width\n        max_size += _max_size\n        min_size += _min_size\n        aspect_ratio += _aspect_ratio\n        label += _label\n    \n    height, width, max_size, min_size, aspect_ratio = \\\n        np.array(height), np.array(width), np.array(max_size), np.array(min_size), np.array(aspect_ratio)   \n    matched_height, matched_width, matched_max_size, matched_min_size, matched_aspect_ratio = \\\n        height[label], width[label], max_size[label], min_size[label], aspect_ratio[label]\n\n    num_thresholds = 100\n    # height, width, max_size, min_size, aspect_ratio = \\\n    height.sort(), width.sort(), max_size.sort(), min_size.sort(), aspect_ratio.sort()\n    # matched_height, matched_width, matched_max_size, matched_min_size, matched_aspect_ratio = \\\n    matched_height.sort(), matched_width.sort(), matched_max_size.sort(), matched_min_size.sort(), matched_aspect_ratio.sort()\n\n    x_axis = np.arange(num_thresholds)[::-1]/num_thresholds + 0.5 / num_thresholds\n\n    # height \n    gt_y, _ = np.histogram(height, bins=num_thresholds, range=(0.0, 1.0))\n    gt_y = np.clip( gt_y[::-1]/len(height), 1e-8, 1.0)\n    add_pr_curve_raw(\n        writer=writer, tag=\'archor_strategy/height_distribute_gt\', precision = gt_y, recall = x_axis, epoch = epoch )\n    add_pr_curve_raw(\n        writer=writer, tag=\'archor_strategy/height_distribute_gt_normalized\', precision = gt_y/max(gt_y), recall = x_axis, epoch = epoch )\n    \n    matched_y, _ = np.histogram(matched_height, bins=num_thresholds,range=(0.0, 1.0))\n    matched_y = np.clip( matched_y[::-1]/len(height), 1e-8, 1.0)\n    add_pr_curve_raw(\n        writer=writer, tag=\'archor_strategy/height_distribute_matched\', precision = matched_y/gt_y, recall = x_axis, epoch = epoch )\n\n    # width\n    gt_y, _ = np.histogram(width, bins=num_thresholds, range=(0.0, 1.0))\n    gt_y = np.clip( gt_y[::-1]/len(width), 1e-8, 1.0)\n    add_pr_curve_raw(\n        writer=writer, tag=\'archor_strategy/width_distribute_gt\', precision = gt_y, recall = x_axis, epoch = epoch )\n    add_pr_curve_raw(\n        writer=writer, tag=\'archor_strategy/width_distribute_gt_normalized\', precision = gt_y/max(gt_y), recall = x_axis, epoch = epoch )\n    \n    matched_y, _ = np.histogram(matched_width, bins=num_thresholds,range=(0.0, 1.0))\n    matched_y = np.clip( matched_y[::-1]/len(width), 1e-8, 1.0)\n    add_pr_curve_raw(\n        writer=writer, tag=\'archor_strategy/width_distribute_matched\', precision = matched_y/gt_y, recall = x_axis, epoch = epoch )\n\n    # max_size\n    gt_y, _ = np.histogram(max_size, bins=num_thresholds, range=(0.0, 1.0))\n    gt_y = np.clip( gt_y[::-1]/len(max_size), 1e-8, 1.0)\n    add_pr_curve_raw(\n        writer=writer, tag=\'archor_strategy/max_size_distribute_gt\', precision = gt_y, recall = x_axis, epoch = epoch )\n    add_pr_curve_raw(\n        writer=writer, tag=\'archor_strategy/max_size_distribute_gt_normalized\', precision = gt_y/max(gt_y), recall = x_axis, epoch = epoch )\n    \n    matched_y, _ = np.histogram(matched_max_size, bins=num_thresholds,range=(0.0, 1.0))\n    matched_y = np.clip( matched_y[::-1]/len(max_size), 1e-8, 1.0)\n    add_pr_curve_raw(\n        writer=writer, tag=\'archor_strategy/max_size_distribute_matched\', precision = matched_y/gt_y, recall = x_axis, epoch = epoch )\n\n    # min_size\n    gt_y, _ = np.histogram(min_size, bins=num_thresholds, range=(0.0, 1.0))\n    gt_y = np.clip( gt_y[::-1]/len(min_size), 1e-8, 1.0)\n    add_pr_curve_raw(\n        writer=writer, tag=\'archor_strategy/min_size_distribute_gt\', precision = gt_y, recall = x_axis, epoch = epoch )\n    add_pr_curve_raw(\n        writer=writer, tag=\'archor_strategy/min_size_distribute_gt_normalized\', precision = gt_y/max(gt_y), recall = x_axis, epoch = epoch )\n    \n    matched_y, _ = np.histogram(matched_min_size, bins=num_thresholds,range=(0.0, 1.0))\n    matched_y = np.clip( matched_y[::-1]/len(min_size), 1e-8, 1.0)\n    add_pr_curve_raw(\n        writer=writer, tag=\'archor_strategy/min_size_distribute_matched\', precision = matched_y/gt_y, recall = x_axis, epoch = epoch )\n    \n    # aspect_ratio\n    gt_y, _ = np.histogram(aspect_ratio, bins=num_thresholds, range=(0.0, 1.0))\n    gt_y = np.clip( gt_y[::-1]/len(aspect_ratio), 1e-8, 1.0)\n    add_pr_curve_raw(\n        writer=writer, tag=\'archor_strategy/aspect_ratio_distribute_gt\', precision = gt_y, recall = x_axis, epoch = epoch )\n    add_pr_curve_raw(\n        writer=writer, tag=\'archor_strategy/aspect_ratio_distribute_gt_normalized\', precision = gt_y/max(gt_y), recall = x_axis, epoch = epoch )\n    \n    matched_y, _ = np.histogram(matched_aspect_ratio, bins=num_thresholds,range=(0.0, 1.0))\n    matched_y = np.clip( matched_y[::-1]/len(aspect_ratio), 1e-8, 1.0)\n    add_pr_curve_raw(\n        writer=writer, tag=\'archor_strategy/aspect_ratio_distribute_matched\', precision = matched_y/gt_y, recall = x_axis, epoch = epoch )'"
lib/layers/functions/__init__.py,0,"b""from .detection import Detect\nfrom .prior_box import PriorBox\n\n\n__all__ = ['Detect', 'PriorBox']\n"""
lib/layers/functions/detection.py,11,"b'import torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nfrom torch.autograd import Function\nfrom torch.autograd import Variable\nfrom lib.utils.box_utils import decode,nms\n# from lib.utils.nms.nms_wrapper import nms\nfrom lib.utils.timer import Timer\n\nclass Detect(Function):\n    """"""At test time, Detect is the final layer of SSD.  Decode location preds,\n    apply non-maximum suppression to location predictions based on conf\n    scores and threshold to a top_k number of output predictions for both\n    confidence score and locations.\n    """"""\n    def __init__(self, cfg, priors):\n        self.num_classes = cfg.NUM_CLASSES\n        self.background_label = cfg.BACKGROUND_LABEL\n        self.conf_thresh = cfg.SCORE_THRESHOLD\n        self.nms_thresh = cfg.IOU_THRESHOLD \n        self.top_k = cfg.MAX_DETECTIONS \n        self.variance = cfg.VARIANCE\n        self.priors = priors\n\n    # def forward(self, predictions, prior):\n    #     """"""\n    #     Args:\n    #         loc_data: (tensor) Loc preds from loc layers\n    #             Shape: [batch,num_priors*4]\n    #         conf_data: (tensor) Shape: Conf preds from conf layers\n    #             Shape: [batch*num_priors,num_classes]\n    #         prior_data: (tensor) Prior boxes and variances from priorbox layers\n    #             Shape: [1,num_priors,4]\n    #     """"""\n    #     loc, conf = predictions\n\n    #     loc_data = loc.data\n    #     conf_data = conf.data\n    #     prior_data = prior.data\n\n    #     num = loc_data.size(0)  # batch size\n    #     num_priors = prior_data.size(0)\n    #     self.boxes = torch.zeros(1, num_priors, 4)\n    #     self.scores = torch.zeros(1, num_priors, self.num_classes)\n\n    #     if num == 1:\n    #         # size batch x num_classes x num_priors\n    #         conf_preds = conf_data.unsqueeze(0)\n\n    #     else:\n    #         conf_preds = conf_data.view(num, num_priors,\n    #                                     self.num_classes)\n    #         self.boxes.expand_(num, num_priors, 4)\n    #         self.scores.expand_(num, num_priors, self.num_classes)\n\n    #     # Decode predictions into bboxes.\n    #     for i in range(num):\n    #         decoded_boxes = decode(loc_data[i], prior_data, self.variance)\n    #         # For each class, perform nms\n    #         conf_scores = conf_preds[i].clone()\n    #         \'\'\'\n    #         c_mask = conf_scores.gt(self.thresh)\n    #         decoded_boxes = decoded_boxes[c_mask]\n    #         conf_scores = conf_scores[c_mask]\n    #         \'\'\'\n\n    #         conf_scores = conf_preds[i].clone()\n    #         num_det = 0\n    #         for cl in range(1, self.num_classes):\n    #             c_mask = conf_scores[cl].gt(self.conf_thresh)\n    #             scores = conf_scores[cl][c_mask]\n    #             if scores.dim() == 0:\n    #                 continue\n    #             l_mask = c_mask.unsqueeze(1).expand_as(decoded_boxes)\n    #             boxes = decoded_boxes[l_mask].view(-1, 4)\n    #             ids, count = nms(boxes, scores, self.nms_thresh, self.top_k)\n    #             self.output[i, cl, :count] = \\\n    #                 torch.cat((scores[ids[:count]].unsqueeze(1),\n    #                            boxes[ids[:count]]), 1)\n\n    #     return self.output\n\n\n    def forward(self, predictions):\n        """"""\n        Args:\n            loc_data: (tensor) Loc preds from loc layers\n                Shape: [batch,num_priors*4]\n            conf_data: (tensor) Shape: Conf preds from conf layers\n                Shape: [batch*num_priors,num_classes]\n            prior_data: (tensor) Prior boxes and variances from priorbox layers\n                Shape: [1,num_priors,4]\n        """"""\n        loc, conf = predictions\n\n        loc_data = loc.data\n        conf_data = conf.data\n        prior_data = self.priors.data\n\n        num = loc_data.size(0)  # batch size\n        num_priors = prior_data.size(0)\n        #self.output.zero_()\n        if num == 1:\n            # size batch x num_classes x num_priors\n            conf_preds = conf_data.t().contiguous().unsqueeze(0)\n        else:\n            conf_preds = conf_data.view(num, num_priors,\n                                        self.num_classes).transpose(2, 1)\n            #self.output.expand_(num, self.num_classes, self.top_k, 5)\n        output = torch.zeros(num, self.num_classes, self.top_k, 5)\n\n        _t = {\'decode\': Timer(), \'misc\': Timer(), \'box_mask\':Timer(), \'score_mask\':Timer(),\'nms\':Timer(), \'cpu\':Timer(),\'sort\':Timer()}\n        gpunms_time = 0\n        scores_time=0\n        box_time=0\n        cpu_tims=0\n        sort_time=0\n        decode_time=0\n        _t[\'misc\'].tic()\n        # Decode predictions into bboxes.\n        for i in range(num):\n            _t[\'decode\'].tic()\n            decoded_boxes = decode(loc_data[i], prior_data, self.variance)\n            decode_time += _t[\'decode\'].toc()\n            # For each class, perform nms\n            conf_scores = conf_preds[i].clone()\n            num_det = 0\n            for cl in range(1, self.num_classes):\n                _t[\'cpu\'].tic()\n                c_mask = conf_scores[cl].gt(self.conf_thresh).nonzero().view(-1)\n                cpu_tims+=_t[\'cpu\'].toc()\n                if c_mask.dim() == 0:\n                    continue\n                _t[\'score_mask\'].tic()\n                scores = conf_scores[cl][c_mask]\n                scores_time+=_t[\'score_mask\'].toc()\n                if scores.dim() == 0:\n                    continue\n                _t[\'box_mask\'].tic()\n                # l_mask = c_mask.unsqueeze(1).expand_as(decoded_boxes)\n                # boxes = decoded_boxes[l_mask].view(-1, 4)\n                boxes = decoded_boxes[c_mask, :]\n                box_time+=_t[\'box_mask\'].toc()\n                # idx of highest scoring and non-overlapping boxes per class\n                _t[\'nms\'].tic()\n                # cls_dets = torch.cat((boxes, scores), 1)\n                # _, order = torch.sort(scores, 0, True)\n                # cls_dets = cls_dets[order]\n                # keep = nms(cls_dets, self.nms_thresh)\n                # cls_dets = cls_dets[keep.view(-1).long()]\n                ids, count = nms(boxes, scores, self.nms_thresh, self.top_k)\n\n                gpunms_time += _t[\'nms\'].toc()\n                output[i, cl, :count] = \\\n                    torch.cat((scores[ids[:count]].unsqueeze(1),\n                               boxes[ids[:count]]), 1)\n        nms_time= _t[\'misc\'].toc()\n        # print(nms_time, cpu_tims, scores_time,box_time,gpunms_time)\n        # flt = self.output.view(-1, 5)\n        # _, idx = flt[:, 0].sort(0)\n        # _, rank = idx.sort(0)\n        # flt[(rank >= self.top_k).unsqueeze(1).expand_as(flt)].fill_(0)\n        return output'"
lib/layers/functions/prior_box.py,1,"b'from __future__ import division\nimport torch\nfrom math import sqrt as sqrt\nfrom itertools import product as product\n\nclass PriorBox(object):\n    """"""Compute priorbox coordinates in center-offset form for each source\n    feature map.\n    """"""\n    def __init__(self, image_size, feature_maps, aspect_ratios, scale, archor_stride=None, archor_offest=None, clip=True):\n        super(PriorBox, self).__init__()\n        self.image_size = image_size #[height, width]\n        self.feature_maps = feature_maps #[(height, width), ...]\n        self.aspect_ratios = aspect_ratios\n        # number of priors for feature map location (either 4 or 6)\n        self.num_priors = len(aspect_ratios)\n        self.clip = clip\n        # scale value\n        if isinstance(scale[0], list):\n            # get min of the result\n            self.scales = [min(s[0] / self.image_size[0], s[1] / self.image_size[1]) for s in scale]\n        elif isinstance(scale[0], float) and len(scale) == 2:\n            num_layers = len(feature_maps)\n            min_scale, max_scale = scale\n            self.scales = [min_scale + (max_scale - min_scale) * i / (num_layers - 1) for i in range(num_layers)] + [1.0]\n        \n        if archor_stride:\n            self.steps = [(steps[0] / self.image_size[0], steps[1] / self.image_size[1]) for steps in archor_stride] \n        else:\n            self.steps = [(1/f_h, 1/f_w) for f_h, f_w in feature_maps]\n\n        if archor_offest:\n            self.offset = [[offset[0] / self.image_size[0], offset[1] * self.image_size[1]] for offset in archor_offest] \n        else:\n            self.offset = [[steps[0] * 0.5, steps[1] * 0.5] for steps in self.steps] \n\n    def forward(self):\n        mean = []\n        # l = 0\n        for k, f in enumerate(self.feature_maps):\n            for i, j in product(range(f[0]), range(f[1])):\n                cx = j * self.steps[k][1] + self.offset[k][1]\n                cy = i * self.steps[k][0] + self.offset[k][0]\n                s_k = self.scales[k]\n\n                # rest of aspect ratios\n                for ar in self.aspect_ratios[k]:\n                    if isinstance(ar, int):\n                        if ar == 1:\n                            # aspect_ratio: 1 Min size\n                            mean += [cx, cy, s_k, s_k]\n\n                            # aspect_ratio: 1 Max size\n                            # rel size: sqrt(s_k * s_(k+1))\n                            s_k_prime = sqrt(s_k * self.scales[k+1])\n                            mean += [cx, cy, s_k_prime, s_k_prime]\n                        else:\n                            ar_sqrt = sqrt(ar)\n                            mean += [cx, cy, s_k*ar_sqrt, s_k/ar_sqrt]\n                            mean += [cx, cy, s_k/ar_sqrt, s_k*ar_sqrt]\n                    elif isinstance(ar, list):\n                        mean += [cx, cy, s_k*ar[0], s_k*ar[1]]\n        #     print(f, self.aspect_ratios[k])\n        # assert False\n        # back to torch land\n        output = torch.Tensor(mean).view(-1, 4)\n        if self.clip:\n            output.clamp_(max=1, min=0)\n        return output'"
lib/layers/modules/__init__.py,0,"b""from .l2norm import L2Norm\nfrom .multibox_loss import MultiBoxLoss\n\n__all__ = ['L2Norm', 'MultiBoxLoss']\n"""
lib/layers/modules/focal_loss.py,10,"b'# -*- coding: utf-8 -*-\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom lib.utils.box_utils import match, log_sum_exp, one_hot_embedding\n\n# I do not fully understand this part, It completely based on https://github.com/kuangliu/pytorch-retinanet/blob/master/loss.py\n\nclass FocalLoss(nn.Module):\n    """"""SSD Weighted Loss Function\n    Focal Loss for Dense Object Detection.\n        \n        Loss(x, class) = - \\alpha (1-softmax(x)[class])^gamma \\log(softmax(x)[class])\n\n    The losses are averaged across observations for each minibatch.\n    Args:\n        alpha(1D Tensor, Variable) : the scalar factor for this criterion\n        gamma(float, double) : gamma > 0; reduces the relative loss for well-classi\xef\xac\x81ed examples (p > .5), \n                                putting more focus on hard, misclassi\xef\xac\x81ed examples\n        size_average(bool): size_average(bool): By default, the losses are averaged over observations for each minibatch.\n                            However, if the field size_average is set to False, the losses are\n                            instead summed for each minibatch.\n    """"""\n\n    def __init__(self, cfg, priors, use_gpu=True):\n        super(FocalLoss, self).__init__()\n        self.use_gpu = use_gpu\n        self.num_classes = cfg.NUM_CLASSES\n        self.background_label = cfg.BACKGROUND_LABEL\n        self.negpos_ratio = cfg.NEGPOS_RATIO\n        self.threshold = cfg.MATCHED_THRESHOLD\n        self.unmatched_threshold = cfg.UNMATCHED_THRESHOLD\n        self.variance = cfg.VARIANCE\n        self.priors = priors\n\n        self.alpha = Variable(torch.ones(self.num_classes, 1) * cfg.alpha)\n        self.gamma = cfg.gamma\n\n\n    def forward(self, predictions, targets):\n        """"""Multibox Loss\n        Args:\n            predictions (tuple): A tuple containing loc preds, conf preds,\n            and prior boxes from SSD net.\n                conf shape: torch.size(batch_size,num_priors,num_classes)\n                loc shape: torch.size(batch_size,num_priors,4)\n                priors shape: torch.size(num_priors,4)\n            ground_truth (tensor): Ground truth boxes and labels for a batch,\n                shape: [batch_size,num_objs,5] (last idx is the label).\n        """"""\n        loc_data, conf_data = predictions\n        num = loc_data.size(0)\n        priors = self.priors\n        # priors = priors[:loc_data.size(1), :]\n        num_priors = (priors.size(0))\n        \n        # match priors (default boxes) and ground truth boxes\n        loc_t = torch.Tensor(num, num_priors, 4)\n        conf_t = torch.LongTensor(num, num_priors)\n        for idx in range(num):\n            truths = targets[idx][:,:-1].data\n            labels = targets[idx][:,-1].data\n            defaults = priors.data\n            match(self.threshold,truths,defaults,self.variance,labels,loc_t,conf_t,idx)\n        if self.use_gpu:\n            loc_t = loc_t.cuda()\n            conf_t = conf_t.cuda()\n        # wrap targets\n        loc_t = Variable(loc_t, requires_grad=False)\n        conf_t = Variable(conf_t,requires_grad=False)\n\n        pos = conf_t > 0\n        num_pos = pos.sum()\n\n        # Localization Loss (Smooth L1)\n        # Shape: [batch,num_priors,4]\n        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)\n        loc_p = loc_data[pos_idx].view(-1,4)\n        loc_t = loc_t[pos_idx].view(-1,4)\n        loss_l = F.smooth_l1_loss(loc_p, loc_t, size_average=False)\n        loss_l/=num_pos.data.sum()\n\n        # Confidence Loss (Focal loss)\n        # Shape: [batch,num_priors,1]\n        loss_c = self.focal_loss(conf_data.view(-1, self.num_classes), conf_t.view(-1,1))\n\n        return loss_l,loss_c\n\n    def focal_loss(self, inputs, targets):\n        \'\'\'Focal loss.\n        mean of losses: L(x,c,l,g) = (Lconf(x, c) + \xce\xb1Lloc(x,l,g)) / N\n        \'\'\'\n        N = inputs.size(0)\n        C = inputs.size(1)\n        P = F.softmax(inputs)\n        \n        class_mask = inputs.data.new(N, C).fill_(0)\n        class_mask = Variable(class_mask)\n        ids = targets.view(-1, 1)\n        class_mask.scatter_(1, ids.data, 1.)\n\n        if inputs.is_cuda and not self.alpha.is_cuda:\n            self.alpha = self.alpha.cuda()\n        alpha = self.alpha[ids.data.view(-1)]\n        probs = (P*class_mask).sum(1).view(-1,1)\n        log_p = probs.log()\n\n        batch_loss = -alpha*(torch.pow((1-probs), self.gamma))*log_p \n\n        loss = batch_loss.mean()\n        return loss'"
lib/layers/modules/l2norm.py,6,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Function\nfrom torch.autograd import Variable\nimport torch.nn.init as init\n\nclass L2Norm(nn.Module):\n    def __init__(self,n_channels, scale):\n        super(L2Norm,self).__init__()\n        self.n_channels = n_channels\n        self.gamma = scale or None\n        self.eps = 1e-10\n        self.weight = nn.Parameter(torch.Tensor(self.n_channels))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        init.constant(self.weight,self.gamma)\n\n    def forward(self, x):\n        norm = x.pow(2).sum(dim=1, keepdim=True).sqrt()+self.eps\n        #x /= norm\n        x = torch.div(x,norm)\n        out = self.weight.unsqueeze(0).unsqueeze(2).unsqueeze(3).expand_as(x) * x\n        return out\n'"
lib/layers/modules/multibox_loss.py,9,"b'# -*- coding: utf-8 -*-\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom lib.utils.box_utils import match, log_sum_exp\n\n\nclass MultiBoxLoss(nn.Module):\n    """"""SSD Weighted Loss Function\n    Compute Targets:\n        1) Produce Confidence Target Indices by matching  ground truth boxes\n           with (default) \'priorboxes\' that have jaccard index > threshold parameter\n           (default threshold: 0.5).\n        2) Produce localization target by \'encoding\' variance into offsets of ground\n           truth boxes and their matched  \'priorboxes\'.\n        3) Hard negative mining to filter the excessive number of negative examples\n           that comes with using a large number of default bounding boxes.\n           (default negative:positive ratio 3:1)\n    Objective Loss:\n        L(x,c,l,g) = (Lconf(x, c) + \xce\xb1Lloc(x,l,g)) / N\n        Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss\n        weighted by \xce\xb1 which is set to 1 by cross val.\n        Args:\n            c: class confidences,\n            l: predicted boxes,\n            g: ground truth boxes\n            N: number of matched default boxes\n        See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n    """"""\n\n    def __init__(self, cfg, priors, use_gpu=True):\n        super(MultiBoxLoss, self).__init__()\n        self.use_gpu = use_gpu\n        self.num_classes = cfg.NUM_CLASSES\n        self.background_label = cfg.BACKGROUND_LABEL\n        self.negpos_ratio = cfg.NEGPOS_RATIO\n        self.threshold = cfg.MATCHED_THRESHOLD\n        self.unmatched_threshold = cfg.UNMATCHED_THRESHOLD\n        self.variance = cfg.VARIANCE\n        self.priors = priors\n\n    def forward(self, predictions, targets):\n        """"""Multibox Loss\n        Args:\n            predictions (tuple): A tuple containing loc preds, conf preds,\n            and prior boxes from SSD net.\n                conf shape: torch.size(batch_size,num_priors,num_classes)\n                loc shape: torch.size(batch_size,num_priors,4)\n                priors shape: torch.size(num_priors,4)\n            ground_truth (tensor): Ground truth boxes and labels for a batch,\n                shape: [batch_size,num_objs,5] (last idx is the label).\n        """"""\n        loc_data, conf_data = predictions\n        num = loc_data.size(0)\n        priors = self.priors\n        # priors = priors[:loc_data.size(1), :]\n        num_priors = (priors.size(0))\n        num_classes = self.num_classes\n\n        # match priors (default boxes) and ground truth boxes\n        loc_t = torch.Tensor(num, num_priors, 4)\n        conf_t = torch.LongTensor(num, num_priors)\n        for idx in range(num):\n            truths = targets[idx][:,:-1].data\n            labels = targets[idx][:,-1].data\n            defaults = priors.data\n            match(self.threshold,truths,defaults,self.variance,labels,loc_t,conf_t,idx)\n        if self.use_gpu:\n            loc_t = loc_t.cuda()\n            conf_t = conf_t.cuda()\n        # wrap targets\n        loc_t = Variable(loc_t, requires_grad=False)\n        conf_t = Variable(conf_t,requires_grad=False)\n\n        pos = conf_t > 0\n        # num_pos = pos.sum()\n\n        # Localization Loss (Smooth L1)\n        # Shape: [batch,num_priors,4]\n        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)\n        loc_p = loc_data[pos_idx].view(-1,4)\n        loc_t = loc_t[pos_idx].view(-1,4)\n        loss_l = F.smooth_l1_loss(loc_p, loc_t, size_average=False)\n\n        # Compute max conf across batch for hard negative mining\n        batch_conf = conf_data.view(-1, self.num_classes)\n        loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1,1))\n\n        # Hard Negative Mining\n        loss_c[pos] = 0 # filter out pos boxes for now\n        loss_c = loss_c.view(num, -1)\n        _,loss_idx = loss_c.sort(1, descending=True)\n        _,idx_rank = loss_idx.sort(1)\n        num_pos = pos.long().sum(1,keepdim=True) #new sum needs to keep the same dim\n        num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1)\n        neg = idx_rank < num_neg.expand_as(idx_rank)\n\n        # Confidence Loss Including Positive and Negative Examples\n        pos_idx = pos.unsqueeze(2).expand_as(conf_data)\n        neg_idx = neg.unsqueeze(2).expand_as(conf_data)\n        conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1,self.num_classes)\n        targets_weighted = conf_t[(pos+neg).gt(0)]\n        loss_c = F.cross_entropy(conf_p, targets_weighted, size_average=False)\n\n        # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + \xce\xb1Lloc(x,l,g)) / N\n\n        N = num_pos.data.sum()\n        loss_l/=N\n        loss_c/=N\n        return loss_l,loss_c\n'"
lib/modeling/nets/__init__.py,0,b''
lib/modeling/nets/darknet.py,1,"b""import torch\nimport torch.nn as nn\n\nfrom collections import namedtuple\nimport functools\n\nConv = namedtuple('Conv', ['stride', 'depth'])\nConvBlock = namedtuple('ConvBlock', ['stride', 'depth', 'num', 't']) # t is the expension factor\nResidualBlock = namedtuple('ResidualBlock', ['stride', 'depth', 'num', 't']) # t is the expension factor\n\n\nCONV_DEFS_19 = [\n    Conv(stride=1, depth=32),\n    'M',\n    Conv(stride=1, depth=64),\n    'M',\n    ConvBlock(stride=1, depth=128, num=2, t=0.5),\n    'M',\n    ConvBlock(stride=1, depth=256, num=2, t=0.5),\n    'M',\n    ConvBlock(stride=1, depth=512, num=3, t=0.5),\n    'M',\n    ConvBlock(stride=1, depth=1024, num=3, t=0.5),\n]\n\nCONV_DEFS_53 = [\n    Conv(stride=1, depth=32),\n    ResidualBlock(stride=2, depth=64, num=2, t=0.5),\n    ResidualBlock(stride=2, depth=128, num=3, t=0.5),\n    ResidualBlock(stride=2, depth=256, num=9, t=0.5),\n    ResidualBlock(stride=2, depth=512, num=9, t=0.5),\n    ResidualBlock(stride=2, depth=1024, num=5, t=0.5),\n]\n\nclass _conv_bn(nn.Module):\n    def __init__(self, inp, oup, stride):\n        super(_conv_bn, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n            nn.BatchNorm2d(oup),\n            nn.LeakyReLU(0.1, inplace=True),\n        )\n        self.depth = oup\n\n    def forward(self, x):\n        return self.conv(x)\n\nclass _conv_block(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio=0.5):\n        super(_conv_block, self).__init__()\n        if stride == 1 and inp == oup:\n            depth = int(oup*expand_ratio)\n            self.conv = nn.Sequential(\n                nn.Conv2d(inp, depth, 1, 1, bias=False),\n                nn.BatchNorm2d(depth),\n                nn.LeakyReLU(0.1, inplace=True),\n                nn.Conv2d(depth, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.LeakyReLU(0.1, inplace=True),\n            )\n        else:\n            self.conv = nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.LeakyReLU(0.1, inplace=True),\n            )\n        self.depth = oup\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass _residual_block(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio=0.5):\n        super(_residual_block, self).__init__()\n        self.use_res_connect = stride == 1 and inp == oup\n        if self.use_res_connect:\n            depth = int(oup*expand_ratio)\n            self.conv = nn.Sequential(\n                nn.Conv2d(inp, depth, 1, 1, bias=False),\n                nn.BatchNorm2d(depth),\n                nn.LeakyReLU(0.1, inplace=True),\n                nn.Conv2d(depth, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.LeakyReLU(0.1, inplace=True),\n            )\n        else:\n            self.conv = nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.LeakyReLU(0.1, inplace=True),\n            )\n        self.depth = oup\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\ndef darknet(conv_defs, depth_multiplier=1.0, min_depth=8):\n    depth = lambda d: max(int(d * depth_multiplier), min_depth)\n    layers = []\n    in_channels = 3\n    for conv_def in conv_defs:\n        if isinstance(conv_def, Conv):\n            layers += [_conv_bn(in_channels, depth(conv_def.depth), conv_def.stride)]\n            in_channels = depth(conv_def.depth)\n        elif isinstance(conv_def, ConvBlock):\n          for n in range(conv_def.num):\n            stride = conv_def.stride if n == 0 else 1\n            layers += [_conv_block(in_channels, depth(conv_def.depth), stride, conv_def.t)]\n            in_channels = depth(conv_def.depth)\n        elif isinstance(conv_def, ResidualBlock):\n          for n in range(conv_def.num):\n            stride = conv_def.stride if n == 0 else 1\n            layers += [_residual_block(in_channels, depth(conv_def.depth), stride, conv_def.t)]\n            in_channels = depth(conv_def.depth)\n        elif conv_def == 'M':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n    return layers\n\ndef wrapped_partial(func, *args, **kwargs):\n    partial_func = functools.partial(func, *args, **kwargs)\n    functools.update_wrapper(partial_func, func)\n    return partial_func\n\ndarknet_19 = wrapped_partial(darknet, conv_defs=CONV_DEFS_19, depth_multiplier=1.0)\ndarknet_53 = wrapped_partial(darknet, conv_defs=CONV_DEFS_53, depth_multiplier=1.0)\n"""
lib/modeling/nets/mobilenet.py,1,"b""import torch\nimport torch.nn as nn\n\nfrom collections import namedtuple\nimport functools\n\nConv = namedtuple('Conv', ['stride', 'depth'])\nDepthSepConv = namedtuple('DepthSepConv', ['stride', 'depth'])\nInvertedResidual = namedtuple('InvertedResidual', ['stride', 'depth', 'num', 't']) # t is the expension factor\n\nV1_CONV_DEFS = [\n    Conv(stride=2, depth=32),\n    DepthSepConv(stride=1, depth=64),\n    DepthSepConv(stride=2, depth=128),\n    DepthSepConv(stride=1, depth=128),\n    DepthSepConv(stride=2, depth=256),\n    DepthSepConv(stride=1, depth=256),\n    DepthSepConv(stride=2, depth=512),\n    DepthSepConv(stride=1, depth=512),\n    DepthSepConv(stride=1, depth=512),\n    DepthSepConv(stride=1, depth=512),\n    DepthSepConv(stride=1, depth=512),\n    DepthSepConv(stride=1, depth=512),\n    DepthSepConv(stride=2, depth=1024),\n    DepthSepConv(stride=1, depth=1024)\n]\n\nV2_CONV_DEFS = [\n    Conv(stride=2, depth=32),\n    InvertedResidual(stride=1, depth=16, num=1, t=1),\n    InvertedResidual(stride=2, depth=24, num=2, t=6),\n    InvertedResidual(stride=2, depth=32, num=3, t=6),\n    InvertedResidual(stride=2, depth=64, num=4, t=6),\n    InvertedResidual(stride=1, depth=96, num=3, t=6),\n    InvertedResidual(stride=2, depth=160, num=3, t=6),\n    InvertedResidual(stride=1, depth=320, num=1, t=6),\n]\n\nclass _conv_bn(nn.Module):\n    def __init__(self, inp, oup, stride):\n        super(_conv_bn, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n            nn.BatchNorm2d(oup),\n            nn.ReLU(inplace=True),\n        )\n        self.depth = oup\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass _conv_dw(nn.Module):\n    def __init__(self, inp, oup, stride):\n        super(_conv_dw, self).__init__()\n        self.conv = nn.Sequential(\n            # dw\n            nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n            nn.BatchNorm2d(inp),\n            nn.ReLU(inplace=True),\n            # pw\n            nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(oup),\n            nn.ReLU(inplace=True),\n        )\n        self.depth = oup\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass _inverted_residual_bottleneck(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(_inverted_residual_bottleneck, self).__init__()\n        self.use_res_connect = stride == 1 and inp == oup\n        self.conv = nn.Sequential(\n            # pw\n            nn.Conv2d(inp, inp * expand_ratio, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(inp * expand_ratio),\n            nn.ReLU6(inplace=True),\n            # dw\n            nn.Conv2d(inp * expand_ratio, inp * expand_ratio, 3, stride, 1, groups=inp * expand_ratio, bias=False),\n            nn.BatchNorm2d(inp * expand_ratio),\n            nn.ReLU6(inplace=True),\n            # pw-linear\n            nn.Conv2d(inp * expand_ratio, oup, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(oup),\n        )\n        self.depth = oup\n        \n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\ndef mobilenet(conv_defs, depth_multiplier=1.0, min_depth=8):\n    depth = lambda d: max(int(d * depth_multiplier), min_depth)\n    layers = []\n    in_channels = 3\n    for conv_def in conv_defs:\n        if isinstance(conv_def, Conv):\n            layers += [_conv_bn(in_channels, depth(conv_def.depth), conv_def.stride)]\n            in_channels = depth(conv_def.depth)\n        elif isinstance(conv_def, DepthSepConv):\n            layers += [_conv_dw(in_channels, depth(conv_def.depth), conv_def.stride)]\n            in_channels = depth(conv_def.depth)\n        elif isinstance(conv_def, InvertedResidual):\n          for n in range(conv_def.num):\n            stride = conv_def.stride if n == 0 else 1\n            layers += [_inverted_residual_bottleneck(in_channels, depth(conv_def.depth), stride, conv_def.t)]\n            in_channels = depth(conv_def.depth)\n    return layers\n\ndef wrapped_partial(func, *args, **kwargs):\n    partial_func = functools.partial(func, *args, **kwargs)\n    functools.update_wrapper(partial_func, func)\n    return partial_func\n\nmobilenet_v1 = wrapped_partial(mobilenet, conv_defs=V1_CONV_DEFS, depth_multiplier=1.0)\nmobilenet_v1_075 = wrapped_partial(mobilenet, conv_defs=V1_CONV_DEFS, depth_multiplier=0.75)\nmobilenet_v1_050 = wrapped_partial(mobilenet, conv_defs=V1_CONV_DEFS, depth_multiplier=0.50)\nmobilenet_v1_025 = wrapped_partial(mobilenet, conv_defs=V1_CONV_DEFS, depth_multiplier=0.25)\n\nmobilenet_v2 = wrapped_partial(mobilenet, conv_defs=V2_CONV_DEFS, depth_multiplier=1.0)\nmobilenet_v2_075 = wrapped_partial(mobilenet, conv_defs=V2_CONV_DEFS, depth_multiplier=0.75)\nmobilenet_v2_050 = wrapped_partial(mobilenet, conv_defs=V2_CONV_DEFS, depth_multiplier=0.50)\nmobilenet_v2_025 = wrapped_partial(mobilenet, conv_defs=V2_CONV_DEFS, depth_multiplier=0.25)"""
lib/modeling/nets/resnet.py,1,"b""import torch\nimport torch.nn as nn\n\nfrom collections import namedtuple\nimport functools\n\nBasicBlock = namedtuple('BasicBlock', ['stride', 'depth', 'num', 't'])\nBottleneck = namedtuple('Bottleneck', ['stride', 'depth', 'num', 't']) # t is the expension factor\n\nV18_CONV_DEFS = [\n    BasicBlock(stride=1, depth=64, num=2, t=1),\n    BasicBlock(stride=2, depth=128, num=2, t=1),\n    BasicBlock(stride=2, depth=256, num=2, t=1),\n    # BasicBlock(stride=2, depth=512, num=2, t=1),\n]\n\nV34_CONV_DEFS = [\n    BasicBlock(stride=1, depth=64, num=3, t=1),\n    BasicBlock(stride=2, depth=128, num=4, t=1),\n    BasicBlock(stride=2, depth=256, num=6, t=1),\n    # BasicBlock(stride=2, depth=512, num=3, t=1),\n]\n\nV50_CONV_DEFS = [\n    Bottleneck(stride=1, depth=64, num=3, t=4),\n    Bottleneck(stride=2, depth=128, num=4, t=4),\n    Bottleneck(stride=2, depth=256, num=6, t=4),\n    # Bottleneck(stride=2, depth=512, num=3, t=4),\n]\n\nV101_CONV_DEFS = [\n    Bottleneck(stride=1, depth=64, num=3, t=4),\n    Bottleneck(stride=2, depth=128, num=4, t=4),\n    Bottleneck(stride=2, depth=256, num=23, t=4),\n    # Bottleneck(stride=2, depth=512, num=3, t=4),\n]\n\nclass _basicblock(nn.Module):\n    def __init__(self, inplanes, planes, stride=1, expansion=1, downsample=None):\n        super(_basicblock, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(planes, planes * expansion, kernel_size=3, stride=1,\n                     padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes * expansion)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass _bottleneck(nn.Module):\n    def __init__(self, inplanes, planes, stride=1, expansion=4, downsample=None):\n        super(_bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\ndef resnet(conv_defs, depth_multiplier=1.0, min_depth=8):\n    depth = lambda d: max(int(d * depth_multiplier), min_depth)\n    layers = [\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n        ]\n    in_channels = 64\n    for conv_def in conv_defs:\n        if conv_def.stride != 1 or in_channels != depth(conv_def.depth * conv_def.t):\n            _downsample = nn.Sequential(\n                nn.Conv2d(in_channels, depth(conv_def.depth * conv_def.t),\n                          kernel_size=1, stride=conv_def.stride, bias=False),\n                nn.BatchNorm2d(depth(conv_def.depth * conv_def.t)),\n            )\n        if isinstance(conv_def, BasicBlock):\n          for n in range(conv_def.num):\n            (stride, downsample) = (conv_def.stride, _downsample) if n == 0 else (1, None)\n            layers += [_basicblock(in_channels, depth(conv_def.depth), stride, conv_def.t, downsample)]\n            in_channels = depth(conv_def.depth * conv_def.t)\n        elif isinstance(conv_def, Bottleneck):\n          for n in range(conv_def.num):\n            (stride, downsample) = (conv_def.stride, _downsample) if n == 0 else (1, None)\n            layers += [_bottleneck(in_channels, depth(conv_def.depth), stride, conv_def.t, downsample)]\n            in_channels = depth(conv_def.depth * conv_def.t)\n    return layers\n\ndef wrapped_partial(func, *args, **kwargs):\n    partial_func = functools.partial(func, *args, **kwargs)\n    functools.update_wrapper(partial_func, func)\n    return partial_func\n\nresnet_18 = wrapped_partial(resnet, conv_defs=V18_CONV_DEFS, depth_multiplier=1.0)\nresnet_34 = wrapped_partial(resnet, conv_defs=V34_CONV_DEFS, depth_multiplier=1.0)\n\nresnet_50 = wrapped_partial(resnet, conv_defs=V50_CONV_DEFS, depth_multiplier=1.0)\nresnet_101 = wrapped_partial(resnet, conv_defs=V101_CONV_DEFS, depth_multiplier=1.0)\n"""
lib/modeling/nets/vgg.py,1,"b""import torch\nimport torch.nn as nn\n\nbase = {\n    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M',\n            512, 512, 512],\n}\n\n# CONV_DEFS_16 = [\n#     Conv(stride=1, depth=64),\n#     Conv(stride=1, depth=64),\n#     'M',\n#     Conv(stride=1, depth=128),\n#     Conv(stride=1, depth=128),\n#     'M'\n#     Conv(stride=1, depth=256),\n#     Conv(stride=1, depth=256),\n#     Conv(stride=1, depth=256),\n#     'M'\n#     Conv(stride=1, depth=512),\n#     Conv(stride=1, depth=512),\n#     Conv(stride=1, depth=512),\n#     'M'\n#     Conv(stride=1, depth=512),\n#     Conv(stride=1, depth=512),\n#     Conv(stride=1, depth=512),\n# ]\n\n# Conv = namedtuple('Conv', ['stride', 'depth'])\n\n# class _conv_bn(nn.Module):\n#     def __init__(self, inp, oup, stride):\n#         super(_conv_bn, self).__init__()\n#         self.conv = nn.Sequential(\n#             nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n#             nn.BatchNorm2d(oup),\n#             nn.ReLU(inplace=True),\n#         )\n#         self.depth = oup\n\n#     def forward(self, x):\n#         return self.conv(x)\n\n\ndef vgg(cfg, i, batch_norm=False):\n    layers = []\n    in_channels = i\n    for v in cfg:\n        if v == 'M':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        elif v == 'C':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = v\n    layers += [\n        nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n        nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(1024, 1024, kernel_size=1),\n        nn.ReLU(inplace=True)]\n    return layers\n\ndef vgg16():\n    return vgg(base['vgg16'], 3)\nvgg16.name='vgg16'"""
lib/modeling/ssds/__init__.py,0,b''
lib/modeling/ssds/fssd.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport os\n\nfrom lib.layers import *\n\nclass FSSD(nn.Module):\n    """"""FSSD: Feature Fusion Single Shot Multibox Detector\n    See: https://arxiv.org/pdf/1712.00960.pdf for more details.\n\n    Args:\n        phase: (string) Can be ""eval"" or ""train"" or ""feature""\n        base: base layers for input\n        extras: extra layers that feed to multibox loc and conf layers\n        head: ""multibox head"" consists of loc and conf conv layers\n        features\xef\xbc\x9a include to feature layers to fusion feature and build pyramids\n        feature_layer: the feature layers for head to loc and conf\n        num_classes: num of classes \n    """"""\n\n    def __init__(self, base, extras, head, features, feature_layer, num_classes):\n        super(FSSD, self).__init__()\n        self.num_classes = num_classes\n        # SSD network\n        self.base = nn.ModuleList(base)\n        self.extras = nn.ModuleList(extras)\n        self.feature_layer = feature_layer[0][0]\n        self.transforms = nn.ModuleList(features[0])\n        self.pyramids = nn.ModuleList(features[1])\n        # print(self.base)\n        # Layer learns to scale the l2 normalized features from conv4_3\n        self.norm = nn.BatchNorm2d(int(feature_layer[0][1][-1]/2)*len(self.transforms),affine=True)\n        # print(self.extras)\n\n        self.loc = nn.ModuleList(head[0])\n        self.conf = nn.ModuleList(head[1])\n        # print(self.loc)\n\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x, phase=\'eval\'):\n        """"""Applies network layers and ops on input image(s) x.\n\n        Args:\n            x: input image or batch of images. Shape: [batch,3,300,300].\n\n        Return:\n            Depending on phase:\n            test:\n                Variable(tensor) of output class label predictions,\n                confidence score, and corresponding location predictions for\n                each object detected. Shape: [batch,topk,7]\n\n            train:\n                list of concat outputs from:\n                    1: confidence layers, Shape: [batch*num_priors,num_classes]\n                    2: localization layers, Shape: [batch,num_priors*4]\n\n            feature:\n                the features maps of the feature extractor\n        """"""\n        sources, transformed, pyramids, loc, conf = [list() for _ in range(5)]\n\n        # apply bases layers and cache source layer outputs\n        for k in range(len(self.base)):\n            x = self.base[k](x)\n            if k in self.feature_layer:\n                sources.append(x)\n\n        # apply extra layers and cache source layer outputs\n        for k, v in enumerate(self.extras):\n            x = v(x)\n            #TODO: different with lite this one should be change\n            if k % 2 == 1:\n                sources.append(x)\n        assert len(self.transforms) == len(sources)\n        upsize = (sources[0].size()[2], sources[0].size()[3])\n\n        for k, v in enumerate(self.transforms):\n            size = None if k == 0 else upsize\n            transformed.append(v(sources[k], size))\n        x = torch.cat(transformed, 1)\n        x = self.norm(x)\n        for k, v in enumerate(self.pyramids):\n            x = v(x)\n            pyramids.append(x)\n\n        if phase == \'feature\':\n            return pyramids\n\n        # apply multibox head to pyramids layers\n        for (x, l, c) in zip(pyramids, self.loc, self.conf):\n            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n\n        if phase == \'eval\':\n            output = (\n                loc.view(loc.size(0), -1, 4),                   # loc preds\n                self.softmax(conf.view(-1, self.num_classes)),  # conf preds\n            )\n        else:\n            output = (\n                loc.view(loc.size(0), -1, 4),\n                conf.view(conf.size(0), -1, self.num_classes),\n            )\n        return output\n\nclass BasicConv(nn.Module):\n    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=False, bias=True):\n        super(BasicConv, self).__init__()\n        self.out_channels = out_planes\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n        self.relu = nn.ReLU(inplace=True) if relu else None\n        # self.up_size = up_size\n        # self.up_sample = nn.Upsample(size=(up_size,up_size),mode=\'bilinear\') if up_size != 0 else None\n\n    def forward(self, x, up_size=None):\n        x = self.conv(x)\n        if self.bn is not None:\n            x = self.bn(x)\n        if self.relu is not None:\n            x = self.relu(x)\n        if up_size is not None:\n            x = F.upsample(x, size=up_size, mode=\'bilinear\')\n            # x = self.up_sample(x)\n        return x\n\ndef _conv_dw(inp, oup, stride=1, padding=0, expand_ratio=1):\n    return nn.Sequential(\n        # pw\n        nn.Conv2d(inp, oup * expand_ratio, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup * expand_ratio),\n        nn.ReLU6(inplace=True),\n        # dw\n        nn.Conv2d(oup * expand_ratio, oup * expand_ratio, 3, stride, padding, groups=oup * expand_ratio, bias=False),\n        nn.BatchNorm2d(oup * expand_ratio),\n        nn.ReLU6(inplace=True),\n        # pw-linear\n        nn.Conv2d(oup * expand_ratio, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n    )\n\ndef add_extras(base, feature_layer, mbox, num_classes, version):\n    extra_layers = []\n    feature_transform_layers = []\n    pyramid_feature_layers = []\n    loc_layers = []\n    conf_layers = []\n    in_channels = None\n    feature_transform_channel = int(feature_layer[0][1][-1]/2)\n    for layer, depth in zip(feature_layer[0][0], feature_layer[0][1]):\n        if \'lite\' in version:\n            if layer == \'S\':\n                extra_layers += [ _conv_dw(in_channels, depth, stride=2, padding=1, expand_ratio=1) ]\n                in_channels = depth\n            elif layer == \'\':\n                extra_layers += [ _conv_dw(in_channels, depth, stride=1, expand_ratio=1) ]\n                in_channels = depth\n            else:\n                in_channels = depth\n        else:\n            if layer == \'S\':\n                extra_layers += [\n                        nn.Conv2d(in_channels, int(depth/2), kernel_size=1),\n                        nn.Conv2d(int(depth/2), depth, kernel_size=3, stride=2, padding=1)  ]\n                in_channels = depth\n            elif layer == \'\':\n                extra_layers += [\n                        nn.Conv2d(in_channels, int(depth/2), kernel_size=1),\n                        nn.Conv2d(int(depth/2), depth, kernel_size=3)  ]\n                in_channels = depth\n            else:\n                in_channels = depth\n        feature_transform_layers += [BasicConv(in_channels, feature_transform_channel, kernel_size=1, padding=0)]\n    \n    in_channels = len(feature_transform_layers) * feature_transform_channel\n    for layer, depth, box in zip(feature_layer[1][0], feature_layer[1][1], mbox):\n        if layer == \'S\':\n            pyramid_feature_layers += [BasicConv(in_channels, depth, kernel_size=3, stride=2, padding=1)]\n            in_channels = depth\n        elif layer == \'\':\n            pad = (0,1)[len(pyramid_feature_layers)==0]\n            pyramid_feature_layers += [BasicConv(in_channels, depth, kernel_size=3, stride=1, padding=pad)]\n            in_channels = depth\n        else:\n            AssertionError(\'Undefined layer\')\n        loc_layers += [nn.Conv2d(in_channels, box * 4, kernel_size=3, padding=1)]\n        conf_layers += [nn.Conv2d(in_channels, box * num_classes, kernel_size=3, padding=1)]\n    return base, extra_layers, (feature_transform_layers, pyramid_feature_layers), (loc_layers, conf_layers)\n\ndef build_fssd(base, feature_layer, mbox, num_classes):\n    base_, extras_, features_, head_ = add_extras(base(), feature_layer, mbox, num_classes, version=\'fssd\')\n    return FSSD(base_, extras_, head_, features_, feature_layer, num_classes)\n\ndef build_fssd_lite(base, feature_layer, mbox, num_classes):\n    base_, extras_, features_, head_ = add_extras(base(), feature_layer, mbox, num_classes, version=\'fssd_lite\')\n    return FSSD(base_, extras_, head_, features_, feature_layer, num_classes)'"
lib/modeling/ssds/fssd_lite.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport os\n\nfrom lib.layers import *\n\nclass FSSDLite(nn.Module):\n    """"""FSSD: Feature Fusion Single Shot Multibox Detector for embeded system\n    See: https://arxiv.org/pdf/1712.00960.pdf for more details.\n\n    Args:\n        phase: (string) Can be ""eval"" or ""train"" or ""feature""\n        base: base layers for input\n        extras: extra layers that feed to multibox loc and conf layers\n        head: ""multibox head"" consists of loc and conf conv layers\n        features\xef\xbc\x9a include to feature layers to fusion feature and build pyramids\n        feature_layer: the feature layers for head to loc and conf\n        num_classes: num of classes \n    """"""\n\n    def __init__(self, base, extras, head, features, feature_layer, num_classes):\n        super(FSSDLite, self).__init__()\n        self.num_classes = num_classes\n        # SSD network\n        self.base = nn.ModuleList(base)\n        self.extras = nn.ModuleList(extras)\n        self.feature_layer = feature_layer[0][0]\n        self.transforms = nn.ModuleList(features[0])\n        self.pyramids = nn.ModuleList(features[1])\n        # print(self.base)\n        self.norm = nn.BatchNorm2d(int(feature_layer[0][1][-1]/2)*len(self.transforms),affine=True)\n        # print(self.extras)\n\n        self.loc = nn.ModuleList(head[0])\n        self.conf = nn.ModuleList(head[1])\n        # print(self.loc)\n\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x, phase=\'eval\'):\n        """"""Applies network layers and ops on input image(s) x.\n\n        Args:\n            x: input image or batch of images. Shape: [batch,3,300,300].\n\n        Return:\n            Depending on phase:\n            test:\n                Variable(tensor) of output class label predictions,\n                confidence score, and corresponding location predictions for\n                each object detected. Shape: [batch,topk,7]\n\n            train:\n                list of concat outputs from:\n                    1: confidence layers, Shape: [batch*num_priors,num_classes]\n                    2: localization layers, Shape: [batch,num_priors*4]\n\n            feature:\n                the features maps of the feature extractor\n        """"""\n        sources, transformed, pyramids, loc, conf = [list() for _ in range(5)]\n\n        # apply bases layers and cache source layer outputs\n        for k in range(len(self.base)):\n            x = self.base[k](x)\n            if k in self.feature_layer:\n                sources.append(x)\n\n        # apply extra layers and cache source layer outputs\n        for k, v in enumerate(self.extras):\n            x = v(x)\n            sources.append(x)\n            # if k % 2 == 1:\n            #     sources.append(x)\n        assert len(self.transforms) == len(sources)\n        upsize = (sources[0].size()[2], sources[0].size()[3])\n        \n        for k, v in enumerate(self.transforms):\n            size = None if k == 0 else upsize\n            transformed.append(v(sources[k], size))\n        x = torch.cat(transformed, 1)\n        x = self.norm(x)\n        for k, v in enumerate(self.pyramids):\n            x = v(x)\n            pyramids.append(x)\n\n        if phase == \'feature\':\n            return pyramids\n\n        # apply multibox head to pyramids layers\n        for (x, l, c) in zip(pyramids, self.loc, self.conf):\n            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n\n        if phase == \'eval\':\n            output = (\n                loc.view(loc.size(0), -1, 4),                   # loc preds\n                self.softmax(conf.view(-1, self.num_classes)),  # conf preds\n            )\n        else:\n            output = (\n                loc.view(loc.size(0), -1, 4),\n                conf.view(conf.size(0), -1, self.num_classes),\n            )\n        return output\n\nclass BasicConv(nn.Module):\n    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=False, bias=True):\n        super(BasicConv, self).__init__()\n        self.out_channels = out_planes\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n        self.relu = nn.ReLU(inplace=True) if relu else None\n        # self.up_size = up_size\n        # self.up_sample = nn.Upsample(size=(up_size,up_size),mode=\'bilinear\') if up_size != 0 else None\n\n    def forward(self, x, up_size=None):\n        x = self.conv(x)\n        if self.bn is not None:\n            x = self.bn(x)\n        if self.relu is not None:\n            x = self.relu(x)\n        if up_size is not None:\n            x = F.upsample(x, size=up_size, mode=\'bilinear\')\n            # x = self.up_sample(x)\n        return x\n\ndef _conv_dw(inp, oup, stride=1, padding=0, expand_ratio=1):\n    return nn.Sequential(\n        # pw\n        nn.Conv2d(inp, oup * expand_ratio, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup * expand_ratio),\n        nn.ReLU6(inplace=True),\n        # dw\n        nn.Conv2d(oup * expand_ratio, oup * expand_ratio, 3, stride, padding, groups=oup * expand_ratio, bias=False),\n        nn.BatchNorm2d(oup * expand_ratio),\n        nn.ReLU6(inplace=True),\n        # pw-linear\n        nn.Conv2d(oup * expand_ratio, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n    )\n\n\ndef add_extras(base, feature_layer, mbox, num_classes):\n    extra_layers = []\n    feature_transform_layers = []\n    pyramid_feature_layers = []\n    loc_layers = []\n    conf_layers = []\n    in_channels = None\n    feature_transform_channel = int(feature_layer[0][1][-1]/2)\n    for layer, depth in zip(feature_layer[0][0], feature_layer[0][1]):\n        if layer == \'S\':\n            extra_layers += [ _conv_dw(in_channels, depth, stride=2, padding=1, expand_ratio=1) ]\n            in_channels = depth\n        elif layer == \'\':\n            extra_layers += [ _conv_dw(in_channels, depth, stride=1, expand_ratio=1) ]\n            in_channels = depth\n        else:\n            in_channels = depth\n        feature_transform_layers += [BasicConv(in_channels, feature_transform_channel, kernel_size=1, padding=0)]\n    \n    in_channels = len(feature_transform_layers) * feature_transform_channel\n    for layer, depth, box in zip(feature_layer[1][0], feature_layer[1][1], mbox):\n        if layer == \'S\':\n            pyramid_feature_layers += [BasicConv(in_channels, depth, kernel_size=3, stride=2, padding=1)]\n            in_channels = depth\n        elif layer == \'\':\n            pad = (0,1)[len(pyramid_feature_layers)==0]\n            pyramid_feature_layers += [BasicConv(in_channels, depth, kernel_size=3, stride=1, padding=pad)]\n            in_channels = depth\n        else:\n            AssertionError(\'Undefined layer\')\n        loc_layers += [nn.Conv2d(in_channels, box * 4, kernel_size=3, padding=1)]\n        conf_layers += [nn.Conv2d(in_channels, box * num_classes, kernel_size=3, padding=1)]\n    return base, extra_layers, (feature_transform_layers, pyramid_feature_layers), (loc_layers, conf_layers)\n\ndef build_fssd_lite(base, feature_layer, mbox, num_classes):\n    base_, extras_, features_, head_ = add_extras(base(), feature_layer, mbox, num_classes)\n    return FSSDLite(base_, extras_, head_, features_, feature_layer, num_classes)'"
lib/modeling/ssds/retina.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport os\n\nfrom lib.layers import *\n\nclass Retina(nn.Module):\n    def __init__(self, base, extras, head, feature_layer, num_classes):\n        super(Retina, self).__init__()\n        self.num_classes = num_classes\n        # SSD network\n        self.base = nn.ModuleList(base)\n        self.extras = nn.ModuleList(extras[1])\n        self.transforms = nn.ModuleList(extras[0])\n        self.loc = nn.ModuleList(head[0])\n        self.conf = nn.ModuleList(head[1])\n        self.softmax = nn.Softmax(dim=-1)\n\n        self.feature_layer = feature_layer[0]\n    \n    def _upsample_add(self, x, y):\n        \'\'\'Upsample and add two feature maps.\n        Args:\n          x: (Variable) top feature map to be upsampled.\n          y: (Variable) lateral feature map.\n        Returns:\n          (Variable) added feature map.\n        Note in PyTorch, when input size is odd, the upsampled feature map\n        with `F.upsample(..., scale_factor=2, mode=\'nearest\')`\n        maybe not equal to the lateral feature map size.\n        e.g.\n        original input size: [N,_,15,15] ->\n        conv2d feature map size: [N,_,8,8] ->\n        upsampled feature map size: [N,_,16,16]\n        So we choose bilinear upsample which supports arbitrary output sizes.\n        \'\'\'\n        _,_,H,W = y.size()\n        return F.upsample(x, size=(H,W), mode=\'bilinear\') + y    \n\n    def forward(self, x, phase=\'eval\'):\n        sources, loc, conf = [list() for _ in range(3)]\n\n        # apply bases layers and cache source layer outputs\n        for k in range(len(self.base)):\n            x = self.base[k](x)\n            if k in self.feature_layer:\n                sources.append(x)\n\n        for i in range(len(sources))[::-1]:\n            if i != len(sources) -1:\n                xx = self.extras[i](self._upsample_add(xx, self.transforms[i](sources[i])))\n            else:\n                xx = self.transforms[i](sources[i])\n            sources[i] = xx\n\n        # apply extra layers and cache source layer outputs\n        for i, v in enumerate(self.extras):\n            if i >= len(sources):\n                x = v(x)\n                sources.append(x)\n\n        if phase == \'feature\':\n            return sources\n\n        # apply multibox head to source layers\n        for x in sources:\n            loc.append(self.loc(x).permute(0, 2, 3, 1).contiguous())\n            conf.append(self.conf(x).permute(0, 2, 3, 1).contiguous())\n        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n\n        if phase == \'eval\':\n            output = (\n                loc.view(loc.size(0), -1, 4),                   # loc preds\n                self.softmax(conf.view(-1, self.num_classes)),  # conf preds\n            )\n        else:\n            output = (\n                loc.view(loc.size(0), -1, 4),\n                conf.view(conf.size(0), -1, self.num_classes),\n            )\n        return output\n        \n\n\ndef add_extras(base, feature_layer, mbox, num_classes, version):\n    extra_layers = []\n    transform_layers = []\n    loc_layers = [Retina_head(box * 4)]\n    conf_layers = [Retina_head(box * num_classes)]\n\n    for layer, in_channels, box in zip(feature_layer[0], feature_layer[1], mbox):\n        if \'lite\' in version:\n            if layer == \'S\':\n                extra_layers += [ _conv_dw(in_channels, 256, stride=2, padding=1, expand_ratio=1) ]\n            elif layer == \'\':\n                extra_layers += [ _conv_dw(in_channels, 256, stride=1, expand_ratio=1) ]\n            else:\n                extra_layers += [ _conv_dw(256, 256, stride=1, padding=1, expand_ratio=1) ]\n                transform_layers += [ _conv_pw(in_channels, 256) ]\n        else:    \n            if layer == \'S\':\n                extra_layers += [ _conv(in_channels, 256, stride=2, padding=1) ]\n            elif layer == \'\':\n                extra_layers += [ _conv(in_channels, 256, stride=1) ]\n            else:\n                extra_layers += [ _conv(256, 256, stride=1, padding=1) ]\n                transform_layers += [ _conv_pw(in_channels, 256) ]\n    return base, (transform_layers, extra_layers), (loc_layers, conf_layers)\n\ndef Retina_head(self, out_planes):\n    layers = []\n    for _ in range(4):\n        layers.append(nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1))\n        layers.append(nn.ReLU(True))\n    layers.append(nn.Conv2d(256, out_planes, kernel_size=3, stride=1, padding=1))\n    return nn.Sequential(*layers)\n\n# based on the implementation in https://github.com/tensorflow/models/blob/master/research/object_detection/models/feature_map_generators.py#L213\n# when the expand_ratio is 1, the implemetation is nearly same. Since the shape is always change, I do not add the shortcut as what mobilenetv2 did.\ndef _conv_dw(inp, oup, stride=1, padding=0, expand_ratio=1):\n    return nn.Sequential(\n        # pw\n        nn.Conv2d(inp, oup * expand_ratio, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup * expand_ratio),\n        nn.ReLU6(inplace=True),\n        # dw\n        nn.Conv2d(oup * expand_ratio, oup * expand_ratio, 3, stride, padding, groups=oup * expand_ratio, bias=False),\n        nn.BatchNorm2d(oup * expand_ratio),\n        nn.ReLU6(inplace=True),\n        # pw-linear\n        nn.Conv2d(oup * expand_ratio, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n    )\n\ndef _conv_pw(inp, oup, stride=1, padding=0):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n    )\n\n\ndef _conv(inp, oup, stride=1, padding=0):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, padding, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU(inplace=True),\n    )\n\n\ndef build_retina(base, feature_layer, mbox, num_classes):\n    """"""RetinaNet in Focal Loss for Dense Object Detection\n    See: https://arxiv.org/pdf/1708.02002.pdffor more details.\n    """"""\n    base_, extras_, head_ = add_extras(base(), feature_layer, mbox, num_classes, version=\'retinanet\')\n    return Retina(base_, extras_, head_, feature_layer, num_classes)\n\ndef build_retina_lite(base, feature_layer, mbox, num_classes):\n    """"""RetinaNet in Focal Loss for Dense Object Detection\n    See: https://arxiv.org/pdf/1708.02002.pdffor more details.\n    """"""\n    base_, extras_, head_ = add_extras(base(), feature_layer, mbox, num_classes, version=\'retinanet_lite\')\n    return SSD(base_, extras_, head_, feature_layer, num_classes)'"
lib/modeling/ssds/rfb.py,9,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport os\n\nfrom lib.layers import *\n\nclass RFB(nn.Module):\n    """"""Receptive Field Block Net for Accurate and Fast Object Detection\n    See: https://arxiv.org/pdf/1711.07767.pdf for more details.\n\n    Args:\n        phase: (string) Can be ""eval"" or ""train"" or ""feature""\n        base: base layers for input\n        extras: extra layers that feed to multibox loc and conf layers\n        norm: norm to add RFB module for previous feature extractor\n        head: ""multibox head"" consists of loc and conf conv layers\n        feature_layer: the feature layers for head to loc and conf\n        num_classes: num of classes \n    """"""\n    def __init__(self, base, extras, norm, head, feature_layer, num_classes):\n        super(RFB, self).__init__()\n        self.num_classes = num_classes\n        # RFB network\n        self.base = nn.ModuleList(base)\n        self.norm = nn.ModuleList(norm)\n        self.extras = nn.ModuleList(extras)\n        \n        self.loc = nn.ModuleList(head[0])\n        self.conf = nn.ModuleList(head[1])\n        self.softmax = nn.Softmax(dim=-1)\n\n        self.feature_layer = feature_layer[0]\n        self.indicator = 0\n        for layer in self.feature_layer:\n            if isinstance(layer, int):\n                continue\n            elif layer == \'\' or layer == \'S\':\n                break\n            else:\n                self.indicator += 1 \n\n\n\n    def forward(self, x, phase=\'eval\'):\n        """"""Applies network layers and ops on input image(s) x.\n\n        Args:\n            x: input image or batch of images. Shape: [batch,3,300,300].\n\n        Return:\n            Depending on phase:\n            test:\n                Variable(tensor) of output class label predictions,\n                confidence score, and corresponding location predictions for\n                each object detected. Shape: [batch,topk,7]\n\n            train:\n                list of concat outputs from:\n                    1: confidence layers, Shape: [batch*num_priors,num_classes]\n                    2: localization layers, Shape: [batch,num_priors*4]\n\n            feature:\n                the features maps of the feature extractor\n        """"""\n        sources, loc, conf = [list() for _ in range(3)]\n\n        # apply bases layers and cache source layer outputs\n        for k in range(len(self.base)):\n            x = self.base[k](x)\n            if k in self.feature_layer:\n                idx = self.feature_layer.index(k)\n                # some xxx\n                if(len(sources)) == 0:\n                    sources.append(self.norm[idx](x))\n                else:\n                    x = self.norm[idx](x)\n                    sources.append(x)\n\n        # apply extra layers and cache source layer outputs\n        for k, v in enumerate(self.extras):\n            x = v(x)\n            if k < self.indicator or k % 2 == 1:\n                sources.append(x)\n\n        if phase == \'feature\':\n            return sources\n\n        # apply multibox head to source layers\n        for (x, l, c) in zip(sources, self.loc, self.conf):\n            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n\n        if phase == \'eval\':\n            output = (\n                loc.view(loc.size(0), -1, 4),                   # loc preds\n                self.softmax(conf.view(-1, self.num_classes)),  # conf preds\n            )\n        else:\n            output = (\n                loc.view(loc.size(0), -1, 4),\n                conf.view(conf.size(0), -1, self.num_classes),\n            )\n        return output\n        \n\nclass BasicConv(nn.Module):\n    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n        super(BasicConv, self).__init__()\n        self.out_channels = out_planes\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n        self.relu = nn.ReLU(inplace=True) if relu else None\n    def forward(self, x):\n        x = self.conv(x)\n        if self.bn is not None:\n            x = self.bn(x)\n        if self.relu is not None:\n            x = self.relu(x)\n        return x\n\nclass BasicSepConv(nn.Module):\n    def __init__(self, in_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n        super(BasicSepConv, self).__init__()\n        self.out_channels = in_planes\n        self.conv = nn.Conv2d(in_planes, in_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n        self.bn = nn.BatchNorm2d(in_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n        self.relu = nn.ReLU(inplace=True) if relu else None\n    def forward(self, x):\n        x = self.conv(x)\n        if self.bn is not None:\n            x = self.bn(x)\n        if self.relu is not None:\n            x = self.relu(x)\n        return x\n\nclass BasicRFB_a(nn.Module):\n\n    def __init__(self, in_planes, out_planes, stride=1, scale = 0.1):\n        super(BasicRFB_a, self).__init__()\n        self.scale = scale\n        self.out_channels = out_planes\n        inter_planes = in_planes //4\n\n        self.branch0 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicConv(inter_planes, inter_planes, kernel_size=3, stride=1, padding=1,relu=False)\n                )\n        self.branch1 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicConv(inter_planes, inter_planes, kernel_size=(3,1), stride=1, padding=(1,0)),\n                BasicConv(inter_planes, inter_planes, kernel_size=3, stride=1, padding=3, dilation=3, relu=False)\n                )\n        self.branch2 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicConv(inter_planes, inter_planes, kernel_size=(1,3), stride=stride, padding=(0,1)),\n                BasicConv(inter_planes, inter_planes, kernel_size=3, stride=1, padding=3, dilation=3, relu=False)\n                )\n        self.branch3 = nn.Sequential(\n                BasicConv(in_planes, inter_planes//2, kernel_size=1, stride=1),\n                BasicConv(inter_planes//2, (inter_planes//4)*3, kernel_size=(1,3), stride=1, padding=(0,1)),\n                BasicConv((inter_planes//4)*3, inter_planes, kernel_size=(3,1), stride=stride, padding=(1,0)),\n                BasicConv(inter_planes, inter_planes, kernel_size=3, stride=1, padding=5, dilation=5, relu=False)\n                )\n\n        self.ConvLinear = BasicConv(4*inter_planes, out_planes, kernel_size=1, stride=1, relu=False)\n        self.shortcut = BasicConv(in_planes, out_planes, kernel_size=1, stride=stride, relu=False)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self,x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n\n        out = torch.cat((x0,x1,x2,x3),1)\n        out = self.ConvLinear(out)\n        short = self.shortcut(x)\n        out = out*self.scale + short\n        out = self.relu(out)\n        return out\n\nclass BasicRFB(nn.Module):\n    def __init__(self, in_planes, out_planes, stride=1, scale = 0.1, visual = 1):\n        super(BasicRFB, self).__init__()\n        self.scale = scale\n        self.out_channels = out_planes\n        inter_planes = in_planes // 8\n        self.branch0 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                #BasicConv(inter_planes, (inter_planes//2)*3, kernel_size=(1,3), stride=1, padding=(0,1)),\n                BasicConv(inter_planes, 2*inter_planes, kernel_size=(3,3), stride=stride, padding=(1,1)),\n                BasicConv(2*inter_planes, 2*inter_planes, kernel_size=3, stride=1, padding=visual+1, dilation=visual+1, relu=False)\n                )\n        self.branch1 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicConv(inter_planes, (inter_planes//2)*3, kernel_size=3, stride=1, padding=1),\n                BasicConv((inter_planes//2)*3, 2*inter_planes, kernel_size=3, stride=stride, padding=1),\n                BasicConv(2*inter_planes, 2*inter_planes, kernel_size=3, stride=1, padding=2*visual+1, dilation=2*visual+1, relu=False)\n                )\n\n        self.ConvLinear = BasicConv(4*inter_planes, out_planes, kernel_size=1, stride=1, relu=False)\n        self.shortcut = BasicConv(in_planes, out_planes, kernel_size=1, stride=stride, relu=False)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self,x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n\n        out = torch.cat((x0,x1),1)\n        out = self.ConvLinear(out)\n        short = self.shortcut(x)\n        out = out*self.scale + short\n        out = self.relu(out)\n\n        return out\n\nclass BasicRFB_a_lite(nn.Module):\n    def __init__(self, in_planes, out_planes, stride=1, scale = 0.1):\n        super(BasicRFB_a_lite, self).__init__()\n        self.scale = scale\n        self.out_channels = out_planes\n        inter_planes = in_planes //4\n\n        self.branch0 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicSepConv(inter_planes, kernel_size=3, stride=1, padding=1, dilation=1, relu=False)\n                )\n        self.branch1 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicConv(inter_planes, inter_planes, kernel_size=(3,1), stride=1, padding=(1,0)),\n                BasicSepConv(inter_planes, kernel_size=3, stride=1, padding=3, dilation=3, relu=False)\n                )\n        self.branch2 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicConv(inter_planes, inter_planes, kernel_size=(1,3), stride=stride, padding=(0,1)),\n                BasicSepConv(inter_planes, kernel_size=3, stride=1, padding=3, dilation=3, relu=False)\n                )\n        self.branch3 = nn.Sequential(\n                BasicConv(in_planes, inter_planes//2, kernel_size=1, stride=1),\n                BasicConv(inter_planes//2, (inter_planes//4)*3, kernel_size=(1,3), stride=1, padding=(0,1)),\n                BasicConv((inter_planes//4)*3, inter_planes, kernel_size=(3,1), stride=stride, padding=(1,0)),\n                BasicSepConv(inter_planes, kernel_size=3, stride=1, padding=5, dilation=5, relu=False)\n                )\n\n        self.ConvLinear = BasicConv(4*inter_planes, out_planes, kernel_size=1, stride=1, relu=False)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self,x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n\n        out = torch.cat((x0,x1,x2,x3),1)\n        out = self.ConvLinear(out)\n        out = out*self.scale + x\n        out = self.relu(out)\n\n        return out\n\nclass BasicRFB_lite(nn.Module):\n    def __init__(self, in_planes, out_planes, stride=1, scale = 0.1):\n        super(BasicRFB_lite, self).__init__()\n        self.scale = scale\n        self.out_channels = out_planes\n        inter_planes = in_planes // 8\n        self.branch1 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicConv(inter_planes, (inter_planes//2)*3, kernel_size=(1,3), stride=1, padding=(0,1)),\n                BasicConv((inter_planes//2)*3, (inter_planes//2)*3, kernel_size=(3,1), stride=stride, padding=(1,0)),\n                BasicSepConv((inter_planes//2)*3, kernel_size=3, stride=1, padding=3, dilation=3, relu=False)\n                )\n        self.branch2 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicConv(inter_planes, (inter_planes//2)*3, kernel_size=3, stride=1, padding=1),\n                BasicConv((inter_planes//2)*3, (inter_planes//2)*3, kernel_size=3, stride=stride, padding=1),\n                BasicSepConv((inter_planes//2)*3, kernel_size=3, stride=1, padding=5, dilation=5, relu=False)\n                )\n\n        self.ConvLinear = BasicConv(3*inter_planes, out_planes, kernel_size=1, stride=1, relu=False)\n        if in_planes == out_planes:\n            self.identity = True\n        else:\n            self.identity = False\n            self.shortcut = BasicConv(in_planes, out_planes, kernel_size=1, stride=stride, relu=False)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self,x):\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n\n        out = torch.cat((x1,x2),1)\n        out = self.ConvLinear(out)\n        if self.identity:\n            out = out*self.scale + x\n        else:\n            short = self.shortcut(x)\n            out = out*self.scale + short\n        out = self.relu(out)\n        return out\n\n\ndef add_extras(base, feature_layer, mbox, num_classes, version):\n    extra_layers = []\n    loc_layers = []\n    conf_layers = []\n    norm_layers = []\n    in_channels = None\n    for layer, depth, box in zip(feature_layer[0], feature_layer[1], mbox):\n        if \'lite\' in version:\n            pass\n        else:\n            if layer == \'RBF\':\n                extra_layers += [BasicRFB(in_channels, depth, stride=2, scale = 1.0, visual=2)]\n                in_channels = depth\n            elif layer == \'S\':\n                extra_layers += [\n                        BasicConv(in_channels, int(depth/2), kernel_size=1),\n                        BasicConv(int(depth/2), depth, kernel_size=3, stride=2, padding=1)  ]\n                in_channels = depth\n            elif layer == \'\':\n                extra_layers += [\n                        BasicConv(in_channels, int(depth/2), kernel_size=1),\n                        BasicConv(int(depth/2), depth, kernel_size=3)  ]\n                in_channels = depth\n            else:\n                if len(norm_layers) == 0:\n                    norm_layers += [BasicRFB_a(depth, depth, stride = 1, scale=1.0)]\n                else:\n                    norm_layers += [BasicRFB(depth, depth, scale = 1.0, visual=2)]\n                in_channels = depth\n        loc_layers += [nn.Conv2d(in_channels, box * 4, kernel_size=3, padding=1)]\n        conf_layers += [nn.Conv2d(in_channels, box * num_classes, kernel_size=3, padding=1)]\n    return base, extra_layers, norm_layers, (loc_layers, conf_layers)\n\ndef build_rfb(base, feature_layer, mbox, num_classes):\n    """"""Receptive Field Block Net for Accurate and Fast Object Detection for embeded system\n    See: https://arxiv.org/pdf/1711.07767.pdf for more details.\n    """"""\n    base_, extras_, norm_, head_ = add_extras(base(), feature_layer, mbox, num_classes, version=\'rfb\')\n    return RFB(base_, extras_, norm_, head_, feature_layer, num_classes)\n\ndef build_rfb_lite(base, feature_layer, mbox, num_classes):\n    """"""Receptive Field Block Net for Accurate and Fast Object Detection for embeded system\n    See: https://arxiv.org/pdf/1711.07767.pdf for more details.\n    """"""\n    base_, extras_, norm_, head_ = add_extras(base(), feature_layer, mbox, num_classes, version=\'rfb_lite\')\n    return RFB(base_, extras_, norm_, head_, feature_layer, num_classes)'"
lib/modeling/ssds/rfb_lite.py,7,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport os\n\nfrom lib.layers import *\n\nclass RFBLite(nn.Module):\n    """"""Receptive Field Block Net for Accurate and Fast Object Detection for embeded system\n    See: https://arxiv.org/pdf/1711.07767.pdf for more details.\n\n    Args:\n        phase: (string) Can be ""eval"" or ""train"" or ""feature""\n        base: base layers for input\n        extras: extra layers that feed to multibox loc and conf layers\n        norm: norm to add RFB module for previous feature extractor\n        head: ""multibox head"" consists of loc and conf conv layers\n        feature_layer: the feature layers for head to loc and conf\n        num_classes: num of classes \n    """"""\n    def __init__(self, base, extras, head, feature_layer, num_classes):\n        super(RFBLite, self).__init__()\n        self.num_classes = num_classes\n        # rfb network\n        self.base = nn.ModuleList(base)\n        self.norm = BasicRFB_a_lite(feature_layer[1][0],feature_layer[1][0],stride = 1,scale=1.0)\n        self.extras = nn.ModuleList(extras)\n\n        self.loc = nn.ModuleList(head[0])\n        self.conf = nn.ModuleList(head[1])\n        self.softmax = nn.Softmax(dim=-1)\n\n\n        self.feature_layer = feature_layer[0]\n        self.indicator = 0\n        for layer in self.feature_layer:\n            if isinstance(layer, int):\n                continue\n            elif layer == \'\' or layer == \'S\':\n                break\n            else:\n                self.indicator += 1 \n\n    def forward(self, x, phase=\'eval\'):\n        """"""Applies network layers and ops on input image(s) x.\n\n        Args:\n            x: input image or batch of images. Shape: [batch,3,300,300].\n\n        Return:\n            Depending on phase:\n            test:\n                Variable(tensor) of output class label predictions,\n                confidence score, and corresponding location predictions for\n                each object detected. Shape: [batch,topk,7]\n\n            train:\n                list of concat outputs from:\n                    1: confidence layers, Shape: [batch*num_priors,num_classes]\n                    2: localization layers, Shape: [batch,num_priors*4]\n\n            feature:\n                the features maps of the feature extractor\n        """"""\n        sources = list()\n        loc = list()\n        conf = list()\n\n        # apply bases layers and cache source layer outputs\n        for k in range(len(self.base)):\n            x = self.base[k](x)\n            if k in self.feature_layer:\n                if len(sources) == 0:\n                    s = self.norm(x)\n                    sources.append(s)\n                else:\n                    sources.append(x)\n\n        # apply extra layers and cache source layer outputs\n        for k, v in enumerate(self.extras):\n            x = v(x)\n            if k < self.indicator or k % 2 == 0:\n                sources.append(x)\n\n        if phase == \'feature\':\n            return sources\n\n        # apply multibox head to source layers\n        for (x, l, c) in zip(sources, self.loc, self.conf):\n            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n\n        if phase == \'eval\':\n            output = (\n                loc.view(loc.size(0), -1, 4),                   # loc preds\n                self.softmax(conf.view(-1, self.num_classes)),  # conf preds\n            )\n        else:\n            output = (\n                loc.view(loc.size(0), -1, 4),\n                conf.view(conf.size(0), -1, self.num_classes),\n            )\n        return output\n\n\nclass BasicConv(nn.Module):\n    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n        super(BasicConv, self).__init__()\n        self.out_channels = out_planes\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n        self.relu = nn.ReLU(inplace=True) if relu else None\n    def forward(self, x):\n        x = self.conv(x)\n        if self.bn is not None:\n            x = self.bn(x)\n        if self.relu is not None:\n            x = self.relu(x)\n        return x\n\nclass BasicSepConv(nn.Module):\n    def __init__(self, in_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n        super(BasicSepConv, self).__init__()\n        self.out_channels = in_planes\n        self.conv = nn.Conv2d(in_planes, in_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n        self.bn = nn.BatchNorm2d(in_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n        self.relu = nn.ReLU(inplace=True) if relu else None\n    def forward(self, x):\n        x = self.conv(x)\n        if self.bn is not None:\n            x = self.bn(x)\n        if self.relu is not None:\n            x = self.relu(x)\n        return x\n\nclass BasicRFB_a_lite(nn.Module):\n    def __init__(self, in_planes, out_planes, stride=1, scale = 0.1):\n        super(BasicRFB_a_lite, self).__init__()\n        self.scale = scale\n        self.out_channels = out_planes\n        inter_planes = in_planes //4\n\n        self.branch0 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicSepConv(inter_planes, kernel_size=3, stride=1, padding=1, dilation=1, relu=False)\n                )\n        self.branch1 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicConv(inter_planes, inter_planes, kernel_size=(3,1), stride=1, padding=(1,0)),\n                BasicSepConv(inter_planes, kernel_size=3, stride=1, padding=3, dilation=3, relu=False)\n                )\n        self.branch2 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicConv(inter_planes, inter_planes, kernel_size=(1,3), stride=stride, padding=(0,1)),\n                BasicSepConv(inter_planes, kernel_size=3, stride=1, padding=3, dilation=3, relu=False)\n                )\n        self.branch3 = nn.Sequential(\n                BasicConv(in_planes, inter_planes//2, kernel_size=1, stride=1),\n                BasicConv(inter_planes//2, (inter_planes//4)*3, kernel_size=(1,3), stride=1, padding=(0,1)),\n                BasicConv((inter_planes//4)*3, inter_planes, kernel_size=(3,1), stride=stride, padding=(1,0)),\n                BasicSepConv(inter_planes, kernel_size=3, stride=1, padding=5, dilation=5, relu=False)\n                )\n\n        self.ConvLinear = BasicConv(4*inter_planes, out_planes, kernel_size=1, stride=1, relu=False)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self,x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n\n        out = torch.cat((x0,x1,x2,x3),1)\n        out = self.ConvLinear(out)\n        out = out*self.scale + x\n        out = self.relu(out)\n\n        return out\n\nclass BasicRFB_lite(nn.Module):\n    def __init__(self, in_planes, out_planes, stride=1, scale = 0.1):\n        super(BasicRFB_lite, self).__init__()\n        self.scale = scale\n        self.out_channels = out_planes\n        inter_planes = in_planes // 8\n        self.branch1 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicConv(inter_planes, (inter_planes//2)*3, kernel_size=(1,3), stride=1, padding=(0,1)),\n                BasicConv((inter_planes//2)*3, (inter_planes//2)*3, kernel_size=(3,1), stride=stride, padding=(1,0)),\n                BasicSepConv((inter_planes//2)*3, kernel_size=3, stride=1, padding=3, dilation=3, relu=False)\n                )\n        self.branch2 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicConv(inter_planes, (inter_planes//2)*3, kernel_size=3, stride=1, padding=1),\n                BasicConv((inter_planes//2)*3, (inter_planes//2)*3, kernel_size=3, stride=stride, padding=1),\n                BasicSepConv((inter_planes//2)*3, kernel_size=3, stride=1, padding=5, dilation=5, relu=False)\n                )\n\n        self.ConvLinear = BasicConv(3*inter_planes, out_planes, kernel_size=1, stride=1, relu=False)\n        if in_planes == out_planes:\n            self.identity = True\n        else:\n            self.identity = False\n            self.shortcut = BasicConv(in_planes, out_planes, kernel_size=1, stride=stride, relu=False)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self,x):\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n\n        out = torch.cat((x1,x2),1)\n        out = self.ConvLinear(out)\n        if self.identity:\n            out = out*self.scale + x\n        else:\n            short = self.shortcut(x)\n            out = out*self.scale + short\n        out = self.relu(out)\n        return out\n\ndef add_extras(base, feature_layer, mbox, num_classes):\n    extra_layers = []\n    loc_layers = []\n    conf_layers = []\n    in_channels = None\n    for layer, depth, box in zip(feature_layer[0], feature_layer[1], mbox):\n        if layer == \'RBF\':\n            extra_layers += [BasicRFB_lite(in_channels, depth, stride=2, scale = 1.0)]\n            in_channels = depth\n        elif layer == \'S\':\n            extra_layers += [\n                    BasicConv(in_channels, int(depth/2), kernel_size=1),\n                    BasicConv(int(depth/2), depth, kernel_size=3, stride=2, padding=1)  ]\n            in_channels = depth\n        elif layer == \'\':\n            extra_layers += [\n                    BasicConv(in_channels, int(depth/2), kernel_size=1),\n                    BasicConv(int(depth/2), depth, kernel_size=3)  ]\n            in_channels = depth\n        else:\n            in_channels = depth\n        loc_layers += [nn.Conv2d(in_channels, box * 4, kernel_size=3, padding=1)]\n        conf_layers += [nn.Conv2d(in_channels, box * num_classes, kernel_size=3, padding=1)]\n    return base, extra_layers, (loc_layers, conf_layers)\n\ndef build_rfb_lite(base, feature_layer, mbox, num_classes):\n    base_, extras_, head_ = add_extras(base(), feature_layer, mbox, num_classes)\n    return RFBLite(base_, extras_, head_, feature_layer, num_classes)'"
lib/modeling/ssds/ssd.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport os\n\nfrom lib.layers import *\n\nclass SSD(nn.Module):\n    """"""Single Shot Multibox Architecture\n    See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n\n    Args:\n        phase: (string) Can be ""eval"" or ""train"" or ""feature""\n        base: base layers for input\n        extras: extra layers that feed to multibox loc and conf layers\n        head: ""multibox head"" consists of loc and conf conv layers\n        feature_layer: the feature layers for head to loc and conf\n        num_classes: num of classes \n    """"""\n\n    def __init__(self, base, extras, head, feature_layer, num_classes):\n        super(SSD, self).__init__()\n        self.num_classes = num_classes\n        # SSD network\n        self.base = nn.ModuleList(base)\n        self.norm = L2Norm(feature_layer[1][0], 20)\n        self.extras = nn.ModuleList(extras)\n\n        self.loc = nn.ModuleList(head[0])\n        self.conf = nn.ModuleList(head[1])\n        self.softmax = nn.Softmax(dim=-1)\n\n        self.feature_layer = feature_layer[0]\n\n    def forward(self, x, phase=\'eval\'):\n        """"""Applies network layers and ops on input image(s) x.\n\n        Args:\n            x: input image or batch of images. Shape: [batch,3,300,300].\n\n        Return:\n            Depending on phase:\n            test:\n                Variable(tensor) of output class label predictions,\n                confidence score, and corresponding location predictions for\n                each object detected. Shape: [batch,topk,7]\n\n            train:\n                list of concat outputs from:\n                    1: confidence layers, Shape: [batch*num_priors,num_classes]\n                    2: localization layers, Shape: [batch,num_priors*4]\n\n            feature:\n                the features maps of the feature extractor\n        """"""\n        sources, loc, conf = [list() for _ in range(3)]\n\n        # apply bases layers and cache source layer outputs\n        for k in range(len(self.base)):\n            x = self.base[k](x)\n            if k in self.feature_layer:\n                if len(sources) == 0:\n                    s = self.norm(x)\n                    sources.append(s)\n                else:\n                    sources.append(x)\n\n        # apply extra layers and cache source layer outputs\n        for k, v in enumerate(self.extras):\n            # TODO:maybe donot needs the relu here\n            x = F.relu(v(x), inplace=True)\n            # TODO:lite is different in here, should be changed\n            if k % 2 == 1:\n                sources.append(x)\n        \n        if phase == \'feature\':\n            return sources\n\n        # apply multibox head to source layers\n        for (x, l, c) in zip(sources, self.loc, self.conf):\n            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n\n        if phase == \'eval\':\n            output = (\n                loc.view(loc.size(0), -1, 4),                   # loc preds\n                self.softmax(conf.view(-1, self.num_classes)),  # conf preds\n            )\n        else:\n            output = (\n                loc.view(loc.size(0), -1, 4),\n                conf.view(conf.size(0), -1, self.num_classes),\n            )\n        return output\n\ndef add_extras(base, feature_layer, mbox, num_classes, version):\n    extra_layers = []\n    loc_layers = []\n    conf_layers = []\n    in_channels = None\n    for layer, depth, box in zip(feature_layer[0], feature_layer[1], mbox):\n        if \'lite\' in version:\n            if layer == \'S\':\n                extra_layers += [ _conv_dw(in_channels, depth, stride=2, padding=1, expand_ratio=1) ]\n                in_channels = depth\n            elif layer == \'\':\n                extra_layers += [ _conv_dw(in_channels, depth, stride=1, expand_ratio=1) ]\n                in_channels = depth\n            else:\n                in_channels = depth\n        else:    \n            if layer == \'S\':\n                extra_layers += [\n                        nn.Conv2d(in_channels, int(depth/2), kernel_size=1),\n                        nn.Conv2d(int(depth/2), depth, kernel_size=3, stride=2, padding=1)  ]\n                in_channels = depth\n            elif layer == \'\':\n                extra_layers += [\n                        nn.Conv2d(in_channels, int(depth/2), kernel_size=1),\n                        nn.Conv2d(int(depth/2), depth, kernel_size=3)  ]\n                in_channels = depth\n            else:\n                in_channels = depth\n        \n        loc_layers += [nn.Conv2d(in_channels, box * 4, kernel_size=3, padding=1)]\n        conf_layers += [nn.Conv2d(in_channels, box * num_classes, kernel_size=3, padding=1)]\n    return base, extra_layers, (loc_layers, conf_layers)\n\n# based on the implementation in https://github.com/tensorflow/models/blob/master/research/object_detection/models/feature_map_generators.py#L213\n# when the expand_ratio is 1, the implemetation is nearly same. Since the shape is always change, I do not add the shortcut as what mobilenetv2 did.\ndef _conv_dw(inp, oup, stride=1, padding=0, expand_ratio=1):\n    return nn.Sequential(\n        # pw\n        nn.Conv2d(inp, oup * expand_ratio, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup * expand_ratio),\n        nn.ReLU6(inplace=True),\n        # dw\n        nn.Conv2d(oup * expand_ratio, oup * expand_ratio, 3, stride, padding, groups=oup * expand_ratio, bias=False),\n        nn.BatchNorm2d(oup * expand_ratio),\n        nn.ReLU6(inplace=True),\n        # pw-linear\n        nn.Conv2d(oup * expand_ratio, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n    )\n\ndef _conv(inp, oup, stride=1, padding=0):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, padding, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU(inplace=True),\n    )\n\n\ndef build_ssd(base, feature_layer, mbox, num_classes):\n    """"""Single Shot Multibox Architecture\n    See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n    """"""\n    base_, extras_, head_ = add_extras(base(), feature_layer, mbox, num_classes, version=\'ssd\')\n    return SSD(base_, extras_, head_, feature_layer, num_classes)\n\ndef build_ssd_lite(base, feature_layer, mbox, num_classes):\n    """"""Single Shot Multibox Architecture for embeded system\n    See: https://arxiv.org/pdf/1512.02325.pdf & \n    https://arxiv.org/pdf/1801.04381.pdf for more details.\n    """"""\n    base_, extras_, head_ = add_extras(base(), feature_layer, mbox, num_classes, version=\'ssd_lite\')\n    return SSD(base_, extras_, head_, feature_layer, num_classes)'"
lib/modeling/ssds/ssd_lite.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport os\n\nfrom lib.layers import *\n\nclass SSDLite(nn.Module):\n    """"""Single Shot Multibox Architecture for embeded system\n    See: https://arxiv.org/pdf/1512.02325.pdf & \n    https://arxiv.org/pdf/1801.04381.pdf for more details.\n\n    Args:\n        phase: (string) Can be ""eval"" or ""train"" or ""feature""\n        base: base layers for input\n        extras: extra layers that feed to multibox loc and conf layers\n        head: ""multibox head"" consists of loc and conf conv layers\n        feature_layer: the feature layers for head to loc and conf\n        num_classes: num of classes \n    """"""\n\n    def __init__(self, base, extras, head, feature_layer, num_classes):\n        super(SSDLite, self).__init__()\n        self.num_classes = num_classes\n        # SSD network\n        self.base = nn.ModuleList(base)\n        self.norm = L2Norm(feature_layer[1][0], 20)\n        self.extras = nn.ModuleList(extras)\n\n        self.loc = nn.ModuleList(head[0])\n        self.conf = nn.ModuleList(head[1])\n        self.softmax = nn.Softmax(dim=-1)\n\n        self.feature_layer = feature_layer[0]\n        \n\n    def forward(self, x, phase=\'eval\'):\n        """"""Applies network layers and ops on input image(s) x.\n\n        Args:\n            x: input image or batch of images. Shape: [batch,3,300,300].\n\n        Return:\n            Depending on phase:\n            test:\n                Variable(tensor) of output class label predictions,\n                confidence score, and corresponding location predictions for\n                each object detected. Shape: [batch,topk,7]\n\n            train:\n                list of concat outputs from:\n                    1: confidence layers, Shape: [batch*num_priors,num_classes]\n                    2: localization layers, Shape: [batch,num_priors*4]\n\n            feature:\n                the features maps of the feature extractor\n        """"""\n        sources = list()\n        loc = list()\n        conf = list()\n\n        # apply bases layers and cache source layer outputs\n        for k in range(len(self.base)):\n            x = self.base[k](x)\n            if k in self.feature_layer:\n                if len(sources) == 0:\n                    s = self.norm(x)\n                    sources.append(s)\n                else:\n                    sources.append(x)\n\n        # apply extra layers and cache source layer outputs\n        for k, v in enumerate(self.extras):\n            x = F.relu(v(x), inplace=True)\n            sources.append(x)\n            # if k % 2 == 1:\n            #     sources.append(x)\n\n        if phase == \'feature\':\n            return sources\n\n        # apply multibox head to source layers\n        for (x, l, c) in zip(sources, self.loc, self.conf):\n            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n\n        if phase == \'eval\':\n            output = (\n                loc.view(loc.size(0), -1, 4),                   # loc preds\n                self.softmax(conf.view(-1, self.num_classes)),  # conf preds\n            )\n        else:\n            output = (\n                loc.view(loc.size(0), -1, 4),\n                conf.view(conf.size(0), -1, self.num_classes),\n            )\n        return output\n\ndef add_extras(base, feature_layer, mbox, num_classes):\n    extra_layers = []\n    loc_layers = []\n    conf_layers = []\n    in_channels = None\n    for layer, depth, box in zip(feature_layer[0], feature_layer[1], mbox):\n        if layer == \'S\':\n            extra_layers += [ _conv_dw(in_channels, depth, stride=2, padding=1, expand_ratio=1) ]\n            in_channels = depth\n        elif layer == \'\':\n            extra_layers += [ _conv_dw(in_channels, depth, stride=1, expand_ratio=1) ]\n            in_channels = depth\n        else:\n            in_channels = depth\n        loc_layers += [nn.Conv2d(in_channels, box * 4, kernel_size=3, padding=1)]\n        conf_layers += [nn.Conv2d(in_channels, box * num_classes, kernel_size=3, padding=1)]\n    return base, extra_layers, (loc_layers, conf_layers)\n\n# based on the implementation in https://github.com/tensorflow/models/blob/master/research/object_detection/models/feature_map_generators.py#L213\n# when the expand_ratio is 1, the implemetation is nearly same. Since the shape is always change, I do not add the shortcut as what mobilenetv2 did.\ndef _conv_dw(inp, oup, stride=1, padding=0, expand_ratio=1):\n    return nn.Sequential(\n        # pw\n        nn.Conv2d(inp, oup * expand_ratio, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup * expand_ratio),\n        nn.ReLU6(inplace=True),\n        # dw\n        nn.Conv2d(oup * expand_ratio, oup * expand_ratio, 3, stride, padding, groups=oup * expand_ratio, bias=False),\n        nn.BatchNorm2d(oup * expand_ratio),\n        nn.ReLU6(inplace=True),\n        # pw-linear\n        nn.Conv2d(oup * expand_ratio, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n    )\n\ndef build_ssd_lite(base, feature_layer, mbox, num_classes):\n    base_, extras_, head_ = add_extras(base(), feature_layer, mbox, num_classes)\n    return SSDLite(base_, extras_, head_, feature_layer, num_classes)'"
lib/modeling/ssds/yolo.py,11,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport os\n\n\nclass YOLO(nn.Module):\n    """"""Single Shot Multibox Architecture\n    See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n\n    Args:\n        phase: (string) Can be ""eval"" or ""train"" or ""feature""\n        base: base layers for input\n        extras: extra layers that feed to multibox loc and conf layers\n        head: ""multibox head"" consists of loc and conf conv layers\n        feature_layer: the feature layers for head to loc and conf\n        num_classes: num of classes \n    """"""\n\n    def __init__(self, base, extras, head, feature_layer, num_classes):\n        super(YOLO, self).__init__()\n        self.num_classes = num_classes\n        # SSD network\n        self.base = nn.ModuleList(base)\n        self.extras = nn.ModuleList(extras)\n\n        self.loc = nn.ModuleList(head[0])\n        self.conf = nn.ModuleList(head[1])\n        self.softmax = nn.Softmax(dim=-1)\n\n        self.feature_layer = [ f for feature in feature_layer[0] for f in feature]\n        # self.feature_index = [ len(feature) for feature in feature_layer[0]]\n        self.feature_index = list()\n        s = -1\n        for feature in feature_layer[0]:\n            s += len(feature)\n            self.feature_index.append(s)\n\n    def forward(self, x, phase=\'eval\'):\n        """"""Applies network layers and ops on input image(s) x.\n\n        Args:\n            x: input image or batch of images. Shape: [batch,3,300,300].\n\n        Return:\n            Depending on phase:\n            test:\n                Variable(tensor) of output class label predictions,\n                confidence score, and corresponding location predictions for\n                each object detected. Shape: [batch,topk,7]\n\n            train:\n                list of concat outputs from:\n                    1: confidence layers, Shape: [batch*num_priors,num_classes]\n                    2: localization layers, Shape: [batch,num_priors*4]\n\n            feature:\n                the features maps of the feature extractor\n        """"""\n        cat = dict()\n        sources, loc, conf = [list() for _ in range(3)]\n\n        # apply bases layers and cache source layer outputs\n        for k in range(len(self.base)):\n            x = self.base[k](x)\n            if k in self.feature_layer:\n                cat[k] = x\n\n\n        # apply extra layers and cache source layer outputs\n        for k, v in enumerate(self.extras):\n            if isinstance(self.feature_layer[k], int):\n                x = v(x, cat[self.feature_layer[k]])\n            else:\n                x = v(x)\n            if k in self.feature_index:\n                sources.append(x)\n\n        if phase == \'feature\':\n            return sources\n\n        # apply multibox head to source layers\n        for (x, l, c) in zip(sources, self.loc, self.conf):\n            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n\n        if phase == \'eval\':\n            output = (\n                loc.view(loc.size(0), -1, 4),                   # loc preds\n                self.softmax(conf.view(-1, self.num_classes)),  # conf preds\n            )\n        else:\n            output = (\n                loc.view(loc.size(0), -1, 4),\n                conf.view(conf.size(0), -1, self.num_classes),\n            )\n        return output\n\n\ndef add_extras(base, feature_layer, mbox, num_classes, version):\n    extra_layers = []\n    loc_layers = []\n    conf_layers = []\n    in_channels = base[-1].depth\n    for layers, depths, box in zip(feature_layer[0], feature_layer[1], mbox):\n        for layer, depth in zip(layers, depths):\n            if layer == \'\':\n                extra_layers += [ _conv_bn(in_channels, depth) ]\n                in_channels = depth\n            elif layer == \'B\':\n                extra_layers += [ _conv_block(in_channels, depth) ]\n                in_channels = depth\n            elif isinstance(layer, int):\n                if version == \'v2\':\n                    extra_layers += [ _router_v2(base[layer].depth, depth) ]\n                    in_channels = in_channels + depth * 4\n                elif version == \'v3\':\n                    extra_layers += [ _router_v3(in_channels, depth) ]\n                    in_channels = depth + base[layer].depth\n            else:\n                AssertionError(\'undefined layer\')\n            \n        loc_layers += [nn.Conv2d(in_channels, box * 4, kernel_size=1)]\n        conf_layers += [nn.Conv2d(in_channels, box * num_classes, kernel_size=1)]\n    return base, extra_layers, (loc_layers, conf_layers)\n\n\nclass _conv_bn(nn.Module):\n    def __init__(self, inp, oup, stride=1):\n        super(_conv_bn, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n            nn.BatchNorm2d(oup),\n            nn.LeakyReLU(0.1, inplace=True),\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass _conv_block(nn.Module):\n    def __init__(self, inp, oup, stride=1, expand_ratio=0.5):\n        super(_conv_block, self).__init__()\n        depth = int(oup*expand_ratio)\n        self.conv = nn.Sequential(\n            nn.Conv2d(inp, depth, 1, 1, bias=False),\n            nn.BatchNorm2d(depth),\n            nn.LeakyReLU(0.1, inplace=True),\n            nn.Conv2d(depth, oup, 3, stride, 1, bias=False),\n            nn.BatchNorm2d(oup),\n            nn.LeakyReLU(0.1, inplace=True),\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass _router_v2(nn.Module):\n    def __init__(self, inp, oup, stride=2):\n        super(_router_v2, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(inp, oup, 1, 1, bias=False),\n            nn.BatchNorm2d(oup),\n            nn.LeakyReLU(0.1, inplace=True),\n        )\n        self.stride = stride\n\n    def forward(self, x1, x2):\n        # prune channel\n        x2 = self.conv(x2)\n        # reorg\n        B, C, H, W = x2.size()\n        s = self.stride\n        x2 = x2.view(B, C, H // s, s, W // s, s).transpose(3, 4).contiguous()\n        x2 = x2.view(B, C, H // s * W // s, s * s).transpose(2, 3).contiguous()\n        x2 = x2.view(B, C, s * s, H // s, W // s).transpose(1, 2).contiguous()\n        x2 = x2.view(B, s * s * C, H // s, W // s)\n        return torch.cat((x1, x2), dim=1)\n\n\nclass _router_v3(nn.Module):\n    def __init__(self, inp, oup, stride=1, bilinear=True):\n        super(_router_v3, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(inp, oup, 1, 1, bias=False),\n            nn.BatchNorm2d(oup),\n            nn.LeakyReLU(0.1, inplace=True),\n        )\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode=\'bilinear\')\n        else:\n            self.up = nn.ConvTranspose2d(oup, oup, 2, stride=2)\n\n    def forward(self, x1, x2):\n        # prune channel\n        x1 = self.conv(x1)\n        # up\n        x1 = self.up(x1)\n        # ideally the following is not needed\n        diffX = x1.size()[2] - x2.size()[2]\n        diffY = x1.size()[3] - x2.size()[3]\n        x2 = F.pad(x2, (diffX // 2, int(diffX / 2),\n                        diffY // 2, int(diffY / 2)))\n        return torch.cat((x1, x2), dim=1)\n\n\n\n\ndef build_yolo_v2(base, feature_layer, mbox, num_classes):\n    base_, extras_, head_ = add_extras(base(), feature_layer, mbox, num_classes, version=\'v2\')\n    return YOLO(base_, extras_, head_, feature_layer, num_classes)\n\ndef build_yolo_v3(base, feature_layer, mbox, num_classes):\n    base_, extras_, head_ = add_extras(base(), feature_layer, mbox, num_classes, version=\'v3\')\n    return YOLO(base_, extras_, head_, feature_layer, num_classes)\n\n\nif __name__ == \'__main__\':\n\n    feature_layer_v2 = [[[\'\', \'\',12, \'\']], [[1024, 1024, 64, 1024]]]\n    mbox_v2 = [5]\n    feature_layer_v3 = [[[\'B\',\'B\',\'B\'], [23,\'B\',\'B\',\'B\'], [14,\'B\',\'B\',\'B\']],\n                        [[1024,1024,1024], [256, 512, 512, 512], [128, 256, 256, 256]]]\n    mbox_v3 = [3, 3, 3]\n\n    from lib.modeling.nets.darknet import *\n\n    # yolo_v2 = build_yolo_v2(darknet_19, feature_layer_v2, mbox_v2, 81)\n    # # print(\'yolo_v2\', yolo_v2)\n    # yolo_v2.eval()\n    # x = torch.rand(1, 3, 416, 416)\n    # x = torch.autograd.Variable(x, volatile=True) #.cuda()\n    # feature_maps = yolo_v2(x, phase=\'feature\')\n    # print([(o.size()[2], o.size()[3]) for o in feature_maps])\n    # out = yolo_v2(x, phase=\'eval\')\n\n\n    yolo_v3 = build_yolo_v3(darknet_53, feature_layer_v3, mbox_v3, 81)\n    print(\'yolo_v3\', yolo_v3)\n    # print(yolo_v3.feature_layer, yolo_v3.feature_index)\n    yolo_v3.eval()\n    x = torch.rand(1, 3, 416, 416)\n    x = torch.autograd.Variable(x, volatile=True) #.cuda()\n    feature_maps = yolo_v3(x, phase=\'feature\')\n    # print([(o.size()[2], o.size()[3]) for o in feature_maps])\n    out = yolo_v3(x, phase=\'eval\')\n    # print(set(yolo_v3.state_dict()))\n    print(set(yolo_v3.base[0].conv[1].state_dict()))\n\n'"
lib/utils/nms/__init__.py,0,b''
lib/utils/nms/build.py,2,"b""from __future__ import print_function\nimport os\nimport torch\nfrom torch.utils.ffi import create_extension\n\n#this_file = os.path.dirname(__file__)\n\nsources = []\nheaders = []\ndefines = []\nwith_cuda = False\n\nif torch.cuda.is_available():\n    print('Including CUDA code.')\n    sources += ['src/nms_cuda.c']\n    headers += ['src/nms_cuda.h']\n    defines += [('WITH_CUDA', None)]\n    with_cuda = True\n\nthis_file = os.path.dirname(os.path.realpath(__file__))\nprint(this_file)\nextra_objects = ['src/nms_cuda_kernel.cu.o']\nextra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\nprint(extra_objects)\n\nffi = create_extension(\n    '_ext.nms',\n    headers=headers,\n    sources=sources,\n    define_macros=defines,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_objects=extra_objects\n)\n\nif __name__ == '__main__':\n    ffi.build()\n"""
lib/utils/nms/nms_gpu.py,0,"b'from __future__ import absolute_import\nimport torch\nimport numpy as np\nfrom ._ext import nms\nimport pdb\n\ndef nms_gpu(dets, thresh):\n\tkeep = dets.new(dets.size(0), 1).zero_().int()\n\tnum_out = dets.new(1).zero_().int()\n\tnms.nms_cuda(keep, dets, num_out, thresh)\n\tkeep = keep[:num_out[0]]\n\treturn keep\n'"
lib/utils/nms/nms_wrapper.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\nimport torch\nfrom lib.utils.nms.nms_gpu import nms_gpu\n\ndef nms(dets, thresh, force_cpu=False):\n    """"""Dispatch to either CPU or GPU NMS implementations.""""""\n    if dets.shape[0] == 0:\n        return []\n    # ---numpy version---\n    # original: return gpu_nms(dets, thresh, device_id=cfg.GPU_ID)\n    # ---pytorch version---\n    return nms_gpu(dets, thresh)\n'"
lib/utils/pycocotools/__init__.py,0,"b""__author__ = 'tylin'\n"""
lib/utils/pycocotools/coco.py,0,"b'__author__ = \'tylin\'\n__version__ = \'2.0\'\n# Interface for accessing the Microsoft COCO dataset.\n\n# Microsoft COCO is a large image dataset designed for object detection,\n# segmentation, and caption generation. pycocotools is a Python API that\n# assists in loading, parsing and visualizing the annotations in COCO.\n# Please visit http://mscoco.org/ for more information on COCO, including\n# for the data, paper, and tutorials. The exact format of the annotations\n# is also described on the COCO website. For example usage of the pycocotools\n# please see pycocotools_demo.ipynb. In addition to this API, please download both\n# the COCO images and annotations in order to run the demo.\n\n# An alternative to using the API is to load the annotations directly\n# into Python dictionary\n# Using the API provides additional utility functions. Note that this API\n# supports both *instance* and *caption* annotations. In the case of\n# captions not all functions are defined (e.g. categories are undefined).\n\n# The following API functions are defined:\n#  COCO       - COCO api class that loads COCO annotation file and prepare data structures.\n#  decodeMask - Decode binary mask M encoded via run-length encoding.\n#  encodeMask - Encode binary mask M using run-length encoding.\n#  getAnnIds  - Get ann ids that satisfy given filter conditions.\n#  getCatIds  - Get cat ids that satisfy given filter conditions.\n#  getImgIds  - Get img ids that satisfy given filter conditions.\n#  loadAnns   - Load anns with the specified ids.\n#  loadCats   - Load cats with the specified ids.\n#  loadImgs   - Load imgs with the specified ids.\n#  annToMask  - Convert segmentation in an annotation to binary mask.\n#  showAnns   - Display the specified annotations.\n#  loadRes    - Load algorithm results and create API for accessing them.\n#  download   - Download COCO images from mscoco.org server.\n# Throughout the API ""ann""=annotation, ""cat""=category, and ""img""=image.\n# Help on each functions can be accessed by: ""help COCO>function"".\n\n# See also COCO>decodeMask,\n# COCO>encodeMask, COCO>getAnnIds, COCO>getCatIds,\n# COCO>getImgIds, COCO>loadAnns, COCO>loadCats,\n# COCO>loadImgs, COCO>annToMask, COCO>showAnns\n\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2014.\n# Licensed under the Simplified BSD License [see bsd.txt]\n\nimport json\nimport time\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import PatchCollection\nfrom matplotlib.patches import Polygon\nimport numpy as np\nimport copy\nimport itertools\nfrom . import mask as maskUtils\nimport os\nfrom collections import defaultdict\nimport sys\nPYTHON_VERSION = sys.version_info[0]\nif PYTHON_VERSION == 2:\n    from urllib import urlretrieve\nelif PYTHON_VERSION == 3:\n    from urllib.request import urlretrieve\n\n\ndef _isArrayLike(obj):\n    return hasattr(obj, \'__iter__\') and hasattr(obj, \'__len__\')\n\n\nclass COCO:\n    def __init__(self, annotation_file=None):\n        """"""\n        Constructor of Microsoft COCO helper class for reading and visualizing annotations.\n        :param annotation_file (str): location of annotation file\n        :param image_folder (str): location to the folder that hosts images.\n        :return:\n        """"""\n        # load dataset\n        self.dataset,self.anns,self.cats,self.imgs = dict(),dict(),dict(),dict()\n        self.imgToAnns, self.catToImgs = defaultdict(list), defaultdict(list)\n        if not annotation_file == None:\n            print(\'loading annotations into memory...\')\n            tic = time.time()\n            dataset = json.load(open(annotation_file, \'r\'))\n            assert type(dataset)==dict, \'annotation file format {} not supported\'.format(type(dataset))\n            print(\'Done (t={:0.2f}s)\'.format(time.time()- tic))\n            self.dataset = dataset\n            self.createIndex()\n\n    def createIndex(self):\n        # create index\n        print(\'creating index...\')\n        anns, cats, imgs = {}, {}, {}\n        imgToAnns,catToImgs = defaultdict(list),defaultdict(list)\n        if \'annotations\' in self.dataset:\n            for ann in self.dataset[\'annotations\']:\n                imgToAnns[ann[\'image_id\']].append(ann)\n                anns[ann[\'id\']] = ann\n\n        if \'images\' in self.dataset:\n            for img in self.dataset[\'images\']:\n                imgs[img[\'id\']] = img\n\n        if \'categories\' in self.dataset:\n            for cat in self.dataset[\'categories\']:\n                cats[cat[\'id\']] = cat\n\n        if \'annotations\' in self.dataset and \'categories\' in self.dataset:\n            for ann in self.dataset[\'annotations\']:\n                catToImgs[ann[\'category_id\']].append(ann[\'image_id\'])\n\n        print(\'index created!\')\n\n        # create class members\n        self.anns = anns\n        self.imgToAnns = imgToAnns\n        self.catToImgs = catToImgs\n        self.imgs = imgs\n        self.cats = cats\n\n    def info(self):\n        """"""\n        Print information about the annotation file.\n        :return:\n        """"""\n        for key, value in self.dataset[\'info\'].items():\n            print(\'{}: {}\'.format(key, value))\n\n    def getAnnIds(self, imgIds=[], catIds=[], areaRng=[], iscrowd=None):\n        """"""\n        Get ann ids that satisfy given filter conditions. default skips that filter\n        :param imgIds  (int array)     : get anns for given imgs\n               catIds  (int array)     : get anns for given cats\n               areaRng (float array)   : get anns for given area range (e.g. [0 inf])\n               iscrowd (boolean)       : get anns for given crowd label (False or True)\n        :return: ids (int array)       : integer array of ann ids\n        """"""\n        imgIds = imgIds if _isArrayLike(imgIds) else [imgIds]\n        catIds = catIds if _isArrayLike(catIds) else [catIds]\n\n        if len(imgIds) == len(catIds) == len(areaRng) == 0:\n            anns = self.dataset[\'annotations\']\n        else:\n            if not len(imgIds) == 0:\n                lists = [self.imgToAnns[imgId] for imgId in imgIds if imgId in self.imgToAnns]\n                anns = list(itertools.chain.from_iterable(lists))\n            else:\n                anns = self.dataset[\'annotations\']\n            anns = anns if len(catIds)  == 0 else [ann for ann in anns if ann[\'category_id\'] in catIds]\n            anns = anns if len(areaRng) == 0 else [ann for ann in anns if ann[\'area\'] > areaRng[0] and ann[\'area\'] < areaRng[1]]\n        if not iscrowd == None:\n            ids = [ann[\'id\'] for ann in anns if ann[\'iscrowd\'] == iscrowd]\n        else:\n            ids = [ann[\'id\'] for ann in anns]\n        return ids\n\n    def getCatIds(self, catNms=[], supNms=[], catIds=[]):\n        """"""\n        filtering parameters. default skips that filter.\n        :param catNms (str array)  : get cats for given cat names\n        :param supNms (str array)  : get cats for given supercategory names\n        :param catIds (int array)  : get cats for given cat ids\n        :return: ids (int array)   : integer array of cat ids\n        """"""\n        catNms = catNms if _isArrayLike(catNms) else [catNms]\n        supNms = supNms if _isArrayLike(supNms) else [supNms]\n        catIds = catIds if _isArrayLike(catIds) else [catIds]\n\n        if len(catNms) == len(supNms) == len(catIds) == 0:\n            cats = self.dataset[\'categories\']\n        else:\n            cats = self.dataset[\'categories\']\n            cats = cats if len(catNms) == 0 else [cat for cat in cats if cat[\'name\']          in catNms]\n            cats = cats if len(supNms) == 0 else [cat for cat in cats if cat[\'supercategory\'] in supNms]\n            cats = cats if len(catIds) == 0 else [cat for cat in cats if cat[\'id\']            in catIds]\n        ids = [cat[\'id\'] for cat in cats]\n        return ids\n\n    def getImgIds(self, imgIds=[], catIds=[]):\n        \'\'\'\n        Get img ids that satisfy given filter conditions.\n        :param imgIds (int array) : get imgs for given ids\n        :param catIds (int array) : get imgs with all given cats\n        :return: ids (int array)  : integer array of img ids\n        \'\'\'\n        imgIds = imgIds if _isArrayLike(imgIds) else [imgIds]\n        catIds = catIds if _isArrayLike(catIds) else [catIds]\n\n        if len(imgIds) == len(catIds) == 0:\n            ids = self.imgs.keys()\n        else:\n            ids = set(imgIds)\n            for i, catId in enumerate(catIds):\n                if i == 0 and len(ids) == 0:\n                    ids = set(self.catToImgs[catId])\n                else:\n                    ids &= set(self.catToImgs[catId])\n        return list(ids)\n\n    def loadAnns(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying anns\n        :return: anns (object array) : loaded ann objects\n        """"""\n        if _isArrayLike(ids):\n            return [self.anns[id] for id in ids]\n        elif type(ids) == int:\n            return [self.anns[ids]]\n\n    def loadCats(self, ids=[]):\n        """"""\n        Load cats with the specified ids.\n        :param ids (int array)       : integer ids specifying cats\n        :return: cats (object array) : loaded cat objects\n        """"""\n        if _isArrayLike(ids):\n            return [self.cats[id] for id in ids]\n        elif type(ids) == int:\n            return [self.cats[ids]]\n\n    def loadImgs(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying img\n        :return: imgs (object array) : loaded img objects\n        """"""\n        if _isArrayLike(ids):\n            return [self.imgs[id] for id in ids]\n        elif type(ids) == int:\n            return [self.imgs[ids]]\n\n    def showAnns(self, anns):\n        """"""\n        Display the specified annotations.\n        :param anns (array of object): annotations to display\n        :return: None\n        """"""\n        if len(anns) == 0:\n            return 0\n        if \'segmentation\' in anns[0] or \'keypoints\' in anns[0]:\n            datasetType = \'instances\'\n        elif \'caption\' in anns[0]:\n            datasetType = \'captions\'\n        else:\n            raise Exception(\'datasetType not supported\')\n        if datasetType == \'instances\':\n            ax = plt.gca()\n            ax.set_autoscale_on(False)\n            polygons = []\n            color = []\n            for ann in anns:\n                c = (np.random.random((1, 3))*0.6+0.4).tolist()[0]\n                if \'segmentation\' in ann:\n                    if type(ann[\'segmentation\']) == list:\n                        # polygon\n                        for seg in ann[\'segmentation\']:\n                            poly = np.array(seg).reshape((int(len(seg)/2), 2))\n                            polygons.append(Polygon(poly))\n                            color.append(c)\n                    else:\n                        # mask\n                        t = self.imgs[ann[\'image_id\']]\n                        if type(ann[\'segmentation\'][\'counts\']) == list:\n                            rle = maskUtils.frPyObjects([ann[\'segmentation\']], t[\'height\'], t[\'width\'])\n                        else:\n                            rle = [ann[\'segmentation\']]\n                        m = maskUtils.decode(rle)\n                        img = np.ones( (m.shape[0], m.shape[1], 3) )\n                        if ann[\'iscrowd\'] == 1:\n                            color_mask = np.array([2.0,166.0,101.0])/255\n                        if ann[\'iscrowd\'] == 0:\n                            color_mask = np.random.random((1, 3)).tolist()[0]\n                        for i in range(3):\n                            img[:,:,i] = color_mask[i]\n                        ax.imshow(np.dstack( (img, m*0.5) ))\n                if \'keypoints\' in ann and type(ann[\'keypoints\']) == list:\n                    # turn skeleton into zero-based index\n                    sks = np.array(self.loadCats(ann[\'category_id\'])[0][\'skeleton\'])-1\n                    kp = np.array(ann[\'keypoints\'])\n                    x = kp[0::3]\n                    y = kp[1::3]\n                    v = kp[2::3]\n                    for sk in sks:\n                        if np.all(v[sk]>0):\n                            plt.plot(x[sk],y[sk], linewidth=3, color=c)\n                    plt.plot(x[v>0], y[v>0],\'o\',markersize=8, markerfacecolor=c, markeredgecolor=\'k\',markeredgewidth=2)\n                    plt.plot(x[v>1], y[v>1],\'o\',markersize=8, markerfacecolor=c, markeredgecolor=c, markeredgewidth=2)\n            p = PatchCollection(polygons, facecolor=color, linewidths=0, alpha=0.4)\n            ax.add_collection(p)\n            p = PatchCollection(polygons, facecolor=\'none\', edgecolors=color, linewidths=2)\n            ax.add_collection(p)\n        elif datasetType == \'captions\':\n            for ann in anns:\n                print(ann[\'caption\'])\n\n    def loadRes(self, resFile):\n        """"""\n        Load result file and return a result api object.\n        :param   resFile (str)     : file name of result file\n        :return: res (obj)         : result api object\n        """"""\n        res = COCO()\n        res.dataset[\'images\'] = [img for img in self.dataset[\'images\']]\n\n        print(\'Loading and preparing results...\')\n        tic = time.time()\n        if type(resFile) == str or type(resFile) == unicode:\n            anns = json.load(open(resFile))\n        elif type(resFile) == np.ndarray:\n            anns = self.loadNumpyAnnotations(resFile)\n        else:\n            anns = resFile\n        assert type(anns) == list, \'results in not an array of objects\'\n        annsImgIds = [ann[\'image_id\'] for ann in anns]\n        assert set(annsImgIds) == (set(annsImgIds) & set(self.getImgIds())), \\\n               \'Results do not correspond to current coco set\'\n        if \'caption\' in anns[0]:\n            imgIds = set([img[\'id\'] for img in res.dataset[\'images\']]) & set([ann[\'image_id\'] for ann in anns])\n            res.dataset[\'images\'] = [img for img in res.dataset[\'images\'] if img[\'id\'] in imgIds]\n            for id, ann in enumerate(anns):\n                ann[\'id\'] = id+1\n        elif \'bbox\' in anns[0] and not anns[0][\'bbox\'] == []:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                bb = ann[\'bbox\']\n                x1, x2, y1, y2 = [bb[0], bb[0]+bb[2], bb[1], bb[1]+bb[3]]\n                if not \'segmentation\' in ann:\n                    ann[\'segmentation\'] = [[x1, y1, x1, y2, x2, y2, x2, y1]]\n                ann[\'area\'] = bb[2]*bb[3]\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        elif \'segmentation\' in anns[0]:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                # now only support compressed RLE format as segmentation results\n                ann[\'area\'] = maskUtils.area(ann[\'segmentation\'])\n                if not \'bbox\' in ann:\n                    ann[\'bbox\'] = maskUtils.toBbox(ann[\'segmentation\'])\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        elif \'keypoints\' in anns[0]:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                s = ann[\'keypoints\']\n                x = s[0::3]\n                y = s[1::3]\n                x0,x1,y0,y1 = np.min(x), np.max(x), np.min(y), np.max(y)\n                ann[\'area\'] = (x1-x0)*(y1-y0)\n                ann[\'id\'] = id + 1\n                ann[\'bbox\'] = [x0,y0,x1-x0,y1-y0]\n        print(\'DONE (t={:0.2f}s)\'.format(time.time()- tic))\n\n        res.dataset[\'annotations\'] = anns\n        res.createIndex()\n        return res\n\n    def download(self, tarDir = None, imgIds = [] ):\n        \'\'\'\n        Download COCO images from mscoco.org server.\n        :param tarDir (str): COCO results directory name\n               imgIds (list): images to be downloaded\n        :return:\n        \'\'\'\n        if tarDir is None:\n            print(\'Please specify target directory\')\n            return -1\n        if len(imgIds) == 0:\n            imgs = self.imgs.values()\n        else:\n            imgs = self.loadImgs(imgIds)\n        N = len(imgs)\n        if not os.path.exists(tarDir):\n            os.makedirs(tarDir)\n        for i, img in enumerate(imgs):\n            tic = time.time()\n            fname = os.path.join(tarDir, img[\'file_name\'])\n            if not os.path.exists(fname):\n                urlretrieve(img[\'coco_url\'], fname)\n            print(\'downloaded {}/{} images (t={:0.1f}s)\'.format(i, N, time.time()- tic))\n\n    def loadNumpyAnnotations(self, data):\n        """"""\n        Convert result data from a numpy array [Nx7] where each row contains {imageID,x1,y1,w,h,score,class}\n        :param  data (numpy.ndarray)\n        :return: annotations (python nested list)\n        """"""\n        print(\'Converting ndarray to lists...\')\n        assert(type(data) == np.ndarray)\n        print(data.shape)\n        assert(data.shape[1] == 7)\n        N = data.shape[0]\n        ann = []\n        for i in range(N):\n            if i % 1000000 == 0:\n                print(\'{}/{}\'.format(i,N))\n            ann += [{\n                \'image_id\'  : int(data[i, 0]),\n                \'bbox\'  : [ data[i, 1], data[i, 2], data[i, 3], data[i, 4] ],\n                \'score\' : data[i, 5],\n                \'category_id\': int(data[i, 6]),\n                }]\n        return ann\n\n    def annToRLE(self, ann):\n        """"""\n        Convert annotation which can be polygons, uncompressed RLE to RLE.\n        :return: binary mask (numpy 2D array)\n        """"""\n        t = self.imgs[ann[\'image_id\']]\n        h, w = t[\'height\'], t[\'width\']\n        segm = ann[\'segmentation\']\n        if type(segm) == list:\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            rles = maskUtils.frPyObjects(segm, h, w)\n            rle = maskUtils.merge(rles)\n        elif type(segm[\'counts\']) == list:\n            # uncompressed RLE\n            rle = maskUtils.frPyObjects(segm, h, w)\n        else:\n            # rle\n            rle = ann[\'segmentation\']\n        return rle\n\n    def annToMask(self, ann):\n        """"""\n        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n        :return: binary mask (numpy 2D array)\n        """"""\n        rle = self.annToRLE(ann)\n        m = maskUtils.decode(rle)\n        return m'"
lib/utils/pycocotools/cocoeval.py,0,"b'__author__ = \'tsungyi\'\n\nimport numpy as np\nimport datetime\nimport time\nfrom collections import defaultdict\nfrom . import mask as maskUtils\nimport copy\n\nclass COCOeval:\n    # Interface for evaluating detection on the Microsoft COCO dataset.\n    #\n    # The usage for CocoEval is as follows:\n    #  cocoGt=..., cocoDt=...       # load dataset and results\n    #  E = CocoEval(cocoGt,cocoDt); # initialize CocoEval object\n    #  E.params.recThrs = ...;      # set parameters as desired\n    #  E.evaluate();                # run per image evaluation\n    #  E.accumulate();              # accumulate per image results\n    #  E.summarize();               # display summary metrics of results\n    # For example usage see evalDemo.m and http://mscoco.org/.\n    #\n    # The evaluation parameters are as follows (defaults in brackets):\n    #  imgIds     - [all] N img ids to use for evaluation\n    #  catIds     - [all] K cat ids to use for evaluation\n    #  iouThrs    - [.5:.05:.95] T=10 IoU thresholds for evaluation\n    #  recThrs    - [0:.01:1] R=101 recall thresholds for evaluation\n    #  areaRng    - [...] A=4 object area ranges for evaluation\n    #  maxDets    - [1 10 100] M=3 thresholds on max detections per image\n    #  iouType    - [\'segm\'] set iouType to \'segm\', \'bbox\' or \'keypoints\'\n    #  iouType replaced the now DEPRECATED useSegm parameter.\n    #  useCats    - [1] if true use category labels for evaluation\n    # Note: if useCats=0 category labels are ignored as in proposal scoring.\n    # Note: multiple areaRngs [Ax2] and maxDets [Mx1] can be specified.\n    #\n    # evaluate(): evaluates detections on every image and every category and\n    # concats the results into the ""evalImgs"" with fields:\n    #  dtIds      - [1xD] id for each of the D detections (dt)\n    #  gtIds      - [1xG] id for each of the G ground truths (gt)\n    #  dtMatches  - [TxD] matching gt id at each IoU or 0\n    #  gtMatches  - [TxG] matching dt id at each IoU or 0\n    #  dtScores   - [1xD] confidence of each dt\n    #  gtIgnore   - [1xG] ignore flag for each gt\n    #  dtIgnore   - [TxD] ignore flag for each dt at each IoU\n    #\n    # accumulate(): accumulates the per-image, per-category evaluation\n    # results in ""evalImgs"" into the dictionary ""eval"" with fields:\n    #  params     - parameters used for evaluation\n    #  date       - date evaluation was performed\n    #  counts     - [T,R,K,A,M] parameter dimensions (see above)\n    #  precision  - [TxRxKxAxM] precision for every evaluation setting\n    #  recall     - [TxKxAxM] max recall for every evaluation setting\n    # Note: precision and recall==-1 for settings with no gt objects.\n    #\n    # See also coco, mask, pycocoDemo, pycocoEvalDemo\n    #\n    # Microsoft COCO Toolbox.      version 2.0\n    # Data, paper, and tutorials available at:  http://mscoco.org/\n    # Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n    # Licensed under the Simplified BSD License [see coco/license.txt]\n    def __init__(self, cocoGt=None, cocoDt=None, iouType=\'segm\'):\n        \'\'\'\n        Initialize CocoEval using coco APIs for gt and dt\n        :param cocoGt: coco object with ground truth annotations\n        :param cocoDt: coco object with detection results\n        :return: None\n        \'\'\'\n        if not iouType:\n            print(\'iouType not specified. use default iouType segm\')\n        self.cocoGt   = cocoGt              # ground truth COCO API\n        self.cocoDt   = cocoDt              # detections COCO API\n        self.params   = {}                  # evaluation parameters\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results [KxAxI] elements\n        self.eval     = {}                  # accumulated evaluation results\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        self.params = Params(iouType=iouType) # parameters\n        self._paramsEval = {}               # parameters for evaluation\n        self.stats = []                     # result summarization\n        self.ious = {}                      # ious between all gts and dts\n        if not cocoGt is None:\n            self.params.imgIds = sorted(cocoGt.getImgIds())\n            self.params.catIds = sorted(cocoGt.getCatIds())\n\n\n    def _prepare(self):\n        \'\'\'\n        Prepare ._gts and ._dts for evaluation based on params\n        :return: None\n        \'\'\'\n        def _toMask(anns, coco):\n            # modify ann[\'segmentation\'] by reference\n            for ann in anns:\n                rle = coco.annToRLE(ann)\n                ann[\'segmentation\'] = rle\n        p = self.params\n        if p.useCats:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n        else:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds))\n\n        # convert ground truth to mask if iouType == \'segm\'\n        if p.iouType == \'segm\':\n            _toMask(gts, self.cocoGt)\n            _toMask(dts, self.cocoDt)\n        # set ignore flag\n        for gt in gts:\n            gt[\'ignore\'] = gt[\'ignore\'] if \'ignore\' in gt else 0\n            gt[\'ignore\'] = \'iscrowd\' in gt and gt[\'iscrowd\']\n            if p.iouType == \'keypoints\':\n                gt[\'ignore\'] = (gt[\'num_keypoints\'] == 0) or gt[\'ignore\']\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        for gt in gts:\n            self._gts[gt[\'image_id\'], gt[\'category_id\']].append(gt)\n        for dt in dts:\n            self._dts[dt[\'image_id\'], dt[\'category_id\']].append(dt)\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results\n        self.eval     = {}                  # accumulated evaluation results\n\n    def evaluate(self):\n        \'\'\'\n        Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n        :return: None\n        \'\'\'\n        tic = time.time()\n        print(\'Running per image evaluation...\')\n        p = self.params\n        # add backward compatibility if useSegm is specified in params\n        if not p.useSegm is None:\n            p.iouType = \'segm\' if p.useSegm == 1 else \'bbox\'\n            print(\'useSegm (deprecated) is not None. Running {} evaluation\'.format(p.iouType))\n        print(\'Evaluate annotation type *{}*\'.format(p.iouType))\n        p.imgIds = list(np.unique(p.imgIds))\n        if p.useCats:\n            p.catIds = list(np.unique(p.catIds))\n        p.maxDets = sorted(p.maxDets)\n        self.params=p\n\n        self._prepare()\n        # loop through images, area range, max detection number\n        catIds = p.catIds if p.useCats else [-1]\n\n        if p.iouType == \'segm\' or p.iouType == \'bbox\':\n            computeIoU = self.computeIoU\n        elif p.iouType == \'keypoints\':\n            computeIoU = self.computeOks\n        self.ious = {(imgId, catId): computeIoU(imgId, catId) \\\n                        for imgId in p.imgIds\n                        for catId in catIds}\n\n        evaluateImg = self.evaluateImg\n        maxDet = p.maxDets[-1]\n        self.evalImgs = [evaluateImg(imgId, catId, areaRng, maxDet)\n                 for catId in catIds\n                 for areaRng in p.areaRng\n                 for imgId in p.imgIds\n             ]\n        self._paramsEval = copy.deepcopy(self.params)\n        toc = time.time()\n        print(\'DONE (t={:0.2f}s).\'.format(toc-tic))\n\n    def computeIoU(self, imgId, catId):\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return []\n        inds = np.argsort([-d[\'score\'] for d in dt], kind=\'mergesort\')\n        dt = [dt[i] for i in inds]\n        if len(dt) > p.maxDets[-1]:\n            dt=dt[0:p.maxDets[-1]]\n\n        if p.iouType == \'segm\':\n            g = [g[\'segmentation\'] for g in gt]\n            d = [d[\'segmentation\'] for d in dt]\n        elif p.iouType == \'bbox\':\n            g = [g[\'bbox\'] for g in gt]\n            d = [d[\'bbox\'] for d in dt]\n        else:\n            raise Exception(\'unknown iouType for iou computation\')\n\n        # compute iou between each dt and gt region\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        ious = maskUtils.iou(d,g,iscrowd)\n        return ious\n\n    def computeOks(self, imgId, catId):\n        p = self.params\n        # dimention here should be Nxm\n        gts = self._gts[imgId, catId]\n        dts = self._dts[imgId, catId]\n        inds = np.argsort([-d[\'score\'] for d in dts], kind=\'mergesort\')\n        dts = [dts[i] for i in inds]\n        if len(dts) > p.maxDets[-1]:\n            dts = dts[0:p.maxDets[-1]]\n        # if len(gts) == 0 and len(dts) == 0:\n        if len(gts) == 0 or len(dts) == 0:\n            return []\n        ious = np.zeros((len(dts), len(gts)))\n        sigmas = np.array([.26, .25, .25, .35, .35, .79, .79, .72, .72, .62,.62, 1.07, 1.07, .87, .87, .89, .89])/10.0\n        vars = (sigmas * 2)**2\n        k = len(sigmas)\n        # compute oks between each detection and ground truth object\n        for j, gt in enumerate(gts):\n            # create bounds for ignore regions(double the gt bbox)\n            g = np.array(gt[\'keypoints\'])\n            xg = g[0::3]; yg = g[1::3]; vg = g[2::3]\n            k1 = np.count_nonzero(vg > 0)\n            bb = gt[\'bbox\']\n            x0 = bb[0] - bb[2]; x1 = bb[0] + bb[2] * 2\n            y0 = bb[1] - bb[3]; y1 = bb[1] + bb[3] * 2\n            for i, dt in enumerate(dts):\n                d = np.array(dt[\'keypoints\'])\n                xd = d[0::3]; yd = d[1::3]\n                if k1>0:\n                    # measure the per-keypoint distance if keypoints visible\n                    dx = xd - xg\n                    dy = yd - yg\n                else:\n                    # measure minimum distance to keypoints in (x0,y0) & (x1,y1)\n                    z = np.zeros((k))\n                    dx = np.max((z, x0-xd),axis=0)+np.max((z, xd-x1),axis=0)\n                    dy = np.max((z, y0-yd),axis=0)+np.max((z, yd-y1),axis=0)\n                e = (dx**2 + dy**2) / vars / (gt[\'area\']+np.spacing(1)) / 2\n                if k1 > 0:\n                    e=e[vg > 0]\n                ious[i, j] = np.sum(np.exp(-e)) / e.shape[0]\n        return ious\n\n    def evaluateImg(self, imgId, catId, aRng, maxDet):\n        \'\'\'\n        perform evaluation for single category and image\n        :return: dict (single image results)\n        \'\'\'\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return None\n\n        for g in gt:\n            if g[\'ignore\'] or (g[\'area\']<aRng[0] or g[\'area\']>aRng[1]):\n                g[\'_ignore\'] = 1\n            else:\n                g[\'_ignore\'] = 0\n\n        # sort dt highest score first, sort gt ignore last\n        gtind = np.argsort([g[\'_ignore\'] for g in gt], kind=\'mergesort\')\n        gt = [gt[i] for i in gtind]\n        dtind = np.argsort([-d[\'score\'] for d in dt], kind=\'mergesort\')\n        dt = [dt[i] for i in dtind[0:maxDet]]\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        # load computed ious\n        ious = self.ious[imgId, catId][:, gtind] if len(self.ious[imgId, catId]) > 0 else self.ious[imgId, catId]\n\n        T = len(p.iouThrs)\n        G = len(gt)\n        D = len(dt)\n        gtm  = np.zeros((T,G))\n        dtm  = np.zeros((T,D))\n        gtIg = np.array([g[\'_ignore\'] for g in gt])\n        dtIg = np.zeros((T,D))\n        if not len(ious)==0:\n            for tind, t in enumerate(p.iouThrs):\n                for dind, d in enumerate(dt):\n                    # information about best match so far (m=-1 -> unmatched)\n                    iou = min([t,1-1e-10])\n                    m   = -1\n                    for gind, g in enumerate(gt):\n                        # if this gt already matched, and not a crowd, continue\n                        if gtm[tind,gind]>0 and not iscrowd[gind]:\n                            continue\n                        # if dt matched to reg gt, and on ignore gt, stop\n                        if m>-1 and gtIg[m]==0 and gtIg[gind]==1:\n                            break\n                        # continue to next gt unless better match made\n                        if ious[dind,gind] < iou:\n                            continue\n                        # if match successful and best so far, store appropriately\n                        iou=ious[dind,gind]\n                        m=gind\n                    # if match made store id of match for both dt and gt\n                    if m ==-1:\n                        continue\n                    dtIg[tind,dind] = gtIg[m]\n                    dtm[tind,dind]  = gt[m][\'id\']\n                    gtm[tind,m]     = d[\'id\']\n        # set unmatched detections outside of area range to ignore\n        a = np.array([d[\'area\']<aRng[0] or d[\'area\']>aRng[1] for d in dt]).reshape((1, len(dt)))\n        dtIg = np.logical_or(dtIg, np.logical_and(dtm==0, np.repeat(a,T,0)))\n        # store results for given image and category\n        return {\n                \'image_id\':     imgId,\n                \'category_id\':  catId,\n                \'aRng\':         aRng,\n                \'maxDet\':       maxDet,\n                \'dtIds\':        [d[\'id\'] for d in dt],\n                \'gtIds\':        [g[\'id\'] for g in gt],\n                \'dtMatches\':    dtm,\n                \'gtMatches\':    gtm,\n                \'dtScores\':     [d[\'score\'] for d in dt],\n                \'gtIgnore\':     gtIg,\n                \'dtIgnore\':     dtIg,\n            }\n\n    def accumulate(self, p = None):\n        \'\'\'\n        Accumulate per image evaluation results and store the result in self.eval\n        :param p: input params for evaluation\n        :return: None\n        \'\'\'\n        print(\'Accumulating evaluation results...\')\n        tic = time.time()\n        if not self.evalImgs:\n            print(\'Please run evaluate() first\')\n        # allows input customized parameters\n        if p is None:\n            p = self.params\n        p.catIds = p.catIds if p.useCats == 1 else [-1]\n        T           = len(p.iouThrs)\n        R           = len(p.recThrs)\n        K           = len(p.catIds) if p.useCats else 1\n        A           = len(p.areaRng)\n        M           = len(p.maxDets)\n        precision   = -np.ones((T,R,K,A,M)) # -1 for the precision of absent categories\n        recall      = -np.ones((T,K,A,M))\n        scores      = -np.ones((T,R,K,A,M))\n\n        # create dictionary for future indexing\n        _pe = self._paramsEval\n        catIds = _pe.catIds if _pe.useCats else [-1]\n        setK = set(catIds)\n        setA = set(map(tuple, _pe.areaRng))\n        setM = set(_pe.maxDets)\n        setI = set(_pe.imgIds)\n        # get inds to evaluate\n        k_list = [n for n, k in enumerate(p.catIds)  if k in setK]\n        m_list = [m for n, m in enumerate(p.maxDets) if m in setM]\n        a_list = [n for n, a in enumerate(map(lambda x: tuple(x), p.areaRng)) if a in setA]\n        i_list = [n for n, i in enumerate(p.imgIds)  if i in setI]\n        I0 = len(_pe.imgIds)\n        A0 = len(_pe.areaRng)\n        # retrieve E at each category, area range, and max number of detections\n        for k, k0 in enumerate(k_list):\n            Nk = k0*A0*I0\n            for a, a0 in enumerate(a_list):\n                Na = a0*I0\n                for m, maxDet in enumerate(m_list):\n                    E = [self.evalImgs[Nk + Na + i] for i in i_list]\n                    E = [e for e in E if not e is None]\n                    if len(E) == 0:\n                        continue\n                    dtScores = np.concatenate([e[\'dtScores\'][0:maxDet] for e in E])\n\n                    # different sorting method generates slightly different results.\n                    # mergesort is used to be consistent as Matlab implementation.\n                    inds = np.argsort(-dtScores, kind=\'mergesort\')\n                    dtScoresSorted = dtScores[inds]\n\n                    dtm  = np.concatenate([e[\'dtMatches\'][:,0:maxDet] for e in E], axis=1)[:,inds]\n                    dtIg = np.concatenate([e[\'dtIgnore\'][:,0:maxDet]  for e in E], axis=1)[:,inds]\n                    gtIg = np.concatenate([e[\'gtIgnore\'] for e in E])\n                    npig = np.count_nonzero(gtIg==0 )\n                    if npig == 0:\n                        continue\n                    tps = np.logical_and(               dtm,  np.logical_not(dtIg) )\n                    fps = np.logical_and(np.logical_not(dtm), np.logical_not(dtIg) )\n\n                    tp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)\n                    fp_sum = np.cumsum(fps, axis=1).astype(dtype=np.float)\n                    for t, (tp, fp) in enumerate(zip(tp_sum, fp_sum)):\n                        tp = np.array(tp)\n                        fp = np.array(fp)\n                        nd = len(tp)\n                        rc = tp / npig\n                        pr = tp / (fp+tp+np.spacing(1))\n                        q  = np.zeros((R,))\n                        ss = np.zeros((R,))\n\n                        if nd:\n                            recall[t,k,a,m] = rc[-1]\n                        else:\n                            recall[t,k,a,m] = 0\n\n                        # numpy is slow without cython optimization for accessing elements\n                        # use python array gets significant speed improvement\n                        pr = pr.tolist(); q = q.tolist()\n\n                        for i in range(nd-1, 0, -1):\n                            if pr[i] > pr[i-1]:\n                                pr[i-1] = pr[i]\n\n                        inds = np.searchsorted(rc, p.recThrs, side=\'left\')\n                        try:\n                            for ri, pi in enumerate(inds):\n                                q[ri] = pr[pi]\n                                ss[ri] = dtScoresSorted[pi]\n                        except:\n                            pass\n                        precision[t,:,k,a,m] = np.array(q)\n                        scores[t,:,k,a,m] = np.array(ss)\n        self.eval = {\n            \'params\': p,\n            \'counts\': [T, R, K, A, M],\n            \'date\': datetime.datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\'),\n            \'precision\': precision,\n            \'recall\':   recall,\n            \'scores\': scores,\n        }\n        toc = time.time()\n        print(\'DONE (t={:0.2f}s).\'.format( toc-tic))\n\n    def summarize(self):\n        \'\'\'\n        Compute and display summary metrics for evaluation results.\n        Note this functin can *only* be applied on the default parameter setting\n        \'\'\'\n        def _summarize( ap=1, iouThr=None, areaRng=\'all\', maxDets=100 ):\n            p = self.params\n            iStr = \' {:<18} {} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}\'\n            titleStr = \'Average Precision\' if ap == 1 else \'Average Recall\'\n            typeStr = \'(AP)\' if ap==1 else \'(AR)\'\n            iouStr = \'{:0.2f}:{:0.2f}\'.format(p.iouThrs[0], p.iouThrs[-1]) \\\n                if iouThr is None else \'{:0.2f}\'.format(iouThr)\n\n            aind = [i for i, aRng in enumerate(p.areaRngLbl) if aRng == areaRng]\n            mind = [i for i, mDet in enumerate(p.maxDets) if mDet == maxDets]\n            if ap == 1:\n                # dimension of precision: [TxRxKxAxM]\n                s = self.eval[\'precision\']\n                # IoU\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                s = s[:,:,:,aind,mind]\n            else:\n                # dimension of recall: [TxKxAxM]\n                s = self.eval[\'recall\']\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                s = s[:,:,aind,mind]\n            if len(s[s>-1])==0:\n                mean_s = -1\n            else:\n                mean_s = np.mean(s[s>-1])\n            print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))\n            return mean_s\n        def _summarizeDets():\n            stats = np.zeros((12,))\n            stats[0] = _summarize(1)\n            stats[1] = _summarize(1, iouThr=.5, maxDets=self.params.maxDets[2])\n            stats[2] = _summarize(1, iouThr=.75, maxDets=self.params.maxDets[2])\n            stats[3] = _summarize(1, areaRng=\'small\', maxDets=self.params.maxDets[2])\n            stats[4] = _summarize(1, areaRng=\'medium\', maxDets=self.params.maxDets[2])\n            stats[5] = _summarize(1, areaRng=\'large\', maxDets=self.params.maxDets[2])\n            stats[6] = _summarize(0, maxDets=self.params.maxDets[0])\n            stats[7] = _summarize(0, maxDets=self.params.maxDets[1])\n            stats[8] = _summarize(0, maxDets=self.params.maxDets[2])\n            stats[9] = _summarize(0, areaRng=\'small\', maxDets=self.params.maxDets[2])\n            stats[10] = _summarize(0, areaRng=\'medium\', maxDets=self.params.maxDets[2])\n            stats[11] = _summarize(0, areaRng=\'large\', maxDets=self.params.maxDets[2])\n            return stats\n        def _summarizeKps():\n            stats = np.zeros((10,))\n            stats[0] = _summarize(1, maxDets=20)\n            stats[1] = _summarize(1, maxDets=20, iouThr=.5)\n            stats[2] = _summarize(1, maxDets=20, iouThr=.75)\n            stats[3] = _summarize(1, maxDets=20, areaRng=\'medium\')\n            stats[4] = _summarize(1, maxDets=20, areaRng=\'large\')\n            stats[5] = _summarize(0, maxDets=20)\n            stats[6] = _summarize(0, maxDets=20, iouThr=.5)\n            stats[7] = _summarize(0, maxDets=20, iouThr=.75)\n            stats[8] = _summarize(0, maxDets=20, areaRng=\'medium\')\n            stats[9] = _summarize(0, maxDets=20, areaRng=\'large\')\n            return stats\n        if not self.eval:\n            raise Exception(\'Please run accumulate() first\')\n        iouType = self.params.iouType\n        if iouType == \'segm\' or iouType == \'bbox\':\n            summarize = _summarizeDets\n        elif iouType == \'keypoints\':\n            summarize = _summarizeKps\n        self.stats = summarize()\n\n    def __str__(self):\n        self.summarize()\n\nclass Params:\n    \'\'\'\n    Params for coco evaluation api\n    \'\'\'\n    def setDetParams(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, np.round((1.00 - .0) / .01) + 1, endpoint=True)\n        self.maxDets = [1, 10, 100]\n        self.areaRng = [[0 ** 2, 1e5 ** 2], [0 ** 2, 32 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n        self.areaRngLbl = [\'all\', \'small\', \'medium\', \'large\']\n        self.useCats = 1\n\n    def setKpParams(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, np.round((1.00 - .0) / .01) + 1, endpoint=True)\n        self.maxDets = [20]\n        self.areaRng = [[0 ** 2, 1e5 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n        self.areaRngLbl = [\'all\', \'medium\', \'large\']\n        self.useCats = 1\n\n    def __init__(self, iouType=\'segm\'):\n        if iouType == \'segm\' or iouType == \'bbox\':\n            self.setDetParams()\n        elif iouType == \'keypoints\':\n            self.setKpParams()\n        else:\n            raise Exception(\'iouType not supported\')\n        self.iouType = iouType\n        # useSegm is deprecated\n        self.useSegm = None'"
lib/utils/pycocotools/mask.py,0,"b'__author__ = \'tsungyi\'\n\nimport lib.utils.pycocotools._mask as _mask\n\n# Interface for manipulating masks stored in RLE format.\n#\n# RLE is a simple yet efficient format for storing binary masks. RLE\n# first divides a vector (or vectorized image) into a series of piecewise\n# constant regions and then for each piece simply stores the length of\n# that piece. For example, given M=[0 0 1 1 1 0 1] the RLE counts would\n# be [2 3 1 1], or for M=[1 1 1 1 1 1 0] the counts would be [0 6 1]\n# (note that the odd counts are always the numbers of zeros). Instead of\n# storing the counts directly, additional compression is achieved with a\n# variable bitrate representation based on a common scheme called LEB128.\n#\n# Compression is greatest given large piecewise constant regions.\n# Specifically, the size of the RLE is proportional to the number of\n# *boundaries* in M (or for an image the number of boundaries in the y\n# direction). Assuming fairly simple shapes, the RLE representation is\n# O(sqrt(n)) where n is number of pixels in the object. Hence space usage\n# is substantially lower, especially for large simple objects (large n).\n#\n# Many common operations on masks can be computed directly using the RLE\n# (without need for decoding). This includes computations such as area,\n# union, intersection, etc. All of these operations are linear in the\n# size of the RLE, in other words they are O(sqrt(n)) where n is the area\n# of the object. Computing these operations on the original mask is O(n).\n# Thus, using the RLE can result in substantial computational savings.\n#\n# The following API functions are defined:\n#  encode         - Encode binary masks using RLE.\n#  decode         - Decode binary masks encoded via RLE.\n#  merge          - Compute union or intersection of encoded masks.\n#  iou            - Compute intersection over union between masks.\n#  area           - Compute area of encoded masks.\n#  toBbox         - Get bounding boxes surrounding encoded masks.\n#  frPyObjects    - Convert polygon, bbox, and uncompressed RLE to encoded RLE mask.\n#\n# Usage:\n#  Rs     = encode( masks )\n#  masks  = decode( Rs )\n#  R      = merge( Rs, intersect=false )\n#  o      = iou( dt, gt, iscrowd )\n#  a      = area( Rs )\n#  bbs    = toBbox( Rs )\n#  Rs     = frPyObjects( [pyObjects], h, w )\n#\n# In the API the following formats are used:\n#  Rs      - [dict] Run-length encoding of binary masks\n#  R       - dict Run-length encoding of binary mask\n#  masks   - [hxwxn] Binary mask(s) (must have type np.ndarray(dtype=uint8) in column-major order)\n#  iscrowd - [nx1] list of np.ndarray. 1 indicates corresponding gt image has crowd region to ignore\n#  bbs     - [nx4] Bounding box(es) stored as [x y w h]\n#  poly    - Polygon stored as [[x1 y1 x2 y2...],[x1 y1 ...],...] (2D list)\n#  dt,gt   - May be either bounding boxes or encoded masks\n# Both poly and bbs are 0-indexed (bbox=[0 0 1 1] encloses first pixel).\n#\n# Finally, a note about the intersection over union (iou) computation.\n# The standard iou of a ground truth (gt) and detected (dt) object is\n#  iou(gt,dt) = area(intersect(gt,dt)) / area(union(gt,dt))\n# For ""crowd"" regions, we use a modified criteria. If a gt object is\n# marked as ""iscrowd"", we allow a dt to match any subregion of the gt.\n# Choosing gt\' in the crowd gt that best matches the dt can be done using\n# gt\'=intersect(dt,gt). Since by definition union(gt\',dt)=dt, computing\n#  iou(gt,dt,iscrowd) = iou(gt\',dt) = area(intersect(gt,dt)) / area(dt)\n# For crowd gt regions we use this modified criteria above for the iou.\n#\n# To compile run ""python setup.py build_ext --inplace""\n# Please do not contact us for help with compiling.\n#\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n# Licensed under the Simplified BSD License [see coco/license.txt]\n\niou         = _mask.iou\nmerge       = _mask.merge\nfrPyObjects = _mask.frPyObjects\n\ndef encode(bimask):\n    if len(bimask.shape) == 3:\n        return _mask.encode(bimask)\n    elif len(bimask.shape) == 2:\n        h, w = bimask.shape\n        return _mask.encode(bimask.reshape((h, w, 1), order=\'F\'))[0]\n\ndef decode(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.decode(rleObjs)\n    else:\n        return _mask.decode([rleObjs])[:,:,0]\n\ndef area(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.area(rleObjs)\n    else:\n        return _mask.area([rleObjs])[0]\n\ndef toBbox(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.toBbox(rleObjs)\n    else:\n        return _mask.toBbox([rleObjs])[0]'"
lib/utils/nms/_ext/__init__.py,0,b''
lib/utils/nms/_ext/nms/__init__.py,1,"b'\nfrom torch.utils.ffi import _wrap_function\nfrom ._nms import lib as _lib, ffi as _ffi\n\n__all__ = []\ndef _import_symbols(locals):\n    for symbol in dir(_lib):\n        fn = getattr(_lib, symbol)\n        if callable(fn):\n            locals[symbol] = _wrap_function(fn, _ffi)\n        else:\n            locals[symbol] = fn\n        __all__.append(symbol)\n\n_import_symbols(locals())\n'"
