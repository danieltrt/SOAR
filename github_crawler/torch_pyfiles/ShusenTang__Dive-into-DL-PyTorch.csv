file_path,api_count,code
code/d2lzh_pytorch/__init__.py,0,b'from .utils import *\n\n'
code/d2lzh_pytorch/utils.py,65,"b'import collections\nimport math\nimport os\nimport random\nimport sys\nimport tarfile\nimport time\nimport json\nimport zipfile\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom collections import namedtuple\n\nfrom IPython import display\nfrom matplotlib import pyplot as plt\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torchtext\nimport torchtext.vocab as Vocab\nimport numpy as np\n\n\nVOC_CLASSES = [\'background\', \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n               \'bottle\', \'bus\', \'car\', \'cat\', \'chair\', \'cow\',\n               \'diningtable\', \'dog\', \'horse\', \'motorbike\', \'person\',\n               \'potted plant\', \'sheep\', \'sofa\', \'train\', \'tv/monitor\']\n\n\nVOC_COLORMAP = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],\n                [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],\n                [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],\n                [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],\n                [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],\n                [0, 64, 128]]\n\n\n\n# ###################### 3.2 ############################\ndef set_figsize(figsize=(3.5, 2.5)):\n    use_svg_display()\n    # \xe8\xae\xbe\xe7\xbd\xae\xe5\x9b\xbe\xe7\x9a\x84\xe5\xb0\xba\xe5\xaf\xb8\n    plt.rcParams[\'figure.figsize\'] = figsize\n\ndef use_svg_display():\n    """"""Use svg format to display plot in jupyter""""""\n    display.set_matplotlib_formats(\'svg\')\n\ndef data_iter(batch_size, features, labels):\n    num_examples = len(features)\n    indices = list(range(num_examples))\n    random.shuffle(indices)  # \xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84\xe8\xaf\xbb\xe5\x8f\x96\xe9\xa1\xba\xe5\xba\x8f\xe6\x98\xaf\xe9\x9a\x8f\xe6\x9c\xba\xe7\x9a\x84\n    for i in range(0, num_examples, batch_size):\n        j = torch.LongTensor(indices[i: min(i + batch_size, num_examples)]) # \xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe6\xac\xa1\xe5\x8f\xaf\xe8\x83\xbd\xe4\xb8\x8d\xe8\xb6\xb3\xe4\xb8\x80\xe4\xb8\xaabatch\n        yield  features.index_select(0, j), labels.index_select(0, j) \n\ndef linreg(X, w, b):\n    return torch.mm(X, w) + b\n\ndef squared_loss(y_hat, y): \n    # \xe6\xb3\xa8\xe6\x84\x8f\xe8\xbf\x99\xe9\x87\x8c\xe8\xbf\x94\xe5\x9b\x9e\xe7\x9a\x84\xe6\x98\xaf\xe5\x90\x91\xe9\x87\x8f, \xe5\x8f\xa6\xe5\xa4\x96, pytorch\xe9\x87\x8c\xe7\x9a\x84MSELoss\xe5\xb9\xb6\xe6\xb2\xa1\xe6\x9c\x89\xe9\x99\xa4\xe4\xbb\xa5 2\n    return ((y_hat - y.view(y_hat.size())) ** 2) / 2\n\ndef sgd(params, lr, batch_size):\n    # \xe4\xb8\xba\xe4\xba\x86\xe5\x92\x8c\xe5\x8e\x9f\xe4\xb9\xa6\xe4\xbf\x9d\xe6\x8c\x81\xe4\xb8\x80\xe8\x87\xb4\xef\xbc\x8c\xe8\xbf\x99\xe9\x87\x8c\xe9\x99\xa4\xe4\xbb\xa5\xe4\xba\x86batch_size\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe5\xba\x94\xe8\xaf\xa5\xe6\x98\xaf\xe4\xb8\x8d\xe7\x94\xa8\xe9\x99\xa4\xe7\x9a\x84\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba\xe4\xb8\x80\xe8\x88\xac\xe7\x94\xa8PyTorch\xe8\xae\xa1\xe7\xae\x97loss\xe6\x97\xb6\xe5\xb0\xb1\xe9\xbb\x98\xe8\xae\xa4\xe5\xb7\xb2\xe7\xbb\x8f\n    # \xe6\xb2\xbfbatch\xe7\xbb\xb4\xe6\xb1\x82\xe4\xba\x86\xe5\xb9\xb3\xe5\x9d\x87\xe4\xba\x86\xe3\x80\x82\n    for param in params:\n        param.data -= lr * param.grad / batch_size # \xe6\xb3\xa8\xe6\x84\x8f\xe8\xbf\x99\xe9\x87\x8c\xe6\x9b\xb4\xe6\x94\xb9param\xe6\x97\xb6\xe7\x94\xa8\xe7\x9a\x84param.data\n\n\n\n# ######################3##### 3.5 #############################\ndef get_fashion_mnist_labels(labels):\n    text_labels = [\'t-shirt\', \'trouser\', \'pullover\', \'dress\', \'coat\',\n                   \'sandal\', \'shirt\', \'sneaker\', \'bag\', \'ankle boot\']\n    return [text_labels[int(i)] for i in labels]\n\ndef show_fashion_mnist(images, labels):\n    use_svg_display()\n    # \xe8\xbf\x99\xe9\x87\x8c\xe7\x9a\x84_\xe8\xa1\xa8\xe7\xa4\xba\xe6\x88\x91\xe4\xbb\xac\xe5\xbf\xbd\xe7\x95\xa5\xef\xbc\x88\xe4\xb8\x8d\xe4\xbd\xbf\xe7\x94\xa8\xef\xbc\x89\xe7\x9a\x84\xe5\x8f\x98\xe9\x87\x8f\n    _, figs = plt.subplots(1, len(images), figsize=(12, 12))\n    for f, img, lbl in zip(figs, images, labels):\n        f.imshow(img.view((28, 28)).numpy())\n        f.set_title(lbl)\n        f.axes.get_xaxis().set_visible(False)\n        f.axes.get_yaxis().set_visible(False)\n    # plt.show()\n\n# 5.6 \xe4\xbf\xae\xe6\x94\xb9\n# def load_data_fashion_mnist(batch_size, root=\'~/Datasets/FashionMNIST\'):\n#     """"""Download the fashion mnist dataset and then load into memory.""""""\n#     transform = transforms.ToTensor()\n#     mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=transform)\n#     mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, download=True, transform=transform)\n#     if sys.platform.startswith(\'win\'):\n#         num_workers = 0  # 0\xe8\xa1\xa8\xe7\xa4\xba\xe4\xb8\x8d\xe7\x94\xa8\xe9\xa2\x9d\xe5\xa4\x96\xe7\x9a\x84\xe8\xbf\x9b\xe7\xa8\x8b\xe6\x9d\xa5\xe5\x8a\xa0\xe9\x80\x9f\xe8\xaf\xbb\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\n#     else:\n#         num_workers = 4\n#     train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n#     test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n#     return train_iter, test_iter\n\n\n\n\n# ########################### 3.6  ###############################\n# (3.13\xe8\x8a\x82\xe4\xbf\xae\xe6\x94\xb9)\n# def evaluate_accuracy(data_iter, net):\n#     acc_sum, n = 0.0, 0\n#     for X, y in data_iter:\n#         acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n#         n += y.shape[0]\n#     return acc_sum / n\n\n\ndef train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,\n              params=None, lr=None, optimizer=None):\n    for epoch in range(num_epochs):\n        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n        for X, y in train_iter:\n            y_hat = net(X)\n            l = loss(y_hat, y).sum()\n            \n            # \xe6\xa2\xaf\xe5\xba\xa6\xe6\xb8\x85\xe9\x9b\xb6\n            if optimizer is not None:\n                optimizer.zero_grad()\n            elif params is not None and params[0].grad is not None:\n                for param in params:\n                    param.grad.data.zero_()\n            \n            l.backward()\n            if optimizer is None:\n                sgd(params, lr, batch_size)\n            else:\n                optimizer.step()  # \xe2\x80\x9csoftmax\xe5\x9b\x9e\xe5\xbd\x92\xe7\x9a\x84\xe7\xae\x80\xe6\xb4\x81\xe5\xae\x9e\xe7\x8e\xb0\xe2\x80\x9d\xe4\xb8\x80\xe8\x8a\x82\xe5\xb0\x86\xe7\x94\xa8\xe5\x88\xb0\n            \n            \n            train_l_sum += l.item()\n            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n            n += y.shape[0]\n        test_acc = evaluate_accuracy(test_iter, net)\n        print(\'epoch %d, loss %.4f, train acc %.3f, test acc %.3f\'\n              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n\n\n\n\n# ########################### 3.7 #####################################3\nclass FlattenLayer(torch.nn.Module):\n    def __init__(self):\n        super(FlattenLayer, self).__init__()\n    def forward(self, x): # x shape: (batch, *, *, ...)\n        return x.view(x.shape[0], -1)\n\n\n# ########################### 3.11 ###############################\ndef semilogy(x_vals, y_vals, x_label, y_label, x2_vals=None, y2_vals=None,\n             legend=None, figsize=(3.5, 2.5)):\n    set_figsize(figsize)\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n    plt.semilogy(x_vals, y_vals)\n    if x2_vals and y2_vals:\n        plt.semilogy(x2_vals, y2_vals, linestyle=\':\')\n        plt.legend(legend)\n    # plt.show()\n\n\n\n\n# ############################# 3.13 ##############################\n# 5.5 \xe4\xbf\xae\xe6\x94\xb9\n# def evaluate_accuracy(data_iter, net):\n#     acc_sum, n = 0.0, 0\n#     for X, y in data_iter:\n#         if isinstance(net, torch.nn.Module):\n#             net.eval() # \xe8\xaf\x84\xe4\xbc\xb0\xe6\xa8\xa1\xe5\xbc\x8f, \xe8\xbf\x99\xe4\xbc\x9a\xe5\x85\xb3\xe9\x97\xaddropout\n#             acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n#             net.train() # \xe6\x94\xb9\xe5\x9b\x9e\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\xbc\x8f\n#         else: # \xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\n#             if(\'is_training\' in net.__code__.co_varnames): # \xe5\xa6\x82\xe6\x9e\x9c\xe6\x9c\x89is_training\xe8\xbf\x99\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\n#                 # \xe5\xb0\x86is_training\xe8\xae\xbe\xe7\xbd\xae\xe6\x88\x90False\n#                 acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n#             else:\n#                 acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n#         n += y.shape[0]\n#     return acc_sum / n\n\n\n\n\n\n\n# ########################### 5.1 #########################\ndef corr2d(X, K):  \n    h, w = K.shape\n    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n    for i in range(Y.shape[0]):\n        for j in range(Y.shape[1]):\n            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()\n    return Y\n\n\n\n# ############################ 5.5 #########################\ndef evaluate_accuracy(data_iter, net, device=None):\n    if device is None and isinstance(net, torch.nn.Module):\n        # \xe5\xa6\x82\xe6\x9e\x9c\xe6\xb2\xa1\xe6\x8c\x87\xe5\xae\x9adevice\xe5\xb0\xb1\xe4\xbd\xbf\xe7\x94\xa8net\xe7\x9a\x84device\n        device = list(net.parameters())[0].device \n    acc_sum, n = 0.0, 0\n    with torch.no_grad():\n        for X, y in data_iter:\n            if isinstance(net, torch.nn.Module):\n                net.eval() # \xe8\xaf\x84\xe4\xbc\xb0\xe6\xa8\xa1\xe5\xbc\x8f, \xe8\xbf\x99\xe4\xbc\x9a\xe5\x85\xb3\xe9\x97\xaddropout\n                acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n                net.train() # \xe6\x94\xb9\xe5\x9b\x9e\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\xbc\x8f\n            else: # \xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b, 3.13\xe8\x8a\x82\xe4\xb9\x8b\xe5\x90\x8e\xe4\xb8\x8d\xe4\xbc\x9a\xe7\x94\xa8\xe5\x88\xb0, \xe4\xb8\x8d\xe8\x80\x83\xe8\x99\x91GPU\n                if(\'is_training\' in net.__code__.co_varnames): # \xe5\xa6\x82\xe6\x9e\x9c\xe6\x9c\x89is_training\xe8\xbf\x99\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\n                    # \xe5\xb0\x86is_training\xe8\xae\xbe\xe7\xbd\xae\xe6\x88\x90False\n                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n                else:\n                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n            n += y.shape[0]\n    return acc_sum / n\n\ndef train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs):\n    net = net.to(device)\n    print(""training on "", device)\n    loss = torch.nn.CrossEntropyLoss()\n    for epoch in range(num_epochs):\n        train_l_sum, train_acc_sum, n, batch_count, start = 0.0, 0.0, 0, 0, time.time()\n        for X, y in train_iter:\n            X = X.to(device)\n            y = y.to(device)\n            y_hat = net(X)\n            l = loss(y_hat, y)\n            optimizer.zero_grad()\n            l.backward()\n            optimizer.step()\n            train_l_sum += l.cpu().item()\n            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n            n += y.shape[0]\n            batch_count += 1\n        test_acc = evaluate_accuracy(test_iter, net)\n        print(\'epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec\'\n              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))\n\n\n\n# ########################## 5.6 #########################3\ndef load_data_fashion_mnist(batch_size, resize=None, root=\'~/Datasets/FashionMNIST\'):\n    """"""Download the fashion mnist dataset and then load into memory.""""""\n    trans = []\n    if resize:\n        trans.append(torchvision.transforms.Resize(size=resize))\n    trans.append(torchvision.transforms.ToTensor())\n    \n    transform = torchvision.transforms.Compose(trans)\n    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=transform)\n    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, download=True, transform=transform)\n    if sys.platform.startswith(\'win\'):\n        num_workers = 0  # 0\xe8\xa1\xa8\xe7\xa4\xba\xe4\xb8\x8d\xe7\x94\xa8\xe9\xa2\x9d\xe5\xa4\x96\xe7\x9a\x84\xe8\xbf\x9b\xe7\xa8\x8b\xe6\x9d\xa5\xe5\x8a\xa0\xe9\x80\x9f\xe8\xaf\xbb\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\n    else:\n        num_workers = 4\n    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n    return train_iter, test_iter\n\n\n\n############################# 5.8 ##############################\nclass GlobalAvgPool2d(nn.Module):\n    # \xe5\x85\xa8\xe5\xb1\x80\xe5\xb9\xb3\xe5\x9d\x87\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\xe5\x8f\xaf\xe9\x80\x9a\xe8\xbf\x87\xe5\xb0\x86\xe6\xb1\xa0\xe5\x8c\x96\xe7\xaa\x97\xe5\x8f\xa3\xe5\xbd\xa2\xe7\x8a\xb6\xe8\xae\xbe\xe7\xbd\xae\xe6\x88\x90\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe9\xab\x98\xe5\x92\x8c\xe5\xae\xbd\xe5\xae\x9e\xe7\x8e\xb0\n    def __init__(self):\n        super(GlobalAvgPool2d, self).__init__()\n    def forward(self, x):\n        return F.avg_pool2d(x, kernel_size=x.size()[2:])\n\n\n\n# ########################### 5.11 ################################\nclass Residual(nn.Module): \n    def __init__(self, in_channels, out_channels, use_1x1conv=False, stride=1):\n        super(Residual, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n        if use_1x1conv:\n            self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n        else:\n            self.conv3 = None\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n    def forward(self, X):\n        Y = F.relu(self.bn1(self.conv1(X)))\n        Y = self.bn2(self.conv2(Y))\n        if self.conv3:\n            X = self.conv3(X)\n        return F.relu(Y + X)\n\ndef resnet_block(in_channels, out_channels, num_residuals, first_block=False):\n    if first_block:\n        assert in_channels == out_channels # \xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9d\x97\xe7\x9a\x84\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xe5\x90\x8c\xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xe4\xb8\x80\xe8\x87\xb4\n    blk = []\n    for i in range(num_residuals):\n        if i == 0 and not first_block:\n            blk.append(Residual(in_channels, out_channels, use_1x1conv=True, stride=2))\n        else:\n            blk.append(Residual(out_channels, out_channels))\n    return nn.Sequential(*blk)\n    \ndef resnet18(output=10, in_channels=3):\n    net = nn.Sequential(\n        nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3),\n        nn.BatchNorm2d(64), \n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n    net.add_module(""resnet_block1"", resnet_block(64, 64, 2, first_block=True))\n    net.add_module(""resnet_block2"", resnet_block(64, 128, 2))\n    net.add_module(""resnet_block3"", resnet_block(128, 256, 2))\n    net.add_module(""resnet_block4"", resnet_block(256, 512, 2))\n    net.add_module(""global_avg_pool"", GlobalAvgPool2d()) # GlobalAvgPool2d\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba: (Batch, 512, 1, 1)\n    net.add_module(""fc"", nn.Sequential(FlattenLayer(), nn.Linear(512, output))) \n    return net\n\n\n\n# ############################## 6.3 ##################################3\ndef load_data_jay_lyrics():\n    """"""\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x91\xa8\xe6\x9d\xb0\xe4\xbc\xa6\xe6\xad\x8c\xe8\xaf\x8d\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86""""""\n    with zipfile.ZipFile(\'../../data/jaychou_lyrics.txt.zip\') as zin:\n        with zin.open(\'jaychou_lyrics.txt\') as f:\n            corpus_chars = f.read().decode(\'utf-8\')\n    corpus_chars = corpus_chars.replace(\'\\n\', \' \').replace(\'\\r\', \' \')\n    corpus_chars = corpus_chars[0:10000]\n    idx_to_char = list(set(corpus_chars))\n    char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n    vocab_size = len(char_to_idx)\n    corpus_indices = [char_to_idx[char] for char in corpus_chars]\n    return corpus_indices, char_to_idx, idx_to_char, vocab_size\n\ndef data_iter_random(corpus_indices, batch_size, num_steps, device=None):\n    # \xe5\x87\x8f1\xe6\x98\xaf\xe5\x9b\xa0\xe4\xb8\xba\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe7\xb4\xa2\xe5\xbc\x95x\xe6\x98\xaf\xe7\x9b\xb8\xe5\xba\x94\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe7\xb4\xa2\xe5\xbc\x95y\xe5\x8a\xa01\n    num_examples = (len(corpus_indices) - 1) // num_steps\n    epoch_size = num_examples // batch_size\n    example_indices = list(range(num_examples))\n    random.shuffle(example_indices)\n\n    # \xe8\xbf\x94\xe5\x9b\x9e\xe4\xbb\x8epos\xe5\xbc\x80\xe5\xa7\x8b\xe7\x9a\x84\xe9\x95\xbf\xe4\xb8\xbanum_steps\xe7\x9a\x84\xe5\xba\x8f\xe5\x88\x97\n    def _data(pos):\n        return corpus_indices[pos: pos + num_steps]\n    if device is None:\n        device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    \n    for i in range(epoch_size):\n        # \xe6\xaf\x8f\xe6\xac\xa1\xe8\xaf\xbb\xe5\x8f\x96batch_size\xe4\xb8\xaa\xe9\x9a\x8f\xe6\x9c\xba\xe6\xa0\xb7\xe6\x9c\xac\n        i = i * batch_size\n        batch_indices = example_indices[i: i + batch_size]\n        X = [_data(j * num_steps) for j in batch_indices]\n        Y = [_data(j * num_steps + 1) for j in batch_indices]\n        yield torch.tensor(X, dtype=torch.float32, device=device), torch.tensor(Y, dtype=torch.float32, device=device)\n\ndef data_iter_consecutive(corpus_indices, batch_size, num_steps, device=None):\n    if device is None:\n        device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    corpus_indices = torch.tensor(corpus_indices, dtype=torch.float32, device=device)\n    data_len = len(corpus_indices)\n    batch_len = data_len // batch_size\n    indices = corpus_indices[0: batch_size*batch_len].view(batch_size, batch_len)\n    epoch_size = (batch_len - 1) // num_steps\n    for i in range(epoch_size):\n        i = i * num_steps\n        X = indices[:, i: i + num_steps]\n        Y = indices[:, i + 1: i + num_steps + 1]\n        yield X, Y\n\n\n\n\n\n# ###################################### 6.4 ######################################\ndef one_hot(x, n_class, dtype=torch.float32): \n    # X shape: (batch), output shape: (batch, n_class)\n    x = x.long()\n    res = torch.zeros(x.shape[0], n_class, dtype=dtype, device=x.device)\n    res.scatter_(1, x.view(-1, 1), 1)\n    return res\n\ndef to_onehot(X, n_class):  \n    # X shape: (batch, seq_len), output: seq_len elements of (batch, n_class)\n    return [one_hot(X[:, i], n_class) for i in range(X.shape[1])]\n\ndef predict_rnn(prefix, num_chars, rnn, params, init_rnn_state,\n                num_hiddens, vocab_size, device, idx_to_char, char_to_idx):\n    state = init_rnn_state(1, num_hiddens, device)\n    output = [char_to_idx[prefix[0]]]\n    for t in range(num_chars + len(prefix) - 1):\n        # \xe5\xb0\x86\xe4\xb8\x8a\xe4\xb8\x80\xe6\x97\xb6\xe9\x97\xb4\xe6\xad\xa5\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe4\xbd\x9c\xe4\xb8\xba\xe5\xbd\x93\xe5\x89\x8d\xe6\x97\xb6\xe9\x97\xb4\xe6\xad\xa5\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\n        X = to_onehot(torch.tensor([[output[-1]]], device=device), vocab_size)\n        # \xe8\xae\xa1\xe7\xae\x97\xe8\xbe\x93\xe5\x87\xba\xe5\x92\x8c\xe6\x9b\xb4\xe6\x96\xb0\xe9\x9a\x90\xe8\x97\x8f\xe7\x8a\xb6\xe6\x80\x81\n        (Y, state) = rnn(X, state, params)\n        # \xe4\xb8\x8b\xe4\xb8\x80\xe4\xb8\xaa\xe6\x97\xb6\xe9\x97\xb4\xe6\xad\xa5\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe6\x98\xafprefix\xe9\x87\x8c\xe7\x9a\x84\xe5\xad\x97\xe7\xac\xa6\xe6\x88\x96\xe8\x80\x85\xe5\xbd\x93\xe5\x89\x8d\xe7\x9a\x84\xe6\x9c\x80\xe4\xbd\xb3\xe9\xa2\x84\xe6\xb5\x8b\xe5\xad\x97\xe7\xac\xa6\n        if t < len(prefix) - 1:\n            output.append(char_to_idx[prefix[t + 1]])\n        else:\n            output.append(int(Y[0].argmax(dim=1).item()))\n    return \'\'.join([idx_to_char[i] for i in output])\n\ndef grad_clipping(params, theta, device):\n    norm = torch.tensor([0.0], device=device)\n    for param in params:\n        norm += (param.grad.data ** 2).sum()\n    norm = norm.sqrt().item()\n    if norm > theta:\n        for param in params:\n            param.grad.data *= (theta / norm)\n\ndef train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n                          vocab_size, device, corpus_indices, idx_to_char,\n                          char_to_idx, is_random_iter, num_epochs, num_steps,\n                          lr, clipping_theta, batch_size, pred_period,\n                          pred_len, prefixes):\n    if is_random_iter:\n        data_iter_fn = data_iter_random\n    else:\n        data_iter_fn = data_iter_consecutive\n    params = get_params()\n    loss = nn.CrossEntropyLoss()\n\n    for epoch in range(num_epochs):\n        if not is_random_iter:  # \xe5\xa6\x82\xe4\xbd\xbf\xe7\x94\xa8\xe7\x9b\xb8\xe9\x82\xbb\xe9\x87\x87\xe6\xa0\xb7\xef\xbc\x8c\xe5\x9c\xa8epoch\xe5\xbc\x80\xe5\xa7\x8b\xe6\x97\xb6\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe9\x9a\x90\xe8\x97\x8f\xe7\x8a\xb6\xe6\x80\x81\n            state = init_rnn_state(batch_size, num_hiddens, device)\n        l_sum, n, start = 0.0, 0, time.time()\n        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, device)\n        for X, Y in data_iter:\n            if is_random_iter:  # \xe5\xa6\x82\xe4\xbd\xbf\xe7\x94\xa8\xe9\x9a\x8f\xe6\x9c\xba\xe9\x87\x87\xe6\xa0\xb7\xef\xbc\x8c\xe5\x9c\xa8\xe6\xaf\x8f\xe4\xb8\xaa\xe5\xb0\x8f\xe6\x89\xb9\xe9\x87\x8f\xe6\x9b\xb4\xe6\x96\xb0\xe5\x89\x8d\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe9\x9a\x90\xe8\x97\x8f\xe7\x8a\xb6\xe6\x80\x81\n                state = init_rnn_state(batch_size, num_hiddens, device)\n            else: \n            # \xe5\x90\xa6\xe5\x88\x99\xe9\x9c\x80\xe8\xa6\x81\xe4\xbd\xbf\xe7\x94\xa8detach\xe5\x87\xbd\xe6\x95\xb0\xe4\xbb\x8e\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe5\x88\x86\xe7\xa6\xbb\xe9\x9a\x90\xe8\x97\x8f\xe7\x8a\xb6\xe6\x80\x81, \xe8\xbf\x99\xe6\x98\xaf\xe4\xb8\xba\xe4\xba\x86\n            # \xe4\xbd\xbf\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe8\xae\xa1\xe7\xae\x97\xe5\x8f\xaa\xe4\xbe\x9d\xe8\xb5\x96\xe4\xb8\x80\xe6\xac\xa1\xe8\xbf\xad\xe4\xbb\xa3\xe8\xaf\xbb\xe5\x8f\x96\xe7\x9a\x84\xe5\xb0\x8f\xe6\x89\xb9\xe9\x87\x8f\xe5\xba\x8f\xe5\x88\x97(\xe9\x98\xb2\xe6\xad\xa2\xe6\xa2\xaf\xe5\xba\xa6\xe8\xae\xa1\xe7\xae\x97\xe5\xbc\x80\xe9\x94\x80\xe5\xa4\xaa\xe5\xa4\xa7)\n                for s in state:\n                    s.detach_()\n            \n            inputs = to_onehot(X, vocab_size)\n            # outputs\xe6\x9c\x89num_steps\xe4\xb8\xaa\xe5\xbd\xa2\xe7\x8a\xb6\xe4\xb8\xba(batch_size, vocab_size)\xe7\x9a\x84\xe7\x9f\xa9\xe9\x98\xb5\n            (outputs, state) = rnn(inputs, state, params)\n            # \xe6\x8b\xbc\xe6\x8e\xa5\xe4\xb9\x8b\xe5\x90\x8e\xe5\xbd\xa2\xe7\x8a\xb6\xe4\xb8\xba(num_steps * batch_size, vocab_size)\n            outputs = torch.cat(outputs, dim=0)\n            # Y\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\xe6\x98\xaf(batch_size, num_steps)\xef\xbc\x8c\xe8\xbd\xac\xe7\xbd\xae\xe5\x90\x8e\xe5\x86\x8d\xe5\x8f\x98\xe6\x88\x90\xe9\x95\xbf\xe5\xba\xa6\xe4\xb8\xba\n            # batch * num_steps \xe7\x9a\x84\xe5\x90\x91\xe9\x87\x8f\xef\xbc\x8c\xe8\xbf\x99\xe6\xa0\xb7\xe8\xb7\x9f\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe8\xa1\x8c\xe4\xb8\x80\xe4\xb8\x80\xe5\xaf\xb9\xe5\xba\x94\n            y = torch.transpose(Y, 0, 1).contiguous().view(-1)\n            # \xe4\xbd\xbf\xe7\x94\xa8\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\xe6\x8d\x9f\xe5\xa4\xb1\xe8\xae\xa1\xe7\xae\x97\xe5\xb9\xb3\xe5\x9d\x87\xe5\x88\x86\xe7\xb1\xbb\xe8\xaf\xaf\xe5\xb7\xae\n            l = loss(outputs, y.long())\n            \n            # \xe6\xa2\xaf\xe5\xba\xa6\xe6\xb8\x850\n            if params[0].grad is not None:\n                for param in params:\n                    param.grad.data.zero_()\n            l.backward()\n            grad_clipping(params, clipping_theta, device)  # \xe8\xa3\x81\xe5\x89\xaa\xe6\xa2\xaf\xe5\xba\xa6\n            sgd(params, lr, 1)  # \xe5\x9b\xa0\xe4\xb8\xba\xe8\xaf\xaf\xe5\xb7\xae\xe5\xb7\xb2\xe7\xbb\x8f\xe5\x8f\x96\xe8\xbf\x87\xe5\x9d\x87\xe5\x80\xbc\xef\xbc\x8c\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8d\xe7\x94\xa8\xe5\x86\x8d\xe5\x81\x9a\xe5\xb9\xb3\xe5\x9d\x87\n            l_sum += l.item() * y.shape[0]\n            n += y.shape[0]\n\n        if (epoch + 1) % pred_period == 0:\n            print(\'epoch %d, perplexity %f, time %.2f sec\' % (\n                epoch + 1, math.exp(l_sum / n), time.time() - start))\n            for prefix in prefixes:\n                print(\' -\', predict_rnn(prefix, pred_len, rnn, params, init_rnn_state,\n                    num_hiddens, vocab_size, device, idx_to_char, char_to_idx))\n\n                \n                \n                \n# ################################### 6.5 ################################################\nclass RNNModel(nn.Module):\n    def __init__(self, rnn_layer, vocab_size):\n        super(RNNModel, self).__init__()\n        self.rnn = rnn_layer\n        self.hidden_size = rnn_layer.hidden_size * (2 if rnn_layer.bidirectional else 1) \n        self.vocab_size = vocab_size\n        self.dense = nn.Linear(self.hidden_size, vocab_size)\n        self.state = None\n\n    def forward(self, inputs, state): # inputs: (batch, seq_len)\n        # \xe8\x8e\xb7\xe5\x8f\x96one-hot\xe5\x90\x91\xe9\x87\x8f\xe8\xa1\xa8\xe7\xa4\xba\n        X = to_onehot(inputs, self.vocab_size) # X\xe6\x98\xaf\xe4\xb8\xaalist\n        Y, self.state = self.rnn(torch.stack(X), state)\n        # \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe4\xbc\x9a\xe9\xa6\x96\xe5\x85\x88\xe5\xb0\x86Y\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\xe5\x8f\x98\xe6\x88\x90(num_steps * batch_size, num_hiddens)\xef\xbc\x8c\xe5\xae\x83\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\n        # \xe5\xbd\xa2\xe7\x8a\xb6\xe4\xb8\xba(num_steps * batch_size, vocab_size)\n        output = self.dense(Y.view(-1, Y.shape[-1]))\n        return output, self.state\n\ndef predict_rnn_pytorch(prefix, num_chars, model, vocab_size, device, idx_to_char,\n                      char_to_idx):\n    state = None\n    output = [char_to_idx[prefix[0]]] # output\xe4\xbc\x9a\xe8\xae\xb0\xe5\xbd\x95prefix\xe5\x8a\xa0\xe4\xb8\x8a\xe8\xbe\x93\xe5\x87\xba\n    for t in range(num_chars + len(prefix) - 1):\n        X = torch.tensor([output[-1]], device=device).view(1, 1)\n        if state is not None:\n            if isinstance(state, tuple): # LSTM, state:(h, c)  \n                state = (state[0].to(device), state[1].to(device))\n            else:   \n                state = state.to(device)\n            \n        (Y, state) = model(X, state)  # \xe5\x89\x8d\xe5\x90\x91\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe4\xbc\xa0\xe5\x85\xa5\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0\n        if t < len(prefix) - 1:\n            output.append(char_to_idx[prefix[t + 1]])\n        else:\n            output.append(int(Y.argmax(dim=1).item()))\n    return \'\'.join([idx_to_char[i] for i in output])\n\ndef train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,\n                                corpus_indices, idx_to_char, char_to_idx,\n                                num_epochs, num_steps, lr, clipping_theta,\n                                batch_size, pred_period, pred_len, prefixes):\n    loss = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    model.to(device)\n    state = None\n    for epoch in range(num_epochs):\n        l_sum, n, start = 0.0, 0, time.time()\n        data_iter = data_iter_consecutive(corpus_indices, batch_size, num_steps, device) # \xe7\x9b\xb8\xe9\x82\xbb\xe9\x87\x87\xe6\xa0\xb7\n        for X, Y in data_iter:\n            if state is not None:\n                # \xe4\xbd\xbf\xe7\x94\xa8detach\xe5\x87\xbd\xe6\x95\xb0\xe4\xbb\x8e\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe5\x88\x86\xe7\xa6\xbb\xe9\x9a\x90\xe8\x97\x8f\xe7\x8a\xb6\xe6\x80\x81, \xe8\xbf\x99\xe6\x98\xaf\xe4\xb8\xba\xe4\xba\x86\n                # \xe4\xbd\xbf\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe8\xae\xa1\xe7\xae\x97\xe5\x8f\xaa\xe4\xbe\x9d\xe8\xb5\x96\xe4\xb8\x80\xe6\xac\xa1\xe8\xbf\xad\xe4\xbb\xa3\xe8\xaf\xbb\xe5\x8f\x96\xe7\x9a\x84\xe5\xb0\x8f\xe6\x89\xb9\xe9\x87\x8f\xe5\xba\x8f\xe5\x88\x97(\xe9\x98\xb2\xe6\xad\xa2\xe6\xa2\xaf\xe5\xba\xa6\xe8\xae\xa1\xe7\xae\x97\xe5\xbc\x80\xe9\x94\x80\xe5\xa4\xaa\xe5\xa4\xa7)\n                if isinstance (state, tuple): # LSTM, state:(h, c)  \n                    state = (state[0].detach(), state[1].detach())\n                else:   \n                    state = state.detach()\n    \n            (output, state) = model(X, state) # output: \xe5\xbd\xa2\xe7\x8a\xb6\xe4\xb8\xba(num_steps * batch_size, vocab_size)\n            \n            # Y\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\xe6\x98\xaf(batch_size, num_steps)\xef\xbc\x8c\xe8\xbd\xac\xe7\xbd\xae\xe5\x90\x8e\xe5\x86\x8d\xe5\x8f\x98\xe6\x88\x90\xe9\x95\xbf\xe5\xba\xa6\xe4\xb8\xba\n            # batch * num_steps \xe7\x9a\x84\xe5\x90\x91\xe9\x87\x8f\xef\xbc\x8c\xe8\xbf\x99\xe6\xa0\xb7\xe8\xb7\x9f\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe8\xa1\x8c\xe4\xb8\x80\xe4\xb8\x80\xe5\xaf\xb9\xe5\xba\x94\n            y = torch.transpose(Y, 0, 1).contiguous().view(-1)\n            l = loss(output, y.long())\n            \n            optimizer.zero_grad()\n            l.backward()\n            # \xe6\xa2\xaf\xe5\xba\xa6\xe8\xa3\x81\xe5\x89\xaa\n            grad_clipping(model.parameters(), clipping_theta, device)\n            optimizer.step()\n            l_sum += l.item() * y.shape[0]\n            n += y.shape[0]\n        \n        try:\n            perplexity = math.exp(l_sum / n)\n        except OverflowError:\n            perplexity = float(\'inf\')\n        if (epoch + 1) % pred_period == 0:\n            print(\'epoch %d, perplexity %f, time %.2f sec\' % (\n                epoch + 1, perplexity, time.time() - start))\n            for prefix in prefixes:\n                print(\' -\', predict_rnn_pytorch(\n                    prefix, pred_len, model, vocab_size, device, idx_to_char,\n                    char_to_idx))\n\n\n\n\n# ######################################## 7.2 ###############################################\ndef train_2d(trainer):  \n    x1, x2, s1, s2 = -5, -2, 0, 0  # s1\xe5\x92\x8cs2\xe6\x98\xaf\xe8\x87\xaa\xe5\x8f\x98\xe9\x87\x8f\xe7\x8a\xb6\xe6\x80\x81\xef\xbc\x8c\xe6\x9c\xac\xe7\xab\xa0\xe5\x90\x8e\xe7\xbb\xad\xe5\x87\xa0\xe8\x8a\x82\xe4\xbc\x9a\xe4\xbd\xbf\xe7\x94\xa8\n    results = [(x1, x2)]\n    for i in range(20):\n        x1, x2, s1, s2 = trainer(x1, x2, s1, s2)\n        results.append((x1, x2))\n    print(\'epoch %d, x1 %f, x2 %f\' % (i + 1, x1, x2))\n    return results\n\ndef show_trace_2d(f, results):  \n    plt.plot(*zip(*results), \'-o\', color=\'#ff7f0e\')\n    x1, x2 = np.meshgrid(np.arange(-5.5, 1.0, 0.1), np.arange(-3.0, 1.0, 0.1))\n    plt.contour(x1, x2, f(x1, x2), colors=\'#1f77b4\')\n    plt.xlabel(\'x1\')\n    plt.ylabel(\'x2\')\n\n\n\n\n# ######################################## 7.3 ###############################################\ndef get_data_ch7():  \n    data = np.genfromtxt(\'../../data/airfoil_self_noise.dat\', delimiter=\'\\t\')\n    data = (data - data.mean(axis=0)) / data.std(axis=0)\n    return torch.tensor(data[:1500, :-1], dtype=torch.float32), \\\n        torch.tensor(data[:1500, -1], dtype=torch.float32) # \xe5\x89\x8d1500\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac(\xe6\xaf\x8f\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac5\xe4\xb8\xaa\xe7\x89\xb9\xe5\xbe\x81)\n\ndef train_ch7(optimizer_fn, states, hyperparams, features, labels,\n              batch_size=10, num_epochs=2):\n    # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\xa8\xa1\xe5\x9e\x8b\n    net, loss = linreg, squared_loss\n    \n    w = torch.nn.Parameter(torch.tensor(np.random.normal(0, 0.01, size=(features.shape[1], 1)), dtype=torch.float32),\n                           requires_grad=True)\n    b = torch.nn.Parameter(torch.zeros(1, dtype=torch.float32), requires_grad=True)\n\n    def eval_loss():\n        return loss(net(features, w, b), labels).mean().item()\n\n    ls = [eval_loss()]\n    data_iter = torch.utils.data.DataLoader(\n        torch.utils.data.TensorDataset(features, labels), batch_size, shuffle=True)\n    \n    for _ in range(num_epochs):\n        start = time.time()\n        for batch_i, (X, y) in enumerate(data_iter):\n            l = loss(net(X, w, b), y).mean()  # \xe4\xbd\xbf\xe7\x94\xa8\xe5\xb9\xb3\xe5\x9d\x87\xe6\x8d\x9f\xe5\xa4\xb1\n            \n            # \xe6\xa2\xaf\xe5\xba\xa6\xe6\xb8\x85\xe9\x9b\xb6\n            if w.grad is not None:\n                w.grad.data.zero_()\n                b.grad.data.zero_()\n                \n            l.backward()\n            optimizer_fn([w, b], states, hyperparams)  # \xe8\xbf\xad\xe4\xbb\xa3\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0\n            if (batch_i + 1) * batch_size % 100 == 0:\n                ls.append(eval_loss())  # \xe6\xaf\x8f100\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\xe8\xae\xb0\xe5\xbd\x95\xe4\xb8\x8b\xe5\xbd\x93\xe5\x89\x8d\xe8\xae\xad\xe7\xbb\x83\xe8\xaf\xaf\xe5\xb7\xae\n    # \xe6\x89\x93\xe5\x8d\xb0\xe7\xbb\x93\xe6\x9e\x9c\xe5\x92\x8c\xe4\xbd\x9c\xe5\x9b\xbe\n    print(\'loss: %f, %f sec per epoch\' % (ls[-1], time.time() - start))\n    set_figsize()\n    plt.plot(np.linspace(0, num_epochs, len(ls)), ls)\n    plt.xlabel(\'epoch\')\n    plt.ylabel(\'loss\')\n\n# \xe6\x9c\xac\xe5\x87\xbd\xe6\x95\xb0\xe4\xb8\x8e\xe5\x8e\x9f\xe4\xb9\xa6\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe6\x98\xaf\xe8\xbf\x99\xe9\x87\x8c\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe5\x87\xbd\xe6\x95\xb0\xe8\x80\x8c\xe4\xb8\x8d\xe6\x98\xaf\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe7\x9a\x84\xe5\x90\x8d\xe5\xad\x97\n# \xe4\xbe\x8b\xe5\xa6\x82: optimizer_fn=torch.optim.SGD, optimizer_hyperparams={""lr"": 0.05}\ndef train_pytorch_ch7(optimizer_fn, optimizer_hyperparams, features, labels,\n                    batch_size=10, num_epochs=2):\n    # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\xa8\xa1\xe5\x9e\x8b\n    net = nn.Sequential(\n        nn.Linear(features.shape[-1], 1)\n    )\n    loss = nn.MSELoss()\n    optimizer = optimizer_fn(net.parameters(), **optimizer_hyperparams)\n\n    def eval_loss():\n        return loss(net(features).view(-1), labels).item() / 2\n\n    ls = [eval_loss()]\n    data_iter = torch.utils.data.DataLoader(\n        torch.utils.data.TensorDataset(features, labels), batch_size, shuffle=True)\n\n    for _ in range(num_epochs):\n        start = time.time()\n        for batch_i, (X, y) in enumerate(data_iter):\n            # \xe9\x99\xa4\xe4\xbb\xa52\xe6\x98\xaf\xe4\xb8\xba\xe4\xba\x86\xe5\x92\x8ctrain_ch7\xe4\xbf\x9d\xe6\x8c\x81\xe4\xb8\x80\xe8\x87\xb4, \xe5\x9b\xa0\xe4\xb8\xbasquared_loss\xe4\xb8\xad\xe9\x99\xa4\xe4\xba\x862\n            l = loss(net(X).view(-1), y) / 2 \n            \n            optimizer.zero_grad()\n            l.backward()\n            optimizer.step()\n            if (batch_i + 1) * batch_size % 100 == 0:\n                ls.append(eval_loss())\n    # \xe6\x89\x93\xe5\x8d\xb0\xe7\xbb\x93\xe6\x9e\x9c\xe5\x92\x8c\xe4\xbd\x9c\xe5\x9b\xbe\n    print(\'loss: %f, %f sec per epoch\' % (ls[-1], time.time() - start))\n    set_figsize()\n    plt.plot(np.linspace(0, num_epochs, len(ls)), ls)\n    plt.xlabel(\'epoch\')\n    plt.ylabel(\'loss\')\n\n\n\n\n############################## 8.3 ##################################\nclass Benchmark():\n    def __init__(self, prefix=None):\n        self.prefix = prefix + \' \' if prefix else \'\'\n\n    def __enter__(self):\n        self.start = time.time()\n\n    def __exit__(self, *args):\n        print(\'%stime: %.4f sec\' % (self.prefix, time.time() - self.start))\n\n\n\n\n\n# ########################### 9.1 ########################################\ndef show_images(imgs, num_rows, num_cols, scale=2):\n    figsize = (num_cols * scale, num_rows * scale)\n    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n    for i in range(num_rows):\n        for j in range(num_cols):\n            axes[i][j].imshow(imgs[i * num_cols + j])\n            axes[i][j].axes.get_xaxis().set_visible(False)\n            axes[i][j].axes.get_yaxis().set_visible(False)\n    return axes\n\ndef train(train_iter, test_iter, net, loss, optimizer, device, num_epochs):\n    net = net.to(device)\n    print(""training on "", device)\n    batch_count = 0\n    for epoch in range(num_epochs):\n        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n        for X, y in train_iter:\n            X = X.to(device)\n            y = y.to(device)\n            y_hat = net(X)\n            l = loss(y_hat, y) \n            optimizer.zero_grad()\n            l.backward()\n            optimizer.step()\n            train_l_sum += l.cpu().item()\n            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n            n += y.shape[0]\n            batch_count += 1\n        test_acc = evaluate_accuracy(test_iter, net)\n        print(\'epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec\'\n              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))\n\n\n\n\n############################## 9.3 #####################\ndef bbox_to_rect(bbox, color):\n    # \xe5\xb0\x86\xe8\xbe\xb9\xe7\x95\x8c\xe6\xa1\x86(\xe5\xb7\xa6\xe4\xb8\x8ax, \xe5\xb7\xa6\xe4\xb8\x8ay, \xe5\x8f\xb3\xe4\xb8\x8bx, \xe5\x8f\xb3\xe4\xb8\x8by)\xe6\xa0\xbc\xe5\xbc\x8f\xe8\xbd\xac\xe6\x8d\xa2\xe6\x88\x90matplotlib\xe6\xa0\xbc\xe5\xbc\x8f\xef\xbc\x9a\n    # ((\xe5\xb7\xa6\xe4\xb8\x8ax, \xe5\xb7\xa6\xe4\xb8\x8ay), \xe5\xae\xbd, \xe9\xab\x98)\n    return plt.Rectangle(\n        xy=(bbox[0], bbox[1]), width=bbox[2]-bbox[0], height=bbox[3]-bbox[1],\n        fill=False, edgecolor=color, linewidth=2)\n\n\n\n\n############################ 9.4 ###########################\ndef MultiBoxPrior(feature_map, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5]):\n    """"""\n    # \xe6\x8c\x89\xe7\x85\xa7\xe3\x80\x8c9.4.1. \xe7\x94\x9f\xe6\x88\x90\xe5\xa4\x9a\xe4\xb8\xaa\xe9\x94\x9a\xe6\xa1\x86\xe3\x80\x8d\xe6\x89\x80\xe8\xae\xb2\xe7\x9a\x84\xe5\xae\x9e\xe7\x8e\xb0, anchor\xe8\xa1\xa8\xe7\xa4\xba\xe6\x88\x90(xmin, ymin, xmax, ymax).\n    https://zh.d2l.ai/chapter_computer-vision/anchor.html\n    Args:\n        feature_map: torch tensor, Shape: [N, C, H, W].\n        sizes: List of sizes (0~1) of generated MultiBoxPriores. \n        ratios: List of aspect ratios (non-negative) of generated MultiBoxPriores. \n    Returns:\n        anchors of shape (1, num_anchors, 4). \xe7\x94\xb1\xe4\xba\x8ebatch\xe9\x87\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe9\x83\xbd\xe4\xb8\x80\xe6\xa0\xb7, \xe6\x89\x80\xe4\xbb\xa5\xe7\xac\xac\xe4\xb8\x80\xe7\xbb\xb4\xe4\xb8\xba1\n    """"""\n    pairs = [] # pair of (size, sqrt(ration))\n    for r in ratios:\n        pairs.append([sizes[0], math.sqrt(r)])\n    for s in sizes[1:]:\n        pairs.append([s, math.sqrt(ratios[0])])\n    \n    pairs = np.array(pairs)\n    \n    ss1 = pairs[:, 0] * pairs[:, 1] # size * sqrt(ration)\n    ss2 = pairs[:, 0] / pairs[:, 1] # size / sqrt(ration)\n    \n    base_anchors = np.stack([-ss1, -ss2, ss1, ss2], axis=1) / 2\n    \n    h, w = feature_map.shape[-2:]\n    shifts_x = np.arange(0, w) / w\n    shifts_y = np.arange(0, h) / h\n    shift_x, shift_y = np.meshgrid(shifts_x, shifts_y)\n    shift_x = shift_x.reshape(-1)\n    shift_y = shift_y.reshape(-1)\n    shifts = np.stack((shift_x, shift_y, shift_x, shift_y), axis=1)\n    \n    anchors = shifts.reshape((-1, 1, 4)) + base_anchors.reshape((1, -1, 4))\n    \n    return torch.tensor(anchors, dtype=torch.float32).view(1, -1, 4)\n\ndef show_bboxes(axes, bboxes, labels=None, colors=None):\n    def _make_list(obj, default_values=None):\n        if obj is None:\n            obj = default_values\n        elif not isinstance(obj, (list, tuple)):\n            obj = [obj]\n        return obj\n\n    labels = _make_list(labels)\n    colors = _make_list(colors, [\'b\', \'g\', \'r\', \'m\', \'c\'])\n    for i, bbox in enumerate(bboxes):\n        color = colors[i % len(colors)]\n        rect = bbox_to_rect(bbox.detach().cpu().numpy(), color)\n        axes.add_patch(rect)\n        if labels and len(labels) > i:\n            text_color = \'k\' if color == \'w\' else \'w\'\n            axes.text(rect.xy[0], rect.xy[1], labels[i],\n                      va=\'center\', ha=\'center\', fontsize=6, color=text_color,\n                      bbox=dict(facecolor=color, lw=0))\n\ndef compute_intersection(set_1, set_2):\n    """"""\n    \xe8\xae\xa1\xe7\xae\x97anchor\xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84\xe4\xba\xa4\xe9\x9b\x86\n    Args:\n        set_1: a tensor of dimensions (n1, 4), anchor\xe8\xa1\xa8\xe7\xa4\xba\xe6\x88\x90(xmin, ymin, xmax, ymax)\n        set_2: a tensor of dimensions (n2, 4), anchor\xe8\xa1\xa8\xe7\xa4\xba\xe6\x88\x90(xmin, ymin, xmax, ymax)\n    Returns:\n        intersection of each of the boxes in set 1 with respect to each of the boxes in set 2, shape: (n1, n2)\n    """"""\n    # PyTorch auto-broadcasts singleton dimensions\n    lower_bounds = torch.max(set_1[:, :2].unsqueeze(1), set_2[:, :2].unsqueeze(0))  # (n1, n2, 2)\n    upper_bounds = torch.min(set_1[:, 2:].unsqueeze(1), set_2[:, 2:].unsqueeze(0))  # (n1, n2, 2)\n    intersection_dims = torch.clamp(upper_bounds - lower_bounds, min=0)  # (n1, n2, 2)\n    return intersection_dims[:, :, 0] * intersection_dims[:, :, 1]  # (n1, n2)\n\ndef compute_jaccard(set_1, set_2):\n    """"""\n    \xe8\xae\xa1\xe7\xae\x97anchor\xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84Jaccard\xe7\xb3\xbb\xe6\x95\xb0(IoU)\n    Args:\n        set_1: a tensor of dimensions (n1, 4), anchor\xe8\xa1\xa8\xe7\xa4\xba\xe6\x88\x90(xmin, ymin, xmax, ymax)\n        set_2: a tensor of dimensions (n2, 4), anchor\xe8\xa1\xa8\xe7\xa4\xba\xe6\x88\x90(xmin, ymin, xmax, ymax)\n    Returns:\n        Jaccard Overlap of each of the boxes in set 1 with respect to each of the boxes in set 2, shape: (n1, n2)\n    """"""\n    # Find intersections\n    intersection = compute_intersection(set_1, set_2)  # (n1, n2)\n\n    # Find areas of each box in both sets\n    areas_set_1 = (set_1[:, 2] - set_1[:, 0]) * (set_1[:, 3] - set_1[:, 1])  # (n1)\n    areas_set_2 = (set_2[:, 2] - set_2[:, 0]) * (set_2[:, 3] - set_2[:, 1])  # (n2)\n\n    # Find the union\n    # PyTorch auto-broadcasts singleton dimensions\n    union = areas_set_1.unsqueeze(1) + areas_set_2.unsqueeze(0) - intersection  # (n1, n2)\n\n    return intersection / union  # (n1, n2)\n\ndef assign_anchor(bb, anchor, jaccard_threshold=0.5):\n    """"""\n    # \xe6\x8c\x89\xe7\x85\xa7\xe3\x80\x8c9.4.1. \xe7\x94\x9f\xe6\x88\x90\xe5\xa4\x9a\xe4\xb8\xaa\xe9\x94\x9a\xe6\xa1\x86\xe3\x80\x8d\xe5\x9b\xbe9.3\xe6\x89\x80\xe8\xae\xb2\xe4\xb8\xba\xe6\xaf\x8f\xe4\xb8\xaaanchor\xe5\x88\x86\xe9\x85\x8d\xe7\x9c\x9f\xe5\xae\x9e\xe7\x9a\x84bb, anchor\xe8\xa1\xa8\xe7\xa4\xba\xe6\x88\x90\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96(xmin, ymin, xmax, ymax).\n    https://zh.d2l.ai/chapter_computer-vision/anchor.html\n    Args:\n        bb: \xe7\x9c\x9f\xe5\xae\x9e\xe8\xbe\xb9\xe7\x95\x8c\xe6\xa1\x86(bounding box), shape:\xef\xbc\x88nb, 4\xef\xbc\x89\n        anchor: \xe5\xbe\x85\xe5\x88\x86\xe9\x85\x8d\xe7\x9a\x84anchor, shape:\xef\xbc\x88na, 4\xef\xbc\x89\n        jaccard_threshold: \xe9\xa2\x84\xe5\x85\x88\xe8\xae\xbe\xe5\xae\x9a\xe7\x9a\x84\xe9\x98\x88\xe5\x80\xbc\n    Returns:\n        assigned_idx: shape: (na, ), \xe6\xaf\x8f\xe4\xb8\xaaanchor\xe5\x88\x86\xe9\x85\x8d\xe7\x9a\x84\xe7\x9c\x9f\xe5\xae\x9ebb\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe7\xb4\xa2\xe5\xbc\x95, \xe8\x8b\xa5\xe6\x9c\xaa\xe5\x88\x86\xe9\x85\x8d\xe4\xbb\xbb\xe4\xbd\x95bb\xe5\x88\x99\xe4\xb8\xba-1\n    """"""\n    na = anchor.shape[0]\n    nb = bb.shape[0]\n    jaccard = compute_jaccard(anchor, bb).detach().cpu().numpy() # shape: (na, nb)\n    assigned_idx = np.ones(na) * -1  # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x85\xa8\xe4\xb8\xba-1\n    \n    # \xe5\x85\x88\xe4\xb8\xba\xe6\xaf\x8f\xe4\xb8\xaabb\xe5\x88\x86\xe9\x85\x8d\xe4\xb8\x80\xe4\xb8\xaaanchor(\xe4\xb8\x8d\xe8\xa6\x81\xe6\xb1\x82\xe6\xbb\xa1\xe8\xb6\xb3jaccard_threshold)\n    jaccard_cp = jaccard.copy()\n    for j in range(nb):\n        i = np.argmax(jaccard_cp[:, j])\n        assigned_idx[i] = j\n        jaccard_cp[i, :] = float(""-inf"") # \xe8\xb5\x8b\xe5\x80\xbc\xe4\xb8\xba\xe8\xb4\x9f\xe6\x97\xa0\xe7\xa9\xb7, \xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8e\xe5\x8e\xbb\xe6\x8e\x89\xe8\xbf\x99\xe4\xb8\x80\xe8\xa1\x8c\n     \n    # \xe5\xa4\x84\xe7\x90\x86\xe8\xbf\x98\xe6\x9c\xaa\xe8\xa2\xab\xe5\x88\x86\xe9\x85\x8d\xe7\x9a\x84anchor, \xe8\xa6\x81\xe6\xb1\x82\xe6\xbb\xa1\xe8\xb6\xb3jaccard_threshold\n    for i in range(na):\n        if assigned_idx[i] == -1:\n            j = np.argmax(jaccard[i, :])\n            if jaccard[i, j] >= jaccard_threshold:\n                assigned_idx[i] = j\n    \n    return torch.tensor(assigned_idx, dtype=torch.long)\n\ndef xy_to_cxcy(xy):\n    """"""\n    \xe5\xb0\x86(x_min, y_min, x_max, y_max)\xe5\xbd\xa2\xe5\xbc\x8f\xe7\x9a\x84anchor\xe8\xbd\xac\xe6\x8d\xa2\xe6\x88\x90(center_x, center_y, w, h)\xe5\xbd\xa2\xe5\xbc\x8f\xe7\x9a\x84.\n    https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py\n    Args:\n        xy: bounding boxes in boundary coordinates, a tensor of size (n_boxes, 4)\n    Returns: \n        bounding boxes in center-size coordinates, a tensor of size (n_boxes, 4)\n    """"""\n    return torch.cat([(xy[:, 2:] + xy[:, :2]) / 2,  # c_x, c_y\n                      xy[:, 2:] - xy[:, :2]], 1)  # w, h\n\ndef MultiBoxTarget(anchor, label):\n    """"""\n    # \xe6\x8c\x89\xe7\x85\xa7\xe3\x80\x8c9.4.1. \xe7\x94\x9f\xe6\x88\x90\xe5\xa4\x9a\xe4\xb8\xaa\xe9\x94\x9a\xe6\xa1\x86\xe3\x80\x8d\xe6\x89\x80\xe8\xae\xb2\xe7\x9a\x84\xe5\xae\x9e\xe7\x8e\xb0, anchor\xe8\xa1\xa8\xe7\xa4\xba\xe6\x88\x90\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96(xmin, ymin, xmax, ymax).\n    https://zh.d2l.ai/chapter_computer-vision/anchor.html\n    Args:\n        anchor: torch tensor, \xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe9\x94\x9a\xe6\xa1\x86, \xe4\xb8\x80\xe8\x88\xac\xe6\x98\xaf\xe9\x80\x9a\xe8\xbf\x87MultiBoxPrior\xe7\x94\x9f\xe6\x88\x90, shape:\xef\xbc\x881\xef\xbc\x8c\xe9\x94\x9a\xe6\xa1\x86\xe6\x80\xbb\xe6\x95\xb0\xef\xbc\x8c4\xef\xbc\x89\n        label: \xe7\x9c\x9f\xe5\xae\x9e\xe6\xa0\x87\xe7\xad\xbe, shape\xe4\xb8\xba(bn, \xe6\xaf\x8f\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe6\x9c\x80\xe5\xa4\x9a\xe7\x9a\x84\xe7\x9c\x9f\xe5\xae\x9e\xe9\x94\x9a\xe6\xa1\x86\xe6\x95\xb0, 5)\n               \xe7\xac\xac\xe4\xba\x8c\xe7\xbb\xb4\xe4\xb8\xad\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe7\xbb\x99\xe5\xae\x9a\xe5\x9b\xbe\xe7\x89\x87\xe6\xb2\xa1\xe6\x9c\x89\xe8\xbf\x99\xe4\xb9\x88\xe5\xa4\x9a\xe9\x94\x9a\xe6\xa1\x86, \xe5\x8f\xaf\xe4\xbb\xa5\xe5\x85\x88\xe7\x94\xa8-1\xe5\xa1\xab\xe5\x85\x85\xe7\xa9\xba\xe7\x99\xbd, \xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe7\xbb\xb4\xe4\xb8\xad\xe7\x9a\x84\xe5\x85\x83\xe7\xb4\xa0\xe4\xb8\xba[\xe7\xb1\xbb\xe5\x88\xab\xe6\xa0\x87\xe7\xad\xbe, \xe5\x9b\x9b\xe4\xb8\xaa\xe5\x9d\x90\xe6\xa0\x87\xe5\x80\xbc]\n    Returns:\n        \xe5\x88\x97\xe8\xa1\xa8, [bbox_offset, bbox_mask, cls_labels]\n        bbox_offset: \xe6\xaf\x8f\xe4\xb8\xaa\xe9\x94\x9a\xe6\xa1\x86\xe7\x9a\x84\xe6\xa0\x87\xe6\xb3\xa8\xe5\x81\x8f\xe7\xa7\xbb\xe9\x87\x8f\xef\xbc\x8c\xe5\xbd\xa2\xe7\x8a\xb6\xe4\xb8\xba(bn\xef\xbc\x8c\xe9\x94\x9a\xe6\xa1\x86\xe6\x80\xbb\xe6\x95\xb0*4)\n        bbox_mask: \xe5\xbd\xa2\xe7\x8a\xb6\xe5\x90\x8cbbox_offset, \xe6\xaf\x8f\xe4\xb8\xaa\xe9\x94\x9a\xe6\xa1\x86\xe7\x9a\x84\xe6\x8e\xa9\xe7\xa0\x81, \xe4\xb8\x80\xe4\xb8\x80\xe5\xaf\xb9\xe5\xba\x94\xe4\xb8\x8a\xe9\x9d\xa2\xe7\x9a\x84\xe5\x81\x8f\xe7\xa7\xbb\xe9\x87\x8f, \xe8\xb4\x9f\xe7\xb1\xbb\xe9\x94\x9a\xe6\xa1\x86(\xe8\x83\x8c\xe6\x99\xaf)\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe6\x8e\xa9\xe7\xa0\x81\xe5\x9d\x87\xe4\xb8\xba0, \xe6\xad\xa3\xe7\xb1\xbb\xe9\x94\x9a\xe6\xa1\x86\xe7\x9a\x84\xe6\x8e\xa9\xe7\xa0\x81\xe5\x9d\x87\xe4\xb8\xba1\n        cls_labels: \xe6\xaf\x8f\xe4\xb8\xaa\xe9\x94\x9a\xe6\xa1\x86\xe7\x9a\x84\xe6\xa0\x87\xe6\xb3\xa8\xe7\xb1\xbb\xe5\x88\xab, \xe5\x85\xb6\xe4\xb8\xad0\xe8\xa1\xa8\xe7\xa4\xba\xe4\xb8\xba\xe8\x83\x8c\xe6\x99\xaf, \xe5\xbd\xa2\xe7\x8a\xb6\xe4\xb8\xba(bn\xef\xbc\x8c\xe9\x94\x9a\xe6\xa1\x86\xe6\x80\xbb\xe6\x95\xb0)\n    """"""\n    assert len(anchor.shape) == 3 and len(label.shape) == 3\n    bn = label.shape[0]\n    \n    def MultiBoxTarget_one(anc, lab, eps=1e-6):\n        """"""\n        MultiBoxTarget\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe8\xbe\x85\xe5\x8a\xa9\xe5\x87\xbd\xe6\x95\xb0, \xe5\xa4\x84\xe7\x90\x86batch\xe4\xb8\xad\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\n        Args:\n            anc: shape of (\xe9\x94\x9a\xe6\xa1\x86\xe6\x80\xbb\xe6\x95\xb0, 4)\n            lab: shape of (\xe7\x9c\x9f\xe5\xae\x9e\xe9\x94\x9a\xe6\xa1\x86\xe6\x95\xb0, 5), 5\xe4\xbb\xa3\xe8\xa1\xa8[\xe7\xb1\xbb\xe5\x88\xab\xe6\xa0\x87\xe7\xad\xbe, \xe5\x9b\x9b\xe4\xb8\xaa\xe5\x9d\x90\xe6\xa0\x87\xe5\x80\xbc]\n            eps: \xe4\xb8\x80\xe4\xb8\xaa\xe6\x9e\x81\xe5\xb0\x8f\xe5\x80\xbc, \xe9\x98\xb2\xe6\xad\xa2log0\n        Returns:\n            offset: (\xe9\x94\x9a\xe6\xa1\x86\xe6\x80\xbb\xe6\x95\xb0*4, )\n            bbox_mask: (\xe9\x94\x9a\xe6\xa1\x86\xe6\x80\xbb\xe6\x95\xb0*4, ), 0\xe4\xbb\xa3\xe8\xa1\xa8\xe8\x83\x8c\xe6\x99\xaf, 1\xe4\xbb\xa3\xe8\xa1\xa8\xe9\x9d\x9e\xe8\x83\x8c\xe6\x99\xaf\n            cls_labels: (\xe9\x94\x9a\xe6\xa1\x86\xe6\x80\xbb\xe6\x95\xb0, 4), 0\xe4\xbb\xa3\xe8\xa1\xa8\xe8\x83\x8c\xe6\x99\xaf\n        """"""\n        an = anc.shape[0]\n        assigned_idx = assign_anchor(lab[:, 1:], anc) # (\xe9\x94\x9a\xe6\xa1\x86\xe6\x80\xbb\xe6\x95\xb0, )\n        bbox_mask = ((assigned_idx >= 0).float().unsqueeze(-1)).repeat(1, 4) # (\xe9\x94\x9a\xe6\xa1\x86\xe6\x80\xbb\xe6\x95\xb0, 4)\n\n        cls_labels = torch.zeros(an, dtype=torch.long) # 0\xe8\xa1\xa8\xe7\xa4\xba\xe8\x83\x8c\xe6\x99\xaf\n        assigned_bb = torch.zeros((an, 4), dtype=torch.float32) # \xe6\x89\x80\xe6\x9c\x89anchor\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84bb\xe5\x9d\x90\xe6\xa0\x87\n        for i in range(an):\n            bb_idx = assigned_idx[i]\n            if bb_idx >= 0: # \xe5\x8d\xb3\xe9\x9d\x9e\xe8\x83\x8c\xe6\x99\xaf\n                cls_labels[i] = lab[bb_idx, 0].long().item() + 1 # \xe6\xb3\xa8\xe6\x84\x8f\xe8\xa6\x81\xe5\x8a\xa0\xe4\xb8\x80\n                assigned_bb[i, :] = lab[bb_idx, 1:]\n\n        center_anc = xy_to_cxcy(anc) # (center_x, center_y, w, h)\n        center_assigned_bb = xy_to_cxcy(assigned_bb)\n\n        offset_xy = 10.0 * (center_assigned_bb[:, :2] - center_anc[:, :2]) / center_anc[:, 2:]\n        offset_wh = 5.0 * torch.log(eps + center_assigned_bb[:, 2:] / center_anc[:, 2:])\n        offset = torch.cat([offset_xy, offset_wh], dim = 1) * bbox_mask # (\xe9\x94\x9a\xe6\xa1\x86\xe6\x80\xbb\xe6\x95\xb0, 4)\n\n        return offset.view(-1), bbox_mask.view(-1), cls_labels\n    \n    batch_offset = []\n    batch_mask = []\n    batch_cls_labels = []\n    for b in range(bn):\n        offset, bbox_mask, cls_labels = MultiBoxTarget_one(anchor[0, :, :], label[b, :, :])\n        \n        batch_offset.append(offset)\n        batch_mask.append(bbox_mask)\n        batch_cls_labels.append(cls_labels)\n    \n    bbox_offset = torch.stack(batch_offset)\n    bbox_mask = torch.stack(batch_mask)\n    cls_labels = torch.stack(batch_cls_labels)\n    \n    return [bbox_offset, bbox_mask, cls_labels]\n\n\nPred_BB_Info = namedtuple(""Pred_BB_Info"", [""index"", ""class_id"", ""confidence"", ""xyxy""])\ndef non_max_suppression(bb_info_list, nms_threshold = 0.5):\n    """"""\n    \xe9\x9d\x9e\xe6\x9e\x81\xe5\xa4\xa7\xe6\x8a\x91\xe5\x88\xb6\xe5\xa4\x84\xe7\x90\x86\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe8\xbe\xb9\xe7\x95\x8c\xe6\xa1\x86\n    Args:\n        bb_info_list: Pred_BB_Info\xe7\x9a\x84\xe5\x88\x97\xe8\xa1\xa8, \xe5\x8c\x85\xe5\x90\xab\xe9\xa2\x84\xe6\xb5\x8b\xe7\xb1\xbb\xe5\x88\xab\xe3\x80\x81\xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6\xe7\xad\x89\xe4\xbf\xa1\xe6\x81\xaf\n        nms_threshold: \xe9\x98\x88\xe5\x80\xbc\n    Returns:\n        output: Pred_BB_Info\xe7\x9a\x84\xe5\x88\x97\xe8\xa1\xa8, \xe5\x8f\xaa\xe4\xbf\x9d\xe7\x95\x99\xe8\xbf\x87\xe6\xbb\xa4\xe5\x90\x8e\xe7\x9a\x84\xe8\xbe\xb9\xe7\x95\x8c\xe6\xa1\x86\xe4\xbf\xa1\xe6\x81\xaf\n    """"""\n    output = []\n    # \xe5\x85\x88\xe6\xa0\xb9\xe6\x8d\xae\xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6\xe4\xbb\x8e\xe9\xab\x98\xe5\x88\xb0\xe4\xbd\x8e\xe6\x8e\x92\xe5\xba\x8f\n    sorted_bb_info_list = sorted(bb_info_list, key = lambda x: x.confidence, reverse=True)\n\n    while len(sorted_bb_info_list) != 0:\n        best = sorted_bb_info_list.pop(0)\n        output.append(best)\n        \n        if len(sorted_bb_info_list) == 0:\n            break\n\n        bb_xyxy = []\n        for bb in sorted_bb_info_list:\n            bb_xyxy.append(bb.xyxy)\n        \n        iou = compute_jaccard(torch.tensor([best.xyxy]), \n                              torch.tensor(bb_xyxy))[0] # shape: (len(sorted_bb_info_list), )\n        \n        n = len(sorted_bb_info_list)\n        sorted_bb_info_list = [sorted_bb_info_list[i] for i in range(n) if iou[i] <= nms_threshold]\n    return output\n\ndef MultiBoxDetection(cls_prob, loc_pred, anchor, nms_threshold = 0.5):\n    """"""\n    # \xe6\x8c\x89\xe7\x85\xa7\xe3\x80\x8c9.4.1. \xe7\x94\x9f\xe6\x88\x90\xe5\xa4\x9a\xe4\xb8\xaa\xe9\x94\x9a\xe6\xa1\x86\xe3\x80\x8d\xe6\x89\x80\xe8\xae\xb2\xe7\x9a\x84\xe5\xae\x9e\xe7\x8e\xb0, anchor\xe8\xa1\xa8\xe7\xa4\xba\xe6\x88\x90\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96(xmin, ymin, xmax, ymax).\n    https://zh.d2l.ai/chapter_computer-vision/anchor.html\n    Args:\n        cls_prob: \xe7\xbb\x8f\xe8\xbf\x87softmax\xe5\x90\x8e\xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84\xe5\x90\x84\xe4\xb8\xaa\xe9\x94\x9a\xe6\xa1\x86\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa6\x82\xe7\x8e\x87, shape:(bn, \xe9\xa2\x84\xe6\xb5\x8b\xe6\x80\xbb\xe7\xb1\xbb\xe5\x88\xab\xe6\x95\xb0+1, \xe9\x94\x9a\xe6\xa1\x86\xe4\xb8\xaa\xe6\x95\xb0)\n        loc_pred: \xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe5\x90\x84\xe4\xb8\xaa\xe9\x94\x9a\xe6\xa1\x86\xe7\x9a\x84\xe5\x81\x8f\xe7\xa7\xbb\xe9\x87\x8f, shape:(bn, \xe9\x94\x9a\xe6\xa1\x86\xe4\xb8\xaa\xe6\x95\xb0*4)\n        anchor: MultiBoxPrior\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe9\xbb\x98\xe8\xae\xa4\xe9\x94\x9a\xe6\xa1\x86, shape: (1, \xe9\x94\x9a\xe6\xa1\x86\xe4\xb8\xaa\xe6\x95\xb0, 4)\n        nms_threshold: \xe9\x9d\x9e\xe6\x9e\x81\xe5\xa4\xa7\xe6\x8a\x91\xe5\x88\xb6\xe4\xb8\xad\xe7\x9a\x84\xe9\x98\x88\xe5\x80\xbc\n    Returns:\n        \xe6\x89\x80\xe6\x9c\x89\xe9\x94\x9a\xe6\xa1\x86\xe7\x9a\x84\xe4\xbf\xa1\xe6\x81\xaf, shape: (bn, \xe9\x94\x9a\xe6\xa1\x86\xe4\xb8\xaa\xe6\x95\xb0, 6)\n        \xe6\xaf\x8f\xe4\xb8\xaa\xe9\x94\x9a\xe6\xa1\x86\xe4\xbf\xa1\xe6\x81\xaf\xe7\x94\xb1[class_id, confidence, xmin, ymin, xmax, ymax]\xe8\xa1\xa8\xe7\xa4\xba\n        class_id=-1 \xe8\xa1\xa8\xe7\xa4\xba\xe8\x83\x8c\xe6\x99\xaf\xe6\x88\x96\xe5\x9c\xa8\xe9\x9d\x9e\xe6\x9e\x81\xe5\xa4\xa7\xe5\x80\xbc\xe6\x8a\x91\xe5\x88\xb6\xe4\xb8\xad\xe8\xa2\xab\xe7\xa7\xbb\xe9\x99\xa4\xe4\xba\x86\n    """"""\n    assert len(cls_prob.shape) == 3 and len(loc_pred.shape) == 2 and len(anchor.shape) == 3\n    bn = cls_prob.shape[0]\n    \n    def MultiBoxDetection_one(c_p, l_p, anc, nms_threshold = 0.5):\n        """"""\n        MultiBoxDetection\xe7\x9a\x84\xe8\xbe\x85\xe5\x8a\xa9\xe5\x87\xbd\xe6\x95\xb0, \xe5\xa4\x84\xe7\x90\x86batch\xe4\xb8\xad\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\n        Args:\n            c_p: (\xe9\xa2\x84\xe6\xb5\x8b\xe6\x80\xbb\xe7\xb1\xbb\xe5\x88\xab\xe6\x95\xb0+1, \xe9\x94\x9a\xe6\xa1\x86\xe4\xb8\xaa\xe6\x95\xb0)\n            l_p: (\xe9\x94\x9a\xe6\xa1\x86\xe4\xb8\xaa\xe6\x95\xb0*4, )\n            anc: (\xe9\x94\x9a\xe6\xa1\x86\xe4\xb8\xaa\xe6\x95\xb0, 4)\n            nms_threshold: \xe9\x9d\x9e\xe6\x9e\x81\xe5\xa4\xa7\xe6\x8a\x91\xe5\x88\xb6\xe4\xb8\xad\xe7\x9a\x84\xe9\x98\x88\xe5\x80\xbc\n        Return:\n            output: (\xe9\x94\x9a\xe6\xa1\x86\xe4\xb8\xaa\xe6\x95\xb0, 6)\n        """"""\n        pred_bb_num = c_p.shape[1]\n        anc = (anc + l_p.view(pred_bb_num, 4)).detach().cpu().numpy() # \xe5\x8a\xa0\xe4\xb8\x8a\xe5\x81\x8f\xe7\xa7\xbb\xe9\x87\x8f\n        \n        confidence, class_id = torch.max(c_p, 0)\n        confidence = confidence.detach().cpu().numpy()\n        class_id = class_id.detach().cpu().numpy()\n        \n        pred_bb_info = [Pred_BB_Info(\n                            index = i,\n                            class_id = class_id[i] - 1, # \xe6\xad\xa3\xe7\xb1\xbblabel\xe4\xbb\x8e0\xe5\xbc\x80\xe5\xa7\x8b\n                            confidence = confidence[i],\n                            xyxy=[*anc[i]]) # xyxy\xe6\x98\xaf\xe4\xb8\xaa\xe5\x88\x97\xe8\xa1\xa8\n                        for i in range(pred_bb_num)]\n        \n        # \xe6\xad\xa3\xe7\xb1\xbb\xe7\x9a\x84index\n        obj_bb_idx = [bb.index for bb in non_max_suppression(pred_bb_info, nms_threshold)]\n        \n        output = []\n        for bb in pred_bb_info:\n            output.append([\n                (bb.class_id if bb.index in obj_bb_idx else -1.0),\n                bb.confidence,\n                *bb.xyxy\n            ])\n            \n        return torch.tensor(output) # shape: (\xe9\x94\x9a\xe6\xa1\x86\xe4\xb8\xaa\xe6\x95\xb0, 6)\n    \n    batch_output = []\n    for b in range(bn):\n        batch_output.append(MultiBoxDetection_one(cls_prob[b], loc_pred[b], anchor[0], nms_threshold))\n    \n    return torch.stack(batch_output)\n\n\n\n# ################################# 9.6 ############################\nclass PikachuDetDataset(torch.utils.data.Dataset):\n    """"""\xe7\x9a\xae\xe5\x8d\xa1\xe4\xb8\x98\xe6\xa3\x80\xe6\xb5\x8b\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\xb1\xbb""""""\n    def __init__(self, data_dir, part, image_size=(256, 256)):\n        assert part in [""train"", ""val""]\n        self.image_size = image_size\n        self.image_dir = os.path.join(data_dir, part, ""images"")\n        \n        with open(os.path.join(data_dir, part, ""label.json"")) as f:\n            self.label = json.load(f)\n            \n        self.transform = torchvision.transforms.Compose([\n            # \xe5\xb0\x86 PIL \xe5\x9b\xbe\xe7\x89\x87\xe8\xbd\xac\xe6\x8d\xa2\xe6\x88\x90\xe4\xbd\x8d\xe4\xba\x8e[0.0, 1.0]\xe7\x9a\x84floatTensor, shape (C x H x W)\n            torchvision.transforms.ToTensor()])\n            \n    def __len__(self):\n        return len(self.label)\n    \n    def __getitem__(self, index):\n        image_path = str(index + 1) + "".png""\n        \n        cls = self.label[image_path][""class""]\n        label = np.array([cls] + self.label[image_path][""loc""], \n                         dtype=""float32"")[None, :]\n        \n        PIL_img = Image.open(os.path.join(self.image_dir, image_path)\n                            ).convert(\'RGB\').resize(self.image_size)\n        img = self.transform(PIL_img)\n        \n        sample = {\n            ""label"": label, # shape: (1, 5) [class, xmin, ymin, xmax, ymax]\n            ""image"": img    # shape: (3, *image_size)\n        }\n        \n        return sample\n\ndef load_data_pikachu(batch_size, edge_size=256, data_dir = \'../../data/pikachu\'):  \n    """"""edge_size\xef\xbc\x9a\xe8\xbe\x93\xe5\x87\xba\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe5\xae\xbd\xe5\x92\x8c\xe9\xab\x98""""""\n    image_size = (edge_size, edge_size)\n    train_dataset = PikachuDetDataset(data_dir, \'train\', image_size)\n    val_dataset = PikachuDetDataset(data_dir, \'val\', image_size)\n    \n\n    train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n                                             shuffle=True, num_workers=4)\n\n    val_iter = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,\n                                           shuffle=False, num_workers=4)\n    return train_iter, val_iter\n\n\n# ################################# 9.9 #########################\ndef read_voc_images(root=""../../data/VOCdevkit/VOC2012"", \n                    is_train=True, max_num=None):\n    txt_fname = \'%s/ImageSets/Segmentation/%s\' % (\n        root, \'train.txt\' if is_train else \'val.txt\')\n    with open(txt_fname, \'r\') as f:\n        images = f.read().split()\n    if max_num is not None:\n        images = images[:min(max_num, len(images))]\n    features, labels = [None] * len(images), [None] * len(images)\n    for i, fname in tqdm(enumerate(images)):\n        features[i] = Image.open(\'%s/JPEGImages/%s.jpg\' % (root, fname)).convert(""RGB"")\n        labels[i] = Image.open(\'%s/SegmentationClass/%s.png\' % (root, fname)).convert(""RGB"")\n    return features, labels # PIL image\n\n# colormap2label = torch.zeros(256 ** 3, dtype=torch.uint8)\n# for i, colormap in enumerate(VOC_COLORMAP):\n#     colormap2label[(colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i\ndef voc_label_indices(colormap, colormap2label):\n    """"""\n    convert colormap (PIL image) to colormap2label (uint8 tensor).\n    """"""\n    colormap = np.array(colormap.convert(""RGB"")).astype(\'int32\')\n    idx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256\n           + colormap[:, :, 2])\n    return colormap2label[idx]\n\ndef voc_rand_crop(feature, label, height, width):\n    """"""\n    Random crop feature (PIL image) and label (PIL image).\n    """"""\n    i, j, h, w = torchvision.transforms.RandomCrop.get_params(\n            feature, output_size=(height, width))\n    \n    feature = torchvision.transforms.functional.crop(feature, i, j, h, w)\n    label = torchvision.transforms.functional.crop(label, i, j, h, w)    \n\n    return feature, label\n\nclass VOCSegDataset(torch.utils.data.Dataset):\n    def __init__(self, is_train, crop_size, voc_dir, colormap2label, max_num=None):\n        """"""\n        crop_size: (h, w)\n        """"""\n        self.rgb_mean = np.array([0.485, 0.456, 0.406])\n        self.rgb_std = np.array([0.229, 0.224, 0.225])\n        self.tsf = torchvision.transforms.Compose([\n            torchvision.transforms.ToTensor(),\n            torchvision.transforms.Normalize(mean=self.rgb_mean, \n                                             std=self.rgb_std)\n        ])\n        \n        self.crop_size = crop_size # (h, w)\n        features, labels = read_voc_images(root=voc_dir, \n                                           is_train=is_train, \n                                           max_num=max_num)\n        self.features = self.filter(features) # PIL image\n        self.labels = self.filter(labels)     # PIL image\n        self.colormap2label = colormap2label\n        print(\'read \' + str(len(self.features)) + \' valid examples\')\n\n    def filter(self, imgs):\n        return [img for img in imgs if (\n            img.size[1] >= self.crop_size[0] and\n            img.size[0] >= self.crop_size[1])]\n\n    def __getitem__(self, idx):\n        feature, label = voc_rand_crop(self.features[idx], self.labels[idx],\n                                       *self.crop_size)\n        \n        return (self.tsf(feature),\n                voc_label_indices(label, self.colormap2label))\n\n    def __len__(self):\n        return len(self.features)\n\n\n\n# ############################# 10.7 ##########################\ndef read_imdb(folder=\'train\', data_root=""/S1/CSCL/tangss/Datasets/aclImdb""): \n    data = []\n    for label in [\'pos\', \'neg\']:\n        folder_name = os.path.join(data_root, folder, label)\n        for file in tqdm(os.listdir(folder_name)):\n            with open(os.path.join(folder_name, file), \'rb\') as f:\n                review = f.read().decode(\'utf-8\').replace(\'\\n\', \'\').lower()\n                data.append([review, 1 if label == \'pos\' else 0])\n    random.shuffle(data)\n    return data\n\ndef get_tokenized_imdb(data):\n    """"""\n    data: list of [string, label]\n    """"""\n    def tokenizer(text):\n        return [tok.lower() for tok in text.split(\' \')]\n    return [tokenizer(review) for review, _ in data]\n\ndef get_vocab_imdb(data):\n    tokenized_data = get_tokenized_imdb(data)\n    counter = collections.Counter([tk for st in tokenized_data for tk in st])\n    return torchtext.vocab.Vocab(counter, min_freq=5)\n\ndef preprocess_imdb(data, vocab):\n    max_l = 500  # \xe5\xb0\x86\xe6\xaf\x8f\xe6\x9d\xa1\xe8\xaf\x84\xe8\xae\xba\xe9\x80\x9a\xe8\xbf\x87\xe6\x88\xaa\xe6\x96\xad\xe6\x88\x96\xe8\x80\x85\xe8\xa1\xa50\xef\xbc\x8c\xe4\xbd\xbf\xe5\xbe\x97\xe9\x95\xbf\xe5\xba\xa6\xe5\x8f\x98\xe6\x88\x90500\n\n    def pad(x):\n        return x[:max_l] if len(x) > max_l else x + [0] * (max_l - len(x))\n\n    tokenized_data = get_tokenized_imdb(data)\n    features = torch.tensor([pad([vocab.stoi[word] for word in words]) for words in tokenized_data])\n    labels = torch.tensor([score for _, score in data])\n    return features, labels\n\ndef load_pretrained_embedding(words, pretrained_vocab):\n    """"""\xe4\xbb\x8e\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84vocab\xe4\xb8\xad\xe6\x8f\x90\xe5\x8f\x96\xe5\x87\xbawords\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f""""""\n    embed = torch.zeros(len(words), pretrained_vocab.vectors[0].shape[0]) # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe4\xb8\xba0\n    oov_count = 0 # out of vocabulary\n    for i, word in enumerate(words):\n        try:\n            idx = pretrained_vocab.stoi[word]\n            embed[i, :] = pretrained_vocab.vectors[idx]\n        except KeyError:\n            oov_count += 1\n    if oov_count > 0:\n        print(""There are %d oov words."" % oov_count)\n    return embed\n\ndef predict_sentiment(net, vocab, sentence):\n    """"""sentence\xe6\x98\xaf\xe8\xaf\x8d\xe8\xaf\xad\xe7\x9a\x84\xe5\x88\x97\xe8\xa1\xa8""""""\n    device = list(net.parameters())[0].device\n    sentence = torch.tensor([vocab.stoi[word] for word in sentence], device=device)\n    label = torch.argmax(net(sentence.view((1, -1))), dim=1)\n    return \'positive\' if label.item() == 1 else \'negative\''"
