file_path,api_count,code
mtcnn_test.py,0,"b'import cv2\nfrom mtcnn.core.detect import create_mtcnn_net, MtcnnDetector\nfrom mtcnn.core.vision import vis_face\n\n\n\n\nif __name__ == \'__main__\':\n\n    pnet, rnet, onet = create_mtcnn_net(p_model_path=""./original_model/pnet_epoch.pt"", r_model_path=""./original_model/rnet_epoch.pt"", o_model_path=""./original_model/onet_epoch.pt"", use_cuda=False)\n    mtcnn_detector = MtcnnDetector(pnet=pnet, rnet=rnet, onet=onet, min_face_size=24)\n\n    img = cv2.imread(""./s_l.jpg"")\n    img_bg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    #b, g, r = cv2.split(img)\n    #img2 = cv2.merge([r, g, b])\n\n    bboxs, landmarks = mtcnn_detector.detect_face(img)\n    # print box_align\n    save_name = \'r_4.jpg\'\n    vis_face(img_bg,bboxs,landmarks, save_name)\n'"
anno_store/__init__.py,0,b''
log/__init__.py,0,b''
model_store/__init__.py,0,b''
mtcnn/__init__.py,0,b''
mtcnn/config.py,0,"b'import os\n\n\nMODEL_STORE_DIR = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))+""/model_store""\n\n\nANNO_STORE_DIR = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))+""/anno_store""\n\n\nLOG_DIR = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))+""/log""\n\n\nUSE_CUDA = True\n\n\nTRAIN_BATCH_SIZE = 512\n\nTRAIN_LR = 0.01\n\nEND_EPOCH = 10\n\n\nPNET_POSTIVE_ANNO_FILENAME = ""pos_12.txt""\nPNET_NEGATIVE_ANNO_FILENAME = ""neg_12.txt""\nPNET_PART_ANNO_FILENAME = ""part_12.txt""\nPNET_LANDMARK_ANNO_FILENAME = ""landmark_12.txt""\n\n\nRNET_POSTIVE_ANNO_FILENAME = ""pos_24.txt""\nRNET_NEGATIVE_ANNO_FILENAME = ""neg_24.txt""\nRNET_PART_ANNO_FILENAME = ""part_24.txt""\nRNET_LANDMARK_ANNO_FILENAME = ""landmark_24.txt""\n\n\nONET_POSTIVE_ANNO_FILENAME = ""pos_48.txt""\nONET_NEGATIVE_ANNO_FILENAME = ""neg_48.txt""\nONET_PART_ANNO_FILENAME = ""part_48.txt""\nONET_LANDMARK_ANNO_FILENAME = ""landmark_48.txt""\n\nPNET_TRAIN_IMGLIST_FILENAME = ""imglist_anno_12.txt""\nRNET_TRAIN_IMGLIST_FILENAME = ""imglist_anno_24.txt""\nONET_TRAIN_IMGLIST_FILENAME = ""imglist_anno_48.txt""'"
anno_store/tool/change.py,0,"b'import numpy as np\n\n""""""\nchange (x1, y1, w, h) to (x1, y1, x2, y2)\n""""""\n\n# original annotations file\nwith open(\'anno_train.txt\') as f:\n\tcontents = f.readlines()\n\ncoordinates = []\nnames = []\nfor content in contents:\n\tname = content.split(\' \')[0]\n\tcoordinate = np.array(content.split(\' \')[1:-1], dtype=np.int32).reshape(-1, 4)\n\tnames.append(name)\n\tcoordinates.append(coordinate)\n\nfor coordinate in coordinates:\n\tcoordinate[:, 2] = coordinate[:, 0] + coordinate[:, 2]\n\tcoordinate[:, 3] = coordinate[:, 1] + coordinate[:, 3]\n\n# modified annotations file\nwith open(\'anno_train_fixed.txt\', \'w\') as f:\n\tfor n, c in zip(names, coordinates):\n\t\ta = str(list(c.reshape(1, -1)[0, :]))[1:-1].split(\',\')\n\t\ts = \'\'\n\t\tfor i in a:\n\t\t\ts = s + i\n\t\tcontent = n + \' \' + s + \'\\n\'\n\n\t\tf.write(content)\n'"
mtcnn/core/__init__.py,0,b''
mtcnn/core/detect.py,10,"b'import cv2\nimport time\nimport numpy as np\nimport torch\nfrom torch.autograd.variable import Variable\nfrom mtcnn.core.models import PNet,RNet,ONet\nimport mtcnn.core.utils as utils\nimport mtcnn.core.image_tools as image_tools\n\n\ndef create_mtcnn_net(p_model_path=None, r_model_path=None, o_model_path=None, use_cuda=True):\n\n    pnet, rnet, onet = None, None, None\n\n    if p_model_path is not None:\n        pnet = PNet(use_cuda=use_cuda)\n        if(use_cuda):\n            print(\'p_model_path:{0}\'.format(p_model_path))\n            pnet.load_state_dict(torch.load(p_model_path))\n            pnet.cuda()\n        else:\n            # forcing all GPU tensors to be in CPU while loading\n            pnet.load_state_dict(torch.load(p_model_path, map_location=lambda storage, loc: storage))\n        pnet.eval()\n\n    if r_model_path is not None:\n        rnet = RNet(use_cuda=use_cuda)\n        if (use_cuda):\n            print(\'r_model_path:{0}\'.format(r_model_path))\n            rnet.load_state_dict(torch.load(r_model_path))\n            rnet.cuda()\n        else:\n            rnet.load_state_dict(torch.load(r_model_path, map_location=lambda storage, loc: storage))\n        rnet.eval()\n\n    if o_model_path is not None:\n        onet = ONet(use_cuda=use_cuda)\n        if (use_cuda):\n            print(\'o_model_path:{0}\'.format(o_model_path))\n            onet.load_state_dict(torch.load(o_model_path))\n            onet.cuda()\n        else:\n            onet.load_state_dict(torch.load(o_model_path, map_location=lambda storage, loc: storage))\n        onet.eval()\n\n    return pnet,rnet,onet\n\n\n\n\nclass MtcnnDetector(object):\n    """"""\n        P,R,O net face detection and landmarks align\n    """"""\n    def  __init__(self,\n                 pnet = None,\n                 rnet = None,\n                 onet = None,\n                 min_face_size=12,\n                 stride=2,\n                 threshold=[0.6, 0.7, 0.7],\n                 scale_factor=0.709,\n                 ):\n\n        self.pnet_detector = pnet\n        self.rnet_detector = rnet\n        self.onet_detector = onet\n        self.min_face_size = min_face_size\n        self.stride=stride\n        self.thresh = threshold\n        self.scale_factor = scale_factor\n\n\n    def unique_image_format(self,im):\n        if not isinstance(im,np.ndarray):\n            if im.mode == \'I\':\n                im = np.array(im, np.int32, copy=False)\n            elif im.mode == \'I;16\':\n                im = np.array(im, np.int16, copy=False)\n            else:\n                im = np.asarray(im)\n        return im\n\n    def square_bbox(self, bbox):\n        """"""\n            convert bbox to square\n        Parameters:\n        ----------\n            bbox: numpy array , shape n x m\n                input bbox\n        Returns:\n        -------\n            a square bbox\n        """"""\n        square_bbox = bbox.copy()\n\n        # x2 - x1\n        # y2 - y1\n        h = bbox[:, 3] - bbox[:, 1] + 1\n        w = bbox[:, 2] - bbox[:, 0] + 1\n        l = np.maximum(h,w)\n        # x1 = x1 + w*0.5 - l*0.5\n        # y1 = y1 + h*0.5 - l*0.5\n        square_bbox[:, 0] = bbox[:, 0] + w*0.5 - l*0.5\n        square_bbox[:, 1] = bbox[:, 1] + h*0.5 - l*0.5\n\n        # x2 = x1 + l - 1\n        # y2 = y1 + l - 1\n        square_bbox[:, 2] = square_bbox[:, 0] + l - 1\n        square_bbox[:, 3] = square_bbox[:, 1] + l - 1\n        return square_bbox\n\n\n    def generate_bounding_box(self, map, reg, scale, threshold):\n        """"""\n            generate bbox from feature map\n        Parameters:\n        ----------\n            map: numpy array , n x m x 1\n                detect score for each position\n            reg: numpy array , n x m x 4\n                bbox\n            scale: float number\n                scale of this detection\n            threshold: float number\n                detect threshold\n        Returns:\n        -------\n            bbox array\n        """"""\n        stride = 2\n        cellsize = 12 # receptive field\n\n        t_index = np.where(map > threshold)\n        # print(\'shape of t_index:{0}\'.format(len(t_index)))\n        # print(\'t_index{0}\'.format(t_index))\n        # time.sleep(5)\n\n        # find nothing\n        if t_index[0].size == 0:\n            return np.array([])\n\n        # reg = (1, n, m, 4)\n        # choose bounding box whose socre are larger than threshold\n        dx1, dy1, dx2, dy2 = [reg[0, t_index[0], t_index[1], i] for i in range(4)]\n        # print(dx1.shape)\n        # time.sleep(5)\n        reg = np.array([dx1, dy1, dx2, dy2])\n        # print(\'shape of reg{0}\'.format(reg.shape))\n\n        # lefteye_dx, lefteye_dy, righteye_dx, righteye_dy, nose_dx, nose_dy, \\\n        # leftmouth_dx, leftmouth_dy, rightmouth_dx, rightmouth_dy = [landmarks[0, t_index[0], t_index[1], i] for i in range(10)]\n        #\n        # landmarks = np.array([lefteye_dx, lefteye_dy, righteye_dx, righteye_dy, nose_dx, nose_dy, leftmouth_dx, leftmouth_dy, rightmouth_dx, rightmouth_dy])\n\n        # abtain score of classification which larger than threshold\n        # t_index[0]: choose the first column of t_index\n        # t_index[1]: choose the second column of t_index\n        score = map[t_index[0], t_index[1], 0]\n\n        # hence t_index[1] means column, t_index[1] is the value of x\n        # hence t_index[0] means row, t_index[0] is the value of y\n        boundingbox = np.vstack([np.round((stride * t_index[1]) / scale),            # x1 of prediction box in original image\n                                 np.round((stride * t_index[0]) / scale),            # y1 of prediction box in original image\n                                 np.round((stride * t_index[1] + cellsize) / scale), # x2 of prediction box in original image\n                                 np.round((stride * t_index[0] + cellsize) / scale), # y2 of prediction box in original image\n                                                                                     # reconstruct the box in original image\n                                 score,\n                                 reg,\n                                 # landmarks\n                                 ])\n\n        return boundingbox.T\n\n\n    def resize_image(self, img, scale):\n        """"""\n            resize image and transform dimention to [batchsize, channel, height, width]\n        Parameters:\n        ----------\n            img: numpy array , height x width x channel\n                input image, channels in BGR order here\n            scale: float number\n                scale factor of resize operation\n        Returns:\n        -------\n            transformed image tensor , 1 x channel x height x width\n        """"""\n        height, width, channels = img.shape\n        new_height = int(height * scale)     # resized new height\n        new_width = int(width * scale)       # resized new width\n        new_dim = (new_width, new_height)\n        img_resized = cv2.resize(img, new_dim, interpolation=cv2.INTER_LINEAR)      # resized image\n        return img_resized\n\n\n    def pad(self, bboxes, w, h):\n        """"""\n            pad the the boxes\n        Parameters:\n        ----------\n            bboxes: numpy array, n x 5\n                input bboxes\n            w: float number\n                width of the input image\n            h: float number\n                height of the input image\n        Returns :\n        ------\n            dy, dx : numpy array, n x 1\n                start point of the bbox in target image\n            edy, edx : numpy array, n x 1\n                end point of the bbox in target image\n            y, x : numpy array, n x 1\n                start point of the bbox in original image\n            ex, ex : numpy array, n x 1\n                end point of the bbox in original image\n            tmph, tmpw: numpy array, n x 1\n                height and width of the bbox\n        """"""\n        # width and height\n        tmpw = (bboxes[:, 2] - bboxes[:, 0] + 1).astype(np.int32)\n        tmph = (bboxes[:, 3] - bboxes[:, 1] + 1).astype(np.int32)\n        numbox = bboxes.shape[0]\n\n        dx = np.zeros((numbox, ))\n        dy = np.zeros((numbox, ))\n        edx, edy  = tmpw.copy()-1, tmph.copy()-1\n        # x, y: start point of the bbox in original image\n        # ex, ey: end point of the bbox in original image\n        x, y, ex, ey = bboxes[:, 0], bboxes[:, 1], bboxes[:, 2], bboxes[:, 3]\n\n        tmp_index = np.where(ex > w-1)\n        edx[tmp_index] = tmpw[tmp_index] + w - 2 - ex[tmp_index]\n        ex[tmp_index] = w - 1\n\n        tmp_index = np.where(ey > h-1)\n        edy[tmp_index] = tmph[tmp_index] + h - 2 - ey[tmp_index]\n        ey[tmp_index] = h - 1\n\n        tmp_index = np.where(x < 0)\n        dx[tmp_index] = 0 - x[tmp_index]\n        x[tmp_index] = 0\n\n        tmp_index = np.where(y < 0)\n        dy[tmp_index] = 0 - y[tmp_index]\n        y[tmp_index] = 0\n\n        return_list = [dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph]\n        return_list = [item.astype(np.int32) for item in return_list]\n\n        return return_list\n\n\n    def detect_pnet(self, im):\n        """"""Get face candidates through pnet\n\n        Parameters:\n        ----------\n        im: numpy array\n            input image array\n            one batch\n\n        Returns:\n        -------\n        boxes: numpy array\n            detected boxes before calibration\n        boxes_align: numpy array\n            boxes after calibration\n        """"""\n\n        # im = self.unique_image_format(im)\n\n        # original wider face data\n        h, w, c = im.shape\n\n        net_size = 12\n\n        current_scale = float(net_size) / self.min_face_size    # find initial scale\n        # print(\'imgshape:{0}, current_scale:{1}\'.format(im.shape, current_scale))\n        im_resized = self.resize_image(im, current_scale) # scale = 1.0\n        current_height, current_width, _ = im_resized.shape\n\n        # fcn\n        all_boxes = list()\n        i = 0\n        while min(current_height, current_width) > net_size:\n            # print(i)\n            feed_imgs = []\n            image_tensor = image_tools.convert_image_to_tensor(im_resized)\n            feed_imgs.append(image_tensor)\n            feed_imgs = torch.stack(feed_imgs)\n            feed_imgs = Variable(feed_imgs)\n\n            if self.pnet_detector.use_cuda:\n                feed_imgs = feed_imgs.cuda()\n\n            # self.pnet_detector is a trained pnet torch model\n\n            # receptive field is 12\xc3\x9712\n            # 12\xc3\x9712 --> score\n            # 12\xc3\x9712 --> bounding box\n            cls_map, reg = self.pnet_detector(feed_imgs)\n\n            cls_map_np = image_tools.convert_chwTensor_to_hwcNumpy(cls_map.cpu())\n            reg_np = image_tools.convert_chwTensor_to_hwcNumpy(reg.cpu())\n            # print(cls_map_np.shape, reg_np.shape) # cls_map_np = (1, n, m, 1) reg_np.shape = (1, n, m 4)\n            # time.sleep(5)\n            # landmark_np = image_tools.convert_chwTensor_to_hwcNumpy(landmark.cpu())\n\n            # self.threshold[0] = 0.6\n            # print(cls_map_np[0,:,:].shape)\n            # time.sleep(4)\n\n            # boxes = [x1, y1, x2, y2, score, reg]\n            boxes = self.generate_bounding_box(cls_map_np[ 0, :, :], reg_np, current_scale, self.thresh[0])\n\n            # generate pyramid images\n            current_scale *= self.scale_factor # self.scale_factor = 0.709\n            im_resized = self.resize_image(im, current_scale)\n            current_height, current_width, _ = im_resized.shape\n\n            if boxes.size == 0:\n                continue\n\n            # non-maximum suppresion\n            keep = utils.nms(boxes[:, :5], 0.5, \'Union\')\n            boxes = boxes[keep]\n            # print(boxes.shape)\n            all_boxes.append(boxes)\n            # i+=1\n\n        if len(all_boxes) == 0:\n            return None, None\n\n        all_boxes = np.vstack(all_boxes)\n        # print(""shape of all boxes {0}"".format(all_boxes.shape))\n        # time.sleep(5)\n\n        # merge the detection from first stage\n        keep = utils.nms(all_boxes[:, 0:5], 0.7, \'Union\')\n        all_boxes = all_boxes[keep]\n        # boxes = all_boxes[:, :5]\n\n        # x2 - x1\n        # y2 - y1\n        bw = all_boxes[:, 2] - all_boxes[:, 0] + 1\n        bh = all_boxes[:, 3] - all_boxes[:, 1] + 1\n\n        # landmark_keep = all_boxes[:, 9:].reshape((5,2))\n\n\n        boxes = np.vstack([all_boxes[:,0],\n                   all_boxes[:,1],\n                   all_boxes[:,2],\n                   all_boxes[:,3],\n                   all_boxes[:,4],\n                   # all_boxes[:, 0] + all_boxes[:, 9] * bw,\n                   # all_boxes[:, 1] + all_boxes[:,10] * bh,\n                   # all_boxes[:, 0] + all_boxes[:, 11] * bw,\n                   # all_boxes[:, 1] + all_boxes[:, 12] * bh,\n                   # all_boxes[:, 0] + all_boxes[:, 13] * bw,\n                   # all_boxes[:, 1] + all_boxes[:, 14] * bh,\n                   # all_boxes[:, 0] + all_boxes[:, 15] * bw,\n                   # all_boxes[:, 1] + all_boxes[:, 16] * bh,\n                   # all_boxes[:, 0] + all_boxes[:, 17] * bw,\n                   # all_boxes[:, 1] + all_boxes[:, 18] * bh\n                  ])\n\n        boxes = boxes.T\n\n        # boxes = boxes = [x1, y1, x2, y2, score, reg] reg= [px1, py1, px2, py2] (in prediction)\n        align_topx = all_boxes[:, 0] + all_boxes[:, 5] * bw\n        align_topy = all_boxes[:, 1] + all_boxes[:, 6] * bh\n        align_bottomx = all_boxes[:, 2] + all_boxes[:, 7] * bw\n        align_bottomy = all_boxes[:, 3] + all_boxes[:, 8] * bh\n\n        # refine the boxes\n        boxes_align = np.vstack([ align_topx,\n                              align_topy,\n                              align_bottomx,\n                              align_bottomy,\n                              all_boxes[:, 4],\n                              # align_topx + all_boxes[:,9] * bw,\n                              # align_topy + all_boxes[:,10] * bh,\n                              # align_topx + all_boxes[:,11] * bw,\n                              # align_topy + all_boxes[:,12] * bh,\n                              # align_topx + all_boxes[:,13] * bw,\n                              # align_topy + all_boxes[:,14] * bh,\n                              # align_topx + all_boxes[:,15] * bw,\n                              # align_topy + all_boxes[:,16] * bh,\n                              # align_topx + all_boxes[:,17] * bw,\n                              # align_topy + all_boxes[:,18] * bh,\n                              ])\n        boxes_align = boxes_align.T\n\n        return boxes, boxes_align\n\n    def detect_rnet(self, im, dets):\n        """"""Get face candidates using rnet\n\n        Parameters:\n        ----------\n        im: numpy array\n            input image array\n        dets: numpy array\n            detection results of pnet\n\n        Returns:\n        -------\n        boxes: numpy array\n            detected boxes before calibration\n        boxes_align: numpy array\n            boxes after calibration\n        """"""\n        # im: an input image\n        h, w, c = im.shape\n\n        if dets is None:\n            return None,None\n\n        # (705, 5) = [x1, y1, x2, y2, score, reg]\n        # print(""pnet detection {0}"".format(dets.shape))\n        # time.sleep(5)\n\n        # return square boxes\n        dets = self.square_bbox(dets)\n        # rounds\n        dets[:, 0:4] = np.round(dets[:, 0:4])\n\n        [dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph] = self.pad(dets, w, h)\n        num_boxes = dets.shape[0]\n\n        \'\'\'\n        # helper for setting RNet batch size\n        batch_size = self.rnet_detector.batch_size\n        ratio = float(num_boxes) / batch_size\n        if ratio > 3 or ratio < 0.3:\n            print ""You may need to reset RNet batch size if this info appears frequently, \\\n        face candidates:%d, current batch_size:%d""%(num_boxes, batch_size)\n        \'\'\'\n\n        # cropped_ims_tensors = np.zeros((num_boxes, 3, 24, 24), dtype=np.float32)\n        cropped_ims_tensors = []\n        for i in range(num_boxes):\n            tmp = np.zeros((tmph[i], tmpw[i], 3), dtype=np.uint8)\n            tmp[dy[i]:edy[i]+1, dx[i]:edx[i]+1, :] = im[y[i]:ey[i]+1, x[i]:ex[i]+1, :]\n            crop_im = cv2.resize(tmp, (24, 24))\n            crop_im_tensor = image_tools.convert_image_to_tensor(crop_im)\n            # cropped_ims_tensors[i, :, :, :] = crop_im_tensor\n            cropped_ims_tensors.append(crop_im_tensor)\n        feed_imgs = Variable(torch.stack(cropped_ims_tensors))\n\n        if self.rnet_detector.use_cuda:\n            feed_imgs = feed_imgs.cuda()\n\n        cls_map, reg = self.rnet_detector(feed_imgs)\n\n        cls_map = cls_map.cpu().data.numpy()\n        reg = reg.cpu().data.numpy()\n        # landmark = landmark.cpu().data.numpy()\n\n\n        keep_inds = np.where(cls_map > self.thresh[1])[0]\n\n        if len(keep_inds) > 0:\n            boxes = dets[keep_inds]\n            cls = cls_map[keep_inds]\n            reg = reg[keep_inds]\n            # landmark = landmark[keep_inds]\n        else:\n            return None, None\n\n        keep = utils.nms(boxes, 0.7)\n\n        if len(keep) == 0:\n            return None, None\n\n        keep_cls = cls[keep]\n        keep_boxes = boxes[keep]\n        keep_reg = reg[keep]\n        # keep_landmark = landmark[keep]\n\n\n        bw = keep_boxes[:, 2] - keep_boxes[:, 0] + 1\n        bh = keep_boxes[:, 3] - keep_boxes[:, 1] + 1\n\n\n        boxes = np.vstack([ keep_boxes[:,0],\n                              keep_boxes[:,1],\n                              keep_boxes[:,2],\n                              keep_boxes[:,3],\n                              keep_cls[:,0],\n                              # keep_boxes[:,0] + keep_landmark[:, 0] * bw,\n                              # keep_boxes[:,1] + keep_landmark[:, 1] * bh,\n                              # keep_boxes[:,0] + keep_landmark[:, 2] * bw,\n                              # keep_boxes[:,1] + keep_landmark[:, 3] * bh,\n                              # keep_boxes[:,0] + keep_landmark[:, 4] * bw,\n                              # keep_boxes[:,1] + keep_landmark[:, 5] * bh,\n                              # keep_boxes[:,0] + keep_landmark[:, 6] * bw,\n                              # keep_boxes[:,1] + keep_landmark[:, 7] * bh,\n                              # keep_boxes[:,0] + keep_landmark[:, 8] * bw,\n                              # keep_boxes[:,1] + keep_landmark[:, 9] * bh,\n                            ])\n\n        align_topx = keep_boxes[:,0] + keep_reg[:,0] * bw\n        align_topy = keep_boxes[:,1] + keep_reg[:,1] * bh\n        align_bottomx = keep_boxes[:,2] + keep_reg[:,2] * bw\n        align_bottomy = keep_boxes[:,3] + keep_reg[:,3] * bh\n\n        boxes_align = np.vstack([align_topx,\n                               align_topy,\n                               align_bottomx,\n                               align_bottomy,\n                               keep_cls[:, 0],\n                               # align_topx + keep_landmark[:, 0] * bw,\n                               # align_topy + keep_landmark[:, 1] * bh,\n                               # align_topx + keep_landmark[:, 2] * bw,\n                               # align_topy + keep_landmark[:, 3] * bh,\n                               # align_topx + keep_landmark[:, 4] * bw,\n                               # align_topy + keep_landmark[:, 5] * bh,\n                               # align_topx + keep_landmark[:, 6] * bw,\n                               # align_topy + keep_landmark[:, 7] * bh,\n                               # align_topx + keep_landmark[:, 8] * bw,\n                               # align_topy + keep_landmark[:, 9] * bh,\n                             ])\n\n        boxes = boxes.T\n        boxes_align = boxes_align.T\n\n        return boxes, boxes_align\n\n    def detect_onet(self, im, dets):\n        """"""Get face candidates using onet\n\n        Parameters:\n        ----------\n        im: numpy array\n            input image array\n        dets: numpy array\n            detection results of rnet\n\n        Returns:\n        -------\n        boxes_align: numpy array\n            boxes after calibration\n        landmarks_align: numpy array\n            landmarks after calibration\n\n        """"""\n        h, w, c = im.shape\n\n        if dets is None:\n            return None, None\n\n        dets = self.square_bbox(dets)\n        dets[:, 0:4] = np.round(dets[:, 0:4])\n\n        [dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph] = self.pad(dets, w, h)\n        num_boxes = dets.shape[0]\n\n\n        # cropped_ims_tensors = np.zeros((num_boxes, 3, 24, 24), dtype=np.float32)\n        cropped_ims_tensors = []\n        for i in range(num_boxes):\n            tmp = np.zeros((tmph[i], tmpw[i], 3), dtype=np.uint8)\n            # crop input image\n            tmp[dy[i]:edy[i] + 1, dx[i]:edx[i] + 1, :] = im[y[i]:ey[i] + 1, x[i]:ex[i] + 1, :]\n            crop_im = cv2.resize(tmp, (48, 48))\n            crop_im_tensor = image_tools.convert_image_to_tensor(crop_im)\n            # cropped_ims_tensors[i, :, :, :] = crop_im_tensor\n            cropped_ims_tensors.append(crop_im_tensor)\n        feed_imgs = Variable(torch.stack(cropped_ims_tensors))\n\n        if self.rnet_detector.use_cuda:\n            feed_imgs = feed_imgs.cuda()\n\n        cls_map, reg, landmark = self.onet_detector(feed_imgs)\n\n        cls_map = cls_map.cpu().data.numpy()\n        reg = reg.cpu().data.numpy()\n        landmark = landmark.cpu().data.numpy()\n\n        keep_inds = np.where(cls_map > self.thresh[2])[0]\n\n        if len(keep_inds) > 0:\n            boxes = dets[keep_inds]\n            cls = cls_map[keep_inds]\n            reg = reg[keep_inds]\n            landmark = landmark[keep_inds]\n        else:\n            return None, None\n\n        keep = utils.nms(boxes, 0.7, mode=""Minimum"")\n\n        if len(keep) == 0:\n            return None, None\n\n        keep_cls = cls[keep]\n        keep_boxes = boxes[keep]\n        keep_reg = reg[keep]\n        keep_landmark = landmark[keep]\n\n        bw = keep_boxes[:, 2] - keep_boxes[:, 0] + 1\n        bh = keep_boxes[:, 3] - keep_boxes[:, 1] + 1\n\n\n        align_topx = keep_boxes[:, 0] + keep_reg[:, 0] * bw\n        align_topy = keep_boxes[:, 1] + keep_reg[:, 1] * bh\n        align_bottomx = keep_boxes[:, 2] + keep_reg[:, 2] * bw\n        align_bottomy = keep_boxes[:, 3] + keep_reg[:, 3] * bh\n\n        align_landmark_topx = keep_boxes[:, 0]\n        align_landmark_topy = keep_boxes[:, 1]\n\n\n\n\n        boxes_align = np.vstack([align_topx,\n                                 align_topy,\n                                 align_bottomx,\n                                 align_bottomy,\n                                 keep_cls[:, 0],\n                                 # align_topx + keep_landmark[:, 0] * bw,\n                                 # align_topy + keep_landmark[:, 1] * bh,\n                                 # align_topx + keep_landmark[:, 2] * bw,\n                                 # align_topy + keep_landmark[:, 3] * bh,\n                                 # align_topx + keep_landmark[:, 4] * bw,\n                                 # align_topy + keep_landmark[:, 5] * bh,\n                                 # align_topx + keep_landmark[:, 6] * bw,\n                                 # align_topy + keep_landmark[:, 7] * bh,\n                                 # align_topx + keep_landmark[:, 8] * bw,\n                                 # align_topy + keep_landmark[:, 9] * bh,\n                                 ])\n\n        boxes_align = boxes_align.T\n\n        landmark =  np.vstack([\n                                 align_landmark_topx + keep_landmark[:, 0] * bw,\n                                 align_landmark_topy + keep_landmark[:, 1] * bh,\n                                 align_landmark_topx + keep_landmark[:, 2] * bw,\n                                 align_landmark_topy + keep_landmark[:, 3] * bh,\n                                 align_landmark_topx + keep_landmark[:, 4] * bw,\n                                 align_landmark_topy + keep_landmark[:, 5] * bh,\n                                 align_landmark_topx + keep_landmark[:, 6] * bw,\n                                 align_landmark_topy + keep_landmark[:, 7] * bh,\n                                 align_landmark_topx + keep_landmark[:, 8] * bw,\n                                 align_landmark_topy + keep_landmark[:, 9] * bh,\n                                 ])\n\n        landmark_align = landmark.T\n\n        return boxes_align, landmark_align\n\n\n    def detect_face(self,img):\n        """"""Detect face over image\n        """"""\n        boxes_align = np.array([])\n        landmark_align =np.array([])\n\n        t = time.time()\n\n        # pnet\n        if self.pnet_detector:\n            boxes, boxes_align = self.detect_pnet(img)\n            if boxes_align is None:\n                return np.array([]), np.array([])\n\n            t1 = time.time() - t\n            t = time.time()\n\n        # rnet\n        if self.rnet_detector:\n            boxes, boxes_align = self.detect_rnet(img, boxes_align)\n            if boxes_align is None:\n                return np.array([]), np.array([])\n\n            t2 = time.time() - t\n            t = time.time()\n\n        # onet\n        if self.onet_detector:\n            boxes_align, landmark_align = self.detect_onet(img, boxes_align)\n            if boxes_align is None:\n                return np.array([]), np.array([])\n\n            t3 = time.time() - t\n            t = time.time()\n            print(""time cost "" + \'{:.3f}\'.format(t1+t2+t3) + \'  pnet {:.3f}  rnet {:.3f}  onet {:.3f}\'.format(t1, t2, t3))\n\n        return boxes_align, landmark_align\n'"
mtcnn/core/image_reader.py,0,"b'import numpy as np\nimport cv2\n\n\n\nclass TrainImageReader:\n    def __init__(self, imdb, im_size, batch_size=128, shuffle=False):\n\n        self.imdb = imdb\n        self.batch_size = batch_size\n        self.im_size = im_size\n        self.shuffle = shuffle\n\n        self.cur = 0\n        self.size = len(imdb)\n        self.index = np.arange(self.size)\n        self.num_classes = 2\n\n        self.batch = None\n        self.data = None\n        self.label = None\n\n        self.label_names= [\'label\', \'bbox_target\', \'landmark_target\']\n        self.reset()\n        self.get_batch()\n\n    def reset(self):\n        self.cur = 0\n        if self.shuffle:\n            np.random.shuffle(self.index)\n\n    def iter_next(self):\n        return self.cur + self.batch_size <= self.size\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        return self.next()\n\n    def next(self):\n        if self.iter_next():\n            self.get_batch()\n            self.cur += self.batch_size\n            return self.data,self.label\n        else:\n            raise StopIteration\n\n    def getindex(self):\n        return self.cur / self.batch_size\n\n    def getpad(self):\n        if self.cur + self.batch_size > self.size:\n            return self.cur + self.batch_size - self.size\n        else:\n            return 0\n\n    def get_batch(self):\n        cur_from = self.cur\n        cur_to = min(cur_from + self.batch_size, self.size)\n        imdb = [self.imdb[self.index[i]] for i in range(cur_from, cur_to)]\n        data, label = get_minibatch(imdb)\n        self.data = data[\'data\']\n        self.label = [label[name] for name in self.label_names]\n\n\n\nclass TestImageLoader:\n    def __init__(self, imdb, batch_size=1, shuffle=False):\n        self.imdb = imdb\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.size = len(imdb)\n        self.index = np.arange(self.size)\n\n        self.cur = 0\n        self.data = None\n        self.label = None\n\n        self.reset()\n        self.get_batch()\n\n    def reset(self):\n        self.cur = 0\n        if self.shuffle:\n            np.random.shuffle(self.index)\n\n    def iter_next(self):\n        return self.cur + self.batch_size <= self.size\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        return self.next()\n\n    def next(self):\n        if self.iter_next():\n            self.get_batch()\n            self.cur += self.batch_size\n            return self.data\n        else:\n            raise StopIteration\n\n    def getindex(self):\n        return self.cur / self.batch_size\n\n    def getpad(self):\n        if self.cur + self.batch_size > self.size:\n            return self.cur + self.batch_size - self.size\n        else:\n            return 0\n\n    def get_batch(self):\n        cur_from = self.cur\n        cur_to = min(cur_from + self.batch_size, self.size)\n        imdb = [self.imdb[self.index[i]] for i in range(cur_from, cur_to)]\n        data= get_testbatch(imdb)\n        self.data=data[\'data\']\n\n\n\n\ndef get_minibatch(imdb):\n\n    # im_size: 12, 24 or 48\n    num_images = len(imdb)\n    processed_ims = list()\n    cls_label = list()\n    bbox_reg_target = list()\n    landmark_reg_target = list()\n\n    for i in range(num_images):\n        im = cv2.imread(imdb[i][\'image\'])\n        #im = Image.open(imdb[i][\'image\'])\n\n        if imdb[i][\'flipped\']:\n            im = im[:, ::-1, :]\n            #im = im.transpose(Image.FLIP_LEFT_RIGHT)\n\n        cls = imdb[i][\'label\']\n        bbox_target = imdb[i][\'bbox_target\']\n        landmark = imdb[i][\'landmark_target\']\n\n        processed_ims.append(im)\n        cls_label.append(cls)\n        bbox_reg_target.append(bbox_target)\n        landmark_reg_target.append(landmark)\n\n    im_array = np.asarray(processed_ims)\n\n    label_array = np.array(cls_label)\n\n    bbox_target_array = np.vstack(bbox_reg_target)\n\n    landmark_target_array = np.vstack(landmark_reg_target)\n\n    data = {\'data\': im_array}\n    label = {\'label\': label_array,\n             \'bbox_target\': bbox_target_array,\n             \'landmark_target\': landmark_target_array\n             }\n\n    return data, label\n\n\ndef get_testbatch(imdb):\n    assert len(imdb) == 1, ""Single batch only""\n    im = cv2.imread(imdb[0][\'image\'])\n    data = {\'data\': im}\n    return data'"
mtcnn/core/image_tools.py,3,"b'import torchvision.transforms as transforms\nimport torch\nfrom torch.autograd.variable import Variable\nimport numpy as np\n\ntransform = transforms.ToTensor()\n\ndef convert_image_to_tensor(image):\n    """"""convert an image to pytorch tensor\n\n        Parameters:\n        ----------\n        image: numpy array , h * w * c\n\n        Returns:\n        -------\n        image_tensor: pytorch.FloatTensor, c * h * w\n        """"""\n    # image = image.astype(np.float32)\n    return transform(image)\n    # return transform(image)\n\n\ndef convert_chwTensor_to_hwcNumpy(tensor):\n    """"""convert a group images pytorch tensor(count * c * h * w) to numpy array images(count * h * w * c)\n            Parameters:\n            ----------\n            tensor: numpy array , count * c * h * w\n\n            Returns:\n            -------\n            numpy array images: count * h * w * c\n            """"""\n\n    if isinstance(tensor, Variable):\n        return np.transpose(tensor.data.numpy(), (0,2,3,1))\n    elif isinstance(tensor, torch.FloatTensor):\n        return np.transpose(tensor.numpy(), (0,2,3,1))\n    else:\n        raise Exception(""covert b*c*h*w tensor to b*h*w*c numpy error.This tensor must have 4 dimension."")'"
mtcnn/core/imagedb.py,0,"b'import os\nimport numpy as np\n\nclass ImageDB(object):\n    def __init__(self, image_annotation_file, prefix_path=\'\', mode=\'train\'):\n        self.prefix_path = prefix_path\n        self.image_annotation_file = image_annotation_file\n        self.classes = [\'__background__\', \'face\']\n        self.num_classes = 2\n        self.image_set_index = self.load_image_set_index()\n        self.num_images = len(self.image_set_index)\n        self.mode = mode\n\n\n    def load_image_set_index(self):\n        """"""Get image index\n\n        Parameters:\n        ----------\n        Returns:\n        -------\n        image_set_index: str\n            relative path of image\n        """"""\n        assert os.path.exists(self.image_annotation_file), \'Path does not exist: {}\'.format(self.image_annotation_file)\n        with open(self.image_annotation_file, \'r\') as f:\n            image_set_index = [x.strip().split(\' \')[0] for x in f.readlines()]\n        return image_set_index\n\n\n    def load_imdb(self):\n        """"""Get and save ground truth image database\n\n        Parameters:\n        ----------\n        Returns:\n        -------\n        gt_imdb: dict\n            image database with annotations\n        """"""\n        #cache_file = os.path.join(self.cache_path, self.name + \'_gt_roidb.pkl\')\n        #if os.path.exists(cache_file):\n        #    with open(cache_file, \'rb\') as f:\n        #        imdb = cPickle.load(f)\n        #    print \'{} gt imdb loaded from {}\'.format(self.name, cache_file)\n        #    return imdb\n\n        gt_imdb = self.load_annotations()\n\n        #with open(cache_file, \'wb\') as f:\n        #    cPickle.dump(gt_imdb, f, cPickle.HIGHEST_PROTOCOL)\n        return gt_imdb\n\n\n    def real_image_path(self, index):\n        """"""Given image index, return full path\n\n        Parameters:\n        ----------\n        index: str\n            relative path of image\n        Returns:\n        -------\n        image_file: str\n            full path of image\n        """"""\n\n        index = index.replace(""\\\\"", ""/"")\n\n        if not os.path.exists(index):\n            image_file = os.path.join(self.prefix_path, index)\n        else:\n            image_file=index\n        if not image_file.endswith(\'.jpg\'):\n            image_file = image_file + \'.jpg\'\n        assert os.path.exists(image_file), \'Path does not exist: {}\'.format(image_file)\n        return image_file\n\n\n    def load_annotations(self,annotion_type=1):\n        """"""Load annotations\n\n        Parameters:\n        ----------\n        annotion_type: int\n                      0:dsadsa\n                      1:dsadsa\n        Returns:\n        -------\n        imdb: dict\n            image database with annotations\n        """"""\n\n        assert os.path.exists(self.image_annotation_file), \'annotations not found at {}\'.format(self.image_annotation_file)\n        with open(self.image_annotation_file, \'r\') as f:\n            annotations = f.readlines()\n\n        imdb = []\n        for i in range(self.num_images):\n            annotation = annotations[i].strip().split(\' \')\n            index = annotation[0]\n            im_path = self.real_image_path(index)\n            imdb_ = dict()\n            imdb_[\'image\'] = im_path\n\n            if self.mode == \'test\':\n               # gt_boxes = map(float, annotation[1:])\n               # boxes = np.array(bbox, dtype=np.float32).reshape(-1, 4)\n               # imdb_[\'gt_boxes\'] = boxes\n                pass\n            else:\n                label = annotation[1]\n                imdb_[\'label\'] = int(label)\n                imdb_[\'flipped\'] = False\n                imdb_[\'bbox_target\'] = np.zeros((4,))\n                imdb_[\'landmark_target\'] = np.zeros((10,))\n                if len(annotation[2:])==4:\n                    bbox_target = annotation[2:6]\n                    imdb_[\'bbox_target\'] = np.array(bbox_target).astype(float)\n                if len(annotation[2:])==14:\n                    bbox_target = annotation[2:6]\n                    imdb_[\'bbox_target\'] = np.array(bbox_target).astype(float)\n                    landmark = annotation[6:]\n                    imdb_[\'landmark_target\'] = np.array(landmark).astype(float)\n            imdb.append(imdb_)\n\n        return imdb\n\n\n    def append_flipped_images(self, imdb):\n        """"""append flipped images to imdb\n\n        Parameters:\n        ----------\n        imdb: imdb\n            image database\n        Returns:\n        -------\n        imdb: dict\n            image database with flipped image annotations added\n        """"""\n        print(\'append flipped images to imdb\', len(imdb))\n        for i in range(len(imdb)):\n            imdb_ = imdb[i]\n            m_bbox = imdb_[\'bbox_target\'].copy()\n            m_bbox[0], m_bbox[2] = -m_bbox[2], -m_bbox[0]\n\n            landmark_ = imdb_[\'landmark_target\'].copy()\n            landmark_ = landmark_.reshape((5, 2))\n            landmark_ = np.asarray([(1 - x, y) for (x, y) in landmark_])\n            landmark_[[0, 1]] = landmark_[[1, 0]]\n            landmark_[[3, 4]] = landmark_[[4, 3]]\n\n            item = {\'image\': imdb_[\'image\'],\n                     \'label\': imdb_[\'label\'],\n                     \'bbox_target\': m_bbox,\n                     \'landmark_target\': landmark_.reshape((10)),\n                     \'flipped\': True}\n\n            imdb.append(item)\n        self.image_set_index *= 2\n        return imdb\n'"
mtcnn/core/models.py,24,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef weights_init(m):\n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n        nn.init.xavier_uniform(m.weight.data)\n        nn.init.constant(m.bias, 0.1)\n\n\n\nclass LossFn:\n    def __init__(self, cls_factor=1, box_factor=1, landmark_factor=1):\n        # loss function\n        self.cls_factor = cls_factor\n        self.box_factor = box_factor\n        self.land_factor = landmark_factor\n        self.loss_cls = nn.BCELoss() # binary cross entropy\n        self.loss_box = nn.MSELoss() # mean square error\n        self.loss_landmark = nn.MSELoss()\n\n\n    def cls_loss(self,gt_label,pred_label):\n        pred_label = torch.squeeze(pred_label)\n        gt_label = torch.squeeze(gt_label)\n        # get the mask element which >= 0, only 0 and 1 can effect the detection loss\n        mask = torch.ge(gt_label,0)\n        valid_gt_label = torch.masked_select(gt_label,mask)\n        valid_pred_label = torch.masked_select(pred_label,mask)\n        return self.loss_cls(valid_pred_label,valid_gt_label)*self.cls_factor\n\n\n    def box_loss(self,gt_label,gt_offset,pred_offset):\n        pred_offset = torch.squeeze(pred_offset)\n        gt_offset = torch.squeeze(gt_offset)\n        gt_label = torch.squeeze(gt_label)\n\n        #get the mask element which != 0\n        unmask = torch.eq(gt_label,0)\n        mask = torch.eq(unmask,0)\n        #convert mask to dim index\n        chose_index = torch.nonzero(mask.data)\n        chose_index = torch.squeeze(chose_index)\n        #only valid element can effect the loss\n        valid_gt_offset = gt_offset[chose_index,:]\n        valid_pred_offset = pred_offset[chose_index,:]\n        return self.loss_box(valid_pred_offset,valid_gt_offset)*self.box_factor\n\n\n    def landmark_loss(self,gt_label,gt_landmark,pred_landmark):\n        pred_landmark = torch.squeeze(pred_landmark)\n        gt_landmark = torch.squeeze(gt_landmark)\n        gt_label = torch.squeeze(gt_label)\n        mask = torch.eq(gt_label,-2)\n\n        chose_index = torch.nonzero(mask.data)\n        chose_index = torch.squeeze(chose_index)\n\n        valid_gt_landmark = gt_landmark[chose_index, :]\n        valid_pred_landmark = pred_landmark[chose_index, :]\n        return self.loss_landmark(valid_pred_landmark,valid_gt_landmark)*self.land_factor\n\n\n\n\n\nclass PNet(nn.Module):\n    ''' PNet '''\n\n    def __init__(self, is_train=False, use_cuda=True):\n        super(PNet, self).__init__()\n        self.is_train = is_train\n        self.use_cuda = use_cuda\n\n        # backend\n        self.pre_layer = nn.Sequential(\n            nn.Conv2d(3, 10, kernel_size=3, stride=1),  # conv1\n            nn.PReLU(),  # PReLU1\n            nn.MaxPool2d(kernel_size=2, stride=2),  # pool1\n            nn.Conv2d(10, 16, kernel_size=3, stride=1),  # conv2\n            nn.PReLU(),  # PReLU2\n            nn.Conv2d(16, 32, kernel_size=3, stride=1),  # conv3\n            nn.PReLU()  # PReLU3\n        )\n        # detection\n        self.conv4_1 = nn.Conv2d(32, 1, kernel_size=1, stride=1)\n        # bounding box regresion\n        self.conv4_2 = nn.Conv2d(32, 4, kernel_size=1, stride=1)\n        # landmark localization\n        self.conv4_3 = nn.Conv2d(32, 10, kernel_size=1, stride=1)\n\n        # weight initiation with xavier\n        self.apply(weights_init)\n\n    def forward(self, x):\n        x = self.pre_layer(x)\n        label = F.sigmoid(self.conv4_1(x))\n        offset = self.conv4_2(x)\n        # landmark = self.conv4_3(x)\n\n        if self.is_train is True:\n            # label_loss = LossUtil.label_loss(self.gt_label,torch.squeeze(label))\n            # bbox_loss = LossUtil.bbox_loss(self.gt_bbox,torch.squeeze(offset))\n            return label,offset\n        #landmark = self.conv4_3(x)\n        return label, offset\n\n\n\n\n\nclass RNet(nn.Module):\n    ''' RNet '''\n\n    def __init__(self,is_train=False, use_cuda=True):\n        super(RNet, self).__init__()\n        self.is_train = is_train\n        self.use_cuda = use_cuda\n        # backend\n        self.pre_layer = nn.Sequential(\n            nn.Conv2d(3, 28, kernel_size=3, stride=1),  # conv1\n            nn.PReLU(),  # prelu1\n            nn.MaxPool2d(kernel_size=3, stride=2),  # pool1\n            nn.Conv2d(28, 48, kernel_size=3, stride=1),  # conv2\n            nn.PReLU(),  # prelu2\n            nn.MaxPool2d(kernel_size=3, stride=2),  # pool2\n            nn.Conv2d(48, 64, kernel_size=2, stride=1),  # conv3\n            nn.PReLU()  # prelu3\n\n        )\n        self.conv4 = nn.Linear(64*2*2, 128)  # conv4\n        self.prelu4 = nn.PReLU()  # prelu4\n        # detection\n        self.conv5_1 = nn.Linear(128, 1)\n        # bounding box regression\n        self.conv5_2 = nn.Linear(128, 4)\n        # lanbmark localization\n        self.conv5_3 = nn.Linear(128, 10)\n        # weight initiation weih xavier\n        self.apply(weights_init)\n\n    def forward(self, x):\n        # backend\n        x = self.pre_layer(x)\n        x = x.view(x.size(0), -1)\n        x = self.conv4(x)\n        x = self.prelu4(x)\n        # detection\n        det = torch.sigmoid(self.conv5_1(x))\n        box = self.conv5_2(x)\n        # landmark = self.conv5_3(x)\n\n        if self.is_train is True:\n            return det, box\n        #landmard = self.conv5_3(x)\n        return det, box\n\n\n\n\nclass ONet(nn.Module):\n    ''' RNet '''\n\n    def __init__(self,is_train=False, use_cuda=True):\n        super(ONet, self).__init__()\n        self.is_train = is_train\n        self.use_cuda = use_cuda\n        # backend\n        self.pre_layer = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, stride=1),  # conv1\n            nn.PReLU(),  # prelu1\n            nn.MaxPool2d(kernel_size=3, stride=2),  # pool1\n            nn.Conv2d(32, 64, kernel_size=3, stride=1),  # conv2\n            nn.PReLU(),  # prelu2\n            nn.MaxPool2d(kernel_size=3, stride=2),  # pool2\n            nn.Conv2d(64, 64, kernel_size=3, stride=1),  # conv3\n            nn.PReLU(), # prelu3\n            nn.MaxPool2d(kernel_size=2,stride=2), # pool3\n            nn.Conv2d(64,128,kernel_size=2,stride=1), # conv4\n            nn.PReLU() # prelu4\n        )\n        self.conv5 = nn.Linear(128*2*2, 256)  # conv5\n        self.prelu5 = nn.PReLU()  # prelu5\n        # detection\n        self.conv6_1 = nn.Linear(256, 1)\n        # bounding box regression\n        self.conv6_2 = nn.Linear(256, 4)\n        # lanbmark localization\n        self.conv6_3 = nn.Linear(256, 10)\n        # weight initiation weih xavier\n        self.apply(weights_init)\n\n    def forward(self, x):\n        # backend\n        x = self.pre_layer(x)\n        x = x.view(x.size(0), -1)\n        x = self.conv5(x)\n        x = self.prelu5(x)\n        # detection\n        det = torch.sigmoid(self.conv6_1(x))\n        box = self.conv6_2(x)\n        landmark = self.conv6_3(x)\n        if self.is_train is True:\n            return det, box, landmark\n        #landmard = self.conv5_3(x)\n        return det, box, landmark\n\n\n\n\n\n# Residual Block\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.downsample = downsample\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        if self.downsample:\n            residual = self.downsample(x)\n        out += residual\n        out = self.relu(out)\n        return out\n\n\n\n# ResNet Module\nclass ResNet(nn.Module):\n    def __init__(self, block, num_classes=10):\n        super(ResNet, self).__init__()\n        self.in_channels = 16\n        self.conv = nn.Conv2d(3, 16,kernel_size=3)\n        self.bn = nn.BatchNorm2d(16)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self.make_layer(block, 16, 3)\n        self.layer2 = self.make_layer(block, 32, 3, 2)\n        self.layer3 = self.make_layer(block, 64, 3, 2)\n        self.avg_pool = nn.AvgPool2d(8)\n        self.fc = nn.Linear(64, num_classes)\n\n    def make_layer(self, block, out_channels, blocks, stride=1):\n        downsample = None\n        if (stride != 1) or (self.in_channels != out_channels):\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_channels, out_channels, kernel_size=3, stride=stride),\n                nn.BatchNorm2d(out_channels))\n        layers = []\n        layers.append(block(self.in_channels, out_channels, stride, downsample))\n        self.in_channels = out_channels\n        for i in range(1, blocks):\n            layers.append(block(out_channels, out_channels))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.conv(x)\n        out = self.bn(out)\n        out = self.relu(out)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.avg_pool(out)\n        out = out.view(out.size(0), -1)\n        out = self.fc(out)\n        return out"""
mtcnn/core/nms.py,0,"b'import numpy as np\n\n\ndef torch_nms(dets, thresh, mode=""Union""):\n    """"""\n    greedily select boxes with high confidence\n    keep boxes overlap <= thresh\n    rule out overlap > thresh\n    :param dets: [[x1, y1, x2, y2 score]]\n    :param thresh: retain overlap <= thresh\n    :return: indexes to keep\n    """"""\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n    scores = dets[:, 4]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        if mode == ""Union"":\n            ovr = inter / (areas[i] + areas[order[1:]] - inter)\n        elif mode == ""Minimum"":\n            ovr = inter / np.minimum(areas[i], areas[order[1:]])\n\n        inds = np.where(ovr <= thresh)[0]\n        order = order[inds + 1]\n\n    return keep\n'"
mtcnn/core/resnet_inception_v2.py,8,"b'import torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\nimport os\nimport sys\n\n\nclass BasicConv2d(nn.Module):\n    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding,\n                              bias=False)  # verify bias false\n        self.bn = nn.BatchNorm2d(out_planes, eps=0.001, momentum=0, affine=True)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n\nclass Mixed_5b(nn.Module):\n    def __init__(self):\n        super(Mixed_5b, self).__init__()\n\n        self.branch0 = BasicConv2d(192, 96, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(192, 48, kernel_size=1, stride=1),\n            BasicConv2d(48, 64, kernel_size=5, stride=1, padding=2)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(192, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(96, 96, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            BasicConv2d(192, 64, kernel_size=1, stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Block35(nn.Module):\n    def __init__(self, scale=1.0):\n        super(Block35, self).__init__()\n\n        self.scale = scale\n\n        self.branch0 = BasicConv2d(320, 32, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(320, 32, kernel_size=1, stride=1),\n            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(320, 32, kernel_size=1, stride=1),\n            BasicConv2d(32, 48, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(48, 64, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.conv2d = nn.Conv2d(128, 320, kernel_size=1, stride=1)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        out = self.relu(out)\n        return out\n\n\nclass Mixed_6a(nn.Module):\n    def __init__(self):\n        super(Mixed_6a, self).__init__()\n\n        self.branch0 = BasicConv2d(320, 384, kernel_size=3, stride=2)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(320, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(256, 384, kernel_size=3, stride=2)\n        )\n\n        self.branch2 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        return out\n\n\nclass Block17(nn.Module):\n    def __init__(self, scale=1.0):\n        super(Block17, self).__init__()\n\n        self.scale = scale\n\n        self.branch0 = BasicConv2d(1088, 192, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(1088, 128, kernel_size=1, stride=1),\n            BasicConv2d(128, 160, kernel_size=(1, 7), stride=1, padding=(0, 3)),\n            BasicConv2d(160, 192, kernel_size=(7, 1), stride=1, padding=(3, 0))\n        )\n\n        self.conv2d = nn.Conv2d(384, 1088, kernel_size=1, stride=1)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        out = torch.cat((x0, x1), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        out = self.relu(out)\n        return out\n\n\nclass Mixed_7a(nn.Module):\n    def __init__(self):\n        super(Mixed_7a, self).__init__()\n\n        self.branch0 = nn.Sequential(\n            BasicConv2d(1088, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 384, kernel_size=3, stride=2)\n        )\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(1088, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 288, kernel_size=3, stride=2)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(1088, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 288, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(288, 320, kernel_size=3, stride=2)\n        )\n\n        self.branch3 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Block8(nn.Module):\n    def __init__(self, scale=1.0, noReLU=False):\n        super(Block8, self).__init__()\n\n        self.scale = scale\n        self.noReLU = noReLU\n\n        self.branch0 = BasicConv2d(2080, 192, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(2080, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 224, kernel_size=(1, 3), stride=1, padding=(0, 1)),\n            BasicConv2d(224, 256, kernel_size=(3, 1), stride=1, padding=(1, 0))\n        )\n\n        self.conv2d = nn.Conv2d(448, 2080, kernel_size=1, stride=1)\n        if not self.noReLU:\n            self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        out = torch.cat((x0, x1), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        if not self.noReLU:\n            out = self.relu(out)\n        return out\n\n\nclass InceptionResnetV2(nn.Module):\n    def __init__(self, num_classes=1001):\n        super(InceptionResnetV2, self).__init__()\n        self.conv2d_1a = BasicConv2d(3, 32, kernel_size=3, stride=2)\n        self.conv2d_2a = BasicConv2d(32, 32, kernel_size=3, stride=1)\n        self.conv2d_2b = BasicConv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.maxpool_3a = nn.MaxPool2d(3, stride=2)\n        self.conv2d_3b = BasicConv2d(64, 80, kernel_size=1, stride=1)\n        self.conv2d_4a = BasicConv2d(80, 192, kernel_size=3, stride=1)\n        self.maxpool_5a = nn.MaxPool2d(3, stride=2)\n        self.mixed_5b = Mixed_5b()\n        self.repeat = nn.Sequential(\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17)\n        )\n        self.mixed_6a = Mixed_6a()\n        self.repeat_1 = nn.Sequential(\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10)\n        )\n        self.mixed_7a = Mixed_7a()\n        self.repeat_2 = nn.Sequential(\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20)\n        )\n        self.block8 = Block8(noReLU=True)\n        self.conv2d_7b = BasicConv2d(2080, 1536, kernel_size=1, stride=1)\n        self.avgpool_1a = nn.AvgPool2d(8, count_include_pad=False)\n        self.classif = nn.Linear(1536, num_classes)\n\n    def forward(self, x):\n        x = self.conv2d_1a(x)\n        x = self.conv2d_2a(x)\n        x = self.conv2d_2b(x)\n        x = self.maxpool_3a(x)\n        x = self.conv2d_3b(x)\n        x = self.conv2d_4a(x)\n        x = self.maxpool_5a(x)\n        x = self.mixed_5b(x)\n        x = self.repeat(x)\n        x = self.mixed_6a(x)\n        x = self.repeat_1(x)\n        x = self.mixed_7a(x)\n        x = self.repeat_2(x)\n        x = self.block8(x)\n        x = self.conv2d_7b(x)\n        x = self.avgpool_1a(x)\n        x = x.view(x.size(0), -1)\n        x = self.classif(x)\n        return x'"
mtcnn/core/utils.py,0,"b'import numpy as np\nimport time\n\ndef IoU(box, boxes):\n    """"""Compute IoU between detect box and gt boxes\n\n    Parameters:\n    ----------\n    box: numpy array , shape (5, ): x1, y1, x2, y2, score\n        input box\n    boxes: numpy array, shape (n, 4): x1, y1, x2, y2\n        input ground truth boxes\n\n    Returns:\n    -------\n    ovr: numpy.array, shape (n, )\n        IoU\n    """"""\n    box_area = (box[2] - box[0] + 1) * (box[3] - box[1] + 1)\n    area = (boxes[:, 2] - boxes[:, 0] + 1) * (boxes[:, 3] - boxes[:, 1] + 1)\n    xx1 = np.maximum(box[0], boxes[:, 0])\n    yy1 = np.maximum(box[1], boxes[:, 1])\n    xx2 = np.minimum(box[2], boxes[:, 2])\n    yy2 = np.minimum(box[3], boxes[:, 3])\n\n    # compute the width and height of the bounding box\n    w = np.maximum(0, xx2 - xx1 + 1)\n    h = np.maximum(0, yy2 - yy1 + 1)\n\n    inter = w * h\n    ovr = np.true_divide(inter,(box_area + area - inter))\n    #ovr = inter / (box_area + area - inter)\n    return ovr\n\n\ndef convert_to_square(bbox):\n    """"""Convert bbox to square\n\n    Parameters:\n    ----------\n    bbox: numpy array , shape n x 5\n        input bbox\n\n    Returns:\n    -------\n    square bbox\n    """"""\n    square_bbox = bbox.copy()\n\n    h = bbox[:, 3] - bbox[:, 1] + 1\n    w = bbox[:, 2] - bbox[:, 0] + 1\n    max_side = np.maximum(h,w)\n    square_bbox[:, 0] = bbox[:, 0] + w*0.5 - max_side*0.5\n    square_bbox[:, 1] = bbox[:, 1] + h*0.5 - max_side*0.5\n    square_bbox[:, 2] = square_bbox[:, 0] + max_side - 1\n    square_bbox[:, 3] = square_bbox[:, 1] + max_side - 1\n    return square_bbox\n\n# non-maximum suppression: eleminates the box which have large interception with the box which have the largest score\ndef nms(dets, thresh, mode=""Union""):\n    """"""\n    greedily select boxes with high confidence\n    keep boxes overlap <= thresh\n    rule out overlap > thresh\n    :param dets: [[x1, y1, x2, y2 score]]\n    :param thresh: retain overlap <= thresh\n    :return: indexes to keep\n    """"""\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n    scores = dets[:, 4]\n\n    # shape of x1 = (454,), shape of scores = (454,)\n    # print(""shape of x1 = {0}, shape of scores = {1}"".format(x1.shape, scores.shape))\n    # time.sleep(5)\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1] # argsort: ascending order then [::-1] reverse the order --> descending order\n    # print(""shape of order {0}"".format(order.size)) # (454,)\n    # time.sleep(5)\n\n    # eleminates the box which have large interception with the box which have the largest score in order\n    # matain the box with largest score and boxes don\'t have large interception with it\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n\n        # cacaulate the IOU between box which have largest score with other boxes\n        if mode == ""Union"":\n            # area[i]: the area of largest score\n            ovr = inter / (areas[i] + areas[order[1:]] - inter)\n        elif mode == ""Minimum"":\n            ovr = inter / np.minimum(areas[i], areas[order[1:]])\n\n        inds = np.where(ovr <= thresh)[0]\n        order = order[inds + 1] # +1: eliminates the first element in order\n        # print(inds)\n        # print(""shape of order {0}"".format(order.shape))  # (454,)\n        # time.sleep(2)\n\n    return keep\n\n\n\n\n'"
mtcnn/core/vision.py,0,"b'from matplotlib.patches import Circle\nimport os\nimport sys \nsys.path.append(os.getcwd())\n\ndef vis_two(im_array, dets1, dets2, thresh=0.9):\n    """"""Visualize detection results before and after calibration\n\n    Parameters:\n    ----------\n    im_array: numpy.ndarray, shape(1, c, h, w)\n        test image in rgb\n    dets1: numpy.ndarray([[x1 y1 x2 y2 score]])\n        detection results before calibration\n    dets2: numpy.ndarray([[x1 y1 x2 y2 score]])\n        detection results after calibration\n    thresh: float\n        boxes with scores > thresh will be drawn in red otherwise yellow\n\n    Returns:\n    -------\n    """"""\n    import matplotlib.pyplot as plt\n    import random\n\n    figure = plt.figure()\n    plt.subplot(121)\n    plt.imshow(im_array)\n    color = \'yellow\'\n\n    for i in range(dets1.shape[0]):\n        bbox = dets1[i, :4]\n        landmarks = dets1[i, 5:]\n        score = dets1[i, 4]\n        if score > thresh:\n            rect = plt.Rectangle((bbox[0], bbox[1]),\n                                 bbox[2] - bbox[0],\n                                 bbox[3] - bbox[1], fill=False,\n                                 edgecolor=\'red\', linewidth=0.7)\n            plt.gca().add_patch(rect)\n            landmarks = landmarks.reshape((5,2))\n            for j in range(5):\n                plt.scatter(landmarks[j,0],landmarks[j,1],c=\'yellow\',linewidths=0.1, marker=\'x\', s=5)\n\n\n            # plt.gca().text(bbox[0], bbox[1] - 2,\n            #                \'{:.3f}\'.format(score),\n            #                bbox=dict(facecolor=\'blue\', alpha=0.5), fontsize=12, color=\'white\')\n        # else:\n        #     rect = plt.Rectangle((bbox[0], bbox[1]),\n        #                          bbox[2] - bbox[0],\n        #                          bbox[3] - bbox[1], fill=False,\n        #                          edgecolor=color, linewidth=0.5)\n        #     plt.gca().add_patch(rect)\n\n    plt.subplot(122)\n    plt.imshow(im_array)\n    color = \'yellow\'\n\n    for i in range(dets2.shape[0]):\n        bbox = dets2[i, :4]\n        landmarks = dets1[i, 5:]\n        score = dets2[i, 4]\n        if score > thresh:\n            rect = plt.Rectangle((bbox[0], bbox[1]),\n                                 bbox[2] - bbox[0],\n                                 bbox[3] - bbox[1], fill=False,\n                                 edgecolor=\'red\', linewidth=0.7)\n            plt.gca().add_patch(rect)\n\n            landmarks = landmarks.reshape((5, 2))\n            for j in range(5):\n                plt.scatter(landmarks[j, 0], landmarks[j, 1], c=\'yellow\',linewidths=0.1, marker=\'x\', s=5)\n\n            # plt.gca().text(bbox[0], bbox[1] - 2,\n            #                \'{:.3f}\'.format(score),\n            #                bbox=dict(facecolor=\'blue\', alpha=0.5), fontsize=12, color=\'white\')\n        # else:\n        #     rect = plt.Rectangle((bbox[0], bbox[1]),\n        #                          bbox[2] - bbox[0],\n        #                          bbox[3] - bbox[1], fill=False,\n        #                          edgecolor=color, linewidth=0.5)\n        #     plt.gca().add_patch(rect)\n    plt.show()\n\n\ndef vis_face(im_array, dets, landmarks, save_name):\n    """"""Visualize detection results before and after calibration\n\n    Parameters:\n    ----------\n    im_array: numpy.ndarray, shape(1, c, h, w)\n        test image in rgb\n    dets1: numpy.ndarray([[x1 y1 x2 y2 score]])\n        detection results before calibration\n    dets2: numpy.ndarray([[x1 y1 x2 y2 score]])\n        detection results after calibration\n    thresh: float\n        boxes with scores > thresh will be drawn in red otherwise yellow\n\n    Returns:\n    -------\n    """"""\n    import matplotlib.pyplot as plt\n    import random\n    import pylab\n\n    figure = pylab.figure()\n    # plt.subplot(121)\n    pylab.imshow(im_array)\n\n    for i in range(dets.shape[0]):\n        bbox = dets[i, :4]\n\n        rect = pylab.Rectangle((bbox[0], bbox[1]),\n                             bbox[2] - bbox[0],\n                             bbox[3] - bbox[1], fill=False,\n                             edgecolor=\'yellow\', linewidth=0.9)\n        pylab.gca().add_patch(rect)\n\n    if landmarks is not None:\n        for i in range(landmarks.shape[0]):\n            landmarks_one = landmarks[i, :]\n            landmarks_one = landmarks_one.reshape((5, 2))\n            for j in range(5):\n                # pylab.scatter(landmarks_one[j, 0], landmarks_one[j, 1], c=\'yellow\', linewidths=0.1, marker=\'x\', s=5)\n\n                cir1 = Circle(xy=(landmarks_one[j, 0], landmarks_one[j, 1]), radius=2, alpha=0.4, color=""red"")\n                pylab.gca().add_patch(cir1)\n                # plt.gca().text(bbox[0], bbox[1] - 2,\n                #                \'{:.3f}\'.format(score),\n                #                bbox=dict(facecolor=\'blue\', alpha=0.5), fontsize=12, color=\'white\')\n                # else:\n                #     rect = plt.Rectangle((bbox[0], bbox[1]),\n                #                          bbox[2] - bbox[0],\n                #                          bbox[3] - bbox[1], fill=False,\n                #                          edgecolor=color, linewidth=0.5)\n                #     plt.gca().add_patch(rect)\n        pylab.savefig(save_name)\n        pylab.show()'"
mtcnn/data_preprocess/assemble.py,0,"b""\nimport os\nimport numpy.random as npr\nimport numpy as np\n\ndef assemble_data(output_file, anno_file_list=[]):\n\n    #assemble the pos, neg, part annotations to one file\n    size = 12\n\n    if len(anno_file_list)==0:\n        return 0\n\n    if os.path.exists(output_file):\n        os.remove(output_file)\n\n    for anno_file in anno_file_list:\n        with open(anno_file, 'r') as f:\n            print(anno_file)\n            anno_lines = f.readlines()\n\n        base_num = 250000\n\n        if len(anno_lines) > base_num * 3:\n            idx_keep = npr.choice(len(anno_lines), size=base_num * 3, replace=True)\n        elif len(anno_lines) > 100000:\n            idx_keep = npr.choice(len(anno_lines), size=len(anno_lines), replace=True)\n        else:\n            idx_keep = np.arange(len(anno_lines))\n            np.random.shuffle(idx_keep)\n        chose_count = 0\n        with open(output_file, 'a+') as f:\n            for idx in idx_keep:\n                # write lables of pos, neg, part images\n                f.write(anno_lines[idx])\n                chose_count+=1\n\n    return chose_count\n"""
mtcnn/data_preprocess/assemble_onet_imglist.py,0,"b'import os\nimport sys\nsys.path.append(os.getcwd())\nimport mtcnn.data_preprocess.assemble as assemble\n\nonet_postive_file = \'./anno_store/pos_48.txt\'\nonet_part_file = \'./anno_store/part_48.txt\'\nonet_neg_file = \'./anno_store/neg_48.txt\'\nonet_landmark_file = \'./anno_store/landmark_48.txt\'\nimglist_filename = \'./anno_store/imglist_anno_48.txt\'\n\nif __name__ == \'__main__\':\n\n    anno_list = []\n\n    anno_list.append(onet_postive_file)\n    anno_list.append(onet_part_file)\n    anno_list.append(onet_neg_file)\n    anno_list.append(onet_landmark_file)\n\n    chose_count = assemble.assemble_data(imglist_filename ,anno_list)\n    print(""ONet train annotation result file path:%s"" % imglist_filename)\n'"
mtcnn/data_preprocess/assemble_pnet_imglist.py,0,"b'import os\nimport sys\nsys.path.append(os.getcwd())\nimport mtcnn.data_preprocess.assemble as assemble\n\npnet_postive_file = \'./anno_store/pos_12.txt\'\npnet_part_file = \'./anno_store/part_12.txt\'\npnet_neg_file = \'./anno_store/neg_12.txt\'\npnet_landmark_file = \'./anno_store/landmark_12.txt\'\nimglist_filename = \'./anno_store/imglist_anno_12.txt\'\n\nif __name__ == \'__main__\':\n\n    anno_list = []\n\n    anno_list.append(pnet_postive_file)\n    anno_list.append(pnet_part_file)\n    anno_list.append(pnet_neg_file)\n    # anno_list.append(pnet_landmark_file)\n\n    chose_count = assemble.assemble_data(imglist_filename ,anno_list)\n    print(""PNet train annotation result file path:%s"" % imglist_filename)\n'"
mtcnn/data_preprocess/assemble_rnet_imglist.py,0,"b'import os\nimport sys\nsys.path.append(os.getcwd())\nimport mtcnn.data_preprocess.assemble as assemble\n\nrnet_postive_file = \'./anno_store/pos_24.txt\'\nrnet_part_file = \'./anno_store/part_24.txt\'\nrnet_neg_file = \'./anno_store/neg_24.txt\'\nrnet_landmark_file = \'./anno_store/landmark_24.txt\'\nimglist_filename = \'./anno_store/imglist_anno_24.txt\'\n\nif __name__ == \'__main__\':\n\n    anno_list = []\n\n    anno_list.append(rnet_postive_file)\n    anno_list.append(rnet_part_file)\n    anno_list.append(rnet_neg_file)\n    # anno_list.append(pnet_landmark_file)\n\n    chose_count = assemble.assemble_data(imglist_filename ,anno_list)\n    print(""PNet train annotation result file path:%s"" % imglist_filename)\n'"
mtcnn/data_preprocess/gen_Onet_train_data.py,0,"b'import argparse\nimport os\nimport sys\nsys.path.append(os.getcwd())\nimport cv2\nimport numpy as np\nfrom mtcnn.core.detect import MtcnnDetector,create_mtcnn_net\nfrom mtcnn.core.imagedb import ImageDB\nfrom mtcnn.core.image_reader import TestImageLoader\nimport time\n\nfrom six.moves import cPickle\nfrom mtcnn.core.utils import convert_to_square,IoU\nimport mtcnn.core.vision as vision\n\nprefix_path = \'\'\ntraindata_store = \'./data_set/train\'\npnet_model_file = \'./model_store/pnet_epoch.pt\'\nrnet_model_file = \'./model_store/rnet_epoch.pt\'\nannotation_file = \'./anno_store/anno_train_test.txt\'\nuse_cuda = True\n\ndef gen_onet_data(data_dir, anno_file, pnet_model_file, rnet_model_file, prefix_path=\'\', use_cuda=True, vis=False):\n\n\n    pnet, rnet, _ = create_mtcnn_net(p_model_path=pnet_model_file, r_model_path=rnet_model_file, use_cuda=use_cuda)\n    mtcnn_detector = MtcnnDetector(pnet=pnet, rnet=rnet, min_face_size=12)\n\n    imagedb = ImageDB(anno_file,mode=""test"",prefix_path=prefix_path)\n    imdb = imagedb.load_imdb()\n    image_reader = TestImageLoader(imdb,1,False)\n\n    all_boxes = list()\n    batch_idx = 0\n\n    print(\'size:%d\' % image_reader.size)\n    for databatch in image_reader:\n        if batch_idx % 50 == 0:\n            print(""%d images done"" % batch_idx)\n\n        im = databatch\n\n        t = time.time()\n\n        # pnet detection = [x1, y1, x2, y2, score, reg]\n        p_boxes, p_boxes_align = mtcnn_detector.detect_pnet(im=im)\n\n        # rnet detection\n        boxes, boxes_align = mtcnn_detector.detect_rnet(im=im, dets=p_boxes_align)\n\n        if boxes_align is None:\n            all_boxes.append(np.array([]))\n            batch_idx += 1\n            continue\n        if vis:\n            rgb_im = cv2.cvtColor(np.asarray(im), cv2.COLOR_BGR2RGB)\n            vision.vis_two(rgb_im, boxes, boxes_align)\n\n        t1 = time.time() - t\n        t = time.time()\n        all_boxes.append(boxes_align)\n        batch_idx += 1\n\n    save_path = \'./model_store\'\n\n    if not os.path.exists(save_path):\n        os.mkdir(save_path)\n\n    save_file = os.path.join(save_path, ""detections_%d.pkl"" % int(time.time()))\n    with open(save_file, \'wb\') as f:\n        cPickle.dump(all_boxes, f, cPickle.HIGHEST_PROTOCOL)\n\n\n    gen_onet_sample_data(data_dir,anno_file,save_file,prefix_path)\n\n\n\n\n\n\ndef gen_onet_sample_data(data_dir,anno_file,det_boxs_file,prefix):\n\n    neg_save_dir = os.path.join(data_dir, ""48/negative"")\n    pos_save_dir = os.path.join(data_dir, ""48/positive"")\n    part_save_dir = os.path.join(data_dir, ""48/part"")\n\n    for dir_path in [neg_save_dir, pos_save_dir, part_save_dir]:\n        if not os.path.exists(dir_path):\n            os.makedirs(dir_path)\n\n\n    # load ground truth from annotation file\n    # format of each line: image/path [x1,y1,x2,y2] for each gt_box in this image\n\n    with open(anno_file, \'r\') as f:\n        annotations = f.readlines()\n\n    image_size = 48\n    net = ""onet""\n\n    im_idx_list = list()\n    gt_boxes_list = list()\n    num_of_images = len(annotations)\n    print(""processing %d images in total"" % num_of_images)\n\n    for annotation in annotations:\n        annotation = annotation.strip().split(\' \')\n        im_idx = os.path.join(prefix,annotation[0])\n\n        boxes = list(map(float, annotation[1:]))\n        boxes = np.array(boxes, dtype=np.float32).reshape(-1, 4)\n        im_idx_list.append(im_idx)\n        gt_boxes_list.append(boxes)\n\n    save_path = \'./anno_store\'\n    if not os.path.exists(save_path):\n        os.makedirs(save_path)\n\n    f1 = open(os.path.join(save_path, \'pos_%d.txt\' % image_size), \'w\')\n    f2 = open(os.path.join(save_path, \'neg_%d.txt\' % image_size), \'w\')\n    f3 = open(os.path.join(save_path, \'part_%d.txt\' % image_size), \'w\')\n\n    det_handle = open(det_boxs_file, \'rb\')\n\n    det_boxes = cPickle.load(det_handle)\n    print(len(det_boxes), num_of_images)\n    # assert len(det_boxes) == num_of_images, ""incorrect detections or ground truths""\n\n    # index of neg, pos and part face, used as their image names\n    n_idx = 0\n    p_idx = 0\n    d_idx = 0\n    image_done = 0\n    for im_idx, dets, gts in zip(im_idx_list, det_boxes, gt_boxes_list):\n        if image_done % 100 == 0:\n            print(""%d images done"" % image_done)\n        image_done += 1\n\n        if dets.shape[0] == 0:\n            continue\n        img = cv2.imread(im_idx)\n        dets = convert_to_square(dets)\n        dets[:, 0:4] = np.round(dets[:, 0:4])\n\n        for box in dets:\n            x_left, y_top, x_right, y_bottom = box[0:4].astype(int)\n            width = x_right - x_left + 1\n            height = y_bottom - y_top + 1\n\n            # ignore box that is too small or beyond image border\n            if width < 20 or x_left < 0 or y_top < 0 or x_right > img.shape[1] - 1 or y_bottom > img.shape[0] - 1:\n                continue\n\n            # compute intersection over union(IoU) between current box and all gt boxes\n            Iou = IoU(box, gts)\n            cropped_im = img[y_top:y_bottom + 1, x_left:x_right + 1, :]\n            resized_im = cv2.resize(cropped_im, (image_size, image_size),\n                                    interpolation=cv2.INTER_LINEAR)\n\n            # save negative images and write label\n            if np.max(Iou) < 0.3:\n                # Iou with all gts must below 0.3\n                save_file = os.path.join(neg_save_dir, ""%s.jpg"" % n_idx)\n                f2.write(save_file + \' 0\\n\')\n                cv2.imwrite(save_file, resized_im)\n                n_idx += 1\n            else:\n                # find gt_box with the highest iou\n                idx = np.argmax(Iou)\n                assigned_gt = gts[idx]\n                x1, y1, x2, y2 = assigned_gt\n\n                # compute bbox reg label\n                offset_x1 = (x1 - x_left) / float(width)\n                offset_y1 = (y1 - y_top) / float(height)\n                offset_x2 = (x2 - x_right) / float(width)\n                offset_y2 = (y2 - y_bottom) / float(height)\n\n                # save positive and part-face images and write labels\n                if np.max(Iou) >= 0.65:\n                    save_file = os.path.join(pos_save_dir, ""%s.jpg"" % p_idx)\n                    f1.write(save_file + \' 1 %.2f %.2f %.2f %.2f\\n\' % (\n                    offset_x1, offset_y1, offset_x2, offset_y2))\n                    cv2.imwrite(save_file, resized_im)\n                    p_idx += 1\n\n                elif np.max(Iou) >= 0.4:\n                    save_file = os.path.join(part_save_dir, ""%s.jpg"" % d_idx)\n                    f3.write(save_file + \' -1 %.2f %.2f %.2f %.2f\\n\' % (\n                    offset_x1, offset_y1, offset_x2, offset_y2))\n                    cv2.imwrite(save_file, resized_im)\n                    d_idx += 1\n    f1.close()\n    f2.close()\n    f3.close()\n\n\n\ndef model_store_path():\n    return os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))+""/model_store""\n\nif __name__ == \'__main__\':\n\n    gen_onet_data(traindata_store, annotation_file, pnet_model_file, rnet_model_file, prefix_path, use_cuda)\n'"
mtcnn/data_preprocess/gen_Pnet_train_data.py,0,"b'""""""\n    2018-10-20 15:50:20\n    generate positive, negative, positive images whose size are 12*12 and feed into PNet\n""""""\nimport sys\nimport numpy as np\nimport cv2\nimport os\nsys.path.append(os.getcwd())\nimport numpy as np\nfrom mtcnn.data_preprocess.utils import IoU\n\nprefix = \'\'\nanno_file = ""./anno_store/anno_train.txt""\nim_dir = ""./data_set/face_detection/WIDERFACE/WIDER_train/WIDER_train/images""\npos_save_dir = ""./data_set/train/12/positive""\npart_save_dir = ""./data_set/train/12/part""\nneg_save_dir = \'./data_set/train/12/negative\'\n\nif not os.path.exists(pos_save_dir):\n    os.mkdir(pos_save_dir)\nif not os.path.exists(part_save_dir):\n    os.mkdir(part_save_dir)\nif not os.path.exists(neg_save_dir):\n    os.mkdir(neg_save_dir)\n\n# store labels of positive, negative, part images\nf1 = open(os.path.join(\'./anno_store\', \'pos_12.txt\'), \'w\')\nf2 = open(os.path.join(\'./anno_store\', \'neg_12.txt\'), \'w\')\nf3 = open(os.path.join(\'./anno_store\', \'part_12.txt\'), \'w\')\n\n# anno_file: store labels of the wider face training data\nwith open(anno_file, \'r\') as f:\n    annotations = f.readlines()\nnum = len(annotations)\nprint(""%d pics in total"" % num)\n\np_idx = 0 # positive\nn_idx = 0 # negative\nd_idx = 0 # dont care\nidx = 0\nbox_idx = 0\nfor annotation in annotations:\n    annotation = annotation.strip().split(\' \')\n    im_path = os.path.join(prefix, annotation[0])\n    print(im_path)\n    bbox = list(map(float, annotation[1:]))\n    boxes = np.array(bbox, dtype=np.int32).reshape(-1, 4)\n    img = cv2.imread(im_path)\n    idx += 1\n    if idx % 100 == 0:\n        print(idx, ""images done"")\n\n    height, width, channel = img.shape\n\n    neg_num = 0\n    while neg_num < 50:\n        size = np.random.randint(12, min(width, height) / 2)\n        nx = np.random.randint(0, width - size)\n        ny = np.random.randint(0, height - size)\n        crop_box = np.array([nx, ny, nx + size, ny + size])\n\n        Iou = IoU(crop_box, boxes)\n\n        cropped_im = img[ny: ny + size, nx: nx + size, :]\n        resized_im = cv2.resize(cropped_im, (12, 12), interpolation=cv2.INTER_LINEAR)\n\n        if np.max(Iou) < 0.3:\n            # Iou with all gts must below 0.3\n            save_file = os.path.join(neg_save_dir, ""%s.jpg"" % n_idx)\n            f2.write(save_file + \' 0\\n\')\n            cv2.imwrite(save_file, resized_im)\n            n_idx += 1\n            neg_num += 1\n\n    for box in boxes:\n        # box (x_left, y_top, x_right, y_bottom)\n        x1, y1, x2, y2 = box\n        # w = x2 - x1 + 1\n        # h = y2 - y1 + 1\n        w = x2 - x1 + 1\n        h = y2 - y1 + 1\n\n        # ignore small faces\n        # in case the ground truth boxes of small faces are not accurate\n        if max(w, h) < 40 or x1 < 0 or y1 < 0:\n            continue\n\n        # generate negative examples that have overlap with gt\n        for i in range(5):\n            size = np.random.randint(12, min(width, height) / 2)\n            # delta_x and delta_y are offsets of (x1, y1)\n\n            delta_x = np.random.randint(max(-size, -x1), w)\n            delta_y = np.random.randint(max(-size, -y1), h)\n            nx1 = max(0, x1 + delta_x)\n            ny1 = max(0, y1 + delta_y)\n\n            if nx1 + size > width or ny1 + size > height:\n                continue\n            crop_box = np.array([nx1, ny1, nx1 + size, ny1 + size])\n            Iou = IoU(crop_box, boxes)\n\n            cropped_im = img[ny1: ny1 + size, nx1: nx1 + size, :]\n            resized_im = cv2.resize(cropped_im, (12, 12), interpolation=cv2.INTER_LINEAR)\n\n            if np.max(Iou) < 0.3:\n                # Iou with all gts must below 0.3\n                save_file = os.path.join(neg_save_dir, ""%s.jpg"" % n_idx)\n                f2.write(save_file + \' 0\\n\')\n                cv2.imwrite(save_file, resized_im)\n                n_idx += 1\n\n        # generate positive examples and part faces\n        for i in range(20):\n            size = np.random.randint(int(min(w, h) * 0.8), np.ceil(1.25 * max(w, h)))\n\n            # delta here is the offset of box center\n            delta_x = np.random.randint(-w * 0.2, w * 0.2)\n            delta_y = np.random.randint(-h * 0.2, h * 0.2)\n\n            nx1 = max(x1 + w / 2 + delta_x - size / 2, 0)\n            ny1 = max(y1 + h / 2 + delta_y - size / 2, 0)\n            nx2 = nx1 + size\n            ny2 = ny1 + size\n\n            if nx2 > width or ny2 > height:\n                continue\n            crop_box = np.array([nx1, ny1, nx2, ny2])\n\n            offset_x1 = (x1 - nx1) / float(size)\n            offset_y1 = (y1 - ny1) / float(size)\n            offset_x2 = (x2 - nx2) / float(size)\n            offset_y2 = (y2 - ny2) / float(size)\n\n            cropped_im = img[int(ny1): int(ny2), int(nx1): int(nx2), :]\n            resized_im = cv2.resize(cropped_im, (12, 12), interpolation=cv2.INTER_LINEAR)\n\n            box_ = box.reshape(1, -1)\n            if IoU(crop_box, box_) >= 0.65:\n                save_file = os.path.join(pos_save_dir, ""%s.jpg"" % p_idx)\n                f1.write(save_file + \' 1 %.2f %.2f %.2f %.2f\\n\' % (offset_x1, offset_y1, offset_x2, offset_y2))\n                cv2.imwrite(save_file, resized_im)\n                p_idx += 1\n            elif IoU(crop_box, box_) >= 0.4:\n                save_file = os.path.join(part_save_dir, ""%s.jpg"" % d_idx)\n                f3.write(save_file + \' -1 %.2f %.2f %.2f %.2f\\n\' % (offset_x1, offset_y1, offset_x2, offset_y2))\n                cv2.imwrite(save_file, resized_im)\n                d_idx += 1\n        box_idx += 1\n        print(""%s images done, pos: %s part: %s neg: %s"" % (idx, p_idx, d_idx, n_idx))\n\nf1.close()\nf2.close()\nf3.close()\n'"
mtcnn/data_preprocess/gen_Rnet_train_data.py,0,"b'import argparse\n\n\nimport cv2\nimport numpy as np\nimport sys\nimport os\nsys.path.append(os.getcwd())\nfrom mtcnn.core.detect import MtcnnDetector,create_mtcnn_net\nfrom mtcnn.core.imagedb import ImageDB\nfrom mtcnn.core.image_reader import TestImageLoader\nimport time\n\nfrom six.moves import cPickle\nfrom mtcnn.core.utils import convert_to_square,IoU\nimport mtcnn.config as config\nimport mtcnn.core.vision as vision\n\nprefix_path = \'\'\ntraindata_store = \'./data_set/train\'\npnet_model_file = \'./model_store/pnet_epoch.pt\'\n\nannotation_file = \'./anno_store/anno_train_test.txt\'\nuse_cuda = True\n\ndef gen_rnet_data(data_dir, anno_file, pnet_model_file, prefix_path=\'\', use_cuda=True, vis=False):\n\n    """"""\n    :param data_dir: train data\n    :param anno_file:\n    :param pnet_model_file:\n    :param prefix_path:\n    :param use_cuda:\n    :param vis:\n    :return:\n    """"""\n\n\n    # load trained pnet model\n    pnet, _, _ = create_mtcnn_net(p_model_path=pnet_model_file, use_cuda=use_cuda)\n    mtcnn_detector = MtcnnDetector(pnet=pnet,min_face_size=12)\n\n    # load original_anno_file, length = 12880\n    imagedb = ImageDB(anno_file,mode=""test"",prefix_path=prefix_path)\n    imdb = imagedb.load_imdb()\n    image_reader = TestImageLoader(imdb,1,False)\n\n    all_boxes = list()\n    batch_idx = 0\n\n    print(\'size:%d\' %image_reader.size)\n    for databatch in image_reader:\n        if batch_idx % 100 == 0:\n            print (""%d images done"" % batch_idx)\n        im = databatch\n\n        t = time.time()\n\n        # obtain boxes and aligned boxes\n        boxes, boxes_align = mtcnn_detector.detect_pnet(im=im)\n        if boxes_align is None:\n            all_boxes.append(np.array([]))\n            batch_idx += 1\n            continue\n        if vis:\n            rgb_im = cv2.cvtColor(np.asarray(im), cv2.COLOR_BGR2RGB)\n            vision.vis_two(rgb_im, boxes, boxes_align)\n\n        t1 = time.time() - t\n        t = time.time()\n        all_boxes.append(boxes_align)\n        batch_idx += 1\n        # if batch_idx == 100:\n            # break\n        # print(""shape of all boxes {0}"".format(all_boxes))\n        # time.sleep(5)\n\n    # save_path = model_store_path()\n    # \'./model_store\'\n    save_path = \'./model_store\'\n\n    if not os.path.exists(save_path):\n        os.mkdir(save_path)\n\n    save_file = os.path.join(save_path, ""detections_%d.pkl"" % int(time.time()))\n    with open(save_file, \'wb\') as f:\n        cPickle.dump(all_boxes, f, cPickle.HIGHEST_PROTOCOL)\n\n\n    gen_rnet_sample_data(data_dir, anno_file, save_file, prefix_path)\n\n\n\ndef gen_rnet_sample_data(data_dir, anno_file, det_boxs_file, prefix_path):\n\n    """"""\n    :param data_dir:\n    :param anno_file: original annotations file of wider face data\n    :param det_boxs_file: detection boxes file\n    :param prefix_path:\n    :return:\n    """"""\n\n    neg_save_dir = os.path.join(data_dir, ""24/negative"")\n    pos_save_dir = os.path.join(data_dir, ""24/positive"")\n    part_save_dir = os.path.join(data_dir, ""24/part"")\n\n\n    for dir_path in [neg_save_dir, pos_save_dir, part_save_dir]:\n        # print(dir_path)\n        if not os.path.exists(dir_path):\n            os.makedirs(dir_path)\n\n\n    # load ground truth from annotation file\n    # format of each line: image/path [x1,y1,x2,y2] for each gt_box in this image\n\n    with open(anno_file, \'r\') as f:\n        annotations = f.readlines()\n\n    image_size = 24\n    net = ""rnet""\n\n    im_idx_list = list()\n    gt_boxes_list = list()\n    num_of_images = len(annotations)\n    print (""processing %d images in total"" % num_of_images)\n\n    for annotation in annotations:\n        annotation = annotation.strip().split(\' \')\n        im_idx = os.path.join(prefix_path,annotation[0])\n        # im_idx = annotation[0]\n\n        boxes = list(map(float, annotation[1:]))\n        boxes = np.array(boxes, dtype=np.float32).reshape(-1, 4)\n        im_idx_list.append(im_idx)\n        gt_boxes_list.append(boxes)\n\n\n    # \'./anno_store\'\n    save_path = \'./anno_store\'\n    if not os.path.exists(save_path):\n        os.makedirs(save_path)\n\n    f1 = open(os.path.join(save_path, \'pos_%d.txt\' % image_size), \'w\')\n    f2 = open(os.path.join(save_path, \'neg_%d.txt\' % image_size), \'w\')\n    f3 = open(os.path.join(save_path, \'part_%d.txt\' % image_size), \'w\')\n\n    # print(det_boxs_file)\n    det_handle = open(det_boxs_file, \'rb\')\n\n    det_boxes = cPickle.load(det_handle)\n\n    # an image contain many boxes stored in an array\n    print(len(det_boxes), num_of_images)\n    # assert len(det_boxes) == num_of_images, ""incorrect detections or ground truths""\n\n    # index of neg, pos and part face, used as their image names\n    n_idx = 0\n    p_idx = 0\n    d_idx = 0\n    image_done = 0\n    for im_idx, dets, gts in zip(im_idx_list, det_boxes, gt_boxes_list):\n\n        # if (im_idx+1) == 100:\n            # break\n\n        gts = np.array(gts, dtype=np.float32).reshape(-1, 4)\n        if image_done % 100 == 0:\n            print(""%d images done"" % image_done)\n        image_done += 1\n\n        if dets.shape[0] == 0:\n            continue\n        img = cv2.imread(im_idx)\n        # change to square\n        dets = convert_to_square(dets)\n        dets[:, 0:4] = np.round(dets[:, 0:4])\n        neg_num = 0\n        for box in dets:\n            x_left, y_top, x_right, y_bottom, _ = box.astype(int)\n            width = x_right - x_left + 1\n            height = y_bottom - y_top + 1\n\n            # ignore box that is too small or beyond image border\n            if width < 20 or x_left < 0 or y_top < 0 or x_right > img.shape[1] - 1 or y_bottom > img.shape[0] - 1:\n                continue\n\n            # compute intersection over union(IoU) between current box and all gt boxes\n            Iou = IoU(box, gts)\n            cropped_im = img[y_top:y_bottom + 1, x_left:x_right + 1, :]\n            resized_im = cv2.resize(cropped_im, (image_size, image_size),\n                                    interpolation=cv2.INTER_LINEAR)\n\n            # save negative images and write label\n            # Iou with all gts must below 0.3\n            if np.max(Iou) < 0.3 and neg_num < 60:\n                # save the examples\n                save_file = os.path.join(neg_save_dir, ""%s.jpg"" % n_idx)\n                # print(save_file)\n                f2.write(save_file + \' 0\\n\')\n                cv2.imwrite(save_file, resized_im)\n                n_idx += 1\n                neg_num += 1\n            else:\n                # find gt_box with the highest iou\n                idx = np.argmax(Iou)\n                assigned_gt = gts[idx]\n                x1, y1, x2, y2 = assigned_gt\n\n                # compute bbox reg label\n                offset_x1 = (x1 - x_left) / float(width)\n                offset_y1 = (y1 - y_top) / float(height)\n                offset_x2 = (x2 - x_right) / float(width)\n                offset_y2 = (y2 - y_bottom) / float(height)\n\n                # save positive and part-face images and write labels\n                if np.max(Iou) >= 0.65:\n                    save_file = os.path.join(pos_save_dir, ""%s.jpg"" % p_idx)\n                    f1.write(save_file + \' 1 %.2f %.2f %.2f %.2f\\n\' % (\n                        offset_x1, offset_y1, offset_x2, offset_y2))\n                    cv2.imwrite(save_file, resized_im)\n                    p_idx += 1\n\n                elif np.max(Iou) >= 0.4:\n                    save_file = os.path.join(part_save_dir, ""%s.jpg"" % d_idx)\n                    f3.write(save_file + \' -1 %.2f %.2f %.2f %.2f\\n\' % (\n                        offset_x1, offset_y1, offset_x2, offset_y2))\n                    cv2.imwrite(save_file, resized_im)\n                    d_idx += 1\n    f1.close()\n    f2.close()\n    f3.close()\n\ndef model_store_path():\n    return os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))+""/model_store""\n\nif __name__ == \'__main__\':\n\n    gen_rnet_data(traindata_store, annotation_file, pnet_model_file, prefix_path, use_cuda)\n'"
mtcnn/data_preprocess/gen_landmark_48.py,0,"b'# coding: utf-8\nimport os\nimport cv2\nimport random\nimport sys\nsys.path.append(os.getcwd())\nimport numpy as np\nimport mtcnn.core.utils as utils\nimport time\n\nprefix_path = \'\'\ntraindata_store = \'./data_set/train\'\nannotation_file = ""./data_set/face_landmark/CNN_FacePoint/train/trainImageList.txt""\n\ndef gen_data(anno_file, data_dir, prefix):\n\n\n    size = 48\n    image_id = 0\n\n    landmark_imgs_save_dir = os.path.join(data_dir,""48/landmark"")\n    if not os.path.exists(landmark_imgs_save_dir):\n        os.makedirs(landmark_imgs_save_dir)\n\n    anno_dir = \'./anno_store\'\n    if not os.path.exists(anno_dir):\n        os.makedirs(anno_dir)\n\n    landmark_anno_filename = ""landmark_48.txt""\n    save_landmark_anno = os.path.join(anno_dir,landmark_anno_filename)\n\n    # print(save_landmark_anno)\n    # time.sleep(5)\n    f = open(save_landmark_anno, \'w\')\n    # dstdir = ""train_landmark_few""\n\n    with open(anno_file, \'r\') as f2:\n        annotations = f2.readlines()\n\n    num = len(annotations)\n    print(""%d total images"" % num)\n\n    l_idx =0\n    idx = 0\n    # image_path bbox landmark(5*2)\n    for annotation in annotations:\n        # print imgPath\n\n        annotation = annotation.strip().split(\' \')\n\n        assert len(annotation)==15,""each line should have 15 element""\n\n        im_path = os.path.join(\'./data_set/face_landmark/CNN_FacePoint/train/\',annotation[0].replace(""\\\\"", ""/""))\n\n        gt_box = list(map(float, annotation[1:5]))\n        # gt_box = [gt_box[0], gt_box[2], gt_box[1], gt_box[3]]\n\n\n        gt_box = np.array(gt_box, dtype=np.int32)\n\n        landmark = list(map(float, annotation[5:]))\n        landmark = np.array(landmark, dtype=np.float)\n\n        img = cv2.imread(im_path)\n        # print(im_path)\n        assert (img is not None)\n\n        height, width, channel = img.shape\n        # crop_face = img[gt_box[1]:gt_box[3]+1, gt_box[0]:gt_box[2]+1]\n        # crop_face = cv2.resize(crop_face,(size,size))\n\n        idx = idx + 1\n        if idx % 100 == 0:\n            print(""%d images done, landmark images: %d""%(idx,l_idx))\n        # print(im_path)\n        # print(gt_box)\n        x1, x2, y1, y2 = gt_box\n        gt_box[1] = y1\n        gt_box[2] = x2\n        # time.sleep(5)\n\n        # gt\'s width\n        w = x2 - x1 + 1\n        # gt\'s height\n        h = y2 - y1 + 1\n        if max(w, h) < 40 or x1 < 0 or y1 < 0:\n            continue\n        # random shift\n        for i in range(10):\n            bbox_size = np.random.randint(int(min(w, h) * 0.8), np.ceil(1.25 * max(w, h)))\n            delta_x = np.random.randint(-w * 0.2, w * 0.2)\n            delta_y = np.random.randint(-h * 0.2, h * 0.2)\n            nx1 = max(x1 + w / 2 - bbox_size / 2 + delta_x, 0)\n            ny1 = max(y1 + h / 2 - bbox_size / 2 + delta_y, 0)\n\n            nx2 = nx1 + bbox_size\n            ny2 = ny1 + bbox_size\n            if nx2 > width or ny2 > height:\n                continue\n            crop_box = np.array([nx1, ny1, nx2, ny2])\n            cropped_im = img[int(ny1):int(ny2) + 1, int(nx1):int(nx2) + 1, :]\n            resized_im = cv2.resize(cropped_im, (size, size),interpolation=cv2.INTER_LINEAR)\n\n            offset_x1 = (x1 - nx1) / float(bbox_size)\n            offset_y1 = (y1 - ny1) / float(bbox_size)\n            offset_x2 = (x2 - nx2) / float(bbox_size)\n            offset_y2 = (y2 - ny2) / float(bbox_size)\n\n            offset_left_eye_x = (landmark[0] - nx1) / float(bbox_size)\n            offset_left_eye_y = (landmark[1] - ny1) / float(bbox_size)\n\n            offset_right_eye_x = (landmark[2] - nx1) / float(bbox_size)\n            offset_right_eye_y = (landmark[3] - ny1) / float(bbox_size)\n\n            offset_nose_x = (landmark[4] - nx1) / float(bbox_size)\n            offset_nose_y = (landmark[5] - ny1) / float(bbox_size)\n\n            offset_left_mouth_x = (landmark[6] - nx1) / float(bbox_size)\n            offset_left_mouth_y = (landmark[7] - ny1) / float(bbox_size)\n\n            offset_right_mouth_x = (landmark[8] - nx1) / float(bbox_size)\n            offset_right_mouth_y = (landmark[9] - ny1) / float(bbox_size)\n\n\n            # cal iou\n            iou = utils.IoU(crop_box.astype(np.float), np.expand_dims(gt_box.astype(np.float), 0))\n            # print(iou)\n            if iou > 0.65:\n                save_file = os.path.join(landmark_imgs_save_dir, ""%s.jpg"" % l_idx)\n                cv2.imwrite(save_file, resized_im)\n\n                f.write(save_file + \' -2 %.2f %.2f %.2f %.2f %.2f %.2f %.2f %.2f %.2f %.2f %.2f %.2f %.2f %.2f \\n\' % \\\n                (offset_x1, offset_y1, offset_x2, offset_y2, \\\n                offset_left_eye_x,offset_left_eye_y,offset_right_eye_x,offset_right_eye_y,offset_nose_x,offset_nose_y,offset_left_mouth_x,offset_left_mouth_y,offset_right_mouth_x,offset_right_mouth_y))\n                # print(save_file)\n                # print(save_landmark_anno)\n                l_idx += 1\n\n    f.close()\n\n\nif __name__ == \'__main__\':\n\n    gen_data(annotation_file, traindata_store, prefix_path)\n\n\n'"
mtcnn/data_preprocess/utils.py,0,"b'import numpy as np\n\ndef IoU(box, boxes):\n    """"""Compute IoU between detect box and gt boxes\n\n    Parameters:\n    ----------\n    box: numpy array , shape (5, ): x1, y1, x2, y2, score\n        input box\n    boxes: numpy array, shape (n, 4): x1, y1, x2, y2\n        input ground truth boxes\n\n    Returns:\n    -------\n    ovr: numpy.array, shape (n, )\n        IoU\n    """"""\n    # box = (x1, y1, x2, y2)\n    box_area = (box[2] - box[0] + 1) * (box[3] - box[1] + 1)\n    area = (boxes[:, 2] - boxes[:, 0] + 1) * (boxes[:, 3] - boxes[:, 1] + 1)\n    \n    # abtain the offset of the interception of union between crop_box and gt_box\n    xx1 = np.maximum(box[0], boxes[:, 0])\n    yy1 = np.maximum(box[1], boxes[:, 1])\n    xx2 = np.minimum(box[2], boxes[:, 2])\n    yy2 = np.minimum(box[3], boxes[:, 3])\n\n    # compute the width and height of the bounding box\n    w = np.maximum(0, xx2 - xx1 + 1)\n    h = np.maximum(0, yy2 - yy1 + 1)\n\n    inter = w * h\n    ovr = inter / (box_area + area - inter)\n    return ovr\n\n\ndef convert_to_square(bbox):\n    """"""Convert bbox to square\n\n    Parameters:\n    ----------\n    bbox: numpy array , shape n x 5\n        input bbox\n\n    Returns:\n    -------\n    square bbox\n    """"""\n    square_bbox = bbox.copy()\n\n    h = bbox[:, 3] - bbox[:, 1] + 1\n    w = bbox[:, 2] - bbox[:, 0] + 1\n    max_side = np.maximum(h,w)\n    square_bbox[:, 0] = bbox[:, 0] + w*0.5 - max_side*0.5\n    square_bbox[:, 1] = bbox[:, 1] + h*0.5 - max_side*0.5\n    square_bbox[:, 2] = square_bbox[:, 0] + max_side - 1\n    square_bbox[:, 3] = square_bbox[:, 1] + max_side - 1\n    return square_bbox\n'"
mtcnn/train_net/__init__.py,0,b''
mtcnn/train_net/train.py,30,"b'from mtcnn.core.image_reader import TrainImageReader\nimport datetime\nimport os\nfrom mtcnn.core.models import PNet,RNet,ONet,LossFn\nimport torch\nfrom torch.autograd import Variable\nimport mtcnn.core.image_tools as image_tools\nimport numpy as np\n\n\ndef compute_accuracy(prob_cls, gt_cls):\n\n    prob_cls = torch.squeeze(prob_cls)\n    gt_cls = torch.squeeze(gt_cls)\n\n    #we only need the detection which >= 0\n    mask = torch.ge(gt_cls,0)\n    #get valid element\n    valid_gt_cls = torch.masked_select(gt_cls,mask)\n    valid_prob_cls = torch.masked_select(prob_cls,mask)\n    size = min(valid_gt_cls.size()[0], valid_prob_cls.size()[0])\n    prob_ones = torch.ge(valid_prob_cls,0.6).float()\n    right_ones = torch.eq(prob_ones,valid_gt_cls).float()\n\n    ## if size == 0 meaning that your gt_labels are all negative, landmark or part\n\n    return torch.div(torch.mul(torch.sum(right_ones),float(1.0)),float(size))  ## divided by zero meaning that your gt_labels are all negative, landmark or part\n\n\ndef train_pnet(model_store_path, end_epoch,imdb,\n              batch_size,frequent=10,base_lr=0.01,use_cuda=True):\n\n    if not os.path.exists(model_store_path):\n        os.makedirs(model_store_path)\n\n    lossfn = LossFn()\n    net = PNet(is_train=True, use_cuda=use_cuda)\n    net.train()\n\n    if use_cuda:\n        net.cuda()\n    optimizer = torch.optim.Adam(net.parameters(), lr=base_lr)\n\n    train_data=TrainImageReader(imdb,12,batch_size,shuffle=True)\n\n    frequent = 10\n    for cur_epoch in range(1,end_epoch+1):\n        train_data.reset() # shuffle\n\n        for batch_idx,(image,(gt_label,gt_bbox,gt_landmark))in enumerate(train_data):\n\n            im_tensor = [ image_tools.convert_image_to_tensor(image[i,:,:,:]) for i in range(image.shape[0]) ]\n            im_tensor = torch.stack(im_tensor)\n\n            im_tensor = Variable(im_tensor)\n            gt_label = Variable(torch.from_numpy(gt_label).float())\n\n            gt_bbox = Variable(torch.from_numpy(gt_bbox).float())\n            # gt_landmark = Variable(torch.from_numpy(gt_landmark).float())\n\n            if use_cuda:\n                im_tensor = im_tensor.cuda()\n                gt_label = gt_label.cuda()\n                gt_bbox = gt_bbox.cuda()\n                # gt_landmark = gt_landmark.cuda()\n\n            cls_pred, box_offset_pred = net(im_tensor)\n            # all_loss, cls_loss, offset_loss = lossfn.loss(gt_label=label_y,gt_offset=bbox_y, pred_label=cls_pred, pred_offset=box_offset_pred)\n\n            cls_loss = lossfn.cls_loss(gt_label,cls_pred)\n            box_offset_loss = lossfn.box_loss(gt_label,gt_bbox,box_offset_pred)\n            # landmark_loss = lossfn.landmark_loss(gt_label,gt_landmark,landmark_offset_pred)\n\n            all_loss = cls_loss*1.0+box_offset_loss*0.5\n\n            if batch_idx %frequent==0:\n                accuracy=compute_accuracy(cls_pred,gt_label)\n\n                show1 = accuracy.data.cpu().numpy()\n                show2 = cls_loss.data.cpu().numpy()\n                show3 = box_offset_loss.data.cpu().numpy()\n                # show4 = landmark_loss.data.cpu().numpy()\n                show5 = all_loss.data.cpu().numpy()\n\n                print(""%s : Epoch: %d, Step: %d, accuracy: %s, det loss: %s, bbox loss: %s, all_loss: %s, lr:%s ""%(datetime.datetime.now(),cur_epoch,batch_idx, show1,show2,show3,show5,base_lr))\n\n            optimizer.zero_grad()\n            all_loss.backward()\n            optimizer.step()\n\n        torch.save(net.state_dict(), os.path.join(model_store_path,""pnet_epoch_%d.pt"" % cur_epoch))\n        torch.save(net, os.path.join(model_store_path,""pnet_epoch_model_%d.pkl"" % cur_epoch))\n\n\n\n\ndef train_rnet(model_store_path, end_epoch,imdb,\n              batch_size,frequent=50,base_lr=0.01,use_cuda=True):\n\n    if not os.path.exists(model_store_path):\n        os.makedirs(model_store_path)\n\n    lossfn = LossFn()\n    net = RNet(is_train=True, use_cuda=use_cuda)\n    net.train()\n    if use_cuda:\n        net.cuda()\n\n    optimizer = torch.optim.Adam(net.parameters(), lr=base_lr)\n\n    train_data=TrainImageReader(imdb,24,batch_size,shuffle=True)\n\n\n    for cur_epoch in range(1,end_epoch+1):\n        train_data.reset()\n\n        for batch_idx,(image,(gt_label,gt_bbox,gt_landmark))in enumerate(train_data):\n\n            im_tensor = [ image_tools.convert_image_to_tensor(image[i,:,:,:]) for i in range(image.shape[0]) ]\n            im_tensor = torch.stack(im_tensor)\n\n            im_tensor = Variable(im_tensor)\n            gt_label = Variable(torch.from_numpy(gt_label).float())\n\n            gt_bbox = Variable(torch.from_numpy(gt_bbox).float())\n            gt_landmark = Variable(torch.from_numpy(gt_landmark).float())\n\n            if use_cuda:\n                im_tensor = im_tensor.cuda()\n                gt_label = gt_label.cuda()\n                gt_bbox = gt_bbox.cuda()\n                gt_landmark = gt_landmark.cuda()\n\n            cls_pred, box_offset_pred = net(im_tensor)\n            # all_loss, cls_loss, offset_loss = lossfn.loss(gt_label=label_y,gt_offset=bbox_y, pred_label=cls_pred, pred_offset=box_offset_pred)\n\n            cls_loss = lossfn.cls_loss(gt_label,cls_pred)\n            box_offset_loss = lossfn.box_loss(gt_label,gt_bbox,box_offset_pred)\n            # landmark_loss = lossfn.landmark_loss(gt_label,gt_landmark,landmark_offset_pred)\n\n            all_loss = cls_loss*1.0+box_offset_loss*0.5\n\n            if batch_idx%frequent==0:\n                accuracy=compute_accuracy(cls_pred,gt_label)\n\n                show1 = accuracy.data.cpu().numpy()\n                show2 = cls_loss.data.cpu().numpy()\n                show3 = box_offset_loss.data.cpu().numpy()\n                # show4 = landmark_loss.data.cpu().numpy()\n                show5 = all_loss.data.cpu().numpy()\n\n                print(""%s : Epoch: %d, Step: %d, accuracy: %s, det loss: %s, bbox loss: %s, all_loss: %s, lr:%s ""%(datetime.datetime.now(), cur_epoch, batch_idx, show1, show2, show3, show5, base_lr))\n\n            optimizer.zero_grad()\n            all_loss.backward()\n            optimizer.step()\n\n        torch.save(net.state_dict(), os.path.join(model_store_path,""rnet_epoch_%d.pt"" % cur_epoch))\n        torch.save(net, os.path.join(model_store_path,""rnet_epoch_model_%d.pkl"" % cur_epoch))\n\n\ndef train_onet(model_store_path, end_epoch,imdb,\n              batch_size,frequent=50,base_lr=0.01,use_cuda=True):\n\n    if not os.path.exists(model_store_path):\n        os.makedirs(model_store_path)\n\n    lossfn = LossFn()\n    net = ONet(is_train=True)\n    net.train()\n    print(use_cuda)\n    if use_cuda:\n        net.cuda()\n\n    optimizer = torch.optim.Adam(net.parameters(), lr=base_lr)\n\n    train_data=TrainImageReader(imdb,48,batch_size,shuffle=True)\n\n\n    for cur_epoch in range(1,end_epoch+1):\n\n        train_data.reset()\n\n        for batch_idx,(image,(gt_label,gt_bbox,gt_landmark))in enumerate(train_data):\n            # print(""batch id {0}"".format(batch_idx))\n            im_tensor = [ image_tools.convert_image_to_tensor(image[i,:,:,:]) for i in range(image.shape[0]) ]\n            im_tensor = torch.stack(im_tensor)\n\n            im_tensor = Variable(im_tensor)\n            gt_label = Variable(torch.from_numpy(gt_label).float())\n\n            gt_bbox = Variable(torch.from_numpy(gt_bbox).float())\n            gt_landmark = Variable(torch.from_numpy(gt_landmark).float())\n\n            if use_cuda:\n                im_tensor = im_tensor.cuda()\n                gt_label = gt_label.cuda()\n                gt_bbox = gt_bbox.cuda()\n                gt_landmark = gt_landmark.cuda()\n\n            cls_pred, box_offset_pred, landmark_offset_pred = net(im_tensor)\n\n            # all_loss, cls_loss, offset_loss = lossfn.loss(gt_label=label_y,gt_offset=bbox_y, pred_label=cls_pred, pred_offset=box_offset_pred)\n\n            cls_loss = lossfn.cls_loss(gt_label,cls_pred)\n            box_offset_loss = lossfn.box_loss(gt_label,gt_bbox,box_offset_pred)\n            landmark_loss = lossfn.landmark_loss(gt_label,gt_landmark,landmark_offset_pred)\n\n            all_loss = cls_loss*0.8+box_offset_loss*0.6+landmark_loss*1.5\n\n            if batch_idx%frequent==0:\n                accuracy=compute_accuracy(cls_pred,gt_label)\n\n                show1 = accuracy.data.cpu().numpy()\n                show2 = cls_loss.data.cpu().numpy()\n                show3 = box_offset_loss.data.cpu().numpy()\n                show4 = landmark_loss.data.cpu().numpy()\n                show5 = all_loss.data.cpu().numpy()\n\n                print(""%s : Epoch: %d, Step: %d, accuracy: %s, det loss: %s, bbox loss: %s, landmark loss: %s, all_loss: %s, lr:%s ""%(datetime.datetime.now(),cur_epoch,batch_idx, show1,show2,show3,show4,show5,base_lr))\n\n            optimizer.zero_grad()\n            all_loss.backward()\n            optimizer.step()\n\n        torch.save(net.state_dict(), os.path.join(model_store_path,""onet_epoch_%d.pt"" % cur_epoch))\n        torch.save(net, os.path.join(model_store_path,""onet_epoch_model_%d.pkl"" % cur_epoch))\n'"
mtcnn/train_net/train_o_net.py,0,"b'import os\nimport sys\nsys.path.append(os.getcwd())\nfrom mtcnn.core.imagedb import ImageDB\nimport mtcnn.train_net.train as train\nimport mtcnn.config as config\n\n\n\ndef train_net(annotation_file, model_store_path,\n                end_epoch=16, frequent=200, lr=0.01, batch_size=128, use_cuda=True):\n\n    imagedb = ImageDB(annotation_file)\n    gt_imdb = imagedb.load_imdb()\n    gt_imdb = imagedb.append_flipped_images(gt_imdb)\n\n    train.train_onet(model_store_path=model_store_path, end_epoch=end_epoch, imdb=gt_imdb, batch_size=batch_size, frequent=frequent, base_lr=lr, use_cuda=use_cuda)\n\nif __name__ == \'__main__\':\n\n    print(\'train ONet argument:\')\n\n    annotation_file = ""./anno_store/imglist_anno_48.txt""\n    model_store_path = ""./model_store""\n    end_epoch = 10\n    lr = 0.001\n    batch_size = 64\n\n    use_cuda = True\n    frequent = 50\n\n\n    train_net(annotation_file, model_store_path,\n                end_epoch, frequent, lr, batch_size, use_cuda)\n'"
mtcnn/train_net/train_p_net.py,0,"b""import argparse\nimport sys\nimport os\nsys.path.append(os.getcwd())\nfrom mtcnn.core.imagedb import ImageDB\nfrom mtcnn.train_net.train import train_pnet\nimport mtcnn.config as config\n\nannotation_file = './anno_store/imglist_anno_12.txt'\nmodel_store_path = './model_store'\nend_epoch = 10\nfrequent = 200\nlr = 0.01\nbatch_size = 512\nuse_cuda = True\n\n\ndef train_net(annotation_file, model_store_path,\n                end_epoch=16, frequent=200, lr=0.01, batch_size=128, use_cuda=False):\n\n    imagedb = ImageDB(annotation_file)\n    gt_imdb = imagedb.load_imdb()\n    gt_imdb = imagedb.append_flipped_images(gt_imdb)\n    train_pnet(model_store_path=model_store_path, end_epoch=end_epoch, imdb=gt_imdb, batch_size=batch_size, frequent=frequent, base_lr=lr, use_cuda=use_cuda)\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Train PNet',\n                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n\n    parser.add_argument('--anno_file', dest='annotation_file',\n                        default=os.path.join(config.ANNO_STORE_DIR,config.PNET_TRAIN_IMGLIST_FILENAME), help='training data annotation file', type=str)\n    parser.add_argument('--model_path', dest='model_store_path', help='training model store directory',\n                        default=config.MODEL_STORE_DIR, type=str)\n    parser.add_argument('--end_epoch', dest='end_epoch', help='end epoch of training',\n                        default=config.END_EPOCH, type=int)\n    parser.add_argument('--frequent', dest='frequent', help='frequency of logging',\n                        default=200, type=int)\n    parser.add_argument('--lr', dest='lr', help='learning rate',\n                        default=config.TRAIN_LR, type=float)\n    parser.add_argument('--batch_size', dest='batch_size', help='train batch size',\n                        default=config.TRAIN_BATCH_SIZE, type=int)\n    parser.add_argument('--gpu', dest='use_cuda', help='train with gpu',\n                        default=config.USE_CUDA, type=bool)\n    parser.add_argument('--prefix_path', dest='', help='training data annotation images prefix root path', type=str)\n\n    args = parser.parse_args()\n    return args\n\nif __name__ == '__main__':\n    # args = parse_args()\n    print('train Pnet argument:')\n    # print(args)\n\n\n\n    train_net(annotation_file, model_store_path,\n                end_epoch, frequent, lr, batch_size, use_cuda)\n\n    # train_net(annotation_file=args.annotation_file, model_store_path=args.model_store_path,\n    #             end_epoch=args.end_epoch, frequent=args.frequent, lr=args.lr, batch_size=args.batch_size, use_cuda=args.use_cuda)\n"""
mtcnn/train_net/train_r_net.py,0,"b'import argparse\nimport sys\nimport os\nsys.path.append(os.getcwd())\nfrom mtcnn.core.imagedb import ImageDB\nimport mtcnn.train_net.train as train\nimport mtcnn.config as config\n\n\n\n\ndef train_net(annotation_file, model_store_path,\n                end_epoch=16, frequent=200, lr=0.01, batch_size=128, use_cuda=False):\n\n    imagedb = ImageDB(annotation_file)\n    print(imagedb.num_images)\n    gt_imdb = imagedb.load_imdb()\n    gt_imdb = imagedb.append_flipped_images(gt_imdb)\n\n    train.train_rnet(model_store_path=model_store_path, end_epoch=end_epoch, imdb=gt_imdb, batch_size=batch_size, frequent=frequent, base_lr=lr, use_cuda=use_cuda)\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'Train  RNet\',\n                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n\n    parser.add_argument(\'--anno_file\', dest=\'annotation_file\',\n                        default=os.path.join(config.ANNO_STORE_DIR,config.RNET_TRAIN_IMGLIST_FILENAME), help=\'training data annotation file\', type=str)\n    parser.add_argument(\'--model_path\', dest=\'model_store_path\', help=\'training model store directory\',\n                        default=config.MODEL_STORE_DIR, type=str)\n    parser.add_argument(\'--end_epoch\', dest=\'end_epoch\', help=\'end epoch of training\',\n                        default=config.END_EPOCH, type=int)\n    parser.add_argument(\'--frequent\', dest=\'frequent\', help=\'frequency of logging\',\n                        default=200, type=int)\n    parser.add_argument(\'--lr\', dest=\'lr\', help=\'learning rate\',\n                        default=config.TRAIN_LR, type=float)\n    parser.add_argument(\'--batch_size\', dest=\'batch_size\', help=\'train batch size\',\n                        default=config.TRAIN_BATCH_SIZE, type=int)\n    parser.add_argument(\'--gpu\', dest=\'use_cuda\', help=\'train with gpu\',\n                        default=config.USE_CUDA, type=bool)\n    parser.add_argument(\'--prefix_path\', dest=\'\', help=\'training data annotation images prefix root path\', type=str)\n\n    args = parser.parse_args()\n    return args\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    print(\'train Rnet argument:\')\n    print(args)\n\n    annotation_file = ""./anno_store/imglist_anno_24.txt""\n    model_store_path = ""./model_store""\n    end_epoch = 10\n    lr = 0.01\n    batch_size = 32\n\n    use_cuda = True\n    frequent = 200\n\n    train_net(annotation_file, model_store_path, end_epoch, frequent, lr, batch_size, use_cuda)\n\n    # train_net(annotation_file=args.annotation_file, model_store_path=args.model_store_path,\n    #             end_epoch=args.end_epoch, frequent=args.frequent, lr=args.lr, batch_size=args.batch_size, use_cuda=args.use_cuda)\n'"
anno_store/tool/format/transform.py,0,"b'import os\nimport sys\nsys.path.append(os.getcwd())\nfrom wider_loader import WIDER\nimport cv2\nimport time\n\n""""""\n modify .mat to .txt \n""""""\n\n#wider face original images path\npath_to_image = \'./data_set/face_detection/WIDERFACE/WIDER_train/WIDER_train/images\'\n\n#matlab file path\nfile_to_label = \'./data_set/face_detection/WIDERFACE/wider_face_split/wider_face_split/wider_face_train.mat\'\n\n#target file path\ntarget_file = \'./anno_store/anno_train.txt\'\n\nwider = WIDER(file_to_label, path_to_image)\n\nline_count = 0\nbox_count = 0\n\nprint(\'start transforming....\')\nt = time.time()\n\nwith open(target_file, \'w+\') as f:\n    # press ctrl-C to stop the process\n    for data in wider.next():\n        line = []\n        line.append(str(data.image_name))\n        line_count += 1\n        for i,box in enumerate(data.bboxes):\n            box_count += 1\n            for j,bvalue in enumerate(box):\n                line.append(str(bvalue))\n\n        line.append(\'\\n\')\n\n        line_str = \' \'.join(line)\n        f.write(line_str)\n\nst = time.time()-t\nprint(\'end transforming\')\n\nprint(\'spend time:%d\'%st)\nprint(\'total line(images):%d\'%line_count)\nprint(\'total boxes(faces):%d\'%box_count)\n'"
anno_store/tool/format/wider_loader.py,0,"b'import os\nfrom scipy.io import loadmat\n\nclass DATA:\n    def __init__(self, image_name, bboxes):\n        self.image_name = image_name\n        self.bboxes = bboxes\n\n\nclass WIDER(object):\n    def __init__(self, file_to_label, path_to_image=None):\n        self.file_to_label = file_to_label\n        self.path_to_image = path_to_image\n\n        self.f = loadmat(file_to_label)\n        self.event_list = self.f[\'event_list\']\n        self.file_list = self.f[\'file_list\']\n        self.face_bbx_list = self.f[\'face_bbx_list\']\n\n    def next(self):\n        for event_idx, event in enumerate(self.event_list):\n            # fix error of ""can\'t not .. bytes and strings""\n            e = str(event[0][0].encode(\'utf-8\'))[2:-1]\n            for file, bbx in zip(self.file_list[event_idx][0],\n                                 self.face_bbx_list[event_idx][0]):\n                f = file[0][0].encode(\'utf-8\')\n                # print(type(e), type(f))  # bytes, bytes\n                # fix error of ""can\'t not .. bytes and strings""\n                f = str(f)[2:-1]\n                # path_of_image = os.path.join(self.path_to_image, str(e), str(f)) + "".jpg""\n                path_of_image = self.path_to_image + \'/\' + e + \'/\' + f + "".jpg""\n                # print(path_of_image)\n\n                bboxes = []\n                bbx0 = bbx[0]\n                for i in range(bbx0.shape[0]):\n                    xmin, ymin, xmax, ymax = bbx0[i]\n                    bboxes.append((int(xmin), int(ymin), int(xmax), int(ymax)))\n                yield DATA(path_of_image, bboxes)\n                    \n'"
