file_path,api_count,code
setup.py,5,"b'r""""""\nParse additional arguments along with the setup.py arguments such as install, build, distribute, sdist, etc.\n\n\nUsage:\n\n  python setup.py install <additional_flags>..<additional_flags> <additional_arg>=<value>..<additional_arg>=<value>\n\n  export CXX=<C++ compiler>; python setup.py install <additional_flags>..<additional_flags> <additional_arg>=<value>..<additional_arg>=<value>\n\n\nExamples:\n\n  python setup.py install --force_cuda --cuda_home=/usr/local/cuda\n  export CXX=g++7; python setup.py install --force_cuda --cuda_home=/usr/local/cuda\n\n\nAdditional flags:\n\n  --cpu_only: Force building only a CPU version. However, if\n      torch.cuda.is_available() is False, it will default to CPU_ONLY.\n\n  --force_cuda: If torch.cuda.is_available() is false, but you have a working\n      nvcc, compile cuda files. --force_cuda will supercede --cpu_only.\n\n\nAdditional arguments:\n\n  --blas=<value> : type of blas library to use for CPU matrix multiplications.\n      Options: [openblas, mkl, atlas, blas]. By default, it will use the first\n      numpy blas library it finds.\n\n  --cuda_home=<value> : a directory that contains <value>/bin/nvcc and\n      <value>/lib64/libcudart.so. By default, use\n      `torch.utils.cpp_extension._find_cuda_home()`.\n\n  --blas_include_dirs=<comma_separated_values> : additional include dirs. Only\n      activated when --blas=<value> is set.\n\n  --blas_library_dirs=<comma_separated_values> : additional library dirs. Only\n      activated when --blas=<value> is set.\n""""""\nimport sys\n\nif sys.version_info < (3, 6):\n    sys.stdout.write(\n        ""Minkowski Engine requires Python 3.6 or higher. Please use anaconda https://www.anaconda.com/distribution/ for an isolated python environment.\\n""\n    )\n    sys.exit(1)\n\ntry:\n    import torch\nexcept ImportError:\n    raise ImportError(""Pytorch not found. Please install pytorch first."")\n\nimport codecs\nimport os\nimport re\nimport subprocess\nfrom sys import argv, platform\nfrom setuptools import setup\nfrom torch.utils.cpp_extension import CppExtension, CUDAExtension, BuildExtension\n\nfrom distutils.sysconfig import get_python_inc\n\nif platform == ""win32"":\n    raise ImportError(""Windows is currently not supported."")\nelif platform == ""darwin"":\n    # Set the distutils to use clang instead of g++ for valid std\n    os.environ[""CC""] = ""/usr/local/opt/llvm/bin/clang""\n    os.environ[""CXX""] = ""/usr/local/opt/llvm/bin/clang""\n\nhere = os.path.abspath(os.path.dirname(__file__))\n\n\ndef read(*parts):\n    with codecs.open(os.path.join(here, *parts), ""r"") as fp:\n        return fp.read()\n\n\ndef find_version(*file_paths):\n    version_file = read(*file_paths)\n    version_match = re.search(r""^__version__ = [\'\\""]([^\'\\""]*)[\'\\""]"", version_file, re.M)\n    if version_match:\n        return version_match.group(1)\n    raise RuntimeError(""Unable to find version string."")\n\n\ndef run_command(*args):\n    subprocess.check_call(args)\n\n\ndef _argparse(pattern, argv, is_flag=True):\n    if is_flag:\n        found = pattern in argv\n        if found:\n            argv.remove(pattern)\n        return found, argv\n    else:\n        arr = [arg for arg in argv if pattern in arg]\n        if len(arr) == 0:  # not found\n            return False, argv\n        else:\n            assert ""="" in arr[0], f""{arr[0]} requires a value.""\n            argv.remove(arr[0])\n            return arr[0].split(""="")[1], argv\n\n\n# For cpu only build\nCPU_ONLY, argv = _argparse(""--cpu_only"", argv)\nCPU_ONLY = CPU_ONLY or not torch.cuda.is_available()\nKEEP_OBJS, argv = _argparse(""--keep_objs"", argv)\nFORCE_CUDA, argv = _argparse(""--force_cuda"", argv)\n\n# args with return value\nCUDA_HOME, argv = _argparse(""--cuda_home"", argv, False)\nBLAS, argv = _argparse(""--blas"", argv, False)\nBLAS_INCLUDE_DIRS, argv = _argparse(""--blas_include_dirs"", argv, False)\nBLAS_LIBRARY_DIRS, argv = _argparse(""--blas_library_dirs"", argv, False)\n\nExtension = CUDAExtension\ncompile_args = [\n    ""make"",\n    ""-j%d"" % min(os.cpu_count(), 12),  # parallel compilation\n    ""PYTHON="" + sys.executable,  # curr python\n]\n\nextra_compile_args = [""-Wno-deprecated-declarations""]\nextra_link_args = []\nlibraries = [""minkowski""]\n\n# extra_compile_args+=[\'-g\']  # Uncomment for debugging\nif CPU_ONLY and not FORCE_CUDA:\n    print(""--------------------------------"")\n    print(""| WARNING: CPU_ONLY build set  |"")\n    print(""--------------------------------"")\n    compile_args += [""CPU_ONLY=1""]\n    extra_compile_args += [""-DCPU_ONLY""]\n    Extension = CppExtension\nelse:\n    # system python installation\n    libraries.append(""cusparse"")\n\nif not (CUDA_HOME is False):  # False when not set, str otherwise\n    print(f""Using CUDA_HOME={CUDA_HOME}"")\n    compile_args += [f""CUDA_HOME={CUDA_HOME}""]\n\nif KEEP_OBJS:\n    print(""\\nUsing built objects"")\n\nBLAS_LIST = [""openblas"", ""mkl"", ""atlas"", ""blas""]\nif not (BLAS is False):  # False only when not set, str otherwise\n    assert BLAS in BLAS_LIST\n    libraries.append(BLAS)\n    if not (BLAS_INCLUDE_DIRS is False):\n        compile_args += [f""BLAS_INCLUDE_DIRS={BLAS_INCLUDE_DIRS}""]\n    if not (BLAS_LIBRARY_DIRS is False):\n        extra_link_args += [f""-Wl,-rpath,{BLAS_LIBRARY_DIRS}""]\nelse:\n    # find the default BLAS library\n    import numpy.distutils.system_info as sysinfo\n\n    # Search blas in this order\n    for blas in BLAS_LIST:\n        if ""libraries"" in sysinfo.get_info(blas):\n            BLAS = blas\n            libraries += sysinfo.get_info(blas)[""libraries""]\n            break\n    else:\n        # BLAS not found\n        raise ImportError(\n            \' \\\n\\nBLAS not found from numpy.distutils.system_info.get_info. \\\n\\nPlease specify BLAS with: python setup.py install --blas=openblas"" \\\n\\nfor more information, please visit https://github.com/StanfordVL/MinkowskiEngine/wiki/Installation\'\n        )\n\nprint(f""\\nUsing BLAS={BLAS}"")\n\ncompile_args += [""BLAS="" + BLAS]\n\nif ""darwin"" in platform:\n    extra_compile_args += [""-stdlib=libc++""]\n\nif not KEEP_OBJS:\n    run_command(""make"", ""clean"")\n\nrun_command(*compile_args)\n\n# Python interface\nsetup(\n    name=""MinkowskiEngine"",\n    version=find_version(""MinkowskiEngine"", ""__init__.py""),\n    install_requires=[""torch"", ""numpy""],\n    packages=[""MinkowskiEngine"", ""MinkowskiEngine.utils"", ""MinkowskiEngine.modules""],\n    package_dir={""MinkowskiEngine"": ""./MinkowskiEngine""},\n    ext_modules=[\n        Extension(\n            name=""MinkowskiEngineBackend"",\n            include_dirs=[here, get_python_inc() + ""/..""],\n            library_dirs=[""objs""],\n            sources=[""pybind/minkowski.cpp"",],\n            libraries=libraries,\n            extra_compile_args=extra_compile_args,\n            extra_link_args=extra_link_args,\n        )\n    ],\n    cmdclass={""build_ext"": BuildExtension},\n    author=""Christopher Choy"",\n    author_email=""chrischoy@ai.stanford.edu"",\n    description=""a convolutional neural network library for sparse tensors"",\n    long_description=read(""README.md""),\n    long_description_content_type=""text/markdown"",\n    url=""https://github.com/StanfordVL/MinkowskiEngine"",\n    keywords=[\n        ""pytorch"",\n        ""Minkowski Engine"",\n        ""Sparse Tensor"",\n        ""Convolutional Neural Networks"",\n        ""3D Vision"",\n        ""Deep Learning"",\n    ],\n    zip_safe=False,\n    classifiers=[\n        # https://pypi.org/classifiers/\n        ""Environment :: Console"",\n        ""Development Status :: 3 - Alpha"",\n        ""Intended Audience :: Developers"",\n        ""Intended Audience :: Other Audience"",\n        ""Intended Audience :: Science/Research"",\n        ""License :: OSI Approved :: MIT License"",\n        ""Natural Language :: English"",\n        ""Programming Language :: C++"",\n        ""Programming Language :: Python :: 3.6"",\n        ""Programming Language :: Python :: 3.7"",\n        ""Programming Language :: Python :: 3.8"",\n        ""Topic :: Multimedia :: Graphics"",\n        ""Topic :: Scientific/Engineering"",\n        ""Topic :: Scientific/Engineering :: Artificial Intelligence"",\n        ""Topic :: Scientific/Engineering :: Mathematics"",\n        ""Topic :: Scientific/Engineering :: Physics"",\n        ""Topic :: Scientific/Engineering :: Visualization"",\n    ],\n    python_requires="">=3.6"",\n)\n'"
MinkowskiEngine/Common.py,36,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport math\nfrom collections import Sequence\nimport numpy as np\nfrom enum import Enum\nfrom itertools import product\nfrom typing import Union\n\nimport torch\n\nfrom torch.nn import Module\n\nimport MinkowskiEngineBackend as MEB\n\n\nclass GlobalPoolingMode(Enum):\n    """"""\n    Define the global pooling mode\n    """"""\n    AUTO = 0, \'AUTO\'\n    INDEX_SELECT = 1, \'INDEX_SELECT\'\n    SPARSE = 2, \'SPARSE\'\n\n    def __new__(cls, value, name):\n        member = object.__new__(cls)\n        member._value_ = value\n        member.fullname = name\n        return member\n\n    def __int__(self):\n        return self.value\n\n\nclass RegionType(Enum):\n    """"""\n    Define the kernel region type\n    """"""\n    HYPERCUBE = 0, \'HYPERCUBE\'\n    HYPERCROSS = 1, \'HYPERCROSS\'\n    CUSTOM = 2, \'CUSTOM\'\n    HYBRID = 3, \'HYBRID\'\n\n    def __new__(cls, value, name):\n        member = object.__new__(cls)\n        member._value_ = value\n        member.fullname = name\n        return member\n\n    def __int__(self):\n        return self.value\n\n\ndef convert_to_int_list(arg: Union[int, Sequence, np.ndarray, torch.Tensor],\n                        dimension: int):\n    if isinstance(arg, list):\n        assert len(arg) == dimension\n        return arg\n\n    if isinstance(arg, (Sequence, np.ndarray, torch.Tensor)):\n        tmp = [i for i in arg]\n        assert len(tmp) == dimension\n    elif np.isscalar(arg):  # Assume that it is a scalar\n        tmp = [int(arg) for i in range(dimension)]\n    else:\n        raise ValueError(\'Input must be a scalar or a sequence\')\n\n    return tmp\n\n\ndef convert_to_int_tensor(\n        arg: Union[int, Sequence, np.ndarray, torch.IntTensor], dimension: int):\n    if isinstance(arg, torch.IntTensor):\n        assert arg.numel() == dimension\n        return arg\n\n    if isinstance(arg, (Sequence, np.ndarray)):\n        tmp = torch.IntTensor([i for i in arg])\n        assert tmp.numel() == dimension\n    elif np.isscalar(arg):  # Assume that it is a scalar\n        tmp = torch.IntTensor([int(arg) for i in range(dimension)])\n    else:\n        raise ValueError(\'Input must be a scalar or a sequence\')\n\n    return tmp\n\n\ndef prep_args(tensor_stride: Union[int, Sequence, np.ndarray, torch.IntTensor],\n              stride: Union[int, Sequence, np.ndarray, torch.IntTensor],\n              kernel_size: Union[int, Sequence, np.ndarray, torch.IntTensor],\n              dilation: Union[int, Sequence, np.ndarray, torch.IntTensor],\n              region_type: Union[int, RegionType],\n              D=-1):\n    assert torch.prod(\n        kernel_size > 0\n    ), f""kernel_size must be a positive integer, provided {kernel_size}""\n    assert D > 0, f""dimension must be a positive integer, {D}""\n    tensor_stride = convert_to_int_tensor(tensor_stride, D)\n    stride = convert_to_int_tensor(stride, D)\n    kernel_size = convert_to_int_tensor(kernel_size, D)\n    dilation = convert_to_int_tensor(dilation, D)\n    region_type = int(region_type)\n    return tensor_stride, stride, kernel_size, dilation, region_type,\n\n\ndef get_postfix(tensor: torch.Tensor):\n    postfix = \'GPU\' if tensor.is_cuda else \'CPU\'\n    if isinstance(tensor, torch.DoubleTensor) or isinstance(\n            tensor, torch.cuda.DoubleTensor):\n        postfix += \'d\'\n    else:\n        postfix += \'f\'\n    return postfix\n\n\ndef get_kernel_volume(region_type, kernel_size, region_offset, axis_types,\n                      dimension):\n    """"""\n    when center is True, the custom region_offset will be centered at the\n    origin. Currently, for HYPERCUBE, HYPERCROSS with odd kernel sizes cannot\n    use center=False.\n    """"""\n    if region_type == RegionType.HYPERCUBE:\n        assert region_offset is None, ""Region offset must be None when region_type is given""\n        assert axis_types is None, ""Axis types must be None when region_type is given""\n        # Typical convolution kernel\n        assert torch.prod(kernel_size > 0) == 1\n\n        # Convolution kernel with even numbered kernel size not defined.\n        kernel_volume = int(torch.prod(kernel_size))\n\n    elif region_type == RegionType.HYPERCROSS:\n        assert torch.prod(kernel_size > 0) == 1, ""kernel_size must be positive""\n        assert (\n            kernel_size %\n            2).prod() == 1, ""kernel_size must be odd for region_type HYPERCROSS""\n        # 0th: itself, (1, 2) for 0th dim neighbors, (3, 4) for 1th dim ...\n        kernel_volume = int(torch.sum(kernel_size - 1) + 1)\n\n    elif region_type == RegionType.HYBRID:\n        assert region_offset is None, \\\n            ""region_offset must be None when region_type is HYBRID""\n        kernel_size_list = kernel_size.tolist()\n        kernel_volume = 1\n        # First HYPERCUBE\n        for axis_type, curr_kernel_size, d in \\\n                zip(axis_types, kernel_size_list, range(dimension)):\n            if axis_type == RegionType.HYPERCUBE:\n                kernel_volume *= curr_kernel_size\n\n        # Second, HYPERCROSS\n        for axis_type, curr_kernel_size, d in \\\n                zip(axis_types, kernel_size_list, range(dimension)):\n            if axis_type == RegionType.HYPERCROSS:\n                kernel_volume += (curr_kernel_size - 1)\n\n    elif region_type == RegionType.CUSTOM:\n        assert region_offset.numel(\n        ) > 0, ""region_offset must be non empty when region_type is CUSTOM""\n        assert region_offset.size(\n            1\n        ) == dimension, ""region_offset must have the same dimension as the network""\n        kernel_volume = int(region_offset.size(0))\n\n    else:\n        raise NotImplementedError()\n\n    return kernel_volume\n\n\ndef convert_region_type(\n        region_type: RegionType,\n        tensor_stride: Union[Sequence, np.ndarray, torch.IntTensor],\n        kernel_size: Union[Sequence, np.ndarray, torch.IntTensor],\n        up_stride: Union[Sequence, np.ndarray, torch.IntTensor],\n        dilation: Union[Sequence, np.ndarray, torch.IntTensor],\n        region_offset: Union[Sequence, np.ndarray, torch.IntTensor],\n        axis_types: Union[Sequence, np.ndarray, torch.IntTensor],\n        dimension: int,\n        center: bool = True):\n    """"""\n    when center is True, the custom region_offset will be centered at the\n    origin. Currently, for HYPERCUBE, HYPERCROSS with odd kernel sizes cannot\n    use center=False.\n\n    up_stride: stride for conv_transpose, otherwise set it as 1\n    """"""\n    if region_type == RegionType.HYPERCUBE:\n        assert region_offset is None, ""Region offset must be None when region_type is given""\n        assert axis_types is None, ""Axis types must be None when region_type is given""\n        # Typical convolution kernel\n        assert torch.prod(kernel_size > 0) == 1\n        # assert torch.unique(dilation).numel() == 1\n        kernel_volume = int(torch.prod(kernel_size))\n\n    elif region_type == RegionType.HYPERCROSS:\n        assert torch.prod(kernel_size > 0) == 1, ""kernel_size must be positive""\n        assert (\n            kernel_size %\n            2).prod() == 1, ""kernel_size must be odd for region_type HYPERCROSS""\n        # 0th: itself, (1, 2) for 0th dim neighbors, (3, 4) for 1th dim ...\n        kernel_volume = int(torch.sum(kernel_size - 1) + 1)\n\n    elif region_type == RegionType.HYBRID:\n        assert region_offset is None, \\\n            ""region_offset must be None when region_type is HYBRID""\n        region_offset = [[\n            0,\n        ] * dimension]\n        kernel_size_list = kernel_size.tolist()\n        # First HYPERCUBE\n        for axis_type, curr_kernel_size, d in \\\n                zip(axis_types, kernel_size_list, range(dimension)):\n            new_offset = []\n            if axis_type == RegionType.HYPERCUBE:\n                for offset in region_offset:\n                    for curr_offset in range(curr_kernel_size):\n                        off_center = int(\n                            math.floor(\n                                (curr_kernel_size - 1) / 2)) if center else 0\n                        offset = offset.copy()  # Do not modify the original\n                        # Exclude the coord (0, 0, ..., 0)\n                        if curr_offset == off_center:\n                            continue\n                        offset[d] = (curr_offset - off_center) * \\\n                            dilation[d] * (tensor_stride[d] / up_stride[d])\n                        new_offset.append(offset)\n            region_offset.extend(new_offset)\n\n        # Second, HYPERCROSS\n        for axis_type, curr_kernel_size, d in \\\n                zip(axis_types, kernel_size_list, range(dimension)):\n            new_offset = []\n            if axis_type == RegionType.HYPERCROSS:\n                for curr_offset in range(curr_kernel_size):\n                    off_center = int(math.floor(\n                        (curr_kernel_size - 1) / 2)) if center else 0\n                    offset = [\n                        0,\n                    ] * dimension\n                    # Exclude the coord (0, 0, ..., 0)\n                    if curr_offset == off_center:\n                        continue\n                    offset[d] = (curr_offset - off_center) * \\\n                        dilation[d] * (tensor_stride[d] / up_stride[d])\n                    new_offset.append(offset)\n            region_offset.extend(new_offset)\n\n        # Convert to CUSTOM type\n        region_type = RegionType.CUSTOM\n        region_offset = torch.IntTensor(region_offset)\n        kernel_volume = int(region_offset.size(0))\n\n    elif region_type == RegionType.CUSTOM:\n        assert region_offset.numel(\n        ) > 0, ""region_offset must be non empty when region_type is CUSTOM""\n        assert region_offset.size(\n            1\n        ) == dimension, ""region_offset must have the same dimension as the network""\n        kernel_volume = int(region_offset.size(0))\n        assert isinstance(\n            region_offset.dtype,\n            torch.IntTensor), ""region_offset must be a torch.IntTensor.""\n    else:\n        raise NotImplementedError()\n\n    if region_offset is None:\n        region_offset = torch.IntTensor()\n\n    return region_type, region_offset, kernel_volume\n\n\nclass KernelGenerator:\n\n    def __init__(self,\n                 kernel_size=-1,\n                 stride=1,\n                 dilation=1,\n                 is_transpose=False,\n                 region_type=RegionType.HYPERCUBE,\n                 region_offsets=None,\n                 axis_types=None,\n                 dimension=-1):\n        r""""""\n            :attr:`region_type` (RegionType, optional): defines the kernel\n            shape. Please refer to MinkowskiEngine.Comon for details.\n\n            :attr:`region_offset` (torch.IntTensor, optional): when the\n            :attr:`region_type` is :attr:`RegionType.CUSTOM`, the convolution\n            kernel uses the provided `region_offset` to define offsets. It\n            should be a matrix of size :math:`N \\times D` where :math:`N` is\n            the number of offsets and :math:`D` is the dimension of the\n            space.\n\n            :attr:`axis_types` (list of RegionType, optional): If given, it\n            uses different methods to create a kernel for each axis. e.g., when\n            it is `[RegionType.HYPERCUBE, RegionType.HYPERCUBE,\n            RegionType.HYPERCROSS]`, the kernel would be rectangular for the\n            first two dimensions and cross shaped for the thrid dimension.\n        """"""\n        assert dimension > 0\n        assert isinstance(region_type, RegionType)\n\n        stride = convert_to_int_tensor(stride, dimension)\n        kernel_size = convert_to_int_tensor(kernel_size, dimension)\n        dilation = convert_to_int_tensor(dilation, dimension)\n\n        self.cache = {}\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.dilation = dilation\n        self.region_type = region_type\n        self.region_offsets = region_offsets\n        self.axis_types = axis_types\n        self.dimension = dimension\n        self.kernel_volume = get_kernel_volume(region_type, kernel_size,\n                                               region_offsets, axis_types,\n                                               dimension)\n\n    def get_kernel(self, tensor_stride, is_transpose):\n        assert len(tensor_stride) == self.dimension\n        if tuple(tensor_stride) not in self.cache:\n            up_stride = self.stride \\\n                if is_transpose else torch.Tensor([1, ] * self.dimension)\n\n            self.cache[tuple(tensor_stride)] = convert_region_type(\n                self.region_type, tensor_stride, self.kernel_size, up_stride,\n                self.dilation, self.region_offsets, self.axis_types,\n                self.dimension)\n\n        return self.cache[tuple(tensor_stride)]\n\n\nclass MinkowskiModuleBase(Module):\n    pass\n\n\ndef get_minkowski_function(name, variable):\n    fn_name = name + get_postfix(variable)\n    if hasattr(MEB, fn_name):\n        return getattr(MEB, fn_name)\n    else:\n        if variable.is_cuda:\n            raise ValueError(\n                f""Function {fn_name} not available. Please compile MinkowskiEngine where `torch.cuda.is_available()` is `True`.""\n            )\n        else:\n            raise ValueError(f""Function {fn_name} not available."")\n'"
MinkowskiEngine/MinkowskiBroadcast.py,3,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nfrom enum import Enum\n\nimport torch\nfrom torch.nn import Module\nfrom torch.autograd import Function\n\nfrom SparseTensor import SparseTensor\nfrom Common import get_minkowski_function\n\n\nclass OperationType(Enum):\n    ADDITION = 0\n    MULTIPLICATION = 1\n\n\nop_to_int = {i: i.value for i in OperationType}\n\n\ndef operation_type_to_int(op):\n    assert isinstance(op, OperationType)\n    return op_to_int[op]\n\n\nclass MinkowskiBroadcastFunction(Function):\n\n    @staticmethod\n    def forward(ctx, input_features, input_features_global, operation_type,\n                in_coords_key, glob_coords_key, coords_manager):\n        assert input_features.shape[1] == input_features_global.shape[1]\n        assert input_features.type() == input_features_global.type()\n        assert isinstance(operation_type, OperationType)\n        if not input_features.is_contiguous():\n            input_features = input_features.contiguous()\n        if not input_features_global.is_contiguous():\n            input_features_global = input_features_global.contiguous()\n\n        ctx.op = operation_type_to_int(operation_type)\n\n        ctx.in_feat = input_features\n        ctx.in_feat_glob = input_features_global\n        ctx.in_coords_key = in_coords_key\n        ctx.glob_coords_key = glob_coords_key\n        ctx.coords_manager = coords_manager\n\n        fw_fn = get_minkowski_function(\'BroadcastForward\', input_features)\n        out_feat = fw_fn(ctx.in_feat, ctx.in_feat_glob, ctx.op,\n                         ctx.in_coords_key.CPPCoordsKey,\n                         ctx.glob_coords_key.CPPCoordsKey,\n                         ctx.coords_manager.CPPCoordsManager)\n        return out_feat\n\n    @staticmethod\n    def backward(ctx, grad_out_feat):\n        if not grad_out_feat.is_contiguous():\n            grad_out_feat = grad_out_feat.contiguous()\n\n        grad_in_feat = grad_out_feat.new()\n        grad_in_feat_glob = grad_out_feat.new()\n        bw_fn = get_minkowski_function(\'BroadcastBackward\', grad_out_feat)\n        bw_fn(ctx.in_feat, grad_in_feat, ctx.in_feat_glob, grad_in_feat_glob,\n              grad_out_feat, ctx.op, ctx.in_coords_key.CPPCoordsKey,\n              ctx.glob_coords_key.CPPCoordsKey,\n              ctx.coords_manager.CPPCoordsManager)\n        return grad_in_feat, grad_in_feat_glob, None, None, None, None\n\n\nclass AbstractMinkowskiBroadcast(Module):\n\n    def __init__(self, operation_type):\n        super(AbstractMinkowskiBroadcast, self).__init__()\n        assert isinstance(operation_type, OperationType)\n\n        self.operation_type = operation_type\n\n        self.broadcast = MinkowskiBroadcastFunction()\n\n    def forward(self, input, input_glob):\n        assert isinstance(input, SparseTensor)\n\n        output = self.broadcast.apply(input.F, input_glob.F,\n                                      self.operation_type, input.coords_key,\n                                      input_glob.coords_key, input.coords_man)\n        return SparseTensor(\n            output,\n            coords_key=input.coords_key,\n            coords_manager=input.coords_man)\n\n    def __repr__(self):\n        return self.__class__.__name__\n\n\nclass MinkowskiBroadcastAddition(AbstractMinkowskiBroadcast):\n    r""""""Broadcast the reduced features to all input coordinates.\n\n    .. math::\n\n        \\mathbf{y}_\\mathbf{u} = \\mathbf{x}_{1, \\mathbf{u}} + \\mathbf{x}_2\n        \\; \\text{for} \\; \\mathbf{u} \\in \\mathcal{C}^\\text{in}\n\n\n    For all input :math:`\\mathbf{x}_\\mathbf{u}`, add :math:`\\mathbf{x}_2`. The\n    output coordinates will be the same as the input coordinates\n    :math:`\\mathcal{C}^\\text{in} = \\mathcal{C}^\\text{out}`.\n\n    .. note::\n        The first argument takes a sparse tensor; the second argument takes\n        features that are reduced to the origin. This can be typically done with\n        the global reduction such as the :attr:`MinkowskiGlobalPooling`.\n\n    """"""\n\n    def __init__(self):\n        AbstractMinkowskiBroadcast.__init__(self, OperationType.ADDITION)\n\n\nclass MinkowskiBroadcastMultiplication(AbstractMinkowskiBroadcast):\n    r""""""Broadcast reduced features to all input coordinates.\n\n    .. math::\n\n        \\mathbf{y}_\\mathbf{u} = \\mathbf{x}_{1, \\mathbf{u}} \\times \\mathbf{x}_2\n        \\; \\text{for} \\; \\mathbf{u} \\in \\mathcal{C}^\\text{in}\n\n\n    For all input :math:`\\mathbf{x}_\\mathbf{u}`, multiply :math:`\\mathbf{x}_2`\n    element-wise. The output coordinates will be the same as the input\n    coordinates :math:`\\mathcal{C}^\\text{in} = \\mathcal{C}^\\text{out}`.\n\n    .. note::\n        The first argument takes a sparse tensor; the second argument takes\n        features that are reduced to the origin. This can be typically done with\n        the global reduction such as the :attr:`MinkowskiGlobalPooling`.\n\n    """"""\n\n    def __init__(self):\n        AbstractMinkowskiBroadcast.__init__(self, OperationType.MULTIPLICATION)\n\n\nclass MinkowskiBroadcast(Module):\n    r""""""Broadcast reduced features to all input coordinates.\n\n    .. math::\n\n        \\mathbf{y}_\\mathbf{u} = \\mathbf{x}_2 \\; \\text{for} \\; \\mathbf{u} \\in\n        \\mathcal{C}^\\text{in}\n\n\n    For all input :math:`\\mathbf{x}_\\mathbf{u}`, copy value :math:`\\mathbf{x}_2`\n    element-wise. The output coordinates will be the same as the input\n    coordinates :math:`\\mathcal{C}^\\text{in} = \\mathcal{C}^\\text{out}`. The\n    first input :math:`\\mathbf{x}_1` is only used for defining the output\n    coordinates.\n\n    .. note::\n        The first argument takes a sparse tensor; the second argument takes\n        features that are reduced to the origin. This can be typically done with\n        the global reduction such as the :attr:`MinkowskiGlobalPooling`.\n\n    """"""\n\n    def __repr__(self):\n        return self.__class__.__name__\n\n    def forward(self, input, input_glob):\n        assert isinstance(input, SparseTensor)\n        assert isinstance(input_glob, SparseTensor)\n\n        broadcast_feat = input.F.new(len(input), input_glob.size()[1])\n        row_inds = input.coords_man.get_row_indices_per_batch(input.coords_key)\n        for b, row_ind in enumerate(row_inds):\n            broadcast_feat[row_ind] = input_glob.F[b]\n\n        return SparseTensor(\n            broadcast_feat,\n            coords_key=input.coords_key,\n            coords_manager=input.coords_man)\n\n\nclass MinkowskiBroadcastConcatenation(MinkowskiBroadcast):\n    r""""""Broadcast reduced features to all input coordinates and concatenate to the input.\n\n    .. math::\n\n        \\mathbf{y}_\\mathbf{u} = [\\mathbf{x}_{1,\\mathbf{u}}, \\mathbf{x}_2] \\;\n        \\text{for} \\; \\mathbf{u} \\in \\mathcal{C}^\\text{in}\n\n\n    For all input :math:`\\mathbf{x}_\\mathbf{u}`, concatenate vector\n    :math:`\\mathbf{x}_2`. :math:`[\\cdot, \\cdot]` is a concatenation operator.\n    The output coordinates will be the same as the input coordinates\n    :math:`\\mathcal{C}^\\text{in} = \\mathcal{C}^\\text{out}`.\n\n    .. note::\n        The first argument takes a sparse tensor; the second argument takes\n        features that are reduced to the origin. This can be typically done with\n        the global reduction such as the :attr:`MinkowskiGlobalPooling`.\n\n    """"""\n\n    def forward(self, input, input_glob):\n        assert isinstance(input, SparseTensor)\n        assert isinstance(input_glob, SparseTensor)\n\n        broadcast_feat = input.F.new(len(input), input_glob.size()[1])\n        row_inds = input.coords_man.get_row_indices_per_batch(input.coords_key)\n        for b, row_ind in enumerate(row_inds):\n            broadcast_feat[row_ind] = input_glob.F[b]\n\n        broadcast_cat = torch.cat((input.F, broadcast_feat), dim=1)\n        return SparseTensor(\n            broadcast_cat,\n            coords_key=input.coords_key,\n            coords_manager=input.coords_man)\n'"
MinkowskiEngine/MinkowskiChannelwiseConvolution.py,4,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport math\nfrom typing import Union\n\nimport torch\nfrom torch.nn import Parameter\n\nfrom SparseTensor import SparseTensor\nfrom Common import RegionType, MinkowskiModuleBase, KernelGenerator, \\\n    prep_args, convert_to_int_list, convert_to_int_tensor\nfrom MinkowskiCoords import CoordsKey\n\n\nclass MinkowskiChannelwiseConvolution(MinkowskiModuleBase):\n    r""""""Channelwise (Depthwise) Convolution layer for a sparse tensor.\n\n\n    .. math::\n\n        \\mathbf{x}_\\mathbf{u} = \\sum_{\\mathbf{i} \\in \\mathcal{N}^D(\\mathbf{u}, K) \\cap\n        \\mathcal{C}^\\text{in}} W_\\mathbf{i} \\odot \\mathbf{x}_{\\mathbf{i} +\n        \\mathbf{u}} \\;\\text{for} \\; \\mathbf{u} \\in \\mathcal{C}^\\text{out}\n\n    where :math:`K` is the kernel size and :math:`\\mathcal{N}^D(\\mathbf{u}, K)\n    \\cap \\mathcal{C}^\\text{in}` is the set of offsets that are at most :math:`\\left\n    \\lceil{\\frac{1}{2}(K - 1)} \\right \\rceil` away from :math:`\\mathbf{u}`\n    defined in :math:`\\mathcal{S}^\\text{in}`. :math:`\\odot` indicates the\n    elementwise product.\n\n    .. note::\n        For even :math:`K`, the kernel offset :math:`\\mathcal{N}^D`\n        implementation is different from the above definition. The offsets\n        range from :math:`\\mathbf{i} \\in [0, K)^D, \\; \\mathbf{i} \\in\n        \\mathbb{Z}_+^D`.\n\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 kernel_size=-1,\n                 stride=1,\n                 dilation=1,\n                 has_bias=False,\n                 kernel_generator=None,\n                 dimension=-1):\n        r""""""convolution on a sparse tensor\n\n        Args:\n            :attr:`in_channels` (int): the number of input channels in the\n            input tensor.\n\n            :attr:`kernel_size` (int, optional): the size of the kernel in the\n            output tensor. If not provided, :attr:`region_offset` should be\n            :attr:`RegionType.CUSTOM` and :attr:`region_offset` should be a 2D\n            matrix with size :math:`N\\times D` such that it lists all :math:`N`\n            offsets in D-dimension.\n\n            :attr:`stride` (int, or list, optional): stride size of the\n            convolution layer. If non-identity is used, the output coordinates\n            will be at least :attr:`stride` :math:`\\times` :attr:`tensor_stride`\n            away. When a list is given, the length must be D; each element will\n            be used for stride size for the specific axis.\n\n            :attr:`dilation` (int, or list, optional): dilation size for the\n            convolution kernel. When a list is given, the length must be D and\n            each element is an axis specific dilation. All elements must be > 0.\n\n            :attr:`has_bias` (bool, optional): if True, the convolution layer\n            has a bias.\n\n            :attr:`kernel_generator` (:attr:`MinkowskiEngine.KernelGenerator`,\n            optional): defines the custom kernel shape.\n\n            :attr:`dimension` (int): the spatial dimension of the space where\n            all the inputs and the network are defined. For example, images are\n            in a 2D space, meshes and 3D shapes are in a 3D space.\n\n        """"""\n\n        super(MinkowskiChannelwiseConvolution, self).__init__()\n        assert dimension > 0, f""dimension must be a positive integer, {dimension}""\n\n        if kernel_generator is None:\n            kernel_generator = KernelGenerator(\n                kernel_size=kernel_size,\n                stride=stride,\n                dilation=dilation,\n                dimension=dimension)\n        else:\n            kernel_size = kernel_generator.kernel_size\n\n        stride = convert_to_int_tensor(stride, dimension)\n        kernel_size = convert_to_int_tensor(kernel_size, dimension)\n        dilation = convert_to_int_tensor(dilation, dimension)\n\n        kernel_volume = kernel_generator.kernel_volume\n\n        self.in_channels = in_channels\n        self.kernel_size = kernel_size\n        self.kernel_volume = kernel_volume\n        self.stride = stride\n        self.dilation = dilation\n        self.kernel_generator = kernel_generator\n        self.dimension = dimension\n        self.use_mm = False  # use matrix multiplication when kernel is 1\n\n        Tensor = torch.FloatTensor\n        self.kernel_shape = (self.kernel_volume, self.in_channels)\n\n        self.kernel = Parameter(Tensor(*self.kernel_shape))\n        self.bias = Parameter(Tensor(1, in_channels)) if has_bias else None\n        self.has_bias = has_bias\n        self.reset_parameters()\n\n    def forward(self,\n                input: SparseTensor,\n                coords: Union[torch.IntTensor, CoordsKey, SparseTensor] = None):\n        r""""""\n        :attr:`input` (`MinkowskiEngine.SparseTensor`): Input sparse tensor to apply a\n        convolution on.\n\n        :attr:`coords` ((`torch.IntTensor`, `MinkowskiEngine.CoordsKey`,\n        `MinkowskiEngine.SparseTensor`), optional): If provided, generate\n        results on the provided coordinates. None by default.\n\n        """"""\n        assert isinstance(input, SparseTensor)\n        assert input.D == self.dimension\n\n        # Create a region_offset\n        self.region_type_, self.region_offset_, _ = \\\n            self.kernel_generator.get_kernel(input.tensor_stride, False)\n\n        cm = input.coords_man\n        in_key = input.coords_key\n        on_gpu = input.device.type != \'cpu\'\n\n        out_key = cm.stride(in_key, self.stride)\n        N_out = cm.get_coords_size_by_coords_key(out_key)\n        out_F = input._F.new(N_out, self.in_channels).zero_()\n\n        in_maps, out_maps = cm.get_kernel_map(\n            in_key,\n            out_key,\n            self.stride,\n            self.kernel_size,\n            self.dilation,\n            self.region_type_,\n            self.region_offset_,\n            is_transpose=False,\n            is_pool=False,\n            on_gpu=on_gpu)\n\n        for k in range(self.kernel_volume):\n            out_F[out_maps[k]] += input.F[in_maps[k]] * self.kernel[k]\n\n        if self.has_bias:\n            out_F += self.bias\n\n        return SparseTensor(out_F, coords_key=out_key, coords_manager=cm)\n\n    def reset_parameters(self, is_transpose=False):\n        n = (self.out_channels\n             if is_transpose else self.in_channels) * self.kernel_volume\n        stdv = 1. / math.sqrt(n)\n        self.kernel.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.uniform_(-stdv, stdv)\n\n    def __repr__(self):\n        s = \'(in={}, region_type={}, \'.format(self.in_channels,\n                                              self.kernel_generator.region_type)\n        if self.kernel_generator.region_type in [\n                RegionType.HYBRID, RegionType.CUSTOM\n        ]:\n            s += \'kernel_volume={}, \'.format(self.kernel_volume)\n        else:\n            s += \'kernel_size={}, \'.format(self.kernel_size.tolist())\n        s += \'stride={}, dilation={})\'.format(self.stride.tolist(),\n                                              self.dilation.tolist())\n        return self.__class__.__name__ + s\n'"
MinkowskiEngine/MinkowskiConvolution.py,10,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport math\nfrom typing import Union\n\nimport torch\nfrom torch.autograd import Function\nfrom torch.nn import Parameter\n\nfrom SparseTensor import SparseTensor, _get_coords_key\nfrom Common import RegionType, MinkowskiModuleBase, KernelGenerator, \\\n    prep_args, convert_to_int_list, convert_to_int_tensor, \\\n    get_minkowski_function\nfrom MinkowskiCoords import CoordsKey, save_ctx\n\n\nclass MinkowskiConvolutionFunction(Function):\n\n    @staticmethod\n    def forward(ctx,\n                input_features,\n                kernel,\n                tensor_stride=1,\n                stride=1,\n                kernel_size=-1,\n                dilation=1,\n                region_type=0,\n                region_offset=None,\n                in_coords_key=None,\n                out_coords_key=None,\n                coords_manager=None):\n        """"""\n        region_type=0 HyperCube\n        """"""\n        # Prep arguments\n        # Kernel shape (n_spatial_kernels, in_nfeat, out_nfeat)\n        assert input_features.shape[1] == kernel.shape[1], \\\n            ""The input shape "" + str(list(input_features.shape)) + \\\n            "" does not match the kernel shape "" + str(list(kernel.shape))\n        if out_coords_key is None:\n            out_coords_key = CoordsKey(in_coords_key.D)\n        assert in_coords_key.D == out_coords_key.D\n        assert input_features.type() == kernel.type(), \\\n            f""Type mismatch input: {input_features.type()} != kernel: {kernel.type()}""\n        if not input_features.is_contiguous():\n            input_features = input_features.contiguous()\n\n        tensor_stride, stride, kernel_size, dilation, region_type = prep_args(\n            tensor_stride, stride, kernel_size, dilation, region_type,\n            in_coords_key.D)\n\n        if region_offset is None:\n            region_offset = torch.IntTensor()\n\n        ctx.in_feat = input_features\n        ctx.kernel = kernel\n        ctx = save_ctx(ctx, tensor_stride, stride, kernel_size, dilation,\n                       region_type, in_coords_key, out_coords_key,\n                       coords_manager)\n\n        D = in_coords_key.D\n        out_feat = input_features.new()\n\n        fw_fn = get_minkowski_function(\'ConvolutionForward\', input_features)\n        fw_fn(ctx.in_feat, out_feat, kernel,\n              convert_to_int_list(ctx.tensor_stride, D),\n              convert_to_int_list(ctx.stride, D),\n              convert_to_int_list(ctx.kernel_size, D),\n              convert_to_int_list(ctx.dilation, D), region_type, region_offset,\n              ctx.in_coords_key.CPPCoordsKey, ctx.out_coords_key.CPPCoordsKey,\n              ctx.coords_man.CPPCoordsManager)\n        return out_feat\n\n    @staticmethod\n    def backward(ctx, grad_out_feat):\n        if not grad_out_feat.is_contiguous():\n            grad_out_feat = grad_out_feat.contiguous()\n\n        grad_in_feat = grad_out_feat.new()\n        grad_kernel = grad_out_feat.new()\n        D = ctx.in_coords_key.D\n        bw_fn = get_minkowski_function(\'ConvolutionBackward\', grad_out_feat)\n        bw_fn(ctx.in_feat, grad_in_feat, grad_out_feat, ctx.kernel, grad_kernel,\n              convert_to_int_list(ctx.tensor_stride, D),\n              convert_to_int_list(ctx.stride, D),\n              convert_to_int_list(ctx.kernel_size, D),\n              convert_to_int_list(ctx.dilation, D), ctx.region_type,\n              ctx.in_coords_key.CPPCoordsKey, ctx.out_coords_key.CPPCoordsKey,\n              ctx.coords_man.CPPCoordsManager)\n        return grad_in_feat, grad_kernel, None, None, None, None, None, None, None, None, None\n\n\nclass MinkowskiConvolutionTransposeFunction(Function):\n\n    @staticmethod\n    def forward(ctx,\n                input_features,\n                kernel,\n                tensor_stride=1,\n                stride=1,\n                kernel_size=-1,\n                dilation=1,\n                region_type=0,\n                region_offset=None,\n                generate_new_coords=False,\n                in_coords_key=None,\n                out_coords_key=None,\n                coords_manager=None):\n        """"""\n        region_type=0 HyperCube\n        """"""\n        # Prep arguments\n        # Kernel shape (n_spatial_kernels, in_nfeat, out_nfeat)\n        assert input_features.shape[1] == kernel.shape[1], \\\n            ""The input shape "" + str(list(input_features.shape)) + \\\n            "" does not match the kernel shape "" + str(list(kernel.shape))\n        if out_coords_key is None:\n            out_coords_key = CoordsKey(in_coords_key.D)\n        assert in_coords_key.D == out_coords_key.D\n        assert input_features.type() == kernel.type(), \\\n            f""Type mismatch input: {input_features.type()} != kernel: {kernel.type()}""\n        if not input_features.is_contiguous():\n            input_features = input_features.contiguous()\n\n        tensor_stride, stride, kernel_size, dilation, region_type = prep_args(\n            tensor_stride, stride, kernel_size, dilation, region_type,\n            in_coords_key.D)\n\n        if region_offset is None:\n            region_offset = torch.IntTensor()\n\n        ctx.in_feat = input_features\n        ctx.kernel = kernel\n        ctx = save_ctx(ctx, tensor_stride, stride, kernel_size, dilation,\n                       region_type, in_coords_key, out_coords_key,\n                       coords_manager)\n\n        D = in_coords_key.D\n        out_feat = input_features.new()\n\n        fw_fn = get_minkowski_function(\'ConvolutionTransposeForward\',\n                                       input_features)\n        fw_fn(ctx.in_feat, out_feat, kernel,\n              convert_to_int_list(ctx.tensor_stride, D),\n              convert_to_int_list(ctx.stride, D),\n              convert_to_int_list(ctx.kernel_size, D),\n              convert_to_int_list(ctx.dilation, D), region_type, region_offset,\n              ctx.in_coords_key.CPPCoordsKey, ctx.out_coords_key.CPPCoordsKey,\n              ctx.coords_man.CPPCoordsManager, generate_new_coords)\n        return out_feat\n\n    @staticmethod\n    def backward(ctx, grad_out_feat):\n        if not grad_out_feat.is_contiguous():\n            grad_out_feat = grad_out_feat.contiguous()\n\n        grad_in_feat = grad_out_feat.new()\n        grad_kernel = grad_out_feat.new()\n        D = ctx.in_coords_key.D\n        bw_fn = get_minkowski_function(\'ConvolutionTransposeBackward\',\n                                       grad_out_feat)\n        bw_fn(ctx.in_feat, grad_in_feat, grad_out_feat, ctx.kernel, grad_kernel,\n              convert_to_int_list(ctx.tensor_stride, D),\n              convert_to_int_list(ctx.stride, D),\n              convert_to_int_list(ctx.kernel_size, D),\n              convert_to_int_list(ctx.dilation, D), ctx.region_type,\n              ctx.in_coords_key.CPPCoordsKey, ctx.out_coords_key.CPPCoordsKey,\n              ctx.coords_man.CPPCoordsManager)\n        return grad_in_feat, grad_kernel, None, None, None, None, None, None, None, None, None, None\n\n\nclass MinkowskiConvolutionBase(MinkowskiModuleBase):\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size=-1,\n                 stride=1,\n                 dilation=1,\n                 has_bias=False,\n                 kernel_generator=None,\n                 is_transpose=False,\n                 dimension=-1):\n        super(MinkowskiConvolutionBase, self).__init__()\n        assert dimension > 0, f""dimension must be a positive integer, {dimension}""\n\n        if kernel_generator is None:\n            kernel_generator = KernelGenerator(\n                kernel_size=kernel_size,\n                stride=stride,\n                dilation=dilation,\n                dimension=dimension)\n        else:\n            kernel_size = kernel_generator.kernel_size\n\n        stride = convert_to_int_tensor(stride, dimension)\n        kernel_size = convert_to_int_tensor(kernel_size, dimension)\n        dilation = convert_to_int_tensor(dilation, dimension)\n\n        kernel_volume = kernel_generator.kernel_volume\n\n        self.is_transpose = is_transpose\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.kernel_volume = kernel_volume\n        self.stride = stride\n        self.dilation = dilation\n        self.kernel_generator = kernel_generator\n        self.dimension = dimension\n        self.use_mm = False  # use matrix multiplication when kernel is 1\n\n        Tensor = torch.FloatTensor\n        if torch.prod(kernel_size) == 1 and torch.prod(stride) == 1:\n            self.kernel_shape = (self.in_channels, self.out_channels)\n            self.use_mm = True\n        else:\n            self.kernel_shape = (self.kernel_volume, self.in_channels,\n                                 self.out_channels)\n\n        self.kernel = Parameter(Tensor(*self.kernel_shape))\n        self.bias = Parameter(Tensor(1, out_channels)) if has_bias else None\n        self.has_bias = has_bias\n\n    def forward(self,\n                input: SparseTensor,\n                coords: Union[torch.IntTensor, CoordsKey, SparseTensor] = None):\n        r""""""\n        :attr:`input` (`MinkowskiEngine.SparseTensor`): Input sparse tensor to apply a\n        convolution on.\n\n        :attr:`coords` ((`torch.IntTensor`, `MinkowskiEngine.CoordsKey`,\n        `MinkowskiEngine.SparseTensor`), optional): If provided, generate\n        results on the provided coordinates. None by default.\n\n        """"""\n        assert isinstance(input, SparseTensor)\n        assert input.D == self.dimension\n\n        # Create a region_offset\n        self.region_type_, self.region_offset_, _ = \\\n            self.kernel_generator.get_kernel(input.tensor_stride, self.is_transpose)\n\n        if self.use_mm and coords is None:\n            # If the kernel_size == 1, the convolution is simply a matrix\n            # multiplication\n            outfeat = input.F.mm(self.kernel)\n            out_coords_key = input.coords_key\n        else:\n            if self.is_transpose:\n                conv = MinkowskiConvolutionTransposeFunction()\n            else:\n                conv = MinkowskiConvolutionFunction()\n            # Get a new coords key or extract one from the coords\n            out_coords_key = _get_coords_key(input, coords)\n            outfeat = conv.apply(input.F, self.kernel, input.tensor_stride,\n                                 self.stride, self.kernel_size, self.dilation,\n                                 self.region_type_, self.region_offset_,\n                                 input.coords_key, out_coords_key,\n                                 input.coords_man)\n        if self.has_bias:\n            outfeat += self.bias\n\n        return SparseTensor(\n            outfeat, coords_key=out_coords_key, coords_manager=input.coords_man)\n\n    def reset_parameters(self, is_transpose=False):\n        n = (self.out_channels\n             if is_transpose else self.in_channels) * self.kernel_volume\n        stdv = 1. / math.sqrt(n)\n        self.kernel.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.uniform_(-stdv, stdv)\n\n    def __repr__(self):\n        s = \'(in={}, out={}, region_type={}, \'.format(\n            self.in_channels, self.out_channels,\n            self.kernel_generator.region_type)\n        if self.kernel_generator.region_type in [\n                RegionType.HYBRID, RegionType.CUSTOM\n        ]:\n            s += \'kernel_volume={}, \'.format(self.kernel_volume)\n        else:\n            s += \'kernel_size={}, \'.format(self.kernel_size.tolist())\n        s += \'stride={}, dilation={})\'.format(self.stride.tolist(),\n                                              self.dilation.tolist())\n        return self.__class__.__name__ + s\n\n\nclass MinkowskiConvolution(MinkowskiConvolutionBase):\n    r""""""Convolution layer for a sparse tensor.\n\n\n    .. math::\n\n        \\mathbf{x}_\\mathbf{u} = \\sum_{\\mathbf{i} \\in \\mathcal{N}^D(\\mathbf{u}, K,\n        \\mathcal{C}^\\text{in})} W_\\mathbf{i} \\mathbf{x}_{\\mathbf{i} +\n        \\mathbf{u}} \\;\\text{for} \\; \\mathbf{u} \\in \\mathcal{C}^\\text{out}\n\n    where :math:`K` is the kernel size and :math:`\\mathcal{N}^D(\\mathbf{u}, K,\n    \\mathcal{C}^\\text{in})` is the set of offsets that are at most :math:`\\left\n    \\lceil{\\frac{1}{2}(K - 1)} \\right \\rceil` away from :math:`\\mathbf{u}`\n    definied in :math:`\\mathcal{S}^\\text{in}`.\n\n    .. note::\n        For even :math:`K`, the kernel offset :math:`\\mathcal{N}^D`\n        implementation is different from the above definition. The offsets\n        range from :math:`\\mathbf{i} \\in [0, K)^D, \\; \\mathbf{i} \\in\n        \\mathbb{Z}_+^D`.\n\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size=-1,\n                 stride=1,\n                 dilation=1,\n                 has_bias=False,\n                 kernel_generator=None,\n                 dimension=None):\n        r""""""convolution on a sparse tensor\n\n        Args:\n            :attr:`in_channels` (int): the number of input channels in the\n            input tensor.\n\n            :attr:`out_channels` (int): the number of output channels in the\n            output tensor.\n\n            :attr:`kernel_size` (int, optional): the size of the kernel in the\n            output tensor. If not provided, :attr:`region_offset` should be\n            :attr:`RegionType.CUSTOM` and :attr:`region_offset` should be a 2D\n            matrix with size :math:`N\\times D` such that it lists all :math:`N`\n            offsets in D-dimension.\n\n            :attr:`stride` (int, or list, optional): stride size of the\n            convolution layer. If non-identity is used, the output coordinates\n            will be at least :attr:`stride` :math:`\\times` :attr:`tensor_stride`\n            away. When a list is given, the length must be D; each element will\n            be used for stride size for the specific axis.\n\n            :attr:`dilation` (int, or list, optional): dilation size for the\n            convolution kernel. When a list is given, the length must be D and\n            each element is an axis specific dilation. All elements must be > 0.\n\n            :attr:`has_bias` (bool, optional): if True, the convolution layer\n            has a bias.\n\n            :attr:`kernel_generator` (:attr:`MinkowskiEngine.KernelGenerator`,\n            optional): defines custom kernel shape.\n\n            :attr:`dimension` (int): the spatial dimension of the space where\n            all the inputs and the network are defined. For example, images are\n            in a 2D space, meshes and 3D shapes are in a 3D space.\n\n        """"""\n        MinkowskiConvolutionBase.__init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            dilation,\n            has_bias,\n            kernel_generator,\n            is_transpose=False,\n            dimension=dimension)\n        self.reset_parameters()\n\n\nclass MinkowskiConvolutionTranspose(MinkowskiConvolutionBase):\n    r""""""A generalized sparse transposed convolution or deconvolution layer.\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size=-1,\n                 stride=1,\n                 dilation=1,\n                 has_bias=False,\n                 kernel_generator=None,\n                 generate_new_coords=False,\n                 dimension=None):\n        r""""""a generalized sparse transposed convolution layer.\n\n        Args:\n            :attr:`in_channels` (int): the number of input channels in the\n            input tensor.\n\n            :attr:`out_channels` (int): the number of output channels in the\n            output tensor.\n\n            :attr:`kernel_size` (int, optional): the size of the kernel in the\n            output tensor. If not provided, :attr:`region_offset` should be\n            :attr:`RegionType.CUSTOM` and :attr:`region_offset` should be a 2D\n            matrix with size :math:`N\\times D` such that it lists all :math:`N`\n            offsets in D-dimension.\n\n            :attr:`stride` (int, or list, optional): stride size that defines\n            upsampling rate. If non-identity is used, the output coordinates\n            will be :attr:`tensor_stride` / :attr:`stride` apart.  When a list is\n            given, the length must be D; each element will be used for stride\n            size for the specific axis.\n\n            :attr:`dilation` (int, or list, optional): dilation size for the\n            convolution kernel. When a list is given, the length must be D and\n            each element is an axis specific dilation. All elements must be > 0.\n\n            :attr:`has_bias` (bool, optional): if True, the convolution layer\n            has a bias.\n\n            :attr:`kernel_generator` (:attr:`MinkowskiEngine.KernelGenerator`,\n            optional): defines custom kernel shape.\n\n            :attr:`generate_new_coords` (bool, optional): Force generation of\n            new coordinates. When True, the output coordinates will be the\n            outer product of the kernel shape and the input coordinates.\n            `False` by defaul.\n\n            :attr:`dimension` (int): the spatial dimension of the space where\n            all the inputs and the network are defined. For example, images are\n            in a 2D space, meshes and 3D shapes are in a 3D space.\n\n        .. note:\n            TODO: support `kernel_size` > `stride`.\n\n        """"""\n        MinkowskiConvolutionBase.__init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            dilation,\n            has_bias,\n            kernel_generator,\n            is_transpose=True,\n            dimension=dimension)\n        self.reset_parameters(True)\n        self.generate_new_coords = generate_new_coords\n\n    def forward(self,\n                input: SparseTensor,\n                coords: Union[torch.IntTensor, CoordsKey, SparseTensor] = None):\n        r""""""\n        :attr:`input` (`MinkowskiEngine.SparseTensor`): Input sparse tensor to apply a\n        convolution on.\n\n        :attr:`coords` ((`torch.IntTensor`, `MinkowskiEngine.CoordsKey`,\n        `MinkowskiEngine.SparseTensor`), optional): If provided, generate\n        results on the provided coordinates. None by default.\n\n        """"""\n        assert isinstance(input, SparseTensor)\n        assert input.D == self.dimension\n\n        # Create a region_offset\n        self.region_type_, self.region_offset_, _ = \\\n            self.kernel_generator.get_kernel(input.tensor_stride, self.is_transpose)\n\n        if self.use_mm and coords is None:\n            # If the kernel_size == 1, the convolution is simply a matrix\n            # multiplication\n            outfeat = input.F.mm(self.kernel)\n            out_coords_key = input.coords_key\n        else:\n            # Get a new coords key or extract one from the coords\n            out_coords_key = _get_coords_key(input, coords, tensor_stride=1)\n            outfeat = MinkowskiConvolutionTransposeFunction().apply(\n                input.F, self.kernel, input.tensor_stride, self.stride,\n                self.kernel_size, self.dilation, self.region_type_,\n                self.region_offset_, self.generate_new_coords, input.coords_key,\n                out_coords_key, input.coords_man)\n        if self.has_bias:\n            outfeat += self.bias\n\n        return SparseTensor(\n            outfeat, coords_key=out_coords_key, coords_manager=input.coords_man)\n'"
MinkowskiEngine/MinkowskiCoords.py,22,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport os\nimport numpy as np\nfrom collections import Sequence\nfrom typing import Union, List\n\nimport torch\nfrom Common import convert_to_int_list, convert_to_int_tensor, prep_args\nimport MinkowskiEngineBackend as MEB\nfrom MinkowskiEngineBackend import MemoryManagerBackend\n\nCPU_COUNT = os.cpu_count()\nif \'OMP_NUM_THREADS\' in os.environ:\n    CPU_COUNT = int(os.environ[\'OMP_NUM_THREADS\'])\n\n_memory_manager_backend = MemoryManagerBackend.PYTORCH\n\n\ndef set_memory_manager_backend(backend: MemoryManagerBackend):\n    r""""""Set the GPU memory manager backend\n\n    By default, the Minkowski Engine will use the pytorch memory pool to\n    allocate temporary GPU memory slots. This allows the pytorch backend to\n    effectively reuse the memory pool shared between the pytorch backend and\n    the Minkowski Engine. It tends to allow training with larger batch sizes\n    given a fixed GPU memory. However, pytorch memory manager tend to be slower\n    than allocating GPU directly using raw CUDA calls.\n\n    By default, the Minkowski Engine uses\n    :attr:`ME.MemoryManagerBackend.PYTORCH` for memory management.\n\n    Example::\n\n       >>> import MinkowskiEngine as ME\n       >>> # Set the GPU memory manager backend to raw CUDA calls\n       >>> ME.set_memory_manager_backend(ME.MemoryManagerBackend.CUDA)\n       >>> # Set the GPU memory manager backend to the pytorch memory pool\n       >>> ME.set_memory_manager_backend(ME.MemoryManagerBackend.PYTORCH)\n\n    """"""\n    assert isinstance(backend, MemoryManagerBackend), \\\n        f""Input must be an instance of MemoryManagerBackend not {backend}""\n    global _memory_manager_backend\n    _memory_manager_backend = backend\n\n\nclass CoordsKey():\n\n    def __init__(self, D):\n        self.D = D\n        self.CPPCoordsKey = MEB.CoordsKey()\n        self.CPPCoordsKey.setDimension(D)\n\n    def isKeySet(self):\n        return self.CPPCoordsKey.isKeySet()\n\n    def setKey(self, key):\n        self.CPPCoordsKey.setKey(key)\n\n    def getKey(self):\n        return self.CPPCoordsKey.getKey()\n\n    def setTensorStride(self, tensor_stride):\n        tensor_stride = convert_to_int_list(tensor_stride, self.D)\n        self.CPPCoordsKey.setTensorStride(tensor_stride)\n\n    def getTensorStride(self):\n        return self.CPPCoordsKey.getTensorStride()\n\n    def __repr__(self):\n        return str(self.CPPCoordsKey)\n\n    def __eq__(self, other):\n        assert isinstance(other, CoordsKey)\n        return self.getKey() == other.getKey()\n\n\nclass CoordsManager():\n\n    def __init__(self,\n                 num_threads: int = -1,\n                 memory_manager_backend: MemoryManagerBackend = None,\n                 D: int = -1):\n        if D < 1:\n            raise ValueError(f""Invalid dimension {D}"")\n        self.D = D\n        if num_threads < 0:\n            num_threads = min(CPU_COUNT, 20)\n        if memory_manager_backend is None:\n            global _memory_manager_backend\n            memory_manager_backend = _memory_manager_backend\n        coords_man = MEB.CoordsManager(num_threads, memory_manager_backend)\n        self.CPPCoordsManager = coords_man\n\n    def initialize(self,\n                   coords: torch.IntTensor,\n                   coords_key: CoordsKey,\n                   force_creation: bool = False,\n                   force_remap: bool = False,\n                   allow_duplicate_coords: bool = False,\n                   return_inverse: bool = False) -> torch.LongTensor:\n        assert isinstance(coords_key, CoordsKey)\n        unique_index = torch.LongTensor()\n        inverse_mapping = torch.LongTensor()\n        self.CPPCoordsManager.initializeCoords(\n            coords, unique_index, inverse_mapping, coords_key.CPPCoordsKey,\n            force_creation, force_remap, allow_duplicate_coords, return_inverse)\n        return unique_index, inverse_mapping\n\n    def create_coords_key(self,\n                          coords: torch.IntTensor,\n                          tensor_stride: int = 1,\n                          force_creation: bool = False,\n                          force_remap: bool = False,\n                          allow_duplicate_coords: bool = False) -> CoordsKey:\n        coords_key = CoordsKey(self.D)\n        coords_key.setTensorStride(tensor_stride)\n        unique_index, inverse_mapping = self.initialize(\n            coords,\n            coords_key,\n            force_creation=True,\n            force_remap=True,\n            allow_duplicate_coords=True)\n        # Set the tensor stride\n        tensor_stride = convert_to_int_list(tensor_stride, self.D)\n        coords_key.setTensorStride(tensor_stride)\n\n        return coords_key\n\n    def stride(self,\n               coords_key: CoordsKey,\n               stride: Union[int, Sequence, np.ndarray, torch.Tensor],\n               force_creation: bool = False):\n        assert isinstance(coords_key, CoordsKey)\n        stride = convert_to_int_list(stride, self.D)\n\n        strided_key = CoordsKey(self.D)\n        tensor_stride = coords_key.getTensorStride()\n        strided_key.setTensorStride(\n            [t * s for t, s in zip(tensor_stride, stride)])\n\n        # Identity stride will return the same coords key.\n        strided_key.setKey(\n            self.CPPCoordsManager.createStridedCoords(coords_key.getKey(),\n                                                      tensor_stride, stride,\n                                                      force_creation))\n        return strided_key\n\n    def reduce(self):\n        origin_key = CoordsKey(self.D)\n        origin_key.setTensorStride(convert_to_int_list(0, self.D))\n        origin_key.setKey(self.CPPCoordsManager.createOriginCoords(self.D))\n        return origin_key\n\n    def transposed_stride(\n            self,\n            coords_key: CoordsKey,\n            stride: Union[int, Sequence, np.ndarray, torch.Tensor],\n            kernel_size: Union[int, Sequence, np.ndarray, torch.Tensor],\n            dilation: Union[int, Sequence, np.ndarray, torch.Tensor],\n            force_creation: bool = False):\n        assert isinstance(coords_key, CoordsKey)\n        stride = convert_to_int_list(stride, self.D)\n        kernel_size = convert_to_int_list(kernel_size, self.D)\n        dilation = convert_to_int_list(dilation, self.D)\n        region_type = 0\n        region_offset = torch.IntTensor()\n\n        strided_key = CoordsKey(self.D)\n        tensor_stride = coords_key.getTensorStride()\n        strided_key.setTensorStride(\n            [int(t / s) for t, s in zip(tensor_stride, stride)])\n\n        strided_key.setKey(\n            self.CPPCoordsManager.createTransposedStridedRegionCoords(\n                coords_key.getKey(), coords_key.getTensorStride(), stride,\n                kernel_size, dilation, region_type, region_offset,\n                force_creation))\n        return strided_key\n\n    def _get_coords_key(self, key_or_tensor_strides):\n        assert isinstance(key_or_tensor_strides, CoordsKey) or \\\n            isinstance(key_or_tensor_strides, (Sequence, np.ndarray, torch.IntTensor, int)), \\\n            f""The input must be either a CoordsKey or tensor_stride of type (int, list, tuple, array, Tensor). Invalid: {key_or_tensor_strides}""\n        if isinstance(key_or_tensor_strides, CoordsKey):\n            # Do nothing and return the input\n            return key_or_tensor_strides\n        else:\n            tensor_strides = convert_to_int_list(key_or_tensor_strides, self.D)\n            key = self.CPPCoordsManager.getCoordsKey(tensor_strides)\n            coords_key = CoordsKey(self.D)\n            coords_key.setKey(key)\n            coords_key.setTensorStride(tensor_strides)\n            return coords_key\n\n    def get_coords(self, coords_key_or_tensor_strides):\n        coords_key = self._get_coords_key(coords_key_or_tensor_strides)\n        coords = torch.IntTensor()\n        self.CPPCoordsManager.getCoords(coords, coords_key.CPPCoordsKey)\n        return coords\n\n    def get_batch_size(self):\n        return self.CPPCoordsManager.getBatchSize()\n\n    def get_batch_indices(self):\n        return self.CPPCoordsManager.getBatchIndices()\n\n    def set_origin_coords_key(self, coords_key: CoordsKey):\n        self.CPPCoordsManager.setOriginCoordsKey(coords_key.CPPCoordsKey)\n\n    def get_row_indices_per_batch(self, coords_key, out_coords_key=None):\n        r""""""Return a list of lists of row indices per batch.\n\n        The corresponding batch indices are accessible by `get_batch_indices`.\n\n        .. code-block:: python\n\n           sp_tensor = ME.SparseTensor(features, coords=coordinates)\n           row_indices = sp_tensor.coords_man.get_row_indices_per_batch(sp_tensor.coords_key)\n\n        """"""\n        assert isinstance(coords_key, CoordsKey)\n        if out_coords_key is None:\n            out_coords_key = CoordsKey(self.D)\n        return self.CPPCoordsManager.getRowIndicesPerBatch(\n            coords_key.CPPCoordsKey, out_coords_key.CPPCoordsKey)\n\n    def get_row_indices_at(self, coords_key, batch_index):\n        r""""""Return an torch.LongTensor of row indices for the specified batch index\n\n        .. code-block:: python\n\n           sp_tensor = ME.SparseTensor(features, coords=coordinates)\n           row_indices = sp_tensor.coords_man.get_row_indices_at(sp_tensor.coords_key, batch_index)\n\n        """"""\n        assert isinstance(coords_key, CoordsKey)\n        out_coords_key = CoordsKey(self.D)\n        return self.CPPCoordsManager.getRowIndicesAtBatchIndex(\n            coords_key.CPPCoordsKey, out_coords_key.CPPCoordsKey, batch_index)\n\n    def get_kernel_map(self,\n                       in_key_or_tensor_strides,\n                       out_key_or_tensor_strides,\n                       stride=1,\n                       kernel_size=3,\n                       dilation=1,\n                       region_type=0,\n                       region_offset=None,\n                       is_transpose=False,\n                       is_pool=False,\n                       on_gpu=False):\n        r""""""Get kernel in-out maps for the specified coords keys or tensor strides.\n\n        """"""\n        # region type 1 iteration with kernel_size 1 is invalid\n        if isinstance(kernel_size, torch.Tensor):\n            assert (kernel_size >\n                    0).all(), f""Invalid kernel size: {kernel_size}""\n            if (kernel_size == 1).all() == 1:\n                region_type = 0\n        elif isinstance(kernel_size, int):\n            assert kernel_size > 0, f""Invalid kernel size: {kernel_size}""\n            if kernel_size == 1:\n                region_type = 0\n\n        if isinstance(in_key_or_tensor_strides, CoordsKey):\n            in_tensor_strides = in_key_or_tensor_strides.getTensorStride()\n        else:\n            in_tensor_strides = in_key_or_tensor_strides\n        if region_offset is None:\n            region_offset = torch.IntTensor()\n\n        in_coords_key = self._get_coords_key(in_key_or_tensor_strides)\n        out_coords_key = self._get_coords_key(out_key_or_tensor_strides)\n\n        tensor_strides = convert_to_int_tensor(in_tensor_strides, self.D)\n        strides = convert_to_int_tensor(stride, self.D)\n        kernel_sizes = convert_to_int_tensor(kernel_size, self.D)\n        dilations = convert_to_int_tensor(dilation, self.D)\n        D = in_coords_key.D\n        tensor_strides, strides, kernel_sizes, dilations, region_type = prep_args(\n            tensor_strides, strides, kernel_sizes, dilations, region_type, D)\n        if on_gpu:\n            assert hasattr(\n                self.CPPCoordsManager, \'getKernelMapGPU\'\n            ), f""Function getKernelMapGPU not available. Please compile MinkowskiEngine where `torch.cuda.is_available()` is `True`.""\n            kernel_map_fn = getattr(self.CPPCoordsManager, \'getKernelMapGPU\')\n        else:\n            kernel_map_fn = self.CPPCoordsManager.getKernelMap\n        kernel_map = kernel_map_fn(\n            convert_to_int_list(tensor_strides, D),  #\n            convert_to_int_list(strides, D),  #\n            convert_to_int_list(kernel_sizes, D),  #\n            convert_to_int_list(dilations, D),  #\n            region_type,\n            region_offset,\n            in_coords_key.CPPCoordsKey,\n            out_coords_key.CPPCoordsKey,\n            is_transpose,\n            is_pool)\n\n        return kernel_map\n\n    def get_coords_map(self, in_key_or_tensor_strides,\n                       out_key_or_tensor_strides):\n        r""""""Extract input coords indices that maps to output coords indices.\n\n        .. code-block:: python\n\n           sp_tensor = ME.SparseTensor(features, coords=coordinates)\n           out_sp_tensor = stride_2_conv(sp_tensor)\n\n           cm = sp_tensor.coords_man\n           # cm = out_sp_tensor.coords_man  # doesn\'t matter which tensor you pick\n           ins, outs = cm.get_coords_map(1,  # in stride\n                                         2)  # out stride\n           for i, o in zip(ins, outs):\n              print(f""{i} -> {o}"")\n\n        """"""\n        in_coords_key = self._get_coords_key(in_key_or_tensor_strides)\n        out_coords_key = self._get_coords_key(out_key_or_tensor_strides)\n\n        return self.CPPCoordsManager.getCoordsMap(in_coords_key.CPPCoordsKey,\n                                                  out_coords_key.CPPCoordsKey)\n\n    def get_union_map(self, in_keys: List[CoordsKey], out_key: CoordsKey):\n        r""""""Generates a union of coordinate sets and returns the mapping from input sets to the new output coordinates.\n\n        Args:\n            :attr:`in_keys` (List[CoordsKey]): A list of coordinate keys to\n            create a union on.\n\n            :attr:`out_key` (CoordsKey): the placeholder for the coords key of\n            the generated union coords hash map.\n\n        Returns:\n            :attr:`in_maps` (List[Tensor[int]]): A list of long tensors that contain mapping from inputs to the union output. Please see the example for more details.\n            :attr:`out_maps` (List[Tensor[int]]): A list of long tensors that contain a mapping from input to the union output. Please see the example for more details.\n\n        Example::\n\n            >>> # Adding two sparse tensors: A, B\n            >>> out_key = CoordsKey(coords_man.D)\n            >>> ins, outs = coords_man.get_union_map((A.coords_key, B.coords_key), out_key)\n            >>> N = coords_man.get_coords_size_by_coords_key(out_key)\n            >>> out_F = torch.zeros((N, A.F.size(1)), dtype=A.dtype)\n            >>> out_F[outs[0]] = A.F[ins[0]]\n            >>> out_F[outs[1]] += B.F[ins[1]]\n\n        """"""\n        return self.CPPCoordsManager.getUnionMap(\n            [key.CPPCoordsKey for key in in_keys], out_key.CPPCoordsKey)\n\n    def get_coords_size_by_coords_key(self, coords_key):\n        assert isinstance(coords_key, CoordsKey)\n        return self.CPPCoordsManager.getCoordsSize(coords_key.CPPCoordsKey)\n\n    def get_mapping_by_tensor_strides(self, in_tensor_strides,\n                                      out_tensor_strides):\n        in_key = self._get_coords_key(in_tensor_strides)\n        out_key = self._get_coords_key(out_tensor_strides)\n        return self.get_mapping_by_coords_key(in_key, out_key)\n\n    def permute_label(self,\n                      label,\n                      max_label,\n                      target_tensor_stride,\n                      label_tensor_stride=1):\n        if target_tensor_stride == label_tensor_stride:\n            return label\n\n        label_coords_key = self._get_coords_key(label_tensor_stride)\n        target_coords_key = self._get_coords_key(target_tensor_stride)\n\n        permutation = self.get_mapping_by_coords_key(label_coords_key,\n                                                     target_coords_key)\n        nrows = self.get_coords_size_by_coords_key(target_coords_key)\n\n        label = label.contiguous().numpy()\n        permutation = permutation.numpy()\n\n        counter = np.zeros((nrows, max_label), dtype=\'int32\')\n        np.add.at(counter, (permutation, label), 1)\n        return torch.from_numpy(np.argmax(counter, 1))\n\n    def print_diagnostics(self, coords_key: CoordsKey):\n        assert isinstance(coords_key, CoordsKey)\n        self.CPPCoordsManager.printDiagnostics(coords_key.CPPCoordsKey)\n\n    def __repr__(self):\n        return str(self.CPPCoordsManager)\n\n\ndef save_ctx(\n        ctx,  # function object context\n        tensor_stride: torch.IntTensor,\n        stride: torch.IntTensor,\n        kernel_size: torch.IntTensor,\n        dilation: torch.IntTensor,\n        region_type: int,\n        in_coords_key: CoordsKey,\n        out_coords_key: CoordsKey,\n        coords_man: CoordsManager):\n    ctx.tensor_stride = tensor_stride\n    ctx.stride = stride\n    ctx.kernel_size = kernel_size\n    ctx.dilation = dilation\n    ctx.region_type = region_type\n    ctx.in_coords_key = in_coords_key\n    ctx.out_coords_key = out_coords_key\n    ctx.coords_man = coords_man\n    return ctx\n'"
MinkowskiEngine/MinkowskiFunctional.py,1,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport torch.nn.functional as F\n\nfrom SparseTensor import SparseTensor\n\n\n# Activations\ndef relu(input, *args, **kwargs):\n  output = F.relu(input.F, *args, **kwargs)\n  return SparseTensor(output, coords_key=input.coords_key, coords_manager=input.coords_man)\n\n\ndef prelu(input, *args, **kwargs):\n  output = F.prelu(input.F, *args, **kwargs)\n  return SparseTensor(output, coords_key=input.coords_key, coords_manager=input.coords_man)\n\n\ndef leaky_relu(input, *args, **kwargs):\n  output = F.leaky_relu(input.F, *args, **kwargs)\n  return SparseTensor(output, coords_key=input.coords_key, coords_manager=input.coords_man)\n\n\ndef elu(input, *args, **kwargs):\n  output = F.elu(input.F, *args, **kwargs)\n  return SparseTensor(output, coords_key=input.coords_key, coords_manager=input.coords_man)\n\n\ndef celu(input, *args, **kwargs):\n  output = F.celu(input.F, *args, **kwargs)\n  return SparseTensor(output, coords_key=input.coords_key, coords_manager=input.coords_man)\n\n\ndef softmax(input, *args, **kwargs):\n  output = F.softmax(input.F, *args, **kwargs)\n  return SparseTensor(output, coords_key=input.coords_key, coords_manager=input.coords_man)\n\n\ndef log_softmax(input, *args, **kwargs):\n  output = F.log_softmax(input.F, *args, **kwargs)\n  return SparseTensor(output, coords_key=input.coords_key, coords_manager=input.coords_man)\n\n\ndef sigmoid(input, *args, **kwargs):\n  output = F.sigmoid(input.F, *args, **kwargs)\n  return SparseTensor(output, coords_key=input.coords_key, coords_manager=input.coords_man)\n\n\ndef tanh(input, *args, **kwargs):\n  output = F.tanh(input.F, *args, **kwargs)\n  return SparseTensor(output, coords_key=input.coords_key, coords_manager=input.coords_man)\n\n\n# Dropouts\ndef dropout(input, *args, **kwargs):\n  output = F.dropout(input.F, *args, **kwargs)\n  return SparseTensor(output, coords_key=input.coords_key, coords_manager=input.coords_man)\n\n\n# Normalization\ndef normalize(input, *args, **kwargs):\n  output = F.normalize(input.F, *args, **kwargs)\n  return SparseTensor(output, coords_key=input.coords_key, coords_manager=input.coords_man)\n\n\n# Loss functions\ndef binary_cross_entropy(input, target, *args, **kwargs):\n  output = F.binary_cross_entropy(input.F, target, *args, **kwargs)\n  return SparseTensor(output, coords_key=input.coords_key, coords_manager=input.coords_man)\n\n\ndef binary_cross_entropy_with_logits(input, target, *args, **kwargs):\n  output = F.binary_cross_entropy_with_logits(input.F, target, *args, **kwargs)\n  return SparseTensor(output, coords_key=input.coords_key, coords_manager=input.coords_man)\n\n\ndef cross_entropy(input, target, *args, **kwargs):\n  output = F.cross_entropy(input.F, target, *args, **kwargs)\n  return SparseTensor(output, coords_key=input.coords_key, coords_manager=input.coords_man)\n\n\ndef kl_div(input, target, *args, **kwargs):\n  output = F.kl_div(input.F, target, *args, **kwargs)\n  return SparseTensor(output, coords_key=input.coords_key, coords_manager=input.coords_man)\n\n\ndef l1_loss(input, target, *args, **kwargs):\n  output = F.l1_loss(input.F, target, *args, **kwargs)\n  return SparseTensor(output, coords_key=input.coords_key, coords_manager=input.coords_man)\n\n\ndef mse_loss(input, target, *args, **kwargs):\n  output = F.mse_loss(input.F, target, *args, **kwargs)\n  return SparseTensor(output, coords_key=input.coords_key, coords_manager=input.coords_man)\n\n\ndef nll_loss(input, target, *args, **kwargs):\n  output = F.nll_loss(input.F, target, *args, **kwargs)\n  return SparseTensor(output, coords_key=input.coords_key, coords_manager=input.coords_man)\n'"
MinkowskiEngine/MinkowskiNetwork.py,6,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nfrom abc import ABC, abstractmethod\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\n\nimport MinkowskiEngineBackend as MEB\nfrom SparseTensor import SparseTensor\nfrom Common import convert_to_int_tensor\n\n\nclass MinkowskiNetwork(nn.Module, ABC):\n    """"""\n    MinkowskiNetwork: an abstract class for sparse convnets.\n\n    Note: All modules that use the same coordinates must use the same net_metadata\n    """"""\n\n    def __init__(self, D):\n        super(MinkowskiNetwork, self).__init__()\n        self.D = D\n\n    @abstractmethod\n    def forward(self, x):\n        pass\n\n    def init(self, x):\n        """"""\n        Initialize coordinates if it does not exist\n        """"""\n        nrows = self.get_nrows(1)\n        if nrows < 0:\n            if isinstance(x, SparseTensor):\n                self.initialize_coords(x.coords_man)\n            else:\n                raise ValueError(\'Initialize input coordinates\')\n        elif nrows != x.F.size(0):\n            raise ValueError(\'Input size does not match the coordinate size\')\n\n    def get_index_map(self, coords, tensor_stride):\n        r""""""\n        Get the current coords (with duplicates) index map.\n\n        If `tensor_stride > 1`, use\n\n        .. code-block:: python\n\n           coords = torch.cat(((coords[:, :D] / tensor_stride) * tensor_stride, coords[:, D:]), dim=1)\n\n        """"""\n        assert isinstance(coords, torch.IntTensor), ""Coord must be IntTensor""\n        index_map = torch.IntTensor()\n        tensor_stride = convert_to_int_tensor(tensor_stride, self.D)\n        success = MEB.get_index_map(coords.contiguous(), index_map,\n                                    tensor_stride, self.D,\n                                    self.net_metadata.ffi)\n        if success < 0:\n            raise ValueError(\'get_index_map failed\')\n        return index_map\n\n    def permute_label(self, label, max_label, tensor_stride):\n        if tensor_stride == 1 or np.prod(tensor_stride) == 1:\n            return label\n\n        tensor_stride = convert_to_int_tensor(tensor_stride, self.D)\n        permutation = self.get_permutation(tensor_stride, 1)\n        nrows = self.get_nrows(tensor_stride)\n\n        label = label.contiguous().numpy()\n        permutation = permutation.numpy()\n\n        counter = np.zeros((nrows, max_label), dtype=\'int32\')\n        np.add.at(counter, (permutation, label), 1)\n        return torch.from_numpy(np.argmax(counter, 1))\n\n    def permute_feature(self, feat, tensor_stride, dtype=np.float32):\n        tensor_stride = convert_to_int_tensor(tensor_stride, self.D)\n        permutation = self.get_permutation(tensor_stride, 1)\n        nrows = self.get_nrows(tensor_stride)\n\n        feat_np = feat.contiguous().numpy()\n        warped_feat = np.zeros((nrows, feat.size(1)), dtype=dtype)\n        counter = np.zeros((nrows, 1), dtype=\'int32\')\n        for j in range(feat.size(1)):\n            np.add.at(warped_feat, (permutation, j), feat_np[:, j])\n        np.add.at(counter, permutation, 1)\n        warped_feat = warped_feat / counter\n        return torch.from_numpy(warped_feat)\n'"
MinkowskiEngine/MinkowskiNonlinearity.py,11,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport torch\nfrom torch.nn import Module\n\nfrom SparseTensor import SparseTensor\n\n\nclass MinkowskiModuleBase(Module):\n    MODULE = None\n\n    def __init__(self, *args, **kwargs):\n        super(MinkowskiModuleBase, self).__init__()\n        self.module = self.MODULE(*args, **kwargs)\n\n    def forward(self, input):\n        output = self.module(input.F)\n        return SparseTensor(\n            output,\n            coords_key=input.coords_key,\n            coords_manager=input.coords_man)\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'()\'\n\n\nclass MinkowskiReLU(MinkowskiModuleBase):\n    MODULE = torch.nn.ReLU\n\n\nclass MinkowskiPReLU(MinkowskiModuleBase):\n    MODULE = torch.nn.PReLU\n\n\nclass MinkowskiELU(MinkowskiModuleBase):\n    MODULE = torch.nn.ELU\n\n\nclass MinkowskiSELU(MinkowskiModuleBase):\n    MODULE = torch.nn.SELU\n\n\nclass MinkowskiCELU(MinkowskiModuleBase):\n    MODULE = torch.nn.CELU\n\n\nclass MinkowskiDropout(MinkowskiModuleBase):\n    MODULE = torch.nn.Dropout\n\n\nclass MinkowskiThreshold(MinkowskiModuleBase):\n    MODULE = torch.nn.Threshold\n\n\nclass MinkowskiSigmoid(MinkowskiModuleBase):\n    MODULE = torch.nn.Sigmoid\n\n\nclass MinkowskiTanh(MinkowskiModuleBase):\n    MODULE = torch.nn.Tanh\n\n\nclass MinkowskiSoftmax(MinkowskiModuleBase):\n    MODULE = torch.nn.Softmax\n'"
MinkowskiEngine/MinkowskiNormalization.py,14,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Module\nfrom torch.autograd import Function\n\nfrom SparseTensor import SparseTensor\nfrom MinkowskiPooling import MinkowskiGlobalPooling\nfrom MinkowskiBroadcast import MinkowskiBroadcastAddition, MinkowskiBroadcastMultiplication, OperationType, operation_type_to_int\nfrom MinkowskiCoords import CoordsKey\nfrom Common import get_minkowski_function, GlobalPoolingMode\n\n\nclass MinkowskiBatchNorm(Module):\n    r""""""A batch normalization layer for a sparse tensor.\n\n    See the pytorch :attr:`torch.nn.BatchNorm1d` for more details.\n    """"""\n\n    def __init__(self,\n                 num_features,\n                 eps=1e-5,\n                 momentum=0.1,\n                 affine=True,\n                 track_running_stats=True):\n        super(MinkowskiBatchNorm, self).__init__()\n        self.bn = torch.nn.BatchNorm1d(\n            num_features,\n            eps=eps,\n            momentum=momentum,\n            affine=affine,\n            track_running_stats=track_running_stats)\n\n    def forward(self, input):\n        output = self.bn(input.F)\n        return SparseTensor(\n            output,\n            coords_key=input.coords_key,\n            coords_manager=input.coords_man)\n\n    def __repr__(self):\n        s = \'({}, eps={}, momentum={}, affine={}, track_running_stats={})\'.format(\n            self.bn.num_features, self.bn.eps, self.bn.momentum, self.bn.affine,\n            self.bn.track_running_stats)\n        return self.__class__.__name__ + s\n\n\nclass MinkowskiSyncBatchNorm(MinkowskiBatchNorm):\n    r""""""A batch normalization layer with multi GPU synchronization.\n    """"""\n\n    def __init__(self,\n                 num_features,\n                 eps=1e-5,\n                 momentum=0.1,\n                 affine=True,\n                 track_running_stats=True,\n                 process_group=None):\n        Module.__init__(self)\n        self.bn = torch.nn.SyncBatchNorm(\n            num_features,\n            eps=eps,\n            momentum=momentum,\n            affine=affine,\n            track_running_stats=track_running_stats,\n            process_group=process_group)\n\n    def forward(self, input):\n        output = self.bn(input.F)\n        return SparseTensor(\n            output,\n            coords_key=input.coords_key,\n            coords_manager=input.coords_man)\n\n    @classmethod\n    def convert_sync_batchnorm(cls, module, process_group=None):\n        r""""""Helper function to convert\n        :attr:`MinkowskiEngine.MinkowskiBatchNorm` layer in the model to\n        :attr:`MinkowskiEngine.MinkowskiSyncBatchNorm` layer.\n\n        Args:\n            module (nn.Module): containing module\n            process_group (optional): process group to scope synchronization,\n            default is the whole world\n\n        Returns:\n            The original module with the converted\n            :attr:`MinkowskiEngine.MinkowskiSyncBatchNorm` layer\n\n        Example::\n\n            >>> # Network with nn.BatchNorm layer\n            >>> module = torch.nn.Sequential(\n            >>>            torch.nn.Linear(20, 100),\n            >>>            torch.nn.BatchNorm1d(100)\n            >>>          ).cuda()\n            >>> # creating process group (optional)\n            >>> # process_ids is a list of int identifying rank ids.\n            >>> process_group = torch.distributed.new_group(process_ids)\n            >>> sync_bn_module = convert_sync_batchnorm(module, process_group)\n\n        """"""\n        module_output = module\n        if isinstance(module, MinkowskiBatchNorm):\n            module_output = MinkowskiSyncBatchNorm(\n                module.bn.num_features, module.bn.eps, module.bn.momentum,\n                module.bn.affine, module.bn.track_running_stats, process_group)\n            if module.bn.affine:\n                module_output.bn.weight.data = module.bn.weight.data.clone(\n                ).detach()\n                module_output.bn.bias.data = module.bn.bias.data.clone().detach(\n                )\n                # keep reuqires_grad unchanged\n                module_output.bn.weight.requires_grad = module.bn.weight.requires_grad\n                module_output.bn.bias.requires_grad = module.bn.bias.requires_grad\n            module_output.bn.running_mean = module.bn.running_mean\n            module_output.bn.running_var = module.bn.running_var\n            module_output.bn.num_batches_tracked = module.bn.num_batches_tracked\n        for name, child in module.named_children():\n            module_output.add_module(\n                name, cls.convert_sync_batchnorm(child, process_group))\n        del module\n        return module_output\n\n\nclass MinkowskiInstanceNormFunction(Function):\n\n    @staticmethod\n    def forward(ctx,\n                in_feat,\n                mode=GlobalPoolingMode.AUTO,\n                in_coords_key=None,\n                glob_coords_key=None,\n                coords_manager=None):\n        assert isinstance(mode, GlobalPoolingMode), \\\n            f""Mode must be an instance of GlobalPoolingMode, {mode}""\n        if glob_coords_key is None:\n            glob_coords_key = CoordsKey(in_coords_key.D)\n\n        gpool_forward = get_minkowski_function(\'GlobalPoolingForward\', in_feat)\n        broadcast_forward = get_minkowski_function(\'BroadcastForward\', in_feat)\n        add = operation_type_to_int(OperationType.ADDITION)\n        multiply = operation_type_to_int(OperationType.MULTIPLICATION)\n\n        mean = in_feat.new()\n        num_nonzero = in_feat.new()\n\n        cpp_in_coords_key = in_coords_key.CPPCoordsKey\n        cpp_glob_coords_key = glob_coords_key.CPPCoordsKey\n        cpp_coords_manager = coords_manager.CPPCoordsManager\n\n        mean, num_nonzero = gpool_forward(in_feat, cpp_in_coords_key,\n                                          cpp_glob_coords_key,\n                                          cpp_coords_manager, True, mode.value)\n        # X - \\mu\n        centered_feat = broadcast_forward(in_feat, -mean, add,\n                                          cpp_in_coords_key,\n                                          cpp_glob_coords_key,\n                                          cpp_coords_manager)\n\n        # Variance = 1/N \\sum (X - \\mu) ** 2\n        variance, num_nonzero = gpool_forward(centered_feat**2,\n                                              cpp_in_coords_key,\n                                              cpp_glob_coords_key,\n                                              cpp_coords_manager, True,\n                                              mode.value)\n\n        # norm_feat = (X - \\mu) / \\sigma\n        inv_std = 1 / (variance + 1e-8).sqrt()\n        norm_feat = broadcast_forward(centered_feat, inv_std, multiply,\n                                      cpp_in_coords_key, cpp_glob_coords_key,\n                                      cpp_coords_manager)\n\n        ctx.mode = mode\n        ctx.in_coords_key, ctx.glob_coords_key = in_coords_key, glob_coords_key\n        ctx.coords_manager = coords_manager\n        # For GPU tensors, must use save_for_backward.\n        ctx.save_for_backward(inv_std, norm_feat)\n        return norm_feat\n\n    @staticmethod\n    def backward(ctx, out_grad):\n        # https://kevinzakka.github.io/2016/09/14/batch_normalization/\n        in_coords_key, glob_coords_key = ctx.in_coords_key, ctx.glob_coords_key\n        coords_manager = ctx.coords_manager\n\n        # To prevent the memory leakage, compute the norm again\n        inv_std, norm_feat = ctx.saved_tensors\n\n        gpool_forward = get_minkowski_function(\'GlobalPoolingForward\', out_grad)\n        broadcast_forward = get_minkowski_function(\'BroadcastForward\', out_grad)\n        add = operation_type_to_int(OperationType.ADDITION)\n        multiply = operation_type_to_int(OperationType.MULTIPLICATION)\n\n        cpp_in_coords_key = in_coords_key.CPPCoordsKey\n        cpp_glob_coords_key = glob_coords_key.CPPCoordsKey\n        cpp_coords_manager = coords_manager.CPPCoordsManager\n\n        # 1/N \\sum dout\n        mean_dout, num_nonzero = gpool_forward(out_grad, cpp_in_coords_key,\n                                               cpp_glob_coords_key,\n                                               cpp_coords_manager, True,\n                                               ctx.mode.value)\n\n        # 1/N \\sum (dout * out)\n        mean_dout_feat, num_nonzero = gpool_forward(out_grad * norm_feat,\n                                                    cpp_in_coords_key,\n                                                    cpp_glob_coords_key,\n                                                    cpp_coords_manager, True,\n                                                    ctx.mode.value)\n\n        # out * 1/N \\sum (dout * out)\n        feat_mean_dout_feat = broadcast_forward(norm_feat, mean_dout_feat,\n                                                multiply, cpp_in_coords_key,\n                                                cpp_glob_coords_key,\n                                                cpp_coords_manager)\n\n        unnorm_din = broadcast_forward(out_grad - feat_mean_dout_feat,\n                                       -mean_dout, add, cpp_in_coords_key,\n                                       cpp_glob_coords_key, cpp_coords_manager)\n\n        norm_din = broadcast_forward(unnorm_din, inv_std, multiply,\n                                     cpp_in_coords_key, cpp_glob_coords_key,\n                                     cpp_coords_manager)\n\n        return norm_din, None, None, None, None\n\n\nclass MinkowskiStableInstanceNorm(Module):\n\n    def __init__(self, num_features):\n        Module.__init__(self)\n        self.num_features = num_features\n        self.eps = 1e-6\n        self.weight = nn.Parameter(torch.ones(1, num_features))\n        self.bias = nn.Parameter(torch.zeros(1, num_features))\n\n        self.mean_in = MinkowskiGlobalPooling()\n        self.glob_sum = MinkowskiBroadcastAddition()\n        self.glob_sum2 = MinkowskiBroadcastAddition()\n        self.glob_mean = MinkowskiGlobalPooling()\n        self.glob_times = MinkowskiBroadcastMultiplication()\n        self.reset_parameters()\n\n    def __repr__(self):\n        s = f\'(nchannels={self.num_features})\'\n        return self.__class__.__name__ + s\n\n    def reset_parameters(self):\n        self.weight.data.fill_(1)\n        self.bias.data.zero_()\n\n    def forward(self, x):\n        neg_mean_in = self.mean_in(\n            SparseTensor(\n                -x.F, coords_key=x.coords_key, coords_manager=x.coords_man))\n        centered_in = self.glob_sum(x, neg_mean_in)\n        temp = SparseTensor(\n            centered_in.F**2,\n            coords_key=centered_in.coords_key,\n            coords_manager=centered_in.coords_man)\n        var_in = self.glob_mean(temp)\n        instd_in = SparseTensor(\n            1 / (var_in.F + self.eps).sqrt(),\n            coords_key=var_in.coords_key,\n            coords_manager=var_in.coords_man)\n\n        x = self.glob_times(self.glob_sum2(x, neg_mean_in), instd_in)\n        return SparseTensor(\n            x.F * self.weight + self.bias,\n            coords_key=x.coords_key,\n            coords_manager=x.coords_man)\n\n\nclass MinkowskiInstanceNorm(Module):\n    r""""""A instance normalization layer for a sparse tensor.\n\n    """"""\n\n    def __init__(self, num_features, mode=GlobalPoolingMode.AUTO):\n        r""""""\n        Args:\n\n            num_features (int): the dimension of the input feautres.\n\n            mode (GlobalPoolingModel, optional): The internal global pooling computation mode.\n        """"""\n        Module.__init__(self)\n        self.num_features = num_features\n        self.weight = nn.Parameter(torch.ones(1, num_features))\n        self.bias = nn.Parameter(torch.zeros(1, num_features))\n        self.reset_parameters()\n        self.mode = mode\n        self.inst_norm = MinkowskiInstanceNormFunction()\n\n    def __repr__(self):\n        s = f\'(nchannels={self.num_features})\'\n        return self.__class__.__name__ + s\n\n    def reset_parameters(self):\n        self.weight.data.fill_(1)\n        self.bias.data.zero_()\n\n    def forward(self, input):\n        assert isinstance(input, SparseTensor)\n\n        output = self.inst_norm.apply(input.F, self.mode, input.coords_key,\n                                      None, input.coords_man)\n        output = output * self.weight + self.bias\n\n        return SparseTensor(\n            output,\n            coords_key=input.coords_key,\n            coords_manager=input.coords_man)\n'"
MinkowskiEngine/MinkowskiOps.py,3,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport torch\nfrom torch.nn.modules import Module\nfrom SparseTensor import SparseTensor, COORDS_MAN_DIFFERENT_ERROR, COORDS_KEY_DIFFERENT_ERROR\n\n\nclass MinkowskiLinear(Module):\n\n    def __init__(self, in_features, out_features, bias=True):\n        super(MinkowskiLinear, self).__init__()\n        self.linear = torch.nn.Linear(in_features, out_features, bias=bias)\n\n    def forward(self, input):\n        output = self.linear(input.F)\n        return SparseTensor(\n            output,\n            coords_key=input.coords_key,\n            coords_manager=input.coords_man)\n\n    def __repr__(self):\n        s = \'(in_features={}, out_features={}, bias={})\'.format(\n            self.linear.in_features, self.linear.out_features,\n            self.linear.bias is not None)\n        return self.__class__.__name__ + s\n\n\ndef cat(*sparse_tensors):\n    r""""""Concatenate sparse tensors\n\n    Concatenate sparse tensor features. All sparse tensors must have the same\n    `coords_key` (the same coordinates). To concatenate sparse tensors with\n    different sparsity patterns, use SparseTensor binary operations, or\n    :attr:`MinkowskiEngine.MinkowskiUnion`.\n\n    Example::\n\n       >>> import MinkowskiEngine as ME\n       >>> sin = ME.SparseTensor(feats, coords)\n       >>> sin2 = ME.SparseTensor(feats2, coords_key=sin.coords_key, coords_man=sin.coords_man)\n       >>> sout = UNet(sin)  # Returns an output sparse tensor on the same coordinates\n       >>> sout2 = ME.cat(sin, sin2, sout)  # Can concatenate multiple sparse tensors\n\n    """"""\n    for s in sparse_tensors:\n        assert isinstance(s, SparseTensor), ""Inputs must be sparse tensors.""\n    coords_man = sparse_tensors[0].coords_man\n    coords_key = sparse_tensors[0].coords_key\n    for s in sparse_tensors:\n        assert coords_man == s.coords_man, COORDS_MAN_DIFFERENT_ERROR\n        assert coords_key == s.coords_key, COORDS_KEY_DIFFERENT_ERROR\n    tens = []\n    for s in sparse_tensors:\n        tens.append(s.F)\n    return SparseTensor(\n        torch.cat(tens, dim=1),\n        coords_key=sparse_tensors[0].coords_key,\n        coords_manager=coords_man)\n'"
MinkowskiEngine/MinkowskiPooling.py,11,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nfrom typing import Union\n\nimport torch\nfrom torch.autograd import Function\n\nfrom SparseTensor import SparseTensor, _get_coords_key\nfrom Common import KernelGenerator, RegionType, GlobalPoolingMode, \\\n    MinkowskiModuleBase, \\\n    convert_to_int_list, convert_to_int_tensor, \\\n    prep_args, get_minkowski_function\nfrom MinkowskiCoords import CoordsKey, save_ctx\n\n\nclass MinkowskiMaxPoolingFunction(Function):\n\n    @staticmethod\n    def forward(ctx,\n                input_features,\n                tensor_stride=1,\n                stride=1,\n                kernel_size=-1,\n                dilation=1,\n                region_type=0,\n                region_offset=None,\n                in_coords_key=None,\n                out_coords_key=None,\n                coords_manager=None):\n        assert isinstance(region_type, RegionType)\n        if out_coords_key is None:\n            out_coords_key = CoordsKey(in_coords_key.D)\n        assert in_coords_key.D == out_coords_key.D\n        if not input_features.is_contiguous():\n            input_features = input_features.contiguous()\n\n        tensor_stride, stride, kernel_size, dilation, region_type = prep_args(\n            tensor_stride, stride, kernel_size, dilation, region_type,\n            in_coords_key.D)\n\n        if region_offset is None:\n            region_offset = torch.IntTensor()\n\n        ctx.in_feat = input_features\n        ctx = save_ctx(ctx, tensor_stride, stride, kernel_size, dilation,\n                       region_type, in_coords_key, out_coords_key,\n                       coords_manager)\n\n        D = in_coords_key.D\n        out_feat = input_features.new()\n        max_index = input_features.new().int()\n\n        ctx.max_index = max_index\n\n        fw_fn = get_minkowski_function(\'MaxPoolingForward\', input_features)\n        fw_fn(input_features, out_feat, max_index,\n              convert_to_int_list(ctx.tensor_stride, D),\n              convert_to_int_list(ctx.stride, D),\n              convert_to_int_list(ctx.kernel_size, D),\n              convert_to_int_list(ctx.dilation, D), region_type, region_offset,\n              ctx.in_coords_key.CPPCoordsKey, ctx.out_coords_key.CPPCoordsKey,\n              ctx.coords_man.CPPCoordsManager)\n        return out_feat\n\n    @staticmethod\n    def backward(ctx, grad_out_feat):\n        if not grad_out_feat.is_contiguous():\n            grad_out_feat = grad_out_feat.contiguous()\n\n        grad_in_feat = grad_out_feat.new()\n        D = ctx.in_coords_key.D\n        bw_fn = get_minkowski_function(\'MaxPoolingBackward\', grad_out_feat)\n        bw_fn(ctx.in_feat, grad_in_feat, grad_out_feat, ctx.max_index,\n              convert_to_int_list(ctx.tensor_stride, D),\n              convert_to_int_list(ctx.stride, D),\n              convert_to_int_list(ctx.kernel_size, D),\n              convert_to_int_list(ctx.dilation, D), ctx.region_type,\n              ctx.in_coords_key.CPPCoordsKey, ctx.out_coords_key.CPPCoordsKey,\n              ctx.coords_man.CPPCoordsManager)\n        return grad_in_feat, None, None, None, None, None, None, None, None, None\n\n\nclass MinkowskiAvgPoolingFunction(Function):\n    \'\'\'\n    Due to ctx.num_nonzero = in_feat.new()....,\n    Should the function be called multiple times, this function must be first\n    instantiated and then reused every time it needs to be called. Otherwise,\n    PyTorch cannot free, out_feat, ctx.num_nonzero, which are initialized inside\n    the ffi function.\n    \'\'\'\n\n    @staticmethod\n    def forward(ctx,\n                input_features,\n                tensor_stride=1,\n                stride=1,\n                kernel_size=-1,\n                dilation=1,\n                region_type=0,\n                region_offset=None,\n                average=True,\n                in_coords_key=None,\n                out_coords_key=None,\n                coords_manager=None):\n        assert isinstance(region_type, RegionType)\n        if out_coords_key is None:\n            out_coords_key = CoordsKey(in_coords_key.D)\n        assert in_coords_key.D == out_coords_key.D\n        if not input_features.is_contiguous():\n            input_features = input_features.contiguous()\n\n        tensor_stride, stride, kernel_size, dilation, region_type = prep_args(\n            tensor_stride, stride, kernel_size, dilation, region_type,\n            in_coords_key.D)\n\n        if region_offset is None:\n            region_offset = torch.IntTensor()\n\n        ctx.in_feat = input_features\n        ctx = save_ctx(ctx, tensor_stride, stride, kernel_size, dilation,\n                       region_type, in_coords_key, out_coords_key,\n                       coords_manager)\n        ctx.use_avg = average\n\n        D = in_coords_key.D\n        out_feat = input_features.new()\n        ctx.num_nonzero = input_features.new()\n\n        fw_fn = get_minkowski_function(\'AvgPoolingForward\', input_features)\n        fw_fn(ctx.in_feat, out_feat, ctx.num_nonzero,\n              convert_to_int_list(ctx.tensor_stride, D),\n              convert_to_int_list(ctx.stride, D),\n              convert_to_int_list(ctx.kernel_size, D),\n              convert_to_int_list(ctx.dilation, D), region_type, region_offset,\n              ctx.in_coords_key.CPPCoordsKey, ctx.out_coords_key.CPPCoordsKey,\n              ctx.coords_man.CPPCoordsManager, ctx.use_avg)\n        return out_feat\n\n    @staticmethod\n    def backward(ctx, grad_out_feat):\n        if not grad_out_feat.is_contiguous():\n            grad_out_feat = grad_out_feat.contiguous()\n\n        grad_in_feat = grad_out_feat.new()\n        D = ctx.in_coords_key.D\n        bw_fn = get_minkowski_function(\'AvgPoolingBackward\', grad_out_feat)\n        bw_fn(ctx.in_feat, grad_in_feat, grad_out_feat, ctx.num_nonzero,\n              convert_to_int_list(ctx.tensor_stride, D),\n              convert_to_int_list(ctx.stride, D),\n              convert_to_int_list(ctx.kernel_size, D),\n              convert_to_int_list(ctx.dilation, D), ctx.region_type,\n              ctx.in_coords_key.CPPCoordsKey, ctx.out_coords_key.CPPCoordsKey,\n              ctx.coords_man.CPPCoordsManager, ctx.use_avg)\n        return grad_in_feat, None, None, None, None, None, None, None, None, None, None\n\n\nclass MinkowskiPoolingBase(MinkowskiModuleBase):\n\n    def __init__(self,\n                 kernel_size,\n                 stride=1,\n                 dilation=1,\n                 kernel_generator=None,\n                 is_transpose=False,\n                 average=True,\n                 dimension=-1):\n        super(MinkowskiPoolingBase, self).__init__()\n        assert dimension > 0, f""dimension must be a positive integer, {dimension}""\n\n        stride = convert_to_int_tensor(stride, dimension)\n        kernel_size = convert_to_int_tensor(kernel_size, dimension)\n        dilation = convert_to_int_tensor(dilation, dimension)\n        if torch.prod(kernel_size) == 1 and torch.prod(stride) == 1:\n            raise ValueError(\'Trivial input output mapping\')\n\n        if kernel_generator is None:\n            kernel_generator = KernelGenerator(\n                kernel_size=kernel_size,\n                stride=stride,\n                dilation=dilation,\n                dimension=dimension)\n\n        self.is_transpose = is_transpose\n        self.average = average\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.dilation = dilation\n        self.kernel_generator = kernel_generator\n        self.dimension = dimension\n\n    def forward(self,\n                input: SparseTensor,\n                coords: Union[torch.IntTensor, CoordsKey, SparseTensor] = None):\n        r""""""\n        :attr:`input` (`MinkowskiEngine.SparseTensor`): Input sparse tensor to apply a\n        convolution on.\n\n        :attr:`coords` ((`torch.IntTensor`, `MinkowskiEngine.CoordsKey`,\n        `MinkowskiEngine.SparseTensor`), optional): If provided, generate\n        results on the provided coordinates. None by default.\n\n        """"""\n        assert isinstance(input, SparseTensor)\n        assert input.D == self.dimension\n\n        # Create a region_offset\n        self.region_type_, self.region_offset_, _ = \\\n            self.kernel_generator.get_kernel(input.tensor_stride, self.is_transpose)\n\n        # Get a new coords key or extract one from the coords\n        out_coords_key = _get_coords_key(input, coords)\n\n        output = self.pooling.apply(input.F, input.tensor_stride, self.stride,\n                                    self.kernel_size, self.dilation,\n                                    self.region_type_, self.region_offset_,\n                                    self.average, input.coords_key,\n                                    out_coords_key, input.coords_man)\n\n        return SparseTensor(\n            output, coords_key=out_coords_key, coords_manager=input.coords_man)\n\n    def __repr__(self):\n        s = \'(kernel_size={}, stride={}, dilation={})\'.format(\n            self.kernel_size, self.stride, self.dilation)\n        return self.__class__.__name__ + s\n\n\nclass MinkowskiAvgPooling(MinkowskiPoolingBase):\n    r""""""Average input features within a kernel.\n\n    .. math::\n\n        \\mathbf{y}_\\mathbf{u} = \\frac{1}{|\\mathcal{N}^D(\\mathbf{u},\n        \\mathcal{C}^\\text{in})|} \\sum_{\\mathbf{i} \\in \\mathcal{N}^D(\\mathbf{u},\n        \\mathcal{C}^\\text{in})} \\mathbf{x}_{\\mathbf{u} + \\mathbf{i}}\n        \\; \\text{for} \\; \\mathbf{u} \\in \\mathcal{C}^\\text{out}\n\n    For each output :math:`\\mathbf{u}` in :math:`\\mathcal{C}^\\text{out}`,\n    average input features.\n\n    .. note::\n\n        An average layer first computes the cardinality of the input features,\n        the number of input features for each output, and divide the sum of the\n        input features by the cardinality. For a dense tensor, the cardinality\n        is a constant, the volume of a kernel. However, for a sparse tensor, the\n        cardinality varies depending on the number of input features per output.\n        Thus, the average pooling for a sparse tensor is not equivalent to the\n        conventional average pooling layer for a dense tensor. Please refer to\n        the :attr:`MinkowskiSumPooling` for the equivalent layer.\n\n    .. note::\n\n       The engine will generate the in-out mapping corresponding to a\n       pooling function faster if the kernel sizes is equal to the stride\n       sizes, e.g. `kernel_size = [2, 1], stride = [2, 1]`.\n\n       If you use a U-network architecture, use the transposed version of\n       the same function for up-sampling. e.g. `pool =\n       MinkowskiSumPooling(kernel_size=2, stride=2, D=D)`, then use the\n       `unpool = MinkowskiPoolingTranspose(kernel_size=2, stride=2, D=D)`.\n\n    """"""\n\n    def __init__(self,\n                 kernel_size=-1,\n                 stride=1,\n                 dilation=1,\n                 kernel_generator=None,\n                 dimension=None):\n        r""""""a high-dimensional sparse average pooling layer.\n\n        Args:\n            :attr:`kernel_size` (int, optional): the size of the kernel in the\n            output tensor. If not provided, :attr:`region_offset` should be\n            :attr:`RegionType.CUSTOM` and :attr:`region_offset` should be a 2D\n            matrix with size :math:`N\\times D` such that it lists all :math:`N`\n            offsets in D-dimension.\n\n            :attr:`stride` (int, or list, optional): stride size of the\n            convolution layer. If non-identity is used, the output coordinates\n            will be at least :attr:`stride` :math:`\\times` :attr:`tensor_stride`\n            away. When a list is given, the length must be D; each element will\n            be used for stride size for the specific axis.\n\n            :attr:`dilation` (int, or list, optional): dilation size for the\n            convolution kernel. When a list is given, the length must be D and\n            each element is an axis specific dilation. All elements must be > 0.\n\n            :attr:`kernel_generator` (:attr:`MinkowskiEngine.KernelGenerator`,\n            optional): define custom kernel shape.\n\n            :attr:`dimension` (int): the spatial dimension of the space where\n            all the inputs and the network are defined. For example, images are\n            in a 2D space, meshes and 3D shapes are in a 3D space.\n\n        .. warning::\n\n           Custom kernel shapes are not supported when kernel_size == stride.\n\n        """"""\n        is_transpose = False\n        MinkowskiPoolingBase.__init__(\n            self,\n            kernel_size,\n            stride,\n            dilation,\n            kernel_generator,\n            is_transpose,\n            average=True,\n            dimension=dimension)\n        self.pooling = MinkowskiAvgPoolingFunction()\n\n\nclass MinkowskiSumPooling(MinkowskiPoolingBase):\n    r""""""Sum all input features within a kernel.\n\n    .. math::\n\n        \\mathbf{y}_\\mathbf{u} = \\sum_{\\mathbf{i} \\in \\mathcal{N}^D(\\mathbf{u},\n        \\mathcal{C}^\\text{in})} \\mathbf{x}_{\\mathbf{u} + \\mathbf{i}}\n        \\; \\text{for} \\; \\mathbf{u} \\in \\mathcal{C}^\\text{out}\n\n    For each output :math:`\\mathbf{u}` in :math:`\\mathcal{C}^\\text{out}`,\n    average input features.\n\n    .. note::\n\n        An average layer first computes the cardinality of the input features,\n        the number of input features for each output, and divide the sum of the\n        input features by the cardinality. For a dense tensor, the cardinality\n        is a constant, the volume of a kernel. However, for a sparse tensor, the\n        cardinality varies depending on the number of input features per output.\n        Thus, averaging the input features with the cardinality may not be\n        equivalent to the conventional average pooling for a dense tensor.\n        This layer provides an alternative that does not divide the sum by the\n        cardinality.\n\n    .. note::\n\n       The engine will generate the in-out mapping corresponding to a\n       pooling function faster if the kernel sizes is equal to the stride\n       sizes, e.g. `kernel_size = [2, 1], stride = [2, 1]`.\n\n       If you use a U-network architecture, use the transposed version of\n       the same function for up-sampling. e.g. `pool =\n       MinkowskiSumPooling(kernel_size=2, stride=2, D=D)`, then use the\n       `unpool = MinkowskiPoolingTranspose(kernel_size=2, stride=2, D=D)`.\n\n\n    """"""\n\n    def __init__(self,\n                 kernel_size,\n                 stride=1,\n                 dilation=1,\n                 kernel_generator=None,\n                 dimension=None):\n        r""""""a high-dimensional sum pooling layer\n\n        Args:\n            :attr:`kernel_size` (int, optional): the size of the kernel in the\n            output tensor. If not provided, :attr:`region_offset` should be\n            :attr:`RegionType.CUSTOM` and :attr:`region_offset` should be a 2D\n            matrix with size :math:`N\\times D` such that it lists all :math:`N`\n            offsets in D-dimension.\n\n            :attr:`stride` (int, or list, optional): stride size of the\n            convolution layer. If non-identity is used, the output coordinates\n            will be at least :attr:`stride` :math:`\\times` :attr:`tensor_stride`\n            away. When a list is given, the length must be D; each element will\n            be used for stride size for the specific axis.\n\n            :attr:`dilation` (int, or list, optional): dilation size for the\n            convolution kernel. When a list is given, the length must be D and\n            each element is an axis specific dilation. All elements must be > 0.\n\n            :attr:`kernel_generator` (:attr:`MinkowskiEngine.KernelGenerator`,\n            optional): define custom kernel shape.\n\n            :attr:`dimension` (int): the spatial dimension of the space where\n            all the inputs and the network are defined. For example, images are\n            in a 2D space, meshes and 3D shapes are in a 3D space.\n\n        .. warning::\n\n           Custom kernel shapes are not supported when kernel_size == stride.\n\n        """"""\n        is_transpose = False\n        MinkowskiPoolingBase.__init__(\n            self,\n            kernel_size,\n            stride,\n            dilation,\n            kernel_generator,\n            is_transpose,\n            average=False,\n            dimension=dimension)\n        self.pooling = MinkowskiAvgPoolingFunction()\n\n\nclass MinkowskiMaxPooling(MinkowskiPoolingBase):\n    r""""""A max pooling layer for a sparse tensor.\n\n    .. math::\n\n        y^c_\\mathbf{u} = \\max_{\\mathbf{i} \\in \\mathcal{N}^D(\\mathbf{u},\n        \\mathcal{C}^\\text{in})} x^c_{\\mathbf{u} + \\mathbf{i}} \\; \\text{for} \\;\n        \\mathbf{u} \\in \\mathcal{C}^\\text{out}\n\n    where :math:`y^c_\\mathbf{u}` is a feature at channel :math:`c` and a\n    coordinate :math:`\\mathbf{u}`.\n\n    .. note::\n\n       The engine will generate the in-out mapping corresponding to a\n       pooling function faster if the kernel sizes is equal to the stride\n       sizes, e.g. `kernel_size = [2, 1], stride = [2, 1]`.\n\n       If you use a U-network architecture, use the transposed version of\n       the same function for up-sampling. e.g. `pool =\n       MinkowskiSumPooling(kernel_size=2, stride=2, D=D)`, then use the\n       `unpool = MinkowskiPoolingTranspose(kernel_size=2, stride=2, D=D)`.\n\n    """"""\n\n    def __init__(self,\n                 kernel_size,\n                 stride=1,\n                 dilation=1,\n                 kernel_generator=None,\n                 dimension=None):\n        r""""""a high-dimensional max pooling layer for sparse tensors.\n\n        Args:\n            :attr:`kernel_size` (int, optional): the size of the kernel in the\n            output tensor. If not provided, :attr:`region_offset` should be\n            :attr:`RegionType.CUSTOM` and :attr:`region_offset` should be a 2D\n            matrix with size :math:`N\\times D` such that it lists all :math:`N`\n            offsets in D-dimension.\n\n            :attr:`stride` (int, or list, optional): stride size of the\n            convolution layer. If non-identity is used, the output coordinates\n            will be at least :attr:`stride` :math:`\\times` :attr:`tensor_stride`\n            away. When a list is given, the length must be D; each element will\n            be used for stride size for the specific axis.\n\n            :attr:`dilation` (int, or list, optional): dilation size for the\n            convolution kernel. When a list is given, the length must be D and\n            each element is an axis specific dilation. All elements must be > 0.\n\n            :attr:`kernel_generator` (:attr:`MinkowskiEngine.KernelGenerator`,\n            optional): define custom kernel shape.\n\n            :attr:`dimension` (int): the spatial dimension of the space where\n            all the inputs and the network are defined. For example, images are\n            in a 2D space, meshes and 3D shapes are in a 3D space.\n\n        .. warning::\n\n           Custom kernel shapes are not supported when kernel_size == stride.\n\n        """"""\n\n        MinkowskiPoolingBase.__init__(\n            self,\n            kernel_size,\n            stride,\n            dilation,\n            kernel_generator,\n            is_transpose=False,\n            dimension=dimension)\n        self.pooling = MinkowskiMaxPoolingFunction()\n\n    def forward(self,\n                input: SparseTensor,\n                coords: Union[torch.IntTensor, CoordsKey, SparseTensor] = None):\n        r""""""\n        :attr:`input` (`MinkowskiEngine.SparseTensor`): Input sparse tensor to apply a\n        convolution on.\n\n        :attr:`coords` ((`torch.IntTensor`, `MinkowskiEngine.CoordsKey`,\n        `MinkowskiEngine.SparseTensor`), optional): If provided, generate\n        results on the provided coordinates. None by default.\n\n        """"""\n        assert isinstance(input, SparseTensor)\n        assert input.D == self.dimension\n\n        # Create a region_offset\n        self.region_type_, self.region_offset_, _ = \\\n            self.kernel_generator.get_kernel(input.tensor_stride, self.is_transpose)\n\n        # Get a new coords key or extract one from the coords\n        out_coords_key = _get_coords_key(input, coords)\n\n        output = self.pooling.apply(input.F, input.tensor_stride, self.stride,\n                                    self.kernel_size, self.dilation,\n                                    self.region_type_, self.region_offset_,\n                                    input.coords_key, out_coords_key,\n                                    input.coords_man)\n        return SparseTensor(\n            output, coords_key=out_coords_key, coords_manager=input.coords_man)\n\n\nclass MinkowskiPoolingTransposeFunction(Function):\n\n    @staticmethod\n    def forward(ctx,\n                input_features,\n                tensor_stride=1,\n                stride=1,\n                kernel_size=-1,\n                dilation=1,\n                region_type=-1,\n                region_offset=None,\n                average=False,\n                in_coords_key=None,\n                out_coords_key=None,\n                coords_manager=None):\n        assert isinstance(region_type, RegionType)\n        if out_coords_key is None:\n            out_coords_key = CoordsKey(in_coords_key.D)\n        assert in_coords_key.D == out_coords_key.D\n        tensor_stride, stride, kernel_size, dilation, region_type = prep_args(\n            tensor_stride, stride, kernel_size, dilation, region_type,\n            in_coords_key.D)\n\n        if region_offset is None:\n            region_offset = torch.IntTensor()\n\n        ctx.in_feat = input_features\n        out_feat = input_features.new()\n        ctx.num_nonzero = input_features.new()\n        ctx = save_ctx(ctx, tensor_stride, stride, kernel_size, dilation,\n                       region_type, in_coords_key, out_coords_key,\n                       coords_manager)\n        D = in_coords_key.D\n        fw_fn = get_minkowski_function(\'PoolingTransposeForward\',\n                                       input_features)\n        fw_fn(ctx.in_feat, out_feat, ctx.num_nonzero,\n              convert_to_int_list(ctx.tensor_stride, D),\n              convert_to_int_list(ctx.stride, D),\n              convert_to_int_list(ctx.kernel_size, D),\n              convert_to_int_list(ctx.dilation, D), region_type, region_offset,\n              ctx.in_coords_key.CPPCoordsKey, ctx.out_coords_key.CPPCoordsKey,\n              ctx.coords_man.CPPCoordsManager)\n        return out_feat\n\n    @staticmethod\n    def backward(ctx, grad_out_feat):\n        grad_in_feat = grad_out_feat.new()\n        D = ctx.in_coords_key.D\n        bw_fn = get_minkowski_function(\'PoolingTransposeBackward\',\n                                       grad_out_feat)\n        bw_fn(ctx.in_feat, grad_in_feat, grad_out_feat, ctx.num_nonzero,\n              convert_to_int_list(ctx.tensor_stride, D),\n              convert_to_int_list(ctx.stride, D),\n              convert_to_int_list(ctx.kernel_size, D),\n              convert_to_int_list(ctx.dilation, D), ctx.region_type,\n              ctx.in_coords_key.CPPCoordsKey, ctx.out_coords_key.CPPCoordsKey,\n              ctx.coords_man.CPPCoordsManager)\n        return grad_in_feat, None, None, None, None, None, None, None, None, None, None\n\n\nclass MinkowskiPoolingTranspose(MinkowskiPoolingBase):\n    r""""""A pooling transpose layer for a sparse tensor.\n\n    Unpool the features and divide it by the number of non zero elements that\n    contributed.\n    """"""\n\n    def __init__(self,\n                 kernel_size,\n                 stride,\n                 dilation=1,\n                 kernel_generator=None,\n                 dimension=None):\n        r""""""a high-dimensional unpooling layer for sparse tensors.\n\n        Args:\n            :attr:`kernel_size` (int, optional): the size of the kernel in the\n            output tensor. If not provided, :attr:`region_offset` should be\n            :attr:`RegionType.CUSTOM` and :attr:`region_offset` should be a 2D\n            matrix with size :math:`N\\times D` such that it lists all :math:`N`\n            offsets in D-dimension.\n\n            :attr:`stride` (int, or list, optional): stride size of the\n            convolution layer. If non-identity is used, the output coordinates\n            will be at least :attr:`stride` :math:`\\times` :attr:`tensor_stride`\n            away. When a list is given, the length must be D; each element will\n            be used for stride size for the specific axis.\n\n            :attr:`dilation` (int, or list, optional): dilation size for the\n            convolution kernel. When a list is given, the length must be D and\n            each element is an axis specific dilation. All elements must be > 0.\n\n            :attr:`kernel_generator` (:attr:`MinkowskiEngine.KernelGenerator`,\n            optional): define custom kernel shape.\n\n            :attr:`dimension` (int): the spatial dimension of the space where\n            all the inputs and the network are defined. For example, images are\n            in a 2D space, meshes and 3D shapes are in a 3D space.\n\n        """"""\n        is_transpose = True\n        MinkowskiPoolingBase.__init__(\n            self,\n            kernel_size,\n            stride,\n            dilation,\n            kernel_generator,\n            is_transpose,\n            average=False,\n            dimension=dimension)\n        self.pooling = MinkowskiPoolingTransposeFunction()\n\n    def forward(self,\n                input: SparseTensor,\n                coords: Union[torch.IntTensor, CoordsKey, SparseTensor] = None):\n        r""""""\n        :attr:`input` (`MinkowskiEngine.SparseTensor`): Input sparse tensor to apply a\n        convolution on.\n\n        :attr:`coords` ((`torch.IntTensor`, `MinkowskiEngine.CoordsKey`,\n        `MinkowskiEngine.SparseTensor`), optional): If provided, generate\n        results on the provided coordinates. None by default.\n\n        """"""\n        assert isinstance(input, SparseTensor)\n        assert input.D == self.dimension\n\n        # Create a region_offset\n        self.region_type_, self.region_offset_, _ = \\\n            self.kernel_generator.get_kernel(input.tensor_stride, self.is_transpose)\n\n        # Get a new coords key or extract one from the coords\n        out_coords_key = _get_coords_key(input, coords)\n\n        output = self.pooling.apply(input.F, input.tensor_stride, self.stride,\n                                    self.kernel_size, self.dilation,\n                                    self.region_type_, self.region_offset_,\n                                    self.average, input.coords_key,\n                                    out_coords_key, input.coords_man)\n\n        return SparseTensor(\n            output, coords_key=out_coords_key, coords_manager=input.coords_man)\n\n\nclass MinkowskiGlobalPoolingFunction(Function):\n\n    @staticmethod\n    def forward(ctx,\n                input_features,\n                average=True,\n                mode=GlobalPoolingMode.AUTO,\n                in_coords_key=None,\n                out_coords_key=None,\n                coords_manager=None):\n        if out_coords_key is None:\n            out_coords_key = CoordsKey(in_coords_key.D)\n        assert isinstance(mode, GlobalPoolingMode), \\\n            f""Mode must be an instance of GlobalPoolingMode, {mode}""\n\n        ctx.in_coords_key = in_coords_key\n        ctx.out_coords_key = out_coords_key\n\n        ctx.in_feat = input_features\n        ctx.average = average\n        ctx.coords_manager = coords_manager\n        ctx.mode = mode.value\n\n        fw_fn = get_minkowski_function(\'GlobalPoolingForward\', input_features)\n        out_feat, num_nonzero = fw_fn(ctx.in_feat,\n                                      ctx.in_coords_key.CPPCoordsKey,\n                                      ctx.out_coords_key.CPPCoordsKey,\n                                      ctx.coords_manager.CPPCoordsManager,\n                                      ctx.average, ctx.mode)\n\n        ctx.num_nonzero = num_nonzero\n\n        return out_feat\n\n    @staticmethod\n    def backward(ctx, grad_out_feat):\n        bw_fn = get_minkowski_function(\'GlobalPoolingBackward\', grad_out_feat)\n        grad_in_feat = bw_fn(ctx.in_feat, grad_out_feat, ctx.num_nonzero,\n                             ctx.in_coords_key.CPPCoordsKey,\n                             ctx.out_coords_key.CPPCoordsKey,\n                             ctx.coords_manager.CPPCoordsManager, ctx.average)\n        return grad_in_feat, None, None, None, None, None\n\n\nclass MinkowskiGlobalPooling(MinkowskiModuleBase):\n    r""""""Pool all input features to one output.\n\n    .. math::\n\n        \\mathbf{y} = \\frac{1}{|\\mathcal{C}^\\text{in}|} \\sum_{\\mathbf{i} \\in\n        \\mathcal{C}^\\text{in}} \\mathbf{x}_{\\mathbf{i}}\n\n    """"""\n\n    def __init__(self, average=True, mode=GlobalPoolingMode.AUTO):\n        r""""""Reduces sparse coords into points at origin, i.e. reduce each point\n        cloud into a point at the origin, returning batch_size number of points\n        [[0, 0, ..., 0], [0, 0, ..., 1],, [0, 0, ..., 2]] where the last elem\n        of the coords is the batch index.\n\n        Args:\n            :attr:`average` (bool): when True, return the averaged output. If\n            not, return the sum of all input features.\n\n        """"""\n        super(MinkowskiGlobalPooling, self).__init__()\n        assert isinstance(mode, GlobalPoolingMode), \\\n            f""Mode must be an instance of GlobalPoolingMode. mode={mode}""\n\n        self.mode = mode\n        self.average = average\n        self.pooling = MinkowskiGlobalPoolingFunction()\n\n    def forward(self, input):\n        assert isinstance(input, SparseTensor)\n\n        out_coords_key = CoordsKey(input.coords_key.D)\n        output = self.pooling.apply(input.F, self.average, self.mode,\n                                    input.coords_key, out_coords_key,\n                                    input.coords_man)\n\n        return SparseTensor(\n            output, coords_key=out_coords_key, coords_manager=input.coords_man)\n\n    def __repr__(self):\n        return self.__class__.__name__ + ""(average="" + str(self.average) + "")""\n\n\nclass MinkowskiGlobalSumPooling(MinkowskiGlobalPooling):\n\n    def __init__(self, mode=GlobalPoolingMode.AUTO):\n        r""""""Reduces sparse coords into points at origin, i.e. reduce each point\n        cloud into a point at the origin, returning batch_size number of points\n        [[0, 0, ..., 0], [0, 0, ..., 1],, [0, 0, ..., 2]] where the last elem\n        of the coords is the batch index.\n\n        """"""\n        MinkowskiGlobalPooling.__init__(self, False, mode=mode)\n\n\nclass MinkowskiGlobalAvgPooling(MinkowskiGlobalPooling):\n\n    def __init__(self, mode=GlobalPoolingMode.AUTO):\n        r""""""Reduces sparse coords into points at origin, i.e. reduce each point\n        cloud into a point at the origin, returning batch_size number of points\n        [[0, 0, ..., 0], [0, 0, ..., 1],, [0, 0, ..., 2]] where the last elem\n        of the coords is the batch index.\n\n        """"""\n        MinkowskiGlobalPooling.__init__(self, True, mode=mode)\n\n\nclass MinkowskiGlobalMaxPoolingFunction(Function):\n\n    @staticmethod\n    def forward(ctx,\n                input_features,\n                in_coords_key=None,\n                out_coords_key=None,\n                coords_manager=None):\n        if out_coords_key is None:\n            out_coords_key = CoordsKey(in_coords_key.D)\n        ctx.in_coords_key = in_coords_key\n        ctx.out_coords_key = out_coords_key\n\n        ctx.in_feat = input_features\n        out_feat = input_features.new()\n\n        max_index = input_features.new().int()\n\n        ctx.max_index = max_index\n        ctx.coords_manager = coords_manager\n\n        fw_fn = get_minkowski_function(\'GlobalMaxPoolingForward\',\n                                       input_features)\n        fw_fn(ctx.in_feat, out_feat, ctx.max_index,\n              ctx.in_coords_key.CPPCoordsKey, ctx.out_coords_key.CPPCoordsKey,\n              ctx.coords_manager.CPPCoordsManager)\n        return out_feat\n\n    @staticmethod\n    def backward(ctx, grad_out_feat):\n        grad_in_feat = grad_out_feat.new()\n        bw_fn = get_minkowski_function(\'GlobalMaxPoolingBackward\',\n                                       grad_out_feat)\n        bw_fn(ctx.in_feat, grad_in_feat, grad_out_feat, ctx.max_index,\n              ctx.in_coords_key.CPPCoordsKey, ctx.out_coords_key.CPPCoordsKey,\n              ctx.coords_manager.CPPCoordsManager)\n        return grad_in_feat, None, None, None, None, None\n\n\nclass MinkowskiGlobalMaxPooling(MinkowskiModuleBase):\n    r""""""Max pool all input features to one output feature at the origin.\n\n    .. math::\n\n        \\mathbf{y} = \\frac{1}{|\\mathcal{C}^\\text{in}|} \\max_{\\mathbf{i} \\in\n        \\mathcal{C}^\\text{in}} \\mathbf{x}_{\\mathbf{i}}\n\n    """"""\n\n    def __init__(self):\n        r""""""Reduces sparse coords into points at origin, i.e. reduce each point\n        cloud into a point at the origin, returning batch_size number of points\n        [[0, 0, ..., 0], [0, 0, ..., 1],, [0, 0, ..., 2]] where the last elem\n        of the coords is the batch index.\n\n        """"""\n        super(MinkowskiGlobalMaxPooling, self).__init__()\n        self.pooling = MinkowskiGlobalMaxPoolingFunction()\n\n    def forward(self, input):\n        assert isinstance(input, SparseTensor)\n\n        out_coords_key = CoordsKey(input.coords_key.D)\n        output = self.pooling.apply(input.F, input.coords_key, out_coords_key,\n                                    input.coords_man)\n\n        return SparseTensor(\n            output, coords_key=out_coords_key, coords_manager=input.coords_man)\n\n    def __repr__(self):\n        return self.__class__.__name__\n'"
MinkowskiEngine/MinkowskiPruning.py,5,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport torch\nfrom torch.nn import Module\nfrom torch.autograd import Function\n\nfrom MinkowskiCoords import CoordsKey\nfrom SparseTensor import SparseTensor\nfrom Common import get_minkowski_function\n\n\nclass MinkowskiPruningFunction(Function):\n\n    @staticmethod\n    def forward(ctx, in_feat, mask, in_coords_key, out_coords_key,\n                coords_manager):\n        assert in_feat.size(0) == mask.size(0)\n        assert isinstance(mask, torch.BoolTensor), ""Mask must be a cpu bool tensor.""\n        if not in_feat.is_contiguous():\n            in_feat = in_feat.contiguous()\n        if not mask.is_contiguous():\n            mask = mask.contiguous()\n\n        ctx.in_coords_key = in_coords_key\n        ctx.out_coords_key = out_coords_key\n        ctx.coords_manager = coords_manager\n\n        out_feat = in_feat.new()\n\n        fw_fn = get_minkowski_function(\'PruningForward\', in_feat)\n        fw_fn(in_feat, out_feat, mask, ctx.in_coords_key.CPPCoordsKey,\n              ctx.out_coords_key.CPPCoordsKey,\n              ctx.coords_manager.CPPCoordsManager)\n        return out_feat\n\n    @staticmethod\n    def backward(ctx, grad_out_feat):\n        if not grad_out_feat.is_contiguous():\n            grad_out_feat = grad_out_feat.contiguous()\n\n        grad_in_feat = grad_out_feat.new()\n        bw_fn = get_minkowski_function(\'PruningBackward\', grad_out_feat)\n        bw_fn(grad_in_feat, grad_out_feat, ctx.in_coords_key.CPPCoordsKey,\n              ctx.out_coords_key.CPPCoordsKey,\n              ctx.coords_manager.CPPCoordsManager)\n        return grad_in_feat, None, None, None, None, None\n\n\nclass MinkowskiPruning(Module):\n    r""""""Remove specified coordinates from a :attr:`MinkowskiEngine.SparseTensor`.\n\n    """"""\n\n    def __init__(self):\n        super(MinkowskiPruning, self).__init__()\n        self.pruning = MinkowskiPruningFunction()\n\n    def forward(self, input, mask):\n        r""""""\n        Args:\n            :attr:`input` (:attr:`MinkowskiEnigne.SparseTensor`): a sparse tensor\n            to remove coordinates from.\n\n            :attr:`mask` (:attr:`torch.BoolTensor`): mask vector that specifies\n            which one to keep. Coordinates with False will be removed.\n\n        Returns:\n            A :attr:`MinkowskiEngine.SparseTensor` with C = coordinates\n            corresponding to `mask == True` F = copy of the features from `mask ==\n            True`.\n\n        Example::\n\n            >>> # Define inputs\n            >>> input = SparseTensor(feats, coords=coords)\n            >>> # Any boolean tensor can be used as the filter\n            >>> mask = torch.rand(feats.size(0)) < 0.5\n            >>> pruning = MinkowskiPruning()\n            >>> output = pruning(input, mask)\n\n        """"""\n        assert isinstance(input, SparseTensor)\n\n        out_coords_key = CoordsKey(input.coords_key.D)\n        output = self.pruning.apply(input.F, mask, input.coords_key,\n                                    out_coords_key, input.coords_man)\n        return SparseTensor(\n            output, coords_key=out_coords_key, coords_manager=input.coords_man)\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'()\'\n'"
MinkowskiEngine/MinkowskiUnion.py,4,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nfrom torch.nn import Module\nfrom torch.autograd import Function\n\nfrom MinkowskiCoords import CoordsKey\nfrom SparseTensor import SparseTensor\nfrom Common import get_minkowski_function\n\n\nclass MinkowskiUnionFunction(Function):\n\n    @staticmethod\n    def forward(ctx, in_coords_keys, out_coords_key, coords_manager, *in_feats):\n        assert isinstance(in_feats, list) or isinstance(in_feats, tuple), \\\n            ""Input must be a list or a set of Tensors""\n        assert len(in_feats) > 1, \\\n            ""input must be a set with at least 2 Tensors""\n\n        in_feats = [in_feat.contiguous() for in_feat in in_feats]\n\n        ctx.in_coords_keys = in_coords_keys\n        ctx.out_coords_key = out_coords_key\n        ctx.coords_manager = coords_manager\n\n        fw_fn = get_minkowski_function(\'UnionForward\', in_feats[0])\n        return fw_fn(in_feats, [key.CPPCoordsKey for key in ctx.in_coords_keys],\n                     ctx.out_coords_key.CPPCoordsKey,\n                     ctx.coords_manager.CPPCoordsManager)\n\n    @staticmethod\n    def backward(ctx, grad_out_feat):\n        if not grad_out_feat.is_contiguous():\n            grad_out_feat = grad_out_feat.contiguous()\n\n        bw_fn = get_minkowski_function(\'UnionBackward\', grad_out_feat)\n        grad_in_feats = bw_fn(grad_out_feat,\n                              [key.CPPCoordsKey for key in ctx.in_coords_keys],\n                              ctx.out_coords_key.CPPCoordsKey,\n                              ctx.coords_manager.CPPCoordsManager)\n        return (None, None, None, *grad_in_feats)\n\n\nclass MinkowskiUnion(Module):\n    r""""""Create a union of all sparse tensors and add overlapping features.\n\n    Args:\n        None\n\n    .. warning::\n       This function is experimental and the usage can be changed in the future updates.\n\n    """"""\n\n    def __init__(self):\n        super(MinkowskiUnion, self).__init__()\n        self.union = MinkowskiUnionFunction()\n\n    def forward(self, *inputs):\n        r""""""\n        Args:\n            A variable number of :attr:`MinkowskiEngine.SparseTensor`\'s.\n\n        Returns:\n            A :attr:`MinkowskiEngine.SparseTensor` with coordinates = union of all\n            input coordinates, and features = sum of all features corresponding to the\n            coordinate.\n\n        Example::\n\n            >>> # Define inputs\n            >>> input1 = SparseTensor(\n            >>>     torch.rand(N, in_channels, dtype=torch.double), coords=coords)\n            >>> # All inputs must share the same coordinate manager\n            >>> input2 = SparseTensor(\n            >>>     torch.rand(N, in_channels, dtype=torch.double),\n            >>>     coords=coords + 1,\n            >>>     coords_manager=input1.coords_man,  # Must use same coords manager\n            >>>     force_creation=True  # The tensor stride [1, 1] already exists.\n            >>> )\n            >>> union = MinkowskiUnion()\n            >>> output = union(input1, iput2)\n\n        """"""\n        for s in inputs:\n            assert isinstance(s, SparseTensor), ""Inputs must be sparse tensors.""\n        assert len(inputs) > 1, \\\n            ""input must be a set with at least 2 SparseTensors""\n\n        out_coords_key = CoordsKey(inputs[0].coords_key.D)\n        output = self.union.apply([input.coords_key for input in inputs],\n                                  out_coords_key, inputs[0].coords_man,\n                                  *[input.F for input in inputs])\n        return SparseTensor(\n            output,\n            coords_key=out_coords_key,\n            coords_manager=inputs[0].coords_man)\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'()\'\n'"
MinkowskiEngine/SparseTensor.py,76,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport os\nimport warnings\nimport torch\nimport copy\nfrom enum import Enum\nfrom typing import Union\nfrom collections import Sequence\nimport numpy as np\n\nfrom Common import convert_to_int_list\nfrom MinkowskiCoords import CoordsKey, CoordsManager\nimport MinkowskiEngineBackend as MEB\nfrom MinkowskiEngineBackend import MemoryManagerBackend\n\n\nclass SparseTensorOperationMode(Enum):\n    """"""\n    `SEPARATE_COORDS_MANAGER`: always create a new coordinate manager.\n    `SHARE_COORDS_MANAGER`: always use the globally defined coordinate manager. Must clear the coordinate manager manually by :attr:`MinkowskiEngine.SparseTensor.clear_global_coords_man`\n    """"""\n    SEPARATE_COORDS_MANAGER = 0\n    SHARE_COORDS_MANAGER = 1\n\n\nclass SparseTensorQuantizationMode(Enum):\n    """"""\n    `RANDOM_SUBSAMPLE`: Subsample one coordinate per each quantization block randomly.\n    `UNWEIGHTED_AVERAGE`: average all features within a quantization block equally.\n    """"""\n    RANDOM_SUBSAMPLE = 0\n    UNWEIGHTED_AVERAGE = 1\n\n\n_sparse_tensor_operation_mode = SparseTensorOperationMode.SEPARATE_COORDS_MANAGER\n_global_coords_man = None\nCOORDS_MAN_DIFFERENT_ERROR = ""SparseTensors must share the same coordinate manager for this operation. Please refer to the SparseTensor creation API (https://stanfordvl.github.io/MinkowskiEngine/sparse_tensor.html) to share the coordinate manager, or set the sparse tensor operation mode with `set_sparse_tensor_operation_mode` to share it by default.""\nCOORDS_KEY_DIFFERENT_ERROR = ""SparseTensors must have the same coords_key.""\n\n\ndef set_sparse_tensor_operation_mode(operation_mode: SparseTensorOperationMode):\n    r""""""Define the sparse tensor coordinate manager operation mode.\n\n    By default, a :attr:`MinkowskiEngine.SparseTensor.SparseTensor`\n    instantiation creates a new coordinate manager that is not shared with\n    other sparse tensors. By setting this function with\n    :attr:`MinkowskiEngine.SparseTensorOperationMode.SHARE_COORDS_MANAGER`, you\n    can share the coordinate manager globally with other sparse tensors.\n    However, you must explicitly clear the coordinate manger after use. Please\n    refer to :attr:`MinkowskiEngine.clear_global_coords_man`.\n\n    Args:\n        :attr:`operation_mode`\n        (:attr:`MinkowskiEngine.SparseTensorOperationMode`): The operation mode\n        for the sparse tensor coordinate manager. By default\n        :attr:`MinkowskiEngine.SparseTensorOperationMode.SEPARATE_COORDS_MANAGER`.\n\n    Example:\n\n        >>> import MinkowskiEngine as ME\n        >>> ME.set_sparse_tensor_operation_mode(ME.SparseTensorOperationMode.SHARE_COORDS_MANAGER)\n        >>> ...\n        >>> a = ME.SparseTensor(coords=A_C, feats=A_F)\n        >>> b = ME.SparseTensor(coords=B_C, feats=B_C)  # coords_man shared\n        >>> ...  # one feed forward and backward\n        >>> ME.clear_global_coords_man()  # Must use to clear the coordinates after one forward/backward\n\n    """"""\n    assert isinstance(operation_mode, SparseTensorOperationMode), \\\n        f""Input must be an instance of SparseTensorOperationMode not {operation_mode}""\n    global _sparse_tensor_operation_mode\n    _sparse_tensor_operation_mode = operation_mode\n\n\ndef sparse_tensor_operation_mode():\n    global _sparse_tensor_operation_mode\n    return copy.deepcopy(_sparse_tensor_operation_mode)\n\n\ndef clear_global_coords_man():\n    r""""""Clear the global coordinate manager cache.\n\n    When you use the operation mode:\n    :attr:`MinkowskiEngine.SparseTensor.SparseTensorOperationMode.SHARE_COORDS_MANAGER`,\n    you must explicitly clear the coordinate manager after each feed forward/backward.\n    """"""\n    global _global_coords_man\n    _global_coords_man = None\n\n\nclass SparseTensor():\n    r""""""A sparse tensor class. Can be accessed via\n    :attr:`MinkowskiEngine.SparseTensor`.\n\n    The :attr:`SparseTensor` class is the basic tensor in MinkowskiEngine. For\n    the definition of a sparse tensor, please visit `the terminology page\n    <https://stanfordvl.github.io/MinkowskiEngine/terminology.html#sparse-tensor>`_.\n    We use the COOrdinate (COO) format to save a sparse tensor `[1]\n    <http://groups.csail.mit.edu/commit/papers/2016/parker-thesis.pdf>`_. This\n    representation is simply a concatenation of coordinates in a matrix\n    :math:`C` and associated features :math:`F`.\n\n    .. math::\n\n       \\mathbf{C} = \\begin{bmatrix}\n       b_1    & x_1^1  & x_1^2  & \\cdots & x_1^D  \\\\\n       \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n       b_N    & x_N^1  & x_N^2  & \\cdots & x_N^D\n       \\end{bmatrix}, \\; \\mathbf{F} = \\begin{bmatrix}\n       \\mathbf{f}_1^T\\\\\n       \\vdots\\\\\n       \\mathbf{f}_N^T\n       \\end{bmatrix}\n\n    where :math:`\\mathbf{x}_i \\in \\mathcal{Z}^D` is a :math:`D`-dimensional\n    coordinate and :math:`b_i \\in \\mathcal{Z}_+` denotes the corresponding\n    batch index. :math:`N` is the number of non-zero elements in the sparse\n    tensor, each with the coordinate :math:`(b_i, x_i^1, x_i^1, \\cdots,\n    x_i^D)`, and the associated feature :math:`\\mathbf{f}_i`. Internally, we\n    handle the batch index as an additional spatial dimension.\n\n    .. warning::\n\n       Before MinkowskiEngine version 0.4, we put the batch indices on the last\n       column. Thus, direct manipulation of coordinates will be incompatible\n       with the latest versions. Instead, please use\n       :attr:`MinkowskiEngine.utils.batched_coordinates` or\n       :attr:`MinkowskiEngine.utils.sparse_collate` to create batched\n       coordinates.\n\n       Also, to access coordinates or features batch-wise, use the functions\n       :attr:`coordinates_at(batch_index : int)`, :attr:`features_at(batch_index : int)` of\n       a sparse tensor. Or to access all batch-wise coordinates and features,\n       `decomposed_coordinates`, `decomposed_features`,\n       `decomposed_coordinates_and_features` of a sparse tensor.\n\n       Example::\n\n           >>> coords, feats = ME.utils.sparse_collate([coords_batch0, coords_batch1], [feats_batch0, feats_batch1])\n           >>> A = ME.SparseTensor(feats=feats, coords=coords)\n           >>> coords_batch0 = A.coordinates_at(batch_index=0)\n           >>> feats_batch1 = A.features_at(batch_index=1)\n           >>> list_of_coords, list_of_featurs = A.decomposed_coordinates_and_features\n\n    """"""\n\n    def __init__(\n            self,\n            feats,\n            coords=None,\n            coords_key=None,\n            coords_manager=None,\n            force_creation=False,\n            allow_duplicate_coords=False,\n            quantization_mode=SparseTensorQuantizationMode.RANDOM_SUBSAMPLE,\n            memory_manager_backend: MemoryManagerBackend = None,\n            tensor_stride=1):\n        r""""""\n\n        Args:\n            :attr:`feats` (:attr:`torch.FloatTensor`,\n            :attr:`torch.DoubleTensor`, :attr:`torch.cuda.FloatTensor`, or\n            :attr:`torch.cuda.DoubleTensor`): The features of the sparse\n            tensor.\n\n            :attr:`coords` (:attr:`torch.IntTensor`): The coordinates\n            associated to the features. If not provided, :attr:`coords_key`\n            must be provided.\n\n            :attr:`coords_key` (:attr:`MinkowskiEngine.CoordsKey`): When the\n            coordinates are already cached in the MinkowskiEngine, we could\n            reuse the same coordinates by simply providing the coordinate hash\n            key. In most case, this process is done automatically. When you\n            provide a `coords_key`, all other arguments will be be ignored.\n\n            :attr:`coords_manager` (:attr:`MinkowskiEngine.CoordsManager`): The\n            MinkowskiEngine creates a dynamic computation graph and all\n            coordinates inside the same computation graph are managed by a\n            CoordsManager object. If not provided, the MinkowskiEngine will\n            create a new computation graph. In most cases, this process is\n            handled automatically and you do not need to use this. When you use\n            it, make sure you understand what you are doing.\n\n            :attr:`force_creation` (:attr:`bool`): Force creation of the\n            coordinates. This allows generating a new set of coordinates even\n            when there exists another set of coordinates with the same\n            tensor stride. This could happen when you manually feed the same\n            :attr:`coords_manager`.\n\n            :attr:`allow_duplicate_coords` (:attr:`bool`): Allow duplicate\n            coordinates when creating the sparse tensor. Internally, it will\n            generate a new unique set of coordinates and use features of at the\n            corresponding unique coordinates. In general, setting\n            `allow_duplicate_coords=True` is not recommended as it could hide\n            obvious errors in your data loading and preprocessing steps. Please\n            refer to the quantization and data loading tutorial on `here\n            <https://stanfordvl.github.io/MinkowskiEngine/demo/training.html>`_\n            for more details.\n\n            :attr:`quantizatino_mode`\n            (:attr:`MinkowskiEngine.SparseTensorQuantizationMode`): Defines the\n            quantization method and how to define features of a sparse tensor.\n            Please refer to :attr:`SparseTensorQuantizationMode` for details.\n\n            :attr:`tensor_stride` (:attr:`int`, :attr:`list`,\n            :attr:`numpy.array`, or :attr:`tensor.Tensor`): The tensor stride\n            of the current sparse tensor. By default, it is 1.\n\n        """"""\n        assert isinstance(feats,\n                          torch.Tensor), ""Features must be a torch.Tensor""\n        assert feats.ndim == 2, f""The feature should be a matrix, The input feature is an order-{feats.ndim} tensor.""\n        assert isinstance(quantization_mode, SparseTensorQuantizationMode)\n        self.quantization_mode = quantization_mode\n\n        if coords is None and coords_key is None:\n            raise ValueError(\'Either coords or coords_key must be provided\')\n\n        if coords_key is None:\n            assert coords_manager is not None or coords is not None\n            D = -1\n            if coords_manager is None:\n                D = coords.size(1) - 1\n            else:\n                D = coords_manager.D\n            coords_key = CoordsKey(D)\n            coords_key.setTensorStride(convert_to_int_list(tensor_stride, D))\n        else:\n            assert isinstance(coords_key, CoordsKey)\n\n        if coords is not None:\n            assert isinstance(coords, torch.Tensor), \\\n                ""Coordinate must be of type torch.Tensor""\n\n            if not isinstance(coords, torch.IntTensor):\n                warnings.warn(\n                    \'Coords implicitly converted to torch.IntTensor. \' +\n                    \'To remove this warning, use `.int()` to convert the \' +\n                    \'coords into an torch.IntTensor\')\n                coords = torch.floor(coords).int()\n\n            if coords.device.type != \'cpu\':\n                warnings.warn(\n                    \'Coords implicitly converted to CPU type. \' +\n                    \'To remove this warning, use `.cpu()` to convert the \' +\n                    \'coords into a CPU type\')\n                coords = coords.cpu()\n\n            assert feats.shape[0] == coords.shape[0], \\\n                ""The number of rows in features and coordinates do not match.""\n\n            coords = coords.contiguous()\n\n        ##########################\n        # Setup CoordsManager\n        ##########################\n        if coords_manager is None:\n            # If set to share the coords man, use the global coords man\n            global _sparse_tensor_operation_mode, _global_coords_man\n            if _sparse_tensor_operation_mode == SparseTensorOperationMode.SHARE_COORDS_MANAGER:\n                if _global_coords_man is None:\n                    _global_coords_man = CoordsManager(\n                        memory_manager_backend=memory_manager_backend,\n                        D=coords.size(1) - 1)\n                coords_manager = _global_coords_man\n            else:\n                assert coords is not None, ""Initial coordinates must be given""\n                coords_manager = CoordsManager(D=coords.size(1) - 1)\n\n        else:\n            assert isinstance(coords_manager, CoordsManager)\n\n        ##########################\n        # Initialize coords\n        ##########################\n        if not coords_key.isKeySet() and coords is not None and len(coords) > 0:\n            if quantization_mode == SparseTensorQuantizationMode.RANDOM_SUBSAMPLE:\n                force_remap = True\n                return_inverse = False\n            elif quantization_mode == SparseTensorQuantizationMode.UNWEIGHTED_AVERAGE:\n                force_remap = True\n                return_inverse = True\n\n            self.unique_index, self.inverse_mapping = coords_manager.initialize(\n                coords,\n                coords_key,\n                force_creation=force_creation,\n                force_remap=force_remap,\n                allow_duplicate_coords=allow_duplicate_coords,\n                return_inverse=return_inverse)\n\n            if quantization_mode == SparseTensorQuantizationMode.UNWEIGHTED_AVERAGE:\n                self._CF = feats\n                self._CC = coords\n                feats = MEB.quantization_average_features(\n                    feats, torch.arange(len(feats)), self.inverse_mapping,\n                    len(self.unique_index), 0)\n                coords = coords[self.unique_index]\n            elif force_remap:\n                assert len(self.unique_index) > 0\n                self._CC = coords\n                self._CF = feats\n                coords = coords[self.unique_index]\n                feats = feats[self.unique_index]\n\n        elif coords is not None:  # empty / invalid coords\n            assert isinstance(coords, torch.IntTensor)\n            assert coords.ndim == 2\n            coords_manager.initialize(\n                coords,\n                coords_key,\n                force_creation=force_creation,\n                force_remap=False,\n                allow_duplicate_coords=False,\n                return_inverse=False)\n        elif coords_key is not None:\n            assert coords_key.isKeySet()\n\n        self._F = feats.contiguous()\n        self._C = coords\n        self.coords_key = coords_key\n        self.coords_man = coords_manager\n\n    @property\n    def tensor_stride(self):\n        return self.coords_key.getTensorStride()\n\n    @tensor_stride.setter\n    def tensor_stride(self, p):\n        r""""""\n        This function is not recommended to be used directly.\n        """"""\n        p = convert_to_int_list(p, self.D)\n        self.coords_key.setTensorStride(p)\n\n    def _get_coords(self):\n        return self.coords_man.get_coords(self.coords_key)\n\n    @property\n    def C(self):\n        r""""""The alias of :attr:`coords`.\n        """"""\n        return self.coords\n\n    @property\n    def coords(self):\n        r""""""\n        The coordinates of the current sparse tensor. The coordinates are\n        represented as a :math:`N \\times (D + 1)` dimensional matrix where\n        :math:`N` is the number of points in the space and :math:`D` is the\n        dimension of the space (e.g. 3 for 3D, 4 for 3D + Time). Additional\n        dimension of the column of the matrix C is for batch indices which is\n        internally treated as an additional spatial dimension to disassociate\n        different instances in a batch.\n        """"""\n        if self._C is None:\n            self._C = self._get_coords()\n        return self._C\n\n    @property\n    def decomposed_coordinates(self):\n        r""""""Returns a list of coordinates per batch.\n\n        Returns a list of torch.IntTensor :math:`C \\in \\mathcal{R}^{N_i\n        \\times D}` coordinates per batch where :math:`N_i` is the number of non\n        zero elements in the :math:`i`th batch index in :math:`D` dimensional\n        space.\n        """"""\n        row_inds_list = self.coords_man.get_row_indices_per_batch(\n            self.coords_key)\n        return [self.C[row_inds, 1:] for row_inds in row_inds_list]\n\n    def coordinates_at(self, batch_index):\n        r""""""Return coordinates at the specified batch index.\n\n        Returns a torch.IntTensor :math:`C \\in \\mathcal{R}^{N_i\n        \\times D}` coordinates at the specified batch index where :math:`N_i`\n        is the number of non zero elements in the :math:`i`th batch index in\n        :math:`D` dimensional space.\n        """"""\n        row_inds = self.coords_man.get_row_indices_at(self.coords_key,\n                                                      batch_index)\n        return self.C[row_inds, 1:]\n\n    @property\n    def F(self):\n        r""""""The alias of :attr:`feats`.\n        """"""\n        return self._F\n\n    @property\n    def feats(self):\n        r""""""\n        The features of the current sparse tensor. The features are :math:`N\n        \\times D_F` where :math:`N` is the number of points in the space and\n        :math:`D_F` is the dimension of each feature vector. Please refer to\n        :attr:`coords` to access the associated coordinates.\n        """"""\n        return self._F\n\n    @property\n    def decomposed_features(self):\n        r""""""Returns a list of features per batch.\n\n        Returns a list of torch.Tensor :math:`C \\in \\mathcal{R}^{N_i\n        \\times N_F}` features per batch where :math:`N_i` is the number of non\n        zero elements in the :math:`i`th batch index in :math:`D` dimensional\n        space.\n        """"""\n        row_inds_list = self.coords_man.get_row_indices_per_batch(\n            self.coords_key)\n        return [self._F[row_inds] for row_inds in row_inds_list]\n\n    def features_at(self, batch_index):\n        r""""""Returns a feature matrix at the specified batch index.\n\n        Returns a torch.Tensor :math:`C \\in \\mathcal{R}^{N\n        \\times N_F}` feature matrix :math:`N` is the number of non\n        zero elements in the specified batch index and :math:`N_F` is the\n        number of channels.\n        """"""\n        row_inds = self.coords_man.get_row_indices_at(self.coords_key,\n                                                      batch_index)\n        return self._F[row_inds]\n\n    def coordinates_and_features_at(self, batch_index):\n        r""""""Returns a coordinate and feature matrix at the specified batch index.\n\n        Returns a coordinate and feature matrix at the specified `batch_index`.\n        The coordinate matrix is a torch.IntTensor :math:`C \\in \\mathcal{R}^{N\n        \\times D}` where :math:`N` is the number of non zero elements in the\n        specified batch index in :math:`D` dimensional space. The feature\n        matrix is a torch.Tensor :math:`C \\in \\mathcal{R}^{N \\times N_F}`\n        matrix :math:`N` is the number of non zero elements in the specified\n        batch index and :math:`N_F` is the number of channels.\n        """"""\n        row_inds = self.coords_man.get_row_indices_at(self.coords_key,\n                                                      batch_index)\n        return self.C[row_inds, 1:], self._F[row_inds]\n\n    @property\n    def decomposed_coordinates_and_features(self):\n        r""""""Returns a list of coordinates and a list of features per batch.abs\n\n        """"""\n        row_inds_list = self.coords_man.get_row_indices_per_batch(\n            self.coords_key)\n        return [self.C[row_inds, 1:] for row_inds in row_inds_list], \\\n            [self._F[row_inds] for row_inds in row_inds_list]\n\n    @property\n    def D(self):\n        r""""""\n        The spatial dimension of the sparse tensor. This is equal to the number\n        of columns of :attr:`C` minus 1.\n        """"""\n        return self.coords_key.D\n\n    @property\n    def dimension(self):\n        r""""""Alias of attr:`D`\n        """"""\n        return self.D\n\n    @property\n    def requires_grad(self):\n        return self._F.requires_grad\n\n    def requires_grad_(self, requires_grad: bool = True):\n        self._F.requires_grad_(requires_grad)\n\n    def float(self):\n        self._F = self._F.float()\n\n    def double(self):\n        self._F = self._F.double()\n\n    def set_tensor_stride(self, s):\n        ss = convert_to_int_list(s, self.D)\n        self.coords_key.setTensorStride(ss)\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'(\' + os.linesep \\\n            + \'  Coords=\' + str(self.C) + os.linesep \\\n            + \'  Feats=\' + str(self.F) + os.linesep \\\n            + \'  coords_key=\' + str(self.coords_key) \\\n            + \'  tensor_stride=\' + str(self.coords_key.getTensorStride()) + os.linesep \\\n            + \'  coords_man=\' + str(self.coords_man) \\\n            + \'  spatial dimension=\' + str(self.D) + \')\'\n\n    def __len__(self):\n        return len(self._F)\n\n    def size(self):\n        return self._F.size()\n\n    @property\n    def shape(self):\n        return self._F.shape\n\n    def to(self, device):\n        self._F = self._F.to(device)\n        return self\n\n    def cpu(self):\n        self._F = self._F.cpu()\n        return self\n\n    @property\n    def device(self):\n        return self._F.device\n\n    @property\n    def dtype(self):\n        return self._F.dtype\n\n    def get_device(self):\n        return self._F.get_device()\n\n    # Operation overloading\n    def __iadd__(self, other):\n        assert isinstance(other, SparseTensor)\n        assert self.coords_man == other.coords_man, COORDS_MAN_DIFFERENT_ERROR\n        assert self.coords_key == other.coords_key, COORDS_KEY_DIFFERENT_ERROR\n\n        self._F += other.F\n        return self\n\n    def __isub__(self, other):\n        assert isinstance(other, SparseTensor)\n        assert self.coords_man == other.coords_man, COORDS_MAN_DIFFERENT_ERROR\n        assert self.coords_key == other.coords_key, COORDS_KEY_DIFFERENT_ERROR\n\n        self._F -= other.F\n        return self\n\n    def __imul__(self, other):\n        assert isinstance(other, SparseTensor)\n        assert self.coords_man == other.coords_man, COORDS_MAN_DIFFERENT_ERROR\n        assert self.coords_key == other.coords_key, COORDS_KEY_DIFFERENT_ERROR\n\n        self._F *= other.F\n        return self\n\n    def __idiv__(self, other):\n        assert isinstance(other, SparseTensor)\n        assert self.coords_man == other.coords_man, COORDS_MAN_DIFFERENT_ERROR\n        assert self.coords_key == other.coords_key, COORDS_KEY_DIFFERENT_ERROR\n\n        self._F /= other.F\n        return self\n\n    def __add__(self, other):\n        r""""""\n        Add its feature with the corresponding feature of the other\n        :attr:`MinkowskiEngine.SparseTensor` or a :attr:`torch.Tensor`\n        element-wise. For coordinates that exist on one sparse tensor but not\n        on the other, features of the counterpart that do not exist will be set\n        to 0.\n        """"""\n        assert isinstance(other, (SparseTensor, torch.Tensor))\n        if isinstance(other, SparseTensor):\n            assert self.coords_man == other.coords_man, COORDS_MAN_DIFFERENT_ERROR\n\n            if self.coords_key == other.coords_key:\n                return SparseTensor(\n                    self._F + other.F,\n                    coords_key=self.coords_key,\n                    coords_manager=self.coords_man)\n            else:\n                # Generate union maps\n                out_key = CoordsKey(self.coords_man.D)\n                ins, outs = self.coords_man.get_union_map(\n                    (self.coords_key, other.coords_key), out_key)\n                N_out = self.coords_man.get_coords_size_by_coords_key(out_key)\n                out_F = torch.zeros((N_out, self._F.size(1)),\n                                    dtype=self.dtype,\n                                    device=self.device)\n                out_F[outs[0]] = self._F[ins[0]]\n                out_F[outs[1]] += other._F[ins[1]]\n                return SparseTensor(\n                    out_F, coords_key=out_key, coords_manager=self.coords_man)\n        else:  # when it is a torch.Tensor\n            return SparseTensor(\n                self._F + other,\n                coords_key=self.coords_key,\n                coords_manager=self.coords_man)\n\n    def __sub__(self, other):\n        r""""""\n        Subtract the feature of the other :attr:`MinkowskiEngine.SparseTensor`\n        or a :attr:`torch.Tensor` from its corresponding feature element-wise.\n        For coordinates that exist on one sparse tensor but not on the other,\n        features of the counterpart that do not exist will be set to 0.\n        """"""\n        assert isinstance(other, (SparseTensor, torch.Tensor))\n        if isinstance(other, SparseTensor):\n            assert self.coords_man == other.coords_man, COORDS_MAN_DIFFERENT_ERROR\n\n            if self.coords_key == other.coords_key:\n                return SparseTensor(\n                    self._F - other.F,\n                    coords_key=self.coords_key,\n                    coords_manager=self.coords_man)\n            else:\n                # Generate union maps\n                out_key = CoordsKey(self.coords_man.D)\n                ins, outs = self.coords_man.get_union_map(\n                    (self.coords_key, other.coords_key), out_key)\n                N_out = self.coords_man.get_coords_size_by_coords_key(out_key)\n                out_F = torch.zeros((N_out, self._F.size(1)),\n                                    dtype=self.dtype,\n                                    device=self.device)\n                out_F[outs[0]] = self._F[ins[0]]\n                out_F[outs[1]] -= other._F[ins[1]]\n                return SparseTensor(\n                    out_F, coords_key=out_key, coords_manager=self.coords_man)\n\n        else:  # when it is a torch.Tensor\n            return SparseTensor(\n                self._F - other,\n                coords_key=self.coords_key,\n                coords_manager=self.coords_man)\n\n    def __mul__(self, other):\n        r""""""\n        Multiply its feature of with the corresponding feature of the other\n        :attr:`MinkowskiEngine.SparseTensor` or a :attr:`torch.Tensor`\n        element-wise. For coordinates that exist on one sparse tensor but not\n        on the other, features of the counterpart that do not exist will be set\n        to 0.\n        """"""\n        assert isinstance(other, (SparseTensor, torch.Tensor))\n        if isinstance(other, SparseTensor):\n            assert self.coords_man == other.coords_man, COORDS_MAN_DIFFERENT_ERROR\n\n            if self.coords_key == other.coords_key:\n                return SparseTensor(\n                    self._F * other.F,\n                    coords_key=self.coords_key,\n                    coords_manager=self.coords_man)\n            else:\n                # Generate union maps\n                out_key = CoordsKey(self.coords_man.D)\n                ins, outs = self.coords_man.get_union_map(\n                    (self.coords_key, other.coords_key), out_key)\n                N_out = self.coords_man.get_coords_size_by_coords_key(out_key)\n                out_F = torch.zeros((N_out, self._F.size(1)),\n                                    dtype=self.dtype,\n                                    device=self.device)\n                out_F[outs[0]] = self._F[ins[0]]\n                out_F[outs[1]] *= other._F[ins[1]]\n                return SparseTensor(\n                    out_F, coords_key=out_key, coords_manager=self.coords_man)\n        else:  # when it is a torch.Tensor\n            return SparseTensor(\n                self._F * other,\n                coords_key=self.coords_key,\n                coords_manager=self.coords_man)\n\n    def __truediv__(self, other):\n        r""""""\n        Divide its feature by the corresponding feature of the other\n        :attr:`MinkowskiEngine.SparseTensor` or a :attr:`torch.Tensor`\n        element-wise. For coordinates that exist on one sparse tensor but not\n        on the other, features of the counterpart that do not exist will be set\n        to 0.\n        """"""\n        assert isinstance(other, (SparseTensor, torch.Tensor))\n        if isinstance(other, SparseTensor):\n            assert self.coords_man == other.coords_man, COORDS_MAN_DIFFERENT_ERROR\n\n            if self.coords_key == other.coords_key:\n                return SparseTensor(\n                    self._F / other.F,\n                    coords_key=self.coords_key,\n                    coords_manager=self.coords_man)\n            else:\n                # Generate union maps\n                out_key = CoordsKey(self.coords_man.D)\n                ins, outs = self.coords_man.get_union_map(\n                    (self.coords_key, other.coords_key), out_key)\n                N_out = self.coords_man.get_coords_size_by_coords_key(out_key)\n                out_F = torch.zeros((N_out, self._F.size(1)),\n                                    dtype=self.dtype,\n                                    device=self.device)\n                out_F[outs[0]] = self._F[ins[0]]\n                out_F[outs[1]] /= other._F[ins[1]]\n                return SparseTensor(\n                    out_F, coords_key=out_key, coords_manager=self.coords_man)\n        else:  # when it is a torch.Tensor\n            return SparseTensor(\n                self._F / other,\n                coords_key=self.coords_key,\n                coords_manager=self.coords_man)\n\n    def __power__(self, power):\n        return SparseTensor(\n            self._F**power,\n            coords_key=self.coords_key,\n            coords_manager=self.coords_man)\n\n    # Conversion functions\n    def sparse(self, min_coords=None, max_coords=None, contract_coords=True):\n        r""""""Convert the :attr:`MinkowskiEngine.SparseTensor` to a torch sparse\n        tensor.\n\n        Args:\n            :attr:`min_coords` (torch.IntTensor, optional): The min\n            coordinates of the output sparse tensor. Must be divisible by the\n            current :attr:`tensor_stride`.\n\n            :attr:`max_coords` (torch.IntTensor, optional): The max coordinates\n            of the output sparse tensor (inclusive). Must be divisible by the\n            current :attr:`tensor_stride`.\n\n            :attr:`contract_coords` (bool, optional): Given True, the output\n            coordinates will be divided by the tensor stride to make features\n            contiguous.\n\n        Returns:\n            :attr:`spare_tensor` (torch.sparse.Tensor): the torch sparse tensor\n            representation of the self in `[Batch Dim, Spatial Dims..., Feature\n            Dim]`. The coordinate of each feature can be accessed via\n            `min_coord + tensor_stride * [the coordinate of the dense tensor]`.\n\n            :attr:`min_coords` (torch.IntTensor): the D-dimensional vector\n            defining the minimum coordinate of the output sparse tensor. If\n            :attr:`contract_coords` is True, the :attr:`min_coords` will also\n            be contracted.\n\n            :attr:`tensor_stride` (torch.IntTensor): the D-dimensional vector\n            defining the stride between tensor elements.\n\n        """"""\n\n        if min_coords is not None:\n            assert isinstance(min_coords, torch.IntTensor)\n            assert min_coords.numel() == self.D\n        if max_coords is not None:\n            assert isinstance(max_coords, torch.IntTensor)\n            assert min_coords.numel() == self.D\n\n        def torch_sparse_Tensor(coords, feats, size=None):\n            if size is None:\n                if feats.dtype == torch.float64:\n                    return torch.sparse.DoubleTensor(coords, feats)\n                elif feats.dtype == torch.float32:\n                    return torch.sparse.FloatTensor(coords, feats)\n                else:\n                    raise ValueError(\'Feature type not supported.\')\n            else:\n                if feats.dtype == torch.float64:\n                    return torch.sparse.DoubleTensor(coords, feats, size)\n                elif feats.dtype == torch.float32:\n                    return torch.sparse.FloatTensor(coords, feats, size)\n                else:\n                    raise ValueError(\'Feature type not supported.\')\n\n        # Use int tensor for all operations\n        tensor_stride = torch.IntTensor(self.tensor_stride)\n\n        # New coordinates\n        coords = self.C\n        coords, batch_indices = coords[:, 1:], coords[:, 0]\n\n        # TODO, batch first\n        if min_coords is None:\n            min_coords, _ = coords.min(0, keepdim=True)\n        elif min_coords.ndim == 1:\n            min_coords = min_coords.unsqueeze(0)\n\n        assert (min_coords % tensor_stride).sum() == 0, \\\n            ""The minimum coordinates must be divisible by the tensor stride.""\n\n        if max_coords is not None:\n            if max_coords.ndim == 1:\n                max_coords = max_coords.unsqueeze(0)\n            assert (max_coords % tensor_stride).sum() == 0, \\\n                ""The maximum coordinates must be divisible by the tensor stride.""\n\n        coords -= min_coords\n\n        if coords.ndim == 1:\n            coords = coords.unsqueeze(1)\n        if batch_indices.ndim == 1:\n            batch_indices = batch_indices.unsqueeze(1)\n\n        # return the contracted tensor\n        if contract_coords:\n            coords = coords // tensor_stride\n            if max_coords is not None:\n                max_coords = max_coords // tensor_stride\n            min_coords = min_coords // tensor_stride\n\n        new_coords = torch.cat((batch_indices, coords), dim=1).long()\n\n        size = None\n        if max_coords is not None:\n            size = max_coords - min_coords + 1  # inclusive\n            # Squeeze to make the size one-dimensional\n            size = size.squeeze()\n\n            max_batch = max(self.coords_man.get_batch_indices())\n            size = torch.Size([max_batch + 1, *size, self.F.size(1)])\n\n        sparse_tensor = torch_sparse_Tensor(new_coords.t().to(self.F.device),\n                                            self.F, size)\n        tensor_stride = torch.IntTensor(self.tensor_stride)\n        return sparse_tensor, min_coords, tensor_stride\n\n    def dense(self, min_coords=None, max_coords=None, contract_coords=True):\n        r""""""Convert the :attr:`MinkowskiEngine.SparseTensor` to a torch dense\n        tensor.\n\n        Args:\n            :attr:`min_coords` (torch.IntTensor, optional): The min\n            coordinates of the output sparse tensor. Must be divisible by the\n            current :attr:`tensor_stride`.\n\n            :attr:`max_coords` (torch.IntTensor, optional): The max coordinates\n            of the output sparse tensor (inclusive). Must be divisible by the\n            current :attr:`tensor_stride`.\n\n            :attr:`contract_coords` (bool, optional): Given True, the output\n            coordinates will be divided by the tensor stride to make features\n            contiguous.\n\n        Returns:\n            :attr:`spare_tensor` (torch.sparse.Tensor): the torch sparse tensor\n            representation of the self in `[Batch Dim, Feature Dim, Spatial\n            Dim..., Spatial Dim]`. The coordinate of each feature can be\n            accessed via `min_coord + tensor_stride * [the coordinate of the\n            dense tensor]`.\n\n            :attr:`min_coords` (torch.IntTensor): the D-dimensional vector\n            defining the minimum coordinate of the output sparse tensor. If\n            :attr:`contract_coords` is True, the :attr:`min_coords` will also\n            be contracted.\n\n            :attr:`tensor_stride` (torch.IntTensor): the D-dimensional vector\n            defining the stride between tensor elements.\n\n        """"""\n        if min_coords is not None:\n            assert isinstance(min_coords, torch.IntTensor)\n            assert min_coords.numel() == self.D\n        if max_coords is not None:\n            assert isinstance(max_coords, torch.IntTensor)\n            assert min_coords.numel() == self.D\n\n        # Use int tensor for all operations\n        tensor_stride = torch.IntTensor(self.tensor_stride)\n\n        # New coordinates\n        coords = self.C\n        coords, batch_indices = coords[:, 1:], coords[:, 0]\n\n        # TODO, batch first\n        if min_coords is None:\n            min_coords, _ = coords.min(0, keepdim=True)\n        elif min_coords.ndim == 1:\n            min_coords = min_coords.unsqueeze(0)\n\n        assert (min_coords % tensor_stride).sum() == 0, \\\n            ""The minimum coordinates must be divisible by the tensor stride.""\n\n        if max_coords is not None:\n            if max_coords.ndim == 1:\n                max_coords = max_coords.unsqueeze(0)\n            assert (max_coords % tensor_stride).sum() == 0, \\\n                ""The maximum coordinates must be divisible by the tensor stride.""\n\n        coords -= min_coords\n\n        if coords.ndim == 1:\n            coords = coords.unsqueeze(1)\n\n        # return the contracted tensor\n        if contract_coords:\n            coords = coords // tensor_stride\n            if max_coords is not None:\n                max_coords = max_coords // tensor_stride\n            min_coords = min_coords // tensor_stride\n\n        size = None\n        nchannels = self.F.size(1)\n        max_batch = max(self.coords_man.get_batch_indices())\n        if max_coords is not None:\n            size = max_coords - min_coords + 1  # inclusive\n            # Squeeze to make the size one-dimensional\n            size = size.squeeze()\n            size = torch.Size([max_batch + 1, nchannels, *size])\n        else:\n            size = coords.max(0)[0] + 1\n            size = torch.Size([max_batch + 1, nchannels, *size.numpy()])\n\n        dense_F = torch.zeros(size, dtype=self.F.dtype, device=self.F.device)\n\n        tcoords = coords.t().long()\n        batch_indices = batch_indices.long()\n        exec(""dense_F[batch_indices, :, "" +\n             "", "".join([f""tcoords[{i}]"" for i in range(len(tcoords))]) +\n             ""] = self.F"")\n\n        tensor_stride = torch.IntTensor(self.tensor_stride)\n        return dense_F, min_coords, tensor_stride\n\n    def slice(self, X, slicing_mode=0):\n        r""""""\n\n        Args:\n           :attr:`X` (:attr:`MinkowskiEngine.SparseTensor`): a sparse tensor\n           that discretized the original input.\n\n           :attr:`slicing_mode`: For future updates.\n\n        Returns:\n           :attr:`sliced_feats` (:attr:`torch.Tensor`): the resulting feature\n           matrix that slices features on the discretized coordinates to the\n           original continuous coordinates that generated the input X.\n\n        Example::\n\n           >>> # coords, feats from a data loader\n           >>> print(len(coords))  # 227742\n           >>> sinput = ME.SparseTensor(coords=coords, feats=feats, quantization_mode=SparseTensorQuantizationMode.UNWEIGHTED_AVERAGE)\n           >>> print(len(sinput))  # 161890 quantization results in fewer voxels\n           >>> soutput = network(sinput)\n           >>> print(len(soutput))  # 161890 Output with the same resolution\n           >>> outputs = soutput.slice(sinput)\n           >>> assert(outputs, torch.Tensor)  # regular differentiable pytorch tensor\n           >>> len(outputs) == len(coords)  # recovers the original ordering and length\n        """"""\n        # Currently only supports unweighted slice.\n        return self.feats[X.inverse_mapping]\n\n    def features_at_coords(self, query_coords: torch.Tensor):\n        r""""""Extract features at the specified coordinate matrix.\n\n        Args:\n           :attr:`query_coords` (:attr:`torch.IntTensor`): a coordinate matrix\n           of size :math:`N \\times (D + 1)` where :math:`D` is the size of the\n           spatial dimension.\n\n        Returns:\n           :attr:`query_feats` (:attr:`torch.Tensor`): a feature matrix of size\n           :math:`N \\times D_F` where :math:`D_F` is the number of channels in\n           the feature. Features for the coordinates that are not found, it will be zero.\n\n           :attr:`valid_rows` (:attr:`list`): a list of row indices that\n           contain valid values. The rest of the rows that are not found in the\n           `query_feats` will be 0.\n\n        """"""\n        cm = self.coords_man\n\n        self_key = self.coords_key\n        query_key = cm.create_coords_key(query_coords)\n\n        self_indices, query_indices = cm.get_kernel_map(\n            self_key, query_key, kernel_size=1)\n        query_feats = torch.zeros((len(query_coords), self._F.size(1)),\n                                  dtype=self.dtype,\n                                  device=self.device)\n\n        if len(self_indices[0]) > 0:\n            query_feats[query_indices[0]] = self._F[self_indices[0]]\n        return query_feats, query_indices[0]\n\n\ndef _get_coords_key(\n        input: SparseTensor,\n        coords: Union[torch.IntTensor, CoordsKey, SparseTensor] = None,\n        tensor_stride: Union[Sequence, np.ndarray, torch.IntTensor] = 1):\n    r""""""Process coords according to its type.\n    """"""\n    if coords is not None:\n        assert isinstance(coords, (CoordsKey, torch.IntTensor, SparseTensor))\n        if isinstance(coords, torch.IntTensor):\n            coords_key = input.coords_man.create_coords_key(\n                coords,\n                tensor_stride=tensor_stride,\n                force_creation=True,\n                force_remap=True,\n                allow_duplicate_coords=True)\n        elif isinstance(coords, SparseTensor):\n            coords_key = coords.coords_key\n        else:  # CoordsKey type due to the previous assertion\n            coords_key = coords\n    else:\n        coords_key = CoordsKey(input.D)\n    return coords_key\n'"
MinkowskiEngine/__init__.py,0,"b'# Copyright (c) 2020 NVIDIA CORPORATION.\n# Copyright (c) 2018-2020 Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\n__version__ = ""0.4.3""\n\nimport os\nimport sys\n\nfile_dir = os.path.dirname(__file__)\nsys.path.append(file_dir)\n\n# Must be imported first to load all required shared libs\nimport torch\n\nfrom MinkowskiEngineBackend import MemoryManagerBackend\n\nfrom SparseTensor import SparseTensor, SparseTensorOperationMode, SparseTensorQuantizationMode, \\\n    set_sparse_tensor_operation_mode, sparse_tensor_operation_mode, clear_global_coords_man\n\nfrom Common import RegionType, convert_to_int_tensor, convert_region_type, \\\n    MinkowskiModuleBase, KernelGenerator, GlobalPoolingMode\n\nfrom MinkowskiCoords import CoordsKey, CoordsManager, set_memory_manager_backend\n\nfrom MinkowskiConvolution import MinkowskiConvolutionFunction, MinkowskiConvolution, \\\n    MinkowskiConvolutionTransposeFunction, MinkowskiConvolutionTranspose\n\nfrom MinkowskiChannelwiseConvolution import MinkowskiChannelwiseConvolution\n\nfrom MinkowskiPooling import MinkowskiAvgPoolingFunction, MinkowskiAvgPooling, \\\n    MinkowskiSumPooling, \\\n    MinkowskiPoolingTransposeFunction, MinkowskiPoolingTranspose, \\\n    MinkowskiGlobalPoolingFunction, MinkowskiGlobalPooling, \\\n    MinkowskiGlobalSumPooling, MinkowskiGlobalAvgPooling, \\\n    MinkowskiGlobalMaxPoolingFunction, MinkowskiGlobalMaxPooling, \\\n    MinkowskiMaxPoolingFunction, MinkowskiMaxPooling\n\nfrom MinkowskiBroadcast import MinkowskiBroadcastFunction, \\\n    MinkowskiBroadcast, MinkowskiBroadcastConcatenation, \\\n    MinkowskiBroadcastAddition, MinkowskiBroadcastMultiplication, OperationType\n\nfrom MinkowskiNonlinearity import MinkowskiReLU, MinkowskiSigmoid, MinkowskiSoftmax, \\\n    MinkowskiPReLU, MinkowskiELU, MinkowskiSELU, MinkowskiCELU, MinkowskiDropout, \\\n    MinkowskiThreshold, MinkowskiTanh\n\nfrom MinkowskiNormalization import MinkowskiBatchNorm, MinkowskiSyncBatchNorm, \\\n    MinkowskiInstanceNorm, MinkowskiInstanceNormFunction, MinkowskiStableInstanceNorm\n\nfrom MinkowskiPruning import MinkowskiPruning, MinkowskiPruningFunction\n\nfrom MinkowskiUnion import MinkowskiUnion, MinkowskiUnionFunction\n\nfrom MinkowskiNetwork import MinkowskiNetwork\n\nimport MinkowskiOps\n\nfrom MinkowskiOps import MinkowskiLinear, cat\n\nimport MinkowskiFunctional\n\nimport MinkowskiEngine.utils as utils\nimport MinkowskiEngine.modules as modules\n'"
docs/conf.py,0,"b'import os\nimport sys\n# import pkg_resources\n# pkg_resources.require(\'MinkowskiEngine==0.4.2a1\')\nimport MinkowskiEngine as ME\n\n# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# sys.path.insert(0, os.path.abspath(\'../\'))\nsys.path.insert(0, os.path.abspath(\'../../\'))\n# autodoc_mock_imports = [\'MinkowskiEngine.examples\']\n\n\n# -- Project information -----------------------------------------------------\n\nproject = \'MinkowskiEngine\'\ncopyright = \'2020, Chris Choy\'\nauthor = \'Chris Choy\'\n\n# The short X.Y version\nversion = ME.__version__\n# The full version, including alpha/beta/rc tags\nrelease = ME.__version__\ngithub_doc_root = \'https://github.com/StanfordVL/MinkowskiEngine\'\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx_markdown_tables\',\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.mathjax\',\n    \'recommonmark\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [\'_build\', \'README.md\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = None\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n# html_theme = \'alabaster\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {\n#     \'github_user\': \'StanfordVL\',\n#     \'github_repo\': \'MinkowskiEngine\',\n#     \'github_banner\': True\n# }\nhtml_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'MinkowskiEngineDoc\'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'MinkowskiEngine.tex\', \'MinkowskiEngine Documentation\',\n     \'Chris Choy\', \'manual\'),\n]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'minkowskiengine\', \'MinkowskiEngine Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'MinkowskiEngine\', \'MinkowskiEngine Documentation\',\n     author, \'MinkowskiEngine\', \'The generalized sparse convolution library.\',\n     \'Miscellaneous\'),\n]\n\n\n# -- Options for Epub output -------------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = project\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#\n# epub_identifier = \'\'\n\n# A unique identification for the text.\n#\n# epub_uid = \'\'\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = [\'search.html\']\n'"
examples/__init__.py,0,b''
examples/common.py,0,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport numpy as np\nimport time\n\nimport torch\n\n\nclass Timer(object):\n    """"""A simple timer.""""""\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.total_time = 0\n        self.calls = 0\n        self.start_time = 0\n        self.diff = 0\n        self.averate_time = 0\n        self.min_time = np.Inf\n\n    def tic(self):\n        # using time.time instead of time.clock because time time.clock\n        # does not normalize for multithreading\n        self.start_time = time.time()\n\n    def toc(self, average=False):\n        self.diff = time.time() - self.start_time\n        self.total_time += self.diff\n        self.calls += 1\n        self.average_time = self.total_time / self.calls\n        if self.diff < self.min_time:\n            self.min_time = self.diff\n        if average:\n            return self.average_time\n        else:\n            return self.diff\n'"
examples/completion.py,13,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport os\nimport sys\nimport subprocess\nimport argparse\nimport logging\nimport numpy as np\nfrom time import time\nimport urllib\n# Must be imported before large libs\ntry:\n    import open3d as o3d\nexcept ImportError:\n    raise ImportError(\'Please install open3d with `pip install open3d`.\')\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.optim as optim\n\nimport MinkowskiEngine as ME\n\nfrom examples.reconstruction import ModelNet40Dataset, InfSampler\n\nM = np.array([[0.80656762, -0.5868724, -0.07091862],\n              [0.3770505, 0.418344, 0.82632997],\n              [-0.45528188, -0.6932309, 0.55870326]])\n\nassert int(\n    o3d.__version__.split(\'.\')[1]\n) >= 8, f\'Requires open3d version >= 0.8, the current version is {o3d.__version__}\'\n\nif not os.path.exists(\'ModelNet40\'):\n    logging.info(\'Downloading the fixed ModelNet40 dataset...\')\n    subprocess.run([""sh"", ""./examples/download_modelnet40.sh""])\n\n\n###############################################################################\n# Utility functions\n###############################################################################\ndef PointCloud(points, colors=None):\n\n    pcd = o3d.geometry.PointCloud()\n    pcd.points = o3d.utility.Vector3dVector(points)\n    if colors is not None:\n        pcd.colors = o3d.utility.Vector3dVector(colors)\n    return pcd\n\n\nclass CollationAndTransformation():\n\n    def __init__(self, resolution):\n        self.resolution = resolution\n\n    def random_crop(self, coords_list):\n        crop_coords_list = []\n        for coords in coords_list:\n            sel = coords[:, 0] < self.resolution / 3\n            crop_coords_list.append(coords[sel])\n        return crop_coords_list\n\n    def __call__(self, list_data):\n        coords, feats, labels = list(zip(*list_data))\n        coords = self.random_crop(coords)\n\n        # Concatenate all lists\n        return {\n            \'coords\': ME.utils.batched_coordinates(coords),\n            \'xyzs\': [torch.from_numpy(feat).float() for feat in feats],\n            \'cropped_coords\': coords,\n            \'labels\': torch.LongTensor(labels),\n        }\n\n\ndef make_data_loader(phase, augment_data, batch_size, shuffle, num_workers,\n                     repeat, config):\n    dset = ModelNet40Dataset(phase, config=config)\n\n    args = {\n        \'batch_size\': batch_size,\n        \'num_workers\': num_workers,\n        \'collate_fn\': CollationAndTransformation(config.resolution),\n        \'pin_memory\': False,\n        \'drop_last\': False\n    }\n\n    if repeat:\n        args[\'sampler\'] = InfSampler(dset, shuffle)\n    else:\n        args[\'shuffle\'] = shuffle\n\n    loader = torch.utils.data.DataLoader(dset, **args)\n\n    return loader\n\n\nch = logging.StreamHandler(sys.stdout)\nlogging.getLogger().setLevel(logging.INFO)\nlogging.basicConfig(\n    format=os.uname()[1].split(\'.\')[0] + \' %(asctime)s %(message)s\',\n    datefmt=\'%m/%d %H:%M:%S\',\n    handlers=[ch])\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--resolution\', type=int, default=128)\nparser.add_argument(\'--max_iter\', type=int, default=30000)\nparser.add_argument(\'--val_freq\', type=int, default=1000)\nparser.add_argument(\'--batch_size\', default=16, type=int)\nparser.add_argument(\'--lr\', default=1e-2, type=float)\nparser.add_argument(\'--momentum\', type=float, default=0.9)\nparser.add_argument(\'--weight_decay\', type=float, default=1e-4)\nparser.add_argument(\'--num_workers\', type=int, default=1)\nparser.add_argument(\'--stat_freq\', type=int, default=50)\nparser.add_argument(\n    \'--weights\', type=str, default=\'modelnet_completion.pth\')\nparser.add_argument(\'--load_optimizer\', type=str, default=\'true\')\nparser.add_argument(\'--train\', action=\'store_true\')\nparser.add_argument(\'--max_visualization\', type=int, default=4)\n\n###############################################################################\n# End of utility functions\n###############################################################################\n\n\nclass CompletionNet(nn.Module):\n\n    ENC_CHANNELS = [16, 32, 64, 128, 256, 512, 1024]\n    DEC_CHANNELS = [16, 32, 64, 128, 256, 512, 1024]\n\n    def __init__(self, resolution, in_nchannel=512):\n        nn.Module.__init__(self)\n\n        self.resolution = resolution\n\n        # Input sparse tensor must have tensor stride 128.\n        enc_ch = self.ENC_CHANNELS\n        dec_ch = self.DEC_CHANNELS\n\n        # Encoder\n        self.enc_block_s1 = nn.Sequential(\n            ME.MinkowskiConvolution(\n                1, enc_ch[0], kernel_size=3, stride=1, dimension=3),\n            ME.MinkowskiBatchNorm(enc_ch[0]),\n            ME.MinkowskiELU(),\n        )\n\n        self.enc_block_s1s2 = nn.Sequential(\n            ME.MinkowskiConvolution(\n                enc_ch[0], enc_ch[1], kernel_size=2, stride=2, dimension=3),\n            ME.MinkowskiBatchNorm(enc_ch[1]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(\n                enc_ch[1], enc_ch[1], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(enc_ch[1]),\n            ME.MinkowskiELU(),\n        )\n\n        self.enc_block_s2s4 = nn.Sequential(\n            ME.MinkowskiConvolution(\n                enc_ch[1], enc_ch[2], kernel_size=2, stride=2, dimension=3),\n            ME.MinkowskiBatchNorm(enc_ch[2]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(\n                enc_ch[2], enc_ch[2], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(enc_ch[2]),\n            ME.MinkowskiELU(),\n        )\n\n        self.enc_block_s4s8 = nn.Sequential(\n            ME.MinkowskiConvolution(\n                enc_ch[2], enc_ch[3], kernel_size=2, stride=2, dimension=3),\n            ME.MinkowskiBatchNorm(enc_ch[3]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(\n                enc_ch[3], enc_ch[3], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(enc_ch[3]),\n            ME.MinkowskiELU(),\n        )\n\n        self.enc_block_s8s16 = nn.Sequential(\n            ME.MinkowskiConvolution(\n                enc_ch[3], enc_ch[4], kernel_size=2, stride=2, dimension=3),\n            ME.MinkowskiBatchNorm(enc_ch[4]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(\n                enc_ch[4], enc_ch[4], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(enc_ch[4]),\n            ME.MinkowskiELU(),\n        )\n\n        self.enc_block_s16s32 = nn.Sequential(\n            ME.MinkowskiConvolution(\n                enc_ch[4], enc_ch[5], kernel_size=2, stride=2, dimension=3),\n            ME.MinkowskiBatchNorm(enc_ch[5]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(\n                enc_ch[5], enc_ch[5], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(enc_ch[5]),\n            ME.MinkowskiELU(),\n        )\n\n        self.enc_block_s32s64 = nn.Sequential(\n            ME.MinkowskiConvolution(\n                enc_ch[5], enc_ch[6], kernel_size=2, stride=2, dimension=3),\n            ME.MinkowskiBatchNorm(enc_ch[6]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(\n                enc_ch[6], enc_ch[6], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(enc_ch[6]),\n            ME.MinkowskiELU(),\n        )\n\n        # Decoder\n        self.dec_block_s64s32 = nn.Sequential(\n            ME.MinkowskiConvolutionTranspose(\n                enc_ch[6],\n                dec_ch[5],\n                kernel_size=4,\n                stride=2,\n                generate_new_coords=True,\n                dimension=3),\n            ME.MinkowskiBatchNorm(dec_ch[5]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(\n                dec_ch[5], dec_ch[5], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(dec_ch[5]),\n            ME.MinkowskiELU(),\n        )\n\n        self.dec_s32_cls = ME.MinkowskiConvolution(\n            dec_ch[5], 1, kernel_size=1, has_bias=True, dimension=3)\n\n        self.dec_block_s32s16 = nn.Sequential(\n            ME.MinkowskiConvolutionTranspose(\n                enc_ch[5],\n                dec_ch[4],\n                kernel_size=2,\n                stride=2,\n                generate_new_coords=True,\n                dimension=3),\n            ME.MinkowskiBatchNorm(dec_ch[4]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(\n                dec_ch[4], dec_ch[4], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(dec_ch[4]),\n            ME.MinkowskiELU(),\n        )\n\n        self.dec_s16_cls = ME.MinkowskiConvolution(\n            dec_ch[4], 1, kernel_size=1, has_bias=True, dimension=3)\n\n        self.dec_block_s16s8 = nn.Sequential(\n            ME.MinkowskiConvolutionTranspose(\n                dec_ch[4],\n                dec_ch[3],\n                kernel_size=2,\n                stride=2,\n                generate_new_coords=True,\n                dimension=3),\n            ME.MinkowskiBatchNorm(dec_ch[3]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(\n                dec_ch[3], dec_ch[3], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(dec_ch[3]),\n            ME.MinkowskiELU(),\n        )\n\n        self.dec_s8_cls = ME.MinkowskiConvolution(\n            dec_ch[3], 1, kernel_size=1, has_bias=True, dimension=3)\n\n        self.dec_block_s8s4 = nn.Sequential(\n            ME.MinkowskiConvolutionTranspose(\n                dec_ch[3],\n                dec_ch[2],\n                kernel_size=2,\n                stride=2,\n                generate_new_coords=True,\n                dimension=3),\n            ME.MinkowskiBatchNorm(dec_ch[2]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(\n                dec_ch[2], dec_ch[2], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(dec_ch[2]),\n            ME.MinkowskiELU(),\n        )\n\n        self.dec_s4_cls = ME.MinkowskiConvolution(\n            dec_ch[2], 1, kernel_size=1, has_bias=True, dimension=3)\n\n        self.dec_block_s4s2 = nn.Sequential(\n            ME.MinkowskiConvolutionTranspose(\n                dec_ch[2],\n                dec_ch[1],\n                kernel_size=2,\n                stride=2,\n                generate_new_coords=True,\n                dimension=3),\n            ME.MinkowskiBatchNorm(dec_ch[1]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(\n                dec_ch[1], dec_ch[1], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(dec_ch[1]),\n            ME.MinkowskiELU(),\n        )\n\n        self.dec_s2_cls = ME.MinkowskiConvolution(\n            dec_ch[1], 1, kernel_size=1, has_bias=True, dimension=3)\n\n        self.dec_block_s2s1 = nn.Sequential(\n            ME.MinkowskiConvolutionTranspose(\n                dec_ch[1],\n                dec_ch[0],\n                kernel_size=2,\n                stride=2,\n                generate_new_coords=True,\n                dimension=3),\n            ME.MinkowskiBatchNorm(dec_ch[0]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(\n                dec_ch[0], dec_ch[0], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(dec_ch[0]),\n            ME.MinkowskiELU(),\n        )\n\n        self.dec_s1_cls = ME.MinkowskiConvolution(\n            dec_ch[0], 1, kernel_size=1, has_bias=True, dimension=3)\n\n        # pruning\n        self.pruning = ME.MinkowskiPruning()\n\n    def get_batch_indices(self, out):\n        return out.coords_man.get_row_indices_per_batch(out.coords_key)\n\n    def get_target(self, out, target_key, kernel_size=1):\n        with torch.no_grad():\n            target = torch.zeros(len(out), dtype=torch.bool)\n            cm = out.coords_man\n            strided_target_key = cm.stride(\n                target_key, out.tensor_stride[0], force_creation=True)\n            ins, outs = cm.get_kernel_map(\n                out.coords_key,\n                strided_target_key,\n                kernel_size=kernel_size,\n                region_type=1)\n            for curr_in in ins:\n                target[curr_in] = 1\n        return target\n\n    def valid_batch_map(self, batch_map):\n        for b in batch_map:\n            if len(b) == 0:\n                return False\n        return True\n\n    def forward(self, partial_in, target_key):\n        out_cls, targets = [], []\n\n        enc_s1 = self.enc_block_s1(partial_in)\n        enc_s2 = self.enc_block_s1s2(enc_s1)\n        enc_s4 = self.enc_block_s2s4(enc_s2)\n        enc_s8 = self.enc_block_s4s8(enc_s4)\n        enc_s16 = self.enc_block_s8s16(enc_s8)\n        enc_s32 = self.enc_block_s16s32(enc_s16)\n        enc_s64 = self.enc_block_s32s64(enc_s32)\n\n        ##################################################\n        # Decoder 64 -> 32\n        ##################################################\n        dec_s32 = self.dec_block_s64s32(enc_s64)\n\n        # Add encoder features\n        dec_s32 = dec_s32 + enc_s32\n        dec_s32_cls = self.dec_s32_cls(dec_s32)\n        keep_s32 = (dec_s32_cls.F > 0).cpu().squeeze()\n\n        target = self.get_target(dec_s32, target_key)\n        targets.append(target)\n        out_cls.append(dec_s32_cls)\n\n        if self.training:\n            keep_s32 += target\n\n        # Remove voxels s32\n        dec_s32 = self.pruning(dec_s32, keep_s32.cpu())\n\n        ##################################################\n        # Decoder 32 -> 16\n        ##################################################\n        dec_s16 = self.dec_block_s32s16(dec_s32)\n\n        # Add encoder features\n        dec_s16 = dec_s16 + enc_s16\n        dec_s16_cls = self.dec_s16_cls(dec_s16)\n        keep_s16 = (dec_s16_cls.F > 0).cpu().squeeze()\n\n        target = self.get_target(dec_s16, target_key)\n        targets.append(target)\n        out_cls.append(dec_s16_cls)\n\n        if self.training:\n            keep_s16 += target\n\n        # Remove voxels s16\n        dec_s16 = self.pruning(dec_s16, keep_s16.cpu())\n\n        ##################################################\n        # Decoder 16 -> 8\n        ##################################################\n        dec_s8 = self.dec_block_s16s8(dec_s16)\n\n        # Add encoder features\n        dec_s8 = dec_s8 + enc_s8\n        dec_s8_cls = self.dec_s8_cls(dec_s8)\n\n        target = self.get_target(dec_s8, target_key)\n        targets.append(target)\n        out_cls.append(dec_s8_cls)\n        keep_s8 = (dec_s8_cls.F > 0).cpu().squeeze()\n\n        if self.training:\n            keep_s8 += target\n\n        # Remove voxels s16\n        dec_s8 = self.pruning(dec_s8, keep_s8.cpu())\n\n        ##################################################\n        # Decoder 8 -> 4\n        ##################################################\n        dec_s4 = self.dec_block_s8s4(dec_s8)\n\n        # Add encoder features\n        dec_s4 = dec_s4 + enc_s4\n        dec_s4_cls = self.dec_s4_cls(dec_s4)\n\n        target = self.get_target(dec_s4, target_key)\n        targets.append(target)\n        out_cls.append(dec_s4_cls)\n        keep_s4 = (dec_s4_cls.F > 0).cpu().squeeze()\n\n        if self.training:\n            keep_s4 += target\n\n        # Remove voxels s4\n        dec_s4 = self.pruning(dec_s4, keep_s4.cpu())\n\n        ##################################################\n        # Decoder 4 -> 2\n        ##################################################\n        dec_s2 = self.dec_block_s4s2(dec_s4)\n\n        # Add encoder features\n        dec_s2 = dec_s2 + enc_s2\n        dec_s2_cls = self.dec_s2_cls(dec_s2)\n\n        target = self.get_target(dec_s2, target_key)\n        targets.append(target)\n        out_cls.append(dec_s2_cls)\n        keep_s2 = (dec_s2_cls.F > 0).cpu().squeeze()\n\n        if self.training:\n            keep_s2 += target\n\n        # Remove voxels s2\n        dec_s2 = self.pruning(dec_s2, keep_s2.cpu())\n\n        ##################################################\n        # Decoder 2 -> 1\n        ##################################################\n        dec_s1 = self.dec_block_s2s1(dec_s2)\n        dec_s1_cls = self.dec_s1_cls(dec_s1)\n\n        # Add encoder features\n        dec_s1 = dec_s1 + enc_s1\n        dec_s1_cls = self.dec_s1_cls(dec_s1)\n\n        target = self.get_target(dec_s1, target_key)\n        targets.append(target)\n        out_cls.append(dec_s1_cls)\n        keep_s1 = (dec_s1_cls.F > 0).cpu().squeeze()\n\n        # Last layer does not require adding the target\n        # if self.training:\n        #     keep_s1 += target\n\n        # Remove voxels s1\n        dec_s1 = self.pruning(dec_s1, keep_s1.cpu())\n\n        return out_cls, targets, dec_s1\n\n\ndef train(net, dataloader, device, config):\n    optimizer = optim.SGD(\n        net.parameters(),\n        lr=config.lr,\n        momentum=config.momentum,\n        weight_decay=config.weight_decay)\n    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.95)\n\n    crit = nn.BCEWithLogitsLoss()\n\n    net.train()\n    train_iter = iter(dataloader)\n    # val_iter = iter(val_dataloader)\n    logging.info(f\'LR: {scheduler.get_lr()}\')\n    for i in range(config.max_iter):\n\n        s = time()\n        data_dict = train_iter.next()\n        d = time() - s\n\n        optimizer.zero_grad()\n\n        in_feat = torch.ones((len(data_dict[\'coords\']), 1))\n\n        sin = ME.SparseTensor(\n            feats=in_feat,\n            coords=data_dict[\'coords\'],\n        ).to(device)\n\n        # Generate target sparse tensor\n        cm = sin.coords_man\n        target_key = cm.create_coords_key(\n            ME.utils.batched_coordinates(data_dict[\'xyzs\']),\n            force_creation=True,\n            allow_duplicate_coords=True)\n\n        # Generate from a dense tensor\n        out_cls, targets, sout = net(sin, target_key)\n        num_layers, loss = len(out_cls), 0\n        losses = []\n        for out_cl, target in zip(out_cls, targets):\n            curr_loss = crit(out_cl.F.squeeze(),\n                             target.type(out_cl.F.dtype).to(device))\n            losses.append(curr_loss.item())\n            loss += curr_loss / num_layers\n\n        loss.backward()\n        optimizer.step()\n        t = time() - s\n\n        if i % config.stat_freq == 0:\n            logging.info(\n                f\'Iter: {i}, Loss: {loss.item():.3e}, Data Loading Time: {d:.3e}, Tot Time: {t:.3e}\'\n            )\n\n        if i % config.val_freq == 0 and i > 0:\n            torch.save(\n                {\n                    \'state_dict\': net.state_dict(),\n                    \'optimizer\': optimizer.state_dict(),\n                    \'scheduler\': scheduler.state_dict(),\n                    \'curr_iter\': i,\n                }, config.weights)\n\n            scheduler.step()\n            logging.info(f\'LR: {scheduler.get_lr()}\')\n\n            net.train()\n\n\ndef visualize(net, dataloader, device, config):\n    net.eval()\n    crit = nn.BCEWithLogitsLoss()\n    n_vis = 0\n\n    for data_dict in dataloader:\n        in_feat = torch.ones((len(data_dict[\'coords\']), 1))\n\n        sin = ME.SparseTensor(\n            feats=in_feat,\n            coords=data_dict[\'coords\'],\n        ).to(device)\n\n        # Generate target sparse tensor\n        cm = sin.coords_man\n        target_key = cm.create_coords_key(\n            ME.utils.batched_coordinates(data_dict[\'xyzs\']),\n            force_creation=True,\n            allow_duplicate_coords=True)\n\n        # Generate from a dense tensor\n        out_cls, targets, sout = net(sin, target_key)\n        num_layers, loss = len(out_cls), 0\n        for out_cl, target in zip(out_cls, targets):\n            loss += crit(out_cl.F.squeeze(),\n                         target.type(out_cl.F.dtype).to(device)) / num_layers\n\n        batch_coords, batch_feats = sout.decomposed_coordinates_and_features\n        for b, (coords, feats) in enumerate(zip(batch_coords, batch_feats)):\n            pcd = PointCloud(coords)\n            pcd.estimate_normals()\n            pcd.translate([0.6 * config.resolution, 0, 0])\n            pcd.rotate(M)\n            opcd = PointCloud(data_dict[\'cropped_coords\'][b])\n            opcd.translate([-0.6 * config.resolution, 0, 0])\n            opcd.estimate_normals()\n            opcd.rotate(M)\n            o3d.visualization.draw_geometries([pcd, opcd])\n\n            n_vis += 1\n            if n_vis > config.max_visualization:\n                return\n\n\nif __name__ == \'__main__\':\n    config = parser.parse_args()\n    logging.info(config)\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n    dataloader = make_data_loader(\n        \'val\',\n        augment_data=True,\n        batch_size=config.batch_size,\n        shuffle=True,\n        num_workers=config.num_workers,\n        repeat=True,\n        config=config)\n    in_nchannel = len(dataloader.dataset)\n\n    net = CompletionNet(config.resolution, in_nchannel=in_nchannel)\n    net.to(device)\n\n    logging.info(net)\n\n    if config.train:\n        train(net, dataloader, device, config)\n    else:\n        if not os.path.exists(config.weights):\n            logging.info(\n                f\'Downloaing pretrained weights. This might take a while...\')\n            urllib.request.urlretrieve(\n                ""https://bit.ly/36d9m1n"", filename=config.weights)\n\n        logging.info(f\'Loading weights from {config.weights}\')\n        checkpoint = torch.load(config.weights)\n        net.load_state_dict(checkpoint[\'state_dict\'])\n\n        visualize(net, dataloader, device, config)\n'"
examples/convolution.py,4,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport torch\n\nimport MinkowskiEngine as ME\n\nfrom tests.common import data_loader\n\n\ndef get_random_coords(dimension=2, tensor_stride=2):\n    torch.manual_seed(0)\n    # Create random coordinates with tensor stride == 2\n    coords = torch.rand(10, dimension + 1)\n    coords[:, :dimension] *= 5  # random coords\n    coords[:, -1] *= 2  # random batch index\n    coords = coords.floor().int()\n    coords = ME.utils.sparse_quantize(coords)\n    coords[:, :dimension] *= tensor_stride  # make the tensor stride 2\n    return coords, tensor_stride\n\n\ndef print_sparse_tensor(tensor):\n    for c, f in zip(tensor.C.numpy(), tensor.F.detach().numpy()):\n        print(f""Coordinate {c} : Feature {f}"")\n\n\ndef conv():\n    in_channels, out_channels, D = 2, 3, 2\n    coords, feats, labels = data_loader(in_channels, batch_size=1)\n\n    # Convolution\n    input = ME.SparseTensor(feats=feats, coords=coords)\n    conv = ME.MinkowskiConvolution(\n        in_channels,\n        out_channels,\n        kernel_size=3,\n        stride=2,\n        has_bias=False,\n        dimension=D)\n\n    output = conv(input)\n\n    print(\'Input:\')\n    print_sparse_tensor(input)\n\n    print(\'Output:\')\n    print_sparse_tensor(output)\n\n    # Convolution transpose and generate new coordinates\n    strided_coords, tensor_stride = get_random_coords()\n\n    input = ME.SparseTensor(\n        feats=torch.rand(len(strided_coords), in_channels),  #\n        coords=strided_coords,\n        tensor_stride=tensor_stride)\n    conv_tr = ME.MinkowskiConvolutionTranspose(\n        in_channels,\n        out_channels,\n        kernel_size=3,\n        stride=2,\n        has_bias=False,\n        dimension=D)\n    output = conv_tr(input)\n\n    print(\'\\nInput:\')\n    print_sparse_tensor(input)\n\n    print(\'Convolution Transpose Output:\')\n    print_sparse_tensor(output)\n\n\ndef conv_on_coords():\n    in_channels, out_channels, D = 2, 3, 2\n    coords, feats, labels = data_loader(in_channels, batch_size=1)\n\n    # Create input with tensor stride == 4\n    strided_coords4, tensor_stride4 = get_random_coords(tensor_stride=4)\n    strided_coords2, tensor_stride2 = get_random_coords(tensor_stride=2)\n    input = ME.SparseTensor(\n        feats=torch.rand(len(strided_coords4), in_channels),  #\n        coords=strided_coords4,\n        tensor_stride=tensor_stride4)\n    cm = input.coords_man\n\n    # Convolution transpose and generate new coordinates\n    conv_tr = ME.MinkowskiConvolutionTranspose(\n        in_channels,\n        out_channels,\n        kernel_size=3,\n        stride=2,\n        has_bias=False,\n        dimension=D)\n\n    pool_tr = ME.MinkowskiPoolingTranspose(\n        kernel_size=2,\n        stride=2,\n        dimension=D)\n\n    # If the there is no coordinates defined for the tensor stride, it will create one\n    # tensor stride 4 -> conv_tr with stride 2 -> tensor stride 2\n    output1 = conv_tr(input)\n    # output1 = pool_tr(input)\n\n    # convolution on the specified coords\n    output2 = conv_tr(input, coords)\n    # output2 = pool_tr(input, coords)\n\n    # convolution on the specified coords with tensor stride == 2\n    coords_key = cm.create_coords_key(strided_coords2, tensor_stride=2)\n    output3 = conv_tr(input, coords_key)\n    # output3 = pool_tr(input, coords_key)\n\n    # convolution on the coordinates of a sparse tensor\n    output4 = conv_tr(input, output1)\n    # output4 = pool_tr(input, output1)\n\n\nif __name__ == \'__main__\':\n    conv()\n    conv_on_coords()\n'"
examples/example.py,5,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport torch\nimport torch.nn as nn\nfrom torch.optim import SGD\n\nimport MinkowskiEngine as ME\n\nfrom tests.common import data_loader\n\n\nclass ExampleNetwork(ME.MinkowskiNetwork):\n\n    def __init__(self, in_feat, out_feat, D):\n        super(ExampleNetwork, self).__init__(D)\n        self.net = nn.Sequential(\n            ME.MinkowskiConvolution(\n                in_channels=in_feat,\n                out_channels=64,\n                kernel_size=3,\n                stride=2,\n                dilation=1,\n                has_bias=False,\n                dimension=D), ME.MinkowskiBatchNorm(64), ME.MinkowskiReLU(),\n            ME.MinkowskiConvolution(\n                in_channels=64,\n                out_channels=128,\n                kernel_size=3,\n                stride=2,\n                dimension=D), ME.MinkowskiBatchNorm(128), ME.MinkowskiReLU(),\n            ME.MinkowskiGlobalPooling(),\n            ME.MinkowskiLinear(128, out_feat))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nif __name__ == \'__main__\':\n    # loss and network\n    criterion = nn.CrossEntropyLoss()\n    net = ExampleNetwork(in_feat=3, out_feat=5, D=2)\n    print(net)\n\n    # a data loader must return a tuple of coords, features, and labels.\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n    net = net.to(device)\n    optimizer = SGD(net.parameters(), lr=1e-1)\n\n    for i in range(10):\n        optimizer.zero_grad()\n\n        # Get new data\n        coords, feat, label = data_loader()\n        input = ME.SparseTensor(feat, coords=coords).to(device)\n        label = label.to(device)\n\n        # Forward\n        output = net(input)\n\n        # Loss\n        loss = criterion(output.F, label)\n        print(\'Iteration: \', i, \', Loss: \', loss.item())\n\n        # Gradient\n        loss.backward()\n        optimizer.step()\n\n    # Saving and loading a network\n    torch.save(net.state_dict(), \'test.pth\')\n    net.load_state_dict(torch.load(\'test.pth\'))\n'"
examples/indoor.py,5,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport os\nimport argparse\nimport numpy as np\nfrom urllib.request import urlretrieve\ntry:\n    import open3d as o3d\nexcept ImportError:\n    raise ImportError(\'Please install open3d with `pip install open3d`.\')\n\nimport torch\nimport MinkowskiEngine as ME\nfrom examples.minkunet import MinkUNet34C\nfrom examples.common import Timer\n\n# Check if the weights and file exist and download\nif not os.path.isfile(\'weights.pth\'):\n    print(\'Downloading weights and a room ply file...\')\n    urlretrieve(""http://cvgl.stanford.edu/data2/minkowskiengine/weights.pth"",\n                \'weights.pth\')\n    urlretrieve(""http://cvgl.stanford.edu/data2/minkowskiengine/1.ply"", \'1.ply\')\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--file_name\', type=str, default=\'1.ply\')\nparser.add_argument(\'--weights\', type=str, default=\'weights.pth\')\nparser.add_argument(\'--use_cpu\', action=\'store_true\')\n\nCLASS_LABELS = (\'wall\', \'floor\', \'cabinet\', \'bed\', \'chair\', \'sofa\', \'table\',\n                \'door\', \'window\', \'bookshelf\', \'picture\', \'counter\', \'desk\',\n                \'curtain\', \'refrigerator\', \'shower curtain\', \'toilet\', \'sink\',\n                \'bathtub\', \'otherfurniture\')\n\nVALID_CLASS_IDS = [\n    1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 16, 24, 28, 33, 34, 36, 39\n]\n\nSCANNET_COLOR_MAP = {\n    0: (0., 0., 0.),\n    1: (174., 199., 232.),\n    2: (152., 223., 138.),\n    3: (31., 119., 180.),\n    4: (255., 187., 120.),\n    5: (188., 189., 34.),\n    6: (140., 86., 75.),\n    7: (255., 152., 150.),\n    8: (214., 39., 40.),\n    9: (197., 176., 213.),\n    10: (148., 103., 189.),\n    11: (196., 156., 148.),\n    12: (23., 190., 207.),\n    14: (247., 182., 210.),\n    15: (66., 188., 102.),\n    16: (219., 219., 141.),\n    17: (140., 57., 197.),\n    18: (202., 185., 52.),\n    19: (51., 176., 203.),\n    20: (200., 54., 131.),\n    21: (92., 193., 61.),\n    22: (78., 71., 183.),\n    23: (172., 114., 82.),\n    24: (255., 127., 14.),\n    25: (91., 163., 138.),\n    26: (153., 98., 156.),\n    27: (140., 153., 101.),\n    28: (158., 218., 229.),\n    29: (100., 125., 154.),\n    30: (178., 127., 135.),\n    32: (146., 111., 194.),\n    33: (44., 160., 44.),\n    34: (112., 128., 144.),\n    35: (96., 207., 209.),\n    36: (227., 119., 194.),\n    37: (213., 92., 176.),\n    38: (94., 106., 211.),\n    39: (82., 84., 163.),\n    40: (100., 85., 144.),\n}\n\n\ndef load_file(file_name):\n    pcd = o3d.io.read_point_cloud(file_name)\n    coords = np.array(pcd.points)\n    colors = np.array(pcd.colors)\n    return coords, colors, pcd\n\n\nif __name__ == \'__main__\':\n    config = parser.parse_args()\n    device = torch.device(\'cuda\' if (\n        torch.cuda.is_available() and not config.use_cpu) else \'cpu\')\n    print(f""Using {device}"")\n    # Define a model and load the weights\n    model = MinkUNet34C(3, 20).to(device)\n    model_dict = torch.load(config.weights)\n    model.load_state_dict(model_dict)\n    model.eval()\n\n    coords, colors, pcd = load_file(config.file_name)\n    # Measure time\n    with torch.no_grad():\n        voxel_size = 0.02\n\n        # Feed-forward pass and get the prediction\n        sinput = ME.SparseTensor(\n            feats=torch.from_numpy(colors).float(),\n            coords=ME.utils.batched_coordinates([coords / voxel_size]),\n            quantization_mode=ME.SparseTensorQuantizationMode.UNWEIGHTED_AVERAGE\n        ).to(device)\n        logits = model(sinput).slice(sinput)\n\n    _, pred = logits.max(1)\n    pred = pred.cpu().numpy()\n\n    # Create a point cloud file\n    pred_pcd = o3d.geometry.PointCloud()\n    # Map color\n    colors = np.array([SCANNET_COLOR_MAP[VALID_CLASS_IDS[l]] for l in pred])\n    pred_pcd.points = o3d.utility.Vector3dVector(coords)\n    pred_pcd.colors = o3d.utility.Vector3dVector(colors / 255)\n\n    # Move the original point cloud\n    pcd.points = o3d.utility.Vector3dVector(\n        np.array(pcd.points) + np.array([0, 5, 0]))\n\n    # Visualize the input point cloud and the prediction\n    o3d.visualization.draw_geometries([pcd, pred_pcd])\n'"
examples/minkunet.py,5,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport torch\nimport torch.nn as nn\nfrom torch.optim import SGD\n\nimport MinkowskiEngine as ME\n\nfrom MinkowskiEngine.modules.resnet_block import BasicBlock, Bottleneck\n\nfrom tests.common import data_loader\nfrom examples.resnet import ResNetBase\n\n\nclass MinkUNetBase(ResNetBase):\n    BLOCK = None\n    PLANES = None\n    DILATIONS = (1, 1, 1, 1, 1, 1, 1, 1)\n    LAYERS = (2, 2, 2, 2, 2, 2, 2, 2)\n    INIT_DIM = 32\n    OUT_TENSOR_STRIDE = 1\n\n    # To use the model, must call initialize_coords before forward pass.\n    # Once data is processed, call clear to reset the model before calling\n    # initialize_coords\n    def __init__(self, in_channels, out_channels, D=3):\n        ResNetBase.__init__(self, in_channels, out_channels, D)\n\n    def network_initialization(self, in_channels, out_channels, D):\n        # Output of the first conv concated to conv6\n        self.inplanes = self.INIT_DIM\n        self.conv0p1s1 = ME.MinkowskiConvolution(\n            in_channels, self.inplanes, kernel_size=5, dimension=D)\n\n        self.bn0 = ME.MinkowskiBatchNorm(self.inplanes)\n\n        self.conv1p1s2 = ME.MinkowskiConvolution(\n            self.inplanes, self.inplanes, kernel_size=2, stride=2, dimension=D)\n        self.bn1 = ME.MinkowskiBatchNorm(self.inplanes)\n\n        self.block1 = self._make_layer(self.BLOCK, self.PLANES[0],\n                                       self.LAYERS[0])\n\n        self.conv2p2s2 = ME.MinkowskiConvolution(\n            self.inplanes, self.inplanes, kernel_size=2, stride=2, dimension=D)\n        self.bn2 = ME.MinkowskiBatchNorm(self.inplanes)\n\n        self.block2 = self._make_layer(self.BLOCK, self.PLANES[1],\n                                       self.LAYERS[1])\n\n        self.conv3p4s2 = ME.MinkowskiConvolution(\n            self.inplanes, self.inplanes, kernel_size=2, stride=2, dimension=D)\n\n        self.bn3 = ME.MinkowskiBatchNorm(self.inplanes)\n        self.block3 = self._make_layer(self.BLOCK, self.PLANES[2],\n                                       self.LAYERS[2])\n\n        self.conv4p8s2 = ME.MinkowskiConvolution(\n            self.inplanes, self.inplanes, kernel_size=2, stride=2, dimension=D)\n        self.bn4 = ME.MinkowskiBatchNorm(self.inplanes)\n        self.block4 = self._make_layer(self.BLOCK, self.PLANES[3],\n                                       self.LAYERS[3])\n\n        self.convtr4p16s2 = ME.MinkowskiConvolutionTranspose(\n            self.inplanes, self.PLANES[4], kernel_size=2, stride=2, dimension=D)\n        self.bntr4 = ME.MinkowskiBatchNorm(self.PLANES[4])\n\n        self.inplanes = self.PLANES[4] + self.PLANES[2] * self.BLOCK.expansion\n        self.block5 = self._make_layer(self.BLOCK, self.PLANES[4],\n                                       self.LAYERS[4])\n        self.convtr5p8s2 = ME.MinkowskiConvolutionTranspose(\n            self.inplanes, self.PLANES[5], kernel_size=2, stride=2, dimension=D)\n        self.bntr5 = ME.MinkowskiBatchNorm(self.PLANES[5])\n\n        self.inplanes = self.PLANES[5] + self.PLANES[1] * self.BLOCK.expansion\n        self.block6 = self._make_layer(self.BLOCK, self.PLANES[5],\n                                       self.LAYERS[5])\n        self.convtr6p4s2 = ME.MinkowskiConvolutionTranspose(\n            self.inplanes, self.PLANES[6], kernel_size=2, stride=2, dimension=D)\n        self.bntr6 = ME.MinkowskiBatchNorm(self.PLANES[6])\n\n        self.inplanes = self.PLANES[6] + self.PLANES[0] * self.BLOCK.expansion\n        self.block7 = self._make_layer(self.BLOCK, self.PLANES[6],\n                                       self.LAYERS[6])\n        self.convtr7p2s2 = ME.MinkowskiConvolutionTranspose(\n            self.inplanes, self.PLANES[7], kernel_size=2, stride=2, dimension=D)\n        self.bntr7 = ME.MinkowskiBatchNorm(self.PLANES[7])\n\n        self.inplanes = self.PLANES[7] + self.INIT_DIM\n        self.block8 = self._make_layer(self.BLOCK, self.PLANES[7],\n                                       self.LAYERS[7])\n\n        self.final = ME.MinkowskiConvolution(\n            self.PLANES[7],\n            out_channels,\n            kernel_size=1,\n            has_bias=True,\n            dimension=D)\n        self.relu = ME.MinkowskiReLU(inplace=True)\n\n    def forward(self, x):\n        out = self.conv0p1s1(x)\n        out = self.bn0(out)\n        out_p1 = self.relu(out)\n\n        out = self.conv1p1s2(out_p1)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out_b1p2 = self.block1(out)\n\n        out = self.conv2p2s2(out_b1p2)\n        out = self.bn2(out)\n        out = self.relu(out)\n        out_b2p4 = self.block2(out)\n\n        out = self.conv3p4s2(out_b2p4)\n        out = self.bn3(out)\n        out = self.relu(out)\n        out_b3p8 = self.block3(out)\n\n        # tensor_stride=16\n        out = self.conv4p8s2(out_b3p8)\n        out = self.bn4(out)\n        out = self.relu(out)\n        out = self.block4(out)\n\n        # tensor_stride=8\n        out = self.convtr4p16s2(out)\n        out = self.bntr4(out)\n        out = self.relu(out)\n\n        out = ME.cat(out, out_b3p8)\n        out = self.block5(out)\n\n        # tensor_stride=4\n        out = self.convtr5p8s2(out)\n        out = self.bntr5(out)\n        out = self.relu(out)\n\n        out = ME.cat(out, out_b2p4)\n        out = self.block6(out)\n\n        # tensor_stride=2\n        out = self.convtr6p4s2(out)\n        out = self.bntr6(out)\n        out = self.relu(out)\n\n        out = ME.cat(out, out_b1p2)\n        out = self.block7(out)\n\n        # tensor_stride=1\n        out = self.convtr7p2s2(out)\n        out = self.bntr7(out)\n        out = self.relu(out)\n\n        out = ME.cat(out, out_p1)\n        out = self.block8(out)\n\n        return self.final(out)\n\n\nclass MinkUNet14(MinkUNetBase):\n    BLOCK = BasicBlock\n    LAYERS = (1, 1, 1, 1, 1, 1, 1, 1)\n\n\nclass MinkUNet18(MinkUNetBase):\n    BLOCK = BasicBlock\n    LAYERS = (2, 2, 2, 2, 2, 2, 2, 2)\n\n\nclass MinkUNet34(MinkUNetBase):\n    BLOCK = BasicBlock\n    LAYERS = (2, 3, 4, 6, 2, 2, 2, 2)\n\n\nclass MinkUNet50(MinkUNetBase):\n    BLOCK = Bottleneck\n    LAYERS = (2, 3, 4, 6, 2, 2, 2, 2)\n\n\nclass MinkUNet101(MinkUNetBase):\n    BLOCK = Bottleneck\n    LAYERS = (2, 3, 4, 23, 2, 2, 2, 2)\n\n\nclass MinkUNet14A(MinkUNet14):\n    PLANES = (32, 64, 128, 256, 128, 128, 96, 96)\n\n\nclass MinkUNet14B(MinkUNet14):\n    PLANES = (32, 64, 128, 256, 128, 128, 128, 128)\n\n\nclass MinkUNet14C(MinkUNet14):\n    PLANES = (32, 64, 128, 256, 192, 192, 128, 128)\n\n\nclass MinkUNet14D(MinkUNet14):\n    PLANES = (32, 64, 128, 256, 384, 384, 384, 384)\n\n\nclass MinkUNet18A(MinkUNet18):\n    PLANES = (32, 64, 128, 256, 128, 128, 96, 96)\n\n\nclass MinkUNet18B(MinkUNet18):\n    PLANES = (32, 64, 128, 256, 128, 128, 128, 128)\n\n\nclass MinkUNet18D(MinkUNet18):\n    PLANES = (32, 64, 128, 256, 384, 384, 384, 384)\n\n\nclass MinkUNet34A(MinkUNet34):\n    PLANES = (32, 64, 128, 256, 256, 128, 64, 64)\n\n\nclass MinkUNet34B(MinkUNet34):\n    PLANES = (32, 64, 128, 256, 256, 128, 64, 32)\n\n\nclass MinkUNet34C(MinkUNet34):\n    PLANES = (32, 64, 128, 256, 256, 128, 96, 96)\n\n\nif __name__ == \'__main__\':\n    # loss and network\n    criterion = nn.CrossEntropyLoss()\n    net = MinkUNet14A(in_channels=3, out_channels=5, D=2)\n    print(net)\n\n    # a data loader must return a tuple of coords, features, and labels.\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n    net = net.to(device)\n    optimizer = SGD(net.parameters(), lr=1e-2)\n\n    for i in range(10):\n        optimizer.zero_grad()\n\n        # Get new data\n        coords, feat, label = data_loader(is_classification=False)\n        input = ME.SparseTensor(feat, coords=coords).to(device)\n        label = label.to(device)\n\n        # Forward\n        output = net(input)\n\n        # Loss\n        loss = criterion(output.F, label)\n        print(\'Iteration: \', i, \', Loss: \', loss.item())\n\n        # Gradient\n        loss.backward()\n        optimizer.step()\n\n    # Saving and loading a network\n    torch.save(net.state_dict(), \'test.pth\')\n    net.load_state_dict(torch.load(\'test.pth\'))\n'"
examples/modelnet40.py,13,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport os\nimport sys\nimport subprocess\nimport argparse\nimport logging\nfrom time import time\n# Must be imported before\ntry:\n    import open3d as o3d\nexcept ImportError:\n    raise ImportError(\'Please install open3d and scipy with `pip install open3d scipy`.\')\n\nimport torch\nimport torch.utils.data\nfrom torch.utils.data.sampler import Sampler\nimport torch.optim as optim\nfrom torchvision.transforms import Compose as VisionCompose\n\nimport numpy as np\nfrom scipy.linalg import expm, norm\n\nimport MinkowskiEngine as ME\nfrom examples.resnet import ResNet50\n\nassert int(\n    o3d.__version__.split(\'.\')[1]\n) >= 8, f\'Requires open3d version >= 0.8, the current version is {o3d.__version__}\'\n\nch = logging.StreamHandler(sys.stdout)\nlogging.getLogger().setLevel(logging.INFO)\nlogging.basicConfig(\n    format=os.uname()[1].split(\'.\')[0] + \' %(asctime)s %(message)s\',\n    datefmt=\'%m/%d %H:%M:%S\',\n    handlers=[ch])\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--voxel_size\', type=float, default=0.05)\nparser.add_argument(\'--max_iter\', type=int, default=120000)\nparser.add_argument(\'--val_freq\', type=int, default=1000)\nparser.add_argument(\'--batch_size\', default=256, type=int)\nparser.add_argument(\'--lr\', default=1e-2, type=float)\nparser.add_argument(\'--momentum\', type=float, default=0.9)\nparser.add_argument(\'--weight_decay\', type=float, default=1e-4)\nparser.add_argument(\'--num_workers\', type=int, default=1)\nparser.add_argument(\'--stat_freq\', type=int, default=50)\nparser.add_argument(\'--weights\', type=str, default=\'modelnet.pth\')\nparser.add_argument(\'--load_optimizer\', type=str, default=\'true\')\n\nif not os.path.exists(\'ModelNet40\'):\n    logging.info(\'Downloading the fixed ModelNet40 dataset...\')\n    subprocess.run([""sh"", ""./examples/download_modelnet40.sh""])\n\n\nclass InfSampler(Sampler):\n    """"""Samples elements randomly, without replacement.\n\n    Arguments:\n        data_source (Dataset): dataset to sample from\n    """"""\n\n    def __init__(self, data_source, shuffle=False):\n        self.data_source = data_source\n        self.shuffle = shuffle\n        self.reset_permutation()\n\n    def reset_permutation(self):\n        perm = len(self.data_source)\n        if self.shuffle:\n            perm = torch.randperm(perm)\n        self._perm = perm.tolist()\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if len(self._perm) == 0:\n            self.reset_permutation()\n        return self._perm.pop()\n\n    def __len__(self):\n        return len(self.data_source)\n\n\ndef resample_mesh(mesh_cad, density=1):\n    \'\'\'\n    https://chrischoy.github.io/research/barycentric-coordinate-for-mesh-sampling/\n    Samples point cloud on the surface of the model defined as vectices and\n    faces. This function uses vectorized operations so fast at the cost of some\n    memory.\n\n    param mesh_cad: low-polygon triangle mesh in o3d.geometry.TriangleMesh\n    param density: density of the point cloud per unit area\n    param return_numpy: return numpy format or open3d pointcloud format\n    return resampled point cloud\n\n    Reference :\n      [1] Barycentric coordinate system\n      \\begin{align}\n        P = (1 - \\sqrt{r_1})A + \\sqrt{r_1} (1 - r_2) B + \\sqrt{r_1} r_2 C\n      \\end{align}\n    \'\'\'\n    faces = np.array(mesh_cad.triangles).astype(int)\n    vertices = np.array(mesh_cad.vertices)\n\n    vec_cross = np.cross(vertices[faces[:, 0], :] - vertices[faces[:, 2], :],\n                         vertices[faces[:, 1], :] - vertices[faces[:, 2], :])\n    face_areas = np.sqrt(np.sum(vec_cross**2, 1))\n\n    n_samples = (np.sum(face_areas) * density).astype(int)\n    # face_areas = face_areas / np.sum(face_areas)\n\n    # Sample exactly n_samples. First, oversample points and remove redundant\n    # Bug fix by Yangyan (yangyan.lee@gmail.com)\n    n_samples_per_face = np.ceil(density * face_areas).astype(int)\n    floor_num = np.sum(n_samples_per_face) - n_samples\n    if floor_num > 0:\n        indices = np.where(n_samples_per_face > 0)[0]\n        floor_indices = np.random.choice(indices, floor_num, replace=True)\n        n_samples_per_face[floor_indices] -= 1\n\n    n_samples = np.sum(n_samples_per_face)\n\n    # Create a vector that contains the face indices\n    sample_face_idx = np.zeros((n_samples,), dtype=int)\n    acc = 0\n    for face_idx, _n_sample in enumerate(n_samples_per_face):\n        sample_face_idx[acc:acc + _n_sample] = face_idx\n        acc += _n_sample\n\n    r = np.random.rand(n_samples, 2)\n    A = vertices[faces[sample_face_idx, 0], :]\n    B = vertices[faces[sample_face_idx, 1], :]\n    C = vertices[faces[sample_face_idx, 2], :]\n\n    P = (1 - np.sqrt(r[:, 0:1])) * A + \\\n        np.sqrt(r[:, 0:1]) * (1 - r[:, 1:]) * B + \\\n        np.sqrt(r[:, 0:1]) * r[:, 1:] * C\n\n    return P\n\n\ndef collate_pointcloud_fn(list_data):\n    new_list_data = []\n    num_removed = 0\n    for data in list_data:\n        if data is not None:\n            new_list_data.append(data)\n        else:\n            num_removed += 1\n\n    list_data = new_list_data\n\n    if len(list_data) == 0:\n        raise ValueError(\'No data in the batch\')\n\n    coords, feats, labels = list(zip(*list_data))\n\n    eff_num_batch = len(coords)\n    assert len(labels) == eff_num_batch\n\n    coords_batch = ME.utils.batched_coordinates(coords)\n    feats_batch = torch.from_numpy(np.vstack(feats)).float()\n\n    # Concatenate all lists\n    return {\n        \'coords\': coords_batch,\n        \'feats\': feats_batch,\n        \'labels\': torch.LongTensor(labels),\n    }\n\n\nclass Compose(VisionCompose):\n\n    def __call__(self, *args):\n        for t in self.transforms:\n            args = t(*args)\n        return args\n\n\nclass RandomRotation:\n\n    def __init__(self, axis=None, max_theta=180):\n        self.axis = axis\n        self.max_theta = max_theta\n\n    def _M(self, axis, theta):\n        return expm(np.cross(np.eye(3), axis / norm(axis) * theta))\n\n    def __call__(self, coords, feats):\n        if self.axis is not None:\n            axis = self.axis\n        else:\n            axis = np.random.rand(3) - 0.5\n        R = self._M(axis, (np.pi * self.max_theta / 180) * 2 *\n                    (np.random.rand(1) - 0.5))\n        R_n = self._M(\n            np.random.rand(3) - 0.5,\n            (np.pi * 15 / 180) * 2 * (np.random.rand(1) - 0.5))\n        return coords @ R @ R_n, feats\n\n\nclass RandomScale:\n\n    def __init__(self, min, max):\n        self.scale = max - min\n        self.bias = min\n\n    def __call__(self, coords, feats):\n        s = self.scale * np.random.rand(1) + self.bias\n        return coords * s, feats\n\n\nclass RandomShear:\n\n    def __call__(self, coords, feats):\n        T = np.eye(3) + 0.1 * np.random.randn(3, 3)\n        return coords @ T, feats\n\n\nclass RandomTranslation:\n\n    def __call__(self, coords, feats):\n        trans = 0.05 * np.random.randn(1, 3)\n        return coords + trans, feats\n\n\nclass ModelNet40Dataset(torch.utils.data.Dataset):\n    AUGMENT = None\n    DATA_FILES = {\n        \'train\': \'train_modelnet40.txt\',\n        \'val\': \'val_modelnet40.txt\',\n        \'test\': \'test_modelnet40.txt\'\n    }\n\n    CATEGORIES = [\n        \'airplane\', \'bathtub\', \'bed\', \'bench\', \'bookshelf\', \'bottle\', \'bowl\',\n        \'car\', \'chair\', \'cone\', \'cup\', \'curtain\', \'desk\', \'door\', \'dresser\',\n        \'flower_pot\', \'glass_box\', \'guitar\', \'keyboard\', \'lamp\', \'laptop\',\n        \'mantel\', \'monitor\', \'night_stand\', \'person\', \'piano\', \'plant\', \'radio\',\n        \'range_hood\', \'sink\', \'sofa\', \'stairs\', \'stool\', \'table\', \'tent\',\n        \'toilet\', \'tv_stand\', \'vase\', \'wardrobe\', \'xbox\'\n    ]\n\n    def __init__(self, phase, transform=None, config=None):\n        self.phase = phase\n        self.files = []\n        self.cache = {}\n        self.data_objects = []\n        self.transform = transform\n        self.voxel_size = config.voxel_size\n        self.last_cache_percent = 0\n\n        self.root = \'./ModelNet40\'\n        self.files = open(os.path.join(self.root,\n                                       self.DATA_FILES[phase])).read().split()\n        logging.info(\n            f""Loading the subset {phase} from {self.root} with {len(self.files)} files""\n        )\n        self.density = 4000\n\n        # Ignore warnings in obj loader\n        o3d.utility.set_verbosity_level(o3d.utility.VerbosityLevel.Error)\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, idx):\n        mesh_file = os.path.join(self.root, self.files[idx])\n        category = self.files[idx].split(\'/\')[0]\n        label = self.CATEGORIES.index(category)\n        if idx in self.cache:\n            xyz = self.cache[idx]\n        else:\n            # Load a mesh, over sample, copy, rotate, voxelization\n            assert os.path.exists(mesh_file)\n            pcd = o3d.io.read_triangle_mesh(mesh_file)\n            # Normalize to fit the mesh inside a unit cube while preserving aspect ratio\n            vertices = np.asarray(pcd.vertices)\n            vmax = vertices.max(0, keepdims=True)\n            vmin = vertices.min(0, keepdims=True)\n            pcd.vertices = o3d.utility.Vector3dVector((vertices - vmin) /\n                                                      (vmax - vmin).max() + 0.5)\n\n            # Oversample points and copy\n            xyz = resample_mesh(pcd, density=self.density)\n            self.cache[idx] = xyz\n            cache_percent = int((len(self.cache) / len(self)) * 100)\n            if cache_percent > 0 and cache_percent % 10 == 0 and cache_percent != self.last_cache_percent:\n                logging.info(\n                    f""Cached {self.phase}: {len(self.cache)} / {len(self)}: {cache_percent}%""\n                )\n                self.last_cache_percent = cache_percent\n\n        # Use color or other features if available\n        feats = np.ones((len(xyz), 1))\n\n        if len(xyz) < 1000:\n            logging.info(\n                f""Skipping {mesh_file}: does not have sufficient CAD sampling density after resampling: {len(xyz)}.""\n            )\n            return None\n\n        if self.transform:\n            xyz, feats = self.transform(xyz, feats)\n\n        # Get coords\n        coords = np.floor(xyz / self.voxel_size)\n\n        return (coords, xyz, label)\n\n\ndef make_data_loader(phase, augment_data, batch_size, shuffle, num_workers,\n                     repeat, config):\n    transformations = []\n    if augment_data:\n        transformations.append(RandomRotation(axis=np.array([0, 0, 1])))\n        transformations.append(RandomTranslation())\n        transformations.append(RandomScale(0.8, 1.2))\n        transformations.append(RandomShear())\n\n    dset = ModelNet40Dataset(\n        phase, transform=Compose(transformations), config=config)\n\n    args = {\n        \'batch_size\': batch_size,\n        \'num_workers\': num_workers,\n        \'collate_fn\': collate_pointcloud_fn,\n        \'pin_memory\': False,\n        \'drop_last\': False\n    }\n\n    if repeat:\n        args[\'sampler\'] = InfSampler(dset, shuffle)\n    else:\n        args[\'shuffle\'] = shuffle\n\n    loader = torch.utils.data.DataLoader(dset, **args)\n\n    return loader\n\n\ndef test(net, test_iter, config, phase=\'val\'):\n    net.eval()\n    num_correct, tot_num = 0, 0\n    for i in range(len(test_iter)):\n        data_dict = test_iter.next()\n        sin = ME.SparseTensor(\n            data_dict[\'coords\'][:, :3] * config.voxel_size,\n            data_dict[\'coords\'].int(),\n            allow_duplicate_coords=True,  # for classification, it doesn\'t matter\n        ).to(device)\n        sout = net(sin)\n        is_correct = data_dict[\'labels\'] == torch.argmax(sout.F, 1).cpu()\n        num_correct += is_correct.sum().item()\n        tot_num += len(sout)\n\n        if i % config.stat_freq == 0:\n            logging.info(\n                f\'{phase} set iter: {i} / {len(test_iter)}, Accuracy : {num_correct / tot_num:.3e}\'\n            )\n    logging.info(f\'{phase} set accuracy : {num_correct / tot_num:.3e}\')\n\n\ndef train(net, device, config):\n    optimizer = optim.SGD(\n        net.parameters(),\n        lr=config.lr,\n        momentum=config.momentum,\n        weight_decay=config.weight_decay)\n    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.95)\n\n    crit = torch.nn.CrossEntropyLoss()\n\n    train_dataloader = make_data_loader(\n        \'train\',\n        augment_data=True,\n        batch_size=config.batch_size,\n        shuffle=True,\n        num_workers=config.num_workers,\n        repeat=True,\n        config=config)\n    val_dataloader = make_data_loader(\n        \'val\',\n        augment_data=False,\n        batch_size=config.batch_size,\n        shuffle=True,\n        num_workers=config.num_workers,\n        repeat=True,\n        config=config)\n\n    curr_iter = 0\n    if os.path.exists(config.weights):\n        checkpoint = torch.load(config.weights)\n        net.load_state_dict(checkpoint[\'state_dict\'])\n        if config.load_optimizer.lower() == \'true\':\n            curr_iter = checkpoint[\'curr_iter\'] + 1\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n            scheduler.load_state_dict(checkpoint[\'scheduler\'])\n\n    net.train()\n    train_iter = iter(train_dataloader)\n    val_iter = iter(val_dataloader)\n    logging.info(f\'LR: {scheduler.get_lr()}\')\n    for i in range(curr_iter, config.max_iter):\n\n        s = time()\n        data_dict = train_iter.next()\n        d = time() - s\n\n        optimizer.zero_grad()\n        sin = ME.SparseTensor(\n            data_dict[\'coords\'][:, :3] * config.voxel_size,\n            data_dict[\'coords\'].int(),\n            allow_duplicate_coords=True,  # for classification, it doesn\'t matter\n        ).to(device)\n        sout = net(sin)\n        loss = crit(sout.F, data_dict[\'labels\'].to(device))\n        loss.backward()\n        optimizer.step()\n        t = time() - s\n\n        if i % config.stat_freq == 0:\n            logging.info(\n                f\'Iter: {i}, Loss: {loss.item():.3e}, Data Loading Time: {d:.3e}, Tot Time: {t:.3e}\'\n            )\n\n        if i % config.val_freq == 0 and i > 0:\n            torch.save(\n                {\n                    \'state_dict\': net.state_dict(),\n                    \'optimizer\': optimizer.state_dict(),\n                    \'scheduler\': scheduler.state_dict(),\n                    \'curr_iter\': i,\n                }, config.weights)\n\n            # Validation\n            logging.info(\'Validation\')\n            test(net, val_iter, config, \'val\')\n\n            scheduler.step()\n            logging.info(f\'LR: {scheduler.get_lr()}\')\n\n            net.train()\n\n\nif __name__ == \'__main__\':\n    print(\'Warning: This process will cache the entire voxelized ModelNet40 dataset, which will take up ~10G of memory.\')\n\n    config = parser.parse_args()\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n    net = ResNet50(3, 40, D=3)\n    net.to(device)\n\n    train(net, device, config)\n\n    test_dataloader = make_data_loader(\n        \'test\',\n        augment_data=False,\n        batch_size=config.batch_size,\n        shuffle=True,\n        num_workers=config.num_workers,\n        repeat=False,\n        config=config)\n\n    logging.info(\'Test\')\n    test(net, iter(test_dataloader), config, \'test\')\n'"
examples/multigpu.py,6,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport os\nimport argparse\nimport numpy as np\nfrom time import time\nfrom urllib.request import urlretrieve\ntry:\n    import open3d as o3d\nexcept ImportError:\n    raise ImportError(\'Please install open3d-python with `pip install open3d`.\')\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import SGD\n\nimport MinkowskiEngine as ME\nfrom examples.minkunet import MinkUNet34C\n\nimport torch.nn.parallel as parallel\n\nif not os.path.isfile(\'weights.pth\'):\n    urlretrieve(""http://cvgl.stanford.edu/data2/minkowskiengine/1.ply"", \'1.ply\')\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--file_name\', type=str, default=\'1.ply\')\nparser.add_argument(\'--batch_size\', type=int, default=4)\nparser.add_argument(\'--max_ngpu\', type=int, default=2)\n\ncache = {}\n\n\ndef load_file(file_name, voxel_size):\n    if file_name not in cache:\n        pcd = o3d.io.read_point_cloud(file_name)\n        cache[file_name] = pcd\n\n    pcd = cache[file_name]\n    coords = np.array(pcd.points)\n    feats = np.array(pcd.colors)\n\n    quantized_coords = np.floor(coords / voxel_size)\n    inds = ME.utils.sparse_quantize(quantized_coords, return_index=True)\n    random_labels = torch.zeros(len(inds))\n\n    return quantized_coords[inds], feats[inds], random_labels\n\n\ndef generate_input(file_name, voxel_size):\n    # Create a batch, this process is done in a data loader during training in parallel.\n    batch = [load_file(file_name, voxel_size)]\n    coordinates_, featrues_, labels_ = list(zip(*batch))\n    coordinates, features, labels = ME.utils.sparse_collate(\n        coordinates_, featrues_, labels_)\n\n    # Normalize features and create a sparse tensor\n    return coordinates, (features - 0.5).float(), labels\n\n\nif __name__ == \'__main__\':\n    # loss and network\n    config = parser.parse_args()\n    num_devices = torch.cuda.device_count()\n    num_devices = min(config.max_ngpu, num_devices)\n    devices = list(range(num_devices))\n    print(\'Testing \', num_devices, \' GPUs. Total batch size: \',\n          num_devices * config.batch_size)\n\n    # For copying the final loss back to one GPU\n    target_device = devices[0]\n\n    # Copy the network to GPU\n    net = MinkUNet34C(3, 20, D=3)\n    net = net.to(target_device)\n\n    # Synchronized batch norm\n    net = ME.MinkowskiSyncBatchNorm.convert_sync_batchnorm(net)\n    optimizer = SGD(net.parameters(), lr=1e-1)\n\n    # Copy the loss layer\n    criterion = nn.CrossEntropyLoss()\n    criterions = parallel.replicate(criterion, devices)\n    min_time = np.inf\n\n    for iteration in range(10):\n        optimizer.zero_grad()\n\n        # Get new data\n        inputs, all_labels = [], []\n        for i in range(num_devices):\n            coordinates, features, labels = generate_input(\n                config.file_name, voxel_size=0.05)\n            with torch.cuda.device(devices[i]):\n                inputs.append(\n                    ME.SparseTensor(features - 0.5,\n                                    coords=coordinates).to(devices[i]))\n            all_labels.append(labels.long().to(devices[i]))\n\n        # The raw version of the parallel_apply\n        st = time()\n        replicas = parallel.replicate(net, devices)\n        outputs = parallel.parallel_apply(replicas, inputs, devices=devices)\n\n        # Extract features from the sparse tensors to use a pytorch criterion\n        out_features = [output.F for output in outputs]\n        losses = parallel.parallel_apply(\n            criterions, tuple(zip(out_features, all_labels)), devices=devices)\n        loss = parallel.gather(losses, target_device, dim=0).mean()\n        t = time() - st\n        min_time = min(t, min_time)\n        print(\'Iteration: \', iteration, \', Loss: \', loss.item(), \', Time: \', t,\n              \', Min time: \', min_time)\n\n        # Gradient\n        loss.backward()\n        optimizer.step()\n'"
examples/pointnet.py,5,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport os\nimport numpy as np\nfrom urllib.request import urlretrieve\ntry:\n    import open3d as o3d\nexcept ImportError:\n    raise ImportError(\'Please install open3d with `pip install open3d`.\')\n\nimport torch\nimport torch.nn as nn\nimport MinkowskiEngine as ME\n\n\nclass STN3d(nn.Module):\n    r""""""Given a sparse tensor, generate a 3x3 transformation matrix per\n    instance.\n    """"""\n    CONV_CHANNELS = [64, 128, 1024, 512, 256]\n    FC_CHANNELS = [512, 256]\n    KERNEL_SIZES = [1, 1, 1]\n    STRIDES = [1, 1, 1]\n\n    def __init__(self, D=3):\n        super(STN3d, self).__init__()\n\n        k = self.KERNEL_SIZES\n        s = self.STRIDES\n        c = self.CONV_CHANNELS\n\n        self.block1 = nn.Sequential(\n            ME.MinkowskiConvolution(\n                3,\n                c[0],\n                kernel_size=k[0],\n                stride=s[0],\n                has_bias=False,\n                dimension=3), ME.MinkowskiInstanceNorm(c[0]),\n            ME.MinkowskiReLU())\n        self.block2 = nn.Sequential(\n            ME.MinkowskiConvolution(\n                c[0],\n                c[1],\n                kernel_size=k[1],\n                stride=s[1],\n                has_bias=False,\n                dimension=3), ME.MinkowskiInstanceNorm(c[1]),\n            ME.MinkowskiReLU())\n        self.block3 = nn.Sequential(\n            ME.MinkowskiConvolution(\n                c[1],\n                c[2],\n                kernel_size=k[2],\n                stride=s[2],\n                has_bias=False,\n                dimension=3), ME.MinkowskiInstanceNorm(c[2]),\n            ME.MinkowskiReLU())\n\n        # Use the kernelsize 1 convolution for linear layers. If kernel size ==\n        # 1, minkowski engine internally uses a linear function.\n        self.block4 = nn.Sequential(\n            ME.MinkowskiConvolution(\n                c[2], c[3], kernel_size=1, has_bias=False, dimension=3),\n            ME.MinkowskiInstanceNorm(c[3]), ME.MinkowskiReLU())\n        self.block5 = nn.Sequential(\n            ME.MinkowskiConvolution(\n                c[3], c[4], kernel_size=1, has_bias=False, dimension=3),\n            ME.MinkowskiInstanceNorm(c[4]), ME.MinkowskiReLU())\n        self.fc6 = ME.MinkowskiConvolution(\n            c[4], 9, kernel_size=1, has_bias=True, dimension=3)\n\n        self.avgpool = ME.MinkowskiGlobalPooling()\n        self.broadcast = ME.MinkowskiBroadcast()\n\n    def forward(self, in_x):\n        x = self.block1(in_x)\n        x = self.block2(x)\n        x = self.block3(x)\n\n        # batch size x channel\n        x = self.avgpool(x)\n\n        x = self.block4(x)\n        x = self.block5(x)\n\n        # get the features batch-wise\n        x = self.fc6(x)\n\n        # Add identity transformation\n        x._F += torch.tensor([[1, 0, 0, 0, 1, 0, 0, 0, 1]],\n                             dtype=x.dtype,\n                             device=x.device).repeat(len(x), 1)\n        # Broadcast the transformation back to the right coordinates of x\n        return self.broadcast(in_x, x)\n\n\nclass PointNetFeature(nn.Module):\n    r""""""\n    You can think of a PointNet as a specialization of a convolutional neural\n    network with kernel_size == 1, and stride == 1 that processes a sparse\n    tensor where features are normalized coordinates.\n\n    This generalization allows the network to process an arbitrary number of\n    points.\n    """"""\n    CONV_CHANNELS = [256, 512, 1024]\n    KERNEL_SIZES = [1, 1, 1]\n    STRIDES = [1, 1, 1]\n\n    def __init__(self):\n        super(PointNetFeature, self).__init__()\n\n        k = self.KERNEL_SIZES\n        s = self.STRIDES\n        c = self.CONV_CHANNELS\n\n        self.stn = STN3d(D=3)\n        self.block1 = nn.Sequential(\n            ME.MinkowskiConvolution(\n                6,\n                c[0],\n                kernel_size=k[0],\n                stride=s[0],\n                has_bias=False,\n                dimension=3), ME.MinkowskiInstanceNorm(c[0]),\n            ME.MinkowskiReLU())\n        self.block2 = nn.Sequential(\n            ME.MinkowskiConvolution(\n                c[0],\n                c[1],\n                kernel_size=k[1],\n                stride=s[1],\n                has_bias=False,\n                dimension=3), ME.MinkowskiInstanceNorm(c[1]),\n            ME.MinkowskiReLU())\n        self.block3 = nn.Sequential(\n            ME.MinkowskiConvolution(\n                c[1],\n                c[2],\n                kernel_size=k[2],\n                stride=s[2],\n                has_bias=False,\n                dimension=3), ME.MinkowskiInstanceNorm(c[2]),\n            ME.MinkowskiReLU())\n\n        self.avgpool = ME.MinkowskiGlobalPooling()\n        self.concat = ME.MinkowskiBroadcastConcatenation()\n\n    def forward(self, x):\n        """"""\n        Input is a spare tensor with features as centered coordinates N x 3\n        """"""\n        assert isinstance(x, ME.SparseTensor)\n        assert x.F.shape[1] == 3\n\n        # Get the transformation\n        T = self.stn(x)\n\n        # Apply the transformation\n        coords_feat_stn = torch.squeeze(\n            torch.bmm(x.F.view(-1, 1, 3), T.F.view(-1, 3, 3)))\n        x = ME.SparseTensor(\n            torch.cat((coords_feat_stn, x.F), 1),\n            coords_key=x.coords_key,\n            coords_manager=x.coords_man)\n\n        point_feat = self.block1(x)\n        x = self.block2(point_feat)\n        x = self.block3(x)\n        glob_feat = self.avgpool(x)\n        return self.concat(point_feat, glob_feat)\n\n\nclass PointNet(nn.Module):\n    r""""""\n    You can think of a PointNet as a specialization of a convolutional neural\n    network with kernel_size == 1, and stride == 1 that processes a sparse\n    tensor where features are normalized coordinates.\n\n    This generalization allows the network to process an arbitrary number of\n    points.\n    """"""\n    CONV_CHANNELS = [512, 256, 128]\n    KERNEL_SIZES = [1, 1, 1]\n    STRIDES = [1, 1, 1]\n\n    def __init__(self, out_channels, D=3):\n        super(PointNet, self).__init__()\n        k = self.KERNEL_SIZES\n        s = self.STRIDES\n        c = self.CONV_CHANNELS\n\n        self.feat = PointNetFeature()\n        self.block1 = nn.Sequential(\n            ME.MinkowskiConvolution(\n                1280,\n                c[0],\n                kernel_size=k[0],\n                stride=s[0],\n                has_bias=False,\n                dimension=3), ME.MinkowskiInstanceNorm(c[0]),\n            ME.MinkowskiReLU())\n        self.block2 = nn.Sequential(\n            ME.MinkowskiConvolution(\n                c[0],\n                c[1],\n                kernel_size=k[1],\n                stride=s[1],\n                has_bias=False,\n                dimension=3), ME.MinkowskiInstanceNorm(c[1]),\n            ME.MinkowskiReLU())\n        self.block3 = nn.Sequential(\n            ME.MinkowskiConvolution(\n                c[1],\n                c[2],\n                kernel_size=k[2],\n                stride=s[2],\n                has_bias=False,\n                dimension=3), ME.MinkowskiInstanceNorm(c[2]),\n            ME.MinkowskiReLU())\n\n        # Last FC layer. Note that kernel_size 1 == linear layer\n        self.conv4 = ME.MinkowskiConvolution(\n            c[2], out_channels, kernel_size=1, has_bias=True, dimension=3)\n\n    def forward(self, x):\n        """"""\n        Assume that x.F (features) are normalized coordinates or centered coordinates\n        """"""\n        assert isinstance(x, ME.SparseTensor)\n        x = self.feat(x)\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        return self.conv4(x)\n\n\nbunny_file = ""bunny.ply""\nif not os.path.isfile(bunny_file):\n    urlretrieve(\n        ""https://raw.githubusercontent.com/naucoin/VTKData/master/Data/bunny.ply"",\n        bunny_file)\n\nif __name__ == \'__main__\':\n    voxel_size = 2e-3  # High resolution grid works better just like high-res image is better for 2D classification\n    pointnet = PointNet(20).float()\n\n    pcd = o3d.io.read_point_cloud(bunny_file)\n\n    # If you need a high-resolution point cloud, sample points using\n    # https://chrischoy.github.io/research/barycentric-coordinate-for-mesh-sampling/\n\n    # Convert to a voxel grid\n    coords = np.array(pcd.points)\n    feats = coords - coords.mean(0)  # Coordinates are features for pointnet\n    quantized_coords = np.floor(coords / voxel_size)\n    inds = ME.utils.sparse_quantize(quantized_coords, return_index=True)\n    quantized_coords, feats = ME.utils.sparse_collate([quantized_coords[inds]],\n                                                      [feats[inds]])\n    sinput = ME.SparseTensor(feats.float(), quantized_coords)\n\n    print(pointnet(sinput))\n'"
examples/reconstruction.py,20,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport os\nimport sys\nimport subprocess\nimport argparse\nimport logging\nimport glob\nimport numpy as np\nfrom time import time\nimport urllib\n# Must be imported before large libs\ntry:\n    import open3d as o3d\nexcept ImportError:\n    raise ImportError(\'Please install open3d and scipy with `pip install open3d scipy`.\')\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.optim as optim\n\nimport MinkowskiEngine as ME\n\nfrom examples.modelnet40 import InfSampler, resample_mesh\n\nM = np.array([[0.80656762, -0.5868724, -0.07091862],\n              [0.3770505, 0.418344, 0.82632997],\n              [-0.45528188, -0.6932309, 0.55870326]])\n\nassert int(\n    o3d.__version__.split(\'.\')[1]\n) >= 8, f\'Requires open3d version >= 0.8, the current version is {o3d.__version__}\'\n\nif not os.path.exists(\'ModelNet40\'):\n    logging.info(\'Downloading the fixed ModelNet40 dataset...\')\n    subprocess.run([""sh"", ""./examples/download_modelnet40.sh""])\n\n\n###############################################################################\n# Utility functions\n###############################################################################\ndef PointCloud(points, colors=None):\n\n    pcd = o3d.geometry.PointCloud()\n    pcd.points = o3d.utility.Vector3dVector(points)\n    if colors is not None:\n        pcd.colors = o3d.utility.Vector3dVector(colors)\n    return pcd\n\n\ndef collate_pointcloud_fn(list_data):\n    coords, feats, labels = list(zip(*list_data))\n\n    # Concatenate all lists\n    return {\n        \'coords\': coords,\n        \'xyzs\': [torch.from_numpy(feat).float() for feat in feats],\n        \'labels\': torch.LongTensor(labels),\n    }\n\n\nclass ModelNet40Dataset(torch.utils.data.Dataset):\n\n    def __init__(self, phase, transform=None, config=None):\n        self.phase = phase\n        self.files = []\n        self.cache = {}\n        self.data_objects = []\n        self.transform = transform\n        self.resolution = config.resolution\n        self.last_cache_percent = 0\n\n        self.root = \'./ModelNet40\'\n        fnames = glob.glob(os.path.join(self.root, \'chair/train/*.off\'))\n        fnames = sorted([os.path.relpath(fname, self.root) for fname in fnames])\n        self.files = fnames\n        assert len(self.files) > 0, ""No file loaded""\n        logging.info(\n            f""Loading the subset {phase} from {self.root} with {len(self.files)} files""\n        )\n        self.density = 30000\n\n        # Ignore warnings in obj loader\n        o3d.utility.set_verbosity_level(o3d.utility.VerbosityLevel.Error)\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, idx):\n        mesh_file = os.path.join(self.root, self.files[idx])\n        if idx in self.cache:\n            xyz = self.cache[idx]\n        else:\n            # Load a mesh, over sample, copy, rotate, voxelization\n            assert os.path.exists(mesh_file)\n            pcd = o3d.io.read_triangle_mesh(mesh_file)\n            # Normalize to fit the mesh inside a unit cube while preserving aspect ratio\n            vertices = np.asarray(pcd.vertices)\n            vmax = vertices.max(0, keepdims=True)\n            vmin = vertices.min(0, keepdims=True)\n            pcd.vertices = o3d.utility.Vector3dVector(\n                (vertices - vmin) / (vmax - vmin).max())\n\n            # Oversample points and copy\n            xyz = resample_mesh(pcd, density=self.density)\n            self.cache[idx] = xyz\n            cache_percent = int((len(self.cache) / len(self)) * 100)\n            if cache_percent > 0 and cache_percent % 10 == 0 and cache_percent != self.last_cache_percent:\n                logging.info(\n                    f""Cached {self.phase}: {len(self.cache)} / {len(self)}: {cache_percent}%""\n                )\n                self.last_cache_percent = cache_percent\n\n        # Use color or other features if available\n        feats = np.ones((len(xyz), 1))\n\n        if len(xyz) < 1000:\n            logging.info(\n                f""Skipping {mesh_file}: does not have sufficient CAD sampling density after resampling: {len(xyz)}.""\n            )\n            return None\n\n        if self.transform:\n            xyz, feats = self.transform(xyz, feats)\n\n        # Get coords\n        xyz = xyz * self.resolution\n        coords = np.floor(xyz)\n        inds = ME.utils.sparse_quantize(coords, return_index=True)\n\n        return (coords[inds], xyz[inds], idx)\n\n\ndef make_data_loader(phase, augment_data, batch_size, shuffle, num_workers,\n                     repeat, config):\n    dset = ModelNet40Dataset(phase, config=config)\n\n    args = {\n        \'batch_size\': batch_size,\n        \'num_workers\': num_workers,\n        \'collate_fn\': collate_pointcloud_fn,\n        \'pin_memory\': False,\n        \'drop_last\': False\n    }\n\n    if repeat:\n        args[\'sampler\'] = InfSampler(dset, shuffle)\n    else:\n        args[\'shuffle\'] = shuffle\n\n    loader = torch.utils.data.DataLoader(dset, **args)\n\n    return loader\n\n\nch = logging.StreamHandler(sys.stdout)\nlogging.getLogger().setLevel(logging.INFO)\nlogging.basicConfig(\n    format=os.uname()[1].split(\'.\')[0] + \' %(asctime)s %(message)s\',\n    datefmt=\'%m/%d %H:%M:%S\',\n    handlers=[ch])\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--resolution\', type=int, default=128)\nparser.add_argument(\'--max_iter\', type=int, default=30000)\nparser.add_argument(\'--val_freq\', type=int, default=1000)\nparser.add_argument(\'--batch_size\', default=16, type=int)\nparser.add_argument(\'--lr\', default=1e-2, type=float)\nparser.add_argument(\'--momentum\', type=float, default=0.9)\nparser.add_argument(\'--weight_decay\', type=float, default=1e-4)\nparser.add_argument(\'--num_workers\', type=int, default=1)\nparser.add_argument(\'--stat_freq\', type=int, default=50)\nparser.add_argument(\n    \'--weights\', type=str, default=\'modelnet_reconstruction.pth\')\nparser.add_argument(\'--load_optimizer\', type=str, default=\'true\')\nparser.add_argument(\'--train\', action=\'store_true\')\nparser.add_argument(\'--max_visualization\', type=int, default=4)\n\n###############################################################################\n# End of utility functions\n###############################################################################\n\n\nclass GenerativeNet(nn.Module):\n\n    CHANNELS = [1024, 512, 256, 128, 64, 32, 16]\n\n    def __init__(self, resolution, in_nchannel=512):\n        nn.Module.__init__(self)\n\n        self.resolution = resolution\n\n        # Input sparse tensor must have tensor stride 128.\n        ch = self.CHANNELS\n\n        # Block 1\n        self.block1 = nn.Sequential(\n            ME.MinkowskiConvolutionTranspose(\n                in_nchannel,\n                ch[0],\n                kernel_size=2,\n                stride=2,\n                generate_new_coords=True,\n                dimension=3),\n            ME.MinkowskiBatchNorm(ch[0]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(ch[0], ch[0], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(ch[0]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolutionTranspose(\n                ch[0],\n                ch[1],\n                kernel_size=2,\n                stride=2,\n                generate_new_coords=True,\n                dimension=3),\n            ME.MinkowskiBatchNorm(ch[1]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(ch[1], ch[1], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(ch[1]),\n            ME.MinkowskiELU(),\n        )\n\n        self.block1_cls = ME.MinkowskiConvolution(\n            ch[1], 1, kernel_size=1, has_bias=True, dimension=3)\n\n        # Block 2\n        self.block2 = nn.Sequential(\n            ME.MinkowskiConvolutionTranspose(\n                ch[1],\n                ch[2],\n                kernel_size=2,\n                stride=2,\n                generate_new_coords=True,\n                dimension=3),\n            ME.MinkowskiBatchNorm(ch[2]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(ch[2], ch[2], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(ch[2]),\n            ME.MinkowskiELU(),\n        )\n\n        self.block2_cls = ME.MinkowskiConvolution(\n            ch[2], 1, kernel_size=1, has_bias=True, dimension=3)\n\n        # Block 3\n        self.block3 = nn.Sequential(\n            ME.MinkowskiConvolutionTranspose(\n                ch[2],\n                ch[3],\n                kernel_size=2,\n                stride=2,\n                generate_new_coords=True,\n                dimension=3),\n            ME.MinkowskiBatchNorm(ch[3]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(ch[3], ch[3], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(ch[3]),\n            ME.MinkowskiELU(),\n        )\n\n        self.block3_cls = ME.MinkowskiConvolution(\n            ch[3], 1, kernel_size=1, has_bias=True, dimension=3)\n\n        # Block 4\n        self.block4 = nn.Sequential(\n            ME.MinkowskiConvolutionTranspose(\n                ch[3],\n                ch[4],\n                kernel_size=2,\n                stride=2,\n                generate_new_coords=True,\n                dimension=3),\n            ME.MinkowskiBatchNorm(ch[4]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(ch[4], ch[4], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(ch[4]),\n            ME.MinkowskiELU(),\n        )\n\n        self.block4_cls = ME.MinkowskiConvolution(\n            ch[4], 1, kernel_size=1, has_bias=True, dimension=3)\n\n        # Block 5\n        self.block5 = nn.Sequential(\n            ME.MinkowskiConvolutionTranspose(\n                ch[4],\n                ch[5],\n                kernel_size=2,\n                stride=2,\n                generate_new_coords=True,\n                dimension=3),\n            ME.MinkowskiBatchNorm(ch[5]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(ch[5], ch[5], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(ch[5]),\n            ME.MinkowskiELU(),\n        )\n\n        self.block5_cls = ME.MinkowskiConvolution(\n            ch[5], 1, kernel_size=1, has_bias=True, dimension=3)\n\n        # Block 6\n        self.block6 = nn.Sequential(\n            ME.MinkowskiConvolutionTranspose(\n                ch[5],\n                ch[6],\n                kernel_size=2,\n                stride=2,\n                generate_new_coords=True,\n                dimension=3),\n            ME.MinkowskiBatchNorm(ch[6]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(ch[6], ch[6], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(ch[6]),\n            ME.MinkowskiELU(),\n        )\n\n        self.block6_cls = ME.MinkowskiConvolution(\n            ch[6], 1, kernel_size=1, has_bias=True, dimension=3)\n\n        # pruning\n        self.pruning = ME.MinkowskiPruning()\n\n    def get_batch_indices(self, out):\n        return out.coords_man.get_row_indices_per_batch(out.coords_key)\n\n    def get_target(self, out, target_key, kernel_size=1):\n        with torch.no_grad():\n            target = torch.zeros(len(out), dtype=torch.bool)\n            cm = out.coords_man\n            strided_target_key = cm.stride(\n                target_key, out.tensor_stride[0], force_creation=True)\n            ins, outs = cm.get_kernel_map(\n                out.coords_key,\n                strided_target_key,\n                kernel_size=kernel_size,\n                region_type=1)\n            for curr_in in ins:\n                target[curr_in] = 1\n        return target\n\n    def valid_batch_map(self, batch_map):\n        for b in batch_map:\n            if len(b) == 0:\n                return False\n        return True\n\n    def forward(self, z, target_key):\n        out_cls, targets = [], []\n\n        # Block1\n        out1 = self.block1(z)\n        out1_cls = self.block1_cls(out1)\n        target = self.get_target(out1, target_key)\n        targets.append(target)\n        out_cls.append(out1_cls)\n        keep1 = (out1_cls.F > 0).cpu().squeeze()\n\n        # If training, force target shape generation, use net.eval() to disable\n        if self.training:\n            keep1 += target\n\n        # Remove voxels 32\n        out1 = self.pruning(out1, keep1.cpu())\n\n        # Block 2\n        out2 = self.block2(out1)\n        out2_cls = self.block2_cls(out2)\n        target = self.get_target(out2, target_key)\n        targets.append(target)\n        out_cls.append(out2_cls)\n        keep2 = (out2_cls.F > 0).cpu().squeeze()\n\n        if self.training:\n            keep2 += target\n\n        # Remove voxels 16\n        out2 = self.pruning(out2, keep2.cpu())\n\n        # Block 3\n        out3 = self.block3(out2)\n        out3_cls = self.block3_cls(out3)\n        target = self.get_target(out3, target_key)\n        targets.append(target)\n        out_cls.append(out3_cls)\n        keep3 = (out3_cls.F > 0).cpu().squeeze()\n\n        if self.training:\n            keep3 += target\n\n        # Remove voxels 8\n        out3 = self.pruning(out3, keep3.cpu())\n\n        # Block 4\n        out4 = self.block4(out3)\n        out4_cls = self.block4_cls(out4)\n        target = self.get_target(out4, target_key)\n        targets.append(target)\n        out_cls.append(out4_cls)\n        keep4 = (out4_cls.F > 0).cpu().squeeze()\n\n        if self.training:\n            keep4 += target\n\n        # Remove voxels 4\n        out4 = self.pruning(out4, keep4.cpu())\n\n        # Block 5\n        out5 = self.block5(out4)\n        out5_cls = self.block5_cls(out5)\n        target = self.get_target(out5, target_key)\n        targets.append(target)\n        out_cls.append(out5_cls)\n        keep5 = (out5_cls.F > 0).cpu().squeeze()\n\n        if self.training:\n            keep5 += target\n\n        # Remove voxels 2\n        out5 = self.pruning(out5, keep5.cpu())\n\n        # Block 5\n        out6 = self.block6(out5)\n        out6_cls = self.block6_cls(out6)\n        target = self.get_target(out6, target_key)\n        targets.append(target)\n        out_cls.append(out6_cls)\n        keep6 = (out6_cls.F > 0).cpu().squeeze()\n\n        # Last layer does not require keep\n        # if self.training:\n        #   keep6 += target\n\n        # Remove voxels 1\n        out6 = self.pruning(out6, keep6.cpu())\n\n        return out_cls, targets, out6\n\n\ndef train(net, dataloader, device, config):\n    in_nchannel = len(dataloader.dataset)\n\n    optimizer = optim.SGD(\n        net.parameters(),\n        lr=config.lr,\n        momentum=config.momentum,\n        weight_decay=config.weight_decay)\n    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.95)\n\n    crit = nn.BCEWithLogitsLoss()\n\n    net.train()\n    train_iter = iter(dataloader)\n    # val_iter = iter(val_dataloader)\n    logging.info(f\'LR: {scheduler.get_lr()}\')\n    for i in range(config.max_iter):\n\n        s = time()\n        data_dict = train_iter.next()\n        d = time() - s\n\n        optimizer.zero_grad()\n        init_coords = torch.zeros((config.batch_size, 4), dtype=torch.int)\n        init_coords[:, 0] = torch.arange(config.batch_size)\n\n        in_feat = torch.zeros((config.batch_size, in_nchannel))\n        in_feat[torch.arange(config.batch_size), data_dict[\'labels\']] = 1\n\n        sin = ME.SparseTensor(\n            feats=in_feat,\n            coords=init_coords,\n            allow_duplicate_coords=True,  # for classification, it doesn\'t matter\n            tensor_stride=config.resolution,\n        ).to(device)\n\n        # Generate target sparse tensor\n        cm = sin.coords_man\n        target_key = cm.create_coords_key(\n            ME.utils.batched_coordinates(data_dict[\'xyzs\']),\n            force_creation=True,\n            allow_duplicate_coords=True)\n\n        # Generate from a dense tensor\n        out_cls, targets, sout = net(sin, target_key)\n        num_layers, loss = len(out_cls), 0\n        losses = []\n        for out_cl, target in zip(out_cls, targets):\n            curr_loss = crit(out_cl.F.squeeze(),\n                             target.type(out_cl.F.dtype).to(device))\n            losses.append(curr_loss.item())\n            loss += curr_loss / num_layers\n\n        loss.backward()\n        optimizer.step()\n        t = time() - s\n\n        if i % config.stat_freq == 0:\n            logging.info(\n                f\'Iter: {i}, Loss: {loss.item():.3e}, Depths: {len(out_cls)} Data Loading Time: {d:.3e}, Tot Time: {t:.3e}\'\n            )\n\n        if i % config.val_freq == 0 and i > 0:\n            torch.save(\n                {\n                    \'state_dict\': net.state_dict(),\n                    \'optimizer\': optimizer.state_dict(),\n                    \'scheduler\': scheduler.state_dict(),\n                    \'curr_iter\': i,\n                }, config.weights)\n\n            scheduler.step()\n            logging.info(f\'LR: {scheduler.get_lr()}\')\n\n            net.train()\n\n\ndef visualize(net, dataloader, device, config):\n    in_nchannel = len(dataloader.dataset)\n    net.eval()\n    crit = nn.BCEWithLogitsLoss()\n    n_vis = 0\n\n    for data_dict in dataloader:\n\n        init_coords = torch.zeros((config.batch_size, 4), dtype=torch.int)\n        init_coords[:, 0] = torch.arange(config.batch_size)\n\n        in_feat = torch.zeros((config.batch_size, in_nchannel))\n        in_feat[torch.arange(config.batch_size), data_dict[\'labels\']] = 1\n\n        sin = ME.SparseTensor(\n            feats=in_feat,\n            coords=init_coords,\n            allow_duplicate_coords=True,  # for classification, it doesn\'t matter\n            tensor_stride=config.resolution,\n        ).to(device)\n\n        # Generate target sparse tensor\n        cm = sin.coords_man\n        target_key = cm.create_coords_key(\n            ME.utils.batched_coordinates(data_dict[\'xyzs\']),\n            force_creation=True,\n            allow_duplicate_coords=True)\n\n        # Generate from a dense tensor\n        out_cls, targets, sout = net(sin, target_key)\n        num_layers, loss = len(out_cls), 0\n        for out_cl, target in zip(out_cls, targets):\n            loss += crit(out_cl.F.squeeze(),\n                         target.type(out_cl.F.dtype).to(device)) / num_layers\n\n        batch_coords, batch_feats = sout.decomposed_coordinates_and_features\n        for b, (coords, feats) in enumerate(zip(batch_coords, batch_feats)):\n            pcd = PointCloud(coords)\n            pcd.estimate_normals()\n            pcd.translate([0.6 * config.resolution, 0, 0])\n            pcd.rotate(M)\n            opcd = PointCloud(data_dict[\'xyzs\'][b])\n            opcd.translate([-0.6 * config.resolution, 0, 0])\n            opcd.estimate_normals()\n            opcd.rotate(M)\n            o3d.visualization.draw_geometries([pcd, opcd])\n\n            n_vis += 1\n            if n_vis > config.max_visualization:\n                return\n\n\nif __name__ == \'__main__\':\n    config = parser.parse_args()\n    logging.info(config)\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n    dataloader = make_data_loader(\n        \'val\',\n        augment_data=True,\n        batch_size=config.batch_size,\n        shuffle=True,\n        num_workers=config.num_workers,\n        repeat=True,\n        config=config)\n    in_nchannel = len(dataloader.dataset)\n\n    net = GenerativeNet(config.resolution, in_nchannel=in_nchannel)\n    net.to(device)\n\n    logging.info(net)\n\n    if config.train:\n        train(net, dataloader, device, config)\n    else:\n        if not os.path.exists(config.weights):\n            logging.info(\n                f\'Downloaing pretrained weights. This might take a while...\')\n            urllib.request.urlretrieve(\n                ""https://bit.ly/36d9m1n"", filename=config.weights)\n\n        logging.info(f\'Loading weights from {config.weights}\')\n        checkpoint = torch.load(config.weights)\n        net.load_state_dict(checkpoint[\'state_dict\'])\n\n        visualize(net, dataloader, device, config)\n'"
examples/resnet.py,5,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport torch\nimport torch.nn as nn\nfrom torch.optim import SGD\n\nimport MinkowskiEngine as ME\nfrom MinkowskiEngine.modules.resnet_block import BasicBlock, Bottleneck\n\nfrom tests.common import data_loader\n\n\nclass ResNetBase(nn.Module):\n    BLOCK = None\n    LAYERS = ()\n    INIT_DIM = 64\n    PLANES = (64, 128, 256, 512)\n\n    def __init__(self, in_channels, out_channels, D=3):\n        nn.Module.__init__(self)\n        self.D = D\n        assert self.BLOCK is not None\n\n        self.network_initialization(in_channels, out_channels, D)\n        self.weight_initialization()\n\n    def network_initialization(self, in_channels, out_channels, D):\n\n        self.inplanes = self.INIT_DIM\n        self.conv1 = ME.MinkowskiConvolution(\n            in_channels, self.inplanes, kernel_size=5, stride=2, dimension=D)\n\n        self.bn1 = ME.MinkowskiBatchNorm(self.inplanes)\n        self.relu = ME.MinkowskiReLU(inplace=True)\n\n        self.pool = ME.MinkowskiAvgPooling(kernel_size=2, stride=2, dimension=D)\n\n        self.layer1 = self._make_layer(\n            self.BLOCK, self.PLANES[0], self.LAYERS[0], stride=2)\n        self.layer2 = self._make_layer(\n            self.BLOCK, self.PLANES[1], self.LAYERS[1], stride=2)\n        self.layer3 = self._make_layer(\n            self.BLOCK, self.PLANES[2], self.LAYERS[2], stride=2)\n        self.layer4 = self._make_layer(\n            self.BLOCK, self.PLANES[3], self.LAYERS[3], stride=2)\n\n        self.conv5 = ME.MinkowskiConvolution(\n            self.inplanes, self.inplanes, kernel_size=3, stride=3, dimension=D)\n        self.bn5 = ME.MinkowskiBatchNorm(self.inplanes)\n\n        self.glob_avg = ME.MinkowskiGlobalMaxPooling()\n\n        self.final = ME.MinkowskiLinear(self.inplanes, out_channels, bias=True)\n\n    def weight_initialization(self):\n        for m in self.modules():\n            if isinstance(m, ME.MinkowskiConvolution):\n                ME.utils.kaiming_normal_(m.kernel, mode=\'fan_out\', nonlinearity=\'relu\')\n\n            if isinstance(m, ME.MinkowskiBatchNorm):\n                nn.init.constant_(m.bn.weight, 1)\n                nn.init.constant_(m.bn.bias, 0)\n\n    def _make_layer(self,\n                    block,\n                    planes,\n                    blocks,\n                    stride=1,\n                    dilation=1,\n                    bn_momentum=0.1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                ME.MinkowskiConvolution(\n                    self.inplanes,\n                    planes * block.expansion,\n                    kernel_size=1,\n                    stride=stride,\n                    dimension=self.D),\n                ME.MinkowskiBatchNorm(planes * block.expansion))\n        layers = []\n        layers.append(\n            block(\n                self.inplanes,\n                planes,\n                stride=stride,\n                dilation=dilation,\n                downsample=downsample,\n                dimension=self.D))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(\n                block(\n                    self.inplanes,\n                    planes,\n                    stride=1,\n                    dilation=dilation,\n                    dimension=self.D))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.pool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.conv5(x)\n        x = self.bn5(x)\n        x = self.relu(x)\n\n        x = self.glob_avg(x)\n        return self.final(x)\n\n\nclass ResNet14(ResNetBase):\n    BLOCK = BasicBlock\n    LAYERS = (1, 1, 1, 1)\n\n\nclass ResNet18(ResNetBase):\n    BLOCK = BasicBlock\n    LAYERS = (2, 2, 2, 2)\n\n\nclass ResNet34(ResNetBase):\n    BLOCK = BasicBlock\n    LAYERS = (3, 4, 6, 3)\n\n\nclass ResNet50(ResNetBase):\n    BLOCK = Bottleneck\n    LAYERS = (3, 4, 6, 3)\n\n\nclass ResNet101(ResNetBase):\n    BLOCK = Bottleneck\n    LAYERS = (3, 4, 23, 3)\n\n\nif __name__ == \'__main__\':\n    # loss and network\n    criterion = nn.CrossEntropyLoss()\n    net = ResNet14(in_channels=3, out_channels=5, D=2)\n    print(net)\n\n    # a data loader must return a tuple of coords, features, and labels.\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n    net = net.to(device)\n    optimizer = SGD(net.parameters(), lr=1e-2)\n\n    for i in range(10):\n        optimizer.zero_grad()\n\n        # Get new data\n        coords, feat, label = data_loader()\n        input = ME.SparseTensor(feat, coords=coords).to(device)\n        label = label.to(device)\n\n        # Forward\n        output = net(input)\n\n        # Loss\n        loss = criterion(output.F, label)\n        print(\'Iteration: \', i, \', Loss: \', loss.item())\n\n        # Gradient\n        loss.backward()\n        optimizer.step()\n\n    # Saving and loading a network\n    torch.save(net.state_dict(), \'test.pth\')\n    net.load_state_dict(torch.load(\'test.pth\'))\n'"
examples/sparse_tensor_basic.py,1,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport torch\nimport MinkowskiEngine as ME\n\ndata_batch_0 = [\n    [0, 0, 2.1, 0, 0],  #\n    [0, 1, 1.4, 3, 0],  #\n    [0, 0, 4.0, 0, 0]\n]\n\ndata_batch_1 = [\n    [1, 0, 0],  #\n    [0, 2, 0],  #\n    [0, 0, 3]\n]\n\n\ndef to_sparse_coo(data):\n    # An intuitive way to extract coordinates and features\n    coords, feats = [], []\n    for i, row in enumerate(data):\n        for j, val in enumerate(row):\n            if val != 0:\n                coords.append([i, j])\n                feats.append([val])\n    return torch.IntTensor(coords), torch.FloatTensor(feats)\n\n\ndef sparse_tensor_initialization():\n    coords, feats = to_sparse_coo(data_batch_0)\n    # collate sparse tensor data to augment batch indices\n    # Note that it is wrapped inside a list!!\n    coords, feats = ME.utils.sparse_collate(coords=[coords], feats=[feats])\n    sparse_tensor = ME.SparseTensor(coords=coords, feats=feats)\n\n\ndef sparse_tensor_arithmetics():\n    coords0, feats0 = to_sparse_coo(data_batch_0)\n    coords0, feats0 = ME.utils.sparse_collate(coords=[coords0], feats=[feats0])\n\n    coords1, feats1 = to_sparse_coo(data_batch_1)\n    coords1, feats1 = ME.utils.sparse_collate(coords=[coords1], feats=[feats1])\n\n    # sparse tensors\n    A = ME.SparseTensor(coords=coords0, feats=feats0)\n    B = ME.SparseTensor(coords=coords1, feats=feats1)\n\n    # The following fails\n    try:\n        C = A + B\n    except AssertionError:\n        pass\n\n    B = ME.SparseTensor(\n        coords=coords1,\n        feats=feats1,\n        coords_manager=A.coords_man,  # must share the same coordinate manager\n        force_creation=True  # must force creation since tensor stride [1] exists\n    )\n\n    C = A + B\n    C = A - B\n    C = A * B\n    C = A / B\n\n    # in place operations\n    # Note that it requires the same coords_key (no need to feed coords)\n    D = ME.SparseTensor(\n        # coords=coords,  not required\n        feats=feats0,\n        coords_manager=A.coords_man,  # must share the same coordinate manager\n        coords_key=A.coords_key  # For inplace, must share the same coords key\n    )\n\n    A += D\n    A -= D\n    A *= D\n    A /= D\n\n    # If you have two or more sparse tensors with the same coords_key, you can concatenate features\n    E = ME.cat(A, D)\n\n\ndef operation_mode():\n    # Set to share the coords_man by default\n    ME.set_sparse_tensor_operation_mode(\n        ME.SparseTensorOperationMode.SHARE_COORDS_MANAGER)\n    print(ME.sparse_tensor_operation_mode())\n\n    coords0, feats0 = to_sparse_coo(data_batch_0)\n    coords0, feats0 = ME.utils.sparse_collate(coords=[coords0], feats=[feats0])\n\n    coords1, feats1 = to_sparse_coo(data_batch_1)\n    coords1, feats1 = ME.utils.sparse_collate(coords=[coords1], feats=[feats1])\n\n    for _ in range(2):\n        # sparse tensors\n        A = ME.SparseTensor(coords=coords0, feats=feats0)\n        B = ME.SparseTensor(\n            coords=coords1,\n            feats=feats1,\n            # coords_manager=A.coords_man,  No need to feed the coords_man\n            force_creation=True)\n\n        C = A + B\n\n        # When done using it for forward and backward, you must cleanup the coords man\n        ME.clear_global_coords_man()\n\n\ndef decomposition():\n    coords0, feats0 = to_sparse_coo(data_batch_0)\n    coords1, feats1 = to_sparse_coo(data_batch_1)\n    coords, feats = ME.utils.sparse_collate(\n        coords=[coords0, coords1], feats=[feats0, feats1])\n\n    # sparse tensors\n    A = ME.SparseTensor(coords=coords, feats=feats)\n    conv = ME.MinkowskiConvolution(\n        in_channels=1, out_channels=2, kernel_size=3, stride=2, dimension=2)\n    B = conv(A)\n\n    # Extract features and coordinates per batch index\n    list_of_coords = B.decomposed_coordinates\n    list_of_feats = B.decomposed_features\n    list_of_coords, list_of_feats = B.decomposed_coordinates_and_features\n\n    # To specify a batch index\n    batch_index = 1\n    coords = B.coordinates_at(batch_index)\n    feats = B.features_at(batch_index)\n\n    # Empty list if given an invalid batch index\n    batch_index = 3\n    print(B.coordinates_at(batch_index))\n\n\nif __name__ == \'__main__\':\n    sparse_tensor_initialization()\n    sparse_tensor_arithmetics()\n    operation_mode()\n    decomposition()\n'"
examples/training.py,5,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\n#\n# ############################################################################\n# Example training to demonstrate usage of MinkowskiEngine with torch dataset\n# and dataloader classes.\n#\n# $ python -m examples.training\n# Epoch: 0 iter: 1, Loss: 0.7992178201675415\n# Epoch: 0 iter: 10, Loss: 0.5555745628145006\n# Epoch: 0 iter: 20, Loss: 0.4025680094957352\n# Epoch: 0 iter: 30, Loss: 0.3157463788986206\n# Epoch: 0 iter: 40, Loss: 0.27348957359790804\n# Epoch: 0 iter: 50, Loss: 0.2690591633319855\n# Epoch: 0 iter: 60, Loss: 0.258208692073822\n# Epoch: 0 iter: 70, Loss: 0.34842072874307634\n# Epoch: 0 iter: 80, Loss: 0.27565130293369294\n# Epoch: 0 iter: 90, Loss: 0.2860450878739357\n# Epoch: 0 iter: 100, Loss: 0.24737665355205535\n# Epoch: 1 iter: 110, Loss: 0.2428090125322342\n# Epoch: 1 iter: 120, Loss: 0.25397603064775465\n# Epoch: 1 iter: 130, Loss: 0.23624965399503708\n# Epoch: 1 iter: 140, Loss: 0.2247777447104454\n# Epoch: 1 iter: 150, Loss: 0.22956613600254058\n# Epoch: 1 iter: 160, Loss: 0.22803852707147598\n# Epoch: 1 iter: 170, Loss: 0.24081039279699326\n# Epoch: 1 iter: 180, Loss: 0.22322929948568343\n# Epoch: 1 iter: 190, Loss: 0.22531934976577758\n# Epoch: 1 iter: 200, Loss: 0.2116936132311821\n#\n# ############################################################################\nimport argparse\nimport numpy as np\n\nimport torch\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport MinkowskiEngine as ME\n\nfrom examples.unet import UNet\n\n\ndef plot(C, L):\n    import matplotlib.pyplot as plt\n    mask = L == 0\n    cC = C[mask].t().numpy()\n    plt.scatter(cC[0], cC[1], c=\'r\', s=0.1)\n    mask = L == 1\n    cC = C[mask].t().numpy()\n    plt.scatter(cC[0], cC[1], c=\'b\', s=0.1)\n    plt.show()\n\n\nclass RandomLineDataset(Dataset):\n\n    # Warning: read using mutable obects for default input arguments in python.\n    def __init__(\n        self,\n        angle_range_rad=[-np.pi, np.pi],\n        line_params=[\n            -1,  # Start\n            1,  # end\n        ],\n        is_linear_noise=True,\n        dataset_size=100,\n        num_samples=10000,\n        quantization_size=0.005):\n\n        self.angle_range_rad = angle_range_rad\n        self.is_linear_noise = is_linear_noise\n        self.line_params = line_params\n        self.dataset_size = dataset_size\n        self.rng = np.random.RandomState(0)\n\n        self.num_samples = num_samples\n        self.num_data = int(0.2 * num_samples)\n        self.num_noise = num_samples - self.num_data\n\n        self.quantization_size = quantization_size\n\n    def __len__(self):\n        return self.dataset_size\n\n    def _uniform_to_angle(self, u):\n        return (self.angle_range_rad[1] -\n                self.angle_range_rad[0]) * u + self.angle_range_rad[0]\n\n    def _sample_noise(self, num, noise_params):\n        noise = noise_params[0] + self.rng.randn(num, 1) * noise_params[1]\n        return noise\n\n    def _sample_xs(self, num):\n        """"""Return random numbers between line_params[0], line_params[1]""""""\n        return (self.line_params[1] - self.line_params[0]) * self.rng.rand(\n            num, 1) + self.line_params[0]\n\n    def __getitem__(self, i):\n        # Regardless of the input index, return randomized data\n        angle, intercept = np.tan(self._uniform_to_angle(\n            self.rng.rand())), self.rng.rand()\n\n        # Line as x = cos(theta) * t, y = sin(theta) * t + intercept and random t\'s\n        # Drop some samples\n        xs_data = self._sample_xs(self.num_data)\n        ys_data = angle * xs_data + intercept + self._sample_noise(\n            self.num_data, [0, 0.1])\n\n        noise = 4 * (self.rng.rand(self.num_noise, 2) - 0.5)\n\n        # Concatenate data\n        input = np.vstack([np.hstack([xs_data, ys_data]), noise])\n        feats = input\n        labels = np.vstack(\n            [np.ones((self.num_data, 1)),\n             np.zeros((self.num_noise, 1))]).astype(np.int32)\n\n        # Quantize the input\n        discrete_coords, unique_feats, unique_labels = ME.utils.sparse_quantize(\n            coords=input,\n            feats=feats,\n            labels=labels,\n            quantization_size=self.quantization_size,\n            ignore_label=-100)\n\n        return discrete_coords, unique_feats, unique_labels\n\n\ndef collation_fn(data_labels):\n    coords, feats, labels = list(zip(*data_labels))\n    coords_batch, feats_batch, labels_batch = [], [], []\n\n    # Generate batched coordinates\n    coords_batch = ME.utils.batched_coordinates(coords)\n\n    # Concatenate all lists\n    feats_batch = torch.from_numpy(np.concatenate(feats, 0)).float()\n    labels_batch = torch.from_numpy(np.concatenate(labels, 0))\n\n    return coords_batch, feats_batch, labels_batch\n\n\ndef main(config):\n    # Binary classification\n    net = UNet(\n        2,  # in nchannel\n        2,  # out_nchannel\n        D=2)\n\n    optimizer = optim.SGD(\n        net.parameters(),\n        lr=config.lr,\n        momentum=config.momentum,\n        weight_decay=config.weight_decay)\n\n    criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n\n    # Dataset, data loader\n    train_dataset = RandomLineDataset()\n\n    train_dataloader = DataLoader(\n        train_dataset,\n        batch_size=config.batch_size,\n        # 1) collate_fn=collation_fn,\n        # 2) collate_fn=ME.utils.batch_sparse_collate,\n        # 3) collate_fn=ME.utils.SparseCollation(),\n        collate_fn=ME.utils.batch_sparse_collate,\n        num_workers=1)\n\n    accum_loss, accum_iter, tot_iter = 0, 0, 0\n\n    for epoch in range(config.max_epochs):\n        train_iter = iter(train_dataloader)\n\n        # Training\n        net.train()\n        for i, data in enumerate(train_iter):\n            coords, feats, labels = data\n            out = net(ME.SparseTensor(feats.float(), coords))\n            optimizer.zero_grad()\n            loss = criterion(out.F.squeeze(), labels.long())\n            loss.backward()\n            optimizer.step()\n\n            accum_loss += loss.item()\n            accum_iter += 1\n            tot_iter += 1\n\n            if tot_iter % 10 == 0 or tot_iter == 1:\n                print(\n                    f\'Epoch: {epoch} iter: {tot_iter}, Loss: {accum_loss / accum_iter}\'\n                )\n                accum_loss, accum_iter = 0, 0\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--batch_size\', default=12, type=int)\n    parser.add_argument(\'--max_epochs\', default=10, type=int)\n    parser.add_argument(\'--lr\', default=0.1, type=float)\n    parser.add_argument(\'--momentum\', type=float, default=0.9)\n    parser.add_argument(\'--weight_decay\', type=float, default=1e-4)\n\n    config = parser.parse_args()\n    main(config)\n'"
examples/unet.py,6,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport torch\n\nimport MinkowskiEngine as ME\nimport MinkowskiEngine.MinkowskiFunctional as MF\n\nfrom tests.common import data_loader\n\n\nclass UNet(ME.MinkowskiNetwork):\n\n    def __init__(self, in_nchannel, out_nchannel, D):\n        super(UNet, self).__init__(D)\n        self.block1 = torch.nn.Sequential(\n            ME.MinkowskiConvolution(\n                in_channels=in_nchannel,\n                out_channels=8,\n                kernel_size=3,\n                stride=1,\n                dimension=D),\n            ME.MinkowskiBatchNorm(8))\n\n        self.block2 = torch.nn.Sequential(\n            ME.MinkowskiConvolution(\n                in_channels=8,\n                out_channels=16,\n                kernel_size=3,\n                stride=2,\n                dimension=D),\n            ME.MinkowskiBatchNorm(16),\n        )\n\n        self.block3 = torch.nn.Sequential(\n            ME.MinkowskiConvolution(\n                in_channels=16,\n                out_channels=32,\n                kernel_size=3,\n                stride=2,\n                dimension=D),\n            ME.MinkowskiBatchNorm(32))\n\n        self.block3_tr = torch.nn.Sequential(\n            ME.MinkowskiConvolutionTranspose(\n                in_channels=32,\n                out_channels=16,\n                kernel_size=3,\n                stride=2,\n                dimension=D),\n            ME.MinkowskiBatchNorm(16))\n\n        self.block2_tr = torch.nn.Sequential(\n            ME.MinkowskiConvolutionTranspose(\n                in_channels=32,\n                out_channels=16,\n                kernel_size=3,\n                stride=2,\n                dimension=D),\n            ME.MinkowskiBatchNorm(16))\n\n        self.conv1_tr = ME.MinkowskiConvolution(\n            in_channels=24,\n            out_channels=out_nchannel,\n            kernel_size=1,\n            stride=1,\n            dimension=D)\n\n    def forward(self, x):\n        out_s1 = self.block1(x)\n        out = MF.relu(out_s1)\n\n        out_s2 = self.block2(out)\n        out = MF.relu(out_s2)\n\n        out_s4 = self.block3(out)\n        out = MF.relu(out_s4)\n\n        out = MF.relu(self.block3_tr(out))\n        out = ME.cat(out, out_s2)\n\n        out = MF.relu(self.block2_tr(out))\n        out = ME.cat(out, out_s1)\n\n        return self.conv1_tr(out)\n\n\nif __name__ == \'__main__\':\n    # loss and network\n    net = UNet(3, 5, D=2)\n    print(net)\n\n    # a data loader must return a tuple of coords, features, and labels.\n    coords, feat, label = data_loader()\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n    net = net.to(device)\n    input = ME.SparseTensor(feat, coords=coords).to(device)\n\n    # Forward\n    output = net(input)\n'"
examples/vae.py,21,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport os\nimport sys\nimport subprocess\nimport argparse\nimport logging\nimport glob\nimport numpy as np\nfrom time import time\nimport urllib\n# Must be imported before large libs\ntry:\n    import open3d as o3d\nexcept ImportError:\n    raise ImportError(\'Please install open3d with `pip install open3d`.\')\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.optim as optim\n\nimport MinkowskiEngine as ME\n\nfrom examples.modelnet40 import InfSampler, resample_mesh\n\nM = np.array([[0.80656762, -0.5868724, -0.07091862],\n              [0.3770505, 0.418344, 0.82632997],\n              [-0.45528188, -0.6932309, 0.55870326]])\n\nassert int(\n    o3d.__version__.split(\'.\')[1]\n) >= 8, f\'Requires open3d version >= 0.8, the current version is {o3d.__version__}\'\n\nif not os.path.exists(\'ModelNet40\'):\n    logging.info(\'Downloading the fixed ModelNet40 dataset...\')\n    subprocess.run([""sh"", ""./examples/download_modelnet40.sh""])\n\n\n###############################################################################\n# Utility functions\n###############################################################################\ndef PointCloud(points, colors=None):\n\n    pcd = o3d.geometry.PointCloud()\n    pcd.points = o3d.utility.Vector3dVector(points)\n    if colors is not None:\n        pcd.colors = o3d.utility.Vector3dVector(colors)\n    return pcd\n\n\ndef collate_pointcloud_fn(list_data):\n    coords, feats, labels = list(zip(*list_data))\n\n    # Concatenate all lists\n    return {\n        \'coords\': ME.utils.batched_coordinates(coords),\n        \'xyzs\': [torch.from_numpy(feat).float() for feat in feats],\n        \'labels\': torch.LongTensor(labels),\n    }\n\n\nclass ModelNet40Dataset(torch.utils.data.Dataset):\n\n    def __init__(self, phase, transform=None, config=None):\n        self.phase = phase\n        self.files = []\n        self.cache = {}\n        self.data_objects = []\n        self.transform = transform\n        self.resolution = config.resolution\n        self.last_cache_percent = 0\n\n        self.root = \'./ModelNet40\'\n        fnames = glob.glob(os.path.join(self.root, f\'chair/{phase}/*.off\'))\n        fnames = sorted([os.path.relpath(fname, self.root) for fname in fnames])\n        self.files = fnames\n        assert len(self.files) > 0, ""No file loaded""\n        logging.info(\n            f""Loading the subset {phase} from {self.root} with {len(self.files)} files""\n        )\n        self.density = 30000\n\n        # Ignore warnings in obj loader\n        o3d.utility.set_verbosity_level(o3d.utility.VerbosityLevel.Error)\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, idx):\n        mesh_file = os.path.join(self.root, self.files[idx])\n        if idx in self.cache:\n            xyz = self.cache[idx]\n        else:\n            # Load a mesh, over sample, copy, rotate, voxelization\n            assert os.path.exists(mesh_file)\n            pcd = o3d.io.read_triangle_mesh(mesh_file)\n            # Normalize to fit the mesh inside a unit cube while preserving aspect ratio\n            vertices = np.asarray(pcd.vertices)\n            vmax = vertices.max(0, keepdims=True)\n            vmin = vertices.min(0, keepdims=True)\n            pcd.vertices = o3d.utility.Vector3dVector(\n                (vertices - vmin) / (vmax - vmin).max())\n\n            # Oversample points and copy\n            xyz = resample_mesh(pcd, density=self.density)\n            self.cache[idx] = xyz\n            cache_percent = int((len(self.cache) / len(self)) * 100)\n            if cache_percent > 0 and cache_percent % 10 == 0 and cache_percent != self.last_cache_percent:\n                logging.info(\n                    f""Cached {self.phase}: {len(self.cache)} / {len(self)}: {cache_percent}%""\n                )\n                self.last_cache_percent = cache_percent\n\n        # Use color or other features if available\n        feats = np.ones((len(xyz), 1))\n\n        if len(xyz) < 1000:\n            logging.info(\n                f""Skipping {mesh_file}: does not have sufficient CAD sampling density after resampling: {len(xyz)}.""\n            )\n            return None\n\n        if self.transform:\n            xyz, feats = self.transform(xyz, feats)\n\n        # Get coords\n        xyz = xyz * self.resolution\n        coords = np.floor(xyz)\n        inds = ME.utils.sparse_quantize(coords, return_index=True)\n\n        return (coords[inds], xyz[inds], idx)\n\n\ndef make_data_loader(phase, augment_data, batch_size, shuffle, num_workers,\n                     repeat, config):\n    dset = ModelNet40Dataset(phase, config=config)\n\n    args = {\n        \'batch_size\': batch_size,\n        \'num_workers\': num_workers,\n        \'collate_fn\': collate_pointcloud_fn,\n        \'pin_memory\': False,\n        \'drop_last\': False\n    }\n\n    if repeat:\n        args[\'sampler\'] = InfSampler(dset, shuffle)\n    else:\n        args[\'shuffle\'] = shuffle\n\n    loader = torch.utils.data.DataLoader(dset, **args)\n\n    return loader\n\n\nch = logging.StreamHandler(sys.stdout)\nlogging.getLogger().setLevel(logging.INFO)\nlogging.basicConfig(\n    format=os.uname()[1].split(\'.\')[0] + \' %(asctime)s %(message)s\',\n    datefmt=\'%m/%d %H:%M:%S\',\n    handlers=[ch])\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--resolution\', type=int, default=128)\nparser.add_argument(\'--max_iter\', type=int, default=30000)\nparser.add_argument(\'--val_freq\', type=int, default=1000)\nparser.add_argument(\'--batch_size\', default=16, type=int)\nparser.add_argument(\'--lr\', default=1e-2, type=float)\nparser.add_argument(\'--momentum\', type=float, default=0.9)\nparser.add_argument(\'--weight_decay\', type=float, default=1e-4)\nparser.add_argument(\'--num_workers\', type=int, default=1)\nparser.add_argument(\'--stat_freq\', type=int, default=50)\nparser.add_argument(\'--weights\', type=str, default=\'modelnet_vae.pth\')\nparser.add_argument(\'--resume\', type=str, default=None)\nparser.add_argument(\'--load_optimizer\', type=str, default=\'true\')\nparser.add_argument(\'--train\', action=\'store_true\')\nparser.add_argument(\'--max_visualization\', type=int, default=4)\n\n###############################################################################\n# End of utility functions\n###############################################################################\n\n\nclass Encoder(nn.Module):\n\n    CHANNELS = [16, 32, 64, 128, 256, 512, 1024]\n\n    def __init__(self):\n        nn.Module.__init__(self)\n\n        # Input sparse tensor must have tensor stride 128.\n        ch = self.CHANNELS\n\n        # Block 1\n        self.block1 = nn.Sequential(\n            ME.MinkowskiConvolution(\n                1, ch[0], kernel_size=3, stride=2, dimension=3),\n            ME.MinkowskiBatchNorm(ch[0]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(ch[0], ch[0], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(ch[0]),\n            ME.MinkowskiELU(),\n        )\n\n        self.block2 = nn.Sequential(\n            ME.MinkowskiConvolution(\n                ch[0], ch[1], kernel_size=3, stride=2, dimension=3),\n            ME.MinkowskiBatchNorm(ch[1]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(ch[1], ch[1], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(ch[1]),\n            ME.MinkowskiELU(),\n        )\n\n        self.block3 = nn.Sequential(\n            ME.MinkowskiConvolution(\n                ch[1], ch[2], kernel_size=3, stride=2, dimension=3),\n            ME.MinkowskiBatchNorm(ch[2]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(ch[2], ch[2], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(ch[2]),\n            ME.MinkowskiELU(),\n        )\n\n        self.block4 = nn.Sequential(\n            ME.MinkowskiConvolution(\n                ch[2], ch[3], kernel_size=3, stride=2, dimension=3),\n            ME.MinkowskiBatchNorm(ch[3]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(ch[3], ch[3], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(ch[3]),\n            ME.MinkowskiELU(),\n        )\n\n        self.block5 = nn.Sequential(\n            ME.MinkowskiConvolution(\n                ch[3], ch[4], kernel_size=3, stride=2, dimension=3),\n            ME.MinkowskiBatchNorm(ch[4]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(ch[4], ch[4], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(ch[4]),\n            ME.MinkowskiELU(),\n        )\n\n        # Block 5\n        self.block6 = nn.Sequential(\n            ME.MinkowskiConvolution(\n                ch[4], ch[5], kernel_size=3, stride=2, dimension=3),\n            ME.MinkowskiBatchNorm(ch[5]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(ch[5], ch[5], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(ch[5]),\n            ME.MinkowskiELU(),\n        )\n\n        # Block 6\n        self.block7 = nn.Sequential(\n            ME.MinkowskiConvolution(\n                ch[5], ch[6], kernel_size=3, stride=2, dimension=3),\n            ME.MinkowskiBatchNorm(ch[6]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(ch[6], ch[6], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(ch[6]),\n            ME.MinkowskiELU(),\n        )\n\n        self.global_pool = ME.MinkowskiGlobalPooling()\n\n        self.linear_mean = ME.MinkowskiLinear(ch[6], ch[6], bias=True)\n        self.linear_log_var = ME.MinkowskiLinear(ch[6], ch[6], bias=True)\n        self.weight_initialization()\n\n    def weight_initialization(self):\n        for m in self.modules():\n            if isinstance(m, ME.MinkowskiConvolution):\n                ME.utils.kaiming_normal_(\n                    m.kernel, mode=\'fan_out\', nonlinearity=\'relu\')\n\n            if isinstance(m, ME.MinkowskiBatchNorm):\n                nn.init.constant_(m.bn.weight, 1)\n                nn.init.constant_(m.bn.bias, 0)\n\n    def forward(self, sinput):\n        out = self.block1(sinput)\n        out = self.block2(out)\n        out = self.block3(out)\n        out = self.block4(out)\n        out = self.block5(out)\n        out = self.block6(out)\n        out = self.block7(out)\n        out = self.global_pool(out)\n        mean = self.linear_mean(out)\n        log_var = self.linear_log_var(out)\n        return mean, log_var\n\n\nclass Decoder(nn.Module):\n\n    CHANNELS = [1024, 512, 256, 128, 64, 32, 16]\n    resolution = 128\n\n    def __init__(self):\n        nn.Module.__init__(self)\n\n        # Input sparse tensor must have tensor stride 128.\n        ch = self.CHANNELS\n\n        # Block 1\n        self.block1 = nn.Sequential(\n            ME.MinkowskiConvolutionTranspose(\n                ch[0],\n                ch[0],\n                kernel_size=2,\n                stride=2,\n                generate_new_coords=True,\n                dimension=3),\n            ME.MinkowskiBatchNorm(ch[0]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(ch[0], ch[0], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(ch[0]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolutionTranspose(\n                ch[0],\n                ch[1],\n                kernel_size=2,\n                stride=2,\n                generate_new_coords=True,\n                dimension=3),\n            ME.MinkowskiBatchNorm(ch[1]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(ch[1], ch[1], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(ch[1]),\n            ME.MinkowskiELU(),\n        )\n\n        self.block1_cls = ME.MinkowskiConvolution(\n            ch[1], 1, kernel_size=1, has_bias=True, dimension=3)\n\n        # Block 2\n        self.block2 = nn.Sequential(\n            ME.MinkowskiConvolutionTranspose(\n                ch[1],\n                ch[2],\n                kernel_size=2,\n                stride=2,\n                generate_new_coords=True,\n                dimension=3),\n            ME.MinkowskiBatchNorm(ch[2]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(ch[2], ch[2], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(ch[2]),\n            ME.MinkowskiELU(),\n        )\n\n        self.block2_cls = ME.MinkowskiConvolution(\n            ch[2], 1, kernel_size=1, has_bias=True, dimension=3)\n\n        # Block 3\n        self.block3 = nn.Sequential(\n            ME.MinkowskiConvolutionTranspose(\n                ch[2],\n                ch[3],\n                kernel_size=2,\n                stride=2,\n                generate_new_coords=True,\n                dimension=3),\n            ME.MinkowskiBatchNorm(ch[3]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(ch[3], ch[3], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(ch[3]),\n            ME.MinkowskiELU(),\n        )\n\n        self.block3_cls = ME.MinkowskiConvolution(\n            ch[3], 1, kernel_size=1, has_bias=True, dimension=3)\n\n        # Block 4\n        self.block4 = nn.Sequential(\n            ME.MinkowskiConvolutionTranspose(\n                ch[3],\n                ch[4],\n                kernel_size=2,\n                stride=2,\n                generate_new_coords=True,\n                dimension=3),\n            ME.MinkowskiBatchNorm(ch[4]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(ch[4], ch[4], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(ch[4]),\n            ME.MinkowskiELU(),\n        )\n\n        self.block4_cls = ME.MinkowskiConvolution(\n            ch[4], 1, kernel_size=1, has_bias=True, dimension=3)\n\n        # Block 5\n        self.block5 = nn.Sequential(\n            ME.MinkowskiConvolutionTranspose(\n                ch[4],\n                ch[5],\n                kernel_size=2,\n                stride=2,\n                generate_new_coords=True,\n                dimension=3),\n            ME.MinkowskiBatchNorm(ch[5]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(ch[5], ch[5], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(ch[5]),\n            ME.MinkowskiELU(),\n        )\n\n        self.block5_cls = ME.MinkowskiConvolution(\n            ch[5], 1, kernel_size=1, has_bias=True, dimension=3)\n\n        # Block 6\n        self.block6 = nn.Sequential(\n            ME.MinkowskiConvolutionTranspose(\n                ch[5],\n                ch[6],\n                kernel_size=2,\n                stride=2,\n                generate_new_coords=True,\n                dimension=3),\n            ME.MinkowskiBatchNorm(ch[6]),\n            ME.MinkowskiELU(),\n            ME.MinkowskiConvolution(ch[6], ch[6], kernel_size=3, dimension=3),\n            ME.MinkowskiBatchNorm(ch[6]),\n            ME.MinkowskiELU(),\n        )\n\n        self.block6_cls = ME.MinkowskiConvolution(\n            ch[6], 1, kernel_size=1, has_bias=True, dimension=3)\n\n        # pruning\n        self.pruning = ME.MinkowskiPruning()\n\n    def get_batch_indices(self, out):\n        return out.coords_man.get_row_indices_per_batch(out.coords_key)\n\n    def get_target(self, out, target_key, kernel_size=1):\n        with torch.no_grad():\n            target = torch.zeros(len(out), dtype=torch.bool)\n            cm = out.coords_man\n            strided_target_key = cm.stride(\n                target_key, out.tensor_stride[0], force_creation=True)\n            ins, outs = cm.get_kernel_map(\n                out.coords_key,\n                strided_target_key,\n                kernel_size=kernel_size,\n                region_type=1)\n            for curr_in in ins:\n                target[curr_in] = 1\n        return target\n\n    def valid_batch_map(self, batch_map):\n        for b in batch_map:\n            if len(b) == 0:\n                return False\n        return True\n\n    def forward(self, z, target_key):\n        out_cls, targets = [], []\n\n        z.set_tensor_stride(self.resolution)\n        # Block1\n        out1 = self.block1(z)\n        out1_cls = self.block1_cls(out1)\n        target = self.get_target(out1, target_key)\n        targets.append(target)\n        out_cls.append(out1_cls)\n        keep1 = (out1_cls.F > 0).cpu().squeeze()\n\n        # If training, force target shape generation, use net.eval() to disable\n        if self.training:\n            keep1 += target\n\n        # Remove voxels 32\n        out1 = self.pruning(out1, keep1.cpu())\n\n        # Block 2\n        out2 = self.block2(out1)\n        out2_cls = self.block2_cls(out2)\n        target = self.get_target(out2, target_key)\n        targets.append(target)\n        out_cls.append(out2_cls)\n        keep2 = (out2_cls.F > 0).cpu().squeeze()\n\n        if self.training:\n            keep2 += target\n\n        # Remove voxels 16\n        out2 = self.pruning(out2, keep2.cpu())\n\n        # Block 3\n        out3 = self.block3(out2)\n        out3_cls = self.block3_cls(out3)\n        target = self.get_target(out3, target_key)\n        targets.append(target)\n        out_cls.append(out3_cls)\n        keep3 = (out3_cls.F > 0).cpu().squeeze()\n\n        if self.training:\n            keep3 += target\n\n        # Remove voxels 8\n        out3 = self.pruning(out3, keep3.cpu())\n\n        # Block 4\n        out4 = self.block4(out3)\n        out4_cls = self.block4_cls(out4)\n        target = self.get_target(out4, target_key)\n        targets.append(target)\n        out_cls.append(out4_cls)\n        keep4 = (out4_cls.F > 0).cpu().squeeze()\n\n        if self.training:\n            keep4 += target\n\n        # Remove voxels 4\n        out4 = self.pruning(out4, keep4.cpu())\n\n        # Block 5\n        out5 = self.block5(out4)\n        out5_cls = self.block5_cls(out5)\n        target = self.get_target(out5, target_key)\n        targets.append(target)\n        out_cls.append(out5_cls)\n        keep5 = (out5_cls.F > 0).cpu().squeeze()\n\n        if self.training:\n            keep5 += target\n\n        # Remove voxels 2\n        out5 = self.pruning(out5, keep5.cpu())\n\n        # Block 5\n        out6 = self.block6(out5)\n        out6_cls = self.block6_cls(out6)\n        target = self.get_target(out6, target_key)\n        targets.append(target)\n        out_cls.append(out6_cls)\n        keep6 = (out6_cls.F > 0).cpu().squeeze()\n\n        # Last layer does not require keep\n        # if self.training:\n        #   keep6 += target\n\n        # Remove voxels 1\n        if keep6.sum() > 0:\n            out6 = self.pruning(out6, keep6.cpu())\n\n        return out_cls, targets, out6\n\n\nclass VAE(nn.Module):\n\n    def __init__(self):\n        nn.Module.__init__(self)\n        self.encoder = Encoder()\n        self.decoder = Decoder()\n\n    def forward(self, sinput, gt_target):\n        means, log_vars = self.encoder(sinput)\n        zs = means\n        if self.training:\n            zs += torch.exp(0.5 * log_vars.F) * torch.randn_like(log_vars.F)\n        out_cls, targets, sout = self.decoder(zs, gt_target)\n        return out_cls, targets, sout, means, log_vars, zs\n\n\ndef train(net, dataloader, device, config):\n    optimizer = optim.SGD(\n        net.parameters(),\n        lr=config.lr,\n        momentum=config.momentum,\n        weight_decay=config.weight_decay)\n    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.95)\n\n    crit = nn.BCEWithLogitsLoss()\n\n    start_iter = 0\n    if config.resume is not None:\n        checkpoint = torch.load(config.resume)\n        print(\'Resuming weights\')\n        net.load_state_dict(checkpoint[\'state_dict\'])\n        optimizer.load_state_dict(checkpoint[\'optimizer\'])\n        scheduler.load_state_dict(checkpoint[\'scheduler\'])\n        start_iter = checkpoint[\'curr_iter\']\n\n    net.train()\n    train_iter = iter(dataloader)\n    # val_iter = iter(val_dataloader)\n    logging.info(f\'LR: {scheduler.get_lr()}\')\n    for i in range(start_iter, config.max_iter):\n\n        s = time()\n        data_dict = train_iter.next()\n        d = time() - s\n\n        optimizer.zero_grad()\n        sin = ME.SparseTensor(\n            torch.ones(len(data_dict[\'coords\']), 1),\n            data_dict[\'coords\'].int(),\n            allow_duplicate_coords=True,  # for classification, it doesn\'t matter\n        ).to(device)\n\n        # Generate target sparse tensor\n        target_key = sin.coords_key\n\n        out_cls, targets, sout, means, log_vars, zs = net(sin, target_key)\n        num_layers, BCE = len(out_cls), 0\n        losses = []\n        for out_cl, target in zip(out_cls, targets):\n            curr_loss = crit(out_cl.F.squeeze(),\n                             target.type(out_cl.F.dtype).to(device))\n            losses.append(curr_loss.item())\n            BCE += curr_loss / num_layers\n\n        KLD = -0.5 * torch.mean(\n            torch.mean(1 + log_vars.F - means.F.pow(2) - log_vars.F.exp(), 1))\n        loss = KLD + BCE\n\n        loss.backward()\n        optimizer.step()\n        t = time() - s\n\n        if i % config.stat_freq == 0:\n            logging.info(\n                f\'Iter: {i}, Loss: {loss.item():.3e}, Depths: {len(out_cls)} Data Loading Time: {d:.3e}, Tot Time: {t:.3e}\'\n            )\n\n        if i % config.val_freq == 0 and i > 0:\n            torch.save(\n                {\n                    \'state_dict\': net.state_dict(),\n                    \'optimizer\': optimizer.state_dict(),\n                    \'scheduler\': scheduler.state_dict(),\n                    \'curr_iter\': i,\n                }, config.weights)\n\n            scheduler.step()\n            logging.info(f\'LR: {scheduler.get_lr()}\')\n\n            net.train()\n\n\ndef visualize(net, dataloader, device, config):\n    net.eval()\n    crit = nn.BCEWithLogitsLoss()\n    n_vis = 0\n\n    for data_dict in dataloader:\n\n        sin = ME.SparseTensor(\n            torch.ones(len(data_dict[\'coords\']), 1),\n            data_dict[\'coords\'].int(),\n            allow_duplicate_coords=True,  # for classification, it doesn\'t matter\n        ).to(device)\n\n        # Generate target sparse tensor\n        target_key = sin.coords_key\n\n        out_cls, targets, sout, means, log_vars, zs = net(sin, target_key)\n        num_layers, BCE = len(out_cls), 0\n        losses = []\n        for out_cl, target in zip(out_cls, targets):\n            curr_loss = crit(out_cl.F.squeeze(),\n                             target.type(out_cl.F.dtype).to(device))\n            losses.append(curr_loss.item())\n            BCE += curr_loss / num_layers\n\n        KLD = -0.5 * torch.mean(\n            torch.sum(1 + log_vars.F - means.F.pow(2) - log_vars.F.exp(), 1))\n        loss = KLD + BCE\n\n        print(loss)\n\n        batch_coords, batch_feats = sout.decomposed_coordinates_and_features\n        for b, (coords, feats) in enumerate(zip(batch_coords, batch_feats)):\n            pcd = PointCloud(coords)\n            pcd.estimate_normals()\n            pcd.translate([0.6 * config.resolution, 0, 0])\n            pcd.rotate(M)\n            opcd = PointCloud(data_dict[\'xyzs\'][b])\n            opcd.translate([-0.6 * config.resolution, 0, 0])\n            opcd.estimate_normals()\n            opcd.rotate(M)\n            o3d.visualization.draw_geometries([pcd, opcd])\n\n            n_vis += 1\n            if n_vis > config.max_visualization:\n                return\n\n\nif __name__ == \'__main__\':\n    config = parser.parse_args()\n    logging.info(config)\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n    net = VAE()\n    net.to(device)\n\n    logging.info(net)\n\n    if config.train:\n        dataloader = make_data_loader(\n            \'train\',\n            augment_data=True,\n            batch_size=config.batch_size,\n            shuffle=True,\n            num_workers=config.num_workers,\n            repeat=True,\n            config=config)\n\n        train(net, dataloader, device, config)\n    else:\n        if not os.path.exists(config.weights):\n            logging.info(\n                f\'Downloaing pretrained weights. This might take a while...\')\n            urllib.request.urlretrieve(\n                ""https://bit.ly/39TvWys"", filename=config.weights)\n\n        logging.info(f\'Loading weights from {config.weights}\')\n        checkpoint = torch.load(config.weights)\n        net.load_state_dict(checkpoint[\'state_dict\'])\n\n        dataloader = make_data_loader(\n            \'test\',\n            augment_data=True,\n            batch_size=config.batch_size,\n            shuffle=True,\n            num_workers=config.num_workers,\n            repeat=True,\n            config=config)\n\n        with torch.no_grad():\n            visualize(net, dataloader, device, config)\n'"
tests/__init__.py,0,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\n'"
tests/broadcast.py,4,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport torch\nimport unittest\n\nfrom MinkowskiEngine import SparseTensor, MinkowskiGlobalPooling, \\\n    MinkowskiBroadcastFunction, MinkowskiBroadcastAddition, \\\n    MinkowskiBroadcastMultiplication, MinkowskiBroadcast, \\\n    MinkowskiBroadcastConcatenation, OperationType\n\nfrom utils.gradcheck import gradcheck\nfrom tests.common import data_loader\n\n\nclass TestBroadcast(unittest.TestCase):\n\n    def test_broadcast_gpu(self):\n        in_channels, D = 2, 2\n        coords, feats, labels = data_loader(in_channels)\n        coords, feats_glob, labels = data_loader(in_channels)\n        feats = feats.double()\n        feats_glob = feats_glob.double()\n        input = SparseTensor(feats, coords=coords)\n        pool = MinkowskiGlobalPooling()\n        input_glob = pool(input)\n        input_glob.F.requires_grad_()\n        broadcast_add = MinkowskiBroadcastAddition()\n        broadcast_mul = MinkowskiBroadcastMultiplication()\n        broadcast_cat = MinkowskiBroadcastConcatenation()\n        cpu_add = broadcast_add(input, input_glob)\n        cpu_mul = broadcast_mul(input, input_glob)\n        cpu_cat = broadcast_cat(input, input_glob)\n\n        # Check backward\n        fn = MinkowskiBroadcastFunction()\n\n        device = torch.device(\'cuda\')\n\n        input = input.to(device)\n        input_glob = input_glob.to(device)\n        gpu_add = broadcast_add(input, input_glob)\n        gpu_mul = broadcast_mul(input, input_glob)\n        gpu_cat = broadcast_cat(input, input_glob)\n\n        self.assertTrue(torch.prod(gpu_add.F.cpu() - cpu_add.F < 1e-5).item() == 1)\n        self.assertTrue(torch.prod(gpu_mul.F.cpu() - cpu_mul.F < 1e-5).item() == 1)\n        self.assertTrue(torch.prod(gpu_cat.F.cpu() - cpu_cat.F < 1e-5).item() == 1)\n\n        self.assertTrue(\n            gradcheck(\n                fn,\n                (input.F, input_glob.F, OperationType.ADDITION,\n                 input.coords_key, input_glob.coords_key, input.coords_man)))\n\n        self.assertTrue(\n            gradcheck(\n                fn,\n                (input.F, input_glob.F, OperationType.MULTIPLICATION,\n                 input.coords_key, input_glob.coords_key, input.coords_man)))\n\n    def test_broadcast(self):\n        in_channels, D = 2, 2\n        coords, feats, labels = data_loader(in_channels)\n        coords, feats_glob, labels = data_loader(in_channels)\n        feats = feats.double()\n        feats_glob = feats_glob.double()\n        input = SparseTensor(feats, coords=coords)\n        pool = MinkowskiGlobalPooling()\n        input_glob = pool(input)\n        input_glob.F.requires_grad_()\n        broadcast = MinkowskiBroadcast()\n        broadcast_cat = MinkowskiBroadcastConcatenation()\n        broadcast_add = MinkowskiBroadcastAddition()\n        broadcast_mul = MinkowskiBroadcastMultiplication()\n        output = broadcast(input, input_glob)\n        print(output)\n        output = broadcast_cat(input, input_glob)\n        print(output)\n        output = broadcast_add(input, input_glob)\n        print(output)\n        output = broadcast_mul(input, input_glob)\n        print(output)\n\n        # Check backward\n        fn = MinkowskiBroadcastFunction()\n\n        self.assertTrue(\n            gradcheck(\n                fn,\n                (input.F, input_glob.F, OperationType.ADDITION,\n                 input.coords_key, input_glob.coords_key, input.coords_man)))\n\n        self.assertTrue(\n            gradcheck(\n                fn,\n                (input.F, input_glob.F, OperationType.MULTIPLICATION,\n                 input.coords_key, input_glob.coords_key, input.coords_man)))\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/chwise_conv.py,4,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport torch\nimport unittest\n\nfrom MinkowskiEngine import SparseTensor, MinkowskiChannelwiseConvolution\nimport MinkowskiEngine as ME\n\n\nfrom tests.common import data_loader\n\n\ndef get_random_coords(dimension=2, tensor_stride=2):\n    torch.manual_seed(0)\n    # Create random coordinates with tensor stride == 2\n    coords = torch.rand(10, dimension + 1)\n    coords[:, :dimension] *= 5  # random coords\n    coords[:, -1] *= 2  # random batch index\n    coords = coords.floor().int()\n    coords = ME.utils.sparse_quantize(coords)\n    coords[:, :dimension] *= tensor_stride  # make the tensor stride 2\n    return coords, tensor_stride\n\n\nclass TestConvolution(unittest.TestCase):\n\n    def test(self):\n        print(f""{self.__class__.__name__}: test"")\n        in_channels, D = 3, 2\n        coords, feats, labels = data_loader(in_channels, batch_size=2)\n\n        # Create random coordinates with tensor stride == 2\n        out_coords, tensor_stride = get_random_coords()\n\n        feats = feats.double()\n        feats.requires_grad_()\n        input = SparseTensor(feats, coords=coords)\n\n        conv = MinkowskiChannelwiseConvolution(\n            in_channels,\n            kernel_size=3,\n            stride=1,\n            has_bias=False,\n            dimension=D).double()\n\n        print(\'Initial input: \', input)\n        output = conv(input)\n        print(\'Conv output: \', output)\n\n        output.F.sum().backward()\n        print(input.F.grad)\n\n    def test_gpu(self):\n        print(f""{self.__class__.__name__}: test_gpu"")\n        if not torch.cuda.is_available():\n            return\n\n        device = torch.device(\'cuda\')\n        in_channels, D = 3, 2\n        coords, feats, labels = data_loader(in_channels, batch_size=2)\n\n        # Create random coordinates with tensor stride == 2\n        out_coords, tensor_stride = get_random_coords()\n\n        feats = feats.double()\n        feats.requires_grad_()\n        input = SparseTensor(feats, coords=coords).to(device)\n        conv = MinkowskiChannelwiseConvolution(\n            in_channels,\n            kernel_size=3,\n            stride=1,\n            has_bias=False,\n            dimension=D).double().to(device)\n\n        print(\'Initial input: \', input)\n        output = conv(input)\n        print(\'Conv output: \', output)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/common.py,3,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport numpy as np\n\nimport torch\nimport MinkowskiEngine as ME\n\n\ndef get_coords(data):\n    coords = []\n    for i, row in enumerate(data):\n        for j, col in enumerate(row):\n            if col != \' \':\n                coords.append([i, j])\n    return np.array(coords)\n\n\ndef data_loader(nchannel=3,\n                max_label=5,\n                is_classification=True,\n                seed=-1,\n                batch_size=2):\n    if seed >= 0:\n        torch.manual_seed(seed)\n\n    data = [\n        ""   X   "",  #\n        ""  X X  "",  #\n        "" XXXXX ""\n    ]\n\n    # Generate coordinates\n    coords = [get_coords(data) for i in range(batch_size)]\n    coords = ME.utils.batched_coordinates(coords)\n\n    # features and labels\n    N = len(coords)\n    feats = torch.randn(N, nchannel)\n    label = (torch.rand(2 if is_classification else N) * max_label).long()\n    return coords, feats, label\n'"
tests/conv.py,4,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport torch\nimport unittest\n\nfrom MinkowskiEngine import SparseTensor, MinkowskiConvolution, MinkowskiConvolutionFunction, \\\n    MinkowskiConvolutionTranspose, MinkowskiConvolutionTransposeFunction\n\nfrom tests.common import data_loader\nfrom utils.gradcheck import gradcheck\n\n\nclass TestConvolution(unittest.TestCase):\n\n    def test_gpu(self):\n        print(f""{self.__class__.__name__}: test_gpu"")\n        if not torch.cuda.is_available():\n            return\n        in_channels, out_channels, D = 2, 3, 2\n        coords, feats, labels = data_loader(in_channels)\n        feats = feats.double()\n        feats.requires_grad_()\n        input = SparseTensor(feats, coords=coords)\n        # Initialize context\n        conv = MinkowskiConvolution(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=2,\n            has_bias=True,\n            dimension=D)\n        print(conv)\n        conv = conv.double()\n        output = conv(input)\n        print(output)\n\n        device = torch.device(\'cuda\')\n        input = input.to(device)\n        conv = conv.to(device)\n        output = conv(input)\n        print(output)\n        print(output.F, output.coords)\n\n        # Check backward\n        fn = MinkowskiConvolutionFunction()\n\n        grad = output.F.clone().zero_()\n        grad[0] = 1\n        output.F.backward(grad)\n\n        self.assertTrue(\n            gradcheck(fn, (input.F, conv.kernel, input.tensor_stride,\n                           conv.stride, conv.kernel_size, conv.dilation,\n                           conv.region_type_, conv.region_offset_,\n                           input.coords_key, None, input.coords_man)))\n\n    def test(self):\n        print(f""{self.__class__.__name__}: test"")\n        in_channels, out_channels, D = 2, 3, 2\n        coords, feats, labels = data_loader(in_channels)\n        feats = feats.double()\n        feats.requires_grad_()\n        input = SparseTensor(feats, coords=coords)\n        # Initialize context\n        conv = MinkowskiConvolution(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=2,\n            has_bias=True,\n            dimension=D)\n        conv = conv.double()\n        output = conv(input)\n        print(output)\n\n        kernel_map = input.coords_man.get_kernel_map(\n            1, 2, stride=2, kernel_size=3)\n        print(kernel_map)\n\n        # Check backward\n        fn = MinkowskiConvolutionFunction()\n\n        self.assertTrue(\n            gradcheck(fn, (input.F, conv.kernel, input.tensor_stride,\n                           conv.stride, conv.kernel_size, conv.dilation,\n                           conv.region_type_, conv.region_offset_,\n                           input.coords_key, None, input.coords_man)))\n\n\nclass TestConvolutionTranspose(unittest.TestCase):\n\n    def test_gpu(self):\n        print(f""{self.__class__.__name__}: test_gpu"")\n        if not torch.cuda.is_available():\n            return\n\n        device = torch.device(\'cuda\')\n        in_channels, out_channels, D = 2, 3, 2\n        coords, feats, labels = data_loader(in_channels)\n        feats = feats.double()\n        feats.requires_grad_()\n        input = SparseTensor(feats, coords=coords).to(device)\n        # Initialize context\n        conv = MinkowskiConvolution(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=2,\n            has_bias=True,\n            dimension=D).double().to(device)\n        conv_tr = MinkowskiConvolutionTranspose(\n            out_channels,\n            in_channels,\n            kernel_size=3,\n            stride=2,\n            has_bias=True,\n            dimension=D).double().to(device)\n        tr_input = conv(input)\n        print(tr_input)\n        output = conv_tr(tr_input)\n        print(output)\n\n        # Check backward\n        fn = MinkowskiConvolutionTransposeFunction()\n\n        self.assertTrue(\n            gradcheck(fn,\n                      (tr_input.F, conv_tr.kernel, tr_input.tensor_stride,\n                       conv_tr.stride, conv_tr.kernel_size, conv_tr.dilation,\n                       conv_tr.region_type_, conv_tr.region_offset_, False,\n                       tr_input.coords_key, None, tr_input.coords_man)))\n\n    def test(self):\n        print(f""{self.__class__.__name__}: test"")\n        in_channels, out_channels, D = 2, 3, 2\n        coords, feats, labels = data_loader(in_channels)\n        feats = feats.double()\n        feats.requires_grad_()\n        input = SparseTensor(feats, coords=coords)\n\n        # Initialize context\n        conv = MinkowskiConvolution(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=2,\n            has_bias=True,\n            dimension=D).double()\n        conv_tr = MinkowskiConvolutionTranspose(\n            out_channels,\n            in_channels,\n            kernel_size=2,\n            stride=2,\n            has_bias=True,\n            dimension=D).double()\n\n        print(\'Initial input: \', input)\n        input = conv(input)\n        print(\'Conv output: \', input)\n\n        output = conv_tr(input)\n        print(\'Conv tr output: \', output)\n\n        # Check backward\n        fn = MinkowskiConvolutionTransposeFunction()\n\n        self.assertTrue(\n            gradcheck(fn,\n                      (input.F, conv_tr.kernel, input.tensor_stride,\n                       conv_tr.stride, conv_tr.kernel_size, conv_tr.dilation,\n                       conv_tr.region_type_, conv_tr.region_offset_, False,\n                       input.coords_key, None, input.coords_man)))\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/conv_on_coords.py,3,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport torch\nimport unittest\n\nfrom MinkowskiEngine import SparseTensor, MinkowskiConvolution, \\\n    MinkowskiConvolutionTranspose\nimport MinkowskiEngine as ME\n\n\nfrom tests.common import data_loader\n\n\ndef get_random_coords(dimension=2, tensor_stride=2):\n    torch.manual_seed(0)\n    # Create random coordinates with tensor stride == 2\n    coords = torch.rand(10, dimension + 1)\n    coords[:, :dimension] *= 5  # random coords\n    coords[:, -1] *= 2  # random batch index\n    coords = coords.floor().int()\n    coords = ME.utils.sparse_quantize(coords)\n    coords[:, :dimension] *= tensor_stride  # make the tensor stride 2\n    return coords, tensor_stride\n\n\nclass TestConvolution(unittest.TestCase):\n\n    def test(self):\n        print(f""{self.__class__.__name__}: test"")\n        in_channels, out_channels, D = 2, 3, 2\n        coords, feats, labels = data_loader(in_channels, batch_size=2)\n\n        # Create random coordinates with tensor stride == 2\n        out_coords, tensor_stride = get_random_coords()\n\n        feats = feats.double()\n        feats.requires_grad_()\n        input = SparseTensor(feats, coords=coords)\n        cm = input.coords_man\n        print(cm._get_coords_key(1))\n\n        conv = MinkowskiConvolution(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=1,\n            has_bias=False,\n            dimension=D).double()\n\n        print(\'Initial input: \', input)\n        print(\'Specified output coords: \', out_coords)\n        output = conv(input, out_coords)\n\n        # To specify the tensor stride\n        out_coords_key = cm.create_coords_key(out_coords, tensor_stride=2)\n        output = conv(input, out_coords_key)\n        print(\'Conv output: \', output)\n\n        output.F.sum().backward()\n        print(input.F.grad)\n\n    def test_tr(self):\n        print(f""{self.__class__.__name__}: test_tr"")\n        in_channels, out_channels, D = 2, 3, 2\n        coords, feats, labels = data_loader(in_channels, batch_size=2)\n        # tensor stride must be at least 2 for convolution transpose with stride 2\n        coords[:, :2] *= 2\n        out_coords = torch.rand(10, 3)\n        out_coords[:, :2] *= 10  # random coords\n        out_coords[:, 2] *= 2  # random batch index\n        out_coords = out_coords.floor().int()\n\n        feats = feats.double()\n        feats.requires_grad_()\n\n        input = SparseTensor(feats, coords=coords, tensor_stride=2)\n        cm = input.coords_man\n        print(cm._get_coords_key(2))\n\n        conv_tr = MinkowskiConvolutionTranspose(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=2,\n            has_bias=False,\n            dimension=D).double()\n        print(\'Initial input: \', input)\n        print(\'Specified output coords: \', out_coords)\n        output = conv_tr(input, out_coords)\n        print(\'Conv output: \', output)\n\n        output.F.sum().backward()\n        print(input.F.grad)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/coords.py,6,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport unittest\n\nimport torch\nimport numpy as np\n\nimport MinkowskiEngine as ME\nfrom MinkowskiEngine import CoordsKey, CoordsManager, MemoryManagerBackend\n\nfrom tests.common import data_loader\n\n\nclass Test(unittest.TestCase):\n\n    def test_hash(self):\n        N, M = 1000, 1000\n        I, J = np.meshgrid(np.arange(N), np.arange(M))\n        I = I.reshape(-1, 1) - 100\n        J = J.reshape(-1, 1) - 100\n        K = np.zeros_like(I)\n        C = np.hstack((I, J, K))\n        coords_manager = CoordsManager(D=2)\n        coords_key = CoordsKey(2)\n        coords_key.setTensorStride(1)\n        coords_manager.initialize(torch.from_numpy(C).int(), coords_key)\n        print(coords_manager)\n\n    def test_coords_key(self):\n        key = CoordsKey(D=1)\n        key.setKey(1)\n        self.assertTrue(key.getKey() == 1)\n        key.setTensorStride([1])\n        print(key)\n\n    def test_coords_manager(self):\n        key = CoordsKey(D=1)\n        key.setTensorStride(1)\n\n        cm = CoordsManager(D=1)\n        coords = torch.IntTensor([[0, 1], [0, 1], [0, 2], [0, 2], [1, 0],\n                                  [1, 0], [1, 1]])\n        unique_coords = torch.unique(coords, dim=0)\n\n        # Initialize map\n        mapping, inverse_mapping = cm.initialize(\n            coords, key, force_remap=True, allow_duplicate_coords=False)\n        self.assertTrue(len(unique_coords) == len(mapping))\n        print(mapping, len(mapping))\n        cm.print_diagnostics(key)\n        print(cm)\n        self.assertTrue(cm.get_batch_size() == 2)\n        self.assertTrue(cm.get_batch_indices() == {0, 1})\n\n        # Create a strided map\n        stride_key = cm.stride(key, [4])\n        strided_coords = cm.get_coords(stride_key)\n        self.assertTrue(len(strided_coords) == 2)\n        cm.print_diagnostics(key)\n        print(cm)\n\n        # Create a transposed stride map\n        transposed_key = cm.transposed_stride(stride_key, [2], [3], [1])\n        print(\'Transposed Stride: \', cm.get_coords(transposed_key))\n        print(cm)\n\n        # Create a transposed stride map\n        transposed_key = cm.transposed_stride(\n            stride_key, [2], [3], [1], force_creation=True)\n        print(\'Forced Transposed Stride: \', cm.get_coords(transposed_key))\n        print(cm)\n\n        # Create a reduction map\n        key = cm.reduce()\n        print(\'Reduction: \', cm.get_coords(key))\n        print(cm)\n\n        print(\'Reduction mapping: \', cm.get_row_indices_per_batch(stride_key))\n        print(cm)\n\n    def test_coords_map(self):\n        coords, _, _ = data_loader(1)\n\n        key = CoordsKey(D=2)\n        key.setTensorStride(1)\n\n        # Initialize map\n        cm = CoordsManager(D=2)\n        mapping, inverse_mapping = cm.initialize(\n            coords, key, force_remap=True, allow_duplicate_coords=False)\n        print(mapping, len(mapping))\n        cm.print_diagnostics(key)\n        print(cm)\n        print(cm.get_batch_size())\n        print(cm.get_batch_indices())\n\n        # Create a strided map\n        stride_key = cm.stride(key, [2, 2])\n        print(\'Stride: \', cm.get_coords(stride_key))\n        cm.print_diagnostics(key)\n        print(cm)\n\n        ins, outs = cm.get_coords_map(1, 2)\n        inc = cm.get_coords(1)\n        outc = cm.get_coords(2)\n        for i, o in zip(ins, outs):\n            print(f""{i}: ({inc[i]}) -> {o}: ({outc[o]})"")\n\n    def test_negative_coords(self):\n        print(\'Negative coords test\')\n        key = CoordsKey(D=1)\n        key.setTensorStride(1)\n\n        cm = CoordsManager(D=1)\n        coords = torch.IntTensor([[0, -3], [0, -2], [0, -1], [0, 0], [0, 1],\n                                  [0, 2], [0, 3]])\n\n        # Initialize map\n        mapping, inverse_mapping = cm.initialize(coords, key)\n        print(mapping, len(mapping))\n        cm.print_diagnostics(key)\n\n        # Create a strided map\n        stride_key = cm.stride(key, [2])\n        strided_coords = cm.get_coords(stride_key).numpy().tolist()\n        self.assertTrue(len(strided_coords) == 4)\n        self.assertTrue([0, -4] in strided_coords)\n        self.assertTrue([0, -2] in strided_coords)\n        self.assertTrue([0, 2] in strided_coords)\n\n        print(\'Stride: \', cm.get_coords(stride_key))\n        cm.print_diagnostics(stride_key)\n        print(cm)\n\n    def test_batch_size_initialize(self):\n        cm = CoordsManager(D=1)\n        coords = torch.IntTensor([[0, -3], [0, -2], [0, -1], [0, 0], [1, 1],\n                                  [1, 2], [1, 3]])\n\n        # key with batch_size 2\n        cm.create_coords_key(coords)\n        self.assertTrue(cm.get_batch_size() == 2)\n\n        coords = torch.IntTensor([[0, -3], [0, -2], [0, -1], [0, 0], [0, 1],\n                                  [0, 2], [0, 3]])\n        cm.create_coords_key(coords)\n\n        self.assertTrue(cm.get_batch_size() == 2)\n\n    def test_memory_manager_backend(self):\n        # Set the global GPU memory manager backend. By default PYTORCH.\n        ME.set_memory_manager_backend(MemoryManagerBackend.PYTORCH)\n        ME.set_memory_manager_backend(MemoryManagerBackend.CUDA)\n\n        # Create a coords man with the specified GPU memory manager backend.\n        # No effect with CPU_ONLY build\n        cm = CoordsManager(memory_manager_backend=MemoryManagerBackend.CUDA, D=2)\n        cm = CoordsManager(memory_manager_backend=MemoryManagerBackend.PYTORCH, D=2)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/dense.py,6,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport unittest\nimport torch\n\nimport MinkowskiEngine as ME\nfrom MinkowskiEngine import SparseTensor, MinkowskiConvolution\n\n\nclass TestDense(unittest.TestCase):\n\n    def test(self):\n        print(f""{self.__class__.__name__}: test_dense"")\n        in_channels, out_channels, D = 2, 3, 2\n        coords1 = torch.IntTensor([[0, 0], [0, 1], [1, 1]])\n        feats1 = torch.DoubleTensor([[1, 2], [3, 4], [5, 6]])\n\n        coords2 = torch.IntTensor([[1, 1], [1, 2], [2, 1]])\n        feats2 = torch.DoubleTensor([[7, 8], [9, 10], [11, 12]])\n        coords, feats = ME.utils.sparse_collate([coords1, coords2], [feats1, feats2])\n        input = SparseTensor(feats, coords=coords)\n        input.requires_grad_()\n        dinput, min_coord, tensor_stride = input.dense()\n        self.assertTrue(dinput[0, 0, 0, 1] == 3)\n        self.assertTrue(dinput[0, 1, 0, 1] == 4)\n        self.assertTrue(dinput[0, 0, 1, 1] == 5)\n        self.assertTrue(dinput[0, 1, 1, 1] == 6)\n\n        self.assertTrue(dinput[1, 0, 1, 1] == 7)\n        self.assertTrue(dinput[1, 1, 1, 1] == 8)\n        self.assertTrue(dinput[1, 0, 2, 1] == 11)\n        self.assertTrue(dinput[1, 1, 2, 1] == 12)\n\n        # Initialize context\n        conv = MinkowskiConvolution(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=2,\n            has_bias=True,\n            dimension=D)\n        conv = conv.double()\n        output = conv(input)\n        print(input.C, output.C)\n\n        # Convert to a dense tensor\n        dense_output, min_coord, tensor_stride = output.dense()\n        print(dense_output.shape)\n        print(dense_output)\n        print(min_coord)\n        print(tensor_stride)\n\n        dense_output, min_coord, tensor_stride = output.dense(\n            min_coords=torch.IntTensor([-2, -2]),\n            max_coords=torch.IntTensor([4, 4]))\n\n        print(dense_output)\n        print(min_coord)\n        print(tensor_stride)\n\n        print(feats.grad)\n\n        loss = dense_output.sum()\n        loss.backward()\n\n        print(feats.grad)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/global.py,3,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport os\nimport argparse\nimport numpy as np\nfrom urllib.request import urlretrieve\ntry:\n    import open3d as o3d\nexcept ImportError:\n    raise ImportError(\'Please install open3d with `pip install open3d`.\')\n\nimport torch\nimport MinkowskiEngine as ME\nfrom examples.common import Timer\n\n# Check if the weights and file exist and download\nif not os.path.isfile(\'1.ply\'):\n    print(\'Downloading a room ply file...\')\n    urlretrieve(""http://cvgl.stanford.edu/data2/minkowskiengine/1.ply"", \'1.ply\')\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--file_name\', type=str, default=\'1.ply\')\nparser.add_argument(\'--voxel_size\', type=float, default=0.02)\nparser.add_argument(\'--batch_size\', type=int, default=1)\nparser.add_argument(\'--max_kernel_size\', type=int, default=7)\n\n\ndef load_file(file_name, voxel_size):\n    pcd = o3d.io.read_point_cloud(file_name)\n    coords = np.array(pcd.points)\n    feats = np.array(pcd.colors)\n\n    quantized_coords = np.floor(coords / voxel_size)\n    inds = ME.utils.sparse_quantize(quantized_coords, return_index=True)\n\n    return quantized_coords[inds], feats[inds], pcd\n\n\ndef generate_input_sparse_tensor(file_name, voxel_size=0.05, batch_size=1):\n    # Create a batch, this process is done in a data loader during training in parallel.\n    batch = [\n        load_file(file_name, voxel_size),\n    ] * batch_size\n    coordinates_, featrues_, pcds = list(zip(*batch))\n    coordinates, features = ME.utils.sparse_collate(coordinates_, featrues_)\n\n    # Normalize features and create a sparse tensor\n    return features, coordinates\n\n\nif __name__ == \'__main__\':\n    config = parser.parse_args()\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n    # Define a model and load the weights\n    feats = [3, 8, 16, 32, 64, 128]\n    features, coordinates = generate_input_sparse_tensor(\n        config.file_name,\n        voxel_size=config.voxel_size,\n        batch_size=config.batch_size)\n\n    pool = ME.MinkowskiGlobalPooling(mode=ME.GlobalPoolingMode.AUTO)\n\n    # Measure time\n    print(\'Forward\')\n    for feat in feats:\n        timer = Timer()\n        features = torch.rand(len(coordinates), feat).to(device)\n\n        # Feed-forward pass and get the prediction\n        for i in range(20):\n            sinput = ME.SparseTensor(features, coords=coordinates).to(device)\n\n            timer.tic()\n            soutput = pool(sinput)\n            timer.toc()\n        print(\n            f\'{timer.min_time:.12f} for feature size: {feat} with {len(sinput)} voxel\'\n        )\n\n    print(\'Backward\')\n    for feat in feats:\n        timer = Timer()\n        sinput._F = torch.rand(len(sinput), feat).to(device).requires_grad_()\n        soutput = pool(sinput)\n        loss = soutput.F.sum()\n        # Feed-forward pass and get the prediction\n        for i in range(20):\n            timer.tic()\n            loss.backward()\n            timer.toc()\n        print(\n            f\'{timer.min_time:.12f} for feature size {feat} with {len(sinput)} voxel\'\n        )\n'"
tests/kernel_map.py,1,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport unittest\n\nimport torch\n\nfrom MinkowskiEngine import SparseTensor, MinkowskiConvolution\n\nfrom tests.common import data_loader\n\n\nclass TestKernelMap(unittest.TestCase):\n\n    def test_kernelmap_gpu(self):\n        print(f""{self.__class__.__name__}: test_kernelmap_gpu"")\n        if not torch.cuda.is_available():\n            return\n\n        in_channels, out_channels, D = 2, 3, 2\n        coords, feats, labels = data_loader(in_channels)\n        feats = feats.double()\n        feats.requires_grad_()\n        input = SparseTensor(feats, coords=coords)\n        cm = input.coords_man\n        ikey = cm._get_coords_key(1)\n        print(\'Input coords: \')\n        cm.print_diagnostics(ikey)\n\n        print(\'Convolution: \')\n\n        # Initialize context\n        conv = MinkowskiConvolution(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=2,\n            has_bias=True,\n            dimension=D).double()\n        output = conv(input)\n\n        iC = input.C.numpy()\n        oC = output.C.numpy()\n        print(iC)\n        print(oC)\n        in_maps, out_maps = output.coords_man.get_kernel_map(\n            1, 2, stride=2, kernel_size=3, on_gpu=True)\n        kernel_index = 0\n        for in_map, out_map in zip(in_maps, out_maps):\n            for i, o in zip(in_map, out_map):\n                print(kernel_index, iC[i], \'->\', oC[o])\n            kernel_index += 1\n        self.assertTrue(sum(len(in_map) for in_map in in_maps) == 26)\n\n    def test_kernelmap(self):\n        print(f""{self.__class__.__name__}: test_kernelmap"")\n        in_channels, out_channels, D = 2, 3, 2\n        coords, feats, labels = data_loader(in_channels)\n        feats = feats.double()\n        feats.requires_grad_()\n        input = SparseTensor(feats, coords=coords)\n        cm = input.coords_man\n        ikey = cm._get_coords_key(1)\n        print(\'Input coords: \')\n        cm.print_diagnostics(ikey)\n\n        print(\'Convolution: \')\n\n        # Initialize context\n        conv = MinkowskiConvolution(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=2,\n            has_bias=True,\n            dimension=D).double()\n        output = conv(input)\n\n        iC = input.C.numpy()\n        oC = output.C.numpy()\n        print(iC)\n        print(oC)\n        in_maps, out_maps = output.coords_man.get_kernel_map(\n            1, 2, stride=2, kernel_size=3)\n        kernel_index = 0\n        for in_map, out_map in zip(in_maps, out_maps):\n            for i, o in zip(in_map, out_map):\n                print(kernel_index, iC[i], \'->\', oC[o])\n            kernel_index += 1\n        self.assertTrue(sum(len(in_map) for in_map in in_maps) == 26)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/norm.py,1,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport torch\nimport unittest\n\nfrom MinkowskiEngine import SparseTensor, MinkowskiInstanceNorm, MinkowskiInstanceNormFunction, GlobalPoolingMode\nfrom utils.gradcheck import gradcheck\n\nfrom tests.common import data_loader\n\n\nclass TestNormalization(unittest.TestCase):\n\n    def test_inst_norm(self):\n        in_channels = 2\n        coords, feats, labels = data_loader(in_channels)\n        feats = feats.double()\n        input = SparseTensor(feats, coords=coords)\n        input.F.requires_grad_()\n        norm = MinkowskiInstanceNorm(num_features=in_channels).double()\n\n        out = norm(input)\n        print(out)\n\n        fn = MinkowskiInstanceNormFunction()\n        self.assertTrue(\n            gradcheck(fn, (input.F, GlobalPoolingMode.AUTO, input.coords_key,\n                           None, input.coords_man)))\n\n    def test_inst_norm_gpu(self):\n        in_channels = 2\n        coords, feats, labels = data_loader(in_channels)\n        feats = feats.double()\n\n        device = torch.device(\'cuda\')\n        input = SparseTensor(feats, coords=coords).to(device)\n        input.F.requires_grad_()\n        norm = MinkowskiInstanceNorm(\n            num_features=in_channels).to(device).double()\n\n        out = norm(input)\n        print(out)\n\n        fn = MinkowskiInstanceNormFunction()\n        self.assertTrue(\n            gradcheck(fn, (input.F, GlobalPoolingMode.AUTO, input.coords_key,\n                           None, input.coords_man)))\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/pool.py,13,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport torch\nimport unittest\n\nfrom MinkowskiEngine import SparseTensor, MinkowskiConvolution, \\\n    MinkowskiSumPooling, \\\n    MinkowskiAvgPoolingFunction, MinkowskiAvgPooling, \\\n    MinkowskiPoolingTransposeFunction, MinkowskiPoolingTranspose, \\\n    MinkowskiGlobalPoolingFunction, MinkowskiGlobalPooling, \\\n    MinkowskiGlobalMaxPoolingFunction, MinkowskiGlobalMaxPooling, \\\n    MinkowskiMaxPoolingFunction, MinkowskiMaxPooling, \\\n    GlobalPoolingMode\n\nfrom utils.gradcheck import gradcheck\nfrom tests.common import data_loader\n\n\nclass TestPooling(unittest.TestCase):\n\n    def test_maxpooling(self):\n        in_channels, D = 2, 2\n        coords, feats, labels = data_loader(in_channels, batch_size=2)\n        feats.requires_grad_()\n        feats = feats.double()\n        input = SparseTensor(feats, coords=coords)\n        pool = MinkowskiMaxPooling(kernel_size=2, stride=2, dimension=D)\n        print(pool)\n        output = pool(input)\n        print(input)\n        print(output)\n        C = output.coords_man\n        print(C.get_coords(2))\n        region_type, _, _ = pool.kernel_generator.cache[(1, 1)]\n        print(\n            C.get_kernel_map(\n                1,\n                2,\n                stride=2,\n                kernel_size=2,\n                region_type=region_type,\n                is_pool=True))\n        # Check backward\n        fn = MinkowskiMaxPoolingFunction()\n\n        # Even numbered kernel_size error!\n        self.assertTrue(\n            gradcheck(\n                fn,\n                (input.F, input.tensor_stride, pool.stride, pool.kernel_size,\n                 pool.dilation, pool.region_type_, pool.region_offset_,\n                 input.coords_key, None, input.coords_man)))\n\n        if not torch.cuda.is_available():\n            return\n\n        device = torch.device(\'cuda\')\n        input = input.to(device)\n        output = pool(input)\n        print(output)\n\n        # Check backward\n        self.assertTrue(\n            gradcheck(\n                fn,\n                (input.F, input.tensor_stride, pool.stride, pool.kernel_size,\n                 pool.dilation, pool.region_type_, pool.region_offset_,\n                 input.coords_key, None, input.coords_man)))\n\n    def test_sumpooling(self):\n        in_channels, D = 2, 2\n        coords, feats, labels = data_loader(in_channels)\n        feats = feats.double()\n        feats.requires_grad_()\n        input = SparseTensor(feats, coords=coords)\n        pool = MinkowskiSumPooling(kernel_size=3, stride=2, dimension=D)\n        output = pool(input)\n        print(output)\n\n        # Check backward\n        fn = MinkowskiAvgPoolingFunction()\n        self.assertTrue(\n            gradcheck(\n                fn,\n                (input.F, input.tensor_stride, pool.stride, pool.kernel_size,\n                 pool.dilation, pool.region_type_, pool.region_offset_, False,\n                 input.coords_key, None, input.coords_man)))\n\n        device = torch.device(\'cuda\')\n        with torch.cuda.device(0):\n            input = input.to(device)\n            pool = pool.to(device)\n            output = pool(input)\n            print(output)\n\n    def test_avgpooling_gpu(self):\n        if not torch.cuda.is_available():\n            return\n\n        in_channels, D = 2, 2\n        coords, feats, labels = data_loader(in_channels)\n        feats = feats.double()\n        feats.requires_grad_()\n        input = SparseTensor(feats, coords=coords)\n        pool = MinkowskiAvgPooling(kernel_size=3, stride=2, dimension=D)\n        output = pool(input)\n        print(output)\n\n        device = torch.device(\'cuda\')\n        with torch.cuda.device(0):\n            input = input.to(device)\n            pool = pool.to(device)\n            output = pool(input)\n            print(output)\n\n        # Check backward\n        fn = MinkowskiAvgPoolingFunction()\n        self.assertTrue(\n            gradcheck(\n                fn,\n                (input.F, input.tensor_stride, pool.stride, pool.kernel_size,\n                 pool.dilation, pool.region_type_, pool.region_offset_, True,\n                 input.coords_key, None, input.coords_man)))\n\n    def test_avgpooling(self):\n        in_channels, D = 2, 2\n        coords, feats, labels = data_loader(in_channels)\n        feats = feats.double()\n        feats.requires_grad_()\n        input = SparseTensor(feats, coords=coords)\n        pool = MinkowskiAvgPooling(kernel_size=3, stride=2, dimension=D)\n        output = pool(input)\n        print(output)\n\n        # Check backward\n        fn = MinkowskiAvgPoolingFunction()\n        self.assertTrue(\n            gradcheck(\n                fn,\n                (input.F, input.tensor_stride, pool.stride, pool.kernel_size,\n                 pool.dilation, pool.region_type_, pool.region_offset_, True,\n                 input.coords_key, None, input.coords_man)))\n\n    def test_global_avgpool(self):\n        in_channels = 2\n        coords, feats, labels = data_loader(in_channels, batch_size=2)\n        feats = feats.double()\n        feats.requires_grad_()\n        input = SparseTensor(feats, coords=coords)\n        pool = MinkowskiGlobalPooling()\n        output = pool(input)\n        print(output)\n\n        # Check backward\n        fn = MinkowskiGlobalPoolingFunction()\n        self.assertTrue(\n            gradcheck(fn, (input.F, True, GlobalPoolingMode.INDEX_SELECT,\n                           input.coords_key, None, input.coords_man)))\n\n        self.assertTrue(\n            gradcheck(fn, (input.F, True, GlobalPoolingMode.SPARSE,\n                           input.coords_key, None, input.coords_man)))\n\n        coords, feats, labels = data_loader(in_channels, batch_size=1)\n        feats = feats.double()\n        feats.requires_grad_()\n        input = SparseTensor(feats, coords=coords)\n        pool = MinkowskiGlobalPooling()\n        output = pool(input)\n        print(output)\n\n        # Check backward\n        fn = MinkowskiGlobalPoolingFunction()\n        self.assertTrue(\n            gradcheck(fn, (input.F, True, GlobalPoolingMode.AUTO,\n                           input.coords_key, None, input.coords_man)))\n\n    def test_global_maxpool(self):\n        in_channels = 2\n        coords, feats, labels = data_loader(in_channels)\n        feats = feats.double()\n        feats.requires_grad_()\n        input = SparseTensor(feats, coords=coords)\n        pool = MinkowskiGlobalMaxPooling()\n        output = pool(input)\n        print(output)\n\n        # Check backward\n        fn = MinkowskiGlobalMaxPoolingFunction()\n        self.assertTrue(\n            gradcheck(fn, (input.F, input.coords_key, None, input.coords_man)))\n\n        if torch.cuda.is_available():\n            input_cuda = input.to(torch.device(0))\n            output_cuda = pool(input)\n            self.assertTrue(torch.allclose(output_cuda.F.cpu(), output.F))\n\n    def test_unpool(self):\n        in_channels, out_channels, D = 2, 3, 2\n        coords, feats, labels = data_loader(in_channels)\n        feats = feats.double()\n        input = SparseTensor(feats, coords=coords)\n        conv = MinkowskiConvolution(\n            in_channels, out_channels, kernel_size=3, stride=2, dimension=D)\n        conv = conv.double()\n        unpool = MinkowskiPoolingTranspose(kernel_size=3, stride=2, dimension=D)\n        input = conv(input)\n        output = unpool(input)\n        print(output)\n\n        # Check backward\n        fn = MinkowskiPoolingTransposeFunction()\n\n        self.assertTrue(\n            gradcheck(fn, (input.F, input.tensor_stride, unpool.stride,\n                           unpool.kernel_size, unpool.dilation,\n                           unpool.region_type_, unpool.region_offset_, False,\n                           input.coords_key, None, input.coords_man)))\n\n    def test_unpooling_gpu(self):\n        if not torch.cuda.is_available():\n            return\n\n        in_channels, out_channels, D = 2, 3, 2\n        coords, feats, labels = data_loader(in_channels)\n        feats = feats.double()\n        input = SparseTensor(feats, coords=coords)\n        conv = MinkowskiConvolution(\n            in_channels, out_channels, kernel_size=3, stride=2, dimension=D)\n        conv = conv.double()\n        unpool = MinkowskiPoolingTranspose(kernel_size=3, stride=2, dimension=D)\n        input = conv(input)\n        output = unpool(input)\n        print(output)\n        # Check backward\n        fn = MinkowskiPoolingTransposeFunction()\n\n        self.assertTrue(\n            gradcheck(fn, (input.F, input.tensor_stride, unpool.stride,\n                           unpool.kernel_size, unpool.dilation,\n                           unpool.region_type_, unpool.region_offset_, False,\n                           input.coords_key, None, input.coords_man)))\n\n        device = torch.device(\'cuda\')\n        with torch.cuda.device(0):\n            input = input.to(device)\n            output = unpool(input)\n            print(output)\n\n        # Check backward\n        self.assertTrue(\n            gradcheck(fn, (input.F, input.tensor_stride, unpool.stride,\n                           unpool.kernel_size, unpool.dilation,\n                           unpool.region_type_, unpool.region_offset_, True,\n                           input.coords_key, None, input.coords_man)))\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/pruning.py,8,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport torch\nimport unittest\n\nfrom MinkowskiEngine import SparseTensor, MinkowskiConvolution, \\\n    MinkowskiConvolutionTranspose, MinkowskiPruning, MinkowskiPruningFunction\n\nfrom utils.gradcheck import gradcheck\nfrom tests.common import data_loader\n\n\nclass TestPruning(unittest.TestCase):\n\n    def test_empty(self):\n        in_channels = 2\n        coords, feats, labels = data_loader(in_channels, batch_size=1)\n        feats = feats.double()\n        feats.requires_grad_()\n        input = SparseTensor(feats, coords=coords)\n        use_feat = torch.BoolTensor(len(input))\n        use_feat.zero_()\n        pruning = MinkowskiPruning()\n        output = pruning(input, use_feat)\n        print(input)\n        print(use_feat)\n        print(output)\n\n        # Check backward\n        fn = MinkowskiPruningFunction()\n        self.assertTrue(\n            gradcheck(fn, (input.F, use_feat, input.coords_key,\n                           output.coords_key, input.coords_man)))\n\n    def test_pruning(self):\n        in_channels, D = 2, 2\n        coords, feats, labels = data_loader(in_channels, batch_size=1)\n        feats = feats.double()\n        feats.requires_grad_()\n        input = SparseTensor(feats, coords=coords)\n        use_feat = torch.rand(feats.size(0)) < 0.5\n        pruning = MinkowskiPruning()\n        output = pruning(input, use_feat)\n        print(input)\n        print(use_feat)\n        print(output)\n\n        # Check backward\n        fn = MinkowskiPruningFunction()\n        self.assertTrue(\n            gradcheck(fn, (input.F, use_feat, input.coords_key,\n                           output.coords_key, input.coords_man)))\n\n        device = torch.device(\'cuda\')\n        with torch.cuda.device(0):\n            input = input.to(device)\n            output = pruning(input, use_feat)\n            print(output)\n\n            self.assertTrue(\n                gradcheck(fn, (input.F, use_feat, input.coords_key,\n                               output.coords_key, input.coords_man)))\n\n    def test_with_convtr(self):\n        channels, D = [2, 3, 4], 2\n        coords, feats, labels = data_loader(channels[0], batch_size=1)\n        feats = feats.double()\n        feats.requires_grad_()\n        # Create a sparse tensor with large tensor strides for upsampling\n        start_tensor_stride = 4\n        input = SparseTensor(\n            feats,\n            coords=coords * start_tensor_stride,\n            tensor_stride=start_tensor_stride)\n        conv_tr1 = MinkowskiConvolutionTranspose(\n            channels[0],\n            channels[1],\n            kernel_size=3,\n            stride=2,\n            generate_new_coords=True,\n            dimension=D).double()\n        conv1 = MinkowskiConvolution(\n            channels[1], channels[1], kernel_size=3, dimension=D).double()\n        conv_tr2 = MinkowskiConvolutionTranspose(\n            channels[1],\n            channels[2],\n            kernel_size=3,\n            stride=2,\n            generate_new_coords=True,\n            dimension=D).double()\n        conv2 = MinkowskiConvolution(\n            channels[2], channels[2], kernel_size=3, dimension=D).double()\n        pruning = MinkowskiPruning()\n\n        out1 = conv_tr1(input)\n        self.assertTrue(torch.prod(torch.abs(out1.F) > 0).item() == 1)\n        out1 = conv1(out1)\n        use_feat = torch.rand(len(out1)) < 0.5\n        out1 = pruning(out1, use_feat)\n\n        out2 = conv_tr2(out1)\n        self.assertTrue(torch.prod(torch.abs(out2.F) > 0).item() == 1)\n        use_feat = torch.rand(len(out2)) < 0.5\n        out2 = pruning(out2, use_feat)\n        out2 = conv2(out2)\n\n        print(out2)\n\n        out2.F.sum().backward()\n\n        # Check gradient flow\n        print(input.F.grad)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/quantization.py,9,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport torch\nimport unittest\nimport numpy as np\n\nfrom MinkowskiEngine.utils import sparse_quantize\nimport MinkowskiEngineBackend as MEB\n\n\nclass TestQuantization(unittest.TestCase):\n\n    def test(self):\n        N = 16575\n        ignore_label = 255\n\n        coords = np.random.rand(N, 3) * 100\n        feats = np.random.rand(N, 4)\n        labels = np.floor(np.random.rand(N) * 3)\n\n        labels = labels.astype(np.int32)\n\n        # Make duplicates\n        coords[:3] = 0\n        labels[:3] = 2\n        quantized_coords, quantized_feats, quantized_labels = sparse_quantize(\n            coords.astype(np.int32), feats, labels, ignore_label)\n\n        print(quantized_labels)\n\n    def test_mapping(self):\n        N = 16575\n        coords = (np.random.rand(N, 3) * 100).astype(np.int32)\n        mapping, inverse_mapping = MEB.quantize_np(coords)\n        print(\'N unique:\', len(mapping), \'N:\', N)\n        self.assertTrue((coords == coords[mapping[inverse_mapping]]).all())\n\n        coords = torch.from_numpy(coords)\n        mapping, inverse_mapping = MEB.quantize_th(coords)\n        print(\'N unique:\', len(mapping), \'N:\', N)\n        self.assertTrue((coords == coords[mapping[inverse_mapping]]).all())\n\n        index, reverse_index = sparse_quantize(\n            coords, return_index=True, return_inverse=True)\n        self.assertTrue((coords == coords[mapping[inverse_mapping]]).all())\n\n    def test_label(self):\n        N = 16575\n        ignore_label = 255\n\n        coords = (np.random.rand(N, 3) * 100).astype(np.int32)\n        feats = np.random.rand(N, 4)\n        labels = np.floor(np.random.rand(N) * 3)\n\n        labels = labels.astype(np.int32)\n\n        # Make duplicates\n        coords[:3] = 0\n        labels[:3] = 2\n\n        mapping, colabels = MEB.quantize_label_np(coords, labels, ignore_label)\n        print(\'Unique labels and counts:\',\n              np.unique(colabels, return_counts=True))\n        print(\'N unique:\', len(mapping), \'N:\', N)\n\n        mapping, colabels = MEB.quantize_label_th(\n            torch.from_numpy(coords), torch.from_numpy(labels), ignore_label)\n        print(\'Unique labels and counts:\',\n              np.unique(colabels, return_counts=True))\n        print(\'N unique:\', len(mapping), \'N:\', N)\n\n        qcoords, qfeats, qlabels = sparse_quantize(coords, feats, labels,\n                                                   ignore_label)\n        self.assertTrue(len(mapping) == len(qcoords))\n\n    def test_collision(self):\n        coords = np.array([[0, 0], [0, 0], [0, 0], [0, 1]], dtype=np.int32)\n        labels = np.array([0, 1, 2, 3], dtype=np.int32)\n\n        unique_coords, colabels = sparse_quantize(\n            coords, labels=labels, ignore_label=255)\n        self.assertTrue(len(unique_coords) == 2)\n        self.assertTrue([0, 0] in unique_coords)\n        self.assertTrue([0, 1] in unique_coords)\n        self.assertTrue(len(colabels) == 2)\n        self.assertTrue(255 in colabels)\n\n        coords = np.array([[0, 0], [0, 1]], dtype=np.int32)\n        discrete_coords = sparse_quantize(coords)\n        self.assertTrue((discrete_coords == unique_coords).all())\n        discrete_coords = sparse_quantize(torch.from_numpy(coords))\n        self.assertTrue(\n            (discrete_coords == torch.from_numpy(unique_coords)).all())\n\n    def test_feature_average(self):\n        coords = torch.IntTensor([[0, 0], [0, 0], [0, 0], [0, 1]])\n        feats = torch.FloatTensor([[0, 1, 2, 3]]).t()\n        mapping, inverse_mapping = MEB.quantize_th(coords)\n        # inverse_mapping is the output map , range is the out map\n        avg_feat = MEB.quantization_average_features(feats,\n                                                     torch.arange(len(feats)),\n                                                     inverse_mapping,\n                                                     len(mapping), 0)\n        self.assertTrue(1 in avg_feat)\n        self.assertTrue(3 in avg_feat)\n\n    def test_quantization_size(self):\n        coords = torch.randn((1000, 3), dtype=torch.float)\n        feats = torch.randn((1000, 10), dtype=torch.float)\n        res = sparse_quantize(coords, feats, quantization_size=0.1)\n        print(res[0].shape, res[1].shape)\n        res = sparse_quantize(\n            coords.numpy(), feats.numpy(), quantization_size=0.1)\n        print(res[0].shape, res[1].shape)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/sparse_tensor.py,13,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport unittest\nimport torch\n\nfrom MinkowskiEngine import (SparseTensor, SparseTensorOperationMode,\n                             SparseTensorQuantizationMode,\n                             set_sparse_tensor_operation_mode)\n\nfrom tests.common import data_loader\n\n\nclass Test(unittest.TestCase):\n\n    def test(self):\n        print(f""{self.__class__.__name__}: test SparseTensor"")\n        coords, feats, labels = data_loader(nchannel=2)\n        input = SparseTensor(feats, coords=coords)\n        print(input)\n\n    def test_empty(self):\n        print(f""{self.__class__.__name__}: test_empty SparseTensor"")\n        feats = torch.FloatTensor(0, 16)\n        coords = torch.IntTensor(0, 4)\n        input = SparseTensor(feats, coords=coords)\n        print(input)\n\n    def test_force_creation(self):\n        print(f""{self.__class__.__name__}: test_force_creation"")\n        coords, feats, labels = data_loader(nchannel=2)\n        input1 = SparseTensor(feats, coords=coords, tensor_stride=1)\n        input2 = SparseTensor(\n            feats,\n            coords=coords,\n            tensor_stride=1,\n            coords_manager=input1.coords_man,\n            force_creation=True)\n        print(input2)\n\n    def test_duplicate_coords(self):\n        print(f""{self.__class__.__name__}: test_duplicate_coords"")\n        coords, feats, labels = data_loader(nchannel=2)\n        # create duplicate coords\n        coords[0] = coords[1]\n        coords[2] = coords[3]\n        input = SparseTensor(feats, coords=coords, allow_duplicate_coords=True)\n        self.assertTrue(len(input) == len(coords) - 2)\n        print(coords)\n        print(input)\n        input = SparseTensor(\n            feats,\n            coords=coords,\n            quantization_mode=SparseTensorQuantizationMode.UNWEIGHTED_AVERAGE)\n        self.assertTrue(len(coords) == 16)\n        self.assertTrue(len(input) == 14)\n\n        # 1D\n        coords = torch.IntTensor([[0, 1], [0, 1], [0, 2], [0, 2], [1, 0],\n                                  [1, 0], [1, 1]])\n        feats = torch.FloatTensor([[0, 1, 2, 3, 5, 6, 7]]).T\n        # 0.5, 2.5, 5.5, 7\n        sinput = SparseTensor(\n            coords=coords,\n            feats=feats,\n            quantization_mode=SparseTensorQuantizationMode.UNWEIGHTED_AVERAGE)\n        self.assertTrue(len(sinput) == 4)\n        self.assertTrue(0.5 in sinput.feats)\n        self.assertTrue(2.5 in sinput.feats)\n        self.assertTrue(5.5 in sinput.feats)\n        self.assertTrue(7 in sinput.feats)\n        self.assertTrue(len(sinput.slice(sinput)) == len(coords))\n\n    def test_extraction(self):\n        coords = torch.IntTensor([[0, 0], [0, 1], [0, 2], [2, 0], [2, 2]])\n        feats = torch.FloatTensor([[1.1, 2.1, 3.1, 4.1, 5.1]]).t()\n        X = SparseTensor(feats, coords)\n        C0 = X.coordinates_at(0)\n        F0 = X.features_at(0)\n        self.assertTrue(0 in C0)\n        self.assertTrue(1 in C0)\n        self.assertTrue(2 in C0)\n\n        self.assertTrue(1.1 in F0)\n        self.assertTrue(2.1 in F0)\n        self.assertTrue(3.1 in F0)\n\n        CC0, FC0 = X.coordinates_and_features_at(0)\n        self.assertTrue((C0 == CC0).all())\n        self.assertTrue((F0 == FC0).all())\n\n        coords, feats = X.decomposed_coordinates_and_features\n        for c, f in zip(coords, feats):\n            self.assertEqual(c.numel(), f.numel())\n            print(c, f)\n\n        feats, valid_inds = X.features_at_coords(torch.IntTensor([[0, 0], [2, 2], [-1, -1]]))\n        self.assertTrue(feats[0, 0] == 1.1)\n        self.assertTrue(feats[1, 0] == 5.1)\n        self.assertTrue(feats[2, 0] == 0)\n        self.assertTrue(len(valid_inds) == 2)\n\n    def test_operation_mode(self):\n        # Set to use the global sparse tensor coords manager by default\n        set_sparse_tensor_operation_mode(\n            SparseTensorOperationMode.SHARE_COORDS_MANAGER)\n\n        coords, feats, labels = data_loader(nchannel=2)\n\n        # Create a sparse tensor on two different coordinates.\n        A = SparseTensor(torch.rand(feats.shape), coords, force_creation=True)\n        B = SparseTensor(\n            torch.rand(4, 2),\n            torch.IntTensor([[0, 0, 0], [1, 1, 1], [0, 1, 0], [1, 0, 1]]),\n            force_creation=True)\n\n        self.assertTrue(A.coords_man == B.coords_man)\n\n        A.requires_grad_(True)\n        B.requires_grad_(True)\n\n        C = A + B\n\n        C.F.sum().backward()\n\n        self.assertTrue(torch.all(A.F.grad == 1).item())\n        self.assertTrue(torch.all(B.F.grad == 1).item())\n\n        C = A - B\n        C = A * B\n        C = A / B\n\n        # Inplace\n        A.requires_grad_(False)\n        D = SparseTensor(torch.rand(feats.shape), coords_key=A.coords_key)\n        A -= D\n        A *= D\n        A /= D\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/strided_conv.py,3,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport os\nimport argparse\nimport numpy as np\nfrom urllib.request import urlretrieve\ntry:\n    import open3d as o3d\nexcept ImportError:\n    raise ImportError(\'Please install open3d with `pip install open3d`.\')\n\nimport torch\nimport MinkowskiEngine as ME\nfrom examples.common import Timer\n\n# Check if the weights and file exist and download\nif not os.path.isfile(\'1.ply\'):\n    print(\'Downloading a room ply file...\')\n    urlretrieve(""http://cvgl.stanford.edu/data2/minkowskiengine/1.ply"", \'1.ply\')\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--file_name\', type=str, default=\'1.ply\')\nparser.add_argument(\'--voxel_size\', type=float, default=0.02)\nparser.add_argument(\'--batch_size\', type=int, default=1)\nparser.add_argument(\'--max_kernel_size\', type=int, default=7)\n\n\ndef load_file(file_name, voxel_size):\n    pcd = o3d.io.read_point_cloud(file_name)\n    coords = np.array(pcd.points)\n    feats = np.array(pcd.colors)\n\n    quantized_coords = np.floor(coords / voxel_size)\n    inds = ME.utils.sparse_quantize(quantized_coords, return_index=True)\n\n    return quantized_coords[inds], feats[inds], pcd\n\n\ndef generate_input_sparse_tensor(file_name, voxel_size=0.05, batch_size=1):\n    # Create a batch, this process is done in a data loader during training in parallel.\n    batch = [\n        load_file(file_name, voxel_size),\n    ] * batch_size\n    coordinates_, featrues_, pcds = list(zip(*batch))\n    coordinates, features = ME.utils.sparse_collate(coordinates_, featrues_)\n\n    # Normalize features and create a sparse tensor\n    return features, coordinates\n\n\nif __name__ == \'__main__\':\n    config = parser.parse_args()\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n    # Define a model and load the weights\n    all_convs = {}\n    for k in range(3, config.max_kernel_size + 1, 2):\n        for in_ch in [3, 8, 16, 32, 64, 128]:\n            for out_ch in [16, 32, 64, 128, 256]:\n                all_convs[(k, in_ch, out_ch)] = ME.MinkowskiConvolution(\n                    in_channels=in_ch,\n                    out_channels=out_ch,\n                    kernel_size=k,\n                    stride=2,\n                    dimension=3).to(device)\n\n    # Measure time\n    print(\'Initialization time\')\n    features, coordinates = generate_input_sparse_tensor(\n        config.file_name,\n        voxel_size=config.voxel_size,\n        batch_size=config.batch_size)\n\n    timer = Timer()\n    for i in range(20):\n        timer.tic()\n        sinput = ME.SparseTensor(features, coords=coordinates).to(device)\n        timer.toc()\n\n    print(f\'{timer.min_time:.12f} for initialization of {len(sinput)} voxels\')\n\n    print(\'Forward\')\n    for k, conv in all_convs.items():\n        timer = Timer()\n        features = torch.rand(len(coordinates), k[1]).to(device)\n\n        # Feed-forward pass and get the prediction\n        for i in range(20):\n            sinput = ME.SparseTensor(features, coords=coordinates).to(device)\n\n            timer.tic()\n            soutput = conv(sinput)\n            timer.toc()\n        print(\n            f\'{timer.min_time:.12f} for {k} strided convolution with {len(sinput)} voxel\'\n        )\n\n    print(\'Backward\')\n    for k, conv in all_convs.items():\n        timer = Timer()\n        sinput._F = torch.rand(len(sinput), k[1]).to(device)\n\n        soutput = conv(sinput)\n        loss = soutput.F.sum()\n        # Feed-forward pass and get the prediction\n        for i in range(20):\n            timer.tic()\n            loss.backward()\n            timer.toc()\n        print(\n            f\'{timer.min_time:.12f} for {k} strided convolution with {len(sinput)} voxel\'\n        )\n'"
tests/union.py,8,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport torch\nimport unittest\n\nimport MinkowskiEngine as ME\nfrom MinkowskiEngine import SparseTensor, MinkowskiUnion\n\n\nclass TestUnion(unittest.TestCase):\n\n    def test_union(self):\n        coords1 = torch.IntTensor([[0, 0], [0, 1]])\n        coords2 = torch.IntTensor([[0, 1], [1, 1]])\n        feats1 = torch.DoubleTensor([[1], [2]])\n        feats2 = torch.DoubleTensor([[3], [4]])\n        input1 = SparseTensor(coords=ME.utils.batched_coordinates([coords1]), feats=feats1)\n\n        input2 = SparseTensor(\n            feats=feats2,\n            coords=ME.utils.batched_coordinates([coords2]),\n            coords_manager=input1.coords_man,  # Must use same coords manager\n            force_creation=True  # The tensor stride [1, 1] already exists.\n        )\n\n        input1.requires_grad_()\n        input2.requires_grad_()\n        union = MinkowskiUnion()\n        output = union(input1, input2)\n        print(output)\n\n        self.assertTrue(len(output) == 3)\n        self.assertTrue(5 in output.F)\n        output.F.sum().backward()\n\n        # Grad of sum feature is 1.\n        self.assertTrue(torch.prod(input1.F.grad) == 1)\n        self.assertTrue(torch.prod(input2.F.grad) == 1)\n\n        device = torch.device(\'cuda\')\n        with torch.cuda.device(0):\n            input1, input2 = input1.to(device), input2.to(device)\n            output = union(input1, input2)\n\n            output.F.sum().backward()\n            print(output)\n            self.assertTrue(len(output) == 3)\n            self.assertTrue(5 in output.F)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
MinkowskiEngine/modules/__init__.py,0,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\n'"
MinkowskiEngine/modules/resnet_block.py,1,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport torch.nn as nn\n\nimport MinkowskiEngine as ME\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 stride=1,\n                 dilation=1,\n                 downsample=None,\n                 bn_momentum=0.1,\n                 dimension=-1):\n        super(BasicBlock, self).__init__()\n        assert dimension > 0\n\n        self.conv1 = ME.MinkowskiConvolution(\n            inplanes, planes, kernel_size=3, stride=stride, dilation=dilation, dimension=dimension)\n        self.norm1 = ME.MinkowskiBatchNorm(planes, momentum=bn_momentum)\n        self.conv2 = ME.MinkowskiConvolution(\n            planes, planes, kernel_size=3, stride=1, dilation=dilation, dimension=dimension)\n        self.norm2 = ME.MinkowskiBatchNorm(planes, momentum=bn_momentum)\n        self.relu = ME.MinkowskiReLU(inplace=True)\n        self.downsample = downsample\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.norm1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.norm2(out)\n\n        if self.downsample is not None:\n          residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 stride=1,\n                 dilation=1,\n                 downsample=None,\n                 bn_momentum=0.1,\n                 dimension=-1):\n        super(Bottleneck, self).__init__()\n        assert dimension > 0\n\n        self.conv1 = ME.MinkowskiConvolution(\n            inplanes, planes, kernel_size=1, dimension=dimension)\n        self.norm1 = ME.MinkowskiBatchNorm(planes, momentum=bn_momentum)\n\n        self.conv2 = ME.MinkowskiConvolution(\n            planes, planes, kernel_size=3, stride=stride, dilation=dilation, dimension=dimension)\n        self.norm2 = ME.MinkowskiBatchNorm(planes, momentum=bn_momentum)\n\n        self.conv3 = ME.MinkowskiConvolution(\n            planes, planes * self.expansion, kernel_size=1, dimension=dimension)\n        self.norm3 = ME.MinkowskiBatchNorm(\n            planes * self.expansion, momentum=bn_momentum)\n\n        self.relu = ME.MinkowskiReLU(inplace=True)\n        self.downsample = downsample\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.norm1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.norm2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.norm3(out)\n\n        if self.downsample is not None:\n          residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n'"
MinkowskiEngine/modules/senet_block.py,1,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport torch.nn as nn\n\nimport MinkowskiEngine as ME\n\nfrom MinkowskiEngine.modules.resnet_block import BasicBlock, Bottleneck\n\n\nclass SELayer(nn.Module):\n\n    def __init__(self, channel, reduction=16, D=-1):\n        # Global coords does not require coords_key\n        super(SELayer, self).__init__()\n        self.fc = nn.Sequential(\n            ME.MinkowskiLinear(channel, channel // reduction),\n            ME.MinkowskiReLU(inplace=True),\n            ME.MinkowskiLinear(channel // reduction, channel),\n            ME.MinkowskiSigmoid())\n        self.pooling = ME.MinkowskiGlobalPooling()\n        self.broadcast_mul = ME.MinkowskiBroadcastMultiplication()\n\n    def forward(self, x):\n        y = self.pooling(x)\n        y = self.fc(y)\n        return self.broadcast_mul(x, y)\n\n\nclass SEBasicBlock(BasicBlock):\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 stride=1,\n                 dilation=1,\n                 downsample=None,\n                 reduction=16,\n                 D=-1):\n        super(SEBasicBlock, self).__init__(\n            inplanes,\n            planes,\n            stride=stride,\n            dilation=dilation,\n            downsample=downsample,\n            D=D)\n        self.se = SELayer(planes, reduction=reduction, D=D)\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.norm1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.norm2(out)\n        out = self.se(out)\n\n        if self.downsample is not None:\n          residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBottleneck(Bottleneck):\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 stride=1,\n                 dilation=1,\n                 downsample=None,\n                 D=3,\n                 reduction=16):\n        super(SEBottleneck, self).__init__(\n            inplanes,\n            planes,\n            stride=stride,\n            dilation=dilation,\n            downsample=downsample,\n            D=D)\n        self.se = SELayer(planes * self.expansion, reduction=reduction, D=D)\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.norm1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.norm2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.norm3(out)\n        out = self.se(out)\n\n        if self.downsample is not None:\n          residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n'"
MinkowskiEngine/utils/__init__.py,0,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nfrom .quantization import sparse_quantize, ravel_hash_vec, fnv_hash_vec\nfrom .collation import SparseCollation, batched_coordinates, sparse_collate, batch_sparse_collate\nfrom .gradcheck import gradcheck\nfrom .coords import get_coords_map\nfrom .init import kaiming_normal_\n'"
MinkowskiEngine/utils/collation.py,21,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport numpy as np\nimport torch\nimport logging\nimport collections.abc\n\n\ndef batched_coordinates(coords):\n    r""""""Create a `ME.SparseTensor` coordinates from a sequence of coordinates\n\n    Given a list of either numpy or pytorch tensor coordinates, return the\n    batched coordinates suitable for `ME.SparseTensor`.\n\n    Args:\n        :attr:`coords` (a sequence of `torch.Tensor` or `numpy.ndarray`): a\n        list of coordinates.\n\n    Returns:\n        :attr:`coords` (`torch.IntTensor`): a batched coordinates.\n\n    .. warning::\n\n       From v0.4, the batch index will be prepended before all coordinates.\n\n    """"""\n    assert isinstance(\n        coords, collections.abc.Sequence), ""The coordinates must be a sequence.""\n    assert np.array([cs.ndim == 2 for cs in coords]).all(), \\\n        ""All coordinates must be in a 2D array.""\n    D = np.unique(np.array([cs.shape[1] for cs in coords]))\n    assert len(D) == 1, f""Dimension of the array mismatch. All dimensions: {D}""\n    D = D[0]\n\n    # Create a batched coordinates\n    N = np.array([len(cs) for cs in coords]).sum()\n    bcoords = torch.IntTensor(N, D + 1)  # uninitialized\n\n    s = 0\n    for b, cs in enumerate(coords):\n        if isinstance(cs, np.ndarray):\n            cs = torch.from_numpy(np.floor(cs).astype(np.int32))\n        else:\n            if isinstance(cs, torch.IntTensor) or isinstance(cs, torch.LongTensor):\n                cs = cs\n            else:\n                cs = cs.floor()\n\n        cn = len(cs)\n        # BATCH_FIRST:\n        bcoords[s:s + cn, 1:] = cs.int()\n        bcoords[s:s + cn, 0] = b\n        s += cn\n    return bcoords\n\n\ndef sparse_collate(coords, feats, labels=None):\n    r""""""Create input arguments for a sparse tensor `the documentation\n    <https://stanfordvl.github.io/MinkowskiEngine/sparse_tensor.html>`_.\n\n    Convert a set of coordinates and features into the batch coordinates and\n    batch features.\n\n    Args:\n        :attr:`coords` (set of `torch.Tensor` or `numpy.ndarray`): a set of coordinates.\n\n        :attr:`feats` (set of `torch.Tensor` or `numpy.ndarray`): a set of features.\n\n        :attr:`labels` (set of `torch.Tensor` or `numpy.ndarray`): a set of labels\n        associated to the inputs.\n\n    """"""\n    use_label = False if labels is None else True\n    feats_batch, labels_batch = [], []\n    assert isinstance(coords, collections.abc.Sequence), \\\n            ""The coordinates must be a sequence of arrays or tensors.""\n    assert isinstance(feats, collections.abc.Sequence), \\\n            ""The features must be a sequence of arrays or tensors.""\n    D = np.unique(np.array([cs.shape[1] for cs in coords]))\n    assert len(D) == 1, f""Dimension of the array mismatch. All dimensions: {D}""\n    D = D[0]\n\n    if use_label:\n        assert isinstance(labels, collections.abc.Sequence), \\\n            ""The labels must be a sequence of arrays or tensors.""\n\n    N = np.array([len(cs) for cs in coords]).sum()\n    Nf = np.array([len(fs) for fs in feats]).sum()\n    assert N == Nf, f""Coordinate length {N} != Feature length {Nf}""\n\n    batch_id = 0\n    s = 0  # start index\n    bcoords = torch.IntTensor(N, D + 1)  # uninitialized batched coords\n    for coord, feat in zip(coords, feats):\n        if isinstance(coord, np.ndarray):\n            coord = torch.from_numpy(coord)\n        else:\n            assert isinstance(coord, torch.Tensor), \\\n                ""Coords must be of type numpy.ndarray or torch.Tensor""\n        coord = coord.int()\n\n        if isinstance(feat, np.ndarray):\n            feat = torch.from_numpy(feat)\n        else:\n            assert isinstance(feat, torch.Tensor), \\\n                ""Features must be of type numpy.ndarray or torch.Tensor""\n\n        # Labels\n        if use_label:\n            label = labels[batch_id]\n            if isinstance(label, np.ndarray):\n                label = torch.from_numpy(label)\n            labels_batch.append(label)\n\n        cn = coord.shape[0]\n        # Batched coords\n        bcoords[s:s + cn, 1:] = coord\n        bcoords[s:s + cn, 0] = batch_id\n\n        # Features\n        feats_batch.append(feat)\n\n        # Post processing steps\n        batch_id += 1\n        s += cn\n\n    # Concatenate all lists\n    feats_batch = torch.cat(feats_batch, 0)\n    if use_label:\n        if isinstance(labels_batch[0], torch.Tensor):\n            labels_batch = torch.cat(labels_batch, 0)\n        return bcoords, feats_batch, labels_batch\n    else:\n        return bcoords, feats_batch\n\n\ndef batch_sparse_collate(data):\n    r""""""The wrapper function that can be used in in conjunction with\n    `torch.utils.data.DataLoader` to generate inputs for a sparse tensor.\n\n    Please refer to `the training example\n    <https://stanfordvl.github.io/MinkowskiEngine/demo/training.html>`_ for the\n    usage.\n\n    Args:\n        :attr:`data`: list of (coordinates, features, labels) tuples.\n\n    """"""\n    return sparse_collate(*list(zip(*data)))\n\n\nclass SparseCollation:\n    r""""""Generates collate function for coords, feats, labels.\n\n    Please refer to `the training example\n    <https://stanfordvl.github.io/MinkowskiEngine/demo/training.html>`_ for the\n    usage.\n\n    Args:\n        :attr:`limit_numpoints` (int): If positive integer, limits batch size\n        so that the number of input coordinates is below limit_numpoints. If 0\n        or False, concatenate all points. -1 by default.\n\n    Example::\n\n        >>> data_loader = torch.utils.data.DataLoader(\n        >>>     dataset,\n        >>>     ...,\n        >>>     collate_fn=SparseCollation())\n        >>> for d in iter(data_loader):\n        >>>     print(d)\n\n    """"""\n\n    def __init__(self, limit_numpoints=-1):\n        self.limit_numpoints = limit_numpoints\n\n    def __call__(self, list_data):\n        coords, feats, labels = list(zip(*list_data))\n        coords_batch, feats_batch, labels_batch = [], [], []\n\n        batch_num_points = 0\n        for batch_id, _ in enumerate(coords):\n            num_points = coords[batch_id].shape[0]\n            batch_num_points += num_points\n            if self.limit_numpoints > 0 and batch_num_points > self.limit_numpoints:\n                num_full_points = sum(len(c) for c in coords)\n                num_full_batch_size = len(coords)\n                logging.warning(\n                    f\'\\tCannot fit {num_full_points} points into\'\n                    \' {self.limit_numpoints} points limit. Truncating batch \'\n                    f\'size at {batch_id} out of {num_full_batch_size} with \'\n                    f\'{batch_num_points - num_points}.\')\n                break\n            coords_batch.append(coords[batch_id])\n            feats_batch.append(feats[batch_id])\n            labels_batch.append(labels[batch_id])\n\n        # Concatenate all lists\n        return sparse_collate(coords_batch, feats_batch, labels_batch)\n'"
MinkowskiEngine/utils/coords.py,2,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nfrom SparseTensor import SparseTensor\n\n\ndef get_coords_map(x, y):\n    r""""""Get mapping between sparse tensor 1 and sparse tensor 2.\n\n    Args:\n        :attr:`x` (:attr:`MinkowskiEngine.SparseTensor`): a sparse tensor with\n        `x.tensor_stride` <= `y.tensor_stride`.\n\n        :attr:`y` (:attr:`MinkowskiEngine.SparseTensor`): a sparse tensor with\n        `x.tensor_stride` <= `y.tensor_stride`.\n\n    Returns:\n        :attr:`x_indices` (:attr:`torch.LongTensor`): the indices of x that\n        corresponds to the returned indices of y.\n\n        :attr:`x_indices` (:attr:`torch.LongTensor`): the indices of y that\n        corresponds to the returned indices of x.\n\n    Example::\n\n        .. code-block:: python\n\n           sp_tensor = ME.SparseTensor(features, coords=coordinates)\n           out_sp_tensor = stride_2_conv(sp_tensor)\n\n           ins, outs = get_coords_map(sp_tensor, out_sp_tensor)\n           for i, o in zip(ins, outs):\n              print(f""{i} -> {o}"")\n\n    """"""\n    assert isinstance(x, SparseTensor)\n    assert isinstance(y, SparseTensor)\n    assert x.coords_man == y.coords_man, ""X and Y are using different CoordinateManagers. Y must be derived from X through strided conv/pool/etc.""\n    return x.coords_man.get_coords_map(x.coords_key, y.coords_key)\n'"
MinkowskiEngine/utils/gradcheck.py,14,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport torch\nimport torch.testing\nimport warnings\nfrom typing import Callable, Union, Optional\n\nfrom torch.autograd.gradcheck import _as_tuple, _differentiable_outputs, get_analytical_jacobian, get_numerical_jacobian, iter_tensors\n\n\ndef gradcheck(\n    func,\n    inputs,\n    eps: float = 1e-6,\n    atol: float = 1e-5,\n    rtol: float = 1e-3,\n    raise_exception: bool = True,\n    check_sparse_nnz: bool = False,\n    nondet_tol: float = 0.0\n) -> bool:\n    r""""""Check gradients computed via small finite differences against analytical\n    gradients w.r.t. tensors in :attr:`inputs` that are of floating point or complex type\n    and with ``requires_grad=True``.\n    The check between numerical and analytical gradients uses :func:`~torch.allclose`.\n    .. note::\n        The default values are designed for :attr:`input` of double precision.\n        This check will likely fail if :attr:`input` is of less precision, e.g.,\n        ``FloatTensor``.\n    .. warning::\n       If any checked tensor in :attr:`input` has overlapping memory, i.e.,\n       different indices pointing to the same memory address (e.g., from\n       :func:`torch.expand`), this check will likely fail because the numerical\n       gradients computed by point perturbation at such indices will change\n       values at all other indices that share the same memory address.\n    Args:\n        func (function): a Python function that takes Tensor inputs and returns\n            a Tensor or a tuple of Tensors\n        inputs (tuple of Tensor or Tensor): inputs to the function\n        eps (float, optional): perturbation for finite differences\n        atol (float, optional): absolute tolerance\n        rtol (float, optional): relative tolerance\n        raise_exception (bool, optional): indicating whether to raise an exception if\n            the check fails. The exception gives more information about the\n            exact nature of the failure. This is helpful when debugging gradchecks.\n        check_sparse_nnz (bool, optional): if True, gradcheck allows for SparseTensor input,\n            and for any SparseTensor at input, gradcheck will perform check at nnz positions only.\n        nondet_tol (float, optional): tolerance for non-determinism. When running\n            identical inputs through the differentiation, the results must either match\n            exactly (default, 0.0) or be within this tolerance.\n    Returns:\n        True if all differences satisfy allclose condition\n    """"""\n    def fail_test(msg):\n        if raise_exception:\n            raise RuntimeError(msg)\n        return False\n\n    tupled_inputs = _as_tuple(inputs)\n    if any(t.is_sparse for t in tupled_inputs if isinstance(t, torch.Tensor)) and not check_sparse_nnz:\n        return fail_test(\'gradcheck expects all tensor inputs are dense when check_sparse_nnz is set to False.\')\n\n    # Make sure that gradients are saved for at least one input\n    any_input_requiring_grad = False\n    for idx, inp in enumerate(tupled_inputs):\n        if isinstance(inp, torch.Tensor) and inp.requires_grad:\n            if not (inp.dtype == torch.float64 or inp.dtype == torch.complex128):\n                warnings.warn(\n                    \'The {}th input requires gradient and \'\n                    \'is not a double precision floating point or complex. \'\n                    \'This check will likely fail if all the inputs are \'\n                    \'not of double precision floating point or complex. \')\n            content = inp._values() if inp.is_sparse else inp\n            if content.layout is not torch._mkldnn and any([s == 0 for s in content.stride()]):\n                raise RuntimeError(\n                    \'The {}th input has a dimension with stride 0. gradcheck only \'\n                    \'supports inputs that are non-overlapping to be able to \'\n                    \'compute the numerical gradients correctly. You should call \'\n                    \'.contiguous on the input before passing it to gradcheck.\')\n            any_input_requiring_grad = True\n            inp.retain_grad()\n    if not any_input_requiring_grad:\n        raise ValueError(\n            \'gradcheck expects at least one input tensor to require gradient, \'\n            \'but none of the them have requires_grad=True.\')\n\n    func_out = func.apply(*tupled_inputs)\n    output = _differentiable_outputs(func_out)\n\n    if not output:\n        for i, o in enumerate(func_out):\n            def fn(input):\n                return _as_tuple(func.apply(*input))[i]\n            numerical = get_numerical_jacobian(fn, tupled_inputs, eps=eps)\n            for n in numerical:\n                if torch.ne(n, 0).sum() > 0:\n                    return fail_test(\'Numerical gradient for function expected to be zero\')\n        return True\n\n    for i, o in enumerate(output):\n        if not o.requires_grad:\n            continue\n\n        def fn(input):\n            return _as_tuple(func.apply(*input))[i]\n\n        analytical, reentrant, correct_grad_sizes = get_analytical_jacobian(tupled_inputs, o, nondet_tol=nondet_tol)\n        numerical = get_numerical_jacobian(fn, tupled_inputs, eps=eps)\n\n        if not correct_grad_sizes:\n            return fail_test(\'Analytical gradient has incorrect size\')\n\n        for j, (a, n) in enumerate(zip(analytical, numerical)):\n            if a.numel() != 0 or n.numel() != 0:\n                if not torch.allclose(a, n, rtol, atol):\n                    return fail_test(\'Jacobian mismatch for output %d with respect to input %d,\\n\'\n                                     \'numerical:%s\\nanalytical:%s\\n\' % (i, j, n, a))\n\n        if not reentrant:\n            return fail_test(\'Backward is not reentrant, i.e., running backward with same \'\n                             \'input and grad_output multiple times gives different values, \'\n                             \'although analytical gradient matches numerical gradient. \'\n                             \'The tolerance for nondeterminism was {}.\'.format(nondet_tol))\n\n    # check if the backward multiplies by grad_output\n    output = _differentiable_outputs(func.apply(*tupled_inputs))\n    if any([o.requires_grad for o in output]):\n        diff_input_list = list(iter_tensors(tupled_inputs, True))\n        if not diff_input_list:\n            raise RuntimeError(""no Tensors requiring grad found in input"")\n        grads_input = torch.autograd.grad(output, diff_input_list,\n                                          [torch.zeros_like(o, memory_format=torch.legacy_contiguous_format) for o in output],\n                                          allow_unused=True)\n        for gi, i in zip(grads_input, diff_input_list):\n            if gi is None:\n                continue\n            if isinstance(gi, torch.Tensor) and gi.layout != torch.strided:\n                if gi.layout != i.layout:\n                    return fail_test(\'grad is incorrect layout (\' + str(gi.layout) + \' is not \' + str(i.layout) + \')\')\n                if gi.layout == torch.sparse_coo:\n                    if gi.sparse_dim() != i.sparse_dim():\n                        return fail_test(\'grad is sparse tensor, but has incorrect sparse_dim\')\n                    if gi.dense_dim() != i.dense_dim():\n                        return fail_test(\'grad is sparse tensor, but has incorrect dense_dim\')\n                gi = gi.to_dense()\n                i = i.to_dense()\n            if not gi.eq(0).all():\n                return fail_test(\'backward not multiplied by grad_output\')\n            if gi.dtype != i.dtype or gi.device != i.device or gi.is_sparse != i.is_sparse:\n                return fail_test(""grad is incorrect type"")\n            if gi.size() != i.size():\n                return fail_test(\'grad is incorrect size\')\n\n    return True\n'"
MinkowskiEngine/utils/init.py,2,"b'import math\nimport torch\n\n\ndef _calculate_fan_in_and_fan_out(tensor):\n    dimensions = tensor.dim()\n    if dimensions < 2:\n        raise ValueError(\n            ""Fan in and fan out can not be computed for tensor with fewer than 2 dimensions""\n        )\n\n    if dimensions == 2:  # Linear\n        fan_in = tensor.size(1)\n        fan_out = tensor.size(0)\n    else:\n        num_input_fmaps = tensor.size(1)\n        num_output_fmaps = tensor.size(2)\n        receptive_field_size = tensor.size(0)\n        fan_in = num_input_fmaps * receptive_field_size\n        fan_out = num_output_fmaps * receptive_field_size\n\n    return fan_in, fan_out\n\n\ndef _calculate_correct_fan(tensor, mode):\n    mode = mode.lower()\n    valid_modes = [\'fan_in\', \'fan_out\']\n    if mode not in valid_modes:\n        raise ValueError(""Mode {} not supported, please use one of {}"".format(\n            mode, valid_modes))\n\n    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)\n    return fan_in if mode == \'fan_in\' else fan_out\n\n\ndef kaiming_normal_(tensor, a=0, mode=\'fan_in\', nonlinearity=\'leaky_relu\'):\n    fan = _calculate_correct_fan(tensor, mode)\n    gain = torch.nn.init.calculate_gain(nonlinearity, a)\n    std = gain / math.sqrt(fan)\n    with torch.no_grad():\n        return tensor.normal_(0, std)\n'"
MinkowskiEngine/utils/quantization.py,20,"b'# Copyright (c) Chris Choy (chrischoy@ai.stanford.edu).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the ""Software""), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Please cite ""4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural\n# Networks"", CVPR\'19 (https://arxiv.org/abs/1904.08755) if you use any part\n# of the code.\nimport torch\nimport numpy as np\nfrom collections import Sequence\nimport MinkowskiEngineBackend as MEB\n\n\ndef fnv_hash_vec(arr):\n    """"""\n    FNV64-1A\n    """"""\n    assert arr.ndim == 2\n    # Floor first for negative coordinates\n    arr = arr.copy()\n    arr = arr.astype(np.uint64, copy=False)\n    hashed_arr = np.uint64(14695981039346656037) * \\\n        np.ones(arr.shape[0], dtype=np.uint64)\n    for j in range(arr.shape[1]):\n        hashed_arr *= np.uint64(1099511628211)\n        hashed_arr = np.bitwise_xor(hashed_arr, arr[:, j])\n    return hashed_arr\n\n\ndef ravel_hash_vec(arr):\n    """"""\n    Ravel the coordinates after subtracting the min coordinates.\n    """"""\n    assert arr.ndim == 2\n    arr = arr.copy()\n    arr -= arr.min(0)\n    arr = arr.astype(np.uint64, copy=False)\n    arr_max = arr.max(0).astype(np.uint64) + 1\n\n    keys = np.zeros(arr.shape[0], dtype=np.uint64)\n    # Fortran style indexing\n    for j in range(arr.shape[1] - 1):\n        keys += arr[:, j]\n        keys *= arr_max[j + 1]\n    keys += arr[:, -1]\n    return keys\n\n\ndef quantize(coords):\n    r""""""Returns a unique index map and an inverse index map.\n\n    Args:\n        :attr:`coords` (:attr:`numpy.ndarray` or :attr:`torch.Tensor`): a\n        matrix of size :math:`N \\times D` where :math:`N` is the number of\n        points in the :math:`D` dimensional space.\n\n    Returns:\n        :attr:`unique_map` (:attr:`numpy.ndarray` or :attr:`torch.Tensor`): a\n        list of indices that defines unique coordinates.\n        :attr:`coords[unique_map]` is the unique coordinates.\n\n        :attr:`inverse_map` (:attr:`numpy.ndarray` or :attr:`torch.Tensor`): a\n        list of indices that defines the inverse map that recovers the original\n        coordinates.  :attr:`coords[unique_map[inverse_map]] == coords`\n\n    Example::\n\n       >>> unique_map, inverse_map = quantize(coords)\n       >>> unique_coords = coords[unique_map]\n       >>> print(unique_coords[inverse_map] == coords)  # True, ..., True\n       >>> print(coords[unique_map[inverse_map]] == coords)  # True, ..., True\n\n    """"""\n    assert isinstance(coords, np.ndarray) or isinstance(coords, torch.Tensor), \\\n        ""Invalid coords type""\n    if isinstance(coords, np.ndarray):\n        assert coords.dtype == np.int32, f""Invalid coords type {coords.dtype} != np.int32""\n        return MEB.quantize_np(coords.astype(np.int32))\n    else:\n        # Type check done inside\n        return MEB.quantize_th(coords.int())\n\n\ndef quantize_label(coords, labels, ignore_label):\n    assert isinstance(coords, np.ndarray) or isinstance(coords, torch.Tensor), \\\n        ""Invalid coords type""\n    if isinstance(coords, np.ndarray):\n        assert isinstance(labels, np.ndarray)\n        assert coords.dtype == np.int32, f""Invalid coords type {coords.dtype} != np.int32""\n        assert labels.dtype == np.int32, f""Invalid label type {labels.dtype} != np.int32""\n        return MEB.quantize_label_np(coords, labels, ignore_label)\n    else:\n        assert isinstance(labels, torch.Tensor)\n        # Type check done inside\n        return MEB.quantize_label_th(coords, labels.int(), ignore_label)\n\n\ndef sparse_quantize(coords,\n                    feats=None,\n                    labels=None,\n                    ignore_label=-100,\n                    return_index=False,\n                    return_inverse=False,\n                    quantization_size=None):\n    r""""""Given coordinates, and features (optionally labels), the function\n    generates quantized (voxelized) coordinates.\n\n    Args:\n        :attr:`coords` (:attr:`numpy.ndarray` or :attr:`torch.Tensor`): a\n        matrix of size :math:`N \\times D` where :math:`N` is the number of\n        points in the :math:`D` dimensional space.\n\n        :attr:`feats` (:attr:`numpy.ndarray` or :attr:`torch.Tensor`, optional): a\n        matrix of size :math:`N \\times D_F` where :math:`N` is the number of\n        points and :math:`D_F` is the dimension of the features. Must have the\n        same container as `coords` (i.e. if `coords` is a torch.Tensor, `feats`\n        must also be a torch.Tensor).\n\n        :attr:`labels` (:attr:`numpy.ndarray` or :attr:`torch.IntTensor`,\n        optional): integer labels associated to eah coordinates.  Must have the\n        same container as `coords` (i.e. if `coords` is a torch.Tensor,\n        `labels` must also be a torch.Tensor). For classification where a set\n        of points are mapped to one label, do not feed the labels.\n\n        :attr:`ignore_label` (:attr:`int`, optional): the int value of the\n        IGNORE LABEL.\n        :attr:`torch.nn.CrossEntropyLoss(ignore_index=ignore_label)`\n\n        :attr:`return_index` (:attr:`bool`, optional): set True if you want the\n        indices of the quantized coordinates. False by default.\n\n        :attr:`return_inverse` (:attr:`bool`, optional): set True if you want\n        the indices that can recover the discretized original coordinates.\n        False by default. `return_index` must be True when `return_reverse` is True.\n\n        Example::\n\n           >>> unique_map, inverse_map = sparse_quantize(discrete_coords, return_index=True, return_inverse=True)\n           >>> unique_coords = discrete_coords[unique_map]\n           >>> print(unique_coords[inverse_map] == discrete_coords)  # True\n\n        :attr:`quantization_size` (:attr:`float`, :attr:`list`, or\n        :attr:`numpy.ndarray`, optional): the length of the each side of the\n        hyperrectangle of of the grid cell.\n\n     Example::\n\n        >>> # Segmentation\n        >>> criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n        >>> coords, feats, labels = MinkowskiEngine.utils.sparse_quantize(\n        >>>     coords, feats, labels, ignore_label=-100, quantization_size=0.1)\n        >>> output = net(MinkowskiEngine.SparseTensor(feats, coords))\n        >>> loss = criterion(output.F, labels.long())\n        >>>\n        >>> # Classification\n        >>> criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n        >>> coords, feats = MinkowskiEngine.utils.sparse_quantize(coords, feats)\n        >>> output = net(MinkowskiEngine.SparseTensor(feats, coords))\n        >>> loss = criterion(output.F, labels.long())\n\n\n    """"""\n    assert isinstance(coords, np.ndarray) or isinstance(coords, torch.Tensor), \\\n        \'Coords must be either np.array or torch.Tensor.\'\n\n    use_label = labels is not None\n    use_feat = feats is not None\n\n    assert coords.ndim == 2, \\\n        ""The coordinates must be a 2D matrix. The shape of the input is "" + str(coords.shape)\n\n    if return_inverse:\n        assert return_index, ""return_reverse must be set with return_index""\n\n    if use_feat:\n        assert feats.ndim == 2\n        assert coords.shape[0] == feats.shape[0]\n\n    if use_label:\n        assert coords.shape[0] == len(labels)\n\n    dimension = coords.shape[1]\n    # Quantize the coordinates\n    if quantization_size is not None:\n        if isinstance(quantization_size, (Sequence, np.ndarray, torch.Tensor)):\n            assert len(\n                quantization_size\n            ) == dimension, ""Quantization size and coordinates size mismatch.""\n            if isinstance(coords, np.ndarray):\n                quantization_size = np.array([i for i in quantization_size])\n                discrete_coords = np.floor(coords / quantization_size)\n            else:\n                quantization_size = torch.Tensor([i for i in quantization_size])\n                discrete_coords = (coords / quantization_size).floor()\n\n        elif np.isscalar(quantization_size):  # Assume that it is a scalar\n\n            if quantization_size == 1:\n                discrete_coords = coords\n            else:\n                discrete_coords = np.floor(coords / quantization_size)\n        else:\n            raise ValueError(\'Not supported type for quantization_size.\')\n    else:\n        discrete_coords = coords\n\n    discrete_coords = np.floor(discrete_coords)\n    if isinstance(coords, np.ndarray):\n        discrete_coords = discrete_coords.astype(np.int32)\n    else:\n        discrete_coords = discrete_coords.int()\n\n    # Return values accordingly\n    if use_label:\n        mapping, colabels = quantize_label(discrete_coords, labels,\n                                           ignore_label)\n\n        if return_index:\n            return mapping, colabels\n        else:\n            if use_feat:\n                return discrete_coords[mapping], feats[mapping], colabels\n            else:\n                return discrete_coords[mapping], colabels\n\n    else:\n        unique_map, inverse_map = quantize(discrete_coords)\n        if return_index:\n            if return_inverse:\n                return unique_map, inverse_map\n            else:\n                return unique_map\n        else:\n            if use_feat:\n                return discrete_coords[unique_map], feats[unique_map]\n            else:\n                return discrete_coords[unique_map]\n'"
