file_path,api_count,code
sample.py,3,"b'""""""A minimal sample script for illustration of basic usage of NCE module""""""\n\nimport torch\nfrom nce import IndexLinear\n\nclass_freq = [1, 2, 3, 4, 5, 6, 7]  # an unigram class probability\nfreq_count = torch.FloatTensor(class_freq)\nnoise = freq_count / freq_count.sum()\n\nnce_linear = IndexLinear(\n    embedding_dim=100,  # input dim\n    num_classes=300000,  # output dim\n    noise=noise,\n)\n\ninput = torch.Tensor(200, 100)\ntarget = torch.ones(200, 1).long()\n# training mode\nloss = nce_linear(target, input).mean()\nprint(loss.item())\n\n# evaluation mode for fast probability computation\nnce_linear.eval()\nprob = nce_linear(target, input).mean()\nprint(prob.item())\n'"
setup.py,0,"b'import setuptools\n\n\nsetuptools.setup(\n    name=\'pytorch-nce\',\n    version=\'0.0.1\',\n    author=\'Kaiyu Shi\',\n    author_email=\'skyisno.1@gmail.com\',\n    description=\'An NCE implementation in pytorch\',\n    long_description=open(\'README.md\').read(),\n    long_description_content_type=\'text/markdown\',\n    url=\'https://github.com/Stonesjtu/Pytorch-NCE\',\n    packages=[\'nce\'],\n    classifiers=[\n        ""License :: OSI Approved :: MIT License"",\n        ""Operating System :: OS Independent"",\n    ],\n    install_requires=[\n        \'torch >= 1.0.0\',\n    ],\n)\n'"
example/data.py,4,"b'""""""data utils of this language model: corpus reader and noise data generator""""""\n\nimport os\n\nimport torch\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom tqdm import tqdm\n\nfrom vocab import get_vocab, BOS, EOS\n\n\nclass LMDataset(Dataset):\n    """"""dataset that zero-pads all sentence into same length\n\n    Attributes:\n        - vocab: Vocab object which holds the vocabulary info\n        - file_path: the directory of all train, test and valid corpus\n        - bptt: truncated BPTT length, items after such length will be\n        ignored\n\n    Parameters:\n        - vocab: a word-to-index mapping, will build a new one if\n        not provided\n    """"""\n\n    def __init__(self, file_path, vocab=None, bptt=35):\n        super(LMDataset, self).__init__()\n        self.vocab = vocab\n        self.file_path = file_path\n        self.bptt = bptt\n        self.tokenize(file_path)\n\n    def tokenize(self, path):\n        """"""Tokenizes a text file.""""""\n        assert os.path.exists(path)\n        with open(path, \'r\') as f:\n            sentences = []\n            for sentence in tqdm(f, desc=\'Processing file: {}\'.format(path)):\n                sentences.append(sentence.split())\n        self.data = sentences\n\n    def __getitem__(self, index):\n        raw_sentence = self.data[index]\n        # truncate the sequence length to maximum of BPTT\n        sentence = [BOS] + raw_sentence[:self.bptt] + [BOS]\n        return [self.vocab.word2idx[word] for word in sentence]\n\n    def __len__(self):\n        return len(self.data)\n\n\nclass ContLMDataset(LMDataset):\n    """"""dataset that cat the sentences into one long sequence and chunk\n\n    Each training sample is a chunked version and of same length.\n\n    Attributes:\n        - vocab: Vocab object which holds the vocabulary info\n        - file_path: the directory of all train, test and valid corpus\n        - bptt: sequence length\n    """"""\n\n    def tokenize(self, path):\n        """"""Tokenizes a text file.""""""\n        assert os.path.exists(path)\n        # add the start of sentence token\n        sentence_sep = [BOS]\n        with open(path, \'r\') as f:\n            sentences = [BOS]\n            for sentence in tqdm(f, desc=\'Processing file: {}\'.format(path)):\n                sentences += sentence.split() + sentence_sep\n        # split into list of tokens\n        self.data = sentences\n\n    def __getitem__(self, index):\n        sentence = self.data[index * self.bptt:(index + 1) * self.bptt]\n        return [self.vocab.word2idx[word] for word in sentence]\n\n    def __len__(self):\n        return len(self.data) // self.bptt\n\n\ndef pad_collate_fn(batch):\n    """"""Pad the list of word indexes into 2-D LongTensor""""""\n    length = [len(sentence) for sentence in batch]\n    return pad_sequence([torch.LongTensor(s) for s in batch], batch_first=True), torch.LongTensor(length)\n\n\nclass Corpus(object):\n    def __init__(self, path, vocab_path=None, batch_size=1, shuffle=False,\n                 pin_memory=False, update_vocab=False, min_freq=1,\n                 concat=False, bptt=35):\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.pin_memory = pin_memory\n        self.base_path = path\n        self.update_vocab = update_vocab\n        self.bptt = bptt\n        self.concat = concat\n\n        self.vocab = get_vocab(path, [\'train.txt\'], min_freq=min_freq, vocab_file=vocab_path)\n        if self.concat:\n            # set the frequencies for special tokens by miracle trial\n            self.vocab.idx2count[1] = self.vocab.freqs[BOS]  # <s>\n            self.vocab.idx2count[2] = 0  # </s>\n\n        self.train = self.get_dataloader(\'train.txt\', self.batch_size)\n        self.valid = self.get_dataloader(\'valid.txt\', self.batch_size)\n        self.test = self.get_dataloader(\'test.txt\', 1)\n\n    def get_dataloader(self, filename, bs=1):\n        full_path = os.path.join(self.base_path, filename)\n        if self.concat:\n            dataset = ContLMDataset(full_path, vocab=self.vocab, bptt=self.bptt)\n        else:\n            dataset = LMDataset(full_path, vocab=self.vocab, bptt=self.bptt)\n        return DataLoader(\n            dataset=dataset,\n            batch_size=bs,\n            shuffle=self.shuffle,\n            pin_memory=self.pin_memory,\n            collate_fn=pad_collate_fn,\n            # num_workers=1,\n            # waiting for a new torch version to support\n            # drop_last=True,\n        )\n'"
example/generic_model.py,2,"b'""""""Main container for common language model""""""\nimport torch\nimport torch.nn as nn\n\nfrom utils import get_mask\n\nclass GenModel(nn.Module):\n    """"""Container module with an encoder, a recurrent module, and a criterion (decoder and loss function).""""""\n\n    def __init__(self, criterion, dropout=0.2):\n        super(GenModel, self).__init__()\n        self.criterion = criterion\n\n    def forward(self, input, target, length):\n\n        mask = get_mask(length.data, cut_tail=0)\n\n        # <s> is non-sense in this model, thus the loss should be\n        # masked manually\n        effective_target = target[:, 1:].contiguous()\n        loss = self.criterion(effective_target, input)\n        loss = torch.masked_select(loss, mask)\n\n        return loss.mean()\n'"
example/main.py,10,"b'#!/usr/bin/env python\n\nimport sys\nimport time\nimport math\n\nfrom tqdm import tqdm\n\nimport torch\nimport torch.optim as optim\n\nimport data\nfrom model import RNNModel\nfrom utils import process_data, build_unigram_noise, setup_parser, setup_logger\nfrom generic_model import GenModel\nfrom nce import IndexGRU, IndexLinear\n\n\nparser = setup_parser()\nargs = parser.parse_args()\nlogger = setup_logger(\'{}\'.format(args.save))\nlogger.info(args)\nmodel_path = \'./saved_model/{}\'.format(args.save)\n\n# Set the random seed manually for reproducibility.\ntorch.manual_seed(args.seed)\nif torch.cuda.is_available():\n    if not args.cuda:\n        logger.warning(\'You have a CUDA device, so you should probably run with --cuda\')\n    else:\n        torch.cuda.manual_seed(args.seed)\n\n#################################################################\n# Load data\n#################################################################\ncorpus = data.Corpus(\n    path=args.data,\n    vocab_path=args.vocab,\n    batch_size=args.batch_size,\n    shuffle=True,\n    pin_memory=args.cuda,\n    min_freq=args.min_freq,\n    concat=args.concat,\n    bptt=args.bptt,\n)\n\nntoken = len(corpus.vocab)\nlogger.info(\'Vocabulary size is {}\'.format(ntoken))\n\n################################################################## Build the criterion and model, setup the NCE module\n#################################################################\n\ndef build_model():\n    """"""Build the model according to CLI arguments\n\n    Global Dependencies:\n        - corpus\n        - args\n    """"""\n    # noise for soise sampling in NCE\n    noise = build_unigram_noise(\n        torch.FloatTensor(corpus.vocab.idx2count)\n    )\n\n    norm_term = \'auto\' if args.norm_term == -1 else args.norm_term\n    # setting up NCELoss modules\n    if args.index_module == \'linear\':\n        criterion = IndexLinear(\n            args.emsize,\n            ntoken,\n            noise=noise,\n            noise_ratio=args.noise_ratio,\n            norm_term=norm_term,\n            loss_type=args.loss,\n            reduction=\'none\',\n        )\n        model = RNNModel(\n            ntoken, args.emsize, args.nhid, args.nlayers,\n            criterion=criterion, dropout=args.dropout,\n        )\n    elif args.index_module == \'gru\':\n        if args.nlayers != 1:\n            logger.warning(\'Falling into one layer GRU due to Index_GRU supporting\')\n        nce_criterion = IndexGRU(\n            ntoken, args.emsize, args.nhid,\n            args.dropout,\n            noise=noise,\n            noise_ratio=args.noise_ratio,\n            norm_term=norm_term,\n        )\n        model = GenModel(\n            criterion=nce_criterion,\n        )\n    else:\n        logger.error(\'The index module [%s] is not supported yet\' % args.index_module)\n        raise(NotImplementedError(\'index module not supported\'))\n\n    if args.cuda:\n        model.cuda()\n\n    logger.info(\'model definition:\\n %s\', model)\n    return model\n\nmodel = build_model()\nsep_target = args.index_module == \'linear\'\n#################################################################\n# Training code\n#################################################################\n\n\ndef train(model, data_source, epoch, lr=1.0, weight_decay=1e-5, momentum=0.9):\n    optimizer = optim.SGD(\n        params=model.parameters(),\n        lr=lr,\n        momentum=momentum,\n        weight_decay=weight_decay\n    )\n    # Turn on training mode which enables dropout.\n    model.train()\n    model.criterion.loss_type = args.loss\n    total_loss = 0\n    pbar = tqdm(data_source, desc=\'Training PPL: ....\')\n    for num_batch, data_batch in enumerate(pbar):\n        optimizer.zero_grad()\n        data, target, length = process_data(data_batch, cuda=args.cuda, sep_target=sep_target)\n        loss = model(data, target, length)\n        loss.backward()\n\n        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n        optimizer.step()\n\n        total_loss += loss.item()\n\n        if args.prof:\n            break\n        if num_batch % args.log_interval == 0 and num_batch > 0:\n            cur_loss = total_loss / args.log_interval\n            ppl = math.exp(cur_loss)\n            logger.debug(\n                \'| epoch {:3d} | {:5d}/{:5d} batches \'\n                \'| lr {:02.2f} | loss {:5.2f} | ppl {:8.2f}\'.format(\n                    epoch, num_batch, len(corpus.train),\n                    lr, cur_loss, ppl\n                  )\n            )\n            pbar.set_description(\'Training PPL %.1f\' % ppl)\n            total_loss = 0\n\ndef evaluate(model, data_source, cuda=args.cuda):\n    # Turn on evaluation mode which disables dropout.\n    model.eval()\n    model.criterion.loss_type = \'full\'\n\n    eval_loss = 0\n    total_length = 0\n\n    with torch.no_grad():\n        for data_batch in data_source:\n            data, target, length = process_data(data_batch, cuda=cuda, sep_target=sep_target)\n\n            loss = model(data, target, length)\n            cur_length = int(length.data.sum())\n            eval_loss += loss.item() * cur_length\n            total_length += cur_length\n\n    model.criterion.loss_type = args.loss\n\n    return math.exp(eval_loss/total_length)\n\n\ndef run_epoch(epoch, lr, best_val_ppl):\n    """"""A training epoch includes training, evaluation and logging""""""\n    epoch_start_time = time.time()\n    train(model, corpus.train, epoch=epoch, lr=lr, weight_decay=args.weight_decay)\n    val_ppl = evaluate(model, corpus.valid)\n    logger.warning(\n        \'| end of epoch {:3d} | time: {:5.2f}s |\'\n        \'valid ppl {:8.2f}\'.format(\n            epoch,\n            (time.time() - epoch_start_time),\n            val_ppl)\n    )\n    torch.save(model, model_path + \'.epoch_{}\'.format(epoch))\n    # Save the model if the validation loss is the best we\'ve seen so far.\n    if not best_val_ppl or val_ppl < best_val_ppl:\n        torch.save(model, model_path)\n        best_val_ppl = val_ppl\n    else:\n        # Anneal the learning rate if no improvement has been seen in the\n        # validation dataset.\n        lr /= args.lr_decay\n    return lr, best_val_ppl\n\nif __name__ == \'__main__\':\n    lr = args.lr\n    best_val_ppl = None\n    if args.train:\n        # At any point you can hit Ctrl + C to break out of training early.\n        try:\n            for epoch in range(1, args.epochs + 1):\n                lr, best_val_ppl = run_epoch(epoch, lr, best_val_ppl)\n                if args.prof:\n                    break\n        except KeyboardInterrupt:\n            logger.warning(\'Exiting from training early\')\n\n    else:\n        # Load the best saved model.\n        logger.warning(\'Evaluating existing model {}\'.format(args.save))\n        model = torch.load(model_path)\n\n    # Run on test data.\n    test_ppl = evaluate(model, corpus.test)\n    logger.warning(\'| End of training | test ppl {:8.2f}\'.format(test_ppl))\n    sys.stdout.flush()\n'"
example/model.py,2,"b'""""""Main container for common language model""""""\nimport torch\nimport torch.nn as nn\n\nfrom utils import get_mask\n\nclass RNNModel(nn.Module):\n    """"""Container module with an encoder, a recurrent module, and a criterion (decoder and loss function).""""""\n\n    def __init__(self, ntoken, ninp, nhid, nlayers, criterion, dropout=0.5):\n        super(RNNModel, self).__init__()\n        self.drop = nn.Dropout(dropout)\n        self.encoder = nn.Embedding(ntoken, ninp)\n        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout, batch_first=True)\n        # Usually we use the same # dim in both input and output embedding\n        self.proj = nn.Linear(nhid, ninp)\n\n        self.nhid = nhid\n        self.nlayers = nlayers\n        self.criterion = criterion\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        init_range = 0.1\n        self.encoder.weight.data.uniform_(-init_range, init_range)\n\n    def _rnn(self, input):\n        \'\'\'Serves as the encoder and recurrent layer\'\'\'\n        emb = self.drop(self.encoder(input))\n        output, unused_hidden = self.rnn(emb)\n        output = self.proj(output)\n        output = self.drop(output)\n        return output\n\n\n    def forward(self, input, target, length):\n\n        mask = get_mask(length.data, max_len=input.size(1))\n        rnn_output = self._rnn(input)\n        loss = self.criterion(target, rnn_output)\n        loss = torch.masked_select(loss, mask)\n\n        return loss.mean()\n'"
example/rescore.py,2,"b""import math\nimport sys\n\nimport tqdm\nimport torch\n\nfrom data import Corpus\nfrom utils import process_data\n\n\nMODEL_FILE = sys.argv[1]\nCORPUS_PATH = '../dataset/swb-rescore'\n# VOCAB_PATH = '../language_model/data/swb-bpe-tri/vocab.txt'\n#################################################################\n# Load data\n#################################################################\ncorpus = Corpus(\n    path=CORPUS_PATH,\n    # vocab_path=VOCAB_PATH,\n    min_freq=3,\n    batch_size=1,\n    shuffle=False,\n    pin_memory=True,\n    # concat=True,\n    bptt=120,\n)\n\n\nmodel = torch.load(MODEL_FILE)\n\nprint('vocabulary size: ', len(corpus.vocab.idx2word))\nprint('sample words: ', corpus.vocab.idx2word[:10])\n\n\ndata_source = corpus.test\n# Turn on evaluation mode which disables dropout.\nmodel.eval()\nmodel.criterion.loss_type = 'full'\nmodel.criterion.noise_ratio = 500\nprint('Rescoring using loss: {}'.format(model.criterion.loss_type))\n\n# GRU does not support ce mode right now\neval_loss = 0\ntotal_length = 0\n\nscores = []\n\ndebug = False\n\nwith torch.no_grad():\n    for data_batch in tqdm.tqdm(data_source):\n        data, target, length = process_data(data_batch, cuda=True, sep_target=True)\n\n        if debug:\n            print(model(data, target, length.cuda()))\n            continue\n        loss = model(data, target, length.cuda()).item()\n        loss *= length.sum().item()\n        eval_loss += loss\n        total_length += length.sum().item()\n        score = - loss / math.log(2)  # change the base from e to 2\n        scores.append('{:.8f}'.format(score))\n\nprint('PPL: ', math.exp(eval_loss / total_length))\nwith open('./score.txt', 'w') as f_out:\n    f_out.write('\\n'.join(scores))\n"""
example/utils.py,2,"b'# Collections of some helper functions\nimport logging\nimport argparse\n\nimport torch\n\n\ndef setup_parser():\n    parser = argparse.ArgumentParser(\n        description=\'PyTorch PennTreeBank NCE Language Model\')\n    parser.add_argument(\'--data\', type=str, default=\'./data/penn\',\n                        help=\'location of the data corpus\')\n    parser.add_argument(\'--vocab\', type=str, default=None,\n                        help=\'location of the vocabulary file, without which will use vocab of training corpus\')\n    parser.add_argument(\'--min-freq\', type=int, default=1,\n                        help=\'minimal frequency for word to build vocabulary\')\n    parser.add_argument(\'--emsize\', type=int, default=200,\n                        help=\'size of word embeddings\')\n    parser.add_argument(\'--nhid\', type=int, default=200,\n                        help=\'number of hidden units per layer\')\n    parser.add_argument(\'--nlayers\', type=int, default=2,\n                        help=\'number of layers\')\n    parser.add_argument(\'--lr\', type=float, default=1.0,\n                        help=\'initial learning rate\')\n    parser.add_argument(\'--bptt\', type=int, default=35,\n                        help=\'truncated bptt length\')\n    parser.add_argument(\'--concat\', action=\'store_true\',\n                        help=\'Use concatenated sentences chunked into length of bptt\')\n    parser.add_argument(\'--weight-decay\', type=float, default=1e-5,\n                        help=\'initial weight decay\')\n    parser.add_argument(\'--lr-decay\', type=float, default=2,\n                        help=\'learning rate decay when no progress is observed on validation set\')\n    parser.add_argument(\'--clip\', type=float, default=0.25,\n                        help=\'gradient clipping\')\n    parser.add_argument(\'--epochs\', type=int, default=40,\n                        help=\'upper epoch limit\')\n    parser.add_argument(\'--batch-size\', type=int, default=20, metavar=\'N\',\n                        help=\'batch size\')\n    parser.add_argument(\'--dropout\', type=float, default=0.2,\n                        help=\'dropout applied to layers (0 = no dropout)\')\n    parser.add_argument(\'--seed\', type=int, default=1111,\n                        help=\'random seed\')\n    parser.add_argument(\'--cuda\', action=\'store_true\',\n                        help=\'use CUDA\')\n    parser.add_argument(\'--log-interval\', type=int, default=200, metavar=\'N\',\n                        help=\'report interval\')\n    parser.add_argument(\'--save\', type=str, default=\'model.pt\',\n                        help=\'path to save the final model\')\n    parser.add_argument(\'--loss\', type=str, default=\'full\',\n                        help=\'use one of [nce, full, sampled, mix] as loss \'\n                        \'function\')\n    parser.add_argument(\'--index-module\', type=str, default=\'linear\',\n                        help=\'index module to use in NCELoss wrapper\')\n    parser.add_argument(\'--noise-ratio\', type=int, default=50,\n                        help=\'set the noise ratio of NCE sampling, the noise\'\n                        \' is shared among batch\')\n    parser.add_argument(\'--norm-term\', type=int, default=-1,\n                        help=\'set the log normalization term of NCE sampling\')\n    parser.add_argument(\'--train\', action=\'store_true\',\n                        help=\'set train mode, otherwise only evaluation is\'\n                        \' performed\')\n    parser.add_argument(\'--tb-name\', type=str, default=None,\n                        help=\'the name which would be used in tensorboard \'\n                        \'record\')\n    parser.add_argument(\'--prof\', action=\'store_true\',\n                        help=\'Enable profiling mode, will execute only one \'\n                        \'batch data\')\n    return parser\n\n\ndef setup_logger(logger_name):\n    logger = logging.getLogger(logger_name)\n    logger.setLevel(logging.DEBUG)\n\n    # create console handler with a higher log level\n    ch = logging.StreamHandler()\n    ch.setLevel(logging.WARNING)\n\n    # create file handler which logs even debug messages\n    fh = logging.FileHandler(\'log/%s.log\' % logger_name)\n    fh.setLevel(logging.DEBUG)\n\n    formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')\n    fh.setFormatter(formatter)\n    # ch.setFormatter(formatter)\n\n    logger.addHandler(ch)\n    logger.addHandler(fh)\n\n    return logger\n\n\n# Get the mask matrix of a batched input\ndef get_mask(lengths, cut_tail=0, max_len=None):\n    """"""\n    Creates a boolean mask from sequence lengths.\n    """"""\n    assert lengths.min() >= cut_tail\n    batch_size = lengths.numel()\n    max_len = max_len or lengths.max()\n    mask = (torch.arange(0, max_len)\n            .type_as(lengths)\n            .repeat(batch_size, 1)\n            .lt(lengths.unsqueeze(1)))\n    return mask\n\n\ndef process_data(data_batch, cuda=False, sep_target=True):\n    """"""A data pre-processing util which construct the input `Tensor` for model\n\n    Args:\n        - data_batch: a batched data from `PaddedDataset`\n        - cuda: indicates whether to put data into GPU\n        - sep_target: return separated input and target if turned on\n\n    Returns:\n        - input: the input data batch\n        - target: target data if `sep_target` is True, else a duplicated input\n        - effective_length: the useful sentence length for loss computation <s> is ignored\n        """"""\n\n    batch_sentence, length = data_batch\n    if cuda:\n        batch_sentence = batch_sentence.cuda()\n        length = length.cuda()\n\n    # cut the padded sentence to max sentence length in this batch\n    max_len = length.max()\n    batch_sentence = batch_sentence[:, :max_len]\n\n    # the useful sentence length for loss computation <s> is ignored\n    effective_length = length - 1\n\n    if sep_target:\n        data = batch_sentence[:, :-1]\n        target = batch_sentence[:, 1:]\n    else:\n        data = batch_sentence\n        target = batch_sentence\n\n    data = data.contiguous()\n    target = target.contiguous()\n    effective_length = effective_length\n\n    return data, target, effective_length\n\n\ndef build_unigram_noise(freq):\n    """"""build the unigram noise from a list of frequency\n    Parameters:\n        freq: a tensor of #occurrences of the corresponding index\n    Return:\n        unigram_noise: a torch.Tensor with size ntokens,\n        elements indicate the probability distribution\n    """"""\n    total = freq.sum()\n    noise = freq / total\n    assert abs(noise.sum() - 1) < 0.001\n    return noise\n'"
example/vocab.py,0,"b'""""""Build the vocabulary from corpus\n\nThis file is forked from pytorch/text repo at Github.com""""""\nimport os\nimport dill as pickle\nimport logging\nfrom collections import defaultdict, Counter\n\nfrom tqdm import tqdm\nlogger = logging.getLogger(__name__)\n\nUNK = \'<unk>\'  # unknown word\nBOS = \'<s>\'  # sentence start\nEOS = \'</s>\'  # sentence end\n\n\ndef _default_unk_index():\n    return 0\n\n\ndef load_freq(freq_file):\n    """"""Load the frequency from text file""""""\n    counter = Counter()\n    with open(freq_file) as f:\n        for line in f:\n            word, freq = line.split(\' \')\n            counter[word] = freq\n    return counter\n\n\ndef write_freq(counter, freq_file):\n    """"""Write the word-frequency pairs into text file\n\n    File format:\n\n        word1 freq1\n        word2 freq2\n\n    """"""\n    # sort by frequency, then alphabetically\n    words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])\n    words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n    with open(freq_file, \'w\') as f:\n        for word, freq in words_and_frequencies:\n            f.writelines(\'{} {}\\n\'.format(word, freq))\n\n\nclass Vocab(object):\n    """"""Defines a vocabulary object that will be used to numericalize a field.\n    Attributes:\n        freqs: A collections.Counter object holding the frequencies of tokens\n            in the data used to build the Vocab.\n        word2idx: A collections.defaultdict instance mapping token strings to\n            numerical identifiers.\n        idx2word: A list of token strings indexed by their numerical\n        identifiers.\n    """"""\n    def __init__(self, counter, max_size=None, min_freq=1):\n        """"""Create a Vocab object from a collections.Counter.\n        Arguments:\n            counter: collections.Counter object holding the frequencies of\n                each value found in the data.\n            max_size: The maximum size of the vocabulary, or None for no\n                maximum. Default: None.\n            min_freq: The minimum frequency needed to include a token in the\n                vocabulary. Values less than 1 will be set to 1. Default: 1.\n        """"""\n        self.freqs = counter\n        self.max_size = max_size\n        self.min_freq = min_freq\n        self.specials = [UNK, BOS, EOS]\n        self.build()\n\n    def build(self, force_vocab=[]):\n        """"""Build the required vocabulary according to attributes\n\n        We need an explicit <unk> for NCE because this improve the precision of\n        word frequency estimation in noise sampling\n\n        Args:\n            - force_vocab: force the vocabulary to be within this vocab\n        """"""\n        counter = self.freqs.copy()\n        if force_vocab:\n            min_freq = 1\n        min_freq = max(self.min_freq, 1)\n\n        # delete the special tokens from given vocabulary\n        force_vocab = [w for w in force_vocab if w not in self.specials]\n\n        # Do not count the BOS and UNK as frequency term\n        for word in self.specials:\n            del counter[word]\n\n        self.idx2word = self.specials + force_vocab\n        max_size = None if self.max_size is None else self.max_size + len(self.idx2word)\n\n        # sort by frequency, then alphabetically\n        words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])\n        words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n\n        unk_freq = 0\n        for word, freq in words_and_frequencies:\n\n            # for words not in force_vocab and with freq<th, throw to <unk>\n            if freq < min_freq and word not in force_vocab:\n                unk_freq += freq\n            elif len(self.idx2word) != max_size and not force_vocab:\n                self.idx2word.append(word)\n\n        self.word2idx = defaultdict(_default_unk_index)\n        self.word2idx.update({\n            word: idx for idx, word in enumerate(self.idx2word)\n        })\n\n        self.idx2count = [self.freqs[word] for word in self.idx2word]\n        # set the frequencies for special tokens by miracle trial\n        self.idx2count[0] += unk_freq  # <unk>\n        self.idx2count[1] = self.freqs[\'<s>\']  # <s>\n        self.idx2count[2] = 0  # </s>\n\n    def __eq__(self, other):\n        if self.freqs != other.freqs:\n            return False\n        if self.word2idx != other.word2idx:\n            return False\n        if self.idx2word != other.idx2word:\n            return False\n        return True\n\n    def __len__(self):\n        return len(self.idx2word)\n\n    def extend(self, v, sort=False):\n        words = sorted(v.idx2word) if sort else v.idx2word\n        # TODO: speedup the dependency\n        for w in words:\n            if w not in self.word2idx:\n                self.idx2word.append(w)\n                self.word2idx[w] = len(self.idx2word) - 1\n\n\ndef check_vocab(vocab):\n    """"""A util function to check the vocabulary correctness""""""\n    # one word for one index\n    assert len(vocab.idx2word) == len(vocab.word2idx)\n\n    # no duplicate words in idx2word\n    assert len(set(vocab.idx2word)) == len(vocab.idx2word)\n\n\ndef get_vocab(base_path, file_list, min_freq=1, force_recount=False, vocab_file=None):\n    """"""Build vocabulary file with each line the word and frequency\n\n    The vocabulary object is cached at the first build, aiming at reducing\n    the time cost for pre-process during training large dataset\n\n    Args:\n        - sentences: sentences with BOS and EOS\n        - min_freq: minimal frequency to truncate\n        - force_recount: force a re-count of word frequency regardless of the\n        Count cache file\n        - vocab_file: a specific vocabulary file. If not None, the returned\n        vocabulary will only count the words in vocab_file, with others treated\n        as <unk>\n\n    Return:\n        - vocab: the Vocab object\n    """"""\n    counter = Counter()\n    cache_file = os.path.join(base_path, \'vocab.pkl\')\n\n    if os.path.exists(cache_file) and not force_recount:\n        logger.debug(\'Load cached vocabulary object\')\n        vocab = pickle.load(open(cache_file, \'rb\'))\n        if min_freq:\n            vocab.min_freq = min_freq\n        logger.debug(\'Load cached vocabulary object finished\')\n    else:\n        logger.debug(\'Refreshing vocabulary\')\n        for filename in file_list:\n            full_path = os.path.join(base_path, filename)\n            for line in tqdm(open(full_path, \'r\'), desc=\'Building vocabulary: \'):\n                counter.update(line.split())\n                counter.update([BOS, EOS])\n        vocab = Vocab(counter, min_freq=min_freq)\n        logger.debug(\'Refreshing vocabulary finished\')\n\n        # saving for future uses\n        freq_file = os.path.join(base_path, \'freq.txt\')\n        write_freq(vocab.freqs, freq_file)\n        pickle.dump(vocab, open(cache_file, \'wb\'))\n\n    force_vocab = []\n    if vocab_file:\n        with open(vocab_file) as f:\n            force_vocab = [line.strip() for line in f]\n    vocab.build(force_vocab=force_vocab)\n    check_vocab(vocab)\n    return vocab\n'"
nce/__init__.py,0,b'from nce.index_linear import IndexLinear\nfrom nce.index_gru import IndexGRU\nfrom nce.nce_loss import NCELoss\n'
nce/alias_multinomial.py,4,"b'from math import isclose\n\nimport torch\n\nclass AliasMultinomial(torch.nn.Module):\n    \'\'\'Alias sampling method to speedup multinomial sampling\n\n    The alias method treats multinomial sampling as a combination of uniform sampling and\n    bernoulli sampling. It achieves significant acceleration when repeatedly sampling from\n    the save multinomial distribution.\n\n    Attributes:\n        - probs: the probability density of desired multinomial distribution\n\n    Refs:\n        - https://hips.seas.harvard.edu/blog/2013/03/03/the-alias-method-efficient-sampling-with-many-discrete-outcomes/\n    \'\'\'\n    def __init__(self, probs):\n        super(AliasMultinomial, self).__init__()\n\n        assert isclose(probs.sum().item(), 1), \'The noise distribution must sum to 1\'\n        cpu_probs = probs.cpu()\n        K = len(probs)\n\n        # such a name helps to avoid the namespace check for nn.Module\n        self_prob = [0] * K\n        self_alias = [0] * K\n\n        # Sort the data into the outcomes with probabilities\n        # that are larger and smaller than 1/K.\n        smaller = []\n        larger = []\n        for idx, prob in enumerate(cpu_probs):\n            self_prob[idx] = K*prob\n            if self_prob[idx] < 1.0:\n                smaller.append(idx)\n            else:\n                larger.append(idx)\n\n        # Loop though and create little binary mixtures that\n        # appropriately allocate the larger outcomes over the\n        # overall uniform mixture.\n        while len(smaller) > 0 and len(larger) > 0:\n            small = smaller.pop()\n            large = larger.pop()\n\n            self_alias[small] = large\n            self_prob[large] = (self_prob[large] - 1.0) + self_prob[small]\n\n            if self_prob[large] < 1.0:\n                smaller.append(large)\n            else:\n                larger.append(large)\n\n        for last_one in smaller+larger:\n            self_prob[last_one] = 1\n\n        self.register_buffer(\'prob\', torch.Tensor(self_prob))\n        self.register_buffer(\'alias\', torch.LongTensor(self_alias))\n\n    def draw(self, *size):\n        """"""Draw N samples from multinomial\n\n        Args:\n            - size: the output size of samples\n        """"""\n        max_value = self.alias.size(0)\n\n        kk = self.alias.new(*size).random_(0, max_value).long().view(-1)\n        prob = self.prob[kk]\n        alias = self.alias[kk]\n        # b is whether a random number is greater than q\n        b = torch.bernoulli(prob).long()\n        oq = kk.mul(b)\n        oj = alias.mul(1 - b)\n\n        return (oq + oj).view(size)\n\n'"
nce/index_gru.py,1,"b'""""""An indexed module bundle for generic NCE module""""""\n\nimport torch\nimport torch.nn as nn\n\nfrom .nce_loss import NCELoss\n\nclass IndexGRU(NCELoss):\n    """"""An indexed module for generic NCE\n\n    This module is a container of nn.Embedding,\n    one layer GRU and a linear regressor.\n\n    Attributes:\n        - ntoken: size of vocabulary\n        - ninp: embedding dimension, also the input dimension of GRU\n        - nhid: GRU size\n        - dropout: dropout rate for Embedding, GRU and Linear module\n        the GRU is not dropout due to only one layer.\n\n    Parameters:\n        - target_idx:(B, N) padded target index\n        - noise_idx:(B, N, Nr) padded noise index\n    """"""\n\n    def __init__(self, ntoken, ninp, nhid, dropout,\n                 *args, **kwargs):\n        super(IndexGRU, self).__init__(*args, **kwargs)\n\n        self.ntoken = ntoken\n        self.nhid = nhid\n        self.ninp = ninp\n\n        self.drop = nn.Dropout(dropout)\n        self.encoder = nn.Sequential(\n            nn.Embedding(ntoken, ninp),\n            self.drop,\n        )\n        # this GRU only outputs the hidden for last layer, so only 1 layer is supported\n        self.rnn = nn.GRU(ninp, nhid, num_layers=1, dropout=dropout, batch_first=True)\n        self.scorer = nn.Sequential(\n            self.drop,\n            nn.Linear(nhid, 1),\n        )\n\n    def get_score(self, target_idx, noise_idx, input):\n\n        # if not self.nce:\n        #     #TODO: evaluate Perplexity\n        #     raise(NotImplementedError(\'CE evaluation mode for GRU is not implemented yet\'))\n\n        input_emb = self.encoder(input) # (B, N, E)\n        # The noise for <s> (sentence start) is non-sense\n        rnn_output, _last_hidden = self.rnn(input_emb) # (B, N, H)\n\n        target_score = self.get_target_score(target_idx, rnn_output)\n\n        if noise_idx is None:\n            return target_score\n\n        noise_score = self.get_noise_score(noise_idx, rnn_output)\n\n        return target_score, noise_score\n\n    def ce_loss(self, target_idx, input):\n        """"""Compute the CrossEntropyLoss of target index given input\n\n        The loss is an approximation to real CrossEntropyLoss. Due to\n        the limitation of generic NCE structure, the score among the whole\n        vocabulary is intractable to compute.\n\n        Args:\n            - target_idx: the batched target index\n            - input: batched input\n\n        Returns:\n            - output: the loss for each target_idx\n        """"""\n        target_score = self.get_score(target_idx, None, input) - self.norm_term\n        return - target_score\n\n\n    def get_target_score(self, noise_idx, rnn_output):\n        """"""Get the score of target word given supervised context\n\n        Args:\n            - target_idx: (B, N) the target word index\n            - rnn_output: output of rnn model\n\n        Return:\n            - target_score: (B, N) score for target word index\n        """"""\n        # there\'s a time-step shift in the following code.\n        # because noise_output goes through one more RNN cell\n        effective_rnn_output = rnn_output[:, 1:]\n        return self.scorer(effective_rnn_output).squeeze()\n\n\n    def get_noise_score(self, noise_idx, rnn_output):\n        """"""Get the score of noise given supervised context\n\n        Args:\n            - noise_idx: (B, N, N_r) the noise word index\n            - rnn_output: output of rnn model\n\n        Return:\n            - noise_score: (B, N, N_r) score for noise word index\n        """"""\n\n        noise_emb = self.encoder(noise_idx.view(-1))\n        noise_ratio = noise_idx.size(2)\n\n        # rnn_output of </s> is useless for sentence scoring\n        batched_rnn_output = rnn_output[:,:-1].unsqueeze(2).expand(\n            -1, -1, noise_ratio, -1\n        ).contiguous().view(1, -1, self.nhid)\n\n        noise_output, _last_hidden = self.rnn(\n            noise_emb.view(-1, 1, self.nhid),\n            batched_rnn_output,\n        )\n\n        noise_score = self.scorer(noise_output).view_as(noise_idx)\n        return noise_score\n'"
nce/index_linear.py,8,"b'""""""An index linear class for generic NCE module""""""\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .nce_loss import NCELoss\n\n\nclass IndexLinear(NCELoss):\n    """"""A linear layer that only decodes the results of provided indices\n\n    Args:\n        target_idx: indices of target words\n        noise_idx: indices of noise words\n        input: input matrix\n\n    Shape:\n        - target_idx :math:`(B, N)` where `max(M) <= N` B is batch size\n        - noise_idx :math:`(B, N, N_r)` where `max(M) <= N`\n        - Input :math:`(B, N, in\\_features)`\n\n    Return:\n        - target_score :math:`(N, 1)`\n        - noise_score :math:`(N, N_r)` the un-normalized score\n    """"""\n\n    def __init__(self, embedding_dim, num_classes, *args, **kwargs):\n        super(IndexLinear, self).__init__(*args, **kwargs)\n        # use Embedding to store the output embedding\n        # it\'s efficient when it comes sparse update of gradients\n        self.emb = nn.Embedding(num_classes, embedding_dim)\n        # self.bias = nn.Parameter(torch.Tensor(num_classes))\n        self.bias = nn.Embedding(num_classes, 1)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.emb.embedding_dim)\n        self.emb.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            # initialize the bias with unigram instead of uniform\n            self.bias.weight.data = torch.unsqueeze(\n                self.logprob_noise + self.norm_term, 1\n            )\n\n    def get_score(self, target_idx, noise_idx, input):\n        """"""\n        Shape:\n            - target_idx: :math:`(B, L)` where `B` is batch size\n            `L` is sequence length\n            - noise_idx: :math:`(B, L, N_r)` where `N_r is noise ratio`\n            - input: :math:`(B, L, E)` where `E = output embedding size`\n        """"""\n\n        if self.per_word:\n            return self._compute_sampled_logit(\n                target_idx, noise_idx, input\n            )\n        else:\n            return self._compute_sampled_logit_batched(\n                target_idx, noise_idx, input\n            )\n\n    def _compute_sampled_logit(self, target_idx, noise_idx, input):\n        """"""compute the logits of given indices based on input vector\n\n        Args:\n            - target_idx: :math:`B, L, 1`\n            - noise_idx: :math:`B, L, N_r` target_idx and noise_idx are\n            concatenated into one single index matrix for performance\n            - input: :math:`(B, L, E)` where `E = vector dimension`\n\n        Returns:\n            - target_score: :math:`(B, L)` the computed logits of target_idx\n            - noise_score: :math:`(B, L, N_r)` the computed logits of noise_idx\n        """"""\n\n        # the size will be used to pack the output of indexlinear\n        original_size = target_idx.size()\n\n        # flatten the following matrix\n        input = input.contiguous().view(-1, 1, input.size(-1))  # N,1,E\n        target_idx = target_idx.view(-1).unsqueeze(-1)  # N,1\n        noise_idx = noise_idx.view(-1, noise_idx.size(-1))  # N,Nr\n        indices = torch.cat([target_idx, noise_idx], dim=-1)\n\n        # the pytorch\'s [] operator can\'t BP correctly with redundant indices\n        # before version 0.2.0\n        # [] operator is much slower than index_select in pytorch-0.4.0\n\n        # index_select is faster than pure embedding look-up which is weird\n        # 20it/s vs. 14 it/s\n        # target_batch = self.emb(indices)\n        # bias = self.bias(indices).squeeze(2)\n        target_batch = self.emb.weight.index_select(0, indices.view(-1)).view(*indices.size(), -1)\n        bias = self.bias.weight.index_select(0, indices.view(-1)).view_as(indices)\n        # the element-wise multiplication is automatically broadcasted\n        logits = torch.sum(input * target_batch, dim=2) + bias\n        logits = logits.view(*original_size, -1)\n\n        target_score, noise_score = logits[:, :, 0], logits[:, :, 1:]\n        return target_score, noise_score\n\n    def _compute_sampled_logit_batched(self, target_idx, noise_idx, input):\n        """"""compute the logits of given indices based on input vector\n\n        A batched version, it speeds up computation and puts less burden on\n        sampling methods.\n\n        Args:\n            - target_idx: :math:`B, L, 1` flatten to `(N)` where `N=BXL`\n            - noise_idx: :math:`B, L, N_r`, noises at the dim along B and L\n            should be the same, flatten to `N_r`\n            - input: :math:`(B, L, E)` where `E = vector dimension`\n\n        Returns:\n            - target_score: :math:`(B, L)` the computed logits of target_idx\n            - noise_score: :math:`(B, L, N_r)` the computed logits of noise_idx\n        """"""\n\n        original_size = target_idx.size()\n\n        # flatten the following matrix\n        input = input.contiguous().view(-1, input.size(-1))\n        target_idx = target_idx.view(-1)\n        noise_idx = noise_idx[0, 0].view(-1)\n\n        target_batch = self.emb(target_idx)\n        # target_bias = self.bias.index_select(0, target_idx)  # N\n        target_bias = self.bias(target_idx).squeeze(1)  # N\n        target_score = torch.sum(input * target_batch, dim=1) + target_bias  # N X E * N X E\n\n        noise_batch = self.emb(noise_idx)  # Nr X H\n        # noise_bias = self.bias.index_select(0, noise_idx).unsqueeze(0)  # Nr\n        noise_bias = self.bias(noise_idx)  # 1, Nr\n        noise_score = torch.matmul(\n            input, noise_batch.t()\n        ) + noise_bias.t()  # N X Nr\n        return target_score.view(original_size), noise_score.view(*original_size, -1)\n\n    def ce_loss(self, target_idx, input):\n        score = F.linear(input, self.emb.weight, self.bias.weight.squeeze(1))  # (N, V)\n        loss = self.ce(\n            score.view(-1, score.size(-1)),\n            target_idx.view(-1)\n        ).view_as(target_idx)\n        return loss\n'"
nce/nce_loss.py,8,"b'""""""A generic NCE wrapper which speedup the training and inferencing""""""\n\nimport math\n\nimport torch\nimport torch.nn as nn\nfrom .alias_multinomial import AliasMultinomial\n\n# A backoff probability to stabilize log operation\nBACKOFF_PROB = 1e-10\n\n\nclass NCELoss(nn.Module):\n    """"""Noise Contrastive Estimation\n\n    NCE is to eliminate the computational cost of softmax\n    normalization.\n\n    There are 3 loss modes in this NCELoss module:\n        - nce: enable the NCE approximation\n        - sampled: enabled sampled softmax approximation\n        - full: use the original cross entropy as default loss\n    They can be switched by directly setting `nce.loss_type = \'nce\'`.\n\n    Ref:\n        X.Chen etal Recurrent neural network language\n        model training with noise contrastive estimation\n        for speech recognition\n        https://core.ac.uk/download/pdf/42338485.pdf\n\n    Attributes:\n        noise: the distribution of noise\n        noise_ratio: $\\frac{#noises}{#real data samples}$ (k in paper)\n        norm_term: the normalization term (lnZ in paper), can be heuristically\n        determined by the number of classes, plz refer to the code.\n        reduction: reduce methods, same with pytorch\'s loss framework, \'none\',\n        \'elementwise_mean\' and \'sum\' are supported.\n        loss_type: loss type of this module, currently \'full\', \'sampled\', \'nce\'\n        are supported\n\n    Shape:\n        - noise: :math:`(V)` where `V = vocabulary size`\n        - target: :math:`(B, N)`\n        - loss: a scalar loss by default, :math:`(B, N)` if `reduction=\'none\'`\n\n    Input:\n        target: the supervised training label.\n        args&kwargs: extra arguments passed to underlying index module\n\n    Return:\n        loss: if `reduction=\'sum\' or \'elementwise_mean\'` the scalar NCELoss ready for backward,\n        else the loss matrix for every individual targets.\n    """"""\n\n    def __init__(self,\n                 noise,\n                 noise_ratio=100,\n                 norm_term=\'auto\',\n                 reduction=\'elementwise_mean\',\n                 per_word=False,\n                 loss_type=\'nce\',\n                 ):\n        super(NCELoss, self).__init__()\n\n        # Re-norm the given noise frequency list and compensate words with\n        # extremely low prob for numeric stability\n        probs = noise / noise.sum()\n        probs = probs.clamp(min=BACKOFF_PROB)\n        renormed_probs = probs / probs.sum()\n\n        self.register_buffer(\'logprob_noise\', renormed_probs.log())\n        self.alias = AliasMultinomial(renormed_probs)\n\n        self.noise_ratio = noise_ratio\n        if norm_term == \'auto\':\n            self.norm_term = math.log(noise.numel())\n        else:\n            self.norm_term = norm_term\n        self.reduction = reduction\n        self.per_word = per_word\n        self.bce_with_logits = nn.BCEWithLogitsLoss(reduction=\'none\')\n        self.ce = nn.CrossEntropyLoss(reduction=\'none\')\n        self.loss_type = loss_type\n\n    def forward(self, target, *args, **kwargs):\n        """"""compute the loss with output and the desired target\n\n        The `forward` is the same among all NCELoss submodules, it\n        takes care of generating noises and calculating the loss\n        given target and noise scores.\n        """"""\n\n        batch = target.size(0)\n        max_len = target.size(1)\n        if self.loss_type != \'full\':\n\n            noise_samples = self.get_noise(batch, max_len)\n\n            # B,N,Nr\n            logit_noise_in_noise = self.logprob_noise[noise_samples.data.view(-1)].view_as(noise_samples)\n            logit_target_in_noise = self.logprob_noise[target.data.view(-1)].view_as(target)\n\n            # (B,N), (B,N,Nr)\n            logit_target_in_model, logit_noise_in_model = self._get_logit(target, noise_samples, *args, **kwargs)\n\n            if self.loss_type == \'nce\':\n                if self.training:\n                    loss = self.nce_loss(\n                        logit_target_in_model, logit_noise_in_model,\n                        logit_noise_in_noise, logit_target_in_noise,\n                    )\n                else:\n                    # directly output the approximated posterior\n                    loss = - logit_target_in_model\n            elif self.loss_type == \'sampled\':\n                loss = self.sampled_softmax_loss(\n                    logit_target_in_model, logit_noise_in_model,\n                    logit_noise_in_noise, logit_target_in_noise,\n                )\n            # NOTE: The mix mode is still under investigation\n            elif self.loss_type == \'mix\' and self.training:\n                loss = 0.5 * self.nce_loss(\n                    logit_target_in_model, logit_noise_in_model,\n                    logit_noise_in_noise, logit_target_in_noise,\n                )\n                loss += 0.5 * self.sampled_softmax_loss(\n                    logit_target_in_model, logit_noise_in_model,\n                    logit_noise_in_noise, logit_target_in_noise,\n                )\n\n            else:\n                current_stage = \'training\' if self.training else \'inference\'\n                raise NotImplementedError(\n                    \'loss type {} not implemented at {}\'.format(\n                        self.loss_type, current_stage\n                    )\n                )\n\n        else:\n            # Fallback into conventional cross entropy\n            loss = self.ce_loss(target, *args, **kwargs)\n\n        if self.reduction == \'elementwise_mean\':\n            return loss.mean()\n        elif self.reduction == \'sum\':\n            return loss.sum()\n        else:\n            return loss\n\n    def get_noise(self, batch_size, max_len):\n        """"""Generate noise samples from noise distribution""""""\n\n        noise_size = (batch_size, max_len, self.noise_ratio)\n        if self.per_word:\n            noise_samples = self.alias.draw(*noise_size)\n        else:\n            noise_samples = self.alias.draw(1, 1, self.noise_ratio).expand(*noise_size)\n\n        noise_samples = noise_samples.contiguous()\n        return noise_samples\n\n    def _get_logit(self, target_idx, noise_idx, *args, **kwargs):\n        """"""Get the logits of NCE estimated probability for target and noise\n\n        Both NCE and sampled softmax Loss are unchanged when the probabilities are scaled\n        evenly, here we subtract the maximum value as in softmax, for numeric stability.\n\n        Shape:\n            - Target_idx: :math:`(N)`\n            - Noise_idx: :math:`(N, N_r)` where `N_r = noise ratio`\n        """"""\n\n        target_logit, noise_logit = self.get_score(target_idx, noise_idx, *args, **kwargs)\n\n        target_logit = target_logit.sub(self.norm_term)\n        noise_logit = noise_logit.sub(self.norm_term)\n        return target_logit, noise_logit\n\n    def get_score(self, target_idx, noise_idx, *args, **kwargs):\n        """"""Get the target and noise score\n\n        Usually logits are used as score.\n        This method should be override by inherit classes\n\n        Returns:\n            - target_score: real valued score for each target index\n            - noise_score: real valued score for each noise index\n        """"""\n        raise NotImplementedError()\n\n    def ce_loss(self, target_idx, *args, **kwargs):\n        """"""Get the conventional CrossEntropyLoss\n\n        The returned loss should be of the same size of `target`\n\n        Args:\n            - target_idx: batched target index\n            - args, kwargs: any arbitrary input if needed by sub-class\n\n        Returns:\n            - loss: the estimated loss for each target\n        """"""\n        raise NotImplementedError()\n\n    def nce_loss(self, logit_target_in_model, logit_noise_in_model, logit_noise_in_noise, logit_target_in_noise):\n        """"""Compute the classification loss given all four probabilities\n\n        Args:\n            - logit_target_in_model: logit of target words given by the model (RNN)\n            - logit_noise_in_model: logit of noise words given by the model\n            - logit_noise_in_noise: logit of noise words given by the noise distribution\n            - logit_target_in_noise: logit of target words given by the noise distribution\n\n        Returns:\n            - loss: a mis-classification loss for every single case\n        """"""\n\n        # NOTE: prob <= 1 is not guaranteed\n        logit_model = torch.cat([logit_target_in_model.unsqueeze(2), logit_noise_in_model], dim=2)\n        logit_noise = torch.cat([logit_target_in_noise.unsqueeze(2), logit_noise_in_noise], dim=2)\n\n        # predicted probability of the word comes from true data distribution\n        # The posterior can be computed as following\n        # p_true = logit_model.exp() / (logit_model.exp() + self.noise_ratio * logit_noise.exp())\n        # For numeric stability we compute the logits of true label and\n        # directly use bce_with_logits.\n        # Ref https://pytorch.org/docs/stable/nn.html?highlight=bce#torch.nn.BCEWithLogitsLoss\n        logit_true = logit_model - logit_noise - math.log(self.noise_ratio)\n\n        label = torch.zeros_like(logit_model)\n        label[:, :, 0] = 1\n\n        loss = self.bce_with_logits(logit_true, label).sum(dim=2)\n        return loss\n\n    def sampled_softmax_loss(self, logit_target_in_model, logit_noise_in_model, logit_noise_in_noise, logit_target_in_noise):\n        """"""Compute the sampled softmax loss based on the tensorflow\'s impl""""""\n        logits = torch.cat([logit_target_in_model.unsqueeze(2), logit_noise_in_model], dim=2)\n        q_logits = torch.cat([logit_target_in_noise.unsqueeze(2), logit_noise_in_noise], dim=2)\n        # subtract Q for correction of biased sampling\n        logits = logits - q_logits\n        labels = torch.zeros_like(logits.narrow(2, 0, 1)).squeeze(2).long()\n        loss = self.ce(\n            logits.view(-1, logits.size(-1)),\n            labels.view(-1),\n        ).view_as(labels)\n\n        return loss\n'"
test/test_evaluation.py,0,"b'from main import evaluate, corpus\n\n\ndef test_evaluate():\n    assert corpus.test\n    assert evaluate(corpus.test)\n'"
test/test_perplexity.py,7,"b'import torch\nfrom torch.autograd import Variable\n\nfrom ../main.py import corpus, eval_cross_entropy\n\n\nMOCK_OUTPUT = torch.Tensor(\n    [[1, 2, 0],\n     [-1, -2, 0],\n     [0.5, 0.2, -0.3]]\n)\n\nMOCK_LABEL = torch.LongTensor(\n    [1, 2, 1]\n).unsqueeze(0)\n\nMOCK_LENGTH = torch.LongTensor(\n    [2]\n)\n\nEXPECT_LOSS = 0.8\n\n\ndef test_import():\n    assert torch\n    assert corpus\n\n\ndef around(source, target):\n    return abs(source - target) <= 0.2\n\n\ndef test_eval_cross_entropy():\n    loss = eval_cross_entropy(\n        Variable(MOCK_OUTPUT)[:MOCK_LENGTH[0]].unsqueeze(0),\n        Variable(MOCK_LABEL)[:, :MOCK_LENGTH[0]],\n        MOCK_LENGTH\n    )\n    assert around(loss, EXPECT_LOSS)\n\ndef test_batch_eval_cross_entropy():\n    BATCH_MOCK_OUTPUT = torch.stack([MOCK_OUTPUT, MOCK_OUTPUT])\n    BATCH_MOCK_LABEL = torch.stack([MOCK_LABEL, MOCK_LABEL])\n    BATCH_MOCK_LENGTH = torch.LongTensor(\n        [3, 2]\n    )\n\n    BATCH_MOCK_OUTPUT = Variable(BATCH_MOCK_OUTPUT)\n    BATCH_MOCK_LABEL = Variable(BATCH_MOCK_LABEL)\n\n    loss = eval_cross_entropy(BATCH_MOCK_OUTPUT, BATCH_MOCK_LABEL, BATCH_MOCK_LENGTH)\n\n    assert around(loss / 5, 0.5)\n'"
