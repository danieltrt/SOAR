file_path,api_count,code
setup.py,0,"b'from setuptools import setup\n\ntry:\n    long_description = open(\'README.md\').read()\nexcept FileNotFoundError:\n    long_description = \'\'\n\nsetup(\n    name=\'pytorch_memlab\',\n    version=\'0.1.0\',\n    licence=\'MIT\',\n    description=\'A lab to do simple and accurate memory experiments on pytorch\',\n    long_description=long_description,\n    long_description_content_type=\'text/markdown\',\n    classifiers=[\n        ""Programming Language :: Python"",\n        ""Topic :: Software Development :: Libraries :: Python Modules"",\n        ],\n    keywords=\'pytorch memory profile\',\n    author=\'Kaiyu Shi\',\n    author_email=\'skyisno.1@gmail.com\',\n    url=\'https://github.com/Stonesjtu/pytorch_memlab\',\n    license=\'MIT\',\n    include_package_data=True,\n    zip_safe=True,\n    install_requires=[\n        \'setuptools\',\n        \'calmsize\',\n        \'pandas>=0.18\',\n        \'torch>=1.4\',\n    ],\n    extras_require={\n        \'ipython\': [\'IPython>=0.13\'],\n        \'test\': [\'pytest\'],\n    },\n    packages=[\'pytorch_memlab\'],\n)\n'"
pytorch_memlab/__init__.py,0,"b'from .courtesy import Courtesy\nfrom .mem_reporter import MemReporter\nfrom .line_profiler import LineProfiler, profile, profile_every, set_target_gpu, clear_global_line_profiler\ntry:\n    from .line_profiler.extension import load_ipython_extension\nexcept ImportError:\n    pass\n'"
pytorch_memlab/courtesy.py,4,"b'import gc\nimport torch\n\n\nclass Courtesy():\n    """"""A class to yield CUDA memory at any time in the training\n\n    The whole save/load is a bit tricky because all data transfer should\n    be inplace operation and gradient agnostic\n    """"""\n    def __init__(self):\n        self.loc_map = {}\n\n    def yield_memory(self):\n        """"""Transfer all the CUDA tensors into CPU memory""""""\n        tensors = [obj for obj in gc.get_objects() if isinstance(obj, torch.Tensor)]\n        for t in tensors:\n            # in case tensors appear more than once\n            if t not in self.loc_map:\n                self.loc_map[t] = t.device\n\n            t.data = t.data.cpu()\n            # parameters have one more wrapper for .data\n            if isinstance(t, torch.nn.Parameter):\n                # sometimes Parameter does not have grad\n                try:\n                    t.grad.data = t.grad.cpu()\n                finally:\n                    pass\n        torch.cuda.empty_cache()\n\n    def restore(self):\n        """"""Restore the tensors into original CUDA devices""""""\n        for t, device in self.loc_map.items():\n            t.data = t.data.to(device)\n            if isinstance(t, torch.nn.Parameter):\n                # sometimes Parameter does not have grad\n                try:\n                    t.grad = t.grad.to(device)\n                finally:\n                    pass\n        self.loc_map.clear()\n\n    def __enter__(self):\n        self.yield_memory()\n        return self\n\n    def __exit__(self, *args):\n        self.restore()\n'"
pytorch_memlab/mem_reporter.py,8,"b'import math\nimport gc\nfrom collections import defaultdict\n\nimport torch\nfrom .utils import readable_size\n\nLEN = 79\n\n# some pytorch low-level memory management constant\n# the minimal allocate memory size (Byte)\nPYTORCH_MIN_ALLOCATE = 2 ** 9\n# the minimal cache memory size (Byte)\nPYTORCH_MIN_CACHE = 2 ** 20\n\nclass MemReporter():\n    """"""A memory reporter that collects tensors and memory usages\n\n    Parameters:\n        - model: an extra nn.Module can be passed to infer the name\n        of Tensors\n\n    """"""\n    def __init__(self, model=None):\n        self.tensor_name = defaultdict(list)\n        self.device_mapping = defaultdict(list)\n        self.device_tensor_stat = {}\n        # to numbering the unknown tensors\n        self.name_idx = 0\n        if model is not None:\n            assert isinstance(model, torch.nn.Module)\n            # for model with tying weight, multiple parameters may share\n            # the same underlying tensor\n            for name, param in model.named_parameters():\n                self.tensor_name[param].append(name)\n        for param, name in self.tensor_name.items():\n            self.tensor_name[param] = \'+\'.join(name)\n\n    def _get_tensor_name(self, tensor):\n        if tensor in self.tensor_name:\n            name = self.tensor_name[tensor]\n        # use numbering if no name can be inferred\n        else:\n            name = type(tensor).__name__ + str(self.name_idx)\n            self.tensor_name[tensor] = name\n            self.name_idx += 1\n        return name\n\n    def collect_tensor(self):\n        """"""Collect all tensor objects tracked by python\n\n        NOTICE:\n            - the buffers for backward which is implemented in C++ are\n            not tracked by python\'s reference counting.\n            - the gradients(.grad) of Parameters is not collected, and\n            I don\'t know why.\n        """"""\n        #FIXME: make the grad tensor collected by gc\n        objects = gc.get_objects()\n        tensors = [obj for obj in objects if isinstance(obj, torch.Tensor)]\n        for t in tensors:\n            self.device_mapping[t.device].append(t)\n\n    def get_stats(self):\n        """"""Get the memory stat of tensors and then release them\n\n        As a memory profiler, we cannot hold the reference to any tensors, which\n        causes possibly inaccurate memory usage stats, so we delete the tensors after\n        getting required stats""""""\n        visited_data = {}\n        self.device_tensor_stat.clear()\n\n        def get_tensor_stat(tensor):\n            """"""Get the stat of a single tensor\n\n            Returns:\n                - stat: a tuple containing (tensor_name, tensor_size,\n            tensor_numel, tensor_memory)\n            """"""\n            assert isinstance(tensor, torch.Tensor)\n\n            name = self._get_tensor_name(tensor)\n\n            numel = tensor.numel()\n            element_size = tensor.element_size()\n            fact_numel = tensor.storage().size()\n            fact_memory_size = fact_numel * element_size\n            # since pytorch allocate at least 512 Bytes for any tensor, round\n            # up to a multiple of 512\n            memory_size = math.ceil(fact_memory_size / PYTORCH_MIN_ALLOCATE) \\\n                    * PYTORCH_MIN_ALLOCATE\n\n            # tensor.storage should be the actual object related to memory\n            # allocation\n            data_ptr = tensor.storage().data_ptr()\n            if data_ptr in visited_data:\n                name = \'{}(->{})\'.format(\n                    name,\n                    visited_data[data_ptr],\n                )\n                # don\'t count the memory for reusing same underlying storage\n                memory_size = 0\n            else:\n                visited_data[data_ptr] = name\n\n            size = tuple(tensor.size())\n            # torch scalar has empty size\n            if not size:\n                size = (1,)\n\n            return (name, size, numel, memory_size)\n\n        for device, tensors in self.device_mapping.items():\n            tensor_stats = []\n            for tensor in tensors:\n\n                if tensor.numel() == 0:\n                    continue\n                stat = get_tensor_stat(tensor)  # (name, shape, numel, memory_size)\n                tensor_stats.append(stat)\n                if isinstance(tensor, torch.nn.Parameter):\n                    if tensor.grad is not None:\n                        # manually specify the name of gradient tensor\n                        self.tensor_name[tensor.grad] = \'{}.grad\'.format(\n                            self._get_tensor_name(tensor)\n                        )\n                        stat = get_tensor_stat(tensor.grad)\n                        tensor_stats.append(stat)\n\n            self.device_tensor_stat[device] = tensor_stats\n\n        self.device_mapping.clear()\n\n    def print_stats(self, verbose=False, target_device=None):\n        # header\n        show_reuse = verbose\n        template_format = \'{:<40s}{:>20s}{:>10s}\'\n        print(template_format.format(\'Element type\', \'Size\', \'Used MEM\') )\n        for device, tensor_stats in self.device_tensor_stat.items():\n            # By default, if the target_device is not specified,\n            # print tensors on all devices\n            if target_device is not None and device != target_device:\n                continue\n            print(\'-\' * LEN)\n            print(\'Storage on {}\'.format(device))\n            total_mem = 0\n            total_numel = 0\n            for stat in tensor_stats:\n                name, size, numel, mem = stat\n                if not show_reuse:\n                    name = name.split(\'(\')[0]\n                print(template_format.format(\n                    str(name),\n                    str(size),\n                    readable_size(mem),\n                ))\n                total_mem += mem\n                total_numel += numel\n\n            print(\'-\'*LEN)\n            print(\'Total Tensors: {} \\tUsed Memory: {}\'.format(\n                total_numel, readable_size(total_mem),\n            ))\n\n            if device != torch.device(\'cpu\'):\n                with torch.cuda.device(device):\n                    memory_allocated = torch.cuda.memory_allocated()\n                print(\'The allocated memory on {}: {}\'.format(\n                    device, readable_size(memory_allocated),\n                ))\n                if memory_allocated != total_mem:\n                    print(\'Memory differs due to the matrix alignment or\'\n                          \' invisible gradient buffer tensors\')\n            print(\'-\'*LEN)\n\n    def report(self, verbose=False, device=None):\n        """"""Interface for end-users to directly print the memory usage\n\n        args:\n            - verbose: flag to show tensor.storage reuse information\n            - device: `torch.device` object, specify the target device\n            to report detailed memory usage. It will print memory usage\n            on all devices if not specified. Usually we only want to\n            print the memory usage on CUDA devices.\n\n        """"""\n        self.collect_tensor()\n        self.get_stats()\n        self.print_stats(verbose, target_device=device)\n'"
pytorch_memlab/utils.py,0,"b""from math import isnan\nfrom calmsize import size as calmsize\n\ndef readable_size(num_bytes):\n    return '' if isnan(num_bytes) else '{:.2f}'.format(calmsize(num_bytes))\n"""
test/__init__.py,0,b''
test/test_courtesy.py,4,"b""import torch\n\nfrom pytorch_memlab import Courtesy, MemReporter\n\ndef test_reporter():\n    linear = torch.nn.Linear(1024, 1024).cuda()\n    inp = torch.Tensor(512, 1024).cuda()\n\n    out = linear(inp).mean()\n    out.backward()\n\n    reporter = MemReporter(linear)\n    reporter.report()\n    ct = Courtesy()\n    ct.yield_memory()\n    print('gpu>>>>>>>>>>>>>>>>>>cpu')\n    reporter.report()\n    ct.restore()\n    print('cpu>>>>>>>>>>>>>>>>>>gpu')\n    reporter.report()\n\ndef test_courtesy_context():\n    linear = torch.nn.Linear(1024, 1024).cuda()\n    inp = torch.Tensor(512, 1024).cuda()\n\n    out = linear(inp).mean()\n    out.backward()\n\n    reporter = MemReporter(linear)\n    with Courtesy() as ct:\n        print('gpu>>>>>>>>>>>>>>>>>>cpu')\n        reporter.report()\n"""
test/test_line_profiler.py,35,"b'import pytest\nimport torch\nimport numpy as np\n\nfrom pytorch_memlab import LineProfiler, profile, profile_every, set_target_gpu, clear_global_line_profiler\n\ndef test_display():\n\n    def work():\n        # comment\n        linear = torch.nn.Linear(100, 100).cuda()\n        linear_2 = torch.nn.Linear(100, 100).cuda()\n        linear_3 = torch.nn.Linear(100, 100).cuda()\n\n    def work_3():\n        lstm = torch.nn.LSTM(1000, 1000).cuda()\n\n    def work_2():\n        # comment\n        linear = torch.nn.Linear(100, 100).cuda()\n        linear_2 = torch.nn.Linear(100, 100).cuda()\n        linear_3 = torch.nn.Linear(100, 100).cuda()\n        work_3()\n    \n    with LineProfiler(work, work_2) as prof:\n        work()\n        work_2()\n\n    return prof.display()\n\n\ndef test_line_report():\n\n    def work():\n        # comment\n        linear = torch.nn.Linear(100, 100).cuda()\n        linear_2 = torch.nn.Linear(100, 100).cuda()\n        linear_3 = torch.nn.Linear(100, 100).cuda()\n\n    def work_3():\n        lstm = torch.nn.LSTM(1000, 1000).cuda()\n\n    def work_2():\n        # comment\n        linear = torch.nn.Linear(100, 100).cuda()\n        linear_2 = torch.nn.Linear(100, 100).cuda()\n        linear_3 = torch.nn.Linear(100, 100).cuda()\n        work_3()\n\n    line_profiler = LineProfiler(work, work_2)\n    line_profiler.enable()\n\n    work()\n    work_2()\n\n    line_profiler.disable()\n    line_profiler.print_stats()\n\ndef test_line_report_decorator():\n    clear_global_line_profiler()\n\n    @profile_every(output_interval=3)\n    def work():\n        # comment\n        linear = torch.nn.Linear(100, 100).cuda()\n        linear_2 = torch.nn.Linear(100, 100).cuda()\n        linear_3 = torch.nn.Linear(100, 100).cuda()\n\n    @profile_every(output_interval=1)\n    def work2():\n        # comment\n        linear = torch.nn.Linear(100, 100).cuda()\n        linear_2 = torch.nn.Linear(100, 100).cuda()\n        linear_3 = torch.nn.Linear(100, 100).cuda()\n    work()\n    work2()\n    work()\n    work()\n\ndef test_line_report_method():\n    clear_global_line_profiler()\n\n    class Net(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.linear = torch.nn.Linear(100, 100).cuda()\n            self.drop = torch.nn.Dropout(0.1)\n\n        @profile_every(1)\n        def forward(self, inp):\n            return self.drop(self.linear(inp))\n\n    net = Net()\n    inp = torch.Tensor(50, 100).cuda()\n    net(inp)\n\ndef test_line_report_profile():\n    clear_global_line_profiler()\n\n    @profile\n    def work():\n        # comment\n        linear = torch.nn.Linear(100, 100).cuda()\n        linear_2 = torch.nn.Linear(100, 100).cuda()\n        linear_3 = torch.nn.Linear(100, 100).cuda()\n\n    work()\n    work()\n\ndef test_line_report_profile_set_gpu():\n    clear_global_line_profiler()\n\n    @profile\n    def work():\n        # comment\n        set_target_gpu(1)\n        linear = torch.nn.Linear(100, 100).cuda(1)\n        set_target_gpu(0)\n        linear_2 = torch.nn.Linear(100, 100).cuda(0)\n        linear_3 = torch.nn.Linear(100, 100).cuda(1)\n\n    work()\n    work()\n\ndef test_line_report_profile_interrupt():\n    clear_global_line_profiler()\n\n    @profile\n    def work():\n        # comment\n        linear = torch.nn.Linear(100, 100).cuda()\n        linear_2 = torch.nn.Linear(100, 100).cuda()\n        linear_3 = torch.nn.Linear(100, 100).cuda()\n\n    @profile_every(1)\n    def work2():\n        linear_2 = torch.nn.Linear(100, 100).cuda()\n        linear_3 = torch.nn.Linear(100, 100).cuda()\n\n    work()\n    work2()\n    raise KeyboardInterrupt'"
test/test_mem_reporter.py,12,"b""import torch\nfrom pytorch_memlab import MemReporter\n\nimport pytest\n\n\nconcentrate_mode = False\n\ndef test_reporter():\n    linear = torch.nn.Linear(1024, 1024).cuda()\n    inp = torch.Tensor(512, 1024).cuda()\n    reporter = MemReporter(linear)\n\n    out = linear(inp*(inp+3)).mean()\n    reporter.report()\n    out.backward()\n\n    reporter.report()\n\n@pytest.mark.skipif(concentrate_mode, reason='concentrate')\ndef test_reporter_tie_weight():\n    linear = torch.nn.Linear(1024, 1024).cuda()\n    linear_2 = torch.nn.Linear(1024, 1024).cuda()\n    linear_2.weight = linear.weight\n    container = torch.nn.Sequential(\n        linear, linear_2\n    )\n    reporter = MemReporter(container)\n    inp = torch.Tensor(512, 1024).cuda()\n\n    out = container(inp).mean()\n    out.backward()\n\n    reporter = MemReporter(container)\n    reporter.report()\n\n@pytest.mark.skipif(concentrate_mode, reason='concentrate')\ndef test_reporter_LSTM():\n    lstm = torch.nn.LSTM(256, 256, num_layers=1).cuda()\n    # lstm.flatten_parameters()\n    inp = torch.Tensor(256, 256, 256).cuda()\n    out, _ = lstm(inp)\n    out.mean().backward()\n\n    reporter = MemReporter(lstm)\n    reporter.report()\n\n@pytest.mark.skipif(concentrate_mode, reason='concentrate')\ndef test_reporter_device():\n    lstm_cpu = torch.nn.LSTM(256, 256)\n    lstm = torch.nn.LSTM(256, 256, num_layers=1).cuda()\n    # lstm.flatten_parameters()\n    inp = torch.Tensor(256, 256, 256).cuda()\n    out, _ = lstm(inp)\n    out.mean().backward()\n\n    reporter = MemReporter(lstm)\n    reporter.report()\n    reporter.report(device=torch.device('cuda:0'))\n"""
pytorch_memlab/line_profiler/__init__.py,0,"b'from .line_profiler import LineProfiler\nfrom .profile import profile, profile_every, set_target_gpu, clear_global_line_profiler\n'"
pytorch_memlab/line_profiler/extension.py,1,"b'""""""IPython & notebook extension interface""""""\nfrom tempfile import mkstemp\n\nfrom IPython.core.magic import (\n    Magics,\n    magics_class,\n    line_cell_magic,\n    needs_local_scope,\n)\nfrom IPython.core.magic_arguments import magic_arguments, argument, parse_argstring\n\nfrom .line_profiler import LineProfiler, DEFAULT_COLUMNS\n\n\nclass UsageError(Exception):\n    pass\n\n\n@magics_class\nclass MemlabMagics(Magics):\n    @magic_arguments()\n    @argument(\'--function\',\n              \'-f\',\n              metavar=\'FUNC\',\n              action=\'append\',\n              default=[],\n              help=""""""Function to profile. Can be specified multiple times to profile multiple\n                   functions"""""")\n    @argument(\'--column\',\n              \'-c\',\n              metavar=\'COLS\',\n              action=\'append\',\n              default=[],\n              help=""""""Columns to display. Can be specified multiple times to profile multiple\n                   functions. See the Torch CUDA spec at\n                   https://pytorch.org/docs/stable/cuda.html#torch.cuda.memory_stats for details."""""")\n    @argument(\'-D\',\n              \'--no_default_columns\',\n              action=\'store_true\',\n              help=\'Hide the default columns of \' + "", "".join(DEFAULT_COLUMNS))\n    @argument(\'-r\',\n              \'--return-profiler\',\n              action=\'store_true\',\n              help=\'Return LineProfiler object for introspection\')\n    @argument(\'-g\',\n              \'--gpu\',\n              metavar=\'GPU_ID\',\n              default=0,\n              type=int,\n              help=\'Profile memory usage of this GPU\')\n    @argument(\'-q\',\n              \'--quiet\',\n              action=\'store_true\',\n              help=\'Don\\\'t print out profile results\')\n    @argument(\'statement\',\n              nargs=\'*\',\n              default=None,\n              help=\'Code to run under profiler. You can omit this in cell magic mode.\')\n    @argument(\'-T\',\n              \'--dump-profile\',\n              metavar=\'OUTPUT\',\n              help=\'Dump text profile output to file\')\n    @line_cell_magic\n    @needs_local_scope\n    def mlrun(self, line=None, cell=None, local_ns=None):\n        """"""Execute a statement/cell under the PyTorch Memlab profiler to collect CUDA memory\n        allocation information on a per-line basis.\n        """"""\n        args = parse_argstring(self.mlrun, line)\n        global_ns = self.shell.user_global_ns\n\n        funcs = []\n        for name in args.function:\n            try:\n                fn = eval(name, global_ns, local_ns)\n                funcs.append(fn)\n            except NameError as e:\n                raise UsageError(\'Could not find function {!r}.\\n{}: {}\'.format(\n                    name, e.__class__.__name__, e)\n                )\n        profiler = LineProfiler(*funcs, target_gpu=args.gpu)\n        if cell is not None:\n            code = cell\n        else:\n            assert args.statement is not None\n            code = \'\\n\'.join(args.statement)\n        with profiler:\n            exec(compile(code, filename=\'<ipython>\', mode=\'exec\'), local_ns)\n\n        if args.dump_profile is not None:\n            with open(args.dump_profile, \'w\') as f:\n                profiler.print_stats(stream=f)\n\n        if args.return_profiler:\n            return profiler\n        else:\n            defaults = [] if args.no_default_columns else DEFAULT_COLUMNS\n            return profiler.display(columns=defaults + args.column)\n\n\ndef load_ipython_extension(ipython):\n    ipython.register_magics(MemlabMagics)\n'"
pytorch_memlab/line_profiler/line_profiler.py,7,"b'import inspect\nimport sys\n\nimport torch\n\nfrom .line_records import LineRecords\nfrom ..utils import readable_size\n\n# Seaborn\'s `muted` color cycle\nDEFAULT_COLUMNS = [\'active_bytes.all.peak\', \'reserved_bytes.all.peak\']\n\n\nclass LineProfiler:\n    """"""Profile the CUDA memory usage info for each line in pytorch\n\n    This class registers callbacks for added functions to profiling them line\n    by line, and collects all the statistics in CUDA memory. Usually you may\n    want to use simpler wrapper below `profile` or `profile_every`.\n\n    The CUDA memory is collected only on the **current** cuda device.\n\n    Usage:\n        ```python\n        with LineProfiler(func) as lp:\n            func\n        lp.display()\n\n        ```python\n        lp = LineProfiler(func)\n        lp.enable()\n        func()\n        lp.disable()\n        lp.display()\n        ```\n    """"""\n\n    def __init__(self, *functions, target_gpu=0, **kwargs):\n        self.target_gpu = target_gpu\n        self._code_infos = {}\n        self._raw_line_records = []\n        self.enabled = False\n        for func in functions:\n            self.add_function(func)\n\n    def add_function(self, func):\n        """""" Record line profiling information for the given Python function.\n        """"""\n        try:\n            # We need to use the hash here because pandas will later expect something\n            # orderable for its index\n            code_hash = hash(func.__code__)\n        except AttributeError:\n            import warnings\n            warnings.warn(\n                ""Could not extract a code object for the object %r"" % (func,))\n            return\n        if code_hash not in self._code_infos:\n            first_line = inspect.getsourcelines(func)[1]\n            self._code_infos[code_hash] = {\n                \'func\': func,\n                \'first_line\': first_line,\n                \'prev_line\': first_line,\n                \'prev_record\': -1,\n            }\n\n        # re-register the newer trace_callback\n        if self.enabled:\n            self.register_callback()\n\n    def __enter__(self):\n        self.enable()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.disable()\n\n    def register_callback(self):\n        """"""Register the trace_callback only on demand""""""\n        if self._code_infos:\n            sys.settrace(self._trace_callback)\n\n    def _reset_cuda_stats(self):\n        torch.cuda.reset_peak_memory_stats()\n        torch.cuda.reset_accumulated_memory_stats()\n\n    def enable(self):\n        self.enabled = True\n\n        try:\n            torch.cuda.empty_cache()\n            self._reset_cuda_stats()\n        except AssertionError as e:\n            print(\'Could not reset CUDA stats and cache: \' + str(e))\n\n        self.register_callback()\n\n    def disable(self):\n        self.enabled = False\n        sys.settrace(None)\n\n    def clear(self):\n        """"""Clear the state of the line profiler""""""\n        self._code_infos = {}\n        self._raw_line_records = []\n\n    def _trace_callback(self, frame, event, arg):\n        """"""Trace the execution of python line-by-line""""""\n\n        if event == \'call\':\n            return self._trace_callback\n\n        code_hash = hash(frame.f_code)\n        if event in [\'line\', \'return\'] and code_hash in self._code_infos:\n            code_info = self._code_infos[code_hash]\n            with torch.cuda.device(self.target_gpu):\n                self._raw_line_records.append({\n                    \'code_hash\': code_hash,\n                    \'line\': code_info[\'prev_line\'],\n                    \'prev_record_idx\': code_info[\'prev_record\'],\n                    **torch.cuda.memory_stats()})\n                self._reset_cuda_stats()\n\n            if event == \'line\':\n                code_info[\'prev_line\'] = frame.f_lineno\n                code_info[\'prev_record\'] = len(self._raw_line_records)-1\n            elif event == \'return\':\n                code_info[\'prev_line\'] = code_info[\'first_line\']\n                code_info[\'prev_record\'] = -1\n\n    def display(self, func=None, columns=DEFAULT_COLUMNS):\n        """"""Display the profiling results on either IPython or CLI\n\n        The columns are explained in the PyTorch documentation:\n        https://pytorch.org/docs/stable/cuda.html#torch.cuda.memory_stats\n\n        .. note:: To work, this needs to be the last thing returned in the IPython statement or cell.\n\n        Args:\n            func (str): the function name of interest, None for all registered function\n            columns (list of str): the column names of interest, See PyTorch\'s doc for available names.\n\n        Returns:\n            RecordsDisplay: Returns an object that\'ll display the recorded stats in the IPython console\n        """"""\n        return LineRecords(self._raw_line_records, self._code_infos).display(func, columns)\n\n    def print_stats(self, func=None, columns=DEFAULT_COLUMNS, stream=sys.stdout):\n        """"""Print the text profiling results to stream\n\n        The columns are explained in the PyTorch documentation:\n        https://pytorch.org/docs/stable/cuda.html#torch.cuda.memory_stats\n\n        Args:\n            func (str): the function name of interest, None for all registered function\n            columns (list of str): the column names of interest, See PyTorch\'s doc for available names\n            stream (IO-like object): the stream to write to\n        """"""\n        stream.write(str(self.display(func, columns)))\n'"
pytorch_memlab/line_profiler/line_records.py,4,"b'""""""Class and helper functions for processing and displaying line records""""""\nimport inspect\nimport pandas as pd\n\nfrom ..utils import readable_size\n\n\nCOLORS = [\n    \'#4878d0\', \'#ee854a\', \'#6acc64\', \'#d65f5f\', \'#956cb4\',\n    \'#8c613c\', \'#dc7ec0\', \'#797979\', \'#d5bb67\', \'#82c6e2\',\n]\n\n\ndef _accumulate_line_records(raw_line_records):\n    """"""The raw records give the memory stats between successive lines executed by the profiler.\n    But we want the memory stats between successive lines in our functions! The two diverge when\n    a function we\'re profiling calls another function we\'re profiling, since then Torch will have\n    its peak/allocated/freed memory stats reset on each line of the called function.\n\n    To fix that, here we look at each line record in turn, and for peak stats we take the\n    maximum since the last record _in the same function_. For allocated/freed stats, we take the\n    sum since the last record in the same function.\n    """"""\n\n    # We\'ll do this in numpy because indexing lots of rows and columns in pandas is dog-slow.\n    raw = pd.DataFrame(raw_line_records)\n    acc_mask = raw.columns.str.match(r\'.*(allocated|freed)$\')\n    peak_mask = raw.columns.str.match(r\'.*(peak)$\')\n    acc_raw, peak_raw = raw.loc[:, acc_mask].values, raw.loc[:, peak_mask].values\n    acc_refined, peak_refined = acc_raw.copy(), peak_raw.copy()\n\n    for row, record in enumerate(raw_line_records):\n        if record[\'prev_record_idx\'] == -1:\n            # No previous data to accumulate from\n            continue\n        if record[\'prev_record_idx\'] == row-1:\n            # Previous record was the previous line, so no need to accumulate anything\n            continue\n\n        # Another profiled function has been called since the last record, so we need to\n        # accumulate the allocated/freed/peaks of the intervening records into this one.\n        acc_refined[row] = acc_raw[record[\'prev_record_idx\']+1:row+1].sum(0)\n        peak_refined[row] = peak_raw[record[\'prev_record_idx\']+1:row+1].max(0)\n\n    refined = raw.copy()\n    refined.loc[:, acc_mask] = acc_refined\n    refined.loc[:, peak_mask] = peak_refined\n    return refined\n\n\ndef _line_records(raw_line_records, code_infos):\n    """"""Converts the raw line records to a nicely-shaped dataframe whose values reflect\n    the memory usage of lines of _functions_ rather than lines of _execution_. See the\n    `_accumulate_line_records` docstring for more detail.""""""\n    # Column spec: https://pytorch.org/docs/stable/cuda.html#torch.cuda.memory_stats\n    qual_names = {\n        code_hash: info[\'func\'].__qualname__ for code_hash, info in code_infos.items()}\n    records = (_accumulate_line_records(raw_line_records)\n               .assign(qual_name=lambda df: df.code_hash.map(qual_names))\n               .set_index([\'qual_name\', \'line\'])\n               .drop([\'code_hash\', \'num_alloc_retries\', \'num_ooms\', \'prev_record_idx\'], 1))\n    records.columns = pd.MultiIndex.from_tuples(\n        [c.split(\'.\') for c in records.columns])\n\n    return records\n\n\ndef _extract_line_records(line_records, func=None, columns=None):\n    """"""Extracts the subset of a line_records dataframe pertinent to a given set of functions and\n    columns""""""\n    if func is not None:\n        # Support both passing the function directly and passing a qual name/list of qual names\n        line_records = line_records.loc[[func.__qualname__] if callable(func) else func]\n\n    if columns is not None:\n        columns = [tuple(c.split(\'.\')) for c in columns]\n        if not all(len(c) == 3 for c in columns):\n            raise ValueError(\'Each column name should have three dot-separated parts\')\n        if not all(c in line_records.columns for c in columns):\n            options = "", "".join(""."".join(c)\n                                for c in line_records.columns.tolist())\n            raise ValueError(\n                \'The column names should be fields of torch.cuda.memory_stat(). Options are: \' + options)\n        line_records = line_records.loc[:, columns]\n\n    return line_records\n\n\nclass LineRecords:\n    """"""Class for processing raw line records and display on IPython & CLI\n    """"""\n\n    def __init__(self, raw_line_records, code_infos):\n        super().__init__()\n        self._raw_line_records = raw_line_records\n        self._code_infos = code_infos\n\n    def display(self, func, columns):\n        """"""Display the records to either notebook or CLI\n\n        The columns are explained in the PyTorch documentation:\n        https://pytorch.org/docs/stable/cuda.html#torch.cuda.memory_stats\n\n        .. note:: Make this call the last one in a notebook cell\n\n        Args:\n            func (str): the function name of interest, None for all registered function\n            columns (list of str): the column names of interest, See PyTorch\'s doc for available names.\n\n        Returns:\n            RecordsDisplay: a IPython friendly object which converts records to HTML or plain text\n        """"""\n        line_records = self._filter_raw_line_records(func, columns)\n        return RecordsDisplay(line_records, self._code_infos)\n\n    def _filter_raw_line_records(self, func, columns):\n        """"""Get the line records\n\n        The columns are explained in the PyTorch documentation:\n        https://pytorch.org/docs/stable/cuda.html#torch.cuda.memory_stats\n\n        Args:\n            func (str): the function name of interest, None for all registered function\n            columns (list of str): the column names of interest, See PyTorch\'s doc for available names.\n\n        Returns:\n            pd.DataFrame: a (line, statistic)-indexed dataframe of memory stats.\n        """"""\n        if len(self._raw_line_records) == 0:\n            return pd.DataFrame(index=pd.MultiIndex.from_product([[], []]), columns=columns)\n\n        line_records = _line_records(self._raw_line_records, self._code_infos)\n        line_records = _extract_line_records(line_records, func, columns)\n\n        if len(line_records) > 0:\n            line_records = line_records.groupby(level=[0, 1]).max()\n\n        return line_records\n\n\nclass RecordsDisplay:\n    """"""Class for processing raw line records and display on IPython & CLI\n\n    IPython\'s rich display functionality [requires we return](https://ipython.readthedocs.io/en/stable/config/integrating.html)\n    an object that has a `_repr_html_` method for when HTML rendering is supported, and\n    a `__repr__` method for when only text is available\n    """"""\n    def __init__(self, line_records, code_infos):\n        super().__init__()\n        self._line_records = line_records\n        self._code_infos = code_infos\n        self._merged_line_records = self._merge_line_records_with_code()\n\n    def _merge_line_records_with_code(self):\n        merged_records = {}\n        for _, info in self._code_infos.items():\n            qual_name = info[\'func\'].__qualname__\n            if qual_name in self._line_records.index.get_level_values(0):\n                lines, start_line = inspect.getsourcelines(info[\'func\'])\n                lines = pd.DataFrame.from_dict({\n                    \'line\': range(start_line, start_line + len(lines)),\n                    \'code\': lines})\n                lines.columns = pd.MultiIndex.from_product([lines.columns, [\'\'], [\'\']])\n\n                merged_records[qual_name] = pd.merge(\n                    self._line_records.loc[qual_name], lines,\n                    right_on=\'line\', left_index=True, how=\'right\')\n        return merged_records\n\n    def __repr__(self):\n        """"""Renders the stats as text""""""\n        if len(self._line_records) == 0:\n            return \'No data collected\\n\'\n\n        is_byte_col = self._line_records.columns.get_level_values(0).str.contains(\'byte\')\n        byte_cols = self._line_records.columns[is_byte_col]\n\n        string = {}\n        for qual_name, merged in self._merge_line_records_with_code().items():\n            maxlen = max(len(c) for c in merged.code)\n            left_align = \'{{:{maxlen}s}}\'.format(maxlen=maxlen)\n            merged[byte_cols] = merged[byte_cols].applymap(readable_size)\n\n            # This is a mess, but I can\'t find any other way to left-align text strings.\n            code_header = (left_align.format(\'code\'), \'\', \'\')\n            merged[code_header] = merged[\'code\'].apply(lambda l: left_align.format(l.rstrip(\'\\n\\r\')))\n            merged = merged.drop(\'code\', 1, level=0)\n\n            string[qual_name] = merged.to_string(index=False)\n\n        return \'\\n\\n\'.join([\'## {q}\\n\\n{c}\\n\'.format(q=q, c=c) for q, c in string.items()])\n\n    def _repr_html_(self):\n        """"""Renders the stats as HTML""""""\n        if len(self._line_records) == 0:\n            return \'<p>No data collected</p>\'\n\n        is_byte_col = self._line_records.columns.get_level_values(0).str.contains(\'byte\')\n        byte_cols = self._line_records.columns[is_byte_col]\n        maxes = self._line_records.max()\n\n        html = {}\n        for qual_name, merged in self._merge_line_records_with_code().items():\n\n            style = merged.style\n\n            # Style the bar charts\n            for i, c in enumerate(self._line_records.columns):\n                style = style.bar([c], color=COLORS[i % len(COLORS)],\n                                  width=99, vmin=0, vmax=maxes[c])\n\n            # Style the text\n            html[qual_name] = (style\n                                .format({c: readable_size for c in byte_cols})\n                                .set_properties(\n                                    subset=[\'code\'], **{\n                                        \'text-align\': \'left\',\n                                        \'white-space\': \'pre\',\n                                        \'font-family\': \'monospace\'})\n                                .set_table_styles([{\n                                    \'selector\': \'th\',\n                                    \'props\': [(\'text-align\', \'left\')]}])\n                                .hide_index()\n                                .render())\n\n        template = \'<h3><span style=""font-family: monospace"">{q}</span></h3><div>{c}</div>\'\n        return \'\\n\'.join(template.format(q=q, c=c) for q, c in html.items())\n'"
pytorch_memlab/line_profiler/profile.py,7,"b'from functools import wraps\nfrom .line_profiler import LineProfiler, DEFAULT_COLUMNS\n\n\nglobal_line_profiler = LineProfiler()\nglobal_line_profiler.enable()\n\n\ndef clear_global_line_profiler():\n    """"""Clears the state of the global line profiler""""""\n    global_line_profiler.clear()\n\n\ndef set_target_gpu(gpu_id):\n    """"""Set the target GPU id to profile memory\n\n    Because of the lack of output space, only one GPU\'s memory usage is shown\n    in line profiler. However you can use this function to switch target GPU\n    to profile on. The GPU switch can be performed before profiling and even\n    in the profiled functions.\n\n    Args:\n        - gpu_id: cuda index to profile the memory on,\n                  also accepts `torch.device` object.\n    """"""\n    global_line_profiler.target_gpu = gpu_id\n\n\ndef profile(func, columns=DEFAULT_COLUMNS):\n    """"""Profile the CUDA memory usage of target function line by line\n\n    The profiling results will be printed at exiting, KeyboardInterrupt raised.\n    The CUDA memory is collected only on the **current** cuda device.\n\n    The columns are explained in the PyTorch documentation:\n    https://pytorch.org/docs/stable/cuda.html#torch.cuda.memory_stats\n\n    Args:\n        func: the function or method to profile on\n        columns (list of str): the column names of interest, See PyTorch\'s doc for available names.\n\n    Usage:\n        ```python\n        @profile\n        def foo():\n            linear = torch.nn.Linear(100, 100).cuda()\n\n        foo()\n\n        class Foo(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.linear = torch.nn.Linear(100, 100).cuda()\n\n            @profile\n            def forward(self, inp):\n                return self.linear(inp)\n\n        inp = torch.Tensor(50, 100).cuda()\n        foo = Foo()\n        foo(inp)\n        ```\n    """"""\n    import atexit\n    global_line_profiler.add_function(func)\n\n    def print_stats_atexit():\n        global_line_profiler.print_stats(func, columns)\n    atexit.register(print_stats_atexit)\n\n    return func\n\n\ndef profile_every(output_interval=1, enable=True, columns=DEFAULT_COLUMNS):\n    """"""Profile the CUDA memory usage of target function line by line\n\n    Prints the profiling output every `output_interval` execution of the target\n    function\n    The CUDA memory is collected only on the **current** cuda device.\n\n    The columns are explained in the PyTorch documentation:\n    https://pytorch.org/docs/stable/cuda.html#torch.cuda.memory_stats\n\n    Args:\n        enable (bool): whether to enable the profiling mode, so users don\'t have to\n                       modify any source code for enabling and disabling profiling.\n        output_interval (int): frequency of output the profiling results\n        columns (list of str): the column names of interest, See PyTorch\'s doc for available names.\n    """"""\n\n    def inner_decorator(func):\n        func.cur_idx = 1\n\n        if enable:\n            global_line_profiler.add_function(func)\n\n        @wraps(func)\n        def run_func(*args, **kwargs):\n            res = func(*args, **kwargs)\n            if enable:\n                if func.cur_idx % output_interval == 0:\n                    global_line_profiler.print_stats(func, columns)\n\n                func.cur_idx += 1\n            return res\n\n        return run_func\n    return inner_decorator\n'"
