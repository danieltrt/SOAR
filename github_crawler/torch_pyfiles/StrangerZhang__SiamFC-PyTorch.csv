file_path,api_count,code
bin/convert_pretrained_model.py,9,"b'import torch\nimport re\nimport numpy as np\nimport argparse\n\nfrom scipy import io as sio\nfrom tqdm import tqdm\n\n# code adapted from https://github.com/bilylee/SiamFC-TensorFlow/blob/master/utils/train_utils.py\ndef convert(mat_path):\n    """"""Get parameter from .mat file into parms(dict)""""""\n\n    def squeeze(vars_):\n    # Matlab save some params with shape (*, 1)\n    # However, we don\'t need the trailing dimension in TensorFlow.\n        if isinstance(vars_, (list, tuple)):\n            return [np.squeeze(v, 1) for v in vars_]\n        else:\n            return np.squeeze(vars_, 1)\n\n    netparams = sio.loadmat(mat_path)[""net""][""params""][0][0]\n    params = dict()\n\n    name_map = {(1, \'conv\'): 0,  (1, \'bn\'): 1,\n                (2, \'conv\'): 4,  (2, \'bn\'): 5,\n                (3, \'conv\'): 8,  (3, \'bn\'): 9,\n                (4, \'conv\'): 11, (4, \'bn\'): 12,\n                (5, \'conv\'): 14}\n    for i in tqdm(range(netparams.size)):\n        param = netparams[0][i]\n        name = param[""name""][0]\n        value = param[""value""]\n        value_size = param[""value""].shape[0]\n\n        match = re.match(r""([a-z]+)([0-9]+)([a-z]+)"", name, re.I)\n        if match:\n            items = match.groups()\n        elif name == \'adjust_f\':\n            continue\n        elif name == \'adjust_b\':\n            params[\'corr_bias\'] = torch.from_numpy(squeeze(value))\n            continue\n\n\n        op, layer, types = items\n        layer = int(layer)\n        if layer in [1, 2, 3, 4, 5]:\n            idx = name_map[(layer, op)]\n            if op == \'conv\':  # convolution\n                if types == \'f\':\n                    params[\'features.{}.weight\'.format(idx)] = torch.from_numpy(value.transpose(3, 2, 0, 1))\n                elif types == \'b\':#  and layer == 5:\n                    value = squeeze(value)\n                    params[\'features.{}.bias\'.format(idx)] = torch.from_numpy(value)\n            elif op == \'bn\':  # batch normalization\n                if types == \'x\':\n                    m, v = squeeze(np.split(value, 2, 1))\n                    params[\'features.{}.running_mean\'.format(idx)] = torch.from_numpy(m)\n                    params[\'features.{}.running_var\'.format(idx)] = torch.from_numpy(np.square(v))\n                    # params[\'features.{}.num_batches_tracked\'.format(idx)] = torch.zeros(0)\n                elif types == \'m\':\n                    value = squeeze(value)\n                    params[\'features.{}.weight\'.format(idx)] = torch.from_numpy(value)\n                elif types == \'b\':\n                    value = squeeze(value)\n                    params[\'features.{}.bias\'.format(idx)] = torch.from_numpy(value)\n            else:\n                raise Exception\n    return params\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--mat_path\', type=str, default=""./models/2016-08-17.net.mat"")\n    args = parser.parse_args()\n    params = convert(args.mat_path)\n    torch.save(params, ""./models/siamfc_pretrained.pth"")\n'"
bin/create_dataset.py,0,"b'import numpy as np\nimport pickle\nimport os\nimport cv2\nimport functools\nimport xml.etree.ElementTree as ET\nimport sys\nsys.path.append(os.getcwd())\n\nfrom multiprocessing import Pool\nfrom fire import Fire\nfrom tqdm import tqdm\nfrom glob import glob\n\nfrom siamfc import config, get_instance_image\n\ndef worker(output_dir, video_dir):\n    image_names = glob(os.path.join(video_dir, \'*.JPEG\'))\n    image_names = sorted(image_names,\n                        key=lambda x:int(x.split(\'/\')[-1].split(\'.\')[0]))\n    video_name = video_dir.split(\'/\')[-1]\n    save_folder = os.path.join(output_dir, video_name)\n    if not os.path.exists(save_folder):\n        os.mkdir(save_folder)\n    trajs = {}\n    for image_name in image_names:\n        img = cv2.imread(image_name)\n        img_mean = tuple(map(int, img.mean(axis=(0, 1))))\n        anno_name = image_name.replace(\'Data\', \'Annotations\')\n        anno_name = anno_name.replace(\'JPEG\', \'xml\')\n        tree = ET.parse(anno_name)\n        root = tree.getroot()\n        bboxes = []\n        filename = root.find(\'filename\').text\n        for obj in root.iter(\'object\'):\n            bbox = obj.find(\'bndbox\')\n            bbox = list(map(int, [bbox.find(\'xmin\').text,\n                                  bbox.find(\'ymin\').text,\n                                  bbox.find(\'xmax\').text,\n                                  bbox.find(\'ymax\').text]))\n            trkid = int(obj.find(\'trackid\').text)\n            if trkid in trajs:\n                trajs[trkid].append(filename)\n            else:\n                trajs[trkid] = [filename]\n            instance_img, _, _ = get_instance_image(img, bbox,\n                    config.exemplar_size, config.instance_size, config.context_amount, img_mean)\n            instance_img_name = os.path.join(save_folder, filename+"".{:02d}.x.jpg"".format(trkid))\n            cv2.imwrite(instance_img_name, instance_img)\n    return video_name, trajs\n\ndef processing(data_dir, output_dir, num_threads=32):\n    # get all 4417 videos\n    video_dir = os.path.join(data_dir, \'Data/VID\')\n    all_videos = glob(os.path.join(video_dir, \'train/ILSVRC2015_VID_train_0000/*\')) + \\\n                 glob(os.path.join(video_dir, \'train/ILSVRC2015_VID_train_0001/*\')) + \\\n                 glob(os.path.join(video_dir, \'train/ILSVRC2015_VID_train_0002/*\')) + \\\n                 glob(os.path.join(video_dir, \'train/ILSVRC2015_VID_train_0003/*\')) + \\\n                 glob(os.path.join(video_dir, \'val/*\'))\n    meta_data = []\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    with Pool(processes=num_threads) as pool:\n        for ret in tqdm(pool.imap_unordered(\n            functools.partial(worker, output_dir), all_videos), total=len(all_videos)):\n            meta_data.append(ret)\n\n    # save meta data\n    pickle.dump(meta_data, open(os.path.join(output_dir, ""meta_data.pkl""), \'wb\'))\n\n\nif __name__ == \'__main__\':\n    Fire(processing)\n\n'"
bin/create_lmdb.py,0,"b""import lmdb\nimport cv2\nimport numpy as np\nimport os \nimport hashlib\nimport functools\n\nfrom glob import glob\nfrom fire import Fire\nfrom tqdm import tqdm\nfrom multiprocessing import Pool\n\ndef worker(video_name):\n    image_names = glob(video_name+'/*')\n    kv = {}\n    for image_name in image_names:\n        img = cv2.imread(image_name)\n        _, img_encode = cv2.imencode('.jpg', img)\n        img_encode = img_encode.tobytes()\n        kv[hashlib.md5(image_name.encode()).digest()] = img_encode\n    return kv\n\ndef create_lmdb(data_dir, output_dir, num_threads):\n    video_names = glob(data_dir+'/*')\n    video_names = [x for x in video_names if os.path.isdir(x)]\n    db = lmdb.open(output_dir, map_size=int(50e9))\n    with Pool(processes=num_threads) as pool:\n        for ret in tqdm(pool.imap_unordered(\n            functools.partial(worker), video_names), total=len(video_names)):\n            with db.begin(write=True) as txn:\n                for k, v in ret.items():\n                    txn.put(k, v)\n\nif __name__ == '__main__':\n    Fire(create_lmdb)\n\n"""
bin/demo_siamfc.py,0,"b'import glob\nimport os\nimport pandas as pd\nimport argparse\nimport numpy as np\nimport cv2\nimport time\nimport sys\nsys.path.append(os.getcwd())\n\nfrom fire import Fire\nfrom tqdm import tqdm\n\nfrom siamfc import SiamFCTracker\n\ndef main(video_dir, gpu_id,  model_path):\n    # load videos\n    filenames = sorted(glob.glob(os.path.join(video_dir, ""img/*.jpg"")),\n           key=lambda x: int(os.path.basename(x).split(\'.\')[0]))\n    frames = [cv2.cvtColor(cv2.imread(filename), cv2.COLOR_BGR2RGB) for filename in filenames]\n    gt_bboxes = pd.read_csv(os.path.join(video_dir, ""groundtruth_rect.txt""), sep=\'\\t|,| \',\n            header=None, names=[\'xmin\', \'ymin\', \'width\', \'height\'],\n            engine=\'python\')\n\n    title = video_dir.split(\'/\')[-1]\n    # starting tracking\n    tracker = SiamFCTracker(model_path, gpu_id)\n    for idx, frame in enumerate(frames):\n        if idx == 0:\n            bbox = gt_bboxes.iloc[0].values\n            tracker.init(frame, bbox)\n            bbox = (bbox[0]-1, bbox[1]-1,\n                    bbox[0]+bbox[2]-1, bbox[1]+bbox[3]-1)\n        else: \n            bbox = tracker.update(frame)\n        # bbox xmin ymin xmax ymax\n        frame = cv2.rectangle(frame,\n                              (int(bbox[0]), int(bbox[1])),\n                              (int(bbox[2]), int(bbox[3])),\n                              (0, 255, 0),\n                              2)\n        gt_bbox = gt_bboxes.iloc[idx].values\n        gt_bbox = (gt_bbox[0], gt_bbox[1],\n                   gt_bbox[0]+gt_bbox[2], gt_bbox[1]+gt_bbox[3])\n        frame = cv2.rectangle(frame,\n                              (int(gt_bbox[0]-1), int(gt_bbox[1]-1)), # 0-index\n                              (int(gt_bbox[2]-1), int(gt_bbox[3]-1)),\n                              (255, 0, 0),\n                              1)\n        if len(frame.shape) == 3:\n            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n        frame = cv2.putText(frame, str(idx), (5, 20), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (0, 255, 0), 1)\n        cv2.imshow(title, frame)\n        cv2.waitKey(30)\n\nif __name__ == ""__main__"":\n    Fire(main)\n'"
bin/run_SiamFC.py,0,"b""import numpy as np\nimport time\nfrom siamfc import SiamFCTracker, config\nimport cv2\nimport glob\nimport os\n\n\ndef run_SiamFC(seq, rp, saveimage):\n    x = seq.init_rect[0]\n    y = seq.init_rect[1]\n    w = seq.init_rect[2]\n    h = seq.init_rect[3]\n\n    tic = time.clock()\n    # starting tracking\n    tracker = SiamFCTracker(config.model_path, config.gpu_id)\n    res = []\n    for idx, frame in enumerate(seq.s_frames):\n        frame = cv2.cvtColor(cv2.imread(frame), cv2.COLOR_BGR2RGB)\n        if idx == 0:\n            bbox = (x, y, w, h)\n            tracker.init(frame, bbox)\n            bbox = (bbox[0], bbox[1], bbox[0]+bbox[2], bbox[1]+bbox[3]) # 1-idx\n        else:\n            bbox = tracker.update(frame)\n        res.append((bbox[0], bbox[1], bbox[2]-bbox[0], bbox[3]-bbox[1])) # 1-idx\n    duration = time.clock() - tic\n    result = {}\n    result['res'] = res\n    result['type'] = 'rect'\n    result['fps'] = round(seq.len / duration, 3)\n    return result\n\n"""
bin/train_siamfc.py,0,"b""import os\nimport sys\nsys.path.append(os.getcwd())\nfrom fire import Fire\n\nfrom siamfc import train\n\nif __name__ == '__main__':\n    Fire(train)\n"""
siamfc/__init__.py,0,b'from .tracker import SiamFCTracker\nfrom .train import train\nfrom .config import config\nfrom .utils import get_instance_image\nfrom .dataset import ImagnetVIDDataset\nfrom .alexnet import SiameseAlexNet\n\n\n'
siamfc/alexnet.py,10,"b""import torch\nimport numpy as np\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom .custom_transforms import ToTensor\n\nfrom torchvision.models import alexnet\nfrom torch.autograd import Variable\nfrom torch import nn\n\nfrom .config import config\n\nclass SiameseAlexNet(nn.Module):\n    def __init__(self, gpu_id, train=True):\n        super(SiameseAlexNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, 11, 2),\n            nn.BatchNorm2d(96),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(3, 2),\n            nn.Conv2d(96, 256, 5, 1, groups=2),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(3, 2),\n            nn.Conv2d(256, 384, 3, 1),\n            nn.BatchNorm2d(384),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 384, 3, 1, groups=2),\n            nn.BatchNorm2d(384),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, 3, 1, groups=2)\n        )\n        self.corr_bias = nn.Parameter(torch.zeros(1))\n        if train:\n            gt, weight = self._create_gt_mask((config.train_response_sz, config.train_response_sz))\n            with torch.cuda.device(gpu_id):\n                self.train_gt = torch.from_numpy(gt).cuda()\n                self.train_weight = torch.from_numpy(weight).cuda()\n            gt, weight = self._create_gt_mask((config.response_sz, config.response_sz))\n            with torch.cuda.device(gpu_id):\n                self.valid_gt = torch.from_numpy(gt).cuda()\n                self.valid_weight = torch.from_numpy(weight).cuda()\n        self.exemplar = None\n        self.gpu_id = gpu_id\n\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight.data, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        exemplar, instance = x\n        if exemplar is not None and instance is not None:\n            batch_size = exemplar.shape[0]\n            exemplar = self.features(exemplar)\n            instance = self.features(instance)\n            score_map = []\n            N, C, H, W = instance.shape\n            instance = instance.view(1, -1, H, W)\n            score = F.conv2d(instance, exemplar, groups=N) * config.response_scale \\\n                    + self.corr_bias\n            return score.transpose(0, 1)\n        elif exemplar is not None and instance is None:\n            # inference used\n            self.exemplar = self.features(exemplar)\n            self.exemplar = torch.cat([self.exemplar for _ in range(3)], dim=0)\n        else:\n            # inference used we don't need to scale the reponse or add bias\n            instance = self.features(instance)\n            N, _, H, W = instance.shape\n            instance = instance.view(1, -1, H, W)\n            score = F.conv2d(instance, self.exemplar, groups=N)\n            return score.transpose(0, 1)\n\n    def loss(self, pred):\n        return F.binary_cross_entropy_with_logits(pred, self.gt)\n\n    def weighted_loss(self, pred):\n        if self.training:\n            return F.binary_cross_entropy_with_logits(pred, self.train_gt,\n                    self.train_weight, reduction='sum') / config.train_batch_size # normalize the batch_size\n        else:\n            return F.binary_cross_entropy_with_logits(pred, self.valid_gt,\n                    self.valid_weight, reduction='sum') / config.train_batch_size # normalize the batch_size\n\n    def _create_gt_mask(self, shape):\n        # same for all pairs\n        h, w = shape\n        y = np.arange(h, dtype=np.float32) - (h-1) / 2.\n        x = np.arange(w, dtype=np.float32) - (w-1) / 2.\n        y, x = np.meshgrid(y, x)\n        dist = np.sqrt(x**2 + y**2)\n        mask = np.zeros((h, w))\n        mask[dist <= config.radius / config.total_stride] = 1\n        mask = mask[np.newaxis, :, :]\n        weights = np.ones_like(mask)\n        weights[mask == 1] = 0.5 / np.sum(mask == 1)\n        weights[mask == 0] = 0.5 / np.sum(mask == 0)\n        mask = np.repeat(mask, config.train_batch_size, axis=0)[:, np.newaxis, :, :]\n        return mask.astype(np.float32), weights.astype(np.float32)\n"""
siamfc/config.py,0,"b""\nclass Config:\n    # dataset related\n    exemplar_size = 127                    # exemplar size\n    instance_size = 255                    # instance size\n    context_amount = 0.5                   # context amount\n\n    # training related\n    num_per_epoch = 53200                  # num of samples per epoch\n    train_ratio = 0.9                      # training ratio of VID dataset\n    frame_range = 100                      # frame range of choosing the instance\n    train_batch_size = 8                   # training batch size\n    valid_batch_size = 8                   # validation batch size\n    train_num_workers = 8                  # number of workers of train dataloader\n    valid_num_workers = 8                  # number of workers of validation dataloader\n    lr = 1e-2                              # learning rate of SGD\n    momentum = 0.0                         # momentum of SGD\n    weight_decay = 0.0                     # weight decay of optimizator\n    step_size = 25                         # step size of LR_Schedular\n    gamma = 0.1                            # decay rate of LR_Schedular\n    epoch = 30                             # total epoch\n    seed = 1234                            # seed to sample training videos\n    log_dir = './models/logs'              # log dirs\n    radius = 16                            # radius of positive label\n    response_scale = 1e-3                  # normalize of response\n    max_translate = 3                      # max translation of random shift\n\n    # tracking related\n    scale_step = 1.0375                    # scale step of instance image\n    num_scale = 3                          # number of scales\n    scale_lr = 0.59                        # scale learning rate\n    response_up_stride = 16                # response upsample stride\n    response_sz = 17                       # response size\n    train_response_sz = 15                 # train response size\n    window_influence = 0.176               # window influence\n    scale_penalty = 0.9745                 # scale penalty\n    total_stride = 8                       # total stride of backbone\n    sample_type = 'uniform'\n    gray_ratio = 0.25\n    blur_ratio = 0.15\n\nconfig = Config()\n"""
siamfc/custom_transforms.py,1,"b'import torch\nimport numpy as np\nimport cv2\n\nclass RandomStretch(object):\n    def __init__(self, max_stretch=0.05):\n        """"""Random resize image according to the stretch\n        Args:\n            max_stretch(float): 0 to 1 value   \n        """"""\n        self.max_stretch = max_stretch\n\n    def __call__(self, sample):\n        """"""\n        Args:\n            sample(numpy array): 3 or 1 dim image\n        """"""\n        scale_h = 1.0 + np.random.uniform(-self.max_stretch, self.max_stretch)\n        scale_w = 1.0 + np.random.uniform(-self.max_stretch, self.max_stretch)\n        h, w = sample.shape[:2]\n        shape = (int(h * scale_h), int(w * scale_w))\n        return cv2.resize(sample, shape, cv2.INTER_LINEAR)\n\nclass CenterCrop(object):\n    def __init__(self, size):\n        """"""Crop the image in the center according the given size \n            if size greater than image size, zero padding will adpot\n        Args:\n            size (tuple): desired size\n        """"""\n        self.size = size\n\n    def __call__(self, sample):\n        """"""\n        Args:\n            sample(numpy array): 3 or 1 dim image\n        """"""\n        shape = sample.shape[:2]\n        cy, cx = (shape[0]-1) // 2, (shape[1]-1) // 2\n        ymin, xmin = cy - self.size[0]//2, cx - self.size[1] // 2\n        ymax, xmax = cy + self.size[0]//2 + self.size[0] % 2,\\\n                     cx + self.size[1]//2 + self.size[1] % 2\n        left = right = top = bottom = 0\n        im_h, im_w = shape\n        if xmin < 0:\n            left = int(abs(xmin))\n        if xmax > im_w:\n            right = int(xmax - im_w)\n        if ymin < 0:\n            top = int(abs(ymin))\n        if ymax > im_h:\n            bottom = int(ymax - im_h)\n\n        xmin = int(max(0, xmin))\n        xmax = int(min(im_w, xmax))\n        ymin = int(max(0, ymin))\n        ymax = int(min(im_h, ymax))\n        im_patch = sample[ymin:ymax, xmin:xmax]\n        if left != 0 or right !=0 or top!=0 or bottom!=0:\n            im_patch = cv2.copyMakeBorder(im_patch, top, bottom, left, right,\n                    cv2.BORDER_CONSTANT, value=0)\n        return im_patch\n\nclass RandomCrop(object):\n    def __init__(self, size, max_translate):\n        """"""Crop the image in the center according the given size \n            if size greater than image size, zero padding will adpot\n        Args:\n            size (tuple): desired size\n            max_translate: max translate of random shift\n        """"""\n        self.size = size\n        self.max_translate = max_translate\n\n    def __call__(self, sample):\n        """"""\n        Args:\n            sample(numpy array): 3 or 1 dim image\n        """"""\n        shape = sample.shape[:2]\n        cy_o = (shape[0] - 1) // 2\n        cx_o = (shape[1] - 1) // 2\n        cy = np.random.randint(cy_o - self.max_translate, \n                               cy_o + self.max_translate+1)\n        cx = np.random.randint(cx_o - self.max_translate,\n                               cx_o + self.max_translate+1)\n        assert abs(cy-cy_o) <= self.max_translate and \\\n                abs(cx-cx_o) <= self.max_translate\n        ymin = cy - self.size[0] // 2\n        xmin = cx - self.size[1] // 2\n        ymax = cy + self.size[0] // 2 + self.size[0] % 2\n        xmax = cx + self.size[1] // 2 + self.size[1] % 2\n        left = right = top = bottom = 0\n        im_h, im_w = shape\n        if xmin < 0:\n            left = int(abs(xmin))\n        if xmax > im_w:\n            right = int(xmax - im_w)\n        if ymin < 0:\n            top = int(abs(ymin))\n        if ymax > im_h:\n            bottom = int(ymax - im_h)\n\n        xmin = int(max(0, xmin))\n        xmax = int(min(im_w, xmax))\n        ymin = int(max(0, ymin))\n        ymax = int(min(im_h, ymax))\n        im_patch = sample[ymin:ymax, xmin:xmax]\n        if left != 0 or right !=0 or top!=0 or bottom!=0:\n            im_patch = cv2.copyMakeBorder(im_patch, top, bottom, left, right,\n                    cv2.BORDER_CONSTANT, value=0)\n        return im_patch\n\nclass ColorAug(object):\n    def __init__(self, type_in=\'z\'):\n        if type_in == \'z\':\n            rgb_var = np.array([[3.2586416e+03,2.8992207e+03,2.6392236e+03],\n                                       [2.8992207e+03,3.0958174e+03,2.9321748e+03],\n                                       [2.6392236e+03,2.9321748e+03,3.4533721e+03]])\n        if type_in == \'x\':\n            rgb_var = np.array([[2.4847285e+03,2.1796064e+03,1.9766885e+03],\n                                       [2.1796064e+03,2.3441289e+03,2.2357402e+03],\n                                       [1.9766885e+03,2.2357402e+03,2.7369697e+03]])\n        self.v, _ = np.linalg.eig(rgb_var)\n        self.v = np.sqrt(self.v)\n\n    def __call__(self, sample):\n        return sample + 0.1 * self.v * np.random.randn(3)\n\n\nclass RandomBlur(object):\n    def __init__(self, ratio):\n        self.ratio = ratio\n\n    def __call__(self, sample):\n        if np.random.rand(1) < self.ratio:\n            # random kernel size\n            kernel_size = np.random.choice([3, 5, 7])\n            # random gaussian sigma\n            sigma = np.random.rand() * 5\n            return cv2.GaussianBlur(sample, (kernel_size,kernel_size), sigma)\n        else:\n            return sample\n\nclass Normalize(object):\n    def __init__(self):\n        self.mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n        self.std = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n\n    def __call__(self, sample):\n        return (sample / 255. - self.mean) / self.std\n\nclass ToTensor(object):\n    def __call__(self, sample):\n        sample = sample.transpose(2, 0, 1)\n        return torch.from_numpy(sample.astype(np.float32))\n'"
siamfc/dataset.py,1,"b'import torch\nimport cv2\nimport os\nimport numpy as np\nimport pickle\nimport lmdb\nimport hashlib\nfrom torch.utils.data.dataset import Dataset\n\nfrom .config import config\n\nclass ImagnetVIDDataset(Dataset):\n    def __init__(self, db, video_names, data_dir, z_transforms, x_transforms, training=True):\n        self.video_names = video_names\n        self.data_dir = data_dir\n        self.z_transforms = z_transforms\n        self.x_transforms = x_transforms\n        meta_data_path = os.path.join(data_dir, \'meta_data.pkl\')\n        self.meta_data = pickle.load(open(meta_data_path, \'rb\'))\n        self.meta_data = {x[0]:x[1] for x in self.meta_data}\n        # filter traj len less than 2\n        for key in self.meta_data.keys():\n            trajs = self.meta_data[key]\n            for trkid in list(trajs.keys()):\n                if len(trajs[trkid]) < 2:\n                    del trajs[trkid]\n\n        self.txn = db.begin(write=False)\n        self.num = len(self.video_names) if config.num_per_epoch is None or not training\\\n                else config.num_per_epoch\n\n    def imread(self, path):\n        key = hashlib.md5(path.encode()).digest()\n        img_buffer = self.txn.get(key)\n        img_buffer = np.frombuffer(img_buffer, np.uint8)\n        img = cv2.imdecode(img_buffer, cv2.IMREAD_COLOR)\n        return img\n\n    def _sample_weights(self, center, low_idx, high_idx, s_type=\'uniform\'):\n        weights = list(range(low_idx, high_idx))\n        weights.remove(center)\n        weights = np.array(weights)\n        if s_type == \'linear\':\n            weights = abs(weights - center)\n        elif s_type == \'sqrt\':\n            weights = np.sqrt(abs(weights - center))\n        elif s_type == \'uniform\':\n            weights = np.ones_like(weights)\n        return weights / sum(weights)\n\n    def __getitem__(self, idx):\n        idx = idx % len(self.video_names)\n        video = self.video_names[idx]\n        trajs = self.meta_data[video]\n        # sample one trajs\n        trkid = np.random.choice(list(trajs.keys()))\n        traj = trajs[trkid]\n        assert len(traj) > 1, ""video_name: {}"".format(video)\n        # sample exemplar\n        exemplar_idx = np.random.choice(list(range(len(traj))))\n        exemplar_name = os.path.join(self.data_dir, video, traj[exemplar_idx]+"".{:02d}.x.jpg"".format(trkid))\n        exemplar_img = self.imread(exemplar_name)\n        exemplar_img = cv2.cvtColor(exemplar_img, cv2.COLOR_BGR2RGB)\n        # sample instance\n        low_idx = max(0, exemplar_idx - config.frame_range)\n        up_idx = min(len(traj), exemplar_idx + config.frame_range)\n\n        # create sample weight, if the sample are far away from center\n        # the probability being choosen are high\n        weights = self._sample_weights(exemplar_idx, low_idx, up_idx, config.sample_type)\n        instance = np.random.choice(traj[low_idx:exemplar_idx] + traj[exemplar_idx+1:up_idx], p=weights)\n        instance_name = os.path.join(self.data_dir, video, instance+"".{:02d}.x.jpg"".format(trkid))\n        instance_img = self.imread(instance_name)\n        instance_img = cv2.cvtColor(instance_img, cv2.COLOR_BGR2RGB)\n        if np.random.rand(1) < config.gray_ratio:\n            exemplar_img = cv2.cvtColor(exemplar_img, cv2.COLOR_RGB2GRAY)\n            exemplar_img = cv2.cvtColor(exemplar_img, cv2.COLOR_GRAY2RGB)\n            instance_img = cv2.cvtColor(instance_img, cv2.COLOR_RGB2GRAY)\n            instance_img = cv2.cvtColor(instance_img, cv2.COLOR_GRAY2RGB)\n        exemplar_img = self.z_transforms(exemplar_img)\n        instance_img = self.x_transforms(instance_img)\n        return exemplar_img, instance_img\n\n    def __len__(self):\n        return self.num\n'"
siamfc/tracker.py,8,"b'import numpy as np\nimport cv2\nimport torch\nimport torch.nn.functional as F\nimport time\nimport warnings\nimport torchvision.transforms as transforms\n\nfrom torch.autograd import Variable\n\nfrom .alexnet import SiameseAlexNet\nfrom .config import config\nfrom .custom_transforms import ToTensor\nfrom .utils import get_exemplar_image, get_pyramid_instance_image, get_instance_image\n\ntorch.set_num_threads(1) # otherwise pytorch will take all cpus\n\nclass SiamFCTracker:\n    def __init__(self, model_path, gpu_id):\n        self.gpu_id = gpu_id\n        with torch.cuda.device(gpu_id):\n            self.model = SiameseAlexNet(gpu_id, train=False)\n            self.model.load_state_dict(torch.load(model_path))\n            self.model = self.model.cuda()\n            self.model.eval() \n        self.transforms = transforms.Compose([\n            ToTensor()\n        ])\n\n    def _cosine_window(self, size):\n        """"""\n            get the cosine window\n        """"""\n        cos_window = np.hanning(int(size[0]))[:, np.newaxis].dot(np.hanning(int(size[1]))[np.newaxis, :])\n        cos_window = cos_window.astype(np.float32)\n        cos_window /= np.sum(cos_window)\n        return cos_window\n\n    def init(self, frame, bbox):\n        """""" initialize siamfc tracker\n        Args:\n            frame: an RGB image\n            bbox: one-based bounding box [x, y, width, height]\n        """"""\n        self.bbox = (bbox[0]-1, bbox[1]-1, bbox[0]-1+bbox[2], bbox[1]-1+bbox[3]) # zero based\n        self.pos = np.array([bbox[0]-1+(bbox[2]-1)/2, bbox[1]-1+(bbox[3]-1)/2])  # center x, center y, zero based\n        self.target_sz = np.array([bbox[2], bbox[3]])                            # width, height\n        # get exemplar img\n        self.img_mean = tuple(map(int, frame.mean(axis=(0, 1))))\n        exemplar_img, scale_z, s_z = get_exemplar_image(frame, self.bbox,\n                config.exemplar_size, config.context_amount, self.img_mean)\n\n        # get exemplar feature\n        exemplar_img = self.transforms(exemplar_img)[None,:,:,:]\n        with torch.cuda.device(self.gpu_id):\n            exemplar_img_var = Variable(exemplar_img.cuda())\n            self.model((exemplar_img_var, None))\n\n        self.penalty = np.ones((config.num_scale)) * config.scale_penalty\n        self.penalty[config.num_scale//2] = 1\n\n        # create cosine window\n        self.interp_response_sz = config.response_up_stride * config.response_sz\n        self.cosine_window = self._cosine_window((self.interp_response_sz, self.interp_response_sz))\n\n        # create scalse\n        self.scales = config.scale_step ** np.arange(np.ceil(config.num_scale/2)-config.num_scale,\n                np.floor(config.num_scale/2)+1)\n\n        # create s_x\n        self.s_x = s_z + (config.instance_size-config.exemplar_size) / scale_z\n\n        # arbitrary scale saturation\n        self.min_s_x = 0.2 * self.s_x\n        self.max_s_x = 5 * self.s_x\n\n    def update(self, frame):\n        """"""track object based on the previous frame\n        Args:\n            frame: an RGB image\n\n        Returns:\n            bbox: tuple of 1-based bounding box(xmin, ymin, xmax, ymax)\n        """"""\n        size_x_scales = self.s_x * self.scales\n        pyramid = get_pyramid_instance_image(frame, self.pos, config.instance_size, size_x_scales, self.img_mean)\n        instance_imgs = torch.cat([self.transforms(x)[None,:,:,:] for x in pyramid], dim=0)\n        with torch.cuda.device(self.gpu_id):\n            instance_imgs_var = Variable(instance_imgs.cuda())\n            response_maps = self.model((None, instance_imgs_var))\n            response_maps = response_maps.data.cpu().numpy().squeeze()\n            response_maps_up = [cv2.resize(x, (self.interp_response_sz, self.interp_response_sz), cv2.INTER_CUBIC)\n             for x in response_maps]\n        # get max score\n        max_score = np.array([x.max() for x in response_maps_up]) * self.penalty\n\n        # penalty scale change\n        scale_idx = max_score.argmax()\n        response_map = response_maps_up[scale_idx]\n        response_map -= response_map.min()\n        response_map /= response_map.sum()\n        response_map = (1 - config.window_influence) * response_map + \\\n                config.window_influence * self.cosine_window\n        max_r, max_c = np.unravel_index(response_map.argmax(), response_map.shape)\n        # displacement in interpolation response\n        disp_response_interp = np.array([max_c, max_r]) - (self.interp_response_sz-1) / 2.\n        # displacement in input\n        disp_response_input = disp_response_interp * config.total_stride / config.response_up_stride\n        # displacement in frame\n        scale = self.scales[scale_idx]\n        disp_response_frame = disp_response_input * (self.s_x * scale) / config.instance_size\n        # position in frame coordinates\n        self.pos += disp_response_frame\n        # scale damping and saturation\n        self.s_x *= ((1 - config.scale_lr) + config.scale_lr * scale)\n        self.s_x = max(self.min_s_x, min(self.max_s_x, self.s_x))\n        self.target_sz = ((1 - config.scale_lr) + config.scale_lr * scale) * self.target_sz\n        bbox = (self.pos[0] - self.target_sz[0]/2 + 1, # xmin   convert to 1-based\n                self.pos[1] - self.target_sz[1]/2 + 1, # ymin\n                self.pos[0] + self.target_sz[0]/2 + 1, # xmax\n                self.pos[1] + self.target_sz[1]/2 + 1) # ymax\n        return bbox\n'"
siamfc/train.py,9,"b'import torch\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision\nimport numpy as np\nimport pandas as pd\nimport os\nimport cv2\nimport pickle\nimport lmdb\n\nfrom fire import Fire\nfrom torch.autograd import Variable\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.utils.data import DataLoader\nfrom glob import glob\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom tensorboardX import SummaryWriter\n\nfrom .config import config\nfrom .alexnet import SiameseAlexNet\nfrom .dataset import ImagnetVIDDataset\nfrom .custom_transforms import Normalize, ToTensor, RandomStretch, \\\n    RandomCrop, CenterCrop, RandomBlur, ColorAug\n\ntorch.manual_seed(1234)\n\ndef train(gpu_id, data_dir):\n    # loading meta data\n    meta_data_path = os.path.join(data_dir, ""meta_data.pkl"")\n    meta_data = pickle.load(open(meta_data_path,\'rb\'))\n    all_videos = [x[0] for x in meta_data]\n\n    # split train/valid dataset\n    train_videos, valid_videos = train_test_split(all_videos, \n            test_size=1-config.train_ratio, random_state=config.seed)\n\n    # define transforms\n    random_crop_size = config.instance_size - 2 * config.total_stride\n    train_z_transforms = transforms.Compose([\n        RandomStretch(),\n        CenterCrop((config.exemplar_size, config.exemplar_size)),\n        ToTensor()\n    ])\n    train_x_transforms = transforms.Compose([\n        RandomStretch(),\n        RandomCrop((random_crop_size, random_crop_size),\n                    config.max_translate),\n        ToTensor()\n    ])\n    valid_z_transforms = transforms.Compose([\n        CenterCrop((config.exemplar_size, config.exemplar_size)),\n        ToTensor()\n    ])\n    valid_x_transforms = transforms.Compose([\n        ToTensor()\n    ])\n\n    # open lmdb\n    db = lmdb.open(data_dir+\'.lmdb\', readonly=True, map_size=int(50e9))\n\n    # create dataset\n    train_dataset = ImagnetVIDDataset(db, train_videos, data_dir,\n            train_z_transforms, train_x_transforms)\n    valid_dataset = ImagnetVIDDataset(db, valid_videos, data_dir,\n            valid_z_transforms, valid_x_transforms, training=False)\n    \n    # create dataloader\n    trainloader = DataLoader(train_dataset, batch_size=config.train_batch_size,\n            shuffle=True, pin_memory=True, num_workers=config.train_num_workers, drop_last=True)\n    validloader = DataLoader(valid_dataset, batch_size=config.valid_batch_size,\n            shuffle=False, pin_memory=True, num_workers=config.valid_num_workers, drop_last=True)\n\n    # create summary writer\n    if not os.path.exists(config.log_dir):\n        os.mkdir(config.log_dir)\n    summary_writer = SummaryWriter(config.log_dir)\n\n    # start training\n    with torch.cuda.device(gpu_id):\n        model = SiameseAlexNet(gpu_id, train=True)\n        model.init_weights()\n        model = model.cuda()\n        optimizer = torch.optim.SGD(model.parameters(), lr=config.lr,\n                momentum=config.momentum, weight_decay=config.weight_decay)\n        scheduler = StepLR(optimizer, step_size=config.step_size, \n                gamma=config.gamma)\n\n        for epoch in range(config.epoch):\n            train_loss = []\n            model.train()\n            for i, data in enumerate(tqdm(trainloader)):\n                exemplar_imgs, instance_imgs = data\n                exemplar_var, instance_var = Variable(exemplar_imgs.cuda()), \\\n                        Variable(instance_imgs.cuda())\n                optimizer.zero_grad()\n                outputs = model((exemplar_var, instance_var))\n                loss = model.weighted_loss(outputs)\n                loss.backward()\n                optimizer.step()\n                step = epoch * len(trainloader) + i\n                summary_writer.add_scalar(\'train/loss\', loss.data, step)\n                train_loss.append(loss.data)\n            train_loss = np.mean(train_loss)\n\n            valid_loss = []\n            model.eval()\n            for i, data in enumerate(tqdm(validloader)):\n                exemplar_imgs, instance_imgs = data\n                exemplar_var, instance_var = Variable(exemplar_imgs.cuda()),\\\n                                             Variable(instance_imgs.cuda())\n                outputs = model((exemplar_var, instance_var))\n                loss = model.weighted_loss(outputs)\n                valid_loss.append(loss.data)\n            valid_loss = np.mean(valid_loss)\n            print(""EPOCH %d valid_loss: %.4f, train_loss: %.4f"" %\n                    (epoch, valid_loss, train_loss))\n            summary_writer.add_scalar(\'valid/loss\', \n                    valid_loss, (epoch+1)*len(trainloader))\n            torch.save(model.cpu().state_dict(), \n                    ""./models/siamfc_{}.pth"".format(epoch+1))\n            model.cuda()\n            scheduler.step()\n'"
siamfc/utils.py,0,"b'import numpy as np\nimport cv2\n\ndef get_center(x):\n    return (x - 1.) / 2.\n\ndef xyxy2cxcywh(bbox):\n    return get_center(bbox[0]+bbox[2]), \\\n           get_center(bbox[1]+bbox[3]), \\\n           (bbox[2]-bbox[0]), \\\n           (bbox[3]-bbox[1])\n\ndef crop_and_pad(img, cx, cy, model_sz, original_sz, img_mean=None):\n    xmin = cx - original_sz // 2\n    xmax = cx + original_sz // 2\n    ymin = cy - original_sz // 2\n    ymax = cy + original_sz // 2\n    im_h, im_w, _ = img.shape\n\n    left = right = top = bottom = 0\n    if xmin < 0:\n        left = int(abs(xmin))\n    if xmax > im_w:\n        right = int(xmax - im_w)\n    if ymin < 0:\n        top = int(abs(ymin))\n    if ymax > im_h:\n        bottom = int(ymax - im_h)\n\n    xmin = int(max(0, xmin))\n    xmax = int(min(im_w, xmax))\n    ymin = int(max(0, ymin))\n    ymax = int(min(im_h, ymax))\n    im_patch = img[ymin:ymax, xmin:xmax]\n    if left != 0 or right !=0 or top!=0 or bottom!=0:\n        if img_mean is None:\n            img_mean = tuple(map(int, img.mean(axis=(0, 1))))\n        im_patch = cv2.copyMakeBorder(im_patch, top, bottom, left, right,\n                cv2.BORDER_CONSTANT, value=img_mean)\n    if model_sz != original_sz:\n        im_patch = cv2.resize(im_patch, (model_sz, model_sz))\n    return im_patch\n\ndef get_exemplar_image(img, bbox, size_z, context_amount, img_mean=None):\n    cx, cy, w, h = xyxy2cxcywh(bbox)\n    wc_z = w + context_amount * (w+h)\n    hc_z = h + context_amount * (w+h)\n    s_z = np.sqrt(wc_z * hc_z)\n    scale_z = size_z / s_z\n    exemplar_img = crop_and_pad(img, cx, cy, size_z, s_z, img_mean)\n    return exemplar_img, scale_z, s_z\n\ndef get_instance_image(img, bbox, size_z, size_x, context_amount, img_mean=None):\n    cx, cy, w, h = xyxy2cxcywh(bbox)\n    wc_z = w + context_amount * (w+h)\n    hc_z = h + context_amount * (w+h)\n    s_z = np.sqrt(wc_z * hc_z)\n    scale_z = size_z / s_z\n    d_search = (size_x - size_z) / 2\n    pad = d_search / scale_z\n    s_x = s_z + 2 * pad\n    scale_x = size_x / s_x\n    instance_img = crop_and_pad(img, cx, cy, size_x, s_x, img_mean)\n    return instance_img, scale_x, s_x\n\ndef get_pyramid_instance_image(img, center, size_x, size_x_scales, img_mean=None):\n    if img_mean is None:\n        img_mean = tuple(map(int, img.mean(axis=(0, 1))))\n    pyramid = [crop_and_pad(img, center[0], center[1], size_x, size_x_scale, img_mean)\n            for size_x_scale in size_x_scales]\n    return pyramid\n'"
