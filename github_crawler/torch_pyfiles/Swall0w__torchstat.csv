file_path,api_count,code
example.py,2,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchstat import stat\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(56180, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 56180)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n\nif __name__ == '__main__':\n    model = Net()\n    stat(model, (3, 224, 224))\n"""
setup.py,0,"b'#!usr/bin/env python3\n\nimport re\nfrom os import path\n\nfrom setuptools import find_packages, setup\n\npackage_name = ""torchstat""\nroot_dir = path.abspath(path.dirname(__file__))\n\n\ndef _requirements():\n    return [name.rstrip() for name in open(path.join(root_dir, \'requirements.txt\'), encoding=\'utf-8\').readlines()]\n\n\ndef _test_requirements():\n    return [name.rstrip() for name in open(path.join(root_dir, \'test_requirements.txt\'), encoding=\'utf-8\').readlines()]\n\n\nwith open(path.join(root_dir, package_name, \'__init__.py\'), encoding=\'utf-8\') as f:\n    init_text = f.read()\n    version = re.search(r\'__version__ = [\\\'\\""](.+?)[\\\'\\""]\', init_text).group(1)\n    author = re.search(r\'__author__ =\\s*[\\\'\\""](.+?)[\\\'\\""]\', init_text).group(1)\n    url = re.search(r\'__url__ =\\s*[\\\'\\""](.+?)[\\\'\\""]\', init_text).group(1)\n\nassert version\nassert author\nassert url\n\nwith open(\'README.md\', encoding=\'utf-8\') as f:\n    long_description = f.read()\n\nsetup(\n    name=package_name,\n    version=version,\n    description=""torchstat: The Pytorch Model Analyzer."",\n    long_description=long_description,\n    long_description_content_type=\'text/markdown\',\n    author=author,\n    url=url,\n\n    install_requires = _requirements(),\n    tests_requires = _test_requirements(),\n    include_package_data=True,\n\n    license=license,\n    packages=find_packages(exclude=(\'tests\')),\n    test_suite=\'tests\',\n    entry_points=""""""\n    [console_scripts]\n    torchstat = torchstat.__main__:main\n    """""",\n    )\n'"
torchstat/__init__.py,0,"b""__copyright__ = 'Copyright (C) 2018 Swall0w'\n__version__ = '0.0.7'\n__author__ = 'Swall0w'\n__url__ = 'https://github.com/Swall0w/torchstat'\n\nfrom torchstat.compute_memory import compute_memory\nfrom torchstat.compute_madd import compute_madd\nfrom torchstat.compute_flops import compute_flops\nfrom torchstat.stat_tree import StatTree, StatNode\nfrom torchstat.model_hook import ModelHook\nfrom torchstat.reporter import report_format\nfrom torchstat.statistics import stat, ModelStat\n\n__all__ = ['report_format', 'StatTree', 'StatNode', 'compute_madd',\n           'compute_flops', 'ModelHook', 'stat', 'ModelStat', '__main__',\n           'compute_memory']\n"""
torchstat/__main__.py,0,"b""from torchstat import stat\nimport argparse\nimport importlib.util\nimport torch\n\n\ndef arg():\n    parser = argparse.ArgumentParser(description='Torch model statistics')\n    parser.add_argument('--file', '-f', type=str,\n                        help='Module file.')\n    parser.add_argument('--model', '-m', type=str,\n                        help='Model name')\n    parser.add_argument('--size', '-s', type=str, default='3x224x224',\n                        help='Input size. channels x height x width (default: 3x224x224)')\n    return parser.parse_args()\n\n\ndef main():\n    args = arg()\n    try:\n        spec = importlib.util.spec_from_file_location('models', args.file)\n        module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(module)\n        model = getattr(module, args.model)()\n    except Exception:\n        import traceback\n        print(f'Tried to import {args.model} from {args.file}. but failed.')\n        traceback.print_exc()\n\n        import sys\n        sys.exit()\n\n    input_size = tuple(int(x) for x in args.size.split('x'))\n    stat(model, input_size, query_granularity=1)\n"""
torchstat/compute_flops.py,1,"b'import torch.nn as nn\nimport torch\nimport numpy as np\n\n\ndef compute_flops(module, inp, out):\n    if isinstance(module, nn.Conv2d):\n        return compute_Conv2d_flops(module, inp, out)\n    elif isinstance(module, nn.BatchNorm2d):\n        return compute_BatchNorm2d_flops(module, inp, out)\n    elif isinstance(module, (nn.AvgPool2d, nn.MaxPool2d)):\n        return compute_Pool2d_flops(module, inp, out)\n    elif isinstance(module, (nn.ReLU, nn.ReLU6, nn.PReLU, nn.ELU, nn.LeakyReLU)):\n        return compute_ReLU_flops(module, inp, out)\n    elif isinstance(module, nn.Upsample):\n        return compute_Upsample_flops(module, inp, out)\n    elif isinstance(module, nn.Linear):\n        return compute_Linear_flops(module, inp, out)\n    else:\n        print(f""[Flops]: {type(module).__name__} is not supported!"")\n        return 0\n    pass\n\n\ndef compute_Conv2d_flops(module, inp, out):\n    # Can have multiple inputs, getting the first one\n    assert isinstance(module, nn.Conv2d)\n    assert len(inp.size()) == 4 and len(inp.size()) == len(out.size())\n\n    batch_size = inp.size()[0]\n    in_c = inp.size()[1]\n    k_h, k_w = module.kernel_size\n    out_c, out_h, out_w = out.size()[1:]\n    groups = module.groups\n\n    filters_per_channel = out_c // groups\n    conv_per_position_flops = k_h * k_w * in_c * filters_per_channel\n    active_elements_count = batch_size * out_h * out_w\n\n    total_conv_flops = conv_per_position_flops * active_elements_count\n\n    bias_flops = 0\n    if module.bias is not None:\n        bias_flops = out_c * active_elements_count\n\n    total_flops = total_conv_flops + bias_flops\n    return total_flops\n\n\ndef compute_BatchNorm2d_flops(module, inp, out):\n    assert isinstance(module, nn.BatchNorm2d)\n    assert len(inp.size()) == 4 and len(inp.size()) == len(out.size())\n    in_c, in_h, in_w = inp.size()[1:]\n    batch_flops = np.prod(inp.shape)\n    if module.affine:\n        batch_flops *= 2\n    return batch_flops\n\n\ndef compute_ReLU_flops(module, inp, out):\n    assert isinstance(module, (nn.ReLU, nn.ReLU6, nn.PReLU, nn.ELU, nn.LeakyReLU))\n    batch_size = inp.size()[0]\n    active_elements_count = batch_size\n\n    for s in inp.size()[1:]:\n        active_elements_count *= s\n\n    return active_elements_count\n\n\ndef compute_Pool2d_flops(module, inp, out):\n    assert isinstance(module, nn.MaxPool2d) or isinstance(module, nn.AvgPool2d)\n    assert len(inp.size()) == 4 and len(inp.size()) == len(out.size())\n    return np.prod(inp.shape)\n\n\ndef compute_Linear_flops(module, inp, out):\n    assert isinstance(module, nn.Linear)\n    assert len(inp.size()) == 2 and len(out.size()) == 2\n    batch_size = inp.size()[0]\n    return batch_size * inp.size()[1] * out.size()[1]\n\ndef compute_Upsample_flops(module, inp, out):\n    assert isinstance(module, nn.Upsample)\n    output_size = out[0]\n    batch_size = inp.size()[0]\n    output_elements_count = batch_size\n    for s in output_size.shape[1:]:\n        output_elements_count *= s\n\n    return output_elements_count\n'"
torchstat/compute_madd.py,1,"b'""""""\ncompute Multiply-Adds(MAdd) of each leaf module\n""""""\n\nimport torch.nn as nn\n\n\ndef compute_Conv2d_madd(module, inp, out):\n    assert isinstance(module, nn.Conv2d)\n    assert len(inp.size()) == 4 and len(inp.size()) == len(out.size())\n\n    in_c = inp.size()[1]\n    k_h, k_w = module.kernel_size\n    out_c, out_h, out_w = out.size()[1:]\n    groups = module.groups\n\n    # ops per output element\n    kernel_mul = k_h * k_w * (in_c // groups)\n    kernel_add = kernel_mul - 1 + (0 if module.bias is None else 1)\n\n    kernel_mul_group = kernel_mul * out_h * out_w * (out_c // groups)\n    kernel_add_group = kernel_add * out_h * out_w * (out_c // groups)\n\n    total_mul = kernel_mul_group * groups\n    total_add = kernel_add_group * groups\n\n    return total_mul + total_add\n\n\ndef compute_ConvTranspose2d_madd(module, inp, out):\n    assert isinstance(module, nn.ConvTranspose2d)\n    assert len(inp.size()) == 4 and len(inp.size()) == len(out.size())\n\n    in_c, in_h, in_w = inp.size()[1:]\n    k_h, k_w = module.kernel_size\n    out_c, out_h, out_w = out.size()[1:]\n    groups = module.groups\n\n    kernel_mul = k_h * k_w * (in_c // groups)\n    kernel_add = kernel_mul - 1 + (0 if module.bias is None else 1)\n\n    kernel_mul_group = kernel_mul * in_h * in_w * (out_c // groups)\n    kernel_add_group = kernel_add * in_h * in_w * (out_c // groups)\n\n    total_mul = kernel_mul_group * groups\n    total_add = kernel_add_group * groups\n\n    return total_mul + total_add\n\n\ndef compute_BatchNorm2d_madd(module, inp, out):\n    assert isinstance(module, nn.BatchNorm2d)\n    assert len(inp.size()) == 4 and len(inp.size()) == len(out.size())\n\n    in_c, in_h, in_w = inp.size()[1:]\n\n    # 1. sub mean\n    # 2. div standard deviation\n    # 3. mul alpha\n    # 4. add beta\n    return 4 * in_c * in_h * in_w\n\n\ndef compute_MaxPool2d_madd(module, inp, out):\n    assert isinstance(module, nn.MaxPool2d)\n    assert len(inp.size()) == 4 and len(inp.size()) == len(out.size())\n\n    if isinstance(module.kernel_size, (tuple, list)):\n        k_h, k_w = module.kernel_size\n    else:\n        k_h, k_w = module.kernel_size, module.kernel_size\n    out_c, out_h, out_w = out.size()[1:]\n\n    return (k_h * k_w - 1) * out_h * out_w * out_c\n\n\ndef compute_AvgPool2d_madd(module, inp, out):\n    assert isinstance(module, nn.AvgPool2d)\n    assert len(inp.size()) == 4 and len(inp.size()) == len(out.size())\n\n    if isinstance(module.kernel_size, (tuple, list)):\n        k_h, k_w = module.kernel_size\n    else:\n        k_h, k_w = module.kernel_size, module.kernel_size\n    out_c, out_h, out_w = out.size()[1:]\n\n    kernel_add = k_h * k_w - 1\n    kernel_avg = 1\n\n    return (kernel_add + kernel_avg) * (out_h * out_w) * out_c\n\n\ndef compute_ReLU_madd(module, inp, out):\n    assert isinstance(module, (nn.ReLU, nn.ReLU6))\n\n    count = 1\n    for i in inp.size()[1:]:\n        count *= i\n    return count\n\n\ndef compute_Softmax_madd(module, inp, out):\n    assert isinstance(module, nn.Softmax)\n    assert len(inp.size()) > 1\n\n    count = 1\n    for s in inp.size()[1:]:\n        count *= s\n    exp = count\n    add = count - 1\n    div = count\n    return exp + add + div\n\n\ndef compute_Linear_madd(module, inp, out):\n    assert isinstance(module, nn.Linear)\n    assert len(inp.size()) == 2 and len(out.size()) == 2\n\n    num_in_features = inp.size()[1]\n    num_out_features = out.size()[1]\n\n    mul = num_in_features\n    add = num_in_features - 1\n    return num_out_features * (mul + add)\n\n\ndef compute_Bilinear_madd(module, inp1, inp2, out):\n    assert isinstance(module, nn.Bilinear)\n    assert len(inp1.size()) == 2 and len(inp2.size()) == 2 and len(out.size()) == 2\n\n    num_in_features_1 = inp1.size()[1]\n    num_in_features_2 = inp2.size()[1]\n    num_out_features = out.size()[1]\n\n    mul = num_in_features_1 * num_in_features_2 + num_in_features_2\n    add = num_in_features_1 * num_in_features_2 + num_in_features_2 - 1\n    return num_out_features * (mul + add)\n\n\ndef compute_madd(module, inp, out):\n    if isinstance(module, nn.Conv2d):\n        return compute_Conv2d_madd(module, inp, out)\n    elif isinstance(module, nn.ConvTranspose2d):\n        return compute_ConvTranspose2d_madd(module, inp, out)\n    elif isinstance(module, nn.BatchNorm2d):\n        return compute_BatchNorm2d_madd(module, inp, out)\n    elif isinstance(module, nn.MaxPool2d):\n        return compute_MaxPool2d_madd(module, inp, out)\n    elif isinstance(module, nn.AvgPool2d):\n        return compute_AvgPool2d_madd(module, inp, out)\n    elif isinstance(module, (nn.ReLU, nn.ReLU6)):\n        return compute_ReLU_madd(module, inp, out)\n    elif isinstance(module, nn.Softmax):\n        return compute_Softmax_madd(module, inp, out)\n    elif isinstance(module, nn.Linear):\n        return compute_Linear_madd(module, inp, out)\n    elif isinstance(module, nn.Bilinear):\n        return compute_Bilinear_madd(module, inp[0], inp[1], out)\n    else:\n        print(f""[MAdd]: {type(module).__name__} is not supported!"")\n        return 0\n'"
torchstat/compute_memory.py,1,"b'import torch.nn as nn\nimport torch\nimport numpy as np\n\n\ndef compute_memory(module, inp, out):\n    if isinstance(module, (nn.ReLU, nn.ReLU6, nn.ELU, nn.LeakyReLU)):\n        return compute_ReLU_memory(module, inp, out)\n    elif isinstance(module, nn.PReLU):\n        return compute_PReLU_memory(module, inp, out)\n    elif isinstance(module, nn.Conv2d):\n        return compute_Conv2d_memory(module, inp, out)\n    elif isinstance(module, nn.BatchNorm2d):\n        return compute_BatchNorm2d_memory(module, inp, out)\n    elif isinstance(module, nn.Linear):\n        return compute_Linear_memory(module, inp, out)\n    elif isinstance(module, (nn.AvgPool2d, nn.MaxPool2d)):\n        return compute_Pool2d_memory(module, inp, out)\n    else:\n        print(f""[Memory]: {type(module).__name__} is not supported!"")\n        return (0, 0)\n    pass\n\n\ndef num_params(module):\n    return sum(p.numel() for p in module.parameters() if p.requires_grad)\n\n\ndef compute_ReLU_memory(module, inp, out):\n    assert isinstance(module, (nn.ReLU, nn.ReLU6, nn.ELU, nn.LeakyReLU))\n    batch_size = inp.size()[0]\n    mread = batch_size * inp.size()[1:].numel()\n    mwrite = batch_size * inp.size()[1:].numel()\n\n    return (mread, mwrite)\n\n\ndef compute_PReLU_memory(module, inp, out):\n    assert isinstance(module, (nn.PReLU))\n    batch_size = inp.size()[0]\n    mread = batch_size * (inp.size()[1:].numel() + num_params(module))\n    mwrite = batch_size * inp.size()[1:].numel()\n\n    return (mread, mwrite)\n\n\ndef compute_Conv2d_memory(module, inp, out):\n    # Can have multiple inputs, getting the first one\n    assert isinstance(module, nn.Conv2d)\n    assert len(inp.size()) == 4 and len(inp.size()) == len(out.size())\n\n    batch_size = inp.size()[0]\n    in_c = inp.size()[1]\n    out_c, out_h, out_w = out.size()[1:]\n\n    # This includes weighs with bias if the module contains it.\n    mread = batch_size * (inp.size()[1:].numel() + num_params(module))\n    mwrite = batch_size * out_c * out_h * out_w\n    return (mread, mwrite)\n\n\ndef compute_BatchNorm2d_memory(module, inp, out):\n    assert isinstance(module, nn.BatchNorm2d)\n    assert len(inp.size()) == 4 and len(inp.size()) == len(out.size())\n    batch_size, in_c, in_h, in_w = inp.size()\n\n    mread = batch_size * (inp.size()[1:].numel() + 2 * in_c)\n    mwrite = inp.size().numel()\n    return (mread, mwrite)\n\n\ndef compute_Linear_memory(module, inp, out):\n    assert isinstance(module, nn.Linear)\n    assert len(inp.size()) == 2 and len(out.size()) == 2\n    batch_size = inp.size()[0]\n    mread = batch_size * (inp.size()[1:].numel() + num_params(module))\n    mwrite = out.size().numel()\n\n    return (mread, mwrite)\n\n\ndef compute_Pool2d_memory(module, inp, out):\n    assert isinstance(module, (nn.MaxPool2d, nn.AvgPool2d))\n    assert len(inp.size()) == 4 and len(inp.size()) == len(out.size())\n    batch_size = inp.size()[0]\n    mread = batch_size * inp.size()[1:].numel()\n    mwrite = batch_size * out.size()[1:].numel()\n    return (mread, mwrite)\n'"
torchstat/model_hook.py,19,"b""import time\nfrom collections import OrderedDict\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom torchstat import compute_madd\nfrom torchstat import compute_flops\nfrom torchstat import compute_memory\n\n\nclass ModelHook(object):\n    def __init__(self, model, input_size):\n        assert isinstance(model, nn.Module)\n        assert isinstance(input_size, (list, tuple))\n\n        self._model = model\n        self._input_size = input_size\n        self._origin_call = dict()  # sub module call hook\n\n        self._hook_model()\n        x = torch.rand(1, *self._input_size)  # add module duration time\n        self._model.eval()\n        self._model(x)\n\n    @staticmethod\n    def _register_buffer(module):\n        assert isinstance(module, nn.Module)\n\n        if len(list(module.children())) > 0:\n            return\n\n        module.register_buffer('input_shape', torch.zeros(3).int())\n        module.register_buffer('output_shape', torch.zeros(3).int())\n        module.register_buffer('parameter_quantity', torch.zeros(1).int())\n        module.register_buffer('inference_memory', torch.zeros(1).long())\n        module.register_buffer('MAdd', torch.zeros(1).long())\n        module.register_buffer('duration', torch.zeros(1).float())\n        module.register_buffer('Flops', torch.zeros(1).long())\n        module.register_buffer('Memory', torch.zeros(2).long())\n\n    def _sub_module_call_hook(self):\n        def wrap_call(module, *input, **kwargs):\n            assert module.__class__ in self._origin_call\n\n            # Itemsize for memory\n            itemsize = input[0].detach().numpy().itemsize\n\n            start = time.time()\n            output = self._origin_call[module.__class__](module, *input, **kwargs)\n            end = time.time()\n            module.duration = torch.from_numpy(\n                np.array([end - start], dtype=np.float32))\n\n            module.input_shape = torch.from_numpy(\n                np.array(input[0].size()[1:], dtype=np.int32))\n            module.output_shape = torch.from_numpy(\n                np.array(output.size()[1:], dtype=np.int32))\n\n            parameter_quantity = 0\n            # iterate through parameters and count num params\n            for name, p in module._parameters.items():\n                parameter_quantity += (0 if p is None else torch.numel(p.data))\n            module.parameter_quantity = torch.from_numpy(\n                np.array([parameter_quantity], dtype=np.long))\n\n            inference_memory = 1\n            for s in output.size()[1:]:\n                inference_memory *= s\n            # memory += parameters_number  # exclude parameter memory\n            inference_memory = inference_memory * 4 / (1024 ** 2)  # shown as MB unit\n            module.inference_memory = torch.from_numpy(\n                np.array([inference_memory], dtype=np.float32))\n\n            if len(input) == 1:\n                madd = compute_madd(module, input[0], output)\n                flops = compute_flops(module, input[0], output)\n                Memory = compute_memory(module, input[0], output)\n            elif len(input) > 1:\n                madd = compute_madd(module, input, output)\n                flops = compute_flops(module, input, output)\n                Memory = compute_memory(module, input, output)\n            else:  # error\n                madd = 0\n                flops = 0\n                Memory = (0, 0)\n            module.MAdd = torch.from_numpy(\n                np.array([madd], dtype=np.int64))\n            module.Flops = torch.from_numpy(\n                np.array([flops], dtype=np.int64))\n            Memory = np.array(Memory, dtype=np.int32) * itemsize\n            module.Memory = torch.from_numpy(Memory)\n\n            return output\n\n        for module in self._model.modules():\n            if len(list(module.children())) == 0 and module.__class__ not in self._origin_call:\n                self._origin_call[module.__class__] = module.__class__.__call__\n                module.__class__.__call__ = wrap_call\n\n    def _hook_model(self):\n        self._model.apply(self._register_buffer)\n        self._sub_module_call_hook()\n\n    @staticmethod\n    def _retrieve_leaf_modules(model):\n        leaf_modules = []\n        for name, m in model.named_modules():\n            if len(list(m.children())) == 0:\n                leaf_modules.append((name, m))\n        return leaf_modules\n\n    def retrieve_leaf_modules(self):\n        return OrderedDict(self._retrieve_leaf_modules(self._model))\n"""
torchstat/reporter.py,0,"b'import pandas as pd\n\n\npd.set_option(\'display.width\', 1000)\npd.set_option(\'display.max_rows\', 10000)\npd.set_option(\'display.max_columns\', 10000)\n\n\ndef round_value(value, binary=False):\n    divisor = 1024. if binary else 1000.\n\n    if value // divisor**4 > 0:\n        return str(round(value / divisor**4, 2)) + \'T\'\n    elif value // divisor**3 > 0:\n        return str(round(value / divisor**3, 2)) + \'G\'\n    elif value // divisor**2 > 0:\n        return str(round(value / divisor**2, 2)) + \'M\'\n    elif value // divisor > 0:\n        return str(round(value / divisor, 2)) + \'K\'\n    return str(value)\n\n\ndef report_format(collected_nodes):\n    data = list()\n    for node in collected_nodes:\n        name = node.name\n        input_shape = \' \'.join([\'{:>3d}\'] * len(node.input_shape)).format(\n            *[e for e in node.input_shape])\n        output_shape = \' \'.join([\'{:>3d}\'] * len(node.output_shape)).format(\n            *[e for e in node.output_shape])\n        parameter_quantity = node.parameter_quantity\n        inference_memory = node.inference_memory\n        MAdd = node.MAdd\n        Flops = node.Flops\n        mread, mwrite = [i for i in node.Memory]\n        duration = node.duration\n        data.append([name, input_shape, output_shape, parameter_quantity,\n                     inference_memory, MAdd, duration, Flops, mread,\n                     mwrite])\n    df = pd.DataFrame(data)\n    df.columns = [\'module name\', \'input shape\', \'output shape\',\n                  \'params\', \'memory(MB)\',\n                  \'MAdd\', \'duration\', \'Flops\', \'MemRead(B)\', \'MemWrite(B)\']\n    df[\'duration[%]\'] = df[\'duration\'] / (df[\'duration\'].sum() + 1e-7)\n    df[\'MemR+W(B)\'] = df[\'MemRead(B)\'] + df[\'MemWrite(B)\']\n    total_parameters_quantity = df[\'params\'].sum()\n    total_memory = df[\'memory(MB)\'].sum()\n    total_operation_quantity = df[\'MAdd\'].sum()\n    total_flops = df[\'Flops\'].sum()\n    total_duration = df[\'duration[%]\'].sum()\n    total_mread = df[\'MemRead(B)\'].sum()\n    total_mwrite = df[\'MemWrite(B)\'].sum()\n    total_memrw = df[\'MemR+W(B)\'].sum()\n    del df[\'duration\']\n\n    # Add Total row\n    total_df = pd.Series([total_parameters_quantity, total_memory,\n                          total_operation_quantity, total_flops,\n                          total_duration, mread, mwrite, total_memrw],\n                         index=[\'params\', \'memory(MB)\', \'MAdd\', \'Flops\', \'duration[%]\',\n                                \'MemRead(B)\', \'MemWrite(B)\', \'MemR+W(B)\'],\n                         name=\'total\')\n    df = df.append(total_df)\n\n    df = df.fillna(\' \')\n    df[\'memory(MB)\'] = df[\'memory(MB)\'].apply(\n        lambda x: \'{:.2f}\'.format(x))\n    df[\'duration[%]\'] = df[\'duration[%]\'].apply(lambda x: \'{:.2%}\'.format(x))\n    df[\'MAdd\'] = df[\'MAdd\'].apply(lambda x: \'{:,}\'.format(x))\n    df[\'Flops\'] = df[\'Flops\'].apply(lambda x: \'{:,}\'.format(x))\n\n    summary = str(df) + \'\\n\'\n    summary += ""="" * len(str(df).split(\'\\n\')[0])\n    summary += \'\\n\'\n    summary += ""Total params: {:,}\\n"".format(total_parameters_quantity)\n\n    summary += ""-"" * len(str(df).split(\'\\n\')[0])\n    summary += \'\\n\'\n    summary += ""Total memory: {:.2f}MB\\n"".format(total_memory)\n    summary += ""Total MAdd: {}MAdd\\n"".format(round_value(total_operation_quantity))\n    summary += ""Total Flops: {}Flops\\n"".format(round_value(total_flops))\n    summary += ""Total MemR+W: {}B\\n"".format(round_value(total_memrw, True))\n    return summary\n'"
torchstat/stat_tree.py,0,"b'import queue\n\n\nclass StatTree(object):\n    def __init__(self, root_node):\n        assert isinstance(root_node, StatNode)\n\n        self.root_node = root_node\n\n    def get_same_level_max_node_depth(self, query_node):\n        if query_node.name == self.root_node.name:\n            return 0\n        same_level_depth = max([child.depth for child in query_node.parent.children])\n        return same_level_depth\n\n    def update_stat_nodes_granularity(self):\n        q = queue.Queue()\n        q.put(self.root_node)\n        while not q.empty():\n            node = q.get()\n            node.granularity = self.get_same_level_max_node_depth(node)\n            for child in node.children:\n                q.put(child)\n\n    def get_collected_stat_nodes(self, query_granularity):\n        self.update_stat_nodes_granularity()\n\n        collected_nodes = []\n        stack = list()\n        stack.append(self.root_node)\n        while len(stack) > 0:\n            node = stack.pop()\n            for child in reversed(node.children):\n                stack.append(child)\n            if node.depth == query_granularity:\n                collected_nodes.append(node)\n            if node.depth < query_granularity <= node.granularity:\n                collected_nodes.append(node)\n        return collected_nodes\n\n\nclass StatNode(object):\n    def __init__(self, name=str(), parent=None):\n        self._name = name\n        self._input_shape = None\n        self._output_shape = None\n        self._parameter_quantity = 0\n        self._inference_memory = 0\n        self._MAdd = 0\n        self._Memory = (0, 0)\n        self._Flops = 0\n        self._duration = 0\n        self._duration_percent = 0\n\n        self._granularity = 1\n        self._depth = 1\n        self.parent = parent\n        self.children = list()\n\n    @property\n    def name(self):\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        self._name = name\n\n    @property\n    def granularity(self):\n        return self._granularity\n\n    @granularity.setter\n    def granularity(self, g):\n        self._granularity = g\n\n    @property\n    def depth(self):\n        d = self._depth\n        if len(self.children) > 0:\n            d += max([child.depth for child in self.children])\n        return d\n\n    @property\n    def input_shape(self):\n        if len(self.children) == 0:  # leaf\n            return self._input_shape\n        else:\n            return self.children[0].input_shape\n\n    @input_shape.setter\n    def input_shape(self, input_shape):\n        assert isinstance(input_shape, (list, tuple))\n        self._input_shape = input_shape\n\n    @property\n    def output_shape(self):\n        if len(self.children) == 0:  # leaf\n            return self._output_shape\n        else:\n            return self.children[-1].output_shape\n\n    @output_shape.setter\n    def output_shape(self, output_shape):\n        assert isinstance(output_shape, (list, tuple))\n        self._output_shape = output_shape\n\n    @property\n    def parameter_quantity(self):\n        # return self.parameters_quantity\n        total_parameter_quantity = self._parameter_quantity\n        for child in self.children:\n            total_parameter_quantity += child.parameter_quantity\n        return total_parameter_quantity\n\n    @parameter_quantity.setter\n    def parameter_quantity(self, parameter_quantity):\n        assert parameter_quantity >= 0\n        self._parameter_quantity = parameter_quantity\n\n    @property\n    def inference_memory(self):\n        total_inference_memory = self._inference_memory\n        for child in self.children:\n            total_inference_memory += child.inference_memory\n        return total_inference_memory\n\n    @inference_memory.setter\n    def inference_memory(self, inference_memory):\n        self._inference_memory = inference_memory\n\n    @property\n    def MAdd(self):\n        total_MAdd = self._MAdd\n        for child in self.children:\n            total_MAdd += child.MAdd\n        return total_MAdd\n\n    @MAdd.setter\n    def MAdd(self, MAdd):\n        self._MAdd = MAdd\n\n    @property\n    def Flops(self):\n        total_Flops = self._Flops\n        for child in self.children:\n            total_Flops += child.Flops\n        return total_Flops\n\n    @Flops.setter\n    def Flops(self, Flops):\n        self._Flops = Flops\n\n    @property\n    def Memory(self):\n        total_Memory = self._Memory\n        for child in self.children:\n            total_Memory[0] += child.Memory[0]\n            total_Memory[1] += child.Memory[1]\n            print(total_Memory)\n        return total_Memory\n\n    @Memory.setter\n    def Memory(self, Memory):\n        assert isinstance(Memory, (list, tuple))\n        self._Memory = Memory\n\n    @property\n    def duration(self):\n        total_duration = self._duration\n        for child in self.children:\n            total_duration += child.duration\n        return total_duration\n\n    @duration.setter\n    def duration(self, duration):\n        self._duration = duration\n\n    def find_child_index(self, child_name):\n        assert isinstance(child_name, str)\n\n        index = -1\n        for i in range(len(self.children)):\n            if child_name == self.children[i].name:\n                index = i\n        return index\n\n    def add_child(self, node):\n        assert isinstance(node, StatNode)\n\n        if self.find_child_index(node.name) == -1:  # not exist\n            self.children.append(node)\n'"
torchstat/statistics.py,1,"b""import torch\nimport torch.nn as nn\nfrom torchstat import ModelHook\nfrom collections import OrderedDict\nfrom torchstat import StatTree, StatNode, report_format\n\n\ndef get_parent_node(root_node, stat_node_name):\n    assert isinstance(root_node, StatNode)\n\n    node = root_node\n    names = stat_node_name.split('.')\n    for i in range(len(names) - 1):\n        node_name = '.'.join(names[0:i+1])\n        child_index = node.find_child_index(node_name)\n        assert child_index != -1\n        node = node.children[child_index]\n    return node\n\n\ndef convert_leaf_modules_to_stat_tree(leaf_modules):\n    assert isinstance(leaf_modules, OrderedDict)\n\n    create_index = 1\n    root_node = StatNode(name='root', parent=None)\n    for leaf_module_name, leaf_module in leaf_modules.items():\n        names = leaf_module_name.split('.')\n        for i in range(len(names)):\n            create_index += 1\n            stat_node_name = '.'.join(names[0:i+1])\n            parent_node = get_parent_node(root_node, stat_node_name)\n            node = StatNode(name=stat_node_name, parent=parent_node)\n            parent_node.add_child(node)\n            if i == len(names) - 1:  # leaf module itself\n                input_shape = leaf_module.input_shape.numpy().tolist()\n                output_shape = leaf_module.output_shape.numpy().tolist()\n                node.input_shape = input_shape\n                node.output_shape = output_shape\n                node.parameter_quantity = leaf_module.parameter_quantity.numpy()[0]\n                node.inference_memory = leaf_module.inference_memory.numpy()[0]\n                node.MAdd = leaf_module.MAdd.numpy()[0]\n                node.Flops = leaf_module.Flops.numpy()[0]\n                node.duration = leaf_module.duration.numpy()[0]\n                node.Memory = leaf_module.Memory.numpy().tolist()\n    return StatTree(root_node)\n\n\nclass ModelStat(object):\n    def __init__(self, model, input_size, query_granularity=1):\n        assert isinstance(model, nn.Module)\n        assert isinstance(input_size, (tuple, list)) and len(input_size) == 3\n        self._model = model\n        self._input_size = input_size\n        self._query_granularity = query_granularity\n\n    def _analyze_model(self):\n        model_hook = ModelHook(self._model, self._input_size)\n        leaf_modules = model_hook.retrieve_leaf_modules()\n        stat_tree = convert_leaf_modules_to_stat_tree(leaf_modules)\n        collected_nodes = stat_tree.get_collected_stat_nodes(self._query_granularity)\n        return collected_nodes\n\n    def show_report(self):\n        collected_nodes = self._analyze_model()\n        report = report_format(collected_nodes)\n        print(report)\n\n\ndef stat(model, input_size, query_granularity=1):\n    ms = ModelStat(model, input_size, query_granularity)\n    ms.show_report()\n"""
