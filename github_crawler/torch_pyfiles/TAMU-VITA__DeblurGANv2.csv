file_path,api_count,code
adversarial_trainer.py,1,"b""import torch\nimport copy\n\n\nclass GANFactory:\n    factories = {}\n\n    def __init__(self):\n        pass\n\n    def add_factory(gan_id, model_factory):\n        GANFactory.factories.put[gan_id] = model_factory\n\n    add_factory = staticmethod(add_factory)\n\n    # A Template Method:\n\n    def create_model(gan_id, net_d=None, criterion=None):\n        if gan_id not in GANFactory.factories:\n            GANFactory.factories[gan_id] = \\\n                eval(gan_id + '.Factory()')\n        return GANFactory.factories[gan_id].create(net_d, criterion)\n\n    create_model = staticmethod(create_model)\n\n\nclass GANTrainer(object):\n    def __init__(self, net_d, criterion):\n        self.net_d = net_d\n        self.criterion = criterion\n\n    def loss_d(self, pred, gt):\n        pass\n\n    def loss_g(self, pred, gt):\n        pass\n\n    def get_params(self):\n        pass\n\n\nclass NoGAN(GANTrainer):\n    def __init__(self, net_d, criterion):\n        GANTrainer.__init__(self, net_d, criterion)\n\n    def loss_d(self, pred, gt):\n        return [0]\n\n    def loss_g(self, pred, gt):\n        return 0\n\n    def get_params(self):\n        return [torch.nn.Parameter(torch.Tensor(1))]\n\n    class Factory:\n        @staticmethod\n        def create(net_d, criterion): return NoGAN(net_d, criterion)\n\n\nclass SingleGAN(GANTrainer):\n    def __init__(self, net_d, criterion):\n        GANTrainer.__init__(self, net_d, criterion)\n        self.net_d = self.net_d.cuda()\n\n    def loss_d(self, pred, gt):\n        return self.criterion(self.net_d, pred, gt)\n\n    def loss_g(self, pred, gt):\n        return self.criterion.get_g_loss(self.net_d, pred, gt)\n\n    def get_params(self):\n        return self.net_d.parameters()\n\n    class Factory:\n        @staticmethod\n        def create(net_d, criterion): return SingleGAN(net_d, criterion)\n\n\nclass DoubleGAN(GANTrainer):\n    def __init__(self, net_d, criterion):\n        GANTrainer.__init__(self, net_d, criterion)\n        self.patch_d = net_d['patch'].cuda()\n        self.full_d = net_d['full'].cuda()\n        self.full_criterion = copy.deepcopy(criterion)\n\n    def loss_d(self, pred, gt):\n        return (self.criterion(self.patch_d, pred, gt) + self.full_criterion(self.full_d, pred, gt)) / 2\n\n    def loss_g(self, pred, gt):\n        return (self.criterion.get_g_loss(self.patch_d, pred, gt) + self.full_criterion.get_g_loss(self.full_d, pred,\n                                                                                                  gt)) / 2\n\n    def get_params(self):\n        return list(self.patch_d.parameters()) + list(self.full_d.parameters())\n\n    class Factory:\n        @staticmethod\n        def create(net_d, criterion): return DoubleGAN(net_d, criterion)\n\n"""
aug.py,0,"b""from typing import List\n\nimport albumentations as albu\n\n\ndef get_transforms(size: int, scope: str = 'geometric', crop='random'):\n    augs = {'strong': albu.Compose([albu.HorizontalFlip(),\n                                    albu.ShiftScaleRotate(shift_limit=0.0, scale_limit=0.2, rotate_limit=20, p=.4),\n                                    albu.ElasticTransform(),\n                                    albu.OpticalDistortion(),\n                                    albu.OneOf([\n                                        albu.CLAHE(clip_limit=2),\n                                        albu.IAASharpen(),\n                                        albu.IAAEmboss(),\n                                        albu.RandomBrightnessContrast(),\n                                        albu.RandomGamma()\n                                    ], p=0.5),\n                                    albu.OneOf([\n                                        albu.RGBShift(),\n                                        albu.HueSaturationValue(),\n                                    ], p=0.5),\n                                    ]),\n            'weak': albu.Compose([albu.HorizontalFlip(),\n                                  ]),\n            'geometric': albu.OneOf([albu.HorizontalFlip(always_apply=True),\n                                     albu.ShiftScaleRotate(always_apply=True),\n                                     albu.Transpose(always_apply=True),\n                                     albu.OpticalDistortion(always_apply=True),\n                                     albu.ElasticTransform(always_apply=True),\n                                     ])\n            }\n\n    aug_fn = augs[scope]\n    crop_fn = {'random': albu.RandomCrop(size, size, always_apply=True),\n               'center': albu.CenterCrop(size, size, always_apply=True)}[crop]\n    pad = albu.PadIfNeeded(size, size)\n\n    pipeline = albu.Compose([aug_fn, crop_fn, pad], additional_targets={'target': 'image'})\n\n    def process(a, b):\n        r = pipeline(image=a, target=b)\n        return r['image'], r['target']\n\n    return process\n\n\ndef get_normalize():\n    normalize = albu.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n    normalize = albu.Compose([normalize], additional_targets={'target': 'image'})\n\n    def process(a, b):\n        r = normalize(image=a, target=b)\n        return r['image'], r['target']\n\n    return process\n\n\ndef _resolve_aug_fn(name):\n    d = {\n        'cutout': albu.Cutout,\n        'rgb_shift': albu.RGBShift,\n        'hsv_shift': albu.HueSaturationValue,\n        'motion_blur': albu.MotionBlur,\n        'median_blur': albu.MedianBlur,\n        'snow': albu.RandomSnow,\n        'shadow': albu.RandomShadow,\n        'fog': albu.RandomFog,\n        'brightness_contrast': albu.RandomBrightnessContrast,\n        'gamma': albu.RandomGamma,\n        'sun_flare': albu.RandomSunFlare,\n        'sharpen': albu.IAASharpen,\n        'jpeg': albu.JpegCompression,\n        'gray': albu.ToGray,\n        # ToDo: pixelize\n        # ToDo: partial gray\n    }\n    return d[name]\n\n\ndef get_corrupt_function(config: List[dict]):\n    augs = []\n    for aug_params in config:\n        name = aug_params.pop('name')\n        cls = _resolve_aug_fn(name)\n        prob = aug_params.pop('prob') if 'prob' in aug_params else .5\n        augs.append(cls(p=prob, **aug_params))\n\n    augs = albu.OneOf(augs)\n\n    def process(x):\n        return augs(image=x)['image']\n\n    return process\n"""
dataset.py,1,"b""import os\nfrom copy import deepcopy\nfrom functools import partial\nfrom glob import glob\nfrom hashlib import sha1\nfrom typing import Callable, Iterable, Optional, Tuple\n\nimport cv2\nimport numpy as np\nfrom glog import logger\nfrom joblib import Parallel, cpu_count, delayed\nfrom skimage.io import imread\nfrom torch.utils.data import Dataset\nfrom tqdm import tqdm\n\nimport aug\n\n\ndef subsample(data: Iterable, bounds: Tuple[float, float], hash_fn: Callable, n_buckets=100, salt='', verbose=True):\n    data = list(data)\n    buckets = split_into_buckets(data, n_buckets=n_buckets, salt=salt, hash_fn=hash_fn)\n\n    lower_bound, upper_bound = [x * n_buckets for x in bounds]\n    msg = f'Subsampling buckets from {lower_bound} to {upper_bound}, total buckets number is {n_buckets}'\n    if salt:\n        msg += f'; salt is {salt}'\n    if verbose:\n        logger.info(msg)\n    return np.array([sample for bucket, sample in zip(buckets, data) if lower_bound <= bucket < upper_bound])\n\n\ndef hash_from_paths(x: Tuple[str, str], salt: str = '') -> str:\n    path_a, path_b = x\n    names = ''.join(map(os.path.basename, (path_a, path_b)))\n    return sha1(f'{names}_{salt}'.encode()).hexdigest()\n\n\ndef split_into_buckets(data: Iterable, n_buckets: int, hash_fn: Callable, salt=''):\n    hashes = map(partial(hash_fn, salt=salt), data)\n    return np.array([int(x, 16) % n_buckets for x in hashes])\n\n\ndef _read_img(x: str):\n    img = cv2.imread(x)\n    if img is None:\n        logger.warning(f'Can not read image {x} with OpenCV, switching to scikit-image')\n        img = imread(x)\n    return img\n\n\nclass PairedDataset(Dataset):\n    def __init__(self,\n                 files_a: Tuple[str],\n                 files_b: Tuple[str],\n                 transform_fn: Callable,\n                 normalize_fn: Callable,\n                 corrupt_fn: Optional[Callable] = None,\n                 preload: bool = True,\n                 preload_size: Optional[int] = 0,\n                 verbose=True):\n\n        assert len(files_a) == len(files_b)\n\n        self.preload = preload\n        self.data_a = files_a\n        self.data_b = files_b\n        self.verbose = verbose\n        self.corrupt_fn = corrupt_fn\n        self.transform_fn = transform_fn\n        self.normalize_fn = normalize_fn\n        logger.info(f'Dataset has been created with {len(self.data_a)} samples')\n\n        if preload:\n            preload_fn = partial(self._bulk_preload, preload_size=preload_size)\n            if files_a == files_b:\n                self.data_a = self.data_b = preload_fn(self.data_a)\n            else:\n                self.data_a, self.data_b = map(preload_fn, (self.data_a, self.data_b))\n            self.preload = True\n\n    def _bulk_preload(self, data: Iterable[str], preload_size: int):\n        jobs = [delayed(self._preload)(x, preload_size=preload_size) for x in data]\n        jobs = tqdm(jobs, desc='preloading images', disable=not self.verbose)\n        return Parallel(n_jobs=cpu_count(), backend='threading')(jobs)\n\n    @staticmethod\n    def _preload(x: str, preload_size: int):\n        img = _read_img(x)\n        if preload_size:\n            h, w, *_ = img.shape\n            h_scale = preload_size / h\n            w_scale = preload_size / w\n            scale = max(h_scale, w_scale)\n            img = cv2.resize(img, fx=scale, fy=scale, dsize=None)\n            assert min(img.shape[:2]) >= preload_size, f'weird img shape: {img.shape}'\n        return img\n\n    def _preprocess(self, img, res):\n        def transpose(x):\n            return np.transpose(x, (2, 0, 1))\n\n        return map(transpose, self.normalize_fn(img, res))\n\n    def __len__(self):\n        return len(self.data_a)\n\n    def __getitem__(self, idx):\n        a, b = self.data_a[idx], self.data_b[idx]\n        if not self.preload:\n            a, b = map(_read_img, (a, b))\n        a, b = self.transform_fn(a, b)\n        if self.corrupt_fn is not None:\n            a = self.corrupt_fn(a)\n        a, b = self._preprocess(a, b)\n        return {'a': a, 'b': b}\n\n    @staticmethod\n    def from_config(config):\n        config = deepcopy(config)\n        files_a, files_b = map(lambda x: sorted(glob(config[x], recursive=True)), ('files_a', 'files_b'))\n        transform_fn = aug.get_transforms(size=config['size'], scope=config['scope'], crop=config['crop'])\n        normalize_fn = aug.get_normalize()\n        corrupt_fn = aug.get_corrupt_function(config['corrupt'])\n\n        hash_fn = hash_from_paths\n        # ToDo: add more hash functions\n        verbose = config.get('verbose', True)\n        data = subsample(data=zip(files_a, files_b),\n                         bounds=config.get('bounds', (0, 1)),\n                         hash_fn=hash_fn,\n                         verbose=verbose)\n\n        files_a, files_b = map(list, zip(*data))\n\n        return PairedDataset(files_a=files_a,\n                             files_b=files_b,\n                             preload=config['preload'],\n                             preload_size=config['preload_size'],\n                             corrupt_fn=corrupt_fn,\n                             normalize_fn=normalize_fn,\n                             transform_fn=transform_fn,\n                             verbose=verbose)\n"""
metric_counter.py,0,"b""import logging\nfrom collections import defaultdict\n\nimport numpy as np\nfrom tensorboardX import SummaryWriter\n\nWINDOW_SIZE = 100\n\n\nclass MetricCounter:\n    def __init__(self, exp_name):\n        self.writer = SummaryWriter(exp_name)\n        logging.basicConfig(filename='{}.log'.format(exp_name), level=logging.DEBUG)\n        self.metrics = defaultdict(list)\n        self.images = defaultdict(list)\n        self.best_metric = 0\n\n    def add_image(self, x: np.ndarray, tag: str):\n        self.images[tag].append(x)\n\n    def clear(self):\n        self.metrics = defaultdict(list)\n        self.images = defaultdict(list)\n\n    def add_losses(self, l_G, l_content, l_D=0):\n        for name, value in zip(('G_loss', 'G_loss_content', 'G_loss_adv', 'D_loss'),\n                               (l_G, l_content, l_G - l_content, l_D)):\n            self.metrics[name].append(value)\n\n    def add_metrics(self, psnr, ssim):\n        for name, value in zip(('PSNR', 'SSIM'),\n                               (psnr, ssim)):\n            self.metrics[name].append(value)\n\n    def loss_message(self):\n        metrics = ((k, np.mean(self.metrics[k][-WINDOW_SIZE:])) for k in ('G_loss', 'PSNR', 'SSIM'))\n        return '; '.join(map(lambda x: f'{x[0]}={x[1]:.4f}', metrics))\n\n    def write_to_tensorboard(self, epoch_num, validation=False):\n        scalar_prefix = 'Validation' if validation else 'Train'\n        for tag in ('G_loss', 'D_loss', 'G_loss_adv', 'G_loss_content', 'SSIM', 'PSNR'):\n            self.writer.add_scalar(f'{scalar_prefix}_{tag}', np.mean(self.metrics[tag]), global_step=epoch_num)\n        for tag in self.images:\n            imgs = self.images[tag]\n            if imgs:\n                imgs = np.array(imgs)\n                self.writer.add_images(tag, imgs[:, :, :, ::-1].astype('float32') / 255, dataformats='NHWC',\n                                       global_step=epoch_num)\n                self.images[tag] = []\n\n    def update_best_model(self):\n        cur_metric = np.mean(self.metrics['PSNR'])\n        if self.best_metric < cur_metric:\n            self.best_metric = cur_metric\n            return True\n        return False\n"""
predict.py,4,"b""import os\nfrom glob import glob\nfrom typing import Optional\n\nimport cv2\nimport numpy as np\nimport torch\nimport yaml\nfrom fire import Fire\nfrom tqdm import tqdm\n\nfrom aug import get_normalize\nfrom models.networks import get_generator\n\n\nclass Predictor:\n    def __init__(self, weights_path: str, model_name: str = ''):\n        with open('config/config.yaml') as cfg:\n            config = yaml.load(cfg)\n        model = get_generator(model_name or config['model'])\n        model.load_state_dict(torch.load(weights_path)['model'])\n        self.model = model.cuda()\n        self.model.train(True)\n        # GAN inference should be in train mode to use actual stats in norm layers,\n        # it's not a bug\n        self.normalize_fn = get_normalize()\n\n    @staticmethod\n    def _array_to_batch(x):\n        x = np.transpose(x, (2, 0, 1))\n        x = np.expand_dims(x, 0)\n        return torch.from_numpy(x)\n\n    def _preprocess(self, x: np.ndarray, mask: Optional[np.ndarray]):\n        x, _ = self.normalize_fn(x, x)\n        if mask is None:\n            mask = np.ones_like(x, dtype=np.float32)\n        else:\n            mask = np.round(mask.astype('float32') / 255)\n\n        h, w, _ = x.shape\n        block_size = 32\n        min_height = (h // block_size + 1) * block_size\n        min_width = (w // block_size + 1) * block_size\n\n        pad_params = {'mode': 'constant',\n                      'constant_values': 0,\n                      'pad_width': ((0, min_height - h), (0, min_width - w), (0, 0))\n                      }\n        x = np.pad(x, **pad_params)\n        mask = np.pad(mask, **pad_params)\n\n        return map(self._array_to_batch, (x, mask)), h, w\n\n    @staticmethod\n    def _postprocess(x: torch.Tensor) -> np.ndarray:\n        x, = x\n        x = x.detach().cpu().float().numpy()\n        x = (np.transpose(x, (1, 2, 0)) + 1) / 2.0 * 255.0\n        return x.astype('uint8')\n\n    def __call__(self, img: np.ndarray, mask: Optional[np.ndarray], ignore_mask=True) -> np.ndarray:\n        (img, mask), h, w = self._preprocess(img, mask)\n        with torch.no_grad():\n            inputs = [img.cuda()]\n            if not ignore_mask:\n                inputs += [mask]\n            pred = self.model(*inputs)\n        return self._postprocess(pred)[:h, :w, :]\n\ndef process_video(pairs, predictor, output_dir):\n    for video_filepath, mask in tqdm(pairs):\n        video_filename = os.path.basename(video_filepath)\n        output_filepath = os.path.join(output_dir, os.path.splitext(video_filename)[0]+'_deblur.mp4')\n        video_in = cv2.VideoCapture(video_filepath)\n        fps = video_in.get(cv2.CAP_PROP_FPS)\n        width = int(video_in.get(cv2.CAP_PROP_FRAME_WIDTH))\n        height = int(video_in.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        total_frame_num = int(video_in.get(cv2.CAP_PROP_FRAME_COUNT))\n        video_out = cv2.VideoWriter(output_filepath, cv2.VideoWriter_fourcc(*'MP4V'), fps, (width, height))\n        tqdm.write(f'process {video_filepath} to {output_filepath}, {fps}fps, resolution: {width}x{height}')\n        for frame_num in tqdm(range(total_frame_num), desc=video_filename):\n            res, img = video_in.read()\n            if not res:\n                break\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            pred = predictor(img, mask)\n            pred = cv2.cvtColor(pred, cv2.COLOR_RGB2BGR)\n            video_out.write(pred)\n\ndef main(img_pattern: str,\n         mask_pattern: Optional[str] = None,\n         weights_path='best_fpn.h5',\n         out_dir='submit/',\n         side_by_side: bool = False,\n         video: bool = False):\n    def sorted_glob(pattern):\n        return sorted(glob(pattern))\n\n    imgs = sorted_glob(img_pattern)\n    masks = sorted_glob(mask_pattern) if mask_pattern is not None else [None for _ in imgs]\n    pairs = zip(imgs, masks)\n    names = sorted([os.path.basename(x) for x in glob(img_pattern)])\n    predictor = Predictor(weights_path=weights_path)\n\n    os.makedirs(out_dir, exist_ok=True)\n    if not video:\n        for name, pair in tqdm(zip(names, pairs), total=len(names)):\n            f_img, f_mask = pair\n            img, mask = map(cv2.imread, (f_img, f_mask))\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n            pred = predictor(img, mask)\n            if side_by_side:\n                pred = np.hstack((img, pred))\n            pred = cv2.cvtColor(pred, cv2.COLOR_RGB2BGR)\n            cv2.imwrite(os.path.join(out_dir, name),\n                        pred)\n    else:\n        process_video(pairs, predictor, out_dir)\n\n\nif __name__ == '__main__':\n    Fire(main)\n"""
schedulers.py,1,"b'import math\n\nfrom torch.optim import lr_scheduler\n\n\nclass WarmRestart(lr_scheduler.CosineAnnealingLR):\n    """"""This class implements Stochastic Gradient Descent with Warm Restarts(SGDR): https://arxiv.org/abs/1608.03983.\n\n    Set the learning rate of each parameter group using a cosine annealing schedule, When last_epoch=-1, sets initial lr as lr.\n    This can\'t support scheduler.step(epoch). please keep epoch=None.\n    """"""\n\n    def __init__(self, optimizer, T_max=30, T_mult=1, eta_min=0, last_epoch=-1):\n        """"""implements SGDR\n\n        Parameters:\n        ----------\n        T_max : int\n            Maximum number of epochs.\n        T_mult : int\n            Multiplicative factor of T_max.\n        eta_min : int\n            Minimum learning rate. Default: 0.\n        last_epoch : int\n            The index of last epoch. Default: -1.\n        """"""\n        self.T_mult = T_mult\n        super().__init__(optimizer, T_max, eta_min, last_epoch)\n\n    def get_lr(self):\n        if self.last_epoch == self.T_max:\n            self.last_epoch = 0\n            self.T_max *= self.T_mult\n        return [self.eta_min + (base_lr - self.eta_min) * (1 + math.cos(math.pi * self.last_epoch / self.T_max)) / 2 for\n                base_lr in self.base_lrs]\n\n\nclass LinearDecay(lr_scheduler._LRScheduler):\n    """"""This class implements LinearDecay\n\n    """"""\n\n    def __init__(self, optimizer, num_epochs, start_epoch=0, min_lr=0, last_epoch=-1):\n        """"""implements LinearDecay\n\n        Parameters:\n        ----------\n\n        """"""\n        self.num_epochs = num_epochs\n        self.start_epoch = start_epoch\n        self.min_lr = min_lr\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        if self.last_epoch < self.start_epoch:\n            return self.base_lrs\n        return [base_lr - ((base_lr - self.min_lr) / self.num_epochs) * (self.last_epoch - self.start_epoch) for\n                base_lr in self.base_lrs]\n'"
test_aug.py,0,"b""import unittest\n\nimport numpy as np\n\nfrom aug import get_transforms\n\n\nclass AugTest(unittest.TestCase):\n    @staticmethod\n    def make_images():\n        img = (np.random.rand(100, 100, 3) * 255).astype('uint8')\n        return img.copy(), img.copy()\n\n    def test_aug(self):\n        for scope in ('strong', 'weak'):\n            for crop in ('random', 'center'):\n                aug_pipeline = get_transforms(80, scope=scope, crop=crop)\n                a, b = self.make_images()\n                a, b = aug_pipeline(a, b)\n                np.testing.assert_allclose(a, b)\n"""
test_dataset.py,1,"b""import os\nimport unittest\nfrom shutil import rmtree\nfrom tempfile import mkdtemp\n\nimport cv2\nimport numpy as np\nfrom torch.utils.data import DataLoader\n\nfrom dataset import PairedDataset\n\n\ndef make_img():\n    return (np.random.rand(100, 100, 3) * 255).astype('uint8')\n\n\nclass AugTest(unittest.TestCase):\n    tmp_dir = mkdtemp()\n    raw = os.path.join(tmp_dir, 'raw')\n    gt = os.path.join(tmp_dir, 'gt')\n\n    def setUp(self):\n        for d in (self.raw, self.gt):\n            os.makedirs(d)\n\n        for i in range(5):\n            for d in (self.raw, self.gt):\n                img = make_img()\n                cv2.imwrite(os.path.join(d, f'{i}.png'), img)\n\n    def tearDown(self):\n        rmtree(self.tmp_dir)\n\n    def dataset_gen(self, equal=True):\n        base_config = {'files_a': os.path.join(self.raw, '*.png'),\n                       'files_b': os.path.join(self.raw if equal else self.gt, '*.png'),\n                       'size': 32,\n                       }\n        for b in ([0, 1], [0, 0.9]):\n            for scope in ('strong', 'weak'):\n                for crop in ('random', 'center'):\n                    for preload in (0, 1):\n                        for preload_size in (0, 64):\n                            config = base_config.copy()\n                            config['bounds'] = b\n                            config['scope'] = scope\n                            config['crop'] = crop\n                            config['preload'] = preload\n                            config['preload_size'] = preload_size\n                            config['verbose'] = False\n                            dataset = PairedDataset.from_config(config)\n                            yield dataset\n\n    def test_equal_datasets(self):\n        for dataset in self.dataset_gen(equal=True):\n            dataloader = DataLoader(dataset=dataset,\n                                    batch_size=2,\n                                    shuffle=True,\n                                    drop_last=True)\n            dataloader = iter(dataloader)\n            batch = next(dataloader)\n            a, b = map(lambda x: x.numpy(), map(batch.get, ('a', 'b')))\n\n            np.testing.assert_allclose(a, b)\n\n    def test_datasets(self):\n        for dataset in self.dataset_gen(equal=False):\n            dataloader = DataLoader(dataset=dataset,\n                                    batch_size=2,\n                                    shuffle=True,\n                                    drop_last=True)\n            dataloader = iter(dataloader)\n            batch = next(dataloader)\n            a, b = map(lambda x: x.numpy(), map(batch.get, ('a', 'b')))\n\n            assert not np.all(a == b), 'images should not be the same'\n"""
test_metrics.py,4,"b'from __future__ import print_function\nimport argparse\nimport numpy as np\nimport torch\nimport cv2\nimport yaml\nimport os\nfrom torchvision import models, transforms\nfrom torch.autograd import Variable\nimport shutil\nimport glob\nimport tqdm\nfrom util.metrics import PSNR\nfrom albumentations import Compose, CenterCrop, PadIfNeeded\nfrom PIL import Image\nfrom ssim.ssimlib import SSIM\nfrom models.networks import get_generator\n\n\ndef get_args():\n\tparser = argparse.ArgumentParser(\'Test an image\')\n\tparser.add_argument(\'--img_folder\', required=True, help=\'GoPRO Folder\')\n\tparser.add_argument(\'--weights_path\', required=True, help=\'Weights path\')\n\n\treturn parser.parse_args()\n\n\ndef prepare_dirs(path):\n\tif os.path.exists(path):\n\t\tshutil.rmtree(path)\n\tos.makedirs(path)\n\n\ndef get_gt_image(path):\n\tdir, filename = os.path.split(path)\n\tbase, seq = os.path.split(dir)\n\tbase, _ = os.path.split(base)\n\timg = cv2.cvtColor(cv2.imread(os.path.join(base, \'sharp\', seq, filename)), cv2.COLOR_BGR2RGB)\n\treturn img\n\n\ndef test_image(model, image_path):\n\timg_transforms = transforms.Compose([\n\t\ttransforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n\t])\n\tsize_transform = Compose([\n\t\tPadIfNeeded(736, 1280)\n\t])\n\tcrop = CenterCrop(720, 1280)\n\timg = cv2.imread(image_path)\n\timg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\timg_s = size_transform(image=img)[\'image\']\n\timg_tensor = torch.from_numpy(np.transpose(img_s / 255, (2, 0, 1)).astype(\'float32\'))\n\timg_tensor = img_transforms(img_tensor)\n\twith torch.no_grad():\n\t\timg_tensor = Variable(img_tensor.unsqueeze(0).cuda())\n\t\tresult_image = model(img_tensor)\n\tresult_image = result_image[0].cpu().float().numpy()\n\tresult_image = (np.transpose(result_image, (1, 2, 0)) + 1) / 2.0 * 255.0\n\tresult_image = crop(image=result_image)[\'image\']\n\tresult_image = result_image.astype(\'uint8\')\n\tgt_image = get_gt_image(image_path)\n\t_, filename = os.path.split(image_path)\n\tpsnr = PSNR(result_image, gt_image)\n\tpilFake = Image.fromarray(result_image)\n\tpilReal = Image.fromarray(gt_image)\n\tssim = SSIM(pilFake).cw_ssim_value(pilReal)\n\treturn psnr, ssim\n\n\ndef test(model, files):\n\tpsnr = 0\n\tssim = 0\n\tfor file in tqdm.tqdm(files):\n\t\tcur_psnr, cur_ssim = test_image(model, file)\n\t\tpsnr += cur_psnr\n\t\tssim += cur_ssim\n\tprint(""PSNR = {}"".format(psnr / len(files)))\n\tprint(""SSIM = {}"".format(ssim / len(files)))\n\n\nif __name__ == \'__main__\':\n\targs = get_args()\n\twith open(\'config/config.yaml\') as cfg:\n\t\tconfig = yaml.load(cfg)\n\tmodel = get_generator(config[\'model\'])\n\tmodel.load_state_dict(torch.load(args.weights_path)[\'model\'])\n\tmodel = model.cuda()\n\tfilenames = sorted(glob.glob(args.img_folder + \'/test\' + \'/blur/**/*.png\', recursive=True))\n\ttest(model, filenames)\n'"
train.py,4,"b'import logging\nfrom functools import partial\n\nimport cv2\nimport torch\nimport torch.optim as optim\nimport tqdm\nimport yaml\nfrom joblib import cpu_count\nfrom torch.utils.data import DataLoader\n\nfrom adversarial_trainer import GANFactory\nfrom dataset import PairedDataset\nfrom metric_counter import MetricCounter\nfrom models.losses import get_loss\nfrom models.models import get_model\nfrom models.networks import get_nets\nfrom schedulers import LinearDecay, WarmRestart\n\ncv2.setNumThreads(0)\n\n\nclass Trainer:\n    def __init__(self, config, train: DataLoader, val: DataLoader):\n        self.config = config\n        self.train_dataset = train\n        self.val_dataset = val\n        self.adv_lambda = config[\'model\'][\'adv_lambda\']\n        self.metric_counter = MetricCounter(config[\'experiment_desc\'])\n        self.warmup_epochs = config[\'warmup_num\']\n\n    def train(self):\n        self._init_params()\n        for epoch in range(0, config[\'num_epochs\']):\n            if (epoch == self.warmup_epochs) and not (self.warmup_epochs == 0):\n                self.netG.module.unfreeze()\n                self.optimizer_G = self._get_optim(self.netG.parameters())\n                self.scheduler_G = self._get_scheduler(self.optimizer_G)\n            self._run_epoch(epoch)\n            self._validate(epoch)\n            self.scheduler_G.step()\n            self.scheduler_D.step()\n\n            if self.metric_counter.update_best_model():\n                torch.save({\n                    \'model\': self.netG.state_dict()\n                }, \'best_{}.h5\'.format(self.config[\'experiment_desc\']))\n            torch.save({\n                \'model\': self.netG.state_dict()\n            }, \'last_{}.h5\'.format(self.config[\'experiment_desc\']))\n            print(self.metric_counter.loss_message())\n            logging.debug(""Experiment Name: %s, Epoch: %d, Loss: %s"" % (\n                self.config[\'experiment_desc\'], epoch, self.metric_counter.loss_message()))\n\n    def _run_epoch(self, epoch):\n        self.metric_counter.clear()\n        for param_group in self.optimizer_G.param_groups:\n            lr = param_group[\'lr\']\n\n        epoch_size = config.get(\'train_batches_per_epoch\') or len(self.train_dataset)\n        tq = tqdm.tqdm(self.train_dataset, total=epoch_size)\n        tq.set_description(\'Epoch {}, lr {}\'.format(epoch, lr))\n        i = 0\n        for data in tq:\n            inputs, targets = self.model.get_input(data)\n            outputs = self.netG(inputs)\n            loss_D = self._update_d(outputs, targets)\n            self.optimizer_G.zero_grad()\n            loss_content = self.criterionG(outputs, targets)\n            loss_adv = self.adv_trainer.loss_g(outputs, targets)\n            loss_G = loss_content + self.adv_lambda * loss_adv\n            loss_G.backward()\n            self.optimizer_G.step()\n            self.metric_counter.add_losses(loss_G.item(), loss_content.item(), loss_D)\n            curr_psnr, curr_ssim, img_for_vis = self.model.get_images_and_metrics(inputs, outputs, targets)\n            self.metric_counter.add_metrics(curr_psnr, curr_ssim)\n            tq.set_postfix(loss=self.metric_counter.loss_message())\n            if not i:\n                self.metric_counter.add_image(img_for_vis, tag=\'train\')\n            i += 1\n            if i > epoch_size:\n                break\n        tq.close()\n        self.metric_counter.write_to_tensorboard(epoch)\n\n    def _validate(self, epoch):\n        self.metric_counter.clear()\n        epoch_size = config.get(\'val_batches_per_epoch\') or len(self.val_dataset)\n        tq = tqdm.tqdm(self.val_dataset, total=epoch_size)\n        tq.set_description(\'Validation\')\n        i = 0\n        for data in tq:\n            inputs, targets = self.model.get_input(data)\n            outputs = self.netG(inputs)\n            loss_content = self.criterionG(outputs, targets)\n            loss_adv = self.adv_trainer.loss_g(outputs, targets)\n            loss_G = loss_content + self.adv_lambda * loss_adv\n            self.metric_counter.add_losses(loss_G.item(), loss_content.item())\n            curr_psnr, curr_ssim, img_for_vis = self.model.get_images_and_metrics(inputs, outputs, targets)\n            self.metric_counter.add_metrics(curr_psnr, curr_ssim)\n            if not i:\n                self.metric_counter.add_image(img_for_vis, tag=\'val\')\n            i += 1\n            if i > epoch_size:\n                break\n        tq.close()\n        self.metric_counter.write_to_tensorboard(epoch, validation=True)\n\n    def _update_d(self, outputs, targets):\n        if self.config[\'model\'][\'d_name\'] == \'no_gan\':\n            return 0\n        self.optimizer_D.zero_grad()\n        loss_D = self.adv_lambda * self.adv_trainer.loss_d(outputs, targets)\n        loss_D.backward(retain_graph=True)\n        self.optimizer_D.step()\n        return loss_D.item()\n\n    def _get_optim(self, params):\n        if self.config[\'optimizer\'][\'name\'] == \'adam\':\n            optimizer = optim.Adam(params, lr=self.config[\'optimizer\'][\'lr\'])\n        elif self.config[\'optimizer\'][\'name\'] == \'sgd\':\n            optimizer = optim.SGD(params, lr=self.config[\'optimizer\'][\'lr\'])\n        elif self.config[\'optimizer\'][\'name\'] == \'adadelta\':\n            optimizer = optim.Adadelta(params, lr=self.config[\'optimizer\'][\'lr\'])\n        else:\n            raise ValueError(""Optimizer [%s] not recognized."" % self.config[\'optimizer\'][\'name\'])\n        return optimizer\n\n    def _get_scheduler(self, optimizer):\n        if self.config[\'scheduler\'][\'name\'] == \'plateau\':\n            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n                                                             mode=\'min\',\n                                                             patience=self.config[\'scheduler\'][\'patience\'],\n                                                             factor=self.config[\'scheduler\'][\'factor\'],\n                                                             min_lr=self.config[\'scheduler\'][\'min_lr\'])\n        elif self.config[\'optimizer\'][\'name\'] == \'sgdr\':\n            scheduler = WarmRestart(optimizer)\n        elif self.config[\'scheduler\'][\'name\'] == \'linear\':\n            scheduler = LinearDecay(optimizer,\n                                    min_lr=self.config[\'scheduler\'][\'min_lr\'],\n                                    num_epochs=self.config[\'num_epochs\'],\n                                    start_epoch=self.config[\'scheduler\'][\'start_epoch\'])\n        else:\n            raise ValueError(""Scheduler [%s] not recognized."" % self.config[\'scheduler\'][\'name\'])\n        return scheduler\n\n    @staticmethod\n    def _get_adversarial_trainer(d_name, net_d, criterion_d):\n        if d_name == \'no_gan\':\n            return GANFactory.create_model(\'NoGAN\')\n        elif d_name == \'patch_gan\' or d_name == \'multi_scale\':\n            return GANFactory.create_model(\'SingleGAN\', net_d, criterion_d)\n        elif d_name == \'double_gan\':\n            return GANFactory.create_model(\'DoubleGAN\', net_d, criterion_d)\n        else:\n            raise ValueError(""Discriminator Network [%s] not recognized."" % d_name)\n\n    def _init_params(self):\n        self.criterionG, criterionD = get_loss(self.config[\'model\'])\n        self.netG, netD = get_nets(self.config[\'model\'])\n        self.netG.cuda()\n        self.adv_trainer = self._get_adversarial_trainer(self.config[\'model\'][\'d_name\'], netD, criterionD)\n        self.model = get_model(self.config[\'model\'])\n        self.optimizer_G = self._get_optim(filter(lambda p: p.requires_grad, self.netG.parameters()))\n        self.optimizer_D = self._get_optim(self.adv_trainer.get_params())\n        self.scheduler_G = self._get_scheduler(self.optimizer_G)\n        self.scheduler_D = self._get_scheduler(self.optimizer_D)\n\n\nif __name__ == \'__main__\':\n    with open(\'config/config.yaml\', \'r\') as f:\n        config = yaml.load(f)\n\n    batch_size = config.pop(\'batch_size\')\n    get_dataloader = partial(DataLoader, batch_size=batch_size, num_workers=cpu_count(), shuffle=True, drop_last=True)\n\n    datasets = map(config.pop, (\'train\', \'val\'))\n    datasets = map(PairedDataset.from_config, datasets)\n    train, val = map(get_dataloader, datasets)\n    trainer = Trainer(config, train=train, val=val)\n    trainer.train()\n'"
models/__init__.py,0,b''
models/fpn_densenet.py,2,"b'import torch\nimport torch.nn as nn\n\nfrom torchvision.models import resnet50, densenet121, densenet201\n\n\nclass FPNSegHead(nn.Module):\n    def __init__(self, num_in, num_mid, num_out):\n        super().__init__()\n\n        self.block0 = nn.Conv2d(num_in, num_mid, kernel_size=3, padding=1, bias=False)\n        self.block1 = nn.Conv2d(num_mid, num_out, kernel_size=3, padding=1, bias=False)\n\n    def forward(self, x):\n        x = nn.functional.relu(self.block0(x), inplace=True)\n        x = nn.functional.relu(self.block1(x), inplace=True)\n        return x\n\n\nclass FPNDense(nn.Module):\n\n    def __init__(self, output_ch=3, num_filters=128, num_filters_fpn=256, pretrained=True):\n        super().__init__()\n\n        # Feature Pyramid Network (FPN) with four feature maps of resolutions\n        # 1/4, 1/8, 1/16, 1/32 and `num_filters` filters for all feature maps.\n\n        self.fpn = FPN(num_filters=num_filters_fpn, pretrained=pretrained)\n\n        # The segmentation heads on top of the FPN\n\n        self.head1 = FPNSegHead(num_filters_fpn, num_filters, num_filters)\n        self.head2 = FPNSegHead(num_filters_fpn, num_filters, num_filters)\n        self.head3 = FPNSegHead(num_filters_fpn, num_filters, num_filters)\n        self.head4 = FPNSegHead(num_filters_fpn, num_filters, num_filters)\n\n        self.smooth = nn.Sequential(\n            nn.Conv2d(4 * num_filters, num_filters, kernel_size=3, padding=1),\n            nn.BatchNorm2d(num_filters),\n            nn.ReLU(),\n        )\n\n        self.smooth2 = nn.Sequential(\n            nn.Conv2d(num_filters, num_filters // 2, kernel_size=3, padding=1),\n            nn.BatchNorm2d(num_filters // 2),\n            nn.ReLU(),\n        )\n\n        self.final = nn.Conv2d(num_filters // 2, output_ch, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        map0, map1, map2, map3, map4 = self.fpn(x)\n\n        map4 = nn.functional.upsample(self.head4(map4), scale_factor=8, mode=""nearest"")\n        map3 = nn.functional.upsample(self.head3(map3), scale_factor=4, mode=""nearest"")\n        map2 = nn.functional.upsample(self.head2(map2), scale_factor=2, mode=""nearest"")\n        map1 = nn.functional.upsample(self.head1(map1), scale_factor=1, mode=""nearest"")\n\n        smoothed = self.smooth(torch.cat([map4, map3, map2, map1], dim=1))\n        smoothed = nn.functional.upsample(smoothed, scale_factor=2, mode=""nearest"")\n        smoothed = self.smooth2(smoothed + map0)\n        smoothed = nn.functional.upsample(smoothed, scale_factor=2, mode=""nearest"")\n\n        final = self.final(smoothed)\n\n        nn.Tanh(final)\n\n\nclass FPN(nn.Module):\n\n    def __init__(self, num_filters=256, pretrained=True):\n        """"""Creates an `FPN` instance for feature extraction.\n        Args:\n          num_filters: the number of filters in each output pyramid level\n          pretrained: use ImageNet pre-trained backbone feature extractor\n        """"""\n\n        super().__init__()\n\n        self.features = densenet121(pretrained=pretrained).features\n\n        self.enc0 = nn.Sequential(self.features.conv0,\n                                  self.features.norm0,\n                                  self.features.relu0)\n        self.pool0 = self.features.pool0\n        self.enc1 = self.features.denseblock1  # 256\n        self.enc2 = self.features.denseblock2  # 512\n        self.enc3 = self.features.denseblock3  # 1024\n        self.enc4 = self.features.denseblock4  # 2048\n        self.norm = self.features.norm5  # 2048\n\n        self.tr1 = self.features.transition1  # 256\n        self.tr2 = self.features.transition2  # 512\n        self.tr3 = self.features.transition3  # 1024\n\n        self.lateral4 = nn.Conv2d(1024, num_filters, kernel_size=1, bias=False)\n        self.lateral3 = nn.Conv2d(1024, num_filters, kernel_size=1, bias=False)\n        self.lateral2 = nn.Conv2d(512, num_filters, kernel_size=1, bias=False)\n        self.lateral1 = nn.Conv2d(256, num_filters, kernel_size=1, bias=False)\n        self.lateral0 = nn.Conv2d(64, num_filters // 2, kernel_size=1, bias=False)\n\n    def forward(self, x):\n        # Bottom-up pathway, from ResNet\n        enc0 = self.enc0(x)\n\n        pooled = self.pool0(enc0)\n\n        enc1 = self.enc1(pooled)  # 256\n        tr1 = self.tr1(enc1)\n\n        enc2 = self.enc2(tr1)  # 512\n        tr2 = self.tr2(enc2)\n\n        enc3 = self.enc3(tr2)  # 1024\n        tr3 = self.tr3(enc3)\n\n        enc4 = self.enc4(tr3)  # 2048\n        enc4 = self.norm(enc4)\n\n        # Lateral connections\n\n        lateral4 = self.lateral4(enc4)\n        lateral3 = self.lateral3(enc3)\n        lateral2 = self.lateral2(enc2)\n        lateral1 = self.lateral1(enc1)\n        lateral0 = self.lateral0(enc0)\n\n        # Top-down pathway\n\n        map4 = lateral4\n        map3 = lateral3 + nn.functional.upsample(map4, scale_factor=2, mode=""nearest"")\n        map2 = lateral2 + nn.functional.upsample(map3, scale_factor=2, mode=""nearest"")\n        map1 = lateral1 + nn.functional.upsample(map2, scale_factor=2, mode=""nearest"")\n\n        return lateral0, map1, map2, map3, map4\n'"
models/fpn_inception.py,5,"b'import torch\nimport torch.nn as nn\nfrom pretrainedmodels import inceptionresnetv2\nfrom torchsummary import summary\nimport torch.nn.functional as F\n\nclass FPNHead(nn.Module):\n    def __init__(self, num_in, num_mid, num_out):\n        super().__init__()\n\n        self.block0 = nn.Conv2d(num_in, num_mid, kernel_size=3, padding=1, bias=False)\n        self.block1 = nn.Conv2d(num_mid, num_out, kernel_size=3, padding=1, bias=False)\n\n    def forward(self, x):\n        x = nn.functional.relu(self.block0(x), inplace=True)\n        x = nn.functional.relu(self.block1(x), inplace=True)\n        return x\n\nclass ConvBlock(nn.Module):\n    def __init__(self, num_in, num_out, norm_layer):\n        super().__init__()\n\n        self.block = nn.Sequential(nn.Conv2d(num_in, num_out, kernel_size=3, padding=1),\n                                 norm_layer(num_out),\n                                 nn.ReLU(inplace=True))\n\n    def forward(self, x):\n        x = self.block(x)\n        return x\n\n\nclass FPNInception(nn.Module):\n\n    def __init__(self, norm_layer, output_ch=3, num_filters=128, num_filters_fpn=256):\n        super().__init__()\n\n        # Feature Pyramid Network (FPN) with four feature maps of resolutions\n        # 1/4, 1/8, 1/16, 1/32 and `num_filters` filters for all feature maps.\n        self.fpn = FPN(num_filters=num_filters_fpn, norm_layer=norm_layer)\n\n        # The segmentation heads on top of the FPN\n\n        self.head1 = FPNHead(num_filters_fpn, num_filters, num_filters)\n        self.head2 = FPNHead(num_filters_fpn, num_filters, num_filters)\n        self.head3 = FPNHead(num_filters_fpn, num_filters, num_filters)\n        self.head4 = FPNHead(num_filters_fpn, num_filters, num_filters)\n\n        self.smooth = nn.Sequential(\n            nn.Conv2d(4 * num_filters, num_filters, kernel_size=3, padding=1),\n            norm_layer(num_filters),\n            nn.ReLU(),\n        )\n\n        self.smooth2 = nn.Sequential(\n            nn.Conv2d(num_filters, num_filters // 2, kernel_size=3, padding=1),\n            norm_layer(num_filters // 2),\n            nn.ReLU(),\n        )\n\n        self.final = nn.Conv2d(num_filters // 2, output_ch, kernel_size=3, padding=1)\n\n    def unfreeze(self):\n        self.fpn.unfreeze()\n\n    def forward(self, x):\n        map0, map1, map2, map3, map4 = self.fpn(x)\n\n        map4 = nn.functional.upsample(self.head4(map4), scale_factor=8, mode=""nearest"")\n        map3 = nn.functional.upsample(self.head3(map3), scale_factor=4, mode=""nearest"")\n        map2 = nn.functional.upsample(self.head2(map2), scale_factor=2, mode=""nearest"")\n        map1 = nn.functional.upsample(self.head1(map1), scale_factor=1, mode=""nearest"")\n\n        smoothed = self.smooth(torch.cat([map4, map3, map2, map1], dim=1))\n        smoothed = nn.functional.upsample(smoothed, scale_factor=2, mode=""nearest"")\n        smoothed = self.smooth2(smoothed + map0)\n        smoothed = nn.functional.upsample(smoothed, scale_factor=2, mode=""nearest"")\n\n        final = self.final(smoothed)\n        res = torch.tanh(final) + x\n\n        return torch.clamp(res, min = -1,max = 1)\n\n\nclass FPN(nn.Module):\n\n    def __init__(self, norm_layer, num_filters=256):\n        """"""Creates an `FPN` instance for feature extraction.\n        Args:\n          num_filters: the number of filters in each output pyramid level\n          pretrained: use ImageNet pre-trained backbone feature extractor\n        """"""\n\n        super().__init__()\n        self.inception = inceptionresnetv2(num_classes=1000, pretrained=\'imagenet\')\n\n        self.enc0 = self.inception.conv2d_1a\n        self.enc1 = nn.Sequential(\n            self.inception.conv2d_2a,\n            self.inception.conv2d_2b,\n            self.inception.maxpool_3a,\n        ) # 64\n        self.enc2 = nn.Sequential(\n            self.inception.conv2d_3b,\n            self.inception.conv2d_4a,\n            self.inception.maxpool_5a,\n        )  # 192\n        self.enc3 = nn.Sequential(\n            self.inception.mixed_5b,\n            self.inception.repeat,\n            self.inception.mixed_6a,\n        )   # 1088\n        self.enc4 = nn.Sequential(\n            self.inception.repeat_1,\n            self.inception.mixed_7a,\n        ) #2080\n        self.td1 = nn.Sequential(nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n                                 norm_layer(num_filters),\n                                 nn.ReLU(inplace=True))\n        self.td2 = nn.Sequential(nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n                                 norm_layer(num_filters),\n                                 nn.ReLU(inplace=True))\n        self.td3 = nn.Sequential(nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n                                 norm_layer(num_filters),\n                                 nn.ReLU(inplace=True))\n        self.pad = nn.ReflectionPad2d(1)\n        self.lateral4 = nn.Conv2d(2080, num_filters, kernel_size=1, bias=False)\n        self.lateral3 = nn.Conv2d(1088, num_filters, kernel_size=1, bias=False)\n        self.lateral2 = nn.Conv2d(192, num_filters, kernel_size=1, bias=False)\n        self.lateral1 = nn.Conv2d(64, num_filters, kernel_size=1, bias=False)\n        self.lateral0 = nn.Conv2d(32, num_filters // 2, kernel_size=1, bias=False)\n\n        for param in self.inception.parameters():\n            param.requires_grad = False\n\n    def unfreeze(self):\n        for param in self.inception.parameters():\n            param.requires_grad = True\n\n    def forward(self, x):\n\n        # Bottom-up pathway, from ResNet\n        enc0 = self.enc0(x)\n\n        enc1 = self.enc1(enc0) # 256\n\n        enc2 = self.enc2(enc1) # 512\n\n        enc3 = self.enc3(enc2) # 1024\n\n        enc4 = self.enc4(enc3) # 2048\n\n        # Lateral connections\n\n        lateral4 = self.pad(self.lateral4(enc4))\n        lateral3 = self.pad(self.lateral3(enc3))\n        lateral2 = self.lateral2(enc2)\n        lateral1 = self.pad(self.lateral1(enc1))\n        lateral0 = self.lateral0(enc0)\n\n        # Top-down pathway\n        pad = (1, 2, 1, 2)  # pad last dim by 1 on each side\n        pad1 = (0, 1, 0, 1)\n        map4 = lateral4\n        map3 = self.td1(lateral3 + nn.functional.upsample(map4, scale_factor=2, mode=""nearest""))\n        map2 = self.td2(F.pad(lateral2, pad, ""reflect"") + nn.functional.upsample(map3, scale_factor=2, mode=""nearest""))\n        map1 = self.td3(lateral1 + nn.functional.upsample(map2, scale_factor=2, mode=""nearest""))\n        return F.pad(lateral0, pad1, ""reflect""), map1, map2, map3, map4\n'"
models/fpn_inception_simple.py,5,"b'import torch\nimport torch.nn as nn\nfrom pretrainedmodels import inceptionresnetv2\nfrom torchsummary import summary\nimport torch.nn.functional as F\n\nclass FPNHead(nn.Module):\n    def __init__(self, num_in, num_mid, num_out):\n        super().__init__()\n\n        self.block0 = nn.Conv2d(num_in, num_mid, kernel_size=3, padding=1, bias=False)\n        self.block1 = nn.Conv2d(num_mid, num_out, kernel_size=3, padding=1, bias=False)\n\n    def forward(self, x):\n        x = nn.functional.relu(self.block0(x), inplace=True)\n        x = nn.functional.relu(self.block1(x), inplace=True)\n        return x\n\nclass ConvBlock(nn.Module):\n    def __init__(self, num_in, num_out, norm_layer):\n        super().__init__()\n\n        self.block = nn.Sequential(nn.Conv2d(num_in, num_out, kernel_size=3, padding=1),\n                                 norm_layer(num_out),\n                                 nn.ReLU(inplace=True))\n\n    def forward(self, x):\n        x = self.block(x)\n        return x\n\n\nclass FPNInceptionSimple(nn.Module):\n\n    def __init__(self, norm_layer, output_ch=3, num_filters=128, num_filters_fpn=256):\n        super().__init__()\n\n        # Feature Pyramid Network (FPN) with four feature maps of resolutions\n        # 1/4, 1/8, 1/16, 1/32 and `num_filters` filters for all feature maps.\n        self.fpn = FPN(num_filters=num_filters_fpn, norm_layer=norm_layer)\n\n        # The segmentation heads on top of the FPN\n\n        self.head1 = FPNHead(num_filters_fpn, num_filters, num_filters)\n        self.head2 = FPNHead(num_filters_fpn, num_filters, num_filters)\n        self.head3 = FPNHead(num_filters_fpn, num_filters, num_filters)\n        self.head4 = FPNHead(num_filters_fpn, num_filters, num_filters)\n\n        self.smooth = nn.Sequential(\n            nn.Conv2d(4 * num_filters, num_filters, kernel_size=3, padding=1),\n            norm_layer(num_filters),\n            nn.ReLU(),\n        )\n\n        self.smooth2 = nn.Sequential(\n            nn.Conv2d(num_filters, num_filters // 2, kernel_size=3, padding=1),\n            norm_layer(num_filters // 2),\n            nn.ReLU(),\n        )\n\n        self.final = nn.Conv2d(num_filters // 2, output_ch, kernel_size=3, padding=1)\n\n    def unfreeze(self):\n        self.fpn.unfreeze()\n\n    def forward(self, x):\n\n        map0, map1, map2, map3, map4 = self.fpn(x)\n\n        map4 = nn.functional.upsample(self.head4(map4), scale_factor=8, mode=""nearest"")\n        map3 = nn.functional.upsample(self.head3(map3), scale_factor=4, mode=""nearest"")\n        map2 = nn.functional.upsample(self.head2(map2), scale_factor=2, mode=""nearest"")\n        map1 = nn.functional.upsample(self.head1(map1), scale_factor=1, mode=""nearest"")\n\n        smoothed = self.smooth(torch.cat([map4, map3, map2, map1], dim=1))\n        smoothed = nn.functional.upsample(smoothed, scale_factor=2, mode=""nearest"")\n        smoothed = self.smooth2(smoothed + map0)\n        smoothed = nn.functional.upsample(smoothed, scale_factor=2, mode=""nearest"")\n\n        final = self.final(smoothed)\n        res = torch.tanh(final) + x\n\n        return torch.clamp(res, min = -1,max = 1)\n\n\nclass FPN(nn.Module):\n\n    def __init__(self, norm_layer, num_filters=256):\n        """"""Creates an `FPN` instance for feature extraction.\n        Args:\n          num_filters: the number of filters in each output pyramid level\n          pretrained: use ImageNet pre-trained backbone feature extractor\n        """"""\n\n        super().__init__()\n        self.inception = inceptionresnetv2(num_classes=1000, pretrained=\'imagenet\')\n\n        self.enc0 = self.inception.conv2d_1a\n        self.enc1 = nn.Sequential(\n            self.inception.conv2d_2a,\n            self.inception.conv2d_2b,\n            self.inception.maxpool_3a,\n        ) # 64\n        self.enc2 = nn.Sequential(\n            self.inception.conv2d_3b,\n            self.inception.conv2d_4a,\n            self.inception.maxpool_5a,\n        )  # 192\n        self.enc3 = nn.Sequential(\n            self.inception.mixed_5b,\n            self.inception.repeat,\n            self.inception.mixed_6a,\n        )   # 1088\n        self.enc4 = nn.Sequential(\n            self.inception.repeat_1,\n            self.inception.mixed_7a,\n        ) #2080\n\n        self.pad = nn.ReflectionPad2d(1)\n        self.lateral4 = nn.Conv2d(2080, num_filters, kernel_size=1, bias=False)\n        self.lateral3 = nn.Conv2d(1088, num_filters, kernel_size=1, bias=False)\n        self.lateral2 = nn.Conv2d(192, num_filters, kernel_size=1, bias=False)\n        self.lateral1 = nn.Conv2d(64, num_filters, kernel_size=1, bias=False)\n        self.lateral0 = nn.Conv2d(32, num_filters // 2, kernel_size=1, bias=False)\n\n        for param in self.inception.parameters():\n            param.requires_grad = False\n\n    def unfreeze(self):\n        for param in self.inception.parameters():\n            param.requires_grad = True\n\n    def forward(self, x):\n\n        # Bottom-up pathway, from ResNet\n        enc0 = self.enc0(x)\n\n        enc1 = self.enc1(enc0) # 256\n\n        enc2 = self.enc2(enc1) # 512\n\n        enc3 = self.enc3(enc2) # 1024\n\n        enc4 = self.enc4(enc3) # 2048\n\n        # Lateral connections\n\n        lateral4 = self.pad(self.lateral4(enc4))\n        lateral3 = self.pad(self.lateral3(enc3))\n        lateral2 = self.lateral2(enc2)\n        lateral1 = self.pad(self.lateral1(enc1))\n        lateral0 = self.lateral0(enc0)\n\n        # Top-down pathway\n        pad = (1, 2, 1, 2)  # pad last dim by 1 on each side\n        pad1 = (0, 1, 0, 1)\n        map4 = lateral4\n        map3 = lateral3 + nn.functional.upsample(map4, scale_factor=2, mode=""nearest"")\n        map2 = F.pad(lateral2, pad, ""reflect"") + nn.functional.upsample(map3, scale_factor=2, mode=""nearest"")\n        map1 = lateral1 + nn.functional.upsample(map2, scale_factor=2, mode=""nearest"")\n        return F.pad(lateral0, pad1, ""reflect""), map1, map2, map3, map4\n'"
models/fpn_mobilenet.py,5,"b'import torch\nimport torch.nn as nn\nfrom models.mobilenet_v2 import MobileNetV2\n\nclass FPNHead(nn.Module):\n    def __init__(self, num_in, num_mid, num_out):\n        super().__init__()\n\n        self.block0 = nn.Conv2d(num_in, num_mid, kernel_size=3, padding=1, bias=False)\n        self.block1 = nn.Conv2d(num_mid, num_out, kernel_size=3, padding=1, bias=False)\n\n    def forward(self, x):\n        x = nn.functional.relu(self.block0(x), inplace=True)\n        x = nn.functional.relu(self.block1(x), inplace=True)\n        return x\n\n\nclass FPNMobileNet(nn.Module):\n\n    def __init__(self, norm_layer, output_ch=3, num_filters=64, num_filters_fpn=128, pretrained=True):\n        super().__init__()\n\n        # Feature Pyramid Network (FPN) with four feature maps of resolutions\n        # 1/4, 1/8, 1/16, 1/32 and `num_filters` filters for all feature maps.\n\n        self.fpn = FPN(num_filters=num_filters_fpn, norm_layer = norm_layer, pretrained=pretrained)\n\n        # The segmentation heads on top of the FPN\n\n        self.head1 = FPNHead(num_filters_fpn, num_filters, num_filters)\n        self.head2 = FPNHead(num_filters_fpn, num_filters, num_filters)\n        self.head3 = FPNHead(num_filters_fpn, num_filters, num_filters)\n        self.head4 = FPNHead(num_filters_fpn, num_filters, num_filters)\n\n        self.smooth = nn.Sequential(\n            nn.Conv2d(4 * num_filters, num_filters, kernel_size=3, padding=1),\n            norm_layer(num_filters),\n            nn.ReLU(),\n        )\n\n        self.smooth2 = nn.Sequential(\n            nn.Conv2d(num_filters, num_filters // 2, kernel_size=3, padding=1),\n            norm_layer(num_filters // 2),\n            nn.ReLU(),\n        )\n\n        self.final = nn.Conv2d(num_filters // 2, output_ch, kernel_size=3, padding=1)\n\n    def unfreeze(self):\n        self.fpn.unfreeze()\n\n    def forward(self, x):\n\n        map0, map1, map2, map3, map4 = self.fpn(x)\n\n        map4 = nn.functional.upsample(self.head4(map4), scale_factor=8, mode=""nearest"")\n        map3 = nn.functional.upsample(self.head3(map3), scale_factor=4, mode=""nearest"")\n        map2 = nn.functional.upsample(self.head2(map2), scale_factor=2, mode=""nearest"")\n        map1 = nn.functional.upsample(self.head1(map1), scale_factor=1, mode=""nearest"")\n\n        smoothed = self.smooth(torch.cat([map4, map3, map2, map1], dim=1))\n        smoothed = nn.functional.upsample(smoothed, scale_factor=2, mode=""nearest"")\n        smoothed = self.smooth2(smoothed + map0)\n        smoothed = nn.functional.upsample(smoothed, scale_factor=2, mode=""nearest"")\n\n        final = self.final(smoothed)\n        res = torch.tanh(final) + x\n\n        return torch.clamp(res, min=-1, max=1)\n\n\nclass FPN(nn.Module):\n\n    def __init__(self, norm_layer, num_filters=128, pretrained=True):\n        """"""Creates an `FPN` instance for feature extraction.\n        Args:\n          num_filters: the number of filters in each output pyramid level\n          pretrained: use ImageNet pre-trained backbone feature extractor\n        """"""\n\n        super().__init__()\n        net = MobileNetV2(n_class=1000)\n\n        if pretrained:\n            #Load weights into the project directory\n            state_dict = torch.load(\'mobilenetv2.pth.tar\') # add map_location=\'cpu\' if no gpu\n            net.load_state_dict(state_dict)\n        self.features = net.features\n\n        self.enc0 = nn.Sequential(*self.features[0:2])\n        self.enc1 = nn.Sequential(*self.features[2:4])\n        self.enc2 = nn.Sequential(*self.features[4:7])\n        self.enc3 = nn.Sequential(*self.features[7:11])\n        self.enc4 = nn.Sequential(*self.features[11:16])\n\n        self.td1 = nn.Sequential(nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n                                 norm_layer(num_filters),\n                                 nn.ReLU(inplace=True))\n        self.td2 = nn.Sequential(nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n                                 norm_layer(num_filters),\n                                 nn.ReLU(inplace=True))\n        self.td3 = nn.Sequential(nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n                                 norm_layer(num_filters),\n                                 nn.ReLU(inplace=True))\n\n        self.lateral4 = nn.Conv2d(160, num_filters, kernel_size=1, bias=False)\n        self.lateral3 = nn.Conv2d(64, num_filters, kernel_size=1, bias=False)\n        self.lateral2 = nn.Conv2d(32, num_filters, kernel_size=1, bias=False)\n        self.lateral1 = nn.Conv2d(24, num_filters, kernel_size=1, bias=False)\n        self.lateral0 = nn.Conv2d(16, num_filters // 2, kernel_size=1, bias=False)\n\n        for param in self.features.parameters():\n            param.requires_grad = False\n\n    def unfreeze(self):\n        for param in self.features.parameters():\n            param.requires_grad = True\n\n\n    def forward(self, x):\n\n        # Bottom-up pathway, from ResNet\n        enc0 = self.enc0(x)\n\n        enc1 = self.enc1(enc0) # 256\n\n        enc2 = self.enc2(enc1) # 512\n\n        enc3 = self.enc3(enc2) # 1024\n\n        enc4 = self.enc4(enc3) # 2048\n\n        # Lateral connections\n\n        lateral4 = self.lateral4(enc4)\n        lateral3 = self.lateral3(enc3)\n        lateral2 = self.lateral2(enc2)\n        lateral1 = self.lateral1(enc1)\n        lateral0 = self.lateral0(enc0)\n\n        # Top-down pathway\n        map4 = lateral4\n        map3 = self.td1(lateral3 + nn.functional.upsample(map4, scale_factor=2, mode=""nearest""))\n        map2 = self.td2(lateral2 + nn.functional.upsample(map3, scale_factor=2, mode=""nearest""))\n        map1 = self.td3(lateral1 + nn.functional.upsample(map2, scale_factor=2, mode=""nearest""))\n        return lateral0, map1, map2, map3, map4\n\n'"
models/losses.py,16,"b'import torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\nfrom util.image_pool import ImagePool\n\n\n###############################################################################\n# Functions\n###############################################################################\n\nclass ContentLoss():\n    def initialize(self, loss):\n        self.criterion = loss\n\n    def get_loss(self, fakeIm, realIm):\n        return self.criterion(fakeIm, realIm)\n\n    def __call__(self, fakeIm, realIm):\n        return self.get_loss(fakeIm, realIm)\n\n\nclass PerceptualLoss():\n\n    def contentFunc(self):\n        conv_3_3_layer = 14\n        cnn = models.vgg19(pretrained=True).features\n        cnn = cnn.cuda()\n        model = nn.Sequential()\n        model = model.cuda()\n        model = model.eval()\n        for i, layer in enumerate(list(cnn)):\n            model.add_module(str(i), layer)\n            if i == conv_3_3_layer:\n                break\n        return model\n\n    def initialize(self, loss):\n        with torch.no_grad():\n            self.criterion = loss\n            self.contentFunc = self.contentFunc()\n            self.transform = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\n    def get_loss(self, fakeIm, realIm):\n        fakeIm = (fakeIm + 1) / 2.0\n        realIm = (realIm + 1) / 2.0\n        fakeIm[0, :, :, :] = self.transform(fakeIm[0, :, :, :])\n        realIm[0, :, :, :] = self.transform(realIm[0, :, :, :])\n        f_fake = self.contentFunc.forward(fakeIm)\n        f_real = self.contentFunc.forward(realIm)\n        f_real_no_grad = f_real.detach()\n        loss = self.criterion(f_fake, f_real_no_grad)\n        return 0.006 * torch.mean(loss) + 0.5 * nn.MSELoss()(fakeIm, realIm)\n\n    def __call__(self, fakeIm, realIm):\n        return self.get_loss(fakeIm, realIm)\n\n\nclass GANLoss(nn.Module):\n    def __init__(self, use_l1=True, target_real_label=1.0, target_fake_label=0.0,\n                 tensor=torch.FloatTensor):\n        super(GANLoss, self).__init__()\n        self.real_label = target_real_label\n        self.fake_label = target_fake_label\n        self.real_label_var = None\n        self.fake_label_var = None\n        self.Tensor = tensor\n        if use_l1:\n            self.loss = nn.L1Loss()\n        else:\n            self.loss = nn.BCEWithLogitsLoss()\n\n    def get_target_tensor(self, input, target_is_real):\n        if target_is_real:\n            create_label = ((self.real_label_var is None) or\n                            (self.real_label_var.numel() != input.numel()))\n            if create_label:\n                real_tensor = self.Tensor(input.size()).fill_(self.real_label)\n                self.real_label_var = Variable(real_tensor, requires_grad=False)\n            target_tensor = self.real_label_var\n        else:\n            create_label = ((self.fake_label_var is None) or\n                            (self.fake_label_var.numel() != input.numel()))\n            if create_label:\n                fake_tensor = self.Tensor(input.size()).fill_(self.fake_label)\n                self.fake_label_var = Variable(fake_tensor, requires_grad=False)\n            target_tensor = self.fake_label_var\n        return target_tensor.cuda()\n\n    def __call__(self, input, target_is_real):\n        target_tensor = self.get_target_tensor(input, target_is_real)\n        return self.loss(input, target_tensor)\n\n\nclass DiscLoss(nn.Module):\n    def name(self):\n        return \'DiscLoss\'\n\n    def __init__(self):\n        super(DiscLoss, self).__init__()\n\n        self.criterionGAN = GANLoss(use_l1=False)\n        self.fake_AB_pool = ImagePool(50)\n\n    def get_g_loss(self, net, fakeB, realB):\n        # First, G(A) should fake the discriminator\n        pred_fake = net.forward(fakeB)\n        return self.criterionGAN(pred_fake, 1)\n\n    def get_loss(self, net, fakeB, realB):\n        # Fake\n        # stop backprop to the generator by detaching fake_B\n        # Generated Image Disc Output should be close to zero\n        self.pred_fake = net.forward(fakeB.detach())\n        self.loss_D_fake = self.criterionGAN(self.pred_fake, 0)\n\n        # Real\n        self.pred_real = net.forward(realB)\n        self.loss_D_real = self.criterionGAN(self.pred_real, 1)\n\n        # Combined loss\n        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n        return self.loss_D\n\n    def __call__(self, net, fakeB, realB):\n        return self.get_loss(net, fakeB, realB)\n\n\nclass RelativisticDiscLoss(nn.Module):\n    def name(self):\n        return \'RelativisticDiscLoss\'\n\n    def __init__(self):\n        super(RelativisticDiscLoss, self).__init__()\n\n        self.criterionGAN = GANLoss(use_l1=False)\n        self.fake_pool = ImagePool(50)  # create image buffer to store previously generated images\n        self.real_pool = ImagePool(50)\n\n    def get_g_loss(self, net, fakeB, realB):\n        # First, G(A) should fake the discriminator\n        self.pred_fake = net.forward(fakeB)\n\n        # Real\n        self.pred_real = net.forward(realB)\n        errG = (self.criterionGAN(self.pred_real - torch.mean(self.fake_pool.query()), 0) +\n                self.criterionGAN(self.pred_fake - torch.mean(self.real_pool.query()), 1)) / 2\n        return errG\n\n    def get_loss(self, net, fakeB, realB):\n        # Fake\n        # stop backprop to the generator by detaching fake_B\n        # Generated Image Disc Output should be close to zero\n        self.fake_B = fakeB.detach()\n        self.real_B = realB\n        self.pred_fake = net.forward(fakeB.detach())\n        self.fake_pool.add(self.pred_fake)\n\n        # Real\n        self.pred_real = net.forward(realB)\n        self.real_pool.add(self.pred_real)\n\n        # Combined loss\n        self.loss_D = (self.criterionGAN(self.pred_real - torch.mean(self.fake_pool.query()), 1) +\n                       self.criterionGAN(self.pred_fake - torch.mean(self.real_pool.query()), 0)) / 2\n        return self.loss_D\n\n    def __call__(self, net, fakeB, realB):\n        return self.get_loss(net, fakeB, realB)\n\n\nclass RelativisticDiscLossLS(nn.Module):\n    def name(self):\n        return \'RelativisticDiscLossLS\'\n\n    def __init__(self):\n        super(RelativisticDiscLossLS, self).__init__()\n\n        self.criterionGAN = GANLoss(use_l1=True)\n        self.fake_pool = ImagePool(50)  # create image buffer to store previously generated images\n        self.real_pool = ImagePool(50)\n\n    def get_g_loss(self, net, fakeB, realB):\n        # First, G(A) should fake the discriminator\n        self.pred_fake = net.forward(fakeB)\n\n        # Real\n        self.pred_real = net.forward(realB)\n        errG = (torch.mean((self.pred_real - torch.mean(self.fake_pool.query()) + 1) ** 2) +\n                torch.mean((self.pred_fake - torch.mean(self.real_pool.query()) - 1) ** 2)) / 2\n        return errG\n\n    def get_loss(self, net, fakeB, realB):\n        # Fake\n        # stop backprop to the generator by detaching fake_B\n        # Generated Image Disc Output should be close to zero\n        self.fake_B = fakeB.detach()\n        self.real_B = realB\n        self.pred_fake = net.forward(fakeB.detach())\n        self.fake_pool.add(self.pred_fake)\n\n        # Real\n        self.pred_real = net.forward(realB)\n        self.real_pool.add(self.pred_real)\n\n        # Combined loss\n        self.loss_D = (torch.mean((self.pred_real - torch.mean(self.fake_pool.query()) - 1) ** 2) +\n                       torch.mean((self.pred_fake - torch.mean(self.real_pool.query()) + 1) ** 2)) / 2\n        return self.loss_D\n\n    def __call__(self, net, fakeB, realB):\n        return self.get_loss(net, fakeB, realB)\n\n\nclass DiscLossLS(DiscLoss):\n    def name(self):\n        return \'DiscLossLS\'\n\n    def __init__(self):\n        super(DiscLossLS, self).__init__()\n        self.criterionGAN = GANLoss(use_l1=True)\n\n    def get_g_loss(self, net, fakeB, realB):\n        return DiscLoss.get_g_loss(self, net, fakeB)\n\n    def get_loss(self, net, fakeB, realB):\n        return DiscLoss.get_loss(self, net, fakeB, realB)\n\n\nclass DiscLossWGANGP(DiscLossLS):\n    def name(self):\n        return \'DiscLossWGAN-GP\'\n\n    def __init__(self):\n        super(DiscLossWGANGP, self).__init__()\n        self.LAMBDA = 10\n\n    def get_g_loss(self, net, fakeB, realB):\n        # First, G(A) should fake the discriminator\n        self.D_fake = net.forward(fakeB)\n        return -self.D_fake.mean()\n\n    def calc_gradient_penalty(self, netD, real_data, fake_data):\n        alpha = torch.rand(1, 1)\n        alpha = alpha.expand(real_data.size())\n        alpha = alpha.cuda()\n\n        interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n\n        interpolates = interpolates.cuda()\n        interpolates = Variable(interpolates, requires_grad=True)\n\n        disc_interpolates = netD.forward(interpolates)\n\n        gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n                                  grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n                                  create_graph=True, retain_graph=True, only_inputs=True)[0]\n\n        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * self.LAMBDA\n        return gradient_penalty\n\n    def get_loss(self, net, fakeB, realB):\n        self.D_fake = net.forward(fakeB.detach())\n        self.D_fake = self.D_fake.mean()\n\n        # Real\n        self.D_real = net.forward(realB)\n        self.D_real = self.D_real.mean()\n        # Combined loss\n        self.loss_D = self.D_fake - self.D_real\n        gradient_penalty = self.calc_gradient_penalty(net, realB.data, fakeB.data)\n        return self.loss_D + gradient_penalty\n\n\ndef get_loss(model):\n    if model[\'content_loss\'] == \'perceptual\':\n        content_loss = PerceptualLoss()\n        content_loss.initialize(nn.MSELoss())\n    elif model[\'content_loss\'] == \'l1\':\n        content_loss = ContentLoss()\n        content_loss.initialize(nn.L1Loss())\n    else:\n        raise ValueError(""ContentLoss [%s] not recognized."" % model[\'content_loss\'])\n\n    if model[\'disc_loss\'] == \'wgan-gp\':\n        disc_loss = DiscLossWGANGP()\n    elif model[\'disc_loss\'] == \'lsgan\':\n        disc_loss = DiscLossLS()\n    elif model[\'disc_loss\'] == \'gan\':\n        disc_loss = DiscLoss()\n    elif model[\'disc_loss\'] == \'ragan\':\n        disc_loss = RelativisticDiscLoss()\n    elif model[\'disc_loss\'] == \'ragan-ls\':\n        disc_loss = RelativisticDiscLossLS()\n    else:\n        raise ValueError(""GAN Loss [%s] not recognized."" % model[\'disc_loss\'])\n    return content_loss, disc_loss\n'"
models/mobilenet_v2.py,1,"b'import torch.nn as nn\nimport math\n\n\ndef conv_bn(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\ndef conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        hidden_dim = round(inp * expand_ratio)\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        if expand_ratio == 1:\n            self.conv = nn.Sequential(\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n        else:\n            self.conv = nn.Sequential(\n                # pw\n                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, n_class=1000, input_size=224, width_mult=1.):\n        super(MobileNetV2, self).__init__()\n        block = InvertedResidual\n        input_channel = 32\n        last_channel = 1280\n        interverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # building first layer\n        assert input_size % 32 == 0\n        input_channel = int(input_channel * width_mult)\n        self.last_channel = int(last_channel * width_mult) if width_mult > 1.0 else last_channel\n        self.features = [conv_bn(3, input_channel, 2)]\n        # building inverted residual blocks\n        for t, c, n, s in interverted_residual_setting:\n            output_channel = int(c * width_mult)\n            for i in range(n):\n                if i == 0:\n                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))\n                else:\n                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t))\n                input_channel = output_channel\n        # building last several layers\n        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n        # make it nn.Sequential\n        self.features = nn.Sequential(*self.features)\n\n        # building classifier\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(self.last_channel, n_class),\n        )\n\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.mean(3).mean(2)\n        x = self.classifier(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                n = m.weight.size(1)\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n\n'"
models/models.py,1,"b""import numpy as np\nimport torch.nn as nn\nfrom skimage.measure import compare_ssim as SSIM\n\nfrom util.metrics import PSNR\n\n\nclass DeblurModel(nn.Module):\n    def __init__(self):\n        super(DeblurModel, self).__init__()\n\n    def get_input(self, data):\n        img = data['a']\n        inputs = img\n        targets = data['b']\n        inputs, targets = inputs.cuda(), targets.cuda()\n        return inputs, targets\n\n    def tensor2im(self, image_tensor, imtype=np.uint8):\n        image_numpy = image_tensor[0].cpu().float().numpy()\n        image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n        return image_numpy.astype(imtype)\n\n    def get_images_and_metrics(self, inp, output, target) -> (float, float, np.ndarray):\n        inp = self.tensor2im(inp)\n        fake = self.tensor2im(output.data)\n        real = self.tensor2im(target.data)\n        psnr = PSNR(fake, real)\n        ssim = SSIM(fake, real, multichannel=True)\n        vis_img = np.hstack((inp, fake, real))\n        return psnr, ssim, vis_img\n\n\ndef get_model(model_config):\n    return DeblurModel()\n"""
models/networks.py,4,"b'import torch\nimport torch.nn as nn\nfrom torch.nn import init\nimport functools\nfrom torch.autograd import Variable\nimport numpy as np\nfrom models.fpn_mobilenet import FPNMobileNet\nfrom models.fpn_inception import FPNInception\nfrom models.fpn_inception_simple import FPNInceptionSimple\nfrom models.unet_seresnext import UNetSEResNext\nfrom models.fpn_densenet import FPNDense\n###############################################################################\n# Functions\n###############################################################################\n\n\ndef get_norm_layer(norm_type=\'instance\'):\n    if norm_type == \'batch\':\n        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n    elif norm_type == \'instance\':\n        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=True)\n    else:\n        raise NotImplementedError(\'normalization layer [%s] is not found\' % norm_type)\n    return norm_layer\n\n##############################################################################\n# Classes\n##############################################################################\n\n\n# Defines the generator that consists of Resnet blocks between a few\n# downsampling/upsampling operations.\n# Code and idea originally from Justin Johnson\'s architecture.\n# https://github.com/jcjohnson/fast-neural-style/\nclass ResnetGenerator(nn.Module):\n    def __init__(self, input_nc=3, output_nc=3, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, use_parallel=True, learn_residual=True, padding_type=\'reflect\'):\n        assert(n_blocks >= 0)\n        super(ResnetGenerator, self).__init__()\n        self.input_nc = input_nc\n        self.output_nc = output_nc\n        self.ngf = ngf\n        self.use_parallel = use_parallel\n        self.learn_residual = learn_residual\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        model = [nn.ReflectionPad2d(3),\n                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0,\n                           bias=use_bias),\n                 norm_layer(ngf),\n                 nn.ReLU(True)]\n\n        n_downsampling = 2\n        for i in range(n_downsampling):\n            mult = 2**i\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3,\n                                stride=2, padding=1, bias=use_bias),\n                      norm_layer(ngf * mult * 2),\n                      nn.ReLU(True)]\n\n        mult = 2**n_downsampling\n        for i in range(n_blocks):\n            model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n\n        for i in range(n_downsampling):\n            mult = 2**(n_downsampling - i)\n            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n                                         kernel_size=3, stride=2,\n                                         padding=1, output_padding=1,\n                                         bias=use_bias),\n                      norm_layer(int(ngf * mult / 2)),\n                      nn.ReLU(True)]\n        model += [nn.ReflectionPad2d(3)]\n        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n        model += [nn.Tanh()]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, input):\n        output = self.model(input)\n        if self.learn_residual:\n            output = input + output\n            output = torch.clamp(output,min = -1,max = 1)\n        return output\n\n\n# Define a resnet block\nclass ResnetBlock(nn.Module):\n    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n        super(ResnetBlock, self).__init__()\n        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n\n    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n        conv_block = []\n        p = 0\n        if padding_type == \'reflect\':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == \'replicate\':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == \'zero\':\n            p = 1\n        else:\n            raise NotImplementedError(\'padding [%s] is not implemented\' % padding_type)\n\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n                       norm_layer(dim),\n                       nn.ReLU(True)]\n        if use_dropout:\n            conv_block += [nn.Dropout(0.5)]\n\n        p = 0\n        if padding_type == \'reflect\':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == \'replicate\':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == \'zero\':\n            p = 1\n        else:\n            raise NotImplementedError(\'padding [%s] is not implemented\' % padding_type)\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n                       norm_layer(dim)]\n\n        return nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        out = x + self.conv_block(x)\n        return out\n\n\nclass DicsriminatorTail(nn.Module):\n    def __init__(self, nf_mult, n_layers, ndf=64, norm_layer=nn.BatchNorm2d, use_parallel=True):\n        super(DicsriminatorTail, self).__init__()\n        self.use_parallel = use_parallel\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        kw = 4\n        padw = int(np.ceil((kw-1)/2))\n\n        nf_mult_prev = nf_mult\n        nf_mult = min(2**n_layers, 8)\n        sequence = [\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                      kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n            norm_layer(ndf * nf_mult),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n\n        self.model = nn.Sequential(*sequence)\n\n    def forward(self, input):\n        return self.model(input)\n\n\nclass MultiScaleDiscriminator(nn.Module):\n    def __init__(self, input_nc=3, ndf=64, norm_layer=nn.BatchNorm2d, use_parallel=True):\n        super(MultiScaleDiscriminator, self).__init__()\n        self.use_parallel = use_parallel\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        kw = 4\n        padw = int(np.ceil((kw-1)/2))\n        sequence = [\n            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        nf_mult = 1\n        for n in range(1, 3):\n            nf_mult_prev = nf_mult\n            nf_mult = min(2**n, 8)\n            sequence += [\n                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                          kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n                norm_layer(ndf * nf_mult),\n                nn.LeakyReLU(0.2, True)\n            ]\n\n        self.scale_one = nn.Sequential(*sequence)\n        self.first_tail = DicsriminatorTail(nf_mult=nf_mult, n_layers=3)\n        nf_mult_prev = 4\n        nf_mult = 8\n\n        self.scale_two = nn.Sequential(\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                      kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n            norm_layer(ndf * nf_mult),\n            nn.LeakyReLU(0.2, True))\n        nf_mult_prev = nf_mult\n        self.second_tail = DicsriminatorTail(nf_mult=nf_mult, n_layers=4)\n        self.scale_three = nn.Sequential(\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n            norm_layer(ndf * nf_mult),\n            nn.LeakyReLU(0.2, True))\n        self.third_tail = DicsriminatorTail(nf_mult=nf_mult, n_layers=5)\n\n    def forward(self, input):\n        x = self.scale_one(input)\n        x_1 = self.first_tail(x)\n        x = self.scale_two(x)\n        x_2 = self.second_tail(x)\n        x = self.scale_three(x)\n        x = self.third_tail(x)\n        return [x_1, x_2, x]\n\n\n# Defines the PatchGAN discriminator with the specified arguments.\nclass NLayerDiscriminator(nn.Module):\n    def __init__(self, input_nc=3, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False, use_parallel=True):\n        super(NLayerDiscriminator, self).__init__()\n        self.use_parallel = use_parallel\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        kw = 4\n        padw = int(np.ceil((kw-1)/2))\n        sequence = [\n            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        nf_mult = 1\n        for n in range(1, n_layers):\n            nf_mult_prev = nf_mult\n            nf_mult = min(2**n, 8)\n            sequence += [\n                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                          kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n                norm_layer(ndf * nf_mult),\n                nn.LeakyReLU(0.2, True)\n            ]\n\n        nf_mult_prev = nf_mult\n        nf_mult = min(2**n_layers, 8)\n        sequence += [\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                      kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n            norm_layer(ndf * nf_mult),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n\n        if use_sigmoid:\n            sequence += [nn.Sigmoid()]\n\n        self.model = nn.Sequential(*sequence)\n\n    def forward(self, input):\n        return self.model(input)\n\n\ndef get_fullD(model_config):\n    model_d = NLayerDiscriminator(n_layers=5,\n                                  norm_layer=get_norm_layer(norm_type=model_config[\'norm_layer\']),\n                                  use_sigmoid=False)\n    return model_d\n\n\ndef get_generator(model_config):\n    generator_name = model_config[\'g_name\']\n    if generator_name == \'resnet\':\n        model_g = ResnetGenerator(norm_layer=get_norm_layer(norm_type=model_config[\'norm_layer\']),\n                                  use_dropout=model_config[\'dropout\'],\n                                  n_blocks=model_config[\'blocks\'],\n                                  learn_residual=model_config[\'learn_residual\'])\n    elif generator_name == \'fpn_mobilenet\':\n        model_g = FPNMobileNet(norm_layer=get_norm_layer(norm_type=model_config[\'norm_layer\']))\n    elif generator_name == \'fpn_inception\':\n        model_g = FPNInception(norm_layer=get_norm_layer(norm_type=model_config[\'norm_layer\']))\n    elif generator_name == \'fpn_inception_simple\':\n        model_g = FPNInceptionSimple(norm_layer=get_norm_layer(norm_type=model_config[\'norm_layer\']))\n    elif generator_name == \'fpn_dense\':\n        model_g = FPNDense()\n    elif generator_name == \'unet_seresnext\':\n        model_g = UNetSEResNext(norm_layer=get_norm_layer(norm_type=model_config[\'norm_layer\']),\n                                pretrained=model_config[\'pretrained\'])\n    else:\n        raise ValueError(""Generator Network [%s] not recognized."" % generator_name)\n\n    return nn.DataParallel(model_g)\n\n\ndef get_discriminator(model_config):\n    discriminator_name = model_config[\'d_name\']\n    if discriminator_name == \'no_gan\':\n        model_d = None\n    elif discriminator_name == \'patch_gan\':\n        model_d = NLayerDiscriminator(n_layers=model_config[\'d_layers\'],\n                                      norm_layer=get_norm_layer(norm_type=model_config[\'norm_layer\']),\n                                      use_sigmoid=False)\n        model_d = nn.DataParallel(model_d)\n    elif discriminator_name == \'double_gan\':\n        patch_gan = NLayerDiscriminator(n_layers=model_config[\'d_layers\'],\n                                        norm_layer=get_norm_layer(norm_type=model_config[\'norm_layer\']),\n                                        use_sigmoid=False)\n        patch_gan = nn.DataParallel(patch_gan)\n        full_gan = get_fullD(model_config)\n        full_gan = nn.DataParallel(full_gan)\n        model_d = {\'patch\': patch_gan,\n                   \'full\': full_gan}\n    elif discriminator_name == \'multi_scale\':\n        model_d = MultiScaleDiscriminator(norm_layer=get_norm_layer(norm_type=model_config[\'norm_layer\']))\n        model_d = nn.DataParallel(model_d)\n    else:\n        raise ValueError(""Discriminator Network [%s] not recognized."" % discriminator_name)\n\n    return model_d\n\n\ndef get_nets(model_config):\n    return get_generator(model_config), get_discriminator(model_config)\n'"
models/senet.py,2,"b'from __future__ import print_function, division, absolute_import\nfrom collections import OrderedDict\nimport math\n\nimport torch.nn as nn\nfrom torch.utils import model_zoo\n\n__all__ = [\'SENet\', \'senet154\', \'se_resnet50\', \'se_resnet101\', \'se_resnet152\',\n           \'se_resnext50_32x4d\', \'se_resnext101_32x4d\']\n\npretrained_settings = {\n    \'senet154\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/senet154-c7b49a05.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnet50\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet50-ce0d4300.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnet101\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet101-7e38fcc6.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnet152\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet152-d17c99b7.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnext50_32x4d\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext50_32x4d-a260b3a4.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnext101_32x4d\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext101_32x4d-3b2fe3d8.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n}\n\n\nclass SEModule(nn.Module):\n\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1,\n                             padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1,\n                             padding=0)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        module_input = x\n        x = self.avg_pool(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return module_input * x\n\n\nclass Bottleneck(nn.Module):\n    """"""\n    Base class for bottlenecks that implements `forward()` method.\n    """"""\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = self.se_module(out) + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBottleneck(Bottleneck):\n    """"""\n    Bottleneck for SENet154.\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1)\n        self.bn1 = nn.InstanceNorm2d(planes * 2, affine=False)\n        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n                               stride=stride, padding=1, groups=groups)\n        self.bn2 = nn.InstanceNorm2d(planes * 4, affine=False)\n        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1)\n        self.bn3 = nn.InstanceNorm2d(planes * 4, affine=False)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNetBottleneck(Bottleneck):\n    """"""\n    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n    (the latter is used in the torchvision implementation of ResNet).\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEResNetBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1,\n                               stride=stride)\n        self.bn1 = nn.InstanceNorm2d(planes, affine=False)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1,\n                               groups=groups)\n        self.bn2 = nn.InstanceNorm2d(planes, affine=False)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1)\n        self.bn3 = nn.InstanceNorm2d(planes * 4, affine=False)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNeXtBottleneck(Bottleneck):\n    """"""\n    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None, base_width=4):\n        super(SEResNeXtBottleneck, self).__init__()\n        width = math.floor(planes * (base_width / 64)) * groups\n        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1,\n                               stride=1)\n        self.bn1 = nn.InstanceNorm2d(width, affine=False)\n        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n                               padding=1, groups=groups)\n        self.bn2 = nn.InstanceNorm2d(width, affine=False)\n        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1)\n        self.bn3 = nn.InstanceNorm2d(planes * 4, affine=False)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SENet(nn.Module):\n\n    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n                 downsample_padding=1, num_classes=1000):\n        """"""\n        Parameters\n        ----------\n        block (nn.Module): Bottleneck class.\n            - For SENet154: SEBottleneck\n            - For SE-ResNet models: SEResNetBottleneck\n            - For SE-ResNeXt models:  SEResNeXtBottleneck\n        layers (list of ints): Number of residual blocks for 4 layers of the\n            network (layer1...layer4).\n        groups (int): Number of groups for the 3x3 convolution in each\n            bottleneck block.\n            - For SENet154: 64\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models:  32\n        reduction (int): Reduction ratio for Squeeze-and-Excitation modules.\n            - For all models: 16\n        dropout_p (float or None): Drop probability for the Dropout layer.\n            If `None` the Dropout layer is not used.\n            - For SENet154: 0.2\n            - For SE-ResNet models: None\n            - For SE-ResNeXt models: None\n        inplanes (int):  Number of input channels for layer1.\n            - For SENet154: 128\n            - For SE-ResNet models: 64\n            - For SE-ResNeXt models: 64\n        input_3x3 (bool): If `True`, use three 3x3 convolutions instead of\n            a single 7x7 convolution in layer0.\n            - For SENet154: True\n            - For SE-ResNet models: False\n            - For SE-ResNeXt models: False\n        downsample_kernel_size (int): Kernel size for downsampling convolutions\n            in layer2, layer3 and layer4.\n            - For SENet154: 3\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models: 1\n        downsample_padding (int): Padding for downsampling convolutions in\n            layer2, layer3 and layer4.\n            - For SENet154: 1\n            - For SE-ResNet models: 0\n            - For SE-ResNeXt models: 0\n        num_classes (int): Number of outputs in `last_linear` layer.\n            - For all models: 1000\n        """"""\n        super(SENet, self).__init__()\n        self.inplanes = inplanes\n        if input_3x3:\n            layer0_modules = [\n                (\'conv1\', nn.Conv2d(3, 64, 3, stride=2, padding=1)),\n                (\'bn1\', nn.InstanceNorm2d(64, affine=False)),\n                (\'relu1\', nn.ReLU(inplace=True)),\n                (\'conv2\', nn.Conv2d(64, 64, 3, stride=1, padding=1)),\n                (\'bn2\', nn.InstanceNorm2d(64, affine=False)),\n                (\'relu2\', nn.ReLU(inplace=True)),\n                (\'conv3\', nn.Conv2d(64, inplanes, 3, stride=1, padding=1)),\n                (\'bn3\', nn.InstanceNorm2d(inplanes, affine=False)),\n                (\'relu3\', nn.ReLU(inplace=True)),\n            ]\n        else:\n            layer0_modules = [\n                (\'conv1\', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n                                    padding=3)),\n                (\'bn1\', nn.InstanceNorm2d(inplanes, affine=False)),\n                (\'relu1\', nn.ReLU(inplace=True)),\n            ]\n        # To preserve compatibility with Caffe weights `ceil_mode=True`\n        # is used instead of `padding=1`.\n        layer0_modules.append((\'pool\', nn.MaxPool2d(3, stride=2,\n                                                    ceil_mode=True)))\n        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n        self.layer1 = self._make_layer(\n            block,\n            planes=64,\n            blocks=layers[0],\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=1,\n            downsample_padding=0\n        )\n        self.layer2 = self._make_layer(\n            block,\n            planes=128,\n            blocks=layers[1],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer3 = self._make_layer(\n            block,\n            planes=256,\n            blocks=layers[2],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer4 = self._make_layer(\n            block,\n            planes=512,\n            blocks=layers[3],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.avg_pool = nn.AvgPool2d(7, stride=1)\n        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n                    downsample_kernel_size=1, downsample_padding=0):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=downsample_kernel_size, stride=stride,\n                          padding=downsample_padding),\n                nn.InstanceNorm2d(planes * block.expansion, affine=False),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n                            downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups, reduction))\n\n        return nn.Sequential(*layers)\n\n    def features(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def logits(self, x):\n        x = self.avg_pool(x)\n        if self.dropout is not None:\n            x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.logits(x)\n        return x\n\n\ndef initialize_pretrained_model(model, num_classes, settings):\n    assert num_classes == settings[\'num_classes\'], \\\n        \'num_classes should be {}, but is {}\'.format(\n            settings[\'num_classes\'], num_classes)\n    model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n    model.input_space = settings[\'input_space\']\n    model.input_size = settings[\'input_size\']\n    model.input_range = settings[\'input_range\']\n    model.mean = settings[\'mean\']\n    model.std = settings[\'std\']\n\n\ndef senet154(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEBottleneck, [3, 8, 36, 3], groups=64, reduction=16,\n                  dropout_p=0.2, num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'senet154\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet50(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEResNetBottleneck, [3, 4, 6, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnet50\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet101(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEResNetBottleneck, [3, 4, 23, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnet101\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet152(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEResNetBottleneck, [3, 8, 36, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnet152\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnext50_32x4d(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    return model\n\n\ndef se_resnext101_32x4d(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 23, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnext101_32x4d\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model'"
models/unet_seresnext.py,10,"b'import torch\nfrom torch import nn\nimport torch.nn.parallel\nimport torch.optim\nimport torch.utils.data\nfrom torch.nn import Sequential\nfrom collections import OrderedDict\nimport torchvision\nfrom torch.nn import functional as F\nfrom models.senet import se_resnext50_32x4d\n\n\ndef conv3x3(in_, out):\n    return nn.Conv2d(in_, out, 3, padding=1)\n\n\nclass ConvRelu(nn.Module):\n    def __init__(self, in_, out):\n        super(ConvRelu, self).__init__()\n        self.conv = conv3x3(in_, out)\n        self.activation = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.activation(x)\n        return x\n\nclass UNetSEResNext(nn.Module):\n\n    def __init__(self, num_classes=3, num_filters=32,\n             pretrained=True, is_deconv=True):\n        super().__init__()\n        self.num_classes = num_classes\n        pretrain = \'imagenet\' if pretrained is True else None\n        self.encoder = se_resnext50_32x4d(num_classes=1000, pretrained=pretrain)\n        bottom_channel_nr = 2048\n\n        self.conv1 = self.encoder.layer0\n        #self.se_e1 = SCSEBlock(64)\n        self.conv2 = self.encoder.layer1\n        #self.se_e2 = SCSEBlock(64 * 4)\n        self.conv3 = self.encoder.layer2\n        #self.se_e3 = SCSEBlock(128 * 4)\n        self.conv4 = self.encoder.layer3\n        #self.se_e4 = SCSEBlock(256 * 4)\n        self.conv5 = self.encoder.layer4\n        #self.se_e5 = SCSEBlock(512 * 4)\n\n        self.center = DecoderCenter(bottom_channel_nr, num_filters * 8 *2, num_filters * 8, False)\n\n        self.dec5 = DecoderBlockV(bottom_channel_nr + num_filters * 8, num_filters * 8 * 2, num_filters * 2, is_deconv)\n        #self.se_d5 = SCSEBlock(num_filters * 2)\n        self.dec4 = DecoderBlockV(bottom_channel_nr // 2 + num_filters * 2, num_filters * 8, num_filters * 2, is_deconv)\n        #self.se_d4 = SCSEBlock(num_filters * 2)\n        self.dec3 = DecoderBlockV(bottom_channel_nr // 4 + num_filters * 2, num_filters * 4, num_filters * 2, is_deconv)\n        #self.se_d3 = SCSEBlock(num_filters * 2)\n        self.dec2 = DecoderBlockV(bottom_channel_nr // 8 + num_filters * 2, num_filters * 2, num_filters * 2, is_deconv)\n        #self.se_d2 = SCSEBlock(num_filters * 2)\n        self.dec1 = DecoderBlockV(num_filters * 2, num_filters, num_filters * 2, is_deconv)\n        #self.se_d1 = SCSEBlock(num_filters * 2)\n        self.dec0 = ConvRelu(num_filters * 10, num_filters * 2)\n        self.final = nn.Conv2d(num_filters * 2, num_classes, kernel_size=1)\n\n    def forward(self, x):\n        conv1 = self.conv1(x)\n        #conv1 = self.se_e1(conv1)\n        conv2 = self.conv2(conv1)\n        #conv2 = self.se_e2(conv2)\n        conv3 = self.conv3(conv2)\n        #conv3 = self.se_e3(conv3)\n        conv4 = self.conv4(conv3)\n        #conv4 = self.se_e4(conv4)\n        conv5 = self.conv5(conv4)\n        #conv5 = self.se_e5(conv5)\n\n        center = self.center(conv5)\n        dec5 = self.dec5(torch.cat([center, conv5], 1))\n        #dec5 = self.se_d5(dec5)\n        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n        #dec4 = self.se_d4(dec4)\n        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n        #dec3 = self.se_d3(dec3)\n        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n        #dec2 = self.se_d2(dec2)\n        dec1 = self.dec1(dec2)\n        #dec1 = self.se_d1(dec1)\n\n        f = torch.cat((\n            dec1,\n            F.upsample(dec2, scale_factor=2, mode=\'bilinear\', align_corners=False),\n            F.upsample(dec3, scale_factor=4, mode=\'bilinear\', align_corners=False),\n            F.upsample(dec4, scale_factor=8, mode=\'bilinear\', align_corners=False),\n            F.upsample(dec5, scale_factor=16, mode=\'bilinear\', align_corners=False),\n        ), 1)\n\n        dec0 = self.dec0(f)\n\n        return self.final(dec0)\n\nclass DecoderBlockV(nn.Module):\n    def __init__(self, in_channels, middle_channels, out_channels, is_deconv=True):\n        super(DecoderBlockV, self).__init__()\n        self.in_channels = in_channels\n\n        if is_deconv:\n            self.block = nn.Sequential(\n                ConvRelu(in_channels, middle_channels),\n                nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=4, stride=2,\n                                   padding=1),\n                nn.InstanceNorm2d(out_channels, affine=False),\n                nn.ReLU(inplace=True)\n\n            )\n        else:\n            self.block = nn.Sequential(\n                nn.Upsample(scale_factor=2, mode=\'bilinear\'),\n                ConvRelu(in_channels, middle_channels),\n                ConvRelu(middle_channels, out_channels),\n            )\n\n    def forward(self, x):\n        return self.block(x)\n\n\n\nclass DecoderCenter(nn.Module):\n    def __init__(self, in_channels, middle_channels, out_channels, is_deconv=True):\n        super(DecoderCenter, self).__init__()\n        self.in_channels = in_channels\n\n\n        if is_deconv:\n            """"""\n                Paramaters for Deconvolution were chosen to avoid artifacts, following\n                link https://distill.pub/2016/deconv-checkerboard/\n            """"""\n\n            self.block = nn.Sequential(\n                ConvRelu(in_channels, middle_channels),\n                nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=4, stride=2,\n                                   padding=1),\n                nn.InstanceNorm2d(out_channels, affine=False),\n                nn.ReLU(inplace=True)\n            )\n        else:\n            self.block = nn.Sequential(\n                ConvRelu(in_channels, middle_channels),\n                ConvRelu(middle_channels, out_channels)\n\n            )\n\n    def forward(self, x):\n        return self.block(x)\n'"
util/__init__.py,0,b''
util/image_pool.py,3,"b'import random\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\nfrom collections import deque\n\n\nclass ImagePool():\n    def __init__(self, pool_size):\n        self.pool_size = pool_size\n        self.sample_size = pool_size\n        if self.pool_size > 0:\n            self.num_imgs = 0\n            self.images = deque()\n\n    def add(self, images):\n        if self.pool_size == 0:\n            return images\n        for image in images.data:\n            image = torch.unsqueeze(image, 0)\n            if self.num_imgs < self.pool_size:\n                self.num_imgs = self.num_imgs + 1\n                self.images.append(image)\n            else:\n                self.images.popleft()\n                self.images.append(image)\n\n    def query(self):\n        if len(self.images) > self.sample_size:\n            return_images = list(random.sample(self.images, self.sample_size))\n        else:\n            return_images = list(self.images)\n        return torch.cat(return_images, 0)\n'"
util/metrics.py,3,"b'import math\nfrom math import exp\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\ndef gaussian(window_size, sigma):\n    gauss = torch.Tensor([exp(-(x - window_size // 2) ** 2 / float(2 * sigma ** 2)) for x in range(window_size)])\n    return gauss / gauss.sum()\n\n\ndef create_window(window_size, channel):\n    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n    return window\n\n\ndef SSIM(img1, img2):\n    (_, channel, _, _) = img1.size()\n    window_size = 11\n    window = create_window(window_size, channel)\n\n    if img1.is_cuda:\n        window = window.cuda(img1.get_device())\n    window = window.type_as(img1)\n\n    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)\n    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)\n\n    mu1_sq = mu1.pow(2)\n    mu2_sq = mu2.pow(2)\n    mu1_mu2 = mu1 * mu2\n\n    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq\n    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq\n    sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2\n\n    C1 = 0.01 ** 2\n    C2 = 0.03 ** 2\n\n    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n    return ssim_map.mean()\n\n\ndef PSNR(img1, img2):\n    mse = np.mean((img1 / 255. - img2 / 255.) ** 2)\n    if mse == 0:\n        return 100\n    PIXEL_MAX = 1\n    return 20 * math.log10(PIXEL_MAX / math.sqrt(mse))\n'"
