file_path,api_count,code
predict.py,0,"b'import time\nimport os\nfrom options.test_options import TestOptions\nfrom data.data_loader import CreateDataLoader\nfrom models.models import create_model\nfrom util.visualizer import Visualizer\nfrom pdb import set_trace as st\nfrom util import html\n\nopt = TestOptions().parse()\nopt.nThreads = 1   # test code only supports nThreads = 1\nopt.batchSize = 1  # test code only supports batchSize = 1\nopt.serial_batches = True  # no shuffle\nopt.no_flip = True  # no flip\n\ndata_loader = CreateDataLoader(opt)\ndataset = data_loader.load_data()\nmodel = create_model(opt)\nvisualizer = Visualizer(opt)\n# create website\nweb_dir = os.path.join(""./ablation/"", opt.name, \'%s_%s\' % (opt.phase, opt.which_epoch))\nwebpage = html.HTML(web_dir, \'Experiment = %s, Phase = %s, Epoch = %s\' % (opt.name, opt.phase, opt.which_epoch))\n# test\nprint(len(dataset))\nfor i, data in enumerate(dataset):\n    model.set_input(data)\n    visuals = model.predict()\n    img_path = model.get_image_paths()\n    print(\'process image... %s\' % img_path)\n    visualizer.save_images(webpage, visuals, img_path)\n\nwebpage.save()\n'"
train.py,0,"b""import time\nfrom options.train_options import TrainOptions\nfrom data.data_loader import CreateDataLoader\nfrom models.models import create_model\nfrom util.visualizer import Visualizer\n\ndef get_config(config):\n    import yaml\n    with open(config, 'r') as stream:\n        return yaml.load(stream)\n\nopt = TrainOptions().parse()\nconfig = get_config(opt.config)\ndata_loader = CreateDataLoader(opt)\ndataset = data_loader.load_data()\ndataset_size = len(data_loader)\nprint('#training images = %d' % dataset_size)\n\nmodel = create_model(opt)\nvisualizer = Visualizer(opt)\n\ntotal_steps = 0\n\nfor epoch in range(1, opt.niter + opt.niter_decay + 1):\n    epoch_start_time = time.time()\n    for i, data in enumerate(dataset):\n        iter_start_time = time.time()\n        total_steps += opt.batchSize\n        epoch_iter = total_steps - dataset_size * (epoch - 1)\n        model.set_input(data)\n        model.optimize_parameters(epoch)\n\n        if total_steps % opt.display_freq == 0:\n            visualizer.display_current_results(model.get_current_visuals(), epoch)\n\n        if total_steps % opt.print_freq == 0:\n            errors = model.get_current_errors(epoch)\n            t = (time.time() - iter_start_time) / opt.batchSize\n            visualizer.print_current_errors(epoch, epoch_iter, errors, t)\n            if opt.display_id > 0:\n                visualizer.plot_current_errors(epoch, float(epoch_iter)/dataset_size, opt, errors)\n\n        if total_steps % opt.save_latest_freq == 0:\n            print('saving the latest model (epoch %d, total_steps %d)' %\n                  (epoch, total_steps))\n            model.save('latest')\n\n    if epoch % opt.save_epoch_freq == 0:\n        print('saving the model at the end of epoch %d, iters %d' %\n              (epoch, total_steps))\n        model.save('latest')\n        model.save(epoch)\n\n    print('End of epoch %d / %d \\t Time Taken: %d sec' %\n          (epoch, opt.niter + opt.niter_decay, time.time() - epoch_start_time))\n\n    if opt.new_lr:\n        if epoch == opt.niter:\n            model.update_learning_rate()\n        elif epoch == (opt.niter + 20):\n            model.update_learning_rate()\n        elif epoch == (opt.niter + 70):\n            model.update_learning_rate()\n        elif epoch == (opt.niter + 90):\n            model.update_learning_rate()\n            model.update_learning_rate()\n            model.update_learning_rate()\n            model.update_learning_rate()\n    else:\n        if epoch > opt.niter:\n            model.update_learning_rate()\n"""
data/__init__.py,0,b''
data/aligned_dataset.py,1,"b""import os.path\nimport random\nimport torchvision.transforms as transforms\nimport torch\nfrom data.base_dataset import BaseDataset\nfrom data.image_folder import make_dataset\nfrom PIL import Image\n\n\nclass AlignedDataset(BaseDataset):\n    def initialize(self, opt):\n        self.opt = opt\n        self.root = opt.dataroot\n        self.dir_AB = os.path.join(opt.dataroot, opt.phase)\n\n        self.AB_paths = sorted(make_dataset(self.dir_AB))\n\n        assert(opt.resize_or_crop == 'resize_and_crop')\n\n        transform_list = [transforms.ToTensor(),\n                          transforms.Normalize((0.5, 0.5, 0.5),\n                                               (0.5, 0.5, 0.5))]\n\n        self.transform = transforms.Compose(transform_list)\n\n    def __getitem__(self, index):\n        AB_path = self.AB_paths[index]\n        AB = Image.open(AB_path).convert('RGB')\n        AB = AB.resize((self.opt.loadSize * 2, self.opt.loadSize), Image.BICUBIC)\n        AB = self.transform(AB)\n\n        w_total = AB.size(2)\n        w = int(w_total / 2)\n        h = AB.size(1)\n        w_offset = random.randint(0, max(0, w - self.opt.fineSize - 1))\n        h_offset = random.randint(0, max(0, h - self.opt.fineSize - 1))\n\n        A = AB[:, h_offset:h_offset + self.opt.fineSize,\n               w_offset:w_offset + self.opt.fineSize]\n        B = AB[:, h_offset:h_offset + self.opt.fineSize,\n               w + w_offset:w + w_offset + self.opt.fineSize]\n\n        if (not self.opt.no_flip) and random.random() < 0.5:\n            idx = [i for i in range(A.size(2) - 1, -1, -1)]\n            idx = torch.LongTensor(idx)\n            A = A.index_select(2, idx)\n            B = B.index_select(2, idx)\n\n        return {'A': A, 'B': B,\n                'A_paths': AB_path, 'B_paths': AB_path}\n\n    def __len__(self):\n        return len(self.AB_paths)\n\n    def name(self):\n        return 'AlignedDataset'\n"""
data/base_data_loader.py,0,"b'\nclass BaseDataLoader():\n    def __init__(self):\n        pass\n    \n    def initialize(self, opt):\n        self.opt = opt\n        pass\n\n    def load_data():\n        return None\n\n        \n        \n'"
data/base_dataset.py,1,"b""import torch.utils.data as data\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport random\n\nclass BaseDataset(data.Dataset):\n    def __init__(self):\n        super(BaseDataset, self).__init__()\n\n    def name(self):\n        return 'BaseDataset'\n\n    def initialize(self, opt):\n        pass\n\ndef get_transform(opt):\n    transform_list = []\n    if opt.resize_or_crop == 'resize_and_crop':\n        zoom = 1 + 0.1*radom.randint(0,4)\n        osize = [int(400*zoom), int(600*zoom)]\n        transform_list.append(transforms.Scale(osize, Image.BICUBIC))\n        transform_list.append(transforms.RandomCrop(opt.fineSize))\n    elif opt.resize_or_crop == 'crop':\n        transform_list.append(transforms.RandomCrop(opt.fineSize))\n    elif opt.resize_or_crop == 'scale_width':\n        transform_list.append(transforms.Lambda(\n            lambda img: __scale_width(img, opt.fineSize)))\n    elif opt.resize_or_crop == 'scale_width_and_crop':\n        transform_list.append(transforms.Lambda(\n            lambda img: __scale_width(img, opt.loadSize)))\n        transform_list.append(transforms.RandomCrop(opt.fineSize))  \n    # elif opt.resize_or_crop == 'no':\n    #     osize = [384, 512]\n    #     transform_list.append(transforms.Scale(osize, Image.BICUBIC))\n\n    if opt.isTrain and not opt.no_flip:\n        transform_list.append(transforms.RandomHorizontalFlip())\n\n    transform_list += [transforms.ToTensor(),\n                       transforms.Normalize((0.5, 0.5, 0.5),\n                                            (0.5, 0.5, 0.5))]\n    return transforms.Compose(transform_list)\n\ndef __scale_width(img, target_width):\n    ow, oh = img.size\n    if (ow == target_width):\n        return img\n    w = target_width\n    h = int(target_width * oh / ow)\n    return img.resize((w, h), Image.BICUBIC)\n"""
data/custom_dataset_data_loader.py,2,"b'import torch.utils.data\nfrom data.base_data_loader import BaseDataLoader\n\n\ndef CreateDataset(opt):\n    dataset = None\n    if opt.dataset_mode == \'aligned\':\n        from data.aligned_dataset import AlignedDataset\n        dataset = AlignedDataset()\n    elif opt.dataset_mode == \'unaligned\':\n        from data.unaligned_dataset import UnalignedDataset\n        dataset = UnalignedDataset()\n    elif opt.dataset_mode == \'unaligned_random_crop\':\n        from data.unaligned_random_crop import UnalignedDataset\n        dataset = UnalignedDataset()\n    elif opt.dataset_mode == \'pair\':\n        from data.pair_dataset import PairDataset\n        dataset = PairDataset()\n    elif opt.dataset_mode == \'syn\':\n        from data.syn_dataset import PairDataset\n        dataset = PairDataset()\n    elif opt.dataset_mode == \'single\':\n        from data.single_dataset import SingleDataset\n        dataset = SingleDataset()\n    else:\n        raise ValueError(""Dataset [%s] not recognized."" % opt.dataset_mode)\n\n    print(""dataset [%s] was created"" % (dataset.name()))\n    dataset.initialize(opt)\n    return dataset\n\n\nclass CustomDatasetDataLoader(BaseDataLoader):\n    def name(self):\n        return \'CustomDatasetDataLoader\'\n\n    def initialize(self, opt):\n        BaseDataLoader.initialize(self, opt)\n        self.dataset = CreateDataset(opt)\n        self.dataloader = torch.utils.data.DataLoader(\n            self.dataset,\n            batch_size=opt.batchSize,\n            shuffle=not opt.serial_batches,\n            num_workers=int(opt.nThreads))\n\n    def load_data(self):\n        return self.dataloader\n\n    def __len__(self):\n        return min(len(self.dataset), self.opt.max_dataset_size)\n'"
data/data_loader.py,0,b'\ndef CreateDataLoader(opt):\n    from data.custom_dataset_data_loader import CustomDatasetDataLoader\n    data_loader = CustomDatasetDataLoader()\n    print(data_loader.name())\n    data_loader.initialize(opt)\n    return data_loader\n'
data/image_folder.py,1,"b'###############################################################################\n# Code from\n# https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py\n# Modified the original code so that it also loads images from the current\n# directory as well as the subdirectories\n###############################################################################\n\nimport torch.utils.data as data\n\nfrom PIL import Image\nimport os\nimport os.path\n\nIMG_EXTENSIONS = [\n    \'.jpg\', \'.JPG\', \'.jpeg\', \'.JPEG\',\n    \'.png\', \'.PNG\', \'.ppm\', \'.PPM\', \'.bmp\', \'.BMP\',\n]\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\n\ndef make_dataset(dir):\n    images = []\n    assert os.path.isdir(dir), \'%s is not a valid directory\' % dir\n\n    for root, _, fnames in sorted(os.walk(dir)):\n        for fname in fnames:\n            if is_image_file(fname):\n                path = os.path.join(root, fname)\n                images.append(path)\n\n    return images\n\ndef store_dataset(dir):\n    images = []\n    all_path = []\n    assert os.path.isdir(dir), \'%s is not a valid directory\' % dir\n\n    for root, _, fnames in sorted(os.walk(dir)):\n        for fname in fnames:\n            if is_image_file(fname):\n                path = os.path.join(root, fname)\n                img = Image.open(path).convert(\'RGB\')\n                images.append(img)\n                all_path.append(path)\n\n    return images, all_path\n\n\ndef default_loader(path):\n    return Image.open(path).convert(\'RGB\')\n\n\nclass ImageFolder(data.Dataset):\n\n    def __init__(self, root, transform=None, return_paths=False,\n                 loader=default_loader):\n        imgs = make_dataset(root)\n        if len(imgs) == 0:\n            raise(RuntimeError(""Found 0 images in: "" + root + ""\\n""\n                               ""Supported image extensions are: "" +\n                               "","".join(IMG_EXTENSIONS)))\n\n        self.root = root\n        self.imgs = imgs\n        self.transform = transform\n        self.return_paths = return_paths\n        self.loader = loader\n\n    def __getitem__(self, index):\n        path = self.imgs[index]\n        img = self.loader(path)\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.return_paths:\n            return img, path\n        else:\n            return img\n\n    def __len__(self):\n        return len(self.imgs)\n'"
data/pair_dataset.py,4,"b'import os.path\nimport torchvision.transforms as transforms\nfrom data.base_dataset import BaseDataset, get_transform\nfrom data.image_folder import make_dataset\nfrom PIL import Image\nimport PIL\nimport random\nimport torch\nfrom pdb import set_trace as st\n\n\nclass PairDataset(BaseDataset):\n    def initialize(self, opt):\n        self.opt = opt\n        self.root = opt.dataroot\n        self.dir_A = os.path.join(opt.dataroot, opt.phase + \'A\')\n        self.dir_B = os.path.join(opt.dataroot, opt.phase + \'B\')\n\n        self.A_paths = make_dataset(self.dir_A)\n        self.B_paths = make_dataset(self.dir_B)\n\n        self.A_paths = sorted(self.A_paths)\n        self.B_paths = sorted(self.B_paths)\n        self.A_size = len(self.A_paths)\n        self.B_size = len(self.B_paths)\n        \n        transform_list = []\n        \n        transform_list += [transforms.ToTensor(),\n                          transforms.Normalize((0.5, 0.5, 0.5),\n                                               (0.5, 0.5, 0.5))]\n        # transform_list = [transforms.ToTensor()]\n\n        self.transform = transforms.Compose(transform_list)\n        # self.transform = get_transform(opt)\n\n    def __getitem__(self, index):\n        A_path = self.A_paths[index % self.A_size]\n        B_path = self.B_paths[index % self.B_size]\n\n        A_img = Image.open(A_path).convert(\'RGB\')\n        B_img = Image.open(A_path.replace(""low"", ""normal"").replace(""A"", ""B"")).convert(\'RGB\')\n\n\n        A_img = self.transform(A_img)\n        B_img = self.transform(B_img)\n\n        w = A_img.size(2)\n        h = A_img.size(1)\n        w_offset = random.randint(0, max(0, w - self.opt.fineSize - 1))\n        h_offset = random.randint(0, max(0, h - self.opt.fineSize - 1))\n\n        A_img = A_img[:, h_offset:h_offset + self.opt.fineSize,\n               w_offset:w_offset + self.opt.fineSize]\n        B_img = B_img[:, h_offset:h_offset + self.opt.fineSize,\n               w_offset:w_offset + self.opt.fineSize]\n\n\n        if self.opt.resize_or_crop == \'no\':\n            r,g,b = A_img[0]+1, A_img[1]+1, A_img[2]+1\n            A_gray = 1. - (0.299*r+0.587*g+0.114*b)/2.\n            A_gray = torch.unsqueeze(A_gray, 0)\n            input_img = A_img\n            # A_gray = (1./A_gray)/255.\n        else:\n            \n            \n            # A_gray = (1./A_gray)/255.\n            if (not self.opt.no_flip) and random.random() < 0.5:\n                idx = [i for i in range(A_img.size(2) - 1, -1, -1)]\n                idx = torch.LongTensor(idx)\n                A_img = A_img.index_select(2, idx)\n                B_img = B_img.index_select(2, idx)\n            if (not self.opt.no_flip) and random.random() < 0.5:\n                idx = [i for i in range(A_img.size(1) - 1, -1, -1)]\n                idx = torch.LongTensor(idx)\n                A_img = A_img.index_select(1, idx)\n                B_img = B_img.index_select(1, idx)\n            if (not self.opt.no_flip) and random.random() < 0.5:\n                times = random.randint(self.opt.low_times,self.opt.high_times)/100.\n                input_img = (A_img+1)/2./times\n                input_img = input_img*2-1\n            else:\n                input_img = A_img\n            r,g,b = input_img[0]+1, input_img[1]+1, input_img[2]+1\n            A_gray = 1. - (0.299*r+0.587*g+0.114*b)/2.\n            A_gray = torch.unsqueeze(A_gray, 0)\n        return {\'A\': A_img, \'B\': B_img, \'A_gray\': A_gray, \'input_img\':input_img,\n                \'A_paths\': A_path, \'B_paths\': B_path}\n\n    def __len__(self):\n        return self.A_size\n\n    def name(self):\n        return \'PairDataset\'\n'"
data/single_dataset.py,0,"b""import os.path\nimport torchvision.transforms as transforms\nfrom data.base_dataset import BaseDataset, get_transform\nfrom data.image_folder import make_dataset\nfrom PIL import Image\n\n\nclass SingleDataset(BaseDataset):\n    def initialize(self, opt):\n        self.opt = opt\n        self.root = opt.dataroot\n        self.dir_A = os.path.join(opt.dataroot)\n\n        self.A_paths = make_dataset(self.dir_A)\n\n        self.A_paths = sorted(self.A_paths)\n\n        self.transform = get_transform(opt)\n\n    def __getitem__(self, index):\n        A_path = self.A_paths[index]\n\n        A_img = Image.open(A_path).convert('RGB')\n        A_size = A_img.size\n        A_size = A_size = (A_size[0]//16*16, A_size[1]//16*16)\n        A_img = A_img.resize(A_size, Image.BICUBIC)\n\n        A_img = self.transform(A_img)\n\n        return {'A': A_img, 'A_paths': A_path}\n\n    def __len__(self):\n        return len(self.A_paths)\n\n    def name(self):\n        return 'SingleImageDataset'\n"""
data/syn_dataset.py,4,"b'import os.path\nimport torchvision.transforms as transforms\nfrom data.base_dataset import BaseDataset, get_transform\nfrom data.image_folder import make_dataset\nfrom PIL import Image\nimport PIL\nimport random\nimport torch\nfrom pdb import set_trace as st\n\n\nclass PairDataset(BaseDataset):\n    def initialize(self, opt):\n        self.opt = opt\n        self.root = opt.dataroot\n        self.dir_A = os.path.join(opt.dataroot, opt.phase + \'A\')\n        self.dir_B = os.path.join(opt.dataroot, opt.phase + \'B\')\n\n        self.A_paths = make_dataset(self.dir_A)\n        self.B_paths = make_dataset(self.dir_B)\n\n        self.A_paths = sorted(self.A_paths)\n        self.B_paths = sorted(self.B_paths)\n        self.A_size = len(self.A_paths)\n        self.B_size = len(self.B_paths)\n        \n        transform_list = []\n        \n        transform_list += [transforms.ToTensor(),\n                          transforms.Normalize((0.5, 0.5, 0.5),\n                                               (0.5, 0.5, 0.5))]\n        # transform_list = [transforms.ToTensor()]\n\n        self.transform = transforms.Compose(transform_list)\n        # self.transform = get_transform(opt)\n\n    def __getitem__(self, index):\n        A_path = self.A_paths[index % self.A_size]\n        B_path = self.B_paths[index % self.B_size]\n\n        B_img = Image.open(B_path).convert(\'RGB\')\n        # B_img = Image.open(A_path.replace(""low"", ""normal"").replace(""A"", ""B"")).convert(\'RGB\')\n\n\n        # A_img = self.transform(A_img)\n        B_img = self.transform(B_img)\n\n        w = B_img.size(2)\n        h = B_img.size(1)\n        w_offset = random.randint(0, max(0, w - self.opt.fineSize - 1))\n        h_offset = random.randint(0, max(0, h - self.opt.fineSize - 1))\n\n        B_img = B_img[:, h_offset:h_offset + self.opt.fineSize,\n               w_offset:w_offset + self.opt.fineSize]\n\n\n        if self.opt.resize_or_crop == \'no\':\n            pass\n            # r,g,b = A_img[0]+1, A_img[1]+1, A_img[2]+1\n            # A_gray = 1. - (0.299*r+0.587*g+0.114*b)/2.\n            # A_gray = torch.unsqueeze(A_gray, 0)\n            # input_img = A_img\n            # A_gray = (1./A_gray)/255.\n        else:\n            \n            \n            # A_gray = (1./A_gray)/255.\n            if (not self.opt.no_flip) and random.random() < 0.5:\n                idx = [i for i in range(B_img.size(2) - 1, -1, -1)]\n                idx = torch.LongTensor(idx)\n                B_img = B_img.index_select(2, idx)\n            if (not self.opt.no_flip) and random.random() < 0.5:\n                idx = [i for i in range(B_img.size(1) - 1, -1, -1)]\n                idx = torch.LongTensor(idx)\n                B_img = B_img.index_select(1, idx)\n\n            times = random.randint(self.opt.low_times,self.opt.high_times)/100.\n            input_img = (B_img+1)/2./times\n            input_img = input_img*2-1\n            A_img = input_img\n            r,g,b = input_img[0]+1, input_img[1]+1, input_img[2]+1\n            A_gray = 1. - (0.299*r+0.587*g+0.114*b)/2.\n            A_gray = torch.unsqueeze(A_gray, 0)\n        return {\'A\': A_img, \'B\': B_img, \'A_gray\': A_gray, \'input_img\':input_img,\n                \'A_paths\': A_path, \'B_paths\': B_path}\n\n    def __len__(self):\n        return self.A_size\n\n    def name(self):\n        return \'PairDataset\'\n'"
data/unaligned_dataset.py,5,"b""import torch\nfrom torch import nn\nimport os.path\nimport torchvision.transforms as transforms\nfrom data.base_dataset import BaseDataset, get_transform\nfrom data.image_folder import make_dataset, store_dataset\nimport random\nfrom PIL import Image\nimport PIL\nfrom pdb import set_trace as st\n\ndef pad_tensor(input):\n    \n    height_org, width_org = input.shape[2], input.shape[3]\n    divide = 16\n\n    if width_org % divide != 0 or height_org % divide != 0:\n\n        width_res = width_org % divide\n        height_res = height_org % divide\n        if width_res != 0:\n            width_div = divide - width_res\n            pad_left = int(width_div / 2)\n            pad_right = int(width_div - pad_left)\n        else:\n            pad_left = 0\n            pad_right = 0\n\n        if height_res != 0:\n            height_div = divide - height_res\n            pad_top = int(height_div  / 2)\n            pad_bottom = int(height_div  - pad_top)\n        else:\n            pad_top = 0\n            pad_bottom = 0\n\n            padding = nn.ReflectionPad2d((pad_left, pad_right, pad_top, pad_bottom))\n            input = padding(input).data\n    else:\n        pad_left = 0\n        pad_right = 0\n        pad_top = 0\n        pad_bottom = 0\n\n    height, width = input.shape[2], input.shape[3]\n    assert width % divide == 0, 'width cant divided by stride'\n    assert height % divide == 0, 'height cant divided by stride'\n\n    return input, pad_left, pad_right, pad_top, pad_bottom\n\ndef pad_tensor_back(input, pad_left, pad_right, pad_top, pad_bottom):\n    height, width = input.shape[2], input.shape[3]\n    return input[:,:, pad_top: height - pad_bottom, pad_left: width - pad_right]\n\n\nclass UnalignedDataset(BaseDataset):\n    def initialize(self, opt):\n        self.opt = opt\n        self.root = opt.dataroot\n        self.dir_A = os.path.join(opt.dataroot, opt.phase + 'A')\n        self.dir_B = os.path.join(opt.dataroot, opt.phase + 'B')\n\n        # self.A_paths = make_dataset(self.dir_A)\n        # self.B_paths = make_dataset(self.dir_B)\n        self.A_imgs, self.A_paths = store_dataset(self.dir_A)\n        self.B_imgs, self.B_paths = store_dataset(self.dir_B)\n\n        # self.A_paths = sorted(self.A_paths)\n        # self.B_paths = sorted(self.B_paths)\n        self.A_size = len(self.A_paths)\n        self.B_size = len(self.B_paths)\n        \n        self.transform = get_transform(opt)\n\n    def __getitem__(self, index):\n        # A_path = self.A_paths[index % self.A_size]\n        # B_path = self.B_paths[index % self.B_size]\n\n        # A_img = Image.open(A_path).convert('RGB')\n        # B_img = Image.open(B_path).convert('RGB')\n        A_img = self.A_imgs[index % self.A_size]\n        B_img = self.B_imgs[index % self.B_size]\n        A_path = self.A_paths[index % self.A_size]\n        B_path = self.B_paths[index % self.B_size]\n        # A_size = A_img.size\n        # B_size = B_img.size\n        # A_size = A_size = (A_size[0]//16*16, A_size[1]//16*16)\n        # B_size = B_size = (B_size[0]//16*16, B_size[1]//16*16)\n        # A_img = A_img.resize(A_size, Image.BICUBIC)\n        # B_img = B_img.resize(B_size, Image.BICUBIC)\n        # A_gray = A_img.convert('LA')\n        # A_gray = 255.0-A_gray\n\n        A_img = self.transform(A_img)\n        B_img = self.transform(B_img)\n\n        \n        if self.opt.resize_or_crop == 'no':\n            r,g,b = A_img[0]+1, A_img[1]+1, A_img[2]+1\n            A_gray = 1. - (0.299*r+0.587*g+0.114*b)/2.\n            A_gray = torch.unsqueeze(A_gray, 0)\n            input_img = A_img\n            # A_gray = (1./A_gray)/255.\n        else:\n            w = A_img.size(2)\n            h = A_img.size(1)\n            \n            # A_gray = (1./A_gray)/255.\n            if (not self.opt.no_flip) and random.random() < 0.5:\n                idx = [i for i in range(A_img.size(2) - 1, -1, -1)]\n                idx = torch.LongTensor(idx)\n                A_img = A_img.index_select(2, idx)\n                B_img = B_img.index_select(2, idx)\n            if (not self.opt.no_flip) and random.random() < 0.5:\n                idx = [i for i in range(A_img.size(1) - 1, -1, -1)]\n                idx = torch.LongTensor(idx)\n                A_img = A_img.index_select(1, idx)\n                B_img = B_img.index_select(1, idx)\n            if self.opt.vary == 1 and (not self.opt.no_flip) and random.random() < 0.5:\n                times = random.randint(self.opt.low_times,self.opt.high_times)/100.\n                input_img = (A_img+1)/2./times\n                input_img = input_img*2-1\n            else:\n                input_img = A_img\n            if self.opt.lighten:\n                B_img = (B_img + 1)/2.\n                B_img = (B_img - torch.min(B_img))/(torch.max(B_img) - torch.min(B_img))\n                B_img = B_img*2. -1\n            r,g,b = input_img[0]+1, input_img[1]+1, input_img[2]+1\n            A_gray = 1. - (0.299*r+0.587*g+0.114*b)/2.\n            A_gray = torch.unsqueeze(A_gray, 0)\n        return {'A': A_img, 'B': B_img, 'A_gray': A_gray, 'input_img': input_img,\n                'A_paths': A_path, 'B_paths': B_path}\n\n    def __len__(self):\n        return max(self.A_size, self.B_size)\n\n    def name(self):\n        return 'UnalignedDataset'\n\n\n"""
data/unaligned_random_crop.py,2,"b""import torch\nimport os.path\nimport torchvision.transforms as transforms\nfrom data.base_dataset import BaseDataset, get_transform\nfrom data.image_folder import make_dataset\nimport random\nfrom PIL import Image\nimport PIL\nfrom pdb import set_trace as st\n\n\nclass UnalignedDataset(BaseDataset):\n    def initialize(self, opt):\n        self.opt = opt\n        self.root = opt.dataroot\n        self.dir_A = os.path.join(opt.dataroot, opt.phase + 'A')\n        self.dir_B = os.path.join(opt.dataroot, opt.phase + 'B')\n\n        self.A_paths = make_dataset(self.dir_A)\n        self.B_paths = make_dataset(self.dir_B)\n\n        self.A_paths = sorted(self.A_paths)\n        self.B_paths = sorted(self.B_paths)\n        self.A_size = len(self.A_paths)\n        self.B_size = len(self.B_paths)\n        \n        transform_list = [transforms.ToTensor(),\n                          transforms.Normalize((0.5, 0.5, 0.5),\n                                               (0.5, 0.5, 0.5))]\n\n        self.transform = transforms.Compose(transform_list)\n        # self.transform = get_transform(opt)\n\n    def __getitem__(self, index):\n        A_path = self.A_paths[index % self.A_size]\n        B_path = self.B_paths[index % self.B_size]\n\n        A_img = Image.open(A_path).convert('RGB')\n        B_img = Image.open(B_path).convert('RGB')\n        A_size = A_img.size\n        B_size = B_img.size\n        A_size = A_size = (A_size[0]//16*16, A_size[1]//16*16)\n        B_size = B_size = (B_size[0]//16*16, B_size[1]//16*16)\n        A_img = A_img.resize(A_size, Image.BICUBIC)\n        B_img = B_img.resize(B_size, Image.BICUBIC)\n\n\n        A_img = self.transform(A_img)\n        B_img = self.transform(B_img)\n\n        if self.opt.resize_or_crop == 'no':\n            pass\n        else:\n            w = A_img.size(2)\n            h = A_img.size(1)\n            size = [8,16,22]\n            from random import randint\n            size_index = randint(0,2)\n            Cropsize = size[size_index]*16\n\n            w_offset = random.randint(0, max(0, w - Cropsize - 1))\n            h_offset = random.randint(0, max(0, h - Cropsize - 1))\n\n            A_img = A_img[:, h_offset:h_offset + Cropsize,\n                   w_offset:w_offset + Cropsize]\n\n            if (not self.opt.no_flip) and random.random() < 0.5:\n                idx = [i for i in range(A_img.size(2) - 1, -1, -1)]\n                idx = torch.LongTensor(idx)\n                A_img = A_img.index_select(2, idx)\n                B_img = B_img.index_select(2, idx)\n            if (not self.opt.no_flip) and random.random() < 0.5:\n                idx = [i for i in range(A_img.size(1) - 1, -1, -1)]\n                idx = torch.LongTensor(idx)\n                A_img = A_img.index_select(1, idx)\n                B_img = B_img.index_select(1, idx)\n\n        return {'A': A_img, 'B': B_img,\n                'A_paths': A_path, 'B_paths': B_path}\n\n    def __len__(self):\n        return max(self.A_size, self.B_size)\n\n    def name(self):\n        return 'UnalignedDataset'\n"""
datasets/combine_A_and_B.py,0,"b""from pdb import set_trace as st\nimport os\nimport numpy as np\nimport cv2\nimport argparse\n\nparser = argparse.ArgumentParser('create image pairs')\nparser.add_argument('--fold_A', dest='fold_A', help='input directory for image A', type=str, default='../dataset/50kshoes_edges')\nparser.add_argument('--fold_B', dest='fold_B', help='input directory for image B', type=str, default='../dataset/50kshoes_jpg')\nparser.add_argument('--fold_AB', dest='fold_AB', help='output directory', type=str, default='../dataset/test_AB')\nparser.add_argument('--num_imgs', dest='num_imgs', help='number of images',type=int, default=1000000)\nparser.add_argument('--use_AB', dest='use_AB', help='if true: (0001_A, 0001_B) to (0001_AB)',action='store_true')\nargs = parser.parse_args()\n\nfor arg in vars(args):\n    print('[%s] = ' % arg,  getattr(args, arg))\n\nsplits = os.listdir(args.fold_A)\n\nfor sp in splits:\n    img_fold_A = os.path.join(args.fold_A, sp)\n    img_fold_B = os.path.join(args.fold_B, sp)\n    img_list = os.listdir(img_fold_A)\n    if args.use_AB: \n        img_list = [img_path for img_path in img_list if '_A.' in img_path]\n\n    num_imgs = min(args.num_imgs, len(img_list))\n    print('split = %s, use %d/%d images' % (sp, num_imgs, len(img_list)))\n    img_fold_AB = os.path.join(args.fold_AB, sp)\n    if not os.path.isdir(img_fold_AB):\n        os.makedirs(img_fold_AB)\n    print('split = %s, number of images = %d' % (sp, num_imgs))\n    for n in range(num_imgs):\n        name_A = img_list[n]\n        path_A = os.path.join(img_fold_A, name_A)\n        if args.use_AB:\n            name_B = name_A.replace('_A.', '_B.')\n        else:\n            name_B = name_A\n        path_B = os.path.join(img_fold_B, name_B)\n        if os.path.isfile(path_A) and os.path.isfile(path_B):\n            name_AB = name_A\n            if args.use_AB:\n                name_AB = name_AB.replace('_A.', '.') # remove _A\n            path_AB = os.path.join(img_fold_AB, name_AB)\n            im_A = cv2.imread(path_A, cv2.CV_LOAD_IMAGE_COLOR)\n            im_B = cv2.imread(path_B, cv2.CV_LOAD_IMAGE_COLOR)\n            im_AB = np.concatenate([im_A, im_B], 1)\n            cv2.imwrite(path_AB, im_AB)\n"""
models/Unet_L1.py,7,"b'import numpy as np\nimport torch\nimport os\nfrom collections import OrderedDict\nfrom torch.autograd import Variable\nimport util.util as util\nfrom collections import OrderedDict\nfrom torch.autograd import Variable\nimport itertools\nimport util.util as util\nfrom util.image_pool import ImagePool\nfrom .base_model import BaseModel\nfrom . import networks\nimport sys\n\n\nclass PairModel(BaseModel):\n    def name(self):\n        return \'CycleGANModel\'\n\n    def initialize(self, opt):\n        BaseModel.initialize(self, opt)\n\n        nb = opt.batchSize\n        size = opt.fineSize\n        self.opt = opt\n        self.input_A = self.Tensor(nb, opt.input_nc, size, size)\n        self.input_B = self.Tensor(nb, opt.output_nc, size, size)\n        self.input_img = self.Tensor(nb, opt.input_nc, size, size)\n        self.input_A_gray = self.Tensor(nb, 1, size, size)\n\n        if opt.vgg > 0:\n            self.vgg_loss = networks.PerceptualLoss()\n            self.vgg_loss.cuda()\n            self.vgg = networks.load_vgg16(""./model"")\n            self.vgg.eval()\n            for param in self.vgg.parameters():\n                param.requires_grad = False\n        # load/define networks\n        # The naming conversion is different from those used in the paper\n        # Code (paper): G_A (G), G_B (F), D_A (D_Y), D_B (D_X)\n\n        skip = True if opt.skip > 0 else False\n        self.netG_A = networks.define_G(opt.input_nc, opt.output_nc,\n                                        opt.ngf, opt.which_model_netG, opt.norm, not opt.no_dropout, self.gpu_ids, skip=skip, opt=opt)\n            \n        if not self.isTrain or opt.continue_train:\n            which_epoch = opt.which_epoch\n            self.load_network(self.netG_A, \'G_A\', which_epoch)\n\n        if self.isTrain:\n            self.old_lr = opt.lr\n            self.fake_A_pool = ImagePool(opt.pool_size)\n            self.fake_B_pool = ImagePool(opt.pool_size)\n            # define loss functions\n            if opt.use_wgan:\n                self.criterionGAN = networks.DiscLossWGANGP()\n            else:\n                self.criterionGAN = networks.GANLoss(use_lsgan=not opt.no_lsgan, tensor=self.Tensor)\n            if opt.use_mse:\n                self.criterionCycle = torch.nn.MSELoss()\n            else:\n                self.criterionCycle = torch.nn.L1Loss()\n            self.criterionL1 = torch.nn.L1Loss()\n            self.criterionIdt = torch.nn.L1Loss()\n            # initialize optimizers\n            self.optimizer_G = torch.optim.Adam(self.netG_A.parameters(),\n                                                lr=opt.lr, betas=(opt.beta1, 0.999))\n\n        print(\'---------- Networks initialized -------------\')\n        networks.print_network(self.netG_A)\n        if opt.isTrain:\n            self.netG_A.train()\n        else:\n            self.netG_A.eval()\n        print(\'-----------------------------------------------\')\n\n    def set_input(self, input):\n        AtoB = self.opt.which_direction == \'AtoB\'\n        input_A = input[\'A\' if AtoB else \'B\']\n        input_B = input[\'B\' if AtoB else \'A\']\n        input_img = input[\'input_img\']\n        input_A_gray = input[\'A_gray\']\n        self.input_A.resize_(input_A.size()).copy_(input_A)\n        self.input_A_gray.resize_(input_A_gray.size()).copy_(input_A_gray)\n        self.input_B.resize_(input_B.size()).copy_(input_B)\n        self.input_img.resize_(input_img.size()).copy_(input_img)\n        self.image_paths = input[\'A_paths\' if AtoB else \'B_paths\']\n\n    def forward(self):\n        self.real_A = Variable(self.input_A)\n        self.real_B = Variable(self.input_B)\n        self.real_A_gray = Variable(self.input_A_gray)\n        self.real_img = Variable(self.input_img)\n\n\n    def test(self):\n        self.real_A = Variable(self.input_A, volatile=True)\n        self.fake_B, self.latent_real_A = self.netG_A.forward(self.real_A, self.real_A_gray)\n\n        self.real_B = Variable(self.input_B, volatile=True)\n\n    def predict(self):\n        self.real_A = Variable(self.input_A, volatile=True)\n        self.fake_B, self.latent_real_A = self.netG_A.forward(self.real_A, self.real_A_gray)\n\n        real_A = util.tensor2im(self.real_A.data)\n        fake_B = util.tensor2im(self.fake_B.data)\n        if self.opt.skip == 1:\n            latent_real_A = util.tensor2im(self.latent_real_A.data)\n            return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (""latent_real_A"", latent_real_A)])\n        else:\n            return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B)])\n\n    # get image paths\n    def get_image_paths(self):\n        return self.image_paths\n\n    def backward_G(self):\n\n        self.fake_B, self.latent_real_A = self.netG_A.forward(self.real_A, self.real_A_gray)\n         # = self.latent_real_A + self.opt.skip * self.real_A\n        self.L1_AB = self.criterionL1(self.fake_B, self.real_B) * self.opt.l1\n        self.loss_G = self.L1_AB\n        self.loss_G.backward()\n\n\n    def optimize_parameters(self, epoch):\n        # forward\n        self.forward()\n        # G_A and G_B\n        self.optimizer_G.zero_grad()\n        self.backward_G()\n        self.optimizer_G.step()\n\n\n    def get_current_errors(self, epoch):\n        L1 = self.L1_AB.data[0]\n        loss_G = self.loss_G.data[0]\n        return OrderedDict([(\'L1\', L1), (\'loss_G\', loss_G)])\n\n    def get_current_visuals(self):\n        real_A = util.tensor2im(self.real_A.data)\n        fake_B = util.tensor2im(self.fake_B.data)\n        real_B = util.tensor2im(self.real_B.data)\n        return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'real_B\', real_B)])\n\n    def save(self, label):\n        self.save_network(self.netG_A, \'G_A\', label, self.gpu_ids)\n\n    def update_learning_rate(self):\n        \n        if self.opt.new_lr:\n            lr = self.old_lr/2\n        else:\n            lrd = self.opt.lr / self.opt.niter_decay\n            lr = self.old_lr - lrd\n        for param_group in self.optimizer_G.param_groups:\n            param_group[\'lr\'] = lr\n\n        print(\'update learning rate: %f -> %f\' % (self.old_lr, lr))\n        self.old_lr = lr\n'"
models/__init__.py,0,b''
models/base_model.py,4,"b""import os\nimport torch\n\n\nclass BaseModel():\n    def name(self):\n        return 'BaseModel'\n\n    def initialize(self, opt):\n        self.opt = opt\n        self.gpu_ids = opt.gpu_ids\n        self.isTrain = opt.isTrain\n        self.Tensor = torch.cuda.FloatTensor if self.gpu_ids else torch.Tensor\n        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)\n\n    def set_input(self, input):\n        self.input = input\n\n    def forward(self):\n        pass\n\n    # used in test time, no backprop\n    def test(self):\n        pass\n\n    def get_image_paths(self):\n        pass\n\n    def optimize_parameters(self):\n        pass\n\n    def get_current_visuals(self):\n        return self.input\n\n    def get_current_errors(self):\n        return {}\n\n    def save(self, label):\n        pass\n\n    # helper saving function that can be used by subclasses\n    def save_network(self, network, network_label, epoch_label, gpu_ids):\n        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n        save_path = os.path.join(self.save_dir, save_filename)\n        torch.save(network.cpu().state_dict(), save_path)\n        if len(gpu_ids) and torch.cuda.is_available():\n            network.cuda(device=gpu_ids[0])\n\n    # helper loading function that can be used by subclasses\n    def load_network(self, network, network_label, epoch_label):\n        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n        save_path = os.path.join(self.save_dir, save_filename)\n        network.load_state_dict(torch.load(save_path))\n\n    def update_learning_rate():\n        pass\n"""
models/cycle_gan_model.py,8,"b'import numpy as np\nimport torch\nimport os\nfrom collections import OrderedDict\nfrom torch.autograd import Variable\nimport util.util as util\nfrom collections import OrderedDict\nfrom torch.autograd import Variable\nimport itertools\nimport util.util as util\nfrom util.image_pool import ImagePool\nfrom .base_model import BaseModel\nfrom . import networks\nimport sys\n\n\nclass CycleGANModel(BaseModel):\n    def name(self):\n        return \'CycleGANModel\'\n\n    def initialize(self, opt):\n        BaseModel.initialize(self, opt)\n\n        nb = opt.batchSize\n        size = opt.fineSize\n        self.opt = opt\n        self.input_A = self.Tensor(nb, opt.input_nc, size, size)\n        self.input_B = self.Tensor(nb, opt.output_nc, size, size)\n\n        if opt.vgg > 0:\n            self.vgg_loss = networks.PerceptualLoss()\n            self.vgg_loss.cuda()\n            self.vgg = networks.load_vgg16(""./model"")\n            self.vgg.eval()\n            for param in self.vgg.parameters():\n                param.requires_grad = False\n        # load/define networks\n        # The naming conversion is different from those used in the paper\n        # Code (paper): G_A (G), G_B (F), D_A (D_Y), D_B (D_X)\n\n        skip = True if opt.skip > 0 else False\n        self.netG_A = networks.define_G(opt.input_nc, opt.output_nc,\n                                        opt.ngf, opt.which_model_netG, opt.norm, not opt.no_dropout, self.gpu_ids, skip=skip, opt=opt)\n        self.netG_B = networks.define_G(opt.output_nc, opt.input_nc,\n                                        opt.ngf, opt.which_model_netG, opt.norm, not opt.no_dropout, self.gpu_ids, skip=False, opt=opt)\n\n        if self.isTrain:\n            use_sigmoid = opt.no_lsgan\n            self.netD_A = networks.define_D(opt.output_nc, opt.ndf,\n                                            opt.which_model_netD,\n                                            opt.n_layers_D, opt.norm, use_sigmoid, self.gpu_ids)\n            self.netD_B = networks.define_D(opt.input_nc, opt.ndf,\n                                            opt.which_model_netD,\n                                            opt.n_layers_D, opt.norm, use_sigmoid, self.gpu_ids)\n        if not self.isTrain or opt.continue_train:\n            which_epoch = opt.which_epoch\n            self.load_network(self.netG_A, \'G_A\', which_epoch)\n            self.load_network(self.netG_B, \'G_B\', which_epoch)\n            if self.isTrain:\n                self.load_network(self.netD_A, \'D_A\', which_epoch)\n                self.load_network(self.netD_B, \'D_B\', which_epoch)\n\n        if self.isTrain:\n            self.old_lr = opt.lr\n            self.fake_A_pool = ImagePool(opt.pool_size)\n            self.fake_B_pool = ImagePool(opt.pool_size)\n            # define loss functions\n            if opt.use_wgan:\n                self.criterionGAN = networks.DiscLossWGANGP()\n            else:\n                self.criterionGAN = networks.GANLoss(use_lsgan=not opt.no_lsgan, tensor=self.Tensor)\n            self.criterionCycle = torch.nn.L1Loss()\n            self.criterionL1 = torch.nn.L1Loss()\n            self.criterionIdt = torch.nn.L1Loss()\n            # initialize optimizers\n            self.optimizer_G = torch.optim.Adam(itertools.chain(self.netG_A.parameters(), self.netG_B.parameters()),\n                                                lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizer_D_A = torch.optim.Adam(self.netD_A.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizer_D_B = torch.optim.Adam(self.netD_B.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n\n        print(\'---------- Networks initialized -------------\')\n        networks.print_network(self.netG_A)\n        networks.print_network(self.netG_B)\n        if self.isTrain:\n            networks.print_network(self.netD_A)\n            networks.print_network(self.netD_B)\n        if opt.isTrain:\n            self.netG_A.train()\n            self.netG_B.train()\n        else:\n            self.netG_A.eval()\n            self.netG_B.eval()\n        print(\'-----------------------------------------------\')\n\n    def set_input(self, input):\n        AtoB = self.opt.which_direction == \'AtoB\'\n        input_A = input[\'A\' if AtoB else \'B\']\n        input_B = input[\'B\' if AtoB else \'A\']\n        self.input_A.resize_(input_A.size()).copy_(input_A)\n        self.input_B.resize_(input_B.size()).copy_(input_B)\n        self.image_paths = input[\'A_paths\' if AtoB else \'B_paths\']\n\n    def forward(self):\n        self.real_A = Variable(self.input_A)\n        self.real_B = Variable(self.input_B)\n\n\n    def test(self):\n        self.real_A = Variable(self.input_A, volatile=True)\n        # print(np.transpose(self.real_A.data[0].cpu().float().numpy(),(1,2,0))[:2][:2][:])\n        if self.opt.skip == 1:\n            self.fake_B, self.latent_real_A = self.netG_A.forward(self.real_A)\n        else:\n            self.fake_B = self.netG_A.forward(self.real_A)\n        self.rec_A = self.netG_B.forward(self.fake_B)\n\n        self.real_B = Variable(self.input_B, volatile=True)\n        self.fake_A = self.netG_B.forward(self.real_B)\n        if self.opt.skip == 1:\n            self.rec_B, self.latent_fake_A = self.netG_A.forward(self.fake_A)\n        else:\n            self.rec_B = self.netG_A.forward(self.fake_A)\n\n    def predict(self):\n        self.real_A = Variable(self.input_A, volatile=True)\n        # print(np.transpose(self.real_A.data[0].cpu().float().numpy(),(1,2,0))[:2][:2][:])\n        if self.opt.skip == 1:\n            self.fake_B, self.latent_real_A = self.netG_A.forward(self.real_A)\n        else:\n            self.fake_B = self.netG_A.forward(self.real_A)\n        self.rec_A = self.netG_B.forward(self.fake_B)\n\n        real_A = util.tensor2im(self.real_A.data)\n        fake_B = util.tensor2im(self.fake_B.data)\n        rec_A = util.tensor2im(self.rec_A.data)\n        if self.opt.skip == 1:\n            latent_real_A = util.tensor2im(self.latent_real_A.data)\n            return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (""latent_real_A"", latent_real_A), (""rec_A"", rec_A)])\n        else:\n            return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (""rec_A"", rec_A)])\n\n    # get image paths\n    def get_image_paths(self):\n        return self.image_paths\n\n    def backward_D_basic(self, netD, real, fake):\n        # Real\n        pred_real = netD.forward(real)\n        if self.opt.use_wgan:\n            loss_D_real = pred_real.mean()\n        else:\n            loss_D_real = self.criterionGAN(pred_real, True)\n        # Fake\n        pred_fake = netD.forward(fake.detach())\n        if self.opt.use_wgan:\n            loss_D_fake = pred_fake.mean()\n        else:\n            loss_D_fake = self.criterionGAN(pred_fake, False)\n        # Combined loss\n        if self.opt.use_wgan:\n            loss_D = loss_D_fake - loss_D_real + self.criterionGAN.calc_gradient_penalty(netD, real.data, fake.data)\n        else:\n            loss_D = (loss_D_real + loss_D_fake) * 0.5\n        # backward\n        loss_D.backward()\n        return loss_D\n\n    def backward_D_A(self):\n        fake_B = self.fake_B_pool.query(self.fake_B)\n        self.loss_D_A = self.backward_D_basic(self.netD_A, self.real_B, fake_B)\n\n    def backward_D_B(self):\n        fake_A = self.fake_A_pool.query(self.fake_A)\n        self.loss_D_B = self.backward_D_basic(self.netD_B, self.real_A, fake_A)\n\n    def backward_G(self, epoch):\n        lambda_idt = self.opt.identity\n        lambda_A = self.opt.lambda_A\n        lambda_B = self.opt.lambda_B\n        # Identity loss\n        if lambda_idt > 0:\n            # G_A should be identity if real_B is fed.\n            self.idt_A = self.netG_A.forward(self.real_B)\n            self.loss_idt_A = self.criterionIdt(self.idt_A, self.real_B) * lambda_B * lambda_idt\n            # G_B should be identity if real_A is fed.\n            self.idt_B = self.netG_B.forward(self.real_A)\n            self.loss_idt_B = self.criterionIdt(self.idt_B, self.real_A) * lambda_A * lambda_idt\n        else:\n            self.loss_idt_A = 0\n            self.loss_idt_B = 0\n\n        # GAN loss\n        # D_A(G_A(A))\n        if self.opt.skip == 1:\n            self.fake_B, self.latent_real_A = self.netG_A.forward(self.real_A)\n        else:\n            self.fake_B = self.netG_A.forward(self.real_A)\n         # = self.latent_real_A + self.opt.skip * self.real_A\n        pred_fake = self.netD_A.forward(self.fake_B)\n        if self.opt.use_wgan:\n            self.loss_G_A = -pred_fake.mean()\n        else:\n            self.loss_G_A = self.criterionGAN(pred_fake, True)\n        if self.opt.l1 > 0:\n            self.L1_AB = self.criterionL1(self.fake_B, self.real_B) * self.opt.l1\n        else:\n            self.L1_AB = 0\n        # D_B(G_B(B))\n        self.fake_A = self.netG_B.forward(self.real_B)\n        pred_fake = self.netD_B.forward(self.fake_A)\n        if self.opt.l1 > 0:\n            self.L1_BA = self.criterionL1(self.fake_A, self.real_A) * self.opt.l1\n        else:\n            self.L1_BA = 0\n        if self.opt.use_wgan:\n            self.loss_G_B = -pred_fake.mean()\n        else:\n            self.loss_G_B = self.criterionGAN(pred_fake, True)\n        # Forward cycle loss\n        \n        if lambda_A > 0:\n            self.rec_A = self.netG_B.forward(self.fake_B)\n            self.loss_cycle_A = self.criterionCycle(self.rec_A, self.real_A) * lambda_A\n        else:\n            self.loss_cycle_A = 0\n        # Backward cycle loss\n        \n         # = self.latent_fake_A + self.opt.skip * self.fake_A\n        if lambda_B > 0:\n            if self.opt.skip == 1:\n                self.rec_B, self.latent_fake_A = self.netG_A.forward(self.fake_A)\n            else:\n                self.rec_B = self.netG_A.forward(self.fake_A)\n            self.loss_cycle_B = self.criterionCycle(self.rec_B, self.real_B) * lambda_B\n        else:\n            self.loss_cycle_B = 0\n        self.loss_vgg_a = self.vgg_loss.compute_vgg_loss(self.vgg, self.fake_A, self.real_B) * self.opt.vgg if self.opt.vgg > 0 else 0\n        self.loss_vgg_b = self.vgg_loss.compute_vgg_loss(self.vgg, self.fake_B, self.real_A) * self.opt.vgg if self.opt.vgg > 0 else 0\n        if epoch <= 10:\n            self.loss_vgg_a = 0\n            self.loss_vgg_b = 0\n        # combined loss\n        self.loss_G = self.loss_G_A + self.loss_G_B + self.loss_cycle_A + self.loss_cycle_B + self.loss_vgg_a + self.loss_vgg_b\n        # self.loss_G = self.L1_AB + self.L1_BA\n        self.loss_G.backward()\n\n    def optimize_parameters(self, epoch):\n        # forward\n        self.forward()\n        # G_A and G_B\n        self.optimizer_G.zero_grad()\n        self.backward_G(epoch)\n        self.optimizer_G.step()\n        # D_A\n        self.optimizer_D_A.zero_grad()\n        self.backward_D_A()\n        self.optimizer_D_A.step()\n        # D_B\n        self.optimizer_D_B.zero_grad()\n        self.backward_D_B()\n        self.optimizer_D_B.step()\n\n    def get_current_errors(self, epoch):\n        D_A = self.loss_D_A.data[0]\n        G_A = self.loss_G_A.data[0]\n        Cyc_A = self.loss_cycle_A.data[0]\n        D_B = self.loss_D_B.data[0]\n        G_B = self.loss_G_B.data[0]\n        Cyc_B = self.loss_cycle_B.data[0]\n        if epoch <= 10:\n            vgg = 0\n        else:\n            vgg = (self.loss_vgg_a.data[0] + self.loss_vgg_b.data[0]) / self.opt.vgg if self.opt.vgg > 0 else 0\n        if self.opt.lambda_A > 0.0:\n            return OrderedDict([(\'D_A\', D_A), (\'G_A\', G_A), (\'Cyc_A\', Cyc_A),\n                                (\'D_B\', D_B), (\'G_B\', G_B), (\'Cyc_B\', Cyc_B), (""vgg"", vgg)])\n        else:\n            return OrderedDict([(\'D_A\', D_A), (\'G_A\', G_A), \n                                (\'D_B\', D_B), (\'G_B\', G_B)], (""vgg"", vgg))\n\n    def get_current_visuals(self):\n        real_A = util.tensor2im(self.real_A.data)\n        fake_B = util.tensor2im(self.fake_B.data)\n        if self.opt.skip > 0:\n            latent_real_A = util.tensor2im(self.latent_real_A.data)\n        \n        real_B = util.tensor2im(self.real_B.data)\n        fake_A = util.tensor2im(self.fake_A.data)\n        \n        if self.opt.lambda_A > 0.0:\n            rec_A = util.tensor2im(self.rec_A.data)\n            rec_B = util.tensor2im(self.rec_B.data)\n            if self.opt.skip > 0:\n                latent_fake_A = util.tensor2im(self.latent_fake_A.data)\n                return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'latent_real_A\', latent_real_A), (\'rec_A\', rec_A), \n                                    (\'real_B\', real_B), (\'fake_A\', fake_A), (\'rec_B\', rec_B), (\'latent_fake_A\', latent_fake_A)])\n            else:\n                return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'rec_A\', rec_A), \n                                (\'real_B\', real_B), (\'fake_A\', fake_A), (\'rec_B\', rec_B)])\n        else:\n            if self.opt.skip > 0:\n                return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'latent_real_A\', latent_real_A), \n                                    (\'real_B\', real_B), (\'fake_A\', fake_A)])\n            else:\n                return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B),\n                                    (\'real_B\', real_B), (\'fake_A\', fake_A)])\n\n    def save(self, label):\n        self.save_network(self.netG_A, \'G_A\', label, self.gpu_ids)\n        self.save_network(self.netD_A, \'D_A\', label, self.gpu_ids)\n        self.save_network(self.netG_B, \'G_B\', label, self.gpu_ids)\n        self.save_network(self.netD_B, \'D_B\', label, self.gpu_ids)\n\n    def update_learning_rate(self):\n        lrd = self.opt.lr / self.opt.niter_decay\n        lr = self.old_lr - lrd\n        for param_group in self.optimizer_D_A.param_groups:\n            param_group[\'lr\'] = lr\n        for param_group in self.optimizer_D_B.param_groups:\n            param_group[\'lr\'] = lr\n        for param_group in self.optimizer_G.param_groups:\n            param_group[\'lr\'] = lr\n\n        print(\'update learning rate: %f -> %f\' % (self.old_lr, lr))\n        self.old_lr = lr\n'"
models/models.py,0,"b'\ndef create_model(opt):\n    model = None\n    print(opt.model)\n    if opt.model == \'cycle_gan\':\n        assert(opt.dataset_mode == \'unaligned\')\n        from .cycle_gan_model import CycleGANModel\n        model = CycleGANModel()\n    elif opt.model == \'pix2pix\':\n        assert(opt.dataset_mode == \'pix2pix\')\n        from .pix2pix_model import Pix2PixModel\n        model = Pix2PixModel()\n    elif opt.model == \'pair\':\n        # assert(opt.dataset_mode == \'pair\')\n        # from .pair_model import PairModel\n        from .Unet_L1 import PairModel\n        model = PairModel()\n    elif opt.model == \'single\':\n        # assert(opt.dataset_mode == \'unaligned\')\n        from .single_model import SingleModel\n        model = SingleModel()\n    elif opt.model == \'temp\':\n        # assert(opt.dataset_mode == \'unaligned\')\n        from .temp_model import TempModel\n        model = TempModel()\n    elif opt.model == \'UNIT\':\n        assert(opt.dataset_mode == \'unaligned\')\n        from .unit_model import UNITModel\n        model = UNITModel()\n    elif opt.model == \'test\':\n        assert(opt.dataset_mode == \'single\')\n        from .test_model import TestModel\n        model = TestModel()\n    else:\n        raise ValueError(""Model [%s] not recognized."" % opt.model)\n    model.initialize(opt)\n    print(""model [%s] was created"" % (model.name()))\n    return model\n'"
models/multi_model.py,9,"b'import numpy as np\nimport torch\nimport os\nfrom collections import OrderedDict\nfrom torch.autograd import Variable\nimport util.util as util\nfrom collections import OrderedDict\nfrom torch.autograd import Variable\nimport itertools\nimport util.util as util\nfrom util.image_pool import ImagePool\nfrom .base_model import BaseModel\nfrom . import networks\nimport sys\n\n\nclass MultiModel(BaseModel):\n    def name(self):\n        return \'MultiGANModel\'\n\n    def initialize(self, opt):\n        BaseModel.initialize(self, opt)\n\n        nb = opt.batchSize\n        size = opt.fineSize\n        self.opt = opt\n        self.input_A = self.Tensor(nb, opt.input_nc, size, size)\n        self.input_B = self.Tensor(nb, opt.output_nc, size, size)\n\n        if opt.vgg > 0:\n            self.vgg_loss = networks.PerceptualLoss()\n            self.vgg_loss.cuda()\n            self.vgg = networks.load_vgg16(""./model"")\n            self.vgg.eval()\n            for param in self.vgg.parameters():\n                param.requires_grad = False\n        # load/define networks\n        # The naming conversion is different from those used in the paper\n        # Code (paper): G_A (G), G_B (F), D_A (D_Y), D_B (D_X)\n\n        skip = True if opt.skip > 0 else False\n        self.netG_A = networks.define_G(opt.input_nc, opt.output_nc,\n                                        opt.ngf, opt.which_model_netG, opt.norm, not opt.no_dropout, self.gpu_ids, skip=skip, opt=opt)\n        self.netG_B = networks.define_G(opt.output_nc, opt.input_nc,\n                                        opt.ngf, opt.which_model_netG, opt.norm, not opt.no_dropout, self.gpu_ids, skip=False, opt=opt)\n\n        if self.isTrain:\n            use_sigmoid = opt.no_lsgan\n            self.netD_A = networks.define_D(opt.output_nc, opt.ndf,\n                                            opt.which_model_netD,\n                                            opt.n_layers_D, opt.norm, use_sigmoid, self.gpu_ids)\n            self.netD_B = networks.define_D(opt.input_nc, opt.ndf,\n                                            opt.which_model_netD,\n                                            opt.n_layers_D, opt.norm, use_sigmoid, self.gpu_ids)\n        if not self.isTrain or opt.continue_train:\n            which_epoch = opt.which_epoch\n            self.load_network(self.netG_A, \'G_A\', which_epoch)\n            self.load_network(self.netG_B, \'G_B\', which_epoch)\n            if self.isTrain:\n                self.load_network(self.netD_A, \'D_A\', which_epoch)\n                self.load_network(self.netD_B, \'D_B\', which_epoch)\n\n        if self.isTrain:\n            self.old_lr = opt.lr\n            self.fake_A_pool = ImagePool(opt.pool_size)\n            self.fake_B_pool = ImagePool(opt.pool_size)\n            # define loss functions\n            if opt.use_wgan:\n                self.criterionGAN = networks.DiscLossWGANGP()\n            else:\n                self.criterionGAN = networks.GANLoss(use_lsgan=not opt.no_lsgan, tensor=self.Tensor)\n            if opt.use_mse:\n                self.criterionCycle = torch.nn.MSELoss()\n            else:\n                self.criterionCycle = torch.nn.L1Loss()\n            self.criterionL1 = torch.nn.L1Loss()\n            self.criterionIdt = torch.nn.L1Loss()\n            # initialize optimizers\n            self.optimizer_G = torch.optim.Adam(itertools.chain(self.netG_A.parameters(), self.netG_B.parameters()),\n                                                lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizer_D_A = torch.optim.Adam(self.netD_A.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizer_D_B = torch.optim.Adam(self.netD_B.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n\n        print(\'---------- Networks initialized -------------\')\n        networks.print_network(self.netG_A)\n        networks.print_network(self.netG_B)\n        if self.isTrain:\n            networks.print_network(self.netD_A)\n            networks.print_network(self.netD_B)\n        if opt.isTrain:\n            self.netG_A.train()\n            self.netG_B.train()\n        else:\n            self.netG_A.eval()\n            self.netG_B.eval()\n        print(\'-----------------------------------------------\')\n\n    def set_input(self, input):\n        AtoB = self.opt.which_direction == \'AtoB\'\n        input_A = input[\'A\' if AtoB else \'B\']\n        input_B = input[\'B\' if AtoB else \'A\']\n        self.input_A.resize_(input_A.size()).copy_(input_A)\n        self.input_B.resize_(input_B.size()).copy_(input_B)\n        self.image_paths = input[\'A_paths\' if AtoB else \'B_paths\']\n\n    def forward(self):\n        self.real_A = Variable(self.input_A)\n        self.real_B = Variable(self.input_B)\n\n\n    def test(self):\n        self.real_A = Variable(self.input_A, volatile=True)\n        # print(np.transpose(self.real_A.data[0].cpu().float().numpy(),(1,2,0))[:2][:2][:])\n        if self.opt.skip == 1:\n            self.fake_B, self.latent_real_A = self.netG_A.forward(self.real_A)\n        else:\n            self.fake_B = self.netG_A.forward(self.real_A)\n        self.rec_A = self.netG_B.forward(self.fake_B)\n\n        self.real_B = Variable(self.input_B, volatile=True)\n        self.fake_A = self.netG_B.forward(self.real_B)\n        if self.opt.skip == 1:\n            self.rec_B, self.latent_fake_A = self.netG_A.forward(self.fake_A)\n        else:\n            self.rec_B = self.netG_A.forward(self.fake_A)\n\n    def predict(self):\n        self.real_A = Variable(self.input_A, volatile=True)\n        # print(np.transpose(self.real_A.data[0].cpu().float().numpy(),(1,2,0))[:2][:2][:])\n        if self.opt.skip == 1:\n            self.fake_B, self.latent_real_A = self.netG_A.forward(self.real_A)\n        else:\n            self.fake_B = self.netG_A.forward(self.real_A)\n        self.rec_A = self.netG_B.forward(self.fake_B)\n\n        real_A = util.tensor2im(self.real_A.data)\n        fake_B = util.tensor2im(self.fake_B.data)\n        rec_A = util.tensor2im(self.rec_A.data)\n        if self.opt.skip == 1:\n            latent_real_A = util.tensor2im(self.latent_real_A.data)\n            return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (""latent_real_A"", latent_real_A), (""rec_A"", rec_A)])\n        else:\n            return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (""rec_A"", rec_A)])\n\n    # get image paths\n    def get_image_paths(self):\n        return self.image_paths\n\n    def backward_D_basic(self, netD, real, fake):\n        # Real\n        pred_real = netD.forward(real)\n        if self.opt.use_wgan:\n            loss_D_real = pred_real.mean()\n        else:\n            loss_D_real = self.criterionGAN(pred_real, True)\n        # Fake\n        pred_fake = netD.forward(fake.detach())\n        if self.opt.use_wgan:\n            loss_D_fake = pred_fake.mean()\n        else:\n            loss_D_fake = self.criterionGAN(pred_fake, False)\n        # Combined loss\n        if self.opt.use_wgan:\n            loss_D = loss_D_fake - loss_D_real + self.criterionGAN.calc_gradient_penalty(netD, real.data, fake.data)\n        else:\n            loss_D = (loss_D_real + loss_D_fake) * 0.5\n        # backward\n        loss_D.backward()\n        return loss_D\n\n    def backward_D_A(self):\n        fake_B = self.fake_B_pool.query(self.fake_B)\n        self.loss_D_A = self.backward_D_basic(self.netD_A, self.real_B, fake_B)\n\n    def backward_D_B(self):\n        fake_A = self.fake_A_pool.query(self.fake_A)\n        self.loss_D_B = self.backward_D_basic(self.netD_B, self.real_A, fake_A)\n\n    def backward_G(self):\n        lambda_idt = self.opt.identity\n        lambda_A = self.opt.lambda_A\n        lambda_B = self.opt.lambda_B\n        # Identity loss\n        if lambda_idt > 0:\n            # G_A should be identity if real_B is fed.\n            if self.opt.skip == 1:\n                self.idt_A, _ = self.netG_A.forward(self.real_B)\n            else:\n                self.idt_A = self.netG_A.forward(self.real_B)\n            self.loss_idt_A = self.criterionIdt(self.idt_A, self.real_B) * lambda_B * lambda_idt\n            # G_B should be identity if real_A is fed.\n            self.idt_B = self.netG_B.forward(self.real_A)\n            self.loss_idt_B = self.criterionIdt(self.idt_B, self.real_A) * lambda_A * lambda_idt\n        else:\n            self.loss_idt_A = 0\n            self.loss_idt_B = 0\n\n        # GAN loss\n        # D_A(G_A(A))\n        if self.opt.skip == 1:\n            self.fake_B, self.latent_real_A = self.netG_A.forward(self.real_A)\n        else:\n            self.fake_B = self.netG_A.forward(self.real_A)\n         # = self.latent_real_A + self.opt.skip * self.real_A\n        pred_fake = self.netD_A.forward(self.fake_B)\n        if self.opt.use_wgan:\n            self.loss_G_A = -pred_fake.mean()\n        else:\n            self.loss_G_A = self.criterionGAN(pred_fake, True)\n        self.L1_AB = self.criterionL1(self.fake_B, self.real_B) * self.opt.l1\n        # D_B(G_B(B))\n        self.fake_A = self.netG_B.forward(self.real_B)\n        pred_fake = self.netD_B.forward(self.fake_A)\n        self.L1_BA = self.criterionL1(self.fake_A, self.real_A) * self.opt.l1\n        if self.opt.use_wgan:\n            self.loss_G_B = -pred_fake.mean()\n        else:\n            self.loss_G_B = self.criterionGAN(pred_fake, True)\n        # Forward cycle loss\n        \n        if lambda_A > 0:\n            self.rec_A = self.netG_B.forward(self.fake_B)\n            self.loss_cycle_A = self.criterionCycle(self.rec_A, self.real_A) * lambda_A\n        else:\n            self.loss_cycle_A = 0\n        # Backward cycle loss\n        \n         # = self.latent_fake_A + self.opt.skip * self.fake_A\n        if lambda_B > 0:\n            if self.opt.skip == 1:\n                self.rec_B, self.latent_fake_A = self.netG_A.forward(self.fake_A)\n            else:\n                self.rec_B = self.netG_A.forward(self.fake_A)\n            self.loss_cycle_B = self.criterionCycle(self.rec_B, self.real_B) * lambda_B\n        else:\n            self.loss_cycle_B = 0\n        self.loss_vgg_a = self.vgg_loss.compute_vgg_loss(self.vgg, self.fake_A, self.real_B) * self.opt.vgg if self.opt.vgg > 0 else 0\n        self.loss_vgg_b = self.vgg_loss.compute_vgg_loss(self.vgg, self.fake_B, self.real_A) * self.opt.vgg if self.opt.vgg > 0 else 0\n        # combined loss\n        self.loss_G = self.loss_G_A + self.loss_G_B + self.L1_AB + self.L1_BA + self.loss_cycle_A + self.loss_cycle_B + \\\n                        self.loss_vgg_a + self.loss_vgg_b + \\\n                        self.loss_idt_A + self.loss_idt_B\n        # self.loss_G = self.L1_AB + self.L1_BA\n        self.loss_G.backward()\n\n\n    def optimize_parameters(self):\n        # forward\n        self.forward()\n        # G_A and G_B\n        self.optimizer_G.zero_grad()\n        self.backward_G()\n        self.optimizer_G.step()\n        # D_A\n        self.optimizer_D_A.zero_grad()\n        self.backward_D_A()\n        self.optimizer_D_A.step()\n        # D_B\n        self.optimizer_D_B.zero_grad()\n        self.backward_D_B()\n        self.optimizer_D_B.step()\n\n\n    def get_current_errors(self):\n        D_A = self.loss_D_A.data[0]\n        G_A = self.loss_G_A.data[0]\n        L1 = (self.L1_AB + self.L1_BA).data[0]\n        Cyc_A = self.loss_cycle_A.data[0]\n        D_B = self.loss_D_B.data[0]\n        G_B = self.loss_G_B.data[0]\n        Cyc_B = self.loss_cycle_B.data[0]\n        vgg = (self.loss_vgg_a.data[0] + self.loss_vgg_b.data[0])/self.opt.vgg if self.opt.vgg > 0 else 0\n        if self.opt.identity > 0:\n            idt = self.loss_idt_A.data[0] + self.loss_idt_B.data[0]\n            if self.opt.lambda_A > 0.0:\n                return OrderedDict([(\'D_A\', D_A), (\'G_A\', G_A), (\'L1\', L1), (\'Cyc_A\', Cyc_A),\n                                    (\'D_B\', D_B), (\'G_B\', G_B), (\'Cyc_B\', Cyc_B), (""vgg"", vgg), (""idt"", idt)])\n            else:\n                return OrderedDict([(\'D_A\', D_A), (\'G_A\', G_A), (\'L1\', L1),\n                                    (\'D_B\', D_B), (\'G_B\', G_B)], (""vgg"", vgg), (""idt"", idt))\n        else:\n            if self.opt.lambda_A > 0.0:\n                return OrderedDict([(\'D_A\', D_A), (\'G_A\', G_A), (\'L1\', L1), (\'Cyc_A\', Cyc_A),\n                                    (\'D_B\', D_B), (\'G_B\', G_B), (\'Cyc_B\', Cyc_B), (""vgg"", vgg)])\n            else:\n                return OrderedDict([(\'D_A\', D_A), (\'G_A\', G_A), (\'L1\', L1),\n                                    (\'D_B\', D_B), (\'G_B\', G_B)], (""vgg"", vgg))\n\n    def get_current_visuals(self):\n        real_A = util.tensor2im(self.real_A.data)\n        fake_B = util.tensor2im(self.fake_B.data)\n        if self.opt.skip > 0:\n            latent_real_A = util.tensor2im(self.latent_real_A.data)\n        \n        real_B = util.tensor2im(self.real_B.data)\n        fake_A = util.tensor2im(self.fake_A.data)\n\n        if self.opt.identity > 0:\n            idt_A = util.tensor2im(self.idt_A.data)\n            idt_B = util.tensor2im(self.idt_B.data)\n            if self.opt.lambda_A > 0.0:\n                rec_A = util.tensor2im(self.rec_A.data)\n                rec_B = util.tensor2im(self.rec_B.data)\n                if self.opt.skip > 0:\n                    latent_fake_A = util.tensor2im(self.latent_fake_A.data)\n                    return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'latent_real_A\', latent_real_A), (\'rec_A\', rec_A), \n                                        (\'real_B\', real_B), (\'fake_A\', fake_A), (\'rec_B\', rec_B), (\'latent_fake_A\', latent_fake_A), \n                                        (""idt_A"", idt_A), (""idt_B"", idt_B)])\n                else:\n                    return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'rec_A\', rec_A), \n                                    (\'real_B\', real_B), (\'fake_A\', fake_A), (\'rec_B\', rec_B), (""idt_A"", idt_A), (""idt_B"", idt_B)])\n            else:\n                if self.opt.skip > 0:\n                    return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'latent_real_A\', latent_real_A), \n                                        (\'real_B\', real_B), (\'fake_A\', fake_A), (""idt_A"", idt_A), (""idt_B"", idt_B)])\n                else:\n                    return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B),\n                                        (\'real_B\', real_B), (\'fake_A\', fake_A), (""idt_A"", idt_A), (""idt_B"", idt_B)])\n        else:\n            if self.opt.lambda_A > 0.0:\n                rec_A = util.tensor2im(self.rec_A.data)\n                rec_B = util.tensor2im(self.rec_B.data)\n                if self.opt.skip > 0:\n                    latent_fake_A = util.tensor2im(self.latent_fake_A.data)\n                    return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'latent_real_A\', latent_real_A), (\'rec_A\', rec_A), \n                                        (\'real_B\', real_B), (\'fake_A\', fake_A), (\'rec_B\', rec_B), (\'latent_fake_A\', latent_fake_A)])\n                else:\n                    return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'rec_A\', rec_A), \n                                    (\'real_B\', real_B), (\'fake_A\', fake_A), (\'rec_B\', rec_B)])\n            else:\n                if self.opt.skip > 0:\n                    return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'latent_real_A\', latent_real_A), \n                                        (\'real_B\', real_B), (\'fake_A\', fake_A)])\n                else:\n                    return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B),\n                                        (\'real_B\', real_B), (\'fake_A\', fake_A)])\n\n    def save(self, label):\n        self.save_network(self.netG_A, \'G_A\', label, self.gpu_ids)\n        self.save_network(self.netD_A, \'D_A\', label, self.gpu_ids)\n        self.save_network(self.netG_B, \'G_B\', label, self.gpu_ids)\n        self.save_network(self.netD_B, \'D_B\', label, self.gpu_ids)\n\n    def update_learning_rate(self):\n        \n        if self.opt.new_lr:\n            lr = self.old_lr/2\n        else:\n            lrd = self.opt.lr / self.opt.niter_decay\n            lr = self.old_lr - lrd\n        for param_group in self.optimizer_D_A.param_groups:\n            param_group[\'lr\'] = lr\n        for param_group in self.optimizer_D_B.param_groups:\n            param_group[\'lr\'] = lr\n        for param_group in self.optimizer_G.param_groups:\n            param_group[\'lr\'] = lr\n\n        print(\'update learning rate: %f -> %f\' % (self.old_lr, lr))\n        self.old_lr = lr\n'"
models/networks.py,46,"b'import torch\nimport os\nimport math\nimport torch.nn as nn\nfrom torch.nn import init\nimport functools\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport numpy as np\n# from torch.utils.serialization import load_lua\nfrom lib.nn import SynchronizedBatchNorm2d as SynBN2d\n###############################################################################\n# Functions\n###############################################################################\n\ndef pad_tensor(input):\n    \n    height_org, width_org = input.shape[2], input.shape[3]\n    divide = 16\n\n    if width_org % divide != 0 or height_org % divide != 0:\n\n        width_res = width_org % divide\n        height_res = height_org % divide\n        if width_res != 0:\n            width_div = divide - width_res\n            pad_left = int(width_div / 2)\n            pad_right = int(width_div - pad_left)\n        else:\n            pad_left = 0\n            pad_right = 0\n\n        if height_res != 0:\n            height_div = divide - height_res\n            pad_top = int(height_div  / 2)\n            pad_bottom = int(height_div  - pad_top)\n        else:\n            pad_top = 0\n            pad_bottom = 0\n\n        padding = nn.ReflectionPad2d((pad_left, pad_right, pad_top, pad_bottom))\n        input = padding(input)\n    else:\n        pad_left = 0\n        pad_right = 0\n        pad_top = 0\n        pad_bottom = 0\n\n    height, width = input.data.shape[2], input.data.shape[3]\n    assert width % divide == 0, \'width cant divided by stride\'\n    assert height % divide == 0, \'height cant divided by stride\'\n\n    return input, pad_left, pad_right, pad_top, pad_bottom\n\ndef pad_tensor_back(input, pad_left, pad_right, pad_top, pad_bottom):\n    height, width = input.shape[2], input.shape[3]\n    return input[:,:, pad_top: height - pad_bottom, pad_left: width - pad_right]\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find(\'Conv\') != -1:\n        m.weight.data.normal_(0.0, 0.02)\n    elif classname.find(\'BatchNorm2d\') != -1:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)\n\n\ndef get_norm_layer(norm_type=\'instance\'):\n    if norm_type == \'batch\':\n        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n    elif norm_type == \'instance\':\n        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False)\n    elif norm_type == \'synBN\':\n        norm_layer = functools.partial(SynBN2d, affine=True)\n    else:\n        raise NotImplementedError(\'normalization layer [%s] is not found\' % norm)\n    return norm_layer\n\n\ndef define_G(input_nc, output_nc, ngf, which_model_netG, norm=\'batch\', use_dropout=False, gpu_ids=[], skip=False, opt=None):\n    netG = None\n    use_gpu = len(gpu_ids) > 0\n    norm_layer = get_norm_layer(norm_type=norm)\n\n    if use_gpu:\n        assert(torch.cuda.is_available())\n\n    if which_model_netG == \'resnet_9blocks\':\n        netG = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=9, gpu_ids=gpu_ids)\n    elif which_model_netG == \'resnet_6blocks\':\n        netG = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=6, gpu_ids=gpu_ids)\n    elif which_model_netG == \'unet_128\':\n        netG = UnetGenerator(input_nc, output_nc, 7, ngf, norm_layer=norm_layer, use_dropout=use_dropout, gpu_ids=gpu_ids)\n    elif which_model_netG == \'unet_256\':\n        netG = UnetGenerator(input_nc, output_nc, 8, ngf, norm_layer=norm_layer, use_dropout=use_dropout, gpu_ids=gpu_ids, skip=skip, opt=opt)\n    elif which_model_netG == \'unet_512\':\n        netG = UnetGenerator(input_nc, output_nc, 9, ngf, norm_layer=norm_layer, use_dropout=use_dropout, gpu_ids=gpu_ids, skip=skip, opt=opt)\n    elif which_model_netG == \'sid_unet\':\n        netG = Unet(opt, skip)\n    elif which_model_netG == \'sid_unet_shuffle\':\n        netG = Unet_pixelshuffle(opt, skip)\n    elif which_model_netG == \'sid_unet_resize\':\n        netG = Unet_resize_conv(opt, skip)\n    elif which_model_netG == \'DnCNN\':\n        netG = DnCNN(opt, depth=17, n_channels=64, image_channels=1, use_bnorm=True, kernel_size=3)\n    else:\n        raise NotImplementedError(\'Generator model name [%s] is not recognized\' % which_model_netG)\n    if len(gpu_ids) >= 0:\n        netG.cuda(device=gpu_ids[0])\n        netG = torch.nn.DataParallel(netG, gpu_ids)\n    netG.apply(weights_init)\n    return netG\n\n\ndef define_D(input_nc, ndf, which_model_netD,\n             n_layers_D=3, norm=\'batch\', use_sigmoid=False, gpu_ids=[], patch=False):\n    netD = None\n    use_gpu = len(gpu_ids) > 0\n    norm_layer = get_norm_layer(norm_type=norm)\n\n    if use_gpu:\n        assert(torch.cuda.is_available())\n    if which_model_netD == \'basic\':\n        netD = NLayerDiscriminator(input_nc, ndf, n_layers=3, norm_layer=norm_layer, use_sigmoid=use_sigmoid, gpu_ids=gpu_ids)\n    elif which_model_netD == \'n_layers\':\n        netD = NLayerDiscriminator(input_nc, ndf, n_layers_D, norm_layer=norm_layer, use_sigmoid=use_sigmoid, gpu_ids=gpu_ids)\n    elif which_model_netD == \'no_norm\':\n        netD = NoNormDiscriminator(input_nc, ndf, n_layers_D, use_sigmoid=use_sigmoid, gpu_ids=gpu_ids)\n    elif which_model_netD == \'no_norm_4\':\n        netD = NoNormDiscriminator(input_nc, ndf, n_layers_D, use_sigmoid=use_sigmoid, gpu_ids=gpu_ids)\n    elif which_model_netD == \'no_patchgan\':\n        netD = FCDiscriminator(input_nc, ndf, n_layers_D, use_sigmoid=use_sigmoid, gpu_ids=gpu_ids, patch=patch)\n    else:\n        raise NotImplementedError(\'Discriminator model name [%s] is not recognized\' %\n                                  which_model_netD)\n    if use_gpu:\n        netD.cuda(device=gpu_ids[0])\n        netD = torch.nn.DataParallel(netD, gpu_ids)\n    netD.apply(weights_init)\n    return netD\n\n\ndef print_network(net):\n    num_params = 0\n    for param in net.parameters():\n        num_params += param.numel()\n    print(net)\n    print(\'Total number of parameters: %d\' % num_params)\n\n\n##############################################################################\n# Classes\n##############################################################################\n\n\n# Defines the GAN loss which uses either LSGAN or the regular GAN.\n# When LSGAN is used, it is basically same as MSELoss,\n# but it abstracts away the need to create the target label tensor\n# that has the same size as the input\nclass GANLoss(nn.Module):\n    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0,\n                 tensor=torch.FloatTensor):\n        super(GANLoss, self).__init__()\n        self.real_label = target_real_label\n        self.fake_label = target_fake_label\n        self.real_label_var = None\n        self.fake_label_var = None\n        self.Tensor = tensor\n        if use_lsgan:\n            self.loss = nn.MSELoss()\n        else:\n            self.loss = nn.BCELoss()\n\n    def get_target_tensor(self, input, target_is_real):\n        target_tensor = None\n        if target_is_real:\n            create_label = ((self.real_label_var is None) or\n                            (self.real_label_var.numel() != input.numel()))\n            if create_label:\n                real_tensor = self.Tensor(input.size()).fill_(self.real_label)\n                self.real_label_var = Variable(real_tensor, requires_grad=False)\n            target_tensor = self.real_label_var\n        else:\n            create_label = ((self.fake_label_var is None) or\n                            (self.fake_label_var.numel() != input.numel()))\n            if create_label:\n                fake_tensor = self.Tensor(input.size()).fill_(self.fake_label)\n                self.fake_label_var = Variable(fake_tensor, requires_grad=False)\n            target_tensor = self.fake_label_var\n        return target_tensor\n\n    def __call__(self, input, target_is_real):\n        target_tensor = self.get_target_tensor(input, target_is_real)\n        return self.loss(input, target_tensor)\n\n\n\nclass DiscLossWGANGP():\n    def __init__(self):\n        self.LAMBDA = 10\n        \n    def name(self):\n        return \'DiscLossWGAN-GP\'\n\n    def initialize(self, opt, tensor):\n        # DiscLossLS.initialize(self, opt, tensor)\n        self.LAMBDA = 10\n        \n    # def get_g_loss(self, net, realA, fakeB):\n    #     # First, G(A) should fake the discriminator\n    #     self.D_fake = net.forward(fakeB)\n    #     return -self.D_fake.mean()\n        \n    def calc_gradient_penalty(self, netD, real_data, fake_data):\n        alpha = torch.rand(1, 1)\n        alpha = alpha.expand(real_data.size())\n        alpha = alpha.cuda()\n\n        interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n\n        interpolates = interpolates.cuda()\n        interpolates = Variable(interpolates, requires_grad=True)\n        \n        disc_interpolates = netD.forward(interpolates)\n\n        gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n                                  grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n                                  create_graph=True, retain_graph=True, only_inputs=True)[0]\n\n        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * self.LAMBDA\n        return gradient_penalty\n\n# Defines the generator that consists of Resnet blocks between a few\n# downsampling/upsampling operations.\n# Code and idea originally from Justin Johnson\'s architecture.\n# https://github.com/jcjohnson/fast-neural-style/\nclass ResnetGenerator(nn.Module):\n    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, gpu_ids=[], padding_type=\'reflect\'):\n        assert(n_blocks >= 0)\n        super(ResnetGenerator, self).__init__()\n        self.input_nc = input_nc\n        self.output_nc = output_nc\n        self.ngf = ngf\n        self.gpu_ids = gpu_ids\n\n        model = [nn.ReflectionPad2d(3),\n                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0),\n                 norm_layer(ngf),\n                 nn.ReLU(True)]\n\n        n_downsampling = 2\n        for i in range(n_downsampling):\n            mult = 2**i\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3,\n                                stride=2, padding=1),\n                      norm_layer(ngf * mult * 2),\n                      nn.ReLU(True)]\n\n        mult = 2**n_downsampling\n        for i in range(n_blocks):\n            model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout)]\n\n        for i in range(n_downsampling):\n            mult = 2**(n_downsampling - i)\n            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n                                         kernel_size=3, stride=2,\n                                         padding=1, output_padding=1),\n                      norm_layer(int(ngf * mult / 2)),\n                      nn.ReLU(True)]\n        model += [nn.ReflectionPad2d(3)]\n        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n        model += [nn.Tanh()]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, input):\n        if self.gpu_ids and isinstance(input.data, torch.cuda.FloatTensor):\n            return nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n        else:\n            return self.model(input)\n\n\n# Define a resnet block\nclass ResnetBlock(nn.Module):\n    def __init__(self, dim, padding_type, norm_layer, use_dropout):\n        super(ResnetBlock, self).__init__()\n        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout)\n\n    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout):\n        conv_block = []\n        p = 0\n        if padding_type == \'reflect\':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == \'replicate\':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == \'zero\':\n            p = 1\n        else:\n            raise NotImplementedError(\'padding [%s] is not implemented\' % padding_type)\n\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p),\n                       norm_layer(dim),\n                       nn.ReLU(True)]\n        if use_dropout:\n            conv_block += [nn.Dropout(0.5)]\n\n        p = 0\n        if padding_type == \'reflect\':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == \'replicate\':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == \'zero\':\n            p = 1\n        else:\n            raise NotImplementedError(\'padding [%s] is not implemented\' % padding_type)\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p),\n                       norm_layer(dim)]\n\n        return nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        out = x + self.conv_block(x)\n        return out\n\n\n# Defines the Unet generator.\n# |num_downs|: number of downsamplings in UNet. For example,\n# if |num_downs| == 7, image of size 128x128 will become of size 1x1\n# at the bottleneck\nclass UnetGenerator(nn.Module):\n    def __init__(self, input_nc, output_nc, num_downs, ngf=64,\n                 norm_layer=nn.BatchNorm2d, use_dropout=False, gpu_ids=[], skip=False, opt=None):\n        super(UnetGenerator, self).__init__()\n        self.gpu_ids = gpu_ids\n        self.opt = opt\n        # currently support only input_nc == output_nc\n        assert(input_nc == output_nc)\n\n        # construct unet structure\n        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, norm_layer=norm_layer, innermost=True, opt=opt)\n        for i in range(num_downs - 5):\n            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, unet_block, norm_layer=norm_layer, use_dropout=use_dropout, opt=opt)\n        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, unet_block, norm_layer=norm_layer, opt=opt)\n        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, unet_block, norm_layer=norm_layer, opt=opt)\n        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, unet_block, norm_layer=norm_layer, opt=opt)\n        unet_block = UnetSkipConnectionBlock(output_nc, ngf, unet_block, outermost=True, norm_layer=norm_layer, opt=opt)\n        \n        if skip == True:\n            skipmodule = SkipModule(unet_block, opt)\n            self.model = skipmodule\n        else:\n            self.model = unet_block\n\n    def forward(self, input):\n        if self.gpu_ids and isinstance(input.data, torch.cuda.FloatTensor):\n            return nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n        else:\n            return self.model(input)\n\nclass SkipModule(nn.Module):\n    def __init__(self, submodule, opt):\n        super(SkipModule, self).__init__()\n        self.submodule = submodule\n        self.opt = opt\n\n    def forward(self, x):\n        latent = self.submodule(x)\n        return self.opt.skip*x + latent, latent\n\n\n\n# Defines the submodule with skip connection.\n# X -------------------identity---------------------- X\n#   |-- downsampling -- |submodule| -- upsampling --|\nclass UnetSkipConnectionBlock(nn.Module):\n    def __init__(self, outer_nc, inner_nc,\n                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False, opt=None):\n        super(UnetSkipConnectionBlock, self).__init__()\n        self.outermost = outermost\n\n        downconv = nn.Conv2d(outer_nc, inner_nc, kernel_size=4,\n                             stride=2, padding=1)\n        downrelu = nn.LeakyReLU(0.2, True)\n        downnorm = norm_layer(inner_nc)\n        uprelu = nn.ReLU(True)\n        upnorm = norm_layer(outer_nc)\n\n        if opt.use_norm == 0:\n            if outermost:\n                upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n                                            kernel_size=4, stride=2,\n                                            padding=1)\n                down = [downconv]\n                up = [uprelu, upconv, nn.Tanh()]\n                model = down + [submodule] + up\n            elif innermost:\n                upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n                                            kernel_size=4, stride=2,\n                                            padding=1)\n                down = [downrelu, downconv]\n                up = [uprelu, upconv]\n                model = down + up\n            else:\n                upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n                                            kernel_size=4, stride=2,\n                                            padding=1)\n                down = [downrelu, downconv]\n                up = [uprelu, upconv]\n\n                if use_dropout:\n                    model = down + [submodule] + up + [nn.Dropout(0.5)]\n                else:\n                    model = down + [submodule] + up\n        else:\n            if outermost:\n                upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n                                            kernel_size=4, stride=2,\n                                            padding=1)\n                down = [downconv]\n                up = [uprelu, upconv, nn.Tanh()]\n                model = down + [submodule] + up\n            elif innermost:\n                upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n                                            kernel_size=4, stride=2,\n                                            padding=1)\n                down = [downrelu, downconv]\n                up = [uprelu, upconv, upnorm]\n                model = down + up\n            else:\n                upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n                                            kernel_size=4, stride=2,\n                                            padding=1)\n                down = [downrelu, downconv, downnorm]\n                up = [uprelu, upconv, upnorm]\n\n                if use_dropout:\n                    model = down + [submodule] + up + [nn.Dropout(0.5)]\n                else:\n                    model = down + [submodule] + up\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        if self.outermost:\n            return self.model(x)\n        else:\n            return torch.cat([self.model(x), x], 1)\n\n\n# Defines the PatchGAN discriminator with the specified arguments.\nclass NLayerDiscriminator(nn.Module):\n    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False, gpu_ids=[]):\n        super(NLayerDiscriminator, self).__init__()\n        self.gpu_ids = gpu_ids\n\n        kw = 4\n        padw = int(np.ceil((kw-1)/2))\n        sequence = [\n            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        nf_mult = 1\n        nf_mult_prev = 1\n        for n in range(1, n_layers):\n            nf_mult_prev = nf_mult\n            nf_mult = min(2**n, 8)\n            sequence += [\n                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                          kernel_size=kw, stride=2, padding=padw),\n                norm_layer(ndf * nf_mult),\n                nn.LeakyReLU(0.2, True)\n            ]\n\n        nf_mult_prev = nf_mult\n        nf_mult = min(2**n_layers, 8)\n        sequence += [\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                      kernel_size=kw, stride=1, padding=padw),\n            norm_layer(ndf * nf_mult),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n\n        if use_sigmoid:\n            sequence += [nn.Sigmoid()]\n\n        self.model = nn.Sequential(*sequence)\n\n    def forward(self, input):\n        # if len(self.gpu_ids) and isinstance(input.data, torch.cuda.FloatTensor):\n        #     return nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n        # else:\n        return self.model(input)\n\nclass NoNormDiscriminator(nn.Module):\n    def __init__(self, input_nc, ndf=64, n_layers=3, use_sigmoid=False, gpu_ids=[]):\n        super(NoNormDiscriminator, self).__init__()\n        self.gpu_ids = gpu_ids\n\n        kw = 4\n        padw = int(np.ceil((kw-1)/2))\n        sequence = [\n            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        nf_mult = 1\n        nf_mult_prev = 1\n        for n in range(1, n_layers):\n            nf_mult_prev = nf_mult\n            nf_mult = min(2**n, 8)\n            sequence += [\n                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                          kernel_size=kw, stride=2, padding=padw),\n                nn.LeakyReLU(0.2, True)\n            ]\n\n        nf_mult_prev = nf_mult\n        nf_mult = min(2**n_layers, 8)\n        sequence += [\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                      kernel_size=kw, stride=1, padding=padw),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n\n        if use_sigmoid:\n            sequence += [nn.Sigmoid()]\n\n        self.model = nn.Sequential(*sequence)\n\n    def forward(self, input):\n        # if len(self.gpu_ids) and isinstance(input.data, torch.cuda.FloatTensor):\n        #     return nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n        # else:\n        return self.model(input)\n\nclass FCDiscriminator(nn.Module):\n    def __init__(self, input_nc, ndf=64, n_layers=3, use_sigmoid=False, gpu_ids=[], patch=False):\n        super(FCDiscriminator, self).__init__()\n        self.gpu_ids = gpu_ids\n        self.use_sigmoid = use_sigmoid\n        kw = 4\n        padw = int(np.ceil((kw-1)/2))\n        sequence = [\n            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        nf_mult = 1\n        nf_mult_prev = 1\n        for n in range(1, n_layers):\n            nf_mult_prev = nf_mult\n            nf_mult = min(2**n, 8)\n            sequence += [\n                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                          kernel_size=kw, stride=2, padding=padw),\n                nn.LeakyReLU(0.2, True)\n            ]\n\n        nf_mult_prev = nf_mult\n        nf_mult = min(2**n_layers, 8)\n        sequence += [\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                      kernel_size=kw, stride=1, padding=padw),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n        if patch:\n            self.linear = nn.Linear(7*7,1)\n        else:\n            self.linear = nn.Linear(13*13,1)\n        if use_sigmoid:\n            self.sigmoid = nn.Sigmoid()\n\n        self.model = nn.Sequential(*sequence)\n\n    def forward(self, input):\n        batchsize = input.size()[0]\n        output = self.model(input)\n        output = output.view(batchsize,-1)\n        # print(output.size())\n        output = self.linear(output)\n        if self.use_sigmoid:\n            print(""sigmoid"")\n            output = self.sigmoid(output)\n        return output\n\n\nclass Unet_resize_conv(nn.Module):\n    def __init__(self, opt, skip):\n        super(Unet_resize_conv, self).__init__()\n\n        self.opt = opt\n        self.skip = skip\n        p = 1\n        # self.conv1_1 = nn.Conv2d(4, 32, 3, padding=p)\n        if opt.self_attention:\n            self.conv1_1 = nn.Conv2d(4, 32, 3, padding=p)\n            # self.conv1_1 = nn.Conv2d(3, 32, 3, padding=p)\n            self.downsample_1 = nn.MaxPool2d(2)\n            self.downsample_2 = nn.MaxPool2d(2)\n            self.downsample_3 = nn.MaxPool2d(2)\n            self.downsample_4 = nn.MaxPool2d(2)\n        else:\n            self.conv1_1 = nn.Conv2d(3, 32, 3, padding=p)\n        self.LReLU1_1 = nn.LeakyReLU(0.2, inplace=True)\n        if self.opt.use_norm == 1:\n            self.bn1_1 = SynBN2d(32) if self.opt.syn_norm else nn.BatchNorm2d(32)\n        self.conv1_2 = nn.Conv2d(32, 32, 3, padding=p)\n        self.LReLU1_2 = nn.LeakyReLU(0.2, inplace=True)\n        if self.opt.use_norm == 1:\n            self.bn1_2 = SynBN2d(32) if self.opt.syn_norm else nn.BatchNorm2d(32)\n        self.max_pool1 = nn.AvgPool2d(2) if self.opt.use_avgpool == 1 else nn.MaxPool2d(2)\n\n        self.conv2_1 = nn.Conv2d(32, 64, 3, padding=p)\n        self.LReLU2_1 = nn.LeakyReLU(0.2, inplace=True)\n        if self.opt.use_norm == 1:\n            self.bn2_1 = SynBN2d(64) if self.opt.syn_norm else nn.BatchNorm2d(64)\n        self.conv2_2 = nn.Conv2d(64, 64, 3, padding=p)\n        self.LReLU2_2 = nn.LeakyReLU(0.2, inplace=True)\n        if self.opt.use_norm == 1:\n            self.bn2_2 = SynBN2d(64) if self.opt.syn_norm else nn.BatchNorm2d(64)\n        self.max_pool2 = nn.AvgPool2d(2) if self.opt.use_avgpool == 1 else nn.MaxPool2d(2)\n\n        self.conv3_1 = nn.Conv2d(64, 128, 3, padding=p)\n        self.LReLU3_1 = nn.LeakyReLU(0.2, inplace=True)\n        if self.opt.use_norm == 1:\n            self.bn3_1 = SynBN2d(128) if self.opt.syn_norm else nn.BatchNorm2d(128)\n        self.conv3_2 = nn.Conv2d(128, 128, 3, padding=p)\n        self.LReLU3_2 = nn.LeakyReLU(0.2, inplace=True)\n        if self.opt.use_norm == 1:\n            self.bn3_2 = SynBN2d(128) if self.opt.syn_norm else nn.BatchNorm2d(128)\n        self.max_pool3 = nn.AvgPool2d(2) if self.opt.use_avgpool == 1 else nn.MaxPool2d(2)\n\n        self.conv4_1 = nn.Conv2d(128, 256, 3, padding=p)\n        self.LReLU4_1 = nn.LeakyReLU(0.2, inplace=True)\n        if self.opt.use_norm == 1:\n            self.bn4_1 = SynBN2d(256) if self.opt.syn_norm else nn.BatchNorm2d(256)\n        self.conv4_2 = nn.Conv2d(256, 256, 3, padding=p)\n        self.LReLU4_2 = nn.LeakyReLU(0.2, inplace=True)\n        if self.opt.use_norm == 1:\n            self.bn4_2 = SynBN2d(256) if self.opt.syn_norm else nn.BatchNorm2d(256)\n        self.max_pool4 = nn.AvgPool2d(2) if self.opt.use_avgpool == 1 else nn.MaxPool2d(2)\n\n        self.conv5_1 = nn.Conv2d(256, 512, 3, padding=p)\n        self.LReLU5_1 = nn.LeakyReLU(0.2, inplace=True)\n        if self.opt.use_norm == 1:\n            self.bn5_1 = SynBN2d(512) if self.opt.syn_norm else nn.BatchNorm2d(512)\n        self.conv5_2 = nn.Conv2d(512, 512, 3, padding=p)\n        self.LReLU5_2 = nn.LeakyReLU(0.2, inplace=True)\n        if self.opt.use_norm == 1:\n            self.bn5_2 = SynBN2d(512) if self.opt.syn_norm else nn.BatchNorm2d(512)\n\n        # self.deconv5 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n        self.deconv5 = nn.Conv2d(512, 256, 3, padding=p)\n        self.conv6_1 = nn.Conv2d(512, 256, 3, padding=p)\n        self.LReLU6_1 = nn.LeakyReLU(0.2, inplace=True)\n        if self.opt.use_norm == 1:\n            self.bn6_1 = SynBN2d(256) if self.opt.syn_norm else nn.BatchNorm2d(256)\n        self.conv6_2 = nn.Conv2d(256, 256, 3, padding=p)\n        self.LReLU6_2 = nn.LeakyReLU(0.2, inplace=True)\n        if self.opt.use_norm == 1:\n            self.bn6_2 = SynBN2d(256) if self.opt.syn_norm else nn.BatchNorm2d(256)\n\n        # self.deconv6 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n        self.deconv6 = nn.Conv2d(256, 128, 3, padding=p)\n        self.conv7_1 = nn.Conv2d(256, 128, 3, padding=p)\n        self.LReLU7_1 = nn.LeakyReLU(0.2, inplace=True)\n        if self.opt.use_norm == 1:\n            self.bn7_1 = SynBN2d(128) if self.opt.syn_norm else nn.BatchNorm2d(128)\n        self.conv7_2 = nn.Conv2d(128, 128, 3, padding=p)\n        self.LReLU7_2 = nn.LeakyReLU(0.2, inplace=True)\n        if self.opt.use_norm == 1:\n            self.bn7_2 = SynBN2d(128) if self.opt.syn_norm else nn.BatchNorm2d(128)\n\n        # self.deconv7 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n        self.deconv7 = nn.Conv2d(128, 64, 3, padding=p)\n        self.conv8_1 = nn.Conv2d(128, 64, 3, padding=p)\n        self.LReLU8_1 = nn.LeakyReLU(0.2, inplace=True)\n        if self.opt.use_norm == 1:\n            self.bn8_1 = SynBN2d(64) if self.opt.syn_norm else nn.BatchNorm2d(64)\n        self.conv8_2 = nn.Conv2d(64, 64, 3, padding=p)\n        self.LReLU8_2 = nn.LeakyReLU(0.2, inplace=True)\n        if self.opt.use_norm == 1:\n            self.bn8_2 = SynBN2d(64) if self.opt.syn_norm else nn.BatchNorm2d(64)\n\n        # self.deconv8 = nn.ConvTranspose2d(64, 32, 2, stride=2)\n        self.deconv8 = nn.Conv2d(64, 32, 3, padding=p)\n        self.conv9_1 = nn.Conv2d(64, 32, 3, padding=p)\n        self.LReLU9_1 = nn.LeakyReLU(0.2, inplace=True)\n        if self.opt.use_norm == 1:\n            self.bn9_1 = SynBN2d(32) if self.opt.syn_norm else nn.BatchNorm2d(32)\n        self.conv9_2 = nn.Conv2d(32, 32, 3, padding=p)\n        self.LReLU9_2 = nn.LeakyReLU(0.2, inplace=True)\n\n        self.conv10 = nn.Conv2d(32, 3, 1)\n        if self.opt.tanh:\n            self.tanh = nn.Tanh()\n\n    def depth_to_space(self, input, block_size):\n        block_size_sq = block_size*block_size\n        output = input.permute(0, 2, 3, 1)\n        (batch_size, d_height, d_width, d_depth) = output.size()\n        s_depth = int(d_depth / block_size_sq)\n        s_width = int(d_width * block_size)\n        s_height = int(d_height * block_size)\n        t_1 = output.resize(batch_size, d_height, d_width, block_size_sq, s_depth)\n        spl = t_1.split(block_size, 3)\n        stack = [t_t.resize(batch_size, d_height, s_width, s_depth) for t_t in spl]\n        output = torch.stack(stack,0).transpose(0,1).permute(0,2,1,3,4).resize(batch_size, s_height, s_width, s_depth)\n        output = output.permute(0, 3, 1, 2)\n        return output\n\n    def forward(self, input, gray):\n        flag = 0\n        if input.size()[3] > 2200:\n            avg = nn.AvgPool2d(2)\n            input = avg(input)\n            gray = avg(gray)\n            flag = 1\n            # pass\n        input, pad_left, pad_right, pad_top, pad_bottom = pad_tensor(input)\n        gray, pad_left, pad_right, pad_top, pad_bottom = pad_tensor(gray)\n        if self.opt.self_attention:\n            gray_2 = self.downsample_1(gray)\n            gray_3 = self.downsample_2(gray_2)\n            gray_4 = self.downsample_3(gray_3)\n            gray_5 = self.downsample_4(gray_4)\n        if self.opt.use_norm == 1:\n            if self.opt.self_attention:\n                x = self.bn1_1(self.LReLU1_1(self.conv1_1(torch.cat((input, gray), 1))))\n                # x = self.bn1_1(self.LReLU1_1(self.conv1_1(input)))\n            else:\n                x = self.bn1_1(self.LReLU1_1(self.conv1_1(input)))\n            conv1 = self.bn1_2(self.LReLU1_2(self.conv1_2(x)))\n            x = self.max_pool1(conv1)\n\n            x = self.bn2_1(self.LReLU2_1(self.conv2_1(x)))\n            conv2 = self.bn2_2(self.LReLU2_2(self.conv2_2(x)))\n            x = self.max_pool2(conv2)\n\n            x = self.bn3_1(self.LReLU3_1(self.conv3_1(x)))\n            conv3 = self.bn3_2(self.LReLU3_2(self.conv3_2(x)))\n            x = self.max_pool3(conv3)\n\n            x = self.bn4_1(self.LReLU4_1(self.conv4_1(x)))\n            conv4 = self.bn4_2(self.LReLU4_2(self.conv4_2(x)))\n            x = self.max_pool4(conv4)\n\n            x = self.bn5_1(self.LReLU5_1(self.conv5_1(x)))\n            x = x*gray_5 if self.opt.self_attention else x\n            conv5 = self.bn5_2(self.LReLU5_2(self.conv5_2(x)))\n            \n            conv5 = F.upsample(conv5, scale_factor=2, mode=\'bilinear\')\n            conv4 = conv4*gray_4 if self.opt.self_attention else conv4\n            up6 = torch.cat([self.deconv5(conv5), conv4], 1)\n            x = self.bn6_1(self.LReLU6_1(self.conv6_1(up6)))\n            conv6 = self.bn6_2(self.LReLU6_2(self.conv6_2(x)))\n\n            conv6 = F.upsample(conv6, scale_factor=2, mode=\'bilinear\')\n            conv3 = conv3*gray_3 if self.opt.self_attention else conv3\n            up7 = torch.cat([self.deconv6(conv6), conv3], 1)\n            x = self.bn7_1(self.LReLU7_1(self.conv7_1(up7)))\n            conv7 = self.bn7_2(self.LReLU7_2(self.conv7_2(x)))\n\n            conv7 = F.upsample(conv7, scale_factor=2, mode=\'bilinear\')\n            conv2 = conv2*gray_2 if self.opt.self_attention else conv2\n            up8 = torch.cat([self.deconv7(conv7), conv2], 1)\n            x = self.bn8_1(self.LReLU8_1(self.conv8_1(up8)))\n            conv8 = self.bn8_2(self.LReLU8_2(self.conv8_2(x)))\n\n            conv8 = F.upsample(conv8, scale_factor=2, mode=\'bilinear\')\n            conv1 = conv1*gray if self.opt.self_attention else conv1\n            up9 = torch.cat([self.deconv8(conv8), conv1], 1)\n            x = self.bn9_1(self.LReLU9_1(self.conv9_1(up9)))\n            conv9 = self.LReLU9_2(self.conv9_2(x))\n\n            latent = self.conv10(conv9)\n\n            if self.opt.times_residual:\n                latent = latent*gray\n\n            # output = self.depth_to_space(conv10, 2)\n            if self.opt.tanh:\n                latent = self.tanh(latent)\n            if self.skip:\n                if self.opt.linear_add:\n                    if self.opt.latent_threshold:\n                        latent = F.relu(latent)\n                    elif self.opt.latent_norm:\n                        latent = (latent - torch.min(latent))/(torch.max(latent)-torch.min(latent))\n                    input = (input - torch.min(input))/(torch.max(input) - torch.min(input))\n                    output = latent + input*self.opt.skip\n                    output = output*2 - 1\n                else:\n                    if self.opt.latent_threshold:\n                        latent = F.relu(latent)\n                    elif self.opt.latent_norm:\n                        latent = (latent - torch.min(latent))/(torch.max(latent)-torch.min(latent))\n                    output = latent + input*self.opt.skip\n            else:\n                output = latent\n\n            if self.opt.linear:\n                output = output/torch.max(torch.abs(output))\n            \n                \n        elif self.opt.use_norm == 0:\n            if self.opt.self_attention:\n                x = self.LReLU1_1(self.conv1_1(torch.cat((input, gray), 1)))\n            else:\n                x = self.LReLU1_1(self.conv1_1(input))\n            conv1 = self.LReLU1_2(self.conv1_2(x))\n            x = self.max_pool1(conv1)\n\n            x = self.LReLU2_1(self.conv2_1(x))\n            conv2 = self.LReLU2_2(self.conv2_2(x))\n            x = self.max_pool2(conv2)\n\n            x = self.LReLU3_1(self.conv3_1(x))\n            conv3 = self.LReLU3_2(self.conv3_2(x))\n            x = self.max_pool3(conv3)\n\n            x = self.LReLU4_1(self.conv4_1(x))\n            conv4 = self.LReLU4_2(self.conv4_2(x))\n            x = self.max_pool4(conv4)\n\n            x = self.LReLU5_1(self.conv5_1(x))\n            x = x*gray_5 if self.opt.self_attention else x\n            conv5 = self.LReLU5_2(self.conv5_2(x))\n\n            conv5 = F.upsample(conv5, scale_factor=2, mode=\'bilinear\')\n            conv4 = conv4*gray_4 if self.opt.self_attention else conv4\n            up6 = torch.cat([self.deconv5(conv5), conv4], 1)\n            x = self.LReLU6_1(self.conv6_1(up6))\n            conv6 = self.LReLU6_2(self.conv6_2(x))\n\n            conv6 = F.upsample(conv6, scale_factor=2, mode=\'bilinear\')\n            conv3 = conv3*gray_3 if self.opt.self_attention else conv3\n            up7 = torch.cat([self.deconv6(conv6), conv3], 1)\n            x = self.LReLU7_1(self.conv7_1(up7))\n            conv7 = self.LReLU7_2(self.conv7_2(x))\n\n            conv7 = F.upsample(conv7, scale_factor=2, mode=\'bilinear\')\n            conv2 = conv2*gray_2 if self.opt.self_attention else conv2\n            up8 = torch.cat([self.deconv7(conv7), conv2], 1)\n            x = self.LReLU8_1(self.conv8_1(up8))\n            conv8 = self.LReLU8_2(self.conv8_2(x))\n\n            conv8 = F.upsample(conv8, scale_factor=2, mode=\'bilinear\')\n            conv1 = conv1*gray if self.opt.self_attention else conv1\n            up9 = torch.cat([self.deconv8(conv8), conv1], 1)\n            x = self.LReLU9_1(self.conv9_1(up9))\n            conv9 = self.LReLU9_2(self.conv9_2(x))\n\n            latent = self.conv10(conv9)\n            \n            if self.opt.times_residual:\n                latent = latent*gray\n\n            if self.opt.tanh:\n                latent = self.tanh(latent)\n            if self.skip:\n                if self.opt.linear_add:\n                    if self.opt.latent_threshold:\n                        latent = F.relu(latent)\n                    elif self.opt.latent_norm:\n                        latent = (latent - torch.min(latent))/(torch.max(latent)-torch.min(latent))\n                    input = (input - torch.min(input))/(torch.max(input) - torch.min(input))\n                    output = latent + input*self.opt.skip\n                    output = output*2 - 1\n                else:\n                    if self.opt.latent_threshold:\n                        latent = F.relu(latent)\n                    elif self.opt.latent_norm:\n                        latent = (latent - torch.min(latent))/(torch.max(latent)-torch.min(latent))\n                    output = latent + input*self.opt.skip\n            else:\n                output = latent\n\n            if self.opt.linear:\n                output = output/torch.max(torch.abs(output))\n        \n        output = pad_tensor_back(output, pad_left, pad_right, pad_top, pad_bottom)\n        latent = pad_tensor_back(latent, pad_left, pad_right, pad_top, pad_bottom)\n        gray = pad_tensor_back(gray, pad_left, pad_right, pad_top, pad_bottom)\n        if flag == 1:\n            output = F.upsample(output, scale_factor=2, mode=\'bilinear\')\n            gray = F.upsample(gray, scale_factor=2, mode=\'bilinear\')\n        if self.skip:\n            return output, latent\n        else:\n            return output\n\nclass DnCNN(nn.Module):\n    def __init__(self, opt=None, depth=17, n_channels=64, image_channels=1, use_bnorm=True, kernel_size=3):\n        super(DnCNN, self).__init__()\n        kernel_size = 3\n        padding = 1\n        layers = []\n\n        layers.append(nn.Conv2d(in_channels=image_channels, out_channels=n_channels, kernel_size=kernel_size, padding=padding, bias=True))\n        layers.append(nn.ReLU(inplace=True))\n        for _ in range(depth-2):\n            layers.append(nn.Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size, padding=padding, bias=False))\n            layers.append(nn.BatchNorm2d(n_channels, eps=0.0001, momentum = 0.95))\n            layers.append(nn.ReLU(inplace=True))\n        layers.append(nn.Conv2d(in_channels=n_channels, out_channels=image_channels, kernel_size=kernel_size, padding=padding, bias=False))\n        self.dncnn = nn.Sequential(*layers)\n        self._initialize_weights()\n\n    def forward(self, x):\n        y = x\n        out = self.dncnn(x)\n        return y+out\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                init.orthogonal_(m.weight)\n                print(\'init weight\')\n                if m.bias is not None:\n                    init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                init.constant_(m.weight, 1)\n                init.constant_(m.bias, 0)\n\nclass Vgg16(nn.Module):\n    def __init__(self):\n        super(Vgg16, self).__init__()\n        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n\n        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n\n        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n\n        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n\n        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n\n    def forward(self, X, opt):\n        h = F.relu(self.conv1_1(X), inplace=True)\n        h = F.relu(self.conv1_2(h), inplace=True)\n        # relu1_2 = h\n        h = F.max_pool2d(h, kernel_size=2, stride=2)\n\n        h = F.relu(self.conv2_1(h), inplace=True)\n        h = F.relu(self.conv2_2(h), inplace=True)\n        # relu2_2 = h\n        h = F.max_pool2d(h, kernel_size=2, stride=2)\n\n        h = F.relu(self.conv3_1(h), inplace=True)\n        h = F.relu(self.conv3_2(h), inplace=True)\n        h = F.relu(self.conv3_3(h), inplace=True)\n        # relu3_3 = h\n        if opt.vgg_choose != ""no_maxpool"":\n            h = F.max_pool2d(h, kernel_size=2, stride=2)\n\n        h = F.relu(self.conv4_1(h), inplace=True)\n        relu4_1 = h\n        h = F.relu(self.conv4_2(h), inplace=True)\n        relu4_2 = h\n        conv4_3 = self.conv4_3(h)\n        h = F.relu(conv4_3, inplace=True)\n        relu4_3 = h\n\n        if opt.vgg_choose != ""no_maxpool"":\n            if opt.vgg_maxpooling:\n                h = F.max_pool2d(h, kernel_size=2, stride=2)\n        \n        relu5_1 = F.relu(self.conv5_1(h), inplace=True)\n        relu5_2 = F.relu(self.conv5_2(relu5_1), inplace=True)\n        conv5_3 = self.conv5_3(relu5_2) \n        h = F.relu(conv5_3, inplace=True)\n        relu5_3 = h\n        if opt.vgg_choose == ""conv4_3"":\n            return conv4_3\n        elif opt.vgg_choose == ""relu4_2"":\n            return relu4_2\n        elif opt.vgg_choose == ""relu4_1"":\n            return relu4_1\n        elif opt.vgg_choose == ""relu4_3"":\n            return relu4_3\n        elif opt.vgg_choose == ""conv5_3"":\n            return conv5_3\n        elif opt.vgg_choose == ""relu5_1"":\n            return relu5_1\n        elif opt.vgg_choose == ""relu5_2"":\n            return relu5_2\n        elif opt.vgg_choose == ""relu5_3"" or ""maxpool"":\n            return relu5_3\n\ndef vgg_preprocess(batch, opt):\n    tensortype = type(batch.data)\n    (r, g, b) = torch.chunk(batch, 3, dim = 1)\n    batch = torch.cat((b, g, r), dim = 1) # convert RGB to BGR\n    batch = (batch + 1) * 255 * 0.5 # [-1, 1] -> [0, 255]\n    if opt.vgg_mean:\n        mean = tensortype(batch.data.size())\n        mean[:, 0, :, :] = 103.939\n        mean[:, 1, :, :] = 116.779\n        mean[:, 2, :, :] = 123.680\n        batch = batch.sub(Variable(mean)) # subtract mean\n    return batch\n\nclass PerceptualLoss(nn.Module):\n    def __init__(self, opt):\n        super(PerceptualLoss, self).__init__()\n        self.opt = opt\n        self.instancenorm = nn.InstanceNorm2d(512, affine=False)\n\n    def compute_vgg_loss(self, vgg, img, target):\n        img_vgg = vgg_preprocess(img, self.opt)\n        target_vgg = vgg_preprocess(target, self.opt)\n        img_fea = vgg(img_vgg, self.opt)\n        target_fea = vgg(target_vgg, self.opt)\n        if self.opt.no_vgg_instance:\n            return torch.mean((img_fea - target_fea) ** 2)\n        else:\n            return torch.mean((self.instancenorm(img_fea) - self.instancenorm(target_fea)) ** 2)\n\ndef load_vgg16(model_dir, gpu_ids):\n    """""" Use the model from https://github.com/abhiskk/fast-neural-style/blob/master/neural_style/utils.py """"""\n    if not os.path.exists(model_dir):\n        os.mkdir(model_dir)\n    # if not os.path.exists(os.path.join(model_dir, \'vgg16.weight\')):\n    #     if not os.path.exists(os.path.join(model_dir, \'vgg16.t7\')):\n    #         os.system(\'wget https://www.dropbox.com/s/76l3rt4kyi3s8x7/vgg16.t7?dl=1 -O \' + os.path.join(model_dir, \'vgg16.t7\'))\n    #     vgglua = load_lua(os.path.join(model_dir, \'vgg16.t7\'))\n    #     vgg = Vgg16()\n    #     for (src, dst) in zip(vgglua.parameters()[0], vgg.parameters()):\n    #         dst.data[:] = src\n    #     torch.save(vgg.state_dict(), os.path.join(model_dir, \'vgg16.weight\'))\n    vgg = Vgg16()\n    # vgg.cuda()\n    vgg.cuda(device=gpu_ids[0])\n    vgg.load_state_dict(torch.load(os.path.join(model_dir, \'vgg16.weight\')))\n    vgg = torch.nn.DataParallel(vgg, gpu_ids)\n    return vgg\n\n\n\nclass FCN32s(nn.Module):\n    def __init__(self, n_class=21):\n        super(FCN32s, self).__init__()\n        # conv1\n        self.conv1_1 = nn.Conv2d(3, 64, 3, padding=100)\n        self.relu1_1 = nn.ReLU(inplace=True)\n        self.conv1_2 = nn.Conv2d(64, 64, 3, padding=1)\n        self.relu1_2 = nn.ReLU(inplace=True)\n        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/2\n\n        # conv2\n        self.conv2_1 = nn.Conv2d(64, 128, 3, padding=1)\n        self.relu2_1 = nn.ReLU(inplace=True)\n        self.conv2_2 = nn.Conv2d(128, 128, 3, padding=1)\n        self.relu2_2 = nn.ReLU(inplace=True)\n        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/4\n\n        # conv3\n        self.conv3_1 = nn.Conv2d(128, 256, 3, padding=1)\n        self.relu3_1 = nn.ReLU(inplace=True)\n        self.conv3_2 = nn.Conv2d(256, 256, 3, padding=1)\n        self.relu3_2 = nn.ReLU(inplace=True)\n        self.conv3_3 = nn.Conv2d(256, 256, 3, padding=1)\n        self.relu3_3 = nn.ReLU(inplace=True)\n        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/8\n\n        # conv4\n        self.conv4_1 = nn.Conv2d(256, 512, 3, padding=1)\n        self.relu4_1 = nn.ReLU(inplace=True)\n        self.conv4_2 = nn.Conv2d(512, 512, 3, padding=1)\n        self.relu4_2 = nn.ReLU(inplace=True)\n        self.conv4_3 = nn.Conv2d(512, 512, 3, padding=1)\n        self.relu4_3 = nn.ReLU(inplace=True)\n        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/16\n\n        # conv5\n        self.conv5_1 = nn.Conv2d(512, 512, 3, padding=1)\n        self.relu5_1 = nn.ReLU(inplace=True)\n        self.conv5_2 = nn.Conv2d(512, 512, 3, padding=1)\n        self.relu5_2 = nn.ReLU(inplace=True)\n        self.conv5_3 = nn.Conv2d(512, 512, 3, padding=1)\n        self.relu5_3 = nn.ReLU(inplace=True)\n        self.pool5 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/32\n\n        # fc6\n        self.fc6 = nn.Conv2d(512, 4096, 7)\n        self.relu6 = nn.ReLU(inplace=True)\n        self.drop6 = nn.Dropout2d()\n\n        # fc7\n        self.fc7 = nn.Conv2d(4096, 4096, 1)\n        self.relu7 = nn.ReLU(inplace=True)\n        self.drop7 = nn.Dropout2d()\n\n        self.score_fr = nn.Conv2d(4096, n_class, 1)\n        self.upscore = nn.ConvTranspose2d(n_class, n_class, 64, stride=32,\n                                          bias=False)\n\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                m.weight.data.zero_()\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            if isinstance(m, nn.ConvTranspose2d):\n                assert m.kernel_size[0] == m.kernel_size[1]\n                initial_weight = get_upsampling_weight(\n                    m.in_channels, m.out_channels, m.kernel_size[0])\n                m.weight.data.copy_(initial_weight)\n\n    def forward(self, x):\n        h = x\n        h = self.relu1_1(self.conv1_1(h))\n        h = self.relu1_2(self.conv1_2(h))\n        h = self.pool1(h)\n\n        h = self.relu2_1(self.conv2_1(h))\n        h = self.relu2_2(self.conv2_2(h))\n        h = self.pool2(h)\n\n        h = self.relu3_1(self.conv3_1(h))\n        h = self.relu3_2(self.conv3_2(h))\n        h = self.relu3_3(self.conv3_3(h))\n        h = self.pool3(h)\n\n        h = self.relu4_1(self.conv4_1(h))\n        h = self.relu4_2(self.conv4_2(h))\n        h = self.relu4_3(self.conv4_3(h))\n        h = self.pool4(h)\n\n        h = self.relu5_1(self.conv5_1(h))\n        h = self.relu5_2(self.conv5_2(h))\n        h = self.relu5_3(self.conv5_3(h))\n        h = self.pool5(h)\n\n        h = self.relu6(self.fc6(h))\n        h = self.drop6(h)\n\n        h = self.relu7(self.fc7(h))\n        h = self.drop7(h)\n\n        h = self.score_fr(h)\n\n        h = self.upscore(h)\n        h = h[:, :, 19:19 + x.size()[2], 19:19 + x.size()[3]].contiguous()\n        return h\n\ndef load_fcn(model_dir):\n    fcn = FCN32s()\n    fcn.load_state_dict(torch.load(os.path.join(model_dir, \'fcn32s_from_caffe.pth\')))\n    fcn.cuda()\n    return fcn\n\nclass SemanticLoss(nn.Module):\n    def __init__(self, opt):\n        super(SemanticLoss, self).__init__()\n        self.opt = opt\n        self.instancenorm = nn.InstanceNorm2d(21, affine=False)\n\n    def compute_fcn_loss(self, fcn, img, target):\n        img_fcn = vgg_preprocess(img, self.opt)\n        target_fcn = vgg_preprocess(target, self.opt)\n        img_fea = fcn(img_fcn)\n        target_fea = fcn(target_fcn)\n        return torch.mean((self.instancenorm(img_fea) - self.instancenorm(target_fea)) ** 2)\n'"
models/pair_model.py,9,"b'import numpy as np\nimport torch\nimport os\nfrom collections import OrderedDict\nfrom torch.autograd import Variable\nimport util.util as util\nfrom collections import OrderedDict\nfrom torch.autograd import Variable\nimport itertools\nimport util.util as util\nfrom util.image_pool import ImagePool\nfrom .base_model import BaseModel\nfrom . import networks\nimport sys\n\n\nclass PairModel(BaseModel):\n    def name(self):\n        return \'CycleGANModel\'\n\n    def initialize(self, opt):\n        BaseModel.initialize(self, opt)\n\n        nb = opt.batchSize\n        size = opt.fineSize\n        self.opt = opt\n        self.input_A = self.Tensor(nb, opt.input_nc, size, size)\n        self.input_B = self.Tensor(nb, opt.output_nc, size, size)\n\n        if opt.vgg > 0:\n            self.vgg_loss = networks.PerceptualLoss()\n            self.vgg_loss.cuda()\n            self.vgg = networks.load_vgg16(""./model"")\n            self.vgg.eval()\n            for param in self.vgg.parameters():\n                param.requires_grad = False\n        # load/define networks\n        # The naming conversion is different from those used in the paper\n        # Code (paper): G_A (G), G_B (F), D_A (D_Y), D_B (D_X)\n\n        skip = True if opt.skip > 0 else False\n        self.netG_A = networks.define_G(opt.input_nc, opt.output_nc,\n                                        opt.ngf, opt.which_model_netG, opt.norm, not opt.no_dropout, self.gpu_ids, skip=skip, opt=opt)\n        self.netG_B = networks.define_G(opt.output_nc, opt.input_nc,\n                                        opt.ngf, opt.which_model_netG, opt.norm, not opt.no_dropout, self.gpu_ids, skip=False, opt=opt)\n\n        if self.isTrain:\n            use_sigmoid = opt.no_lsgan\n            self.netD_A = networks.define_D(opt.output_nc, opt.ndf,\n                                            opt.which_model_netD,\n                                            opt.n_layers_D, opt.norm, use_sigmoid, self.gpu_ids)\n            self.netD_B = networks.define_D(opt.input_nc, opt.ndf,\n                                            opt.which_model_netD,\n                                            opt.n_layers_D, opt.norm, use_sigmoid, self.gpu_ids)\n        if not self.isTrain or opt.continue_train:\n            which_epoch = opt.which_epoch\n            self.load_network(self.netG_A, \'G_A\', which_epoch)\n            self.load_network(self.netG_B, \'G_B\', which_epoch)\n            if self.isTrain:\n                self.load_network(self.netD_A, \'D_A\', which_epoch)\n                self.load_network(self.netD_B, \'D_B\', which_epoch)\n\n        if self.isTrain:\n            self.old_lr = opt.lr\n            self.fake_A_pool = ImagePool(opt.pool_size)\n            self.fake_B_pool = ImagePool(opt.pool_size)\n            # define loss functions\n            if opt.use_wgan:\n                self.criterionGAN = networks.DiscLossWGANGP()\n            else:\n                self.criterionGAN = networks.GANLoss(use_lsgan=not opt.no_lsgan, tensor=self.Tensor)\n            if opt.use_mse:\n                self.criterionCycle = torch.nn.MSELoss()\n            else:\n                self.criterionCycle = torch.nn.L1Loss()\n            self.criterionL1 = torch.nn.L1Loss()\n            self.criterionIdt = torch.nn.L1Loss()\n            # initialize optimizers\n            self.optimizer_G = torch.optim.Adam(itertools.chain(self.netG_A.parameters(), self.netG_B.parameters()),\n                                                lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizer_D_A = torch.optim.Adam(self.netD_A.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizer_D_B = torch.optim.Adam(self.netD_B.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n\n        print(\'---------- Networks initialized -------------\')\n        networks.print_network(self.netG_A)\n        networks.print_network(self.netG_B)\n        if self.isTrain:\n            networks.print_network(self.netD_A)\n            networks.print_network(self.netD_B)\n        if opt.isTrain:\n            self.netG_A.train()\n            self.netG_B.train()\n        else:\n            self.netG_A.eval()\n            self.netG_B.eval()\n        print(\'-----------------------------------------------\')\n\n    def set_input(self, input):\n        AtoB = self.opt.which_direction == \'AtoB\'\n        input_A = input[\'A\' if AtoB else \'B\']\n        input_B = input[\'B\' if AtoB else \'A\']\n        self.input_A.resize_(input_A.size()).copy_(input_A)\n        self.input_B.resize_(input_B.size()).copy_(input_B)\n        self.image_paths = input[\'A_paths\' if AtoB else \'B_paths\']\n\n    def forward(self):\n        self.real_A = Variable(self.input_A)\n        self.real_B = Variable(self.input_B)\n\n\n    def test(self):\n        self.real_A = Variable(self.input_A, volatile=True)\n        # print(np.transpose(self.real_A.data[0].cpu().float().numpy(),(1,2,0))[:2][:2][:])\n        if self.opt.skip == 1:\n            self.fake_B, self.latent_real_A = self.netG_A.forward(self.real_A)\n        else:\n            self.fake_B = self.netG_A.forward(self.real_A)\n        self.rec_A = self.netG_B.forward(self.fake_B)\n\n        self.real_B = Variable(self.input_B, volatile=True)\n        self.fake_A = self.netG_B.forward(self.real_B)\n        if self.opt.skip == 1:\n            self.rec_B, self.latent_fake_A = self.netG_A.forward(self.fake_A)\n        else:\n            self.rec_B = self.netG_A.forward(self.fake_A)\n\n    def predict(self):\n        self.real_A = Variable(self.input_A, volatile=True)\n        # print(np.transpose(self.real_A.data[0].cpu().float().numpy(),(1,2,0))[:2][:2][:])\n        if self.opt.skip == 1:\n            self.fake_B, self.latent_real_A = self.netG_A.forward(self.real_A)\n        else:\n            self.fake_B = self.netG_A.forward(self.real_A)\n        self.rec_A = self.netG_B.forward(self.fake_B)\n\n        real_A = util.tensor2im(self.real_A.data)\n        fake_B = util.tensor2im(self.fake_B.data)\n        rec_A = util.tensor2im(self.rec_A.data)\n        if self.opt.skip == 1:\n            latent_real_A = util.tensor2im(self.latent_real_A.data)\n            return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (""latent_real_A"", latent_real_A), (""rec_A"", rec_A)])\n        else:\n            return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (""rec_A"", rec_A)])\n\n    # get image paths\n    def get_image_paths(self):\n        return self.image_paths\n\n    def backward_D_basic(self, netD, real, fake):\n        # Real\n        pred_real = netD.forward(real)\n        if self.opt.use_wgan:\n            loss_D_real = pred_real.mean()\n        else:\n            loss_D_real = self.criterionGAN(pred_real, True)\n        # Fake\n        pred_fake = netD.forward(fake.detach())\n        if self.opt.use_wgan:\n            loss_D_fake = pred_fake.mean()\n        else:\n            loss_D_fake = self.criterionGAN(pred_fake, False)\n        # Combined loss\n        if self.opt.use_wgan:\n            loss_D = loss_D_fake - loss_D_real + self.criterionGAN.calc_gradient_penalty(netD, real.data, fake.data)\n        else:\n            loss_D = (loss_D_real + loss_D_fake) * 0.5\n        # backward\n        loss_D.backward()\n        return loss_D\n\n    def backward_D_A(self):\n        fake_B = self.fake_B_pool.query(self.fake_B)\n        self.loss_D_A = self.backward_D_basic(self.netD_A, self.real_B, fake_B)\n\n    def backward_D_B(self):\n        fake_A = self.fake_A_pool.query(self.fake_A)\n        self.loss_D_B = self.backward_D_basic(self.netD_B, self.real_A, fake_A)\n\n    def backward_G(self):\n        lambda_idt = self.opt.identity\n        lambda_A = self.opt.lambda_A\n        lambda_B = self.opt.lambda_B\n        # Identity loss\n        if lambda_idt > 0:\n            # G_A should be identity if real_B is fed.\n            if self.opt.skip == 1:\n                self.idt_A, _ = self.netG_A.forward(self.real_B)\n            else:\n                self.idt_A = self.netG_A.forward(self.real_B)\n            self.loss_idt_A = self.criterionIdt(self.idt_A, self.real_B) * lambda_B * lambda_idt\n            # G_B should be identity if real_A is fed.\n            self.idt_B = self.netG_B.forward(self.real_A)\n            self.loss_idt_B = self.criterionIdt(self.idt_B, self.real_A) * lambda_A * lambda_idt\n        else:\n            self.loss_idt_A = 0\n            self.loss_idt_B = 0\n\n        # GAN loss\n        # D_A(G_A(A))\n        if self.opt.skip == 1:\n            self.fake_B, self.latent_real_A = self.netG_A.forward(self.real_A)\n        else:\n            self.fake_B = self.netG_A.forward(self.real_A)\n         # = self.latent_real_A + self.opt.skip * self.real_A\n        pred_fake = self.netD_A.forward(self.fake_B)\n        if self.opt.use_wgan:\n            self.loss_G_A = -pred_fake.mean()\n        else:\n            self.loss_G_A = self.criterionGAN(pred_fake, True)\n        self.L1_AB = self.criterionL1(self.fake_B, self.real_B) * self.opt.l1\n        # D_B(G_B(B))\n        self.fake_A = self.netG_B.forward(self.real_B)\n        pred_fake = self.netD_B.forward(self.fake_A)\n        self.L1_BA = self.criterionL1(self.fake_A, self.real_A) * self.opt.l1\n        if self.opt.use_wgan:\n            self.loss_G_B = -pred_fake.mean()\n        else:\n            self.loss_G_B = self.criterionGAN(pred_fake, True)\n        # Forward cycle loss\n        \n        if lambda_A > 0:\n            self.rec_A = self.netG_B.forward(self.fake_B)\n            self.loss_cycle_A = self.criterionCycle(self.rec_A, self.real_A) * lambda_A\n        else:\n            self.loss_cycle_A = 0\n        # Backward cycle loss\n        \n         # = self.latent_fake_A + self.opt.skip * self.fake_A\n        if lambda_B > 0:\n            if self.opt.skip == 1:\n                self.rec_B, self.latent_fake_A = self.netG_A.forward(self.fake_A)\n            else:\n                self.rec_B = self.netG_A.forward(self.fake_A)\n            self.loss_cycle_B = self.criterionCycle(self.rec_B, self.real_B) * lambda_B\n        else:\n            self.loss_cycle_B = 0\n        self.loss_vgg_a = self.vgg_loss.compute_vgg_loss(self.vgg, self.fake_A, self.real_B) * self.opt.vgg if self.opt.vgg > 0 else 0\n        self.loss_vgg_b = self.vgg_loss.compute_vgg_loss(self.vgg, self.fake_B, self.real_A) * self.opt.vgg if self.opt.vgg > 0 else 0\n        # combined loss\n        self.loss_G = self.loss_G_A + self.loss_G_B + self.L1_AB + self.L1_BA + self.loss_cycle_A + self.loss_cycle_B + \\\n                        self.loss_vgg_a + self.loss_vgg_b + \\\n                        self.loss_idt_A + self.loss_idt_B\n        # self.loss_G = self.L1_AB + self.L1_BA\n        self.loss_G.backward()\n\n\n    def optimize_parameters(self):\n        # forward\n        self.forward()\n        # G_A and G_B\n        self.optimizer_G.zero_grad()\n        self.backward_G()\n        self.optimizer_G.step()\n        # D_A\n        self.optimizer_D_A.zero_grad()\n        self.backward_D_A()\n        self.optimizer_D_A.step()\n        # D_B\n        self.optimizer_D_B.zero_grad()\n        self.backward_D_B()\n        self.optimizer_D_B.step()\n\n\n    def get_current_errors(self):\n        D_A = self.loss_D_A.data[0]\n        G_A = self.loss_G_A.data[0]\n        L1 = (self.L1_AB + self.L1_BA).data[0]\n        Cyc_A = self.loss_cycle_A.data[0]\n        D_B = self.loss_D_B.data[0]\n        G_B = self.loss_G_B.data[0]\n        Cyc_B = self.loss_cycle_B.data[0]\n        vgg = (self.loss_vgg_a.data[0] + self.loss_vgg_b.data[0])/self.opt.vgg if self.opt.vgg > 0 else 0\n        if self.opt.identity > 0:\n            idt = self.loss_idt_A.data[0] + self.loss_idt_B.data[0]\n            if self.opt.lambda_A > 0.0:\n                return OrderedDict([(\'D_A\', D_A), (\'G_A\', G_A), (\'L1\', L1), (\'Cyc_A\', Cyc_A),\n                                    (\'D_B\', D_B), (\'G_B\', G_B), (\'Cyc_B\', Cyc_B), (""vgg"", vgg), (""idt"", idt)])\n            else:\n                return OrderedDict([(\'D_A\', D_A), (\'G_A\', G_A), (\'L1\', L1),\n                                    (\'D_B\', D_B), (\'G_B\', G_B)], (""vgg"", vgg), (""idt"", idt))\n        else:\n            if self.opt.lambda_A > 0.0:\n                return OrderedDict([(\'D_A\', D_A), (\'G_A\', G_A), (\'L1\', L1), (\'Cyc_A\', Cyc_A),\n                                    (\'D_B\', D_B), (\'G_B\', G_B), (\'Cyc_B\', Cyc_B), (""vgg"", vgg)])\n            else:\n                return OrderedDict([(\'D_A\', D_A), (\'G_A\', G_A), (\'L1\', L1),\n                                    (\'D_B\', D_B), (\'G_B\', G_B)], (""vgg"", vgg))\n\n    def get_current_visuals(self):\n        real_A = util.tensor2im(self.real_A.data)\n        fake_B = util.tensor2im(self.fake_B.data)\n        if self.opt.skip > 0:\n            latent_real_A = util.tensor2im(self.latent_real_A.data)\n        \n        real_B = util.tensor2im(self.real_B.data)\n        fake_A = util.tensor2im(self.fake_A.data)\n\n        if self.opt.identity > 0:\n            idt_A = util.tensor2im(self.idt_A.data)\n            idt_B = util.tensor2im(self.idt_B.data)\n            if self.opt.lambda_A > 0.0:\n                rec_A = util.tensor2im(self.rec_A.data)\n                rec_B = util.tensor2im(self.rec_B.data)\n                if self.opt.skip > 0:\n                    latent_fake_A = util.tensor2im(self.latent_fake_A.data)\n                    return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'latent_real_A\', latent_real_A), (\'rec_A\', rec_A), \n                                        (\'real_B\', real_B), (\'fake_A\', fake_A), (\'rec_B\', rec_B), (\'latent_fake_A\', latent_fake_A), \n                                        (""idt_A"", idt_A), (""idt_B"", idt_B)])\n                else:\n                    return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'rec_A\', rec_A), \n                                    (\'real_B\', real_B), (\'fake_A\', fake_A), (\'rec_B\', rec_B), (""idt_A"", idt_A), (""idt_B"", idt_B)])\n            else:\n                if self.opt.skip > 0:\n                    return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'latent_real_A\', latent_real_A), \n                                        (\'real_B\', real_B), (\'fake_A\', fake_A), (""idt_A"", idt_A), (""idt_B"", idt_B)])\n                else:\n                    return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B),\n                                        (\'real_B\', real_B), (\'fake_A\', fake_A), (""idt_A"", idt_A), (""idt_B"", idt_B)])\n        else:\n            if self.opt.lambda_A > 0.0:\n                rec_A = util.tensor2im(self.rec_A.data)\n                rec_B = util.tensor2im(self.rec_B.data)\n                if self.opt.skip > 0:\n                    latent_fake_A = util.tensor2im(self.latent_fake_A.data)\n                    return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'latent_real_A\', latent_real_A), (\'rec_A\', rec_A), \n                                        (\'real_B\', real_B), (\'fake_A\', fake_A), (\'rec_B\', rec_B), (\'latent_fake_A\', latent_fake_A)])\n                else:\n                    return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'rec_A\', rec_A), \n                                    (\'real_B\', real_B), (\'fake_A\', fake_A), (\'rec_B\', rec_B)])\n            else:\n                if self.opt.skip > 0:\n                    return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'latent_real_A\', latent_real_A), \n                                        (\'real_B\', real_B), (\'fake_A\', fake_A)])\n                else:\n                    return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B),\n                                        (\'real_B\', real_B), (\'fake_A\', fake_A)])\n\n    def save(self, label):\n        self.save_network(self.netG_A, \'G_A\', label, self.gpu_ids)\n        self.save_network(self.netD_A, \'D_A\', label, self.gpu_ids)\n        self.save_network(self.netG_B, \'G_B\', label, self.gpu_ids)\n        self.save_network(self.netD_B, \'D_B\', label, self.gpu_ids)\n\n    def update_learning_rate(self):\n        \n        if self.opt.new_lr:\n            lr = self.old_lr/2\n        else:\n            lrd = self.opt.lr / self.opt.niter_decay\n            lr = self.old_lr - lrd\n        for param_group in self.optimizer_D_A.param_groups:\n            param_group[\'lr\'] = lr\n        for param_group in self.optimizer_D_B.param_groups:\n            param_group[\'lr\'] = lr\n        for param_group in self.optimizer_G.param_groups:\n            param_group[\'lr\'] = lr\n\n        print(\'update learning rate: %f -> %f\' % (self.old_lr, lr))\n        self.old_lr = lr\n'"
models/pix2pix_model.py,7,"b""import numpy as np\nimport torch\nimport os\nfrom collections import OrderedDict\nfrom torch.autograd import Variable\nimport util.util as util\nfrom util.image_pool import ImagePool\nfrom .base_model import BaseModel\nfrom . import networks\n\n\nclass Pix2PixModel(BaseModel):\n    def name(self):\n        return 'Pix2PixModel'\n\n    def initialize(self, opt):\n        BaseModel.initialize(self, opt)\n        self.isTrain = opt.isTrain\n        # define tensors\n        self.input_A = self.Tensor(opt.batchSize, opt.input_nc,\n                                   opt.fineSize, opt.fineSize)\n        self.input_B = self.Tensor(opt.batchSize, opt.output_nc,\n                                   opt.fineSize, opt.fineSize)\n\n        # load/define networks\n        self.netG = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf,\n                                      opt.which_model_netG, opt.norm, not opt.no_dropout, self.gpu_ids)\n        if self.isTrain:\n            use_sigmoid = opt.no_lsgan\n            self.netD = networks.define_D(opt.input_nc + opt.output_nc, opt.ndf,\n                                          opt.which_model_netD,\n                                          opt.n_layers_D, opt.norm, use_sigmoid, self.gpu_ids)\n        if not self.isTrain or opt.continue_train:\n            self.load_network(self.netG, 'G', opt.which_epoch)\n            if self.isTrain:\n                self.load_network(self.netD, 'D', opt.which_epoch)\n\n        if self.isTrain:\n            self.fake_AB_pool = ImagePool(opt.pool_size)\n            self.old_lr = opt.lr\n            # define loss functions\n            self.criterionGAN = networks.GANLoss(use_lsgan=not opt.no_lsgan, tensor=self.Tensor)\n            self.criterionL1 = torch.nn.L1Loss()\n\n            # initialize optimizers\n            self.optimizer_G = torch.optim.Adam(self.netG.parameters(),\n                                                lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizer_D = torch.optim.Adam(self.netD.parameters(),\n                                                lr=opt.lr, betas=(opt.beta1, 0.999))\n\n        print('---------- Networks initialized -------------')\n        networks.print_network(self.netG)\n        if self.isTrain:\n            networks.print_network(self.netD)\n        print('-----------------------------------------------')\n\n    def set_input(self, input):\n        AtoB = self.opt.which_direction == 'AtoB'\n        input_A = input['A' if AtoB else 'B']\n        input_B = input['B' if AtoB else 'A']\n        self.input_A.resize_(input_A.size()).copy_(input_A)\n        self.input_B.resize_(input_B.size()).copy_(input_B)\n        self.image_paths = input['A_paths' if AtoB else 'B_paths']\n\n    def forward(self):\n        self.real_A = Variable(self.input_A)\n        self.fake_B = self.netG.forward(self.real_A)\n        self.real_B = Variable(self.input_B)\n\n    # no backprop gradients\n    def test(self):\n        self.real_A = Variable(self.input_A, volatile=True)\n        self.fake_B = self.netG.forward(self.real_A)\n        self.real_B = Variable(self.input_B, volatile=True)\n\n    # get image paths\n    def get_image_paths(self):\n        return self.image_paths\n\n    def backward_D(self):\n        # Fake\n        # stop backprop to the generator by detaching fake_B\n        fake_AB = self.fake_AB_pool.query(torch.cat((self.real_A, self.fake_B), 1))\n        self.pred_fake = self.netD.forward(fake_AB.detach())\n        self.loss_D_fake = self.criterionGAN(self.pred_fake, False)\n\n        # Real\n        real_AB = torch.cat((self.real_A, self.real_B), 1)\n        self.pred_real = self.netD.forward(real_AB)\n        self.loss_D_real = self.criterionGAN(self.pred_real, True)\n\n        # Combined loss\n        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n\n        self.loss_D.backward()\n\n    def backward_G(self):\n        # First, G(A) should fake the discriminator\n        fake_AB = torch.cat((self.real_A, self.fake_B), 1)\n        pred_fake = self.netD.forward(fake_AB)\n        self.loss_G_GAN = self.criterionGAN(pred_fake, True)\n\n        # Second, G(A) = B\n        self.loss_G_L1 = self.criterionL1(self.fake_B, self.real_B) * self.opt.lambda_A\n\n        self.loss_G = self.loss_G_GAN + self.loss_G_L1\n\n        self.loss_G.backward()\n\n    def optimize_parameters(self):\n        self.forward()\n\n        self.optimizer_D.zero_grad()\n        self.backward_D()\n        self.optimizer_D.step()\n\n        self.optimizer_G.zero_grad()\n        self.backward_G()\n        self.optimizer_G.step()\n\n    def get_current_errors(self):\n        return OrderedDict([('G_GAN', self.loss_G_GAN.data[0]),\n                            ('G_L1', self.loss_G_L1.data[0]),\n                            ('D_real', self.loss_D_real.data[0]),\n                            ('D_fake', self.loss_D_fake.data[0])\n                            ])\n\n    def get_current_visuals(self):\n        real_A = util.tensor2im(self.real_A.data)\n        fake_B = util.tensor2im(self.fake_B.data)\n        real_B = util.tensor2im(self.real_B.data)\n        return OrderedDict([('real_A', real_A), ('fake_B', fake_B), ('real_B', real_B)])\n\n    def save(self, label):\n        self.save_network(self.netG, 'G', label, self.gpu_ids)\n        self.save_network(self.netD, 'D', label, self.gpu_ids)\n\n    def update_learning_rate(self):\n        lrd = self.opt.lr / self.opt.niter_decay\n        lr = self.old_lr - lrd\n        for param_group in self.optimizer_D.param_groups:\n            param_group['lr'] = lr\n        for param_group in self.optimizer_G.param_groups:\n            param_group['lr'] = lr\n        print('update learning rate: %f -> %f' % (self.old_lr, lr))\n        self.old_lr = lr\n"""
models/single_model.py,23,"b'import numpy as np\nimport torch\nfrom torch import nn\nimport os\nfrom collections import OrderedDict\nfrom torch.autograd import Variable\nimport util.util as util\nfrom collections import OrderedDict\nfrom torch.autograd import Variable\nimport itertools\nimport util.util as util\nfrom util.image_pool import ImagePool\nfrom .base_model import BaseModel\nimport random\nfrom . import networks\nimport sys\n\n\nclass SingleModel(BaseModel):\n    def name(self):\n        return \'SingleGANModel\'\n\n    def initialize(self, opt):\n        BaseModel.initialize(self, opt)\n\n        nb = opt.batchSize\n        size = opt.fineSize\n        self.opt = opt\n        self.input_A = self.Tensor(nb, opt.input_nc, size, size)\n        self.input_B = self.Tensor(nb, opt.output_nc, size, size)\n        self.input_img = self.Tensor(nb, opt.input_nc, size, size)\n        self.input_A_gray = self.Tensor(nb, 1, size, size)\n\n        if opt.vgg > 0:\n            self.vgg_loss = networks.PerceptualLoss(opt)\n            if self.opt.IN_vgg:\n                self.vgg_patch_loss = networks.PerceptualLoss(opt)\n                self.vgg_patch_loss.cuda()\n            self.vgg_loss.cuda()\n            self.vgg = networks.load_vgg16(""./model"", self.gpu_ids)\n            self.vgg.eval()\n            for param in self.vgg.parameters():\n                param.requires_grad = False\n        elif opt.fcn > 0:\n            self.fcn_loss = networks.SemanticLoss(opt)\n            self.fcn_loss.cuda()\n            self.fcn = networks.load_fcn(""./model"")\n            self.fcn.eval()\n            for param in self.fcn.parameters():\n                param.requires_grad = False\n        # load/define networks\n        # The naming conversion is different from those used in the paper\n        # Code (paper): G_A (G), G_B (F), D_A (D_Y), D_B (D_X)\n\n        skip = True if opt.skip > 0 else False\n        self.netG_A = networks.define_G(opt.input_nc, opt.output_nc,\n                                        opt.ngf, opt.which_model_netG, opt.norm, not opt.no_dropout, self.gpu_ids, skip=skip, opt=opt)\n        # self.netG_B = networks.define_G(opt.output_nc, opt.input_nc,\n        #                                 opt.ngf, opt.which_model_netG, opt.norm, not opt.no_dropout, self.gpu_ids, skip=False, opt=opt)\n\n        if self.isTrain:\n            use_sigmoid = opt.no_lsgan\n            self.netD_A = networks.define_D(opt.output_nc, opt.ndf,\n                                            opt.which_model_netD,\n                                            opt.n_layers_D, opt.norm, use_sigmoid, self.gpu_ids, False)\n            if self.opt.patchD:\n                self.netD_P = networks.define_D(opt.input_nc, opt.ndf,\n                                            opt.which_model_netD,\n                                            opt.n_layers_patchD, opt.norm, use_sigmoid, self.gpu_ids, True)\n        if not self.isTrain or opt.continue_train:\n            which_epoch = opt.which_epoch\n            self.load_network(self.netG_A, \'G_A\', which_epoch)\n            # self.load_network(self.netG_B, \'G_B\', which_epoch)\n            if self.isTrain:\n                self.load_network(self.netD_A, \'D_A\', which_epoch)\n                if self.opt.patchD:\n                    self.load_network(self.netD_P, \'D_P\', which_epoch)\n\n        if self.isTrain:\n            self.old_lr = opt.lr\n            # self.fake_A_pool = ImagePool(opt.pool_size)\n            self.fake_B_pool = ImagePool(opt.pool_size)\n            # define loss functions\n            if opt.use_wgan:\n                self.criterionGAN = networks.DiscLossWGANGP()\n            else:\n                self.criterionGAN = networks.GANLoss(use_lsgan=not opt.no_lsgan, tensor=self.Tensor)\n            if opt.use_mse:\n                self.criterionCycle = torch.nn.MSELoss()\n            else:\n                self.criterionCycle = torch.nn.L1Loss()\n            self.criterionL1 = torch.nn.L1Loss()\n            self.criterionIdt = torch.nn.L1Loss()\n            # initialize optimizers\n            self.optimizer_G = torch.optim.Adam(self.netG_A.parameters(),\n                                                lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizer_D_A = torch.optim.Adam(self.netD_A.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n            if self.opt.patchD:\n                self.optimizer_D_P = torch.optim.Adam(self.netD_P.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n\n        print(\'---------- Networks initialized -------------\')\n        networks.print_network(self.netG_A)\n        # networks.print_network(self.netG_B)\n        if self.isTrain:\n            networks.print_network(self.netD_A)\n            if self.opt.patchD:\n                networks.print_network(self.netD_P)\n            # networks.print_network(self.netD_B)\n        if opt.isTrain:\n            self.netG_A.train()\n            # self.netG_B.train()\n        else:\n            self.netG_A.eval()\n            # self.netG_B.eval()\n        print(\'-----------------------------------------------\')\n\n    def set_input(self, input):\n        AtoB = self.opt.which_direction == \'AtoB\'\n        input_A = input[\'A\' if AtoB else \'B\']\n        input_B = input[\'B\' if AtoB else \'A\']\n        input_img = input[\'input_img\']\n        input_A_gray = input[\'A_gray\']\n        self.input_A.resize_(input_A.size()).copy_(input_A)\n        self.input_A_gray.resize_(input_A_gray.size()).copy_(input_A_gray)\n        self.input_B.resize_(input_B.size()).copy_(input_B)\n        self.input_img.resize_(input_img.size()).copy_(input_img)\n        self.image_paths = input[\'A_paths\' if AtoB else \'B_paths\']\n\n    \n\n\n    def test(self):\n        self.real_A = Variable(self.input_A, volatile=True)\n        self.real_A_gray = Variable(self.input_A_gray, volatile=True)\n        if self.opt.noise > 0:\n            self.noise = Variable(torch.cuda.FloatTensor(self.real_A.size()).normal_(mean=0, std=self.opt.noise/255.))\n            self.real_A = self.real_A + self.noise\n        if self.opt.input_linear:\n            self.real_A = (self.real_A - torch.min(self.real_A))/(torch.max(self.real_A) - torch.min(self.real_A))\n        # print(np.transpose(self.real_A.data[0].cpu().float().numpy(),(1,2,0))[:2][:2][:])\n        if self.opt.skip == 1:\n            self.fake_B, self.latent_real_A = self.netG_A.forward(self.real_A, self.real_A_gray)\n        else:\n            self.fake_B = self.netG_A.forward(self.real_A, self.real_A_gray)\n        # self.rec_A = self.netG_B.forward(self.fake_B)\n\n        self.real_B = Variable(self.input_B, volatile=True)\n\n\n    def predict(self):\n        self.real_A = Variable(self.input_A, volatile=True)\n        self.real_A_gray = Variable(self.input_A_gray, volatile=True)\n        if self.opt.noise > 0:\n            self.noise = Variable(torch.cuda.FloatTensor(self.real_A.size()).normal_(mean=0, std=self.opt.noise/255.))\n            self.real_A = self.real_A + self.noise\n        if self.opt.input_linear:\n            self.real_A = (self.real_A - torch.min(self.real_A))/(torch.max(self.real_A) - torch.min(self.real_A))\n        # print(np.transpose(self.real_A.data[0].cpu().float().numpy(),(1,2,0))[:2][:2][:])\n        if self.opt.skip == 1:\n            self.fake_B, self.latent_real_A = self.netG_A.forward(self.real_A, self.real_A_gray)\n        else:\n            self.fake_B = self.netG_A.forward(self.real_A, self.real_A_gray)\n        # self.rec_A = self.netG_B.forward(self.fake_B)\n\n        real_A = util.tensor2im(self.real_A.data)\n        fake_B = util.tensor2im(self.fake_B.data)\n        A_gray = util.atten2im(self.real_A_gray.data)\n        # rec_A = util.tensor2im(self.rec_A.data)\n        # if self.opt.skip == 1:\n        #     latent_real_A = util.tensor2im(self.latent_real_A.data)\n        #     latent_show = util.latent2im(self.latent_real_A.data)\n        #     max_image = util.max2im(self.fake_B.data, self.latent_real_A.data)\n        #     return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'latent_real_A\', latent_real_A),\n        #                     (\'latent_show\', latent_show), (\'max_image\', max_image), (\'A_gray\', A_gray)])\n        # else:\n        #     return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B)])\n        # return OrderedDict([(\'fake_B\', fake_B)])\n        return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B)])\n\n    # get image paths\n    def get_image_paths(self):\n        return self.image_paths\n\n    def backward_D_basic(self, netD, real, fake, use_ragan):\n        # Real\n        pred_real = netD.forward(real)\n        pred_fake = netD.forward(fake.detach())\n        if self.opt.use_wgan:\n            loss_D_real = pred_real.mean()\n            loss_D_fake = pred_fake.mean()\n            loss_D = loss_D_fake - loss_D_real + self.criterionGAN.calc_gradient_penalty(netD, \n                                                real.data, fake.data)\n        elif self.opt.use_ragan and use_ragan:\n            loss_D = (self.criterionGAN(pred_real - torch.mean(pred_fake), True) +\n                                      self.criterionGAN(pred_fake - torch.mean(pred_real), False)) / 2\n        else:\n            loss_D_real = self.criterionGAN(pred_real, True)\n            loss_D_fake = self.criterionGAN(pred_fake, False)\n            loss_D = (loss_D_real + loss_D_fake) * 0.5\n        # loss_D.backward()\n        return loss_D\n\n    def backward_D_A(self):\n        fake_B = self.fake_B_pool.query(self.fake_B)\n        fake_B = self.fake_B\n        self.loss_D_A = self.backward_D_basic(self.netD_A, self.real_B, fake_B, True)\n        self.loss_D_A.backward()\n    \n    def backward_D_P(self):\n        if self.opt.hybrid_loss:\n            loss_D_P = self.backward_D_basic(self.netD_P, self.real_patch, self.fake_patch, False)\n            if self.opt.patchD_3 > 0:\n                for i in range(self.opt.patchD_3):\n                    loss_D_P += self.backward_D_basic(self.netD_P, self.real_patch_1[i], self.fake_patch_1[i], False)\n                self.loss_D_P = loss_D_P/float(self.opt.patchD_3 + 1)\n            else:\n                self.loss_D_P = loss_D_P\n        else:\n            loss_D_P = self.backward_D_basic(self.netD_P, self.real_patch, self.fake_patch, True)\n            if self.opt.patchD_3 > 0:\n                for i in range(self.opt.patchD_3):\n                    loss_D_P += self.backward_D_basic(self.netD_P, self.real_patch_1[i], self.fake_patch_1[i], True)\n                self.loss_D_P = loss_D_P/float(self.opt.patchD_3 + 1)\n            else:\n                self.loss_D_P = loss_D_P\n        if self.opt.D_P_times2:\n            self.loss_D_P = self.loss_D_P*2\n        self.loss_D_P.backward()\n\n    # def backward_D_B(self):\n    #     fake_A = self.fake_A_pool.query(self.fake_A)\n    #     self.loss_D_B = self.backward_D_basic(self.netD_B, self.real_A, fake_A)\n    def forward(self):\n        self.real_A = Variable(self.input_A)\n        self.real_B = Variable(self.input_B)\n        self.real_A_gray = Variable(self.input_A_gray)\n        self.real_img = Variable(self.input_img)\n        if self.opt.noise > 0:\n            self.noise = Variable(torch.cuda.FloatTensor(self.real_A.size()).normal_(mean=0, std=self.opt.noise/255.))\n            self.real_A = self.real_A + self.noise\n        if self.opt.input_linear:\n            self.real_A = (self.real_A - torch.min(self.real_A))/(torch.max(self.real_A) - torch.min(self.real_A))\n        if self.opt.skip == 1:\n            self.fake_B, self.latent_real_A = self.netG_A.forward(self.real_img, self.real_A_gray)\n        else:\n            self.fake_B = self.netG_A.forward(self.real_img, self.real_A_gray)\n        if self.opt.patchD:\n            w = self.real_A.size(3)\n            h = self.real_A.size(2)\n            w_offset = random.randint(0, max(0, w - self.opt.patchSize - 1))\n            h_offset = random.randint(0, max(0, h - self.opt.patchSize - 1))\n\n            self.fake_patch = self.fake_B[:,:, h_offset:h_offset + self.opt.patchSize,\n                   w_offset:w_offset + self.opt.patchSize]\n            self.real_patch = self.real_B[:,:, h_offset:h_offset + self.opt.patchSize,\n                   w_offset:w_offset + self.opt.patchSize]\n            self.input_patch = self.real_A[:,:, h_offset:h_offset + self.opt.patchSize,\n                   w_offset:w_offset + self.opt.patchSize]\n        if self.opt.patchD_3 > 0:\n            self.fake_patch_1 = []\n            self.real_patch_1 = []\n            self.input_patch_1 = []\n            w = self.real_A.size(3)\n            h = self.real_A.size(2)\n            for i in range(self.opt.patchD_3):\n                w_offset_1 = random.randint(0, max(0, w - self.opt.patchSize - 1))\n                h_offset_1 = random.randint(0, max(0, h - self.opt.patchSize - 1))\n                self.fake_patch_1.append(self.fake_B[:,:, h_offset_1:h_offset_1 + self.opt.patchSize,\n                    w_offset_1:w_offset_1 + self.opt.patchSize])\n                self.real_patch_1.append(self.real_B[:,:, h_offset_1:h_offset_1 + self.opt.patchSize,\n                    w_offset_1:w_offset_1 + self.opt.patchSize])\n                self.input_patch_1.append(self.real_A[:,:, h_offset_1:h_offset_1 + self.opt.patchSize,\n                    w_offset_1:w_offset_1 + self.opt.patchSize])\n\n            # w_offset_2 = random.randint(0, max(0, w - self.opt.patchSize - 1))\n            # h_offset_2 = random.randint(0, max(0, h - self.opt.patchSize - 1))\n            # self.fake_patch_2 = self.fake_B[:,:, h_offset_2:h_offset_2 + self.opt.patchSize,\n            #        w_offset_2:w_offset_2 + self.opt.patchSize]\n            # self.real_patch_2 = self.real_B[:,:, h_offset_2:h_offset_2 + self.opt.patchSize,\n            #        w_offset_2:w_offset_2 + self.opt.patchSize]\n            # self.input_patch_2 = self.real_A[:,:, h_offset_2:h_offset_2 + self.opt.patchSize,\n            #        w_offset_2:w_offset_2 + self.opt.patchSize]\n\n    def backward_G(self, epoch):\n        pred_fake = self.netD_A.forward(self.fake_B)\n        if self.opt.use_wgan:\n            self.loss_G_A = -pred_fake.mean()\n        elif self.opt.use_ragan:\n            pred_real = self.netD_A.forward(self.real_B)\n\n            self.loss_G_A = (self.criterionGAN(pred_real - torch.mean(pred_fake), False) +\n                                      self.criterionGAN(pred_fake - torch.mean(pred_real), True)) / 2\n            \n        else:\n            self.loss_G_A = self.criterionGAN(pred_fake, True)\n        \n        loss_G_A = 0\n        if self.opt.patchD:\n            pred_fake_patch = self.netD_P.forward(self.fake_patch)\n            if self.opt.hybrid_loss:\n                loss_G_A += self.criterionGAN(pred_fake_patch, True)\n            else:\n                pred_real_patch = self.netD_P.forward(self.real_patch)\n                \n                loss_G_A += (self.criterionGAN(pred_real_patch - torch.mean(pred_fake_patch), False) +\n                                      self.criterionGAN(pred_fake_patch - torch.mean(pred_real_patch), True)) / 2\n        if self.opt.patchD_3 > 0:   \n            for i in range(self.opt.patchD_3):\n                pred_fake_patch_1 = self.netD_P.forward(self.fake_patch_1[i])\n                if self.opt.hybrid_loss:\n                    loss_G_A += self.criterionGAN(pred_fake_patch_1, True)\n                else:\n                    pred_real_patch_1 = self.netD_P.forward(self.real_patch_1[i])\n                    \n                    loss_G_A += (self.criterionGAN(pred_real_patch_1 - torch.mean(pred_fake_patch_1), False) +\n                                        self.criterionGAN(pred_fake_patch_1 - torch.mean(pred_real_patch_1), True)) / 2\n                    \n            if not self.opt.D_P_times2:\n                self.loss_G_A += loss_G_A/float(self.opt.patchD_3 + 1)\n            else:\n                self.loss_G_A += loss_G_A/float(self.opt.patchD_3 + 1)*2\n        else:\n            if not self.opt.D_P_times2:\n                self.loss_G_A += loss_G_A\n            else:\n                self.loss_G_A += loss_G_A*2\n                \n        if epoch < 0:\n            vgg_w = 0\n        else:\n            vgg_w = 1\n        if self.opt.vgg > 0:\n            self.loss_vgg_b = self.vgg_loss.compute_vgg_loss(self.vgg, \n                    self.fake_B, self.real_A) * self.opt.vgg if self.opt.vgg > 0 else 0\n            if self.opt.patch_vgg:\n                if not self.opt.IN_vgg:\n                    loss_vgg_patch = self.vgg_loss.compute_vgg_loss(self.vgg, \n                    self.fake_patch, self.input_patch) * self.opt.vgg\n                else:\n                    loss_vgg_patch = self.vgg_patch_loss.compute_vgg_loss(self.vgg, \n                    self.fake_patch, self.input_patch) * self.opt.vgg\n                if self.opt.patchD_3 > 0:\n                    for i in range(self.opt.patchD_3):\n                        if not self.opt.IN_vgg:\n                            loss_vgg_patch += self.vgg_loss.compute_vgg_loss(self.vgg, \n                                self.fake_patch_1[i], self.input_patch_1[i]) * self.opt.vgg\n                        else:\n                            loss_vgg_patch += self.vgg_patch_loss.compute_vgg_loss(self.vgg, \n                                self.fake_patch_1[i], self.input_patch_1[i]) * self.opt.vgg\n                    self.loss_vgg_b += loss_vgg_patch/float(self.opt.patchD_3 + 1)\n                else:\n                    self.loss_vgg_b += loss_vgg_patch\n            self.loss_G = self.loss_G_A + self.loss_vgg_b*vgg_w\n        elif self.opt.fcn > 0:\n            self.loss_fcn_b = self.fcn_loss.compute_fcn_loss(self.fcn, \n                    self.fake_B, self.real_A) * self.opt.fcn if self.opt.fcn > 0 else 0\n            if self.opt.patchD:\n                loss_fcn_patch = self.fcn_loss.compute_vgg_loss(self.fcn, \n                    self.fake_patch, self.input_patch) * self.opt.fcn\n                if self.opt.patchD_3 > 0:\n                    for i in range(self.opt.patchD_3):\n                        loss_fcn_patch += self.fcn_loss.compute_vgg_loss(self.fcn, \n                            self.fake_patch_1[i], self.input_patch_1[i]) * self.opt.fcn\n                    self.loss_fcn_b += loss_fcn_patch/float(self.opt.patchD_3 + 1)\n                else:\n                    self.loss_fcn_b += loss_fcn_patch\n            self.loss_G = self.loss_G_A + self.loss_fcn_b*vgg_w\n        # self.loss_G = self.L1_AB + self.L1_BA\n        self.loss_G.backward()\n\n\n    # def optimize_parameters(self, epoch):\n    #     # forward\n    #     self.forward()\n    #     # G_A and G_B\n    #     self.optimizer_G.zero_grad()\n    #     self.backward_G(epoch)\n    #     self.optimizer_G.step()\n    #     # D_A\n    #     self.optimizer_D_A.zero_grad()\n    #     self.backward_D_A()\n    #     self.optimizer_D_A.step()\n    #     if self.opt.patchD:\n    #         self.forward()\n    #         self.optimizer_D_P.zero_grad()\n    #         self.backward_D_P()\n    #         self.optimizer_D_P.step()\n        # D_B\n        # self.optimizer_D_B.zero_grad()\n        # self.backward_D_B()\n        # self.optimizer_D_B.step()\n    def optimize_parameters(self, epoch):\n        # forward\n        self.forward()\n        # G_A and G_B\n        self.optimizer_G.zero_grad()\n        self.backward_G(epoch)\n        self.optimizer_G.step()\n        # D_A\n        self.optimizer_D_A.zero_grad()\n        self.backward_D_A()\n        if not self.opt.patchD:\n            self.optimizer_D_A.step()\n        else:\n            # self.forward()\n            self.optimizer_D_P.zero_grad()\n            self.backward_D_P()\n            self.optimizer_D_A.step()\n            self.optimizer_D_P.step()\n\n\n    def get_current_errors(self, epoch):\n        D_A = self.loss_D_A.data[0]\n        D_P = self.loss_D_P.data[0] if self.opt.patchD else 0\n        G_A = self.loss_G_A.data[0]\n        if self.opt.vgg > 0:\n            vgg = self.loss_vgg_b.data[0]/self.opt.vgg if self.opt.vgg > 0 else 0\n            return OrderedDict([(\'D_A\', D_A), (\'G_A\', G_A), (""vgg"", vgg), (""D_P"", D_P)])\n        elif self.opt.fcn > 0:\n            fcn = self.loss_fcn_b.data[0]/self.opt.fcn if self.opt.fcn > 0 else 0\n            return OrderedDict([(\'D_A\', D_A), (\'G_A\', G_A), (""fcn"", fcn), (""D_P"", D_P)])\n        \n\n    def get_current_visuals(self):\n        real_A = util.tensor2im(self.real_A.data)\n        fake_B = util.tensor2im(self.fake_B.data)\n        real_B = util.tensor2im(self.real_B.data)\n        if self.opt.skip > 0:\n            latent_real_A = util.tensor2im(self.latent_real_A.data)\n            latent_show = util.latent2im(self.latent_real_A.data)\n            if self.opt.patchD:\n                fake_patch = util.tensor2im(self.fake_patch.data)\n                real_patch = util.tensor2im(self.real_patch.data)\n                if self.opt.patch_vgg:\n                    input_patch = util.tensor2im(self.input_patch.data)\n                    if not self.opt.self_attention:\n                        return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'latent_real_A\', latent_real_A),\n                                (\'latent_show\', latent_show), (\'real_B\', real_B), (\'real_patch\', real_patch),\n                                (\'fake_patch\', fake_patch), (\'input_patch\', input_patch)])\n                    else:\n                        self_attention = util.atten2im(self.real_A_gray.data)\n                        return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'latent_real_A\', latent_real_A),\n                                (\'latent_show\', latent_show), (\'real_B\', real_B), (\'real_patch\', real_patch),\n                                (\'fake_patch\', fake_patch), (\'input_patch\', input_patch), (\'self_attention\', self_attention)])\n                else:\n                    if not self.opt.self_attention:\n                        return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'latent_real_A\', latent_real_A),\n                                (\'latent_show\', latent_show), (\'real_B\', real_B), (\'real_patch\', real_patch),\n                                (\'fake_patch\', fake_patch)])\n                    else:\n                        self_attention = util.atten2im(self.real_A_gray.data)\n                        return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'latent_real_A\', latent_real_A),\n                                (\'latent_show\', latent_show), (\'real_B\', real_B), (\'real_patch\', real_patch),\n                                (\'fake_patch\', fake_patch), (\'self_attention\', self_attention)])\n            else:\n                if not self.opt.self_attention:\n                    return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'latent_real_A\', latent_real_A),\n                                (\'latent_show\', latent_show), (\'real_B\', real_B)])\n                else:\n                    self_attention = util.atten2im(self.real_A_gray.data)\n                    return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'real_B\', real_B),\n                                    (\'latent_real_A\', latent_real_A), (\'latent_show\', latent_show),\n                                    (\'self_attention\', self_attention)])\n        else:\n            if not self.opt.self_attention:\n                return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'real_B\', real_B)])\n            else:\n                self_attention = util.atten2im(self.real_A_gray.data)\n                return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'real_B\', real_B),\n                                    (\'self_attention\', self_attention)])\n\n    def save(self, label):\n        self.save_network(self.netG_A, \'G_A\', label, self.gpu_ids)\n        self.save_network(self.netD_A, \'D_A\', label, self.gpu_ids)\n        if self.opt.patchD:\n            self.save_network(self.netD_P, \'D_P\', label, self.gpu_ids)\n        # self.save_network(self.netG_B, \'G_B\', label, self.gpu_ids)\n        # self.save_network(self.netD_B, \'D_B\', label, self.gpu_ids)\n\n    def update_learning_rate(self):\n        \n        if self.opt.new_lr:\n            lr = self.old_lr/2\n        else:\n            lrd = self.opt.lr / self.opt.niter_decay\n            lr = self.old_lr - lrd\n        for param_group in self.optimizer_D_A.param_groups:\n            param_group[\'lr\'] = lr\n        if self.opt.patchD:\n            for param_group in self.optimizer_D_P.param_groups:\n                param_group[\'lr\'] = lr\n        for param_group in self.optimizer_G.param_groups:\n            param_group[\'lr\'] = lr\n\n        print(\'update learning rate: %f -> %f\' % (self.old_lr, lr))\n        self.old_lr = lr\n'"
models/temp_model.py,23,"b'import numpy as np\nimport torch\nfrom torch import nn\nimport os\nfrom collections import OrderedDict\nfrom torch.autograd import Variable\nimport util.util as util\nfrom collections import OrderedDict\nfrom torch.autograd import Variable\nimport itertools\nimport util.util as util\nfrom util.image_pool import ImagePool\nfrom .base_model import BaseModel\nimport random\nfrom . import networks\nimport sys\n\n\nclass TempModel(BaseModel):\n    def name(self):\n        return \'TempModel\'\n\n    def initialize(self, opt):\n        BaseModel.initialize(self, opt)\n\n        nb = opt.batchSize\n        size = opt.fineSize\n        self.opt = opt\n        self.input_A = self.Tensor(nb, opt.input_nc, size, size)\n        self.input_B = self.Tensor(nb, opt.output_nc, size, size)\n        self.input_img = self.Tensor(nb, opt.input_nc, size, size)\n        self.input_A_gray = self.Tensor(nb, 1, size, size)\n\n        if opt.vgg > 0:\n            self.vgg_loss = networks.PerceptualLoss(opt)\n            # if self.opt.IN_vgg:\n            #     self.vgg_patch_loss = networks.PerceptualLoss(opt)\n            #     self.vgg_patch_loss.cuda()\n            self.vgg_loss.cuda()\n            self.vgg = networks.load_vgg16(""./model"", self.gpu_ids)\n            self.vgg.eval()\n            for param in self.vgg.parameters():\n                param.requires_grad = False\n        elif opt.fcn > 0:\n            self.fcn_loss = networks.SemanticLoss(opt)\n            self.fcn_loss.cuda()\n            self.fcn = networks.load_fcn(""./model"")\n            self.fcn.eval()\n            for param in self.fcn.parameters():\n                param.requires_grad = False\n        # load/define networks\n        # The naming conversion is different from those used in the paper\n        # Code (paper): G_A (G), G_B (F), D_A (D_Y), D_B (D_X)\n\n        skip = True if opt.skip > 0 else False\n        self.netG_A = networks.define_G(opt.input_nc, opt.output_nc,\n                                        opt.ngf, opt.which_model_netG, opt.norm, not opt.no_dropout, self.gpu_ids, skip=skip, opt=opt)\n        # self.netG_B = networks.define_G(opt.output_nc, opt.input_nc,\n        #                                 opt.ngf, opt.which_model_netG, opt.norm, not opt.no_dropout, self.gpu_ids, skip=False, opt=opt)\n\n        if self.isTrain:\n            use_sigmoid = opt.no_lsgan\n            self.netD_A = networks.define_D(opt.output_nc, opt.ndf,\n                                            opt.which_model_netD,\n                                            opt.n_layers_D, opt.norm, use_sigmoid, self.gpu_ids, False)\n            if self.opt.patchD:\n                self.netD_P = networks.define_D(opt.input_nc, opt.ndf,\n                                            opt.which_model_netD,\n                                            opt.n_layers_patchD, opt.norm, use_sigmoid, self.gpu_ids, True)\n        if not self.isTrain or opt.continue_train:\n            which_epoch = opt.which_epoch\n            self.load_network(self.netG_A, \'G_A\', which_epoch)\n            # self.load_network(self.netG_B, \'G_B\', which_epoch)\n            if self.isTrain:\n                self.load_network(self.netD_A, \'D_A\', which_epoch)\n                if self.opt.patchD:\n                    self.load_network(self.netD_P, \'D_P\', which_epoch)\n\n        if self.isTrain:\n            self.old_lr = opt.lr\n            # self.fake_A_pool = ImagePool(opt.pool_size)\n            self.fake_B_pool = ImagePool(opt.pool_size)\n            # define loss functions\n            if opt.use_wgan:\n                self.criterionGAN = networks.DiscLossWGANGP()\n            else:\n                self.criterionGAN = networks.GANLoss(use_lsgan=not opt.no_lsgan, tensor=self.Tensor)\n            if opt.use_mse:\n                self.criterionCycle = torch.nn.MSELoss()\n            else:\n                self.criterionCycle = torch.nn.L1Loss()\n            self.criterionL1 = torch.nn.L1Loss()\n            self.criterionIdt = torch.nn.L1Loss()\n            # initialize optimizers\n            self.optimizer_G = torch.optim.Adam(self.netG_A.parameters(),\n                                                lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizer_D_A = torch.optim.Adam(self.netD_A.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n            # if self.opt.patchD:\n            #     self.optimizer_D_P = torch.optim.Adam(self.netD_P.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n\n        print(\'---------- Networks initialized -------------\')\n        networks.print_network(self.netG_A)\n        # networks.print_network(self.netG_B)\n        if self.isTrain:\n            networks.print_network(self.netD_A)\n            # if self.opt.patchD:\n            #     networks.print_network(self.netD_P)\n            # networks.print_network(self.netD_B)\n        if opt.isTrain:\n            self.netG_A.train()\n            # self.netG_B.train()\n        else:\n            self.netG_A.eval()\n            # self.netG_B.eval()\n        print(\'-----------------------------------------------\')\n\n    def set_input(self, input):\n        AtoB = self.opt.which_direction == \'AtoB\'\n        input_A = input[\'A\' if AtoB else \'B\']\n        input_B = input[\'B\' if AtoB else \'A\']\n        input_img = input[\'input_img\']\n        input_A_gray = input[\'A_gray\']\n        self.input_A.resize_(input_A.size()).copy_(input_A)\n        self.input_A_gray.resize_(input_A_gray.size()).copy_(input_A_gray)\n        self.input_B.resize_(input_B.size()).copy_(input_B)\n        self.input_img.resize_(input_img.size()).copy_(input_img)\n        self.image_paths = input[\'A_paths\' if AtoB else \'B_paths\']\n\n    \n\n\n    def test(self):\n        self.real_A = Variable(self.input_A, volatile=True)\n        self.real_A_gray = Variable(self.input_A_gray, volatile=True)\n        if self.opt.noise > 0:\n            self.noise = Variable(torch.cuda.FloatTensor(self.real_A.size()).normal_(mean=0, std=self.opt.noise/255.))\n            self.real_A = self.real_A + self.noise\n        if self.opt.input_linear:\n            self.real_A = (self.real_A - torch.min(self.real_A))/(torch.max(self.real_A) - torch.min(self.real_A))\n        # print(np.transpose(self.real_A.data[0].cpu().float().numpy(),(1,2,0))[:2][:2][:])\n        if self.opt.skip == 1:\n            self.fake_B, self.latent_real_A = self.netG_A.forward(self.real_A, self.real_A_gray)\n        else:\n            self.fake_B = self.netG_A.forward(self.real_A, self.real_A_gray)\n        # self.rec_A = self.netG_B.forward(self.fake_B)\n\n        self.real_B = Variable(self.input_B, volatile=True)\n\n\n    def predict(self):\n        self.real_A = Variable(self.input_A, volatile=True)\n        self.real_A_gray = Variable(self.input_A_gray, volatile=True)\n        # if self.opt.noise > 0:\n        #     self.noise = Variable(torch.cuda.FloatTensor(self.real_A.size()).normal_(mean=0, std=self.opt.noise/255.))\n        #     self.real_A = self.real_A + self.noise\n        # if self.opt.input_linear:\n        #     self.real_A = (self.real_A - torch.min(self.real_A))/(torch.max(self.real_A) - torch.min(self.real_A))\n        # print(np.transpose(self.real_A.data[0].cpu().float().numpy(),(1,2,0))[:2][:2][:])\n        if self.opt.skip == 1:\n            self.fake_B, self.latent_real_A = self.netG_A.forward(self.real_A, self.real_A_gray)\n        else:\n            self.fake_B = self.netG_A.forward(self.real_A, self.real_A_gray)\n        # self.rec_A = self.netG_B.forward(self.fake_B)\n\n        real_A = util.tensor2im(self.real_A.data)\n        fake_B = util.tensor2im(self.fake_B.data)\n        A_gray = util.atten2im(self.real_A_gray.data)\n        # rec_A = util.tensor2im(self.rec_A.data)\n        # if self.opt.skip == 1:\n        #     latent_real_A = util.tensor2im(self.latent_real_A.data)\n        #     latent_show = util.latent2im(self.latent_real_A.data)\n        #     max_image = util.max2im(self.fake_B.data, self.latent_real_A.data)\n        #     return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'latent_real_A\', latent_real_A),\n        #                     (\'latent_show\', latent_show), (\'max_image\', max_image), (\'A_gray\', A_gray)])\n        # else:\n        #     return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B)])\n        # return OrderedDict([(\'fake_B\', fake_B)])\n        return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B)])\n\n    # get image paths\n    def get_image_paths(self):\n        return self.image_paths\n\n    def backward_D_basic(self, netD, real, fake, use_ragan):\n        # Real\n        pred_real = netD.forward(real)\n        pred_fake = netD.forward(fake.detach())\n        if self.opt.use_wgan:\n            loss_D_real = pred_real.mean()\n            loss_D_fake = pred_fake.mean()\n            loss_D = loss_D_fake - loss_D_real + self.criterionGAN.calc_gradient_penalty(netD, \n                                                real.data, fake.data)\n        elif self.opt.use_ragan and use_ragan:\n            loss_D = (self.criterionGAN(pred_real - torch.mean(pred_fake), True) +\n                                      self.criterionGAN(pred_fake - torch.mean(pred_real), False)) / 2\n        else:\n            loss_D_real = self.criterionGAN(pred_real, True)\n            loss_D_fake = self.criterionGAN(pred_fake, False)\n            loss_D = (loss_D_real + loss_D_fake) * 0.5\n        # loss_D.backward()\n        return loss_D\n\n    def backward_D_A(self):\n        fake_B = self.fake_B_pool.query(self.fake_B)\n        fake_B = self.fake_B\n        self.loss_D_A = self.backward_D_basic(self.netD_A, self.real_B, fake_B, True)\n        self.loss_D_A.backward()\n    \n    def backward_D_P(self):\n        if self.opt.hybrid_loss:\n            loss_D_P = self.backward_D_basic(self.netD_P, self.real_patch, self.fake_patch, False)\n            if self.opt.patchD_3 > 0:\n                for i in range(self.opt.patchD_3):\n                    loss_D_P += self.backward_D_basic(self.netD_P, self.real_patch_1[i], self.fake_patch_1[i], False)\n                self.loss_D_P = loss_D_P/float(self.opt.patchD_3 + 1)\n            else:\n                self.loss_D_P = loss_D_P\n        else:\n            loss_D_P = self.backward_D_basic(self.netD_P, self.real_patch, self.fake_patch, True)\n            if self.opt.patchD_3 > 0:\n                for i in range(self.opt.patchD_3):\n                    loss_D_P += self.backward_D_basic(self.netD_P, self.real_patch_1[i], self.fake_patch_1[i], True)\n                self.loss_D_P = loss_D_P/float(self.opt.patchD_3 + 1)\n            else:\n                self.loss_D_P = loss_D_P\n        if self.opt.D_P_times2:\n            self.loss_D_P = self.loss_D_P*2\n        self.loss_D_P.backward()\n\n    # def backward_D_B(self):\n    #     fake_A = self.fake_A_pool.query(self.fake_A)\n    #     self.loss_D_B = self.backward_D_basic(self.netD_B, self.real_A, fake_A)\n    def forward(self):\n        self.real_A = Variable(self.input_A)\n        self.real_B = Variable(self.input_B)\n        self.real_A_gray = Variable(self.input_A_gray)\n        self.real_img = Variable(self.input_img)\n        if self.opt.noise > 0:\n            self.noise = Variable(torch.cuda.FloatTensor(self.real_A.size()).normal_(mean=0, std=self.opt.noise/255.))\n            self.real_A = self.real_A + self.noise\n        if self.opt.input_linear:\n            self.real_A = (self.real_A - torch.min(self.real_A))/(torch.max(self.real_A) - torch.min(self.real_A))\n        if self.opt.skip == 1:\n            self.fake_B, self.latent_real_A = self.netG_A.forward(self.real_img, self.real_A_gray)\n        else:\n            self.fake_B = self.netG_A.forward(self.real_img, self.real_A_gray)\n        if self.opt.patchD:\n            w = self.real_A.size(3)\n            h = self.real_A.size(2)\n            w_offset = random.randint(0, max(0, w - self.opt.patchSize - 1))\n            h_offset = random.randint(0, max(0, h - self.opt.patchSize - 1))\n\n            self.fake_patch = self.fake_B[:,:, h_offset:h_offset + self.opt.patchSize,\n                   w_offset:w_offset + self.opt.patchSize]\n            self.real_patch = self.real_B[:,:, h_offset:h_offset + self.opt.patchSize,\n                   w_offset:w_offset + self.opt.patchSize]\n            self.input_patch = self.real_A[:,:, h_offset:h_offset + self.opt.patchSize,\n                   w_offset:w_offset + self.opt.patchSize]\n        if self.opt.patchD_3 > 0:\n            self.fake_patch_1 = []\n            self.real_patch_1 = []\n            self.input_patch_1 = []\n            w = self.real_A.size(3)\n            h = self.real_A.size(2)\n            for i in range(self.opt.patchD_3):\n                w_offset_1 = random.randint(0, max(0, w - self.opt.patchSize - 1))\n                h_offset_1 = random.randint(0, max(0, h - self.opt.patchSize - 1))\n                self.fake_patch_1.append(self.fake_B[:,:, h_offset_1:h_offset_1 + self.opt.patchSize,\n                    w_offset_1:w_offset_1 + self.opt.patchSize])\n                self.real_patch_1.append(self.real_B[:,:, h_offset_1:h_offset_1 + self.opt.patchSize,\n                    w_offset_1:w_offset_1 + self.opt.patchSize])\n                self.input_patch_1.append(self.real_A[:,:, h_offset_1:h_offset_1 + self.opt.patchSize,\n                    w_offset_1:w_offset_1 + self.opt.patchSize])\n\n            w_offset_2 = random.randint(0, max(0, w - self.opt.patchSize - 1))\n            h_offset_2 = random.randint(0, max(0, h - self.opt.patchSize - 1))\n            self.fake_patch_2 = self.fake_B[:,:, h_offset_2:h_offset_2 + self.opt.patchSize,\n                   w_offset_2:w_offset_2 + self.opt.patchSize]\n            self.real_patch_2 = self.real_B[:,:, h_offset_2:h_offset_2 + self.opt.patchSize,\n                   w_offset_2:w_offset_2 + self.opt.patchSize]\n            self.input_patch_2 = self.real_A[:,:, h_offset_2:h_offset_2 + self.opt.patchSize,\n                   w_offset_2:w_offset_2 + self.opt.patchSize]\n\n    def backward_G(self, epoch):\n        pred_fake = self.netD_A.forward(self.fake_B)\n        if self.opt.use_wgan:\n            self.loss_G_A = -pred_fake.mean()\n        elif self.opt.use_ragan:\n            pred_real = self.netD_A.forward(self.real_B)\n\n            self.loss_G_A = (self.criterionGAN(pred_real - torch.mean(pred_fake), False) +\n                                      self.criterionGAN(pred_fake - torch.mean(pred_real), True)) / 2\n            \n        else:\n            self.loss_G_A = self.criterionGAN(pred_fake, True)\n        \n        loss_G_A = 0\n        if self.opt.patchD:\n            pred_fake_patch = self.netD_P.forward(self.fake_patch)\n            if self.opt.hybrid_loss:\n                loss_G_A += self.criterionGAN(pred_fake_patch, True)\n            else:\n                pred_real_patch = self.netD_P.forward(self.real_patch)\n                \n                loss_G_A += (self.criterionGAN(pred_real_patch - torch.mean(pred_fake_patch), False) +\n                                      self.criterionGAN(pred_fake_patch - torch.mean(pred_real_patch), True)) / 2\n            self.loss_G_A += loss_G_A\n        if self.opt.patchD_3 > 0:   \n            for i in range(self.opt.patchD_3):\n                pred_fake_patch_1 = self.netD_P.forward(self.fake_patch_1[i])\n                if self.opt.hybrid_loss:\n                    loss_G_A += self.criterionGAN(pred_fake_patch_1, True)\n                else:\n                    pred_real_patch_1 = self.netD_P.forward(self.real_patch_1[i])\n                    \n                    loss_G_A += (self.criterionGAN(pred_real_patch_1 - torch.mean(pred_fake_patch_1), False) +\n                                        self.criterionGAN(pred_fake_patch_1 - torch.mean(pred_real_patch_1), True)) / 2\n                    \n            if not self.opt.D_P_times2:\n                self.loss_G_A += loss_G_A/float(self.opt.patchD_3 + 1)\n            else:\n                self.loss_G_A += loss_G_A/float(self.opt.patchD_3 + 1)*2\n        else:\n            if not self.opt.D_P_times2:\n        self.loss_G_A += loss_G_A\n            else:\n                self.loss_G_A += loss_G_A*2\n                \n        if epoch < 0:\n            vgg_w = 0\n        else:\n            vgg_w = 1\n        if self.opt.vgg > 0:\n            self.loss_vgg_b = self.vgg_loss.compute_vgg_loss(self.vgg, \n                    self.fake_B, self.real_A) * self.opt.vgg if self.opt.vgg > 0 else 0\n            if self.opt.patch_vgg:\n                if not self.opt.IN_vgg:\n                    loss_vgg_patch = self.vgg_loss.compute_vgg_loss(self.vgg, \n                    self.fake_patch, self.input_patch) * self.opt.vgg\n                else:\n                    loss_vgg_patch = self.vgg_patch_loss.compute_vgg_loss(self.vgg, \n                    self.fake_patch, self.input_patch) * self.opt.vgg\n                if self.opt.patchD_3 > 0:\n                    for i in range(self.opt.patchD_3):\n                        if not self.opt.IN_vgg:\n                            loss_vgg_patch += self.vgg_loss.compute_vgg_loss(self.vgg, \n                                self.fake_patch_1[i], self.input_patch_1[i]) * self.opt.vgg\n                        else:\n                            loss_vgg_patch += self.vgg_patch_loss.compute_vgg_loss(self.vgg, \n                                self.fake_patch_1[i], self.input_patch_1[i]) * self.opt.vgg\n                    self.loss_vgg_b += loss_vgg_patch/float(self.opt.patchD_3 + 1)\n                else:\n                    self.loss_vgg_b += loss_vgg_patch\n            self.loss_G = self.loss_G_A + self.loss_vgg_b*vgg_w\n        elif self.opt.fcn > 0:\n            self.loss_fcn_b = self.fcn_loss.compute_fcn_loss(self.fcn, \n                    self.fake_B, self.real_A) * self.opt.fcn if self.opt.fcn > 0 else 0\n            if self.opt.patchD:\n                loss_fcn_patch = self.fcn_loss.compute_vgg_loss(self.fcn, \n                    self.fake_patch, self.input_patch) * self.opt.fcn\n                if self.opt.patchD_3 > 0:\n                    for i in range(self.opt.patchD_3):\n                        loss_fcn_patch += self.fcn_loss.compute_vgg_loss(self.fcn, \n                            self.fake_patch_1[i], self.input_patch_1[i]) * self.opt.fcn\n                    self.loss_fcn_b += loss_fcn_patch/float(self.opt.patchD_3 + 1)\n                else:\n                    self.loss_fcn_b += loss_fcn_patch\n            self.loss_G = self.loss_G_A + self.loss_fcn_b*vgg_w\n        # self.loss_G = self.L1_AB + self.L1_BA\n        self.loss_G.backward()\n\n\n    # def optimize_parameters(self, epoch):\n    #     # forward\n    #     self.forward()\n    #     # G_A and G_B\n    #     self.optimizer_G.zero_grad()\n    #     self.backward_G(epoch)\n    #     self.optimizer_G.step()\n    #     # D_A\n    #     self.optimizer_D_A.zero_grad()\n    #     self.backward_D_A()\n    #     self.optimizer_D_A.step()\n    #     if self.opt.patchD:\n    #         self.forward()\n    #         self.optimizer_D_P.zero_grad()\n    #         self.backward_D_P()\n    #         self.optimizer_D_P.step()\n        # D_B\n        # self.optimizer_D_B.zero_grad()\n        # self.backward_D_B()\n        # self.optimizer_D_B.step()\n    def optimize_parameters(self, epoch):\n        # forward\n        self.forward()\n        # G_A and G_B\n        self.optimizer_G.zero_grad()\n        self.backward_G(epoch)\n        self.optimizer_G.step()\n        # D_A\n        self.optimizer_D_A.zero_grad()\n        self.backward_D_A()\n        if not self.opt.patchD:\n            self.optimizer_D_A.step()\n        else:\n            # self.forward()\n            self.optimizer_D_P.zero_grad()\n            self.backward_D_P()\n            self.optimizer_D_A.step()\n            self.optimizer_D_P.step()\n\n\n    def get_current_errors(self, epoch):\n        D_A = self.loss_D_A.data[0]\n        D_P = self.loss_D_P.data[0] if self.opt.patchD else 0\n        G_A = self.loss_G_A.data[0]\n        if self.opt.vgg > 0:\n            vgg = self.loss_vgg_b.data[0]/self.opt.vgg if self.opt.vgg > 0 else 0\n            return OrderedDict([(\'D_A\', D_A), (\'G_A\', G_A), (""vgg"", vgg)])\n        elif self.opt.fcn > 0:\n            fcn = self.loss_fcn_b.data[0]/self.opt.fcn if self.opt.fcn > 0 else 0\n            return OrderedDict([(\'D_A\', D_A), (\'G_A\', G_A), (""fcn"", fcn)])\n        \n\n    def get_current_visuals(self):\n        real_A = util.tensor2im(self.real_A.data)\n        fake_B = util.tensor2im(self.fake_B.data)\n        real_B = util.tensor2im(self.real_B.data)\n        if self.opt.skip > 0:\n            latent_real_A = util.tensor2im(self.latent_real_A.data)\n            latent_show = util.latent2im(self.latent_real_A.data)\n            return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'latent_real_A\', latent_real_A),\n                                (\'latent_show\', latent_show), (\'real_B\', real_B)])\n        #     if self.opt.patchD:\n        #         fake_patch = util.tensor2im(self.fake_patch.data)\n        #         real_patch = util.tensor2im(self.real_patch.data)\n        #         if self.opt.patch_vgg:\n        #             input_patch = util.tensor2im(self.input_patch.data)\n        #             if not self.opt.self_attention:\n        #                 return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'latent_real_A\', latent_real_A),\n        #                         (\'latent_show\', latent_show), (\'real_B\', real_B), (\'real_patch\', real_patch),\n        #                         (\'fake_patch\', fake_patch), (\'input_patch\', input_patch)])\n        #             else:\n        #                 self_attention = util.atten2im(self.real_A_gray.data)\n        #                 return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'latent_real_A\', latent_real_A),\n        #                         (\'latent_show\', latent_show), (\'real_B\', real_B), (\'real_patch\', real_patch),\n        #                         (\'fake_patch\', fake_patch), (\'input_patch\', input_patch), (\'self_attention\', self_attention)])\n        #         else:\n        #             if not self.opt.self_attention:\n        #                 return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'latent_real_A\', latent_real_A),\n        #                         (\'latent_show\', latent_show), (\'real_B\', real_B), (\'real_patch\', real_patch),\n        #                         (\'fake_patch\', fake_patch)])\n        #             else:\n        #                 self_attention = util.atten2im(self.real_A_gray.data)\n        #                 return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'latent_real_A\', latent_real_A),\n        #                         (\'latent_show\', latent_show), (\'real_B\', real_B), (\'real_patch\', real_patch),\n        #                         (\'fake_patch\', fake_patch), (\'self_attention\', self_attention)])\n        #     else:\n        #         if not self.opt.self_attention:\n        #             return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'latent_real_A\', latent_real_A),\n        #                         (\'latent_show\', latent_show), (\'real_B\', real_B)])\n        #         else:\n        #             self_attention = util.atten2im(self.real_A_gray.data)\n        #             return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'real_B\', real_B),\n        #                             (\'latent_real_A\', latent_real_A), (\'latent_show\', latent_show),\n        #                             (\'self_attention\', self_attention)])\n        # else:\n        #     if not self.opt.self_attention:\n        #         return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'real_B\', real_B)])\n        #     else:\n        #         self_attention = util.atten2im(self.real_A_gray.data)\n        #         return OrderedDict([(\'real_A\', real_A), (\'fake_B\', fake_B), (\'real_B\', real_B),\n        #                             (\'self_attention\', self_attention)])\n\n    def save(self, label):\n        self.save_network(self.netG_A, \'G_A\', label, self.gpu_ids)\n        self.save_network(self.netD_A, \'D_A\', label, self.gpu_ids)\n        if self.opt.patchD:\n            self.save_network(self.netD_P, \'D_P\', label, self.gpu_ids)\n        # self.save_network(self.netG_B, \'G_B\', label, self.gpu_ids)\n        # self.save_network(self.netD_B, \'D_B\', label, self.gpu_ids)\n\n    def update_learning_rate(self):\n        \n        if self.opt.new_lr:\n            lr = self.old_lr/2\n        else:\n            lrd = self.opt.lr / self.opt.niter_decay\n            lr = self.old_lr - lrd\n        for param_group in self.optimizer_D_A.param_groups:\n            param_group[\'lr\'] = lr\n        if self.opt.patchD:\n            for param_group in self.optimizer_D_P.param_groups:\n                param_group[\'lr\'] = lr\n        for param_group in self.optimizer_G.param_groups:\n            param_group[\'lr\'] = lr\n\n        print(\'update learning rate: %f -> %f\' % (self.old_lr, lr))\n        self.old_lr = lr\n'"
models/test_model.py,1,"b""from torch.autograd import Variable\nfrom collections import OrderedDict\nimport util.util as util\nfrom .base_model import BaseModel\nfrom . import networks\n\n\nclass TestModel(BaseModel):\n    def name(self):\n        return 'TestModel'\n\n    def initialize(self, opt):\n        assert(not opt.isTrain)\n        BaseModel.initialize(self, opt)\n        self.input_A = self.Tensor(opt.batchSize, opt.input_nc, opt.fineSize, opt.fineSize)\n\n        self.netG = networks.define_G(opt.input_nc, opt.output_nc,\n                                      opt.ngf, opt.which_model_netG,\n                                      opt.norm, not opt.no_dropout,\n                                      self.gpu_ids)\n        which_epoch = opt.which_epoch\n        self.load_network(self.netG, 'G', which_epoch)\n\n        print('---------- Networks initialized -------------')\n        networks.print_network(self.netG)\n        print('-----------------------------------------------')\n\n    def set_input(self, input):\n        # we need to use single_dataset mode\n        input_A = input['A']\n        self.input_A.resize_(input_A.size()).copy_(input_A)\n        self.image_paths = input['A_paths']\n\n    def test(self):\n        self.real_A = Variable(self.input_A)\n        self.fake_B = self.netG.forward(self.real_A)\n\n    # get image paths\n    def get_image_paths(self):\n        return self.image_paths\n\n    def get_current_visuals(self):\n        real_A = util.tensor2im(self.real_A.data)\n        fake_B = util.tensor2im(self.fake_B.data)\n        return OrderedDict([('real_A', real_A), ('fake_B', fake_B)])\n"""
models/unit_model.py,10,"b""import numpy as np\nimport torch\nimport os\nfrom collections import OrderedDict\nfrom torch.autograd import Variable\nimport itertools\nimport util.util as util\nfrom util.util import weights_init, get_model_list, vgg_preprocess, load_vgg16, get_scheduler\nfrom util.image_pool import ImagePool\nfrom .base_model import BaseModel\nfrom . import networks\nfrom .unit_network import *\nimport sys\n\ndef get_config(config):\n    import yaml\n    with open(config, 'r') as stream:\n        return yaml.load(stream)\n\nclass UNITModel(BaseModel):\n    def name(self):\n        return 'UNITModel'\n\n    def initialize(self, opt):\n        BaseModel.initialize(self, opt)\n        \n        self.config = get_config(opt.config)\n        nb = opt.batchSize\n        size = opt.fineSize\n        self.input_A = self.Tensor(nb, opt.input_nc, size, size)\n        self.input_B = self.Tensor(nb, opt.output_nc, size, size)\n\n        # load/define networks\n        # The naming conversion is different from those used in the paper\n        # Code (paper): G_A (G), G_B (F), D_A (D_Y), D_B (D_X)\n\n        self.gen_a = VAEGen(self.config['input_dim_a'], self.config['gen'])\n        self.gen_b = VAEGen(self.config['input_dim_a'], self.config['gen'])\n\n        if self.isTrain:\n            self.dis_a = MsImageDis(self.config['input_dim_a'], self.config['dis'])  # discriminator for domain a\n            self.dis_b = MsImageDis(self.config['input_dim_b'], self.config['dis'])  # discriminator for domain b\n        if not self.isTrain or opt.continue_train:\n            which_epoch = opt.which_epoch\n            self.load_network(self.gen_a, 'G_A', which_epoch)\n            self.load_network(self.gen_b, 'G_B', which_epoch)\n            if self.isTrain:\n                self.load_network(self.dis_a, 'D_A', which_epoch)\n                self.load_network(self.dis_b, 'D_B', which_epoch)\n\n        if self.isTrain:\n            self.old_lr = self.config['lr']\n            self.fake_A_pool = ImagePool(opt.pool_size)\n            self.fake_B_pool = ImagePool(opt.pool_size)\n            # define loss functions\n            # Setup the optimizers\n            beta1 = self.config['beta1']\n            beta2 = self.config['beta2']\n            dis_params = list(self.dis_a.parameters()) + list(self.dis_b.parameters())\n            gen_params = list(self.gen_a.parameters()) + list(self.gen_b.parameters())\n            self.dis_opt = torch.optim.Adam([p for p in dis_params if p.requires_grad],\n                                            lr=self.config['lr'], betas=(beta1, beta2), weight_decay=self.config['weight_decay'])\n            self.gen_opt = torch.optim.Adam([p for p in gen_params if p.requires_grad],\n                                            lr=self.config['lr'], betas=(beta1, beta2), weight_decay=self.config['weight_decay'])\n            self.dis_scheduler = get_scheduler(self.dis_opt, self.config)\n            self.gen_scheduler = get_scheduler(self.gen_opt, self.config)\n\n            # Network weight initialization\n            # self.apply(weights_init(self.config['init']))\n            self.dis_a.apply(weights_init('gaussian'))\n            self.dis_b.apply(weights_init('gaussian'))\n\n            # Load VGG model if needed\n            if 'vgg_w' in self.config.keys() and self.config['vgg_w'] > 0:\n                self.vgg = load_vgg16(self.config['vgg_model_path'] + '/models')\n                self.vgg.eval()\n                for param in self.vgg.parameters():\n                    param.requires_grad = False\n        self.gen_a.cuda()\n        self.gen_b.cuda()\n        self.dis_a.cuda()\n        self.dis_b.cuda()\n\n        print('---------- Networks initialized -------------')\n        networks.print_network(self.gen_a)\n        networks.print_network(self.gen_b)\n        if self.isTrain:\n            networks.print_network(self.dis_a)\n            networks.print_network(self.dis_b)\n        print('-----------------------------------------------')\n\n    def set_input(self, input):\n        AtoB = self.opt.which_direction == 'AtoB'\n        input_A = input['A' if AtoB else 'B']\n        input_B = input['B' if AtoB else 'A']\n        self.input_A.resize_(input_A.size()).copy_(input_A)\n        self.input_B.resize_(input_B.size()).copy_(input_B)\n        self.image_paths = input['A_paths' if AtoB else 'B_paths']\n        self.real_A = Variable(self.input_A.cuda())\n        self.real_B = Variable(self.input_B.cuda())\n\n    # def forward(self):\n        # self.real_A = Variable(self.input_A)\n        # self.real_B = Variable(self.input_B)\n\n    def test(self):\n        self.real_A = Variable(self.input_A.cuda(), volatile=True)\n        self.real_B = Variable(self.input_B.cuda(), volatile=True)\n        h_a, n_a = self.gen_a.encode(self.real_A)\n        h_b, n_b = self.gen_b.encode(self.real_B)\n        x_a_recon = self.gen_a.decode(h_a + n_a) + x_a*1\n        x_b_recon = self.gen_b.decode(h_b + n_b) + x_b*1\n        x_ba = self.gen_a.decode(h_b + n_b) + x_b*1\n        x_ab = self.gen_b.decode(h_a + n_a) + x_a*1\n        h_b_recon, n_b_recon = self.gen_a.encode(x_ba)\n        h_a_recon, n_a_recon = self.gen_b.encode(x_ab)\n        x_aba = self.gen_a.decode(h_a_recon + n_a_recon) + x_ab*1 if self.config['recon_x_cyc_w'] > 0 else None\n        x_bab = self.gen_b.decode(h_b_recon + n_b_recon) + x_ba*1 if self.config['recon_x_cyc_w'] > 0 else None\n        self.x_a_recon, self.x_ab, self.x_aba = x_a_recon, x_ab, x_aba\n        self.x_b_recon, self.x_ba, self.x_bab = x_b_recon, x_ba, x_bab\n\n    # get image paths\n    def get_image_paths(self):\n        return self.image_paths\n\n\n    def optimize_parameters(self):\n        self.gen_update(self.real_A, self.real_B)\n        self.dis_update(self.real_A, self.real_B)\n\n    def recon_criterion(self, input, target):\n        return torch.mean(torch.abs(input - target))\n\n    def forward(self, x_a, x_b):\n        self.eval()\n        x_a.volatile = True\n        x_b.volatile = True\n        h_a, _ = self.gen_a.encode(x_a)\n        h_b, _ = self.gen_b.encode(x_b)\n        x_ba = self.gen_a.decode(h_b)\n        x_ab = self.gen_b.decode(h_a)\n        self.train()\n        return x_ab, x_ba\n\n    def __compute_kl(self, mu):\n        # def _compute_kl(self, mu, sd):\n        # mu_2 = torch.pow(mu, 2)\n        # sd_2 = torch.pow(sd, 2)\n        # encoding_loss = (mu_2 + sd_2 - torch.log(sd_2)).sum() / mu_2.size(0)\n        # return encoding_loss\n        mu_2 = torch.pow(mu, 2)\n        encoding_loss = torch.mean(mu_2)\n        return encoding_loss\n\n    def gen_update(self, x_a, x_b):\n        self.gen_opt.zero_grad()\n        # encode\n        h_a, n_a = self.gen_a.encode(x_a)\n        h_b, n_b = self.gen_b.encode(x_b)\n        # decode (within domain)\n        x_a_recon = self.gen_a.decode(h_a + n_a) + 0*x_a\n        x_b_recon = self.gen_b.decode(h_b + n_b) + 0*x_b\n        # decode (cross domain)\n        x_ba = self.gen_a.decode(h_b + n_b) + 0*x_b\n        x_ab = self.gen_b.decode(h_a + n_a) + 0*x_a\n        # encode again\n        h_b_recon, n_b_recon = self.gen_a.encode(x_ba)\n        h_a_recon, n_a_recon = self.gen_b.encode(x_ab)\n        # decode again (if needed)\n        x_aba = self.gen_a.decode(h_a_recon + n_a_recon) +  0*x_ab if self.config['recon_x_cyc_w'] > 0 else None\n        x_bab = self.gen_b.decode(h_b_recon + n_b_recon) + 0*x_ba if self.config['recon_x_cyc_w'] > 0 else None\n\n        # reconstruction loss\n        self.loss_gen_recon_x_a = self.recon_criterion(x_a_recon, x_a)\n        self.loss_gen_recon_x_b = self.recon_criterion(x_b_recon, x_b)\n        self.loss_gen_recon_kl_a = self.__compute_kl(h_a)\n        self.loss_gen_recon_kl_b = self.__compute_kl(h_b)\n        self.loss_gen_cyc_x_a = self.recon_criterion(x_aba, x_a)\n        self.loss_gen_cyc_x_b = self.recon_criterion(x_bab, x_b)\n        self.loss_gen_recon_kl_cyc_aba = self.__compute_kl(h_a_recon)\n        self.loss_gen_recon_kl_cyc_bab = self.__compute_kl(h_b_recon)\n        # GAN loss\n        self.loss_gen_adv_a = self.dis_a.calc_gen_loss(x_ba)\n        self.loss_gen_adv_b = self.dis_b.calc_gen_loss(x_ab)\n        # domain-invariant perceptual loss\n        self.loss_gen_vgg_a = self.compute_vgg_loss(self.vgg, x_ba, x_b) if self.config['vgg_w'] > 0 else 0\n        self.loss_gen_vgg_b = self.compute_vgg_loss(self.vgg, x_ab, x_a) if self.config['vgg_w'] > 0 else 0\n        # total loss\n        self.loss_gen_total = self.config['gan_w'] * self.loss_gen_adv_a + \\\n                              self.config['gan_w'] * self.loss_gen_adv_b + \\\n                              self.config['recon_x_w'] * self.loss_gen_recon_x_a + \\\n                              self.config['recon_kl_w'] * self.loss_gen_recon_kl_a + \\\n                              self.config['recon_x_w'] * self.loss_gen_recon_x_b + \\\n                              self.config['recon_kl_w'] * self.loss_gen_recon_kl_b + \\\n                              self.config['recon_x_cyc_w'] * self.loss_gen_cyc_x_a + \\\n                              self.config['recon_kl_cyc_w'] * self.loss_gen_recon_kl_cyc_aba + \\\n                              self.config['recon_x_cyc_w'] * self.loss_gen_cyc_x_b + \\\n                              self.config['recon_kl_cyc_w'] * self.loss_gen_recon_kl_cyc_bab + \\\n                              self.config['vgg_w'] * self.loss_gen_vgg_a + \\\n                              self.config['vgg_w'] * self.loss_gen_vgg_b\n        self.loss_gen_total.backward()\n        self.gen_opt.step()\n        self.x_a_recon, self.x_ab, self.x_aba = x_a_recon, x_ab, x_aba\n        self.x_b_recon, self.x_ba, self.x_bab = x_b_recon, x_ba, x_bab\n\n    def compute_vgg_loss(self, vgg, img, target):\n        img_vgg = vgg_preprocess(img)\n        target_vgg = vgg_preprocess(target)\n        img_fea = vgg(img_vgg)\n        target_fea = vgg(target_vgg)\n        return torch.mean((self.instancenorm(img_fea) - self.instancenorm(target_fea)) ** 2)\n\n    def dis_update(self, x_a, x_b):\n        self.dis_opt.zero_grad()\n        # encode\n        h_a, n_a = self.gen_a.encode(x_a)\n        h_b, n_b = self.gen_b.encode(x_b)\n        # decode (cross domain)\n        x_ba = self.gen_a.decode(h_b + n_b)\n        x_ab = self.gen_b.decode(h_a + n_a)\n        # D loss\n        self.loss_dis_a = self.dis_a.calc_dis_loss(x_ba.detach(), x_a)\n        self.loss_dis_b = self.dis_b.calc_dis_loss(x_ab.detach(), x_b)\n        self.loss_dis_total = self.config['gan_w'] * self.loss_dis_a + self.config['gan_w'] * self.loss_dis_b\n        self.loss_dis_total.backward()\n        self.dis_opt.step()\n\n    def get_current_errors(self):\n        D_A = self.loss_dis_a.data[0]\n        G_A = self.loss_gen_adv_a.data[0]\n        kl_A = self.loss_gen_recon_kl_a.data[0]\n        Cyc_A = self.loss_gen_cyc_x_a.data[0]\n        D_B = self.loss_dis_b.data[0]\n        G_B = self.loss_gen_adv_b.data[0]\n        kl_B = self.loss_gen_recon_kl_b.data[0]\n        Cyc_B = self.loss_gen_cyc_x_b.data[0]\n        if self.config['vgg_w'] > 0:\n            vgg_A = self.loss_gen_vgg_a\n            vgg_B = self.loss_gen_vgg_b\n            return OrderedDict([('D_A', D_A), ('G_A', G_A), ('Cyc_A', Cyc_A), ('kl_A', kl_A), ('vgg_A', vgg_A),\n                                ('D_B', D_B), ('G_B', G_B), ('Cyc_B', Cyc_B), ('kl_B', kl_B), ('vgg_B', vgg_B)])\n        else:\n            return OrderedDict([('D_A', D_A), ('G_A', G_A), ('kl_A', kl_A), ('Cyc_A', Cyc_A), \n                                ('D_B', D_B), ('G_B', G_B), ('kl_B', kl_B), ('Cyc_B', Cyc_B)])\n\n    def get_current_visuals(self):\n        real_A = util.tensor2im(self.real_A.data)\n        recon_A = util.tensor2im(self.x_a_recon.data)\n        A_B = util.tensor2im(self.x_ab.data)\n        ABA = util.tensor2im(self.x_aba.data)\n        real_B = util.tensor2im(self.real_B.data)\n        recon_B = util.tensor2im(self.x_b_recon.data)\n        B_A = util.tensor2im(self.x_ba.data)\n        BAB = util.tensor2im(self.x_b_recon.data)\n        return OrderedDict([('real_A', real_A), ('A_B', A_B), ('recon_A', recon_A), ('ABA', ABA),\n                            ('real_B', real_B), ('B_A', B_A), ('recon_B', recon_B), ('BAB', BAB)])\n\n    def save(self, label):\n        self.save_network(self.gen_a, 'G_A', label, self.gpu_ids)\n        self.save_network(self.dis_a, 'D_A', label, self.gpu_ids)\n        self.save_network(self.gen_b, 'G_B', label, self.gpu_ids)\n        self.save_network(self.dis_b, 'D_B', label, self.gpu_ids)\n\n    def update_learning_rate(self):\n        lrd = self.config['lr'] / self.opt.niter_decay\n        lr = self.old_lr - lrd\n        for param_group in self.gen_a.param_groups:\n            param_group['lr'] = lr\n        for param_group in self.gen_b.param_groups:\n            param_group['lr'] = lr\n        for param_group in self.dis_a.param_groups:\n            param_group['lr'] = lr\n        for param_group in self.dis_b.param_groups:\n            param_group['lr'] = lr\n\n        print('update learning rate: %f -> %f' % (self.old_lr, lr))\n        self.old_lr = lr"""
models/unit_network.py,15,"b'""""""\nCopyright (C) 2018 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\nfrom torch import nn\nfrom torch.autograd import Variable\nimport torch\nimport torch.nn.functional as F\ntry:\n    from itertools import izip as zip\nexcept ImportError: # will be 3.x series\n    pass\n\n##################################################################################\n# Discriminator\n##################################################################################\n\nclass MsImageDis(nn.Module):\n    # Multi-scale discriminator architecture\n    def __init__(self, input_dim, params):\n        super(MsImageDis, self).__init__()\n        self.n_layer = params[\'n_layer\']\n        self.gan_type = params[\'gan_type\']\n        self.dim = params[\'dim\']\n        self.norm = params[\'norm\']\n        self.activ = params[\'activ\']\n        self.num_scales = params[\'num_scales\']\n        self.pad_type = params[\'pad_type\']\n        self.input_dim = input_dim\n        self.downsample = nn.AvgPool2d(3, stride=2, padding=[1, 1], count_include_pad=False)\n        self.cnns = nn.ModuleList()\n        for _ in range(self.num_scales):\n            self.cnns.append(self._make_net())\n\n    def _make_net(self):\n        dim = self.dim\n        cnn_x = []\n        cnn_x += [Conv2dBlock(self.input_dim, dim, 4, 2, 1, norm=\'none\', activation=self.activ, pad_type=self.pad_type)]\n        for i in range(self.n_layer - 1):\n            cnn_x += [Conv2dBlock(dim, dim * 2, 4, 2, 1, norm=self.norm, activation=self.activ, pad_type=self.pad_type)]\n            dim *= 2\n        cnn_x += [nn.Conv2d(dim, 1, 1, 1, 0)]\n        cnn_x = nn.Sequential(*cnn_x)\n        return cnn_x\n\n    def forward(self, x):\n        outputs = []\n        for model in self.cnns:\n            outputs.append(model(x))\n            x = self.downsample(x)\n        return outputs\n\n    def calc_dis_loss(self, input_fake, input_real):\n        # calculate the loss to train D\n        outs0 = self.forward(input_fake)\n        outs1 = self.forward(input_real)\n        loss = 0\n\n        for it, (out0, out1) in enumerate(zip(outs0, outs1)):\n            if self.gan_type == \'lsgan\':\n                loss += torch.mean((out0 - 0)**2) + torch.mean((out1 - 1)**2)\n            elif self.gan_type == \'nsgan\':\n                all0 = Variable(torch.zeros_like(out0.data).cuda(), requires_grad=False)\n                all1 = Variable(torch.ones_like(out1.data).cuda(), requires_grad=False)\n                loss += torch.mean(F.binary_cross_entropy(F.sigmoid(out0), all0) +\n                                   F.binary_cross_entropy(F.sigmoid(out1), all1))\n            else:\n                assert 0, ""Unsupported GAN type: {}"".format(self.gan_type)\n        return loss\n\n    def calc_gen_loss(self, input_fake):\n        # calculate the loss to train G\n        outs0 = self.forward(input_fake)\n        loss = 0\n        for it, (out0) in enumerate(outs0):\n            if self.gan_type == \'lsgan\':\n                loss += torch.mean((out0 - 1)**2) # LSGAN\n            elif self.gan_type == \'nsgan\':\n                all1 = Variable(torch.ones_like(out0.data).cuda(), requires_grad=False)\n                loss += torch.mean(F.binary_cross_entropy(F.sigmoid(out0), all1))\n            else:\n                assert 0, ""Unsupported GAN type: {}"".format(self.gan_type)\n        return loss\n\n##################################################################################\n# Generator\n##################################################################################\n\nclass AdaINGen(nn.Module):\n    # AdaIN auto-encoder architecture\n    def __init__(self, input_dim, params):\n        super(AdaINGen, self).__init__()\n        dim = params[\'dim\']\n        style_dim = params[\'style_dim\']\n        n_downsample = params[\'n_downsample\']\n        n_res = params[\'n_res\']\n        activ = params[\'activ\']\n        pad_type = params[\'pad_type\']\n        mlp_dim = params[\'mlp_dim\']\n\n        # style encoder\n        self.enc_style = StyleEncoder(4, input_dim, dim, style_dim, norm=\'none\', activ=activ, pad_type=pad_type)\n\n        # content encoder\n        self.enc_content = ContentEncoder(n_downsample, n_res, input_dim, dim, \'in\', activ, pad_type=pad_type)\n        self.dec = Decoder(n_downsample, n_res, self.enc_content.output_dim, input_dim, res_norm=\'adain\', activ=activ, pad_type=pad_type)\n\n        # MLP to generate AdaIN parameters\n        self.mlp = MLP(style_dim, self.get_num_adain_params(self.dec), mlp_dim, 3, norm=\'none\', activ=activ)\n\n    def forward(self, images):\n        # reconstruct an image\n        content, style_fake = self.encode(images)\n        images_recon = self.decode(content, style_fake)\n        return images_recon\n\n    def encode(self, images):\n        # encode an image to its content and style codes\n        style_fake = self.enc_style(images)\n        content = self.enc_content(images)\n        return content, style_fake\n\n    def decode(self, content, style):\n        # decode content and style codes to an image\n        adain_params = self.mlp(style)\n        self.assign_adain_params(adain_params, self.dec)\n        images = self.dec(content)\n        return images\n\n    def assign_adain_params(self, adain_params, model):\n        # assign the adain_params to the AdaIN layers in model\n        for m in model.modules():\n            if m.__class__.__name__ == ""AdaptiveInstanceNorm2d"":\n                mean = adain_params[:, :m.num_features]\n                std = adain_params[:, m.num_features:2*m.num_features]\n                m.bias = mean.contiguous().view(-1)\n                m.weight = std.contiguous().view(-1)\n                if adain_params.size(1) > 2*m.num_features:\n                    adain_params = adain_params[:, 2*m.num_features:]\n\n    def get_num_adain_params(self, model):\n        # return the number of AdaIN parameters needed by the model\n        num_adain_params = 0\n        for m in model.modules():\n            if m.__class__.__name__ == ""AdaptiveInstanceNorm2d"":\n                num_adain_params += 2*m.num_features\n        return num_adain_params\n\n\nclass VAEGen(nn.Module):\n    # VAE architecture\n    def __init__(self, input_dim, params):\n        super(VAEGen, self).__init__()\n        dim = params[\'dim\']\n        n_downsample = params[\'n_downsample\']\n        n_res = params[\'n_res\']\n        activ = params[\'activ\']\n        pad_type = params[\'pad_type\']\n\n        # content encoder\n        self.enc = ContentEncoder(n_downsample, n_res, input_dim, dim, \'in\', activ, pad_type=pad_type)\n        self.dec = Decoder(n_downsample, n_res, self.enc.output_dim, input_dim, res_norm=\'in\', activ=activ, pad_type=pad_type)\n\n    def forward(self, images):\n        # This is a reduced VAE implementation where we assume the outputs are multivariate Gaussian distribution with mean = hiddens and std_dev = all ones.\n        hiddens = self.encode(images)\n        if self.training == True:\n            noise = Variable(torch.randn(hiddens.size()).cuda(hiddens.data.get_device()))\n            images_recon = self.decode(hiddens + noise)\n        else:\n            images_recon = self.decode(hiddens)\n        return images_recon, hiddens\n\n    def encode(self, images):\n        hiddens = self.enc(images)\n        noise = Variable(torch.randn(hiddens.size()).cuda(hiddens.data.get_device()))\n        return hiddens, noise\n\n    def decode(self, hiddens):\n        images = self.dec(hiddens)\n        return images\n\n\n##################################################################################\n# Encoder and Decoders\n##################################################################################\n\nclass StyleEncoder(nn.Module):\n    def __init__(self, n_downsample, input_dim, dim, style_dim, norm, activ, pad_type):\n        super(StyleEncoder, self).__init__()\n        self.model = []\n        self.model += [Conv2dBlock(input_dim, dim, 7, 1, 3, norm=norm, activation=activ, pad_type=pad_type)]\n        for i in range(2):\n            self.model += [Conv2dBlock(dim, 2 * dim, 4, 2, 1, norm=norm, activation=activ, pad_type=pad_type)]\n            dim *= 2\n        for i in range(n_downsample - 2):\n            self.model += [Conv2dBlock(dim, dim, 4, 2, 1, norm=norm, activation=activ, pad_type=pad_type)]\n        self.model += [nn.AdaptiveAvgPool2d(1)] # global average pooling\n        self.model += [nn.Conv2d(dim, style_dim, 1, 1, 0)]\n        self.model = nn.Sequential(*self.model)\n        self.output_dim = dim\n\n    def forward(self, x):\n        return self.model(x)\n\nclass ContentEncoder(nn.Module):\n    def __init__(self, n_downsample, n_res, input_dim, dim, norm, activ, pad_type):\n        super(ContentEncoder, self).__init__()\n        self.model = []\n        self.model += [Conv2dBlock(input_dim, dim, 7, 1, 3, norm=norm, activation=activ, pad_type=pad_type)]\n        # downsampling blocks\n        for i in range(n_downsample):\n            self.model += [Conv2dBlock(dim, 2 * dim, 4, 2, 1, norm=norm, activation=activ, pad_type=pad_type)]\n            dim *= 2\n        # residual blocks\n        self.model += [ResBlocks(n_res, dim, norm=norm, activation=activ, pad_type=pad_type)]\n        self.model = nn.Sequential(*self.model)\n        self.output_dim = dim\n\n    def forward(self, x):\n        return self.model(x)\n\nclass Decoder(nn.Module):\n    def __init__(self, n_upsample, n_res, dim, output_dim, res_norm=\'adain\', activ=\'relu\', pad_type=\'zero\'):\n        super(Decoder, self).__init__()\n\n        self.model = []\n        # AdaIN residual blocks\n        self.model += [ResBlocks(n_res, dim, res_norm, activ, pad_type=pad_type)]\n        # upsampling blocks\n        for i in range(n_upsample):\n            self.model += [nn.Upsample(scale_factor=2),\n                           Conv2dBlock(dim, dim // 2, 5, 1, 2, norm=\'ln\', activation=activ, pad_type=pad_type)]\n            dim //= 2\n        # use reflection padding in the last conv layer\n        self.model += [Conv2dBlock(dim, output_dim, 7, 1, 3, norm=\'none\', activation=\'tanh\', pad_type=pad_type)]\n        self.model = nn.Sequential(*self.model)\n\n    def forward(self, x):\n        return self.model(x)\n\n##################################################################################\n# Sequential Models\n##################################################################################\nclass ResBlocks(nn.Module):\n    def __init__(self, num_blocks, dim, norm=\'in\', activation=\'relu\', pad_type=\'zero\'):\n        super(ResBlocks, self).__init__()\n        self.model = []\n        for i in range(num_blocks):\n            self.model += [ResBlock(dim, norm=norm, activation=activation, pad_type=pad_type)]\n        self.model = nn.Sequential(*self.model)\n\n    def forward(self, x):\n        return self.model(x)\n\nclass MLP(nn.Module):\n    def __init__(self, input_dim, output_dim, dim, n_blk, norm=\'none\', activ=\'relu\'):\n\n        super(MLP, self).__init__()\n        self.model = []\n        self.model += [LinearBlock(input_dim, dim, norm=norm, activation=activ)]\n        for i in range(n_blk - 2):\n            self.model += [LinearBlock(dim, dim, norm=norm, activation=activ)]\n        self.model += [LinearBlock(dim, output_dim, norm=\'none\', activation=\'none\')] # no output activations\n        self.model = nn.Sequential(*self.model)\n\n    def forward(self, x):\n        return self.model(x.view(x.size(0), -1))\n\n##################################################################################\n# Basic Blocks\n##################################################################################\nclass ResBlock(nn.Module):\n    def __init__(self, dim, norm=\'in\', activation=\'relu\', pad_type=\'zero\'):\n        super(ResBlock, self).__init__()\n\n        model = []\n        model += [Conv2dBlock(dim ,dim, 3, 1, 1, norm=norm, activation=activation, pad_type=pad_type)]\n        model += [Conv2dBlock(dim ,dim, 3, 1, 1, norm=norm, activation=\'none\', pad_type=pad_type)]\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        residual = x\n        out = self.model(x)\n        out += residual\n        return out\n\nclass Conv2dBlock(nn.Module):\n    def __init__(self, input_dim ,output_dim, kernel_size, stride,\n                 padding=0, norm=\'none\', activation=\'relu\', pad_type=\'zero\'):\n        super(Conv2dBlock, self).__init__()\n        self.use_bias = True\n        # initialize padding\n        if pad_type == \'reflect\':\n            self.pad = nn.ReflectionPad2d(padding)\n        elif pad_type == \'replicate\':\n            self.pad = nn.ReplicationPad2d(padding)\n        elif pad_type == \'zero\':\n            self.pad = nn.ZeroPad2d(padding)\n        else:\n            assert 0, ""Unsupported padding type: {}"".format(pad_type)\n\n        # initialize normalization\n        norm_dim = output_dim\n        if norm == \'bn\':\n            self.norm = nn.BatchNorm2d(norm_dim)\n        elif norm == \'in\':\n            self.norm = nn.InstanceNorm2d(norm_dim)\n        elif norm == \'ln\':\n            self.norm = LayerNorm(norm_dim)\n        elif norm == \'adain\':\n            self.norm = AdaptiveInstanceNorm2d(norm_dim)\n        elif norm == \'none\':\n            self.norm = None\n        else:\n            assert 0, ""Unsupported normalization: {}"".format(norm)\n\n        # initialize activation\n        if activation == \'relu\':\n            self.activation = nn.ReLU(inplace=True)\n        elif activation == \'lrelu\':\n            self.activation = nn.LeakyReLU(0.2, inplace=True)\n        elif activation == \'prelu\':\n            self.activation = nn.PReLU()\n        elif activation == \'selu\':\n            self.activation = nn.SELU(inplace=True)\n        elif activation == \'tanh\':\n            self.activation = nn.Tanh()\n        elif activation == \'none\':\n            self.activation = None\n        else:\n            assert 0, ""Unsupported activation: {}"".format(activation)\n\n        # initialize convolution\n        self.conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride, bias=self.use_bias)\n\n    def forward(self, x):\n        x = self.conv(self.pad(x))\n        if self.norm:\n            x = self.norm(x)\n        if self.activation:\n            x = self.activation(x)\n        return x\n\nclass LinearBlock(nn.Module):\n    def __init__(self, input_dim, output_dim, norm=\'none\', activation=\'relu\'):\n        super(LinearBlock, self).__init__()\n        use_bias = True\n        # initialize fully connected layer\n        self.fc = nn.Linear(input_dim, output_dim, bias=use_bias)\n\n        # initialize normalization\n        norm_dim = output_dim\n        if norm == \'bn\':\n            self.norm = nn.BatchNorm1d(norm_dim)\n        elif norm == \'in\':\n            self.norm = nn.InstanceNorm1d(norm_dim)\n        elif norm == \'ln\':\n            self.norm = LayerNorm(norm_dim)\n        elif norm == \'none\':\n            self.norm = None\n        else:\n            assert 0, ""Unsupported normalization: {}"".format(norm)\n\n        # initialize activation\n        if activation == \'relu\':\n            self.activation = nn.ReLU(inplace=True)\n        elif activation == \'lrelu\':\n            self.activation = nn.LeakyReLU(0.2, inplace=True)\n        elif activation == \'prelu\':\n            self.activation = nn.PReLU()\n        elif activation == \'selu\':\n            self.activation = nn.SELU(inplace=True)\n        elif activation == \'tanh\':\n            self.activation = nn.Tanh()\n        elif activation == \'none\':\n            self.activation = None\n        else:\n            assert 0, ""Unsupported activation: {}"".format(activation)\n\n    def forward(self, x):\n        out = self.fc(x)\n        if self.norm:\n            out = self.norm(out)\n        if self.activation:\n            out = self.activation(out)\n        return out\n\n##################################################################################\n# VGG network definition\n##################################################################################\nclass Vgg16(nn.Module):\n    def __init__(self):\n        super(Vgg16, self).__init__()\n        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n\n        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n\n        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n\n        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n\n        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n\n    def forward(self, X):\n        h = F.relu(self.conv1_1(X), inplace=True)\n        h = F.relu(self.conv1_2(h), inplace=True)\n        # relu1_2 = h\n        h = F.max_pool2d(h, kernel_size=2, stride=2)\n\n        h = F.relu(self.conv2_1(h), inplace=True)\n        h = F.relu(self.conv2_2(h), inplace=True)\n        # relu2_2 = h\n        h = F.max_pool2d(h, kernel_size=2, stride=2)\n\n        h = F.relu(self.conv3_1(h), inplace=True)\n        h = F.relu(self.conv3_2(h), inplace=True)\n        h = F.relu(self.conv3_3(h), inplace=True)\n        # relu3_3 = h\n        h = F.max_pool2d(h, kernel_size=2, stride=2)\n\n        h = F.relu(self.conv4_1(h), inplace=True)\n        h = F.relu(self.conv4_2(h), inplace=True)\n        h = F.relu(self.conv4_3(h), inplace=True)\n        # relu4_3 = h\n\n        h = F.relu(self.conv5_1(h), inplace=True)\n        h = F.relu(self.conv5_2(h), inplace=True)\n        h = F.relu(self.conv5_3(h), inplace=True)\n        relu5_3 = h\n\n        return relu5_3\n        # return [relu1_2, relu2_2, relu3_3, relu4_3]\n\n##################################################################################\n# Normalization layers\n##################################################################################\nclass AdaptiveInstanceNorm2d(nn.Module):\n    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n        super(AdaptiveInstanceNorm2d, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n        # weight and bias are dynamically assigned\n        self.weight = None\n        self.bias = None\n        # just dummy buffers, not used\n        self.register_buffer(\'running_mean\', torch.zeros(num_features))\n        self.register_buffer(\'running_var\', torch.ones(num_features))\n\n    def forward(self, x):\n        assert self.weight is not None and self.bias is not None, ""Please assign weight and bias before calling AdaIN!""\n        b, c = x.size(0), x.size(1)\n        running_mean = self.running_mean.repeat(b)\n        running_var = self.running_var.repeat(b)\n\n        # Apply instance norm\n        x_reshaped = x.contiguous().view(1, b * c, *x.size()[2:])\n\n        out = F.batch_norm(\n            x_reshaped, running_mean, running_var, self.weight, self.bias,\n            True, self.momentum, self.eps)\n\n        return out.view(b, c, *x.size()[2:])\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'(\' + str(self.num_features) + \')\'\n\nclass LayerNorm(nn.Module):\n    def __init__(self, num_features, eps=1e-5, affine=True):\n        super(LayerNorm, self).__init__()\n        self.num_features = num_features\n        self.affine = affine\n        self.eps = eps\n\n        if self.affine:\n            self.gamma = nn.Parameter(torch.Tensor(num_features).uniform_())\n            self.beta = nn.Parameter(torch.zeros(num_features))\n\n    def forward(self, x):\n        shape = [-1] + [1] * (x.dim() - 1)\n        mean = x.view(x.size(0), -1).mean(1).view(*shape)\n        std = x.view(x.size(0), -1).std(1).view(*shape)\n        x = (x - mean) / (std + self.eps)\n\n        if self.affine:\n            shape = [1, -1] + [1] * (x.dim() - 2)\n            x = x * self.gamma.view(*shape) + self.beta.view(*shape)\n        return x'"
options/._single_unet_conv_add_bs32_BN_nonormDlayer5_3_final_lsgan_64patchD_P_vgg.py,0,b'\x00\x05\x16\x07\x00\x02\x00\x00Mac OS X        \x00\x02\x00\x00\x00\t\x00\x00\x002\x00\x00\x0e\xb0\x00\x00\x00\x02\x00\x00\x0e\xe2\x00\x00\x01\x1e\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00ATTR\x00\x00\x11\xb1\x00\x00\x0e\xe2\x00\x00\x00\xa8\x00\x00\x00*\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\xa8\x00\x00\x00*\x00\x00$com.apple.metadata:_kMDItemUserTags\x00\x00bplist00\xa0\x08\x00\x00\x00\x00\x00\x00\x01\x01\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\t\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x1eThis resource fork intentionally left blank   \x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x1e\x00\x00\x00\x00\x00\x00\x00\x00\x00\x1c\x00\x1e\xff\xff'
options/__init__.py,0,b''
options/base_options.py,1,"b'import argparse\nimport os\nfrom util import util\nimport torch\n\nclass BaseOptions():\n    def __init__(self):\n        self.parser = argparse.ArgumentParser()\n        self.initialized = False\n\n    def initialize(self):\n        self.parser.add_argument(\'--dataroot\', required=True, help=\'path to images (should have subfolders trainA, trainB, valA, valB, etc)\')\n        self.parser.add_argument(\'--batchSize\', type=int, default=1, help=\'input batch size\')\n        self.parser.add_argument(\'--loadSize\', type=int, default=286, help=\'scale images to this size\')\n        self.parser.add_argument(\'--fineSize\', type=int, default=256, help=\'then crop to this size\')\n        self.parser.add_argument(\'--patchSize\', type=int, default=64, help=\'then crop to this size\')\n        self.parser.add_argument(\'--input_nc\', type=int, default=3, help=\'# of input image channels\')\n        self.parser.add_argument(\'--output_nc\', type=int, default=3, help=\'# of output image channels\')\n        self.parser.add_argument(\'--ngf\', type=int, default=64, help=\'# of gen filters in first conv layer\')\n        self.parser.add_argument(\'--ndf\', type=int, default=64, help=\'# of discrim filters in first conv layer\')\n        self.parser.add_argument(\'--which_model_netD\', type=str, default=\'basic\', help=\'selects model to use for netD\')\n        self.parser.add_argument(\'--which_model_netG\', type=str, default=\'unet_256\', help=\'selects model to use for netG\')\n        self.parser.add_argument(\'--n_layers_D\', type=int, default=3, help=\'only used if which_model_netD==n_layers\')\n        self.parser.add_argument(\'--n_layers_patchD\', type=int, default=3, help=\'only used if which_model_netD==n_layers\')\n        self.parser.add_argument(\'--gpu_ids\', type=str, default=\'0\', help=\'gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU\')\n        self.parser.add_argument(\'--name\', type=str, default=\'experiment_name\', help=\'name of the experiment. It decides where to store samples and models\')\n        self.parser.add_argument(\'--dataset_mode\', type=str, default=\'unaligned\', help=\'chooses how datasets are loaded. [unaligned | aligned | single]\')\n        self.parser.add_argument(\'--model\', type=str, default=\'cycle_gan\',\n                                 help=\'chooses which model to use. cycle_gan, pix2pix, test\')\n        self.parser.add_argument(\'--which_direction\', type=str, default=\'AtoB\', help=\'AtoB or BtoA\')\n        self.parser.add_argument(\'--nThreads\', default=4, type=int, help=\'# threads for loading data\')\n        self.parser.add_argument(\'--checkpoints_dir\', type=str, default=\'./checkpoints\', help=\'models are saved here\')\n        self.parser.add_argument(\'--norm\', type=str, default=\'instance\', help=\'instance normalization or batch normalization\')\n        self.parser.add_argument(\'--serial_batches\', action=\'store_true\', help=\'if true, takes images in order to make batches, otherwise takes them randomly\')\n        self.parser.add_argument(\'--display_winsize\', type=int, default=256,  help=\'display window size\')\n        self.parser.add_argument(\'--display_id\', type=int, default=1, help=\'window id of the web display\')\n        self.parser.add_argument(\'--display_port\', type=int, default=8097, help=\'visdom port of the web display\')\n        self.parser.add_argument(\'--display_single_pane_ncols\', type=int, default=0, help=\'if positive, display all images in a single visdom web panel with certain number of images per row.\')\n        self.parser.add_argument(\'--identity\', type=float, default=0.0, help=\'use identity mapping. Setting identity other than 1 has an effect of scaling the weight of the identity mapping loss. For example, if the weight of the identity loss should be 10 times smaller than the weight of the reconstruction loss, please set optidentity = 0.1\')\n        self.parser.add_argument(\'--no_dropout\', action=\'store_true\', help=\'no dropout for the generator\')\n        self.parser.add_argument(\'--lambda_A\', type=float, default=10.0, help=\'weight for cycle loss (A -> B -> A)\')\n        self.parser.add_argument(\'--lambda_B\', type=float, default=10.0, help=\'weight for cycle loss (B -> A -> B)\')\n        self.parser.add_argument(\'--max_dataset_size\', type=int, default=float(""inf""), help=\'Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.\')\n        self.parser.add_argument(\'--resize_or_crop\', type=str, default=\'crop\', help=\'scaling and cropping of images at load time [resize_and_crop|crop|scale_width|scale_width_and_crop]\')\n        self.parser.add_argument(\'--no_flip\', action=\'store_true\', help=\'if specified, do not flip the images for data augmentation\')\n        self.parser.add_argument(\'--skip\', type=float, default=0.8, help=\'B = net.forward(A) + skip*A\')\n        self.parser.add_argument(\'--use_mse\', action=\'store_true\', help=\'MSELoss\')\n        self.parser.add_argument(\'--l1\', type=float, default=10.0, help=\'L1 loss weight is 10.0\')\n        self.parser.add_argument(\'--use_norm\', type=float, default=1, help=\'L1 loss weight is 10.0\')\n        self.parser.add_argument(\'--use_wgan\', type=float, default=0, help=\'use wgan-gp\')\n        self.parser.add_argument(\'--use_ragan\', action=\'store_true\', help=\'use ragan\')\n        self.parser.add_argument(\'--vgg\', type=float, default=0, help=\'use perceptrual loss\')\n        self.parser.add_argument(\'--vgg_mean\', action=\'store_true\', help=\'substract mean in vgg loss\')\n        self.parser.add_argument(\'--vgg_choose\', type=str, default=\'relu5_3\', help=\'choose layer for vgg\')\n        self.parser.add_argument(\'--no_vgg_instance\', action=\'store_true\', help=\'vgg instance normalization\')\n        self.parser.add_argument(\'--vgg_maxpooling\', action=\'store_true\', help=\'normalize attention map\')\n        self.parser.add_argument(\'--IN_vgg\', action=\'store_true\', help=\'patch vgg individual\')\n        self.parser.add_argument(\'--fcn\', type=float, default=0, help=\'use semantic loss\')\n        self.parser.add_argument(\'--use_avgpool\', type=float, default=0, help=\'use perceptrual loss\')\n        self.parser.add_argument(\'--instance_norm\', type=float, default=0, help=\'use instance normalization\')\n        self.parser.add_argument(\'--syn_norm\', action=\'store_true\', help=\'use synchronize batch normalization\')\n        self.parser.add_argument(\'--tanh\', action=\'store_true\', help=\'tanh\')\n        self.parser.add_argument(\'--linear\', action=\'store_true\', help=\'tanh\')\n        self.parser.add_argument(\'--new_lr\', action=\'store_true\', help=\'tanh\')\n        self.parser.add_argument(\'--multiply\', action=\'store_true\', help=\'tanh\')\n        self.parser.add_argument(\'--noise\', type=float, default=0, help=\'variance of noise\')\n        self.parser.add_argument(\'--input_linear\', action=\'store_true\', help=\'lieanr scaling input\')\n        self.parser.add_argument(\'--linear_add\', action=\'store_true\', help=\'lieanr scaling input\')\n        self.parser.add_argument(\'--latent_threshold\', action=\'store_true\', help=\'lieanr scaling input\')\n        self.parser.add_argument(\'--latent_norm\', action=\'store_true\', help=\'lieanr scaling input\')\n        self.parser.add_argument(\'--patchD\', action=\'store_true\', help=\'use patch discriminator\')\n        self.parser.add_argument(\'--patchD_3\', type=int, default=0, help=\'choose the number of crop for patch discriminator\')\n        self.parser.add_argument(\'--D_P_times2\', action=\'store_true\', help=\'loss_D_P *= 2\')\n        self.parser.add_argument(\'--patch_vgg\', action=\'store_true\', help=\'use vgg loss between each patch\')\n        self.parser.add_argument(\'--hybrid_loss\', action=\'store_true\', help=\'use lsgan and ragan separately\')\n        self.parser.add_argument(\'--self_attention\', action=\'store_true\', help=\'adding attention on the input of generator\')\n        self.parser.add_argument(\'--times_residual\', action=\'store_true\', help=\'output = input + residual*attention\')\n        self.parser.add_argument(\'--low_times\', type=int, default=200, help=\'choose the number of crop for patch discriminator\')\n        self.parser.add_argument(\'--high_times\', type=int, default=400, help=\'choose the number of crop for patch discriminator\')\n        self.parser.add_argument(\'--norm_attention\', action=\'store_true\', help=\'normalize attention map\')\n        self.parser.add_argument(\'--vary\', type=int, default=1, help=\'use light data augmentation\')\n        self.parser.add_argument(\'--lighten\', action=\'store_true\', help=\'normalize attention map\')\n        self.initialized = True\n\n    def parse(self):\n        if not self.initialized:\n            self.initialize()\n        self.opt = self.parser.parse_args()\n        self.opt.isTrain = self.isTrain   # train or test\n\n        str_ids = self.opt.gpu_ids.split(\',\')\n        self.opt.gpu_ids = []\n        for str_id in str_ids:\n            id = int(str_id)\n            if id >= 0:\n                self.opt.gpu_ids.append(id)\n        \n        # set gpu ids\n        if len(self.opt.gpu_ids) > 0:\n            torch.cuda.set_device(self.opt.gpu_ids[0])\n\n        args = vars(self.opt)\n\n        print(\'------------ Options -------------\')\n        for k, v in sorted(args.items()):\n            print(\'%s: %s\' % (str(k), str(v)))\n        print(\'-------------- End ----------------\')\n\n        # save to the disk\n        expr_dir = os.path.join(self.opt.checkpoints_dir, self.opt.name)\n        util.mkdirs(expr_dir)\n        file_name = os.path.join(expr_dir, \'opt.txt\')\n        with open(file_name, \'wt\') as opt_file:\n            opt_file.write(\'------------ Options -------------\\n\')\n            for k, v in sorted(args.items()):\n                opt_file.write(\'%s: %s\\n\' % (str(k), str(v)))\n            opt_file.write(\'-------------- End ----------------\\n\')\n        return self.opt\n'"
options/single_unet_conv_add_bs32_BN_nonormDlayer5_3_final_lsgan_64patchD_P_vgg.py,0,"b'import os\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""--port"", type=str, default=""8097"")\nparser.add_argument(""--train"", action=\'store_true\')\nparser.add_argument(""--test"", action=\'store_true\')\nparser.add_argument(""--predict"", action=\'store_true\')\nopt = parser.parse_args()\n\nif opt.train:\n\tos.system(""python train.py \\\n\t\t--dataroot /vita1_ssd1/yifan/final_dataset \\\n\t\t--no_dropout \\\n\t\t--name single_unet_conv_add_bs32_BN_nonormDlayer5_3_final_lsgan_64patchD_P_vgg \\\n\t\t--model single \\\n\t\t--dataset_mode unaligned \\\n\t\t--which_model_netG sid_unet_resize \\\n        --which_model_netD no_norm_4 \\\n        --n_layers_D 5 \\\n        --n_layers_patchD 3 \\\n        --patchD \\\n\t\t--fineSize 320 \\\n        --patchSize 64 \\\n\t\t--skip 1 \\\n\t\t--batchSize 30 \\\n\t\t--use_norm 1 \\\n\t\t--use_wgan 0 \\\n\t\t--instance_norm 0 \\\n\t\t--vgg 1 \\\n\t\t--gpu_ids 0,1,2,3 \\\n\t\t--display_port="" + opt.port)\n\nelif opt.test:\n\tfor i in range(20):\n\t        os.system(""python test.py \\\n\t        \t--dataroot /vita1_ssd1/yifan/compete_LOL \\\n\t        \t--name single_unet_conv_add_bs32_BN_nonormDlayer5_3_final_lsgan_64patchD_P_vgg \\\n\t        \t--model single \\\n\t        \t--which_direction AtoB \\\n\t        \t--no_dropout \\\n\t        \t--dataset_mode pair \\\n\t        \t--which_model_netG sid_unet_resize \\\n\t        \t--skip 1 \\\n\t        \t--use_norm 1 \\\n\t        \t--use_wgan 0 \\\n\t        \t--instance_norm 0 \\\n\t        \t--which_epoch "" + str(i*5+100))\n\nelif opt.predict:\n\tfor i in range(20):\n\t        os.system(""python predict.py \\\n\t        \t--dataroot /vita1_ssd1/yifan/common_dataset \\\n\t        \t--name single_unet_conv_add_bs32_BN_nonormDlayer5_3_final_lsgan_64patchD_P_vgg \\\n\t        \t--model single \\\n\t        \t--which_direction AtoB \\\n\t        \t--no_dropout \\\n\t        \t--dataset_mode unaligned \\\n\t        \t--which_model_netG sid_unet_resize \\\n\t        \t--skip 1 \\\n\t        \t--use_norm 1 \\\n\t        \t--use_wgan 0 \\\n\t        \t--instance_norm 0 --resize_or_crop=\'no\'\\\n\t        \t--which_epoch "" + str(200 - i*10))\n'"
options/test_options.py,0,"b'from .base_options import BaseOptions\n\n\nclass TestOptions(BaseOptions):\n    def initialize(self):\n        BaseOptions.initialize(self)\n        self.parser.add_argument(\'--ntest\', type=int, default=float(""inf""), help=\'# of test examples.\')\n        self.parser.add_argument(\'--results_dir\', type=str, default=\'./results/\', help=\'saves results here.\')\n        self.parser.add_argument(\'--aspect_ratio\', type=float, default=1.0, help=\'aspect ratio of result images\')\n        self.parser.add_argument(\'--phase\', type=str, default=\'test\', help=\'train, val, test, etc\')\n        self.parser.add_argument(\'--which_epoch\', type=str, default=\'latest\', help=\'which epoch to load? set to latest to use latest cached model\')\n        self.parser.add_argument(\'--how_many\', type=int, default=50, help=\'how many test images to run\')\n        self.isTrain = False\n'"
options/train_options.py,0,"b""from .base_options import BaseOptions\n\n\nclass TrainOptions(BaseOptions):\n    def initialize(self):\n        BaseOptions.initialize(self)\n        self.parser.add_argument('--display_freq', type=int, default=30, help='frequency of showing training results on screen')\n        self.parser.add_argument('--print_freq', type=int, default=100, help='frequency of showing training results on console')\n        self.parser.add_argument('--save_latest_freq', type=int, default=5000, help='frequency of saving the latest results')\n        self.parser.add_argument('--save_epoch_freq', type=int, default=5, help='frequency of saving checkpoints at the end of epochs')\n        self.parser.add_argument('--continue_train', action='store_true', help='continue training: load the latest model')\n        self.parser.add_argument('--phase', type=str, default='train', help='train, val, test, etc')\n        self.parser.add_argument('--which_epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n        self.parser.add_argument('--niter', type=int, default=100, help='# of iter at starting learning rate')\n        self.parser.add_argument('--niter_decay', type=int, default=100, help='# of iter to linearly decay learning rate to zero')\n        self.parser.add_argument('--beta1', type=float, default=0.5, help='momentum term of adam')\n        self.parser.add_argument('--lr', type=float, default=0.0001, help='initial learning rate for adam')\n        self.parser.add_argument('--no_lsgan', action='store_true', help='do *not* use least square GAN, if false, use vanilla GAN')\n        self.parser.add_argument('--pool_size', type=int, default=50, help='the size of image buffer that stores previously generated images')\n        self.parser.add_argument('--no_html', action='store_true', help='do not save intermediate training results to [opt.checkpoints_dir]/[opt.name]/web/')\n        self.parser.add_argument('--config', type=str, default='configs/unit_gta2city_folder.yaml', help='Path to the config file.')\n        self.isTrain = True\n"""
scripts/script.py,0,"b'import os\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""--port"", type=str, default=""8097"")\nparser.add_argument(""--train"", action=\'store_true\')\nparser.add_argument(""--predict"", action=\'store_true\')\nopt = parser.parse_args()\n\nif opt.train:\n\tos.system(""python train.py \\\n\t\t--dataroot ../final_dataset \\\n\t\t--no_dropout \\\n\t\t--name enlightening \\\n\t\t--model single \\\n\t\t--dataset_mode unaligned \\\n\t\t--which_model_netG sid_unet_resize \\\n        --which_model_netD no_norm_4 \\\n        --patchD \\\n        --patch_vgg \\\n        --patchD_3 5 \\\n        --n_layers_D 5 \\\n        --n_layers_patchD 4 \\\n\t\t--fineSize 320 \\\n        --patchSize 32 \\\n\t\t--skip 1 \\\n\t\t--batchSize 32 \\\n        --self_attention \\\n\t\t--use_norm 1 \\\n\t\t--use_wgan 0 \\\n        --use_ragan \\\n        --hybrid_loss \\\n        --times_residual \\\n\t\t--instance_norm 0 \\\n\t\t--vgg 1 \\\n        --vgg_choose relu5_1 \\\n\t\t--gpu_ids 0,1,2 \\\n\t\t--display_port="" + opt.port)\n\nelif opt.predict:\n\tfor i in range(1):\n\t        os.system(""python predict.py \\\n\t        \t--dataroot ../test_dataset \\\n\t        \t--name enlightening \\\n\t        \t--model single \\\n\t        \t--which_direction AtoB \\\n\t        \t--no_dropout \\\n\t        \t--dataset_mode unaligned \\\n\t        \t--which_model_netG sid_unet_resize \\\n\t        \t--skip 1 \\\n\t        \t--use_norm 1 \\\n\t        \t--use_wgan 0 \\\n                --self_attention \\\n                --times_residual \\\n\t        \t--instance_norm 0 --resize_or_crop=\'no\'\\\n\t        \t--which_epoch "" + str(200 - i*5))'"
seg/PSPNet.py,0,b''
seg/resnet.py,0,b''
util/__init__.py,0,b''
util/get_data.py,0,"b'from __future__ import print_function\nimport os\nimport tarfile\nimport requests\nfrom warnings import warn\nfrom zipfile import ZipFile\nfrom bs4 import BeautifulSoup\nfrom os.path import abspath, isdir, join, basename\n\n\nclass GetData(object):\n    """"""\n\n    Download CycleGAN or Pix2Pix Data.\n\n    Args:\n        technique : str\n            One of: \'cyclegan\' or \'pix2pix\'.\n        verbose : bool\n            If True, print additional information.\n\n    Examples:\n        >>> from util.get_data import GetData\n        >>> gd = GetData(technique=\'cyclegan\')\n        >>> new_data_path = gd.get(save_path=\'./datasets\')  # options will be displayed.\n\n    """"""\n\n    def __init__(self, technique=\'cyclegan\', verbose=True):\n        url_dict = {\n            \'pix2pix\': \'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets\',\n            \'cyclegan\': \'https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets\'\n        }\n        self.url = url_dict.get(technique.lower())\n        self._verbose = verbose\n\n    def _print(self, text):\n        if self._verbose:\n            print(text)\n\n    @staticmethod\n    def _get_options(r):\n        soup = BeautifulSoup(r.text, \'lxml\')\n        options = [h.text for h in soup.find_all(\'a\', href=True)\n                   if h.text.endswith((\'.zip\', \'tar.gz\'))]\n        return options\n\n    def _present_options(self):\n        r = requests.get(self.url)\n        options = self._get_options(r)\n        print(\'Options:\\n\')\n        for i, o in enumerate(options):\n            print(""{0}: {1}"".format(i, o))\n        choice = input(""\\nPlease enter the number of the ""\n                       ""dataset above you wish to download:"")\n        return options[int(choice)]\n\n    def _download_data(self, dataset_url, save_path):\n        if not isdir(save_path):\n            os.makedirs(save_path)\n\n        base = basename(dataset_url)\n        temp_save_path = join(save_path, base)\n\n        with open(temp_save_path, ""wb"") as f:\n            r = requests.get(dataset_url)\n            f.write(r.content)\n\n        if base.endswith(\'.tar.gz\'):\n            obj = tarfile.open(temp_save_path)\n        elif base.endswith(\'.zip\'):\n            obj = ZipFile(temp_save_path, \'r\')\n        else:\n            raise ValueError(""Unknown File Type: {0}."".format(base))\n\n        self._print(""Unpacking Data..."")\n        obj.extractall(save_path)\n        obj.close()\n        os.remove(temp_save_path)\n\n    def get(self, save_path, dataset=None):\n        """"""\n\n        Download a dataset.\n\n        Args:\n            save_path : str\n                A directory to save the data to.\n            dataset : str, optional\n                A specific dataset to download.\n                Note: this must include the file extension.\n                If None, options will be presented for you\n                to choose from.\n\n        Returns:\n            save_path_full : str\n                The absolute path to the downloaded data.\n\n        """"""\n        if dataset is None:\n            selected_dataset = self._present_options()\n        else:\n            selected_dataset = dataset\n\n        save_path_full = join(save_path, selected_dataset.split(\'.\')[0])\n\n        if isdir(save_path_full):\n            warn(""\\n\'{0}\' already exists. Voiding Download."".format(\n                save_path_full))\n        else:\n            self._print(\'Downloading Data...\')\n            url = ""{0}/{1}"".format(self.url, selected_dataset)\n            self._download_data(url, save_path=save_path)\n\n        return abspath(save_path_full)\n'"
util/html.py,0,"b'import dominate\nfrom dominate.tags import *\nimport os\n\n\nclass HTML:\n    def __init__(self, web_dir, title, reflesh=0):\n        self.title = title\n        self.web_dir = web_dir\n        self.img_dir = os.path.join(self.web_dir, \'images\')\n        if not os.path.exists(self.web_dir):\n            os.makedirs(self.web_dir)\n        if not os.path.exists(self.img_dir):\n            os.makedirs(self.img_dir)\n        # print(self.img_dir)\n\n        self.doc = dominate.document(title=title)\n        if reflesh > 0:\n            with self.doc.head:\n                meta(http_equiv=""reflesh"", content=str(reflesh))\n\n    def get_image_dir(self):\n        return self.img_dir\n\n    def add_header(self, str):\n        with self.doc:\n            h3(str)\n\n    def add_table(self, border=1):\n        self.t = table(border=border, style=""table-layout: fixed;"")\n        self.doc.add(self.t)\n\n    def add_images(self, ims, txts, links, width=400):\n        self.add_table()\n        with self.t:\n            with tr():\n                for im, txt, link in zip(ims, txts, links):\n                    with td(style=""word-wrap: break-word;"", halign=""center"", valign=""top""):\n                        with p():\n                            with a(href=os.path.join(\'images\', link)):\n                                img(style=""width:%dpx"" % width, src=os.path.join(\'images\', im))\n                            br()\n                            p(txt)\n\n    def save(self):\n        html_file = \'%s/index.html\' % self.web_dir\n        f = open(html_file, \'wt\')\n        f.write(self.doc.render())\n        f.close()\n\n\nif __name__ == \'__main__\':\n    html = HTML(\'web/\', \'test_html\')\n    html.add_header(\'hello world\')\n\n    ims = []\n    txts = []\n    links = []\n    for n in range(4):\n        ims.append(\'image_%d.png\' % n)\n        txts.append(\'text_%d\' % n)\n        links.append(\'image_%d.png\' % n)\n    html.add_images(ims, txts, links)\n    html.save()\n'"
util/image_pool.py,3,"b'import random\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\nclass ImagePool():\n    def __init__(self, pool_size):\n        self.pool_size = pool_size\n        if self.pool_size > 0:\n            self.num_imgs = 0\n            self.images = []\n\n    def query(self, images):\n        if self.pool_size == 0:\n            return images\n        return_images = []\n        for image in images.data:\n            image = torch.unsqueeze(image, 0)\n            if self.num_imgs < self.pool_size:\n                self.num_imgs = self.num_imgs + 1\n                self.images.append(image)\n                return_images.append(image)\n            else:\n                p = random.uniform(0, 1)\n                if p > 0.5:\n                    random_id = random.randint(0, self.pool_size-1)\n                    tmp = self.images[random_id].clone()\n                    self.images[random_id] = image\n                    return_images.append(tmp)\n                else:\n                    return_images.append(image)\n        return_images = Variable(torch.cat(return_images, 0))\n        return return_images\n'"
util/png.py,0,"b'import struct\nimport zlib\n\ndef encode(buf, width, height):\n  """""" buf: must be bytes or a bytearray in py3, a regular string in py2. formatted RGBRGB... """"""\n  assert (width * height * 3 == len(buf))\n  bpp = 3\n\n  def raw_data():\n    # reverse the vertical line order and add null bytes at the start\n    row_bytes = width * bpp\n    for row_start in range((height - 1) * width * bpp, -1, -row_bytes):\n      yield b\'\\x00\'\n      yield buf[row_start:row_start + row_bytes]\n\n  def chunk(tag, data):\n    return [\n        struct.pack(""!I"", len(data)),\n        tag,\n        data,\n        struct.pack(""!I"", 0xFFFFFFFF & zlib.crc32(data, zlib.crc32(tag)))\n      ]\n\n  SIGNATURE = b\'\\x89PNG\\r\\n\\x1a\\n\'\n  COLOR_TYPE_RGB = 2\n  COLOR_TYPE_RGBA = 6\n  bit_depth = 8\n  return b\'\'.join(\n      [ SIGNATURE ] +\n      chunk(b\'IHDR\', struct.pack(""!2I5B"", width, height, bit_depth, COLOR_TYPE_RGB, 0, 0, 0)) +\n      chunk(b\'IDAT\', zlib.compress(b\'\'.join(raw_data()), 9)) +\n      chunk(b\'IEND\', b\'\')\n    )\n'"
util/util.py,9,"b'# from __future__ import print_function\nimport numpy as np\nfrom PIL import Image\nimport inspect, re\nimport numpy as np\nimport torch\nimport os\nimport collections\nfrom torch.optim import lr_scheduler\nimport torch.nn.init as init\n\n\n# Converts a Tensor into a Numpy array\n# |imtype|: the desired type of the converted numpy array\ndef tensor2im(image_tensor, imtype=np.uint8):\n    image_numpy = image_tensor[0].cpu().float().numpy()\n    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n    image_numpy = np.maximum(image_numpy, 0)\n    image_numpy = np.minimum(image_numpy, 255)\n    return image_numpy.astype(imtype)\n\ndef atten2im(image_tensor, imtype=np.uint8):\n    image_tensor = image_tensor[0]\n    image_tensor = torch.cat((image_tensor, image_tensor, image_tensor), 0)\n    image_numpy = image_tensor.cpu().float().numpy()\n    image_numpy = (np.transpose(image_numpy, (1, 2, 0))) * 255.0\n    image_numpy = image_numpy/(image_numpy.max()/255.0)\n    return image_numpy.astype(imtype)\n\ndef latent2im(image_tensor, imtype=np.uint8):\n    # image_tensor = (image_tensor - torch.min(image_tensor))/(torch.max(image_tensor)-torch.min(image_tensor))\n    image_numpy = image_tensor[0].cpu().float().numpy()\n    image_numpy = (np.transpose(image_numpy, (1, 2, 0))) * 255.0\n    image_numpy = np.maximum(image_numpy, 0)\n    image_numpy = np.minimum(image_numpy, 255)\n    return image_numpy.astype(imtype)\n\ndef max2im(image_1, image_2, imtype=np.uint8):\n    image_1 = image_1[0].cpu().float().numpy()\n    image_2 = image_2[0].cpu().float().numpy()\n    image_1 = (np.transpose(image_1, (1, 2, 0)) + 1) / 2.0 * 255.0\n    image_2 = (np.transpose(image_2, (1, 2, 0))) * 255.0\n    output = np.maximum(image_1, image_2)\n    output = np.maximum(output, 0)\n    output = np.minimum(output, 255)\n    return output.astype(imtype)\n\ndef variable2im(image_tensor, imtype=np.uint8):\n    image_numpy = image_tensor[0].data.cpu().float().numpy()\n    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n    return image_numpy.astype(imtype)\n\n\ndef diagnose_network(net, name=\'network\'):\n    mean = 0.0\n    count = 0\n    for param in net.parameters():\n        if param.grad is not None:\n            mean += torch.mean(torch.abs(param.grad.data))\n            count += 1\n    if count > 0:\n        mean = mean / count\n    print(name)\n    print(mean)\n\n\ndef save_image(image_numpy, image_path):\n    image_pil = Image.fromarray(image_numpy)\n    image_pil.save(image_path)\n\ndef info(object, spacing=10, collapse=1):\n    """"""Print methods and doc strings.\n    Takes module, class, list, dictionary, or string.""""""\n    methodList = [e for e in dir(object) if isinstance(getattr(object, e), collections.Callable)]\n    processFunc = collapse and (lambda s: "" "".join(s.split())) or (lambda s: s)\n    print( ""\\n"".join([""%s %s"" %\n                     (method.ljust(spacing),\n                      processFunc(str(getattr(object, method).__doc__)))\n                     for method in methodList]) )\n\ndef varname(p):\n    for line in inspect.getframeinfo(inspect.currentframe().f_back)[3]:\n        m = re.search(r\'\\bvarname\\s*\\(\\s*([A-Za-z_][A-Za-z0-9_]*)\\s*\\)\', line)\n        if m:\n            return m.group(1)\n\ndef print_numpy(x, val=True, shp=False):\n    x = x.astype(np.float64)\n    if shp:\n        print(\'shape,\', x.shape)\n    if val:\n        x = x.flatten()\n        print(\'mean = %3.3f, min = %3.3f, max = %3.3f, median = %3.3f, std=%3.3f\' % (\n            np.mean(x), np.min(x), np.max(x), np.median(x), np.std(x)))\n\n\ndef mkdirs(paths):\n    if isinstance(paths, list) and not isinstance(paths, str):\n        for path in paths:\n            mkdir(path)\n    else:\n        mkdir(paths)\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\ndef get_model_list(dirname, key):\n    if os.path.exists(dirname) is False:\n        return None\n    gen_models = [os.path.join(dirname, f) for f in os.listdir(dirname) if\n                  os.path.isfile(os.path.join(dirname, f)) and key in f and "".pt"" in f]\n    if gen_models is None:\n        return None\n    gen_models.sort()\n    last_model_name = gen_models[-1]\n    return last_model_name\n\n\ndef load_vgg16(model_dir):\n    """""" Use the model from https://github.com/abhiskk/fast-neural-style/blob/master/neural_style/utils.py """"""\n    if not os.path.exists(model_dir):\n        os.mkdir(model_dir)\n    if not os.path.exists(os.path.join(model_dir, \'vgg16.weight\')):\n        if not os.path.exists(os.path.join(model_dir, \'vgg16.t7\')):\n            os.system(\'wget https://www.dropbox.com/s/76l3rt4kyi3s8x7/vgg16.t7?dl=1 -O \' + os.path.join(model_dir, \'vgg16.t7\'))\n        vgglua = load_lua(os.path.join(model_dir, \'vgg16.t7\'))\n        vgg = Vgg16()\n        for (src, dst) in zip(vgglua.parameters()[0], vgg.parameters()):\n            dst.data[:] = src\n        torch.save(vgg.state_dict(), os.path.join(model_dir, \'vgg16.weight\'))\n    vgg = Vgg16()\n    vgg.load_state_dict(torch.load(os.path.join(model_dir, \'vgg16.weight\')))\n    return vgg\n\n\ndef vgg_preprocess(batch):\n    tensortype = type(batch.data)\n    (r, g, b) = torch.chunk(batch, 3, dim = 1)\n    batch = torch.cat((b, g, r), dim = 1) # convert RGB to BGR\n    batch = (batch + 1) * 255 * 0.5 # [-1, 1] -> [0, 255]\n    mean = tensortype(batch.data.size())\n    mean[:, 0, :, :] = 103.939\n    mean[:, 1, :, :] = 116.779\n    mean[:, 2, :, :] = 123.680\n    batch = batch.sub(Variable(mean)) # subtract mean\n    return batch\n\n\ndef get_scheduler(optimizer, hyperparameters, iterations=-1):\n    if \'lr_policy\' not in hyperparameters or hyperparameters[\'lr_policy\'] == \'constant\':\n        scheduler = None # constant scheduler\n    elif hyperparameters[\'lr_policy\'] == \'step\':\n        scheduler = lr_scheduler.StepLR(optimizer, step_size=hyperparameters[\'step_size\'],\n                                        gamma=hyperparameters[\'gamma\'], last_epoch=iterations)\n    else:\n        return NotImplementedError(\'learning rate policy [%s] is not implemented\', hyperparameters[\'lr_policy\'])\n    return scheduler\n\n\ndef weights_init(init_type=\'gaussian\'):\n    def init_fun(m):\n        classname = m.__class__.__name__\n        if (classname.find(\'Conv\') == 0 or classname.find(\'Linear\') == 0) and hasattr(m, \'weight\'):\n            # print m.__class__.__name__\n            if init_type == \'gaussian\':\n                init.normal(m.weight.data, 0.0, 0.02)\n            elif init_type == \'xavier\':\n                init.xavier_normal(m.weight.data, gain=math.sqrt(2))\n            elif init_type == \'kaiming\':\n                init.kaiming_normal(m.weight.data, a=0, mode=\'fan_in\')\n            elif init_type == \'orthogonal\':\n                init.orthogonal(m.weight.data, gain=math.sqrt(2))\n            elif init_type == \'default\':\n                pass\n            else:\n                assert 0, ""Unsupported initialization: {}"".format(init_type)\n            if hasattr(m, \'bias\') and m.bias is not None:\n                init.constant(m.bias.data, 0.0)\n\n    return init_fun'"
util/visualizer.py,0,"b'import numpy as np\nimport os\nimport ntpath\nimport time\nfrom . import util\nfrom . import html\n\nclass Visualizer():\n    def __init__(self, opt):\n        # self.opt = opt\n        self.display_id = opt.display_id\n        self.use_html = opt.isTrain and not opt.no_html\n        self.win_size = opt.display_winsize\n        self.name = opt.name\n        if self.display_id > 0:\n            import visdom\n            self.vis = visdom.Visdom(port = opt.display_port)\n            self.display_single_pane_ncols = opt.display_single_pane_ncols\n\n        if self.use_html:\n            self.web_dir = os.path.join(opt.checkpoints_dir, opt.name, \'web\')\n            self.img_dir = os.path.join(self.web_dir, \'images\')\n            print(\'create web directory %s...\' % self.web_dir)\n            util.mkdirs([self.web_dir, self.img_dir])\n        self.log_name = os.path.join(opt.checkpoints_dir, opt.name, \'loss_log.txt\')\n        with open(self.log_name, ""a"") as log_file:\n            now = time.strftime(""%c"")\n            log_file.write(\'================ Training Loss (%s) ================\\n\' % now)\n\n    # |visuals|: dictionary of images to display or save\n    def display_current_results(self, visuals, epoch):\n        if self.display_id > 0: # show images in the browser\n            if self.display_single_pane_ncols > 0:\n                h, w = next(iter(visuals.values())).shape[:2]\n                table_css = """"""<style>\n    table {border-collapse: separate; border-spacing:4px; white-space:nowrap; text-align:center}\n    table td {width: %dpx; height: %dpx; padding: 4px; outline: 4px solid black}\n</style>"""""" % (w, h)\n                ncols = self.display_single_pane_ncols\n                title = self.name\n                label_html = \'\'\n                label_html_row = \'\'\n                nrows = int(np.ceil(len(visuals.items()) / ncols))\n                images = []\n                idx = 0\n                for label, image_numpy in visuals.items():\n                    label_html_row += \'<td>%s</td>\' % label\n                    images.append(image_numpy.transpose([2, 0, 1]))\n                    idx += 1\n                    if idx % ncols == 0:\n                        label_html += \'<tr>%s</tr>\' % label_html_row\n                        label_html_row = \'\'\n                white_image = np.ones_like(image_numpy.transpose([2, 0, 1]))*255\n                while idx % ncols != 0:\n                    images.append(white_image)\n                    label_html_row += \'<td></td>\'\n                    idx += 1\n                if label_html_row != \'\':\n                    label_html += \'<tr>%s</tr>\' % label_html_row\n                # pane col = image row\n                self.vis.images(images, nrow=ncols, win=self.display_id + 1,\n                                padding=2, opts=dict(title=title + \' images\'))\n                label_html = \'<table>%s</table>\' % label_html\n                self.vis.text(table_css + label_html, win = self.display_id + 2,\n                              opts=dict(title=title + \' labels\'))\n            else:\n                idx = 1\n                for label, image_numpy in visuals.items():\n                    #image_numpy = np.flipud(image_numpy)\n                    self.vis.image(image_numpy.transpose([2,0,1]), opts=dict(title=label),\n                                       win=self.display_id + idx)\n                    idx += 1\n\n        if self.use_html: # save images to a html file\n            for label, image_numpy in visuals.items():\n                img_path = os.path.join(self.img_dir, \'epoch%.3d_%s.png\' % (epoch, label))\n                util.save_image(image_numpy, img_path)\n            # update website\n            webpage = html.HTML(self.web_dir, \'Experiment name = %s\' % self.name, reflesh=1)\n            for n in range(epoch, 0, -1):\n                webpage.add_header(\'epoch [%d]\' % n)\n                ims = []\n                txts = []\n                links = []\n\n                for label, image_numpy in visuals.items():\n                    img_path = \'epoch%.3d_%s.png\' % (n, label)\n                    ims.append(img_path)\n                    txts.append(label)\n                    links.append(img_path)\n                webpage.add_images(ims, txts, links, width=self.win_size)\n            webpage.save()\n\n    # errors: dictionary of error labels and values\n    def plot_current_errors(self, epoch, counter_ratio, opt, errors):\n        if not hasattr(self, \'plot_data\'):\n            self.plot_data = {\'X\':[],\'Y\':[], \'legend\':list(errors.keys())}\n        self.plot_data[\'X\'].append(epoch + counter_ratio)\n        self.plot_data[\'Y\'].append([errors[k] for k in self.plot_data[\'legend\']])\n        self.vis.line(\n            X=np.stack([np.array(self.plot_data[\'X\'])]*len(self.plot_data[\'legend\']),1),\n            Y=np.array(self.plot_data[\'Y\']),\n            opts={\n                \'title\': self.name + \' loss over time\',\n                \'legend\': self.plot_data[\'legend\'],\n                \'xlabel\': \'epoch\',\n                \'ylabel\': \'loss\'},\n            win=self.display_id)\n\n    # errors: same format as |errors| of plotCurrentErrors\n    def print_current_errors(self, epoch, i, errors, t):\n        message = \'(epoch: %d, iters: %d, time: %.3f) \' % (epoch, i, t)\n        for k, v in errors.items():\n            message += \'%s: %.3f \' % (k, v)\n\n        print(message)\n        with open(self.log_name, ""a"") as log_file:\n            log_file.write(\'%s\\n\' % message)\n\n    # save image to the disk\n    def save_images(self, webpage, visuals, image_path):\n        image_dir = webpage.get_image_dir()\n        short_path = ntpath.basename(image_path[0])\n        name = os.path.splitext(short_path)[0]\n\n        webpage.add_header(name)\n        ims = []\n        txts = []\n        links = []\n\n        for label, image_numpy in visuals.items():\n            image_name = \'%s_%s.png\' % (name, label)\n            save_path = os.path.join(image_dir, image_name)\n            util.save_image(image_numpy, save_path)\n\n            ims.append(image_name)\n            txts.append(label)\n            links.append(image_name)\n        webpage.add_images(ims, txts, links, width=self.win_size)\n\n\n    def save_images_demo(self, webpage, visuals, image_path):\n        image_dir = webpage.get_image_dir()\n        short_path = ntpath.basename(image_path[0])\n        name = os.path.splitext(short_path)[0]\n\n        webpage.add_header(name)\n        ims = []\n        txts = []\n        links = []\n\n        for label, image_numpy in visuals.items():\n            image_name = \'%s.jpg\' % (name)\n            save_path = os.path.join(image_dir, image_name)\n            util.save_image(image_numpy, save_path)\n\n            ims.append(image_name)\n            txts.append(label)\n            links.append(image_name)\n        webpage.add_images(ims, txts, links, width=self.win_size)\n'"
lib/nn/__init__.py,0,"b'from .modules import *\nfrom .parallel import UserScatteredDataParallel, user_scattered_collate, async_copy_to\n'"
lib/utils/__init__.py,0,b'from .th import *\n'
lib/utils/th.py,3,"b""import torch\nfrom torch.autograd import Variable\nimport numpy as np\nimport collections\n\n__all__ = ['as_variable', 'as_numpy', 'mark_volatile']\n\ndef as_variable(obj):\n    if isinstance(obj, Variable):\n        return obj\n    if isinstance(obj, collections.Sequence):\n        return [as_variable(v) for v in obj]\n    elif isinstance(obj, collections.Mapping):\n        return {k: as_variable(v) for k, v in obj.items()}\n    else:\n        return Variable(obj)\n\ndef as_numpy(obj):\n    if isinstance(obj, collections.Sequence):\n        return [as_numpy(v) for v in obj]\n    elif isinstance(obj, collections.Mapping):\n        return {k: as_numpy(v) for k, v in obj.items()}\n    elif isinstance(obj, Variable):\n        return obj.data.cpu().numpy()\n    elif torch.is_tensor(obj):\n        return obj.cpu().numpy()\n    else:\n        return np.array(obj)\n\ndef mark_volatile(obj):\n    if torch.is_tensor(obj):\n        obj = Variable(obj)\n    if isinstance(obj, Variable):\n        obj.no_grad = True\n        return obj\n    elif isinstance(obj, collections.Mapping):\n        return {k: mark_volatile(o) for k, o in obj.items()}\n    elif isinstance(obj, collections.Sequence):\n        return [mark_volatile(o) for o in obj]\n    else:\n        return obj\n"""
lib/nn/modules/__init__.py,0,"b'# -*- coding: utf-8 -*-\n# File   : __init__.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n# \n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nfrom .batchnorm import SynchronizedBatchNorm1d, SynchronizedBatchNorm2d, SynchronizedBatchNorm3d\nfrom .replicate import DataParallelWithCallback, patch_replication_callback\n'"
lib/nn/modules/batchnorm.py,9,"b'# -*- coding: utf-8 -*-\n# File   : batchnorm.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n# \n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport collections\n\nimport torch\nimport torch.nn.functional as F\n\nfrom torch.nn.modules.batchnorm import _BatchNorm\nfrom torch.nn.parallel._functions import ReduceAddCoalesced, Broadcast\n\nfrom .comm import SyncMaster\n\n__all__ = [\'SynchronizedBatchNorm1d\', \'SynchronizedBatchNorm2d\', \'SynchronizedBatchNorm3d\']\n\n\ndef _sum_ft(tensor):\n    """"""sum over the first and last dimention""""""\n    return tensor.sum(dim=0).sum(dim=-1)\n\n\ndef _unsqueeze_ft(tensor):\n    """"""add new dementions at the front and the tail""""""\n    return tensor.unsqueeze(0).unsqueeze(-1)\n\n\n_ChildMessage = collections.namedtuple(\'_ChildMessage\', [\'sum\', \'ssum\', \'sum_size\'])\n_MasterMessage = collections.namedtuple(\'_MasterMessage\', [\'sum\', \'inv_std\'])\n\n\nclass _SynchronizedBatchNorm(_BatchNorm):\n    def __init__(self, num_features, eps=1e-5, momentum=0.001, affine=True):\n        super(_SynchronizedBatchNorm, self).__init__(num_features, eps=eps, momentum=momentum, affine=affine)\n\n        self._sync_master = SyncMaster(self._data_parallel_master)\n\n        self._is_parallel = False\n        self._parallel_id = None\n        self._slave_pipe = None\n\n        # customed batch norm statistics\n        self._moving_average_fraction = 1. - momentum\n        self.register_buffer(\'_tmp_running_mean\', torch.zeros(self.num_features))\n        self.register_buffer(\'_tmp_running_var\', torch.ones(self.num_features))\n        self.register_buffer(\'_running_iter\', torch.ones(1))\n        self._tmp_running_mean = self.running_mean.clone() * self._running_iter\n        self._tmp_running_var = self.running_var.clone() * self._running_iter\n\n    def forward(self, input):\n        # If it is not parallel computation or is in evaluation mode, use PyTorch\'s implementation.\n        if not (self._is_parallel and self.training):\n            return F.batch_norm(\n                input, self.running_mean, self.running_var, self.weight, self.bias,\n                self.training, self.momentum, self.eps)\n\n        # Resize the input to (B, C, -1).\n        input_shape = input.size()\n        input = input.view(input.size(0), self.num_features, -1)\n\n        # Compute the sum and square-sum.\n        sum_size = input.size(0) * input.size(2)\n        input_sum = _sum_ft(input)\n        input_ssum = _sum_ft(input ** 2)\n\n        # Reduce-and-broadcast the statistics.\n        if self._parallel_id == 0:\n            mean, inv_std = self._sync_master.run_master(_ChildMessage(input_sum, input_ssum, sum_size))\n        else:\n            mean, inv_std = self._slave_pipe.run_slave(_ChildMessage(input_sum, input_ssum, sum_size))\n\n        # Compute the output.\n        if self.affine:\n            # MJY:: Fuse the multiplication for speed.\n            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std * self.weight) + _unsqueeze_ft(self.bias)\n        else:\n            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std)\n\n        # Reshape it.\n        return output.view(input_shape)\n\n    def __data_parallel_replicate__(self, ctx, copy_id):\n        self._is_parallel = True\n        self._parallel_id = copy_id\n\n        # parallel_id == 0 means master device.\n        if self._parallel_id == 0:\n            ctx.sync_master = self._sync_master\n        else:\n            self._slave_pipe = ctx.sync_master.register_slave(copy_id)\n\n    def _data_parallel_master(self, intermediates):\n        """"""Reduce the sum and square-sum, compute the statistics, and broadcast it.""""""\n        intermediates = sorted(intermediates, key=lambda i: i[1].sum.get_device())\n\n        to_reduce = [i[1][:2] for i in intermediates]\n        to_reduce = [j for i in to_reduce for j in i]  # flatten\n        target_gpus = [i[1].sum.get_device() for i in intermediates]\n\n        sum_size = sum([i[1].sum_size for i in intermediates])\n        sum_, ssum = ReduceAddCoalesced.apply(target_gpus[0], 2, *to_reduce)\n\n        mean, inv_std = self._compute_mean_std(sum_, ssum, sum_size)\n\n        broadcasted = Broadcast.apply(target_gpus, mean, inv_std)\n\n        outputs = []\n        for i, rec in enumerate(intermediates):\n            outputs.append((rec[0], _MasterMessage(*broadcasted[i*2:i*2+2])))\n\n        return outputs\n\n    def _add_weighted(self, dest, delta, alpha=1, beta=1, bias=0):\n        """"""return *dest* by `dest := dest*alpha + delta*beta + bias`""""""\n        return dest * alpha + delta * beta + bias\n\n    def _compute_mean_std(self, sum_, ssum, size):\n        """"""Compute the mean and standard-deviation with sum and square-sum. This method\n        also maintains the moving average on the master device.""""""\n        assert size > 1, \'BatchNorm computes unbiased standard-deviation, which requires size > 1.\'\n        mean = sum_ / size\n        sumvar = ssum - sum_ * mean\n        unbias_var = sumvar / (size - 1)\n        bias_var = sumvar / size\n\n        self._tmp_running_mean = self._add_weighted(self._tmp_running_mean, mean.data, alpha=self._moving_average_fraction)\n        self._tmp_running_var = self._add_weighted(self._tmp_running_var, unbias_var.data, alpha=self._moving_average_fraction)\n        self._running_iter = self._add_weighted(self._running_iter, 1, alpha=self._moving_average_fraction)\n\n        self.running_mean = self._tmp_running_mean / self._running_iter\n        self.running_var = self._tmp_running_var / self._running_iter\n\n        return mean, bias_var.clamp(self.eps) ** -0.5\n\n\nclass SynchronizedBatchNorm1d(_SynchronizedBatchNorm):\n    r""""""Applies Synchronized Batch Normalization over a 2d or 3d input that is seen as a\n    mini-batch.\n\n    .. math::\n\n        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n\n    This module differs from the built-in PyTorch BatchNorm1d as the mean and\n    standard-deviation are reduced across all devices during training.\n\n    For example, when one uses `nn.DataParallel` to wrap the network during\n    training, PyTorch\'s implementation normalize the tensor on each device using\n    the statistics only on that device, which accelerated the computation and\n    is also easy to implement, but the statistics might be inaccurate.\n    Instead, in this synchronized version, the statistics will be computed\n    over all training samples distributed on multiple devices.\n    \n    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n    as the built-in PyTorch implementation.\n\n    The mean and standard-deviation are calculated per-dimension over\n    the mini-batches and gamma and beta are learnable parameter vectors\n    of size C (where C is the input size).\n\n    During training, this layer keeps a running estimate of its computed mean\n    and variance. The running sum is kept with a default momentum of 0.1.\n\n    During evaluation, this running mean/variance is used for normalization.\n\n    Because the BatchNorm is done over the `C` dimension, computing statistics\n    on `(N, L)` slices, it\'s common terminology to call this Temporal BatchNorm\n\n    Args:\n        num_features: num_features from an expected input of size\n            `batch_size x num_features [x width]`\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Default: 0.1\n        affine: a boolean value that when set to ``True``, gives the layer learnable\n            affine parameters. Default: ``True``\n\n    Shape:\n        - Input: :math:`(N, C)` or :math:`(N, C, L)`\n        - Output: :math:`(N, C)` or :math:`(N, C, L)` (same shape as input)\n\n    Examples:\n        >>> # With Learnable Parameters\n        >>> m = SynchronizedBatchNorm1d(100)\n        >>> # Without Learnable Parameters\n        >>> m = SynchronizedBatchNorm1d(100, affine=False)\n        >>> input = torch.autograd.Variable(torch.randn(20, 100))\n        >>> output = m(input)\n    """"""\n\n    def _check_input_dim(self, input):\n        if input.dim() != 2 and input.dim() != 3:\n            raise ValueError(\'expected 2D or 3D input (got {}D input)\'\n                             .format(input.dim()))\n        super(SynchronizedBatchNorm1d, self)._check_input_dim(input)\n\n\nclass SynchronizedBatchNorm2d(_SynchronizedBatchNorm):\n    r""""""Applies Batch Normalization over a 4d input that is seen as a mini-batch\n    of 3d inputs\n\n    .. math::\n\n        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n\n    This module differs from the built-in PyTorch BatchNorm2d as the mean and\n    standard-deviation are reduced across all devices during training.\n\n    For example, when one uses `nn.DataParallel` to wrap the network during\n    training, PyTorch\'s implementation normalize the tensor on each device using\n    the statistics only on that device, which accelerated the computation and\n    is also easy to implement, but the statistics might be inaccurate.\n    Instead, in this synchronized version, the statistics will be computed\n    over all training samples distributed on multiple devices.\n    \n    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n    as the built-in PyTorch implementation.\n\n    The mean and standard-deviation are calculated per-dimension over\n    the mini-batches and gamma and beta are learnable parameter vectors\n    of size C (where C is the input size).\n\n    During training, this layer keeps a running estimate of its computed mean\n    and variance. The running sum is kept with a default momentum of 0.1.\n\n    During evaluation, this running mean/variance is used for normalization.\n\n    Because the BatchNorm is done over the `C` dimension, computing statistics\n    on `(N, H, W)` slices, it\'s common terminology to call this Spatial BatchNorm\n\n    Args:\n        num_features: num_features from an expected input of\n            size batch_size x num_features x height x width\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Default: 0.1\n        affine: a boolean value that when set to ``True``, gives the layer learnable\n            affine parameters. Default: ``True``\n\n    Shape:\n        - Input: :math:`(N, C, H, W)`\n        - Output: :math:`(N, C, H, W)` (same shape as input)\n\n    Examples:\n        >>> # With Learnable Parameters\n        >>> m = SynchronizedBatchNorm2d(100)\n        >>> # Without Learnable Parameters\n        >>> m = SynchronizedBatchNorm2d(100, affine=False)\n        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45))\n        >>> output = m(input)\n    """"""\n\n    def _check_input_dim(self, input):\n        if input.dim() != 4:\n            raise ValueError(\'expected 4D input (got {}D input)\'\n                             .format(input.dim()))\n        super(SynchronizedBatchNorm2d, self)._check_input_dim(input)\n\n\nclass SynchronizedBatchNorm3d(_SynchronizedBatchNorm):\n    r""""""Applies Batch Normalization over a 5d input that is seen as a mini-batch\n    of 4d inputs\n\n    .. math::\n\n        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n\n    This module differs from the built-in PyTorch BatchNorm3d as the mean and\n    standard-deviation are reduced across all devices during training.\n\n    For example, when one uses `nn.DataParallel` to wrap the network during\n    training, PyTorch\'s implementation normalize the tensor on each device using\n    the statistics only on that device, which accelerated the computation and\n    is also easy to implement, but the statistics might be inaccurate.\n    Instead, in this synchronized version, the statistics will be computed\n    over all training samples distributed on multiple devices.\n    \n    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n    as the built-in PyTorch implementation.\n\n    The mean and standard-deviation are calculated per-dimension over\n    the mini-batches and gamma and beta are learnable parameter vectors\n    of size C (where C is the input size).\n\n    During training, this layer keeps a running estimate of its computed mean\n    and variance. The running sum is kept with a default momentum of 0.1.\n\n    During evaluation, this running mean/variance is used for normalization.\n\n    Because the BatchNorm is done over the `C` dimension, computing statistics\n    on `(N, D, H, W)` slices, it\'s common terminology to call this Volumetric BatchNorm\n    or Spatio-temporal BatchNorm\n\n    Args:\n        num_features: num_features from an expected input of\n            size batch_size x num_features x depth x height x width\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Default: 0.1\n        affine: a boolean value that when set to ``True``, gives the layer learnable\n            affine parameters. Default: ``True``\n\n    Shape:\n        - Input: :math:`(N, C, D, H, W)`\n        - Output: :math:`(N, C, D, H, W)` (same shape as input)\n\n    Examples:\n        >>> # With Learnable Parameters\n        >>> m = SynchronizedBatchNorm3d(100)\n        >>> # Without Learnable Parameters\n        >>> m = SynchronizedBatchNorm3d(100, affine=False)\n        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45, 10))\n        >>> output = m(input)\n    """"""\n\n    def _check_input_dim(self, input):\n        if input.dim() != 5:\n            raise ValueError(\'expected 5D input (got {}D input)\'\n                             .format(input.dim()))\n        super(SynchronizedBatchNorm3d, self)._check_input_dim(input)\n'"
lib/nn/modules/comm.py,0,"b'# -*- coding: utf-8 -*-\n# File   : comm.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n# \n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport queue\nimport collections\nimport threading\n\n__all__ = [\'FutureResult\', \'SlavePipe\', \'SyncMaster\']\n\n\nclass FutureResult(object):\n    """"""A thread-safe future implementation. Used only as one-to-one pipe.""""""\n\n    def __init__(self):\n        self._result = None\n        self._lock = threading.Lock()\n        self._cond = threading.Condition(self._lock)\n\n    def put(self, result):\n        with self._lock:\n            assert self._result is None, \'Previous result has\\\'t been fetched.\'\n            self._result = result\n            self._cond.notify()\n\n    def get(self):\n        with self._lock:\n            if self._result is None:\n                self._cond.wait()\n\n            res = self._result\n            self._result = None\n            return res\n\n\n_MasterRegistry = collections.namedtuple(\'MasterRegistry\', [\'result\'])\n_SlavePipeBase = collections.namedtuple(\'_SlavePipeBase\', [\'identifier\', \'queue\', \'result\'])\n\n\nclass SlavePipe(_SlavePipeBase):\n    """"""Pipe for master-slave communication.""""""\n\n    def run_slave(self, msg):\n        self.queue.put((self.identifier, msg))\n        ret = self.result.get()\n        self.queue.put(True)\n        return ret\n\n\nclass SyncMaster(object):\n    """"""An abstract `SyncMaster` object.\n\n    - During the replication, as the data parallel will trigger an callback of each module, all slave devices should\n    call `register(id)` and obtain an `SlavePipe` to communicate with the master.\n    - During the forward pass, master device invokes `run_master`, all messages from slave devices will be collected,\n    and passed to a registered callback.\n    - After receiving the messages, the master device should gather the information and determine to message passed\n    back to each slave devices.\n    """"""\n\n    def __init__(self, master_callback):\n        """"""\n\n        Args:\n            master_callback: a callback to be invoked after having collected messages from slave devices.\n        """"""\n        self._master_callback = master_callback\n        self._queue = queue.Queue()\n        self._registry = collections.OrderedDict()\n        self._activated = False\n\n    def register_slave(self, identifier):\n        """"""\n        Register an slave device.\n\n        Args:\n            identifier: an identifier, usually is the device id.\n\n        Returns: a `SlavePipe` object which can be used to communicate with the master device.\n\n        """"""\n        if self._activated:\n            assert self._queue.empty(), \'Queue is not clean before next initialization.\'\n            self._activated = False\n            self._registry.clear()\n        future = FutureResult()\n        self._registry[identifier] = _MasterRegistry(future)\n        return SlavePipe(identifier, self._queue, future)\n\n    def run_master(self, master_msg):\n        """"""\n        Main entry for the master device in each forward pass.\n        The messages were first collected from each devices (including the master device), and then\n        an callback will be invoked to compute the message to be sent back to each devices\n        (including the master device).\n\n        Args:\n            master_msg: the message that the master want to send to itself. This will be placed as the first\n            message when calling `master_callback`. For detailed usage, see `_SynchronizedBatchNorm` for an example.\n\n        Returns: the message to be sent back to the master device.\n\n        """"""\n        self._activated = True\n\n        intermediates = [(0, master_msg)]\n        for i in range(self.nr_slaves):\n            intermediates.append(self._queue.get())\n\n        results = self._master_callback(intermediates)\n        assert results[0][0] == 0, \'The first result should belongs to the master.\'\n\n        for i, res in results:\n            if i == 0:\n                continue\n            self._registry[i].result.put(res)\n\n        for i in range(self.nr_slaves):\n            assert self._queue.get() is True\n\n        return results[0][1]\n\n    @property\n    def nr_slaves(self):\n        return len(self._registry)\n'"
lib/nn/modules/replicate.py,1,"b'# -*- coding: utf-8 -*-\n# File   : replicate.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n# \n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport functools\n\nfrom torch.nn.parallel.data_parallel import DataParallel\n\n__all__ = [\n    \'CallbackContext\',\n    \'execute_replication_callbacks\',\n    \'DataParallelWithCallback\',\n    \'patch_replication_callback\'\n]\n\n\nclass CallbackContext(object):\n    pass\n\n\ndef execute_replication_callbacks(modules):\n    """"""\n    Execute an replication callback `__data_parallel_replicate__` on each module created by original replication.\n\n    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\n\n    Note that, as all modules are isomorphism, we assign each sub-module with a context\n    (shared among multiple copies of this module on different devices).\n    Through this context, different copies can share some information.\n\n    We guarantee that the callback on the master copy (the first copy) will be called ahead of calling the callback\n    of any slave copies.\n    """"""\n    master_copy = modules[0]\n    nr_modules = len(list(master_copy.modules()))\n    ctxs = [CallbackContext() for _ in range(nr_modules)]\n\n    for i, module in enumerate(modules):\n        for j, m in enumerate(module.modules()):\n            if hasattr(m, \'__data_parallel_replicate__\'):\n                m.__data_parallel_replicate__(ctxs[j], i)\n\n\nclass DataParallelWithCallback(DataParallel):\n    """"""\n    Data Parallel with a replication callback.\n\n    An replication callback `__data_parallel_replicate__` of each module will be invoked after being created by\n    original `replicate` function.\n    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\n\n    Examples:\n        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        > sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n        # sync_bn.__data_parallel_replicate__ will be invoked.\n    """"""\n\n    def replicate(self, module, device_ids):\n        modules = super(DataParallelWithCallback, self).replicate(module, device_ids)\n        execute_replication_callbacks(modules)\n        return modules\n\n\ndef patch_replication_callback(data_parallel):\n    """"""\n    Monkey-patch an existing `DataParallel` object. Add the replication callback.\n    Useful when you have customized `DataParallel` implementation.\n\n    Examples:\n        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        > sync_bn = DataParallel(sync_bn, device_ids=[0, 1])\n        > patch_replication_callback(sync_bn)\n        # this is equivalent to\n        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        > sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n    """"""\n\n    assert isinstance(data_parallel, DataParallel)\n\n    old_replicate = data_parallel.replicate\n\n    @functools.wraps(old_replicate)\n    def new_replicate(module, device_ids):\n        modules = old_replicate(module, device_ids)\n        execute_replication_callbacks(modules)\n        return modules\n\n    data_parallel.replicate = new_replicate\n'"
lib/nn/modules/unittest.py,1,"b""# -*- coding: utf-8 -*-\n# File   : unittest.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n# \n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport unittest\n\nimport numpy as np\nfrom torch.autograd import Variable\n\n\ndef as_numpy(v):\n    if isinstance(v, Variable):\n        v = v.data\n    return v.cpu().numpy()\n\n\nclass TorchTestCase(unittest.TestCase):\n    def assertTensorClose(self, a, b, atol=1e-3, rtol=1e-3):\n        npa, npb = as_numpy(a), as_numpy(b)\n        self.assertTrue(\n                np.allclose(npa, npb, atol=atol),\n                'Tensor close check failed\\n{}\\n{}\\nadiff={}, rdiff={}'.format(a, b, np.abs(npa - npb).max(), np.abs((npa - npb) / np.fmax(npa, 1e-5)).max())\n        )\n"""
lib/nn/parallel/__init__.py,0,"b'from .data_parallel import UserScatteredDataParallel, user_scattered_collate, async_copy_to\n'"
lib/nn/parallel/data_parallel.py,5,"b'# -*- coding: utf8 -*-\n\nimport torch.cuda as cuda\nimport torch.nn as nn\nimport torch\nimport collections\nfrom torch.nn.parallel._functions import Gather\n\n\n__all__ = [\'UserScatteredDataParallel\', \'user_scattered_collate\', \'async_copy_to\']\n\n\ndef async_copy_to(obj, dev, main_stream=None):\n    if torch.is_tensor(obj):\n        v = obj.cuda(dev, non_blocking=True)\n        if main_stream is not None:\n            v.data.record_stream(main_stream)\n        return v\n    elif isinstance(obj, collections.Mapping):\n        return {k: async_copy_to(o, dev, main_stream) for k, o in obj.items()}\n    elif isinstance(obj, collections.Sequence):\n        return [async_copy_to(o, dev, main_stream) for o in obj]\n    else:\n        return obj\n\n\ndef dict_gather(outputs, target_device, dim=0):\n    """"""\n    Gathers variables from different GPUs on a specified device\n      (-1 means the CPU), with dictionary support.\n    """"""\n    def gather_map(outputs):\n        out = outputs[0]\n        if torch.is_tensor(out):\n            # MJY(20180330) HACK:: force nr_dims > 0\n            if out.dim() == 0:\n                outputs = [o.unsqueeze(0) for o in outputs]\n            return Gather.apply(target_device, dim, *outputs)\n        elif out is None:\n            return None\n        elif isinstance(out, collections.Mapping):\n            return {k: gather_map([o[k] for o in outputs]) for k in out}\n        elif isinstance(out, collections.Sequence):\n            return type(out)(map(gather_map, zip(*outputs)))\n    return gather_map(outputs)\n\n\nclass DictGatherDataParallel(nn.DataParallel):\n    def gather(self, outputs, output_device):\n        return dict_gather(outputs, output_device, dim=self.dim)\n\n\nclass UserScatteredDataParallel(DictGatherDataParallel):\n    def scatter(self, inputs, kwargs, device_ids):\n        assert len(inputs) == 1\n        inputs = inputs[0]\n        inputs = _async_copy_stream(inputs, device_ids)\n        inputs = [[i] for i in inputs]\n        assert len(kwargs) == 0\n        kwargs = [{} for _ in range(len(inputs))]\n\n        return inputs, kwargs\n\n\ndef user_scattered_collate(batch):\n    return batch\n\n\ndef _async_copy(inputs, device_ids):\n    nr_devs = len(device_ids)\n    assert type(inputs) in (tuple, list)\n    assert len(inputs) == nr_devs\n\n    outputs = []\n    for i, dev in zip(inputs, device_ids):\n        with cuda.device(dev):\n            outputs.append(async_copy_to(i, dev))\n\n    return tuple(outputs)\n\n\ndef _async_copy_stream(inputs, device_ids):\n    nr_devs = len(device_ids)\n    assert type(inputs) in (tuple, list)\n    assert len(inputs) == nr_devs\n\n    outputs = []\n    streams = [_get_stream(d) for d in device_ids]\n    for i, dev, stream in zip(inputs, device_ids, streams):\n        with cuda.device(dev):\n            main_stream = cuda.current_stream()\n            with cuda.stream(stream):\n                outputs.append(async_copy_to(i, dev, main_stream=main_stream))\n            main_stream.wait_stream(stream)\n\n    return outputs\n\n\n""""""Adapted from: torch/nn/parallel/_functions.py""""""\n# background streams used for copying\n_streams = None\n\n\ndef _get_stream(device):\n    """"""Gets a background stream for copying between CPU and GPU""""""\n    global _streams\n    if device == -1:\n        return None\n    if _streams is None:\n        _streams = [None] * cuda.device_count()\n    if _streams[device] is None: _streams[device] = cuda.Stream(device)\n    return _streams[device]\n'"
lib/utils/data/__init__.py,0,"b'\nfrom .dataset import Dataset, TensorDataset, ConcatDataset\nfrom .dataloader import DataLoader\n'"
lib/utils/data/dataloader.py,24,"b'import torch\nimport torch.multiprocessing as multiprocessing\nfrom torch._C import _set_worker_signal_handlers, _update_worker_pids, \\\n    _remove_worker_pids, _error_if_any_worker_fails\nfrom .sampler import SequentialSampler, RandomSampler, BatchSampler\nimport signal\nimport functools\nimport collections\nimport re\nimport sys\nimport threading\nimport traceback\nfrom torch._six import string_classes, int_classes\nimport numpy as np\n\nif sys.version_info[0] == 2:\n    import Queue as queue\nelse:\n    import queue\n\n\nclass ExceptionWrapper(object):\n    r""Wraps an exception plus traceback to communicate across threads""\n\n    def __init__(self, exc_info):\n        self.exc_type = exc_info[0]\n        self.exc_msg = """".join(traceback.format_exception(*exc_info))\n\n\n_use_shared_memory = False\n""""""Whether to use shared memory in default_collate""""""\n\n\ndef _worker_loop(dataset, index_queue, data_queue, collate_fn, seed, init_fn, worker_id):\n    global _use_shared_memory\n    _use_shared_memory = True\n\n    # Intialize C side signal handlers for SIGBUS and SIGSEGV. Python signal\n    # module\'s handlers are executed after Python returns from C low-level\n    # handlers, likely when the same fatal signal happened again already.\n    # https://docs.python.org/3/library/signal.html Sec. 18.8.1.1\n    _set_worker_signal_handlers()\n\n    torch.set_num_threads(1)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n\n    if init_fn is not None:\n        init_fn(worker_id)\n\n    while True:\n        r = index_queue.get()\n        if r is None:\n            break\n        idx, batch_indices = r\n        try:\n            samples = collate_fn([dataset[i] for i in batch_indices])\n        except Exception:\n            data_queue.put((idx, ExceptionWrapper(sys.exc_info())))\n        else:\n            data_queue.put((idx, samples))\n\n\ndef _worker_manager_loop(in_queue, out_queue, done_event, pin_memory, device_id):\n    if pin_memory:\n        torch.cuda.set_device(device_id)\n\n    while True:\n        try:\n            r = in_queue.get()\n        except Exception:\n            if done_event.is_set():\n                return\n            raise\n        if r is None:\n            break\n        if isinstance(r[1], ExceptionWrapper):\n            out_queue.put(r)\n            continue\n        idx, batch = r\n        try:\n            if pin_memory:\n                batch = pin_memory_batch(batch)\n        except Exception:\n            out_queue.put((idx, ExceptionWrapper(sys.exc_info())))\n        else:\n            out_queue.put((idx, batch))\n\nnumpy_type_map = {\n    \'float64\': torch.DoubleTensor,\n    \'float32\': torch.FloatTensor,\n    \'float16\': torch.HalfTensor,\n    \'int64\': torch.LongTensor,\n    \'int32\': torch.IntTensor,\n    \'int16\': torch.ShortTensor,\n    \'int8\': torch.CharTensor,\n    \'uint8\': torch.ByteTensor,\n}\n\n\ndef default_collate(batch):\n    ""Puts each data field into a tensor with outer dimension batch size""\n\n    error_msg = ""batch must contain tensors, numbers, dicts or lists; found {}""\n    elem_type = type(batch[0])\n    if torch.is_tensor(batch[0]):\n        out = None\n        if _use_shared_memory:\n            # If we\'re in a background process, concatenate directly into a\n            # shared memory tensor to avoid an extra copy\n            numel = sum([x.numel() for x in batch])\n            storage = batch[0].storage()._new_shared(numel)\n            out = batch[0].new(storage)\n        return torch.stack(batch, 0, out=out)\n    elif elem_type.__module__ == \'numpy\' and elem_type.__name__ != \'str_\' \\\n            and elem_type.__name__ != \'string_\':\n        elem = batch[0]\n        if elem_type.__name__ == \'ndarray\':\n            # array of string classes and object\n            if re.search(\'[SaUO]\', elem.dtype.str) is not None:\n                raise TypeError(error_msg.format(elem.dtype))\n\n            return torch.stack([torch.from_numpy(b) for b in batch], 0)\n        if elem.shape == ():  # scalars\n            py_type = float if elem.dtype.name.startswith(\'float\') else int\n            return numpy_type_map[elem.dtype.name](list(map(py_type, batch)))\n    elif isinstance(batch[0], int_classes):\n        return torch.LongTensor(batch)\n    elif isinstance(batch[0], float):\n        return torch.DoubleTensor(batch)\n    elif isinstance(batch[0], string_classes):\n        return batch\n    elif isinstance(batch[0], collections.Mapping):\n        return {key: default_collate([d[key] for d in batch]) for key in batch[0]}\n    elif isinstance(batch[0], collections.Sequence):\n        transposed = zip(*batch)\n        return [default_collate(samples) for samples in transposed]\n\n    raise TypeError((error_msg.format(type(batch[0]))))\n\n\ndef pin_memory_batch(batch):\n    if torch.is_tensor(batch):\n        return batch.pin_memory()\n    elif isinstance(batch, string_classes):\n        return batch\n    elif isinstance(batch, collections.Mapping):\n        return {k: pin_memory_batch(sample) for k, sample in batch.items()}\n    elif isinstance(batch, collections.Sequence):\n        return [pin_memory_batch(sample) for sample in batch]\n    else:\n        return batch\n\n\n_SIGCHLD_handler_set = False\n""""""Whether SIGCHLD handler is set for DataLoader worker failures. Only one\nhandler needs to be set for all DataLoaders in a process.""""""\n\n\ndef _set_SIGCHLD_handler():\n    # Windows doesn\'t support SIGCHLD handler\n    if sys.platform == \'win32\':\n        return\n    # can\'t set signal in child threads\n    if not isinstance(threading.current_thread(), threading._MainThread):\n        return\n    global _SIGCHLD_handler_set\n    if _SIGCHLD_handler_set:\n        return\n    previous_handler = signal.getsignal(signal.SIGCHLD)\n    if not callable(previous_handler):\n        previous_handler = None\n\n    def handler(signum, frame):\n        # This following call uses `waitid` with WNOHANG from C side. Therefore,\n        # Python can still get and update the process status successfully.\n        _error_if_any_worker_fails()\n        if previous_handler is not None:\n            previous_handler(signum, frame)\n\n    signal.signal(signal.SIGCHLD, handler)\n    _SIGCHLD_handler_set = True\n\n\nclass DataLoaderIter(object):\n    ""Iterates once over the DataLoader\'s dataset, as specified by the sampler""\n\n    def __init__(self, loader):\n        self.dataset = loader.dataset\n        self.collate_fn = loader.collate_fn\n        self.batch_sampler = loader.batch_sampler\n        self.num_workers = loader.num_workers\n        self.pin_memory = loader.pin_memory and torch.cuda.is_available()\n        self.timeout = loader.timeout\n        self.done_event = threading.Event()\n\n        self.sample_iter = iter(self.batch_sampler)\n\n        if self.num_workers > 0:\n            self.worker_init_fn = loader.worker_init_fn\n            self.index_queue = multiprocessing.SimpleQueue()\n            self.worker_result_queue = multiprocessing.SimpleQueue()\n            self.batches_outstanding = 0\n            self.worker_pids_set = False\n            self.shutdown = False\n            self.send_idx = 0\n            self.rcvd_idx = 0\n            self.reorder_dict = {}\n\n            base_seed = torch.LongTensor(1).random_(0, 2**31-1)[0]\n            self.workers = [\n                multiprocessing.Process(\n                    target=_worker_loop,\n                    args=(self.dataset, self.index_queue, self.worker_result_queue, self.collate_fn,\n                          base_seed + i, self.worker_init_fn, i))\n                for i in range(self.num_workers)]\n\n            if self.pin_memory or self.timeout > 0:\n                self.data_queue = queue.Queue()\n                if self.pin_memory:\n                    maybe_device_id = torch.cuda.current_device()\n                else:\n                    # do not initialize cuda context if not necessary\n                    maybe_device_id = None\n                self.worker_manager_thread = threading.Thread(\n                    target=_worker_manager_loop,\n                    args=(self.worker_result_queue, self.data_queue, self.done_event, self.pin_memory,\n                          maybe_device_id))\n                self.worker_manager_thread.daemon = True\n                self.worker_manager_thread.start()\n            else:\n                self.data_queue = self.worker_result_queue\n\n            for w in self.workers:\n                w.daemon = True  # ensure that the worker exits on process exit\n                w.start()\n\n            _update_worker_pids(id(self), tuple(w.pid for w in self.workers))\n            _set_SIGCHLD_handler()\n            self.worker_pids_set = True\n\n            # prime the prefetch loop\n            for _ in range(2 * self.num_workers):\n                self._put_indices()\n\n    def __len__(self):\n        return len(self.batch_sampler)\n\n    def _get_batch(self):\n        if self.timeout > 0:\n            try:\n                return self.data_queue.get(timeout=self.timeout)\n            except queue.Empty:\n                raise RuntimeError(\'DataLoader timed out after {} seconds\'.format(self.timeout))\n        else:\n            return self.data_queue.get()\n\n    def __next__(self):\n        if self.num_workers == 0:  # same-process loading\n            indices = next(self.sample_iter)  # may raise StopIteration\n            batch = self.collate_fn([self.dataset[i] for i in indices])\n            if self.pin_memory:\n                batch = pin_memory_batch(batch)\n            return batch\n\n        # check if the next sample has already been generated\n        if self.rcvd_idx in self.reorder_dict:\n            batch = self.reorder_dict.pop(self.rcvd_idx)\n            return self._process_next_batch(batch)\n\n        if self.batches_outstanding == 0:\n            self._shutdown_workers()\n            raise StopIteration\n\n        while True:\n            assert (not self.shutdown and self.batches_outstanding > 0)\n            idx, batch = self._get_batch()\n            self.batches_outstanding -= 1\n            if idx != self.rcvd_idx:\n                # store out-of-order samples\n                self.reorder_dict[idx] = batch\n                continue\n            return self._process_next_batch(batch)\n\n    next = __next__  # Python 2 compatibility\n\n    def __iter__(self):\n        return self\n\n    def _put_indices(self):\n        assert self.batches_outstanding < 2 * self.num_workers\n        indices = next(self.sample_iter, None)\n        if indices is None:\n            return\n        self.index_queue.put((self.send_idx, indices))\n        self.batches_outstanding += 1\n        self.send_idx += 1\n\n    def _process_next_batch(self, batch):\n        self.rcvd_idx += 1\n        self._put_indices()\n        if isinstance(batch, ExceptionWrapper):\n            raise batch.exc_type(batch.exc_msg)\n        return batch\n\n    def __getstate__(self):\n        # TODO: add limited pickling support for sharing an iterator\n        # across multiple threads for HOGWILD.\n        # Probably the best way to do this is by moving the sample pushing\n        # to a separate thread and then just sharing the data queue\n        # but signalling the end is tricky without a non-blocking API\n        raise NotImplementedError(""DataLoaderIterator cannot be pickled"")\n\n    def _shutdown_workers(self):\n        try:\n            if not self.shutdown:\n                self.shutdown = True\n                self.done_event.set()\n                # if worker_manager_thread is waiting to put\n                while not self.data_queue.empty():\n                    self.data_queue.get()\n                for _ in self.workers:\n                    self.index_queue.put(None)\n                # done_event should be sufficient to exit worker_manager_thread,\n                # but be safe here and put another None\n                self.worker_result_queue.put(None)\n        finally:\n            # removes pids no matter what\n            if self.worker_pids_set:\n                _remove_worker_pids(id(self))\n                self.worker_pids_set = False\n\n    def __del__(self):\n        if self.num_workers > 0:\n            self._shutdown_workers()\n\n\nclass DataLoader(object):\n    """"""\n    Data loader. Combines a dataset and a sampler, and provides\n    single- or multi-process iterators over the dataset.\n\n    Arguments:\n        dataset (Dataset): dataset from which to load the data.\n        batch_size (int, optional): how many samples per batch to load\n            (default: 1).\n        shuffle (bool, optional): set to ``True`` to have the data reshuffled\n            at every epoch (default: False).\n        sampler (Sampler, optional): defines the strategy to draw samples from\n            the dataset. If specified, ``shuffle`` must be False.\n        batch_sampler (Sampler, optional): like sampler, but returns a batch of\n            indices at a time. Mutually exclusive with batch_size, shuffle,\n            sampler, and drop_last.\n        num_workers (int, optional): how many subprocesses to use for data\n            loading. 0 means that the data will be loaded in the main process.\n            (default: 0)\n        collate_fn (callable, optional): merges a list of samples to form a mini-batch.\n        pin_memory (bool, optional): If ``True``, the data loader will copy tensors\n            into CUDA pinned memory before returning them.\n        drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n            if the dataset size is not divisible by the batch size. If ``False`` and\n            the size of dataset is not divisible by the batch size, then the last batch\n            will be smaller. (default: False)\n        timeout (numeric, optional): if positive, the timeout value for collecting a batch\n            from workers. Should always be non-negative. (default: 0)\n        worker_init_fn (callable, optional): If not None, this will be called on each\n            worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n            input, after seeding and before data loading. (default: None)\n\n    .. note:: By default, each worker will have its PyTorch seed set to\n              ``base_seed + worker_id``, where ``base_seed`` is a long generated\n              by main process using its RNG. You may use ``torch.initial_seed()`` to access\n              this value in :attr:`worker_init_fn`, which can be used to set other seeds\n              (e.g. NumPy) before data loading.\n\n    .. warning:: If ``spawn\'\' start method is used, :attr:`worker_init_fn` cannot be an\n                 unpicklable object, e.g., a lambda function.\n    """"""\n\n    def __init__(self, dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None,\n                 num_workers=0, collate_fn=default_collate, pin_memory=False, drop_last=False,\n                 timeout=0, worker_init_fn=None):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.collate_fn = collate_fn\n        self.pin_memory = pin_memory\n        self.drop_last = drop_last\n        self.timeout = timeout\n        self.worker_init_fn = worker_init_fn\n\n        if timeout < 0:\n            raise ValueError(\'timeout option should be non-negative\')\n\n        if batch_sampler is not None:\n            if batch_size > 1 or shuffle or sampler is not None or drop_last:\n                raise ValueError(\'batch_sampler is mutually exclusive with \'\n                                 \'batch_size, shuffle, sampler, and drop_last\')\n\n        if sampler is not None and shuffle:\n            raise ValueError(\'sampler is mutually exclusive with shuffle\')\n\n        if self.num_workers < 0:\n            raise ValueError(\'num_workers cannot be negative; \'\n                             \'use num_workers=0 to disable multiprocessing.\')\n\n        if batch_sampler is None:\n            if sampler is None:\n                if shuffle:\n                    sampler = RandomSampler(dataset)\n                else:\n                    sampler = SequentialSampler(dataset)\n            batch_sampler = BatchSampler(sampler, batch_size, drop_last)\n\n        self.sampler = sampler\n        self.batch_sampler = batch_sampler\n\n    def __iter__(self):\n        return DataLoaderIter(self)\n\n    def __len__(self):\n        return len(self.batch_sampler)\n'"
lib/utils/data/dataset.py,1,"b'import bisect\nimport warnings\n\nfrom torch._utils import _accumulate\nfrom torch import randperm\n\n\nclass Dataset(object):\n    """"""An abstract class representing a Dataset.\n\n    All other datasets should subclass it. All subclasses should override\n    ``__len__``, that provides the size of the dataset, and ``__getitem__``,\n    supporting integer indexing in range from 0 to len(self) exclusive.\n    """"""\n\n    def __getitem__(self, index):\n        raise NotImplementedError\n\n    def __len__(self):\n        raise NotImplementedError\n\n    def __add__(self, other):\n        return ConcatDataset([self, other])\n\n\nclass TensorDataset(Dataset):\n    """"""Dataset wrapping data and target tensors.\n\n    Each sample will be retrieved by indexing both tensors along the first\n    dimension.\n\n    Arguments:\n        data_tensor (Tensor): contains sample data.\n        target_tensor (Tensor): contains sample targets (labels).\n    """"""\n\n    def __init__(self, data_tensor, target_tensor):\n        assert data_tensor.size(0) == target_tensor.size(0)\n        self.data_tensor = data_tensor\n        self.target_tensor = target_tensor\n\n    def __getitem__(self, index):\n        return self.data_tensor[index], self.target_tensor[index]\n\n    def __len__(self):\n        return self.data_tensor.size(0)\n\n\nclass ConcatDataset(Dataset):\n    """"""\n    Dataset to concatenate multiple datasets.\n    Purpose: useful to assemble different existing datasets, possibly\n    large-scale datasets as the concatenation operation is done in an\n    on-the-fly manner.\n\n    Arguments:\n        datasets (iterable): List of datasets to be concatenated\n    """"""\n\n    @staticmethod\n    def cumsum(sequence):\n        r, s = [], 0\n        for e in sequence:\n            l = len(e)\n            r.append(l + s)\n            s += l\n        return r\n\n    def __init__(self, datasets):\n        super(ConcatDataset, self).__init__()\n        assert len(datasets) > 0, \'datasets should not be an empty iterable\'\n        self.datasets = list(datasets)\n        self.cumulative_sizes = self.cumsum(self.datasets)\n\n    def __len__(self):\n        return self.cumulative_sizes[-1]\n\n    def __getitem__(self, idx):\n        dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)\n        if dataset_idx == 0:\n            sample_idx = idx\n        else:\n            sample_idx = idx - self.cumulative_sizes[dataset_idx - 1]\n        return self.datasets[dataset_idx][sample_idx]\n\n    @property\n    def cummulative_sizes(self):\n        warnings.warn(""cummulative_sizes attribute is renamed to ""\n                      ""cumulative_sizes"", DeprecationWarning, stacklevel=2)\n        return self.cumulative_sizes\n\n\nclass Subset(Dataset):\n    def __init__(self, dataset, indices):\n        self.dataset = dataset\n        self.indices = indices\n\n    def __getitem__(self, idx):\n        return self.dataset[self.indices[idx]]\n\n    def __len__(self):\n        return len(self.indices)\n\n\ndef random_split(dataset, lengths):\n    """"""\n    Randomly split a dataset into non-overlapping new datasets of given lengths\n    ds\n\n    Arguments:\n        dataset (Dataset): Dataset to be split\n        lengths (iterable): lengths of splits to be produced\n    """"""\n    if sum(lengths) != len(dataset):\n        raise ValueError(""Sum of input lengths does not equal the length of the input dataset!"")\n\n    indices = randperm(sum(lengths))\n    return [Subset(dataset, indices[offset - length:offset]) for offset, length in zip(_accumulate(lengths), lengths)]\n'"
lib/utils/data/distributed.py,4,"b'import math\nimport torch\nfrom .sampler import Sampler\nfrom torch.distributed import get_world_size, get_rank\n\n\nclass DistributedSampler(Sampler):\n    """"""Sampler that restricts data loading to a subset of the dataset.\n\n    It is especially useful in conjunction with\n    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSampler instance as a DataLoader sampler,\n    and load a subset of the original dataset that is exclusive to it.\n\n    .. note::\n        Dataset is assumed to be of constant size.\n\n    Arguments:\n        dataset: Dataset used for sampling.\n        num_replicas (optional): Number of processes participating in\n            distributed training.\n        rank (optional): Rank of the current process within num_replicas.\n    """"""\n\n    def __init__(self, dataset, num_replicas=None, rank=None):\n        if num_replicas is None:\n            num_replicas = get_world_size()\n        if rank is None:\n            rank = get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.num_samples = int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))\n        self.total_size = self.num_samples * self.num_replicas\n\n    def __iter__(self):\n        # deterministically shuffle based on epoch\n        g = torch.Generator()\n        g.manual_seed(self.epoch)\n        indices = list(torch.randperm(len(self.dataset), generator=g))\n\n        # add extra samples to make it evenly divisible\n        indices += indices[:(self.total_size - len(indices))]\n        assert len(indices) == self.total_size\n\n        # subsample\n        offset = self.num_samples * self.rank\n        indices = indices[offset:offset + self.num_samples]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch\n'"
lib/utils/data/sampler.py,4,"b'import torch\n\n\nclass Sampler(object):\n    """"""Base class for all Samplers.\n\n    Every Sampler subclass has to provide an __iter__ method, providing a way\n    to iterate over indices of dataset elements, and a __len__ method that\n    returns the length of the returned iterators.\n    """"""\n\n    def __init__(self, data_source):\n        pass\n\n    def __iter__(self):\n        raise NotImplementedError\n\n    def __len__(self):\n        raise NotImplementedError\n\n\nclass SequentialSampler(Sampler):\n    """"""Samples elements sequentially, always in the same order.\n\n    Arguments:\n        data_source (Dataset): dataset to sample from\n    """"""\n\n    def __init__(self, data_source):\n        self.data_source = data_source\n\n    def __iter__(self):\n        return iter(range(len(self.data_source)))\n\n    def __len__(self):\n        return len(self.data_source)\n\n\nclass RandomSampler(Sampler):\n    """"""Samples elements randomly, without replacement.\n\n    Arguments:\n        data_source (Dataset): dataset to sample from\n    """"""\n\n    def __init__(self, data_source):\n        self.data_source = data_source\n\n    def __iter__(self):\n        return iter(torch.randperm(len(self.data_source)).long())\n\n    def __len__(self):\n        return len(self.data_source)\n\n\nclass SubsetRandomSampler(Sampler):\n    """"""Samples elements randomly from a given list of indices, without replacement.\n\n    Arguments:\n        indices (list): a list of indices\n    """"""\n\n    def __init__(self, indices):\n        self.indices = indices\n\n    def __iter__(self):\n        return (self.indices[i] for i in torch.randperm(len(self.indices)))\n\n    def __len__(self):\n        return len(self.indices)\n\n\nclass WeightedRandomSampler(Sampler):\n    """"""Samples elements from [0,..,len(weights)-1] with given probabilities (weights).\n\n    Arguments:\n        weights (list)   : a list of weights, not necessary summing up to one\n        num_samples (int): number of samples to draw\n        replacement (bool): if ``True``, samples are drawn with replacement.\n            If not, they are drawn without replacement, which means that when a\n            sample index is drawn for a row, it cannot be drawn again for that row.\n    """"""\n\n    def __init__(self, weights, num_samples, replacement=True):\n        self.weights = torch.DoubleTensor(weights)\n        self.num_samples = num_samples\n        self.replacement = replacement\n\n    def __iter__(self):\n        return iter(torch.multinomial(self.weights, self.num_samples, self.replacement))\n\n    def __len__(self):\n        return self.num_samples\n\n\nclass BatchSampler(object):\n    """"""Wraps another sampler to yield a mini-batch of indices.\n\n    Args:\n        sampler (Sampler): Base sampler.\n        batch_size (int): Size of mini-batch.\n        drop_last (bool): If ``True``, the sampler will drop the last batch if\n            its size would be less than ``batch_size``\n\n    Example:\n        >>> list(BatchSampler(range(10), batch_size=3, drop_last=False))\n        [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n        >>> list(BatchSampler(range(10), batch_size=3, drop_last=True))\n        [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n    """"""\n\n    def __init__(self, sampler, batch_size, drop_last):\n        self.sampler = sampler\n        self.batch_size = batch_size\n        self.drop_last = drop_last\n\n    def __iter__(self):\n        batch = []\n        for idx in self.sampler:\n            batch.append(idx)\n            if len(batch) == self.batch_size:\n                yield batch\n                batch = []\n        if len(batch) > 0 and not self.drop_last:\n            yield batch\n\n    def __len__(self):\n        if self.drop_last:\n            return len(self.sampler) // self.batch_size\n        else:\n            return (len(self.sampler) + self.batch_size - 1) // self.batch_size\n'"
lib/nn/modules/tests/test_numeric_batchnorm.py,5,"b""# -*- coding: utf-8 -*-\n# File   : test_numeric_batchnorm.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n# \n# This file is part of Synchronized-BatchNorm-PyTorch.\n\nimport unittest\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nfrom sync_batchnorm.unittest import TorchTestCase\n\n\ndef handy_var(a, unbias=True):\n    n = a.size(0)\n    asum = a.sum(dim=0)\n    as_sum = (a ** 2).sum(dim=0)  # a square sum\n    sumvar = as_sum - asum * asum / n\n    if unbias:\n        return sumvar / (n - 1)\n    else:\n        return sumvar / n\n\n\nclass NumericTestCase(TorchTestCase):\n    def testNumericBatchNorm(self):\n        a = torch.rand(16, 10)\n        bn = nn.BatchNorm2d(10, momentum=1, eps=1e-5, affine=False)\n        bn.train()\n\n        a_var1 = Variable(a, requires_grad=True)\n        b_var1 = bn(a_var1)\n        loss1 = b_var1.sum()\n        loss1.backward()\n\n        a_var2 = Variable(a, requires_grad=True)\n        a_mean2 = a_var2.mean(dim=0, keepdim=True)\n        a_std2 = torch.sqrt(handy_var(a_var2, unbias=False).clamp(min=1e-5))\n        # a_std2 = torch.sqrt(a_var2.var(dim=0, keepdim=True, unbiased=False) + 1e-5)\n        b_var2 = (a_var2 - a_mean2) / a_std2\n        loss2 = b_var2.sum()\n        loss2.backward()\n\n        self.assertTensorClose(bn.running_mean, a.mean(dim=0))\n        self.assertTensorClose(bn.running_var, handy_var(a))\n        self.assertTensorClose(a_var1.data, a_var2.data)\n        self.assertTensorClose(b_var1.data, b_var2.data)\n        self.assertTensorClose(a_var1.grad, a_var2.grad)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
lib/nn/modules/tests/test_sync_batchnorm.py,7,"b'# -*- coding: utf-8 -*-\n# File   : test_sync_batchnorm.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n# \n# This file is part of Synchronized-BatchNorm-PyTorch.\n\nimport unittest\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nfrom sync_batchnorm import SynchronizedBatchNorm1d, SynchronizedBatchNorm2d, DataParallelWithCallback\nfrom sync_batchnorm.unittest import TorchTestCase\n\n\ndef handy_var(a, unbias=True):\n    n = a.size(0)\n    asum = a.sum(dim=0)\n    as_sum = (a ** 2).sum(dim=0)  # a square sum\n    sumvar = as_sum - asum * asum / n\n    if unbias:\n        return sumvar / (n - 1)\n    else:\n        return sumvar / n\n\n\ndef _find_bn(module):\n    for m in module.modules():\n        if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, SynchronizedBatchNorm1d, SynchronizedBatchNorm2d)):\n            return m\n\n\nclass SyncTestCase(TorchTestCase):\n    def _syncParameters(self, bn1, bn2):\n        bn1.reset_parameters()\n        bn2.reset_parameters()\n        if bn1.affine and bn2.affine:\n            bn2.weight.data.copy_(bn1.weight.data)\n            bn2.bias.data.copy_(bn1.bias.data)\n\n    def _checkBatchNormResult(self, bn1, bn2, input, is_train, cuda=False):\n        """"""Check the forward and backward for the customized batch normalization.""""""\n        bn1.train(mode=is_train)\n        bn2.train(mode=is_train)\n\n        if cuda:\n            input = input.cuda()\n\n        self._syncParameters(_find_bn(bn1), _find_bn(bn2))\n\n        input1 = Variable(input, requires_grad=True)\n        output1 = bn1(input1)\n        output1.sum().backward()\n        input2 = Variable(input, requires_grad=True)\n        output2 = bn2(input2)\n        output2.sum().backward()\n\n        self.assertTensorClose(input1.data, input2.data)\n        self.assertTensorClose(output1.data, output2.data)\n        self.assertTensorClose(input1.grad, input2.grad)\n        self.assertTensorClose(_find_bn(bn1).running_mean, _find_bn(bn2).running_mean)\n        self.assertTensorClose(_find_bn(bn1).running_var, _find_bn(bn2).running_var)\n\n    def testSyncBatchNormNormalTrain(self):\n        bn = nn.BatchNorm1d(10)\n        sync_bn = SynchronizedBatchNorm1d(10)\n\n        self._checkBatchNormResult(bn, sync_bn, torch.rand(16, 10), True)\n\n    def testSyncBatchNormNormalEval(self):\n        bn = nn.BatchNorm1d(10)\n        sync_bn = SynchronizedBatchNorm1d(10)\n\n        self._checkBatchNormResult(bn, sync_bn, torch.rand(16, 10), False)\n\n    def testSyncBatchNormSyncTrain(self):\n        bn = nn.BatchNorm1d(10, eps=1e-5, affine=False)\n        sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n\n        bn.cuda()\n        sync_bn.cuda()\n\n        self._checkBatchNormResult(bn, sync_bn, torch.rand(16, 10), True, cuda=True)\n\n    def testSyncBatchNormSyncEval(self):\n        bn = nn.BatchNorm1d(10, eps=1e-5, affine=False)\n        sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n\n        bn.cuda()\n        sync_bn.cuda()\n\n        self._checkBatchNormResult(bn, sync_bn, torch.rand(16, 10), False, cuda=True)\n\n    def testSyncBatchNorm2DSyncTrain(self):\n        bn = nn.BatchNorm2d(10)\n        sync_bn = SynchronizedBatchNorm2d(10)\n        sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n\n        bn.cuda()\n        sync_bn.cuda()\n\n        self._checkBatchNormResult(bn, sync_bn, torch.rand(16, 10, 16, 16), True, cuda=True)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
