file_path,api_count,code
src/models.py,48,"b'import torch\nimport torch.nn as nn\nfrom torch.nn import Linear, Conv2d, BatchNorm2d, LeakyReLU, ConvTranspose2d, ReLU, Tanh, InstanceNorm2d, ReplicationPad2d\nimport torch.nn.functional as F\nimport random\nfrom utils import gaussian, to_var, to_data, save_image\nfrom vgg import GramMSELoss, SemanticFeature\nimport numpy as np\nimport math\nimport torch.autograd as autograd\nfrom torch.autograd import Variable\nimport os\n\nid = 0 # for saving network output to file during training\n\n#######################  Texture Network\n# based on Convolution-BatchNorm-ReLU\nclass myTConv(nn.Module):\n    def __init__(self, num_filter=128, stride=1, in_channels=128):\n        super(myTConv, self).__init__()\n        \n        self.pad = ReplicationPad2d(padding=1)\n        self.conv = Conv2d(out_channels=num_filter, kernel_size=3, \n                           stride=stride, padding=0, in_channels=in_channels)\n        self.bn = BatchNorm2d(num_features=num_filter, track_running_stats=True)\n        self.relu = ReLU()\n            \n    def forward(self, x):\n        return self.relu(self.bn(self.conv(self.pad(x))))\n\nclass myTBlock(nn.Module):\n    def __init__(self, num_filter=128, p=0.0):\n        super(myTBlock, self).__init__()\n        \n        self.myconv = myTConv(num_filter=num_filter, stride=1, in_channels=128)\n        self.pad = ReplicationPad2d(padding=1)\n        self.conv = Conv2d(out_channels=num_filter, kernel_size=3, padding=0, in_channels=128)\n        self.bn = BatchNorm2d(num_features=num_filter, track_running_stats=True)\n        self.relu = ReLU()\n        self.dropout = nn.Dropout(p=p)\n        \n    def forward(self, x):\n        return self.dropout(self.relu(x+self.bn(self.conv(self.pad(self.myconv(x))))))\n\nclass TextureGenerator(nn.Module):\n    def __init__(self, ngf = 32, n_layers = 5):\n        super(TextureGenerator, self).__init__()\n        \n        modelList = []\n        modelList.append(ReplicationPad2d(padding=4))\n        modelList.append(Conv2d(out_channels=ngf, kernel_size=9, padding=0, in_channels=3))\n        modelList.append(ReLU())\n        modelList.append(myTConv(ngf*2, 2, ngf))\n        modelList.append(myTConv(ngf*4, 2, ngf*2))\n        \n        for n in range(int(n_layers/2)): \n            modelList.append(myTBlock(ngf*4, p=0.0))\n        # dropout to make model more robust\n        modelList.append(myTBlock(ngf*4, p=0.5))\n        for n in range(int(n_layers/2)+1,n_layers):\n            modelList.append(myTBlock(ngf*4, p=0.0))  \n        \n        modelList.append(ConvTranspose2d(out_channels=ngf*2, kernel_size=4, stride=2, padding=0, in_channels=ngf*4))\n        modelList.append(BatchNorm2d(num_features=ngf*2, track_running_stats=True))\n        modelList.append(ReLU())\n        modelList.append(ConvTranspose2d(out_channels=ngf, kernel_size=4, stride=2, padding=0, in_channels=ngf*2))\n        modelList.append(BatchNorm2d(num_features=ngf, track_running_stats=True))\n        modelList.append(ReLU())\n        modelList.append(ReplicationPad2d(padding=1))\n        modelList.append(Conv2d(out_channels=3, kernel_size=9, padding=0, in_channels=ngf))\n        modelList.append(Tanh())\n        self.model = nn.Sequential(*modelList)\n        \n    def forward(self, x):\n        return self.model(x)\n\n###################### Glyph Network    \n# based on Convolution-BatchNorm-LeakyReLU    \nclass myGConv(nn.Module):\n    def __init__(self, num_filter=128, stride=1, in_channels=128):\n        super(myGConv, self).__init__()\n        self.pad = ReplicationPad2d(padding=1)\n        self.conv = Conv2d(out_channels=num_filter, kernel_size=3, \n                           stride=stride, padding=0, in_channels=in_channels)\n        self.bn = BatchNorm2d(num_features=num_filter, track_running_stats=True)\n        # either ReLU or LeakyReLU is OK\n        self.relu = LeakyReLU(0.2)\n            \n    def forward(self, x):\n        return self.relu(self.bn(self.conv(self.pad(x))))\n\nclass myGBlock(nn.Module):\n    def __init__(self, num_filter=128):\n        super(myGBlock, self).__init__()\n        \n        self.myconv = myGConv(num_filter=num_filter, stride=1, in_channels=num_filter)\n        self.pad = ReplicationPad2d(padding=1)\n        self.conv = Conv2d(out_channels=num_filter, kernel_size=3, padding=0, in_channels=num_filter)\n        self.bn = BatchNorm2d(num_features=num_filter, track_running_stats=True)\n        \n    def forward(self, x):\n        return x+self.bn(self.conv(self.pad(self.myconv(x))))\n\n# Controllable ResBlock\nclass myGCombineBlock(nn.Module):\n    def __init__(self, num_filter=128, p=0.0):\n        super(myGCombineBlock, self).__init__()\n        \n        self.myBlock1 = myGBlock(num_filter=num_filter)\n        self.myBlock2 = myGBlock(num_filter=num_filter)\n        self.relu = LeakyReLU(0.2)\n        self.label = 1.0\n        self.dropout = nn.Dropout(p=p)\n        \n    def myCopy(self):\n        self.myBlock1.load_state_dict(self.myBlock2.state_dict())\n        \n    def forward(self, x):\n        return self.dropout(self.relu(self.myBlock1(x)*self.label + self.myBlock2(x)*(1.0-self.label)))\n    \nclass GlyphGenerator(nn.Module):\n    def __init__(self, ngf=32, n_layers = 5):\n        super(GlyphGenerator, self).__init__()\n        \n        encoder = []\n        encoder.append(ReplicationPad2d(padding=4))\n        encoder.append(Conv2d(out_channels=ngf, kernel_size=9, padding=0, in_channels=3))\n        encoder.append(LeakyReLU(0.2))\n        encoder.append(myGConv(ngf*2, 2, ngf))\n        encoder.append(myGConv(ngf*4, 2, ngf*2))\n\n        transformer = []\n        for n in range(int(n_layers/2)-1):\n            transformer.append(myGCombineBlock(ngf*4,p=0.0))\n        # dropout to make model more robust    \n        transformer.append(myGCombineBlock(ngf*4,p=0.5))\n        transformer.append(myGCombineBlock(ngf*4,p=0.5))\n        for n in range(int(n_layers/2)+1,n_layers):\n            transformer.append(myGCombineBlock(ngf*4,p=0.0))  \n        \n        decoder = []\n        decoder.append(ConvTranspose2d(out_channels=ngf*2, kernel_size=4, stride=2, padding=0, in_channels=ngf*4))\n        decoder.append(BatchNorm2d(num_features=ngf*2, track_running_stats=True))\n        decoder.append(LeakyReLU(0.2))\n        decoder.append(ConvTranspose2d(out_channels=ngf, kernel_size=4, stride=2, padding=0, in_channels=ngf*2))\n        decoder.append(BatchNorm2d(num_features=ngf, track_running_stats=True))\n        decoder.append(LeakyReLU(0.2))\n        decoder.append(ReplicationPad2d(padding=1))\n        decoder.append(Conv2d(out_channels=3, kernel_size=9, padding=0, in_channels=ngf))\n        decoder.append(Tanh())\n        \n        self.encoder = nn.Sequential(*encoder)\n        self.transformer = nn.Sequential(*transformer)\n        self.decoder = nn.Sequential(*decoder)\n    \n    def myCopy(self):\n        for myCombineBlock in self.transformer:\n            myCombineBlock.myCopy()\n            \n    # controlled by Controllable ResBlcok    \n    def forward(self, x, l):\n        for myCombineBlock in self.transformer:\n            # label smoothing [-1,1]-->[0.9,0.1]\n            myCombineBlock.label = (1.0-l)*0.4+0.1\n        out0 = self.encoder(x)\n        out1 = self.transformer(out0)\n        out2 = self.decoder(out1)\n        return out2\n\n    \n##################### Sketch Module \n# based on Convolution-InstanceNorm-ReLU   \n# Smoothness Block\nclass myBlur(nn.Module):\n    def __init__(self, kernel_size=121, channels=3):\n        super(myBlur, self).__init__()\n        kernel_size = int(int(kernel_size/2)*2)+1\n        self.kernel_size=kernel_size\n        self.channels = channels\n        self.GF = nn.Conv2d(in_channels=channels, out_channels=channels,\n                                    kernel_size=kernel_size, groups=channels, bias=False)\n        x_cord = torch.arange(self.kernel_size+0.)\n        x_grid = x_cord.repeat(self.kernel_size).view(self.kernel_size, self.kernel_size)\n        y_grid = x_grid.t()\n        self.xy_grid = torch.stack([x_grid, y_grid], dim=-1)\n        self.mean = (self.kernel_size - 1)//2\n        self.diff = -torch.sum((self.xy_grid - self.mean)**2., dim=-1)\n        self.gaussian_filter = nn.Conv2d(in_channels=self.channels, out_channels=self.channels,\n                                    kernel_size=self.kernel_size, groups=self.channels, bias=False)\n\n        self.gaussian_filter.weight.requires_grad = False\n        \n    def forward(self, x, sigma, gpu):\n        sigma = sigma * 8. + 16.\n        variance = sigma**2.\n        gaussian_kernel = (1./(2.*math.pi*variance)) * torch.exp(self.diff /(2*variance))\n        gaussian_kernel = gaussian_kernel / torch.sum(gaussian_kernel)\n        gaussian_kernel = gaussian_kernel.view(1, 1, self.kernel_size, self.kernel_size)\n        gaussian_kernel = gaussian_kernel.repeat(self.channels, 1, 1, 1)\n        if gpu:\n            gaussian_kernel = gaussian_kernel.cuda()\n        self.gaussian_filter.weight.data = gaussian_kernel\n        return self.gaussian_filter(F.pad(x, (self.mean,self.mean,self.mean,self.mean), ""replicate"")) \n\nclass mySConv(nn.Module):\n    def __init__(self, num_filter=128, stride=1, in_channels=128):\n        super(mySConv, self).__init__()\n        \n        self.conv = Conv2d(out_channels=num_filter, kernel_size=3, \n                           stride=stride, padding=1, in_channels=in_channels)\n        self.bn = InstanceNorm2d(num_features=num_filter)\n        self.relu = ReLU()\n            \n    def forward(self, x):\n        return self.relu(self.bn(self.conv(x)))\n\nclass mySBlock(nn.Module):\n    def __init__(self, num_filter=128):\n        super(mySBlock, self).__init__()\n        \n        self.myconv = mySConv(num_filter=num_filter, stride=1, in_channels=num_filter)\n        self.conv = Conv2d(out_channels=num_filter, kernel_size=3, padding=1, in_channels=num_filter)\n        self.bn = InstanceNorm2d(num_features=num_filter)\n        self.relu = ReLU()\n        \n    def forward(self, x):\n        return self.relu(x+self.bn(self.conv(self.myconv(x))))\n    \n# Transformation Block\nclass SketchGenerator(nn.Module):\n    def __init__(self, in_channels = 4, ngf = 32, n_layers = 5):\n        super(SketchGenerator, self).__init__()\n        \n        encoder = []\n        encoder.append(Conv2d(out_channels=ngf, kernel_size=9, padding=4, in_channels=in_channels))\n        encoder.append(ReLU())\n        encoder.append(mySConv(ngf*2, 2, ngf))\n        encoder.append(mySConv(ngf*4, 2, ngf*2))\n        \n        transformer = []\n        for n in range(n_layers):\n            transformer.append(mySBlock(ngf*4+1))\n        \n        decoder1 = []\n        decoder2 = []\n        decoder3 = []\n        decoder1.append(ConvTranspose2d(out_channels=ngf*2, kernel_size=4, stride=2, padding=0, in_channels=ngf*4+2))\n        decoder1.append(InstanceNorm2d(num_features=ngf*2))\n        decoder1.append(ReLU())\n        decoder2.append(ConvTranspose2d(out_channels=ngf, kernel_size=4, stride=2, padding=0, in_channels=ngf*2+1))\n        decoder2.append(InstanceNorm2d(num_features=ngf))\n        decoder2.append(ReLU())\n        decoder3.append(Conv2d(out_channels=3, kernel_size=9, padding=1, in_channels=ngf+1))\n        decoder3.append(Tanh())\n        \n        self.encoder = nn.Sequential(*encoder)\n        self.transformer = nn.Sequential(*transformer)\n        self.decoder1 = nn.Sequential(*decoder1)\n        self.decoder2 = nn.Sequential(*decoder2)\n        self.decoder3 = nn.Sequential(*decoder3)\n    \n    # controlled by label concatenation\n    def forward(self, x, l):\n        l_img = l.expand(l.size(0), l.size(1), x.size(2), x.size(3))\n        out0 = self.encoder(torch.cat([x, l_img], 1))\n        l_img0 = l.expand(l.size(0), l.size(1), out0.size(2), out0.size(3))\n        out1 = self.transformer(torch.cat([out0, l_img0], 1))\n        l_img1 = l.expand(l.size(0), l.size(1), out1.size(2), out1.size(3))\n        out2 = self.decoder1(torch.cat([out1, l_img1], 1))\n        l_img2 = l.expand(l.size(0), l.size(1), out2.size(2), out2.size(3))\n        out3 = self.decoder2(torch.cat([out2, l_img2], 1))\n        l_img3 = l.expand(l.size(0), l.size(1), out3.size(2), out3.size(3))\n        out4 = self.decoder3(torch.cat([out3, l_img3], 1))\n        return out4\n    \n    \n################ Discriminators\n# Glyph and Texture Networks: BN\n# Sketch Module: IN, multilayer\nclass Discriminator(nn.Module):\n    def __init__(self, in_channels, ndf=32, n_layers=3, multilayer=False, IN=False):\n        super(Discriminator, self).__init__()\n        \n        modelList = []    \n        outlist1 = []\n        outlist2 = []\n        kernel_size = 4\n        padding = int(np.ceil((kernel_size - 1)/2))\n        modelList.append(Conv2d(out_channels=ndf, kernel_size=kernel_size, stride=2,\n                              padding=2, in_channels=in_channels))\n        modelList.append(LeakyReLU(0.2))\n\n        nf_mult = 1\n        for n in range(1, n_layers):\n            nf_mult_prev = nf_mult\n            nf_mult = min(2 ** n, 4)\n            modelList.append(Conv2d(out_channels=ndf * nf_mult, kernel_size=kernel_size, stride=2,\n                                  padding=2, in_channels=ndf * nf_mult_prev))\n            if IN:\n                modelList.append(InstanceNorm2d(num_features=ndf * nf_mult))\n            else:\n                modelList.append(BatchNorm2d(num_features=ndf * nf_mult, track_running_stats=True))\n            modelList.append(LeakyReLU(0.2))\n\n        nf_mult_prev = nf_mult\n        nf_mult = min(2 ** n_layers, 4)\n        outlist1.append(Conv2d(out_channels=1, kernel_size=kernel_size, stride=1,\n                              padding=padding, in_channels=ndf * nf_mult_prev))\n        \n        outlist2.append(Conv2d(out_channels=ndf * nf_mult, kernel_size=kernel_size, stride=1,\n                              padding=padding, in_channels=ndf * nf_mult_prev))\n        if IN:\n            outlist2.append(InstanceNorm2d(num_features=ndf * nf_mult))\n        else:\n            outlist2.append(BatchNorm2d(num_features=ndf * nf_mult, track_running_stats=True))\n        outlist2.append(LeakyReLU(0.2))\n        outlist2.append(Conv2d(out_channels=1, kernel_size=kernel_size, stride=1,\n                              padding=padding, in_channels=ndf * nf_mult))\n        self.model = nn.Sequential(*modelList)\n        self.out1 = nn.Sequential(*outlist1)\n        self.out2 = nn.Sequential(*outlist2)\n        self.multilayer = multilayer\n        \n    def forward(self, x):\n        y = self.model(x)\n        out2 = self.out2(y)\n        if self.multilayer:\n            out1 = self.out1(y)\n            return torch.cat((out1.view(-1), out2.view(-1)), dim=0)\n        else:\n            return out2.view(-1)\n\n        \n######################## Sketch Module\nclass SketchModule(nn.Module):\n    def __init__(self, G_layers = 6, D_layers = 5, ngf = 32, ndf = 32, gpu=True):\n        super(SketchModule, self).__init__()\n        \n        self.G_layers = G_layers\n        self.D_layers = D_layers\n        self.ngf = ngf\n        self.ndf = ndf\n        self.gpu = gpu\n        self.lambda_l1 = 100\n        self.lambda_gp = 10\n        self.lambda_adv = 1\n        self.loss = nn.L1Loss()\n        \n        # Sketch Module = transformationBlock + smoothnessBlock\n        # transformationBlock\n        self.transBlock = SketchGenerator(4, self.ngf, self.G_layers)\n        self.D_B = Discriminator(7, self.ndf, self.D_layers, True, True)\n        # smoothnessBlock\n        self.smoothBlock = myBlur()\n        \n        self.trainerG = torch.optim.Adam(self.transBlock.parameters(), lr=0.0002, betas=(0.5, 0.999))\n        self.trainerD = torch.optim.Adam(self.D_B.parameters(), lr=0.0002, betas=(0.5, 0.999))\n    \n    # FOR TESTING\n    def forward(self, t, l):\n        l = torch.tensor(l).float()\n        tl = self.smoothBlock(t, l, self.gpu)\n        label = l.repeat(1, 1, 1, 1)\n        label = label.cuda() if self.gpu else label\n        return self.transBlock(tl, label)\n    \n    # FOR TRAINING\n    # init weight\n    def init_networks(self, weights_init):\n        self.transBlock.apply(weights_init)\n        self.D_B.apply(weights_init)\n        \n    # WGAN-GP: calculate gradient penalty \n    def calc_gradient_penalty(self, netD, real_data, fake_data):\n        alpha = torch.rand(real_data.shape[0], 1, 1, 1)\n        alpha = alpha.cuda() if self.gpu else alpha\n\n        interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n        interpolates = Variable(interpolates, requires_grad=True)\n\n        disc_interpolates = netD(interpolates)\n\n        gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n                              grad_outputs=torch.ones(disc_interpolates.size()).cuda() \n                              if self.gpu else torch.ones(disc_interpolates.size()),\n                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n\n        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n        return gradient_penalty\n    \n    def update_discriminator(self, t, l):\n        label = torch.tensor(l).float()\n        label = label.repeat(t.size(0), 1, 1, 1)\n        label = to_var(label) if self.gpu else label\n        real_label = label.expand(label.size(0), label.size(1), t.size(2), t.size(3))    \n        with torch.no_grad():\n            tl = self.smoothBlock(t, l, self.gpu)\n            fake_text = self.transBlock(tl, label)\n            # print(tl.size(), real_label.size(), fake_text.size())\n            fake_concat = torch.cat((tl, real_label, fake_text), dim=1)\n        fake_output = self.D_B(fake_concat)\n        real_concat = torch.cat((tl, real_label, t), dim=1)\n        real_output = self.D_B(real_concat)\n        gp = self.calc_gradient_penalty(self.D_B, real_concat.data, fake_concat.data)\n        LBadv = self.lambda_adv*(fake_output.mean() - real_output.mean() + self.lambda_gp * gp)\n        self.trainerD.zero_grad()\n        LBadv.backward()\n        self.trainerD.step()\n        return (real_output.mean() - fake_output.mean()).data.mean() * self.lambda_adv\n    \n    def update_generator(self, t, l):\n        label = torch.tensor(l).float()\n        label = label.repeat(t.size(0), 1, 1, 1)\n        label = label.cuda() if self.gpu else label\n        real_label = label.expand(label.size(0), label.size(1), t.size(2), t.size(3)) \n        tl = self.smoothBlock(t, l, self.gpu)\n        fake_text = self.transBlock(tl, label)\n        fake_concat = torch.cat((tl, real_label, fake_text), dim=1)\n        fake_output = self.D_B(fake_concat)\n        LBadv = -fake_output.mean() * self.lambda_adv\n        LBrec = self.loss(fake_text, t) * self.lambda_l1\n        LB = LBadv + LBrec\n        self.trainerG.zero_grad()\n        LB.backward()\n        self.trainerG.step()\n        #global id\n        #if id % 50 == 0:\n        #    viz_img = to_data(torch.cat((t[0], tl[0], fake_text[0]), dim=2))\n        #    save_image(viz_img, \'../output/deblur_result%d.jpg\'%id)\n        #id += 1\n        return LBadv.data.mean(), LBrec.data.mean()\n    \n    def one_pass(self, t, scales):\n        l = random.choice(scales)\n        LDadv = self.update_discriminator(t, l)\n        LGadv, Lrec = self.update_generator(t, l)\n        return [LDadv,LGadv,Lrec]\n    \n    \n######################## ShapeMatchingGAN\nclass ShapeMatchingGAN(nn.Module):\n    def __init__(self, GS_nlayers = 6, DS_nlayers = 5, GS_nf = 32, DS_nf = 32,\n                 GT_nlayers = 6, DT_nlayers = 5, GT_nf = 32, DT_nf = 32, gpu=True):\n        super(ShapeMatchingGAN, self).__init__()\n        \n        self.GS_nlayers = GS_nlayers\n        self.DS_nlayers = DS_nlayers\n        self.GS_nf = GS_nf\n        self.DS_nf = DS_nf\n        self.GT_nlayers = GT_nlayers\n        self.DT_nlayers = DT_nlayers\n        self.GT_nf = GT_nf\n        self.DT_nf = DT_nf        \n        self.gpu = gpu\n        self.lambda_l1 = 100\n        self.lambda_gp = 10\n        self.lambda_sadv = 0.1\n        self.lambda_gly = 1.0\n        self.lambda_tadv = 1.0\n        self.lambda_sty = 0.01\n        self.style_weights = [1e3/n**2 for n in [64,128,256,512,512]]\n        self.loss = nn.L1Loss()\n        self.gramloss = GramMSELoss()\n        self.gramloss = self.gramloss.cuda() if self.gpu else self.gramloss\n        self.getmask = SemanticFeature()\n        for param in self.getmask.parameters():\n            param.requires_grad = False\n\n        self.G_S = GlyphGenerator(self.GS_nf, self.GS_nlayers)\n        self.D_S = Discriminator(3, self.DS_nf, self.DS_nlayers)\n        self.G_T = TextureGenerator(self.GT_nf, self.GT_nlayers)\n        self.D_T = Discriminator(6, self.DT_nf, self.DT_nlayers)\n        \n        self.trainerG_S = torch.optim.Adam(self.G_S.parameters(), lr=0.0002, betas=(0.5, 0.999))\n        self.trainerD_S = torch.optim.Adam(self.D_S.parameters(), lr=0.0002, betas=(0.5, 0.999))\n        self.trainerG_T = torch.optim.Adam(self.G_T.parameters(), lr=0.0002, betas=(0.5, 0.999))\n        self.trainerD_T = torch.optim.Adam(self.D_T.parameters(), lr=0.0002, betas=(0.5, 0.999))    \n    \n    # FOR TESTING\n    def forward(self, x, l):\n        x[:,0:1] = gaussian(x[:,0:1], stddev=0.2)\n        xl = self.G_S(x, l) \n        xl[:,0:1] = gaussian(xl[:,0:1], stddev=0.2)\n        return self.G_T(xl)\n            \n    # FOR TRAINING\n    # init weight\n    def init_networks(self, weights_init):\n        self.G_S.apply(weights_init)\n        self.D_S.apply(weights_init)\n        self.G_T.apply(weights_init)\n        self.D_T.apply(weights_init)\n        \n    # WGAN-GP: calculate gradient penalty \n    def calc_gradient_penalty(self, netD, real_data, fake_data):\n        alpha = torch.rand(real_data.shape[0], 1, 1, 1)\n        alpha = alpha.cuda() if self.gpu else alpha\n\n        interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n        interpolates = Variable(interpolates, requires_grad=True)\n\n        disc_interpolates = netD(interpolates)\n\n        gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n                              grad_outputs=torch.ones(disc_interpolates.size()).cuda() \n                              if self.gpu else torch.ones(disc_interpolates.size()),\n                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n\n        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n        return gradient_penalty\n    \n    def update_structure_discriminator(self, x, xl, l):   \n        with torch.no_grad():\n            fake_x = self.G_S(xl, l)\n        fake_output = self.D_S(fake_x)\n        real_output = self.D_S(x)\n        gp = self.calc_gradient_penalty(self.D_S, x.data, fake_x.data)\n        LSadv = self.lambda_sadv*(fake_output.mean() - real_output.mean() + self.lambda_gp * gp)\n        self.trainerD_S.zero_grad()\n        LSadv.backward()\n        self.trainerD_S.step()\n        return (real_output.mean() - fake_output.mean()).data.mean()*self.lambda_sadv\n    \n    def update_structure_generator(self, x, xl, l, t=None):\n        fake_x = self.G_S(xl, l)\n        fake_output = self.D_S(fake_x)\n        LSadv = -fake_output.mean()*self.lambda_sadv\n        LSrec = self.loss(fake_x, x) * self.lambda_l1\n        LS = LSadv + LSrec\n        if t is not None:\n            # weight map based on the distance field \n            # whose pixel value increases with its distance to the nearest text contour point of t\n            Mt = (t[:,1:2]+t[:,2:3])*0.5+1.0\n            t_noise = t.clone()\n            t_noise[:,0:1] = gaussian(t_noise[:,0:1], stddev=0.2)\n            fake_t = self.G_S(t_noise, l)\n            LSgly = self.loss(fake_t*Mt, t*Mt) * self.lambda_gly\n            LS = LS + LSgly\n        self.trainerG_S.zero_grad()\n        LS.backward()\n        self.trainerG_S.step()\n        #global id\n        #if id % 60 == 0:\n        #    viz_img = to_data(torch.cat((x[0], xl[0], fake_x[0]), dim=2))\n        #    save_image(viz_img, \'../output/structure_result%d.jpg\'%id)\n        #id += 1\n        return LSadv.data.mean(), LSrec.data.mean(), LSgly.data.mean() if t is not None else 0\n    \n    def structure_one_pass(self, x, xl, l, t=None):\n        LDadv = self.update_structure_discriminator(x, xl, l)\n        LGadv, Lrec, Lgly = self.update_structure_generator(x, xl, l, t)\n        return [LDadv, LGadv, Lrec, Lgly]    \n    \n    def update_texture_discriminator(self, x, y):\n        with torch.no_grad():\n            fake_y = self.G_T(x)          \n            fake_concat = torch.cat((x, fake_y), dim=1)\n        fake_output = self.D_T(fake_concat)\n        real_concat = torch.cat((x, y), dim=1)\n        real_output = self.D_T(real_concat)\n        gp = self.calc_gradient_penalty(self.D_T, real_concat.data, fake_concat.data)\n        LTadv = self.lambda_tadv*(fake_output.mean() - real_output.mean() + self.lambda_gp * gp)\n        self.trainerD_T.zero_grad()\n        LTadv.backward()\n        self.trainerD_T.step()\n        return (real_output.mean() - fake_output.mean()).data.mean()*self.lambda_tadv        \n\n    def update_texture_generator(self, x, y, t=None, l=None, VGGfeatures=None, style_targets=None):\n        fake_y = self.G_T(x)\n        fake_concat = torch.cat((x, fake_y), dim=1)\n        fake_output = self.D_T(fake_concat)\n        LTadv = -fake_output.mean()*self.lambda_tadv\n        Lrec = self.loss(fake_y, y) * self.lambda_l1\n        LT = LTadv + Lrec\n        if t is not None:\n            with torch.no_grad():\n                t[:,0:1] = gaussian(t[:,0:1], stddev=0.2)\n                source_mask = self.G_S(t, l).detach()\n                source = source_mask.clone()\n                source[:,0:1] = gaussian(source[:,0:1], stddev=0.2)\n                smaps_fore = [(A.detach()+1)*0.5 for A in self.getmask(source_mask[:,0:1])]\n                smaps_back = [1-A for A in smaps_fore]\n            fake_t = self.G_T(source)\n            out = VGGfeatures(fake_t)\n            style_losses1 = [self.style_weights[a] * self.gramloss(A*smaps_fore[a], style_targets[0][a]) for a,A in enumerate(out)]\n            style_losses2 = [self.style_weights[a] * self.gramloss(A*smaps_back[a], style_targets[1][a]) for a,A in enumerate(out)]\n            Lsty = (sum(style_losses1)+ sum(style_losses2)) * self.lambda_sty\n            LT = LT + Lsty\n        #global id\n        #if id % 20 == 0:\n        #    viz_img = to_data(torch.cat((x[0], y[0], fake_y[0]), dim=2))\n        #    save_image(viz_img, \'../output/texturee_result%d.jpg\'%id)\n        #id += 1             \n        self.trainerG_T.zero_grad()\n        LT.backward()\n        self.trainerG_T.step()   \n        return LTadv.data.mean(), Lrec.data.mean(), Lsty.data.mean() if t is not None else 0\n    \n    def texture_one_pass(self, x, y, t=None, l=None, VGGfeatures=None, style_targets=None):\n        LDadv = self.update_texture_discriminator(x, y)\n        LGadv, Lrec, Lsty = self.update_texture_generator(x, y, t, l, VGGfeatures, style_targets)\n        return [LDadv, LGadv, Lrec, Lsty]\n    \n    def save_structure_model(self, filepath, filename):     \n        torch.save(self.G_S.state_dict(), os.path.join(filepath, filename+\'-GS.ckpt\'))\n        torch.save(self.D_S.state_dict(), os.path.join(filepath, filename+\'-DS.ckpt\'))\n    def save_texture_model(self, filepath, filename):\n        torch.save(self.G_T.state_dict(), os.path.join(filepath, filename+\'-GT.ckpt\'))\n        torch.save(self.D_T.state_dict(), os.path.join(filepath, filename+\'-DT.ckpt\'))\n'"
src/options.py,0,"b""import argparse\n\nclass TestOptions():\n    def __init__(self):\n        self.parser = argparse.ArgumentParser()\n\n        # data loader related\n        self.parser.add_argument('--text_name', type=str, default='../data/rawtext/yaheiB/val/0801.png', help='path of the text image')\n        self.parser.add_argument('--scale', type=float, default=0.0, help='glyph deformation degree,  0~1 for single scale, -1 for multiple scales in 0~1 with step of scale_step')\n        self.parser.add_argument('--scale_step', type=float, default=0.2, help='scale step')\n        self.parser.add_argument('--text_type', type=int, default=0, help='0 for distance-based text image, 1 for black and white text image')\n\n        # ouptput related\n        self.parser.add_argument('--name', type=str, default='output', help='file name of the outputs')\n        self.parser.add_argument('--result_dir', type=str, default='../output/', help='path for saving result images')\n\n        # model related\n        self.parser.add_argument('--structure_model', type=str, default='../save/fire-GS-iccv.ckpt', help='specified the dir of saved structure transfer models')\n        self.parser.add_argument('--texture_model', type=str, default='../save/fire-GT-iccv.ckpt', help='specified the dir of saved texture transfer models')\n        self.parser.add_argument('--gpu', action='store_true', default=False, help='Whether using gpu')\n\n    def parse(self):\n        self.opt = self.parser.parse_args()\n        args = vars(self.opt)\n        print('--- load options ---')\n        for name, value in sorted(args.items()):\n            print('%s: %s' % (str(name), str(value)))\n        return self.opt\n\nclass TrainSketchOptions():\n    def __init__(self):\n        self.parser = argparse.ArgumentParser()\n\n        ## Sketch Module\n        # data loader related\n        self.parser.add_argument('--text_path', type=str, default='../data/rawtext/yaheiB/train', help='path of the text images')\n        self.parser.add_argument('--augment_text_path', type=str, default='../data/rawtext/augment/', help='path of the augmented text images')        \n        self.parser.add_argument('--text_datasize', type=int, default=708, help='how many text images are loaded for training')\n        self.parser.add_argument('--augment_text_datasize', type=int, default=5, help='how many augmented text images are loaded for training')\n\n        # ouptput related\n        self.parser.add_argument('--save_GB_name', type=str, default='../save/GB.ckpt', help='path of the trained model to be saved')\n        \n        # model related\n        self.parser.add_argument('--GB_nlayers', type=int, default=6, help='number of layers in Generator')\n        self.parser.add_argument('--DB_nlayers', type=int, default=5, help='number of layers in Discriminator')  \n        self.parser.add_argument('--GB_nf', type=int, default=32, help='number of features in the first layer of the Generator')\n        self.parser.add_argument('--DB_nf', type=int, default=32, help='number of features in the first layer of the Discriminator')          \n        # trainingg related\n        self.parser.add_argument('--epochs', type=int, default=3, help='epoch number')\n        self.parser.add_argument('--batchsize', type=int, default=16, help='batch size')\n        self.parser.add_argument('--Btraining_num', type=int, default=12800, help='how many training images in each epoch')\n        self.parser.add_argument('--gpu', action='store_true', default=False, help='Whether using gpu')\n\n    def parse(self):\n        self.opt = self.parser.parse_args()\n        args = vars(self.opt)\n        print('--- load options ---')\n        for name, value in sorted(args.items()):\n            print('%s: %s' % (str(name), str(value)))\n        return self.opt    \n    \n    \nclass TrainShapeMatchingOptions():\n    def __init__(self):\n        self.parser = argparse.ArgumentParser()\n        \n        ## Structure and Texture \n        # data loader realted \n        self.parser.add_argument('--style_name', type=str, default='../data/style/fire.png', help='path of the style image')   \n        self.parser.add_argument('--load_GS_name', type=str, default='../save/fire-GS.ckpt', help='path to load the saved G_S model when training G_T')   \n        self.parser.add_argument('--Sanglejitter', action='store_true', default=False, help='Whether rotating the training images when training G_S')          \n        self.parser.add_argument('--Tanglejitter', action='store_true', default=False, help='Whether rotating the training images when training G_T')    \n        # ouptput related\n        self.parser.add_argument('--save_path', type=str, default='../save/', help='dir of the trained model to be saved')   \n        self.parser.add_argument('--save_name', type=str, default='fire', help='fikename of the trained model to be saved')          \n        # model related\n        self.parser.add_argument('--GS_nlayers', type=int, default=6, help='number of layers in structure generator G_S')\n        self.parser.add_argument('--DS_nlayers', type=int, default=4, help='number of layers in structure discriminator D_S')  \n        self.parser.add_argument('--GS_nf', type=int, default=32, help='number of features in the first layer of G_S')\n        self.parser.add_argument('--DS_nf', type=int, default=32, help='number of features in the first layer of D_S') \n        self.parser.add_argument('--GT_nlayers', type=int, default=6, help='number of layers in texture generator G_T')\n        self.parser.add_argument('--DT_nlayers', type=int, default=4, help='number of layers in texture discriminator D_T')  \n        self.parser.add_argument('--GT_nf', type=int, default=32, help='number of features in the first layer of G_T')\n        self.parser.add_argument('--DT_nf', type=int, default=32, help='number of features in the first layer of D_T')                      \n        # trainingg related\n        self.parser.add_argument('--scale_num', type=int, default=4, help='how many scales are uniformly sampled between [0,1]')\n        self.parser.add_argument('--step1_epochs', type=int, default=30, help='epoch number of training G_S on the max scale {1}')\n        self.parser.add_argument('--step2_epochs', type=int, default=40, help='epoch number of training G_S on the two extreme scales {0, 1}') \n        self.parser.add_argument('--step3_epochs', type=int, default=80, help='epoch number of training G_S on the full scales')     \n        self.parser.add_argument('--step4_epochs', type=int, default=10, help='epoch number of training G_S with glyph loss')\n        self.parser.add_argument('--texture_step1_epochs', type=int, default=40, help='epoch number of training G_T without style loss')\n        self.parser.add_argument('--texture_step2_epochs', type=int, default=10, help='epoch number of training G_T with style loss')\n        self.parser.add_argument('--batchsize', type=int, default=16, help='batch size')\n        self.parser.add_argument('--subimg_size', type=int, default=256, help='size of sub-images, which are cropped from a single image to form a training set')\n        self.parser.add_argument('--Straining_num', type=int, default=2560, help='how many training images in each epoch for strcture transfer')        \n        self.parser.add_argument('--Ttraining_num', type=int, default=800, help='how many training images in each epoch for texture transfer')\n        self.parser.add_argument('--glyph_preserve', action='store_true', default=False, help='Whether using glyph loss to preserve the text legibility')        \n        self.parser.add_argument('--style_loss', action='store_true', default=False, help='Whether using style loss')\n        self.parser.add_argument('--gpu', action='store_true', default=False, help='Whether using gpu')\n\n        ## Sketch Module\n        # data loader related\n        self.parser.add_argument('--text_path', type=str, default='../data/rawtext/yaheiB/train', help='path of the text images')     \n        self.parser.add_argument('--text_datasize', type=int, default=708, help='how many text images are loaded for training')\n        # model related\n        self.parser.add_argument('--GB_nlayers', type=int, default=6, help='number of layers in Generator G_B')\n        self.parser.add_argument('--DB_nlayers', type=int, default=5, help='number of layers in Discriminator D_B')  \n        self.parser.add_argument('--GB_nf', type=int, default=32, help='number of features in the first layer of G_B')\n        self.parser.add_argument('--DB_nf', type=int, default=32, help='number of features in the first layer of D_B') \n        self.parser.add_argument('--load_GB_name', type=str, default='../save/GB-iccv.ckpt', help='specified the dir of saved Sketch Module')\n\n    def parse(self):\n        self.opt = self.parser.parse_args()\n        args = vars(self.opt)\n        print('--- load options ---')\n        for name, value in sorted(args.items()):\n            print('%s: %s' % (str(name), str(value)))\n        return self.opt    """
src/test.py,3,"b'from options import TestOptions\nimport torch\nfrom models import GlyphGenerator, TextureGenerator\nfrom utils import load_image, to_data, to_var, visualize, save_image, gaussian\nimport os\n#os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""\n\ndef main():\n    # parse options\n    parser = TestOptions()\n    opts = parser.parse()\n\n    # data loader\n    print(\'--- load data ---\')\n    text = load_image(opts.text_name, opts.text_type)\n    label = opts.scale\n    step = opts.scale_step * 2.0\n    if opts.gpu:\n        text = to_var(text)\n    \n    # model\n    print(\'--- load model ---\')\n    netGlyph = GlyphGenerator(n_layers=6, ngf=32)\n    netTexture = TextureGenerator(n_layers=6)\n    netGlyph.load_state_dict(torch.load(opts.structure_model))\n    netTexture.load_state_dict(torch.load(opts.texture_model))\n    if opts.gpu:\n        netGlyph.cuda()\n        netTexture.cuda()\n    netGlyph.eval()\n    netTexture.eval()\n    \n    print(\'--- testing ---\')\n    text[:,0:1] = gaussian(text[:,0:1], stddev=0.2)\n    if label == -1:\n        scale = -1.0\n        noise = text.data.new(text[:,0:1].size()).normal_(0, 0.2)\n        result = []\n        while scale <= 1.0: \n            img_str = netGlyph(text, scale) \n            img_str[:,0:1] = torch.clamp(img_str[:,0:1] + noise, -1, 1)\n            result1 = netTexture(img_str).detach()\n            result = result + [result1]\n            scale = scale + step\n    else:\n        img_str = netGlyph(text, label*2.0-1.0) \n        img_str[:,0:1] = gaussian(img_str[:,0:1], stddev=0.2)\n        result = [netTexture(img_str)]\n    \n    if opts.gpu:\n        for i in range(len(result)):              \n            result[i] = to_data(result[i])\n        \n    print(\'--- save ---\')\n    # directory\n    if not os.path.exists(opts.result_dir):\n        os.mkdir(opts.result_dir)         \n    for i in range(len(result)):     \n        if label == -1:\n            result_filename = os.path.join(opts.result_dir, (opts.name+\'_\'+str(i)+\'.png\'))\n        else:\n            result_filename = os.path.join(opts.result_dir, (opts.name+\'.png\'))\n        save_image(result[i][0], result_filename)\n\nif __name__ == \'__main__\':\n    main()\n'"
src/trainSketchModule.py,1,"b'from __future__ import print_function\nimport torch\nfrom models import SketchModule\nfrom utils import load_image, to_data, to_var, visualize, save_image, gaussian, weights_init\nfrom utils import load_train_batchfnames, prepare_text_batch\nfrom options import TrainSketchOptions\nimport os\n#os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""\n\n\ndef main():\n    # parse options\n    parser = TrainSketchOptions()\n    opts = parser.parse()\n\n    # create model\n    print(\'--- create model ---\')\n    netSketch = SketchModule(opts.GB_nlayers, opts.DB_nlayers, opts.GB_nf, opts.DB_nf, opts.gpu)\n    if opts.gpu:\n        netSketch.cuda()\n    netSketch.init_networks(weights_init)\n    netSketch.train()\n\n    print(\'--- training ---\')\n    for epoch in range(opts.epochs):\n        itr = 0\n        fnames = load_train_batchfnames(opts.text_path, opts.batchsize, \n                                        opts.text_datasize, trainnum=opts.Btraining_num)\n        fnames2 = load_train_batchfnames(opts.augment_text_path, opts.batchsize, \n                                        opts.augment_text_datasize, trainnum=opts.Btraining_num)\n        for ii in range(len(fnames)):\n            fnames[ii][0:opts.batchsize//2-1] = fnames2[ii][0:opts.batchsize//2-1]\n        for fname in fnames:\n            itr += 1\n            t = prepare_text_batch(fname, anglejitter=True)\n            t = to_var(t) if opts.gpu else t\n            losses = netSketch.one_pass(t, [l/4.-1. for l in range(0,9)])      \n            print(\'Epoch [%d/%d][%03d/%03d]\' %(epoch+1, opts.epochs,itr,len(fnames)), end=\': \')\n            print(\'LDadv: %+.3f, LGadv: %+.3f, Lrec: %+.3f\'%(losses[0], losses[1], losses[2]))\n\n    print(\'--- save ---\')\n    # directory\n    torch.save(netSketch.state_dict(), opts.save_GB_name)    \n\nif __name__ == \'__main__\':\n    main()\n'"
src/trainStructureTransfer.py,1,"b'from __future__ import print_function\nimport torch\nfrom models import SketchModule, ShapeMatchingGAN\nfrom utils import load_image, to_data, to_var, visualize, save_image, gaussian, weights_init\nfrom utils import load_train_batchfnames, prepare_text_batch, load_style_image_pair, cropping_training_batches\nimport random\nfrom options import TrainShapeMatchingOptions\nimport os\n#os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""\n\n\ndef main():\n    # parse options\n    parser = TrainShapeMatchingOptions()\n    opts = parser.parse()\n\n    # create model\n    print(\'--- create model ---\')\n    netShapeM = ShapeMatchingGAN(opts.GS_nlayers, opts.DS_nlayers, opts.GS_nf, opts.DS_nf,\n                     opts.GT_nlayers, opts.DT_nlayers, opts.GT_nf, opts.DT_nf, opts.gpu)\n    netSketch = SketchModule(opts.GB_nlayers, opts.DB_nlayers, opts.GB_nf, opts.DB_nf, opts.gpu)\n\n    if opts.gpu:\n        netShapeM.cuda()\n        netSketch.cuda()\n    netShapeM.init_networks(weights_init)\n    netShapeM.train()\n\n    netSketch.load_state_dict(torch.load(opts.load_GB_name))\n    netSketch.eval()\n\n    print(\'--- training ---\')\n    # load image pair\n    scales = [l*2.0/(opts.scale_num-1)-1 for l in range(opts.scale_num)]\n    Xl, X, _, Noise = load_style_image_pair(opts.style_name, scales, netSketch, opts.gpu)\n    Xl = [to_var(a) for a in Xl] if opts.gpu else Xl\n    X = to_var(X) if opts.gpu else X\n    Noise = to_var(Noise) if opts.gpu else Noise\n    for epoch in range(opts.step1_epochs):\n        for i in range(opts.Straining_num/opts.batchsize):\n            idx = opts.scale_num-1\n            xl, x = cropping_training_batches(Xl[idx], X, Noise, opts.batchsize, \n                                      opts.Sanglejitter, opts.subimg_size, opts.subimg_size)\n            losses = netShapeM.structure_one_pass(x, xl, scales[idx])\n            print(\'Step1, Epoch [%02d/%02d][%03d/%03d]\' %(epoch+1, opts.step1_epochs, i+1, \n                                                          opts.Straining_num/opts.batchsize), end=\': \')\n            print(\'LDadv: %+.3f, LGadv: %+.3f, Lrec: %+.3f, Lgly: %+.3f\'%(losses[0], losses[1], losses[2], losses[3]))\n    netShapeM.G_S.myCopy()\n    for epoch in range(opts.step2_epochs):\n        for i in range(opts.Straining_num/opts.batchsize):\n            idx = random.choice([0, opts.scale_num-1])\n            xl, x = cropping_training_batches(Xl[idx], X, Noise, opts.batchsize, \n                                      opts.Sanglejitter, opts.subimg_size, opts.subimg_size)\n            losses = netShapeM.structure_one_pass(x, xl, scales[idx])\n            print(\'Step2, Epoch [%02d/%02d][%03d/%03d]\' %(epoch+1, opts.step2_epochs, i+1, \n                                                          opts.Straining_num/opts.batchsize), end=\': \')\n            print(\'LDadv: %+.3f, LGadv: %+.3f, Lrec: %+.3f, Lgly: %+.3f\'%(losses[0], losses[1], losses[2], losses[3]))\n    for epoch in range(opts.step3_epochs):\n        for i in range(opts.Straining_num/opts.batchsize):\n            idx = random.choice(range(opts.scale_num))\n            xl, x = cropping_training_batches(Xl[idx], X, Noise, opts.batchsize, \n                                      opts.Sanglejitter, opts.subimg_size, opts.subimg_size)\n            losses = netShapeM.structure_one_pass(x, xl, scales[idx])  \n            print(\'Step3, Epoch [%02d/%02d][%03d/%03d]\' %(epoch+1, opts.step3_epochs, i+1, \n                                                          opts.Straining_num/opts.batchsize), end=\': \')\n            print(\'LDadv: %+.3f, LGadv: %+.3f, Lrec: %+.3f, Lgly: %+.3f\'%(losses[0], losses[1], losses[2], losses[3]))\n    if opts.glyph_preserve:\n        fnames = load_train_batchfnames(opts.text_path, opts.batchsize, \n                                        opts.text_datasize, opts.Straining_num)\n        for epoch in range(opts.step4_epochs):\n            itr = 0\n            for fname in fnames:\n                itr += 1\n                t = prepare_text_batch(fname, anglejitter=False)\n                idx = random.choice(range(opts.scale_num))\n                xl, x = cropping_training_batches(Xl[idx], X, Noise, opts.batchsize, \n                                          opts.Sanglejitter, opts.subimg_size, opts.subimg_size)\n                t = to_var(x) if opts.gpu else t\n                losses = netShapeM.structure_one_pass(x, xl, scales[idx], t)  \n                print(\'Step4, Epoch [%02d/%02d][%03d/%03d]\' %(epoch+1, opts.step4_epochs, itr+1, \n                                                          len(fnames)), end=\': \')\n                print(\'LDadv: %+.3f, LGadv: %+.3f, Lrec: %+.3f, Lgly: %+.3f\'%(losses[0], losses[1], losses[2], losses[3])) \n\n    print(\'--- save ---\')\n    # directory\n    netShapeM.save_structure_model(opts.save_path, opts.save_name)    \n\nif __name__ == \'__main__\':\n    main()'"
src/trainTextureTransfer.py,1,"b'from __future__ import print_function\nimport torch\nfrom models import SketchModule, ShapeMatchingGAN\nfrom utils import load_image, to_data, to_var, visualize, save_image, gaussian, weights_init\nfrom utils import load_train_batchfnames, prepare_text_batch, load_style_image_pair, cropping_training_batches\nimport random\nfrom vgg import get_GRAM, VGGFeature\nimport torchvision.models as models\nfrom options import TrainShapeMatchingOptions\nimport os\n#os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""\n\ndef main():\n    # parse options\n    parser = TrainShapeMatchingOptions()\n    opts = parser.parse()\n\n    # create model\n    print(\'--- create model ---\')\n    netShapeM = ShapeMatchingGAN(opts.GS_nlayers, opts.DS_nlayers, opts.GS_nf, opts.DS_nf,\n                     opts.GT_nlayers, opts.DT_nlayers, opts.GT_nf, opts.DT_nf, opts.gpu)\n\n    if opts.gpu:\n        netShapeM.cuda()\n    netShapeM.init_networks(weights_init)\n    netShapeM.train()\n\n    if opts.style_loss:\n        netShapeM.G_S.load_state_dict(torch.load(opts.load_GS_name))  \n        netShapeM.G_S.eval()\n        VGGNet = models.vgg19(pretrained=True).features\n        VGGfeatures = VGGFeature(VGGNet, opts.gpu)\n        for param in VGGfeatures.parameters():\n            param.requires_grad = False\n        if opts.gpu:\n            VGGfeatures.cuda()\n        style_targets = get_GRAM(opts.style_name, VGGfeatures, opts.batchsize, opts.gpu)\n\n    print(\'--- training ---\')\n    # load image pair\n    _, X, Y, Noise = load_style_image_pair(opts.style_name, gpu=opts.gpu)\n    Y = to_var(Y) if opts.gpu else Y\n    X = to_var(X) if opts.gpu else X\n    Noise = to_var(Noise) if opts.gpu else Noise\n    for epoch in range(opts.texture_step1_epochs):\n        for i in range(opts.Ttraining_num/opts.batchsize):\n            x, y = cropping_training_batches(X, Y, Noise, opts.batchsize, \n                                      opts.Tanglejitter, opts.subimg_size, opts.subimg_size)\n            losses = netShapeM.texture_one_pass(x, y)\n            print(\'Step1, Epoch [%02d/%02d][%03d/%03d]\' %(epoch+1, opts.texture_step1_epochs, i+1,\n                                                         opts.Ttraining_num/opts.batchsize), end=\': \')\n            print(\'LDadv: %+.3f, LGadv: %+.3f, Lrec: %+.3f, Lsty: %+.3f\'%(losses[0], losses[1], losses[2], losses[3])) \n    if opts.style_loss:\n        fnames = load_train_batchfnames(opts.text_path, opts.batchsize, \n                                        opts.text_datasize, trainnum=opts.Ttraining_num)\n        for epoch in range(opts.texture_step2_epochs):\n            itr = 0\n            for fname in fnames:\n                itr += 1\n                t = prepare_text_batch(fname, anglejitter=False)\n                x, y = cropping_training_batches(X, Y, Noise, opts.batchsize, \n                                      opts.Tanglejitter, opts.subimg_size, opts.subimg_size)\n                t = to_var(t) if opts.gpu else t\n                losses = netShapeM.texture_one_pass(x, y, t, 0, VGGfeatures, style_targets)  \n                print(\'Step2, Epoch [%02d/%02d][%03d/%03d]\' %(epoch+1, opts.texture_step2_epochs, \n                                                             itr, len(fnames)), end=\': \')\n                print(\'LDadv: %+.3f, LGadv: %+.3f, Lrec: %+.3f, Lsty: %+.3f\'%(losses[0], losses[1], losses[2], losses[3])) \n\n    print(\'--- save ---\')\n    # directory\n    netShapeM.save_texture_model(opts.save_path, opts.save_name)   \n\nif __name__ == \'__main__\':\n    main()'"
src/utils.py,10,"b'import torch\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nimport numpy as np\nimport cv2\nimport scipy.ndimage as pyimg\nimport random\nimport os\n\npil2tensor = transforms.Compose([\ntransforms.ToTensor(),\ntransforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5,0.5,0.5])])\ntensor2pil = transforms.ToPILImage()\n\ndef to_var(x):\n    """"""Convert tensor to variable.""""""\n    if torch.cuda.is_available():\n        x = x.cuda()\n    return Variable(x)\n\ndef to_data(x):\n    """"""Convert variable to tensor.""""""\n    if torch.cuda.is_available():\n        x = x.cpu()\n    return x.data\n\ndef visualize(img_arr):\n    plt.imshow(((img_arr.numpy().transpose(1, 2, 0) + 1.0) * 127.5).astype(np.uint8))\n    plt.axis(\'off\')\n    \ndef load_image(filename, load_type=0):\n    transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5,0.5,0.5]),\n    ])\n    \n    if load_type == 0:\n        img = Image.open(filename)\n    else:\n        img = text_image_preprocessing(filename)\n    img = transform(img)\n        \n    return img.unsqueeze(dim=0)\n\ndef save_image(img, filename):\n    tmp = ((img.numpy().transpose(1, 2, 0) + 1.0) * 127.5).astype(np.uint8)\n    cv2.imwrite(filename, cv2.cvtColor(tmp, cv2.COLOR_RGB2BGR))\n\n# black and white text image to distance-based text image\ndef text_image_preprocessing(filename):\n    I = np.array(Image.open(filename))\n    BW = I[:,:,0] > 127\n    G_channel = pyimg.distance_transform_edt(BW)\n    G_channel[G_channel>32]=32\n    B_channel = pyimg.distance_transform_edt(1-BW)\n    B_channel[B_channel>200]=200\n    I[:,:,1] = G_channel.astype(\'uint8\')\n    I[:,:,2] = B_channel.astype(\'uint8\')\n    return Image.fromarray(I)\n\ndef gaussian(ins, mean = 0, stddev = 0.2):\n    noise = ins.data.new(ins.size()).normal_(mean, stddev)\n    return torch.clamp(ins + noise, -1, 1)\n\n# custom weights initialization called on netG and netD\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find(\'Conv\') != -1 and classname.find(\'my\') == -1:\n        m.weight.data.normal_(0.0, 0.02)\n        m.bias.data.fill_(0)\n    elif classname.find(\'BatchNorm\') != -1:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)\n        \n# prepare batched filenames of all training data\n# [[list of file names in one batch],[list of file names in one batch],...,[]]\ndef load_train_batchfnames(path, batch_size, usenum=3, trainnum=100000):\n    trainnum = int((int(trainnum) / int(batch_size)) * batch_size)\n    fnames = [(\'%04d.png\' % (i%usenum)) for i in range(trainnum)]\n    random.shuffle(fnames)\n    trainbatches = [([]) for _ in range(trainnum//batch_size)]\n    count = 0\n    for i in range(trainnum//batch_size):\n        traindatas = []\n        for j in range(batch_size):\n            traindatas += [os.path.join(path, fnames[count])]\n            count = count + 1\n        trainbatches[i] += traindatas\n    return trainbatches\n\n# prepare a batch of text images {t}\ndef prepare_text_batch(batchfnames, wd=256, ht=256, anglejitter=False):\n    img_list = []\n    for fname in batchfnames:\n        img = Image.open(fname)     \n        ori_wd, ori_ht = img.size\n        w = random.randint(0,ori_wd-wd)\n        h = random.randint(0,ori_ht-ht)\n        img = img.crop((w,h,w+wd,h+ht))\n        if anglejitter:\n            random_angle = 90 * random.randint(0,3)\n            img = img.rotate(random_angle)\n        img = pil2tensor(img)         \n        img = img.unsqueeze(dim=0)\n        img_list.append(img)\n    return torch.cat(img_list, dim=0)\n\n# prepare {Xl, X, Y, fixed_noise} in PIL format from one image pair [X,Y]\ndef load_style_image_pair(filename, scales=[-1.0,-1./3,1./3,1.0], sketchmodule=None, gpu=True):\n    img = Image.open(filename) \n    ori_wd, ori_ht = img.size\n    ori_wd = ori_wd / 2\n    X = pil2tensor(img.crop((0,0,ori_wd,ori_ht))).unsqueeze(dim=0)\n    Y = pil2tensor(img.crop((ori_wd,0,ori_wd*2,ori_ht))).unsqueeze(dim=0)\n    Xls = []    \n    Noise = torch.tensor(0).float().repeat(1, 1, 1).expand(3, ori_ht, ori_wd)\n    Noise = Noise.data.new(Noise.size()).normal_(0, 0.2)\n    Noise = Noise.unsqueeze(dim=0)\n    #Noise = tensor2pil((Noise+1)/2)    \n    if sketchmodule is not None:\n        X_ = to_var(X) if gpu else X\n        for l in scales:  \n            with torch.no_grad():\n                Xl = sketchmodule(X_, l).detach()\n            Xls.append(to_data(Xl) if gpu else Xl)            \n    return [Xls, X, Y, Noise]\n\n\ndef rotate_tensor(x, angle):\n    if angle == 1:\n        return x.transpose(2, 3).flip(2)\n    elif angle == 2:\n        return x.flip(2).flip(3)\n    elif angle == 3:\n        return x.transpose(2, 3).flip(3)\n    else:\n        return x\n    \n# crop subimages for training \n# for structure transfer:  [Input,Output]=[Xl, X]\n# for texture transfer:  [Input,Output]=[X, Y]\ndef cropping_training_batches(Input, Output, Noise, batchsize=16, anglejitter=False, wd=256, ht=256):\n    img_list = []\n    ori_wd = Input.size(2)\n    ori_ht = Input.size(3)\n    for i in range(batchsize):\n        w = random.randint(0,ori_wd-wd)\n        h = random.randint(0,ori_ht-ht)\n        input = Input[:,:,w:w+wd,h:h+ht].clone()\n        output = Output[:,:,w:w+wd,h:h+ht]\n        noise = Noise[:,:,w:w+wd,h:h+ht]\n        if anglejitter:\n            random_angle = random.randint(0,3)\n            input = rotate_tensor(input, random_angle)\n            output = rotate_tensor(output, random_angle)\n            noise = rotate_tensor(noise, random_angle)        \n        input[:,0] = torch.clamp(input[:,0] + noise[:,0], -1, 1)        \n        img_list.append(torch.cat((input,output), dim = 1))        \n    data = torch.cat(img_list, dim=0)\n    ins = data[:,0:3,:,:]\n    outs = data[:,3:,:,:]\n    return ins, outs\n'"
src/vgg.py,7,"b'import torch\nimport torch.nn as nn\nfrom torch.nn import Linear, Conv2d, BatchNorm2d, LeakyReLU, ConvTranspose2d, ReLU, Tanh, InstanceNorm2d, ReplicationPad2d\nimport torch.nn.functional as F\nimport random\nfrom utils import load_image, to_data, to_var\n\n#######################  Texture Network\nclass VGGFeature(nn.Module):\n    def __init__(self, cnn, gpu=True):\n        super(VGGFeature, self).__init__()\n        \n        self.model1 = cnn[:2]\n        self.model2 = cnn[2:7]\n        self.model3 = cnn[7:12]\n        self.model4 = cnn[12:21]\n        self.model5 = cnn[21:30]\n        cnn_normalization_mean = (torch.tensor([0.485, 0.456, 0.406]) + 1) * 0.5\n        cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]) * 0.5\n        self.cnn_normalization_mean = cnn_normalization_mean.view(-1, 1, 1)\n        self.cnn_normalization_std = cnn_normalization_std.view(-1, 1, 1)\n        if gpu:\n            self.cnn_normalization_mean = self.cnn_normalization_mean.cuda()\n            self.cnn_normalization_std = self.cnn_normalization_std.cuda()\n        \n    def forward(self, x):\n        x = (x - self.cnn_normalization_mean) / self.cnn_normalization_std\n        conv1_1 = self.model1(x)\n        conv2_1 = self.model2(conv1_1)\n        conv3_1 = self.model3(conv2_1)\n        conv4_1 = self.model4(conv3_1)\n        conv5_1 = self.model5(conv4_1)\n        return [conv1_1, conv2_1, conv3_1, conv4_1, conv5_1]\n    \n# gram matrix and loss\nclass GramMatrix(nn.Module):\n    def forward(self, input):\n        b,c,h,w = input.size()\n        F = input.view(b, c, h*w)\n        G = torch.bmm(F, F.transpose(1,2)) \n        G.div_(h*w)\n        return G\n\nclass GramMSELoss(nn.Module):\n    def forward(self, input, target):\n        out = nn.MSELoss()(GramMatrix()(input), target)\n        return(out)\n\nclass SemanticFeature(nn.Module):\n    def __init__(self):\n        super(SemanticFeature, self).__init__()\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n    def forward(self, x):\n        conv1_1 = x\n        conv2_1 = self.pool1(conv1_1)\n        conv3_1 = self.pool2(conv2_1)\n        conv4_1 = self.pool3(conv3_1)\n        conv5_1 = self.pool4(conv4_1)\n        return [conv1_1, conv2_1, conv3_1, conv4_1, conv5_1]    \n    \ndef get_GRAM(filename, VGGFeatures, batchsize, gpu=True):\n    getmask = SemanticFeature()\n    for param in getmask.parameters():\n        param.requires_grad = False\n    \n    img = load_image(filename)\n    target = img[:,:,:,img.size(3)/2:img.size(3)]\n    target_mask = img[:,:,:,0:img.size(3)/2]\n    if gpu:\n        getmask.cuda()\n        target = to_var(target)\n        target_mask = to_var(target_mask)\n\n    with torch.no_grad():\n        tmaps_fore = [(A.detach()+1)*0.5 for A in getmask(target_mask[:,0:1])]\n        tmaps_back = [1-A for A in tmaps_fore]\n        style_targets1 = [GramMatrix()(A*tmaps_fore[a]).detach() for a, A in enumerate(VGGFeatures(target))]\n        style_targets2 = [GramMatrix()(A*tmaps_back[a]).detach() for a, A in enumerate(VGGFeatures(target))]\n\n    for i in range(len(style_targets1)):\n        style_targets1[i] = style_targets1[i].repeat(batchsize,1,1)\n        style_targets2[i] = style_targets2[i].repeat(batchsize,1,1)    \n        \n    return [style_targets1, style_targets2]'"
