file_path,api_count,code
setup.py,0,"b'from setuptools import setup\n\nsetup(\n    name=""cogdl"",\n    version=""0.0.1"",\n    install_requires=[\n        ""torch"",\n        ""networkx"",\n        ""matplotlib"",\n        ""tqdm"",\n        ""numpy"",\n        ""scipy"",\n        ""six"",\n        ""gensim"",\n        ""grave"",\n        ""scikit_learn"",\n        ""tabulate"",\n        ""black"",\n        ""pytest"",\n        ""coveralls"",\n        ""sphinx"",\n        ""sphinx_rtd_theme"",\n        ""recommonmark"",\n    ],\n)\n'"
cogdl/options.py,0,"b'import argparse\n\nfrom cogdl.datasets import DATASET_REGISTRY\nfrom cogdl.models import MODEL_REGISTRY\nfrom cogdl.tasks import TASK_REGISTRY\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(conflict_handler=""resolve"")\n    # fmt: off\n    # parser.add_argument(\'--log-interval\', type=int, default=1000, metavar=\'N\',\n    #                     help=\'log progress every N batches (when progress bar is disabled)\')\n    # parser.add_argument(\'--tensorboard-logdir\', metavar=\'DIR\', default=\'\',\n    #                     help=\'path to save logs for tensorboard, should match --logdir \'\n    #                          \'of running tensorboard (default: no tensorboard logging)\')\n    parser.add_argument(\'--seed\', default=[1], type=int, nargs=\'+\', metavar=\'N\',\n                        help=\'pseudo random number generator seed\')\n    parser.add_argument(\'--max-epoch\', default=500, type=int)\n    parser.add_argument(""--patience"", type=int, default=100)\n    parser.add_argument(\'--lr\', default=0.01, type=float)\n    parser.add_argument(\'--weight-decay\', default=5e-4, type=float)\n    parser.add_argument(\'--cpu\', action=\'store_true\', help=\'use CPU instead of CUDA\')\n    parser.add_argument(\'--device-id\', default=[0], type=int, nargs=\'+\',\n                       help=\'which GPU to use\')\n    parser.add_argument(\'--save-dir\', default=\'.\', type=str)\n    parser.add_argument(\'--enhance\', action=\'store_true\', help=\'use prone to enhance embedding\')\n\n    # fmt: on\n    return parser\n\n\ndef add_task_args(parser):\n    group = parser.add_argument_group(""Task configuration"")\n    # fmt: off\n    group.add_argument(\'--task\', \'-t\', default=\'node_classification\', metavar=\'TASK\', required=True,\n                       choices=TASK_REGISTRY.keys(),\n                       help=\'Task\')\n    # fmt: on\n    return group\n\n\ndef add_dataset_args(parser):\n    group = parser.add_argument_group(""Dataset and data loading"")\n    # fmt: off\n    group.add_argument(\'--dataset\', \'-dt\', metavar=\'DATASET\', nargs=\'+\', required=True,\n                       choices=DATASET_REGISTRY.keys(),\n                       help=\'Dataset\')\n    # fmt: on\n    return group\n\n\ndef add_model_args(parser):\n    group = parser.add_argument_group(""Model configuration"")\n    # fmt: off\n    group.add_argument(\'--model\', \'-m\', metavar=\'MODEL\', nargs=\'+\', required=True,\n                       choices=MODEL_REGISTRY.keys(),\n                       help=\'Model Architecture\')\n    # fmt: on\n    return group\n\n\ndef get_training_parser():\n    parser = get_parser()\n    add_task_args(parser)\n    add_dataset_args(parser)\n    add_model_args(parser)\n    return parser\n\n\ndef get_display_data_parser():\n    parser = get_parser()\n    add_dataset_args(parser)\n    parser.add_argument(""--depth"", default=3, type=int)\n\n    return parser\n\n\ndef get_download_data_parser():\n    parser = get_parser()\n    add_dataset_args(parser)\n\n    return parser\n\n\ndef parse_args_and_arch(parser, args):\n    """"""The parser doesn\'t know about model-specific args, so we parse twice.""""""\n    # args, _ = parser.parse_known_args()\n\n    # Add *-specific args to parser.\n    TASK_REGISTRY[args.task].add_args(parser)\n    for model in args.model:\n        MODEL_REGISTRY[model].add_args(parser)\n    # Parse a second time.\n    args = parser.parse_args()\n\n    return args\n'"
cogdl/utils.py,0,"b'class ArgClass(object):\n    def __init__(self):\n        pass\n\ndef build_args_from_dict(dic):\n    args = ArgClass()\n    for key, value in dic.items():\n        args.__setattr__(key, value)\n    return args\n\nif __name__ == ""__main__"":\n    args = build_args_from_dict({\'a\': 1, \'b\': 2})\n    print(args.a, args.b)\n'"
scripts/display_data.py,0,"b'import argparse\nimport random\nimport numpy as np\nimport os.path as osp\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport networkx as nx\n\nfrom cogdl import options\nfrom cogdl.datasets import build_dataset, build_dataset_from_name\n\nfrom grave import plot_network, use_attributes\n\nfrom tabulate import tabulate\nfrom torch_geometric.datasets import Planetoid\n\n\ndef plot_graph(args):\n    if not isinstance(args.dataset, list):\n        args.dataset = [args.dataset]\n\n    for name in args.dataset:\n        dataset = build_dataset_from_name(name)\n        data = dataset[0]\n\n        depth = args.depth\n        pic_file = osp.join(args.save_dir, f""display_{name}.png"")\n\n        col_names = [\n            ""Dataset"",\n            ""#nodes"",\n            ""#edges"",\n            ""#features"",\n            ""#classes"",\n            ""#labeled data"",\n        ]\n        tab_data = [\n            [\n                name,\n                data.x.shape[0],\n                data.edge_index.shape[1],\n                data.x.shape[1],\n                len(set(data.y.numpy())),\n                sum(data.train_mask.numpy()),\n            ]\n        ]\n        print(tabulate(tab_data, headers=col_names, tablefmt=""psql""))\n\n        G = nx.Graph()\n        G.add_edges_from(\n            [\n                tuple(data.edge_index[:, i].numpy())\n                for i in range(data.edge_index.shape[1])\n            ]\n        )\n\n        s = random.choice(list(G.nodes()))\n        q = [s]\n        node_set = set([s])\n        node_index = {s: 0}\n        max_index = 1\n        for _ in range(depth):\n            nq = []\n            for x in q:\n                for key in G[x].keys():\n                    if key not in node_set:\n                        nq.append(key)\n                        node_set.add(key)\n                        node_index[key] = node_index[x] + 1\n            if len(nq) > 0:\n                max_index += 1\n            q = nq\n\n        cmap = cm.rainbow(np.linspace(0.0, 1.0, max_index))\n\n        for node, index in node_index.items():\n            G.nodes[node][""color""] = cmap[index]\n            G.nodes[node][""size""] = (max_index - index) * 50\n\n        fig, ax = plt.subplots()\n        plot_network(G.subgraph(list(node_set)), node_style=use_attributes())\n        plt.savefig(pic_file)\n        print(f""Sampled ego network saved to {pic_file} ."")\n\n\nif __name__ == ""__main__"":\n    """"""\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--seed\', \'-s\', type=int, default=0, help=\'random seed\')\n    parser.add_argument(\'--depth\', \'-d\', type=int, default=3, help=\'neighborhood depth\')\n    parser.add_argument(\'--name\', \'-n\', type=str, default=\'Cora\', help=\'dataset name\')\n    parser.add_argument(\'--file\', \'-f\', type=str, default=\'graph.jpg\', help=\'saved file name\')\n    args = parser.parse_args()\n    """"""\n    parser = options.get_display_data_parser()\n    args = parser.parse_args()\n\n    if isinstance(args.seed, list):\n        args.seed = args.seed[0]\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n\n    plot_graph(args)\n'"
scripts/download.py,0,"b'import numpy as np\n\nfrom cogdl import options\nfrom cogdl.datasets import build_dataset_from_name\n\ndef download_datasets(args):\n    if not isinstance(args.dataset, list):\n        args.dataset = [args.dataset]\n\n    for name in args.dataset:\n        dataset = build_dataset_from_name(name)\n        data = dataset[0]\n\n\nif __name__ == ""__main__"":\n    parser = options.get_download_data_parser()\n    args = parser.parse_args()\n\n    download_datasets(args)\n'"
scripts/parallel_train.py,5,"b'import copy\nimport itertools\nimport os\nimport random\nimport time\nfrom collections import defaultdict, namedtuple\n\nimport numpy as np\nimport torch\nimport torch.multiprocessing as mp\nimport torch.nn.functional as F\nfrom tabulate import tabulate\nfrom tqdm import tqdm\n\nfrom cogdl import options\nfrom cogdl.datasets import build_dataset\nfrom cogdl.tasks import build_task\nfrom train import gen_variants, tabulate_results\n\n\ndef main(args):\n    if torch.cuda.is_available() and not args.cpu:\n        pid = mp.current_process().pid\n        torch.cuda.set_device(args.pid_to_cuda[pid])\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n\n    task = build_task(args)\n    result = task.train()\n    return result\n\n\ndef gen_variants(**items):\n    Variant = namedtuple(""Variant"", items.keys())\n    return itertools.starmap(Variant, itertools.product(*items.values()))\n\n\ndef getpid(_):\n    # HACK to get different pids\n    time.sleep(1)\n    return mp.current_process().pid\n\n\nif __name__ == ""__main__"":\n    # Magic for making multiprocessing work for PyTorch\n    mp.set_start_method(""spawn"")\n\n    parser = options.get_training_parser()\n    args, _ = parser.parse_known_args()\n    args = options.parse_args_and_arch(parser, args)\n\n    # Make sure datasets are downloaded first\n    datasets = args.dataset\n    for dataset in datasets:\n        args.dataset = dataset\n        _ = build_dataset(args)\n    args.dataset = datasets\n\n    print(args)\n    variants = list(\n        gen_variants(dataset=args.dataset, model=args.model, seed=args.seed)\n    )\n\n    device_ids = args.device_id\n    if args.cpu:\n        num_workers = 1\n    else:\n        num_workers = len(device_ids)\n    print(""num_workers"", num_workers)\n\n    results_dict = defaultdict(list)\n    with mp.Pool(processes=num_workers) as pool:\n        # Map process to cuda device\n        pids = pool.map(getpid, range(num_workers))\n        pid_to_cuda = dict(zip(pids, device_ids))\n        # yield all variants\n        def variant_args_generator():\n            """"""Form variants as group with size of num_workers""""""\n            for variant in variants:\n                args.pid_to_cuda = pid_to_cuda\n                args.dataset, args.model, args.seed = variant\n                yield copy.deepcopy(args)\n\n        # Collect results\n        results = pool.map(main, variant_args_generator())\n        for variant, result in zip(variants, results):\n            results_dict[variant[:-1]].append(result)\n\n    # Average for different seeds\n    col_names = [""Variant""] + list(results_dict[variant[:-1]][-1].keys())\n    tab_data = tabulate_results(results_dict)\n    print(tabulate(tab_data, headers=col_names, tablefmt=""github""))\n'"
scripts/train.py,4,"b'import copy\nimport itertools\nimport os\nimport random\nimport time\nfrom collections import defaultdict, namedtuple\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom tabulate import tabulate\nfrom tqdm import tqdm\n\nfrom cogdl import options\nfrom cogdl.tasks import build_task\n\n\ndef main(args):\n    if torch.cuda.is_available() and not args.cpu:\n        torch.cuda.set_device(args.device_id[0])\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n\n    task = build_task(args)\n    result = task.train()\n    return result\n\n\ndef gen_variants(**items):\n    Variant = namedtuple(""Variant"", items.keys())\n    return itertools.starmap(Variant, itertools.product(*items.values()))\n\n\ndef variant_args_generator(args, variants):\n    """"""Form variants as group with size of num_workers""""""\n    for variant in variants:\n        args.dataset, args.model, args.seed = variant\n        yield copy.deepcopy(args)\n\n\ndef tabulate_results(results_dict):\n    # Average for different seeds\n    tab_data = []\n    for variant in results_dict:\n        results = np.array([list(res.values()) for res in results_dict[variant]])\n        tab_data.append(\n            [variant]\n            + list(\n                itertools.starmap(\n                    lambda x, y: f""{x:.4f}\xc2\xb1{y:.4f}"",\n                    zip(\n                        np.mean(results, axis=0).tolist(),\n                        np.std(results, axis=0).tolist(),\n                    ),\n                )\n            )\n        )\n    return tab_data\n\n\nif __name__ == ""__main__"":\n    parser = options.get_training_parser()\n    args, _ = parser.parse_known_args()\n    args = options.parse_args_and_arch(parser, args)\n    print(args)\n    assert len(args.device_id) == 1\n    variants = list(\n        gen_variants(dataset=args.dataset, model=args.model, seed=args.seed)\n    )\n\n    # Collect results\n    results_dict = defaultdict(list)\n    results = [main(args) for args in variant_args_generator(args, variants)]\n    for variant, result in zip(variants, results):\n        results_dict[variant[:-1]].append(result)\n\n    col_names = [""Variant""] + list(results_dict[variant[:-1]][-1].keys())\n    tab_data = tabulate_results(results_dict)\n    print(tabulate(tab_data, headers=col_names, tablefmt=""github""))\n'"
cogdl/data/__init__.py,0,"b'from .data import Data\nfrom .batch import Batch\nfrom .dataset import Dataset\nfrom .in_memory_dataset import InMemoryDataset\nfrom .dataloader import DataLoader, DataListLoader, DenseDataLoader\nfrom .download import download_url\nfrom .extract import extract_tar, extract_zip, extract_bz2, extract_gz\n\n__all__ = [\n    ""Data"",\n    ""Batch"",\n    ""Dataset"",\n    ""InMemoryDataset"",\n    ""DataLoader"",\n    ""DataListLoader"",\n    ""DenseDataLoader"",\n    ""download_url"",\n    ""extract_tar"",\n    ""extract_zip"",\n    ""extract_bz2"",\n    ""extract_gz"",\n]\n'"
cogdl/data/batch.py,9,"b'import re\n\nimport torch\nfrom cogdl.data import Data\n\n\nclass Batch(Data):\n    r""""""A plain old python object modeling a batch of graphs as one big\n    (dicconnected) graph. With :class:`cogdl.data.Data` being the\n    base class, all its methods can also be used here.\n    In addition, single graphs can be reconstructed via the assignment vector\n    :obj:`batch`, which maps each node to its respective graph identifier.\n    """"""\n\n    def __init__(self, batch=None, **kwargs):\n        super(Batch, self).__init__(**kwargs)\n        self.batch = batch\n        self.__data_class__ = Data\n        self.__slices__ = None\n\n    @staticmethod\n    def from_data_list(data_list, follow_batch=[]):\n        r""""""Constructs a batch object from a python list holding\n        :class:`torch_geometric.data.Data` objects.\n        The assignment vector :obj:`batch` is created on the fly.\n        Additionally, creates assignment batch vectors for each key in\n        :obj:`follow_batch`.""""""\n\n        keys = [set(data.keys) for data in data_list]\n        keys = list(set.union(*keys))\n        assert \'batch\' not in keys\n\n        batch = Batch()\n        batch.__data_class__ = data_list[0].__class__\n        batch.__slices__ = {key: [0] for key in keys}\n\n        for key in keys:\n            batch[key] = []\n\n        for key in follow_batch:\n            batch[\'{}_batch\'.format(key)] = []\n\n        cumsum = {key: 0 for key in keys}\n        batch.batch = []\n        for i, data in enumerate(data_list):\n            for key in data.keys:\n                item = data[key]\n                if torch.is_tensor(item) and item.dtype != torch.bool:\n                    item = item + cumsum[key]\n                if torch.is_tensor(item):\n                    size = item.size(data.cat_dim(key, data[key]))\n                else:\n                    size = 1\n                batch.__slices__[key].append(size + batch.__slices__[key][-1])\n                cumsum[key] = cumsum[key] + data.__inc__(key, item)\n                batch[key].append(item)\n\n                if key in follow_batch:\n                    item = torch.full((size, ), i, dtype=torch.long)\n                    batch[\'{}_batch\'.format(key)].append(item)\n\n            num_nodes = data.num_nodes\n            if num_nodes is not None:\n                item = torch.full((num_nodes, ), i, dtype=torch.long)\n                batch.batch.append(item)\n\n        if num_nodes is None:\n            batch.batch = None\n        for key in batch.keys:\n            item = batch[key][0]\n            if torch.is_tensor(item):\n                batch[key] = torch.cat(batch[key],\n                                       dim=data_list[0].cat_dim(key, item))\n            elif isinstance(item, int) or isinstance(item, float):\n                batch[key] = torch.tensor(batch[key])\n        return batch.contiguous()\n\n    def cumsum(self, key, item):\n        r""""""If :obj:`True`, the attribute :obj:`key` with content :obj:`item`\n        should be added up cumulatively before concatenated together.\n\n        .. note::\n\n            This method is for internal use only, and should only be overridden\n            if the batch concatenation process is corrupted for a specific data\n            attribute.\n        """"""\n        return bool(re.search(""(index|face)"", key))\n\n    def to_data_list(self):\n        r""""""Reconstructs the list of :class:`torch_geometric.data.Data` objects\n        from the batch object.\n        The batch object must have been created via :meth:`from_data_list` in\n        order to be able reconstruct the initial objects.""""""\n\n        if self.__slices__ is None:\n            raise RuntimeError(\n                (\'Cannot reconstruct data list from batch because the batch \'\n                 \'object was not created using Batch.from_data_list()\'))\n\n        keys = [key for key in self.keys if key[-5:] != \'batch\']\n        cumsum = {key: 0 for key in keys}\n        data_list = []\n        for i in range(len(self.__slices__[keys[0]]) - 1):\n            data = self.__data_class__()\n            for key in keys:\n                if torch.is_tensor(self[key]):\n                    data[key] = self[key].narrow(\n                        data.cat_dim(key,\n                                         self[key]), self.__slices__[key][i],\n                        self.__slices__[key][i + 1] - self.__slices__[key][i])\n                    if self[key].dtype != torch.bool:\n                        data[key] = data[key] - cumsum[key]\n                else:\n                    data[key] = self[key][self.__slices__[key][i]:self.\n                                          __slices__[key][i + 1]]\n                cumsum[key] = cumsum[key] + data.__inc__(key, data[key])\n            data_list.append(data)\n\n        return data_list\n\n\n    @property\n    def num_graphs(self):\n        """"""Returns the number of graphs in the batch.""""""\n        return self.batch[-1].item() + 1\n'"
cogdl/data/data.py,2,"b'import re\n\nimport torch\n\n\nclass Data(object):\n    r""""""A plain old python object modeling a single graph with various\n    (optional) attributes:\n\n    Args:\n        x (Tensor, optional): Node feature matrix with shape :obj:`[num_nodes,\n            num_node_features]`. (default: :obj:`None`)\n        edge_index (LongTensor, optional): Graph connectivity in COO format\n            with shape :obj:`[2, num_edges]`. (default: :obj:`None`)\n        edge_attr (Tensor, optional): Edge feature matrix with shape\n            :obj:`[num_edges, num_edge_features]`. (default: :obj:`None`)\n        y (Tensor, optional): Graph or node targets with arbitrary shape.\n            (default: :obj:`None`)\n        pos (Tensor, optional): Node position matrix with shape\n            :obj:`[num_nodes, num_dimensions]`. (default: :obj:`None`)\n\n    The data object is not restricted to these attributes and can be extented\n    by any other additional data.\n    """"""\n\n    def __init__(self, x=None, edge_index=None, edge_attr=None, y=None, pos=None):\n        self.x = x\n        self.edge_index = edge_index\n        self.edge_attr = edge_attr\n        self.y = y\n        self.pos = pos\n\n    @staticmethod\n    def from_dict(dictionary):\n        r""""""Creates a data object from a python dictionary.""""""\n        data = Data()\n        for key, item in dictionary.items():\n            data[key] = item\n        return data\n\n    def __getitem__(self, key):\n        r""""""Gets the data of the attribute :obj:`key`.""""""\n        return getattr(self, key)\n\n    def __setitem__(self, key, value):\n        """"""Sets the attribute :obj:`key` to :obj:`value`.""""""\n        setattr(self, key, value)\n\n    @property\n    def keys(self):\n        r""""""Returns all names of graph attributes.""""""\n        keys = [key for key in self.__dict__.keys() if self[key] is not None]\n        keys = [key for key in keys if key[:2] != \'__\' and key[-2:] != \'__\']\n        return keys\n    \n    def __len__(self):\n        r""""""Returns the number of all present attributes.""""""\n        return len(self.keys)\n\n    def __contains__(self, key):\n        r""""""Returns :obj:`True`, if the attribute :obj:`key` is present in the\n        data.""""""\n        return key in self.keys\n\n    def __iter__(self):\n        r""""""Iterates over all present attributes in the data, yielding their\n        attribute names and content.""""""\n        for key in sorted(self.keys):\n            yield key, self[key]\n\n    def __call__(self, *keys):\n        r""""""Iterates over all attributes :obj:`*keys` in the data, yielding\n        their attribute names and content.\n        If :obj:`*keys` is not given this method will iterative over all\n        present attributes.""""""\n        for key in sorted(self.keys) if not keys else keys:\n            if self[key] is not None:\n                yield key, self[key]\n\n    def cat_dim(self, key, value):\n        r""""""Returns the dimension in which the attribute :obj:`key` with\n        content :obj:`value` gets concatenated when creating batches.\n\n        .. note::\n\n            This method is for internal use only, and should only be overridden\n            if the batch concatenation process is corrupted for a specific data\n            attribute.\n        """"""\n        # `*index*` and `*face*` should be concatenated in the last dimension,\n        # everything else in the first dimension.\n        return -1 if bool(re.search(""(index|face)"", key)) else 0\n    \n    def __inc__(self, key, value):\n        r""""""""Returns the incremental count to cumulatively increase the value\n        of the next attribute of :obj:`key` when creating batches.\n\n        .. note::\n\n            This method is for internal use only, and should only be overridden\n            if the batch concatenation process is corrupted for a specific data\n            attribute.\n        """"""\n        # Only `*index*` and `*face*` should be cumulatively summed up when\n        # creating batches.\n        return self.num_nodes if bool(re.search(\'(index|face)\', key)) else 0\n\n    @property\n    def num_edges(self):\n        r""""""Returns the number of edges in the graph.""""""\n        for key, item in self(""edge_index"", ""edge_attr""):\n            return item.size(self.cat_dim(key, item))\n        return None\n\n    @property\n    def num_features(self):\n        r""""""Returns the number of features per node in the graph.""""""\n        return 1 if self.x.dim() == 1 else self.x.size(1)\n\n    @property\n    def num_nodes(self):\n        if self.x is not None:\n            return self.x.shape[0]\n        return torch.max(self.edge_index)+1\n\n    def is_coalesced(self):\n        r""""""Returns :obj:`True`, if edge indices are ordered and do not contain\n        duplicate entries.""""""\n        row, col = self.edge_index\n        index = self.num_nodes * row + col\n        return row.size(0) == torch.unique(index).size(0)\n\n    def apply(self, func, *keys):\n        r""""""Applies the function :obj:`func` to all attributes :obj:`*keys`.\n        If :obj:`*keys` is not given, :obj:`func` is applied to all present\n        attributes.\n        """"""\n        for key, item in self(*keys):\n            self[key] = func(item)\n        return self\n\n    def contiguous(self, *keys):\n        r""""""Ensures a contiguous memory layout for all attributes :obj:`*keys`.\n        If :obj:`*keys` is not given, all present attributes are ensured to\n        have a contiguous memory layout.""""""\n        return self.apply(lambda x: x.contiguous(), *keys)\n\n    def to(self, device, *keys):\n        r""""""Performs tensor dtype and/or device conversion to all attributes\n        :obj:`*keys`.\n        If :obj:`*keys` is not given, the conversion is applied to all present\n        attributes.""""""\n        return self.apply(lambda x: x.to(device), *keys)\n\n    def cuda(self, *keys):\n        return self.apply(lambda x: x.cuda(), *keys)\n\n    def clone(self):\n        return Data.from_dict({k: v.clone() for k, v in self})\n\n    def __repr__(self):\n        info = [""{}={}"".format(key, list(item.size())) for key, item in self]\n        return ""{}({})"".format(self.__class__.__name__, "", "".join(info))\n'"
cogdl/data/dataloader.py,5,"b'import torch.utils.data\nfrom torch.utils.data.dataloader import default_collate\n\nfrom cogdl.data import Batch\n\n\nclass DataLoader(torch.utils.data.DataLoader):\n    r""""""Data loader which merges data objects from a\n    :class:`cogdl.data.dataset` to a mini-batch.\n\n    Args:\n        dataset (Dataset): The dataset from which to load the data.\n        batch_size (int, optional): How may samples per batch to load.\n            (default: :obj:`1`)\n        shuffle (bool, optional): If set to :obj:`True`, the data will be\n            reshuffled at every epoch (default: :obj:`True`)\n    """"""\n\n    def __init__(self, dataset, batch_size=1, shuffle=True, **kwargs):\n        super(DataLoader, self).__init__(\n            dataset,\n            batch_size,\n            shuffle,\n            collate_fn=lambda data_list: Batch.from_data_list(data_list),\n            **kwargs\n        )\n\n\nclass DataListLoader(torch.utils.data.DataLoader):\n    r""""""Data loader which merges data objects from a\n    :class:`cogdl.data.dataset` to a python list.\n\n    .. note::\n\n        This data loader should be used for multi-gpu support via\n        :class:`cogdl.nn.DataParallel`.\n\n    Args:\n        dataset (Dataset): The dataset from which to load the data.\n        batch_size (int, optional): How may samples per batch to load.\n            (default: :obj:`1`)\n        shuffle (bool, optional): If set to :obj:`True`, the data will be\n            reshuffled at every epoch (default: :obj:`True`)\n    """"""\n\n    def __init__(self, dataset, batch_size=1, shuffle=True, **kwargs):\n        super(DataListLoader, self).__init__(\n            dataset,\n            batch_size,\n            shuffle,\n            collate_fn=lambda data_list: data_list,\n            **kwargs\n        )\n\n\nclass DenseDataLoader(torch.utils.data.DataLoader):\n    r""""""Data loader which merges data objects from a\n    :class:`cogdl.data.dataset` to a mini-batch.\n\n    .. note::\n\n        To make use of this data loader, all graphs in the dataset needs to\n        have the same shape for each its attributes.\n        Therefore, this data loader should only be used when working with\n        *dense* adjacency matrices.\n\n    Args:\n        dataset (Dataset): The dataset from which to load the data.\n        batch_size (int, optional): How may samples per batch to load.\n            (default: :obj:`1`)\n        shuffle (bool, optional): If set to :obj:`True`, the data will be\n            reshuffled at every epoch (default: :obj:`True`)\n    """"""\n\n    def __init__(self, dataset, batch_size=1, shuffle=True, **kwargs):\n        def dense_collate(data_list):\n            batch = Batch()\n            for key in data_list[0].keys:\n                batch[key] = default_collate([d[key] for d in data_list])\n            return batch\n\n        super(DenseDataLoader, self).__init__(\n            dataset, batch_size, shuffle, collate_fn=dense_collate, **kwargs\n        )\n'"
cogdl/data/dataset.py,2,"b'import collections\nimport os.path as osp\n\nimport torch.utils.data\n\nfrom .makedirs import makedirs\n\n\ndef to_list(x):\n    if not isinstance(x, collections.Iterable) or isinstance(x, str):\n        x = [x]\n    return x\n\n\ndef files_exist(files):\n    return all([osp.exists(f) for f in files])\n\n\nclass Dataset(torch.utils.data.Dataset):\n    r""""""Dataset base class for creating graph datasets.\n    See `here <https://rusty1s.github.io/pycogdl/build/html/notes/\n    create_dataset.html>`__ for the accompanying tutorial.\n\n    Args:\n        root (string): Root directory where the dataset should be saved.\n        transform (callable, optional): A function/transform that takes in an\n            :obj:`cogdl.data.Data` object and returns a transformed\n            version. The data object will be transformed before every access.\n            (default: :obj:`None`)\n        pre_transform (callable, optional): A function/transform that takes in\n            an :obj:`cogdl.data.Data` object and returns a\n            transformed version. The data object will be transformed before\n            being saved to disk. (default: :obj:`None`)\n        pre_filter (callable, optional): A function that takes in an\n            :obj:`cogdl.data.Data` object and returns a boolean\n            value, indicating whether the data object should be included in the\n            final dataset. (default: :obj:`None`)\n    """"""\n\n    @property\n    def raw_file_names(self):\n        r""""""The name of the files to find in the :obj:`self.raw_dir` folder in\n        order to skip the download.""""""\n        raise NotImplementedError\n\n    @property\n    def processed_file_names(self):\n        r""""""The name of the files to find in the :obj:`self.processed_dir`\n        folder in order to skip the processing.""""""\n        raise NotImplementedError\n\n    def download(self):\n        r""""""Downloads the dataset to the :obj:`self.raw_dir` folder.""""""\n        raise NotImplementedError\n\n    def process(self):\n        r""""""Processes the dataset to the :obj:`self.processed_dir` folder.""""""\n        raise NotImplementedError\n\n    def __len__(self):\n        r""""""The number of examples in the dataset.""""""\n        raise NotImplementedError\n\n    def get(self, idx):\n        r""""""Gets the data object at index :obj:`idx`.""""""\n        raise NotImplementedError\n\n    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n        super(Dataset, self).__init__()\n\n        self.root = osp.expanduser(osp.normpath(root))\n        self.raw_dir = osp.join(self.root, ""raw"")\n        self.processed_dir = osp.join(self.root, ""processed"")\n        self.transform = transform\n        self.pre_transform = pre_transform\n        self.pre_filter = pre_filter\n\n        self._download()\n        self._process()\n\n    @property\n    def num_features(self):\n        r""""""Returns the number of features per node in the graph.""""""\n        return self[0].num_features\n\n    @property\n    def raw_paths(self):\n        r""""""The filepaths to find in order to skip the download.""""""\n        files = to_list(self.raw_file_names)\n        return [osp.join(self.raw_dir, f) for f in files]\n\n    @property\n    def processed_paths(self):\n        r""""""The filepaths to find in the :obj:`self.processed_dir`\n        folder in order to skip the processing.""""""\n        files = to_list(self.processed_file_names)\n        return [osp.join(self.processed_dir, f) for f in files]\n\n    def _download(self):\n        if files_exist(self.raw_paths):  # pragma: no cover\n            return\n\n        makedirs(self.raw_dir)\n        self.download()\n\n    def _process(self):\n        if files_exist(self.processed_paths):  # pragma: no cover\n            return\n\n        print(""Processing..."")\n\n        makedirs(self.processed_dir)\n        self.process()\n\n        print(""Done!"")\n\n    def __getitem__(self, idx):  # pragma: no cover\n        r""""""Gets the data object at index :obj:`idx` and transforms it (in case\n        a :obj:`self.transform` is given).""""""\n        data = self.get(idx)\n        data = data if self.transform is None else self.transform(data)\n        return data\n\n    def __repr__(self):  # pragma: no cover\n        return ""{}({})"".format(self.__class__.__name__, len(self))\n'"
cogdl/data/download.py,0,"b'from __future__ import print_function\n\nimport os.path as osp\nfrom six.moves import urllib\n\nfrom .makedirs import makedirs\n\n\ndef download_url(url, folder, name=None, log=True):\n    r""""""Downloads the content of an URL to a specific folder.\n\n    Args:\n        url (string): The url.\n        folder (string): The folder.\n        log (bool, optional): If :obj:`False`, will not print anything to the\n            console. (default: :obj:`True`)\n    """"""\n    if log:\n        print(""Downloading"", url)\n\n    makedirs(folder)\n\n    data = urllib.request.urlopen(url)\n    if name is None:\n        filename = url.rpartition(""/"")[2]\n    else:\n        filename = name\n    path = osp.join(folder, filename)\n\n    with open(path, ""wb"") as f:\n        f.write(data.read())\n\n    return path\n'"
cogdl/data/extract.py,0,"b'from __future__ import print_function\n\nimport os.path as osp\nimport tarfile\nimport zipfile\nimport bz2\nimport gzip\n\n\ndef maybe_log(path, log=True):\n    if log:\n        print(""Extracting"", path)\n\n\ndef extract_tar(path, folder, mode=""r:gz"", log=True):\n    r""""""Extracts a tar archive to a specific folder.\n\n    Args:\n        path (string): The path to the tar archive.\n        folder (string): The folder.\n        mode (string, optional): The compression mode. (default: :obj:`""r:gz""`)\n        log (bool, optional): If :obj:`False`, will not print anything to the\n            console. (default: :obj:`True`)\n    """"""\n    maybe_log(path, log)\n    with tarfile.open(path, mode) as f:\n        f.extractall(folder)\n\n\ndef extract_zip(path, folder, log=True):\n    r""""""Extracts a zip archive to a specific folder.\n\n    Args:\n        path (string): The path to the tar archive.\n        folder (string): The folder.\n        log (bool, optional): If :obj:`False`, will not print anything to the\n            console. (default: :obj:`True`)\n    """"""\n    maybe_log(path, log)\n    with zipfile.ZipFile(path, ""r"") as f:\n        f.extractall(folder)\n\n\ndef extract_bz2(path, folder, log=True):\n    maybe_log(path, log)\n    with bz2.open(path, ""r"") as r:\n        with open(osp.join(folder, ""."".join(path.split(""."")[:-1])), ""wb"") as w:\n            w.write(r.read())\n\n\ndef extract_gz(path, folder, log=True):\n    maybe_log(path, log)\n    with gzip.open(path, ""r"") as r:\n        with open(osp.join(folder, ""."".join(path.split(""."")[:-1])), ""wb"") as w:\n            w.write(r.read())\n'"
cogdl/data/in_memory_dataset.py,5,"b'from itertools import repeat, product\n\nimport torch\nfrom cogdl.data import Dataset, Data\n\n\nclass InMemoryDataset(Dataset):\n    r""""""Dataset base class for creating graph datasets which fit completely\n    into memory.\n    See `here <https://rusty1s.github.io/pycogdl/build/html/notes/\n    create_dataset.html#creating-in-memory-datasets>`__ for the accompanying\n    tutorial.\n\n    Args:\n        root (string): Root directory where the dataset should be saved.\n        transform (callable, optional): A function/transform that takes in an\n            :obj:`cogdl.data.Data` object and returns a transformed\n            version. The data object will be transformed before every access.\n            (default: :obj:`None`)\n        pre_transform (callable, optional): A function/transform that takes in\n            an :obj:`cogdl.data.Data` object and returns a\n            transformed version. The data object will be transformed before\n            being saved to disk. (default: :obj:`None`)\n        pre_filter (callable, optional): A function that takes in an\n            :obj:`cogdl.data.Data` object and returns a boolean\n            value, indicating whether the data object should be included in the\n            final dataset. (default: :obj:`None`)\n    """"""\n\n    @property\n    def raw_file_names(self):\n        r""""""The name of the files to find in the :obj:`self.raw_dir` folder in\n        order to skip the download.""""""\n        raise NotImplementedError\n\n    @property\n    def processed_file_names(self):\n        r""""""The name of the files to find in the :obj:`self.processed_dir`\n        folder in order to skip the processing.""""""\n        raise NotImplementedError\n\n    def download(self):\n        r""""""Downloads the dataset to the :obj:`self.raw_dir` folder.""""""\n        raise NotImplementedError\n\n    def process(self):\n        r""""""Processes the dataset to the :obj:`self.processed_dir` folder.""""""\n        raise NotImplementedError\n\n    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n        super(InMemoryDataset, self).__init__(\n            root, transform, pre_transform, pre_filter\n        )\n        self.data, self.slices = None, None\n\n    @property\n    def num_classes(self):\n        r""""""The number of classes in the dataset.""""""\n        data = self.data\n        return data.y.max().item() + 1 if data.y.dim() == 1 else data.y.size(1)\n\n    def __len__(self):\n        return self.slices[list(self.slices.keys())[0]].size(0) - 1\n\n    def __getitem__(self, idx):\n        r""""""Gets the data object at index :obj:`idx` and transforms it (in case\n        a :obj:`self.transform` is given).\n        Returns a data object, if :obj:`idx` is a scalar, and a new dataset in\n        case :obj:`idx` is a slicing object, *e.g.*, :obj:`[2:5]`, a LongTensor\n        or a ByteTensor.""""""\n        if isinstance(idx, int):\n            data = self.get(idx)\n            data = data if self.transform is None else self.transform(data)\n            return data\n        elif isinstance(idx, slice):\n            return self._indexing(range(*idx.indices(len(self))))\n        elif isinstance(idx, torch.LongTensor):\n            return self._indexing(idx)\n        elif isinstance(idx, torch.ByteTensor):\n            return self._indexing(idx.nonzero())\n\n        raise IndexError(\n            ""Only integers, slices (`:`) and long or byte tensors are valid ""\n            ""indices (got {})."".format(type(idx).__name__)\n        )\n\n    def shuffle(self):\n        r""""""Randomly shuffles the examples in the dataset.""""""\n        return self._indexing(torch.randperm(len(self)))\n\n    def get(self, idx):\n        data = Data()\n        for key in self.data.keys:\n            item, slices = self.data[key], self.slices[key]\n            s = list(repeat(slice(None), item.dim()))\n            s[self.data.cat_dim(key, item)] = slice(slices[idx], slices[idx + 1])\n            data[key] = item[s]\n        return data\n\n    def _indexing(self, index):\n        copy = self.__class__.__new__(self.__class__)\n        copy.__dict__ = self.__dict__.copy()\n        copy.data, copy.slices = self.collate([self.get(i) for i in index])\n        return copy\n\n    def collate(self, data_list):\n        r""""""Collates a python list of data objects to the internal storage\n        format of :class:`cogdl.data.InMemoryDataset`.""""""\n        keys = data_list[0].keys\n        data = Data()\n\n        for key in keys:\n            data[key] = []\n        slices = {key: [0] for key in keys}\n\n        for item, key in product(data_list, keys):\n            data[key].append(item[key])\n            s = slices[key][-1] + item[key].size(item.cat_dim(key, item[key]))\n            slices[key].append(s)\n\n        for key in keys:\n            data[key] = torch.cat(\n                data[key], dim=data_list[0].cat_dim(key, data_list[0][key])\n            )\n            slices[key] = torch.LongTensor(slices[key])\n\n        return data, slices\n'"
cogdl/data/makedirs.py,0,b'import os\nimport os.path as osp\nimport errno\n\n\ndef makedirs(path):\n    try:\n        os.makedirs(osp.expanduser(osp.normpath(path)))\n    except OSError as e:\n        if e.errno != errno.EEXIST and osp.isdir(path):\n            raise e\n'
cogdl/datasets/__init__.py,0,"b'import importlib\nimport os\n\nfrom cogdl.data.dataset import Dataset\n\ntry:\n    import torch_geometric\nexcept ImportError:\n    pyg = False\nelse:\n    pyg = True\n\nDATASET_REGISTRY = {}\n\n\ndef register_dataset(name):\n    """"""\n    New dataset types can be added to cogdl with the :func:`register_dataset`\n    function decorator.\n\n    For example::\n\n        @register_dataset(\'my_dataset\')\n        class MyDataset():\n            (...)\n\n    Args:\n        name (str): the name of the dataset\n    """"""\n\n    def register_dataset_cls(cls):\n        if name in DATASET_REGISTRY:\n            raise ValueError(""Cannot register duplicate dataset ({})"".format(name))\n        if not issubclass(cls, Dataset) and (\n            pyg and not issubclass(cls, torch_geometric.data.Dataset)\n        ):\n            raise ValueError(\n                ""Dataset ({}: {}) must extend cogdl.data.Dataset"".format(\n                    name, cls.__name__\n                )\n            )\n        DATASET_REGISTRY[name] = cls\n        return cls\n\n    return register_dataset_cls\n\n\n# automatically import any Python files in the datasets/ directory\nfor file in os.listdir(os.path.dirname(__file__)):\n    if file.endswith("".py"") and not file.startswith(""_""):\n        dataset_name = file[: file.find("".py"")]\n        if not pyg and dataset_name.startswith(""pyg""):\n            continue\n        module = importlib.import_module(""cogdl.datasets."" + dataset_name)\n\n\ndef build_dataset(args):\n    return DATASET_REGISTRY[args.dataset]()\n\n\ndef build_dataset_from_name(dataset):\n    return DATASET_REGISTRY[dataset]()\n'"
cogdl/datasets/edgelist_label.py,4,"b'import json\nimport os\nimport os.path as osp\nimport sys\nfrom itertools import product\n\nimport networkx as nx\nimport numpy as np\nimport torch\n\nfrom cogdl.data import Data, Dataset, download_url\n\nfrom . import register_dataset\n\n\ndef read_edgelist_label_data(folder, prefix):\n    graph_path = osp.join(folder, ""{}.ungraph"".format(prefix))\n    cmty_path = osp.join(folder, ""{}.cmty"".format(prefix))\n\n    G = nx.read_edgelist(graph_path, nodetype=int, create_using=nx.Graph())\n    num_node = G.number_of_nodes()\n    print(""edge number: "", num_node)\n    with open(graph_path) as f:\n        context = f.readlines()\n        print(""edge number: "", len(context))\n        edge_index = np.zeros((2, len(context)))\n        for i, line in enumerate(context):\n            edge_index[:, i] = list(map(int, line.strip().split(""\\t"")))\n    edge_index = torch.from_numpy(edge_index).to(torch.int)\n\n    with open(cmty_path) as f:\n        context = f.readlines()\n        print(""class number: "", len(context))\n        label = np.zeros((num_node, len(context)))\n\n        for i, line in enumerate(context):\n            line = map(int, line.strip().split(""\\t""))\n            for node in line:\n                label[node, i] = 1\n\n    y = torch.from_numpy(label).to(torch.float)\n    data = Data(x=None, edge_index=edge_index, y=y)\n\n    return data\n\n\nclass EdgelistLabel(Dataset):\n    r""""""networks from the https://github.com/THUDM/ProNE/raw/master/data\n\n    Args:\n        root (string): Root directory where the dataset should be saved.\n        name (string): The name of the dataset (:obj:`""Wikipedia""`).\n    """"""\n\n    url = ""https://github.com/THUDM/ProNE/raw/master/data""\n\n    def __init__(self, root, name):\n        self.name = name\n        super(EdgelistLabel, self).__init__(root)\n        self.data = torch.load(self.processed_paths[0])\n\n    @property\n    def raw_file_names(self):\n        splits = [self.name]\n        files = [""ungraph"", ""cmty""]\n        return [""{}.{}"".format(s, f) for s, f in product(splits, files)]\n\n    @property\n    def processed_file_names(self):\n        return [""data.pt""]\n\n    def get(self, idx):\n        assert idx == 0\n        return self.data\n\n    def download(self):\n        for name in self.raw_file_names:\n            download_url(""{}/{}"".format(self.url, name), self.raw_dir)\n\n    def process(self):\n        data = read_edgelist_label_data(self.raw_dir, self.name)\n        torch.save(data, self.processed_paths[0])\n\n\n@register_dataset(""dblp"")\nclass DBLP(EdgelistLabel):\n    def __init__(self):\n        dataset = ""dblp""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        super(DBLP, self).__init__(path, dataset)\n'"
cogdl/datasets/edgelist_label_type.py,6,"b'import json\nimport os\nimport os.path as osp\nimport sys\nfrom itertools import product\n\nimport networkx as nx\nimport numpy as np\nimport torch\n\nfrom cogdl.data import Data, Dataset, download_url\n\nfrom . import register_dataset\n\n\ndef read_edgelist_label_type_data(folder, prefix):\n    graph_path = osp.join(folder, ""{}.ungraph"".format(prefix))\n    cmty_path = osp.join(folder, ""{}.cmty"".format(prefix))\n    type_path = osp.join(folder, ""{}.nt"".format(prefix))\n\n    G = nx.read_edgelist(graph_path, nodetype=int, create_using=nx.DiGraph())\n    num_node = G.number_of_nodes()\n    print(""node number: "", num_node)\n    with open(graph_path) as f:\n        context = f.readlines()\n        print(""edge number: "", len(context))\n        edge_index = np.zeros((2, len(context)))\n        edge_attr = np.ones((len(context)))\n        for i, line in enumerate(context):\n            edge = list(map(float, line.strip().split(""\\t"")))\n            edge_index[:, i] = edge[:2]\n            if len(edge) == 3: edge_attr[i] = edge[-1]\n    \n    edge_index = torch.from_numpy(edge_index).to(torch.int)\n    edge_attr = torch.from_numpy(edge_attr).to(torch.float)\n    with open(cmty_path) as f:\n        context = f.readlines()\n        print(""class number: "", len(context))\n        label = np.zeros((num_node, len(context)))\n        for i, line in enumerate(context):\n            line = map(int, line.strip().split(""\\t""))\n            for node in line:\n                label[node, i] = 1\n    \n    node_type = np.zeros((num_node,))\n    with open(type_path) as f:\n        context = f.readlines()\n        for i, line in enumerate(context):\n            node, ntype = map(int, line.strip().split(""\\t""))\n            node_type[node] = ntype\n\n    y = torch.from_numpy(label).to(torch.float)\n    pos = torch.from_numpy(node_type).to(torch.int)\n    data = Data(x=None, edge_index=edge_index, edge_attr=edge_attr, y=y, pos=pos)\n\n    return data\n\n\nclass EdgelistLabelType(Dataset):\n    r""""""networks from the https://github.com/THUDM/ProNE/raw/master/data\n\n    Args:\n        root (string): Root directory where the dataset should be saved.\n        name (string): The name of the dataset (:obj:`""Wikipedia""`).\n    """"""\n\n    url = ""https://github.com/THUDM/ProNE/raw/master/data""\n\n    def __init__(self, root, name):\n        self.name = name\n        super(EdgelistLabelType, self).__init__(root)\n        self.data = torch.load(self.processed_paths[0])\n\n    @property\n    def raw_file_names(self):\n        splits = [self.name]\n        files = [""ungraph"", ""cmty"", ""nt""]\n        return [""{}.{}"".format(s, f) for s, f in product(splits, files)]\n\n    @property\n    def processed_file_names(self):\n        return [""data.pt""]\n\n    def get(self, idx):\n        assert idx == 0\n        return self.data\n\n    def download(self):\n        for name in self.raw_file_names:\n            download_url(""{}/{}"".format(self.url, name), self.raw_dir)\n\n    def process(self):\n        data = read_edgelist_label_type_data(self.raw_dir, self.name)\n        torch.save(data, self.processed_paths[0])\n\n\n@register_dataset(""test"")\nclass Test(EdgelistLabelType):\n    def __init__(self):\n        dataset = ""test""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        super(Test, self).__init__(path, dataset)\n\n\n\n@register_dataset(""aminer"")\nclass Aminer(EdgelistLabelType):\n    def __init__(self):\n        dataset = ""aminer""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        super(Aminer, self).__init__(path, dataset)\n'"
cogdl/datasets/gatne.py,2,"b'import os.path as osp\nimport sys\n\nimport torch\n\nfrom cogdl.data import Data, Dataset, download_url\n\nfrom . import register_dataset\n\n\ndef read_gatne_data(folder):\n    train_data = {}\n    with open(osp.join(folder, ""{}"".format(""train.txt"")), ""r"") as f:\n        for line in f:\n            items = line.strip().split()\n            if items[0] not in train_data:\n                train_data[items[0]] = []\n            train_data[items[0]].append([int(items[1]), int(items[2])])\n\n    valid_data = {}\n    with open(osp.join(folder, ""{}"".format(""valid.txt"")), ""r"") as f:\n        for line in f:\n            items = line.strip().split()\n            if items[0] not in valid_data:\n                valid_data[items[0]] = [[], []]\n            valid_data[items[0]][1 - int(items[3])].append(\n                [int(items[1]), int(items[2])]\n            )\n\n    test_data = {}\n    with open(osp.join(folder, ""{}"".format(""test.txt"")), ""r"") as f:\n        for line in f:\n            items = line.strip().split()\n            if items[0] not in test_data:\n                test_data[items[0]] = [[], []]\n            test_data[items[0]][1 - int(items[3])].append(\n                [int(items[1]), int(items[2])]\n            )\n\n    data = Data()\n    data.train_data = train_data\n    data.valid_data = valid_data\n    data.test_data = test_data\n    return data\n\n\nclass GatneDataset(Dataset):\n    r""""""The network datasets ""Amazon"", ""Twitter"" and ""YouTube"" from the\n    `""Representation Learning for Attributed Multiplex Heterogeneous Network""\n    <https://arxiv.org/abs/1905.01669>`_ paper.\n\n    Args:\n        root (string): Root directory where the dataset should be saved.\n        name (string): The name of the dataset (:obj:`""Amazon""`,\n            :obj:`""Twitter""`, :obj:`""YouTube""`).\n    """"""\n\n    url = ""https://github.com/THUDM/GATNE/raw/master/data""\n\n    def __init__(self, root, name):\n        self.name = name\n        super(GatneDataset, self).__init__(root)\n        self.data = torch.load(self.processed_paths[0])\n\n    @property\n    def raw_file_names(self):\n        names = [""train.txt"", ""valid.txt"", ""test.txt""]\n        return names\n\n    @property\n    def processed_file_names(self):\n        return [""data.pt""]\n\n    def get(self, idx):\n        assert idx == 0\n        return self.data\n\n    def download(self):\n        for name in self.raw_file_names:\n            download_url(""{}/{}/{}"".format(self.url, self.name.lower(), name), self.raw_dir)\n\n    def process(self):\n        data = read_gatne_data(self.raw_dir)\n        torch.save(data, self.processed_paths[0])\n\n    def __repr__(self):\n        return ""{}()"".format(self.name)\n\n\n@register_dataset(""amazon"")\nclass AmazonDataset(GatneDataset):\n    def __init__(self):\n        dataset = ""amazon""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        super(AmazonDataset, self).__init__(path, dataset)\n\n\n@register_dataset(""twitter"")\nclass TwitterDataset(GatneDataset):\n    def __init__(self):\n        dataset = ""twitter""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        super(TwitterDataset, self).__init__(path, dataset)\n\n\n@register_dataset(""youtube"")\nclass YouTubeDataset(GatneDataset):\n    def __init__(self):\n        dataset = ""youtube""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        super(YouTubeDataset, self).__init__(path, dataset)\n'"
cogdl/datasets/gtn_data.py,20,"b'import sys\nimport time\nimport os\nimport os.path as osp\nimport requests\nimport shutil\nimport tqdm\nimport pickle\nimport numpy as np\n\nimport torch\n\nfrom cogdl.data import Data, Dataset, download_url\n\nfrom . import register_dataset\n\n\ndef untar(path, fname, deleteTar=True):\n    """"""\n    Unpacks the given archive file to the same directory, then (by default)\n    deletes the archive file.\n    """"""\n    print(\'unpacking \' + fname)\n    fullpath = os.path.join(path, fname)\n    shutil.unpack_archive(fullpath, path)\n    if deleteTar:\n        os.remove(fullpath)\n\nclass GTNDataset(Dataset):\n    r""""""The network datasets ""ACM"", ""DBLP"" and ""IMDB"" from the\n    `""Graph Transformer Networks""\n    <https://arxiv.org/abs/1911.06455>`_ paper.\n    \n    Args:\n        root (string): Root directory where the dataset should be saved.\n        name (string): The name of the dataset (:obj:`""gtn-acm""`,\n            :obj:`""gtn-dblp""`, :obj:`""gtn-imdb""`).\n    """"""\n\n    def __init__(self, root, name):\n        self.name = name\n        self.url = f\'https://github.com/cenyk1230/gtn-data/blob/master/{name}.zip?raw=true\'\n        super(GTNDataset, self).__init__(root)\n        self.data = torch.load(self.processed_paths[0])\n        self.num_classes = torch.max(self.data.train_target).item() + 1\n        self.num_edge = len(self.data.adj)\n        self.num_nodes = self.data.x.shape[0]\n\n\n    @property\n    def raw_file_names(self):\n        names = [""edges.pkl"", ""labels.pkl"", ""node_features.pkl""]\n        return names\n\n    @property\n    def processed_file_names(self):\n        return [""data.pt""]\n\n    def read_gtn_data(self, folder):\n        edges = pickle.load(open(osp.join(folder, \'edges.pkl\'), \'rb\'))\n        labels = pickle.load(open(osp.join(folder, \'labels.pkl\'), \'rb\'))\n        node_features = pickle.load(open(osp.join(folder, \'node_features.pkl\'), \'rb\'))\n\n        data = Data()\n        data.x = torch.from_numpy(node_features).type(torch.FloatTensor)\n\n        num_nodes = edges[0].shape[0]\n\n        node_type = np.zeros((num_nodes), dtype=int)\n        assert len(edges)==4\n        assert len(edges[0].nonzero())==2\n        \n        node_type[edges[0].nonzero()[0]] = 0\n        node_type[edges[0].nonzero()[1]] = 1\n        node_type[edges[1].nonzero()[0]] = 1\n        node_type[edges[1].nonzero()[1]] = 0       \n        node_type[edges[2].nonzero()[0]] = 0\n        node_type[edges[2].nonzero()[1]] = 2\n        node_type[edges[3].nonzero()[0]] = 2\n        node_type[edges[3].nonzero()[1]] = 0\n          \n        print(node_type)\n        data.pos = torch.from_numpy(node_type)\n        \n        edge_list = []\n        for i, edge in enumerate(edges):\n            edge_tmp = torch.from_numpy(np.vstack((edge.nonzero()[0], edge.nonzero()[1]))).type(torch.LongTensor)\n            edge_list.append(edge_tmp)\n        data.edge_index = torch.cat(edge_list, 1)\n        \n        A = []                     \n        for i,edge in enumerate(edges):\n            edge_tmp = torch.from_numpy(np.vstack((edge.nonzero()[0], edge.nonzero()[1]))).type(torch.LongTensor)\n            value_tmp = torch.ones(edge_tmp.shape[1]).type(torch.FloatTensor)\n            A.append((edge_tmp,value_tmp))\n        edge_tmp = torch.stack((torch.arange(0,num_nodes),torch.arange(0,num_nodes))).type(torch.LongTensor)\n        value_tmp = torch.ones(num_nodes).type(torch.FloatTensor)\n        A.append((edge_tmp,value_tmp))\n        data.adj = A\n\n        data.train_node = torch.from_numpy(np.array(labels[0])[:,0]).type(torch.LongTensor)\n        data.train_target = torch.from_numpy(np.array(labels[0])[:,1]).type(torch.LongTensor)\n        data.valid_node = torch.from_numpy(np.array(labels[1])[:,0]).type(torch.LongTensor)\n        data.valid_target = torch.from_numpy(np.array(labels[1])[:,1]).type(torch.LongTensor)\n        data.test_node = torch.from_numpy(np.array(labels[2])[:,0]).type(torch.LongTensor)\n        data.test_target = torch.from_numpy(np.array(labels[2])[:,1]).type(torch.LongTensor)\n        \n        y = np.zeros((num_nodes), dtype=int)\n        x_index = torch.cat((data.train_node, data.valid_node, data.test_node))\n        y_index = torch.cat((data.train_target, data.valid_target, data.test_target))\n        y[x_index.numpy()] = y_index.numpy()\n        data.y = torch.from_numpy(y)\n        self.data = data\n\n    def get(self, idx):\n        assert idx == 0\n        return self.data\n    \n    def apply_to_device(self, device):\n        self.data.x = self.data.x.to(device)\n\n        self.data.train_node = self.data.train_node.to(device)\n        self.data.valid_node = self.data.valid_node.to(device)\n        self.data.test_node = self.data.test_node.to(device)\n\n        self.data.train_target = self.data.train_target.to(device)\n        self.data.valid_target = self.data.valid_target.to(device)\n        self.data.test_target = self.data.test_target.to(device)\n\n        new_adj = []\n        for (t1, t2) in self.data.adj:\n            new_adj.append((t1.to(device), t2.to(device)))\n        self.data.adj = new_adj\n\n    def download(self):\n        download_url(self.url, self.raw_dir, name=self.name + \'.zip\')\n        untar(self.raw_dir, self.name + \'.zip\')\n\n    def process(self):\n        self.read_gtn_data(self.raw_dir)\n        torch.save(self.data, self.processed_paths[0])\n\n    def __repr__(self):\n        return ""{}()"".format(self.name)\n\n\n@register_dataset(""gtn-acm"")\nclass ACM_GTNDataset(GTNDataset):\n    def __init__(self):\n        dataset = ""gtn-acm""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        super(ACM_GTNDataset, self).__init__(path, dataset)\n\n\n@register_dataset(""gtn-dblp"")\nclass DBLP_GTNDataset(GTNDataset):\n    def __init__(self):\n        dataset = ""gtn-dblp""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        super(DBLP_GTNDataset, self).__init__(path, dataset)\n\n\n@register_dataset(""gtn-imdb"")\nclass IMDB_GTNDataset(GTNDataset):\n    def __init__(self):\n        dataset = ""gtn-imdb""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        super(IMDB_GTNDataset, self).__init__(path, dataset)\n'"
cogdl/datasets/han_data.py,14,"b'import sys\nimport time\nimport os\nimport os.path as osp\nimport requests\nimport shutil\nimport tqdm\nimport pickle\nimport numpy as np\nimport scipy.io as sio\nimport scipy.sparse as sp\n\nimport torch\n\nfrom cogdl.data import Data, Dataset, download_url\n\nfrom . import register_dataset\n\n\ndef untar(path, fname, deleteTar=True):\n    """"""\n    Unpacks the given archive file to the same directory, then (by default)\n    deletes the archive file.\n    """"""\n    print(\'unpacking \' + fname)\n    fullpath = os.path.join(path, fname)\n    shutil.unpack_archive(fullpath, path)\n    if deleteTar:\n        os.remove(fullpath)\n\ndef sample_mask(idx, l):\n    """"""Create mask.""""""\n    mask = np.zeros(l)\n    mask[idx] = 1\n    return np.array(mask, dtype=np.bool)\n\nclass HANDataset(Dataset):\n    r""""""The network datasets ""ACM"", ""DBLP"" and ""IMDB"" from the\n    `""Heterogeneous Graph Attention Network""\n    <https://arxiv.org/abs/1903.07293>`_ paper.\n    \n    Args:\n        root (string): Root directory where the dataset should be saved.\n        name (string): The name of the dataset (:obj:`""han-acm""`,\n            :obj:`""han-dblp""`, :obj:`""han-imdb""`).\n    """"""\n\n    def __init__(self, root, name):\n        self.name = name\n        self.url = f\'https://github.com/cenyk1230/han-data/blob/master/{name}.zip?raw=true\'\n        super(HANDataset, self).__init__(root)\n        self.data = torch.load(self.processed_paths[0])\n        self.num_classes = torch.max(self.data.train_target).item() + 1\n        self.num_edge = len(self.data.adj)\n        self.num_nodes = self.data.x.shape[0]\n\n    @property\n    def raw_file_names(self):\n        names = [""data.mat""]\n        return names\n\n    @property\n    def processed_file_names(self):\n        return [""data.pt""]\n\n    def read_gtn_data(self, folder):\n        data = sio.loadmat(osp.join(folder, \'data.mat\'))\n        if self.name == \'han-acm\' or self.name == \'han-imdb\':\n            truelabels, truefeatures = data[\'label\'], data[\'feature\'].astype(float)\n        elif self.name == \'han-dblp\':\n            truelabels, truefeatures = data[\'label\'], data[\'features\'].astype(float)\n        num_nodes = truefeatures.shape[0]\n        if self.name == \'han-acm\':\n            rownetworks = [data[\'PAP\'] - np.eye(num_nodes), data[\'PLP\'] - np.eye(num_nodes)]\n        elif self.name == \'han-dblp\':\n            rownetworks = [data[\'net_APA\'] - np.eye(num_nodes), data[\'net_APCPA\'] - np.eye(num_nodes), data[\'net_APTPA\'] - np.eye(num_nodes)]\n        elif self.name == \'han-imdb\':\n            rownetworks = [data[\'MAM\'] - np.eye(num_nodes), data[\'MDM\'] - np.eye(num_nodes), data[\'MYM\'] - np.eye(num_nodes)]\n\n        y = truelabels\n        train_idx = data[\'train_idx\']\n        val_idx = data[\'val_idx\']\n        test_idx = data[\'test_idx\']\n\n        train_mask = sample_mask(train_idx, y.shape[0])\n        val_mask = sample_mask(val_idx, y.shape[0])\n        test_mask = sample_mask(test_idx, y.shape[0])\n\n        y_train = np.argmax(y[train_mask, :], axis=1)\n        y_val = np.argmax(y[val_mask, :], axis=1)\n        y_test = np.argmax(y[test_mask, :], axis=1)\n\n        data = Data()\n        A = []                     \n        for i, edge in enumerate(rownetworks):\n            edge_tmp = torch.from_numpy(np.vstack((edge.nonzero()[0], edge.nonzero()[1]))).type(torch.LongTensor)\n            value_tmp = torch.ones(edge_tmp.shape[1]).type(torch.FloatTensor)\n            A.append((edge_tmp, value_tmp))\n        edge_tmp = torch.stack((torch.arange(0,num_nodes), torch.arange(0,num_nodes))).type(torch.LongTensor)\n        value_tmp = torch.ones(num_nodes).type(torch.FloatTensor)\n        A.append((edge_tmp, value_tmp))\n        data.adj = A\n\n        data.x = torch.from_numpy(truefeatures).type(torch.FloatTensor)\n\n        data.train_node = torch.from_numpy(train_idx[0]).type(torch.LongTensor)\n        data.train_target = torch.from_numpy(y_train).type(torch.LongTensor)\n        data.valid_node = torch.from_numpy(val_idx[0]).type(torch.LongTensor)\n        data.valid_target = torch.from_numpy(y_val).type(torch.LongTensor)\n        data.test_node = torch.from_numpy(test_idx[0]).type(torch.LongTensor)\n        data.test_target = torch.from_numpy(y_test).type(torch.LongTensor)\n\n        self.data = data\n\n    def get(self, idx):\n        assert idx == 0\n        return self.data\n    \n    def apply_to_device(self, device):\n        self.data.x = self.data.x.to(device)\n\n        self.data.train_node = self.data.train_node.to(device)\n        self.data.valid_node = self.data.valid_node.to(device)\n        self.data.test_node = self.data.test_node.to(device)\n\n        self.data.train_target = self.data.train_target.to(device)\n        self.data.valid_target = self.data.valid_target.to(device)\n        self.data.test_target = self.data.test_target.to(device)\n\n        new_adj = []\n        for (t1, t2) in self.data.adj:\n            new_adj.append((t1.to(device), t2.to(device)))\n        self.data.adj = new_adj\n\n    def download(self):\n        download_url(self.url, self.raw_dir, name=self.name + \'.zip\')\n        untar(self.raw_dir, self.name + \'.zip\')\n\n    def process(self):\n        self.read_gtn_data(self.raw_dir)\n        torch.save(self.data, self.processed_paths[0])\n\n    def __repr__(self):\n        return ""{}()"".format(self.name)\n\n\n@register_dataset(""han-acm"")\nclass ACM_HANDataset(HANDataset):\n    def __init__(self):\n        dataset = ""han-acm""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        super(ACM_HANDataset, self).__init__(path, dataset)\n\n\n@register_dataset(""han-dblp"")\nclass DBLP_HANDataset(HANDataset):\n    def __init__(self):\n        dataset = ""han-dblp""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        super(DBLP_HANDataset, self).__init__(path, dataset)\n\n\n@register_dataset(""han-imdb"")\nclass IMDB_HANDataset(HANDataset):\n    def __init__(self):\n        dataset = ""han-imdb""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        super(IMDB_HANDataset, self).__init__(path, dataset)\n'"
cogdl/datasets/matlab_matrix.py,5,"b'import json\nimport os\nimport os.path as osp\nfrom itertools import product\n\nimport numpy as np\nimport scipy.io\nimport torch\n\nfrom cogdl.data import Data, Dataset, download_url\n\nfrom . import register_dataset\n\n\nclass MatlabMatrix(Dataset):\n    r""""""networks from the http://leitang.net/code/social-dimension/data/ or http://snap.stanford.edu/node2vec/\n\n    Args:\n        root (string): Root directory where the dataset should be saved.\n        name (string): The name of the dataset (:obj:`""Blogcatalog""`).\n    """"""\n\n    def __init__(self, root, name, url):\n        self.name = name\n        self.url = url\n        super(MatlabMatrix, self).__init__(root)\n        self.data = torch.load(self.processed_paths[0])\n\n    @property\n    def raw_file_names(self):\n        splits = [self.name]\n        files = [""mat""]\n        return [""{}.{}"".format(s, f) for s, f in product(splits, files)]\n\n    @property\n    def processed_file_names(self):\n        return [""data.pt""]\n\n    def download(self):\n        for name in self.raw_file_names:\n            download_url(""{}{}"".format(self.url, name), self.raw_dir)\n\n    def get(self, idx):\n        assert idx == 0\n        return self.data\n\n    def process(self):\n        path = osp.join(self.raw_dir, ""{}.mat"".format(self.name))\n        smat = scipy.io.loadmat(path)\n        adj_matrix, group = smat[""network""], smat[""group""]\n\n        y = torch.from_numpy(group.todense()).to(torch.float)\n\n        row_ind, col_ind = adj_matrix.nonzero()\n        edge_index = torch.stack([torch.tensor(row_ind), torch.tensor(col_ind)], dim=0)\n        edge_attr = torch.tensor(adj_matrix[row_ind, col_ind])\n\n        data = Data(edge_index=edge_index, edge_attr=edge_attr, x=None, y=y)\n\n        torch.save(data, self.processed_paths[0])\n\n\n@register_dataset(""blogcatalog"")\nclass BlogcatalogDataset(MatlabMatrix):\n    def __init__(self):\n        dataset, filename = ""blogcatalog"", ""blogcatalog""\n        url = ""http://leitang.net/code/social-dimension/data/""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        super(BlogcatalogDataset, self).__init__(path, filename, url)\n\n\n@register_dataset(""flickr"")\nclass FlickrDataset(MatlabMatrix):\n    def __init__(self):\n        dataset, filename = ""flickr"", ""flickr""\n        url = ""http://leitang.net/code/social-dimension/data/""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        super(FlickrDataset, self).__init__(path, filename, url)\n\n\n@register_dataset(""wikipedia"")\nclass WikipediaDataset(MatlabMatrix):\n    def __init__(self):\n        dataset, filename = ""wikipedia"", ""POS""\n        url = ""http://snap.stanford.edu/node2vec/""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        super(WikipediaDataset, self).__init__(path, filename, url)\n\n\n@register_dataset(""ppi"")\nclass PPIDataset(MatlabMatrix):\n    def __init__(self):\n        dataset, filename = ""ppi"", ""Homo_sapiens""\n        url = ""http://snap.stanford.edu/node2vec/""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        super(PPIDataset, self).__init__(path, filename, url)\n'"
cogdl/datasets/modelnet.py,0,"b'import os\nimport os.path as osp\nimport shutil\nimport glob\nimport numpy as np\n\nimport torch_geometric.transforms as T\nfrom torch_geometric.datasets import ModelNet\nfrom cogdl.data import Data, Dataset, download_url, InMemoryDataset\nfrom . import register_dataset\n\n\nclass ModelNet10(ModelNet):\n    def __init__(self, train):\n        dataset = ""ModelNet10""\n        pre_transform, transform = T.NormalizeScale(), T.SamplePoints(1024)\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        if not osp.exists(path):\n            ModelNet(path, ""10"", transform, pre_transform)\n        super(ModelNet10, self).__init__(path, name=""10"", train=train, transform=transform, pre_transform=pre_transform)\n\n\nclass ModelNet40(ModelNet):\n    def __init__(self, train):\n        dataset = ""ModelNet40""\n        pre_transform, transform = T.NormalizeScale(), T.SamplePoints(1024)\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        if not osp.exists(path):\n            ModelNet(path, ""40"", transform, pre_transform)\n        super(ModelNet40, self).__init__(path, name=""40"", train=train, transform=transform, pre_transform=pre_transform)\n\n\n@register_dataset(""ModelNet10"")\nclass ModelNetData10(ModelNet):\n    def __init__(self):\n        dataset = ""ModelNet10""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        self.train_data = ModelNet10(True)\n        self.test_data = ModelNet10(False)\n        self.num_graphs = len(self.train_data) + len(self.test_data)\n\n        super(ModelNetData10, self).__init__(path, name=""10"")\n\n    def get_all(self):\n        return self.train_data, self.test_data\n\n    def __getitem__(self, item):\n        if item < len(self.train_data):\n            return self.train_data[item]\n        return self.test_data[item]\n\n    def __len__(self):\n        return len(self.train_data) + len(self.test_data)\n\n    @property\n    def train_index(self):\n        return 0, len(self.train_data)\n\n    @property\n    def test_index(self):\n        return len(self.train_data), self.num_graphs\n\n\n@register_dataset(""ModelNet40"")\nclass ModelNetData40(ModelNet):\n    def __init__(self):\n        dataset = ""ModelNet40""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        self.train_data = ModelNet40(True)\n        self.test_data = ModelNet40(False)\n        self.num_graphs = len(self.train_data) + len(self.test_data)\n\n        super(ModelNetData40, self).__init__(path, name=""40"")\n\n    def get_all(self):\n        return self.train_data, self.test_data\n\n    def __getitem__(self, item):\n        if item < len(self.train_data):\n            return self.train_data[item]\n        return self.test_data[item]\n\n    def __len__(self):\n        return len(self.train_data) + len(self.test_data)\n\n    @property\n    def train_index(self):\n        return 0, len(self.train_data)\n\n    @property\n    def test_index(self):\n        return len(self.train_data), self.num_graphs\n'"
cogdl/datasets/pyg.py,3,"b'import os.path as osp\n\nimport torch\n\nimport torch_geometric.transforms as T\nfrom torch_geometric.datasets import Planetoid, Reddit, TUDataset, QM9, ModelNet\nfrom torch_geometric.utils import remove_self_loops\nfrom . import register_dataset\n\n\n@register_dataset(""cora"")\nclass CoraDataset(Planetoid):\n    def __init__(self):\n        dataset = ""Cora""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        if not osp.exists(path):\n            Planetoid(path, dataset, transform=T.TargetIndegree())\n        super(CoraDataset, self).__init__(path, dataset, transform=T.TargetIndegree())\n\n\n\n@register_dataset(""citeseer"")\nclass CiteSeerDataset(Planetoid):\n    def __init__(self):\n        dataset = ""CiteSeer""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        if not osp.exists(path):\n            Planetoid(path, dataset, transform=T.TargetIndegree())\n        super(CiteSeerDataset, self).__init__(path, dataset, transform=T.TargetIndegree())\n\n\n@register_dataset(""pubmed"")\nclass PubMedDataset(Planetoid):\n    def __init__(self):\n        dataset = ""PubMed""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        if not osp.exists(path):\n            Planetoid(path, dataset, transform=T.TargetIndegree())\n        super(PubMedDataset, self).__init__(path, dataset, transform=T.TargetIndegree())\n\n\n@register_dataset(""reddit"")\nclass RedditDataset(Reddit):\n    def __init__(self):\n        dataset = ""Reddit""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        if not osp.exists(path):\n            Reddit(path)\n        super(RedditDataset, self).__init__(path, transform=T.TargetIndegree())\n\n\n@register_dataset(""mutag"")\nclass MUTAGDataset(TUDataset):\n    def __init__(self):\n        dataset = ""MUTAG""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        if not osp.exists(path):\n            TUDataset(path, name=dataset)\n        super(MUTAGDataset, self).__init__(path, name=dataset)\n\n\n@register_dataset(""imdb-b"")\nclass ImdbBinaryDataset(TUDataset):\n    def __init__(self):\n        dataset = ""IMDB-BINARY""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        if not osp.exists(path):\n            TUDataset(path, name=dataset)\n        super(ImdbBinaryDataset, self).__init__(path, name=dataset)\n\n\n@register_dataset(""imdb-m"")\nclass ImdbMultiDataset(TUDataset):\n    def __init__(self):\n        dataset = ""IMDB-MULTI""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        if not osp.exists(path):\n            TUDataset(path, name=dataset)\n        super(ImdbMultiDataset, self).__init__(path, name=dataset)\n\n\n@register_dataset(""collab"")\nclass CollabDataset(TUDataset):\n    def __init__(self):\n        dataset = ""COLLAB""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        if not osp.exists(path):\n            TUDataset(path, name=dataset)\n        super(CollabDataset, self).__init__(path, name=dataset)\n\n\n@register_dataset(""proteins"")\nclass ProtainsDataset(TUDataset):\n    def __init__(self):\n        dataset = ""PROTEINS""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        if not osp.exists(path):\n            TUDataset(path, name=dataset)\n        super(ProtainsDataset, self).__init__(path, name=dataset)\n\n\n@register_dataset(""reddit-b"")\nclass RedditBinary(TUDataset):\n    def __init__(self):\n        dataset = ""REDDIT-BINARY""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        if not osp.exists(path):\n            TUDataset(path, name=dataset)\n        super(RedditBinary, self).__init__(path, name=dataset)\n\n\n@register_dataset(""reddit-multi-5k"")\nclass RedditMulti5K(TUDataset):\n    def __init__(self):\n        dataset = ""REDDIT-MULTI-5K""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        if not osp.exists(path):\n            TUDataset(path, name=dataset)\n        super(RedditMulti5K, self).__init__(path, name=dataset)\n\n\n\n@register_dataset(""reddit-multi-12k"")\nclass RedditMulti12K(TUDataset):\n    def __init__(self):\n        dataset = ""REDDIT-MULTI-12K""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        if not osp.exists(path):\n            TUDataset(path, name=dataset)\n        super(RedditMulti12K, self).__init__(path, name=dataset)\n\n\n@register_dataset(""ptc-mr"")\nclass PTCMRDataset(TUDataset):\n    def __init__(self):\n        dataset = ""PTC_MR""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        if not osp.exists(path):\n            TUDataset(path, name=dataset)\n        super(PTCMRDataset, self).__init__(path, name=dataset)\n\n\n@register_dataset(""nci1"")\nclass NCT1Dataset(TUDataset):\n    def __init__(self):\n        dataset = ""NCI1""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        if not osp.exists(path):\n            TUDataset(path, name=dataset)\n        super(NCT1Dataset, self).__init__(path, name=dataset)\n\n\n@register_dataset(""nci109"")\nclass NCT109Dataset(TUDataset):\n    def __init__(self):\n        dataset = ""NCI109""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        if not osp.exists(path):\n            TUDataset(path, name=dataset)\n        super(NCT109Dataset, self).__init__(path, name=dataset)\n\n\n@register_dataset(""enzymes"")\nclass ENZYMES(TUDataset):\n    def __init__(self):\n        dataset = ""ENZYMES""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n        if not osp.exists(path):\n            TUDataset(path, name=dataset)\n        super(ENZYMES, self).__init__(path, name=dataset)\n\n    def __getitem__(self, idx):\n        if isinstance(idx, int):\n            data = self.get(self.indices()[idx])\n            data = data if self.transform is None else self.transform(data)\n            edge_nodes = data.edge_index.max() + 1\n            if edge_nodes < data.x.size(0):\n                data.x = data.x[:edge_nodes]\n            return data\n        else:\n            return self.index_select(idx)\n\n\n@register_dataset(""qm9"")\nclass QM9Dataset(QM9):\n    def __init__(self):\n        dataset = ""QM9""\n        path = osp.join(osp.dirname(osp.realpath(__file__)), ""../.."", ""data"", dataset)\n\n        target=0\n        class MyTransform(object):\n            def __call__(self, data):\n                # Specify target.\n                data.y = data.y[:, target]\n                return data\n\n        class Complete(object):\n            def __call__(self, data):\n                device = data.edge_index.device\n                row = torch.arange(data.num_nodes, dtype=torch.long, device=device)\n                col = torch.arange(data.num_nodes, dtype=torch.long, device=device)\n                row = row.view(-1, 1).repeat(1, data.num_nodes).view(-1)\n                col = col.repeat(data.num_nodes)\n                edge_index = torch.stack([row, col], dim=0)\n                edge_attr = None\n                if data.edge_attr is not None:\n                    idx = data.edge_index[0] * data.num_nodes + data.edge_index[1]\n                    size = list(data.edge_attr.size())\n                    size[0] = data.num_nodes * data.num_nodes\n                    edge_attr = data.edge_attr.new_zeros(size)\n                    edge_attr[idx] = data.edge_attr\n                edge_index, edge_attr = remove_self_loops(edge_index, edge_attr)\n                data.edge_attr = edge_attr\n                data.edge_index = edge_index\n                return data\n\n        transform = T.Compose([MyTransform(), Complete(), T.Distance(norm=False)])\n        if not osp.exists(path):\n            QM9(path)\n        super(QM9Dataset, self).__init__(path)'"
cogdl/layers/__init__.py,0,"b'from .maggregator import MeanAggregator\nfrom .se_layer import SELayer\nfrom .mixhop_layer import MixHopLayer\n\n__all__ = [""SELayer"", ""MeanAggregator"", ""MixHopLayer""]\n'"
cogdl/layers/maggregator.py,4,"b'import torch\nimport torch.nn as nn\n\n\nclass MeanAggregator(torch.nn.Module):\n    def __init__(\n        self, in_channels, out_channels, improved=False, cached=False, bias=True\n    ):\n        super(MeanAggregator, self).__init__()\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.improved = improved\n        self.cached = cached\n        self.cached_result = None\n\n        self.linear = nn.Linear(in_channels, out_channels, bias)\n\n    @staticmethod\n    def norm(x, edge_index):\n        # here edge_index is already a sparse tensor\n        deg = torch.sparse.sum(edge_index, 1)\n        deg_inv = deg.pow(-1).to_dense()\n\n        x = torch.matmul(edge_index, x)\n        #  print(x,deg_inv)\n        x = x.t() * deg_inv\n        #  x\xef\xbc\x9a512*dim, edge_weight\xef\xbc\x9a256*512\n\n        return x.t()\n\n    def forward(self, x, edge_index, edge_weight=None, bias=True):\n        """"""""""""\n        x = self.linear(x)\n        x = self.norm(x, edge_index)\n        return x\n\n    def update(self, aggr_out):\n        if self.bias is not None:\n            aggr_out = aggr_out + self.bias\n        return aggr_out\n\n    def __repr__(self):\n        return ""{}({}, {})"".format(\n            self.__class__.__name__, self.in_channels, self.out_channels\n        )\n'"
cogdl/layers/mixhop_layer.py,10,"b'import math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.parameter import Parameter\n\nclass MixHopLayer(nn.Module):\n\n    def __init__(self, num_features, adj_pows, dim_per_pow):\n        super(MixHopLayer, self).__init__()\n        self.num_features = num_features\n        self.adj_pows = adj_pows\n        self.dim_per_pow = dim_per_pow\n        self.total_dim = 0\n        self.linears = torch.nn.ModuleList()\n        for dim in dim_per_pow:\n            self.linears.append(nn.Linear(num_features, dim))\n            self.total_dim += dim\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        for linear in self.linears:\n            linear.reset_parameters()\n\n    def adj_pow_x(self, x, adj, p):\n        for _ in range(p):\n            x = torch.spmm(adj, x)\n        return x\n\n    def forward(self, x, edge_index):\n        adj = torch.sparse_coo_tensor(\n            edge_index,\n            torch.ones(edge_index.shape[1]).float(),\n            (x.shape[0], x.shape[0]),\n            device=x.device\n        )\n        output_list = []\n        for p, linear in zip(self.adj_pows, self.linears):\n            output = linear(self.adj_pow_x(x, adj, p))\n            output_list.append(output)\n        \n        return torch.cat(output_list, dim=1)\n\nif __name__ == ""__main__"":\n    layer = MixHopLayer(10, [1, 3], [16, 32])\n    x = torch.ones(5, 10)\n    adj = torch.LongTensor([[0, 1, 1, 2, 2, 3, 4], [1, 2, 3, 0, 4, 4, 1]])\n    output = layer(x, adj)\n    print(output.shape)\n'"
cogdl/layers/se_layer.py,2,"b'import torch\nimport torch.nn as nn\n\n\nclass SELayer(nn.Module):\n    """"""Squeeze-and-excitation networks""""""\n\n    def __init__(self, in_channels, se_channels):\n        super(SELayer, self).__init__()\n\n        self.in_channels = in_channels\n        self.se_channels = se_channels\n\n        self.encoder_decoder = nn.Sequential(\n            nn.Linear(in_channels, se_channels),\n            nn.ELU(),\n            nn.Linear(se_channels, in_channels),\n            nn.Sigmoid(),\n        )\n\n        # self.reset_parameters()\n\n    def forward(self, x):\n        """"""""""""\n        # Aggregate input representation\n        x_global = torch.mean(x, dim=0)\n        # Compute reweighting vector s\n        s = self.encoder_decoder(x_global)\n\n        return x * s\n'"
cogdl/models/__init__.py,1,"b'import argparse\nimport importlib\nimport os\n\nimport numpy as np\nimport torch.nn as nn\n\nfrom .base_model import BaseModel\n\ntry:\n    import torch_geometric\nexcept ImportError:\n    pyg = False\n    print(""Failed to import PyTorch Geometric"")\nelse:\n    pyg = True\n\nMODEL_REGISTRY = {}\n\n\ndef register_model(name):\n    """"""\n    New model types can be added to cogdl with the :func:`register_model`\n    function decorator.\n\n    For example::\n\n        @register_model(\'gat\')\n        class GAT(BaseModel):\n            (...)\n\n    Args:\n        name (str): the name of the model\n    """"""\n\n    def register_model_cls(cls):\n        if name in MODEL_REGISTRY:\n            raise ValueError(""Cannot register duplicate model ({})"".format(name))\n        if not issubclass(cls, BaseModel):\n            raise ValueError(\n                ""Model ({}: {}) must extend BaseModel"".format(name, cls.__name__)\n            )\n        MODEL_REGISTRY[name] = cls\n        return cls\n\n    return register_model_cls\n\n\ndef alias_setup(probs):\n    """"""\n    Compute utility lists for non-uniform sampling from discrete distributions.\n    Refer to https://hips.seas.harvard.edu/blog/2013/03/03/the-alias-method-efficient-sampling-with-many-discrete-outcomes/\n    for details\n    """"""\n    K = len(probs)\n    q = np.zeros(K)\n    J = np.zeros(K, dtype=np.int)\n\n    smaller = []\n    larger = []\n    for kk, prob in enumerate(probs):\n        q[kk] = K * prob\n        if q[kk] < 1.0:\n            smaller.append(kk)\n        else:\n            larger.append(kk)\n\n    while len(smaller) > 0 and len(larger) > 0:\n        small = smaller.pop()\n        large = larger.pop()\n\n        J[small] = large\n        q[large] = q[large] + q[small] - 1.0\n        if q[large] < 1.0:\n            smaller.append(large)\n        else:\n            larger.append(large)\n\n    return J, q\n\n\ndef alias_draw(J, q):\n    """"""\n    Draw sample from a non-uniform discrete distribution using alias sampling.\n    """"""\n    K = len(J)\n\n    kk = int(np.floor(np.random.rand() * K))\n    if np.random.rand() < q[kk]:\n        return kk\n    else:\n        return J[kk]\n\n\n# automatically import any Python files in the models/ directory\nfor root, dirs, files in os.walk(os.path.dirname(__file__)):\n    for file in files:\n        if file.endswith("".py"") and not file.startswith(""_""):\n            model_name = file[: file.find("".py"")]\n            if not pyg and model_name.startswith(""pyg""):\n                continue\n            model_name = os.path.join(root, model_name)\n            model_name = model_name[model_name.find(""models"") :].replace(os.sep, ""."")\n            module = importlib.import_module(""cogdl."" + model_name)\n\n\ndef build_model(args):\n    return MODEL_REGISTRY[args.model].build_model_from_args(args)\n'"
cogdl/models/base_model.py,1,"b'import torch\nimport torch.nn as nn\n\n\nclass BaseModel(nn.Module):\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        pass\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        """"""Build a new model instance.""""""\n        raise NotImplementedError(\n            ""Models must implement the build_model_from_args method""\n        )\n'"
cogdl/tasks/__init__.py,1,"b'import argparse\nimport importlib\nimport os\n\nimport torch.nn as nn\n\nfrom .base_task import BaseTask\n\nTASK_REGISTRY = {}\n\n\ndef register_task(name):\n    """"""\n    New task types can be added to cogdl with the :func:`register_task`\n    function decorator.\n\n    For example::\n\n        @register_task(\'node_classification\')\n        class NodeClassification(BaseTask):\n            (...)\n\n    Args:\n        name (str): the name of the task\n    """"""\n\n    def register_task_cls(cls):\n        if name in TASK_REGISTRY:\n            raise ValueError(""Cannot register duplicate task ({})"".format(name))\n        if not issubclass(cls, BaseTask):\n            raise ValueError(\n                ""Task ({}: {}) must extend BaseTask"".format(name, cls.__name__)\n            )\n        TASK_REGISTRY[name] = cls\n        return cls\n\n    return register_task_cls\n\n\n# automatically import any Python files in the tasks/ directory\nfor file in os.listdir(os.path.dirname(__file__)):\n    if file.endswith("".py"") and not file.startswith(""_""):\n        task_name = file[: file.find("".py"")]\n        module = importlib.import_module(""cogdl.tasks."" + task_name)\n\n\ndef build_task(args):\n    return TASK_REGISTRY[args.task](args)\n'"
cogdl/tasks/base_task.py,1,"b'import torch\nimport torch.nn as nn\n\n\nclass BaseTask(object):\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        pass\n\n    # @classmethod\n    # def build_task_from_args(cls, args):\n    #     """"""Build a new task instance.""""""\n    #     raise NotImplementedError(\n    #         ""Tasks must implement the build_task_from_args method""\n    #     )\n\n    def __init__(self, args):\n        pass\n\n    def train(self, num_epoch):\n        raise NotImplementedError\n'"
cogdl/tasks/community_detection.py,1,"b'import copy\nimport random\nimport warnings\nfrom collections import defaultdict\n\nimport networkx as nx\nfrom networkx.algorithms import community\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, normalized_mutual_info_score\n\nfrom cogdl import options\nfrom cogdl.datasets import build_dataset\nfrom cogdl.data import Dataset, InMemoryDataset\nfrom cogdl.models import build_model\n\nfrom . import BaseTask, register_task\n\n\n@register_task(""community_detection"")\nclass CommunityDetection(BaseTask):\n    """"""Node classification task.""""""\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(""--hidden-size"", type=int, default=128)\n        parser.add_argument(""--num-shuffle"", type=int, default=5)\n        # fmt: on\n\n    def __init__(self, args):\n        super(CommunityDetection, self).__init__(args)\n        dataset = build_dataset(args)\n        self.data = dataset[0]\n        if issubclass(dataset.__class__.__bases__[0], InMemoryDataset):\n            self.num_nodes = self.data.y.shape[0]\n            self.num_classes = dataset.num_classes\n        else:\n            self.num_nodes, self.num_classes = self.data.y.shape\n\n        self.label = np.argmax(self.data.y, axis=1)\n        self.model = build_model(args)\n        self.hidden_size = args.hidden_size\n        self.num_shuffle = args.num_shuffle\n        self.is_weighted = self.data.edge_attr is not None\n\n    def train(self):\n        G = nx.Graph()\n        if self.is_weighted:\n            edges, weight = (\n                self.data.edge_index.t().tolist(),\n                self.data.edge_attr.tolist(),\n            )\n            G.add_weighted_edges_from(\n                [(edges[i][0], edges[i][1], weight[0][i]) for i in range(len(edges))]\n            )\n        else:\n            G.add_edges_from(self.data.edge_index.t().tolist())\n        partition = community.greedy_modularity_communities(G)\n        base_label = [0] * G.number_of_nodes()\n        for i, node_set in enumerate(partition):\n            for node in node_set:\n                base_label[node] = i\n        nmi_score = normalized_mutual_info_score(self.label, base_label)\n        print(""NMI score of greedy modularity optimize algorithm: "", nmi_score)\n        \n        embeddings = self.model.train(G)\n\n        # Map node2id\n        features_matrix = np.zeros((self.num_nodes, self.hidden_size))\n        for vid, node in enumerate(G.nodes()):\n            features_matrix[node] = embeddings[vid]\n            \n        return self._evaluate(features_matrix)\n\n    def _evaluate(self, features_matrix):\n        clusters = [30, 40, 50, 60, 70]\n        all_results = defaultdict(list)\n        for num_cluster in clusters:\n            for _ in range(self.num_shuffle):\n                model = KMeans(n_clusters=num_cluster).fit(features_matrix)\n                # sc_score = silhouette_score(features_matrix, model.labels_, metric=\'euclidean\')\n                nmi_score = normalized_mutual_info_score(self.label, model.labels_)\n                all_results[num_cluster].append(nmi_score)\n                \n        return dict(\n            (\n                f""normalized_mutual_info_score {num_cluster}"",\n                sum(all_results[num_cluster]) / len(all_results[num_cluster]),\n            )\n            for num_cluster in sorted(all_results.keys())\n        )'"
cogdl/tasks/graph_classification.py,12,"b'import copy\nimport random\n\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.utils import add_remaining_self_loops\nfrom torch_scatter import scatter_add\nfrom tqdm import tqdm\n\nfrom cogdl.datasets import build_dataset\nfrom cogdl.data import DataLoader, Data\nfrom cogdl.models import build_model\n\nfrom . import BaseTask, register_task\n\n\ndef node_degree_as_feature(data):\n    r""""""\n    Set each node feature as one-hot encoding of degree\n    :param data: a list of class Data\n    :return: a list of class Data\n    """"""\n    max_degree = 0\n    degrees = []\n    for graph in data:\n        edge_index =graph.edge_index\n        edge_weight = torch.ones((edge_index.size(1),), device=edge_index.device)\n        fill_value = 1\n        num_nodes = graph.num_nodes\n        edge_index, edge_weight = add_remaining_self_loops(\n            edge_index, edge_weight, fill_value, num_nodes\n        )\n        row, col = edge_index\n        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes).long()\n        degrees.append(deg.cpu()-1)\n        max_degree = max(torch.max(deg), max_degree)\n    max_degree = int(max_degree)\n    for i in range(len(data)):\n        one_hot = torch.zeros(data[i].num_nodes, max_degree).scatter_(1, degrees[i].unsqueeze(1), 1)\n        data[i].x = one_hot.to(data[i].y.device)\n    return data\n\n\ndef uniform_node_feature(data):\n    r""""""Set each node feature to the same""""""\n    feat_dim = 2\n    init_feat = torch.rand(1, feat_dim)\n    for i in range(len(data)):\n        data[i].x = init_feat.repeat(1, data[i].num_nodes)\n    return data\n\n\n@register_task(""graph_classification"")\nclass GraphClassification(BaseTask):\n    r""""""Superiviced graph classification task.""""""\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(""--degree-feature"", dest=""degree_feature"", action=""store_true"")\n        parser.add_argument(""--gamma"", type=float, default=0.5)\n        parser.add_argument(""--uniform-feature"", action=""store_true"")\n        parser.add_argument(""--lr"", type=float, default=0.001)\n        parser.add_argument(""--kfold"", dest=""kfold"", action=""store_true"")\n        # fmt: on\n\n    def __init__(self, args):\n        super(GraphClassification, self).__init__(args)\n        dataset = build_dataset(args)\n\n        args.max_graph_size = max([ds.num_nodes for ds in dataset])\n        args.num_features = dataset.num_features\n        args.num_classes = dataset.num_classes\n        args.use_unsup = False\n\n        self.args = args\n        self.kfold = args.kfold\n        self.folds = 10\n        self.device = args.device_id[0] if not args.cpu else \'cpu\'\n        self.data = self.generate_data(dataset, args)\n\n        model = build_model(args)\n        self.model = model.to(self.device)\n        self.patience = args.patience\n        self.max_epoch = args.max_epoch\n\n        self.train_loader, self.val_loader, self.test_loader = self.model.split_dataset(self.data, args)\n\n        self.optimizer = torch.optim.Adam(\n            self.model.parameters(), lr=args.lr, weight_decay=args.weight_decay\n        )\n        self.scheduler = torch.optim.lr_scheduler.StepLR(\n            optimizer=self.optimizer,\n            step_size=50,\n            gamma=0.5\n        )\n\n    def train(self):\n        if self.kfold:\n            return self._kfold_train()\n        else:\n            return self._train()\n\n    def _train(self):\n        epoch_iter = tqdm(range(self.max_epoch))\n        patience = 0\n        best_score = 0\n        best_model = None\n        best_loss = np.inf\n        max_score = 0\n        min_loss = np.inf\n\n        for epoch in epoch_iter:\n            self.scheduler.step()\n            self._train_step()\n            train_acc, train_loss = self._test_step(split=""train"")\n            val_acc, val_loss = self._test_step(split=""valid"")\n            epoch_iter.set_description(\n                f""Epoch: {epoch:03d}, Train: {train_acc:.4f}, Val: {val_acc:.4f}, TrainLoss:{train_loss: .4f}, ValLoss: {val_loss: .4f}""\n            )\n            if val_loss < min_loss or val_acc > max_score:\n                if val_loss <= best_loss:  # and val_acc >= best_score:\n                    best_loss = val_loss\n                    best_score = val_acc\n                    best_model = copy.deepcopy(self.model)\n                min_loss = np.min((min_loss, val_loss))\n                max_score = np.max((max_score, val_acc))\n                patience = 0\n            else:\n                patience += 1\n                if patience == self.patience:\n                    self.model = best_model\n                    epoch_iter.close()\n                    break\n        test_acc, _ = self._test_step(split=""test"")\n        print(f""Test accuracy = {test_acc}"")\n        return dict(Acc=test_acc)\n\n    def _train_step(self):\n        self.model.train()\n        loss_n = 0\n        for batch in self.train_loader:\n            batch = batch.to(self.device)\n            self.optimizer.zero_grad()\n            output, loss = self.model(batch)\n            loss_n += loss.item()\n            loss.backward()\n            self.optimizer.step()\n\n    def _test_step(self, split=""val""):\n        self.model.eval()\n        if split == ""train"":\n            loader = self.train_loader\n        elif split == ""test"":\n            loader = self.test_loader\n        elif split == ""valid"":\n            loader = self.val_loader\n        else:\n            raise ValueError\n        loss_n = []\n        pred = []\n        y = []\n        with torch.no_grad():\n            for batch in loader:\n                batch = batch.to(self.device)\n                predict, loss = self.model(batch)\n                loss_n.append(loss.item())\n                y.append(batch.y)\n                pred.extend(predict)\n        y = torch.cat(y).to(self.device)\n\n        pred = torch.stack(pred, dim=0)\n        pred = pred.max(1)[1]\n        acc = pred.eq(y).sum().item() / len(y)\n        return acc, sum(loss_n)/len(loss_n)\n\n    def _kfold_train(self):\n        y = [x.y for x in self.data]\n        kf = StratifiedKFold(n_splits=self.folds, shuffle=True, random_state=self.args.seed)\n        acc = []\n        for train_index, test_index in kf.split(self.data, y=y):\n            model = build_model(self.args)\n            self.model = model.to(self.device)\n\n            droplast = self.args.model == \'diffpool\'\n            self.train_loader = DataLoader([self.data[i] for i in train_index],\n                                           batch_size=self.args.batch_size,\n                                           drop_last=droplast)\n            self.test_loader = DataLoader([self.data[i] for i in test_index],\n                                          batch_size=self.args.batch_size,\n                                          drop_last=droplast)\n            self.val_loader = DataLoader([self.data[i] for i in test_index],\n                                         batch_size=self.args.batch_size,\n                                         drop_last=droplast)\n            self.optimizer = torch.optim.Adam(\n                self.model.parameters(), lr=self.args.lr, weight_decay=self.args.weight_decay\n            )\n            self.scheduler = torch.optim.lr_scheduler.StepLR(\n                optimizer=self.optimizer,\n                step_size=50,\n                gamma=0.5\n            )\n\n            res = self._train()\n            acc.append(res[""Acc""])\n        return dict(Acc=sum(acc)/len(acc))\n\n    def generate_data(self, dataset, args):\n        if ""ModelNet"" in str(type(dataset).__name__):\n            train_set, test_set = dataset.get_all()\n            args.num_features = 3\n            return {""train"": train_set, ""test"": test_set}\n        else:\n            datalist = []\n            if isinstance(dataset[0], Data):\n                return dataset\n            for idata in dataset:\n                data = Data()\n                for key in idata.keys:\n                    data[key] = idata[key]\n                datalist.append(data)\n\n            if args.degree_feature:\n                datalist = node_degree_as_feature(datalist)\n                args.num_features = datalist[0].num_features\n            return datalist\n'"
cogdl/tasks/heterogeneous_node_classification.py,3,"b'import copy\nimport random\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\nfrom cogdl import options\nfrom cogdl.datasets import build_dataset\nfrom cogdl.models import build_model\n\nfrom . import BaseTask, register_task\n\n\n@register_task(""heterogeneous_node_classification"")\nclass HeterogeneousNodeClassification(BaseTask):\n    """"""Heterogeneous Node classification task.""""""\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        # fmt: off\n        # parser.add_argument(""--num-features"", type=int)\n        # fmt: on\n\n    def __init__(self, args):\n        super(HeterogeneousNodeClassification, self).__init__(args)\n\n        self.device = torch.device(\'cpu\' if args.cpu else \'cuda\')\n        dataset = build_dataset(args)\n        if not args.cpu:\n            dataset.apply_to_device(self.device)\n        self.data = dataset.data\n\n        args.num_features = dataset.num_features\n        args.num_classes = dataset.num_classes\n        args.num_edge = dataset.num_edge\n        args.num_nodes = dataset.num_nodes\n        model = build_model(args)\n        self.model = model.to(self.device)\n        self.patience = args.patience\n        self.max_epoch = args.max_epoch\n\n        self.optimizer = torch.optim.Adam(\n            self.model.parameters(), lr=args.lr, weight_decay=args.weight_decay\n        )\n\n    def train(self):\n        epoch_iter = tqdm(range(self.max_epoch))\n        patience = 0\n        best_score = 0\n        best_loss = np.inf\n        max_score = 0\n        min_loss = np.inf\n        for epoch in epoch_iter:\n            self._train_step()\n            train_acc, _ = self._test_step(split=""train"")\n            val_acc, val_loss = self._test_step(split=""val"")\n            epoch_iter.set_description(\n                f""Epoch: {epoch:03d}, Train: {train_acc:.4f}, Val: {val_acc:.4f}""\n            )\n            if val_loss <= min_loss or val_acc >= max_score:\n                if val_acc >= best_score:\n                    best_loss = val_loss\n                    best_score = val_acc\n                    best_model = copy.deepcopy(self.model.state_dict())\n                min_loss = np.min((min_loss, val_loss))\n                max_score = np.max((max_score, val_acc))\n                patience = 0\n            else:\n                patience += 1\n                if patience == self.patience:\n                    self.model.load_state_dict(best_model)\n                    epoch_iter.close()\n                    break\n        test_f1, _ = self._test_step(split=""test"")\n        print(f""Test f1 = {test_f1}"")\n        return dict(f1=test_f1)\n\n    def _train_step(self):\n        self.model.train()\n        self.optimizer.zero_grad()\n        self.model.loss(self.data).backward()\n        self.optimizer.step()\n\n    def _test_step(self, split=""val""):\n        self.model.eval()\n        if split == \'train\':\n            loss, f1 = self.model.evaluate(self.data, self.data.train_node, self.data.train_target)\n        elif split == \'val\':\n            loss, f1 = self.model.evaluate(self.data, self.data.valid_node, self.data.valid_target)\n        else:\n            loss, f1 = self.model.evaluate(self.data, self.data.test_node, self.data.test_target)\n        return f1, loss\n'"
cogdl/tasks/influence_maximization.py,1,"b'import copy\nimport random\nimport warnings\nfrom collections import defaultdict\n\nimport networkx as nx\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nfrom cogdl import options\nfrom cogdl.datasets import build_dataset\nfrom cogdl.data import Dataset, InMemoryDataset\nfrom cogdl.models import build_model\n\nfrom . import BaseTask, register_task\nfrom queue import PriorityQueue as PQueue\n\n\n\n@register_task(""influence_maximization"")\nclass InfluenceMaximization(BaseTask):\n    """"""Node classification task.""""""\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(""--hidden-size"", type=int, default=128)\n        parser.add_argument(""--num-seed"", type=int, default=20)\n        parser.add_argument(""--num-simulation"", type=int, default=1)\n        parser.add_argument(""--decay"", type=float, default=0.5)\n\n    \n        # fmt: on\n\n    def __init__(self, args):\n        super(InfluenceMaximization, self).__init__(args)\n        dataset = build_dataset(args)\n        self.data = dataset[0]\n        if issubclass(dataset.__class__.__bases__[0], InMemoryDataset):\n            self.num_nodes = self.data.y.shape[0]\n            self.num_classes = dataset.num_classes\n        else:\n            self.num_nodes, self.num_classes = self.data.y.shape\n\n        self.label = np.argmax(self.data.y, axis=1)\n        self.model = build_model(args)\n        self.is_weighted = self.data.edge_attr is not None\n        self.hidden_size = args.hidden_size\n        self.num_simulation= args.num_simulation\n        self.num_seed = args.num_seed\n        self.decay = args.decay\n\n\n    def train(self):\n        G = nx.Graph()\n        if self.is_weighted:\n            edges, weight = (\n                self.data.edge_index.t().tolist(),\n                self.data.edge_attr.tolist(),\n            )\n            G.add_weighted_edges_from(\n                [(edges[i][0], edges[i][1], weight[0][i]) for i in range(len(edges))]\n            )\n        else:\n            G.add_edges_from(self.data.edge_index.t().tolist())\n        \n        embeddings = self.model.train(G)\n\n        # Map node2id\n        features_matrix = np.zeros((self.num_nodes, self.hidden_size))\n        for vid, node in enumerate(G.nodes()):\n            features_matrix[node] = embeddings[vid]\n            \n        return self._evaluate(G, features_matrix)\n\n    def _evaluate(self, G, features_matrix):\n        thresholds = [0.7, 0.8]\n        all_results = defaultdict(list)\n        for threshold in thresholds:\n            print(""influence maximization with threshold :"", threshold)\n            influence_score, seed_list = self._influence_maximazation(G, features_matrix, threshold)\n            all_results[threshold].append(influence_score)\n                \n        return dict(\n            (\n                f""influence score {threshold}"",\n                sum(all_results[threshold]) / len(all_results[threshold]),\n            )\n            for threshold in sorted(all_results.keys())\n        )\n        \n    def _influence_maximazation(self, G, features_matrix, threshold):\n        num_node = G.number_of_nodes()\n        Q = PQueue()\n        seed_list = []\n        \n        for node in G.nodes():\n            act_num = self._simulation(G, features_matrix, node, threshold)\n            Q.put((-1* act_num, [node, 1]))\n            # print(""Inside PriorityQueue: "", Q.queue) \n        \n        total_num = 0.0\n        for i in range(self.num_seed):\n            while not Q.empty():\n                tp = Q.get()\n                act_num, node, k = -tp[0], tp[1][0], tp[1][1]\n                if k == i:\n                    total_num += act_num\n                    seed_list.append(node)\n                    break\n                else:\n                    act_num = self._simulation(G, features_matrix, node, threshold)\n                    Q.put((-1*(act_num - total_num), [node, i]))\n        \n        return total_num / num_node, seed_list\n    \n    \n    def _simulation(self, G, features_matrix, seed, threshold):\n        res = 0\n        num_node = G.number_of_nodes()\n        for _ in range(self.num_simulation):\n            active_list, active_flag = [], [False] * num_node  \n            active_flag[seed] = True\n            active_list.append(seed)\n            \n            i = 0\n            while i < len(active_list):\n                v = active_list[i]\n                for target in list(G.neighbors(v)):\n                    if active_flag[target] is True: continue \n                    vec1, vec2 = features_matrix[v], features_matrix[target]\n                    # prob = 1.0 / G.degree(target)\n                    prob = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n                    # if np.random.rand() <= prob  * self.decay:\n                    #     active_flag[target] = True\n                    #     active_list.append(target)\n                    if prob >= threshold:\n                        active_flag[target] = True\n                        active_list.append(target)\n                i += 1\n            res += len(active_list) - 1\n            # print(""number of influence node"", len(active_list))\n\n        return res / self.num_simulation'"
cogdl/tasks/link_prediction.py,2,"b'import random\nfrom collections import defaultdict\n\nimport copy\nimport networkx as nx\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom gensim.models.keyedvectors import Vocab\nfrom six import iteritems\nfrom sklearn.metrics import auc, f1_score, precision_recall_curve, roc_auc_score\nfrom tqdm import tqdm\n\nfrom cogdl import options\nfrom cogdl.datasets import build_dataset\nfrom cogdl.models import build_model\n\nfrom . import BaseTask, register_task\n\n\ndef divide_data(input_list, division_rate):\n    local_division = len(input_list) * np.cumsum(np.array(division_rate))\n    random.shuffle(input_list)\n    return [\n        input_list[\n            int(round(local_division[i - 1]))\n            if i > 0\n            else 0 : int(round(local_division[i]))\n        ]\n        for i in range(len(local_division))\n    ]\n\n\ndef randomly_choose_false_edges(nodes, true_edges, num):\n    true_edges_set = set(true_edges)\n    tmp_list = list()\n    all_flag = False\n    for _ in range(num):\n        trial = 0\n        while True:\n            x = nodes[random.randint(0, len(nodes) - 1)]\n            y = nodes[random.randint(0, len(nodes) - 1)]\n            trial += 1\n            if trial >= 1000:\n                all_flag = True\n                break\n            if x != y and (x, y) not in true_edges_set and (y, x) not in true_edges_set:\n                tmp_list.append((x, y))\n                break\n        if all_flag:\n            break\n    return tmp_list\n\n\ndef gen_node_pairs(train_data, valid_data, test_data):\n    G = nx.Graph()\n    G.add_edges_from(train_data)\n\n    training_nodes = set(list(G.nodes()))\n    valid_true_data = []\n    test_true_data = []\n    for u, v in valid_data:\n        if u in training_nodes and v in training_nodes:\n            valid_true_data.append((u, v))\n    for u, v in test_data:\n        if u in training_nodes and v in training_nodes:\n            test_true_data.append((u, v))\n    valid_false_data = randomly_choose_false_edges(\n        list(training_nodes), train_data, len(valid_data)\n    )\n    test_false_data = randomly_choose_false_edges(\n        list(training_nodes), train_data, len(test_data)\n    )\n    return (\n        (valid_true_data, valid_false_data),\n        (test_true_data, test_false_data),\n    )\n\n\ndef get_score(embs, node1, node2):\n    vector1 = embs[int(node1)]\n    vector2 = embs[int(node2)]\n    return np.dot(vector1, vector2) / (\n        np.linalg.norm(vector1) * np.linalg.norm(vector2)\n    )\n\n\ndef evaluate(embs, true_edges, false_edges):\n    true_list = list()\n    prediction_list = list()\n    for edge in true_edges:\n        true_list.append(1)\n        prediction_list.append(get_score(embs, edge[0], edge[1]))\n\n    for edge in false_edges:\n        true_list.append(0)\n        prediction_list.append(get_score(embs, edge[0], edge[1]))\n\n    sorted_pred = prediction_list[:]\n    sorted_pred.sort()\n    threshold = sorted_pred[-len(true_edges)]\n\n    y_pred = np.zeros(len(prediction_list), dtype=np.int32)\n    for i in range(len(prediction_list)):\n        if prediction_list[i] >= threshold:\n            y_pred[i] = 1\n\n    y_true = np.array(true_list)\n    y_scores = np.array(prediction_list)\n    ps, rs, _ = precision_recall_curve(y_true, y_scores)\n    return roc_auc_score(y_true, y_scores), f1_score(y_true, y_pred), auc(rs, ps)\n\n\n@register_task(""link_prediction"")\nclass LinkPrediction(BaseTask):\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(""--hidden-size"", type=int, default=128)\n        parser.add_argument(""--negative-ratio"", type=int, default=5)\n        # fmt: on\n\n    def __init__(self, args):\n        super(LinkPrediction, self).__init__(args)\n\n        dataset = build_dataset(args)\n        data = dataset[0]\n        self.data = data\n        if hasattr(dataset, ""num_features""):\n            args.num_features = dataset.num_features\n        model = build_model(args)\n        self.model = model\n        self.patience = args.patience\n        self.max_epoch = args.max_epoch\n\n        edge_list = self.data.edge_index.numpy()\n        edge_list = list(zip(edge_list[0], edge_list[1]))\n        edge_set = set()\n        for edge in edge_list:\n            if (edge[0], edge[1]) not in edge_set and (edge[1], edge[0]) not in edge_set:\n                edge_set.add(edge)\n        edge_list = list(edge_set)\n        self.train_data, self.valid_data, self.test_data = divide_data(\n            edge_list, [0.85, 0.05, 0.10]\n        )\n\n        self.valid_data, self.test_data = gen_node_pairs(\n            self.train_data, self.valid_data, self.test_data\n        )\n\n    def train(self):\n        G = nx.Graph()\n        G.add_edges_from(self.train_data)\n        embeddings = self.model.train(G)\n\n        embs = dict()\n        for vid, node in enumerate(G.nodes()):\n            embs[node] = embeddings[vid]\n\n        roc_auc, f1_score, pr_auc = evaluate(embs, self.test_data[0], self.test_data[1])\n        print(\n            f""Test ROC-AUC = {roc_auc:.4f}, F1 = {f1_score:.4f}, PR-AUC = {pr_auc:.4f}""\n        )\n        return dict(ROC_AUC=roc_auc, PR_AUC=pr_auc, F1=f1_score)\n'"
cogdl/tasks/multiplex_link_prediction.py,2,"b'import random\nfrom collections import defaultdict\n\nimport copy\nimport networkx as nx\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom gensim.models.keyedvectors import Vocab\nfrom six import iteritems\nfrom sklearn.metrics import auc, f1_score, precision_recall_curve, roc_auc_score\nfrom tqdm import tqdm\n\nfrom cogdl import options\nfrom cogdl.datasets import build_dataset\nfrom cogdl.models import build_model\n\nfrom . import BaseTask, register_task\n\n\ndef get_score(embs, node1, node2):\n    vector1 = embs[int(node1)]\n    vector2 = embs[int(node2)]\n    return np.dot(vector1, vector2) / (\n        np.linalg.norm(vector1) * np.linalg.norm(vector2)\n    )\n\n\ndef evaluate(embs, true_edges, false_edges):\n    true_list = list()\n    prediction_list = list()\n    for edge in true_edges:\n        true_list.append(1)\n        prediction_list.append(get_score(embs, edge[0], edge[1]))\n\n    for edge in false_edges:\n        true_list.append(0)\n        prediction_list.append(get_score(embs, edge[0], edge[1]))\n\n    sorted_pred = prediction_list[:]\n    sorted_pred.sort()\n    threshold = sorted_pred[-len(true_edges)]\n\n    y_pred = np.zeros(len(prediction_list), dtype=np.int32)\n    for i in range(len(prediction_list)):\n        if prediction_list[i] >= threshold:\n            y_pred[i] = 1\n\n    y_true = np.array(true_list)\n    y_scores = np.array(prediction_list)\n    ps, rs, _ = precision_recall_curve(y_true, y_scores)\n    return roc_auc_score(y_true, y_scores), f1_score(y_true, y_pred), auc(rs, ps)\n\n\n@register_task(""multiplex_link_prediction"")\nclass MultiplexLinkPrediction(BaseTask):\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(""--hidden-size"", type=int, default=200)\n        parser.add_argument(""--negative-ratio"", type=int, default=5)\n        parser.add_argument(""--eval-type"", type=str, default=\'all\', nargs=\'+\')\n        # fmt: on\n\n    def __init__(self, args):\n        super(MultiplexLinkPrediction, self).__init__(args)\n\n        dataset = build_dataset(args)\n        data = dataset[0]\n        self.data = data\n        if hasattr(dataset, ""num_features""):\n            args.num_features = dataset.num_features\n        model = build_model(args)\n        self.model = model\n        self.patience = args.patience\n        self.max_epoch = args.max_epoch\n        self.eval_type = args.eval_type\n\n    def train(self):\n        total_roc_auc, total_f1_score, total_pr_auc = [], [], []\n        if hasattr(self.model, ""multiplicity""):\n            all_embs = self.model.train(self.data.train_data)\n        for key in self.data.train_data.keys():\n            if self.eval_type == ""all"" or key in self.eval_type:\n                embs = dict()\n                if not hasattr(self.model, ""multiplicity""):\n                    G = nx.Graph()\n                    G.add_edges_from(self.data.train_data[key])\n                    embeddings = self.model.train(G)\n\n                    for vid, node in enumerate(G.nodes()):\n                        embs[node] = embeddings[vid]\n                else:\n                    embs = all_embs[key]\n                roc_auc, f1_score, pr_auc = evaluate(\n                    embs, self.data.test_data[key][0], self.data.test_data[key][1]\n                )\n                total_roc_auc.append(roc_auc)\n                total_f1_score.append(f1_score)\n                total_pr_auc.append(pr_auc)\n        assert len(total_roc_auc) > 0\n        roc_auc, f1_score, pr_auc = (\n            np.mean(total_roc_auc),\n            np.mean(total_f1_score),\n            np.mean(total_pr_auc),\n        )\n        print(\n            f""Test ROC-AUC = {roc_auc:.4f}, F1 = {f1_score:.4f}, PR-AUC = {pr_auc:.4f}""\n        )\n        return dict(ROC_AUC=roc_auc, PR_AUC=pr_auc, F1=f1_score)\n'"
cogdl/tasks/multiplex_node_classification.py,3,"b'import copy\nimport os\nimport random\nimport warnings\nfrom collections import defaultdict\n\nimport networkx as nx\nimport numpy as np\nimport scipy.sparse as sp\nimport torch\nimport torch.nn.functional as F\nfrom scipy import sparse as sp\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.utils import shuffle as skshuffle\nfrom tqdm import tqdm\n\nfrom cogdl import options\nfrom cogdl.data import Dataset, InMemoryDataset\nfrom cogdl.datasets import build_dataset\nfrom cogdl.models import build_model, register_model\n\nfrom . import BaseTask, register_task\n\nwarnings.filterwarnings(""ignore"")\n\n\n@register_task(""multiplex_node_classification"")\nclass MultiplexNodeClassification(BaseTask):\n    """"""Node classification task.""""""\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(""--hidden-size"", type=int, default=128)\n        # fmt: on\n\n    def __init__(self, args):\n        super(MultiplexNodeClassification, self).__init__(args)\n        dataset = build_dataset(args)\n        self.data = dataset[0]\n        self.label_matrix = self.data.y\n        self.num_nodes, self.num_classes = dataset.num_nodes, dataset.num_classes\n        self.hidden_size = args.hidden_size\n        self.model = build_model(args)\n        \n        self.device = torch.device(\'cpu\' if args.cpu else \'cuda\')\n        self.model = self.model.to(self.device)\n\n\n    def train(self):\n        G = nx.DiGraph()\n        G.add_edges_from(self.data.edge_index.t().tolist())\n        embeddings = self.model.train(G, self.data.pos.tolist())\n        embeddings = np.hstack((embeddings, self.data.x.numpy()))\n                    \n        # Select nodes which have label as training data        \n        train_index = torch.cat((self.data.train_node, self.data.valid_node)).numpy()\n        test_index = self.data.test_node.numpy()\n        y = self.data.y.numpy()\n        \n        X_train, y_train = embeddings[train_index], y[train_index]\n        X_test, y_test = embeddings[test_index], y[test_index]\n        clf = LogisticRegression()\n        clf.fit(X_train, y_train)\n        preds = clf.predict(X_test)\n        test_f1 = f1_score(y_test, preds, average=""micro"")\n\n        return dict(f1=test_f1)\n'"
cogdl/tasks/node_classification.py,3,"b'import copy\nimport random\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\nfrom cogdl import options\nfrom cogdl.datasets import build_dataset\nfrom cogdl.models import build_model\n\nfrom . import BaseTask, register_task\n\n\n@register_task(""node_classification"")\nclass NodeClassification(BaseTask):\n    """"""Node classification task.""""""\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        # fmt: off\n        # parser.add_argument(""--num-features"", type=int)\n        # fmt: on\n\n    def __init__(self, args):\n        super(NodeClassification, self).__init__(args)\n\n        self.device = torch.device(\'cpu\' if args.cpu else \'cuda\')\n        dataset = build_dataset(args)\n        self.data = dataset.data\n        self.data.apply(lambda x: x.to(self.device))\n        args.num_features = dataset.num_features\n        args.num_classes = dataset.num_classes\n        model = build_model(args)\n        self.model = model.to(self.device)\n        self.patience = args.patience\n        self.max_epoch = args.max_epoch\n\n        self.optimizer = torch.optim.Adam(\n            self.model.parameters(), lr=args.lr, weight_decay=args.weight_decay\n        )\n\n    def train(self):\n        epoch_iter = tqdm(range(self.max_epoch))\n        patience = 0\n        best_score = 0\n        best_loss = np.inf\n        max_score = 0\n        min_loss = np.inf\n        for epoch in epoch_iter:\n            self._train_step()\n            train_acc, _ = self._test_step(split=""train"")\n            val_acc, val_loss = self._test_step(split=""val"")\n            epoch_iter.set_description(\n                f""Epoch: {epoch:03d}, Train: {train_acc:.4f}, Val: {val_acc:.4f}""\n            )\n            if val_loss <= min_loss or val_acc >= max_score:\n                if val_loss <= best_loss:  # and val_acc >= best_score:\n                    best_loss = val_loss\n                    best_score = val_acc\n                    best_model = copy.deepcopy(self.model)\n                min_loss = np.min((min_loss, val_loss))\n                max_score = np.max((max_score, val_acc))\n                patience = 0\n            else:\n                patience += 1\n                if patience == self.patience:\n                    self.model = best_model\n                    epoch_iter.close()\n                    break\n        test_acc, _ = self._test_step(split=""test"")\n        print(f""Test accuracy = {test_acc}"")\n        return dict(Acc=test_acc)\n\n    def _train_step(self):\n        self.model.train()\n        self.optimizer.zero_grad()\n        self.model.loss(self.data).backward()\n        self.optimizer.step()\n\n    def _test_step(self, split=""val""):\n        self.model.eval()\n        logits = self.model.predict(self.data)\n        _, mask = list(self.data(f""{split}_mask""))[0]\n        loss = F.nll_loss(logits[mask], self.data.y[mask])\n\n        pred = logits[mask].max(1)[1]\n        acc = pred.eq(self.data.y[mask]).sum().item() / mask.sum().item()\n        return acc, loss\n'"
cogdl/tasks/node_classification_sampling.py,8,"b'import copy\nimport random\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\nfrom cogdl import options\nfrom cogdl.datasets import build_dataset\nfrom cogdl.models import build_model\n\nfrom . import BaseTask, register_task\n\ndef get_batches(train_nodes, train_labels, batch_size=64, shuffle=True):\n    if shuffle:\n        random.shuffle(train_nodes)\n    total = train_nodes.shape[0]\n    for i in range(0, total, batch_size):\n        if i + batch_size <= total:\n            cur_nodes = train_nodes[i: i+batch_size]\n            cur_labels = train_labels[cur_nodes]\n            yield cur_nodes, cur_labels\n\n@register_task(""node_classification_sampling"")\nclass NodeClassificationSampling(BaseTask):\n    """"""Node classification task with sampling.""""""\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(""--batch-size"", type=int, default=20)\n        # fmt: on\n\n    def __init__(self, args):\n        super(NodeClassificationSampling, self).__init__(args)\n\n        self.device = torch.device(\'cpu\' if args.cpu else \'cuda\')\n        dataset = build_dataset(args)\n        self.data = dataset.data\n        self.data.apply(lambda x: x.to(self.device))\n        args.num_features = dataset.num_features\n        args.num_classes = dataset.num_classes\n        model = build_model(args)\n        self.num_nodes = self.data.x.shape[0]\n        self.model = model.to(self.device)\n        self.patience = args.patience\n        self.max_epoch = args.max_epoch\n        self.batch_size = args.batch_size\n\n        self.adj_list = self.data.edge_index.detach().cpu().numpy()\n        self.model.set_adj(self.adj_list, self.num_nodes)\n\n        self.optimizer = torch.optim.Adam(\n            self.model.parameters(), lr=args.lr, weight_decay=args.weight_decay\n        )\n\n    def train(self):\n        epoch_iter = tqdm(range(self.max_epoch))\n        patience = 0\n        best_score = 0\n        best_loss = np.inf\n        max_score = 0\n        min_loss = np.inf\n        for epoch in epoch_iter:\n            self._train_step()\n            train_acc, _ = self._test_step(split=""train"")\n            val_acc, val_loss = self._test_step(split=""val"")\n            epoch_iter.set_description(\n                f""Epoch: {epoch:03d}, Train: {train_acc:.4f}, Val: {val_acc:.4f}""\n            )\n            if val_loss <= min_loss or val_acc >= max_score:\n                if val_loss <= best_loss:  # and val_acc >= best_score:\n                    best_loss = val_loss\n                    best_score = val_acc\n                    best_model = copy.deepcopy(self.model)\n                min_loss = np.min((min_loss, val_loss))\n                max_score = np.max((max_score, val_acc))\n                patience = 0\n            else:\n                patience += 1\n                if patience == self.patience:\n                    self.model = best_model\n                    epoch_iter.close()\n                    break\n        test_acc, _ = self._test_step(split=""test"")\n        print(f""Test accuracy = {test_acc}"")\n        return dict(Acc=test_acc)\n\n    def _train_step(self):\n        self.model.train()\n        train_nodes = np.where(self.data.train_mask.detach().cpu().numpy())[0]\n        train_labels = self.data.y.detach().cpu().numpy()\n        for batch_nodes, batch_labels in get_batches(train_nodes, train_labels, batch_size=self.batch_size):\n            batch_nodes = torch.LongTensor(batch_nodes)\n            batch_labels = torch.LongTensor(batch_labels).to(self.device)\n            sampled_x, sampled_adj, var_loss = self.model.sampling(self.data.x, batch_nodes)\n            self.optimizer.zero_grad()\n            output = self.model(sampled_x, sampled_adj)\n            loss = F.nll_loss(output, batch_labels) + 0.5 * var_loss\n            loss.backward()\n            self.optimizer.step()\n\n    def _test_step(self, split=""val""):\n        self.model.eval()\n        _, mask = list(self.data(f""{split}_mask""))[0]\n        test_nodes = np.where(mask.detach().cpu().numpy())[0]\n        test_labels = self.data.y.detach().cpu().numpy()\n        all_loss = []\n        all_acc = []\n        for batch_nodes, batch_labels in get_batches(test_nodes, test_labels, batch_size=self.batch_size):\n            batch_nodes = torch.LongTensor(batch_nodes)\n            batch_labels = torch.LongTensor(batch_labels).to(self.device)\n            sampled_x, sampled_adj, var_loss = self.model.sampling(self.data.x, batch_nodes)\n            with torch.no_grad():\n                logits = self.model(sampled_x, sampled_adj)\n                loss = F.nll_loss(logits, batch_labels)\n            pred = logits.max(1)[1]\n            acc = pred.eq(self.data.y[batch_nodes]).sum().item() / batch_nodes.shape[0]\n\n            all_loss.append(loss.item())\n            all_acc.append(acc)\n\n        return np.mean(all_acc), np.mean(all_loss)\n'"
cogdl/tasks/unsupervised_graph_classification.py,4,"b'import os\nfrom collections import defaultdict\nimport numpy as np\nfrom sklearn.utils import shuffle as skshuffle\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom cogdl.data import Data, DataLoader, InMemoryDataset\nfrom cogdl.datasets import build_dataset\nfrom cogdl.models import build_model\nfrom . import BaseTask, register_task\nfrom .graph_classification import node_degree_as_feature\nfrom .unsupervised_node_classification import TopKRanker\n\n\n@register_task(""unsupervised_graph_classification"")\nclass UnsupervisedGraphClassification(BaseTask):\n    r""""""Unsupervised graph classification""""""\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(""--lr"", type=float, default=0.001)\n        parser.add_argument(""--num-shuffle"", type=int, default=10)\n        parser.add_argument(""--degree-feature"", dest=""degree_feature"", action=""store_true"")\n        # fmt: on\n\n    def __init__(self, args):\n        super(UnsupervisedGraphClassification, self).__init__(args)\n        self.device = args.device_id[0] if not args.cpu else \'cpu\'\n\n        dataset = build_dataset(args)\n        self.label = np.array([data.y for data in dataset])\n        self.data = [\n            Data(x=data.x, y=data.y, edge_index=data.edge_index, edge_attr=data.edge_attr,\n                 pos=data.pos).apply(lambda x:x.to(self.device))\n            for data in dataset\n        ]\n        args.num_features = dataset.num_features\n        args.num_classes = args.hidden_size\n        args.use_unsup = True\n\n        if args.degree_feature:\n            self.data = node_degree_as_feature(self.data)\n            args.num_features = self.data[0].num_features\n\n        self.num_graphs = len(self.data)\n        self.num_classes = dataset.num_classes\n        # self.label_matrix = np.zeros((self.num_graphs, self.num_classes))\n        # self.label_matrix[range(self.num_graphs), np.array([data.y for data in self.data], dtype=int)] = 1\n\n        self.model = build_model(args)\n        self.model = self.model.to(self.device)\n        self.model_name = args.model\n        self.hidden_size = args.hidden_size\n        self.num_shuffle = args.num_shuffle\n        self.save_dir = args.save_dir\n        self.epoch = args.epoch\n        self.use_nn = args.model in (\'infograph\', )\n\n        if self.use_nn:\n            self.optimizer = torch.optim.Adam(\n                self.model.parameters(), lr=args.lr, weight_decay=args.weight_decay\n            )\n            self.data_loader = DataLoader(self.data, batch_size=args.batch_size, shuffle=True)\n\n    def train(self):\n        if self.use_nn:\n            epoch_iter = tqdm(range(self.epoch))\n            for epoch in epoch_iter:\n                loss_n = 0\n                for batch in self.data_loader:\n                    batch = batch.to(self.device)\n                    predict, loss = self.model(batch)\n                    self.optimizer.zero_grad()\n                    loss.backward()\n                    self.optimizer.step()\n                    loss_n += loss.item()\n                epoch_iter.set_description(\n                    f""Epoch: {epoch:03d}, TrainLoss: {loss_n} ""\n                )\n            with torch.no_grad():\n                self.model.eval()\n                prediction = []\n                label = []\n                for batch in self.data_loader:\n                    batch = batch.to(self.device)\n                    predict, _ = self.model(batch)\n                    prediction.extend(predict.cpu().numpy())\n                    label.extend(batch.y.cpu().numpy())\n                prediction = np.array(prediction).reshape(len(label), -1)\n                label = np.array(label).reshape(-1)\n        else:\n            prediction, loss = self.model(self.data)\n            label = self.label\n\n        if prediction is not None:\n            # self.save_emb(prediction)\n            return self._evaluate(prediction, label)\n\n    def save_emb(self, embs):\n        name = os.path.join(self.save_dir, self.model_name + \'_emb.npy\')\n        np.save(name, embs)\n\n    def _evaluate(self, embeddings, labels):\n        shuffles = []\n        for _ in range(self.num_shuffle):\n            shuffles.append(skshuffle(embeddings, labels))\n        all_results = defaultdict(list)\n        training_percents = [0.1, 0.3, 0.5, 0.7, 0.9]\n\n        for training_percent in training_percents:\n            for shuf in shuffles:\n                training_size = int(training_percent * self.num_graphs)\n                X, y = shuf\n                X_train = X[:training_size, :]\n                y_train = y[:training_size]\n\n                X_test = X[training_size:, :]\n                y_test = y[training_size:]\n                # clf = SVC()\n                # clf.fit(X_train, y_train)\n\n                params = {""C"": [1e-3, 1e-2, 1e-1, 0, 1, 10]}\n                svc = SVC()\n                clf = GridSearchCV(svc, params)\n                clf.fit(X_train, y_train)\n\n                preds = clf.predict(X_test)\n                accuracy = f1_score(y_test, preds, average=""micro"")\n                all_results[training_percent].append(accuracy)\n\n        return dict(\n            (\n                f""Accuracy {train_percent}"",\n                sum(all_results[train_percent]) / len(all_results[train_percent]),\n            )\n            for train_percent in sorted(all_results.keys())\n        )'"
cogdl/tasks/unsupervised_node_classification.py,1,"b'import copy\nimport os\nimport random\nimport warnings\nfrom collections import defaultdict\n\nimport networkx as nx\nimport numpy as np\nimport scipy.sparse as sp\nimport torch\nimport torch.nn.functional as F\nfrom scipy import sparse as sp\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.utils import shuffle as skshuffle\nfrom tqdm import tqdm\n\nfrom cogdl import options\nfrom cogdl.data import Dataset, InMemoryDataset\nfrom cogdl.datasets import build_dataset\nfrom cogdl.models import build_model, register_model\n\nfrom . import BaseTask, register_task\n\nwarnings.filterwarnings(""ignore"")\n\n\n@register_task(""unsupervised_node_classification"")\nclass UnsupervisedNodeClassification(BaseTask):\n    """"""Node classification task.""""""\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(""--hidden-size"", type=int, default=128)\n        parser.add_argument(""--num-shuffle"", type=int, default=5)\n        # fmt: on\n\n    def __init__(self, args):\n        super(UnsupervisedNodeClassification, self).__init__(args)\n        dataset = build_dataset(args)\n        self.data = dataset[0]\n        if issubclass(dataset.__class__.__bases__[0], InMemoryDataset):\n            self.num_nodes = self.data.y.shape[0]\n            self.num_classes = dataset.num_classes\n            self.label_matrix = np.zeros((self.num_nodes, self.num_classes), dtype=int)\n            self.label_matrix[range(self.num_nodes), self.data.y] = 1\n        else:\n            self.label_matrix = self.data.y\n            self.num_nodes, self.num_classes = self.data.y.shape\n\n        self.model = build_model(args)\n        self.model_name = args.model\n        self.hidden_size = args.hidden_size\n        self.num_shuffle = args.num_shuffle\n        self.save_dir = args.save_dir\n        self.enhance = args.enhance\n        self.args = args\n        self.is_weighted = self.data.edge_attr is not None\n\n\n    def enhance_emb(self, G, embs):\n        A = sp.csr_matrix(nx.adjacency_matrix(G))\n        self.args.model = \'prone\'\n        self.args.step, self.args.theta, self.args.mu = 5, 0.5, 0.2\n        model = build_model(self.args)\n        embs = model._chebyshev_gaussian(A, embs)\n        return embs\n    \n    \n    def save_emb(self, embs):\n        name = os.path.join(self.save_dir, self.model_name + \'_emb.npy\')\n        np.save(name, embs)\n\n    def train(self):\n        G = nx.Graph()\n        if self.is_weighted:\n            edges, weight = (\n                self.data.edge_index.t().tolist(),\n                self.data.edge_attr.tolist(),\n            )\n            G.add_weighted_edges_from(\n                [(edges[i][0], edges[i][1], weight[0][i]) for i in range(len(edges))]\n            )\n        else:\n            G.add_edges_from(self.data.edge_index.t().tolist())\n        embeddings = self.model.train(G)\n        if self.enhance is True:\n            embeddings = self.enhance_emb(G, embeddings)\n\n        # Map node2id\n        features_matrix = np.zeros((self.num_nodes, self.hidden_size))\n        for vid, node in enumerate(G.nodes()):\n            features_matrix[node] = embeddings[vid]\n\n        self.save_emb(features_matrix)\n\n        # label nor multi-label\n        label_matrix = sp.csr_matrix(self.label_matrix)\n\n        return self._evaluate(features_matrix, label_matrix, self.num_shuffle)\n\n    def _evaluate(self, features_matrix, label_matrix, num_shuffle):\n        # features_matrix, node2id = utils.load_embeddings(args.emb)\n        # label_matrix = utils.load_labels(args.label, node2id, divi_str="" "")\n\n        # shuffle, to create train/test groups\n        shuffles = []\n        for _ in range(num_shuffle):\n            shuffles.append(skshuffle(features_matrix, label_matrix))\n\n        # score each train/test group\n        all_results = defaultdict(list)\n        # training_percents = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n        training_percents = [0.1, 0.3, 0.5, 0.7, 0.9]\n\n        for train_percent in training_percents:\n            for shuf in shuffles:\n                X, y = shuf\n\n                training_size = int(train_percent * self.num_nodes)\n\n                X_train = X[:training_size, :]\n                y_train = y[:training_size, :]\n\n                X_test = X[training_size:, :]\n                y_test = y[training_size:, :]\n\n                clf = TopKRanker(LogisticRegression())\n                clf.fit(X_train, y_train)\n\n                # find out how many labels should be predicted\n                top_k_list = list(map(int, y_test.sum(axis=1).T.tolist()[0]))\n                preds = clf.predict(X_test, top_k_list)\n                result = f1_score(y_test, preds, average=""micro"")\n                all_results[train_percent].append(result)\n            # print(""micro"", result)\n\n        return dict(\n            (\n                f""Micro-F1 {train_percent}"",\n                sum(all_results[train_percent]) / len(all_results[train_percent]),\n            )\n            for train_percent in sorted(all_results.keys())\n        )\n\n\nclass TopKRanker(OneVsRestClassifier):\n    def predict(self, X, top_k_list):\n        assert X.shape[0] == len(top_k_list)\n        probs = np.asarray(super(TopKRanker, self).predict_proba(X))\n        all_labels = sp.lil_matrix(probs.shape)\n\n        for i, k in enumerate(top_k_list):\n            probs_ = probs[i, :]\n            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n            for label in labels:\n                all_labels[i, label] = 1\n        return all_labels\n'"
docs/source/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\n\n# print(\'current path\', os.path.abspath(\'.\'))\nsys.path.insert(0, os.path.abspath(\'../../cogdl\'))\n# print(sys.path)\n\n# -- Project information -----------------------------------------------------\n\nproject = ""CogDL""\ncopyright = ""2020, KEG""\nauthor = ""KEG""\n\n# The short X.Y version\nversion = """"\n# The full version, including alpha/beta/rc tags\nrelease = """"\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    ""sphinx.ext.autosummary"",\n    ""sphinx.ext.autodoc"",\n    ""sphinx.ext.doctest"",\n    ""sphinx.ext.intersphinx"",\n    ""sphinx.ext.todo"",\n    ""sphinx.ext.coverage"",\n    ""sphinx.ext.mathjax"",\n    ""sphinx.ext.ifconfig"",\n    ""sphinx.ext.viewcode"",\n    ""sphinx.ext.githubpages"",\n    ""recommonmark"",\n    \'autoapi.extension\',\n    \'sphinx_markdown_tables\'\n]\n\nautoapi_dirs = [\'../../cogdl\']\n\n\n# generate autosummary pages\nautosummary_generate = True\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [""_templates""]\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\nsource_suffix = [\'.rst\', \'.md\']\n# source_suffix = "".rst""\n\n# The master toctree document.\nmaster_doc = ""index""\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = []\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = None\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = ""sphinx_rtd_theme""\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [""_static""]\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = ""CogDLdoc""\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [(master_doc, ""CogDL.tex"", ""CogDL Documentation"", ""KEG"", ""manual"")]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [(master_doc, ""cogdl"", ""CogDL Documentation"", [author], 1)]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (\n        master_doc,\n        ""CogDL"",\n        ""CogDL Documentation"",\n        author,\n        ""CogDL"",\n        ""One line description of project."",\n        ""Miscellaneous"",\n    )\n]\n\n\n# -- Options for Epub output -------------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = project\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#\n# epub_identifier = \'\'\n\n# A unique identification for the text.\n#\n# epub_uid = \'\'\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = [""search.html""]\n\n\n# -- Extension configuration -------------------------------------------------\n\n# -- Options for intersphinx extension ---------------------------------------\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {""https://docs.python.org/"": None}\n\n# -- Options for todo extension ----------------------------------------------\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n'"
tests/datasets/test_pyg.py,0,"b'from cogdl.datasets import build_dataset\nfrom cogdl.utils import build_args_from_dict\n\ndef test_cora():\n    args = build_args_from_dict({\'dataset\': \'cora\'})\n    assert args.dataset == \'cora\'\n    cora = build_dataset(args)\n    assert cora.data.num_nodes == 2708\n    assert cora.data.num_edges == 10556\n\nif __name__ == ""__main__"":\n    test_cora()'"
tests/tasks/test_graph_classification.py,1,"b'import torch\nfrom cogdl import options\nfrom cogdl.tasks import build_task\nfrom cogdl.datasets import build_dataset\nfrom cogdl.models import build_model\nfrom cogdl.utils import build_args_from_dict\n\n\ndef get_default_args():\n    cuda_available = torch.cuda.is_available()\n    default_dict = {\n                    \'task\': \'graph_classification\',\n                    \'hidden_size\': 64,\n                    \'dropout\': 0.5,\n                    \'patience\': 1,\n                    \'max_epoch\': 2,\n                    \'cpu\': not cuda_available,\n                    \'lr\': 0.001,\n                    \'kfold\': False,\n                    \'seed\': [0],\n                    \'weight_decay\': 5e-4,\n                    \'gamma\': 0.5,\n                    \'train_ratio\': 0.7,\n                    \'test_ratio\': 0.1,\n                    \'device_id\': [0 if cuda_available else \'cpu\'],\n                    \'degree_feature\': False}\n    return build_args_from_dict(default_dict)\n\n\ndef add_diffpool_args(args):\n    args.num_layers = 2\n    args.num_pooling_layers = 1\n    args.no_link_pred = False\n    args.pooling_ratio = 0.15\n    args.embedding_dim = 64\n    args.hidden_size = 64\n    args.batch_size = 20\n    args.dropout = 0.1\n    return args\n\n\ndef add_gin_args(args):\n    args.epsilon = 0.\n    args.hidden_size = 32\n    args.num_layers = 5\n    args.num_mlp_layers = 2\n    args.train_epsilon = True\n    args.pooling = \'sum\'\n    args.batch_size = 128\n    return args\n\n\ndef add_dgcnn_args(args):\n    args.hidden_size = 64\n    args.batch_size = 20\n    return args\n\n\ndef add_sortpool_args(args):\n    args.hidden_size = 64\n    args.batch_size = 20\n    args.num_layers = 2\n    args.out_channels = 32\n    args.k = 30\n    args.kernel_size = 5\n    return args\n\n\ndef add_patchy_san_args(args):\n    args.hidden_size = 64\n    args.batch_size = 20\n    args.sample = 10\n    args.stride = 1\n    args.neighbor = 10\n    args.iteration = 2\n    args.train_ratio = 0.7\n    args.test_ratio = 0.1\n    return args    \n\ndef test_gin_mutag():\n    args = get_default_args()\n    args = add_gin_args(args)\n    args.dataset = \'mutag\'\n    args.model = \'gin\'\n    args.batch_size = 32\n    task = build_task(args)\n    ret = task.train()\n    assert ret[""Acc""] > 0\n\n\ndef test_gin_imdb_binary():\n    args = get_default_args()\n    args = add_gin_args(args)\n    args.dataset = \'imdb-b\'\n    args.model = \'gin\'\n    args.degree_feature = True\n    task = build_task(args)\n    ret = task.train()\n    assert ret[""Acc""] > 0\n\n\ndef test_gin_proteins():\n    args = get_default_args()\n    args = add_gin_args(args)\n    args.dataset = \'proteins\'\n    args.model = \'gin\'\n    task = build_task(args)\n    ret = task.train()\n    assert ret[""Acc""] > 0\n\n\ndef test_diffpool_mutag():\n    args = get_default_args()\n    args = add_diffpool_args(args)\n    args.dataset = \'mutag\'\n    args.model = \'diffpool\'\n    args.batch_size = 5\n    task = build_task(args)\n    ret = task.train()\n    assert ret[""Acc""] > 0\n\n\ndef test_diffpool_proteins():\n    args = get_default_args()\n    args = add_diffpool_args(args)\n    args.dataset = \'proteins\'\n    args.model = \'diffpool\'\n    args.batch_size = 20\n    task = build_task(args)\n    ret = task.train()\n    assert ret[""Acc""] > 0\n\n\n# def test_dgcnn_modelnet10():\n#     args = get_default_args()\n#     args = add_dgcnn_args(args)\n#     args.dataset = \'ModelNet10\'\n#     args.model = \'pyg_dgcnn\'\n#     task = build_task(args)\n#     ret = task.train()\n#     assert ret[""Acc""] > 0\n\n\ndef test_dgcnn_proteins():\n    args = get_default_args()\n    args = add_dgcnn_args(args)\n    args.dataset = \'proteins\'\n    args.model = \'pyg_dgcnn\'\n    task = build_task(args)\n    ret = task.train()\n    assert ret[""Acc""] > 0\n\n\ndef test_dgcnn_imdb_binary():\n    args = get_default_args()\n    args = add_dgcnn_args(args)\n    args.dataset = \'imdb-b\'\n    args.model = \'pyg_dgcnn\'\n    args.degree_feature = True\n    task = build_task(args)\n    ret = task.train()\n    assert ret[""Acc""] > 0\n\ndef test_sortpool_mutag():\n    args = get_default_args()\n    args = add_sortpool_args(args)\n    args.dataset = \'mutag\'\n    args.model = \'sortpool\'\n    args.batch_size =  20\n    task = build_task(args)\n    ret = task.train()\n    assert ret[""Acc""] > 0\n\n\ndef test_sortpool_proteins():\n    args = get_default_args()\n    args = add_sortpool_args(args)\n    args.dataset = \'proteins\'\n    args.model = \'sortpool\'\n    args.batch_size =  20\n    task = build_task(args)\n    ret = task.train()\n    assert ret[""Acc""] > 0\n\n\ndef test_patchy_san_mutag():\n    args = get_default_args()\n    args = add_patchy_san_args(args)\n    args.dataset = \'mutag\'\n    args.model = \'patchy_san\'\n    args.batch_size =  20\n    task = build_task(args)\n    ret = task.train()\n    assert ret[""Acc""] > 0\n    \n\ndef test_patchy_san_proteins():\n    args = get_default_args()\n    args = add_patchy_san_args(args)\n    args.dataset = \'proteins\'\n    args.model = \'patchy_san\'\n    args.batch_size =  20\n    task = build_task(args)\n    ret = task.train()\n    assert ret[""Acc""] > 0  \n\n\nif __name__ == ""__main__"":\n        \n    test_gin_imdb_binary()\n    test_gin_mutag()\n    test_gin_proteins()\n\n    test_sortpool_mutag()\n    test_sortpool_proteins()\n\n    test_diffpool_mutag()\n    test_diffpool_proteins()\n\n    test_dgcnn_proteins()\n    test_dgcnn_imdb_binary()\n    # test_dgcnn_modelnet10()\n    \n    test_patchy_san_mutag()\n    test_patchy_san_proteins()'"
tests/tasks/test_heterogeneous_node_classification.py,1,"b'import torch\nfrom cogdl import options\nfrom cogdl.tasks import build_task\nfrom cogdl.datasets import build_dataset\nfrom cogdl.models import build_model\nfrom cogdl.utils import build_args_from_dict\n\ndef get_default_args():\n    cuda_available = torch.cuda.is_available()\n    default_dict = {\'hidden_size\': 8,\n                    \'dropout\': 0.5,\n                    \'patience\': 1,\n                    \'max_epoch\': 1,\n                    \'cpu\': not cuda_available,\n                    \'lr\': 0.01,\n                    \'weight_decay\': 5e-4}\n    return build_args_from_dict(default_dict)\n\ndef test_gtn_gtn_imdb():\n    args = get_default_args()\n    args.task = \'heterogeneous_node_classification\'\n    args.dataset = \'gtn-imdb\'\n    args.model = \'gtn\'\n    dataset = build_dataset(args)\n    args.num_features = dataset.num_features\n    args.num_classes = dataset.num_classes\n    args.num_edge = dataset.num_edge\n    args.num_nodes = dataset.num_nodes\n    args.num_channels = 2\n    args.num_layers = 2\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'f1\'] >= 0 and ret[\'f1\'] <= 1\n\ndef test_han_gtn_acm():\n    args = get_default_args()\n    args.task = \'heterogeneous_node_classification\'\n    args.dataset = \'gtn-acm\'\n    args.model = \'han\'\n    dataset = build_dataset(args)\n    args.num_features = dataset.num_features\n    args.num_classes = dataset.num_classes\n    args.num_edge = dataset.num_edge\n    args.num_nodes = dataset.num_nodes\n    args.num_layers = 2\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'f1\'] >= 0 and ret[\'f1\'] <= 1\n\ndef test_han_gtn_dblp():\n    args = get_default_args()\n    args.task = \'heterogeneous_node_classification\'\n    args.dataset = \'gtn-dblp\'\n    args.model = \'han\'\n    dataset = build_dataset(args)\n    args.num_features = dataset.num_features\n    args.num_classes = dataset.num_classes\n    args.num_edge = dataset.num_edge\n    args.num_nodes = dataset.num_nodes\n    args.num_layers = 2\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'f1\'] >= 0 and ret[\'f1\'] <= 1\n\ndef test_han_han_imdb():\n    args = get_default_args()\n    args.task = \'heterogeneous_node_classification\'\n    args.dataset = \'han-imdb\'\n    args.model = \'han\'\n    dataset = build_dataset(args)\n    args.num_features = dataset.num_features\n    args.num_classes = dataset.num_classes\n    args.num_edge = dataset.num_edge\n    args.num_nodes = dataset.num_nodes\n    args.num_layers = 2\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'f1\'] >= 0 and ret[\'f1\'] <= 1\n\ndef test_han_han_acm():\n    args = get_default_args()\n    args.task = \'heterogeneous_node_classification\'\n    args.dataset = \'han-acm\'\n    args.model = \'han\'\n    dataset = build_dataset(args)\n    args.num_features = dataset.num_features\n    args.num_classes = dataset.num_classes\n    args.num_edge = dataset.num_edge\n    args.num_nodes = dataset.num_nodes\n    args.num_layers = 2\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'f1\'] >= 0 and ret[\'f1\'] <= 1\n\ndef test_han_han_dblp():\n    args = get_default_args()\n    args.task = \'heterogeneous_node_classification\'\n    args.dataset = \'han-dblp\'\n    args.model = \'han\'\n    args.cpu = True\n    dataset = build_dataset(args)\n    args.num_features = dataset.num_features\n    args.num_classes = dataset.num_classes\n    args.num_edge = dataset.num_edge\n    args.num_nodes = dataset.num_nodes\n    args.num_layers = 2\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'f1\'] >= 0 and ret[\'f1\'] <= 1\n\nif __name__ == ""__main__"":\n    test_gtn_gtn_imdb()\n    test_han_gtn_acm()\n    test_han_gtn_dblp()\n    test_han_han_imdb()\n    test_han_han_acm()\n    test_han_han_dblp()\n'"
tests/tasks/test_link_prediction.py,0,"b'from cogdl import options\nfrom cogdl.tasks import build_task\nfrom cogdl.datasets import build_dataset\nfrom cogdl.models import build_model\nfrom cogdl.utils import build_args_from_dict\n\ndef get_default_args():\n    default_dict = {\'hidden_size\': 16,\n                    \'negative_ratio\': 3,\n                    \'patience\': 1,\n                    \'max_epoch\': 1,\n                    \'cpu\': True}\n    return build_args_from_dict(default_dict)\n\ndef test_deepwalk_ppi():\n    args = get_default_args()\n    args.task = \'link_prediction\'\n    args.dataset = \'ppi\'\n    args.model = \'deepwalk\'\n    dataset = build_dataset(args)\n    args.walk_length = 5\n    args.walk_num = 1\n    args.window_size = 3\n    args.worker = 5\n    args.iteration = 1\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'ROC_AUC\'] >= 0 and ret[\'ROC_AUC\'] <= 1\n\ndef test_line_wikipedia():\n    args = get_default_args()\n    args.task = \'link_prediction\'\n    args.dataset = \'wikipedia\'\n    args.model = \'line\'\n    dataset = build_dataset(args)\n    args.walk_length = 5\n    args.walk_num = 1\n    args.negative = 3\n    args.batch_size = 20\n    args.alpha = 0.025\n    args.order = 1\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'ROC_AUC\'] >= 0 and ret[\'ROC_AUC\'] <= 1\n\ndef test_node2vec_ppi():\n    args = get_default_args()\n    args.task = \'link_prediction\'\n    args.dataset = \'ppi\'\n    args.model = \'node2vec\'\n    dataset = build_dataset(args)\n    args.walk_length = 5\n    args.walk_num = 1\n    args.window_size = 3\n    args.worker = 5\n    args.iteration = 1\n    args.p = 1.0\n    args.q = 1.0\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'ROC_AUC\'] >= 0 and ret[\'ROC_AUC\'] <= 1\n\ndef test_hope_ppi():\n    args = get_default_args()\n    args.task = \'link_prediction\'\n    args.dataset = \'ppi\'\n    args.model = \'hope\'\n    dataset = build_dataset(args)\n    args.beta = 0.001\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'ROC_AUC\'] >= 0 and ret[\'ROC_AUC\'] <= 1\n\n\ndef test_grarep_ppi():\n    args = get_default_args()\n    args.task = \'link_prediction\'\n    args.dataset = \'ppi\'\n    args.model = \'grarep\'\n    dataset = build_dataset(args)\n    args.step = 1\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'ROC_AUC\'] >= 0 and ret[\'ROC_AUC\'] <= 1\n    \ndef test_netmf_ppi():\n    args = get_default_args()\n    args.task = \'link_prediction\'\n    args.dataset = \'ppi\'\n    args.model = \'netmf\'\n    dataset = build_dataset(args)\n    args.window_size = 2\n    args.rank = 32\n    args.negative = 3\n    args.is_large = False\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'ROC_AUC\'] >= 0 and ret[\'ROC_AUC\'] <= 1\n\ndef test_netsmf_ppi():\n    args = get_default_args()\n    args.task = \'link_prediction\'\n    args.dataset = \'ppi\'\n    args.model = \'netsmf\'\n    dataset = build_dataset(args)\n    args.window_size = 3\n    args.negative = 1\n    args.num_round = 2\n    args.worker = 5\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'ROC_AUC\'] >= 0 and ret[\'ROC_AUC\'] <= 1\n\ndef test_prone_flickr():\n    args = get_default_args()\n    args.task = \'link_prediction\'\n    args.dataset = \'flickr\'\n    args.model = \'prone\'\n    dataset = build_dataset(args)\n    args.step = 3\n    args.theta = 0.5\n    args.mu = 0.2\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'ROC_AUC\'] >= 0 and ret[\'ROC_AUC\'] <= 1\n\ndef test_prone_dblp():\n    args = get_default_args()\n    args.task = \'link_prediction\'\n    args.dataset = \'dblp\'\n    args.model = \'prone\'\n    dataset = build_dataset(args)\n    args.step = 3\n    args.theta = 0.5\n    args.mu = 0.2\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'ROC_AUC\'] >= 0 and ret[\'ROC_AUC\'] <= 1\n\ndef test_sdne_ppi():\n    args = get_default_args()\n    args.task = \'link_prediction\'\n    args.dataset = \'ppi\'\n    args.model = \'sdne\'\n    dataset = build_dataset(args)\n    args.hidden_size1 = 100\n    args.hidden_size2 = 16\n    args.droput = 0.2\n    args.alpha = 0.01\n    args.beta = 5\n    args.nu1 = 1e-4\n    args.nu2 = 1e-3\n    args.max_epoch = 1\n    args.lr = 0.001\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'ROC_AUC\'] >= 0 and ret[\'ROC_AUC\'] <= 1\n\ndef test_dngr_ppi():\n    args = get_default_args()\n    args.task = \'link_prediction\'\n    args.dataset = \'ppi\'\n    args.model = \'dngr\'\n    dataset = build_dataset(args)\n    args.hidden_size1 = 100\n    args.hidden_size2 = 16\n    args.noise = 0.2\n    args.alpha = 0.01\n    args.step = 2\n    args.max_epoch = 1\n    args.lr = 0.001\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'ROC_AUC\'] >= 0 and ret[\'ROC_AUC\'] <= 1\n\nif __name__ == ""__main__"":\n    test_deepwalk_ppi()\n    test_line_wikipedia()\n    test_node2vec_ppi()\n    test_hope_ppi()\n    test_grarep_ppi()\n    test_netmf_ppi()\n    test_netsmf_ppi()\n    test_prone_flickr()\n    test_prone_dblp()\n    test_sdne_ppi()\n    test_dngr_ppi()\n'"
tests/tasks/test_multiplex_link_prediction.py,1,"b'import torch\nfrom cogdl import options\nfrom cogdl.tasks import build_task\nfrom cogdl.datasets import build_dataset\nfrom cogdl.models import build_model\nfrom cogdl.utils import build_args_from_dict\n\ndef get_default_args():\n    cuda_available = torch.cuda.is_available()\n    default_dict = {\'hidden_size\': 16,\n                    \'negative_ratio\': 5,\n                    \'patience\': 1,\n                    \'max_epoch\': 1,\n                    \'eval_type\': \'all\',\n                    \'cpu\': not cuda_available}\n    return build_args_from_dict(default_dict)\n\ndef test_gatne_amazon():\n    args = get_default_args()\n    args.task = \'multiplex_link_prediction\'\n    args.dataset = \'amazon\'\n    args.model = \'gatne\'\n    dataset = build_dataset(args)\n    args.walk_length = 5\n    args.walk_num = 1\n    args.window_size = 3\n    args.worker = 5\n    args.schema = None\n    args.epoch = 1\n    args.batch_size = 128\n    args.edge_dim = 5\n    args.att_dim = 5\n    args.negative_samples = 5\n    args.neighbor_samples = 5\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'ROC_AUC\'] >= 0 and ret[\'ROC_AUC\'] <= 1\n\ndef test_gatne_twitter():\n    args = get_default_args()\n    args.task = \'multiplex_link_prediction\'\n    args.dataset = \'twitter\'\n    args.model = \'gatne\'\n    args.eval_type = [\'1\']\n    dataset = build_dataset(args)\n    args.walk_length = 5\n    args.walk_num = 1\n    args.window_size = 3\n    args.worker = 5\n    args.schema = None\n    args.epoch = 1\n    args.batch_size = 128\n    args.edge_dim = 5\n    args.att_dim = 5\n    args.negative_samples = 5\n    args.neighbor_samples = 5\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'ROC_AUC\'] >= 0 and ret[\'ROC_AUC\'] <= 1\n\ndef test_prone_amazon():\n    args = get_default_args()\n    args.task = \'multiplex_link_prediction\'\n    args.dataset = \'amazon\'\n    args.model = \'prone\'\n    dataset = build_dataset(args)\n    args.step = 5\n    args.theta = 0.5\n    args.mu = 0.2\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'ROC_AUC\'] >= 0 and ret[\'ROC_AUC\'] <= 1\n\ndef test_prone_youtube():\n    args = get_default_args()\n    args.task = \'multiplex_link_prediction\'\n    args.dataset = \'youtube\'\n    args.model = \'prone\'\n    args.eval_type = [\'1\']\n    dataset = build_dataset(args)\n    args.step = 5\n    args.theta = 0.5\n    args.mu = 0.2\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'ROC_AUC\'] >= 0 and ret[\'ROC_AUC\'] <= 1\n\nif __name__ == ""__main__"":\n    test_gatne_amazon()\n    test_gatne_twitter()\n    test_prone_amazon()\n    test_prone_youtube()\n'"
tests/tasks/test_multiplex_node_classification.py,0,"b'from cogdl.tasks import build_task\nfrom cogdl.datasets import build_dataset\nfrom cogdl.models import build_model\nfrom cogdl.utils import build_args_from_dict\n\ndef get_default_args():\n    default_dict = {\'hidden_size\': 16,\n                    \'cpu\': True,\n                    \'enhance\': False,\n                    \'save_dir\': ""."",}\n    return build_args_from_dict(default_dict)\n\ndef test_metapath2vec_gtn_acm():\n    args = get_default_args()\n    args.task = \'multiplex_node_classification\'\n    args.dataset = \'gtn-acm\'\n    args.model = \'metapath2vec\'\n    dataset = build_dataset(args)\n    args.walk_length = 5\n    args.walk_num = 1\n    args.window_size = 3\n    args.worker = 5\n    args.iteration = 1\n    args.schema = ""No""\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'f1\'] > 0\n    \n\ndef test_metapath2vec_gtn_imdb():\n    args = get_default_args()\n    args.task = \'multiplex_node_classification\'\n    args.dataset = \'gtn-imdb\'\n    args.model = \'metapath2vec\'\n    dataset = build_dataset(args)\n    args.walk_length = 5\n    args.walk_num = 1\n    args.window_size = 3\n    args.worker = 5\n    args.iteration = 1\n    args.schema = ""No""\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'f1\'] > 0\n    \n\ndef test_pte_gtn_imdb():\n    args = get_default_args()\n    args.task = \'multiplex_node_classification\'\n    args.dataset = \'gtn-imdb\'\n    args.model = \'pte\'\n    dataset = build_dataset(args)\n    args.walk_length = 5\n    args.walk_num = 1\n    args.negative = 3\n    args.batch_size = 10\n    args.alpha = 0.025\n    args.order = ""No""\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'f1\'] > 0\n\n\ndef test_pte_gtn_dblp():\n    args = get_default_args()\n    args.task = \'multiplex_node_classification\'\n    args.dataset = \'gtn-dblp\'\n    args.model = \'pte\'\n    dataset = build_dataset(args)\n    args.walk_length = 5\n    args.walk_num = 1\n    args.negative = 3\n    args.batch_size = 10\n    args.alpha = 0.025\n    args.order = ""No""\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'f1\'] > 0\n\ndef test_hin2vec_dblp():\n    args = get_default_args()\n    args.task = \'multiplex_node_classification\'\n    args.dataset = \'gtn-dblp\'\n    args.model = \'hin2vec\'\n    dataset = build_dataset(args)\n    args.walk_length = 5\n    args.walk_num = 1\n    args.negative = 3\n    args.batch_size = 1000\n    args.hop = 2\n    args.epoches = 1\n    args.lr = 0.025\n    args.cpu = True\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'f1\'] > 0\n\nif __name__ == ""__main__"":\n    test_metapath2vec_gtn_acm()\n    test_metapath2vec_gtn_imdb()\n    test_pte_gtn_imdb()\n    test_pte_gtn_dblp()\n    test_hin2vec_dblp()\n'"
tests/tasks/test_node_classification.py,1,"b'import torch\nfrom cogdl import options\nfrom cogdl.tasks import build_task\nfrom cogdl.datasets import build_dataset\nfrom cogdl.models import build_model\nfrom cogdl.utils import build_args_from_dict\n\ndef get_default_args():\n    cuda_available = torch.cuda.is_available()\n    default_dict = {\'hidden_size\': 16,\n                    \'dropout\': 0.5,\n                    \'patience\': 1,\n                    \'max_epoch\': 1,\n                    \'cpu\': not cuda_available,\n                    \'lr\': 0.01,\n                    \'weight_decay\': 5e-4}\n    return build_args_from_dict(default_dict)\n\ndef test_gcn_cora():\n    args = get_default_args()\n    args.task = \'node_classification\'\n    args.dataset = \'cora\'\n    args.model = \'gcn\'\n    dataset = build_dataset(args)\n    args.num_features = dataset.num_features\n    args.num_classes = dataset.num_classes\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'Acc\'] >= 0 and ret[\'Acc\'] <= 1\n\ndef test_gat_cora():\n    args = get_default_args()\n    args.task = \'node_classification\'\n    args.dataset = \'cora\'\n    args.model = \'gat\'\n    dataset = build_dataset(args)\n    args.num_features = dataset.num_features\n    args.num_classes = dataset.num_classes\n    args.alpha = 0.2\n    args.nheads = 8\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'Acc\'] >= 0 and ret[\'Acc\'] <= 1\n\ndef test_mlp_pubmed():\n    args = get_default_args()\n    args.task = \'node_classification\'\n    args.dataset = \'pubmed\'\n    args.model = \'mlp\'\n    dataset = build_dataset(args)\n    args.num_features = dataset.num_features\n    args.num_classes = dataset.num_classes\n    args.num_layers = 2\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'Acc\'] >= 0 and ret[\'Acc\'] <= 1\n\ndef test_mixhop_citeseer():\n    args = get_default_args()\n    args.task = \'node_classification\'\n    args.dataset = \'citeseer\'\n    args.model = \'mixhop\'\n    dataset = build_dataset(args)\n    args.num_features = dataset.num_features\n    args.num_classes = dataset.num_classes\n    args.num_layers = 2\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'Acc\'] >= 0 and ret[\'Acc\'] <= 1\n\ndef test_graphsage_cora():\n    args = get_default_args()\n    args.task = \'node_classification\'\n    args.dataset = \'cora\'\n    args.model = \'graphsage\'\n    dataset = build_dataset(args)\n    args.num_features = dataset.num_features\n    args.num_classes = dataset.num_classes\n    args.num_layers = 2\n    args.hidden_size = [128]\n    args.sample_size = [10, 10]\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'Acc\'] >= 0 and ret[\'Acc\'] <= 1\n\ndef test_pyg_cheb_cora():\n    args = get_default_args()\n    args.task = \'node_classification\'\n    args.dataset = \'cora\'\n    args.model = \'pyg_cheb\'\n    dataset = build_dataset(args)\n    args.num_features = dataset.num_features\n    args.num_classes = dataset.num_classes\n    args.num_layers = 2\n    args.filter_size = 5\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'Acc\'] >= 0 and ret[\'Acc\'] <= 1\n\ndef test_pyg_gcn_cora():\n    args = get_default_args()\n    args.task = \'node_classification\'\n    args.dataset = \'cora\'\n    args.model = \'pyg_gcn\'\n    dataset = build_dataset(args)\n    args.num_features = dataset.num_features\n    args.num_classes = dataset.num_classes\n    args.num_layers = 2\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'Acc\'] >= 0 and ret[\'Acc\'] <= 1\n\ndef test_pyg_gat_cora():\n    args = get_default_args()\n    args.task = \'node_classification\'\n    args.dataset = \'cora\'\n    args.model = \'pyg_gat\'\n    dataset = build_dataset(args)\n    args.num_features = dataset.num_features\n    args.num_classes = dataset.num_classes\n    args.num_heads = 8\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'Acc\'] >= 0 and ret[\'Acc\'] <= 1\n\ndef test_pyg_infomax_cora():\n    args = get_default_args()\n    args.task = \'node_classification\'\n    args.dataset = \'cora\'\n    args.model = \'pyg_infomax\'\n    dataset = build_dataset(args)\n    args.num_features = dataset.num_features\n    args.num_classes = dataset.num_classes\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'Acc\'] >= 0 and ret[\'Acc\'] <= 1\n\ndef test_pyg_unet_cora():\n    args = get_default_args()\n    args.task = \'node_classification\'\n    args.dataset = \'cora\'\n    args.model = \'pyg_unet\'\n    dataset = build_dataset(args)\n    args.num_features = dataset.num_features\n    args.num_classes = dataset.num_classes\n    args.num_layers = 2\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'Acc\'] >= 0 and ret[\'Acc\'] <= 1\n\ndef test_pyg_drgcn_cora():\n    args = get_default_args()\n    args.task = \'node_classification\'\n    args.dataset = \'cora\'\n    args.model = \'drgcn\'\n    dataset = build_dataset(args)\n    args.num_features = dataset.num_features\n    args.num_classes = dataset.num_classes\n    args.num_layers = 2\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'Acc\'] >= 0 and ret[\'Acc\'] <= 1\n\ndef test_pyg_drgat_cora():\n    args = get_default_args()\n    args.task = \'node_classification\'\n    args.dataset = \'cora\'\n    args.model = \'drgat\'\n    dataset = build_dataset(args)\n    args.num_features = dataset.num_features\n    args.num_classes = dataset.num_classes\n    args.num_heads = 8\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'Acc\'] >= 0 and ret[\'Acc\'] <= 1\n\nif __name__ == ""__main__"":\n    test_gcn_cora()\n    test_gat_cora()\n    test_mlp_pubmed()\n    test_mixhop_citeseer()\n    test_graphsage_cora()\n    test_pyg_cheb_cora()\n    test_pyg_gcn_cora()\n    test_pyg_gat_cora()\n    test_pyg_infomax_cora()\n    test_pyg_unet_cora()\n    test_pyg_drgcn_cora()\n    test_pyg_drgat_cora()\n'"
tests/tasks/test_node_classification_sampling.py,1,"b'import torch\nfrom cogdl import options\nfrom cogdl.tasks import build_task\nfrom cogdl.datasets import build_dataset\nfrom cogdl.models import build_model\nfrom cogdl.utils import build_args_from_dict\n\ndef get_default_args():\n    cuda_available = torch.cuda.is_available()\n    default_dict = {\'hidden_size\': 16,\n                    \'dropout\': 0.5,\n                    \'patience\': 1,\n                    \'max_epoch\': 1,\n                    \'batch_size\': 20,\n                    \'cpu\': not cuda_available,\n                    \'lr\': 0.01,\n                    \'weight_decay\': 5e-4}\n    return build_args_from_dict(default_dict)\n\ndef test_fastgcn_cora():\n    args = get_default_args()\n    args.task = \'node_classification_sampling\'\n    args.dataset = \'cora\'\n    args.model = \'fastgcn\'\n    dataset = build_dataset(args)\n    args.num_features = dataset.num_features\n    args.num_classes = dataset.num_classes\n    args.num_layers = 3\n    args.sample_size = [512,256,256]\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'Acc\'] >= 0 and ret[\'Acc\'] <= 1\n\ndef test_asgcn_cora():\n    args = get_default_args()\n    args.task = \'node_classification_sampling\'\n    args.dataset = \'cora\'\n    args.model = \'asgcn\'\n    dataset = build_dataset(args)\n    args.num_features = dataset.num_features\n    args.num_classes = dataset.num_classes\n    args.num_layers = 3\n    args.sample_size = [64,64,32]\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'Acc\'] >= 0 and ret[\'Acc\'] <= 1\n\nif __name__ == ""__main__"":\n    test_fastgcn_cora()\n    test_asgcn_cora()\n'"
tests/tasks/test_unsupervised_graph_classification.py,1,"b'import torch\nimport argparse\n\nfrom cogdl import options\nfrom cogdl.tasks import build_task\nfrom cogdl.datasets import build_dataset\nfrom cogdl.models import build_model\nfrom cogdl.utils import build_args_from_dict\n\n\ndef accuracy_check(x):\n    for _, value in x.items():\n        assert value > 0\n\n\ndef get_default_args():\n    cuda_available = torch.cuda.is_available()\n    default_dict = {\'task\': \'unsupervised_graph_classification\',\n                    \'gamma\': 0.5,\n                    \'device_id\': [0 if cuda_available else \'cpu\'],\n                    \'num_shuffle\': 1,\n                    \'save_dir\': \'.\',\n                    \'dropout\': 0.5,\n                    \'patience\': 1,\n                    \'epoch\': 2,\n                    \'cpu\': not cuda_available,\n                    \'lr\': 0.001,\n                    \'weight_decay\': 5e-4}\n    return build_args_from_dict(default_dict)\n\ndef add_infograp_args(args):\n    args.hidden_size = 64\n    args.batch_size = 20\n    args.target = 0\n    args.train_num = 5000\n    args.num_layers = 3\n    args.unsup = True\n    args.epoch = 3\n    args.nn = True\n    args.lr = 0.0001\n    args.train_ratio = 0.7\n    args.test_ratio = 0.1\n    args.model = \'infograph\'\n    args.degree_feature = False\n    return args\n\n\ndef add_graph2vec_args(args):\n    args.hidden_size = 128\n    args.window_size = 0\n    args.min_count = 5\n    args.dm = 0\n    args.sampling = 0.0001\n    args.iteration = 2\n    args.epoch = 4\n    args.nn = False\n    args.lr = 0.001\n    args.model = \'graph2vec\'\n    args.degree_feature = False\n    return args\n\n\ndef add_dgk_args(args):\n    args.hidden_size = 128\n    args.window_size = 2\n    args.min_count = 5\n    args.sampling = 0.0001\n    args.iteration = 2\n    args.epoch = 4\n    args.nn = False\n    args.alpha = 0.01\n    args.model = \'dgk\'\n    args.degree_feature = False\n    return args    \n    \n\ndef test_infograph_proteins():\n    args = get_default_args()\n    args = add_infograp_args(args)\n    args.dataset = \'proteins\'\n    task = build_task(args)\n    ret = task.train()\n    accuracy_check(ret)\n\n\n# def test_infograph_collab():\n#     args = get_default_args()\n#     args = add_infograp_args(args)\n#     args.dataset = \'collab\'\n#     task = build_task(args)\n#     ret = task.train()\n#     accuracy_check(ret)\n\n\ndef test_infograph_imdb_binary():\n    args = get_default_args()\n    args = add_infograp_args(args)\n    args.dataset = \'imdb-b\'\n    args.degree_feature = True\n    task = build_task(args)\n    ret = task.train()\n    accuracy_check(ret)\n\n\ndef test_infograph_mutag():\n    args = get_default_args()\n    args = add_infograp_args(args)\n    args.dataset = \'mutag\'\n    task = build_task(args)\n    ret = task.train()\n    accuracy_check(ret)\n\n\ndef test_graph2vec_mutag():\n    args = get_default_args()\n    args = add_graph2vec_args(args)\n    args.dataset = \'mutag\'\n    print(args)\n    task = build_task(args)\n    ret = task.train()\n    accuracy_check(ret)\n\n\ndef test_graph2vec_proteins():\n    args = get_default_args()\n    args = add_graph2vec_args(args)\n    args.dataset = \'proteins\'\n    print(args)\n    task = build_task(args)\n    ret = task.train()\n    accuracy_check(ret)\n\n\ndef test_dgk_mutag():\n    args = get_default_args()\n    args = add_dgk_args(args)\n    args.dataset = \'mutag\'\n    print(args)\n    task = build_task(args)\n    ret = task.train()\n    accuracy_check(ret)\n\n\ndef test_dgk_proteins():\n    args = get_default_args()\n    args = add_dgk_args(args)\n    args.dataset = \'proteins\'\n    print(args)\n    task = build_task(args)\n    ret = task.train()\n    accuracy_check(ret)\n    \n\nif __name__ == ""__main__"":    \n    test_graph2vec_mutag()\n    test_graph2vec_proteins()\n\n    test_infograph_mutag()\n    test_infograph_imdb_binary()\n    test_infograph_proteins()\n    \n    test_dgk_mutag()\n    test_dgk_proteins()\n    \n\n'"
tests/tasks/test_unsupervised_node_classification.py,0,"b'from cogdl.tasks import build_task\nfrom cogdl.datasets import build_dataset\nfrom cogdl.models import build_model\nfrom cogdl.utils import build_args_from_dict\n\ndef get_default_args():\n    default_dict = {\'hidden_size\': 16,\n                    \'num_shuffle\': 1,\n                    \'cpu\': True,\n                    \'enhance\': False,\n                    \'save_dir\': ""."",}\n    return build_args_from_dict(default_dict)\n\ndef test_deepwalk_wikipedia():\n    args = get_default_args()\n    args.task = \'unsupervised_node_classification\'\n    args.dataset = \'wikipedia\'\n    args.model = \'deepwalk\'\n    dataset = build_dataset(args)\n    args.walk_length = 5\n    args.walk_num = 1\n    args.window_size = 3\n    args.worker = 5\n    args.iteration = 1\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'Micro-F1 0.9\'] > 0\n\n\ndef test_line_ppi():\n    args = get_default_args()\n    args.task = \'unsupervised_node_classification\'\n    args.dataset = \'ppi\'\n    args.model = \'line\'\n    dataset = build_dataset(args)\n    args.walk_length = 1\n    args.walk_num = 1\n    args.negative = 3\n    args.batch_size = 20\n    args.alpha = 0.025\n    args.order = 1\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'Micro-F1 0.9\'] > 0\n\ndef test_node2vec_ppi():\n    args = get_default_args()\n    args.task = \'unsupervised_node_classification\'\n    args.dataset = \'ppi\'\n    args.model = \'node2vec\'\n    dataset = build_dataset(args)\n    args.walk_length = 5\n    args.walk_num = 1\n    args.window_size = 3\n    args.worker = 5\n    args.iteration = 1\n    args.p = 1.0\n    args.q = 1.0\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'Micro-F1 0.9\'] > 0\n\ndef test_hope_ppi():\n    args = get_default_args()\n    args.task = \'unsupervised_node_classification\'\n    args.dataset = \'ppi\'\n    args.model = \'hope\'\n    dataset = build_dataset(args)\n    args.beta = 0.001\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'Micro-F1 0.9\'] > 0    \n\n\ndef test_grarep_ppi():\n    args = get_default_args()\n    args.task = \'unsupervised_node_classification\'\n    args.dataset = \'ppi\'\n    args.model = \'grarep\'\n    dataset = build_dataset(args)\n    args.step = 1\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'Micro-F1 0.9\'] > 0   \n    \ndef test_netmf_ppi():\n    args = get_default_args()\n    args.task = \'unsupervised_node_classification\'\n    args.dataset = \'ppi\'\n    args.model = \'netmf\'\n    dataset = build_dataset(args)\n    args.window_size = 2\n    args.rank = 32\n    args.negative = 3\n    args.is_large = False\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'Micro-F1 0.9\'] > 0  \n\ndef test_netsmf_ppi():\n    args = get_default_args()\n    args.task = \'unsupervised_node_classification\'\n    args.dataset = \'ppi\'\n    args.model = \'netsmf\'\n    dataset = build_dataset(args)\n    args.window_size = 3\n    args.negative = 1\n    args.num_round = 2\n    args.worker = 5\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'Micro-F1 0.9\'] > 0  \n\n\ndef test_prone_blogcatalog():\n    args = get_default_args()\n    args.task = \'unsupervised_node_classification\'\n    args.dataset = \'blogcatalog\'\n    args.model = \'prone\'\n    dataset = build_dataset(args)\n    args.step = 5\n    args.theta = 0.5\n    args.mu = 0.2\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'Micro-F1 0.9\'] > 0\n\ndef test_sdne_ppi():\n    args = get_default_args()\n    args.task = \'unsupervised_node_classification\'\n    args.dataset = \'ppi\'\n    args.model = \'sdne\'\n    dataset = build_dataset(args)\n    args.hidden_size1 = 100\n    args.hidden_size2 = 16\n    args.droput = 0.2\n    args.alpha = 0.01\n    args.beta = 5\n    args.nu1 = 1e-4\n    args.nu2 = 1e-3\n    args.max_epoch = 1\n    args.lr = 0.001\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'Micro-F1 0.9\'] > 0\n\ndef test_dngr_ppi():\n    args = get_default_args()\n    args.task = \'unsupervised_node_classification\'\n    args.dataset = \'ppi\'\n    args.model = \'dngr\'\n    dataset = build_dataset(args)\n    args.hidden_size1 = 100\n    args.hidden_size2 = 16\n    args.noise = 0.2\n    args.alpha = 0.01\n    args.step = 3\n    args.max_epoch = 1\n    args.lr = 0.001\n    model = build_model(args)\n    task = build_task(args)\n    ret = task.train()\n    assert ret[\'Micro-F1 0.9\'] > 0\n    \n\nif __name__ == ""__main__"":\n    test_deepwalk_wikipedia()\n    test_line_ppi()\n    test_node2vec_ppi()\n    test_hope_ppi()\n    test_grarep_ppi()\n    test_netmf_ppi()\n    test_netsmf_ppi()\n    test_prone_blogcatalog()\n    test_sdne_ppi()\n    test_dngr_ppi()\n'"
cogdl/models/emb/__init__.py,0,b''
cogdl/models/emb/deepwalk.py,0,"b'import numpy as np\nimport networkx as nx\nfrom gensim.models import Word2Vec, KeyedVectors\nimport random\nfrom .. import BaseModel, register_model\n\n\n@register_model(""deepwalk"")\nclass DeepWalk(BaseModel):\n    r""""""The DeepWalk model from the `""DeepWalk: Online Learning of Social Representations""\n    <https://arxiv.org/abs/1403.6652>`_ paper\n    \n    Args:\n        hidden_size (int) : The dimension of node representation.\n        walk_length (int) : The walk length.\n        walk_num (int) : The number of walks to sample for each node.\n        window_size (int) : The actual context size which is considered in language model.\n        worker (int) : The number of workers for word2vec.\n        iteration (int) : The number of training iteration in word2vec.\n    """"""\n    \n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(\'--walk-length\', type=int, default=80,\n                            help=\'Length of walk per source. Default is 80.\')\n        parser.add_argument(\'--walk-num\', type=int, default=40,\n                            help=\'Number of walks per source. Default is 40.\')\n        parser.add_argument(\'--window-size\', type=int, default=5,\n                            help=\'Window size of skip-gram model. Default is 5.\')\n        parser.add_argument(\'--worker\', type=int, default=10,\n                            help=\'Number of parallel workers. Default is 10.\')\n        parser.add_argument(\'--iteration\', type=int, default=10,\n                            help=\'Number of iterations. Default is 10.\')\n        # fmt: on\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(\n            args.hidden_size,\n            args.walk_length,\n            args.walk_num,\n            args.window_size,\n            args.worker,\n            args.iteration,\n        )\n\n    def __init__(\n        self, dimension, walk_length, walk_num, window_size, worker, iteration\n    ):\n        super(DeepWalk, self).__init__()\n        self.dimension = dimension\n        self.walk_length = walk_length\n        self.walk_num = walk_num\n        self.window_size = window_size\n        self.worker = worker\n        self.iteration = iteration\n\n    def train(self, G):\n        self.G = G\n        walks = self._simulate_walks(self.walk_length, self.walk_num)\n        walks = [[str(node) for node in walk] for walk in walks]\n        model = Word2Vec(\n            walks,\n            size=self.dimension,\n            window=self.window_size,\n            min_count=0,\n            sg=1,\n            workers=self.worker,\n            iter=self.iteration,\n        )\n        id2node = dict([(vid, node) for vid, node in enumerate(G.nodes())])\n        embeddings = np.asarray([model.wv[str(id2node[i])] for i in range(len(id2node))])\n        return embeddings\n\n    def _walk(self, start_node, walk_length):\n        # Simulate a random walk starting from start node.\n        walk = [start_node]\n        while len(walk) < walk_length:\n            cur = walk[-1]\n            cur_nbrs = list(self.G.neighbors(cur))\n            if len(cur_nbrs) == 0:\n                break\n            k = int(np.floor(np.random.rand() * len(cur_nbrs)))\n            walk.append(cur_nbrs[k])\n        return walk\n\n    def _simulate_walks(self, walk_length, num_walks):\n        # Repeatedly simulate random walks from each node.\n        G = self.G\n        walks = []\n        nodes = list(G.nodes())\n        print(""node number:"", len(nodes))\n        for walk_iter in range(num_walks):\n            print(str(walk_iter + 1), ""/"", str(num_walks))\n            random.shuffle(nodes)\n            for node in nodes:\n                walks.append(self._walk(node, walk_length))\n        return walks\n'"
cogdl/models/emb/dgk.py,0,"b'import hashlib\nfrom joblib import Parallel, delayed\nimport networkx as nx\nimport numpy as np\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom gensim.models.word2vec import Word2Vec\nfrom .. import BaseModel, register_model\n\n\n@register_model(""dgk"")\nclass DeepGraphKernel(BaseModel):\n    r""""""The Hin2vec model from the `""Deep Graph Kernels""\n    <https://dl.acm.org/citation.cfm?id=2783417&CFID=763322570&CFTOKEN=93890155>`_ paper.\n    \n    Args:\n        hidden_size (int) : The dimension of node representation.\n        min_count (int) : Parameter in word2vec.\n        window (int) : The actual context size which is considered in language model.\n        sampling_rate (float) : Parameter in word2vec.\n        iteration (int) : The number of iteration in WL method.\n        epoch (int) : The number of training iteration.\n        alpha (float) : The learning rate of word2vec.\n    """"""\n    @staticmethod\n    def add_args(parser):\n        parser.add_argument(""--hidden-size"", type=int, default=128)\n        parser.add_argument(""--window_size"", type=int, default=5)\n        parser.add_argument(""--min-count"", type=int, default=1)\n        parser.add_argument(""--sampling"", type=float, default=0.0001)\n        parser.add_argument(""--iteration"", type=int, default=2)\n        parser.add_argument(""--epoch"", type=int, default=20)\n        parser.add_argument(""--alpha"", type=float, default=0.01)\n\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(\n            args.hidden_size,\n            args.min_count,\n            args.window_size,\n            args.sampling,\n            args.iteration,\n            args.epoch,\n            args.alpha\n        )\n\n    @staticmethod\n    def feature_extractor(data, rounds, name):\n        graph = nx.from_edgelist(np.array(data.edge_index.T.cpu(), dtype=int))\n        if data.x is not None:\n            feature = {int(key): str(val.argmax(axis=0)) for key, val in enumerate(np.array(data.x.cpu()))}\n        else:\n            feature = dict(nx.degree(graph))\n        graph_wl_features = DeepGraphKernel.wl_iterations(graph, feature, rounds)\n        return graph_wl_features\n\n    @staticmethod\n    def wl_iterations(graph, features, rounds):\n        #TODO: solve hash and number\n        nodes = graph.nodes\n        wl_features = [str(val) for _, val in features.items()]\n        for i in range(rounds):\n            new_feats = {}\n            for node in nodes:\n                neighbors = graph.neighbors(node)\n                neigh_feats = [features[x] for x in neighbors]\n                neigh_feats = [features[node]] + sorted(neigh_feats)\n                hash_feat = hashlib.md5(""_"".join(neigh_feats).encode())\n                hash_feat = hash_feat.hexdigest()\n                new_feats[node] = hash_feat\n            wl_features = wl_features + list(new_feats.values())\n            features = new_feats\n        return wl_features\n\n    def __init__(self, hidden_dim, min_count, window_size, sampling_rate, rounds, epoch, alpha, n_workers=4):\n        super(DeepGraphKernel, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.min_count = min_count\n        self.window = window_size\n        self.sampling_rate = sampling_rate\n        self.n_workers = n_workers\n        self.rounds = rounds\n        self.model = None\n        self.gl_collections = None\n        self.epoch = epoch\n        self.alpha = alpha\n\n    def forward(self, graphs, **kwargs):\n        if self.gl_collections is None:\n            self.gl_collections = Parallel(n_jobs=self.n_workers)(\n                delayed(DeepGraphKernel.feature_extractor)(graph, self.rounds, str(i)) for i, graph in enumerate(graphs)\n            )\n        \n        model = Word2Vec(\n            self.gl_collections,\n            size=self.hidden_dim,\n            window=self.window,\n            min_count=self.min_count,\n            sample=self.sampling_rate,\n            workers=self.n_workers,\n            iter=self.epoch,\n            alpha=self.alpha\n        )\n        vectors = np.asarray([model.wv[str(node)] for node in model.wv.index2word])\n        S = vectors.dot(vectors.T)        \n        node2id = dict(zip(model.wv.index2word, range(len(model.wv.index2word))))\n        \n        num_graph, size_vocab = len(graphs), len(node2id)\n        norm_prob = np.zeros((num_graph, size_vocab))\n        for i, gls in enumerate(self.gl_collections):\n            for gl in gls:\n                if gl in node2id:\n                    norm_prob[i, node2id[gl]] += 1\n            # norm_prob[i] /= sum(norm_prob[i])\n        embedding = norm_prob.dot(S)\n        return embedding, None\n\n    def save_embedding(self, output_path):\n        self.model.wv.save(""model.wv"")\n        self.model.wv.save_word2vec_format(""model.emb"")\n'"
cogdl/models/emb/dngr.py,5,"b'import time\n\nimport networkx as nx\nimport numpy as np\nimport scipy.sparse as sp\nfrom sklearn import preprocessing\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\nfrom .. import BaseModel, register_model\n\n\nclass DNGR_layer(nn.Module):\n    def __init__(self, num_node, hidden_size1, hidden_size2):\n        super(DNGR_layer, self).__init__()\n        self.num_node = num_node\n        self.hidden_size1 = hidden_size1\n        self.hidden_size2 = hidden_size2\n\n        self.encoder = nn.Sequential(\n            nn.Linear(self.num_node, self.hidden_size1),\n            nn.Tanh(),\n            nn.Linear(self.hidden_size1, self.hidden_size2),\n            nn.Tanh(),\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(self.hidden_size2, self.hidden_size1),\n            nn.Tanh(),\n            nn.Linear(self.hidden_size1, self.num_node),\n            nn.Tanh(),\n        )\n\n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return encoded, decoded    \n\n\n@register_model(""dngr"")\nclass DNGR(BaseModel):\n    r""""""The DNGR model from the `""Deep Neural Networks for Learning Graph Representations""\n    <https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12423/11715>`_ paper\n    \n    Args:\n        hidden_size1 (int) : The size of the first hidden layer.\n        hidden_size2 (int) : The size of the second hidden layer.\n        noise (float) : Denoise rate of DAE.\n        alpha (float) : Parameter in DNGR.\n        step (int) : The max step in random surfing.\n        max_epoch (int) : The max epoches in training step.\n        lr (float) : Learning rate in DNGR.\n    """"""\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(""--hidden-size1"", type=int, default=1000, help=""Hidden size in first layer of Auto-Encoder"")\n        parser.add_argument(""--hidden-size2"", type=int, default=128, help=""Hidden size in second layer of Auto-Encoder"")\n        parser.add_argument(""--noise"", type=float, default=0.2, help=""denoise rate of DAE"")\n        parser.add_argument(""--alpha"", type=float, default=0.98, help=""alhpa is a hyperparameter in DNGR"")\n        parser.add_argument(""--step"", type=int, default=10, help=""step is a hyperparameter in DNGR"")\n\n        # fmt: on\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(args.hidden_size1, args.hidden_size2, args.noise, args.alpha, args.step, args.max_epoch, args.lr, args.cpu)\n\n    def __init__(self, hidden_size1, hidden_size2, noise, alpha, step, max_epoch, lr, cpu):\n        super(DNGR, self).__init__()\n        self.hidden_size1 = hidden_size1\n        self.hidden_size2 = hidden_size2\n        self.noise = noise\n        self.alpha = alpha\n        self.step = step\n        self.max_epoch = max_epoch\n        self.lr = lr\n        self.cpu = cpu\n        self.device = torch.device(\'cpu\' if self.cpu else \'cuda\')\n   \n    def scale_matrix(self, mat):\n        mat = mat - np.diag(np.diag(mat))\n        D_inv = np.diagflat(np.reciprocal(np.sum(mat, axis=0)))\n        mat = np.dot(D_inv, mat)\n        return mat\n \n    def random_surfing(self, adj_matrix):\n        # Random Surfing\n\t    adj_matrix = self.scale_matrix(adj_matrix)\n\t    P0 = np.eye(self.num_node, dtype=\'float32\')\n\t    M = np.zeros((self.num_node, self.num_node), dtype=\'float32\')\n\t    P = np.eye(self.num_node, dtype=\'float32\')\n\t    for i in range(0, self.step):\n\t\t    P = self.alpha * np.dot(P, adj_matrix) + (1 - self.alpha) * P0\n\t\t    M = M + P\n\t    return M\n   \n    def get_ppmi_matrix(self, mat):\n        # Get Positive Pairwise Mutual Information(PPMI) matrix\n        mat = self.random_surfing(mat)\n        M = self.scale_matrix(mat)\n        col_s = np.sum(M, axis=0).reshape(1, self.num_node)\n        row_s = np.sum(M, axis=1).reshape(self.num_node, 1)\n        D = np.sum(col_s)\n        rowcol_s = np.dot(row_s, col_s)\n        PPMI = np.log(np.divide(D * M, rowcol_s))\n        \n        PPMI[np.isnan(PPMI)] = 0.0\n        PPMI[np.isinf(PPMI)] = 0.0\n        PPMI[np.isneginf(PPMI)] = 0.0\n        PPMI[PPMI < 0] = 0.0\n        return PPMI\n    \n    def get_denoised_matrix(self, mat):\n        return mat * (np.random.random(mat.shape) > self.noise)\n   \n    def get_emb(self, matrix):\n        ut, s, _ = sp.linalg.svds(matrix, self.hidden_size2)\n        emb_matrix = ut * np.sqrt(s)\n        emb_matrix = preprocessing.normalize(emb_matrix, ""l2"")\n        return emb_matrix\n   \n    def train(self, G):\n        self.num_node = G.number_of_nodes()\n        A = nx.adjacency_matrix(G).todense()\n        PPMI = self.get_ppmi_matrix(A)\n        print(""PPMI matrix compute done"")\n        # return self.get_emb(PPMI)\n        \n        input_mat = torch.from_numpy(self.get_denoised_matrix(PPMI).astype(np.float32))\n        model = DNGR_layer(self.num_node, self.hidden_size1, self.hidden_size2)\n        \n        input_mat = input_mat.to(self.device)\n        model = model.to(self.device)\n        \n        opt = torch.optim.Adam(model.parameters(), lr=self.lr)\n        loss_func = nn.MSELoss()\n\n        epoch_iter = tqdm(range(self.max_epoch))\n        for epoch in epoch_iter:            \n            opt.zero_grad()\n            encoded, decoded = model.forward(input_mat)\n            Loss = loss_func(decoded, input_mat)\n            Loss.backward()\n            epoch_iter.set_description(\n                f""Epoch: {epoch:03d},  Loss: {Loss:.8f}""\n            )\n            opt.step()\n        embedding, _ = model.forward(input_mat)\n        return embedding.detach().cpu().numpy()\n'"
cogdl/models/emb/gatne.py,28,"b'import numpy as np\nimport networkx as nx\nfrom collections import defaultdict\nfrom gensim.models.keyedvectors import Vocab\nfrom six import iteritems\nimport random\nimport math\nimport tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.parameter import Parameter\n\nfrom .. import BaseModel, register_model\n\n\n@register_model(""gatne"")\nclass GATNE(BaseModel):\n    r""""""The GATNE model from the `""Representation Learning for Attributed Multiplex Heterogeneous Network""\n    <https://dl.acm.org/doi/10.1145/3292500.3330964>`_ paper\n\n    Args:\n        walk_length (int) : The walk length.\n        walk_num (int) : The number of walks to sample for each node.\n        window_size (int) : The actual context size which is considered in language model.\n        worker (int) : The number of workers for word2vec.\n        epoch (int) : The number of training epochs.\n        batch_size (int) : The size of each training batch.\n        edge_dim (int) : Number of edge embedding dimensions.\n        att_dim (int) : Number of attention dimensions.\n        negative_samples (int) : Negative samples for optimization.\n        neighbor_samples (int) : Neighbor samples for aggregation\n        schema (str) : The metapath schema used in model. Metapaths are splited with "","", \n        while each node type are connected with ""-"" in each metapath. For example:""0-1-0,0-1-2-1-0""\n    """"""\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(\'--walk-length\', type=int, default=10,\n                            help=\'Length of walk per source. Default is 10.\')\n        parser.add_argument(\'--walk-num\', type=int, default=20,\n                            help=\'Number of walks per source. Default is 20.\')\n        parser.add_argument(\'--window-size\', type=int, default=5,\n                            help=\'Window size of skip-gram model. Default is 5.\')\n        parser.add_argument(\'--worker\', type=int, default=10,\n                            help=\'Number of parallel workers. Default is 10.\')\n        parser.add_argument(\'--epoch\', type=int, default=20,\n                            help=\'Number of epoch. Default is 20.\')\n        parser.add_argument(\'--batch-size\', type=int, default=64,\n                            help=\'Number of batch_size. Default is 64.\')\n        parser.add_argument(\'--edge-dim\', type=int, default=10,\n                            help=\'Number of edge embedding dimensions. Default is 10.\')\n        parser.add_argument(\'--att-dim\', type=int, default=20,\n                            help=\'Number of attention dimensions. Default is 20.\')\n        parser.add_argument(\'--negative-samples\', type=int, default=5,\n                            help=\'Negative samples for optimization. Default is 5.\')\n        parser.add_argument(\'--neighbor-samples\', type=int, default=10,\n                            help=\'Neighbor samples for aggregation. Default is 10.\')\n        parser.add_argument(\'--schema\', type=str, default=None,\n                            help=""Input schema for metapath random walk."")\n        # fmt: on\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(\n            args.hidden_size,\n            args.walk_length,\n            args.walk_num,\n            args.window_size,\n            args.worker,\n            args.epoch,\n            args.batch_size,\n            args.edge_dim,\n            args.att_dim,\n            args.negative_samples,\n            args.neighbor_samples,\n            args.schema,\n        )\n\n    def __init__(\n        self,\n        dimension,\n        walk_length,\n        walk_num,\n        window_size,\n        worker,\n        epoch,\n        batch_size,\n        edge_dim,\n        att_dim,\n        negative_samples,\n        neighbor_samples,\n        schema,\n    ):\n        super(GATNE, self).__init__()\n        self.embedding_size = dimension\n        self.walk_length = walk_length\n        self.walk_num = walk_num\n        self.window_size = window_size\n        self.worker = worker\n        self.epochs = epoch\n        self.batch_size = batch_size\n        self.embedding_u_size = edge_dim\n        self.dim_att = att_dim\n        self.num_sampled = negative_samples\n        self.neighbor_samples = neighbor_samples\n        self.schema = schema\n\n        self.multiplicity = True\n\n        self.device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n    def train(self, network_data):\n        all_walks = generate_walks(network_data, self.walk_num, self.walk_length, schema=self.schema)\n        vocab, index2word = generate_vocab(all_walks)\n        train_pairs = generate_pairs(all_walks, vocab)\n\n        edge_types = list(network_data.keys())\n\n        num_nodes = len(index2word)\n        edge_type_count = len(edge_types)\n\n        epochs = self.epochs\n        batch_size = self.batch_size\n        embedding_size = self.embedding_size\n        embedding_u_size = self.embedding_u_size\n        num_sampled = self.num_sampled\n        dim_att = self.dim_att\n        neighbor_samples = self.neighbor_samples\n\n        neighbors = [[[] for __ in range(edge_type_count)] for _ in range(num_nodes)]\n        for r in range(edge_type_count):\n            g = network_data[edge_types[r]]\n            for (x, y) in g:\n                ix = vocab[x].index\n                iy = vocab[y].index\n                neighbors[ix][r].append(iy)\n                neighbors[iy][r].append(ix)\n            for i in range(num_nodes):\n                if len(neighbors[i][r]) == 0:\n                    neighbors[i][r] = [i] * neighbor_samples\n                elif len(neighbors[i][r]) < neighbor_samples:\n                    neighbors[i][r].extend(\n                        list(\n                            np.random.choice(\n                                neighbors[i][r],\n                                size=neighbor_samples - len(neighbors[i][r]),\n                            )\n                        )\n                    )\n                elif len(neighbors[i][r]) > neighbor_samples:\n                    neighbors[i][r] = list(\n                        np.random.choice(neighbors[i][r], size=neighbor_samples)\n                    )\n\n        model = GATNEModel(\n            num_nodes, embedding_size, embedding_u_size, edge_type_count, dim_att\n        )\n        nsloss = NSLoss(num_nodes, num_sampled, embedding_size)\n\n        model.to(self.device)\n        nsloss.to(self.device)\n\n        optimizer = torch.optim.Adam(\n            [{""params"": model.parameters()}, {""params"": nsloss.parameters()}], lr=1e-4\n        )\n\n        for epoch in range(epochs):\n            random.shuffle(train_pairs)\n            batches = get_batches(train_pairs, neighbors, batch_size)\n\n            data_iter = tqdm.tqdm(\n                batches,\n                desc=""epoch %d"" % (epoch),\n                total=(len(train_pairs) + (batch_size - 1)) // batch_size,\n                bar_format=""{l_bar}{r_bar}"",\n            )\n            avg_loss = 0.0\n\n            for i, data in enumerate(data_iter):\n                optimizer.zero_grad()\n                embs = model(\n                    data[0].to(self.device),\n                    data[2].to(self.device),\n                    data[3].to(self.device),\n                )\n                loss = nsloss(data[0].to(self.device), embs, data[1].to(self.device))\n                loss.backward()\n                optimizer.step()\n\n                avg_loss += loss.item()\n\n                if i % 5000 == 0:\n                    post_fix = {\n                        ""epoch"": epoch,\n                        ""iter"": i,\n                        ""avg_loss"": avg_loss / (i + 1),\n                        ""loss"": loss.item(),\n                    }\n                    data_iter.write(str(post_fix))\n\n        final_model = dict(zip(edge_types, [dict() for _ in range(edge_type_count)]))\n        for i in range(num_nodes):\n            train_inputs = torch.tensor([i for _ in range(edge_type_count)]).to(\n                self.device\n            )\n            train_types = torch.tensor(list(range(edge_type_count))).to(self.device)\n            node_neigh = torch.tensor(\n                [neighbors[i] for _ in range(edge_type_count)]\n            ).to(self.device)\n            node_emb = model(train_inputs, train_types, node_neigh)\n            for j in range(edge_type_count):\n                final_model[edge_types[j]][index2word[i]] = (\n                    node_emb[j].cpu().detach().numpy()\n                )\n        return final_model\n\n\nclass GATNEModel(nn.Module):\n    def __init__(\n        self, num_nodes, embedding_size, embedding_u_size, edge_type_count, dim_a\n    ):\n        super(GATNEModel, self).__init__()\n        self.num_nodes = num_nodes\n        self.embedding_size = embedding_size\n        self.embedding_u_size = embedding_u_size\n        self.edge_type_count = edge_type_count\n        self.dim_a = dim_a\n\n        self.node_embeddings = Parameter(torch.FloatTensor(num_nodes, embedding_size))\n        self.node_type_embeddings = Parameter(\n            torch.FloatTensor(num_nodes, edge_type_count, embedding_u_size)\n        )\n        self.trans_weights = Parameter(\n            torch.FloatTensor(edge_type_count, embedding_u_size, embedding_size)\n        )\n        self.trans_weights_s1 = Parameter(\n            torch.FloatTensor(edge_type_count, embedding_u_size, dim_a)\n        )\n        self.trans_weights_s2 = Parameter(torch.FloatTensor(edge_type_count, dim_a, 1))\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        self.node_embeddings.data.uniform_(-1.0, 1.0)\n        self.node_type_embeddings.data.uniform_(-1.0, 1.0)\n        self.trans_weights.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n        self.trans_weights_s1.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n        self.trans_weights_s2.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n\n    def forward(self, train_inputs, train_types, node_neigh):\n        node_embed = self.node_embeddings[train_inputs]\n        node_embed_neighbors = self.node_type_embeddings[node_neigh]\n        node_embed_tmp = torch.cat(\n            [\n                node_embed_neighbors[:, i, :, i, :].unsqueeze(1)\n                for i in range(self.edge_type_count)\n            ],\n            dim=1,\n        )\n        node_type_embed = torch.sum(node_embed_tmp, dim=2)\n\n        trans_w = self.trans_weights[train_types]\n        trans_w_s1 = self.trans_weights_s1[train_types]\n        trans_w_s2 = self.trans_weights_s2[train_types]\n\n        attention = F.softmax(\n            torch.matmul(\n                F.tanh(torch.matmul(node_type_embed, trans_w_s1)), trans_w_s2\n            ).squeeze()\n        ).unsqueeze(1)\n        node_type_embed = torch.matmul(attention, node_type_embed)\n        node_embed = node_embed + torch.matmul(node_type_embed, trans_w).squeeze()\n\n        last_node_embed = F.normalize(node_embed, dim=1)\n\n        return last_node_embed\n\n\nclass NSLoss(nn.Module):\n    def __init__(self, num_nodes, num_sampled, embedding_size):\n        super(NSLoss, self).__init__()\n        self.num_nodes = num_nodes\n        self.num_sampled = num_sampled\n        self.embedding_size = embedding_size\n        self.weights = Parameter(torch.FloatTensor(num_nodes, embedding_size))\n        self.sample_weights = F.normalize(\n            torch.Tensor(\n                [\n                    (math.log(k + 2) - math.log(k + 1)) / math.log(num_nodes + 1)\n                    for k in range(num_nodes)\n                ]\n            ),\n            dim=0,\n        )\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        self.weights.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n\n    def forward(self, input, embs, label):\n        n = input.shape[0]\n        log_target = torch.log(\n            torch.sigmoid(torch.sum(torch.mul(embs, self.weights[label]), 1))\n        )\n        negs = torch.multinomial(\n            self.sample_weights, self.num_sampled * n, replacement=True\n        ).view(n, self.num_sampled)\n        noise = torch.neg(self.weights[negs])\n        sum_log_sampled = torch.sum(\n            torch.log(torch.sigmoid(torch.bmm(noise, embs.unsqueeze(2)))), 1\n        ).squeeze()\n\n        loss = log_target + sum_log_sampled\n        return -loss.sum() / n\n\n\nclass RWGraph:\n    def __init__(self, nx_G, node_type=None):\n        self.G = nx_G\n        self.node_type = node_type\n\n    def walk(self, walk_length, start, schema=None):\n        # Simulate a random walk starting from start node.\n        G = self.G\n\n        rand = random.Random()\n\n        if schema:\n            schema_items = schema.split(""-"")\n            assert schema_items[0] == schema_items[-1]\n\n        walk = [start]\n        while len(walk) < walk_length:\n            cur = walk[-1]\n            candidates = []\n            for node in G[cur].keys():\n                if (\n                    schema == None\n                    or self.node_type[node]\n                    == schema_items[len(walk) % (len(schema_items) - 1)]\n                ):\n                    candidates.append(node)\n            if candidates:\n                walk.append(rand.choice(candidates))\n            else:\n                break\n        return walk\n\n    def simulate_walks(self, num_walks, walk_length, schema=None):\n        G = self.G\n        walks = []\n        nodes = list(G.nodes())\n        # print(\'Walk iteration:\')\n        if schema is not None:\n            schema_list = schema.split("","")\n        for walk_iter in range(num_walks):\n            random.shuffle(nodes)\n            for node in nodes:\n                if schema is None:\n                    walks.append(self.walk(walk_length=walk_length, start=node))\n                else:\n                    for schema_iter in schema_list:\n                        if schema_iter.split(""-"")[0] == self.node_type[node]:\n                            walks.append(\n                                self.walk(\n                                    walk_length=walk_length,\n                                    start=node,\n                                    schema=schema_iter,\n                                )\n                            )\n\n        return walks\n\n\ndef get_G_from_edges(edges):\n    edge_dict = dict()\n    for edge in edges:\n        edge_key = str(edge[0]) + ""_"" + str(edge[1])\n        if edge_key not in edge_dict:\n            edge_dict[edge_key] = 1\n        else:\n            edge_dict[edge_key] += 1\n    tmp_G = nx.Graph()\n    for edge_key in edge_dict:\n        weight = edge_dict[edge_key]\n        x = int(edge_key.split(""_"")[0])\n        y = int(edge_key.split(""_"")[1])\n        tmp_G.add_edge(x, y)\n        tmp_G[x][y][""weight""] = weight\n    return tmp_G\n\n\ndef generate_pairs(all_walks, vocab, window_size=5):\n    pairs = []\n    skip_window = window_size // 2\n    for layer_id, walks in enumerate(all_walks):\n        for walk in walks:\n            for i in range(len(walk)):\n                for j in range(1, skip_window + 1):\n                    if i - j >= 0:\n                        pairs.append(\n                            (vocab[walk[i]].index, vocab[walk[i - j]].index, layer_id)\n                        )\n                    if i + j < len(walk):\n                        pairs.append(\n                            (vocab[walk[i]].index, vocab[walk[i + j]].index, layer_id)\n                        )\n    return pairs\n\n\ndef generate_vocab(all_walks):\n    index2word = []\n    raw_vocab = defaultdict(int)\n\n    for walks in all_walks:\n        for walk in walks:\n            for word in walk:\n                raw_vocab[word] += 1\n\n    vocab = {}\n    for word, v in iteritems(raw_vocab):\n        vocab[word] = Vocab(count=v, index=len(index2word))\n        index2word.append(word)\n\n    index2word.sort(key=lambda word: vocab[word].count, reverse=True)\n    for i, word in enumerate(index2word):\n        vocab[word].index = i\n\n    return vocab, index2word\n\n\ndef get_batches(pairs, neighbors, batch_size):\n    n_batches = (len(pairs) + (batch_size - 1)) // batch_size\n\n    # result = []\n    for idx in range(n_batches):\n        x, y, t, neigh = [], [], [], []\n        for i in range(batch_size):\n            index = idx * batch_size + i\n            if index >= len(pairs):\n                break\n            x.append(pairs[index][0])\n            y.append(pairs[index][1])\n            t.append(pairs[index][2])\n            neigh.append(neighbors[pairs[index][0]])\n        yield torch.tensor(x), torch.tensor(y), torch.tensor(t), torch.tensor(neigh)\n\n\ndef generate_walks(network_data, num_walks, walk_length, schema=None):\n    if schema is not None:\n        # TODO: node_type = load_node_type(file_name + \'/node_type.txt\')\n        pass\n    else:\n        node_type = None\n\n    all_walks = []\n    for layer_id in network_data:\n        tmp_data = network_data[layer_id]\n        # start to do the random walk on a layer\n\n        layer_walker = RWGraph(get_G_from_edges(tmp_data))\n        layer_walks = layer_walker.simulate_walks(num_walks, walk_length, schema=schema)\n\n        all_walks.append(layer_walks)\n\n    return all_walks\n'"
cogdl/models/emb/graph2vec.py,0,"b'import hashlib\nfrom joblib import Parallel, delayed\nimport networkx as nx\nimport numpy as np\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nimport os\n\nfrom .. import BaseModel, register_model\n\n\n@register_model(""graph2vec"")\nclass Graph2Vec(BaseModel):\n    r""""""The Graph2Vec model from the `""graph2vec: Learning Distributed Representations of Graphs""\n    <https://arxiv.org/abs/1707.05005>`_ paper\n    \n    Args:\n        hidden_size (int) : The dimension of node representation.\n        min_count (int) : Parameter in doc2vec.\n        window_size (int) : The actual context size which is considered in language model.\n        sampling_rate (float) : Parameter in doc2vec.\n        dm (int) :  Parameter in doc2vec.\n        iteration (int) : The number of iteration in WL method.\n        epoch (int) : The max epoches in training step.\n        lr (float) : Learning rate in doc2vec.\n    """"""\n    @staticmethod\n    def add_args(parser):\n        parser.add_argument(""--hidden-size"", type=int, default=128)\n        parser.add_argument(""--window-size"", type=int, default=0)\n        parser.add_argument(""--min-count"", type=int, default=5)\n        parser.add_argument(""--dm"", type=int, default=0)\n        parser.add_argument(""--sampling"", type=float, default=0.0001)\n        parser.add_argument(""--iteration"", type=int, default=2)\n        parser.add_argument(""--epoch"", type=int, default=10)\n        parser.add_argument(""--lr"", type=float, default=0.025)\n\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(\n            args.hidden_size,\n            args.min_count,\n            args.window_size,\n            args.sampling,\n            args.dm,\n            args.iteration,\n            args.epoch,\n            args.lr\n        )\n\n    @staticmethod\n    def feature_extractor(data, rounds, name):\n        graph = nx.from_edgelist(np.array(data.edge_index.T.cpu(), dtype=int))\n        if data.x is not None:\n            feature = {int(key): str(val) for key, val in enumerate(np.array(data.x.cpu()))}\n        else:\n            feature = dict(nx.degree(graph))\n        graph_wl_features = Graph2Vec.wl_iterations(graph, feature, rounds)\n        doc = TaggedDocument(words=graph_wl_features, tags=[""g_"" + name])\n        return doc\n\n    @staticmethod\n    def wl_iterations(graph, features, rounds):\n        #TODO: solve hash and number\n        nodes = graph.nodes\n        wl_features = [str(val) for _, val in features.items()]\n        for i in range(rounds):\n            new_feats = {}\n            for node in nodes:\n                neighbors = graph.neighbors(node)\n                neigh_feats = [features[x] for x in neighbors]\n                neigh_feats = [features[node]] + sorted(neigh_feats)\n                hash_feat = hashlib.md5(""_"".join(neigh_feats).encode())\n                hash_feat = hash_feat.hexdigest()\n                new_feats[node] = hash_feat\n            wl_features = wl_features + list(new_feats.values())\n            features = new_feats\n        return wl_features\n\n    def __init__(self, dimension, min_count, window_size, dm, sampling_rate, rounds, epoch, lr, worker=4):\n        super(Graph2Vec, self).__init__()\n        self.dimension = dimension\n        self.min_count = min_count\n        self.window_size = window_size\n        self.sampling_rate = sampling_rate\n        self.dm = dm\n        self.worker = worker\n        self.rounds = rounds\n        self.model = None\n        self.doc_collections = None\n        self.epoch = epoch\n        self.lr = lr\n\n    def forward(self, graphs, **kwargs):\n        if self.doc_collections is None:\n            self.doc_collections = Parallel(n_jobs=self.worker)(\n                delayed(Graph2Vec.feature_extractor)(graph, self.rounds, str(i)) for i, graph in enumerate(graphs)\n            )\n        self.model = Doc2Vec(\n            self.doc_collections,\n            vector_size=self.dimension,\n            window=self.window_size,\n            min_count=self.min_count,\n            dm=self.dm,\n            sample=self.sampling_rate,\n            workers=self.worker,\n            epochs=self.epoch,\n            alpha=self.lr\n        )\n        vectors = np.array([self.model[""g_""+str(i)] for i in range(len(graphs))])\n        return vectors, None\n\n    def save_embedding(self, output_path):\n        self.model.wv.save(os.path.join(output_path, ""model.wv""))\n        self.model.wv.save_word2vec_format(os.path.join(""model.emb""))\n'"
cogdl/models/emb/grarep.py,0,"b'import numpy as np\nimport networkx as nx\nimport scipy.sparse as sp\nfrom sklearn import preprocessing\nfrom .. import BaseModel, register_model\n\n\n@register_model(""grarep"")\nclass GraRep(BaseModel):\n    r""""""The GraRep model from the `""Grarep: Learning graph representations with global structural information""\n    <http://dl.acm.org/citation.cfm?doid=2806416.2806512>`_ paper.\n    \n    Args:\n        hidden_size (int) : The dimension of node representation.\n        step (int) : The maximum order of transitition probability.\n    """"""\n    \n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(\'--step\', type=int, default=5,\n                            help=\'Number of matrix step in GraRep. Default is 5.\')\n        # fmt: on\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(args.hidden_size, args.step)\n\n    def __init__(self, dimension, step):\n        super(GraRep, self).__init__()\n        self.dimension = dimension\n        self.step = step\n\n    def train(self, G):\n        self.G = G\n        self.num_node = G.number_of_nodes()\n        A = np.asarray(nx.adjacency_matrix(self.G).todense(), dtype=float)\n        A = preprocessing.normalize(A, ""l1"")\n\n        log_beta = np.log(1.0 / self.num_node)\n        A_list = [A]\n        T_list = [sum(A).tolist()]\n        temp = A\n        # calculate A^1, A^2, ... , A^step, respectively\n        for i in range(self.step - 1):\n            temp = temp.dot(A)\n            A_list.append(A)\n            T_list.append(sum(temp).tolist())\n\n        final_emb = np.zeros((self.num_node, 1))\n        for k in range(self.step):\n            for j in range(A.shape[1]):\n                A_list[k][:, j] = (\n                    np.log(A_list[k][:, j] / T_list[k][j] + 1e-20) - log_beta\n                )\n                for i in range(A.shape[0]):\n                    A_list[k][i, j] = max(A_list[k][i, j], 0)\n            # concatenate all k-step representations\n            if k == 0:\n                dimension = self.dimension - int(self.dimension / self.step) * (\n                    self.step - 1\n                )\n                final_emb = self._get_embedding(A_list[k], dimension)\n            else:\n                W = self._get_embedding(A_list[k], self.dimension / self.step)\n                final_emb = np.hstack((final_emb, W))\n\n        self.embeddings = final_emb\n        return self.embeddings\n\n    def _get_embedding(self, matrix, dimension):\n        # get embedding from svd and process normalization for ut\n        ut, s, _ = sp.linalg.svds(matrix, int(dimension))\n        emb_matrix = ut * np.sqrt(s)\n        emb_matrix = preprocessing.normalize(emb_matrix, ""l2"")\n        return emb_matrix\n'"
cogdl/models/emb/hin2vec.py,22,"b'import hashlib\nimport networkx as nx\nimport numpy as np\nimport random\nfrom .. import BaseModel, register_model\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.parameter import Parameter\nfrom tqdm import tqdm\n\n\nclass Hin2vec_layer(nn.Module):\n    def __init__(self, num_node, num_relation, hidden_size, cpu):\n        super(Hin2vec_layer, self).__init__()\n        \n        self.num_node = num_node\n        \n        self.Wx = Parameter(torch.randn(num_node, hidden_size))\n        self.Wr = Parameter(torch.randn(num_relation, hidden_size))\n        \n        self.device = torch.device(\'cpu\' if cpu else \'cuda\')\n        \n        self.X = F.one_hot(torch.arange(num_node), num_node).float().to(self.device)\n        self.R = F.one_hot(torch.arange(num_relation), num_relation).float().to(self.device)\n        self.criterion = nn.CrossEntropyLoss()\n\n    def regulartion(self, embr):\n        clamp_embr = torch.clamp(embr, -6.0, 6.0)\n        sigmod1 = torch.sigmoid(clamp_embr)\n        # return sigmod1\n        re_embr = torch.mul(sigmod1, 1-sigmod1)\n        return re_embr \n\n\n    def forward(self, x, y, r, l):\n        x_one, y_one, r_one = torch.index_select(self.X, 0, x), torch.index_select(self.X, 0, y), torch.index_select(self.R, 0, r)\n        self.embx, self.emby, self.embr = torch.mm(x_one, self.Wx), torch.mm(y_one, self.Wx), torch.mm(r_one, self.Wr)\n        self.re_embr = self.regulartion(self.embr)\n        self.preds = torch.unsqueeze(torch.sigmoid(torch.sum(torch.mul(torch.mul(self.embx, self.emby), self.re_embr), 1)),1)\n        self.logits = torch.cat((self.preds, 1- self.preds), 1)\n        return self.logits, self.criterion(self.logits, l)\n\n\n    def get_emb(self,):\n        x = F.one_hot(torch.arange(0, self.num_node), num_classes=self.num_node).float().to(self.device)\n        return torch.mm(x, self.Wx)\n\n\nclass RWgraph():\n    def __init__(self, nx_G, node_type=None):\n        self.G = nx_G\n        self.node_type = node_type\n\n    def _walk(self, start_node, walk_length):\n        # Simulate a random walk starting from start node.\n        walk = [start_node]\n        while len(walk) < walk_length:\n            cur = walk[-1]\n            cur_nbrs = list(self.G.neighbors(cur))\n            if len(cur_nbrs) == 0:\n                break\n            k = int(np.floor(np.random.rand() * len(cur_nbrs)))\n            walk.append(cur_nbrs[k])\n        return walk\n\n    def _simulate_walks(self, walk_length, num_walks):\n        # Repeatedly simulate random walks from each node.\n        walks = []\n        nodes = list(self.G.nodes())\n        print(""node number:"", len(nodes))\n        for walk_iter in range(num_walks):\n            print(str(walk_iter + 1), ""/"", str(num_walks))\n            random.shuffle(nodes)\n            for node in nodes:\n                walks.append(self._walk(node, walk_length))\n        return walks\n\n    def data_preparation(self, walks, hop, negative):\n        # data preparation via process walks and negative sampling\n        node_type = self.node_type\n        num_node_type = len(set(node_type))\n        type2list = [[] for _ in range(num_node_type)]\n        for node, nt in enumerate(node_type):\n            type2list[nt].append(node)\n        print(""number of type2list"", num_node_type)\n        relation = dict()\n        pairs = []\n        for walk in walks:\n            for i in range(len(walk) - hop):\n                for j in range(1, hop+1):\n                    x, y = walk[i], walk[i+j]\n                    tx, ty = node_type[x], node_type[y]\n                    if x ==y: continue\n                    meta_str = ""-"".join([str(node_type[a]) for a in walk[i:i+j+1]])\n                    if meta_str not in relation:\n                        relation[meta_str] = len(meta_str)\n                    pairs.append([x, y, relation[meta_str], 1])\n                    for k in range(negative):\n                        if random.random() > 0.5:\n                            fx = random.choice(type2list[node_type[x]])\n                            while fx == x:\n                                fx = random.choice(type2list[node_type[x]])\n                            pairs.append([fx, y, relation[meta_str], 0])\n                        else:\n                            fy = random.choice(type2list[node_type[y]])\n                            while fy == y:\n                                fy = random.choice(type2list[node_type[y]])\n                            pairs.append([x, fy, relation[meta_str], 0])                  \n        print(""number of relation"", len(relation))\n        return np.asarray(pairs), relation\n\n\n@register_model(""hin2vec"")\nclass Hin2vec(BaseModel):\n    r""""""The Hin2vec model from the `""HIN2Vec: Explore Meta-paths in Heterogeneous Information Networks for Representation Learning""\n    <https://dl.acm.org/doi/10.1145/3132847.3132953>`_ paper.\n    \n    Args:\n        hidden_size (int) : The dimension of node representation.\n        walk_length (int) : The walk length.\n        walk_num (int) : The number of walks to sample for each node.\n        batch_size (int) : The batch size of training in Hin2vec.\n        hop (int) : The number of hop to construct training samples in Hin2vec.\n        negative (int) : The number of nagative samples for each meta2path pair.\n        epoches (int) : The number of training iteration.\n        lr (float) : The initial learning rate of SGD.\n        cpu (bool) : Use CPU or GPU to train hin2vec.\n    """"""\n    @staticmethod\n    def add_args(parser):\n        parser.add_argument(""--hidden-size"", type=int, default=128)\n        parser.add_argument(\'--walk-length\', type=int, default=80,\n                            help=\'Length of walk per source. Default is 80.\')\n        parser.add_argument(\'--walk-num\', type=int, default=40,\n                            help=\'Number of walks per source. Default is 40.\')\n        parser.add_argument(\'--batch-size\', type=int, default=1000,\n                            help=\'Batch size in SGD training process. Default is 1000.\')\n        parser.add_argument(""--hop"", type=int, default=2)\n        parser.add_argument(""--negative"", type=int, default=5)\n        parser.add_argument(""--epoches"", type=int, default=1)\n\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(\n            args.hidden_size,\n            args.walk_length,\n            args.walk_num,\n            args.batch_size,\n            args.hop,\n            args.negative,\n            args.epoches,\n            args.lr,\n            args.cpu\n        )\n\n    def __init__(self, hidden_dim, walk_length, walk_num, batch_size, hop, negative, epoches, lr, cpu=True):\n        super(Hin2vec, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.walk_length = walk_length\n        self.walk_num = walk_num\n        self.batch_size = batch_size\n        self.hop = hop\n        self.negative = negative\n        self.epoches = epoches\n        self.lr = lr\n        self.cpu = cpu\n        self.device = torch.device(\'cpu\' if self.cpu else \'cuda\')\n        \n\n    def train(self, G, node_type):\n        self.num_node = G.number_of_nodes()\n        rw = RWgraph(G, node_type)\n        walks = rw._simulate_walks(self.walk_length, self.walk_num)\n        pairs, relation = rw.data_preparation(walks, self.hop, self.negative)\n                \n        self.num_relation = len(relation)\n        model = Hin2vec_layer(self.num_node, self.num_relation, self.hidden_dim, self.cpu)\n        self.model = model.to(self.device)\n        \n        num_batch = int(len(pairs) / self.batch_size)\n        print_num_batch = 100\n        print(""number of batch"", num_batch)\n        \n        opt = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n        epoch_iter = tqdm(range(self.epoches))\n        for epoch in epoch_iter:\n            loss_n, pred, label = [], [], []\n            for i in range(num_batch):\n                batch_pairs = torch.from_numpy(pairs[i *self.batch_size :(i+1) * self.batch_size])\n                batch_pairs = batch_pairs.to(self.device)\n                batch_pairs = batch_pairs.T\n                x, y, r, l = batch_pairs[0], batch_pairs[1], batch_pairs[2], batch_pairs[3]\n                opt.zero_grad()\n                logits, loss = self.model.forward(x, y, r, l)\n                \n                loss_n.append(loss.item())\n                label.append(l)\n                pred.extend(logits)\n                if i% print_num_batch ==0 and i!=0:\n                    label = torch.cat(label).to(self.device)\n                    pred = torch.stack(pred, dim=0)\n                    pred = pred.max(1)[1]\n                    acc = pred.eq(label).sum().item() / len(label)\n                    epoch_iter.set_description(\n                    f""Epoch: {i:03d}, Loss: {sum(loss_n)/print_num_batch:.5f}, Acc: {acc:.5f}""\n                    )\n                    loss_n, pred, label = [], [], []\n                    \n                loss.backward()\n                opt.step()\n\n        embedding = self.model.get_emb()\n        return embedding.cpu().detach().numpy()\n\n\n\n\n\n\n'"
cogdl/models/emb/hope.py,0,"b'import numpy as np\nimport networkx as nx\nimport scipy.sparse as sp\nfrom sklearn import preprocessing\nfrom .. import BaseModel, register_model\n\n\n@register_model(""hope"")\nclass HOPE(BaseModel):\n    r""""""The HOPE model from the `""Grarep: Asymmetric transitivity preserving graph embedding""\n    <http://dl.acm.org/citation.cfm?doid=2939672.2939751>`_ paper.\n    \n    Args:\n        hidden_size (int) : The dimension of node representation.\n        beta (float) : Parameter in katz decomposition.\n    """"""\n    \n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(\'--beta\', type=float, default=0.01,\n                            help=\'Parameter of katz for HOPE. Default is 0.01\')\n        # fmt: on\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(args.hidden_size, args.beta)\n\n    def __init__(self, dimension, beta):\n        super(HOPE, self).__init__()\n        self.dimension = dimension\n        self.beta = beta\n\n    def train(self, G):\n        r""""""The author claim that Katz has superior performance in related tasks\n        S_katz = (M_g)^-1 * M_l = (I - beta*A)^-1 * beta*A = (I - beta*A)^-1 * (I - (I -beta*A))\n        = (I - beta*A)^-1 - I\n        """"""\n        self.G = G\n        adj = nx.adjacency_matrix(self.G).todense()\n        n = adj.shape[0]\n        katz_matrix = np.asarray((np.eye(n) - self.beta * np.mat(adj)).I - np.eye(n))\n        self.embeddings = self._get_embedding(katz_matrix, self.dimension)\n        return self.embeddings\n\n    def _get_embedding(self, matrix, dimension):\n        # get embedding from svd and process normalization for ut and vt\n        ut, s, vt = sp.linalg.svds(matrix, int(dimension / 2))\n        emb_matrix_1, emb_matrix_2 = ut, vt.transpose()\n\n        emb_matrix_1 = emb_matrix_1 * np.sqrt(s)\n        emb_matrix_2 = emb_matrix_2 * np.sqrt(s)\n        emb_matrix_1 = preprocessing.normalize(emb_matrix_1, ""l2"")\n        emb_matrix_2 = preprocessing.normalize(emb_matrix_2, ""l2"")\n        features = np.hstack((emb_matrix_1, emb_matrix_2))\n        return features\n'"
cogdl/models/emb/line.py,0,"b'import numpy as np\nimport networkx as nx\nfrom sklearn import preprocessing\nimport time\nfrom tqdm import tqdm\nfrom .. import BaseModel, register_model, alias_draw, alias_setup\n\n\n@register_model(""line"")\nclass LINE(BaseModel):\n    r""""""The LINE model from the `""Line: Large-scale information network embedding""\n    <http://arxiv.org/abs/1503.03578>`_ paper.\n    \n    Args:\n        hidden_size (int) : The dimension of node representation.\n        walk_length (int) : The walk length.\n        walk_num (int) : The number of walks to sample for each node.\n        negative (int) : The number of nagative samples for each edge.\n        batch_size (int) : The batch size of training in LINE.\n        alpha (float) : The initial learning rate of SGD.\n        order (int) : 1 represents perserving 1-st order proximity, 2 represents 2-nd, \n        while 3 means both of them (each of them having dimension/2 node representation).\n    """"""\n    \n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(\'--walk-length\', type=int, default=80,\n                            help=\'Length of walk per source. Default is 80.\')\n        parser.add_argument(\'--walk-num\', type=int, default=20,\n                            help=\'Number of walks per source. Default is 20.\')\n        parser.add_argument(\'--negative\', type=int, default=5,\n                            help=\'Number of negative node in sampling. Default is 5.\')\n        parser.add_argument(\'--batch-size\', type=int, default=1000,\n                            help=\'Batch size in SGD training process. Default is 1000.\')\n        parser.add_argument(\'--alpha\', type=float, default=0.025,\n                            help=\'Initial learning rate of SGD. Default is 0.025.\')\n        parser.add_argument(\'--order\', type=int, default=3,\n                            help=\'Order of proximity in LINE. Default is 3 for 1+2.\')\n        # fmt: on\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(\n            args.hidden_size,\n            args.walk_length,\n            args.walk_num,\n            args.negative,\n            args.batch_size,\n            args.alpha,\n            args.order,\n        )\n\n    def __init__(\n        self, dimension, walk_length, walk_num, negative, batch_size, alpha, order\n    ):\n        self.dimension = dimension\n        self.walk_length = walk_length\n        self.walk_num = walk_num\n        self.negative = negative\n        self.batch_size = batch_size\n        self.init_alpha = alpha\n        self.order = order\n\n    def train(self, G):\n        # run LINE algorithm, 1-order, 2-order or 3(1-order + 2-order)\n        self.G = G\n        self.is_directed = nx.is_directed(self.G)\n        self.num_node = G.number_of_nodes()\n        self.num_edge = G.number_of_edges()\n        self.num_sampling_edge = self.walk_length * self.walk_num * self.num_node\n\n        node2id = dict([(node, vid) for vid, node in enumerate(G.nodes())])\n        self.edges = [[node2id[e[0]], node2id[e[1]]] for e in self.G.edges()]\n        self.edges_prob = np.asarray([G[u][v].get(""weight"", 1.0) for u, v in G.edges()])\n        self.edges_prob /= np.sum(self.edges_prob)\n        self.edges_table, self.edges_prob = alias_setup(self.edges_prob)\n\n        degree_weight = np.asarray([0] * self.num_node)\n        for u, v in G.edges():\n            degree_weight[node2id[u]] += G[u][v].get(""weight"", 1.0)\n            if not self.is_directed:\n                degree_weight[node2id[v]] += G[u][v].get(""weight"", 1.0)\n        self.node_prob = np.power(degree_weight, 0.75)\n        self.node_prob /= np.sum(self.node_prob)\n        self.node_table, self.node_prob = alias_setup(self.node_prob)\n\n        if self.order == 3:\n            self.dimension = int(self.dimension / 2)\n        if self.order == 1 or self.order == 3:\n            print(""train line with 1-order"")\n            print(type(self.dimension))\n            self.emb_vertex = (\n                np.random.random((self.num_node, self.dimension)) - 0.5\n            ) / self.dimension\n            self._train_line(order=1)\n            embedding1 = preprocessing.normalize(self.emb_vertex, ""l2"")\n\n        if self.order == 2 or self.order == 3:\n            print(""train line with 2-order"")\n            self.emb_vertex = (\n                np.random.random((self.num_node, self.dimension)) - 0.5\n            ) / self.dimension\n            self.emb_context = self.emb_vertex\n            self._train_line(order=2)\n            embedding2 = preprocessing.normalize(self.emb_vertex, ""l2"")\n\n        if self.order == 1:\n            self.embeddings = embedding1\n        elif self.order == 2:\n            self.embeddings = embedding2\n        else:\n            print(""concatenate two embedding..."")\n            self.embeddings = np.hstack((embedding1, embedding2))\n        return self.embeddings\n\n    def _update(self, vec_u, vec_v, vec_error, label):\n        # update vetex embedding and vec_error\n        f = 1 / (1 + np.exp(-np.sum(vec_u * vec_v, axis=1)))\n        g = (self.alpha * (label - f)).reshape((len(label), 1))\n        vec_error += g * vec_v\n        vec_v += g * vec_u\n\n    def _train_line(self, order):\n        # train Line model with order\n        self.alpha = self.init_alpha\n        batch_size = self.batch_size\n        t0 = time.time()\n        num_batch = int(self.num_sampling_edge / batch_size)\n        epoch_iter = tqdm(range(num_batch))\n        for b in epoch_iter:\n            if b % 100 == 0:\n                epoch_iter.set_description(\n                    f""Progress: {b *1.0/num_batch * 100:.4f}%, alpha: {self.alpha:.6f}, time: {time.time() - t0:.4f}""\n                )\n                self.alpha = self.init_alpha * max((1 - b * 1.0 / num_batch), 0.0001)\n            u, v = [0] * batch_size, [0] * batch_size\n            for i in range(batch_size):\n                edge_id = alias_draw(self.edges_table, self.edges_prob)\n                u[i], v[i] = self.edges[edge_id]\n                if not self.is_directed and np.random.rand() > 0.5:\n                    v[i], u[i] = self.edges[edge_id]\n\n            vec_error = np.zeros((batch_size, self.dimension))\n            label, target = np.asarray([1 for i in range(batch_size)]), np.asarray(v)\n            for j in range(1 + self.negative):\n                if j != 0:\n                    label = np.asarray([0 for i in range(batch_size)])\n                    for i in range(batch_size):\n                        target[i] = alias_draw(self.node_table, self.node_prob)\n                if order == 1:\n                    self._update(\n                        self.emb_vertex[u], self.emb_vertex[target], vec_error, label\n                    )\n                else:\n                    self._update(\n                        self.emb_vertex[u], self.emb_context[target], vec_error, label\n                    )\n            self.emb_vertex[u] += vec_error\n'"
cogdl/models/emb/metapath2vec.py,0,"b'import numpy as np\nimport networkx as nx\nfrom gensim.models import Word2Vec, KeyedVectors\nimport random\nfrom .. import BaseModel, register_model\n\n\n@register_model(""metapath2vec"")\nclass Metapath2vec(BaseModel):\n    r""""""The Metapath2vec model from the `""metapath2vec: Scalable Representation\n    Learning for Heterogeneous Networks""\n    <https://ericdongyx.github.io/papers/\n    KDD17-dong-chawla-swami-metapath2vec.pdf>`_ paper\n    \n    Args:\n        hidden_size (int) : The dimension of node representation.\n        walk_length (int) : The walk length.\n        walk_num (int) : The number of walks to sample for each node.\n        window_size (int) : The actual context size which is considered in language model.\n        worker (int) : The number of workers for word2vec.\n        iteration (int) : The number of training iteration in word2vec.\n        schema (str) : The metapath schema used in model. Metapaths are splited with "","", \n        while each node type are connected with ""-"" in each metapath. For example:""0-1-0,0-1-2-1-0"".\n    """"""\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(\'--walk-length\', type=int, default=80,\n                            help=\'Length of walk per source. Default is 80.\')\n        parser.add_argument(\'--walk-num\', type=int, default=20,\n                            help=\'Number of walks per source. Default is 20.\')\n        parser.add_argument(\'--window-size\', type=int, default=5,\n                            help=\'Window size of skip-gram model. Default is 5.\')\n        parser.add_argument(\'--worker\', type=int, default=10,\n                            help=\'Number of parallel workers. Default is 10.\')\n        parser.add_argument(\'--iteration\', type=int, default=10,\n                            help=\'Number of iterations. Default is 10.\')\n        parser.add_argument(\'--schema\', type=str, default=""No"",\n                            help=""Input schema for multi-type node representation."")\n        # fmt: on\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(\n            args.hidden_size,\n            args.walk_length,\n            args.walk_num,\n            args.window_size,\n            args.worker,\n            args.iteration,\n            args.schema\n        )\n\n    def __init__(\n        self, dimension, walk_length, walk_num, window_size, worker, iteration, schema\n    ):\n        super(Metapath2vec, self).__init__()\n        self.dimension = dimension\n        self.walk_length = walk_length\n        self.walk_num = walk_num\n        self.window_size = window_size\n        self.worker = worker\n        self.iteration = iteration\n        self.schema = schema\n        self.node_type = None\n\n    def train(self, G, node_type):\n        self.G = G\n        self.node_type = [str(a) for a in node_type]\n        walks = self._simulate_walks(self.walk_length, self.walk_num, self.schema)\n        walks = [[str(node) for node in walk] for walk in walks]\n        model = Word2Vec(\n            walks,\n            size=self.dimension,\n            window=self.window_size,\n            min_count=0,\n            sg=1,\n            workers=self.worker,\n            iter=self.iteration,\n        )\n        id2node = dict([(vid, node) for vid, node in enumerate(G.nodes())])\n        embeddings = np.asarray([model.wv[str(id2node[i])] for i in range(len(id2node))])\n        return embeddings\n\n    def _walk(self, start_node, walk_length, schema=None):\n        # Simulate a random walk starting from start node. \n        # Note that metapaths in schema should be like \'0-1-0\' or \'0-1-2-1-0\'.\n        if schema:\n            schema_items = schema.split(""-"")\n            assert schema_items[0] == schema_items[-1]\n        \n        walk = [start_node]\n        while len(walk) < walk_length:\n            cur = walk[-1]\n            candidates = []\n            for node in list(self.G.neighbors(cur)):\n                if (\n                    schema == None\n                    or self.node_type[node]\n                    == schema_items[len(walk) % (len(schema_items) - 1)]\n                ):\n                    candidates.append(node)\n            if candidates:\n                walk.append(random.choice(candidates))\n            else:\n                break\n        # print(walk)\n        return walk\n\n    def _simulate_walks(self, walk_length, num_walks, schema=""No""):\n        # Repeatedly simulate random walks from each node with metapath schema.\n        G = self.G\n        walks = []\n        nodes = list(G.nodes())\n        if schema != ""No"":\n            schema_list = schema.split("","")\n        print(""node number:"", len(nodes))\n        for walk_iter in range(num_walks):\n            random.shuffle(nodes)\n            print(str(walk_iter + 1), ""/"", str(num_walks))\n            for node in nodes:\n                if schema == ""No"":\n                    walks.append(self._walk(node, walk_length))\n                else:\n                    for schema_iter in schema_list:\n                        if schema_iter.split(""-"")[0] == self.node_type[node]:\n                            walks.append(self._walk(node, walk_length, schema_iter))\n        return walks\n'"
cogdl/models/emb/netmf.py,0,"b'import networkx as nx\nimport numpy as np\nimport scipy.sparse as sp\n\nfrom .. import BaseModel, register_model\n\n\n@register_model(""netmf"")\nclass NetMF(BaseModel):\n    r""""""The NetMF model from the `""Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec""\n    <http://arxiv.org/abs/1710.02971>`_ paper.\n    \n    Args:\n        hidden_size (int) : The dimension of node representation.\n        window_size (int) : The actual context size which is considered in language model.\n        rank (int) : The rank in approximate normalized laplacian.\n        negative (int) : The number of nagative samples in negative sampling.\n        is-large (bool) : When window size is large, use approximated deepwalk matrix to decompose.\n    """"""\n    \n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(""--window-size"", type=int, default=5)\n        parser.add_argument(""--rank"", type=int, default=256)\n        parser.add_argument(""--negative"", type=int, default=1)\n        parser.add_argument(\'--is-large\', action=\'store_true\')\n        # fmt: on\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(\n            args.hidden_size, args.window_size, args.rank, args.negative, args.is_large\n        )\n\n    def __init__(self, dimension, window_size, rank, negative, is_large=False):\n        self.dimension = dimension\n        self.window_size = window_size\n        self.rank = rank\n        self.negative = negative\n        self.is_large = is_large\n\n    def train(self, G):\n        A = sp.csr_matrix(nx.adjacency_matrix(G))\n        if not self.is_large:\n            print(""Running NetMF for a small window size..."")\n            deepwalk_matrix = self._compute_deepwalk_matrix(\n                A, window=self.window_size, b=self.negative\n            )\n\n        else:\n            print(""Running NetMF for a large window size..."")\n            vol = float(A.sum())\n            evals, D_rt_invU = self._approximate_normalized_laplacian(\n                A, rank=self.rank, which=""LA""\n            )\n            deepwalk_matrix = self._approximate_deepwalk_matrix(\n                evals, D_rt_invU, window=self.window_size, vol=vol, b=self.negative\n            )\n        # factorize deepwalk matrix with SVD\n        u, s, _ = sp.linalg.svds(deepwalk_matrix, self.dimension)\n        self.embeddings = sp.diags(np.sqrt(s)).dot(u.T).T\n        return self.embeddings\n\n    def _compute_deepwalk_matrix(self, A, window, b):\n        # directly compute deepwalk matrix\n        n = A.shape[0]\n        vol = float(A.sum())\n        L, d_rt = sp.csgraph.laplacian(A, normed=True, return_diag=True)\n        # X = D^{-1/2} A D^{-1/2}\n        X = sp.identity(n) - L\n        S = np.zeros_like(X)\n        X_power = sp.identity(n)\n        for i in range(window):\n            print(""Compute matrix %d-th power"", i + 1)\n            X_power = X_power.dot(X)\n            S += X_power\n        S *= vol / window / b\n        D_rt_inv = sp.diags(d_rt ** -1)\n        M = D_rt_inv.dot(D_rt_inv.dot(S).T).todense()\n        M[M <= 1] = 1\n        Y = np.log(M)\n        return sp.csr_matrix(Y)\n\n    def _approximate_normalized_laplacian(self, A, rank, which=""LA""):\n        # perform eigen-decomposition of D^{-1/2} A D^{-1/2} and keep top rank eigenpairs\n        n = A.shape[0]\n        L, d_rt = sp.csgraph.laplacian(A, normed=True, return_diag=True)\n        # X = D^{-1/2} W D^{-1/2}\n        X = sp.identity(n) - L\n        print(""Eigen decomposition..."")\n        evals, evecs = sp.linalg.eigsh(X, rank, which=which)\n        print(\n            ""Maximum eigenvalue %f, minimum eigenvalue %f"", np.max(evals), np.min(evals)\n        )\n        print(""Computing D^{-1/2}U.."")\n        D_rt_inv = sp.diags(d_rt ** -1)\n        D_rt_invU = D_rt_inv.dot(evecs)\n        return evals, D_rt_invU\n\n    def _deepwalk_filter(self, evals, window):\n        for i in range(len(evals)):\n            x = evals[i]\n            evals[i] = 1.0 if x >= 1 else x * (1 - x ** window) / (1 - x) / window\n        evals = np.maximum(evals, 0)\n        print(\n            ""After filtering, max eigenvalue=%f, min eigenvalue=%f"",\n            np.max(evals),\n            np.min(evals),\n        )\n        return evals\n\n    def _approximate_deepwalk_matrix(self, evals, D_rt_invU, window, vol, b):\n        # approximate deepwalk matrix\n        evals = self._deepwalk_filter(evals, window=window)\n        X = sp.diags(np.sqrt(evals)).dot(D_rt_invU.T).T\n        M = X.dot(X.T) * vol / b\n        M[M <= 1] = 1\n        Y = np.log(M)\n        print(""Computed DeepWalk matrix with %d non-zero elements"", np.count_nonzero(Y))\n        return sp.csr_matrix(Y)\n'"
cogdl/models/emb/netsmf.py,0,"b'import numpy as np\nimport networkx as nx\nimport scipy.sparse as sp\nfrom sklearn import preprocessing\nfrom sklearn.utils.extmath import randomized_svd\nfrom multiprocessing import Pool\nimport time\n\nfrom .. import BaseModel, register_model, alias_draw, alias_setup\n\n\n@register_model(""netsmf"")\nclass NetSMF(BaseModel):\n    r""""""The NetSMF model from the `""NetSMF: Large-Scale Network Embedding as Sparse Matrix Factorization""\n    <http://arxiv.org/abs/1710.02971>`_ paper.\n    \n    Args:\n        hidden_size (int) : The dimension of node representation.\n        window_size (int) : The actual context size which is considered in language model.\n        negative (int) : The number of nagative samples in negative sampling.\n        num_round (int) : The number of round in NetSMF.\n        worker (int) : The number of workers for NetSMF.\n    """"""\n    \n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(\'--window-size\', type=int, default=10,\n                            help=\'Window size of approximate matrix. Default is 10.\')\n        parser.add_argument(\'--negative\', type=int, default=1,\n                            help=\'Number of negative node in sampling. Default is 1.\')\n        parser.add_argument(\'--num-round\', type=int, default=100,\n                            help=\'Number of round in NetSMF. Default is 100.\')\n        parser.add_argument(\'--worker\', type=int, default=10,\n                            help=\'Number of parallel workers. Default is 10.\')\n        # fmt: on\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(\n            args.hidden_size,\n            args.window_size,\n            args.negative,\n            args.num_round,\n            args.worker,\n        )\n\n    def __init__(self, dimension, window_size, negative, num_round, worker):\n        super(NetSMF, self).__init__()\n        self.dimension = dimension\n        self.window_size = window_size\n        self.negative = negative\n        self.worker = worker\n        self.num_round = num_round\n\n    def train(self, G):\n        self.G = G\n        node2id = dict([(node, vid) for vid, node in enumerate(G.nodes())])\n        self.is_directed = nx.is_directed(self.G)\n        self.num_node = self.G.number_of_nodes()\n        self.num_edge = G.number_of_edges()\n        self.edges = [[node2id[e[0]], node2id[e[1]]] for e in self.G.edges()]\n\n        id2node = dict(zip(node2id.values(), node2id.keys()))\n\n        self.num_neigh = np.asarray(\n            [len(list(self.G.neighbors(id2node[i]))) for i in range(self.num_node)]\n        )\n        self.neighbors = [\n            [node2id[v] for v in self.G.neighbors(id2node[i])]\n            for i in range(self.num_node)\n        ]\n        s = time.time()\n        self.alias_nodes = {}\n        self.node_weight = {}\n        for i in range(self.num_node):\n            unnormalized_probs = [\n                G[id2node[i]][nbr].get(""weight"", 1.0) for nbr in G.neighbors(id2node[i])\n            ]\n            norm_const = sum(unnormalized_probs)\n            normalized_probs = [\n                float(u_prob) / norm_const for u_prob in unnormalized_probs\n            ]\n            self.alias_nodes[i] = alias_setup(normalized_probs)\n            self.node_weight[i] = dict(\n                zip(\n                    [node2id[nbr] for nbr in G.neighbors(id2node[i])],\n                    unnormalized_probs,\n                )\n            )\n\n        t = time.time()\n        print(""alias_nodes"", t - s)\n\n        # run netsmf algorithm with multiprocessing and apply randomized svd\n        print(\n            ""number of sample edges "", self.num_round * self.num_edge * self.window_size\n        )\n        print(""random walk start..."")\n        t0 = time.time()\n        results = []\n        pool = Pool(processes=self.worker)\n        for i in range(self.worker):\n            results.append(pool.apply_async(func=self._random_walk_matrix, args=(i,)))\n        pool.close()\n        pool.join()\n        print(""random walk time"", time.time() - t0)\n\n        matrix = sp.lil_matrix((self.num_node, self.num_node))\n        A = sp.csr_matrix(nx.adjacency_matrix(self.G))\n        degree = sp.diags(np.array(A.sum(axis=0))[0], format=""csr"")\n        degree_inv = degree.power(-1)\n\n        t1 = time.time()\n        for res in results:\n            matrix += res.get()\n        print(""number of nzz"", matrix.nnz)\n        t2 = time.time()\n        print(""construct random walk matrix time"", time.time() - t1)\n\n        L = sp.csgraph.laplacian(matrix, normed=False, return_diag=False)\n        M = degree_inv.dot(degree - L).dot(degree_inv)\n        M = M * A.sum() / self.negative\n        M.data[M.data <= 1] = 1\n        M.data = np.log(M.data)\n        print(""construct matrix sparsifier time"", time.time() - t2)\n\n        embedding = self._get_embedding_rand(M)\n        return embedding\n\n    def _get_embedding_rand(self, matrix):\n        # Sparse randomized tSVD for fast embedding\n        t1 = time.time()\n        l = matrix.shape[0]\n        smat = sp.csc_matrix(matrix)\n        print(""svd sparse"", smat.data.shape[0] * 1.0 / l ** 2)\n        U, Sigma, VT = randomized_svd(\n            smat, n_components=self.dimension, n_iter=5, random_state=None\n        )\n        U = U * np.sqrt(Sigma)\n        U = preprocessing.normalize(U, ""l2"")\n        print(""sparsesvd time"", time.time() - t1)\n        return U\n\n    def _path_sampling(self, u, v, r):\n        # sample a r-length path from edge(u, v) and return path end node\n        k = np.random.randint(r) + 1\n        zp, rand_u, rand_v = 1e-20, k - 1, r - k\n        for i in range(rand_u):\n            new_u = self.neighbors[u][\n                alias_draw(self.alias_nodes[u][0], self.alias_nodes[u][1])\n            ]\n            zp += 2.0 / self.node_weight[u][new_u]\n            u = new_u\n        for j in range(rand_v):\n            new_v = self.neighbors[v][\n                alias_draw(self.alias_nodes[v][0], self.alias_nodes[v][1])\n            ]\n            zp += 2.0 / self.node_weight[v][new_v]\n            v = new_v\n        return u, v, zp\n\n    def _random_walk_matrix(self, pid):\n        # construct matrix based on random walk\n        np.random.seed(pid)\n        matrix = sp.lil_matrix((self.num_node, self.num_node))\n        t0 = time.time()\n        for round in range(int(self.num_round / self.worker)):\n            if round % 10 == 0 and pid == 0:\n                print(\n                    ""round %d / %d, time: %lf""\n                    % (round * self.worker, self.num_round, time.time() - t0)\n                )\n            for i in range(self.num_edge):\n                u, v = self.edges[i]\n                if not self.is_directed and np.random.rand() > 0.5:\n                    v, u = self.edges[i]\n                for r in range(1, self.window_size + 1):\n                    u_, v_, zp = self._path_sampling(u, v, r)\n                    matrix[u_, v_] += 2 * r / self.window_size / self.num_round / zp\n        return matrix\n'"
cogdl/models/emb/node2vec.py,0,"b'import numpy as np\nimport networkx as nx\nfrom gensim.models import Word2Vec, KeyedVectors\nimport random\nimport time\nfrom .. import BaseModel, register_model, alias_draw, alias_setup\n\n\n@register_model(""node2vec"")\nclass Node2vec(BaseModel):\n    r""""""The node2vec model from the `""node2vec: Scalable feature learning for networks""\n    <http://dl.acm.org/citation.cfm?doid=2939672.2939754>`_ paper\n    \n    Args:\n        hidden_size (int) : The dimension of node representation.\n        walk_length (int) : The walk length.\n        walk_num (int) : The number of walks to sample for each node.\n        window_size (int) : The actual context size which is considered in language model.\n        worker (int) : The number of workers for word2vec.\n        iteration (int) : The number of training iteration in word2vec.\n        p (float) : Parameter in node2vec.\n        q (float) : Parameter in node2vec.\n    """"""\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(\'--walk-length\', type=int, default=80,\n                            help=\'Length of walk per source. Default is 80.\')\n        parser.add_argument(\'--walk-num\', type=int, default=40,\n                            help=\'Number of walks per source. Default is 40.\')\n        parser.add_argument(\'--window-size\', type=int, default=5,\n                            help=\'Window size of skip-gram model. Default is 5.\')\n        parser.add_argument(\'--worker\', type=int, default=10,\n                            help=\'Number of parallel workers. Default is 10.\')\n        parser.add_argument(\'--iteration\', type=int, default=10,\n                            help=\'Number of iterations. Default is 10.\')\n        parser.add_argument(\'--p\', type=float, default=1.0,\n                            help=\'Parameter in node2vec. Default is 1.0.\')\n        parser.add_argument(\'--q\', type=float, default=1.0,\n                            help=\'Parameter in node2vec. Default is 1.0.\')\n        # fmt: on\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(\n            args.hidden_size,\n            args.walk_length,\n            args.walk_num,\n            args.window_size,\n            args.worker,\n            args.iteration,\n            args.p,\n            args.q,\n        )\n\n    def __init__(\n        self, dimension, walk_length, walk_num, window_size, worker, iteration, p, q\n    ):\n        super(Node2vec, self).__init__()\n        self.dimension = dimension\n        self.walk_length = walk_length\n        self.walk_num = walk_num\n        self.window_size = window_size\n        self.worker = worker\n        self.iteration = iteration\n        self.p = p\n        self.q = q\n\n    def train(self, G):\n        self.G = G\n        is_directed = nx.is_directed(self.G)\n        for i, j in G.edges():\n            G[i][j][""weight""] = G[i][j].get(""weight"", 1.0)\n            if not is_directed:\n                G[j][i][""weight""] = G[j][i].get(""weight"", 1.0)\n        self._preprocess_transition_probs()\n        walks = self._simulate_walks(self.walk_num, self.walk_length)\n        walks = [[str(node) for node in walk] for walk in walks]\n        model = Word2Vec(\n            walks,\n            size=self.dimension,\n            window=self.window_size,\n            min_count=0,\n            sg=1,\n            workers=self.worker,\n            iter=self.iteration,\n        )\n        id2node = dict([(vid, node) for vid, node in enumerate(G.nodes())])\n        self.embeddings = np.asarray(\n            [model.wv[str(id2node[i])] for i in range(len(id2node))]\n        )\n        return self.embeddings\n\n    def _node2vec_walk(self, walk_length, start_node):\n        # Simulate a random walk starting from start node.\n        G = self.G\n        alias_nodes = self.alias_nodes\n        alias_edges = self.alias_edges\n\n        walk = [start_node]\n\n        while len(walk) < walk_length:\n            cur = walk[-1]\n            cur_nbrs = list(G.neighbors(cur))\n            if len(cur_nbrs) > 0:\n                if len(walk) == 1:\n                    walk.append(\n                        cur_nbrs[alias_draw(alias_nodes[cur][0], alias_nodes[cur][1])]\n                    )\n                else:\n                    prev = walk[-2]\n                    next = cur_nbrs[\n                        alias_draw(\n                            alias_edges[(prev, cur)][0], alias_edges[(prev, cur)][1]\n                        )\n                    ]\n                    walk.append(next)\n            else:\n                break\n\n        return walk\n\n    def _simulate_walks(self, num_walks, walk_length):\n        # Repeatedly simulate random walks from each node.\n        G = self.G\n        walks = []\n        nodes = list(G.nodes())\n        print(""Walk iteration:"")\n        for walk_iter in range(num_walks):\n            if walk_iter % 10 == 0:\n                print(str(walk_iter + 1), ""/"", str(num_walks))\n            random.shuffle(nodes)\n            for node in nodes:\n                walks.append(\n                    self._node2vec_walk(walk_length=walk_length, start_node=node)\n                )\n\n        return walks\n\n    def _get_alias_edge(self, src, dst):\n        # Get the alias edge setup lists for a given edge.\n        G = self.G\n        unnormalized_probs = []\n        for dst_nbr in G.neighbors(dst):\n            if dst_nbr == src:\n                unnormalized_probs.append(G[dst][dst_nbr][""weight""] / self.p)\n            elif G.has_edge(dst_nbr, src):\n                unnormalized_probs.append(G[dst][dst_nbr][""weight""])\n            else:\n                unnormalized_probs.append(G[dst][dst_nbr][""weight""] / self.q)\n        norm_const = sum(unnormalized_probs)\n        normalized_probs = [float(u_prob) / norm_const for u_prob in unnormalized_probs]\n\n        return alias_setup(normalized_probs)\n\n    def _preprocess_transition_probs(self):\n        # Preprocessing of transition probabilities for guiding the random walks.\n        G = self.G\n        is_directed = nx.is_directed(self.G)\n\n        print(len(list(G.nodes())))\n        print(len(list(G.edges())))\n\n        s = time.time()\n        alias_nodes = {}\n        for node in G.nodes():\n            unnormalized_probs = [G[node][nbr][""weight""] for nbr in G.neighbors(node)]\n            norm_const = sum(unnormalized_probs)\n            normalized_probs = [\n                float(u_prob) / norm_const for u_prob in unnormalized_probs\n            ]\n            alias_nodes[node] = alias_setup(normalized_probs)\n\n        t = time.time()\n        print(""alias_nodes"", t - s)\n\n        alias_edges = {}\n        s = time.time()\n\n        if is_directed:\n            for edge in G.edges():\n                alias_edges[edge] = self._get_alias_edge(edge[0], edge[1])\n        else:\n            for edge in G.edges():\n                alias_edges[edge] = self._get_alias_edge(edge[0], edge[1])\n                alias_edges[(edge[1], edge[0])] = self._get_alias_edge(edge[1], edge[0])\n\n        t = time.time()\n        print(""alias_edges"", t - s)\n\n        self.alias_nodes = alias_nodes\n        self.alias_edges = alias_edges\n\n        return\n'"
cogdl/models/emb/prone.py,0,"b'import time\n\nimport networkx as nx\nimport numpy as np\nimport scipy.sparse as sp\nfrom scipy import linalg\nfrom scipy.special import iv\nfrom sklearn import preprocessing\nfrom sklearn.utils.extmath import randomized_svd\n\nfrom .. import BaseModel, register_model\n\n\n@register_model(""prone"")\nclass ProNE(BaseModel):\n    r""""""The ProNE model from the `""ProNE: Fast and Scalable Network Representation Learning""\n    <https://www.ijcai.org/Proceedings/2019/0594.pdf>`_ paper.\n    \n    Args:\n        hidden_size (int) : The dimension of node representation.\n        step (int) : The number of items in the chebyshev expansion.\n        mu (float) : Parameter in ProNE.\n        theta (float) : Parameter in ProNE.\n    """"""    \n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(""--step"", type=int, default=5,\n                            help=""Number of items in the chebyshev expansion"")\n        parser.add_argument(""--mu"", type=float, default=0.2)\n        parser.add_argument(""--theta"", type=float, default=0.5)\n        # fmt: on\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(args.hidden_size, args.step, args.mu, args.theta)\n\n    def __init__(self, dimension, step, mu, theta):\n        super(ProNE, self).__init__()\n        self.dimension = dimension\n        self.step = step\n        self.mu = mu\n        self.theta = theta\n\n    def train(self, G):\n        self.num_node = G.number_of_nodes()\n\n        self.matrix0 = sp.csr_matrix(nx.adjacency_matrix(G))\n\n        t_1 = time.time()\n        features_matrix = self._pre_factorization(self.matrix0, self.matrix0)\n        t_2 = time.time()\n\n        embeddings_matrix = self._chebyshev_gaussian(\n            self.matrix0, features_matrix, self.step, self.mu, self.theta\n        )\n        t_3 = time.time()\n\n        print(""sparse NE time"", t_2 - t_1)\n        print(""spectral Pro time"", t_3 - t_2)\n        self.embeddings = embeddings_matrix\n\n        return self.embeddings\n\n    def _get_embedding_rand(self, matrix):\n        # Sparse randomized tSVD for fast embedding\n        t1 = time.time()\n        l = matrix.shape[0]\n        smat = sp.csc_matrix(matrix)  # convert to sparse CSC format\n        print(""svd sparse"", smat.data.shape[0] * 1.0 / l ** 2)\n        U, Sigma, VT = randomized_svd(\n            smat, n_components=self.dimension, n_iter=5, random_state=None\n        )\n        U = U * np.sqrt(Sigma)\n        U = preprocessing.normalize(U, ""l2"")\n        print(""sparsesvd time"", time.time() - t1)\n        return U\n\n    def _get_embedding_dense(self, matrix, dimension):\n        # get dense embedding via SVD\n        t1 = time.time()\n        U, s, Vh = linalg.svd(\n            matrix, full_matrices=False, check_finite=False, overwrite_a=True\n        )\n        U = np.array(U)\n        U = U[:, :dimension]\n        s = s[:dimension]\n        s = np.sqrt(s)\n        U = U * s\n        U = preprocessing.normalize(U, ""l2"")\n        print(""densesvd time"", time.time() - t1)\n        return U\n\n    def _pre_factorization(self, tran, mask):\n        # Network Embedding as Sparse Matrix Factorization\n        t1 = time.time()\n        l1 = 0.75\n        C1 = preprocessing.normalize(tran, ""l1"")\n        neg = np.array(C1.sum(axis=0))[0] ** l1\n\n        neg = neg / neg.sum()\n\n        neg = sp.diags(neg, format=""csr"")\n        neg = mask.dot(neg)\n        print(""neg"", time.time() - t1)\n\n        C1.data[C1.data <= 0] = 1\n        neg.data[neg.data <= 0] = 1\n\n        C1.data = np.log(C1.data)\n        neg.data = np.log(neg.data)\n\n        C1 -= neg\n        F = C1\n        features_matrix = self._get_embedding_rand(F)\n        return features_matrix\n\n    def _chebyshev_gaussian(self, A, a, order=5, mu=0.5, s=0.2, plus=False, nn=False):\n        # NE Enhancement via Spectral Propagation\n        print(""Chebyshev Series -----------------"")\n        t1 = time.time()\n        num_node = a.shape[0]\n\n        if order == 1:\n            return a\n\n        A = sp.eye(num_node) + A\n        DA = preprocessing.normalize(A, norm=""l1"")\n        L = sp.eye(num_node) - DA\n\n        M = L - mu * sp.eye(num_node)\n\n        Lx0 = a\n        Lx1 = M.dot(a)\n        Lx1 = 0.5 * M.dot(Lx1) - a\n\n        conv = iv(0, s) * Lx0\n        conv -= 2 * iv(1, s) * Lx1\n        for i in range(2, order):\n            Lx2 = M.dot(Lx1)\n            Lx2 = (M.dot(Lx2) - 2 * Lx1) - Lx0\n            #         Lx2 = 2*L.dot(Lx1) - Lx0\n            if i % 2 == 0:\n                conv += 2 * iv(i, s) * Lx2\n            else:\n                conv -= 2 * iv(i, s) * Lx2\n            Lx0 = Lx1\n            Lx1 = Lx2\n            del Lx2\n            print(""Bessell time"", i, time.time() - t1)\n        emb = mm = conv\n        if not plus:\n            mm = A.dot(a - conv)\n        if not nn:\n            emb = self._get_embedding_dense(mm, self.dimension)\n        return emb'"
cogdl/models/emb/pte.py,0,"b'import numpy as np\nimport networkx as nx\nfrom sklearn import preprocessing\nimport time\nfrom tqdm import tqdm\nfrom .. import BaseModel, register_model, alias_draw, alias_setup\n\n\n@register_model(""pte"")\nclass PTE(BaseModel):\n    r""""""The PTE model from the `""PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks""\n    <https://arxiv.org/abs/1508.00200>`_ paper.\n    \n    Args:\n        hidden_size (int) : The dimension of node representation.\n        walk_length (int) : The walk length.\n        walk_num (int) : The number of walks to sample for each node.\n        negative (int) : The number of nagative samples for each edge.\n        batch_size (int) : The batch size of training in PTE.\n        alpha (float) : The initial learning rate of SGD.\n    """"""\n    \n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(\'--walk-length\', type=int, default=80,\n                            help=\'Length of walk per source. Default is 80.\')\n        parser.add_argument(\'--walk-num\', type=int, default=20,\n                            help=\'Number of walks per source. Default is 20.\')\n        parser.add_argument(\'--negative\', type=int, default=5,\n                            help=\'Number of negative node in sampling. Default is 5.\')\n        parser.add_argument(\'--batch-size\', type=int, default=1000,\n                            help=\'Batch size in SGD training process. Default is 1000.\')\n        parser.add_argument(\'--alpha\', type=float, default=0.025,\n                            help=\'Initial learning rate of SGD. Default is 0.025.\')\n        # fmt: on\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(\n            args.hidden_size,\n            args.walk_length,\n            args.walk_num,\n            args.negative,\n            args.batch_size,\n            args.alpha,\n        )\n\n    def __init__(\n        self, dimension, walk_length, walk_num, negative, batch_size, alpha\n    ):\n        super(PTE, self).__init__()\n        self.dimension = dimension\n        self.walk_length = walk_length\n        self.walk_num = walk_num\n        self.negative = negative\n        self.batch_size = batch_size\n        self.init_alpha = alpha\n\n    def train(self, G, node_type):\n        self.G = G\n        self.node_type = node_type\n        self.num_node = G.number_of_nodes()\n        self.num_edge = G.number_of_edges()\n        self.num_sampling_edge = self.walk_length * self.walk_num * self.num_node\n        \n        self.num_node_type = len(set(self.node_type))\n        context_node = [nid for nid, ntype in enumerate(self.node_type) if ntype==0]\n        \n        self.edges, self.edges_prob = [[] for _ in range(self.num_node_type)], []\n        self.node_prob, self.id2node = [], [dict() for _ in range(self.num_node_type)]\n        \n        subgraphs = []\n        for i in range(self.num_node_type):\n            for j in range(i+1, self.num_node_type):\n                context_node = [nid for nid, ntype in enumerate(self.node_type) if ntype==i or ntype ==j]\n                sub_graph = nx.Graph()\n                sub_graph = self.G.subgraph(context_node)\n                if sub_graph.number_of_edges() != 0:\n                    subgraphs.append(sub_graph)\n        self.num_graph = len(subgraphs)\n        print(""number of subgraph"", self.num_graph)\n        \n        for i in range(self.num_graph):\n            self.edges[i] = [[e[0], e[1]] for e in subgraphs[i].edges()]\n            edges_prob = np.asarray([subgraphs[i][u][v].get(""weight"", 1.0) for u, v in self.edges[i]])\n            edges_prob /= np.sum(edges_prob)\n            edges_table_prob = alias_setup(edges_prob)\n            self.edges_prob.append(edges_table_prob)\n            \n            context_node = subgraphs[i].nodes()    \n            self.id2node[i] = dict(zip(range(len(context_node)), context_node))\n            node2id = dict(zip(context_node, range(len(context_node))))\n\n            degree_weight = np.asarray([0] * len(context_node))\n            for u in context_node:\n                for v in list(subgraphs[i].neighbors(u)):\n                    degree_weight[node2id[u]] += subgraphs[i][u][v].get(""weight"", 1.0)\n\n            node_prob = np.power(degree_weight, 0.75)\n            node_prob /= np.sum(node_prob)\n            nodes_table_prob = alias_setup(node_prob) \n            self.node_prob.append(nodes_table_prob)\n\n\n        print(""train pte with 2-order"")\n        self.emb_vertex = (\n            np.random.random((self.num_node, self.dimension)) - 0.5\n        ) / self.dimension\n        self.emb_context = self.emb_vertex\n        self._train_line()\n        embedding = preprocessing.normalize(self.emb_vertex, ""l2"")\n        return embedding\n\n    def _update(self, vec_u, vec_v, vec_error, label):\n        # update vetex embedding and vec_error\n        f = 1 / (1 + np.exp(-np.sum(vec_u * vec_v, axis=1)))\n        g = (self.alpha * (label - f)).reshape((len(label), 1))\n        vec_error += g * vec_v\n        vec_v += g * vec_u\n\n    def _train_line(self,):\n        # train Line model with order\n        self.alpha = self.init_alpha\n        batch_size = self.batch_size\n        t0 = time.time()\n        num_batch = int(self.num_sampling_edge / batch_size)\n        epoch_iter = tqdm(range(num_batch))\n        for b in epoch_iter:\n            if b % 100 == 0:\n                epoch_iter.set_description(\n                    f""Progress: {b *1.0/num_batch * 100:.4f}%, alpha: {self.alpha:.6f}, time: {time.time() - t0:.4f}""\n                )\n                self.alpha = self.init_alpha * max((1 - b * 1.0 / num_batch), 0.0001)\n            \n            for k in range(self.num_graph):\n                u, v = [0] * batch_size, [0] * batch_size\n                for i in range(batch_size):\n                    edge_id = alias_draw(self.edges_prob[k][0], self.edges_prob[k][1])\n                    u[i], v[i] = self.edges[k][edge_id]\n\n                vec_error = np.zeros((batch_size, self.dimension))\n                label, target = np.asarray([1 for i in range(batch_size)]), np.asarray(v)\n                for j in range(1 + self.negative):\n                    if j != 0:\n                        label = np.asarray([0 for i in range(batch_size)])\n                        for i in range(batch_size):\n                            neg_node = alias_draw(self.node_prob[k][0], self.node_prob[k][1])\n                            target[i] = self.id2node[k][neg_node]\n                        self._update(\n                            self.emb_vertex[u], self.emb_context[target], vec_error, label\n                        )\n                self.emb_vertex[u] += vec_error\n'"
cogdl/models/emb/sdne.py,9,"b'import time\n\nimport networkx as nx\nimport numpy as np\nimport scipy.sparse as sp\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\nfrom .. import BaseModel, register_model\n\n\nclass SDNE_layer(nn.Module):\n    def __init__(self, num_node, hidden_size1, hidden_size2, droput, alpha, beta, nu1, nu2):\n        super(SDNE_layer, self).__init__()\n        self.num_node = num_node\n        self.hidden_size1 = hidden_size1\n        self.hidden_size2 = hidden_size2\n        self.droput = droput\n        self.alpha = alpha\n        self.beta = beta\n        self.nu1 = nu1\n        self.nu2 = nu2\n        \n        self.encode0 = nn.Linear(self.num_node, self.hidden_size1)\n        self.encode1 = nn.Linear(self.hidden_size1, self.hidden_size2)\n        self.decode0 = nn.Linear(self.hidden_size2, self.hidden_size1)\n        self.decode1 = nn.Linear(self.hidden_size1, self.num_node)\n\n\n    def forward(self, adj_mat, l_mat):\n        t0 = F.leaky_relu(self.encode0(adj_mat))\n        t0 = F.leaky_relu(self.encode1(t0))\n        self.embedding = t0\n        t0 = F.leaky_relu(self.decode0(t0))\n        t0 = F.leaky_relu(self.decode1(t0))\n         \n        L_1st = 2 * torch.trace(torch.mm(torch.mm(torch.t(self.embedding), l_mat), self.embedding))\n        L_2nd = torch.sum(((adj_mat - t0) * adj_mat * self.beta) * ((adj_mat - t0) *  adj_mat * self.beta))\n        \n        L_reg = 0\n        for param in self.parameters():\n            L_reg += self.nu1 * torch.sum(torch.abs(param)) + self.nu2 * torch.sum(param * param)\n        return self.alpha * L_1st,  L_2nd,  self.alpha * L_1st + L_2nd, L_reg\n\n    def get_emb(self, adj):\n        t0 = self.encode0(adj)\n        t0 = self.encode1(t0)\n        return t0\n\n\n@register_model(""sdne"")\nclass SDNE(BaseModel):\n    r""""""The SDNE model from the `""Structural Deep Network Embedding""\n    <https://www.kdd.org/kdd2016/papers/files/rfp0191-wangAemb.pdf>`_ paper\n    \n    Args:\n        hidden_size1 (int) : The size of the first hidden layer.\n        hidden_size2 (int) : The size of the second hidden layer.\n        droput (float) : Droput rate.\n        alpha (float) : Trade-off parameter between 1-st and 2-nd order objective function in SDNE.\n        beta (float) : Parameter of 2-nd order objective function in SDNE.\n        nu1 (float) : Parameter of l1 normlization in SDNE.\n        nu2 (float) : Parameter of l2 normlization in SDNE.\n        max_epoch (int) : The max epoches in training step.\n        lr (float) : Learning rate in SDNE.\n        cpu (bool) : Use CPU or GPU to train hin2vec.\n    """"""\n    \n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off  \n        parser.add_argument(""--hidden-size1"", type=int, default=1000, help=""Hidden size in first layer of Auto-Encoder"")\n        parser.add_argument(""--hidden-size2"", type=int, default=128, help=""Hidden size in second layer of Auto-Encoder"")\n        parser.add_argument(""--droput"", type=float, default=0.5, help=""Dropout rate"")\n        parser.add_argument(""--alpha"", type=float, default=1e-1, help=""alhpa is a hyperparameter in SDNE"")\n        parser.add_argument(""--beta"", type=float, default=5, help=""beta is a hyperparameter in SDNE"")\n        parser.add_argument(""--nu1"", type=float, default=1e-4, help=""nu1 is a hyperparameter in SDNE"")\n        parser.add_argument(""--nu2"", type=float, default=1e-3, help=""nu2 is a hyperparameter in SDNE"")\n        # fmt: on\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(args.hidden_size1, args.hidden_size2, args.droput, args.alpha, args.beta, args.nu1, args.nu2, args.max_epoch, args.lr, args.cpu)\n\n    def __init__(self, hidden_size1, hidden_size2, droput, alpha, beta, nu1, nu2, max_epoch, lr, cpu):\n        super(SDNE, self).__init__()\n        self.hidden_size1 = hidden_size1\n        self.hidden_size2 = hidden_size2\n        self.droput = droput\n        self.alpha = alpha\n        self.beta = beta\n        self.nu1 = nu1\n        self.nu2 = nu2\n        self.max_epoch = max_epoch\n        self.lr = lr\n        self.cpu = cpu\n        self.device = torch.device(\'cpu\' if self.cpu else \'cuda\')\n   \n    def train(self, G):\n        num_node = G.number_of_nodes()\n        model = SDNE_layer(num_node, self.hidden_size1, self.hidden_size2, self.droput, self.alpha, self.beta, self.nu1, self.nu2)\n        \n        A = torch.from_numpy(nx.adjacency_matrix(G).todense().astype(np.float32))\n        L = torch.from_numpy(nx.laplacian_matrix(G).todense().astype(np.float32))\n        \n        A, L = A.to(self.device), L.to(self.device)\n        model = model.to(self.device)\n        \n        opt = torch.optim.Adam(model.parameters(), lr=self.lr)\n        epoch_iter = tqdm(range(self.max_epoch))\n        for epoch in epoch_iter:            \n            opt.zero_grad()\n            L_1st, L_2nd, L_all, L_reg = model.forward(A, L)\n            Loss = L_all + L_reg\n            Loss.backward()\n            epoch_iter.set_description(\n                f""Epoch: {epoch:03d}, L_1st: {L_1st:.4f}, L_2nd: {L_2nd:.4f}, L_reg: {L_reg:.4f}""\n            )\n            opt.step()\n        embedding = model.get_emb(A)\n        return embedding.detach().cpu().numpy()\n'"
cogdl/models/emb/spectral.py,0,"b'import numpy as np\nimport networkx as nx\nimport scipy.sparse as sp\nfrom sklearn import preprocessing\nfrom .. import BaseModel, register_model\n\n\n@register_model(""spectral"")\nclass Spectral(BaseModel):\n    r""""""The Spectral clustering model from the `""Leveraging social media networks for classi\xef\xac\x81cation""\n    <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.481.5392&rep=rep1&type=pdf>`_ paper\n    \n    Args:\n        hidden_size (int) : The dimension of node representation.\n    """"""    \n    \n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        pass \n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(args.hidden_size)\n\n    def __init__(self, dimension):\n        super(Spectral, self).__init__()\n        self.dimension = dimension\n\n    def train(self, G):\n        matrix = nx.normalized_laplacian_matrix(G).todense()\n        matrix = np.eye(matrix.shape[0]) - np.asarray(matrix)\n        ut, s, _ = sp.linalg.svds(matrix, self.dimension)\n        emb_matrix = ut * np.sqrt(s)\n        emb_matrix = preprocessing.normalize(emb_matrix, ""l2"")\n        return emb_matrix\n\n'"
cogdl/models/nn/__init__.py,0,b''
cogdl/models/nn/asgcn.py,21,"b'import math\nimport random\nimport collections\nimport numpy as np\nfrom scipy import sparse\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.parameter import Parameter\n\nfrom .. import BaseModel, register_model\n\nclass GraphConvolution(nn.Module):\n    """"""\n    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n    """"""\n\n    def __init__(self, in_features, out_features, bias=True):\n        super(GraphConvolution, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n        if bias:\n            self.bias = Parameter(torch.FloatTensor(out_features))\n        else:\n            self.register_parameter(""bias"", None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1.0 / math.sqrt(self.weight.size(1))\n        self.weight.data.normal_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.normal_(-stdv, stdv)\n\n    def forward(self, input, adj):\n        support = torch.mm(input, self.weight)\n        output = torch.spmm(adj, support)\n        if self.bias is not None:\n            return output + self.bias\n        else:\n            return output\n\n    def __repr__(self):\n        return (\n            self.__class__.__name__\n            + "" (""\n            + str(self.in_features)\n            + "" -> ""\n            + str(self.out_features)\n            + "")""\n        )\n\n@register_model(""asgcn"")\nclass ASGCN(BaseModel):\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(""--num-features"", type=int)\n        parser.add_argument(""--num-classes"", type=int)\n        parser.add_argument(""--hidden-size"", type=int, default=64)\n        parser.add_argument(""--dropout"", type=float, default=0.5)\n        parser.add_argument(""--num-layers"", type=int, default=3)\n        parser.add_argument(""--sample-size"", type=int, nargs=\'+\', default=[64,64,32])\n        # fmt: on\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(\n            args.num_features,\n            args.num_classes,\n            args.hidden_size,\n            args.num_layers,\n            args.dropout,\n            args.sample_size,\n        )\n\n    def __init__(self, num_features, num_classes, hidden_size, num_layers, dropout, sample_size):\n        super(ASGCN, self).__init__()\n\n        self.num_features = num_features\n        self.num_classes = num_classes\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.sample_size = sample_size\n\n        self.w_s0 = Parameter(torch.FloatTensor(num_features))\n        self.w_s1 = Parameter(torch.FloatTensor(num_features))\n\n        shapes = [num_features] + [hidden_size] * (num_layers - 1) + [num_classes]\n        self.convs = nn.ModuleList(\n            [\n                GraphConvolution(shapes[layer], shapes[layer + 1])\n                for layer in range(num_layers)\n            ]\n        )\n\n        self.reset_parameters()\n    \n    def reset_parameters(self):\n        stdv = 1.0 / math.sqrt(self.hidden_size)\n        self.w_s0.data.normal_(-stdv, stdv)\n        self.w_s1.data.normal_(-stdv, stdv)\n\n    def set_adj(self, edge_index, num_nodes):\n        self.sparse_adj = sparse.coo_matrix(\n            (np.ones(edge_index.shape[1]), (edge_index[0], edge_index[1])),\n            shape=(num_nodes, num_nodes),\n        ).tocsr()\n        self.num_nodes = num_nodes\n        self.adj = self.compute_adjlist(self.sparse_adj)\n        self.adj = torch.tensor(self.adj)\n\n    def compute_adjlist(self, sp_adj, max_degree=32):\n        """"""Transfer sparse adjacent matrix to adj-list format""""""\n        num_data = sp_adj.shape[0]\n        adj = num_data + np.zeros((num_data+1, max_degree), dtype=np.int32)\n\n        for v in range(num_data):\n            neighbors = np.nonzero(sp_adj[v, :])[1]\n            len_neighbors = len(neighbors)\n            if len_neighbors > max_degree:\n                neighbors = np.random.choice(neighbors, max_degree, replace=False)\n                adj[v] = neighbors\n            else:\n                adj[v, :len_neighbors] = neighbors\n\n        return adj\n\n    def from_adjlist(self, adj):\n        """"""Transfer adj-list format to sparsetensor""""""\n        u_sampled, index = torch.unique(torch.flatten(adj), return_inverse=True)\n\n        row = (torch.range(0, index.shape[0]-1) / adj.shape[1]).long().to(adj.device)\n        col = index\n        values = torch.ones(index.shape[0]).float().to(adj.device)\n        indices = torch.cat([row.unsqueeze(1), col.unsqueeze(1)], axis=1).t()\n        dense_shape = (adj.shape[0], u_sampled.shape[0])\n\n        support = torch.sparse_coo_tensor(indices, values, dense_shape)\n\n        return support, u_sampled.long()\n\n    def _sample_one_layer(self, x, adj, v, sample_size):\n        support, u = self.from_adjlist(adj)\n\n\n        h_v = torch.sum(torch.matmul(x[v], self.w_s1))\n        h_u = torch.matmul(x[u], self.w_s0)\n        attention = (F.relu(h_v + h_u) + 1) * (1.0 / sample_size)\n        g_u = F.relu(h_u) + 1\n\n        p1 = attention * g_u\n        p1 = p1.cpu()\n\n        if self.num_nodes in u:\n            p1[u == self.num_nodes] = 0\n        p1 = p1 / torch.sum(p1)\n\n        samples = torch.multinomial(p1, sample_size, False)\n        u_sampled = u[samples]\n            \n        support_sampled = torch.index_select(support, 1, samples)\n\n        return u_sampled, support_sampled\n\n    def sampling(self, x, v):\n        all_support = [[] for _ in range(self.num_layers)]\n        sampled = v\n        x = torch.cat((x, torch.zeros(1, x.shape[1]).to(x.device)), dim=0)\n        for i in range(self.num_layers - 1, -1, -1):\n            cur_sampled, cur_support = self._sample_one_layer(x, self.adj[sampled], sampled, self.sample_size[i])\n            all_support[i] = cur_support.to(x.device)\n            sampled = cur_sampled\n\n        return x[sampled.to(x.device)], all_support, 0\n\n    def forward(self, x, adj):\n        for index, conv in enumerate(self.convs[:-1]):\n            x = F.relu(conv(x, adj[index]))\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.convs[-1](x, adj[-1])\n        return F.log_softmax(x, dim=1)\n'"
cogdl/models/nn/diffpool.py,29,"b'import numpy as np\nimport random\n\nfrom scipy.linalg import block_diag\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch_geometric.nn import SAGEConv\nfrom torch_geometric.utils import add_remaining_self_loops\n\nfrom .. import BaseModel, register_model\nfrom cogdl.data import DataLoader\n\n\nclass EntropyLoss(nn.Module):\n    # Return Scalar\n    def forward(self, adj, anext, s_l):\n        # entropy.mean(-1).mean(-1): 1/n in node and batch\n        # entropy = (torch.distributions.Categorical(\n            # probs=s_l).entropy()).sum(-1).mean(-1)\n        entropy = (torch.distributions.Categorical(\n            probs=s_l).entropy()).mean()\n        assert not torch.isnan(entropy)\n        return entropy\n\n\nclass LinkPredLoss(nn.Module):\n    def forward(self, adj, anext, s_l):\n        link_pred_loss = (\n            adj - s_l.matmul(s_l.transpose(-1, -2))).norm(dim=(1, 2))\n        link_pred_loss = link_pred_loss / (adj.size(1) * adj.size(2))\n        return link_pred_loss.mean()\n\n\nclass GraphSAGE(nn.Module):\n    r""""""GraphSAGE from `""Inductive Representation Learning on Large Graphs"" <https://arxiv.org/pdf/1706.02216.pdf>`__.\n\n    ..math::\n        h^{i+1}_{\\mathcal{N}(v)}=AGGREGATE_{k}(h_{u}^{k})\n        h^{k+1}_{v} = \\sigma(\\mathbf{W}^{k}\xc2\xb7CONCAT(h_{v}^{k}, h_{\\mathcal{N}(v)}))\n\n    Args:\n        in_feats (int) : Size of each input sample.\n        hidden_dim (int) : Size of hidden layer dimension.\n        out_feats (int) : Size of each output sample.\n        num_layers (int) : Number of GraphSAGE Layers.\n        dropout (float, optional) : Size of dropout, default: ``0.5``.\n        normalize (bool, optional) : Normalze features after each layer if True, default: ``True``.\n    """"""\n    def __init__(self, in_feats, hidden_dim, out_feats, num_layers, dropout=0.5, normalize=False, concat=False, use_bn=False):\n        super(GraphSAGE, self).__init__()\n        self.convlist = nn.ModuleList()\n        self.bn_list = nn.ModuleList()\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.use_bn = use_bn\n        if num_layers == 1:\n            self.convlist.append(SAGEConv(in_feats, out_feats, normalize, concat))\n        else:\n            self.convlist.append(SAGEConv(in_feats, hidden_dim, normalize, concat))\n            if use_bn:\n                self.bn_list.append(nn.BatchNorm1d(hidden_dim))\n            for _ in range(num_layers - 2):\n                self.convlist.append(SAGEConv(hidden_dim, hidden_dim, normalize, concat))\n                if use_bn:\n                    self.bn_list.append(nn.BatchNorm1d(hidden_dim))\n            self.convlist.append(SAGEConv(hidden_dim, out_feats, normalize, concat))\n\n    def forward(self, x, edge_index, edge_weight=None):\n        h = x\n        for i in range(self.num_layers-1):\n            h = F.dropout(h, p=self.dropout, training=self.training)\n            h = self.convlist[i](h, edge_index, edge_weight)\n            if self.use_bn:\n                h = self.bn_list[i](h)\n        return self.convlist[self.num_layers-1](h, edge_index, edge_weight)\n\n\nclass BatchedGraphSAGE(nn.Module):\n    r""""""GraphSAGE with mini-batch\n\n    Args:\n        in_feats (int) : Size of each input sample.\n        out_feats (int) : Size of each output sample.\n        use_bn (bool) : Apply batch normalization if True, default: ``True``.\n        self_loop (bool) : Add self loop if True, default: ``True``.\n    """"""\n    def __init__(self, in_feats, out_feats, use_bn=True, self_loop=True):\n        super(BatchedGraphSAGE, self).__init__()\n        self.self_loop = self_loop\n        self.use_bn = use_bn\n        self.weight = nn.Linear(in_feats, out_feats, bias=True)\n\n        nn.init.xavier_uniform_(self.weight.weight.data, gain=nn.init.calculate_gain(\'relu\'))\n\n    def forward(self, x, adj):\n        device = x.device\n        if self.self_loop:\n            adj = adj + torch.eye(x.shape[1]).to(device)\n        adj = adj / adj.sum(dim=1, keepdim=True)\n        h = torch.matmul(adj, x)\n        h = self.weight(h)\n        h = F.normalize(h, dim=2, p=2)\n        h = F.relu(h)\n        # TODO: shape = [a, 0, b]\n        # if self.use_bn and h.shape[1] > 0:\n        #     self.bn = nn.BatchNorm1d(h.shape[1]).to(device)\n        #     h = self.bn(h)\n        return h\n\n\nclass BatchedDiffPoolLayer(nn.Module):\n    r""""""DIFFPOOL from paper `""Hierarchical Graph Representation Learning\n    with Differentiable Pooling"" <https://arxiv.org/pdf/1806.08804.pdf>`__.\n\n    .. math::\n        X^{(l+1)} = S^{l)}^T Z^{(l)}\n        A^{(l+1)} = S^{(l)}^T A^{(l)} S^{(l)}\n        Z^{(l)} = GNN_{l, embed}(A^{(l)}, X^{(l)})\n        S^{(l)} = softmax(GNN_{l,pool}(A^{(l)}, X^{(l)}))\n\n    Parameters\n    ----------\n    in_feats : int\n        Size of each input sample.\n    out_feats : int\n        Size of each output sample.\n    assign_dim : int\n        Size of next adjacency matrix.\n    batch_size : int\n        Size of each mini-batch.\n    dropout : float, optional\n        Size of dropout, default: ``0.5``.\n    link_pred_loss : bool, optional\n        Use link prediction loss if True, default: ``True``.\n    """"""\n    def __init__(self, in_feats, out_feats, assign_dim, batch_size, dropout=0.5, link_pred_loss=True, entropy_loss=True):\n        super(BatchedDiffPoolLayer, self).__init__()\n        self.assign_dim = assign_dim\n        self.dropout = dropout\n        self.use_link_pred = link_pred_loss\n        self.batch_size = batch_size\n        self.embd_gnn = SAGEConv(in_feats, out_feats, normalize=False)\n        self.pool_gnn = SAGEConv(in_feats, assign_dim, normalize=False)\n\n        self.loss_dict = dict()\n\n\n    def forward(self, x, edge_index, batch, edge_weight=None):\n        embed = self.embd_gnn(x, edge_index)\n        pooled = F.softmax(self.pool_gnn(x, edge_index), dim=-1)\n        device = x.device\n        masked_tensor = []\n        value_set, value_counts = torch.unique(batch, return_counts=True)\n        batch_size = len(value_set)\n        for i in value_counts:\n            masked = torch.ones((i, int(pooled.size()[1]/batch_size)))\n            masked_tensor.append(masked)\n        masked = torch.FloatTensor(block_diag(*masked_tensor)).to(device)\n\n        result = torch.nn.functional.softmax(masked * pooled, dim=-1)\n        result = result * masked\n        result = result / (result.sum(dim=-1, keepdim=True) + 1e-13)\n        # result = masked_softmax(pooled, masked, memory_efficient=False)\n\n        h = torch.matmul(result.t(), embed)\n        if not edge_weight:\n            edge_weight = torch.ones(edge_index.shape[1]).to(x.device)\n        adj = torch.sparse_coo_tensor(edge_index, edge_weight)\n        adj_new = torch.sparse.mm(adj, result)\n        adj_new = torch.mm(result.t(), adj_new)\n\n        if self.use_link_pred:\n            adj_loss = torch.norm((adj.to_dense() - torch.mm(result, result.t()))) / np.power((len(batch)), 2)\n            self.loss_dict[""adj_loss""] = adj_loss\n        entropy_loss = (torch.distributions.Categorical(probs=pooled).entropy()).mean()\n        assert not torch.isnan(entropy_loss)\n        self.loss_dict[""entropy_loss""] = entropy_loss\n        return adj_new, h\n\n    def get_loss(self):\n        loss_n = 0\n        for _, value in self.loss_dict.items():\n            loss_n += value\n        return loss_n\n\n\nclass BatchedDiffPool(nn.Module):\n    r""""""DIFFPOOL layer with batch forward\n\n    Parameters\n    ----------\n    in_feats : int\n        Size of each input sample.\n    next_size : int\n        Size of next adjacency matrix.\n    emb_size : int\n        Dimension of next node feature matrix.\n    use_bn : bool, optional\n        Apply batch normalization if True, default: ``True``.\n    self_loop : bool, optional\n        Add self loop if True, default: ``True``.\n    use_link_loss : bool, optional\n        Use link prediction loss if True, default: ``True``.\n    use_entropy : bool, optioinal\n        Use entropy prediction loss if True, default: ``True``.\n    """"""\n    def __init__(self, in_feats, next_size, emb_size, use_bn=True, self_loop=True, use_link_loss=False, use_entropy=True):\n        super(BatchedDiffPool, self).__init__()\n        self.use_link_loss = use_link_loss\n        self.use_bn = use_bn\n        self.feat_trans = BatchedGraphSAGE(in_feats, emb_size)\n        self.assign_trans = BatchedGraphSAGE(in_feats, next_size)\n\n        self.link_loss = LinkPredLoss()\n        self.entropy = EntropyLoss()\n\n        self.loss_module = nn.ModuleList()\n        if use_link_loss:\n            self.loss_module.append(LinkPredLoss())\n        if use_entropy:\n            self.loss_module.append(EntropyLoss())\n        self.loss = {}\n\n    def forward(self, x, adj):\n        h = self.feat_trans(x, adj)\n        next_l = F.softmax(self.assign_trans(x, adj), dim=-1)\n\n        h = torch.matmul(next_l.transpose(-1, -2), h)\n        next = torch.matmul(next_l.transpose(-1, -2), torch.matmul(adj, next_l))\n\n        for layer in self.loss_module:\n            self.loss[str(type(layer).__name__)] = layer(adj, next, next_l)\n\n        return h, next\n\n    def get_loss(self):\n        value = 0\n        for _, v in self.loss.items():\n            value += v\n        return value\n\n\ndef toBatchedGraph(batch_adj, batch_feat, node_per_pool_graph):\n    adj_list = [batch_adj[i:i+node_per_pool_graph, i:i+node_per_pool_graph]\n                for i in range(0, batch_adj.size()[0], node_per_pool_graph)]\n    feat_list = [batch_feat[i:i+node_per_pool_graph, :] for i in range(0, batch_adj.size()[0], node_per_pool_graph)]\n    adj_list = list(map(lambda x: torch.unsqueeze(x, 0), adj_list))\n    feat_list = list(map(lambda x: torch.unsqueeze(x, 0), feat_list))\n    adj = torch.cat(adj_list, dim=0)\n    feat = torch.cat(feat_list, dim=0)\n    return adj, feat\n\n\n@register_model(""diffpool"")\nclass DiffPool(BaseModel):\n    r""""""DIFFPOOL from paper `Hierarchical Graph Representation Learning\n    with Differentiable Pooling <https://arxiv.org/pdf/1806.08804.pdf>`__.\n\n    Parameters\n    ----------\n    in_feats : int\n        Size of each input sample.\n    hidden_dim : int\n        Size of hidden layer dimension of GNN.\n    embed_dim : int\n        Size of embeded node feature, output size of GNN.\n    num_classes : int\n        Number of target classes.\n    num_layers : int\n        Number of GNN layers.\n    num_pool_layers : int\n        Number of pooling.\n    assign_dim : int\n        Embedding size after the first pooling.\n    pooling_ratio : float\n        Size of each poolling ratio.\n    batch_size : int\n        Size of each mini-batch.\n    dropout : float, optional\n        Size of dropout, default: `0.5`.\n    no_link_pred : bool, optional\n        If True, use link prediction loss, default: `True`.\n    """"""\n\n    @staticmethod\n    def add_args(parser):\n        parser.add_argument(""--num-layers"", type=int, default=2)\n        parser.add_argument(""--num-pooling-layers"", type=int, default=1)\n        parser.add_argument(""--no-link-pred"", dest=""no_link_pred"", action=""store_true"")\n        parser.add_argument(""--pooling-ratio"", type=float, default=0.15)\n        parser.add_argument(""--embedding-dim"", type=int, default=64)\n        parser.add_argument(""--hidden-size"", type=int, default=64)\n        parser.add_argument(""--dropout"", type=float, default=0.1)\n        parser.add_argument(""--batch-size"", type=int, default=20)\n        parser.add_argument(""--train-ratio"", type=float, default=0.7)\n        parser.add_argument(""--test-ratio"", type=float, default=0.1)\n        parser.add_argument(""--lr"", type=float, default=0.001)\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(\n            args.num_features,\n            args.hidden_size,\n            args.embedding_dim,\n            args.num_classes,\n            args.num_layers,\n            args.num_pooling_layers,\n            int(args.max_graph_size * args.pooling_ratio) * args.batch_size,\n            args.pooling_ratio,\n            args.batch_size,\n            args.dropout,\n            args.no_link_pred\n        )\n\n    @classmethod\n    def split_dataset(cls, dataset, args):\n        random.shuffle(dataset)\n        train_size = int(len(dataset) * args.train_ratio)\n        test_size = int(len(dataset) * args.test_ratio)\n        bs = args.batch_size\n        train_loader = DataLoader(dataset[:train_size], batch_size=bs, drop_last=True)\n        test_loader = DataLoader(dataset[-test_size:], batch_size=bs, drop_last=True)\n        if args.train_ratio + args.test_ratio < 1:\n            valid_loader = DataLoader(dataset[train_size:-test_size], batch_size=bs, drop_last=True)\n        else:\n            valid_loader = test_loader\n        return train_loader, valid_loader, test_loader\n\n    def __init__(self, in_feats, hidden_dim, embed_dim, num_classes, num_layers, num_pool_layers,  assign_dim,\n                 pooling_ratio, batch_size, dropout=0.5, no_link_pred=True, concat=False, use_bn=False):\n        super(DiffPool, self).__init__()\n        self.assign_dim = assign_dim\n        self.assign_dim_list = [assign_dim]\n        self.use_bn = use_bn\n        self.dropout = dropout\n        self.use_link_loss = not no_link_pred\n        # assert num_layers > 3, ""layers > 3""\n        self.diffpool_layers = nn.ModuleList()\n        self.before_pooling = GraphSAGE(in_feats, hidden_dim, embed_dim,\n                                        num_layers=num_layers, dropout=dropout, use_bn=self.use_bn)\n        self.init_diffpool = BatchedDiffPoolLayer(embed_dim, hidden_dim, assign_dim, batch_size, dropout, self.use_link_loss)\n\n        pooled_emb_dim = embed_dim\n        self.after_pool = nn.ModuleList()\n        after_per_pool = nn.ModuleList()\n        for _ in range(num_layers-1):\n            after_per_pool.append(BatchedGraphSAGE(hidden_dim, hidden_dim))\n        after_per_pool.append(BatchedGraphSAGE(hidden_dim, pooled_emb_dim))\n        self.after_pool.append(after_per_pool)\n\n        for _ in range(num_pool_layers-1):\n            self.assign_dim = int(self.assign_dim//batch_size * pooling_ratio) * batch_size\n            self.diffpool_layers.append(BatchedDiffPool(\n                pooled_emb_dim, self.assign_dim, hidden_dim, use_bn=self.use_bn, use_link_loss=self.use_link_loss\n            ))\n\n            for _ in range(num_layers - 1):\n                after_per_pool.append(BatchedGraphSAGE(hidden_dim, hidden_dim))\n            after_per_pool.append(BatchedGraphSAGE(hidden_dim, pooled_emb_dim))\n            self.after_pool.append(after_per_pool)\n\n            self.assign_dim_list.append(self.assign_dim)\n\n        if concat:\n            out_dim = pooled_emb_dim * (num_pool_layers+1)\n        else:\n            out_dim = pooled_emb_dim\n        self.fc = nn.Linear(out_dim, num_classes)\n\n    def reset_parameters(self):\n        for i in self.modules():\n            if isinstance(i, nn.Linear):\n                nn.init.xavier_uniform_(i.weight.data, gain=nn.init.calculate_gain(\'relu\'))\n                if i.bias is not None:\n                    nn.init.constant_(i.bias.data, 0.)\n\n    def after_pooling_forward(self, gnn_layers, adj, x, concat=False):\n        readouts = []\n        h = x\n        for layer in gnn_layers:\n            h = layer(h, adj)\n            readouts.append(h)\n        readout = torch.cat(readouts, dim=1)\n        return h\n\n    def forward(self, batch):\n        readouts_all = []\n\n        init_emb = self.before_pooling(batch.x, batch.edge_index)\n        adj, h = self.init_diffpool(init_emb, batch.edge_index, batch.batch)\n        value_set, value_counts = torch.unique(batch.batch, return_counts=True)\n        batch_size = len(value_set)\n        adj, h = toBatchedGraph(adj, h, adj.size(0)//batch_size)\n        h = self.after_pooling_forward(self.after_pool[0], adj, h)\n        readout = torch.sum(h, dim=1)\n        readouts_all.append(readout)\n\n        for i, diff_layer in enumerate(self.diffpool_layers):\n            h, adj = diff_layer(h, adj)\n            h = self.after_pooling_forward(self.after_pool[i+1], adj, h)\n            readout = torch.sum(h, dim=1)\n            readouts_all.append(readout)\n        pred = self.fc(readout)\n        if batch.y is not None:\n            return pred, self.loss(pred, batch.y)\n        return pred, None\n\n    def loss(self, prediction, label):\n        criterion = nn.CrossEntropyLoss()\n        loss_n = criterion(prediction, label)\n        loss_n += self.init_diffpool.get_loss()\n        for layer in self.diffpool_layers:\n            loss_n += layer.get_loss()\n        return loss_n\n\n'"
cogdl/models/nn/fastgcn.py,11,"b'import math\nimport random\nimport collections\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.parameter import Parameter\n\nfrom .. import BaseModel, register_model\n\nclass GraphConvolution(nn.Module):\n    """"""\n    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n    """"""\n\n    def __init__(self, in_features, out_features, bias=True):\n        super(GraphConvolution, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n        if bias:\n            self.bias = Parameter(torch.FloatTensor(out_features))\n        else:\n            self.register_parameter(""bias"", None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1.0 / math.sqrt(self.weight.size(1))\n        self.weight.data.normal_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.normal_(-stdv, stdv)\n\n    def forward(self, input, adj):\n        support = torch.mm(input, self.weight)\n        output = torch.spmm(adj, support)\n        if self.bias is not None:\n            return output + self.bias\n        else:\n            return output\n\n    def __repr__(self):\n        return (\n            self.__class__.__name__\n            + "" (""\n            + str(self.in_features)\n            + "" -> ""\n            + str(self.out_features)\n            + "")""\n        )\n\n@register_model(""fastgcn"")\nclass FastGCN(BaseModel):\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(""--num-features"", type=int)\n        parser.add_argument(""--num-classes"", type=int)\n        parser.add_argument(""--hidden-size"", type=int, default=64)\n        parser.add_argument(""--dropout"", type=float, default=0.5)\n        parser.add_argument(""--num-layers"", type=int, default=3)\n        parser.add_argument(""--sample-size"", type=int, nargs=\'+\', default=[512,256,256])\n        # fmt: on\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(\n            args.num_features,\n            args.num_classes,\n            args.hidden_size,\n            args.num_layers,\n            args.dropout,\n            args.sample_size,\n        )\n\n    def __init__(self, num_features, num_classes, hidden_size, num_layers, dropout, sample_size):\n        super(FastGCN, self).__init__()\n\n        self.num_features = num_features\n        self.num_classes = num_classes\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.sample_size = sample_size\n\n        shapes = [num_features] + [hidden_size] * (num_layers - 1) + [num_classes]\n        self.convs = nn.ModuleList(\n            [\n                GraphConvolution(shapes[layer], shapes[layer + 1])\n                for layer in range(num_layers)\n            ]\n        )\n\n    def set_adj(self, edge_index, num_nodes):\n        self.adj = collections.defaultdict(list)\n        for i in range(edge_index.shape[1]):\n            self.adj[edge_index[0, i]].append(edge_index[1, i])\n            self.adj[edge_index[1, i]].append(edge_index[0, i])\n    \n    def _sample_one_layer(self, sampled, sample_size):\n        total = []\n        for node in sampled:\n            total.extend(self.adj[node])\n        total = list(set(total))\n        if sample_size < len(total):\n            total = random.sample(total, sample_size)\n        return total\n\n    def _generate_adj(self, sample1, sample2):\n        edgelist = []\n        mapping = {}\n        for i in range(len(sample1)):\n            mapping[sample1[i]] = i\n\n        for i in range(len(sample2)):\n            nodes = self.adj[sample2[i]]\n            for node in nodes:\n                if node in mapping:\n                    edgelist.append([mapping[node], i])\n        edgetensor = torch.LongTensor(edgelist)\n        valuetensor = torch.ones(edgetensor.shape[0]).float()\n        t = torch.sparse_coo_tensor(\n            edgetensor.t(), valuetensor, (len(sample1), len(sample2))\n        )\n        return t\n\n    def sampling(self, x, v):\n        all_support = [[] for _ in range(self.num_layers)]\n        sampled = v.detach().cpu().numpy()\n        for i in range(self.num_layers - 1, -1, -1):\n            cur_sampled = self._sample_one_layer(sampled, self.sample_size[i])\n            all_support[i] = self._generate_adj(sampled, cur_sampled).to(x.device)\n            sampled = cur_sampled\n\n        return x[torch.LongTensor(sampled).to(x.device)], all_support, 0\n\n    def forward(self, x, adj):\n        for index, conv in enumerate(self.convs[:-1]):\n            x = F.relu(conv(x, adj[index]))\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.convs[-1](x, adj[-1])\n        return F.log_softmax(x, dim=1)\n'"
cogdl/models/nn/gat.py,27,"b'import math\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.parameter import Parameter\n\nfrom .. import BaseModel, register_model\n\n\nclass GraphAttentionLayer(nn.Module):\n    """"""\n    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n    """"""\n\n    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n        super(GraphAttentionLayer, self).__init__()\n        self.dropout = dropout\n        self.in_features = in_features\n        self.out_features = out_features\n        self.alpha = alpha\n        self.concat = concat\n\n        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n        self.a = nn.Parameter(torch.zeros(size=(2 * out_features, 1)))\n        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n\n        self.leakyrelu = nn.LeakyReLU(self.alpha)\n\n    def forward(self, input, adj):\n        h = torch.mm(input, self.W)\n        N = h.size()[0]\n\n        a_input = torch.cat(\n            [h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1\n        ).view(N, -1, 2 * self.out_features)\n        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n\n        zero_vec = -9e15 * torch.ones_like(e)\n        attention = torch.where(adj > 0, e, zero_vec)\n        attention = F.softmax(attention, dim=1)\n        attention = F.dropout(attention, self.dropout, training=self.training)\n        h_prime = torch.matmul(attention, h)\n\n        if self.concat:\n            return F.elu(h_prime)\n        else:\n            return h_prime\n\n    def __repr__(self):\n        return (\n            self.__class__.__name__\n            + "" (""\n            + str(self.in_features)\n            + "" -> ""\n            + str(self.out_features)\n            + "")""\n        )\n\n\nclass SpecialSpmmFunction(torch.autograd.Function):\n    """"""Special function for only sparse region backpropataion layer.""""""\n\n    @staticmethod\n    def forward(ctx, indices, values, shape, b):\n        assert indices.requires_grad == False\n        a = torch.sparse_coo_tensor(indices, values, shape)\n        ctx.save_for_backward(a, b)\n        ctx.N = shape[0]\n        return torch.matmul(a, b)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        a, b = ctx.saved_tensors\n        grad_values = grad_b = None\n        if ctx.needs_input_grad[1]:\n            grad_a_dense = grad_output.matmul(b.t())\n            edge_idx = a._indices()[0, :] * ctx.N + a._indices()[1, :]\n            grad_values = grad_a_dense.view(-1)[edge_idx]\n        if ctx.needs_input_grad[3]:\n            grad_b = a.t().matmul(grad_output)\n        return None, grad_values, None, grad_b\n\n\nclass SpecialSpmm(nn.Module):\n    def forward(self, indices, values, shape, b):\n        return SpecialSpmmFunction.apply(indices, values, shape, b)\n\n\nclass SpGraphAttentionLayer(nn.Module):\n    """"""\n    Sparse version GAT layer, similar to https://arxiv.org/abs/1710.10903\n    """"""\n\n    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n        super(SpGraphAttentionLayer, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.alpha = alpha\n        self.concat = concat\n\n        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n        nn.init.xavier_normal_(self.W.data, gain=1.414)\n\n        self.a = nn.Parameter(torch.zeros(size=(1, 2 * out_features)))\n        nn.init.xavier_normal_(self.a.data, gain=1.414)\n\n        self.dropout = nn.Dropout(dropout)\n        self.leakyrelu = nn.LeakyReLU(self.alpha)\n        self.special_spmm = SpecialSpmm()\n\n    def forward(self, input, edge):\n        N = input.size()[0]\n\n        h = torch.mm(input, self.W)\n        # h: N x out\n        assert not torch.isnan(h).any()\n\n        # Self-attention on the nodes - Shared attention mechanism\n        edge_h = torch.cat((h[edge[0, :], :], h[edge[1, :], :]), dim=1).t()\n        # edge: 2*D x E\n\n        edge_e = torch.exp(-self.leakyrelu(self.a.mm(edge_h).squeeze()))\n        assert not torch.isnan(edge_e).any()\n        # edge_e: E\n\n        e_rowsum = self.special_spmm(\n            edge, edge_e, torch.Size([N, N]), torch.ones(size=(N, 1)).to(input.device)\n        )\n        # e_rowsum: N x 1\n\n        edge_e = self.dropout(edge_e)\n        # edge_e: E\n\n        h_prime = self.special_spmm(edge, edge_e, torch.Size([N, N]), h)\n        assert not torch.isnan(h_prime).any()\n        # h_prime: N x out\n\n        h_prime = h_prime.div(e_rowsum)\n        # h_prime: N x out\n        assert not torch.isnan(h_prime).any()\n\n        if self.concat:\n            # if this layer is not last layer,\n            return F.elu(h_prime)\n        else:\n            # if this layer is last layer,\n            return h_prime\n\n    def __repr__(self):\n        return (\n            self.__class__.__name__\n            + "" (""\n            + str(self.in_features)\n            + "" -> ""\n            + str(self.out_features)\n            + "")""\n        )\n\n\nclass PetarVGAT(BaseModel):\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(""--num-features"", type=int)\n        parser.add_argument(""--num-classes"", type=int)\n        parser.add_argument(""--hidden-size"", type=int, default=8)\n        parser.add_argument(""--dropout"", type=float, default=0.6)\n        parser.add_argument(""--alpha"", type=float, default=0.2)\n        parser.add_argument(""--nheads"", type=int, default=8)\n        # fmt: on\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(\n            args.num_features,\n            args.hidden_size,\n            args.num_classes,\n            args.dropout,\n            args.alpha,\n            args.nheads,\n        )\n\n    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n        """"""Dense version of GAT.""""""\n        super(PetarVGAT, self).__init__()\n        self.dropout = dropout\n\n        self.attentions = [\n            GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True)\n            for _ in range(nheads)\n        ]\n        for i, attention in enumerate(self.attentions):\n            self.add_module(""attention_{}"".format(i), attention)\n\n        self.out_att = GraphAttentionLayer(\n            nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False\n        )\n\n    def forward(self, x, adj):\n        x = F.dropout(x, self.dropout, training=self.training)\n        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n        x = F.dropout(x, self.dropout, training=self.training)\n        x = F.elu(self.out_att(x, adj))\n        return F.log_softmax(x, dim=1)\n\n\n@register_model(""gat"")\nclass PetarVSpGAT(PetarVGAT):\n    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n        """"""Sparse version of GAT.""""""\n        BaseModel.__init__(self)\n        self.dropout = dropout\n\n        self.attentions = [\n            SpGraphAttentionLayer(\n                nfeat, nhid, dropout=dropout, alpha=alpha, concat=True\n            )\n            for _ in range(nheads)\n        ]\n        for i, attention in enumerate(self.attentions):\n            self.add_module(""attention_{}"".format(i), attention)\n\n        self.out_att = SpGraphAttentionLayer(\n            nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False\n        )\n\n    def forward(self, x, adj):\n        x = F.dropout(x, self.dropout, training=self.training)\n        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n        x = F.dropout(x, self.dropout, training=self.training)\n        x = F.elu(self.out_att(x, adj))\n        return F.log_softmax(x, dim=1)\n    \n    def loss(self, data):\n        return F.nll_loss(\n            self.forward(data.x, data.edge_index)[data.train_mask],\n            data.y[data.train_mask],\n        )\n    \n    def predict(self, data):\n        return self.forward(data.x, data.edge_index)\n'"
cogdl/models/nn/gcn.py,10,"b'import math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.parameter import Parameter\n\nfrom .. import BaseModel, register_model\n\n\nclass GraphConvolution(nn.Module):\n    """"""\n    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n    """"""\n\n    def __init__(self, in_features, out_features, bias=True):\n        super(GraphConvolution, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n        if bias:\n            self.bias = Parameter(torch.FloatTensor(out_features))\n        else:\n            self.register_parameter(""bias"", None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1.0 / math.sqrt(self.weight.size(1))\n        self.weight.data.normal_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.normal_(-stdv, stdv)\n\n    def forward(self, input, edge_index):\n        adj = torch.sparse_coo_tensor(\n            edge_index,\n            torch.ones(edge_index.shape[1]).float(),\n            (input.shape[0], input.shape[0]),\n        ).to(input.device)\n        support = torch.mm(input, self.weight)\n        output = torch.spmm(adj, support)\n        if self.bias is not None:\n            return output + self.bias\n        else:\n            return output\n\n    def __repr__(self):\n        return (\n            self.__class__.__name__\n            + "" (""\n            + str(self.in_features)\n            + "" -> ""\n            + str(self.out_features)\n            + "")""\n        )\n\n\n@register_model(""gcn"")\nclass TKipfGCN(BaseModel):\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(""--num-features"", type=int)\n        parser.add_argument(""--num-classes"", type=int)\n        parser.add_argument(""--hidden-size"", type=int, default=64)\n        parser.add_argument(""--dropout"", type=float, default=0.5)\n        # fmt: on\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(args.num_features, args.hidden_size, args.num_classes, args.dropout)\n\n    def __init__(self, nfeat, nhid, nclass, dropout):\n        super(TKipfGCN, self).__init__()\n\n        self.gc1 = GraphConvolution(nfeat, nhid)\n        self.gc2 = GraphConvolution(nhid, nclass)\n        self.dropout = dropout\n        # self.nonlinear = nn.SELU()\n\n    def forward(self, x, adj):\n        x = F.relu(self.gc1(x, adj))\n        # h1 = x\n        x = F.dropout(x, self.dropout, training=self.training)\n        x = self.gc2(x, adj)\n\n        # x = F.relu(x)\n        # x = torch.sigmoid(x)\n        # return x\n        # h2 = x\n        return F.log_softmax(x, dim=-1)\n    \n    def loss(self, data):\n        return F.nll_loss(\n            self.forward(data.x, data.edge_index)[data.train_mask],\n            data.y[data.train_mask],\n        )\n    \n    def predict(self, data):\n        return self.forward(data.x, data.edge_index)\n'"
cogdl/models/nn/gin.py,8,"b'import random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.utils import remove_self_loops\nfrom torch_scatter import scatter_add\n\nfrom .. import BaseModel, register_model\nfrom cogdl.data import DataLoader\n\n\nclass GINLayer(nn.Module):\n    r""""""Graph Isomorphism Network layer from paper `""How Powerful are Graph\n    Neural Networks?"" <https://arxiv.org/pdf/1810.00826.pdf>`__.\n\n    .. math::\n        h_i^{(l+1)} = f_\\Theta \\left((1 + \\epsilon) h_i^{l} +\n        \\mathrm{sum}\\left(\\left\\{h_j^{l}, j\\in\\mathcal{N}(i)\n        \\right\\}\\right)\\right)\n\n    Parameters\n    ----------\n    apply_func : callable layer function)\n        layer or function applied to update node feature\n    eps : float32, optional\n        Initial `\\epsilon` value.\n    train_eps : bool, optional\n        If True, `\\epsilon` will be a learnable parameter.\n    """"""\n    def __init__(self, apply_func=None, eps=0, train_eps=True):\n        super(GINLayer, self).__init__()\n        if train_eps:\n            self.eps = torch.nn.Parameter(torch.FloatTensor([eps]))\n        else:\n            self.register_buffer(\'eps\', torch.FloatTensor([eps]))\n        self.apply_func = apply_func\n\n    def forward(self, x, edge_index, edge_weight=None):\n        edge_index, _ = remove_self_loops(edge_index)\n        edge_weight = torch.ones(edge_index.shape[1]) if edge_weight is None else edge_weight\n        adj = torch.sparse_coo_tensor(edge_index, edge_weight, (x.shape[0], x.shape[0]))\n        adj = adj.to(x.device)\n        out = (1 + self.eps) * x + torch.spmm(adj, x)\n        if self.apply_func is not None:\n            out = self.apply_func(out)\n        return out\n\n\nclass GINMLP(nn.Module):\n    r""""""Multilayer perception with batch normalization\n\n    .. math::\n        x^{(i+1)} = \\sigma(W^{i}x^{(i)})\n\n    Parameters\n    ----------\n    in_feats : int\n        Size of each input sample.\n    out_feats : int\n        Size of each output sample.\n    hidden_dim : int\n        Size of hidden layer dimension.\n    use_bn : bool, optional\n        Apply batch normalization if True, default: `True).\n    """"""\n    def __init__(self, in_feats, out_feats, hidden_dim, num_layers, use_bn=True, activation=None):\n        super(GINMLP, self).__init__()\n        self.use_bn = use_bn\n        self.nn = nn.ModuleList()\n        if use_bn:\n            self.bn = nn.ModuleList()\n        self.num_layers = num_layers\n        if num_layers < 1:\n            raise ValueError(""number of MLP layers should be positive"")\n        elif num_layers == 1:\n            self.nn.append(nn.Linear(in_feats, out_feats))\n        else:\n            for i in range(num_layers - 1):\n                if i == 0:\n                    self.nn.append(nn.Linear(in_feats, hidden_dim))\n                else:\n                    self.nn.append(nn.Linear(hidden_dim, hidden_dim))\n                if use_bn:\n                    self.bn.append(nn.BatchNorm1d(hidden_dim))\n            self.nn.append(nn.Linear(hidden_dim, out_feats))\n\n    def forward(self, x):\n        h = x\n        for i in range(self.num_layers - 1):\n            h = self.nn[i](h)\n            if self.use_bn:\n                h = self.bn[i](h)\n            h = F.relu(h)\n        return self.nn[self.num_layers - 1](h)\n\n\n@register_model(""gin"")\nclass GIN(BaseModel):\n    r""""""Graph Isomorphism Network from paper `""How Powerful are Graph\n    Neural Networks?"" <https://arxiv.org/pdf/1810.00826.pdf>`__.\n\n    Args:\n        num_layers : int\n            Number of GIN layers\n        in_feats : int\n            Size of each input sample\n        out_feats : int\n            Size of each output sample\n        hidden_dim : int\n            Size of each hidden layer dimension\n        num_mlp_layers : int\n            Number of MLP layers\n        eps : float32, optional\n            Initial `\\epsilon` value, default: ``0``\n        pooling : str, optional\n            Aggregator type to use, default:\xe3\x80\x80``sum``\n        train_eps : bool, optional\n            If True, `\\epsilon` will be a learnable parameter, default: ``True``\n    """"""\n\n    @staticmethod\n    def add_args(parser):\n        parser.add_argument(""--epsilon"", type=float, default=0.)\n        parser.add_argument(""--hidden-size"", type=int, default=32)\n        parser.add_argument(""--num-layers"", type=int, default=3)\n        parser.add_argument(""--num-mlp-layers"", type=int, default=2)\n        parser.add_argument(""--dropout"", type=float, default=0.5)\n        parser.add_argument(""--train-epsilon"", dest=""train_epsilon"", action=""store_false"")\n        parser.add_argument(""--pooling"", type=str, default=\'sum\')\n        parser.add_argument(""--batch-size"", type=int, default=128)\n        parser.add_argument(""--lr"", type=float, default=0.001)\n        parser.add_argument(""--train-ratio"", type=float, default=0.7)\n        parser.add_argument(""--test-ratio"", type=float, default=0.1)\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(\n            args.num_layers,\n            args.num_features,\n            args.num_classes,\n            args.hidden_size,\n            args.num_mlp_layers,\n            args.epsilon,\n            args.pooling,\n            args.train_epsilon,\n            args.dropout,\n            )\n\n    @classmethod\n    def split_dataset(cls, dataset, args):\n        random.shuffle(dataset)\n        train_size = int(len(dataset) * args.train_ratio)\n        test_size = int(len(dataset) * args.test_ratio)\n        bs = args.batch_size\n        train_loader = DataLoader(dataset[:train_size], batch_size=bs)\n        test_loader = DataLoader(dataset[-test_size:], batch_size=bs)\n        if args.train_ratio + args.test_ratio < 1:\n            valid_loader = DataLoader(dataset[train_size:-test_size], batch_size=bs)\n        else:\n            valid_loader = test_loader\n        return train_loader, valid_loader, test_loader\n\n    def __init__(self,\n                 num_layers,\n                 in_feats,\n                 out_feats,\n                 hidden_dim,\n                 num_mlp_layers,\n                 eps=0,\n                 pooling=\'sum\',\n                 train_eps=False,\n                 dropout=0.5,\n                 ):\n        super(GIN, self).__init__()\n        self.gin_layers = nn.ModuleList()\n        self.batch_norm = nn.ModuleList()\n        self.num_layers = num_layers\n        for i in range(num_layers - 1):\n            if i == 0:\n                mlp = GINMLP(in_feats, hidden_dim, hidden_dim, num_mlp_layers)\n            else:\n                mlp = GINMLP(hidden_dim, hidden_dim, hidden_dim, num_mlp_layers)\n            self.gin_layers.append(GINLayer(mlp, eps, train_eps))\n            self.batch_norm.append(nn.BatchNorm1d(hidden_dim))\n\n        self.linear_prediction = nn.ModuleList()\n        for i in range(self.num_layers):\n            if i == 0:\n                self.linear_prediction.append(nn.Linear(in_feats, out_feats))\n            else:\n                self.linear_prediction.append(nn.Linear(hidden_dim, out_feats))\n        self.dropout = nn.Dropout(dropout)\n        self.criterion = torch.nn.CrossEntropyLoss()\n\n    def forward(self, batch):\n        h = batch.x\n        layer_rep = [h]\n        for i in range(self.num_layers-1):\n            h = self.gin_layers[i](h, batch.edge_index)\n            h = self.batch_norm[i](h)\n            h = F.relu(h)\n            layer_rep.append(h)\n\n        final_score = 0\n        for i in range(self.num_layers):\n            # pooled = self.pooling(layer_rep[i], batch, dim=0)\n            pooled = scatter_add(layer_rep[i], batch.batch, dim=0)\n            final_score += self.dropout(self.linear_prediction[i](pooled))\n        final_score = F.softmax(final_score, dim=-1)\n        if batch.y is not None:\n            loss = self.loss(final_score, batch.y)\n            return final_score, loss\n        return final_score, None\n\n    def loss(self, output, label=None):\n        return self.criterion(output, label)\n'"
cogdl/models/nn/graphsage.py,5,"b'import random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom cogdl.layers import MeanAggregator\n\nfrom .. import BaseModel, register_model\n\n\n@register_model(""graphsage"")\nclass Graphsage(BaseModel):\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(""--num-features"", type=int)\n        parser.add_argument(""--num-classes"", type=int)\n        parser.add_argument(""--hidden-size"", type=int, nargs=\'+\',default=[128])\n        parser.add_argument(""--num-layers"", type=int, default=2)\n        parser.add_argument(""--sample-size"",type=int,nargs=\'+\',default=[10,10])\n        parser.add_argument(""--dropout"", type=float, default=0.5)\n        # fmt: on\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(\n            args.num_features,\n            args.num_classes,\n            args.hidden_size,\n            args.num_layers,\n            args.sample_size,\n            args.dropout,\n        )\n\n    # edge index based sampler\n    # @profile\n    def sampler(self, edge_index, num_sample):\n        # print(edge_index)\n        if self.adjlist == {}:\n\n            edge_index = edge_index.t().cpu().tolist()\n            for i in edge_index:\n                if not (i[0] in self.adjlist):\n                    self.adjlist[i[0]] = [i[1]]\n                else:\n                    self.adjlist[i[0]].append(i[1])\n\n        sample_list = []\n        for i in self.adjlist:\n            list = [[i, j] for j in self.adjlist[i]]\n            if len(list) > num_sample:\n                list = random.sample(list, num_sample)\n            sample_list.extend(list)\n\n        edge_idx = torch.LongTensor(sample_list).t()\n        # for i in edge_index\n        # print(""sampled"",edge_index)\n        return edge_idx\n\n    def __init__(\n        self, num_features, num_classes, hidden_size, num_layers, sample_size, dropout\n    ):\n        super(Graphsage, self).__init__()\n        self.adjlist = {}\n        self.num_features = num_features\n        self.num_classes = num_classes\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.sample_size = sample_size\n        self.dropout = dropout\n        shapes = [num_features] + hidden_size + [num_classes]\n        # print(shapes)\n        self.convs = nn.ModuleList(\n            [\n                MeanAggregator(shapes[layer], shapes[layer + 1], cached=True)\n                for layer in range(num_layers)\n            ]\n        )\n\n    # @profile\n    def forward(self, x, edge_index):\n        for i in range(self.num_layers):\n            edge_index_sp = self.sampler(edge_index, self.sample_size[i]).to(x.device)\n            adj_sp = torch.sparse_coo_tensor(\n                edge_index_sp,\n                torch.ones(edge_index_sp.shape[1]).float(),\n                (x.shape[0], x.shape[0]),\n            ).to(x.device)\n            x = self.convs[i](x, adj_sp)\n            if i != self.num_layers - 1:\n                x = F.relu(x)\n                x = F.dropout(x, p=self.dropout, training=self.training)\n        return F.log_softmax(x, dim=1)\n\n    def loss(self, data):\n        return F.nll_loss(\n            self.forward(data.x, data.edge_index)[data.train_mask],\n            data.y[data.train_mask],\n        )\n    \n    def predict(self, data):\n        return self.forward(data.x, data.edge_index)\n'"
cogdl/models/nn/gtn.py,10,"b'import math\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch_scatter import scatter_add\nimport torch_sparse\nfrom torch_geometric.utils.num_nodes import maybe_num_nodes\nfrom torch_geometric.utils import dense_to_sparse, f1_score\nfrom torch_geometric.utils import remove_self_loops, add_self_loops\nfrom torch_geometric.nn import GCNConv\n\nfrom .. import BaseModel, register_model\n\nclass GTConv(nn.Module):\n    \n    def __init__(self, in_channels, out_channels, num_nodes):\n        super(GTConv, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.weight = nn.Parameter(torch.Tensor(out_channels,in_channels))\n        self.bias = None\n        self.scale = nn.Parameter(torch.Tensor([0.1]), requires_grad=False)\n        self.num_nodes = num_nodes\n        self.reset_parameters()\n    def reset_parameters(self):\n        n = self.in_channels\n        nn.init.constant_(self.weight, 1)\n        if self.bias is not None:\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in)\n            nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, A):\n        filter = F.softmax(self.weight, dim=1)\n        num_channels = filter.shape[0]\n        results = []\n        for i in range(num_channels):\n            for j, (edge_index,edge_value) in enumerate(A):\n                if j == 0:\n                    total_edge_index = edge_index\n                    total_edge_value = edge_value*filter[i][j]\n                else:\n                    total_edge_index = torch.cat((total_edge_index, edge_index), dim=1)\n                    total_edge_value = torch.cat((total_edge_value, edge_value*filter[i][j]))\n            index, value = torch_sparse.coalesce(total_edge_index.detach(), total_edge_value, m=self.num_nodes, n=self.num_nodes)\n            results.append((index, value))\n        return results\n\n\nclass GTLayer(nn.Module):\n    \n    def __init__(self, in_channels, out_channels, num_nodes, first=True):\n        super(GTLayer, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.first = first\n        self.num_nodes = num_nodes\n        if self.first == True:\n            self.conv1 = GTConv(in_channels, out_channels, num_nodes)\n            self.conv2 = GTConv(in_channels, out_channels, num_nodes)\n        else:\n            self.conv1 = GTConv(in_channels, out_channels, num_nodes)\n    \n    def forward(self, A, H_=None):\n        if self.first == True:\n            result_A = self.conv1(A)\n            result_B = self.conv2(A)                \n            W = [(F.softmax(self.conv1.weight, dim=1)).detach(),(F.softmax(self.conv2.weight, dim=1)).detach()]\n        else:\n            result_A = H_\n            result_B = self.conv1(A)\n            W = [(F.softmax(self.conv1.weight, dim=1)).detach()]\n        H = []\n        for i in range(len(result_A)):\n            a_edge, a_value = result_A[i]\n            b_edge, b_value = result_B[i]\n            \n            edges, values = torch_sparse.spspmm(a_edge, a_value, b_edge, b_value, self.num_nodes, self.num_nodes, self.num_nodes)\n            H.append((edges, values))\n        return H, W\n\n\n@register_model(""gtn"")\nclass GTN(BaseModel):\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(""--num-features"", type=int)\n        parser.add_argument(""--num-classes"", type=int)\n        parser.add_argument(""--num-nodes"", type=int)\n        parser.add_argument(""--hidden-size"", type=int, default=64)\n        parser.add_argument(""--num-layers"", type=int, default=2)\n        parser.add_argument(""--num-edge"", type=int, default=2)\n        parser.add_argument(""--num-channels"", type=int, default=2)\n        # fmt: on\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(\n            args.num_edge, \n            args.num_channels,\n            args.num_features,\n            args.hidden_size,\n            args.num_classes,\n            args.num_nodes,\n            args.num_layers,\n        )\n\n    def __init__(self, num_edge, num_channels, w_in, w_out, num_class, num_nodes, num_layers):\n        super(GTN, self).__init__()\n        self.num_edge = num_edge\n        self.num_channels = num_channels\n        self.num_nodes = num_nodes\n        self.w_in = w_in\n        self.w_out = w_out\n        self.num_class = num_class\n        self.num_layers = num_layers\n        layers = []\n        for i in range(num_layers):\n            if i == 0:\n                layers.append(GTLayer(num_edge, num_channels, num_nodes, first=True))\n            else:\n                layers.append(GTLayer(num_edge, num_channels, num_nodes, first=False))\n        self.layers = nn.ModuleList(layers)\n        self.cross_entropy_loss = nn.CrossEntropyLoss()\n        self.gcn = GCNConv(in_channels=self.w_in, out_channels=w_out)\n        self.linear1 = nn.Linear(self.w_out*self.num_channels, self.w_out)\n        self.linear2 = nn.Linear(self.w_out, self.num_class)\n\n    def normalization(self, H):\n        norm_H = []\n        for i in range(self.num_channels):\n            edge, value=H[i]\n            edge, value = remove_self_loops(edge, value)\n            deg_row, deg_col = self.norm(edge.detach(), self.num_nodes, value.detach())\n            value = deg_col * value\n            norm_H.append((edge, value))\n        return norm_H\n\n    def norm(self, edge_index, num_nodes, edge_weight, improved=False, dtype=None):\n        with torch.no_grad(): \n            if edge_weight is None:\n                edge_weight = torch.ones((edge_index.size(1), ),\n                                        dtype=dtype,\n                                        device=edge_index.device)\n            edge_weight = edge_weight.view(-1)\n            assert edge_weight.size(0) == edge_index.size(1)\n            row, col = edge_index\n            deg = scatter_add(edge_weight, col, dim=0, dim_size=num_nodes)\n            deg_inv_sqrt = deg.pow(-1)\n            deg_inv_sqrt[deg_inv_sqrt == float(\'inf\')] = 0\n\n        return deg_inv_sqrt[row], deg_inv_sqrt[col]\n\n    def forward(self, A, X, target_x, target):\n        Ws = []\n        for i in range(self.num_layers):\n            if i == 0:\n                H, W = self.layers[i](A)\n            else:\n                H = self.normalization(H)\n                H, W = self.layers[i](A, H)\n            Ws.append(W)\n        for i in range(self.num_channels):\n            if i==0:\n                edge_index, edge_weight = H[i][0], H[i][1]\n                X_ = self.gcn(X,edge_index=edge_index.detach(), edge_weight=edge_weight)\n                X_ = F.relu(X_)\n            else:\n                edge_index, edge_weight = H[i][0], H[i][1]\n                X_ = torch.cat((X_,F.relu(self.gcn(X,edge_index=edge_index.detach(), edge_weight=edge_weight))), dim=1)\n        X_ = self.linear1(X_)\n        X_ = F.relu(X_)\n        #X_ = F.dropout(X_, p=0.5)\n        y = self.linear2(X_[target_x])\n        loss = self.cross_entropy_loss(y, target)\n        return loss, y, Ws\n\n    def loss(self, data):\n        loss, y, _ = self.forward(data.adj, data.x, data.train_node, data.train_target)\n        return loss\n    \n    def evaluate(self, data, nodes, targets):\n        loss, y, _ = self.forward(data.adj, data.x, nodes, targets)\n        f1 = torch.mean(f1_score(torch.argmax(y, dim=1), targets, num_classes=3))\n        return loss.item(), f1.item()\n'"
cogdl/models/nn/han.py,5,"b'import math\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch_scatter import scatter_add\nimport torch_sparse\nfrom torch_geometric.utils.num_nodes import maybe_num_nodes\nfrom torch_geometric.utils import dense_to_sparse, f1_score\nfrom torch_geometric.utils import remove_self_loops, add_self_loops\nfrom torch_geometric.nn import GCNConv, GATConv\n\nfrom .. import BaseModel, register_model\n\nclass AttentionLayer(nn.Module):\n    def __init__(self, num_features):\n        super(AttentionLayer, self).__init__()\n        self.linear = nn.Linear(num_features, 1)\n    \n    def forward(self, x):\n        att = self.linear(x).view(-1, 1, x.shape[1])\n        return torch.matmul(att, x).squeeze(1)\n\nclass HANLayer(nn.Module):\n    def __init__(self, num_edge, w_in, w_out):\n        super(HANLayer, self).__init__()\n        self.gat_layer = nn.ModuleList()\n        for _ in range(num_edge):\n            self.gat_layer.append(GATConv(w_in, w_out // 8, 8))\n        self.att_layer = AttentionLayer(w_out)\n\n    def forward(self, x, adj):\n        output = []\n        for i, edge in enumerate(adj):\n            output.append(self.gat_layer[i](x, edge[0]))\n        output = torch.stack(output, dim=1)\n        \n        return self.att_layer(output)\n\n@register_model(""han"")\nclass HAN(BaseModel):\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(""--num-features"", type=int)\n        parser.add_argument(""--num-classes"", type=int)\n        parser.add_argument(""--num-nodes"", type=int)\n        parser.add_argument(""--hidden-size"", type=int, default=64)\n        parser.add_argument(""--num-layers"", type=int, default=2)\n        parser.add_argument(""--num-edge"", type=int, default=2)\n        # fmt: on\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(\n            args.num_edge, \n            args.num_features,\n            args.hidden_size,\n            args.num_classes,\n            args.num_nodes,\n            args.num_layers,\n        )\n\n    def __init__(self, num_edge, w_in, w_out, num_class, num_nodes, num_layers):\n        super(HAN, self).__init__()\n        self.num_edge = num_edge\n        self.num_nodes = num_nodes\n        self.w_in = w_in\n        self.w_out = w_out\n        self.num_class = num_class\n        self.num_layers = num_layers\n        layers = []\n        for i in range(num_layers):\n            if i == 0:\n                layers.append(HANLayer(num_edge, w_in, w_out))\n            else:\n                layers.append(HANLayer(num_edge, w_out, w_out))\n\n        self.layers = nn.ModuleList(layers)\n        self.cross_entropy_loss = nn.CrossEntropyLoss()\n        self.linear = nn.Linear(self.w_out, self.num_class)\n\n    def forward(self, A, X, target_x, target):\n        for i in range(self.num_layers):\n            X = self.layers[i](X, A)\n\n        y = self.linear(X[target_x])\n        loss = self.cross_entropy_loss(y, target)\n        return loss, y\n\n    def loss(self, data):\n        loss, y = self.forward(data.adj, data.x, data.train_node, data.train_target)\n        return loss\n    \n    def evaluate(self, data, nodes, targets):\n        loss, y = self.forward(data.adj, data.x, nodes, targets)\n        f1 = torch.mean(f1_score(torch.argmax(y, dim=1), targets, num_classes=3))\n        return loss.item(), f1.item()\n'"
cogdl/models/nn/infograph.py,14,"b'import math\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_scatter import scatter_add\nfrom torch_geometric.nn import NNConv, Set2Set\n\nfrom .gin import GINLayer, GINMLP\nfrom cogdl.data import DataLoader\nfrom .. import BaseModel, register_model\n\n\nclass SUPEncoder(torch.nn.Module):\n    r""""""Encoder used in supervised model with Set2set in paper `""Order Matters: Sequence to sequence for sets""\n    <https://arxiv.org/abs/1511.06391>` and NNConv in paper `""Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on\n    Graphs"" <https://arxiv.org/abs/1704.02901>`\n\n    """"""\n    def __init__(self, num_features, dim, num_layers=1):\n        super(SUPEncoder, self).__init__()\n        self.lin0 = torch.nn.Linear(num_features, dim)\n\n        nnu = nn.Sequential(nn.Linear(5, 128), nn.ReLU(), nn.Linear(128, dim * dim))\n        self.conv = NNConv(dim, dim, nnu, aggr=\'mean\', root_weight=False)\n        self.gru = nn.GRU(dim, dim)\n\n        self.set2set = Set2Set(dim, processing_steps=3)\n        # self.lin1 = torch.nn.Linear(2 * dim, dim)\n        # self.lin2 = torch.nn.Linear(dim, 1)\n\n    def forward(self, x, edge_index, batch, edge_attr):\n        out = F.relu(self.lin0(x))\n        h = out.unsqueeze(0)\n\n        feat_map = []\n        for i in range(3):\n            m = F.relu(self.conv(out, edge_index, edge_attr))\n            out, h = self.gru(m.unsqueeze(0), h)\n            out = out.squeeze(0)\n            feat_map.append(out)\n\n        out = self.set2set(out, batch)\n        return out, feat_map[-1]\n\n\nclass Encoder(nn.Module):\n    r""""""Encoder stacked with GIN layers\n\n    Parameters\n    ----------\n    in_feats : int\n        Size of each input sample.\n    hidden_feats : int\n        Size of output embedding.\n    num_layers : int, optional\n        Number of GIN layers, default: ``3``.\n    num_mlp_layers : int, optional\n        Number of MLP layers for each GIN layer, default: ``2``.\n    pooling : str, optional\n        Aggragation type, default : ``sum``.\n\n    """"""\n    def __init__(self, in_feats, hidden_dim, num_layers=3, num_mlp_layers=2, pooling=\'sum\'):\n        super(Encoder, self).__init__()\n        self.num_layers = num_layers\n        self.gnn_layers = nn.ModuleList()\n        self.bn_layers = nn.ModuleList()\n        for i in range(num_layers):\n            if i == 0:\n                mlp = GINMLP(in_feats, hidden_dim, hidden_dim, num_mlp_layers, use_bn=True)\n            else:\n                mlp = GINMLP(hidden_dim, hidden_dim, hidden_dim, num_mlp_layers, use_bn=True)\n            self.gnn_layers.append(GINLayer(\n               mlp, eps=0, train_eps=True\n            ))\n            self.bn_layers.append(nn.BatchNorm1d(hidden_dim))\n\n        if pooling == \'sum\':\n            self.pooling = scatter_add\n        else:\n            raise NotImplementedError\n\n    def forward(self, x, edge_index, batch, *args):\n        if x is None:\n            x = torch.ones((batch.shape[0], 1)).to(batch.device)\n        layer_rep = []\n        for i in range(self.num_layers):\n            x = F.relu(self.bn_layers[i](self.gnn_layers[i](x, edge_index)))\n            layer_rep.append(x)\n\n        pooled_rep = [self.pooling(h, batch, 0) for h in layer_rep]\n        node_rep = torch.cat(layer_rep, dim=1)\n        graph_rep = torch.cat(pooled_rep, dim=1)\n        return graph_rep, node_rep\n\n\nclass FF(nn.Module):\n    r""""""Residual MLP layers.\n\n    ..math::\n        out = \\mathbf{MLP}(x) + \\mathbf{Linear}(x)\n\n    Paramaters\n    ----------\n    in_feats : int\n        Size of each input sample\n    out_feats : int\n        Size of each output sample\n    """"""\n    def __init__(self, in_feats, out_feats):\n        super(FF, self).__init__()\n        self.block = GINMLP(in_feats, out_feats, out_feats, num_layers=3, use_bn=False)\n        self.shortcut = nn.Linear(in_feats, out_feats)\n\n    def forward(self, x):\n        return F.relu(self.block(x)) + self.shortcut(x)\n\n\n@register_model(""infograph"")\nclass InfoGraph(BaseModel):\n    r""""""Implimentation of Infograph in paper `""InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation\n     Learning via Mutual Information Maximization"" <https://openreview.net/forum?id=r1lfF2NYvH>__. `\n\n     Parameters\n     ----------\n     in_feats : int\n        Size of each input sample.\n    out_feats : int\n        Size of each output sample.\n    num_layers : int, optional\n        Number of MLP layers in encoder, default: ``3``.\n    unsup : bool, optional\n        Use unsupervised model if True, default: ``True``.\n    """"""\n\n    @staticmethod\n    def add_args(parser):\n        parser.add_argument(""--hidden-size"", type=int, default=512)\n        parser.add_argument(""--batch-size"", type=int, default=20)\n        parser.add_argument(""--target"", dest=\'target\', type=int, default=0,\n                            help=\'\')\n        parser.add_argument(""--train-num"", dest=\'train_num\', type=int, default=5000)\n        parser.add_argument(""--num-layers"", type=int, default=1)\n        parser.add_argument(""--unsup"", dest=""unsup"", action=""store_false"")\n        parser.add_argument(""--epoch"", type=int, default=15)\n        parser.add_argument(""--lr"", type=float, default=0.0001)\n        parser.add_argument(""--train-ratio"", type=float, default=0.7)\n        parser.add_argument(""--test-ratio"", type=float, default=0.1)\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(\n            args.num_features,\n            args.hidden_size,\n            args.num_classes,\n            args.num_layers,\n            args.unsup\n        )\n\n    @classmethod\n    def split_dataset(cls, dataset, args):\n        if args.dataset == ""qm9"":\n            test_dataset = dataset[:10000]\n            val_dataset = dataset[10000:20000]\n            train_dataset = dataset[20000:20000 + args.train_num]\n            return DataLoader(train_dataset, batch_size=args.batch_size), DataLoader(val_dataset, batch_size=args.batch_size),\\\n                   DataLoader(test_dataset, batch_size=args.batch_size)\n        else:\n            random.shuffle(dataset)\n            train_size = int(len(dataset) * args.train_ratio)\n            test_size = int(len(dataset) * args.test_ratio)\n            bs = args.batch_size\n            train_loader = DataLoader(dataset[:train_size], batch_size=bs)\n            test_loader = DataLoader(dataset[-test_size:], batch_size=bs)\n            if args.train_ratio + args.test_ratio < 1:\n                valid_loader = DataLoader(dataset[train_size:-test_size], batch_size=bs)\n            else:\n                valid_loader = test_loader\n            return train_loader, valid_loader, test_loader\n\n    def __init__(self, in_feats, hidden_dim, out_feats, num_layers=3, unsup=True):\n        super(InfoGraph, self).__init__()\n\n        self.unsup = unsup\n        self.emb_dim = hidden_dim\n        self.out_feats = out_feats\n\n        self.sem_fc1 = nn.Linear(num_layers*hidden_dim, hidden_dim)\n        self.sem_fc2 = nn.Linear(hidden_dim, out_feats)\n\n        if unsup:\n            self.unsup_encoder = Encoder(in_feats, hidden_dim, num_layers)\n            self.register_parameter(""sem_encoder"", None)\n        else:\n            self.unsup_encoder = Encoder(in_feats, hidden_dim, num_layers)\n            self.sem_encoder = Encoder(in_feats, hidden_dim, num_layers)\n        self._fc1 = FF(num_layers*hidden_dim, hidden_dim)\n        self._fc2 = FF(num_layers*hidden_dim, hidden_dim)\n        self.local_dis = FF(num_layers*hidden_dim, hidden_dim)\n        self.global_dis = FF(num_layers*hidden_dim, hidden_dim)\n\n        self.criterion = nn.MSELoss()\n\n    def reset_parameters(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight.data)\n\n    def forward(self, batch):\n        if self.unsup:\n            return self.unsup_forward(batch.x, batch.edge_index, batch.batch)\n        else:\n            return self.sup_forward(batch.x, batch.edge_index, batch.batch, batch.y, batch.edge_attr)\n\n    def sup_forward(self, x, edge_index=None, batch=None, label=None, edge_attr=None):\n        node_feat, graph_feat = self.sem_encoder(x, edge_index, batch, edge_attr)\n        node_feat = F.relu(self.sem_fc1(node_feat))\n        node_feat = self.sem_fc2(node_feat)\n        prediction = F.softmax(node_feat, dim=1)\n        if label is not None:\n            loss = self.sup_loss(prediction, label)\n            loss += self.unsup_loss(x, edge_index, batch)[1]\n            loss += self.unsup_sup_loss(x, edge_index, batch)\n            return prediction, loss\n        return prediction, None\n\n    def unsup_forward(self, x, edge_index=None, batch=None):\n        return self.unsup_loss(x, edge_index, batch)\n\n    def sup_loss(self, prediction, label=None):\n        sup_loss = self.criterion(prediction, label)\n        return sup_loss\n\n    def unsup_loss(self, x, edge_index=None, batch=None):\n        graph_feat, node_feat = self.unsup_encoder(x, edge_index, batch)\n\n        local_encode = self.local_dis(node_feat)\n        global_encode = self.global_dis(graph_feat)\n\n        num_graphs = graph_feat.shape[0]\n        num_nodes = node_feat.shape[0]\n\n        pos_mask = torch.zeros((num_nodes, num_graphs)).to(x.device)\n        neg_mask = torch.ones((num_nodes, num_graphs)).to(x.device)\n        for nid, gid in enumerate(batch):\n            pos_mask[nid][gid] = 1\n            neg_mask[nid][gid] = 0\n        glob_local_mi = torch.mm(local_encode, global_encode.t())\n        loss = InfoGraph.mi_loss(pos_mask, neg_mask, glob_local_mi, num_nodes, num_nodes * (num_graphs-1))\n        return graph_feat, loss\n\n    def unsup_sup_loss(self, x, edge_index, batch):\n        sem_g_feat, _ = self.sem_encoder(x, edge_index, batch)\n        un_g_feat, _ = self.unsup_encoder(x, edge_index, batch)\n\n        sem_encode = self._fc1(sem_g_feat)\n        un_encode = self._fc2(un_g_feat)\n\n        num_graphs = sem_encode.shape[0]\n        pos_mask = torch.eye(num_graphs).to(x.device)\n        neg_mask = 1 - pos_mask\n\n        mi = torch.mm(sem_encode, un_encode.t())\n        loss = InfoGraph.mi_loss(pos_mask, neg_mask, mi, pos_mask.sum(), neg_mask.sum())\n        return loss\n\n    @staticmethod\n    def mi_loss(pos_mask, neg_mask, mi, pos_div, neg_div):\n        pos_mi = pos_mask * mi\n        neg_mi = neg_mask * mi\n        # pos_loss = -(math.log(2.) - F.softplus(-pos_mi)).sum()\n        # neg_loss = (-math.log(2.) + F.softplus(-neg_mi) + neg_mi).sum()\n        pos_loss = F.softplus(-pos_mi).sum()\n        neg_loss = (F.softplus(neg_mi)).sum()\n        return pos_loss/pos_div + neg_loss/neg_div\n'"
cogdl/models/nn/mixhop.py,3,"b'import math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.parameter import Parameter\n\nfrom cogdl.layers import MixHopLayer\n\nfrom .. import BaseModel, register_model\n\n\n@register_model(""mixhop"")\nclass MixHop(BaseModel):\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(""--num-features"", type=int)\n        parser.add_argument(""--num-classes"", type=int)\n        parser.add_argument(""--hidden-size"", type=int, default=64)\n        parser.add_argument(""--num-layers"", type=int, default=2)\n        parser.add_argument(""--dropout"", type=float, default=0.5)\n        # fmt: on\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(\n            args.num_features,\n            args.num_classes,\n            args.hidden_size,\n            args.num_layers,\n            args.dropout,\n        )\n\n    def __init__(self, num_features, num_classes, hidden_size, num_layers, dropout):\n        super(MixHop, self).__init__()\n\n        self.dropout = dropout\n\n        self.num_features = num_features\n        self.num_classes = num_classes\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n\n        shapes = [num_features] + [hidden_size * 2] * (num_layers)\n        self.mixhops = nn.ModuleList(\n            [\n                MixHopLayer(shapes[layer], [1, 2], [hidden_size] * 2)\n                for layer in range(num_layers)\n            ]\n        )\n        self.fc = nn.Linear(hidden_size * 2, num_classes)\n\n    def forward(self, x, edge_index):\n        for mixhop in self.mixhops:\n            x = F.relu(mixhop(x, edge_index))\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.fc(x)\n        return F.log_softmax(x, dim=1)\n\n    def loss(self, data):\n        return F.nll_loss(\n            self.forward(data.x, data.edge_index)[data.train_mask],\n            data.y[data.train_mask],\n        )\n    \n    def predict(self, data):\n        return self.forward(data.x, data.edge_index)\n'"
cogdl/models/nn/mlp.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .. import BaseModel, register_model\n\n\n@register_model(""mlp"")\nclass MLP(BaseModel):\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(""--num-features"", type=int)\n        parser.add_argument(""--num-classes"", type=int)\n        parser.add_argument(""--hidden-size"", type=int, default=16)\n        parser.add_argument(""--num-layers"", type=int, default=2)\n        parser.add_argument(""--dropout"", type=float, default=0.5)\n        # fmt: on\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(\n            args.num_features,\n            args.num_classes,\n            args.hidden_size,\n            args.num_layers,\n            args.dropout,\n        )\n\n    def __init__(self, num_features, num_classes, hidden_size, num_layers, dropout):\n        super(MLP, self).__init__()\n\n        self.num_features = num_features\n        self.num_classes = num_classes\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        shapes = [num_features] + [hidden_size] * (num_layers - 1) + [num_classes]\n        self.mlp = nn.ModuleList(\n            [nn.Linear(shapes[layer], shapes[layer + 1]) for layer in range(num_layers)]\n        )\n\n    def forward(self, x, edge_index):\n        for fc in self.mlp[:-1]:\n            x = F.relu(fc(x))\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.mlp[-1](x)\n        return F.log_softmax(x, dim=1)\n\n    def loss(self, data):\n        return F.nll_loss(\n            self.forward(data.x, data.edge_index)[data.train_mask],\n            data.y[data.train_mask],\n        )\n    \n    def predict(self, data):\n        return self.forward(data.x, data.edge_index)\n'"
cogdl/models/nn/patchy_san.py,5,"b'\n\nimport math\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom cogdl.data import DataLoader\nfrom .. import BaseModel, register_model\n\nimport numpy as np\nimport networkx as nx\nimport functools\n\n\n@register_model(""patchy_san"")\nclass PatchySAN(BaseModel):\n    r""""""The Patchy-SAN model from the `""Learning Convolutional Neural Networks for Graphs""\n    <https://arxiv.org/abs/1605.05273>`_ paper.\n    \n    Args:\n        batch_size (int) : The batch size of training.\n        sample (int) : Number of chosen vertexes.\n        stride (int) : Node selection stride.\n        neighbor (int) : The number of neighbor for each node.\n        iteration (int) : The number of training iteration.\n    """"""\n    @staticmethod\n    def add_args(parser):\n        parser.add_argument(""--batch-size"", type=int, default=20)\n        parser.add_argument(\'--sample\', default=30, type=int, help=\'Number of chosen vertexes\')\n        parser.add_argument(\'--stride\', default=1, type=int, help=\'Stride of chosen vertexes\')\n        parser.add_argument(\'--neighbor\', default=10, type=int, help=\'Number of neighbor in constructing features\')\n        parser.add_argument(\'--iteration\', default=5, type=int, help=\'Number of iteration\')\n        parser.add_argument(""--train-ratio"", type=float, default=0.7)\n        parser.add_argument(""--test-ratio"", type=float, default=0.1)\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(args.batch_size, args.num_features, args.num_classes, args.sample, args.stride, args.neighbor, args.iteration)\n    \n    @classmethod        \n    def split_dataset(self, dataset, args):\n        random.shuffle(dataset)\n        # process each graph and add it into Data() as attribute tx\n        for i, data in enumerate(dataset):\n            new_feature = get_single_feature(dataset[i], args.num_features, args.num_classes, args.sample, args.neighbor, args.stride)\n            dataset[i].tx = torch.from_numpy(new_feature)\n\n        train_size = int(len(dataset) * args.train_ratio)\n        test_size = int(len(dataset) * args.test_ratio)\n        bs = args.batch_size\n        train_loader = DataLoader(dataset[:train_size], batch_size=bs)\n        test_loader = DataLoader(dataset[-test_size:], batch_size=bs)\n        if args.train_ratio + args.test_ratio < 1:\n            valid_loader = DataLoader(dataset[train_size:-test_size], batch_size=bs)\n        else:\n            valid_loader = test_loader\n        return train_loader, valid_loader, test_loader\n\n    def __init__(self, batch_size, num_features, num_classes, num_sample, stride, num_neighbor, iteration):\n        super(PatchySAN, self).__init__()\n        \n        self.batch_size = batch_size\n        self.num_features = num_features\n        self.num_classes = num_classes\n        self.num_sample = num_sample\n        self.stride = stride\n        self.num_neighbor = num_neighbor\n        self.iteration = iteration\n        \n        self.build_model(self.num_features, self.num_sample, self.num_neighbor, self.num_classes)\n\n\n    def build_model(self, num_channel, num_sample, num_neighbor, num_class):\n        rep1, stride1 = 4, 4\n        num_filter1, num_filter2 = 16, 8\n        self.conv1 = nn.Conv1d(num_channel, num_filter1, rep1, stride=stride1, groups=1)\n        self.conv2 = nn.Conv1d(num_filter1, num_filter2, num_neighbor, stride=1, groups=1)\n        \n        num_lin = (int(num_sample * num_neighbor/ stride1 ) - num_neighbor + 1)  * num_filter2\n        self.lin1 = torch.nn.Linear(num_lin, 128)\n        self.lin2 = torch.nn.Linear(128, num_class)\n        \n        self.nn = nn.Sequential(\n            self.conv1,\n            nn.ReLU(),\n            self.conv2,\n            nn.ReLU(),\n            nn.Flatten(),\n            self.lin1,\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            self.lin2,\n            nn.Softmax(),\n        )\n        \n        self.criterion = nn.CrossEntropyLoss()\n        # self.criterion = nn.NLLLoss()\n\n    def forward(self, batch):\n        logits= self.nn(batch.tx)\n        if batch.y is not None:\n            return logits, self.criterion(logits, batch.y)\n        return logits, None\n    \n\ndef assemble_neighbor(G, node, num_neighbor, sorted_nodes):\n    """"""assemble neighbors for node with BFS strategy""""""\n    neighbors_dict = dict()\n    new_neighbors_dict = dict()\n    neighbors_dict[node] = 0\n    new_neighbors_dict[node] = 0\n    # assemble over K neighbors with BFS strategy\n    while len(neighbors_dict) < num_neighbor and len(new_neighbors_dict) > 0:\n        temp_neighbor_dict = dict()\n        for v, d in new_neighbors_dict.items():\n            for new_v in G.neighbors(v):\n                if new_v not in temp_neighbor_dict:\n                    temp_neighbor_dict[new_v] = d + 1\n        n = len(neighbors_dict)\n        for v, d in temp_neighbor_dict.items():\n            if v not in neighbors_dict:\n                neighbors_dict[v] = d\n        new_neighbors_dict = temp_neighbor_dict\n        # break if the number of neighbors do not increase\n        if n == len(neighbors_dict):\n            break\n\n    # add dummy disconnected nodes if number is not suffice\n    while len(neighbors_dict) < num_neighbor:\n        rand_node = sorted_nodes[random.randint(0, len(sorted_nodes) - 1)][0]\n        if rand_node not in neighbors_dict:\n            neighbors_dict[rand_node] = 10\n    return neighbors_dict\n\n\ndef cmp(s1, s2):\n    list1 = [int(l) for l in s1.strip().split("" "")]\n    list2 = [int(l) for l in s2.strip().split("" "")]\n    i = 0\n    while i < len(list1) and i < len(list2):\n        if list1[i] < list2[i]:\n            return -1\n        if list1[i] > list2[i]:\n            return 1\n        i += 1\n    if i < len(list1):\n        return 1\n    elif i < len(list2):\n        return -1\n    else:\n        return 0\n\n\ndef one_dim_wl(graph_list, init_labels, iteration=5):\n    """"""1-dimension Wl method used for node normalization for all the subgraphs""""""\n    sorted_labels = sorted(list(set(init_labels.values())))\n    label_dict = dict([(label, index) for index, label in enumerate(sorted_labels)])\n    graph_label_list = []\n    for t in range(iteration):\n        new_label_dict = dict()\n        # get label for each node in each graph\n        if t == 0:\n            # get label according to nodes\' attribute labels\n            for i in range(len(graph_list)):\n                labels = dict()\n                for id, v in enumerate(graph_list[i].nodes()):\n                    labels[v] = str(init_labels[v])\n                    new_label_dict[labels[v]] = 1\n                graph_label_list.append(labels)\n        else:\n            # get label according to neighbors\' labels\n            for i in range(len(graph_list)):\n                labels = dict()\n                for id, v in enumerate(graph_list[i].nodes()):\n                    neighbor_labels = [graph_label_list[i][v2] for v2 in graph_list[i].neighbors(v)]\n                    sorted_labels = [str(l) for l in sorted(neighbor_labels)]\n                    # concentrate node label and its sorted neighbors\' labels\n                    new_label = str(graph_label_list[i][v]) + "" "" + "" "".join(sorted_labels)\n                    new_label_dict[new_label] = 1\n                    labels[v] = new_label\n                graph_label_list[i] = labels.copy()\n\n        # sort new labels with dictionary order\n        sorted_list = sorted(new_label_dict.keys(), key=functools.cmp_to_key(cmp), reverse=False)\n        for new_label in sorted_list:\n            # add new label to the labels_dict\n            if new_label not in label_dict:\n                label_dict[new_label] = len(label_dict)\n\n        # relabel node labels with new label\n        for i in range(len(graph_list)):\n            for id, v in enumerate(graph_list[i].nodes()):\n                graph_label_list[i][v] = label_dict[graph_label_list[i][v]]\n    return graph_label_list\n\n\ndef node_selection_with_1d_wl(G, features, num_channel, num_sample, num_neighbor, stride):\n    """"""construct features for cnn""""""\n    X = np.zeros((num_channel, num_sample, num_neighbor), dtype=np.float32)\n    node2id = dict([(node, vid) for vid, node in enumerate(G.nodes())])\n    id2node = dict(zip(node2id.values(), node2id.keys()))\n    betweenness = nx.betweenness_centrality(G)\n    sorted_nodes = sorted(betweenness.items(), key=lambda d: d[1], reverse=False)\n    # obtain normalized neighbors\' features for each vertex\n    i = 0\n    j = 0\n    root_list = []\n    distance_list = []\n    graph_list = []\n    while j < num_sample:\n        if i < len(sorted_nodes):\n            neighbors_dict = assemble_neighbor(G, sorted_nodes[i][0], num_neighbor, sorted_nodes)\n            # construct subgraph and sort neighbors with a labeling measurement like degree centrality\n            sub_g = G.subgraph(neighbors_dict.keys())\n            root_list.append(sorted_nodes[i][0])\n            distance_list.append(neighbors_dict)\n            graph_list.append(sub_g)\n        else:\n            # zero receptive field\n            X[:, j, :] = np.zeros((num_channel, num_neighbor), dtype=np.float32)\n        i += stride\n        j += 1\n    init_labels = dict([(v, features[id].argmax(axis=0)) for v, id in node2id.items()])\n    graph_labels_list = one_dim_wl(graph_list, init_labels)\n\n    # finally relabel based on 1d-wl and distance to root node\n    for i in range(len(root_list)):\n        # set root node the first position\n        graph_labels_list[i][root_list[i]] = 0\n        sorted_measurement = dict([(v, [measure, distance_list[i][v]]) for v, measure in graph_labels_list[i].items()])\n        sorted_neighbor = sorted(sorted_measurement.items(), key=lambda d: d[1], reverse=False)[:num_neighbor]\n        reorder_dict = dict(zip(sorted(sorted_measurement.keys()), range(len(sorted_measurement))))\n        X[:, i, :] = features[[reorder_dict[v] for v, measure in sorted_neighbor]].T\n    return X.reshape(num_channel, num_sample * num_neighbor)\n\n\ndef get_single_feature(data, num_features, num_classes, num_sample, num_neighbor, stride=1):\n    """"""construct features""""""\n    data_list = [data]\n    X = np.zeros((len(data_list), num_features, num_sample * num_neighbor), dtype=np.float32)\n    for i in range(len(data_list)):\n        edge_index, features = data_list[i].edge_index, data_list[i].x\n        G = nx.Graph()\n        G.add_edges_from(edge_index.t().tolist())\n        # print(""graph"", i, ""number of node"", G.number_of_nodes(), ""edge"", G.number_of_edges())\n        if G.number_of_nodes() > num_neighbor:\n            X[i] = node_selection_with_1d_wl(G, features.cpu().numpy(), num_features, num_sample, num_neighbor, stride)\n    X = X.astype(np.float32)\n    return X    '"
cogdl/models/nn/pyg_cheb.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn.conv import ChebConv\n\nfrom .. import BaseModel, register_model\n\n\n@register_model(""pyg_cheb"")\nclass Chebyshev(BaseModel):\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(""--num-features"", type=int)\n        parser.add_argument(""--num-classes"", type=int)\n        parser.add_argument(""--hidden-size"", type=int, default=64)\n        parser.add_argument(""--num-layers"", type=int, default=2)\n        parser.add_argument(""--dropout"", type=float, default=0.5)\n        parser.add_argument(""--filter-size"", type=int, default=5)\n        # fmt: on\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(\n            args.num_features,\n            args.num_classes,\n            args.hidden_size,\n            args.num_layers,\n            args.dropout,\n            args.filter_size,\n        )\n\n    def __init__(self, num_features, num_classes, hidden_size, num_layers, dropout, filter_size):\n        super(Chebyshev, self).__init__()\n\n        self.num_features = num_features\n        self.num_classes = num_classes\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.filter_size = filter_size\n        shapes = [num_features] + [hidden_size] * (num_layers - 1) + [num_classes]\n        self.convs = nn.ModuleList(\n            [\n                ChebConv(shapes[layer], shapes[layer + 1], filter_size)\n                for layer in range(num_layers)\n            ]\n        )\n\n    def forward(self, x, edge_index):\n        for conv in self.convs[:-1]:\n            x = F.relu(conv(x, edge_index))\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.convs[-1](x, edge_index)\n        return F.log_softmax(x, dim=1)\n\n    def loss(self, data):\n        return F.nll_loss(\n            self.forward(data.x, data.edge_index)[data.train_mask],\n            data.y[data.train_mask],\n        )\n    \n    def predict(self, data):\n        return self.forward(data.x, data.edge_index)\n'"
cogdl/models/nn/pyg_dgcnn.py,3,"b'import random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import DynamicEdgeConv, global_max_pool\n\nfrom .. import BaseModel, register_model\nfrom .gin import GINMLP\nfrom cogdl.data import DataLoader, Data\n\n\n@register_model(""pyg_dgcnn"")\nclass DGCNN(BaseModel):\n    r""""""EdgeConv and DynamicGraph in paper `""Dynamic Graph CNN for Learning on\n    Point Clouds"" <https://arxiv.org/pdf/1801.07829.pdf>__ .`\n\n    Parameters\n    ----------\n    in_feats : int\n        Size of each input sample.\n    out_feats : int\n        Size of each output sample.\n    hidden_dim : int\n        Dimension of hidden layer embedding.\n    k : int\n        Number of neareast neighbors.\n    """"""\n    @staticmethod\n    def add_args(parser):\n        parser.add_argument(""--hidden-size"", type=int, default=64)\n        parser.add_argument(""--batch-size"", type=int, default=20)\n        parser.add_argument(""--train-ratio"", type=float, default=0.7)\n        parser.add_argument(""--test-ratio"", type=float, default=0.1)\n        parser.add_argument(""--lr"", type=float, default=0.001)\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(\n            args.num_features,\n            args.hidden_size,\n            args.num_classes,\n        )\n\n    @classmethod\n    def split_dataset(cls, dataset, args):\n        if ""ModelNet"" in args.dataset:\n            train_data = [Data(x=d.pos, y=d.y) for d in dataset[""train""]]\n            test_data = [Data(x=d.pos, y=d.y) for d in dataset[""test""]]\n            train_loader = DataLoader(train_data, batch_size=args.batch_size, num_workers=6)\n            test_loader = DataLoader(test_data, batch_size=args.batch_size, num_workers=6, shuffle=False)\n            return train_loader, test_loader, test_loader\n        else:\n            random.shuffle(dataset)\n            train_size = int(len(dataset) * args.train_ratio)\n            test_size = int(len(dataset) * args.test_ratio)\n            bs = args.batch_size\n            train_loader = DataLoader(dataset[:train_size], batch_size=bs)\n            test_loader = DataLoader(dataset[-test_size:], batch_size=bs)\n            if args.train_ratio + args.test_ratio < 1:\n                valid_loader = DataLoader(dataset[train_size:-test_size], batch_size=bs)\n            else:\n                valid_loader = test_loader\n            return train_loader, valid_loader, test_loader\n\n    def __init__(self, in_feats, hidden_dim, out_feats, k=20, dropout=0.5):\n        super(DGCNN, self).__init__()\n        mlp1 = nn.Sequential(GINMLP(2*in_feats, hidden_dim, hidden_dim, num_layers=3), nn.ReLU(), nn.BatchNorm1d(hidden_dim))\n        mlp2 = nn.Sequential(GINMLP(2*hidden_dim, 2*hidden_dim, 2*hidden_dim, num_layers=1), nn.ReLU(), nn.BatchNorm1d(2*hidden_dim))\n        self.conv1 = DynamicEdgeConv(mlp1, k, ""max"")\n        self.conv2 = DynamicEdgeConv(mlp2, k, ""max"")\n        self.linear = nn.Linear(hidden_dim + 2*hidden_dim, 1024)\n        self.final_mlp = nn.Sequential(\n            nn.Linear(1024, 512),\n            nn.BatchNorm1d(512),\n            nn.Dropout(dropout),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.Dropout(dropout),\n            nn.Linear(256, out_feats)\n        )\n\n    def forward(self, batch):\n        h = batch.x\n        h1 = self.conv1(h, batch.batch)\n        h2 = self.conv2(h1, batch.batch)\n        h = self.linear(torch.cat([h1, h2], dim=1))\n        h = global_max_pool(h, batch.batch)\n        out = self.final_mlp(h)\n        out = F.log_softmax(out, dim=-1)\n        if batch.y is not None:\n            loss = F.nll_loss(out, batch.y)\n            return out, loss\n        return out, None\n\n'"
cogdl/models/nn/pyg_drgat.py,2,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn.conv import GATConv\n\nfrom cogdl.layers import SELayer\n\nfrom .. import BaseModel, register_model\n\n\n@register_model(""drgat"")\nclass DrGAT(BaseModel):\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(""--num-features"", type=int)\n        parser.add_argument(""--num-classes"", type=int)\n        parser.add_argument(""--hidden-size"", type=int, default=8)\n        parser.add_argument(""--num-heads"", type=int, default=8)\n        parser.add_argument(""--dropout"", type=float, default=0.6)\n        # fmt: on\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(\n            args.num_features,\n            args.num_classes,\n            args.hidden_size,\n            args.num_heads,\n            args.dropout,\n        )\n\n    def __init__(self, num_features, num_classes, hidden_size, num_heads, dropout):\n        super(DrGAT, self).__init__()\n        self.num_features = num_features\n        self.num_classes = num_classes\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.dropout = dropout\n        self.conv1 = GATConv(\n            num_features, hidden_size, heads=num_heads, dropout=dropout\n        )\n        self.conv2 = GATConv(hidden_size * num_heads, num_classes, dropout=dropout)\n        self.se1 = SELayer(num_features, se_channels=int(np.sqrt(num_features)))\n        self.se2 = SELayer(\n            hidden_size * num_heads, se_channels=int(np.sqrt(hidden_size * num_heads))\n        )\n\n    def forward(self, x, edge_index):\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.se1(x)\n        x = F.elu(self.conv1(x, edge_index))\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.se2(x)\n        x = F.elu(self.conv2(x, edge_index))\n        return F.log_softmax(x, dim=1)\n\n    def loss(self, data):\n        return F.nll_loss(\n            self.forward(data.x, data.edge_index)[data.train_mask],\n            data.y[data.train_mask],\n        )\n    \n    def predict(self, data):\n        return self.forward(data.x, data.edge_index)\n'"
cogdl/models/nn/pyg_drgcn.py,2,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn.conv import GCNConv\n\nfrom cogdl.layers import SELayer\n\nfrom .. import BaseModel, register_model\n\n\n@register_model(""drgcn"")\nclass DrGCN(BaseModel):\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(""--num-features"", type=int)\n        parser.add_argument(""--num-classes"", type=int)\n        parser.add_argument(""--hidden-size"", type=int, default=16)\n        parser.add_argument(""--num-layers"", type=int, default=2)\n        parser.add_argument(""--dropout"", type=float, default=0.5)\n        # fmt: on\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(\n            args.num_features,\n            args.num_classes,\n            args.hidden_size,\n            args.num_layers,\n            args.dropout,\n        )\n\n    def __init__(self, num_features, num_classes, hidden_size, num_layers, dropout):\n        super(DrGCN, self).__init__()\n\n        self.num_features = num_features\n        self.num_classes = num_classes\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        shapes = [num_features] + [hidden_size] * (num_layers - 1) + [num_classes]\n        self.convs = nn.ModuleList(\n            [\n                GCNConv(shapes[layer], shapes[layer + 1], cached=True)\n                for layer in range(num_layers)\n            ]\n        )\n        self.ses = nn.ModuleList(\n            [\n                SELayer(shapes[layer], se_channels=int(np.sqrt(shapes[layer])))\n                for layer in range(num_layers)\n            ]\n        )\n\n    def forward(self, x, edge_index):\n        x = self.ses[0](x)\n        for se, conv in zip(self.ses[1:], self.convs[:-1]):\n            x = F.relu(conv(x, edge_index))\n            x = se(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.convs[-1](x, edge_index)\n        return F.log_softmax(x, dim=1)\n\n    def loss(self, data):\n        return F.nll_loss(\n            self.forward(data.x, data.edge_index)[data.train_mask],\n            data.y[data.train_mask],\n        )\n    \n    def predict(self, data):\n        return self.forward(data.x, data.edge_index)\n'"
cogdl/models/nn/pyg_gat.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn.conv import GATConv\n\nfrom .. import BaseModel, register_model\n\n\n@register_model(""pyg_gat"")\nclass GAT(BaseModel):\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(""--num-features"", type=int)\n        parser.add_argument(""--num-classes"", type=int)\n        parser.add_argument(""--hidden-size"", type=int, default=8)\n        parser.add_argument(""--num-heads"", type=int, default=8)\n        parser.add_argument(""--dropout"", type=float, default=0.6)\n        parser.add_argument(""--lr"", type=float, default=0.005)\n        # fmt: on\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(\n            args.num_features,\n            args.num_classes,\n            args.hidden_size,\n            args.num_heads,\n            args.dropout,\n        )\n\n    def __init__(self, num_features, num_classes, hidden_size, num_heads, dropout):\n        super(GAT, self).__init__()\n        self.num_features = num_features\n        self.num_classes = num_classes\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.dropout = dropout\n        self.conv1 = GATConv(\n            num_features, hidden_size, heads=num_heads, dropout=dropout\n        )\n        self.conv2 = GATConv(hidden_size * num_heads, num_classes, dropout=dropout)\n\n    def forward(self, x, edge_index):\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = F.elu(self.conv1(x, edge_index))\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = F.elu(self.conv2(x, edge_index))\n        return F.log_softmax(x, dim=1)\n\n    def loss(self, data):\n        return F.nll_loss(\n            self.forward(data.x, data.edge_index)[data.train_mask],\n            data.y[data.train_mask],\n        )\n    \n    def predict(self, data):\n        return self.forward(data.x, data.edge_index)\n'"
cogdl/models/nn/pyg_gcn.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn.conv import GCNConv\n\nfrom .. import BaseModel, register_model\n\n\n@register_model(""pyg_gcn"")\nclass GCN(BaseModel):\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(""--num-features"", type=int)\n        parser.add_argument(""--num-classes"", type=int)\n        parser.add_argument(""--hidden-size"", type=int, default=64)\n        parser.add_argument(""--num-layers"", type=int, default=2)\n        parser.add_argument(""--dropout"", type=float, default=0.5)\n        # fmt: on\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(\n            args.num_features,\n            args.num_classes,\n            args.hidden_size,\n            args.num_layers,\n            args.dropout,\n        )\n\n    def __init__(self, num_features, num_classes, hidden_size, num_layers, dropout):\n        super(GCN, self).__init__()\n\n        self.num_features = num_features\n        self.num_classes = num_classes\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        shapes = [num_features] + [hidden_size] * (num_layers - 1) + [num_classes]\n        self.convs = nn.ModuleList(\n            [\n                GCNConv(shapes[layer], shapes[layer + 1], cached=True)\n                for layer in range(num_layers)\n            ]\n        )\n\n    def forward(self, x, edge_index):\n        for conv in self.convs[:-1]:\n            x = F.relu(conv(x, edge_index))\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.convs[-1](x, edge_index)\n        return F.log_softmax(x, dim=1)\n\n    def loss(self, data):\n        return F.nll_loss(\n            self.forward(data.x, data.edge_index)[data.train_mask],\n            data.y[data.train_mask],\n        )\n    \n    def predict(self, data):\n        return self.forward(data.x, data.edge_index)\n'"
cogdl/models/nn/pyg_infomax.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom sklearn.linear_model import LogisticRegression\nfrom torch_geometric.nn import GCNConv, DeepGraphInfomax\n\nfrom .. import BaseModel, register_model\n\nclass Encoder(nn.Module):\n    def __init__(self, in_channels, hidden_channels):\n        super(Encoder, self).__init__()\n        self.conv = GCNConv(in_channels, hidden_channels, cached=True)\n        self.prelu = nn.PReLU(hidden_channels)\n\n    def forward(self, x, edge_index):\n        x = self.conv(x, edge_index)\n        x = self.prelu(x)\n        return x\n\ndef corruption(x, edge_index):\n    return x[torch.randperm(x.size(0))], edge_index\n\n@register_model(""pyg_infomax"")\nclass Infomax(BaseModel):\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(""--num-features"", type=int)\n        parser.add_argument(""--num-classes"", type=int)\n        parser.add_argument(""--hidden-size"", type=int, default=512)\n        # fmt: on\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(\n            args.num_features,\n            args.num_classes,\n            args.hidden_size,\n        )\n\n    def __init__(self, num_features, num_classes, hidden_size):\n        super(Infomax, self).__init__()\n\n        self.num_features = num_features\n        self.num_classes = num_classes\n        self.hidden_size = hidden_size\n        \n        self.model = DeepGraphInfomax(\n            hidden_channels=hidden_size, encoder=Encoder(num_features, hidden_size),\n            summary=lambda z, *args, **kwargs: torch.sigmoid(z.mean(dim=0)),\n            corruption=corruption)\n\n    def forward(self, x, edge_index):\n        return self.model(x, edge_index)\n\n    def loss(self, data):\n        pos_z, neg_z, summary = self.forward(data.x, data.edge_index)\n        loss = self.model.loss(pos_z, neg_z, summary)\n        return loss\n    \n    def predict(self, data):\n        z, _, _ = self.forward(data.x, data.edge_index)\n        clf = LogisticRegression(solver=\'lbfgs\', multi_class=\'auto\', max_iter=150)\n        clf.fit(z[data.train_mask].detach().cpu().numpy(), data.y[data.train_mask].detach().cpu().numpy())\n        logits = torch.Tensor(clf.predict_proba(z.detach().cpu().numpy()))\n        if z.is_cuda:\n            logits = logits.cuda()\n        return logits\n'"
cogdl/models/nn/pyg_unet.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GraphUNet\nfrom torch_geometric.utils import dropout_adj\n\nfrom .. import BaseModel, register_model\n\n\n@register_model(""pyg_unet"")\nclass UNet(BaseModel):\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(""--num-features"", type=int)\n        parser.add_argument(""--num-classes"", type=int)\n        parser.add_argument(""--hidden-size"", type=int, default=64)\n        parser.add_argument(""--num-layers"", type=int, default=2)\n        parser.add_argument(""--dropout"", type=float, default=0.5)\n        # fmt: on\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(\n            args.num_features,\n            args.num_classes,\n            args.hidden_size,\n            args.num_layers,\n            args.dropout,\n        )\n\n    def __init__(self, num_features, num_classes, hidden_size, num_layers, dropout):\n        super(UNet, self).__init__()\n\n        self.num_features = num_features\n        self.num_classes = num_classes\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n\n        self.unet = GraphUNet(num_features, hidden_size, num_classes, depth=3, pool_ratios=[0.5, 0.5])\n\n    def forward(self, x, edge_index):\n        edge_index, _ = dropout_adj(edge_index, p=0.2,\n                                    force_undirected=True,\n                                    num_nodes=x.shape[0],\n                                    training=self.training)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        x = self.unet(x, edge_index)\n        return F.log_softmax(x, dim=1)\n\n    def loss(self, data):\n        return F.nll_loss(\n            self.forward(data.x, data.edge_index)[data.train_mask],\n            data.y[data.train_mask],\n        )\n    \n    def predict(self, data):\n        return self.forward(data.x, data.edge_index)\n'"
cogdl/models/nn/sortpool.py,7,"b'import random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import SAGEConv\n\nfrom .. import BaseModel, register_model\nfrom cogdl.data import DataLoader\n\n\ndef scatter_sum(src, index, dim, dim_size):\n    size = list(src.size())\n    if dim_size is not None:\n        size[dim] = dim_size\n    else:\n        size[dim] = int(index.max()) + 1\n    out = torch.zeros(size, dtype=src.dtype, device=src.device)\n    return out.scatter_add_(dim, index, src)\n\n\ndef spare2dense_batch(x, batch=None, fill_value=0):\n    batch_size = batch[-1] + 1\n    batch_num_nodes = scatter_sum(batch.new_ones(x.size(0)), batch, dim=0, dim_size=batch_size)\n    max_num_nodes = batch_num_nodes.max().item()\n    batch_cum_nodes = torch.cat([batch.new_zeros(1), batch_num_nodes.cumsum(dim=0)])\n\n    idx = torch.arange(x.size(0), dtype=torch.long, device=x.device)\n    idx = idx - batch_cum_nodes[batch] + batch * max_num_nodes\n\n    new_size = [batch_size * max_num_nodes, x.size(1)]\n    out = x.new_full(new_size, fill_value)\n    out[idx] = x\n    out = out.view([batch_size, max_num_nodes, x.size(1)])\n    return out\n\n\n@register_model(""sortpool"")\nclass SortPool(BaseModel):\n    r""""""Implimentation of sortpooling in paper `""An End-to-End Deep Learning\n    Architecture for Graph Classification"" <https://www.cse.wustl.edu/~muhan/papers/AAAI_2018_DGCNN.pdf>__.`\n\n    Parameters\n    ----------\n    in_feats : int\n        Size of each input sample.\n    out_feats : int\n        Size of each output sample.\n    hidden_dim : int\n        Dimension of hidden layer embedding.\n    num_classes : int\n        Number of target classes.\n    num_layers : int\n        Number of graph neural network layers before pooling.\n    k : int, optional\n        Number of selected features to sort, default: ``30``.\n    out_channel : int\n        Number of the first convolution\'s output channels.\n    kernel_size : int\n        Size of the first convolution\'s kernel.\n    dropout : float, optional\n        Size of dropout, default: ``0.5``.\n    """"""\n    @staticmethod\n    def add_args(parser):\n        parser.add_argument(""--hidden-size"", type=int, default=64)\n        parser.add_argument(""--dropout"", type=float, default=0.1)\n        parser.add_argument(""--batch-size"", type=int, default=20)\n        parser.add_argument(""--train-ratio"", type=float, default=0.7)\n        parser.add_argument(""--test-ratio"", type=float, default=0.1)\n        parser.add_argument(""--num-layers"", type=int, default=2)\n        parser.add_argument(""--out-channels"", type=int, default=32)\n        parser.add_argument(""--k"", type=int, default=30)\n        parser.add_argument(""--kernel-size"", type=int, default=5)\n\n    @classmethod\n    def build_model_from_args(cls, args):\n        return cls(\n            args.num_features,\n            args.hidden_size,\n            args.num_classes,\n            args.num_layers,\n            args.out_channels,\n            args.kernel_size,\n            args.k,\n            args.dropout\n        )\n\n    @classmethod\n    def split_dataset(cls, dataset, args):\n        random.shuffle(dataset)\n        train_size = int(len(dataset) * args.train_ratio)\n        test_size = int(len(dataset) * args.test_ratio)\n        bs = args.batch_size\n        train_loader = DataLoader(dataset[:train_size], batch_size=bs)\n        test_loader = DataLoader(dataset[-test_size:], batch_size=bs)\n        if args.train_ratio + args.test_ratio < 1:\n            valid_loader = DataLoader(dataset[train_size:-test_size], batch_size=bs)\n        else:\n            valid_loader = test_loader\n        return train_loader, valid_loader, test_loader\n\n    def __init__(self, in_feats, hidden_dim, num_classes, num_layers, out_channel, kernel_size, k=30, dropout=0.5):\n        super(SortPool, self).__init__()\n        self.k = k\n        self.dropout = dropout\n        self.num_layers = num_layers\n        self.gnn_convs = nn.ModuleList()\n        self.gnn_convs.append(SAGEConv(in_feats, hidden_dim))\n        for _ in range(self.num_layers-1):\n            self.gnn_convs.append(SAGEConv(hidden_dim, hidden_dim))\n        self.conv1d = nn.Conv1d(hidden_dim, out_channel, kernel_size)\n        self.fc1 = nn.Linear(out_channel * (self.k - kernel_size + 1), hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, batch):\n        h = batch.x\n        for i in range(self.num_layers):\n            h = self.gnn_convs[i](h, batch.edge_index)\n            h = F.relu(h)\n\n        h, _ = h.sort(dim=-1)\n        fill_value = h.min().item() - 1\n        batch_h = spare2dense_batch(h, batch.batch, fill_value)\n        batch_size, num_nodes, xdim = batch_h.size()\n\n        _, order = batch_h[:, :, -1].sort(dim=-1, descending=True)\n        order = order + torch.arange(batch_size, dtype=torch.long, device=order.device).view(-1, 1) * num_nodes\n\n        batch_h = batch_h.view(batch_size * num_nodes, xdim)\n        batch_h = batch_h[order].view(batch_size, num_nodes, xdim)\n\n        if num_nodes >= self.k:\n            batch_h = batch_h[:, :self.k].contiguous()\n        else:\n            fill_batch = batch_h.new_full((batch_size, self.k - num_nodes, xdim), fill_value)\n            batch_h = torch.cat([batch_h, fill_batch], dim=1)\n        batch_h[batch_h == fill_value] = 0\n        h = batch_h\n\n        # h = h.view(batch_size, self.k, -1).permute(0, 2, 1) # bn * hidden * k\n        h = h.permute(0, 2, 1) # bn * hidden * k\n        h = F.relu(self.conv1d(h)).view(batch_size, -1)\n        h = F.relu(self.fc1(h))\n        h = F.dropout(h, p=self.dropout, training=self.training)\n        h = self.fc2(h)\n        if batch.y is not None:\n            pred = F.log_softmax(h, dim=-1)\n            loss = F.nll_loss(pred, batch.y)\n            return h, loss\n        return h, None\n\n\n\n\n'"
