file_path,api_count,code
setup.py,0,"b'from distutils.core import setup\n\n""""""\ninstall the packages\n\n""""""\nsetup(name=\'rl_utils\',\n      version=\'0.0\',\n      description=\'rl utils for the rl algorithms\',\n      author=\'Tianhong Dai\',\n      author_email=\'xxx@xxx.com\',\n      url=\'no\',\n      packages=[\'rl_utils\'],\n     )\n'"
rl_utils/__init__.py,0,b''
rl_algorithms/a2c/a2c_agent.py,8,"b""import numpy as np\nimport torch\nfrom models import net\nfrom datetime import datetime\nfrom utils import select_actions, evaluate_actions, discount_with_dones\nimport os\n\nclass a2c_agent:\n    def __init__(self, envs, args):\n        self.envs = envs\n        self.args = args\n        # define the network\n        self.net = net(self.envs.action_space.n)\n        if self.args.cuda:\n            self.net.cuda()\n        # define the optimizer\n        self.optimizer = torch.optim.RMSprop(self.net.parameters(), lr=self.args.lr, eps=self.args.eps, alpha=self.args.alpha)\n        if not os.path.exists(self.args.save_dir):\n            os.mkdir(self.args.save_dir)\n        # check the saved path for envs..\n        self.model_path = self.args.save_dir + self.args.env_name + '/'\n        if not os.path.exists(self.model_path):\n            os.mkdir(self.model_path)\n        # get the obs..\n        self.batch_ob_shape = (self.args.num_workers * self.args.nsteps,) + self.envs.observation_space.shape\n        self.obs = np.zeros((self.args.num_workers,) + self.envs.observation_space.shape, dtype=self.envs.observation_space.dtype.name)\n        self.obs[:] = self.envs.reset()\n        self.dones = [False for _ in range(self.args.num_workers)]\n\n    # train the network..\n    def learn(self):\n        num_updates = self.args.total_frames // (self.args.num_workers * self.args.nsteps)\n        # get the reward to calculate other information\n        episode_rewards = np.zeros((self.args.num_workers, ), dtype=np.float32)\n        final_rewards = np.zeros((self.args.num_workers, ), dtype=np.float32)\n        # start to update\n        for update in range(num_updates):\n            mb_obs, mb_rewards, mb_actions, mb_dones = [],[],[],[]\n            for step in range(self.args.nsteps):\n                with torch.no_grad():\n                    input_tensor = self._get_tensors(self.obs)\n                    _, pi = self.net(input_tensor)\n                # select actions\n                actions = select_actions(pi)\n                cpu_actions = actions.squeeze(1).cpu().numpy()\n                # start to store the information\n                mb_obs.append(np.copy(self.obs))\n                mb_actions.append(cpu_actions)\n                mb_dones.append(self.dones)\n                # step\n                obs, rewards, dones, _ = self.envs.step(cpu_actions)\n                # start to store the rewards\n                self.dones = dones\n                mb_rewards.append(rewards)\n                for n, done in enumerate(dones):\n                    if done:\n                        self.obs[n] = self.obs[n]*0\n                self.obs = obs\n                episode_rewards += rewards\n                # get the masks\n                masks = np.array([0.0 if done else 1.0 for done in dones], dtype=np.float32)\n                final_rewards *= masks\n                final_rewards += (1 - masks) * episode_rewards\n                episode_rewards *= masks\n                # update the obs\n            mb_dones.append(self.dones)\n            # process the rollouts\n            mb_obs = np.asarray(mb_obs, dtype=np.uint8).swapaxes(1, 0).reshape(self.batch_ob_shape)\n            mb_rewards = np.asarray(mb_rewards, dtype=np.float32).swapaxes(1, 0)\n            mb_actions = np.asarray(mb_actions, dtype=np.int32).swapaxes(1, 0)\n            mb_dones = np.asarray(mb_dones, dtype=np.bool).swapaxes(1, 0)\n            mb_masks = mb_dones[:, :-1]\n            mb_dones = mb_dones[:, 1:]\n            # calculate the last value\n            with torch.no_grad():\n                input_tensor = self._get_tensors(self.obs)\n                last_values, _ = self.net(input_tensor)\n            # compute returns\n            for n, (rewards, dones, value) in enumerate(zip(mb_rewards, mb_dones, last_values.detach().cpu().numpy().squeeze())):\n                rewards = rewards.tolist()\n                dones = dones.tolist()\n                if dones[-1] == 0:\n                    rewards = discount_with_dones(rewards+[value], dones+[0], self.args.gamma)[:-1]\n                else:\n                    rewards = discount_with_dones(rewards, dones, self.args.gamma)\n                mb_rewards[n] = rewards\n            mb_rewards = mb_rewards.flatten()\n            mb_actions = mb_actions.flatten()\n            # start to update network\n            vl, al, ent = self._update_network(mb_obs, mb_rewards, mb_actions)\n            if update % self.args.log_interval == 0:\n                print('[{}] Update: {}/{}, Frames: {}, Rewards: {:.1f}, VL: {:.3f}, PL: {:.3f}, Ent: {:.2f}, Min: {}, Max:{}'.format(\\\n                    datetime.now(), update, num_updates, (update+1)*(self.args.num_workers * self.args.nsteps),\\\n                    final_rewards.mean(), vl, al, ent, final_rewards.min(), final_rewards.max()))\n                torch.save(self.net.state_dict(), self.model_path + 'model.pt')\n    \n    # update_network\n    def _update_network(self, obs, returns, actions):\n        # evaluate the actions\n        input_tensor = self._get_tensors(obs)\n        values, pi = self.net(input_tensor)\n        # define the tensor of actions, returns\n        returns = torch.tensor(returns, dtype=torch.float32).unsqueeze(1)\n        actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1)\n        if self.args.cuda:\n            returns = returns.cuda()\n            actions = actions.cuda()\n        # evaluate actions\n        action_log_probs, dist_entropy = evaluate_actions(pi, actions)\n        # calculate advantages...\n        advantages = returns - values\n        # get the value loss\n        value_loss = advantages.pow(2).mean()\n        # get the action loss\n        action_loss = -(advantages.detach() * action_log_probs).mean()\n        # total loss\n        total_loss = action_loss + self.args.value_loss_coef * value_loss - self.args.entropy_coef * dist_entropy\n        # start to update\n        self.optimizer.zero_grad()\n        total_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.net.parameters(), self.args.max_grad_norm)\n        self.optimizer.step()\n        return value_loss.item(), action_loss.item(), dist_entropy.item()\n    \n    # get the tensors...\n    def _get_tensors(self, obs):\n        input_tensor = torch.tensor(np.transpose(obs, (0, 3, 1, 2)), dtype=torch.float32)\n        if self.args.cuda:\n            input_tensor = input_tensor.cuda()\n        return input_tensor\n"""
rl_algorithms/a2c/arguments.py,0,"b""import argparse\n\ndef get_args():\n    parse = argparse.ArgumentParser()\n    parse.add_argument('--gamma', type=float, default=0.99, help='the discount factor of RL')\n    parse.add_argument('--seed', type=int, default=123, help='the random seeds')\n    parse.add_argument('--env-name', type=str, default='BreakoutNoFrameskip-v4', help='the environment name')\n    parse.add_argument('--lr', type=float, default=7e-4, help='learning rate of the algorithm')\n    parse.add_argument('--value-loss-coef', type=float, default=0.5, help='the coefficient of value loss')\n    parse.add_argument('--tau', type=float, default=0.95, help='gae coefficient')\n    parse.add_argument('--cuda', action='store_true', help='use cuda do the training')\n    parse.add_argument('--total-frames', type=int, default=20000000, help='the total frames for training')\n    parse.add_argument('--eps', type=float, default=1e-5, help='param for adam optimizer')\n    parse.add_argument('--save-dir', type=str, default='saved_models/', help='the folder to save models')\n    parse.add_argument('--nsteps', type=int, default=5, help='the steps to update the network')\n    parse.add_argument('--num-workers', type=int, default=16, help='the number of cpu you use')\n    parse.add_argument('--entropy-coef', type=float, default=0.01, help='entropy-reg')\n    parse.add_argument('--log-interval', type=int, default=100, help='the log interval')\n    parse.add_argument('--alpha', type=float, default=0.99, help='the alpha coe of RMSprop')\n    parse.add_argument('--max-grad-norm', type=float, default=0.5, help='the grad clip')\n    parse.add_argument('--use-gae', action='store_true', help='use-gae')\n    parse.add_argument('--log-dir', type=str, default='logs/', help='log dir')\n    parse.add_argument('--env-type', type=str, default='atari', help='the type of the environment')\n\n    args = parse.parse_args()\n\n    return args\n"""
rl_algorithms/a2c/demo.py,3,"b'from arguments import get_args\nfrom models import net\nimport torch\nfrom utils import select_actions\nimport cv2\nimport numpy as np\nfrom rl_utils.env_wrapper.frame_stack import VecFrameStack\nfrom rl_utils.env_wrapper.atari_wrapper import make_atari, wrap_deepmind\n\n# update the current observation\ndef get_tensors(obs):\n    input_tensor = torch.tensor(np.transpose(obs, (2, 0, 1)), dtype=torch.float32).unsqueeze(0)\n    return input_tensor\n\nif __name__ == ""__main__"":\n    args = get_args()\n    # create environment\n    #env = VecFrameStack(wrap_deepmind(make_atari(args.env_name)), 4)\n    env = make_atari(args.env_name)\n    env = wrap_deepmind(env, frame_stack=True)\n    # get the model path\n    model_path = args.save_dir + args.env_name + \'/model.pt\'\n    network = net(env.action_space.n)\n    network.load_state_dict(torch.load(model_path, map_location=lambda storage, loc: storage)) \n    obs = env.reset()\n    while True:\n        env.render()\n        # get the obs\n        with torch.no_grad():\n            input_tensor = get_tensors(obs)\n            _, pi = network(input_tensor)\n        actions = select_actions(pi, True)\n        obs, reward, done, _ = env.step([actions])\n    env.close()\n'"
rl_algorithms/a2c/models.py,2,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# the convolution layer of deepmind\nclass deepmind(nn.Module):\n    def __init__(self):\n        super(deepmind, self).__init__()\n        self.conv1 = nn.Conv2d(4, 32, 8, stride=4)\n        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n        self.conv3 = nn.Conv2d(64, 32, 3, stride=1)\n        self.fc1 = nn.Linear(32 * 7 * 7, 512) \n        # start to do the init...\n        nn.init.orthogonal_(self.conv1.weight.data, gain=nn.init.calculate_gain('relu'))\n        nn.init.orthogonal_(self.conv2.weight.data, gain=nn.init.calculate_gain('relu'))\n        nn.init.orthogonal_(self.conv3.weight.data, gain=nn.init.calculate_gain('relu'))\n        nn.init.orthogonal_(self.fc1.weight.data, gain=nn.init.calculate_gain('relu'))\n        # init the bias...\n        nn.init.constant_(self.conv1.bias.data, 0)\n        nn.init.constant_(self.conv2.bias.data, 0)\n        nn.init.constant_(self.conv3.bias.data, 0)\n        nn.init.constant_(self.fc1.bias.data, 0)\n        \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = x.view(-1, 32 * 7 * 7)\n        x = F.relu(self.fc1(x))\n        return x\n\n# in the initial, just the nature CNN\nclass net(nn.Module):\n    def __init__(self, num_actions):\n        super(net, self).__init__()\n        self.cnn_layer = deepmind()\n        self.critic = nn.Linear(512, 1)\n        self.actor = nn.Linear(512, num_actions)\n        # init the linear layer..\n        nn.init.orthogonal_(self.critic.weight.data)\n        nn.init.constant_(self.critic.bias.data, 0)\n        # init the policy layer...\n        nn.init.orthogonal_(self.actor.weight.data, gain=0.01)\n        nn.init.constant_(self.actor.bias.data, 0)\n\n    def forward(self, inputs):\n        x = self.cnn_layer(inputs / 255.0)\n        value = self.critic(x)\n        pi = F.softmax(self.actor(x), dim=1)\n        return value, pi\n"""
rl_algorithms/a2c/train.py,0,"b""from arguments import get_args\nfrom a2c_agent import a2c_agent\nfrom rl_utils.env_wrapper.create_env import create_multiple_envs\nfrom rl_utils.seeds.seeds import set_seeds\nfrom a2c_agent import a2c_agent\nimport os\n\nif __name__ == '__main__':\n    # set signle thread\n    os.environ['OMP_NUM_THREADS'] = '1'\n    os.environ['MKL_NUM_THREADS'] = '1'\n    # get args\n    args = get_args()\n    # create environments\n    envs = create_multiple_envs(args)\n    # set seeds\n    set_seeds(args)\n    # create trainer\n    a2c_trainer = a2c_agent(envs, args)\n    a2c_trainer.learn()\n    # close the environment\n    envs.close()\n"""
rl_algorithms/a2c/utils.py,2,"b'import torch\nimport numpy as np\nfrom torch.distributions.categorical import Categorical\n\n# select - actions\ndef select_actions(pi, deterministic=False):\n    cate_dist = Categorical(pi)\n    if deterministic:\n        return torch.argmax(pi, dim=1).item()\n    else:\n        return cate_dist.sample().unsqueeze(-1)\n\n# get the action log prob and entropy...\ndef evaluate_actions(pi, actions):\n    cate_dist = Categorical(pi)\n    return cate_dist.log_prob(actions.squeeze(-1)).unsqueeze(-1), cate_dist.entropy().mean()\n\ndef discount_with_dones(rewards, dones, gamma):\n    discounted = []\n    r = 0\n    for reward, done in zip(rewards[::-1], dones[::-1]):\n        r = reward + gamma * r * (1.-done)\n        discounted.append(r)\n    return discounted[::-1]\n'"
rl_algorithms/ddpg/arguments.py,0,"b""import argparse\n\ndef get_args():\n    parse = argparse.ArgumentParser(description='ddpg')\n    parse.add_argument('--env-name', type=str, default='Pendulum-v0', help='the training environment')\n    parse.add_argument('--lr-actor', type=float, default=1e-4, help='the lr of the actor')\n    parse.add_argument('--lr-critic', type=float, default=1e-3, help='the lr of the critic')\n    parse.add_argument('--critic-l2-reg', type=float, default=1e-2, help='the critic reg')\n    parse.add_argument('--gamma', type=float, default=0.99, help='the discount factor')\n    parse.add_argument('--nb-epochs', type=int, default=500, help='the epochs to train the network')\n    parse.add_argument('--nb-cycles', type=int, default=20)\n    parse.add_argument('--nb-train', type=int, default=50, help='number to train the agent')\n    parse.add_argument('--nb-rollout-steps', type=int, default=100, help='steps to collect samples')\n    parse.add_argument('--nb-test-rollouts', type=int, default=10, help='the number of test')\n    parse.add_argument('--batch-size', type=int, default=128, help='the batch size to update network')\n    parse.add_argument('--replay-size', type=int, default=int(1e6), help='the size of the replay buffer')\n    parse.add_argument('--clip-range', type=float, default=5, help='clip range of the observation')\n    parse.add_argument('--save-dir', type=str, default='saved_models/', help='the place save the models')\n    parse.add_argument('--polyak', type=float, default=0.95, help='the expoential weighted coefficient.')\n    parse.add_argument('--total-frames', type=int, default=int(1e6), help='total frames')\n    parse.add_argument('--log-dir', type=str, default='logs', help='place to save log files')\n    parse.add_argument('--env-type', type=str, default=None, help='environment type')\n    parse.add_argument('--seed', type=int, default=123, help='random seed')\n    parse.add_argument('--display-interval', type=int, default=10, help='interval to display')\n    # ddpg not support gpu\n    parse.add_argument('--cuda', action='store_true', help='if use GPU')\n\n    args = parse.parse_args()\n    return args\n"""
rl_algorithms/ddpg/ddpg_agent.py,12,"b'import numpy as np\nfrom models import actor, critic \nimport torch\nimport os\nfrom datetime import datetime\nfrom mpi4py import MPI\nfrom rl_utils.mpi_utils.normalizer import normalizer\nfrom rl_utils.mpi_utils.utils import sync_networks, sync_grads\nfrom rl_utils.experience_replay.experience_replay import replay_buffer\nfrom utils import ounoise\nimport copy\nimport gym\n\n""""""\nddpg algorithms - revised baseline version\n\nsupport MPI training\n\n""""""\n\nclass ddpg_agent:\n    def __init__(self, env, args):\n        self.env = env\n        self.args = args\n        # get the dims and action max of the environment\n        obs_dims = self.env.observation_space.shape[0]\n        self.action_dims = self.env.action_space.shape[0]\n        self.action_max = self.env.action_space.high[0]\n        # define the network\n        self.actor_net = actor(obs_dims, self.action_dims)\n        self.critic_net = critic(obs_dims, self.action_dims)\n        # sync the weights across the mpi\n        sync_networks(self.actor_net)\n        sync_networks(self.critic_net)\n        # build the target newtork\n        self.actor_target_net = copy.deepcopy(self.actor_net)\n        self.critic_target_net = copy.deepcopy(self.critic_net)\n        # create the optimizer\n        self.actor_optim = torch.optim.Adam(self.actor_net.parameters(), self.args.lr_actor)\n        self.critic_optim = torch.optim.Adam(self.critic_net.parameters(), self.args.lr_critic, weight_decay=self.args.critic_l2_reg)\n        # create the replay buffer\n        self.replay_buffer = replay_buffer(self.args.replay_size)\n        # create the normalizer\n        self.o_norm = normalizer(obs_dims, default_clip_range=self.args.clip_range)\n        # create the noise generator\n        self.noise_generator = ounoise(std=0.2, action_dim=self.action_dims)\n        # create the dir to save models\n        if MPI.COMM_WORLD.Get_rank() == 0:\n            if not os.path.exists(self.args.save_dir):\n                os.mkdir(self.args.save_dir)\n            self.model_path = os.path.join(self.args.save_dir, self.args.env_name)\n            if not os.path.exists(self.model_path):\n                os.mkdir(self.model_path)\n        # create a eval environemnt\n        self.eval_env = gym.make(self.args.env_name)\n        # set seeds\n        self.eval_env.seed(self.args.seed * 2 + MPI.COMM_WORLD.Get_rank())\n\n    def learn(self):\n        """"""\n        the learning part\n\n        """"""\n        self.actor_net.train()\n        # reset the environmenr firstly\n        obs = self.env.reset()\n        self.noise_generator.reset()\n        # get the number of epochs\n        nb_epochs = self.args.total_frames // (self.args.nb_rollout_steps * self.args.nb_cycles)\n        for epoch in range(nb_epochs):\n            for _ in range(self.args.nb_cycles):\n                # used to update the normalizer\n                ep_obs = []\n                for _ in range(self.args.nb_rollout_steps):\n                    with torch.no_grad():\n                        inputs_tensor = self._preproc_inputs(obs)\n                        pi = self.actor_net(inputs_tensor)\n                        action = self._select_actions(pi)\n                    # feed actions into the environment\n                    obs_, reward, done, _ = self.env.step(self.action_max * action)\n                    # append the rollout information into the memory\n                    self.replay_buffer.add(obs, action, reward, obs_, float(done))\n                    ep_obs.append(obs.copy())\n                    obs = obs_\n                    # if done, reset the environment\n                    if done:\n                        obs = self.env.reset()\n                        self.noise_generator.reset()\n                # then start to do the update of the normalizer\n                ep_obs = np.array(ep_obs)\n                self.o_norm.update(ep_obs)\n                self.o_norm.recompute_stats()\n                # then start to update the network\n                for _ in range(self.args.nb_train):\n                    a_loss, c_loss = self._update_network()\n                    # update the target network\n                    self._soft_update_target_network(self.actor_target_net, self.actor_net)\n                    self._soft_update_target_network(self.critic_target_net, self.critic_net)\n            # start to do the evaluation\n            success_rate = self._eval_agent()\n            # convert back to normal\n            self.actor_net.train()\n            if epoch % self.args.display_interval == 0:\n                if MPI.COMM_WORLD.Get_rank() == 0:\n                    print(\'[{}] Epoch: {} / {}, Frames: {}, Rewards: {:.3f}, Actor loss: {:.3f}, Critic Loss: {:.3f}\'.format(datetime.now(), \\\n                            epoch, nb_epochs, (epoch+1) * self.args.nb_rollout_steps * self.args.nb_cycles, success_rate, a_loss, c_loss))\n                    torch.save([self.actor_net.state_dict(), self.o_norm.mean, self.o_norm.std], self.model_path + \'/model.pt\')\n\n    # functions to preprocess the image\n    def _preproc_inputs(self, obs):\n        obs_norm = self.o_norm.normalize(obs)\n        inputs_tensor = torch.tensor(obs_norm, dtype=torch.float32).unsqueeze(0)\n        return inputs_tensor\n\n    # this function will choose action for the agent and do the exploration\n    def _select_actions(self, pi):\n        action = pi.cpu().numpy().squeeze()\n        # TODO: Noise type now - only support ounoise\n        # add the gaussian noise\n        #action = action + np.random.normal(0, 0.1, self.action_dims)\n        # add ou noise\n        action = action + self.noise_generator.noise()\n        action = np.clip(action, -1, 1)\n        return action\n    \n    # update the network\n    def _update_network(self):\n        # sample the samples from the replay buffer\n        samples = self.replay_buffer.sample(self.args.batch_size)\n        obses, actions, rewards, obses_next, dones = samples\n        # try to do the normalization of obses\n        norm_obses = self.o_norm.normalize(obses)\n        norm_obses_next = self.o_norm.normalize(obses_next)\n        # transfer them into tensors\n        norm_obses_tensor = torch.tensor(norm_obses, dtype=torch.float32)\n        norm_obses_next_tensor = torch.tensor(norm_obses_next, dtype=torch.float32)\n        actions_tensor = torch.tensor(actions, dtype=torch.float32)\n        rewards_tensor = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n        dones_tensor = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n        with torch.no_grad():\n            actions_next = self.actor_target_net(norm_obses_next_tensor)\n            q_next_value = self.critic_target_net(norm_obses_next_tensor, actions_next)\n            target_q_value = rewards_tensor + (1 - dones_tensor) * self.args.gamma * q_next_value\n        # the real q value\n        real_q_value = self.critic_net(norm_obses_tensor, actions_tensor)\n        critic_loss = (real_q_value - target_q_value).pow(2).mean()\n        # the actor loss\n        actions_real = self.actor_net(norm_obses_tensor)\n        actor_loss = -self.critic_net(norm_obses_tensor, actions_real).mean()\n        # start to update the network\n        self.actor_optim.zero_grad()\n        actor_loss.backward()\n        sync_grads(self.actor_net)\n        self.actor_optim.step()\n        # update the critic network\n        self.critic_optim.zero_grad()\n        critic_loss.backward()\n        sync_grads(self.critic_net)\n        self.critic_optim.step()\n        return actor_loss.item(), critic_loss.item()\n    \n    # soft update the target network...\n    def _soft_update_target_network(self, target, source):\n        for target_param, param in zip(target.parameters(), source.parameters()):\n            target_param.data.copy_((1 - self.args.polyak) * param.data + self.args.polyak * target_param.data)\n    \n    # do the evaluation\n    def _eval_agent(self):\n        self.actor_net.eval()\n        total_success_rate = []\n        for _ in range(self.args.nb_test_rollouts):\n            per_success_rate = []\n            obs = self.eval_env.reset()\n            while True:\n                with torch.no_grad():\n                    inputs_tensor = self._preproc_inputs(obs)\n                    pi = self.actor_net(inputs_tensor)\n                    actions = pi.detach().cpu().numpy().squeeze()\n                    if self.action_dims == 1:\n                        actions = np.array([actions])\n                obs_, reward, done, _ = self.eval_env.step(actions * self.action_max)\n                per_success_rate.append(reward)\n                obs = obs_\n                if done:\n                    break\n            total_success_rate.append(np.sum(per_success_rate))\n        local_success_rate = np.mean(total_success_rate)\n        global_success_rate = MPI.COMM_WORLD.allreduce(local_success_rate, op=MPI.SUM)\n        return global_success_rate / MPI.COMM_WORLD.Get_size()\n'"
rl_algorithms/ddpg/demo.py,3,"b""from arguments import get_args\nimport gym\nfrom models import actor\nimport torch\nimport numpy as np\n\ndef normalize(obs, mean, std, clip):\n    return np.clip((obs - mean) / std, -clip, clip)\n\nif __name__ == '__main__':\n    args = get_args()\n    env = gym.make(args.env_name)\n    # get environment infos\n    obs_dims = env.observation_space.shape[0]\n    action_dims = env.action_space.shape[0]\n    action_max = env.action_space.high[0]\n    # define the network\n    actor_net = actor(obs_dims, action_dims)\n    # load models\n    model_path = args.save_dir + args.env_name + '/model.pt'\n    model, mean, std = torch.load(model_path, map_location=lambda storage, loc: storage)\n    # load models into the network\n    actor_net.load_state_dict(model)\n    for ep in range(10):\n        obs = env.reset()\n        reward_sum = 0\n        while True:\n            env.render()\n            with torch.no_grad():\n                norm_obs = normalize(obs, mean, std, args.clip_range)\n                norm_obs_tensor = torch.tensor(norm_obs, dtype=torch.float32).unsqueeze(0)\n                actions = actor_net(norm_obs_tensor)\n                actions = actions.detach().numpy().squeeze()\n                if action_dims == 1:\n                    actions = np.array([actions])\n            obs_, reward, done, _ = env.step(action_max * actions)\n            reward_sum += reward\n            if done:\n                break\n            obs = obs_\n        print('the episode is: {}, the reward is: {}'.format(ep, reward_sum))\n    env.close()\n"""
rl_algorithms/ddpg/models.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# define the actor network\nclass actor(nn.Module):\n    def __init__(self, obs_dims, action_dims):\n        super(actor, self).__init__()\n        self.fc1 = nn.Linear(obs_dims, 400)\n        self.fc2 = nn.Linear(400, 300)\n        self.action_out = nn.Linear(300, action_dims)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        actions = torch.tanh(self.action_out(x))\n        return actions\n\nclass critic(nn.Module):\n    def __init__(self, obs_dims, action_dims):\n        super(critic, self).__init__()\n        self.fc1 = nn.Linear(obs_dims, 400)\n        self.fc2 = nn.Linear(400 + action_dims, 300)\n        self.q_out = nn.Linear(300, 1)\n\n    def forward(self, x, actions):\n        x = F.relu(self.fc1(x))\n        x = torch.cat([x, actions], dim=1)\n        x = F.relu(self.fc2(x))\n        q_value = self.q_out(x)\n        return q_value\n'"
rl_algorithms/ddpg/train.py,0,"b""from ddpg_agent import ddpg_agent\nfrom arguments import get_args\nfrom rl_utils.seeds.seeds import set_seeds\nfrom rl_utils.env_wrapper.create_env import create_single_env\nfrom mpi4py import MPI\nimport os\n\nif __name__ == '__main__':\n    # set thread and mpi stuff\n    os.environ['OMP_NUM_THREADS'] = '1'\n    os.environ['MKL_NUM_THREADS'] = '1'\n    os.environ['IN_MPI'] = '1'\n    # train the network\n    args = get_args()\n    # build up the environment\n    env = create_single_env(args, MPI.COMM_WORLD.Get_rank())\n    # set the random seeds\n    set_seeds(args, MPI.COMM_WORLD.Get_rank())\n    # start traininng\n    ddpg_trainer = ddpg_agent(env, args)\n    ddpg_trainer.learn()\n    # close the environment\n    env.close()\n"""
rl_algorithms/ddpg/utils.py,0,"b'import numpy as np\nimport torch\n\n# add ounoise here\nclass ounoise():\n    def __init__(self, std, action_dim, mean=0, theta=0.15, dt=1e-2, x0=None):\n        self.std = std\n        self.mean = mean\n        self.action_dim = action_dim\n        self.theta = theta\n        self.dt = dt\n        self.x0 = x0\n    \n    # reset the noise\n    def reset(self):\n        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.action_dim)\n    \n    # generate noise\n    def noise(self):\n        x = self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt + \\\n                self.std * np.sqrt(self.dt) * np.random.normal(size=self.action_dim)\n        self.x_prev = x\n        return x\n'"
rl_algorithms/dqn_algos/arguments.py,0,"b""import argparse\n\ndef get_args():\n    parse = argparse.ArgumentParser()\n    parse.add_argument('--gamma', type=float, default=0.99, help='the discount factor of RL')\n    parse.add_argument('--seed', type=int, default=123, help='the random seeds')\n    parse.add_argument('--env-name', type=str, default='PongNoFrameskip-v4', help='the environment name')\n    parse.add_argument('--batch-size', type=int, default=32, help='the batch size of updating')\n    parse.add_argument('--lr', type=float, default=1e-4, help='learning rate of the algorithm')\n    parse.add_argument('--buffer-size', type=int, default=10000, help='the size of the buffer')\n    parse.add_argument('--cuda', action='store_true', help='if use the gpu')\n    parse.add_argument('--init-ratio', type=float, default=1, help='the initial exploration ratio')\n    parse.add_argument('--exploration_fraction', type=float, default=0.1, help='decide how many steps to do the exploration')\n    parse.add_argument('--final-ratio', type=float, default=0.01, help='the final exploration ratio')\n    parse.add_argument('--grad-norm-clipping', type=float, default=10, help='the gradient clipping')\n    parse.add_argument('--total-timesteps', type=int, default=int(1e7), help='the total timesteps to train network')\n    parse.add_argument('--learning-starts', type=int, default=10000, help='the frames start to learn')\n    parse.add_argument('--train-freq', type=int, default=4, help='the frequency to update the network')\n    parse.add_argument('--target-network-update-freq', type=int, default=1000, help='the frequency to update the target network')\n    parse.add_argument('--save-dir', type=str, default='saved_models/', help='the folder to save models')\n    parse.add_argument('--display-interval', type=int, default=10, help='the display interval')\n    parse.add_argument('--env-type', type=str, default='atari', help='the environment type')\n    parse.add_argument('--log-dir', type=str, default='logs/', help='dir to save log information')\n    parse.add_argument('--use-double-net', action='store_true', help='use double dqn to train the agent')\n    parse.add_argument('--use-dueling', action='store_true', help='use dueling to train the agent')\n\n    args = parse.parse_args()\n\n    return args\n"""
rl_algorithms/dqn_algos/demo.py,4,"b""import numpy as np\nfrom arguments import get_args\nfrom models import net\nimport torch\nfrom rl_utils.env_wrapper.atari_wrapper import make_atari, wrap_deepmind\n\ndef get_tensors(obs):\n    obs = np.transpose(obs, (2, 0, 1))\n    obs = np.expand_dims(obs, 0)\n    obs = torch.tensor(obs, dtype=torch.float32)\n    return obs\n\nif __name__ == '__main__':\n    args = get_args()\n    # create the environment\n    env = make_atari(args.env_name)\n    env = wrap_deepmind(env, frame_stack=True)\n    # create the network\n    net = net(env.action_space.n, args.use_dueling) \n    # model path\n    model_path = args.save_dir + args.env_name + '/model.pt'\n    # load the models\n    net.load_state_dict(torch.load(model_path, map_location=lambda storage, loc: storage))\n    # start to test the demo\n    obs = env.reset()\n    for _ in range(2000):\n        env.render()\n        with torch.no_grad():\n            obs_tensor = get_tensors(obs)\n            action_value = net(obs_tensor)\n        action = torch.argmax(action_value.squeeze()).item()\n        obs, reward, done, _ = env.step(action)\n        if done:\n            obs = env.reset()\n    env.close()\n"""
rl_algorithms/dqn_algos/dqn_agent.py,10,"b""import sys\nimport numpy as np\nfrom models import net\nfrom utils import linear_schedule, select_actions, reward_recorder\nfrom rl_utils.experience_replay.experience_replay import replay_buffer\nimport torch\nfrom datetime import datetime\nimport os\nimport copy\n\n# define the dqn agent\nclass dqn_agent:\n    def __init__(self, env, args):\n        # define some important \n        self.env = env\n        self.args = args \n        # define the network\n        self.net = net(self.env.action_space.n, self.args.use_dueling)\n        # copy the self.net as the \n        self.target_net = copy.deepcopy(self.net)\n        # make sure the target net has the same weights as the network\n        self.target_net.load_state_dict(self.net.state_dict())\n        if self.args.cuda:\n            self.net.cuda()\n            self.target_net.cuda()\n        # define the optimizer\n        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=self.args.lr)\n        # define the replay memory\n        self.buffer = replay_buffer(self.args.buffer_size)\n        # define the linear schedule of the exploration\n        self.exploration_schedule = linear_schedule(int(self.args.total_timesteps * self.args.exploration_fraction), \\\n                                                    self.args.final_ratio, self.args.init_ratio)\n        # create the folder to save the models\n        if not os.path.exists(self.args.save_dir):\n            os.mkdir(self.args.save_dir)\n        # set the environment folder\n        self.model_path = os.path.join(self.args.save_dir, self.args.env_name)\n        if not os.path.exists(self.model_path):\n            os.mkdir(self.model_path)\n\n    # start to do the training\n    def learn(self):\n        # the episode reward\n        episode_reward = reward_recorder()\n        obs = np.array(self.env.reset())\n        td_loss = 0\n        for timestep in range(self.args.total_timesteps):\n            explore_eps = self.exploration_schedule.get_value(timestep)\n            with torch.no_grad():\n                obs_tensor = self._get_tensors(obs)\n                action_value = self.net(obs_tensor)\n            # select actions\n            action = select_actions(action_value, explore_eps)\n            # excute actions\n            obs_, reward, done, _ = self.env.step(action)\n            obs_ = np.array(obs_)\n            # tryint to append the samples\n            self.buffer.add(obs, action, reward, obs_, float(done))\n            obs = obs_\n            # add the rewards\n            episode_reward.add_rewards(reward)\n            if done:\n                obs = np.array(self.env.reset())\n                # start new episode to store rewards\n                episode_reward.start_new_episode()\n            if timestep > self.args.learning_starts and timestep % self.args.train_freq == 0:\n                # start to sample the samples from the replay buffer\n                batch_samples = self.buffer.sample(self.args.batch_size)\n                td_loss = self._update_network(batch_samples)\n            if timestep > self.args.learning_starts and timestep % self.args.target_network_update_freq == 0:\n                # update the target network\n                self.target_net.load_state_dict(self.net.state_dict())\n            if done and episode_reward.num_episodes % self.args.display_interval == 0:\n                print('[{}] Frames: {}, Episode: {}, Mean: {:.3f}, Loss: {:.3f}'.format(datetime.now(), timestep, episode_reward.num_episodes, \\\n                        episode_reward.mean, td_loss))\n                torch.save(self.net.state_dict(), self.model_path + '/model.pt')\n\n    # update the network\n    def _update_network(self, samples):\n        obses, actions, rewards, obses_next, dones = samples\n        # convert the data to tensor\n        obses = self._get_tensors(obses)\n        actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(-1)\n        rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(-1)\n        obses_next = self._get_tensors(obses_next)\n        dones = torch.tensor(1 - dones, dtype=torch.float32).unsqueeze(-1)\n        # convert into gpu\n        if self.args.cuda:\n            actions = actions.cuda()\n            rewards = rewards.cuda()\n            dones = dones.cuda()\n        # calculate the target value\n        with torch.no_grad():\n            # if use the double network architecture\n            if self.args.use_double_net:\n                q_value_ = self.net(obses_next)\n                action_max_idx = torch.argmax(q_value_, dim=1, keepdim=True)\n                target_action_value = self.target_net(obses_next)\n                target_action_max_value = target_action_value.gather(1, action_max_idx)\n            else:\n                target_action_value = self.target_net(obses_next)\n                target_action_max_value, _ = torch.max(target_action_value, dim=1, keepdim=True)\n        # target\n        expected_value = rewards + self.args.gamma * target_action_max_value * dones\n        # get the real q value\n        action_value = self.net(obses)\n        real_value = action_value.gather(1, actions)\n        loss = (expected_value - real_value).pow(2).mean()\n        # start to update\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        return loss.item()\n\n    # get tensors\n    def _get_tensors(self, obs):\n        if obs.ndim == 3:\n            obs = np.transpose(obs, (2, 0, 1))\n            obs = np.expand_dims(obs, 0)\n        elif obs.ndim == 4:\n            obs = np.transpose(obs, (0, 3, 1, 2))\n        obs = torch.tensor(obs, dtype=torch.float32)\n        if self.args.cuda:\n            obs = obs.cuda()\n        return obs\n"""
rl_algorithms/dqn_algos/models.py,3,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# the convolution layer of deepmind\nclass deepmind(nn.Module):\n    def __init__(self):\n        super(deepmind, self).__init__()\n        self.conv1 = nn.Conv2d(4, 32, 8, stride=4)\n        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n        self.conv3 = nn.Conv2d(64, 32, 3, stride=1)\n        \n        # start to do the init...\n        nn.init.orthogonal_(self.conv1.weight.data, gain=nn.init.calculate_gain('relu'))\n        nn.init.orthogonal_(self.conv2.weight.data, gain=nn.init.calculate_gain('relu'))\n        nn.init.orthogonal_(self.conv3.weight.data, gain=nn.init.calculate_gain('relu'))\n        # init the bias...\n        nn.init.constant_(self.conv1.bias.data, 0)\n        nn.init.constant_(self.conv2.bias.data, 0)\n        nn.init.constant_(self.conv3.bias.data, 0)\n        \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = x.view(-1, 32 * 7 * 7)\n\n        return x\n\n# in the initial, just the nature CNN\nclass net(nn.Module):\n    def __init__(self, num_actions, use_dueling=False):\n        super(net, self).__init__()\n        # if use the dueling network\n        self.use_dueling = use_dueling\n        # define the network\n        self.cnn_layer = deepmind()\n        # if not use dueling\n        if not self.use_dueling:\n            self.fc1 = nn.Linear(32 * 7 * 7, 256)\n            self.action_value = nn.Linear(256, num_actions)\n        else:\n            # the layer for dueling network architecture\n            self.action_fc = nn.Linear(32 * 7 * 7, 256)\n            self.state_value_fc = nn.Linear(32 * 7 * 7, 256)\n            self.action_value = nn.Linear(256, num_actions)\n            self.state_value = nn.Linear(256, 1)\n\n    def forward(self, inputs):\n        x = self.cnn_layer(inputs / 255.0)\n        if not self.use_dueling:\n            x = F.relu(self.fc1(x))\n            action_value_out = self.action_value(x)\n        else:\n            # get the action value\n            action_fc = F.relu(self.action_fc(x))\n            action_value = self.action_value(action_fc)\n            # get the state value\n            state_value_fc = F.relu(self.state_value_fc(x))\n            state_value = self.state_value(state_value_fc)\n            # action value mean\n            action_value_mean = torch.mean(action_value, dim=1, keepdim=True)\n            action_value_center = action_value - action_value_mean\n            # Q = V + A\n            action_value_out = state_value + action_value_center\n        return action_value_out\n"""
rl_algorithms/dqn_algos/train.py,0,"b""import sys\nfrom arguments import get_args\nfrom rl_utils.env_wrapper.create_env import create_single_env\nfrom rl_utils.logger import logger, bench\nfrom rl_utils.seeds.seeds import set_seeds\nfrom dqn_agent import dqn_agent\nimport os\nimport numpy as np \n\nif __name__ == '__main__':\n    # get arguments\n    args = get_args()\n    # start to create the environment\n    env = create_single_env(args)\n    # set seeds\n    set_seeds(args)\n    # create trainer\n    dqn_trainer = dqn_agent(env, args)\n    # start to learn \n    dqn_trainer.learn()\n    # finally - close the environment\n    env.close()\n"""
rl_algorithms/dqn_algos/utils.py,0,"b'import numpy as np\nimport random\n\n# linear exploration schedule\nclass linear_schedule:\n    def __init__(self, total_timesteps, final_ratio, init_ratio=1.0):\n        self.total_timesteps = total_timesteps\n        self.final_ratio = final_ratio\n        self.init_ratio = init_ratio\n\n    def get_value(self, timestep):\n        frac = min(float(timestep) / self.total_timesteps, 1.0)\n        return self.init_ratio - frac * (self.init_ratio - self.final_ratio)\n\n# select actions\ndef select_actions(action_value, explore_eps):\n    action_value = action_value.cpu().numpy().squeeze()\n    # select actions\n    action = np.argmax(action_value) if random.random() > explore_eps else np.random.randint(action_value.shape[0])\n    return action\n\n# record the reward info of the dqn experiments\nclass reward_recorder:\n    def __init__(self, history_length=100):\n        self.history_length = history_length\n        # the empty buffer to store rewards \n        self.buffer = [0.0]\n        self._episode_length = 1\n    \n    # add rewards\n    def add_rewards(self, reward):\n        self.buffer[-1] += reward\n\n    # start new episode\n    def start_new_episode(self):\n        if self.get_length >= self.history_length:\n            self.buffer.pop(0)\n        # append new one\n        self.buffer.append(0.0)\n        self._episode_length += 1\n\n    # get length of buffer\n    @property\n    def get_length(self):\n        return len(self.buffer)\n    \n    @property\n    def mean(self):\n        return np.mean(self.buffer)\n    \n    # get the length of total episodes\n    @property \n    def num_episodes(self):\n        return self._episode_length\n'"
rl_algorithms/ppo/arguments.py,0,"b""import argparse\n\ndef get_args():\n    parse = argparse.ArgumentParser()\n    parse.add_argument('--gamma', type=float, default=0.99, help='the discount factor of RL')\n    parse.add_argument('--seed', type=int, default=123, help='the random seeds')\n    parse.add_argument('--num-workers', type=int, default=8, help='the number of workers to collect samples')\n    parse.add_argument('--env-name', type=str, default='PongNoFrameskip-v4', help='the environment name')\n    parse.add_argument('--batch-size', type=int, default=4, help='the batch size of updating')\n    parse.add_argument('--lr', type=float, default=2.5e-4, help='learning rate of the algorithm')\n    parse.add_argument('--epoch', type=int, default=4, help='the epoch during training')\n    parse.add_argument('--nsteps', type=int, default=128, help='the steps to collect samples')\n    parse.add_argument('--vloss-coef', type=float, default=0.5, help='the coefficient of value loss')\n    parse.add_argument('--ent-coef', type=float, default=0.01, help='the entropy loss coefficient')\n    parse.add_argument('--tau', type=float, default=0.95, help='gae coefficient')\n    parse.add_argument('--cuda', action='store_true', help='use cuda do the training')\n    parse.add_argument('--total-frames', type=int, default=20000000, help='the total frames for training')\n    parse.add_argument('--dist', type=str, default='gauss', help='the distributions for sampling actions')\n    parse.add_argument('--eps', type=float, default=1e-5, help='param for adam optimizer')\n    parse.add_argument('--clip', type=float, default=0.1, help='the ratio clip param')\n    parse.add_argument('--save-dir', type=str, default='saved_models/', help='the folder to save models')\n    parse.add_argument('--lr-decay', action='store_true', help='if using the learning rate decay during decay')\n    parse.add_argument('--max-grad-norm', type=float, default=0.5, help='grad norm')\n    parse.add_argument('--display-interval', type=int, default=10, help='the interval that display log information')\n    parse.add_argument('--env-type', type=str, default='atari', help='the type of the environment')\n    parse.add_argument('--log-dir', type=str, default='logs', help='the folders to save the log files')\n\n    args = parse.parse_args()\n\n    return args\n"""
rl_algorithms/ppo/demo.py,6,"b""from arguments import get_args\nfrom models import cnn_net, mlp_net\nimport torch\nimport cv2\nimport numpy as np\nimport gym\nfrom rl_utils.env_wrapper.frame_stack import VecFrameStack\nfrom rl_utils.env_wrapper.atari_wrapper import make_atari, wrap_deepmind\n\n# denormalize\ndef normalize(x, mean, std, clip=10):\n    x -= mean\n    x /= (std + 1e-8)\n    return np.clip(x, -clip, clip)\n\n# get tensors for the agent\ndef get_tensors(obs, env_type, filters=None):\n    if env_type == 'atari':\n        tensor = torch.tensor(np.transpose(obs, (2, 0, 1)), dtype=torch.float32).unsqueeze(0)\n    elif env_type == 'mujoco':\n        tensor = torch.tensor(normalize(obs, filters.rs.mean, filters.rs.std), dtype=torch.float32).unsqueeze(0)\n    return tensor\n\nif __name__ == '__main__':\n    # get the arguments\n    args = get_args()\n    # create the environment\n    if args.env_type == 'atari':\n        env = make_atari(args.env_name)\n        env = wrap_deepmind(env, frame_stack=True)\n    elif args.env_type == 'mujoco':\n        env = gym.make(args.env_name)\n    # get the model path\n    model_path = args.save_dir + args.env_name + '/model.pt'\n    # create the network\n    if args.env_type == 'atari':\n        network = cnn_net(env.action_space.n)\n        network.load_state_dict(torch.load(model_path, map_location=lambda storage, loc: storage))\n        filters = None\n    elif args.env_type == 'mujoco':\n        network = mlp_net(env.observation_space.shape[0], env.action_space.shape[0], args.dist)\n        net_models, filters = torch.load(model_path, map_location=lambda storage, loc: storage)\n        # load models \n        network.load_state_dict(net_models)\n    # start to play the demo\n    obs = env.reset()\n    reward_total = 0\n    # just one episode\n    while True:\n        env.render()\n        with torch.no_grad():\n            obs_tensor = get_tensors(obs, args.env_type, filters)\n            _, pi = network(obs_tensor)\n            # get actions\n            if args.env_type == 'atari':\n                actions = torch.argmax(pi, dim=1).item()\n            elif args.env_type == 'mujoco':\n                if args.dist == 'gauss':\n                    mean, _ = pi\n                    actions = mean.numpy().squeeze()\n                elif args.dist == 'beta':\n                    alpha, beta = pi\n                    actions = (alpha - 1) / (alpha + beta - 2)\n                    actions = actions.numpy().squeeze()\n                    actions = -1 + 2 * actions \n        obs_, reward, done, _ = env.step(actions)\n        reward_total += reward\n        if done:\n            break\n        obs = obs_\n    print('the rewrads is: {}'.format(reward_total))\n"""
rl_algorithms/ppo/models.py,7,"b'import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n""""""\nthis network also include gaussian distribution and beta distribution\n\n""""""\n\nclass mlp_net(nn.Module):\n    def __init__(self, state_size, num_actions, dist_type):\n        super(mlp_net, self).__init__()\n        self.dist_type = dist_type\n        self.fc1_v = nn.Linear(state_size, 64)\n        self.fc2_v = nn.Linear(64, 64)\n        self.fc1_a = nn.Linear(state_size, 64)\n        self.fc2_a = nn.Linear(64, 64)\n        # check the type of distribution\n        if self.dist_type == \'gauss\':\n            self.sigma_log = nn.Parameter(torch.zeros(1, num_actions))\n            self.action_mean = nn.Linear(64, num_actions)\n            self.action_mean.weight.data.mul_(0.1)\n            self.action_mean.bias.data.zero_()\n        elif self.dist_type == \'beta\':\n            self.action_alpha = nn.Linear(64, num_actions)\n            self.action_beta = nn.Linear(64, num_actions)\n            # init..\n            self.action_alpha.weight.data.mul_(0.1)\n            self.action_alpha.bias.data.zero_()\n            self.action_beta.weight.data.mul_(0.1)\n            self.action_beta.bias.data.zero_()\n\n        # define layers to output state value\n        self.value = nn.Linear(64, 1)\n        self.value.weight.data.mul_(0.1)\n        self.value.bias.data.zero_()\n\n    def forward(self, x):\n        x_v = torch.tanh(self.fc1_v(x))\n        x_v = torch.tanh(self.fc2_v(x_v))\n        state_value = self.value(x_v)\n        # output the policy...\n        x_a = torch.tanh(self.fc1_a(x))\n        x_a = torch.tanh(self.fc2_a(x_a))\n        if self.dist_type == \'gauss\':\n            mean = self.action_mean(x_a)\n            sigma_log = self.sigma_log.expand_as(mean)\n            sigma = torch.exp(sigma_log)\n            pi = (mean, sigma)\n        elif self.dist_type == \'beta\':\n            alpha = F.softplus(self.action_alpha(x_a)) + 1\n            beta = F.softplus(self.action_beta(x_a)) + 1\n            pi = (alpha, beta)\n\n        return state_value, pi\n\n# the convolution layer of deepmind\nclass deepmind(nn.Module):\n    def __init__(self):\n        super(deepmind, self).__init__()\n        self.conv1 = nn.Conv2d(4, 32, 8, stride=4)\n        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n        self.conv3 = nn.Conv2d(64, 32, 3, stride=1)\n        self.fc1 = nn.Linear(32 * 7 * 7, 512) \n        # start to do the init...\n        nn.init.orthogonal_(self.conv1.weight.data, gain=nn.init.calculate_gain(\'relu\'))\n        nn.init.orthogonal_(self.conv2.weight.data, gain=nn.init.calculate_gain(\'relu\'))\n        nn.init.orthogonal_(self.conv3.weight.data, gain=nn.init.calculate_gain(\'relu\'))\n        nn.init.orthogonal_(self.fc1.weight.data, gain=nn.init.calculate_gain(\'relu\'))\n        # init the bias...\n        nn.init.constant_(self.conv1.bias.data, 0)\n        nn.init.constant_(self.conv2.bias.data, 0)\n        nn.init.constant_(self.conv3.bias.data, 0)\n        nn.init.constant_(self.fc1.bias.data, 0)\n        \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = x.view(-1, 32 * 7 * 7)\n        x = F.relu(self.fc1(x))\n        return x\n\n# in the initial, just the nature CNN\nclass cnn_net(nn.Module):\n    def __init__(self, num_actions):\n        super(cnn_net, self).__init__()\n        self.cnn_layer = deepmind()\n        self.critic = nn.Linear(512, 1)\n        self.actor = nn.Linear(512, num_actions)\n        # init the linear layer..\n        nn.init.orthogonal_(self.critic.weight.data)\n        nn.init.constant_(self.critic.bias.data, 0)\n        # init the policy layer...\n        nn.init.orthogonal_(self.actor.weight.data, gain=0.01)\n        nn.init.constant_(self.actor.bias.data, 0)\n\n    def forward(self, inputs):\n        x = self.cnn_layer(inputs / 255.0)\n        value = self.critic(x)\n        pi = F.softmax(self.actor(x), dim=1)\n        return value, pi\n'"
rl_algorithms/ppo/ppo_agent.py,14,"b""import numpy as np\nimport torch\nfrom torch import optim\nfrom rl_utils.running_filter.running_filter import ZFilter\nfrom models import cnn_net, mlp_net\nfrom utils import select_actions, evaluate_actions\nfrom datetime import datetime\nimport os\nimport copy\n\nclass ppo_agent:\n    def __init__(self, envs, args):\n        self.envs = envs \n        self.args = args\n        # start to build the network.\n        if self.args.env_type == 'atari':\n            self.net = cnn_net(envs.action_space.n)\n        elif self.args.env_type == 'mujoco':\n            self.net = mlp_net(envs.observation_space.shape[0], envs.action_space.shape[0], self.args.dist)\n        self.old_net = copy.deepcopy(self.net)\n        # if use the cuda...\n        if self.args.cuda:\n            self.net.cuda()\n            self.old_net.cuda()\n        # define the optimizer...\n        self.optimizer = optim.Adam(self.net.parameters(), self.args.lr, eps=self.args.eps)\n        # running filter...\n        if self.args.env_type == 'mujoco':\n            num_states = self.envs.observation_space.shape[0]\n            self.running_state = ZFilter((num_states, ), clip=5)\n        # check saving folder..\n        if not os.path.exists(self.args.save_dir):\n            os.mkdir(self.args.save_dir)\n        # env folder..\n        self.model_path = os.path.join(self.args.save_dir, self.args.env_name)\n        if not os.path.exists(self.model_path):\n            os.mkdir(self.model_path)\n        # get the observation\n        self.batch_ob_shape = (self.args.num_workers * self.args.nsteps, ) + self.envs.observation_space.shape\n        self.obs = np.zeros((self.args.num_workers, ) + self.envs.observation_space.shape, dtype=self.envs.observation_space.dtype.name)\n        if self.args.env_type == 'mujoco':\n            self.obs[:] = np.expand_dims(self.running_state(self.envs.reset()), 0)\n        else:\n            self.obs[:] = self.envs.reset()\n        self.dones = [False for _ in range(self.args.num_workers)]\n\n    # start to train the network...\n    def learn(self):\n        num_updates = self.args.total_frames // (self.args.nsteps * self.args.num_workers)\n        # get the reward to calculate other informations\n        episode_rewards = np.zeros((self.args.num_workers, ), dtype=np.float32)\n        final_rewards = np.zeros((self.args.num_workers, ), dtype=np.float32)\n        for update in range(num_updates):\n            mb_obs, mb_rewards, mb_actions, mb_dones, mb_values = [], [], [], [], []\n            if self.args.lr_decay:\n                self._adjust_learning_rate(update, num_updates)\n            for step in range(self.args.nsteps):\n                with torch.no_grad():\n                    # get tensors\n                    obs_tensor = self._get_tensors(self.obs)\n                    values, pis = self.net(obs_tensor)\n                # select actions\n                actions = select_actions(pis, self.args.dist, self.args.env_type)\n                if self.args.env_type == 'atari':\n                    input_actions = actions \n                else:\n                    if self.args.dist == 'gauss':\n                        input_actions = actions.copy()\n                    elif self.args.dist == 'beta':\n                        input_actions = -1 + 2 * actions\n                # start to store information\n                mb_obs.append(np.copy(self.obs))\n                mb_actions.append(actions)\n                mb_dones.append(self.dones)\n                mb_values.append(values.detach().cpu().numpy().squeeze())\n                # start to excute the actions in the environment\n                obs, rewards, dones, _ = self.envs.step(input_actions)\n                # update dones\n                if self.args.env_type == 'mujoco':\n                    dones = np.array([dones])\n                    rewards = np.array([rewards])\n                self.dones = dones\n                mb_rewards.append(rewards)\n                # clear the observation\n                for n, done in enumerate(dones):\n                    if done:\n                        self.obs[n] = self.obs[n] * 0\n                        if self.args.env_type == 'mujoco':\n                            # reset the environment\n                            obs = self.envs.reset()\n                self.obs = obs if self.args.env_type == 'atari' else np.expand_dims(self.running_state(obs), 0)\n                # process the rewards part -- display the rewards on the screen\n                episode_rewards += rewards\n                masks = np.array([0.0 if done_ else 1.0 for done_ in dones], dtype=np.float32)\n                final_rewards *= masks\n                final_rewards += (1 - masks) * episode_rewards\n                episode_rewards *= masks\n            # process the rollouts\n            mb_obs = np.asarray(mb_obs, dtype=np.float32)\n            mb_rewards = np.asarray(mb_rewards, dtype=np.float32)\n            mb_actions = np.asarray(mb_actions, dtype=np.float32)\n            mb_dones = np.asarray(mb_dones, dtype=np.bool)\n            mb_values = np.asarray(mb_values, dtype=np.float32)\n            if self.args.env_type == 'mujoco':\n                mb_values = np.expand_dims(mb_values, 1)\n            # compute the last state value\n            with torch.no_grad():\n                obs_tensor = self._get_tensors(self.obs)\n                last_values, _ = self.net(obs_tensor)\n                last_values = last_values.detach().cpu().numpy().squeeze()\n            # start to compute advantages...\n            mb_returns = np.zeros_like(mb_rewards)\n            mb_advs = np.zeros_like(mb_rewards)\n            lastgaelam = 0\n            for t in reversed(range(self.args.nsteps)):\n                if t == self.args.nsteps - 1:\n                    nextnonterminal = 1.0 - self.dones\n                    nextvalues = last_values\n                else:\n                    nextnonterminal = 1.0 - mb_dones[t + 1]\n                    nextvalues = mb_values[t + 1]\n                delta = mb_rewards[t] + self.args.gamma * nextvalues * nextnonterminal - mb_values[t]\n                mb_advs[t] = lastgaelam = delta + self.args.gamma * self.args.tau * nextnonterminal * lastgaelam\n            mb_returns = mb_advs + mb_values\n            # after compute the returns, let's process the rollouts\n            mb_obs = mb_obs.swapaxes(0, 1).reshape(self.batch_ob_shape)\n            if self.args.env_type == 'atari':\n                mb_actions = mb_actions.swapaxes(0, 1).flatten()\n            mb_returns = mb_returns.swapaxes(0, 1).flatten()\n            mb_advs = mb_advs.swapaxes(0, 1).flatten()\n            # before update the network, the old network will try to load the weights\n            self.old_net.load_state_dict(self.net.state_dict())\n            # start to update the network\n            pl, vl, ent = self._update_network(mb_obs, mb_actions, mb_returns, mb_advs)\n            # display the training information\n            if update % self.args.display_interval == 0:\n                print('[{}] Update: {} / {}, Frames: {}, Rewards: {:.3f}, Min: {:.3f}, Max: {:.3f}, PL: {:.3f},'\\\n                    'VL: {:.3f}, Ent: {:.3f}'.format(datetime.now(), update, num_updates, (update + 1)*self.args.nsteps*self.args.num_workers, \\\n                    final_rewards.mean(), final_rewards.min(), final_rewards.max(), pl, vl, ent))\n                # save the model\n                if self.args.env_type == 'atari':\n                    torch.save(self.net.state_dict(), self.model_path + '/model.pt')\n                else:\n                    # for the mujoco, we also need to keep the running mean filter!\n                    torch.save([self.net.state_dict(), self.running_state], self.model_path + '/model.pt')\n\n    # update the network\n    def _update_network(self, obs, actions, returns, advantages):\n        inds = np.arange(obs.shape[0])\n        nbatch_train = obs.shape[0] // self.args.batch_size\n        for _ in range(self.args.epoch):\n            np.random.shuffle(inds)\n            for start in range(0, obs.shape[0], nbatch_train):\n                # get the mini-batchs\n                end = start + nbatch_train\n                mbinds = inds[start:end]\n                mb_obs = obs[mbinds]\n                mb_actions = actions[mbinds]\n                mb_returns = returns[mbinds]\n                mb_advs = advantages[mbinds]\n                # convert minibatches to tensor\n                mb_obs = self._get_tensors(mb_obs)\n                mb_actions = torch.tensor(mb_actions, dtype=torch.float32)\n                mb_returns = torch.tensor(mb_returns, dtype=torch.float32).unsqueeze(1)\n                mb_advs = torch.tensor(mb_advs, dtype=torch.float32).unsqueeze(1)\n                # normalize adv\n                mb_advs = (mb_advs - mb_advs.mean()) / (mb_advs.std() + 1e-8)\n                if self.args.cuda:\n                    mb_actions = mb_actions.cuda()\n                    mb_returns = mb_returns.cuda()\n                    mb_advs = mb_advs.cuda()\n                # start to get values\n                mb_values, pis = self.net(mb_obs)\n                # start to calculate the value loss...\n                value_loss = (mb_returns - mb_values).pow(2).mean()\n                # start to calculate the policy loss\n                with torch.no_grad():\n                    _, old_pis = self.old_net(mb_obs)\n                    # get the old log probs\n                    old_log_prob, _ = evaluate_actions(old_pis, mb_actions, self.args.dist, self.args.env_type)\n                    old_log_prob = old_log_prob.detach()\n                # evaluate the current policy\n                log_prob, ent_loss = evaluate_actions(pis, mb_actions, self.args.dist, self.args.env_type)\n                prob_ratio = torch.exp(log_prob - old_log_prob)\n                # surr1\n                surr1 = prob_ratio * mb_advs\n                surr2 = torch.clamp(prob_ratio, 1 - self.args.clip, 1 + self.args.clip) * mb_advs\n                policy_loss = -torch.min(surr1, surr2).mean()\n                # final total loss\n                total_loss = policy_loss + self.args.vloss_coef * value_loss - ent_loss * self.args.ent_coef\n                # clear the grad buffer\n                self.optimizer.zero_grad()\n                total_loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.net.parameters(), self.args.max_grad_norm)\n                # update\n                self.optimizer.step()\n        return policy_loss.item(), value_loss.item(), ent_loss.item()\n\n    # convert the numpy array to tensors\n    def _get_tensors(self, obs):\n        if self.args.env_type == 'atari':\n            obs_tensor = torch.tensor(np.transpose(obs, (0, 3, 1, 2)), dtype=torch.float32)\n        else:\n            obs_tensor = torch.tensor(obs, dtype=torch.float32)\n        # decide if put the tensor on the GPU\n        if self.args.cuda:\n            obs_tensor = obs_tensor.cuda()\n        return obs_tensor\n\n    # adjust the learning rate\n    def _adjust_learning_rate(self, update, num_updates):\n        lr_frac = 1 - (update / num_updates)\n        adjust_lr = self.args.lr * lr_frac\n        for param_group in self.optimizer.param_groups:\n             param_group['lr'] = adjust_lr\n"""
rl_algorithms/ppo/train.py,0,"b""from arguments import get_args\nfrom ppo_agent import ppo_agent\nfrom rl_utils.env_wrapper.create_env import create_multiple_envs, create_single_env\nfrom rl_utils.seeds.seeds import set_seeds\nimport os\n\nif __name__ == '__main__':\n    # set signle thread\n    os.environ['OMP_NUM_THREADS'] = '1'\n    os.environ['MKL_NUM_THREADS'] = '1'\n    # get arguments\n    args = get_args()\n    # start to create the environment\n    if args.env_type == 'atari':\n        envs = create_multiple_envs(args)\n    elif args.env_type == 'mujoco':\n        envs = create_single_env(args)\n    else:\n        raise NotImplementedError\n    # create trainer\n    ppo_trainer = ppo_agent(envs, args)\n    # start to learn\n    ppo_trainer.learn()\n    # close the environment\n    envs.close()\n"""
rl_algorithms/ppo/utils.py,3,"b""import numpy as np\nimport torch\nfrom torch.distributions.normal import Normal\nfrom torch.distributions.beta import Beta\nfrom torch.distributions.categorical import Categorical\nimport random\n\ndef select_actions(pi, dist_type, env_type):\n    if env_type == 'atari':\n        actions = Categorical(pi).sample()\n    else:\n        if dist_type == 'gauss':\n            mean, std = pi\n            actions = Normal(mean, std).sample()\n        elif dist_type == 'beta':\n            alpha, beta = pi\n            actions = Beta(alpha.detach().cpu(), beta.detach().cpu()).sample()\n    # return actions\n    return actions.detach().cpu().numpy().squeeze()\n\ndef evaluate_actions(pi, actions, dist_type, env_type):\n    if env_type == 'atari':\n        cate_dist = Categorical(pi)\n        log_prob = cate_dist.log_prob(actions).unsqueeze(-1)\n        entropy = cate_dist.entropy().mean()\n    else:\n        if dist_type == 'gauss':\n            mean, std = pi\n            normal_dist = Normal(mean, std)\n            log_prob = normal_dist.log_prob(actions).sum(dim=1, keepdim=True)\n            entropy = normal_dist.entropy().mean()\n        elif dist_type == 'beta':\n            alpha, beta = pi\n            beta_dist = Beta(alpha, beta)\n            log_prob = beta_dist.log_prob(actions).sum(dim=1, keepdim=True)\n            entropy = beta_dist.entropy().mean()\n    return log_prob, entropy\n"""
rl_algorithms/sac/arguments.py,0,"b""import argparse\n\n# define the arguments that will be used in the SAC\ndef get_args():\n    parse = argparse.ArgumentParser()\n    parse.add_argument('--env-name', type=str, default='HalfCheetah-v2', help='the environment name')\n    parse.add_argument('--cuda', action='store_true', help='use GPU do the training')\n    parse.add_argument('--seed', type=int, default=123, help='the random seed to reproduce results')\n    parse.add_argument('--hidden-size', type=int, default=256, help='the size of the hidden layer')\n    parse.add_argument('--train-loop-per-epoch', type=int, default=1, help='the training loop per epoch')\n    parse.add_argument('--q-lr', type=float, default=3e-4, help='the learning rate')\n    parse.add_argument('--p-lr', type=float, default=3e-4, help='the learning rate of the actor')\n    parse.add_argument('--n-epochs', type=int, default=int(3e3), help='the number of total epochs')\n    parse.add_argument('--epoch-length', type=int, default=int(1e3), help='the lenght of each epoch')\n    parse.add_argument('--n-updates', type=int, default=int(1e3), help='the number of training updates execute')\n    parse.add_argument('--init-exploration-steps', type=int, default=int(1e3), help='the steps of the initial exploration')\n    parse.add_argument('--init-exploration-policy', type=str, default='gaussian', help='the inital exploration policy')\n    parse.add_argument('--buffer-size', type=int, default=int(1e6), help='the size of the replay buffer')\n    parse.add_argument('--batch-size', type=int, default=256, help='the batch size of samples for training')\n    parse.add_argument('--reward-scale', type=float, default=1, help='the reward scale')\n    parse.add_argument('--gamma', type=float, default=0.99, help='the discount factor')\n    parse.add_argument('--log-std-max', type=float, default=2, help='the maximum log std value')\n    parse.add_argument('--log-std-min', type=float, default=-20, help='the minimum log std value')\n    parse.add_argument('--entropy-weights', type=float, default=0.2, help='the entropy weights')\n    parse.add_argument('--tau', type=float, default=5e-3, help='the soft update coefficient')\n    parse.add_argument('--target-update-interval', type=int, default=1, help='the interval to update target network')\n    parse.add_argument('--update-cycles', type=int, default=int(1e3), help='how many updates apply in the update')\n    parse.add_argument('--eval-episodes', type=int, default=10, help='the episodes that used for evaluation')\n    parse.add_argument('--display-interval', type=int, default=1, help='the display interval')\n    parse.add_argument('--save-dir', type=str, default='saved_models/', help='the place to save models')\n    parse.add_argument('--reg', type=float, default=1e-3, help='the reg term')\n    parse.add_argument('--auto-ent-tuning', action='store_true', help='tune the entorpy automatically')\n    parse.add_argument('--log-dir', type=str, default='logs/', help='dir to save log information')\n    parse.add_argument('--env-type', type=str, default=None, help='environment type')\n\n    return parse.parse_args()\n"""
rl_algorithms/sac/demo.py,4,"b""from arguments import get_args\nimport gym\nimport torch\nimport numpy as np\nfrom models import tanh_gaussian_actor\n\nif __name__ == '__main__':\n    args = get_args()\n    env = gym.make(args.env_name)\n    # get environment infos\n    obs_dims = env.observation_space.shape[0]\n    action_dims = env.action_space.shape[0]\n    action_max = env.action_space.high[0]\n    # define the network\n    actor_net = tanh_gaussian_actor(obs_dims, action_dims, args.hidden_size, args.log_std_min, args.log_std_max)\n    # load models\n    model_path = args.save_dir + args.env_name + '/model.pt'\n    # load the network weights\n    actor_net.load_state_dict(torch.load(model_path, map_location='cpu'))\n    for ep in range(5):\n        obs = env.reset()\n        reward_sum = 0\n        # set the maximum timesteps here...\n        for _ in range(1000):\n            env.render()\n            with torch.no_grad():\n                obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n                mean, std = actor_net(obs_tensor)\n                actions = torch.tanh(mean).detach().numpy().squeeze()\n                if action_dims == 1:\n                    actions = np.array([actions])\n            obs_, reward, done, _ = env.step(action_max * actions)\n            reward_sum += reward\n            if done:\n                break\n            obs = obs_\n        print('the episode is: {}, the reward is: {}'.format(ep, reward_sum))\n    env.close()\n"""
rl_algorithms/sac/models.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# the flatten mlp\nclass flatten_mlp(nn.Module):\n    #TODO: add the initialization method for it\n    def __init__(self, input_dims, hidden_size, action_dims=None):\n        super(flatten_mlp, self).__init__()\n        self.fc1 = nn.Linear(input_dims, hidden_size) if action_dims is None else nn.Linear(input_dims + action_dims, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.q_value = nn.Linear(hidden_size, 1)\n\n    def forward(self, obs, action=None):\n        inputs = torch.cat([obs, action], dim=1) if action is not None else obs\n        x = F.relu(self.fc1(inputs))\n        x = F.relu(self.fc2(x))\n        output = self.q_value(x)\n        return output\n\n# define the policy network - tanh gaussian policy network\n# TODO: Not use the log std\nclass tanh_gaussian_actor(nn.Module):\n    def __init__(self, input_dims, action_dims, hidden_size, log_std_min, log_std_max):\n        super(tanh_gaussian_actor, self).__init__()\n        self.fc1 = nn.Linear(input_dims, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.mean = nn.Linear(hidden_size, action_dims)\n        self.log_std = nn.Linear(hidden_size, action_dims)\n        # the log_std_min and log_std_max\n        self.log_std_min = log_std_min\n        self.log_std_max = log_std_max\n\n    def forward(self, obs):\n        x = F.relu(self.fc1(obs))\n        x = F.relu(self.fc2(x))\n        mean = self.mean(x)\n        log_std = self.log_std(x)\n        # clamp the log std\n        log_std = torch.clamp(log_std, min=self.log_std_min, max=self.log_std_max)\n        # the reparameterization trick\n        # return mean and std\n        return (mean, torch.exp(log_std))\n'"
rl_algorithms/sac/sac_agent.py,18,"b'import numpy as np\nimport torch\nfrom models import flatten_mlp, tanh_gaussian_actor\nfrom rl_utils.experience_replay.experience_replay import replay_buffer\nfrom utils import get_action_info\nfrom datetime import datetime\nimport copy\nimport os\nimport gym\n\n\n""""""\n2019-Nov-12 - start to add the automatically tempature tuning\n\n2019-JUN-05\n\nauthor: Tianhong Dai\n\n""""""\n\n# the soft-actor-critic agent\nclass sac_agent:\n    def __init__(self, env, args):\n        self.args = args\n        self.env = env\n        # create eval environment\n        self.eval_env = gym.make(self.args.env_name)\n        self.eval_env.seed(args.seed * 2)\n        # build up the network that will be used.\n        self.qf1 = flatten_mlp(self.env.observation_space.shape[0], self.args.hidden_size, self.env.action_space.shape[0])\n        self.qf2 = flatten_mlp(self.env.observation_space.shape[0], self.args.hidden_size, self.env.action_space.shape[0])\n        # set the target q functions\n        self.target_qf1 = copy.deepcopy(self.qf1)\n        self.target_qf2 = copy.deepcopy(self.qf2)\n        # build up the policy network\n        self.actor_net = tanh_gaussian_actor(self.env.observation_space.shape[0], self.env.action_space.shape[0], self.args.hidden_size, \\\n                                            self.args.log_std_min, self.args.log_std_max)\n        # define the optimizer for them\n        self.qf1_optim = torch.optim.Adam(self.qf1.parameters(), lr=self.args.q_lr)\n        self.qf2_optim = torch.optim.Adam(self.qf2.parameters(), lr=self.args.q_lr)\n        # the optimizer for the policy network\n        self.actor_optim = torch.optim.Adam(self.actor_net.parameters(), lr=self.args.p_lr)\n        # entorpy target\n        self.target_entropy = -np.prod(self.env.action_space.shape).item()\n        self.log_alpha = torch.zeros(1, requires_grad=True, device=\'cuda\' if self.args.cuda else \'cpu\')\n        # define the optimizer\n        self.alpha_optim = torch.optim.Adam([self.log_alpha], lr=self.args.p_lr)\n        # define the replay buffer\n        self.buffer = replay_buffer(self.args.buffer_size)\n        # get the action max\n        self.action_max = self.env.action_space.high[0]\n        # if use cuda, put tensor onto the gpu\n        if self.args.cuda:\n            self.actor_net.cuda()\n            self.qf1.cuda()\n            self.qf2.cuda()\n            self.target_qf1.cuda()\n            self.target_qf2.cuda()\n        # automatically create the folders to save models\n        if not os.path.exists(self.args.save_dir):\n            os.mkdir(self.args.save_dir)\n        self.model_path = os.path.join(self.args.save_dir, self.args.env_name)\n        if not os.path.exists(self.model_path):\n            os.mkdir(self.model_path)\n\n    # train the agent\n    def learn(self):\n        global_timesteps = 0\n        # before the official training, do the initial exploration to add episodes into the replay buffer\n        self._initial_exploration(exploration_policy=self.args.init_exploration_policy) \n        # reset the environment\n        obs = self.env.reset()\n        for epoch in range(self.args.n_epochs):\n            for _ in range(self.args.train_loop_per_epoch):\n                # for each epoch, it will reset the environment\n                for t in range(self.args.epoch_length):\n                    # start to collect samples\n                    with torch.no_grad():\n                        obs_tensor = self._get_tensor_inputs(obs)\n                        pi = self.actor_net(obs_tensor)\n                        action = get_action_info(pi, cuda=self.args.cuda).select_actions(reparameterize=False)\n                        action = action.cpu().numpy()[0]\n                    # input the actions into the environment\n                    obs_, reward, done, _ = self.env.step(self.action_max * action)\n                    # store the samples\n                    self.buffer.add(obs, action, reward, obs_, float(done))\n                    # reassign the observations\n                    obs = obs_\n                    if done:\n                        # reset the environment\n                        obs = self.env.reset()\n                # after collect the samples, start to update the network\n                for _ in range(self.args.update_cycles):\n                    qf1_loss, qf2_loss, actor_loss, alpha, alpha_loss = self._update_newtork()\n                    # update the target network\n                    if global_timesteps % self.args.target_update_interval == 0:\n                        self._update_target_network(self.target_qf1, self.qf1)\n                        self._update_target_network(self.target_qf2, self.qf2)\n                    global_timesteps += 1\n            # print the log information\n            if epoch % self.args.display_interval == 0:\n                # start to do the evaluation\n                mean_rewards = self._evaluate_agent()\n                print(\'[{}] Epoch: {} / {}, Frames: {}, Rewards: {:.3f}, QF1: {:.3f}, QF2: {:.3f}, AL: {:.3f}, Alpha: {:.5f}, AlphaL: {:.5f}\'.format(datetime.now(), \\\n                            epoch, self.args.n_epochs, (epoch + 1) * self.args.epoch_length, mean_rewards, qf1_loss, qf2_loss, actor_loss, alpha, alpha_loss))\n                # save models\n                torch.save(self.actor_net.state_dict(), self.model_path + \'/model.pt\')\n    \n    # do the initial exploration by using the uniform policy\n    def _initial_exploration(self, exploration_policy=\'gaussian\'):\n        # get the action information of the environment\n        obs = self.env.reset()\n        for _ in range(self.args.init_exploration_steps):\n            if exploration_policy == \'uniform\':\n                raise NotImplementedError\n            elif exploration_policy == \'gaussian\':\n                # the sac does not need normalize?\n                with torch.no_grad():\n                    obs_tensor = self._get_tensor_inputs(obs)\n                    # generate the policy\n                    pi = self.actor_net(obs_tensor)\n                    action = get_action_info(pi).select_actions(reparameterize=False)\n                    action = action.cpu().numpy()[0]\n                # input the action input the environment\n                obs_, reward, done, _ = self.env.step(self.action_max * action)\n                # store the episodes\n                self.buffer.add(obs, action, reward, obs_, float(done))\n                obs = obs_\n                if done:\n                    # if done, reset the environment\n                    obs = self.env.reset()\n        print(""Initial exploration has been finished!"")\n    # get tensors\n    def _get_tensor_inputs(self, obs):\n        obs_tensor = torch.tensor(obs, dtype=torch.float32, device=\'cuda\' if self.args.cuda else \'cpu\').unsqueeze(0)\n        return obs_tensor\n    \n    # update the network\n    def _update_newtork(self):\n        # smaple batch of samples from the replay buffer\n        obses, actions, rewards, obses_, dones = self.buffer.sample(self.args.batch_size)\n        # preprocessing the data into the tensors, will support GPU later\n        obses = torch.tensor(obses, dtype=torch.float32, device=\'cuda\' if self.args.cuda else \'cpu\')\n        actions = torch.tensor(actions, dtype=torch.float32, device=\'cuda\' if self.args.cuda else \'cpu\')\n        rewards = torch.tensor(rewards, dtype=torch.float32, device=\'cuda\' if self.args.cuda else \'cpu\').unsqueeze(-1)\n        obses_ = torch.tensor(obses_, dtype=torch.float32, device=\'cuda\' if self.args.cuda else \'cpu\')\n        inverse_dones = torch.tensor(1 - dones, dtype=torch.float32, device=\'cuda\' if self.args.cuda else \'cpu\').unsqueeze(-1)\n        # start to update the actor network\n        pis = self.actor_net(obses)\n        actions_info = get_action_info(pis, cuda=self.args.cuda)\n        actions_, pre_tanh_value = actions_info.select_actions(reparameterize=True)\n        log_prob = actions_info.get_log_prob(actions_, pre_tanh_value)\n        # use the automatically tuning\n        alpha_loss = -(self.log_alpha * (log_prob + self.target_entropy).detach()).mean()\n        self.alpha_optim.zero_grad()\n        alpha_loss.backward()\n        self.alpha_optim.step()\n        # get the param\n        alpha = self.log_alpha.exp()\n        # get the q_value for new actions\n        q_actions_ = torch.min(self.qf1(obses, actions_), self.qf2(obses, actions_))\n        actor_loss = (alpha * log_prob - q_actions_).mean()\n        # q value function loss\n        q1_value = self.qf1(obses, actions)\n        q2_value = self.qf2(obses, actions)\n        with torch.no_grad():\n            pis_next = self.actor_net(obses_)\n            actions_info_next = get_action_info(pis_next, cuda=self.args.cuda)\n            actions_next_, pre_tanh_value_next = actions_info_next.select_actions(reparameterize=True)\n            log_prob_next = actions_info_next.get_log_prob(actions_next_, pre_tanh_value_next)\n            target_q_value_next = torch.min(self.target_qf1(obses_, actions_next_), self.target_qf2(obses_, actions_next_)) - alpha * log_prob_next\n            target_q_value = self.args.reward_scale * rewards + inverse_dones * self.args.gamma * target_q_value_next \n        qf1_loss = (q1_value - target_q_value).pow(2).mean()\n        qf2_loss = (q2_value - target_q_value).pow(2).mean()\n        # qf1\n        self.qf1_optim.zero_grad()\n        qf1_loss.backward()\n        self.qf1_optim.step()\n        # qf2\n        self.qf2_optim.zero_grad()\n        qf2_loss.backward()\n        self.qf2_optim.step()\n        # policy loss\n        self.actor_optim.zero_grad()\n        actor_loss.backward()\n        self.actor_optim.step()\n        return qf1_loss.item(), qf2_loss.item(), actor_loss.item(), alpha.item(), alpha_loss.item()\n    \n    # update the target network\n    def _update_target_network(self, target, source):\n        for target_param, param in zip(target.parameters(), source.parameters()):\n            target_param.data.copy_(self.args.tau * param.data + (1 - self.args.tau) * target_param.data)\n\n    # evaluate the agent\n    def _evaluate_agent(self):\n        total_reward = 0\n        for _ in range(self.args.eval_episodes):\n            obs = self.eval_env.reset()\n            episode_reward = 0 \n            while True:\n                with torch.no_grad():\n                    obs_tensor = self._get_tensor_inputs(obs)\n                    pi = self.actor_net(obs_tensor)\n                    action = get_action_info(pi, cuda=self.args.cuda).select_actions(exploration=False, reparameterize=False)\n                    action = action.detach().cpu().numpy()[0]\n                # input the action into the environment\n                obs_, reward, done, _ = self.eval_env.step(self.action_max * action)\n                episode_reward += reward\n                if done:\n                    break\n                obs = obs_\n            total_reward += episode_reward\n        return total_reward / self.args.eval_episodes\n'"
rl_algorithms/sac/train.py,0,"b""from arguments import get_args\nfrom sac_agent import sac_agent\nfrom rl_utils.seeds.seeds import set_seeds\nfrom rl_utils.env_wrapper.create_env import create_single_env\n\nif __name__ == '__main__':\n    args = get_args()\n    # build the environment\n    env = create_single_env(args)\n    # set the seeds\n    set_seeds(args)\n    # create the agent\n    sac_trainer = sac_agent(env, args)\n    sac_trainer.learn()\n    # close the environment\n    env.close()\n"""
rl_algorithms/sac/utils.py,13,"b'import numpy as np\nimport torch\nfrom torch.distributions.normal import Normal\nfrom torch.distributions import Distribution\n\n""""""\nthe tanhnormal distributions from rlkit may not stable\n\n""""""\nclass tanh_normal(Distribution):\n    def __init__(self, normal_mean, normal_std, epsilon=1e-6, cuda=False):\n        self.normal_mean = normal_mean\n        self.normal_std = normal_std\n        self.cuda = cuda\n        self.normal = Normal(normal_mean, normal_std)\n        self.epsilon = epsilon\n\n    def sample_n(self, n, return_pre_tanh_value=False):\n        z = self.normal.sample_n(n)\n        if return_pre_tanh_value:\n            return torch.tanh(z), z\n        else:\n            return torch.tanh(z)\n\n    def log_prob(self, value, pre_tanh_value=None):\n        """"""\n        :param value: some value, x\n        :param pre_tanh_value: arctanh(x)\n        :return:\n        """"""\n        if pre_tanh_value is None:\n            pre_tanh_value = torch.log((1 + value) / (1 - value)) / 2\n        return self.normal.log_prob(pre_tanh_value) - torch.log(1 - value * value + self.epsilon)\n\n    def sample(self, return_pretanh_value=False):\n        """"""\n        Gradients will and should *not* pass through this operation.\n\n        See https://github.com/pytorch/pytorch/issues/4620 for discussion.\n        """"""\n        z = self.normal.sample().detach()\n        if return_pretanh_value:\n            return torch.tanh(z), z\n        else:\n            return torch.tanh(z)\n\n    def rsample(self, return_pretanh_value=False):\n        """"""\n        Sampling in the reparameterization case.\n        """"""\n        sample_mean = torch.zeros(self.normal_mean.size(), dtype=torch.float32, device=\'cuda\' if self.cuda else \'cpu\')\n        sample_std = torch.ones(self.normal_std.size(), dtype=torch.float32, device=\'cuda\' if self.cuda else \'cpu\')\n        z = (self.normal_mean + self.normal_std * Normal(sample_mean, sample_std).sample())\n        z.requires_grad_()\n        if return_pretanh_value:\n            return torch.tanh(z), z\n        else:\n            return torch.tanh(z)\n\n# get action_infos\nclass get_action_info:\n    def __init__(self, pis, cuda=False):\n        self.mean, self.std = pis\n        self.dist = tanh_normal(normal_mean=self.mean, normal_std=self.std, cuda=cuda)\n    \n    # select actions\n    def select_actions(self, exploration=True, reparameterize=True):\n        if exploration:\n            if reparameterize:\n                actions, pretanh = self.dist.rsample(return_pretanh_value=True)\n                return actions, pretanh \n            else:\n                actions = self.dist.sample()\n        else:\n            actions = torch.tanh(self.mean)\n        return actions\n\n    def get_log_prob(self, actions, pre_tanh_value):\n        log_prob = self.dist.log_prob(actions, pre_tanh_value=pre_tanh_value)\n        return log_prob.sum(dim=1, keepdim=True)\n'"
rl_algorithms/trpo/arguments.py,0,"b""import argparse\n\ndef get_args():\n    parse = argparse.ArgumentParser()\n    parse.add_argument('--gamma', type=float, default=0.99, help='the discount factor of the RL')\n    parse.add_argument('--env-name', type=str, default='Walker2d-v2', help='the training environment')\n    parse.add_argument('--seed', type=int, default=123, help='the random seed')\n    parse.add_argument('--save-dir', type=str, default='saved_models/', help='the folder to save models')\n    parse.add_argument('--total-timesteps', type=int, default=int(1e6), help='the total frames')\n    parse.add_argument('--nsteps', type=int, default=1024, help='the steps to collect samples')\n    parse.add_argument('--lr', type=float, default=3e-4)\n    parse.add_argument('--batch-size', type=int, default=64, help='the mini batch size ot update the value function')\n    parse.add_argument('--vf-itrs', type=int, default=5, help='the times to update the value network')\n    parse.add_argument('--tau', type=float, default=0.95, help='the param to calculate the gae')\n    parse.add_argument('--damping', type=float, default=0.1, help='the damping coeffificent')\n    parse.add_argument('--max-kl', type=float, default=0.01, help='the max kl divergence')\n    parse.add_argument('--cuda', action='store_true', help='if use gpu')\n    parse.add_argument('--env-type', type=str, default='mujoco', help='the environment type')\n    parse.add_argument('--log-dir', type=str, default='logs', help='folder to save log files')\n\n    args = parse.parse_args()\n\n    return args\n"""
rl_algorithms/trpo/demo.py,3,"b""import numpy as np\nimport torch\nimport gym\nfrom arguments import get_args\nfrom models import network\n\ndef denormalize(x, mean, std, clip=10):\n    x -= mean\n    x /= (std + 1e-8)\n    return np.clip(x, -clip, clip)\n\ndef get_tensors(x):\n    return torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n\nif __name__ == '__main__':\n    args = get_args()\n    # create the environment\n    env = gym.make(args.env_name)\n    # build up the network\n    net = network(env.observation_space.shape[0], env.action_space.shape[0])\n    # load the saved model\n    model_path = args.save_dir + args.env_name + '/model.pt'\n    network_model, filters = torch.load(model_path, map_location=lambda storage, loc: storage)\n    net.load_state_dict(network_model)\n    net.eval()\n    for _ in range(10):\n        obs = denormalize(env.reset(), filters.rs.mean, filters.rs.std)\n        reward_total = 0\n        for _ in range(10000):\n            env.render()\n            obs_tensor = get_tensors(obs)\n            with torch.no_grad():\n                _, (mean, _) = net(obs_tensor)\n                action = mean.numpy().squeeze()\n            obs, reward, done, _ = env.step(action)\n            reward_total += reward\n            obs = denormalize(obs, filters.rs.mean, filters.rs.std)\n            if done:\n                break\n        print('the reward of this episode is: {}'.format(reward_total))\n    env.close()\n"""
rl_algorithms/trpo/models.py,3,"b'import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nclass network(nn.Module):\n    def __init__(self, num_states, num_actions):\n        super(network, self).__init__()\n        # define the critic\n        self.critic = critic(num_states)\n        self.actor = actor(num_states, num_actions)\n\n    def forward(self, x):\n        state_value = self.critic(x)\n        pi = self.actor(x)\n        return state_value, pi\n\nclass critic(nn.Module):\n    def __init__(self, num_states):\n        super(critic, self).__init__()\n        self.fc1 = nn.Linear(num_states, 64)\n        self.fc2 = nn.Linear(64, 64)\n        self.value = nn.Linear(64, 1)\n\n    def forward(self, x):\n        x = F.tanh(self.fc1(x))\n        x = F.tanh(self.fc2(x))\n        value = self.value(x)\n        return value\n\nclass actor(nn.Module):\n    def __init__(self, num_states, num_actions):\n        super(actor, self).__init__()\n        self.fc1 = nn.Linear(num_states, 64)\n        self.fc2 = nn.Linear(64, 64)\n        self.action_mean = nn.Linear(64, num_actions)\n        self.sigma_log = nn.Parameter(torch.zeros(1, num_actions))\n\n    def forward(self, x):\n        x = F.tanh(self.fc1(x))\n        x = F.tanh(self.fc2(x))\n        mean = self.action_mean(x)\n        sigma_log = self.sigma_log.expand_as(mean)\n        sigma = torch.exp(sigma_log)\n        pi = (mean, sigma)\n        \n        return pi\n'"
rl_algorithms/trpo/train.py,0,"b""from arguments import get_args\nfrom rl_utils.seeds.seeds import set_seeds\nfrom rl_utils.env_wrapper.create_env import create_single_env\nfrom trpo_agent import trpo_agent\n\nif __name__ == '__main__':\n    args = get_args()\n    # make environemnts\n    env = create_single_env(args)\n    # set the random seeds\n    set_seeds(args)\n    # create trpo trainer\n    trpo_trainer = trpo_agent(env, args)\n    trpo_trainer.learn()\n    # close the environment\n    env.close()\n"""
rl_algorithms/trpo/trpo_agent.py,23,"b""import torch\nimport numpy as np\nimport os\nfrom models import network\nfrom rl_utils.running_filter.running_filter import ZFilter\nfrom utils import select_actions, eval_actions, conjugated_gradient, line_search, set_flat_params_to\nfrom datetime import datetime\n\nclass trpo_agent:\n    def __init__(self, env, args):\n        self.env = env\n        self.args = args\n        # define the network\n        self.net = network(self.env.observation_space.shape[0], self.env.action_space.shape[0])\n        self.old_net = network(self.env.observation_space.shape[0], self.env.action_space.shape[0])\n        # make sure the net and old net have the same parameters\n        self.old_net.load_state_dict(self.net.state_dict())\n        # define the optimizer\n        self.optimizer = torch.optim.Adam(self.net.critic.parameters(), lr=self.args.lr)\n        # define the running mean filter\n        self.running_state = ZFilter((self.env.observation_space.shape[0],), clip=5)\n        if not os.path.exists(self.args.save_dir):\n            os.mkdir(self.args.save_dir)\n        self.model_path = self.args.save_dir + self.args.env_name + '/'\n        if not os.path.exists(self.model_path):\n            os.mkdir(self.model_path)\n\n    def learn(self):\n        num_updates = self.args.total_timesteps // self.args.nsteps\n        obs = self.running_state(self.env.reset())\n        final_reward = 0\n        episode_reward = 0\n        self.dones = False\n        for update in range(num_updates):\n            mb_obs, mb_rewards, mb_actions, mb_dones, mb_values = [], [], [], [], []\n            for step in range(self.args.nsteps):\n                with torch.no_grad():\n                    obs_tensor = self._get_tensors(obs)\n                    value, pi = self.net(obs_tensor)\n                # select actions\n                actions = select_actions(pi)\n                # store informations\n                mb_obs.append(np.copy(obs))\n                mb_actions.append(actions)\n                mb_dones.append(self.dones)\n                mb_values.append(value.detach().numpy().squeeze())\n                # start to execute actions in the environment\n                obs_, reward, done, _ = self.env.step(actions)\n                self.dones = done\n                mb_rewards.append(reward)\n                if done:\n                    obs_ = self.env.reset()\n                obs = self.running_state(obs_)\n                episode_reward += reward\n                mask = 0.0 if done else 1.0\n                final_reward *= mask\n                final_reward += (1 - mask) * episode_reward\n                episode_reward *= mask\n            # to process the rollouts\n            mb_obs = np.asarray(mb_obs, dtype=np.float32)\n            mb_rewards = np.asarray(mb_rewards, dtype=np.float32)\n            mb_actions = np.asarray(mb_actions, dtype=np.float32)\n            mb_dones = np.asarray(mb_dones, dtype=np.bool)\n            mb_values = np.asarray(mb_values, dtype=np.float32)\n            # compute the last state value\n            with torch.no_grad():\n                obs_tensor = self._get_tensors(obs)\n                last_value, _ = self.net(obs_tensor)\n                last_value = last_value.detach().numpy().squeeze()\n            # compute the advantages\n            mb_returns = np.zeros_like(mb_rewards)\n            mb_advs = np.zeros_like(mb_rewards)\n            lastgaelam = 0\n            for t in reversed(range(self.args.nsteps)):\n                if t == self.args.nsteps - 1:\n                    nextnonterminal = 1.0 - self.dones\n                    nextvalues = last_value\n                else:\n                    nextnonterminal = 1.0 - mb_dones[t + 1]\n                    nextvalues = mb_values[t + 1]\n                delta = mb_rewards[t] + self.args.gamma * nextvalues * nextnonterminal - mb_values[t]\n                mb_advs[t] = lastgaelam = delta + self.args.gamma * self.args.tau * nextnonterminal * lastgaelam\n            mb_returns = mb_advs + mb_values\n            # normalize the advantages\n            mb_advs = (mb_advs - mb_advs.mean()) / (mb_advs.std() + 1e-5)\n            # before the update, make the old network has the parameter of the current network\n            self.old_net.load_state_dict(self.net.state_dict())\n            # start to update the network\n            policy_loss, value_loss = self._update_network(mb_obs, mb_actions, mb_returns, mb_advs)\n            torch.save([self.net.state_dict(), self.running_state], self.model_path + 'model.pt')\n            print('[{}] Update: {} / {}, Frames: {}, Reward: {:.3f}, VL: {:.3f}, PL: {:.3f}'.format(datetime.now(), update, \\\n                    num_updates, (update + 1)*self.args.nsteps, final_reward, value_loss, policy_loss))\n\n    # start to update network\n    def _update_network(self, mb_obs, mb_actions, mb_returns, mb_advs):\n        mb_obs_tensor = torch.tensor(mb_obs, dtype=torch.float32)\n        mb_actions_tensor = torch.tensor(mb_actions, dtype=torch.float32)\n        mb_returns_tensor = torch.tensor(mb_returns, dtype=torch.float32).unsqueeze(1)\n        mb_advs_tensor = torch.tensor(mb_advs, dtype=torch.float32).unsqueeze(1)\n        # try to get the old policy and current policy\n        values, _ = self.net(mb_obs_tensor)\n        with torch.no_grad():\n            _, pi_old = self.old_net(mb_obs_tensor)\n        # get the surr loss\n        surr_loss = self._get_surrogate_loss(mb_obs_tensor, mb_advs_tensor, mb_actions_tensor, pi_old)\n        # comupte the surrogate gardient -> g, Ax = g, where A is the fisher information matrix\n        surr_grad = torch.autograd.grad(surr_loss, self.net.actor.parameters())\n        flat_surr_grad = torch.cat([grad.view(-1) for grad in surr_grad]).data\n        # use the conjugated gradient to calculate the scaled direction vector (natural gradient)\n        nature_grad = conjugated_gradient(self._fisher_vector_product, -flat_surr_grad, 10, mb_obs_tensor, pi_old)\n        # calculate the scaleing ratio\n        non_scale_kl = 0.5 * (nature_grad * self._fisher_vector_product(nature_grad, mb_obs_tensor, pi_old)).sum(0, keepdim=True)\n        scale_ratio = torch.sqrt(non_scale_kl / self.args.max_kl)\n        final_nature_grad = nature_grad / scale_ratio[0]\n        # calculate the expected improvement rate...\n        expected_improve = (-flat_surr_grad * nature_grad).sum(0, keepdim=True) / scale_ratio[0]\n        # get the flat param ...\n        prev_params = torch.cat([param.data.view(-1) for param in self.net.actor.parameters()])\n        # start to do the line search\n        success, new_params = line_search(self.net.actor, self._get_surrogate_loss, prev_params, final_nature_grad, \\\n                                expected_improve, mb_obs_tensor, mb_advs_tensor, mb_actions_tensor, pi_old)\n        set_flat_params_to(self.net.actor, new_params)\n        # then trying to update the critic network\n        inds = np.arange(mb_obs.shape[0])\n        for _ in range(self.args.vf_itrs):\n            np.random.shuffle(inds)\n            for start in range(0, mb_obs.shape[0], self.args.batch_size):\n                end = start + self.args.batch_size\n                mbinds = inds[start:end]\n                mini_obs = mb_obs[mbinds]\n                mini_returns = mb_returns[mbinds]\n                # put things in the tensor\n                mini_obs = torch.tensor(mini_obs, dtype=torch.float32)\n                mini_returns = torch.tensor(mini_returns, dtype=torch.float32).unsqueeze(1)\n                values, _ = self.net(mini_obs)\n                v_loss = (mini_returns - values).pow(2).mean()\n                self.optimizer.zero_grad()\n                v_loss.backward()\n                self.optimizer.step()\n        return surr_loss.item(), v_loss.item()\n\n    # get the surrogate loss\n    def _get_surrogate_loss(self, obs, adv, actions, pi_old):\n        _, pi = self.net(obs)\n        log_prob = eval_actions(pi, actions)\n        old_log_prob = eval_actions(pi_old, actions).detach()\n        surr_loss = -torch.exp(log_prob - old_log_prob) * adv\n        return surr_loss.mean()\n\n    # the product of the fisher informaiton matrix and the nature gradient -> Ax\n    def _fisher_vector_product(self, v, obs, pi_old):\n        kl = self._get_kl(obs, pi_old)\n        kl = kl.mean()\n        # start to calculate the second order gradient of the KL\n        kl_grads = torch.autograd.grad(kl, self.net.actor.parameters(), create_graph=True)\n        flat_kl_grads = torch.cat([grad.view(-1) for grad in kl_grads])\n        kl_v = (flat_kl_grads * torch.autograd.Variable(v)).sum()\n        kl_second_grads = torch.autograd.grad(kl_v, self.net.actor.parameters())\n        flat_kl_second_grads = torch.cat([grad.contiguous().view(-1) for grad in kl_second_grads]).data\n        flat_kl_second_grads = flat_kl_second_grads + self.args.damping * v\n        return flat_kl_second_grads\n\n    # get the kl divergence between two distributions\n    def _get_kl(self, obs, pi_old):\n        mean_old, std_old = pi_old\n        _, pi = self.net(obs)\n        mean, std = pi\n        # start to calculate the kl-divergence\n        kl = -torch.log(std / std_old) + (std.pow(2) + (mean - mean_old).pow(2)) / (2 * std_old.pow(2)) - 0.5\n        return kl.sum(1, keepdim=True)\n \n    # get the tensors\n    def _get_tensors(self, obs):\n        return torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n"""
rl_algorithms/trpo/utils.py,5,"b'import numpy as np\nimport torch\nfrom torch.distributions.normal import Normal\n\n# select actions\ndef select_actions(pi):\n    mean, std = pi\n    normal_dist = Normal(mean, std)\n    return normal_dist.sample().detach().numpy().squeeze()\n\n# evaluate the actions\ndef eval_actions(pi, actions):\n    mean, std = pi\n    normal_dist = Normal(mean, std)\n    return normal_dist.log_prob(actions).sum(dim=1, keepdim=True)\n\n# conjugated gradient\ndef conjugated_gradient(fvp, b, update_steps, obs, pi_old, residual_tol=1e-10):\n    # the initial solution is zero\n    x = torch.zeros(b.size(), dtype=torch.float32)\n    r = b.clone()\n    p = b.clone()\n    rdotr = torch.dot(r, r)\n    for i in range(update_steps):\n        fv_product = fvp(p, obs, pi_old)\n        alpha = rdotr / torch.dot(p, fv_product)\n        x = x + alpha * p\n        r = r - alpha * fv_product\n        new_rdotr = torch.dot(r, r)\n        beta = new_rdotr / rdotr \n        p = r + beta * p\n        rdotr = new_rdotr\n        # if less than residual tot.. break\n        if rdotr < residual_tol:\n            break\n    return x\n\n# line search\ndef line_search(model, loss_fn, x, full_step, expected_rate, obs, adv, actions, pi_old, max_backtracks=10, accept_ratio=0.1):\n    fval = loss_fn(obs, adv, actions, pi_old).data\n    for (_n_backtracks, stepfrac) in enumerate(0.5**np.arange(max_backtracks)):\n        xnew = x + stepfrac * full_step\n        set_flat_params_to(model, xnew)\n        new_fval = loss_fn(obs, adv, actions, pi_old).data\n        actual_improve = fval - new_fval\n        expected_improve = expected_rate * stepfrac\n        ratio = actual_improve / expected_improve\n        if ratio.item() > accept_ratio and actual_improve.item() > 0:\n            return True, xnew\n    return False, x\n\n\ndef set_flat_params_to(model, flat_params):\n    prev_indx = 0\n    for param in model.parameters():\n        flat_size = int(np.prod(list(param.size())))\n        param.data.copy_(flat_params[prev_indx:prev_indx + flat_size].view(param.size()))\n        prev_indx += flat_size\n'"
rl_utils/env_wrapper/__init__.py,0,"b'import os\nfrom abc import ABC, abstractmethod\nimport contextlib\n\nclass AlreadySteppingError(Exception):\n    """"""\n    Raised when an asynchronous step is running while\n    step_async() is called again.\n    """"""\n\n    def __init__(self):\n        msg = \'already running an async step\'\n        Exception.__init__(self, msg)\n\n\nclass NotSteppingError(Exception):\n    """"""\n    Raised when an asynchronous step is not running but\n    step_wait() is called.\n    """"""\n\n    def __init__(self):\n        msg = \'not running an async step\'\n        Exception.__init__(self, msg)\n\n\nclass VecEnv(ABC):\n    """"""\n    An abstract asynchronous, vectorized environment.\n    Used to batch data from multiple copies of an environment, so that\n    each observation becomes an batch of observations, and expected action is a batch of actions to\n    be applied per-environment.\n    """"""\n    closed = False\n    viewer = None\n\n    metadata = {\n        \'render.modes\': [\'human\', \'rgb_array\']\n    }\n\n    def __init__(self, num_envs, observation_space, action_space):\n        self.num_envs = num_envs\n        self.observation_space = observation_space\n        self.action_space = action_space\n\n    @abstractmethod\n    def reset(self):\n        """"""\n        Reset all the environments and return an array of\n        observations, or a dict of observation arrays.\n\n        If step_async is still doing work, that work will\n        be cancelled and step_wait() should not be called\n        until step_async() is invoked again.\n        """"""\n        pass\n\n    @abstractmethod\n    def step_async(self, actions):\n        """"""\n        Tell all the environments to start taking a step\n        with the given actions.\n        Call step_wait() to get the results of the step.\n\n        You should not call this if a step_async run is\n        already pending.\n        """"""\n        pass\n\n    @abstractmethod\n    def step_wait(self):\n        """"""\n        Wait for the step taken with step_async().\n\n        Returns (obs, rews, dones, infos):\n         - obs: an array of observations, or a dict of\n                arrays of observations.\n         - rews: an array of rewards\n         - dones: an array of ""episode done"" booleans\n         - infos: a sequence of info objects\n        """"""\n        pass\n\n    def close_extras(self):\n        """"""\n        Clean up the  extra resources, beyond what\'s in this base class.\n        Only runs when not self.closed.\n        """"""\n        pass\n\n    def close(self):\n        if self.closed:\n            return\n        if self.viewer is not None:\n            self.viewer.close()\n        self.close_extras()\n        self.closed = True\n\n    def step(self, actions):\n        """"""\n        Step the environments synchronously.\n\n        This is available for backwards compatibility.\n        """"""\n        self.step_async(actions)\n        return self.step_wait()\n\n    def render(self, mode=\'human\'):\n        raise NotImplementedError\n\n    def get_images(self):\n        """"""\n        Return RGB images from each environment\n        """"""\n        raise NotImplementedError\n\n    @property\n    def unwrapped(self):\n        if isinstance(self, VecEnvWrapper):\n            return self.venv.unwrapped\n        else:\n            return self\n\n    def get_viewer(self):\n        if self.viewer is None:\n            from gym.envs.classic_control import rendering\n            self.viewer = rendering.SimpleImageViewer()\n        return self.viewer\n\nclass VecEnvWrapper(VecEnv):\n    """"""\n    An environment wrapper that applies to an entire batch\n    of environments at once.\n    """"""\n\n    def __init__(self, venv, observation_space=None, action_space=None):\n        self.venv = venv\n        super().__init__(num_envs=venv.num_envs,\n                        observation_space=observation_space or venv.observation_space,\n                        action_space=action_space or venv.action_space)\n\n    def step_async(self, actions):\n        self.venv.step_async(actions)\n\n    @abstractmethod\n    def reset(self):\n        pass\n\n    @abstractmethod\n    def step_wait(self):\n        pass\n\n    def close(self):\n        return self.venv.close()\n\n    def render(self, mode=\'human\'):\n        return self.venv.render(mode=mode)\n\n    def get_images(self):\n        return self.venv.get_images()\n\n    def __getattr__(self, name):\n        if name.startswith(\'_\'):\n            raise AttributeError(""attempted to get missing private attribute \'{}\'"".format(name))\n        return getattr(self.venv, name)\n\nclass VecEnvObservationWrapper(VecEnvWrapper):\n    @abstractmethod\n    def process(self, obs):\n        pass\n\n    def reset(self):\n        obs = self.venv.reset()\n        return self.process(obs)\n\n    def step_wait(self):\n        obs, rews, dones, infos = self.venv.step_wait()\n        return self.process(obs), rews, dones, infos\n\nclass CloudpickleWrapper(object):\n    """"""\n    Uses cloudpickle to serialize contents (otherwise multiprocessing tries to use pickle)\n    """"""\n\n    def __init__(self, x):\n        self.x = x\n\n    def __getstate__(self):\n        import cloudpickle\n        return cloudpickle.dumps(self.x)\n\n    def __setstate__(self, ob):\n        import pickle\n        self.x = pickle.loads(ob)\n\n@contextlib.contextmanager\ndef clear_mpi_env_vars():\n    """"""\n    from mpi4py import MPI will call MPI_Init by default.  If the child process has MPI environment variables, MPI will think that the child process is an MPI process just like the parent and do bad things such as hang.\n    This context manager is a hacky way to clear those environment variables temporarily such as when we are starting multiprocessing\n    Processes.\n    """"""\n    removed_environment = {}\n    for k, v in list(os.environ.items()):\n        for prefix in [\'OMPI_\', \'PMI_\']:\n            if k.startswith(prefix):\n                removed_environment[k] = v\n                del os.environ[k]\n    try:\n        yield\n    finally:\n        os.environ.update(removed_environment)\n'"
rl_utils/env_wrapper/atari_wrapper.py,0,"b'import numpy as np\nimport os\nos.environ.setdefault(\'PATH\', \'\')\nfrom collections import deque\nimport gym\nfrom gym import spaces\nimport cv2\ncv2.ocl.setUseOpenCL(False)\n\n""""""\nthe wrapper is taken from the openai baselines\n\n""""""\n\nclass NoopResetEnv(gym.Wrapper):\n    def __init__(self, env, noop_max=30):\n        """"""Sample initial states by taking random number of no-ops on reset.\n        No-op is assumed to be action 0.\n        """"""\n        gym.Wrapper.__init__(self, env)\n        self.noop_max = noop_max\n        self.override_num_noops = None\n        self.noop_action = 0\n        assert env.unwrapped.get_action_meanings()[0] == \'NOOP\'\n\n    def reset(self, **kwargs):\n        """""" Do no-op action for a number of steps in [1, noop_max].""""""\n        self.env.reset(**kwargs)\n        if self.override_num_noops is not None:\n            noops = self.override_num_noops\n        else:\n            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1) #pylint: disable=E1101\n        assert noops > 0\n        obs = None\n        for _ in range(noops):\n            obs, _, done, _ = self.env.step(self.noop_action)\n            if done:\n                obs = self.env.reset(**kwargs)\n        return obs\n\n    def step(self, ac):\n        return self.env.step(ac)\n\nclass FireResetEnv(gym.Wrapper):\n    def __init__(self, env):\n        """"""Take action on reset for environments that are fixed until firing.""""""\n        gym.Wrapper.__init__(self, env)\n        assert env.unwrapped.get_action_meanings()[1] == \'FIRE\'\n        assert len(env.unwrapped.get_action_meanings()) >= 3\n\n    def reset(self, **kwargs):\n        self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(1)\n        if done:\n            self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(2)\n        if done:\n            self.env.reset(**kwargs)\n        return obs\n\n    def step(self, ac):\n        return self.env.step(ac)\n\nclass EpisodicLifeEnv(gym.Wrapper):\n    def __init__(self, env):\n        """"""Make end-of-life == end-of-episode, but only reset on true game over.\n        Done by DeepMind for the DQN and co. since it helps value estimation.\n        """"""\n        gym.Wrapper.__init__(self, env)\n        self.lives = 0\n        self.was_real_done  = True\n\n    def step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        self.was_real_done = done\n        # check current lives, make loss of life terminal,\n        # then update lives to handle bonus lives\n        lives = self.env.unwrapped.ale.lives()\n        if lives < self.lives and lives > 0:\n            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n            # so it\'s important to keep lives > 0, so that we only reset once\n            # the environment advertises done.\n            done = True\n        self.lives = lives\n        return obs, reward, done, info\n\n    def reset(self, **kwargs):\n        """"""Reset only when lives are exhausted.\n        This way all states are still reachable even though lives are episodic,\n        and the learner need not know about any of this behind-the-scenes.\n        """"""\n        if self.was_real_done:\n            obs = self.env.reset(**kwargs)\n        else:\n            # no-op step to advance from terminal/lost life state\n            obs, _, _, _ = self.env.step(0)\n        self.lives = self.env.unwrapped.ale.lives()\n        return obs\n\nclass MaxAndSkipEnv(gym.Wrapper):\n    def __init__(self, env, skip=4):\n        """"""Return only every `skip`-th frame""""""\n        gym.Wrapper.__init__(self, env)\n        # most recent raw observations (for max pooling across time steps)\n        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n        self._skip       = skip\n\n    def step(self, action):\n        """"""Repeat action, sum reward, and max over last observations.""""""\n        total_reward = 0.0\n        done = None\n        for i in range(self._skip):\n            obs, reward, done, info = self.env.step(action)\n            if i == self._skip - 2: self._obs_buffer[0] = obs\n            if i == self._skip - 1: self._obs_buffer[1] = obs\n            total_reward += reward\n            if done:\n                break\n        # Note that the observation on the done=True frame\n        # doesn\'t matter\n        max_frame = self._obs_buffer.max(axis=0)\n\n        return max_frame, total_reward, done, info\n\n    def reset(self, **kwargs):\n        return self.env.reset(**kwargs)\n\nclass ClipRewardEnv(gym.RewardWrapper):\n    def __init__(self, env):\n        gym.RewardWrapper.__init__(self, env)\n\n    def reward(self, reward):\n        """"""Bin reward to {+1, 0, -1} by its sign.""""""\n        return np.sign(reward)\n\n\nclass WarpFrame(gym.ObservationWrapper):\n    def __init__(self, env, width=84, height=84, grayscale=True, dict_space_key=None):\n        """"""\n        Warp frames to 84x84 as done in the Nature paper and later work.\n\n        If the environment uses dictionary observations, `dict_space_key` can be specified which indicates which\n        observation should be warped.\n        """"""\n        super().__init__(env)\n        self._width = width\n        self._height = height\n        self._grayscale = grayscale\n        self._key = dict_space_key\n        if self._grayscale:\n            num_colors = 1\n        else:\n            num_colors = 3\n\n        new_space = gym.spaces.Box(\n            low=0,\n            high=255,\n            shape=(self._height, self._width, num_colors),\n            dtype=np.uint8,\n        )\n        if self._key is None:\n            original_space = self.observation_space\n            self.observation_space = new_space\n        else:\n            original_space = self.observation_space.spaces[self._key]\n            self.observation_space.spaces[self._key] = new_space\n        assert original_space.dtype == np.uint8 and len(original_space.shape) == 3\n\n    def observation(self, obs):\n        if self._key is None:\n            frame = obs\n        else:\n            frame = obs[self._key]\n\n        if self._grayscale:\n            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n        frame = cv2.resize(\n            frame, (self._width, self._height), interpolation=cv2.INTER_AREA\n        )\n        if self._grayscale:\n            frame = np.expand_dims(frame, -1)\n\n        if self._key is None:\n            obs = frame\n        else:\n            obs = obs.copy()\n            obs[self._key] = frame\n        return obs\n\n\nclass FrameStack(gym.Wrapper):\n    def __init__(self, env, k):\n        """"""Stack k last frames.\n\n        Returns lazy array, which is much more memory efficient.\n\n        See Also\n        --------\n        baselines.common.atari_wrappers.LazyFrames\n        """"""\n        gym.Wrapper.__init__(self, env)\n        self.k = k\n        self.frames = deque([], maxlen=k)\n        shp = env.observation_space.shape\n        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[:-1] + (shp[-1] * k,)), dtype=env.observation_space.dtype)\n\n    def reset(self):\n        ob = self.env.reset()\n        for _ in range(self.k):\n            self.frames.append(ob)\n        return self._get_ob()\n\n    def step(self, action):\n        ob, reward, done, info = self.env.step(action)\n        self.frames.append(ob)\n        return self._get_ob(), reward, done, info\n\n    def _get_ob(self):\n        assert len(self.frames) == self.k\n        return LazyFrames(list(self.frames))\n\nclass ScaledFloatFrame(gym.ObservationWrapper):\n    def __init__(self, env):\n        gym.ObservationWrapper.__init__(self, env)\n        self.observation_space = gym.spaces.Box(low=0, high=1, shape=env.observation_space.shape, dtype=np.float32)\n\n    def observation(self, observation):\n        # careful! This undoes the memory optimization, use\n        # with smaller replay buffers only.\n        return np.array(observation).astype(np.float32) / 255.0\n\nclass LazyFrames(object):\n    def __init__(self, frames):\n        """"""This object ensures that common frames between the observations are only stored once.\n        It exists purely to optimize memory usage which can be huge for DQN\'s 1M frames replay\n        buffers.\n\n        This object should only be converted to numpy array before being passed to the model.\n\n        You\'d not believe how complex the previous solution was.""""""\n        self._frames = frames\n        self._out = None\n\n    def _force(self):\n        if self._out is None:\n            self._out = np.concatenate(self._frames, axis=-1)\n            self._frames = None\n        return self._out\n\n    def __array__(self, dtype=None):\n        out = self._force()\n        if dtype is not None:\n            out = out.astype(dtype)\n        return out\n\n    def __len__(self):\n        return len(self._force())\n\n    def __getitem__(self, i):\n        return self._force()[i]\n\n    def count(self):\n        frames = self._force()\n        return frames.shape[frames.ndim - 1]\n\n    def frame(self, i):\n        return self._force()[..., i]\n\ndef make_atari(env_id, max_episode_steps=None):\n    env = gym.make(env_id)\n    assert \'NoFrameskip\' in env.spec.id\n    env = NoopResetEnv(env, noop_max=30)\n    env = MaxAndSkipEnv(env, skip=4)\n    if max_episode_steps is not None:\n        env = TimeLimit(env, max_episode_steps=max_episode_steps)\n    return env\n\ndef wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, scale=False):\n    """"""Configure environment for DeepMind-style Atari.\n    """"""\n    if episode_life:\n        env = EpisodicLifeEnv(env)\n    if \'FIRE\' in env.unwrapped.get_action_meanings():\n        env = FireResetEnv(env)\n    env = WarpFrame(env)\n    if scale:\n        env = ScaledFloatFrame(env)\n    if clip_rewards:\n        env = ClipRewardEnv(env)\n    if frame_stack:\n        env = FrameStack(env, 4)\n    return env\n\n# time limit\nclass TimeLimit(gym.Wrapper):\n    def __init__(self, env, max_episode_steps=None):\n        super(TimeLimit, self).__init__(env)\n        self._max_episode_steps = max_episode_steps\n        self._elapsed_steps = 0\n\n    def step(self, ac):\n        observation, reward, done, info = self.env.step(ac)\n        self._elapsed_steps += 1\n        if self._elapsed_steps >= self._max_episode_steps:\n            done = True\n            info[\'TimeLimit.truncated\'] = True\n        return observation, reward, done, info\n\n    def reset(self, **kwargs):\n        self._elapsed_steps = 0\n        return self.env.reset(**kwargs)\n'"
rl_utils/env_wrapper/create_env.py,0,"b'from rl_utils.env_wrapper.atari_wrapper import make_atari, wrap_deepmind\nfrom rl_utils.env_wrapper.multi_envs_wrapper import SubprocVecEnv\nfrom rl_utils.env_wrapper.frame_stack import VecFrameStack\nfrom rl_utils.logger import logger, bench\nimport os\nimport gym\n\n""""""\nthis functions is to create the environments\n\n""""""\n\ndef create_single_env(args, rank=0):\n    # setup the log files\n    if rank == 0:\n        if not os.path.exists(args.log_dir):\n            os.mkdir(args.log_dir)\n        log_path = args.log_dir + \'/{}/\'.format(args.env_name)\n        logger.configure(log_path)\n    # start to create environment\n    if args.env_type == \'atari\':\n        # create the environment\n        env = make_atari(args.env_name)\n        # the monitor\n        env = bench.Monitor(env, logger.get_dir())\n        # use the deepmind environment wrapper\n        env = wrap_deepmind(env, frame_stack=True)\n    else:\n        env = gym.make(args.env_name)\n        # add log information\n        env = bench.Monitor(env, logger.get_dir(), allow_early_resets=True)\n    # set seeds to the environment to make sure the reproducebility\n    env.seed(args.seed + rank)\n    return env\n\n# create multiple environments - for multiple\ndef create_multiple_envs(args):\n    # now only support the atari games\n    if args.env_type == \'atari\':\n        def make_env(rank):\n            def _thunk():\n                if not os.path.exists(args.log_dir):\n                    os.mkdir(args.log_dir)\n                log_path = args.log_dir + \'/{}/\'.format(args.env_name)\n                logger.configure(log_path)\n                env = make_atari(args.env_name)\n                # set the seed for the environment\n                env.seed(args.seed + rank)\n                # set loggler\n                env = bench.Monitor(env, logger.get_dir() and os.path.join(logger.get_dir(), str(rank)))\n                # use the deepmind environment wrapper\n                env = wrap_deepmind(env)\n                return env\n            return _thunk\n            # put into sub processing \n        envs = SubprocVecEnv([make_env(i) for i in range(args.num_workers)])\n        # then, frame stack\n        envs = VecFrameStack(envs, 4)\n    else:\n        raise NotImplementedError\n    return envs\n\n'"
rl_utils/env_wrapper/frame_stack.py,0,"b'from rl_utils.env_wrapper import VecEnvWrapper\nimport numpy as np\nfrom gym import spaces\n\n\nclass VecFrameStack(VecEnvWrapper):\n    def __init__(self, venv, nstack):\n        self.venv = venv\n        self.nstack = nstack\n        wos = venv.observation_space  # wrapped ob space\n        low = np.repeat(wos.low, self.nstack, axis=-1)\n        high = np.repeat(wos.high, self.nstack, axis=-1)\n        self.stackedobs = np.zeros((venv.num_envs,) + low.shape, low.dtype)\n        observation_space = spaces.Box(low=low, high=high, dtype=venv.observation_space.dtype)\n        VecEnvWrapper.__init__(self, venv, observation_space=observation_space)\n\n    def step_wait(self):\n        obs, rews, news, infos = self.venv.step_wait()\n        self.stackedobs = np.roll(self.stackedobs, shift=-1, axis=-1)\n        for (i, new) in enumerate(news):\n            if new:\n                self.stackedobs[i] = 0\n        self.stackedobs[..., -obs.shape[-1]:] = obs\n        return self.stackedobs, rews, news, infos\n\n    def reset(self):\n        obs = self.venv.reset()\n        self.stackedobs[...] = 0\n        self.stackedobs[..., -obs.shape[-1]:] = obs\n        return self.stackedobs\n'"
rl_utils/env_wrapper/multi_envs_wrapper.py,0,"b'import multiprocessing as mp\nimport numpy as np\nfrom rl_utils.env_wrapper import VecEnv, CloudpickleWrapper, clear_mpi_env_vars\n\ndef worker(remote, parent_remote, env_fn_wrapper):\n    parent_remote.close()\n    env = env_fn_wrapper.x()\n    try:\n        while True:\n            cmd, data = remote.recv()\n            if cmd == \'step\':\n                ob, reward, done, info = env.step(data)\n                if done:\n                    ob = env.reset()\n                remote.send((ob, reward, done, info))\n            elif cmd == \'reset\':\n                ob = env.reset()\n                remote.send(ob)\n            elif cmd == \'render\':\n                remote.send(env.render(mode=\'rgb_array\'))\n            elif cmd == \'close\':\n                remote.close()\n                break\n            elif cmd == \'get_spaces_spec\':\n                remote.send((env.observation_space, env.action_space, env.spec))\n            else:\n                raise NotImplementedError\n    except KeyboardInterrupt:\n        print(\'SubprocVecEnv worker: got KeyboardInterrupt\')\n    finally:\n        env.close()\n\n\nclass SubprocVecEnv(VecEnv):\n    """"""\n    VecEnv that runs multiple environments in parallel in subproceses and communicates with them via pipes.\n    Recommended to use when num_envs > 1 and step() can be a bottleneck.\n    """"""\n    def __init__(self, env_fns, spaces=None, context=\'spawn\'):\n        """"""\n        Arguments:\n\n        env_fns: iterable of callables -  functions that create environments to run in subprocesses. Need to be cloud-pickleable\n        """"""\n        self.waiting = False\n        self.closed = False\n        nenvs = len(env_fns)\n        ctx = mp.get_context(context)\n        self.remotes, self.work_remotes = zip(*[ctx.Pipe() for _ in range(nenvs)])\n        self.ps = [ctx.Process(target=worker, args=(work_remote, remote, CloudpickleWrapper(env_fn)))\n                   for (work_remote, remote, env_fn) in zip(self.work_remotes, self.remotes, env_fns)]\n        for p in self.ps:\n            p.daemon = True  # if the main process crashes, we should not cause things to hang\n            with clear_mpi_env_vars():\n                p.start()\n        for remote in self.work_remotes:\n            remote.close()\n\n        self.remotes[0].send((\'get_spaces_spec\', None))\n        observation_space, action_space, self.spec = self.remotes[0].recv()\n        self.viewer = None\n        VecEnv.__init__(self, len(env_fns), observation_space, action_space)\n\n    def step_async(self, actions):\n        self._assert_not_closed()\n        for remote, action in zip(self.remotes, actions):\n            remote.send((\'step\', action))\n        self.waiting = True\n\n    def step_wait(self):\n        self._assert_not_closed()\n        results = [remote.recv() for remote in self.remotes]\n        self.waiting = False\n        obs, rews, dones, infos = zip(*results)\n        return _flatten_obs(obs), np.stack(rews), np.stack(dones), infos\n\n    def reset(self):\n        self._assert_not_closed()\n        for remote in self.remotes:\n            remote.send((\'reset\', None))\n        return _flatten_obs([remote.recv() for remote in self.remotes])\n\n    def close_extras(self):\n        self.closed = True\n        if self.waiting:\n            for remote in self.remotes:\n                remote.recv()\n        for remote in self.remotes:\n            remote.send((\'close\', None))\n        for p in self.ps:\n            p.join()\n\n    def get_images(self):\n        self._assert_not_closed()\n        for pipe in self.remotes:\n            pipe.send((\'render\', None))\n        imgs = [pipe.recv() for pipe in self.remotes]\n        return imgs\n\n    def _assert_not_closed(self):\n        assert not self.closed, ""Trying to operate on a SubprocVecEnv after calling close()""\n\n    def __del__(self):\n        if not self.closed:\n            self.close()\n\ndef _flatten_obs(obs):\n    assert isinstance(obs, (list, tuple))\n    assert len(obs) > 0\n\n    if isinstance(obs[0], dict):\n        keys = obs[0].keys()\n        return {k: np.stack([o[k] for o in obs]) for k in keys}\n    else:\n        return np.stack(obs)\n'"
rl_utils/experience_replay/experience_replay.py,0,"b'import numpy as np\nimport random\n\n""""""\ndefine the replay buffer and corresponding algorithms like PER\n\n""""""\n\nclass replay_buffer:\n    def __init__(self, memory_size):\n        self.storge = []\n        self.memory_size = memory_size\n        self.next_idx = 0\n    \n    # add the samples\n    def add(self, obs, action, reward, obs_, done):\n        data = (obs, action, reward, obs_, done)\n        if self.next_idx >= len(self.storge):\n            self.storge.append(data)\n        else:\n            self.storge[self.next_idx] = data\n        # get the next idx\n        self.next_idx = (self.next_idx + 1) % self.memory_size\n    \n    # encode samples\n    def _encode_sample(self, idx):\n        obses, actions, rewards, obses_, dones = [], [], [], [], []\n        for i in idx:\n            data = self.storge[i]\n            obs, action, reward, obs_, done = data\n            obses.append(np.array(obs, copy=False))\n            actions.append(np.array(action, copy=False))\n            rewards.append(reward)\n            obses_.append(np.array(obs_, copy=False))\n            dones.append(done)\n        return np.array(obses), np.array(actions), np.array(rewards), np.array(obses_), np.array(dones)\n    \n    # sample from the memory\n    def sample(self, batch_size):\n        idxes = [random.randint(0, len(self.storge) - 1) for _ in range(batch_size)]\n        return self._encode_sample(idxes)\n'"
rl_utils/logger/__init__.py,0,b''
rl_utils/logger/bench.py,0,"b'__all__ = [\'Monitor\', \'get_monitor_files\', \'load_results\']\n\nfrom gym.core import Wrapper\nimport time\nfrom glob import glob\nimport csv\nimport os.path as osp\nimport json\n\nclass Monitor(Wrapper):\n    EXT = ""monitor.csv""\n    f = None\n\n    def __init__(self, env, filename, allow_early_resets=False, reset_keywords=(), info_keywords=()):\n        Wrapper.__init__(self, env=env)\n        self.tstart = time.time()\n        if filename:\n            self.results_writer = ResultsWriter(filename,\n                header={""t_start"": time.time(), \'env_id\' : env.spec and env.spec.id},\n                extra_keys=reset_keywords + info_keywords\n            )\n        else:\n            self.results_writer = None\n        self.reset_keywords = reset_keywords\n        self.info_keywords = info_keywords\n        self.allow_early_resets = allow_early_resets\n        self.rewards = None\n        self.needs_reset = True\n        self.episode_rewards = []\n        self.episode_lengths = []\n        self.episode_times = []\n        self.total_steps = 0\n        self.current_reset_info = {} # extra info about the current episode, that was passed in during reset()\n\n    def reset(self, **kwargs):\n        self.reset_state()\n        for k in self.reset_keywords:\n            v = kwargs.get(k)\n            if v is None:\n                raise ValueError(\'Expected you to pass kwarg %s into reset\'%k)\n            self.current_reset_info[k] = v\n        return self.env.reset(**kwargs)\n\n    def reset_state(self):\n        if not self.allow_early_resets and not self.needs_reset:\n            raise RuntimeError(""Tried to reset an environment before done. If you want to allow early resets, wrap your env with Monitor(env, path, allow_early_resets=True)"")\n        self.rewards = []\n        self.needs_reset = False\n\n\n    def step(self, action):\n        if self.needs_reset:\n            raise RuntimeError(""Tried to step environment that needs reset"")\n        ob, rew, done, info = self.env.step(action)\n        self.update(ob, rew, done, info)\n        return (ob, rew, done, info)\n\n    def update(self, ob, rew, done, info):\n        self.rewards.append(rew)\n        if done:\n            self.needs_reset = True\n            eprew = sum(self.rewards)\n            eplen = len(self.rewards)\n            epinfo = {""r"": round(eprew, 6), ""l"": eplen, ""t"": round(time.time() - self.tstart, 6)}\n            for k in self.info_keywords:\n                epinfo[k] = info[k]\n            self.episode_rewards.append(eprew)\n            self.episode_lengths.append(eplen)\n            self.episode_times.append(time.time() - self.tstart)\n            epinfo.update(self.current_reset_info)\n            if self.results_writer:\n                self.results_writer.write_row(epinfo)\n            assert isinstance(info, dict)\n            if isinstance(info, dict):\n                info[\'episode\'] = epinfo\n\n        self.total_steps += 1\n\n    def close(self):\n        if self.f is not None:\n            self.f.close()\n\n    def get_total_steps(self):\n        return self.total_steps\n\n    def get_episode_rewards(self):\n        return self.episode_rewards\n\n    def get_episode_lengths(self):\n        return self.episode_lengths\n\n    def get_episode_times(self):\n        return self.episode_times\n\nclass LoadMonitorResultsError(Exception):\n    pass\n\n\nclass ResultsWriter(object):\n    def __init__(self, filename, header=\'\', extra_keys=()):\n        self.extra_keys = extra_keys\n        assert filename is not None\n        if not filename.endswith(Monitor.EXT):\n            if osp.isdir(filename):\n                filename = osp.join(filename, Monitor.EXT)\n            else:\n                filename = filename + ""."" + Monitor.EXT\n        self.f = open(filename, ""wt"")\n        if isinstance(header, dict):\n            header = \'# {} \\n\'.format(json.dumps(header))\n        self.f.write(header)\n        self.logger = csv.DictWriter(self.f, fieldnames=(\'r\', \'l\', \'t\')+tuple(extra_keys))\n        self.logger.writeheader()\n        self.f.flush()\n\n    def write_row(self, epinfo):\n        if self.logger:\n            self.logger.writerow(epinfo)\n            self.f.flush()\n\n\ndef get_monitor_files(dir):\n    return glob(osp.join(dir, ""*"" + Monitor.EXT))\n\ndef load_results(dir):\n    import pandas\n    monitor_files = (\n        glob(osp.join(dir, ""*monitor.json"")) +\n        glob(osp.join(dir, ""*monitor.csv""))) # get both csv and (old) json files\n    if not monitor_files:\n        raise LoadMonitorResultsError(""no monitor files of the form *%s found in %s"" % (Monitor.EXT, dir))\n    dfs = []\n    headers = []\n    for fname in monitor_files:\n        with open(fname, \'rt\') as fh:\n            if fname.endswith(\'csv\'):\n                firstline = fh.readline()\n                if not firstline:\n                    continue\n                assert firstline[0] == \'#\'\n                header = json.loads(firstline[1:])\n                df = pandas.read_csv(fh, index_col=None)\n                headers.append(header)\n            elif fname.endswith(\'json\'): # Deprecated json format\n                episodes = []\n                lines = fh.readlines()\n                header = json.loads(lines[0])\n                headers.append(header)\n                for line in lines[1:]:\n                    episode = json.loads(line)\n                    episodes.append(episode)\n                df = pandas.DataFrame(episodes)\n            else:\n                assert 0, \'unreachable\'\n            df[\'t\'] += header[\'t_start\']\n        dfs.append(df)\n    df = pandas.concat(dfs)\n    df.sort_values(\'t\', inplace=True)\n    df.reset_index(inplace=True)\n    df[\'t\'] -= min(header[\'t_start\'] for header in headers)\n    df.headers = headers # HACK to preserve backwards compatibility\n    return df\n'"
rl_utils/logger/logger.py,0,"b'import os\nimport sys\nimport shutil\nimport os.path as osp\nimport json\nimport time\nimport datetime\nimport tempfile\nfrom collections import defaultdict\nfrom contextlib import contextmanager\n\nDEBUG = 10\nINFO = 20\nWARN = 30\nERROR = 40\n\nDISABLED = 50\n\nclass KVWriter(object):\n    def writekvs(self, kvs):\n        raise NotImplementedError\n\nclass SeqWriter(object):\n    def writeseq(self, seq):\n        raise NotImplementedError\n\nclass HumanOutputFormat(KVWriter, SeqWriter):\n    def __init__(self, filename_or_file):\n        if isinstance(filename_or_file, str):\n            self.file = open(filename_or_file, \'wt\')\n            self.own_file = True\n        else:\n            assert hasattr(filename_or_file, \'read\'), \'expected file or str, got %s\'%filename_or_file\n            self.file = filename_or_file\n            self.own_file = False\n\n    def writekvs(self, kvs):\n        # Create strings for printing\n        key2str = {}\n        for (key, val) in sorted(kvs.items()):\n            if hasattr(val, \'__float__\'):\n                valstr = \'%-8.3g\' % val\n            else:\n                valstr = str(val)\n            key2str[self._truncate(key)] = self._truncate(valstr)\n\n        # Find max widths\n        if len(key2str) == 0:\n            print(\'WARNING: tried to write empty key-value dict\')\n            return\n        else:\n            keywidth = max(map(len, key2str.keys()))\n            valwidth = max(map(len, key2str.values()))\n\n        # Write out the data\n        dashes = \'-\' * (keywidth + valwidth + 7)\n        lines = [dashes]\n        for (key, val) in sorted(key2str.items(), key=lambda kv: kv[0].lower()):\n            lines.append(\'| %s%s | %s%s |\' % (\n                key,\n                \' \' * (keywidth - len(key)),\n                val,\n                \' \' * (valwidth - len(val)),\n            ))\n        lines.append(dashes)\n        self.file.write(\'\\n\'.join(lines) + \'\\n\')\n\n        # Flush the output to the file\n        self.file.flush()\n\n    def _truncate(self, s):\n        maxlen = 30\n        return s[:maxlen-3] + \'...\' if len(s) > maxlen else s\n\n    def writeseq(self, seq):\n        seq = list(seq)\n        for (i, elem) in enumerate(seq):\n            self.file.write(elem)\n            if i < len(seq) - 1: # add space unless this is the last one\n                self.file.write(\' \')\n        self.file.write(\'\\n\')\n        self.file.flush()\n\n    def close(self):\n        if self.own_file:\n            self.file.close()\n\nclass JSONOutputFormat(KVWriter):\n    def __init__(self, filename):\n        self.file = open(filename, \'wt\')\n\n    def writekvs(self, kvs):\n        for k, v in sorted(kvs.items()):\n            if hasattr(v, \'dtype\'):\n                kvs[k] = float(v)\n        self.file.write(json.dumps(kvs) + \'\\n\')\n        self.file.flush()\n\n    def close(self):\n        self.file.close()\n\nclass CSVOutputFormat(KVWriter):\n    def __init__(self, filename):\n        self.file = open(filename, \'w+t\')\n        self.keys = []\n        self.sep = \',\'\n\n    def writekvs(self, kvs):\n        # Add our current row to the history\n        extra_keys = list(kvs.keys() - self.keys)\n        extra_keys.sort()\n        if extra_keys:\n            self.keys.extend(extra_keys)\n            self.file.seek(0)\n            lines = self.file.readlines()\n            self.file.seek(0)\n            for (i, k) in enumerate(self.keys):\n                if i > 0:\n                    self.file.write(\',\')\n                self.file.write(k)\n            self.file.write(\'\\n\')\n            for line in lines[1:]:\n                self.file.write(line[:-1])\n                self.file.write(self.sep * len(extra_keys))\n                self.file.write(\'\\n\')\n        for (i, k) in enumerate(self.keys):\n            if i > 0:\n                self.file.write(\',\')\n            v = kvs.get(k)\n            if v is not None:\n                self.file.write(str(v))\n        self.file.write(\'\\n\')\n        self.file.flush()\n\n    def close(self):\n        self.file.close()\n\n\nclass TensorBoardOutputFormat(KVWriter):\n    """"""\n    Dumps key/value pairs into TensorBoard\'s numeric format.\n    """"""\n    def __init__(self, dir):\n        os.makedirs(dir, exist_ok=True)\n        self.dir = dir\n        self.step = 1\n        prefix = \'events\'\n        path = osp.join(osp.abspath(dir), prefix)\n        import tensorflow as tf\n        from tensorflow.python import pywrap_tensorflow\n        from tensorflow.core.util import event_pb2\n        from tensorflow.python.util import compat\n        self.tf = tf\n        self.event_pb2 = event_pb2\n        self.pywrap_tensorflow = pywrap_tensorflow\n        self.writer = pywrap_tensorflow.EventsWriter(compat.as_bytes(path))\n\n    def writekvs(self, kvs):\n        def summary_val(k, v):\n            kwargs = {\'tag\': k, \'simple_value\': float(v)}\n            return self.tf.Summary.Value(**kwargs)\n        summary = self.tf.Summary(value=[summary_val(k, v) for k, v in kvs.items()])\n        event = self.event_pb2.Event(wall_time=time.time(), summary=summary)\n        event.step = self.step # is there any reason why you\'d want to specify the step?\n        self.writer.WriteEvent(event)\n        self.writer.Flush()\n        self.step += 1\n\n    def close(self):\n        if self.writer:\n            self.writer.Close()\n            self.writer = None\n\ndef make_output_format(format, ev_dir, log_suffix=\'\'):\n    os.makedirs(ev_dir, exist_ok=True)\n    if format == \'stdout\':\n        return HumanOutputFormat(sys.stdout)\n    elif format == \'log\':\n        return HumanOutputFormat(osp.join(ev_dir, \'log%s.txt\' % log_suffix))\n    elif format == \'json\':\n        return JSONOutputFormat(osp.join(ev_dir, \'progress%s.json\' % log_suffix))\n    elif format == \'csv\':\n        return CSVOutputFormat(osp.join(ev_dir, \'progress%s.csv\' % log_suffix))\n    elif format == \'tensorboard\':\n        return TensorBoardOutputFormat(osp.join(ev_dir, \'tb%s\' % log_suffix))\n    else:\n        raise ValueError(\'Unknown format specified: %s\' % (format,))\n\n# ================================================================\n# API\n# ================================================================\n\ndef logkv(key, val):\n    """"""\n    Log a value of some diagnostic\n    Call this once for each diagnostic quantity, each iteration\n    If called many times, last value will be used.\n    """"""\n    get_current().logkv(key, val)\n\ndef logkv_mean(key, val):\n    """"""\n    The same as logkv(), but if called many times, values averaged.\n    """"""\n    get_current().logkv_mean(key, val)\n\ndef logkvs(d):\n    """"""\n    Log a dictionary of key-value pairs\n    """"""\n    for (k, v) in d.items():\n        logkv(k, v)\n\ndef dumpkvs():\n    """"""\n    Write all of the diagnostics from the current iteration\n    """"""\n    return get_current().dumpkvs()\n\ndef getkvs():\n    return get_current().name2val\n\n\ndef log(*args, level=INFO):\n    """"""\n    Write the sequence of args, with no separators, to the console and output files (if you\'ve configured an output file).\n    """"""\n    get_current().log(*args, level=level)\n\ndef debug(*args):\n    log(*args, level=DEBUG)\n\ndef info(*args):\n    log(*args, level=INFO)\n\ndef warn(*args):\n    log(*args, level=WARN)\n\ndef error(*args):\n    log(*args, level=ERROR)\n\n\ndef set_level(level):\n    """"""\n    Set logging threshold on current logger.\n    """"""\n    get_current().set_level(level)\n\ndef set_comm(comm):\n    get_current().set_comm(comm)\n\ndef get_dir():\n    """"""\n    Get directory that log files are being written to.\n    will be None if there is no output directory (i.e., if you didn\'t call start)\n    """"""\n    return get_current().get_dir()\n\nrecord_tabular = logkv\ndump_tabular = dumpkvs\n\n@contextmanager\ndef profile_kv(scopename):\n    logkey = \'wait_\' + scopename\n    tstart = time.time()\n    try:\n        yield\n    finally:\n        get_current().name2val[logkey] += time.time() - tstart\n\ndef profile(n):\n    """"""\n    Usage:\n    @profile(""my_func"")\n    def my_func(): code\n    """"""\n    def decorator_with_name(func):\n        def func_wrapper(*args, **kwargs):\n            with profile_kv(n):\n                return func(*args, **kwargs)\n        return func_wrapper\n    return decorator_with_name\n\n\n# ================================================================\n# Backend\n# ================================================================\n\ndef get_current():\n    if Logger.CURRENT is None:\n        _configure_default_logger()\n\n    return Logger.CURRENT\n\n\nclass Logger(object):\n    DEFAULT = None  # A logger with no output files. (See right below class definition)\n                    # So that you can still log to the terminal without setting up any output files\n    CURRENT = None  # Current logger being used by the free functions above\n\n    def __init__(self, dir, output_formats, comm=None):\n        self.name2val = defaultdict(float)  # values this iteration\n        self.name2cnt = defaultdict(int)\n        self.level = INFO\n        self.dir = dir\n        self.output_formats = output_formats\n        self.comm = comm\n\n    # Logging API, forwarded\n    # ----------------------------------------\n    def logkv(self, key, val):\n        self.name2val[key] = val\n\n    def logkv_mean(self, key, val):\n        oldval, cnt = self.name2val[key], self.name2cnt[key]\n        self.name2val[key] = oldval*cnt/(cnt+1) + val/(cnt+1)\n        self.name2cnt[key] = cnt + 1\n\n    def dumpkvs(self):\n        if self.comm is None:\n            d = self.name2val\n        else:\n            from baselines.common import mpi_util\n            d = mpi_util.mpi_weighted_mean(self.comm,\n                {name : (val, self.name2cnt.get(name, 1))\n                    for (name, val) in self.name2val.items()})\n            if self.comm.rank != 0:\n                d[\'dummy\'] = 1 # so we don\'t get a warning about empty dict\n        out = d.copy() # Return the dict for unit testing purposes\n        for fmt in self.output_formats:\n            if isinstance(fmt, KVWriter):\n                fmt.writekvs(d)\n        self.name2val.clear()\n        self.name2cnt.clear()\n        return out\n\n    def log(self, *args, level=INFO):\n        if self.level <= level:\n            self._do_log(args)\n\n    # Configuration\n    # ----------------------------------------\n    def set_level(self, level):\n        self.level = level\n\n    def set_comm(self, comm):\n        self.comm = comm\n\n    def get_dir(self):\n        return self.dir\n\n    def close(self):\n        for fmt in self.output_formats:\n            fmt.close()\n\n    # Misc\n    # ----------------------------------------\n    def _do_log(self, args):\n        for fmt in self.output_formats:\n            if isinstance(fmt, SeqWriter):\n                fmt.writeseq(map(str, args))\n\ndef get_rank_without_mpi_import():\n    # check environment variables here instead of importing mpi4py\n    # to avoid calling MPI_Init() when this module is imported\n    for varname in [\'PMI_RANK\', \'OMPI_COMM_WORLD_RANK\']:\n        if varname in os.environ:\n            return int(os.environ[varname])\n    return 0\n\n\ndef configure(dir=None, format_strs=None, comm=None, log_suffix=\'\'):\n    """"""\n    If comm is provided, average all numerical stats across that comm\n    """"""\n    if dir is None:\n        dir = os.getenv(\'OPENAI_LOGDIR\')\n    if dir is None:\n        dir = osp.join(tempfile.gettempdir(),\n            datetime.datetime.now().strftime(""openai-%Y-%m-%d-%H-%M-%S-%f""))\n    assert isinstance(dir, str)\n    dir = os.path.expanduser(dir)\n    os.makedirs(os.path.expanduser(dir), exist_ok=True)\n\n    rank = get_rank_without_mpi_import()\n    if rank > 0:\n        log_suffix = log_suffix + ""-rank%03i"" % rank\n\n    if format_strs is None:\n        if rank == 0:\n            format_strs = os.getenv(\'OPENAI_LOG_FORMAT\', \'stdout,log,csv\').split(\',\')\n        else:\n            format_strs = os.getenv(\'OPENAI_LOG_FORMAT_MPI\', \'log\').split(\',\')\n    format_strs = filter(None, format_strs)\n    output_formats = [make_output_format(f, dir, log_suffix) for f in format_strs]\n\n    Logger.CURRENT = Logger(dir=dir, output_formats=output_formats, comm=comm)\n    if output_formats:\n        log(\'Logging to %s\'%dir)\n\ndef _configure_default_logger():\n    configure()\n    Logger.DEFAULT = Logger.CURRENT\n\ndef reset():\n    if Logger.CURRENT is not Logger.DEFAULT:\n        Logger.CURRENT.close()\n        Logger.CURRENT = Logger.DEFAULT\n        log(\'Reset logger\')\n\n@contextmanager\ndef scoped_configure(dir=None, format_strs=None, comm=None):\n    prevlogger = Logger.CURRENT\n    configure(dir=dir, format_strs=format_strs, comm=comm)\n    try:\n        yield\n    finally:\n        Logger.CURRENT.close()\n        Logger.CURRENT = prevlogger\n\n# ================================================================\n\ndef _demo():\n    info(""hi"")\n    debug(""shouldn\'t appear"")\n    set_level(DEBUG)\n    debug(""should appear"")\n    dir = ""/tmp/testlogging""\n    if os.path.exists(dir):\n        shutil.rmtree(dir)\n    configure(dir=dir)\n    logkv(""a"", 3)\n    logkv(""b"", 2.5)\n    dumpkvs()\n    logkv(""b"", -2.5)\n    logkv(""a"", 5.5)\n    dumpkvs()\n    info(""^^^ should see a = 5.5"")\n    logkv_mean(""b"", -22.5)\n    logkv_mean(""b"", -44.4)\n    logkv(""a"", 5.5)\n    dumpkvs()\n    info(""^^^ should see b = -33.3"")\n\n    logkv(""b"", -2.5)\n    dumpkvs()\n\n    logkv(""a"", ""longasslongasslongasslongasslongasslongassvalue"")\n    dumpkvs()\n\n\n# ================================================================\n# Readers\n# ================================================================\n\ndef read_json(fname):\n    import pandas\n    ds = []\n    with open(fname, \'rt\') as fh:\n        for line in fh:\n            ds.append(json.loads(line))\n    return pandas.DataFrame(ds)\n\ndef read_csv(fname):\n    import pandas\n    return pandas.read_csv(fname, index_col=None, comment=\'#\')\n\ndef read_tb(path):\n    """"""\n    path : a tensorboard file OR a directory, where we will find all TB files\n           of the form events.*\n    """"""\n    import pandas\n    import numpy as np\n    from glob import glob\n    import tensorflow as tf\n    if osp.isdir(path):\n        fnames = glob(osp.join(path, ""events.*""))\n    elif osp.basename(path).startswith(""events.""):\n        fnames = [path]\n    else:\n        raise NotImplementedError(""Expected tensorboard file or directory containing them. Got %s""%path)\n    tag2pairs = defaultdict(list)\n    maxstep = 0\n    for fname in fnames:\n        for summary in tf.train.summary_iterator(fname):\n            if summary.step > 0:\n                for v in summary.summary.value:\n                    pair = (summary.step, v.simple_value)\n                    tag2pairs[v.tag].append(pair)\n                maxstep = max(summary.step, maxstep)\n    data = np.empty((maxstep, len(tag2pairs)))\n    data[:] = np.nan\n    tags = sorted(tag2pairs.keys())\n    for (colidx,tag) in enumerate(tags):\n        pairs = tag2pairs[tag]\n        for (step, value) in pairs:\n            data[step-1, colidx] = value\n    return pandas.DataFrame(data, columns=tags)\n\nif __name__ == ""__main__"":\n    _demo()\n'"
rl_utils/logger/plot.py,0,"b'import numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom rl_utils.bench import load_results\n\nsns.set(style=""dark"")\nsns.set_context(""poster"", font_scale=2, rc={""lines.linewidth"": 2})\nsns.set(rc={""figure.figsize"": (15, 8)})\ncolors = sns.color_palette(palette=\'muted\')\n\n\nX_TIMESTEPS = \'timesteps\'\nX_EPISODES = \'episodes\'\nX_WALLTIME = \'walltime_hrs\'\nPOSSIBLE_X_AXES = [X_TIMESTEPS, X_EPISODES, X_WALLTIME]\nEPISODES_WINDOW = 150\nCOLORS = [\'blue\', \'green\', \'red\', \'cyan\', \'magenta\', \'yellow\', \'black\', \'purple\', \'pink\',\n        \'brown\', \'orange\', \'teal\', \'coral\', \'lightblue\', \'lime\', \'lavender\', \'turquoise\',\n        \'darkgreen\', \'tan\', \'salmon\', \'gold\', \'lightpurple\', \'darkred\', \'darkblue\']\n\ndef rolling_window(a, window):\n    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n    strides = a.strides + (a.strides[-1],)\n    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n\ndef window_func(x, y, window, func):\n    yw = rolling_window(y, window)\n    yw_func = func(yw, axis=-1)\n    return x[window-1:], yw_func\n\ndef ts2xy(ts, xaxis):\n    if xaxis == X_TIMESTEPS:\n        x = np.cumsum(ts.l.values)\n        y = ts.r.values\n    elif xaxis == X_EPISODES:\n        x = np.arange(len(ts))\n        y = ts.r.values\n    elif xaxis == X_WALLTIME:\n        x = ts.t.values / 3600.\n        y = ts.r.values\n    else:\n        raise NotImplementedError\n    return x, y\n\ndef plot_curves(xy_list, xaxis, title, plt_order, beta=False):\n    maxx = max(xy[0][-1] for xy in xy_list)\n    minx = 0\n    if beta == \'dqn\':\n        label = [\'DQN\']\n    elif beta == \'ddqn\':\n        label = [\'Double-DQN\']\n    elif beta == \'dueling\':\n        label = [\'Dueling-DQN\']\n    psub = plt.subplot(plt_order)\n    plt.tight_layout()\n    plt.ticklabel_format(style=\'sci\', axis=\'x\', scilimits=(0,0))\n    for (i, (x, y)) in enumerate(xy_list):\n        #plt.scatter(x, y, s=2)\n        x, y_mean = window_func(x, y, EPISODES_WINDOW, np.mean) #So returns average of last EPISODE_WINDOW episodes\n        psub.plot(x, y_mean, label=label[i])\n    psub.set_xlim([minx, maxx])\n    psub.set_title(title)\n    psub.legend(loc=\'best\')\n    psub.set_xlabel(xaxis)\n    psub.set_ylabel(""rewards"")\n\ndef plot_results(dirs, num_timesteps, xaxis, task_name, plt_order, beta=False):\n    tslist = []\n    for dir in dirs:\n        ts = load_results(dir)\n        ts = ts[ts.l.cumsum() <= num_timesteps]\n        tslist.append(ts)\n    xy_list = [ts2xy(ts, xaxis) for ts in tslist]\n    plot_curves(xy_list, xaxis, task_name, plt_order, beta)\n\ndef main():\n    import argparse\n    import os\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'--dirs\', help=\'List of log directories\', nargs = \'*\', default=\'logs_dqn/\')\n    parser.add_argument(\'--num_timesteps\', type=int, default=int(2e7))\n    parser.add_argument(\'--xaxis\', help = \'Varible on X-axis\', default = X_TIMESTEPS)\n    parser.add_argument(\'--task_name\', help = \'Title of plot\', default = \'BreakoutNoFrameskip-v4\')\n    args = parser.parse_args()\n    env_name = [\'BankHeistNoFrameskip-v4\', \'BreakoutNoFrameskip-v4\', \'KangarooNoFrameskip-v4\', \\\n                \'PongNoFrameskip-v4\', \'SeaquestNoFrameskip-v4\', \'SpaceInvadersNoFrameskip-v4\']\n    dirs = [os.path.abspath(args.dirs + name) for name in env_name] \n    for idx in range(len(dirs)):\n        plot_results([dirs[idx]], args.num_timesteps, args.xaxis, env_name[idx], 231+idx, beta=\'dqn\')\n    double_dirs = [os.path.abspath(\'logs_ddqn/\' + name) for name in env_name]\n    for idx in range(len(dirs)):\n        plot_results([double_dirs[idx]], args.num_timesteps, args.xaxis, env_name[idx], 231+idx, beta=\'ddqn\')\n    dueling_dirs = [os.path.abspath(\'logs/\' + name) for name in env_name] \n    for idx in range(len(dirs)):\n        plot_results([dueling_dirs[idx]], args.num_timesteps, args.xaxis, env_name[idx], 231+idx, beta=\'dueling\')\n    plt.savefig(""dueling.png"")\n\nif __name__ == \'__main__\':\n    main()\n\n'"
rl_utils/mpi_utils/__init__.py,0,b''
rl_utils/mpi_utils/normalizer.py,0,"b""import threading\nimport numpy as np\nfrom mpi4py import MPI\n\nclass normalizer:\n    def __init__(self, size, eps=1e-2, default_clip_range=np.inf):\n        self.size = size\n        self.eps = eps\n        self.default_clip_range = default_clip_range\n        # some local information\n        self.local_sum = np.zeros(self.size, np.float32)\n        self.local_sumsq = np.zeros(self.size, np.float32)\n        self.local_count = np.zeros(1, np.float32)\n        # get the total sum sumsq and sum count\n        self.total_sum = np.zeros(self.size, np.float32)\n        self.total_sumsq = np.zeros(self.size, np.float32)\n        self.total_count = np.ones(1, np.float32)\n        # get the mean and std\n        self.mean = np.zeros(self.size, np.float32)\n        self.std = np.ones(self.size, np.float32)\n        # thread locker\n        self.lock = threading.Lock()\n    \n    # update the parameters of the normalizer\n    def update(self, v):\n        v = v.reshape(-1, self.size)\n        # do the computing\n        with self.lock:\n            self.local_sum += v.sum(axis=0)\n            self.local_sumsq += (np.square(v)).sum(axis=0)\n            self.local_count[0] += v.shape[0]\n\n    # sync the parameters across the cpus\n    def sync(self, local_sum, local_sumsq, local_count):\n        local_sum[...] = self._mpi_average(local_sum)\n        local_sumsq[...] = self._mpi_average(local_sumsq)\n        local_count[...] = self._mpi_average(local_count)\n        return local_sum, local_sumsq, local_count\n\n    def recompute_stats(self):\n        with self.lock:\n            local_count = self.local_count.copy()\n            local_sum = self.local_sum.copy()\n            local_sumsq = self.local_sumsq.copy()\n            # reset\n            self.local_count[...] = 0\n            self.local_sum[...] = 0\n            self.local_sumsq[...] = 0\n        # synrc the stats\n        sync_sum, sync_sumsq, sync_count = self.sync(local_sum, local_sumsq, local_count)\n        # update the total stuff\n        self.total_sum += sync_sum\n        self.total_sumsq += sync_sumsq\n        self.total_count += sync_count\n        # calculate the new mean and std\n        self.mean = self.total_sum / self.total_count\n        self.std = np.sqrt(np.maximum(np.square(self.eps), (self.total_sumsq / self.total_count) - np.square(self.total_sum / self.total_count)))\n    \n    # average across the cpu's data\n    def _mpi_average(self, x):\n        buf = np.zeros_like(x)\n        MPI.COMM_WORLD.Allreduce(x, buf, op=MPI.SUM)\n        buf /= MPI.COMM_WORLD.Get_size()\n        return buf\n\n    # normalize the observation\n    def normalize(self, v, clip_range=None):\n        if clip_range is None:\n            clip_range = self.default_clip_range\n        return np.clip((v - self.mean) / (self.std), -clip_range, clip_range)\n"""
rl_utils/mpi_utils/utils.py,1,"b'from mpi4py import MPI\nimport numpy as np\nimport torch\n\n# sync_networks across the different cores\ndef sync_networks(network):\n    """"""\n    netowrk is the network you want to sync\n\n    """"""\n    comm = MPI.COMM_WORLD\n    flat_params = _get_flat_params_or_grads(network, mode=\'params\')\n    comm.Bcast(flat_params, root=0)\n    # set the flat params back to the network\n    _set_flat_params_or_grads(network, flat_params, mode=\'params\')\n\ndef sync_grads(network):\n    flat_grads = _get_flat_params_or_grads(network, mode=\'grads\')\n    comm = MPI.COMM_WORLD\n    global_grads = np.zeros_like(flat_grads)\n    comm.Allreduce(flat_grads, global_grads, op=MPI.SUM)\n    _set_flat_params_or_grads(network, global_grads, mode=\'grads\')\n\n# get the flat grads or params\ndef _get_flat_params_or_grads(network, mode=\'params\'):\n    """"""\n    include two kinds: grads and params\n\n    """"""\n    attr = \'data\' if mode == \'params\' else \'grad\'\n    return np.concatenate([getattr(param, attr).cpu().numpy().flatten() for param in network.parameters()])\n\ndef _set_flat_params_or_grads(network, flat_params, mode=\'params\'):\n    """"""\n    include two kinds: grads and params\n\n    """"""\n    attr = \'data\' if mode == \'params\' else \'grad\'\n    # the pointer\n    pointer = 0\n    for param in network.parameters():\n        getattr(param, attr).copy_(torch.tensor(flat_params[pointer:pointer + param.data.numel()]).view_as(param.data))\n        pointer += param.data.numel()\n'"
rl_utils/running_filter/__init__.py,0,b''
rl_utils/running_filter/running_filter.py,0,"b'from collections import deque\nimport numpy as np\n\n# this is from the https://github.com/ikostrikov/pytorch-trpo/blob/master/running_state.py\n\n# from https://github.com/joschu/modular_rl\n# http://www.johndcook.com/blog/standard_deviation/\nclass RunningStat(object):\n    def __init__(self, shape):\n        self._n = 0\n        self._M = np.zeros(shape)\n        self._S = np.zeros(shape)\n\n    def push(self, x):\n        x = np.asarray(x)\n        assert x.shape == self._M.shape\n        self._n += 1\n        if self._n == 1:\n            self._M[...] = x\n        else:\n            oldM = self._M.copy()\n            self._M[...] = oldM + (x - oldM) / self._n\n            self._S[...] = self._S + (x - oldM) * (x - self._M)\n\n    @property\n    def n(self):\n        return self._n\n\n    @property\n    def mean(self):\n        return self._M\n\n    @property\n    def var(self):\n        return self._S / (self._n - 1) if self._n > 1 else np.square(self._M)\n\n    @property\n    def std(self):\n        return np.sqrt(self.var)\n\n    @property\n    def shape(self):\n        return self._M.shape\n\n\nclass ZFilter:\n    """"""\n    y = (x-mean)/std\n    using running estimates of mean,std\n    """"""\n\n    def __init__(self, shape, demean=True, destd=True, clip=10.0):\n        self.demean = demean\n        self.destd = destd\n        self.clip = clip\n\n        self.rs = RunningStat(shape)\n\n    def __call__(self, x, update=True):\n        if update: self.rs.push(x)\n        if self.demean:\n            x = x - self.rs.mean\n        if self.destd:\n            x = x / (self.rs.std + 1e-8)\n        if self.clip:\n            x = np.clip(x, -self.clip, self.clip)\n        return x\n\n    def output_shape(self, input_space):\n        return input_space.shape\n'"
rl_utils/seeds/seeds.py,2,"b'import numpy as np\nimport random\nimport torch\n\n# set random seeds for the pytorch, numpy and random\ndef set_seeds(args, rank=0):\n    # set seeds for the numpy\n    np.random.seed(args.seed + rank)\n    # set seeds for the random.random\n    random.seed(args.seed + rank)\n    # set seeds for the pytorch\n    torch.manual_seed(args.seed + rank)\n    if args.cuda:\n        torch.cuda.manual_seed(args.seed + rank)\n'"
