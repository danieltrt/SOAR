file_path,api_count,code
cfg.py,0,"b""# -*- coding: utf-8 -*-\n'''\n@Time          : 2020/05/06 21:05\n@Author        : Tianxiaomo\n@File          : Cfg.py\n@Noice         :\n@Modificattion :\n    @Author    :\n    @Time      :\n    @Detail    :\n\n'''\nfrom easydict import EasyDict\n\nCfg = EasyDict()\nCfg.batch = 64\nCfg.subdivisions = 16\nCfg.width = 608\nCfg.height = 608\nCfg.channels = 3\nCfg.momentum = 0.949\nCfg.decay = 0.0005\nCfg.angle = 0\nCfg.saturation = 1.5\nCfg.exposure = 1.5\nCfg.hue = .1\n\nCfg.learning_rate = 0.00261\nCfg.burn_in = 1000\nCfg.max_batches = 500500\nCfg.steps = [400000, 450000]\nCfg.policy = Cfg.steps\nCfg.scales = .1, .1\n\nCfg.cutmix = 0\nCfg.mosaic = 1\n\nCfg.letter_box = 0\nCfg.jitter = 0.2\nCfg.classes = 80\nCfg.track = 0\nCfg.w = Cfg.width\nCfg.h = Cfg.height\nCfg.flip = 1\nCfg.blur = 0\nCfg.gaussian = 0\nCfg.boxes = 60  # box num\nCfg.TRAIN_EPOCHS = 300\nCfg.train_label = 'data/train.txt'\nCfg.val_label = 'data/val.txt'\nCfg.TRAIN_OPTIMIZER = 'adam'\n'''\nimage_path1 x1,y1,x2,y2,id x1,y1,x2,y2,id x1,y1,x2,y2,id ...\nimage_path2 x1,y1,x2,y2,id x1,y1,x2,y2,id x1,y1,x2,y2,id ...\n...\n'''\n\nif Cfg.mosaic and Cfg.cutmix:\n    Cfg.mixup = 4\nelif Cfg.cutmix:\n    Cfg.mixup = 2\nelif Cfg.mosaic:\n    Cfg.mixup = 3\n\nCfg.checkpoints = 'checkpoints'\nCfg.TRAIN_TENSORBOARD_DIR = 'log'"""
dataset.py,1,"b'# -*- coding: utf-8 -*-\n\'\'\'\n@Time          : 2020/05/06 21:09\n@Author        : Tianxiaomo\n@File          : dataset.py\n@Noice         :\n@Modificattion :\n    @Author    :\n    @Time      :\n    @Detail    :\n\n\'\'\'\nfrom torch.utils.data.dataset import Dataset\n\nimport random\nimport cv2\nimport sys\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n\n\ndef rand_uniform_strong(min, max):\n    if min > max:\n        swap = min\n        min = max\n        max = swap\n    return random.random() * (max - min) + min\n\n\ndef rand_scale(s):\n    scale = rand_uniform_strong(1, s)\n    if random.randint(0, 1) % 2:\n        return scale\n    return 1. / scale\n\n\ndef rand_precalc_random(min, max, random_part):\n    if max < min:\n        swap = min\n        min = max\n        max = swap\n    return (random_part * (max - min)) + min\n\n\ndef fill_truth_detection(bboxes, num_boxes, classes, flip, dx, dy, sx, sy, net_w, net_h):\n    if bboxes.shape[0] == 0:\n        return bboxes, 10000\n    np.random.shuffle(bboxes)\n    bboxes[:, 0] -= dx\n    bboxes[:, 2] -= dx\n    bboxes[:, 1] -= dy\n    bboxes[:, 3] -= dy\n\n    bboxes[:, 0] = np.clip(bboxes[:, 0], 0, sx)\n    bboxes[:, 2] = np.clip(bboxes[:, 2], 0, sx)\n\n    bboxes[:, 1] = np.clip(bboxes[:, 1], 0, sy)\n    bboxes[:, 3] = np.clip(bboxes[:, 3], 0, sy)\n\n    out_box = list(np.where(((bboxes[:, 1] == sy) & (bboxes[:, 3] == sy)) |\n                            ((bboxes[:, 0] == sx) & (bboxes[:, 2] == sx)) |\n                            ((bboxes[:, 1] == 0) & (bboxes[:, 3] == 0)) |\n                            ((bboxes[:, 0] == 0) & (bboxes[:, 2] == 0)))[0])\n    list_box = list(range(bboxes.shape[0]))\n    for i in out_box:\n        list_box.remove(i)\n    bboxes = bboxes[list_box]\n\n    if bboxes.shape[0] == 0:\n        return bboxes, 10000\n\n    bboxes = bboxes[np.where((bboxes[:, 4] < classes) & (bboxes[:, 4] >= 0))[0]]\n\n    if bboxes.shape[0] > num_boxes:\n        bboxes = bboxes[:num_boxes]\n\n    min_w_h = np.array([bboxes[:, 2] - bboxes[:, 0], bboxes[:, 3] - bboxes[:, 1]]).min()\n\n    bboxes[:, 0] *= (net_w / sx)\n    bboxes[:, 2] *= (net_w / sx)\n    bboxes[:, 1] *= (net_h / sy)\n    bboxes[:, 3] *= (net_h / sy)\n\n    if flip:\n        temp = net_w - bboxes[:, 0]\n        bboxes[:, 0] = net_w - bboxes[:, 2]\n        bboxes[:, 2] = temp\n\n    return bboxes, min_w_h\n\n\ndef rect_intersection(a, b):\n    minx = max(a[0], b[0])\n    miny = max(a[1], b[1])\n\n    maxx = min(a[2], b[2])\n    maxy = min(a[3], b[3])\n    return [minx, miny, maxx, maxy]\n\n\ndef image_data_augmentation(mat, w, h, pleft, ptop, swidth, sheight, flip, dhue, dsat, dexp, gaussian_noise, blur,\n                            truth):\n    try:\n        img = mat\n        oh, ow, _ = img.shape\n        pleft, ptop, swidth, sheight = int(pleft), int(ptop), int(swidth), int(sheight)\n        # crop\n        src_rect = [pleft, ptop, swidth + pleft, sheight + ptop]  # x1,y1,x2,y2\n        img_rect = [0, 0, ow, oh]\n        new_src_rect = rect_intersection(src_rect, img_rect)  # \xe4\xba\xa4\xe9\x9b\x86\n\n        dst_rect = [max(0, -pleft), max(0, -ptop), max(0, -pleft) + new_src_rect[2] - new_src_rect[0],\n                    max(0, -ptop) + new_src_rect[3] - new_src_rect[1]]\n        # cv2.Mat sized\n\n        if (src_rect[0] == 0 and src_rect[1] == 0 and src_rect[2] == img.shape[0] and src_rect[3] == img.shape[1]):\n            sized = cv2.resize(img, (w, h), cv2.INTER_LINEAR)\n        else:\n            cropped = np.zeros([sheight, swidth, 3])\n            cropped[:, :, ] = np.mean(img, axis=(0, 1))\n\n            cropped[dst_rect[1]:dst_rect[3], dst_rect[0]:dst_rect[2]] = \\\n                img[new_src_rect[1]:new_src_rect[3], new_src_rect[0]:new_src_rect[2]]\n\n            # resize\n            sized = cv2.resize(cropped, (w, h), cv2.INTER_LINEAR)\n\n        # flip\n        if flip:\n            # cv2.Mat cropped\n            sized = cv2.flip(sized, 1)  # 0 - x-axis, 1 - y-axis, -1 - both axes (x & y)\n\n        # HSV augmentation\n        # cv2.COLOR_BGR2HSV, cv2.COLOR_RGB2HSV, cv2.COLOR_HSV2BGR, cv2.COLOR_HSV2RGB\n        if dsat != 1 or dexp != 1 or dhue != 0:\n            if img.shape[2] >= 3:\n                hsv_src = cv2.cvtColor(sized.astype(np.float32), cv2.COLOR_RGB2HSV)  # RGB to HSV\n                hsv = cv2.split(hsv_src)\n                hsv[1] *= dsat\n                hsv[2] *= dexp\n                hsv[0] += 179 * dhue\n                hsv_src = cv2.merge(hsv)\n                sized = np.clip(cv2.cvtColor(hsv_src, cv2.COLOR_HSV2RGB), 0, 255)  # HSV to RGB (the same as previous)\n            else:\n                sized *= dexp\n\n        if blur:\n            if blur == 1:\n                dst = cv2.GaussianBlur(sized, (17, 17), 0)\n                # cv2.bilateralFilter(sized, dst, 17, 75, 75)\n            else:\n                ksize = (blur / 2) * 2 + 1\n                dst = cv2.GaussianBlur(sized, (ksize, ksize), 0)\n\n            if blur == 1:\n                img_rect = [0, 0, sized.cols, sized.rows]\n                for b in truth:\n                    left = (b.x - b.w / 2.) * sized.shape[1]\n                    width = b.w * sized.shape[1]\n                    top = (b.y - b.h / 2.) * sized.shape[0]\n                    height = b.h * sized.shape[0]\n                    roi(left, top, width, height)\n                    roi = roi & img_rect\n                    dst[roi[0]:roi[0] + roi[2], roi[1]:roi[1] + roi[3]] = sized[roi[0]:roi[0] + roi[2],\n                                                                          roi[1]:roi[1] + roi[3]]\n\n            sized = dst\n\n        if gaussian_noise:\n            noise = np.array(sized.shape)\n            gaussian_noise = min(gaussian_noise, 127)\n            gaussian_noise = max(gaussian_noise, 0)\n            cv2.randn(noise, 0, gaussian_noise)  # mean and variance\n            sized = sized + noise\n    except:\n        print(""OpenCV can\'t augment image: "" + str(w) + "" x "" + str(h))\n        sized = mat\n\n    return sized\n\n\ndef filter_truth(bboxes, dx, dy, sx, sy, xd, yd):\n    bboxes[:, 0] -= dx\n    bboxes[:, 2] -= dx\n    bboxes[:, 1] -= dy\n    bboxes[:, 3] -= dy\n\n    bboxes[:, 0] = np.clip(bboxes[:, 0], 0, sx)\n    bboxes[:, 2] = np.clip(bboxes[:, 2], 0, sx)\n\n    bboxes[:, 1] = np.clip(bboxes[:, 1], 0, sy)\n    bboxes[:, 3] = np.clip(bboxes[:, 3], 0, sy)\n\n    out_box = list(np.where(((bboxes[:, 1] == sy) & (bboxes[:, 3] == sy)) |\n                            ((bboxes[:, 0] == sx) & (bboxes[:, 2] == sx)) |\n                            ((bboxes[:, 1] == 0) & (bboxes[:, 3] == 0)) |\n                            ((bboxes[:, 0] == 0) & (bboxes[:, 2] == 0)))[0])\n    list_box = list(range(bboxes.shape[0]))\n    for i in out_box:\n        list_box.remove(i)\n    bboxes = bboxes[list_box]\n\n    bboxes[:, 0] += xd\n    bboxes[:, 2] += xd\n    bboxes[:, 1] += yd\n    bboxes[:, 3] += yd\n\n    return bboxes\n\n\ndef blend_truth_mosaic(out_img, img, bboxes, w, h, cut_x, cut_y, i_mixup,\n                       left_shift, right_shift, top_shift, bot_shift):\n    left_shift = min(left_shift, w - cut_x)\n    top_shift = min(top_shift, h - cut_y)\n    right_shift = min(right_shift, cut_x)\n    bot_shift = min(bot_shift, cut_y)\n\n    if i_mixup == 0:\n        bboxes = filter_truth(bboxes, left_shift, top_shift, cut_x, cut_y, 0, 0)\n        out_img[:cut_y, :cut_x] = img[top_shift:top_shift + cut_y, left_shift:left_shift + cut_x]\n    if i_mixup == 1:\n        bboxes = filter_truth(bboxes, cut_x - right_shift, top_shift, w - cut_x, cut_y, cut_x, 0)\n        out_img[:cut_y, cut_x:] = img[top_shift:top_shift + cut_y, cut_x - right_shift:w - right_shift]\n    if i_mixup == 2:\n        bboxes = filter_truth(bboxes, left_shift, cut_y - bot_shift, cut_x, h - cut_y, 0, cut_y)\n        out_img[cut_y:, :cut_x] = img[cut_y - bot_shift:h - bot_shift, left_shift:left_shift + cut_x]\n    if i_mixup == 3:\n        bboxes = filter_truth(bboxes, cut_x - right_shift, cut_y - bot_shift, w - cut_x, h - cut_y, cut_x, cut_y)\n        out_img[cut_y:, cut_x:] = img[cut_y - bot_shift:h - bot_shift, cut_x - right_shift:w - right_shift]\n\n    return out_img, bboxes\n\n\ndef draw_box(img, bboxes):\n    for b in bboxes:\n        img = cv2.rectangle(img, (b[0], b[1]), (b[2], b[3]), (0, 255, 0), 2)\n    return img\n\n\nclass Yolo_dataset(Dataset):\n    def __init__(self, lable_path, cfg):\n        super(Yolo_dataset, self).__init__()\n        if cfg.mixup == 2:\n            print(""cutmix=1 - isn\'t supported for Detector"")\n            raise\n        elif cfg.mixup == 2 and cfg.letter_box:\n            print(""Combination: letter_box=1 & mosaic=1 - isn\'t supported, use only 1 of these parameters"")\n            raise\n\n        self.cfg = cfg\n\n        truth = {}\n        f = open(lable_path, \'r\', encoding=\'utf-8\')\n        for line in f.readlines():\n            data = line.split("" "")\n            truth[data[0]] = []\n            for i in data[1:]:\n                truth[data[0]].append([int(j) for j in i.split(\',\')])\n\n        self.truth = truth\n\n    def __len__(self):\n        return len(self.truth.keys())\n\n    def __getitem__(self, index):\n        img_path = list(self.truth.keys())[index]\n        bboxes = np.array(self.truth.get(img_path), dtype=np.float)\n        img_path = os.path.join(self.cfg.dataset_dir, img_path)\n        use_mixup = self.cfg.mixup\n        if random.randint(0, 1):\n            use_mixup = 0\n\n        if use_mixup == 3:\n            min_offset = 0.2\n            cut_x = random.randint(int(self.cfg.w * min_offset), int(self.cfg.w * (1 - min_offset)))\n            cut_y = random.randint(int(self.cfg.h * min_offset), int(self.cfg.h * (1 - min_offset)))\n\n        r1, r2, r3, r4, r_scale = 0, 0, 0, 0, 0\n        dhue, dsat, dexp, flip, blur = 0, 0, 0, 0, 0\n        gaussian_noise = 0\n\n        out_img = np.zeros([self.cfg.h, self.cfg.w, 3])\n        out_bboxes = []\n\n        for i in range(use_mixup + 1):\n            if i != 0:\n                img_path = random.choice(list(self.truth.keys()))\n                bboxes = np.array(self.truth.get(img_path), dtype=np.float)\n                img_path = os.path.join(self.cfg.dataset_dir, img_path)\n            img = cv2.imread(img_path)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            if img is None:\n                continue\n            oh, ow, oc = img.shape\n            dh, dw, dc = np.array(np.array([oh, ow, oc]) * self.cfg.jitter, dtype=np.int)\n\n            dhue = rand_uniform_strong(-self.cfg.hue, self.cfg.hue)\n            dsat = rand_scale(self.cfg.saturation)\n            dexp = rand_scale(self.cfg.exposure)\n\n            pleft = random.randint(-dw, dw)\n            pright = random.randint(-dw, dw)\n            ptop = random.randint(-dh, dh)\n            pbot = random.randint(-dh, dh)\n\n            flip = random.randint(0, 1) if self.cfg.flip else 0\n\n            if (self.cfg.blur):\n                tmp_blur = random.randint(0, 2)  # 0 - disable, 1 - blur background, 2 - blur the whole image\n                if tmp_blur == 0:\n                    blur = 0\n                elif tmp_blur == 1:\n                    blur = 1\n                else:\n                    blur = self.cfg.blur\n\n            if self.cfg.gaussian and random.randint(0, 1):\n                gaussian_noise = self.cfg.gaussian\n            else:\n                gaussian_noise = 0\n\n            if self.cfg.letter_box:\n                img_ar = ow / oh\n                net_ar = self.cfg.w / self.cfg.h\n                result_ar = img_ar / net_ar\n                # print("" ow = %d, oh = %d, w = %d, h = %d, img_ar = %f, net_ar = %f, result_ar = %f \\n"", ow, oh, w, h, img_ar, net_ar, result_ar);\n                if result_ar > 1:  # sheight - should be increased\n                    oh_tmp = ow / net_ar\n                    delta_h = (oh_tmp - oh) / 2\n                    ptop = ptop - delta_h\n                    pbot = pbot - delta_h\n                    # print("" result_ar = %f, oh_tmp = %f, delta_h = %d, ptop = %f, pbot = %f \\n"", result_ar, oh_tmp, delta_h, ptop, pbot);\n                else:  # swidth - should be increased\n                    ow_tmp = oh * net_ar\n                    delta_w = (ow_tmp - ow) / 2\n                    pleft = pleft - delta_w\n                    pright = pright - delta_w\n                    # printf("" result_ar = %f, ow_tmp = %f, delta_w = %d, pleft = %f, pright = %f \\n"", result_ar, ow_tmp, delta_w, pleft, pright);\n\n            swidth = ow - pleft - pright\n            sheight = oh - ptop - pbot\n\n            truth, min_w_h = fill_truth_detection(bboxes, self.cfg.boxes, self.cfg.classes, flip, pleft, ptop, swidth,\n                                                  sheight, self.cfg.w, self.cfg.h)\n            if (min_w_h / 8) < blur and blur > 1:  # disable blur if one of the objects is too small\n                blur = min_w_h / 8\n\n            ai = image_data_augmentation(img, self.cfg.w, self.cfg.h, pleft, ptop, swidth, sheight, flip,\n                                         dhue, dsat, dexp, gaussian_noise, blur, truth)\n\n            if use_mixup == 0:\n                out_img = ai\n                out_bboxes = truth\n            if use_mixup == 1:\n                if i == 0:\n                    old_img = ai.copy()\n                    old_truth = truth.copy()\n                elif i == 1:\n                    out_img = cv2.addWeighted(ai, 0.5, old_img, 0.5)\n                    out_bboxes = np.concatenate([old_truth, truth], axis=0)\n            elif use_mixup == 3:\n                if flip:\n                    tmp = pleft\n                    pleft = pright\n                    pright = tmp\n\n                left_shift = int(min(cut_x, max(0, (-int(pleft) * self.cfg.w / swidth))))\n                top_shift = int(min(cut_y, max(0, (-int(ptop) * self.cfg.h / sheight))))\n\n                right_shift = int(min((self.cfg.w - cut_x), max(0, (-int(pright) * self.cfg.w / swidth))))\n                bot_shift = int(min(self.cfg.h - cut_y, max(0, (-int(pbot) * self.cfg.h / sheight))))\n\n                out_img, out_bbox = blend_truth_mosaic(out_img, ai, truth.copy(), self.cfg.w, self.cfg.h, cut_x,\n                                                       cut_y, i, left_shift, right_shift, top_shift, bot_shift)\n                out_bboxes.append(out_bbox)\n                # print(img_path)\n        if use_mixup == 3:\n            out_bboxes = np.concatenate(out_bboxes, axis=0)\n        out_bboxes1 = np.zeros([self.cfg.boxes, 5])\n        out_bboxes1[:min(out_bboxes.shape[0], self.cfg.boxes)] = out_bboxes[:min(out_bboxes.shape[0], self.cfg.boxes)]\n        return out_img, out_bboxes1\n\n\nif __name__ == ""__main__"":\n    from cfg import Cfg\n\n    random.seed(2020)\n    np.random.seed(2020)\n    Cfg.dataset_dir = \'/mnt/e/Dataset\'\n    dataset = Yolo_dataset(Cfg.train_label, Cfg)\n    for i in range(100):\n        out_img, out_bboxes = dataset.__getitem__(i)\n        a = draw_box(out_img.copy(), out_bboxes.astype(np.int32))\n        plt.imshow(a.astype(np.int32))\n        plt.show()\n'"
demo.py,0,"b'# -*- coding: utf-8 -*-\n\'\'\'\n@Time          : 20/04/25 15:49\n@Author        : huguanghao\n@File          : demo.py\n@Noice         :\n@Modificattion :\n    @Author    :\n    @Time      :\n    @Detail    :\n\'\'\'\n\n# import sys\n# import time\n# from PIL import Image, ImageDraw\n# from models.tiny_yolo import TinyYoloNet\nfrom tool.utils import *\nfrom tool.torch_utils import *\nfrom tool.darknet2pytorch import Darknet\nimport argparse\n\n""""""hyper parameters""""""\nuse_cuda = True\nnum_classes = 80\nif num_classes == 20:\n    namesfile = \'data/voc.names\'\nelif num_classes == 80:\n    namesfile = \'data/coco.names\'\nelse:\n    namesfile = \'data/x.names\'\n\n\ndef detect(cfgfile, weightfile, imgfile):\n    m = Darknet(cfgfile)\n\n    m.print_network()\n    m.load_weights(weightfile)\n    print(\'Loading weights from %s... Done!\' % (weightfile))\n\n    if use_cuda:\n        m.cuda()\n\n    img = Image.open(imgfile).convert(\'RGB\')\n    sized = img.resize((m.width, m.height))\n\n    for i in range(2):\n        start = time.time()\n        boxes = do_detect(m, sized, 0.5, num_classes, 0.4, use_cuda)\n        finish = time.time()\n        if i == 1:\n            print(\'%s: Predicted in %f seconds.\' % (imgfile, (finish - start)))\n\n    class_names = load_class_names(namesfile)\n    plot_boxes(img, boxes, \'predictions.jpg\', class_names)\n\n\ndef detect_imges(cfgfile, weightfile, imgfile_list=[\'data/dog.jpg\', \'data/giraffe.jpg\']):\n    m = Darknet(cfgfile)\n\n    m.print_network()\n    m.load_weights(weightfile)\n    print(\'Loading weights from %s... Done!\' % (weightfile))\n\n    if use_cuda:\n        m.cuda()\n\n    imges = []\n    imges_list = []\n    for imgfile in imgfile_list:\n        img = Image.open(imgfile).convert(\'RGB\')\n        imges_list.append(img)\n        sized = img.resize((m.width, m.height))\n        imges.append(np.expand_dims(np.array(sized), axis=0))\n\n    images = np.concatenate(imges, 0)\n    for i in range(2):\n        start = time.time()\n        boxes = do_detect(m, images, 0.5, num_classes, 0.4, use_cuda)\n        finish = time.time()\n        if i == 1:\n            print(\'%s: Predicted in %f seconds.\' % (imgfile, (finish - start)))\n\n    class_names = load_class_names(namesfile)\n    for i,(img,box) in enumerate(zip(imges_list,boxes)):\n        plot_boxes(img, box, \'predictions{}.jpg\'.format(i), class_names)\n\n\ndef detect_cv2(cfgfile, weightfile, imgfile):\n    import cv2\n    m = Darknet(cfgfile)\n\n    m.print_network()\n    m.load_weights(weightfile)\n    print(\'Loading weights from %s... Done!\' % (weightfile))\n\n    if use_cuda:\n        m.cuda()\n\n    img = cv2.imread(imgfile)\n    sized = cv2.resize(img, (m.width, m.height))\n    sized = cv2.cvtColor(sized, cv2.COLOR_BGR2RGB)\n\n    for i in range(2):\n        start = time.time()\n        boxes = do_detect(m, sized, 0.5, m.num_classes, 0.4, use_cuda)\n        finish = time.time()\n        if i == 1:\n            print(\'%s: Predicted in %f seconds.\' % (imgfile, (finish - start)))\n\n    class_names = load_class_names(namesfile)\n    plot_boxes_cv2(img, boxes, savename=\'predictions.jpg\', class_names=class_names)\n\n\ndef detect_cv2_camera(cfgfile, weightfile):\n    import cv2\n    m = Darknet(cfgfile)\n\n    m.print_network()\n    m.load_weights(weightfile)\n    print(\'Loading weights from %s... Done!\' % (weightfile))\n\n    if use_cuda:\n        m.cuda()\n\n    cap = cv2.VideoCapture(0)\n    # cap = cv2.VideoCapture(""./test.mp4"")\n    cap.set(3, 1280)\n    cap.set(4, 720)\n    print(""Starting the YOLO loop..."")\n\n    while True:\n        ret, img = cap.read()\n        sized = cv2.resize(img, (m.width, m.height))\n        sized = cv2.cvtColor(sized, cv2.COLOR_BGR2RGB)\n\n        start = time.time()\n        boxes = do_detect(m, sized, 0.5, num_classes, 0.4, use_cuda)\n        finish = time.time()\n        print(\'Predicted in %f seconds.\' % (finish - start))\n\n        class_names = load_class_names(namesfile)\n        result_img = plot_boxes_cv2(img, boxes, savename=None, class_names=class_names)\n\n        cv2.imshow(\'Yolo demo\', result_img)\n        cv2.waitKey(1)\n\n    cap.release()\n\n\ndef detect_skimage(cfgfile, weightfile, imgfile):\n    from skimage import io\n    from skimage.transform import resize\n    m = Darknet(cfgfile)\n\n    m.print_network()\n    m.load_weights(weightfile)\n    print(\'Loading weights from %s... Done!\' % (weightfile))\n\n    if use_cuda:\n        m.cuda()\n\n    img = io.imread(imgfile)\n    sized = resize(img, (m.width, m.height)) * 255\n\n    for i in range(2):\n        start = time.time()\n        boxes = do_detect(m, sized, 0.5, m.num_classes, 0.4, use_cuda)\n        finish = time.time()\n        if i == 1:\n            print(\'%s: Predicted in %f seconds.\' % (imgfile, (finish - start)))\n\n    class_names = load_class_names(namesfile)\n    plot_boxes_cv2(img, boxes, savename=\'predictions.jpg\', class_names=class_names)\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(\'Test your image or video by trained model.\')\n    parser.add_argument(\'-cfgfile\', type=str, default=\'./cfg/yolov4.cfg\',\n                        help=\'path of cfg file\', dest=\'cfgfile\')\n    parser.add_argument(\'-weightfile\', type=str,\n                        default=\'./checkpoints/Yolov4_epoch1.pth\',\n                        help=\'path of trained model.\', dest=\'weightfile\')\n    parser.add_argument(\'-imgfile\', type=str,\n                        default=\'./data/mscoco2017/train2017/190109_180343_00154162.jpg\',\n                        help=\'path of your image file.\', dest=\'imgfile\')\n    args = parser.parse_args()\n\n    return args\n\n\nif __name__ == \'__main__\':\n    args = get_args()\n    if args.imgfile:\n        detect(args.cfgfile, args.weightfile, args.imgfile)\n        # detect_imges(args.cfgfile, args.weightfile)\n        # detect_cv2(args.cfgfile, args.weightfile, args.imgfile)\n        # detect_skimage(args.cfgfile, args.weightfile, args.imgfile)\n    else:\n        detect_cv2_camera(args.cfgfile, args.weightfile)\n'"
demo_onnx.py,0,"b'import sys\nimport onnx\nimport os\nimport argparse\nimport numpy as np\nimport cv2\nimport onnxruntime\n\nfrom tool.utils import *\nfrom tool.torch_utils import *\nfrom tool.darknet2onnx import *\n\n\ndef main(cfg_file, weight_file, image_path, batch_size):\n\n    # Transform to onnx as specified batch size\n    fransform_to_onnx(cfg_file, weight_file, batch_size)\n    # Transform to onnx for demo\n    onnx_path_demo = fransform_to_onnx(cfg_file, weight_file, 1)\n\n    session = onnxruntime.InferenceSession(onnx_path_demo)\n    # session = onnx.load(onnx_path)\n    print(""The model expects input shape: "", session.get_inputs()[0].shape)\n\n    image_src = cv2.imread(image_path)\n    detect(session, image_src)\n\n\n\ndef detect(session, image_src):\n    IN_IMAGE_H = session.get_inputs()[0].shape[2]\n    IN_IMAGE_W = session.get_inputs()[0].shape[3]\n\n    # Input\n    resized = cv2.resize(image_src, (IN_IMAGE_W, IN_IMAGE_H), interpolation=cv2.INTER_LINEAR)\n    img_in = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    img_in = np.transpose(img_in, (2, 0, 1)).astype(np.float32)\n    img_in = np.expand_dims(img_in, axis=0)\n    img_in /= 255.0\n    print(""Shape of the network input: "", img_in.shape)\n\n    # Compute\n    input_name = session.get_inputs()[0].name\n    # output, output_exist = session.run([\'decoder.output_conv\', \'lane_exist.linear2\'], {""input.1"": image_np})\n\n    # print(img_in)\n\n    outputs = session.run(None, {input_name: img_in})\n\n    \'\'\'\n    print(len(outputs))\n    print(outputs[0].shape)\n    print(outputs[1].shape)\n    print(outputs[2].shape)\n    print(outputs[3].shape)\n    print(outputs[4].shape)\n    print(outputs[5].shape)\n    print(outputs[6].shape)\n    print(outputs[7].shape)\n    print(outputs[8].shape)\n    \'\'\'\n\n    outputs = [\n        [outputs[0],outputs[1],outputs[2]],\n        [outputs[3],outputs[4],outputs[5]],\n        [outputs[6],outputs[7],outputs[8]]\n    ]\n\n    # print(outputs[2])\n\n    num_classes = 80\n    boxes = post_processing(img_in, 0.5, num_classes, 0.4, outputs)\n\n    if num_classes == 20:\n        namesfile = \'data/voc.names\'\n    elif num_classes == 80:\n        namesfile = \'data/coco.names\'\n    else:\n        namesfile = \'data/names\'\n\n    class_names = load_class_names(namesfile)\n    plot_boxes_cv2(image_src, boxes, savename=\'predictions_onnx.jpg\', class_names=class_names)\n\n\n\nif __name__ == \'__main__\':\n    print(""Converting to onnx and running demo ..."")\n    if len(sys.argv) == 5:\n        cfg_file = sys.argv[1]\n        weight_file = sys.argv[2]\n        image_path = sys.argv[3]\n        batch_size = int(sys.argv[4])\n        main(cfg_file, weight_file, image_path, batch_size)\n    else:\n        print(\'Please run this way:\\n\')\n        print(\'  python demo_onnx.py <cfgFile> <weightFile> <imageFile> <batchSize>\')\n'"
demo_tensorflow.py,0,"b'import sys\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.platform import gfile\n\nimport cv2\nfrom tool.utils import post_processing, load_class_names, plot_boxes_cv2\n\n\ndef demo_tensorflow(tfpb_file=""./weight/yolov4.pb"", image_path=None, print_sensor_name=False):\n    graph_name = \'yolov4\'\n    tf.compat.v1.disable_eager_execution()\n    with tf.compat.v1.Session() as persisted_sess:\n        print(""loading graph..."")\n        with gfile.FastGFile(tfpb_file, \'rb\') as f:\n            graph_def = tf.compat.v1.GraphDef()\n            graph_def.ParseFromString(f.read())\n\n        persisted_sess.graph.as_default()\n        tf.import_graph_def(graph_def, name=graph_name)\n\n        # print all sensor_name\n        if print_sensor_name:\n            tensor_name_list = [tensor.name for tensor in tf.compat.v1.get_default_graph().as_graph_def().node]\n            for tensor_name in tensor_name_list:\n                print(tensor_name)\n\n        inp = persisted_sess.graph.get_tensor_by_name(graph_name + \'/\' + \'input:0\')\n        print(inp.shape)\n        out1 = persisted_sess.graph.get_tensor_by_name(graph_name + \'/\' + \'output_1:0\')\n        out2 = persisted_sess.graph.get_tensor_by_name(graph_name + \'/\' + \'output_2:0\')\n        out3 = persisted_sess.graph.get_tensor_by_name(graph_name + \'/\' + \'output_3:0\')\n        print(out1.shape, out2.shape, out3.shape)\n\n        # image_src = np.random.rand(1, 3, 608, 608).astype(np.float32)  # input image\n        # Input\n        image_src = cv2.imread(image_path)\n        resized = cv2.resize(image_src, (inp.shape[2], inp.shape[3]), interpolation=cv2.INTER_LINEAR)\n        img_in = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n        img_in = np.transpose(img_in, (2, 0, 1)).astype(np.float32)\n        img_in = np.expand_dims(img_in, axis=0)\n        img_in /= 255.0\n        print(""Shape of the network input: "", img_in.shape)\n\n        feed_dict = {inp: img_in}\n\n        outputs = persisted_sess.run([out1, out2, out3], feed_dict)\n        print(outputs[0].shape)\n        print(outputs[1].shape)\n        print(outputs[2].shape)\n\n        boxes = post_processing(img_in, 0.4, outputs)\n\n        num_classes = 80\n        if num_classes == 20:\n            namesfile = \'data/voc.names\'\n        elif num_classes == 80:\n            namesfile = \'data/coco.names\'\n        else:\n            namesfile = \'data/names\'\n\n        class_names = load_class_names(namesfile)\n        result = plot_boxes_cv2(image_src, boxes, savename=None, class_names=class_names)\n        cv2.imshow(""tensorflow predicted"", result)\n        cv2.waitKey()\n\n\nif __name__ == \'__main__\':\n    if len(sys.argv) == 1:\n        sys.argv.append(\'weight/yolov4.pb\')\n        sys.argv.append(\'data/dog.jpg\')\n    if len(sys.argv) == 3:\n        tfpbfile = sys.argv[1]\n        image_path = sys.argv[2]\n        demo_tensorflow(tfpbfile, image_path)\n    else:\n        print(\'Please execute this script this way:\\n\')\n        print(\'  python demo_tensorflow.py <tfpbfile> <imageFile>\')\n'"
demo_trt.py,0,"b'import sys\nimport os\nimport time\nimport argparse\nimport numpy as np\nimport cv2\n# from PIL import Image\nimport tensorrt as trt\nimport pycuda.driver as cuda\nimport pycuda.autoinit\n\nfrom tool.utils import *\nfrom tool.torch_utils import *\n\ntry:\n    # Sometimes python2 does not understand FileNotFoundError\n    FileNotFoundError\nexcept NameError:\n    FileNotFoundError = IOError\n\ndef GiB(val):\n    return val * 1 << 30\n\ndef find_sample_data(description=""Runs a TensorRT Python sample"", subfolder="""", find_files=[]):\n    \'\'\'\n    Parses sample arguments.\n    Args:\n        description (str): Description of the sample.\n        subfolder (str): The subfolder containing data relevant to this sample\n        find_files (str): A list of filenames to find. Each filename will be replaced with an absolute path.\n    Returns:\n        str: Path of data directory.\n    Raises:\n        FileNotFoundError\n    \'\'\'\n\n    # Standard command-line arguments for all samples.\n    kDEFAULT_DATA_ROOT = os.path.join(os.sep, ""usr"", ""src"", ""tensorrt"", ""data"")\n    parser = argparse.ArgumentParser(description=description, formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(""-d"", ""--datadir"", help=""Location of the TensorRT sample data directory."", default=kDEFAULT_DATA_ROOT)\n    args, unknown_args = parser.parse_known_args()\n\n    # If data directory is not specified, use the default.\n    data_root = args.datadir\n    # If the subfolder exists, append it to the path, otherwise use the provided path as-is.\n    subfolder_path = os.path.join(data_root, subfolder)\n    data_path = subfolder_path\n    if not os.path.exists(subfolder_path):\n        print(""WARNING: "" + subfolder_path + "" does not exist. Trying "" + data_root + "" instead."")\n        data_path = data_root\n\n    # Make sure data directory exists.\n    if not (os.path.exists(data_path)):\n        raise FileNotFoundError(data_path + "" does not exist. Please provide the correct data path with the -d option."")\n\n    # Find all requested files.\n    for index, f in enumerate(find_files):\n        find_files[index] = os.path.abspath(os.path.join(data_path, f))\n        if not os.path.exists(find_files[index]):\n            raise FileNotFoundError(find_files[index] + "" does not exist. Please provide the correct data path with the -d option."")\n\n    return data_path, find_files\n\n# Simple helper data class that\'s a little nicer to use than a 2-tuple.\nclass HostDeviceMem(object):\n    def __init__(self, host_mem, device_mem):\n        self.host = host_mem\n        self.device = device_mem\n\n    def __str__(self):\n        return ""Host:\\n"" + str(self.host) + ""\\nDevice:\\n"" + str(self.device)\n\n    def __repr__(self):\n        return self.__str__()\n\n# Allocates all buffers required for an engine, i.e. host/device inputs/outputs.\ndef allocate_buffers(engine):\n    inputs = []\n    outputs = []\n    bindings = []\n    stream = cuda.Stream()\n    for binding in engine:\n        size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n        dtype = trt.nptype(engine.get_binding_dtype(binding))\n        # Allocate host and device buffers\n        host_mem = cuda.pagelocked_empty(size, dtype)\n        device_mem = cuda.mem_alloc(host_mem.nbytes)\n        # Append the device buffer to device bindings.\n        bindings.append(int(device_mem))\n        # Append to the appropriate list.\n        if engine.binding_is_input(binding):\n            inputs.append(HostDeviceMem(host_mem, device_mem))\n        else:\n            outputs.append(HostDeviceMem(host_mem, device_mem))\n    return inputs, outputs, bindings, stream\n\n# This function is generalized for multiple inputs/outputs.\n# inputs and outputs are expected to be lists of HostDeviceMem objects.\ndef do_inference(context, bindings, inputs, outputs, stream, batch_size=1):\n    # Transfer input data to the GPU.\n    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n    # Run inference.\n    context.execute_async(batch_size=batch_size, bindings=bindings, stream_handle=stream.handle)\n    # Transfer predictions back from the GPU.\n    [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n    # Synchronize the stream\n    stream.synchronize()\n    # Return only the host outputs.\n    return [out.host for out in outputs]\n\n\nTRT_LOGGER = trt.Logger()\n\ndef main(engine_path, image_path, image_size):\n    with get_engine(engine_path) as engine, engine.create_execution_context() as context:\n        buffers = allocate_buffers(engine)\n        image_src = cv2.imread(image_path)\n\n        detect(engine, context, buffers, image_src, image_size)\n\n\ndef get_engine(engine_path):\n    # If a serialized engine exists, use it instead of building an engine.\n    print(""Reading engine from file {}"".format(engine_path))\n    with open(engine_path, ""rb"") as f, trt.Runtime(TRT_LOGGER) as runtime:\n        return runtime.deserialize_cuda_engine(f.read())\n\n\n\ndef detect(engine, context, buffers, image_src, image_size):\n    IN_IMAGE_H, IN_IMAGE_W = image_size\n\n    ta = time.time()\n    # Input\n    resized = cv2.resize(image_src, (IN_IMAGE_W, IN_IMAGE_H), interpolation=cv2.INTER_LINEAR)\n    img_in = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    img_in = np.transpose(img_in, (2, 0, 1)).astype(np.float32)\n    img_in = np.expand_dims(img_in, axis=0)\n    img_in /= 255.0\n    img_in = np.ascontiguousarray(img_in)\n    print(""Shape of the network input: "", img_in.shape)\n    # print(img_in)\n\n    inputs, outputs, bindings, stream = buffers\n    print(\'Length of inputs: \', len(inputs))\n    inputs[0].host = img_in\n\n    trt_outputs = do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)\n\n    print(\'Len of outputs: \', len(trt_outputs))\n    # print(trt_outputs)\n    \'\'\'\n    print(\'Shape of outputs: \')\n    print(trt_outputs[0].shape)\n    print(trt_outputs[1].shape)\n    print(trt_outputs[2].shape)\n\n    trt_outputs[0] = trt_outputs[0].reshape(-1, 255, IN_IMAGE_H // 8, IN_IMAGE_W // 8)\n    trt_outputs[1] = trt_outputs[1].reshape(-1, 255, IN_IMAGE_H // 16, IN_IMAGE_W // 16)\n    trt_outputs[2] = trt_outputs[2].reshape(-1, 255, IN_IMAGE_H // 32, IN_IMAGE_W // 32)\n    \'\'\'\n\n    h1 = IN_IMAGE_H // 8\n    w1 = IN_IMAGE_W // 8\n    h2 = IN_IMAGE_H // 16\n    w2 = IN_IMAGE_W // 16\n    h3 = IN_IMAGE_H // 32\n    w3 = IN_IMAGE_W // 32\n\n\n    trt_outputs = [\n        [\n            trt_outputs[0],\n            trt_outputs[1],\n            trt_outputs[2]\n        ],\n        [\n            trt_outputs[3],\n            trt_outputs[4],\n            trt_outputs[5]\n        ],\n        [\n            trt_outputs[6],\n            trt_outputs[7],\n            trt_outputs[8]\n        ]\n    ]\n\n    trt_outputs[0].sort(key=len)\n    trt_outputs[1].sort(key=len)\n    trt_outputs[2].sort(key=len)\n\n    # print(outputs[2])\n    num_classes = 80\n\n    trt_outputs = [\n        [\n            trt_outputs[0][1].reshape(-1, 3 * h1 * w1, 4),\n            trt_outputs[0][2].reshape(-1, 3 * h1 * w1, num_classes),\n            trt_outputs[0][0].reshape(-1, 3 * h1 * w1)\n        ],\n        [\n            trt_outputs[1][1].reshape(-1, 3 * h2 * w2, 4),\n            trt_outputs[1][2].reshape(-1, 3 * h2 * w2, num_classes),\n            trt_outputs[1][0].reshape(-1, 3 * h2 * w2)\n        ],\n        [\n            trt_outputs[2][1].reshape(-1, 3 * h3 * w3, 4),\n            trt_outputs[2][2].reshape(-1, 3 * h3 * w3, num_classes),\n            trt_outputs[2][0].reshape(-1, 3 * h3 * w3)\n        ]\n    ]\n\n    tb = time.time()\n\n    print(\'-----------------------------------\')\n    print(\'    TRT inference time: %f\' % (tb - ta))\n    print(\'-----------------------------------\')\n\n    boxes = post_processing(img_in, 0.5, num_classes, 0.4, trt_outputs)\n\n    if num_classes == 20:\n        namesfile = \'data/voc.names\'\n    elif num_classes == 80:\n        namesfile = \'data/coco.names\'\n    else:\n        namesfile = \'data/names\'\n\n    class_names = load_class_names(namesfile)\n    plot_boxes_cv2(image_src, boxes, savename=\'predictions_trt.jpg\', class_names=class_names)\n\n\n\nif __name__ == \'__main__\':\n    engine_path = sys.argv[1]\n    image_path = sys.argv[2]\n    \n    if len(sys.argv) < 4:\n        image_size = (416, 416)\n    elif len(sys.argv) < 5:\n        image_size = (int(sys.argv[3]), int(sys.argv[3]))\n    else:\n        image_size = (int(sys.argv[3]), int(sys.argv[4]))\n    \n    main(engine_path, image_path, image_size)\n'"
evaluate_on_coco.py,4,"b'""""""\nA script to evaluate the model\'s performance using pre-trained weights using COCO API.\nExample usage: python evaluate_on_coco.py -dir D:\\cocoDataset\\val2017\\val2017 -gta D:\\cocoDataset\\annotatio\nns_trainval2017\\annotations\\instances_val2017.json -c cfg/yolov4-smaller-input.cfg -g 0\nExplanation: set where your images can be found using -dir, then use -gta to point to the ground truth annotations file\nand finally -c to point to the config file you want to use to load the network using.\n""""""\n\nimport argparse\nimport datetime\nimport json\nimport logging\nimport os\nimport sys\nimport time\nfrom collections import defaultdict\n\nimport numpy as np\nimport torch\nfrom PIL import Image, ImageDraw\nfrom easydict import EasyDict as edict\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\nfrom cfg import Cfg\nfrom tool.darknet2pytorch import Darknet\nfrom tool.utils import load_class_names\nfrom tool.torch_utils import do_detect\n\n\ndef get_class_name(cat):\n    class_names = load_class_names(""./data/coco.names"")\n    if cat >= 1 and cat <= 11:\n        cat = cat - 1\n    elif cat >= 13 and cat <= 25:\n        cat = cat - 2\n    elif cat >= 27 and cat <= 28:\n        cat = cat - 3\n    elif cat >= 31 and cat <= 44:\n        cat = cat - 5\n    elif cat >= 46 and cat <= 65:\n        cat = cat - 6\n    elif cat == 67:\n        cat = cat - 7\n    elif cat == 70:\n        cat = cat - 9\n    elif cat >= 72 and cat <= 82:\n        cat = cat - 10\n    elif cat >= 84 and cat <= 90:\n        cat = cat - 11\n    return class_names[cat]\n\ndef convert_cat_id_and_reorientate_bbox(single_annotation):\n    cat = single_annotation[\'category_id\']\n    bbox = single_annotation[\'bbox\']\n    x, y, w, h = bbox\n    x1, y1, x2, y2 = x - w / 2, y - h / 2, x + w / 2, y + h / 2\n    if 0 <= cat <= 10:\n        cat = cat + 1\n    elif 11 <= cat <= 23:\n        cat = cat + 2\n    elif 24 <= cat <= 25:\n        cat = cat + 3\n    elif 26 <= cat <= 39:\n        cat = cat + 5\n    elif 40 <= cat <= 59:\n        cat = cat + 6\n    elif cat == 60:\n        cat = cat + 7\n    elif cat == 61:\n        cat = cat + 9\n    elif 62 <= cat <= 72:\n        cat = cat + 10\n    elif 73 <= cat <= 79:\n        cat = cat + 11\n    single_annotation[\'category_id\'] = cat\n    single_annotation[\'bbox\'] = [x1, y1, w, h]\n    return single_annotation\n\n\n\ndef myconverter(obj):\n    if isinstance(obj, np.integer):\n        return int(obj)\n    elif isinstance(obj, np.floating):\n        return float(obj)\n    elif isinstance(obj, np.ndarray):\n        return obj.tolist()\n    elif isinstance(obj, datetime.datetime):\n        return obj.__str__()\n    else:\n        return obj\n\ndef evaluate_on_coco(cfg, resFile):\n    annType = ""bbox""  # specify type here\n    with open(resFile, \'r\') as f:\n        unsorted_annotations = json.load(f)\n    sorted_annotations = list(sorted(unsorted_annotations, key=lambda single_annotation: single_annotation[""image_id""]))\n    sorted_annotations = list(map(convert_cat_id_and_reorientate_bbox, sorted_annotations))\n    reshaped_annotations = defaultdict(list)\n    for annotation in sorted_annotations:\n        reshaped_annotations[annotation[\'image_id\']].append(annotation)\n\n    with open(\'temp.json\', \'w\') as f:\n        json.dump(sorted_annotations, f)\n\n    cocoGt = COCO(cfg.gt_annotations_path)\n    cocoDt = cocoGt.loadRes(\'temp.json\')\n\n    with open(cfg.gt_annotations_path, \'r\') as f:\n        gt_annotation_raw = json.load(f)\n        gt_annotation_raw_images = gt_annotation_raw[""images""]\n        gt_annotation_raw_labels = gt_annotation_raw[""annotations""]\n\n    rgb_label = (255, 0, 0)\n    rgb_pred = (0, 255, 0)\n\n    for i, image_id in enumerate(reshaped_annotations):\n        image_annotations = reshaped_annotations[image_id]\n        gt_annotation_image_raw = list(filter(\n            lambda image_json: image_json[\'id\'] == image_id, gt_annotation_raw_images\n        ))\n        gt_annotation_labels_raw = list(filter(\n            lambda label_json: label_json[\'image_id\'] == image_id, gt_annotation_raw_labels\n        ))\n        if len(gt_annotation_image_raw) == 1:\n            image_path = os.path.join(cfg.dataset_dir, gt_annotation_image_raw[0][""file_name""])\n            actual_image = Image.open(image_path).convert(\'RGB\')\n            draw = ImageDraw.Draw(actual_image)\n\n            for annotation in image_annotations:\n                x1_pred, y1_pred, w, h = annotation[\'bbox\']\n                x2_pred, y2_pred = x1_pred + w, y1_pred + h\n                cls_id = annotation[\'category_id\']\n                label = get_class_name(cls_id)\n                draw.text((x1_pred, y1_pred), label, fill=rgb_pred)\n                draw.rectangle([x1_pred, y1_pred, x2_pred, y2_pred], outline=rgb_pred)\n            for annotation in gt_annotation_labels_raw:\n                x1_truth, y1_truth, w, h = annotation[\'bbox\']\n                x2_truth, y2_truth = x1_truth + w, y1_truth + h\n                cls_id = annotation[\'category_id\']\n                label = get_class_name(cls_id)\n                draw.text((x1_truth, y1_truth), label, fill=rgb_label)\n                draw.rectangle([x1_truth, y1_truth, x2_truth, y2_truth], outline=rgb_label)\n            actual_image.save(""./data/outcome/predictions_{}"".format(gt_annotation_image_raw[0][""file_name""]))\n        else:\n            print(\'please check\')\n            break\n        if (i + 1) % 100 == 0: # just see first 100\n            break\n\n    imgIds = sorted(cocoGt.getImgIds())\n    cocoEval = COCOeval(cocoGt, cocoDt, annType)\n    cocoEval.params.imgIds = imgIds\n    cocoEval.evaluate()\n    cocoEval.accumulate()\n    cocoEval.summarize()\n\n\ndef test(model, annotations, cfg):\n    if not annotations[""images""]:\n        print(""Annotations do not have \'images\' key"")\n        return\n    images = annotations[""images""]\n    # images = images[:10]\n    resFile = \'data/coco_val_outputs.json\'\n\n    if torch.cuda.is_available():\n        use_cuda = 1\n    else:\n        use_cuda = 0\n\n    # do one forward pass first to circumvent cold start\n    throwaway_image = Image.open(\'data/dog.jpg\').convert(\'RGB\').resize((model.width, model.height))\n    do_detect(model, throwaway_image, 0.5, 80, 0.4, use_cuda)\n    boxes_json = []\n\n    for i, image_annotation in enumerate(images):\n        logging.info(""currently on image: {}/{}"".format(i + 1, len(images)))\n        image_file_name = image_annotation[""file_name""]\n        image_id = image_annotation[""id""]\n        image_height = image_annotation[""height""]\n        image_width = image_annotation[""width""]\n\n        # open and resize each image first\n        img = Image.open(os.path.join(cfg.dataset_dir, image_file_name)).convert(\'RGB\')\n        sized = img.resize((model.width, model.height))\n\n        if use_cuda:\n            model.cuda()\n\n        start = time.time()\n        boxes = do_detect(model, sized, 0.0, 80, 0.4, use_cuda)\n        finish = time.time()\n        if type(boxes) == list:\n            for box in boxes:\n                box_json = {}\n                category_id = box[-1]\n                score = box[-2]\n                bbox_normalized = box[:4]\n                box_json[""category_id""] = int(category_id)\n                box_json[""image_id""] = int(image_id)\n                bbox = []\n                for i, bbox_coord in enumerate(bbox_normalized):\n                    modified_bbox_coord = float(bbox_coord)\n                    if i % 2:\n                        modified_bbox_coord *= image_height\n                    else:\n                        modified_bbox_coord *= image_width\n                    modified_bbox_coord = round(modified_bbox_coord, 2)\n                    bbox.append(modified_bbox_coord)\n                box_json[""bbox_normalized""] = list(map(lambda x: round(float(x), 2), bbox_normalized))\n                box_json[""bbox""] = bbox\n                box_json[""score""] = round(float(score), 2)\n                box_json[""timing""] = float(finish - start)\n                boxes_json.append(box_json)\n                # print(""see box_json: "", box_json)\n                with open(resFile, \'w\') as outfile:\n                    json.dump(boxes_json, outfile, default=myconverter)\n        else:\n            print(""warning: output from model after postprocessing is not a list, ignoring"")\n            return\n\n        # namesfile = \'data/coco.names\'\n        # class_names = load_class_names(namesfile)\n        # plot_boxes(img, boxes, \'data/outcome/predictions_{}.jpg\'.format(image_id), class_names)\n\n    with open(resFile, \'w\') as outfile:\n        json.dump(boxes_json, outfile, default=myconverter)\n\n    evaluate_on_coco(cfg, resFile)\n\n\ndef get_args(**kwargs):\n    cfg = kwargs\n    parser = argparse.ArgumentParser(description=\'Test model on test dataset\',\n                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'-f\', \'--load\', dest=\'load\', type=str, default=None,\n                        help=\'Load model from a .pth file\')\n    parser.add_argument(\'-g\', \'--gpu\', metavar=\'G\', type=str, default=\'-1\',\n                        help=\'GPU\', dest=\'gpu\')\n    parser.add_argument(\'-dir\', \'--data-dir\', type=str, default=None,\n                        help=\'dataset dir\', dest=\'dataset_dir\')\n    parser.add_argument(\'-gta\', \'--ground_truth_annotations\', type=str, default=\'instances_val2017.json\',\n                        help=\'ground truth annotations file\', dest=\'gt_annotations_path\')\n    parser.add_argument(\'-w\', \'--weights_file\', type=str, default=\'weights/yolov4.weights\',\n                        help=\'weights file to load\', dest=\'weights_file\')\n    parser.add_argument(\'-c\', \'--model_config\', type=str, default=\'cfg/yolov4.cfg\',\n                        help=\'model config file to load\', dest=\'model_config\')\n    args = vars(parser.parse_args())\n\n    for k in args.keys():\n        cfg[k] = args.get(k)\n    return edict(cfg)\n\n\ndef init_logger(log_file=None, log_dir=None, log_level=logging.INFO, mode=\'w\', stdout=True):\n    """"""\n    log_dir: \xe6\x97\xa5\xe5\xbf\x97\xe6\x96\x87\xe4\xbb\xb6\xe7\x9a\x84\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xe8\xb7\xaf\xe5\xbe\x84\n    mode: \'a\', append; \'w\', \xe8\xa6\x86\xe7\x9b\x96\xe5\x8e\x9f\xe6\x96\x87\xe4\xbb\xb6\xe5\x86\x99\xe5\x85\xa5.\n    """"""\n    import datetime\n    def get_date_str():\n        now = datetime.datetime.now()\n        return now.strftime(\'%Y-%m-%d_%H-%M-%S\')\n\n    fmt = \'%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s: %(message)s\'\n    if log_dir is None:\n        log_dir = \'~/temp/log/\'\n    if log_file is None:\n        log_file = \'log_\' + get_date_str() + \'.txt\'\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n    log_file = os.path.join(log_dir, log_file)\n    # \xe6\xad\xa4\xe5\xa4\x84\xe4\xb8\x8d\xe8\x83\xbd\xe4\xbd\xbf\xe7\x94\xa8logging\xe8\xbe\x93\xe5\x87\xba\n    print(\'log file path:\' + log_file)\n\n    logging.basicConfig(level=logging.DEBUG,\n                        format=fmt,\n                        filename=log_file,\n                        filemode=mode)\n\n    if stdout:\n        console = logging.StreamHandler(stream=sys.stdout)\n        console.setLevel(log_level)\n        formatter = logging.Formatter(fmt)\n        console.setFormatter(formatter)\n        logging.getLogger(\'\').addHandler(console)\n\n    return logging\n\n\nif __name__ == ""__main__"":\n    logging = init_logger(log_dir=\'log\')\n    cfg = get_args(**Cfg)\n    os.environ[""CUDA_VISIBLE_DEVICES""] = cfg.gpu\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    logging.info(f\'Using device {device}\')\n\n    model = Darknet(cfg.model_config)\n\n    model.print_network()\n    model.load_weights(cfg.weights_file)\n    model.eval()  # set model away from training\n\n    if torch.cuda.device_count() > 1:\n        model = torch.nn.DataParallel(model)\n\n    model.to(device=device)\n\n    annotations_file_path = cfg.gt_annotations_path\n    with open(annotations_file_path) as annotations_file:\n        try:\n            annotations = json.load(annotations_file)\n        except:\n            print(""annotations file not a json"")\n            exit()\n    test(model=model,\n         annotations=annotations,\n         cfg=cfg, )\n'"
models.py,15,"b'import torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nclass Mish(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        x = x * (torch.tanh(torch.nn.functional.softplus(x)))\n        return x\n\n\nclass Upsample(nn.Module):\n    def __init__(self):\n        super(Upsample, self).__init__()\n\n    def forward(self, x, target_size):\n        assert (x.data.dim() == 4)\n        _, _, H, W = target_size\n        return F.interpolate(x, size=(H, W), mode=\'nearest\')\n\n\nclass Conv_Bn_Activation(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, activation, bn=True, bias=False):\n        super().__init__()\n        pad = (kernel_size - 1) // 2\n\n        self.conv = nn.ModuleList()\n        if bias:\n            self.conv.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, pad))\n        else:\n            self.conv.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, pad, bias=False))\n        if bn:\n            self.conv.append(nn.BatchNorm2d(out_channels))\n        if activation == ""mish"":\n            self.conv.append(Mish())\n        elif activation == ""relu"":\n            self.conv.append(nn.ReLU(inplace=True))\n        elif activation == ""leaky"":\n            self.conv.append(nn.LeakyReLU(0.1, inplace=True))\n        elif activation == ""linear"":\n            pass\n        else:\n            print(""activate error !!! {} {} {}"".format(sys._getframe().f_code.co_filename,\n                                                       sys._getframe().f_code.co_name, sys._getframe().f_lineno))\n\n    def forward(self, x):\n        for l in self.conv:\n            x = l(x)\n        return x\n\n\nclass ResBlock(nn.Module):\n    """"""\n    Sequential residual blocks each of which consists of \\\n    two convolution layers.\n    Args:\n        ch (int): number of input and output channels.\n        nblocks (int): number of residual blocks.\n        shortcut (bool): if True, residual tensor addition is enabled.\n    """"""\n\n    def __init__(self, ch, nblocks=1, shortcut=True):\n        super().__init__()\n        self.shortcut = shortcut\n        self.module_list = nn.ModuleList()\n        for i in range(nblocks):\n            resblock_one = nn.ModuleList()\n            resblock_one.append(Conv_Bn_Activation(ch, ch, 1, 1, \'mish\'))\n            resblock_one.append(Conv_Bn_Activation(ch, ch, 3, 1, \'mish\'))\n            self.module_list.append(resblock_one)\n\n    def forward(self, x):\n        for module in self.module_list:\n            h = x\n            for res in module:\n                h = res(h)\n            x = x + h if self.shortcut else h\n        return x\n\n\nclass DownSample1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = Conv_Bn_Activation(3, 32, 3, 1, \'mish\')\n\n        self.conv2 = Conv_Bn_Activation(32, 64, 3, 2, \'mish\')\n        self.conv3 = Conv_Bn_Activation(64, 64, 1, 1, \'mish\')\n        # [route]\n        # layers = -2\n        self.conv4 = Conv_Bn_Activation(64, 64, 1, 1, \'mish\')\n\n        self.conv5 = Conv_Bn_Activation(64, 32, 1, 1, \'mish\')\n        self.conv6 = Conv_Bn_Activation(32, 64, 3, 1, \'mish\')\n        # [shortcut]\n        # from=-3\n        # activation = linear\n\n        self.conv7 = Conv_Bn_Activation(64, 64, 1, 1, \'mish\')\n        # [route]\n        # layers = -1, -7\n        self.conv8 = Conv_Bn_Activation(128, 64, 1, 1, \'mish\')\n\n    def forward(self, input):\n        x1 = self.conv1(input)\n        x2 = self.conv2(x1)\n        x3 = self.conv3(x2)\n        # route -2\n        x4 = self.conv4(x2)\n        x5 = self.conv5(x4)\n        x6 = self.conv6(x5)\n        # shortcut -3\n        x6 = x6 + x4\n\n        x7 = self.conv7(x6)\n        # [route]\n        # layers = -1, -7\n        x7 = torch.cat([x7, x3], dim=1)\n        x8 = self.conv8(x7)\n        return x8\n\n\nclass DownSample2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = Conv_Bn_Activation(64, 128, 3, 2, \'mish\')\n        self.conv2 = Conv_Bn_Activation(128, 64, 1, 1, \'mish\')\n        # r -2\n        self.conv3 = Conv_Bn_Activation(128, 64, 1, 1, \'mish\')\n\n        self.resblock = ResBlock(ch=64, nblocks=2)\n\n        # s -3\n        self.conv4 = Conv_Bn_Activation(64, 64, 1, 1, \'mish\')\n        # r -1 -10\n        self.conv5 = Conv_Bn_Activation(128, 128, 1, 1, \'mish\')\n\n    def forward(self, input):\n        x1 = self.conv1(input)\n        x2 = self.conv2(x1)\n        x3 = self.conv3(x1)\n\n        r = self.resblock(x3)\n        x4 = self.conv4(r)\n\n        x4 = torch.cat([x4, x2], dim=1)\n        x5 = self.conv5(x4)\n        return x5\n\n\nclass DownSample3(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = Conv_Bn_Activation(128, 256, 3, 2, \'mish\')\n        self.conv2 = Conv_Bn_Activation(256, 128, 1, 1, \'mish\')\n        self.conv3 = Conv_Bn_Activation(256, 128, 1, 1, \'mish\')\n\n        self.resblock = ResBlock(ch=128, nblocks=8)\n        self.conv4 = Conv_Bn_Activation(128, 128, 1, 1, \'mish\')\n        self.conv5 = Conv_Bn_Activation(256, 256, 1, 1, \'mish\')\n\n    def forward(self, input):\n        x1 = self.conv1(input)\n        x2 = self.conv2(x1)\n        x3 = self.conv3(x1)\n\n        r = self.resblock(x3)\n        x4 = self.conv4(r)\n\n        x4 = torch.cat([x4, x2], dim=1)\n        x5 = self.conv5(x4)\n        return x5\n\n\nclass DownSample4(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = Conv_Bn_Activation(256, 512, 3, 2, \'mish\')\n        self.conv2 = Conv_Bn_Activation(512, 256, 1, 1, \'mish\')\n        self.conv3 = Conv_Bn_Activation(512, 256, 1, 1, \'mish\')\n\n        self.resblock = ResBlock(ch=256, nblocks=8)\n        self.conv4 = Conv_Bn_Activation(256, 256, 1, 1, \'mish\')\n        self.conv5 = Conv_Bn_Activation(512, 512, 1, 1, \'mish\')\n\n    def forward(self, input):\n        x1 = self.conv1(input)\n        x2 = self.conv2(x1)\n        x3 = self.conv3(x1)\n\n        r = self.resblock(x3)\n        x4 = self.conv4(r)\n\n        x4 = torch.cat([x4, x2], dim=1)\n        x5 = self.conv5(x4)\n        return x5\n\n\nclass DownSample5(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = Conv_Bn_Activation(512, 1024, 3, 2, \'mish\')\n        self.conv2 = Conv_Bn_Activation(1024, 512, 1, 1, \'mish\')\n        self.conv3 = Conv_Bn_Activation(1024, 512, 1, 1, \'mish\')\n\n        self.resblock = ResBlock(ch=512, nblocks=4)\n        self.conv4 = Conv_Bn_Activation(512, 512, 1, 1, \'mish\')\n        self.conv5 = Conv_Bn_Activation(1024, 1024, 1, 1, \'mish\')\n\n    def forward(self, input):\n        x1 = self.conv1(input)\n        x2 = self.conv2(x1)\n        x3 = self.conv3(x1)\n\n        r = self.resblock(x3)\n        x4 = self.conv4(r)\n\n        x4 = torch.cat([x4, x2], dim=1)\n        x5 = self.conv5(x4)\n        return x5\n\n\nclass Neck(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = Conv_Bn_Activation(1024, 512, 1, 1, \'leaky\')\n        self.conv2 = Conv_Bn_Activation(512, 1024, 3, 1, \'leaky\')\n        self.conv3 = Conv_Bn_Activation(1024, 512, 1, 1, \'leaky\')\n        # SPP\n        self.maxpool1 = nn.MaxPool2d(kernel_size=5, stride=1, padding=5 // 2)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=9, stride=1, padding=9 // 2)\n        self.maxpool3 = nn.MaxPool2d(kernel_size=13, stride=1, padding=13 // 2)\n\n        # R -1 -3 -5 -6\n        # SPP\n        self.conv4 = Conv_Bn_Activation(2048, 512, 1, 1, \'leaky\')\n        self.conv5 = Conv_Bn_Activation(512, 1024, 3, 1, \'leaky\')\n        self.conv6 = Conv_Bn_Activation(1024, 512, 1, 1, \'leaky\')\n        self.conv7 = Conv_Bn_Activation(512, 256, 1, 1, \'leaky\')\n        # UP\n        self.upsample1 = Upsample()\n        # R 85\n        self.conv8 = Conv_Bn_Activation(512, 256, 1, 1, \'leaky\')\n        # R -1 -3\n        self.conv9 = Conv_Bn_Activation(512, 256, 1, 1, \'leaky\')\n        self.conv10 = Conv_Bn_Activation(256, 512, 3, 1, \'leaky\')\n        self.conv11 = Conv_Bn_Activation(512, 256, 1, 1, \'leaky\')\n        self.conv12 = Conv_Bn_Activation(256, 512, 3, 1, \'leaky\')\n        self.conv13 = Conv_Bn_Activation(512, 256, 1, 1, \'leaky\')\n        self.conv14 = Conv_Bn_Activation(256, 128, 1, 1, \'leaky\')\n        # UP\n        self.upsample2 = Upsample()\n        # R 54\n        self.conv15 = Conv_Bn_Activation(256, 128, 1, 1, \'leaky\')\n        # R -1 -3\n        self.conv16 = Conv_Bn_Activation(256, 128, 1, 1, \'leaky\')\n        self.conv17 = Conv_Bn_Activation(128, 256, 3, 1, \'leaky\')\n        self.conv18 = Conv_Bn_Activation(256, 128, 1, 1, \'leaky\')\n        self.conv19 = Conv_Bn_Activation(128, 256, 3, 1, \'leaky\')\n        self.conv20 = Conv_Bn_Activation(256, 128, 1, 1, \'leaky\')\n\n    def forward(self, input, downsample4, downsample3):\n        x1 = self.conv1(input)\n        x2 = self.conv2(x1)\n        x3 = self.conv3(x2)\n        # SPP\n        m1 = self.maxpool1(x3)\n        m2 = self.maxpool2(x3)\n        m3 = self.maxpool3(x3)\n        spp = torch.cat([m3, m2, m1, x3], dim=1)\n        # SPP end\n        x4 = self.conv4(spp)\n        x5 = self.conv5(x4)\n        x6 = self.conv6(x5)\n        x7 = self.conv7(x6)\n        # UP\n        up = self.upsample1(x7, downsample4.size())\n        # R 85\n        x8 = self.conv8(downsample4)\n        # R -1 -3\n        x8 = torch.cat([x8, up], dim=1)\n\n        x9 = self.conv9(x8)\n        x10 = self.conv10(x9)\n        x11 = self.conv11(x10)\n        x12 = self.conv12(x11)\n        x13 = self.conv13(x12)\n        x14 = self.conv14(x13)\n\n        # UP\n        up = self.upsample2(x14, downsample3.size())\n        # R 54\n        x15 = self.conv15(downsample3)\n        # R -1 -3\n        x15 = torch.cat([x15, up], dim=1)\n\n        x16 = self.conv16(x15)\n        x17 = self.conv17(x16)\n        x18 = self.conv18(x17)\n        x19 = self.conv19(x18)\n        x20 = self.conv20(x19)\n        return x20, x13, x6\n\n\nclass Yolov4Head(nn.Module):\n    def __init__(self, output_ch):\n        super().__init__()\n        self.conv1 = Conv_Bn_Activation(128, 256, 3, 1, \'leaky\')\n        self.conv2 = Conv_Bn_Activation(256, output_ch, 1, 1, \'linear\', bn=False, bias=True)\n        # self.yolo1 = YoloLayer(anchor_mask=[0, 1, 2], num_classes=80,\n        #                        anchors=[12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401],\n        #                        num_anchors=9, stride=8)\n\n        # R -4\n        self.conv3 = Conv_Bn_Activation(128, 256, 3, 2, \'leaky\')\n\n        # R -1 -16\n        self.conv4 = Conv_Bn_Activation(512, 256, 1, 1, \'leaky\')\n        self.conv5 = Conv_Bn_Activation(256, 512, 3, 1, \'leaky\')\n        self.conv6 = Conv_Bn_Activation(512, 256, 1, 1, \'leaky\')\n        self.conv7 = Conv_Bn_Activation(256, 512, 3, 1, \'leaky\')\n        self.conv8 = Conv_Bn_Activation(512, 256, 1, 1, \'leaky\')\n        self.conv9 = Conv_Bn_Activation(256, 512, 3, 1, \'leaky\')\n        self.conv10 = Conv_Bn_Activation(512, output_ch, 1, 1, \'linear\', bn=False, bias=True)\n        # self.yolo2 = YoloLayer(anchor_mask=[3, 4, 5], num_classes=80,\n        #                        anchors=[12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401],\n        #                        num_anchors=9, stride=16)\n\n        # R -4\n        self.conv11 = Conv_Bn_Activation(256, 512, 3, 2, \'leaky\')\n\n        # R -1 -37\n        self.conv12 = Conv_Bn_Activation(1024, 512, 1, 1, \'leaky\')\n        self.conv13 = Conv_Bn_Activation(512, 1024, 3, 1, \'leaky\')\n        self.conv14 = Conv_Bn_Activation(1024, 512, 1, 1, \'leaky\')\n        self.conv15 = Conv_Bn_Activation(512, 1024, 3, 1, \'leaky\')\n        self.conv16 = Conv_Bn_Activation(1024, 512, 1, 1, \'leaky\')\n        self.conv17 = Conv_Bn_Activation(512, 1024, 3, 1, \'leaky\')\n        self.conv18 = Conv_Bn_Activation(1024, output_ch, 1, 1, \'linear\', bn=False, bias=True)\n        # self.yolo3 = YoloLayer(anchor_mask=[6, 7, 8], num_classes=80,\n        #                        anchors=[12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401],\n        #                        num_anchors=9, stride=32)\n\n    def forward(self, input1, input2, input3):\n        x1 = self.conv1(input1)\n        x2 = self.conv2(x1)\n        # y1 = self.yolo1(x2)\n\n        x3 = self.conv3(input1)\n        # R -1 -16\n        x3 = torch.cat([x3, input2], dim=1)\n        x4 = self.conv4(x3)\n        x5 = self.conv5(x4)\n        x6 = self.conv6(x5)\n        x7 = self.conv7(x6)\n        x8 = self.conv8(x7)\n        x9 = self.conv9(x8)\n        x10 = self.conv10(x9)\n        # y2 = self.yolo2(x10)\n\n        # R -4\n        x11 = self.conv11(x8)\n        # R -1 -37\n        x11 = torch.cat([x11, input3], dim=1)\n\n        x12 = self.conv12(x11)\n        x13 = self.conv13(x12)\n        x14 = self.conv14(x13)\n        x15 = self.conv15(x14)\n        x16 = self.conv16(x15)\n        x17 = self.conv17(x16)\n        x18 = self.conv18(x17)\n        return [x2, x10, x18]\n        # y3 = self.yolo3(x18)\n        # return [y1, y2, y3]\n        # return y3\n\n\nclass Yolov4(nn.Module):\n    def __init__(self, yolov4conv137weight=None, n_classes=80):\n        super().__init__()\n\n        output_ch = (4 + 1 + n_classes) * 3\n\n        # backbone\n        self.down1 = DownSample1()\n        self.down2 = DownSample2()\n        self.down3 = DownSample3()\n        self.down4 = DownSample4()\n        self.down5 = DownSample5()\n        # neck\n        self.neck = Neck()\n        # yolov4conv137\n        if yolov4conv137weight:\n            _model = nn.Sequential(self.down1, self.down2, self.down3, self.down4, self.down5, self.neck)\n            pretrained_dict = torch.load(yolov4conv137weight)\n\n            model_dict = _model.state_dict()\n            # 1. filter out unnecessary keys\n            pretrained_dict = {k1: v for (k, v), k1 in zip(pretrained_dict.items(), model_dict)}\n            # 2. overwrite entries in the existing state dict\n            model_dict.update(pretrained_dict)\n            _model.load_state_dict(model_dict)\n        # head\n        self.head = Yolov4Head(output_ch)\n\n    def forward(self, input):\n        d1 = self.down1(input)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n\n        x20, x13, x6 = self.neck(d5, d4, d3)\n\n        output = self.head(x20, x13, x6)\n        return output\n\n\nif __name__ == ""__main__"":\n    import sys\n    from PIL import Image\n\n    namesfile = None\n    if len(sys.argv) == 4:\n        n_classes = int(sys.argv[1])\n        weightfile = sys.argv[2]\n        imgfile = sys.argv[3]\n    elif len(sys.argv) == 5:\n        n_classes = int(sys.argv[1])\n        weightfile = sys.argv[2]\n        imgfile = sys.argv[3]\n        namesfile = sys.argv[4]\n    else:\n        print(\'Usage: \')\n        print(\'  python models.py num_classes weightfile imgfile namefile\')\n\n    model = Yolov4(n_classes=n_classes)\n\n    pretrained_dict = torch.load(weightfile, map_location=torch.device(\'cpu\'))\n    model.load_state_dict(pretrained_dict)\n\n    if namesfile == None:\n        if n_classes == 20:\n            namesfile = \'data/voc.names\'\n        elif n_classes == 80:\n            namesfile = \'data/coco.names\'\n        else:\n            print(""please give namefile"")\n\n    use_cuda = 0\n    if use_cuda:\n        model.cuda()\n\n    img = Image.open(imgfile).convert(\'RGB\')\n    sized = img.resize((608, 608))\n    from tool.utils import load_class_names, plot_boxes\n    from tool.torch_utils import do_detect\n\n    boxes = do_detect(model, sized, 0.5, n_classes,0.4, use_cuda)\n\n    class_names = load_class_names(namesfile)\n    plot_boxes(img, boxes, \'predictions.jpg\', class_names)\n'"
train.py,44,"b'# -*- coding: utf-8 -*-\n\'\'\'\n@Time          : 2020/05/06 15:07\n@Author        : Tianxiaomo\n@File          : train.py\n@Noice         :\n@Modificattion :\n    @Author    :\n    @Time      :\n    @Detail    :\n\n\'\'\'\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch import optim\nfrom tensorboardX import SummaryWriter\nimport logging\nimport os, sys\nfrom tqdm import tqdm\nfrom dataset import Yolo_dataset\nfrom cfg import Cfg\nfrom models import Yolov4\nimport argparse\nfrom easydict import EasyDict as edict\nfrom torch.nn import functional as F\n\nimport numpy as np\n\n\ndef bboxes_iou(bboxes_a, bboxes_b, xyxy=True):\n    """"""Calculate the Intersection of Unions (IoUs) between bounding boxes.\n    IoU is calculated as a ratio of area of the intersection\n    and area of the union.\n\n    Args:\n        bbox_a (array): An array whose shape is :math:`(N, 4)`.\n            :math:`N` is the number of bounding boxes.\n            The dtype should be :obj:`numpy.float32`.\n        bbox_b (array): An array similar to :obj:`bbox_a`,\n            whose shape is :math:`(K, 4)`.\n            The dtype should be :obj:`numpy.float32`.\n    Returns:\n        array:\n        An array whose shape is :math:`(N, K)`. \\\n        An element at index :math:`(n, k)` contains IoUs between \\\n        :math:`n` th bounding box in :obj:`bbox_a` and :math:`k` th bounding \\\n        box in :obj:`bbox_b`.\n\n    from: https://github.com/chainer/chainercv\n    """"""\n    if bboxes_a.shape[1] != 4 or bboxes_b.shape[1] != 4:\n        raise IndexError\n\n    # top left\n    if xyxy:\n        tl = torch.max(bboxes_a[:, None, :2], bboxes_b[:, :2])\n        # bottom right\n        br = torch.min(bboxes_a[:, None, 2:], bboxes_b[:, 2:])\n        area_a = torch.prod(bboxes_a[:, 2:] - bboxes_a[:, :2], 1)\n        area_b = torch.prod(bboxes_b[:, 2:] - bboxes_b[:, :2], 1)\n    else:\n        tl = torch.max((bboxes_a[:, None, :2] - bboxes_a[:, None, 2:] / 2),\n                       (bboxes_b[:, :2] - bboxes_b[:, 2:] / 2))\n        # bottom right\n        br = torch.min((bboxes_a[:, None, :2] + bboxes_a[:, None, 2:] / 2),\n                       (bboxes_b[:, :2] + bboxes_b[:, 2:] / 2))\n\n        area_a = torch.prod(bboxes_a[:, 2:], 1)\n        area_b = torch.prod(bboxes_b[:, 2:], 1)\n    en = (tl < br).type(tl.type()).prod(dim=2)\n    area_i = torch.prod(br - tl, 2) * en  # * ((tl < br).all())\n    return area_i / (area_a[:, None] + area_b - area_i)\n\n\ndef bboxes_giou(bboxes_a, bboxes_b, xyxy=True):\n    pass\n\n\ndef bboxes_diou(bboxes_a, bboxes_b, xyxy=True):\n    pass\n\n\ndef bboxes_ciou(bboxes_a, bboxes_b, xyxy=True):\n    pass\n\n\nclass Yolo_loss(nn.Module):\n    def __init__(self, n_classes=80, n_anchors=3, device=None, batch=2):\n        super(Yolo_loss, self).__init__()\n        self.device = device\n        self.strides = [8, 16, 32]\n        image_size = 608\n        self.n_classes = n_classes\n        self.n_anchors = n_anchors\n\n        self.anchors = [[12, 16], [19, 36], [40, 28], [36, 75], [76, 55], [72, 146], [142, 110], [192, 243], [459, 401]]\n        self.anch_masks = [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n        self.ignore_thre = 0.5\n\n        self.masked_anchors, self.ref_anchors, self.grid_x, self.grid_y, self.anchor_w, self.anchor_h = [], [], [], [], [], []\n\n        for i in range(3):\n            all_anchors_grid = [(w / self.strides[i], h / self.strides[i]) for w, h in self.anchors]\n            masked_anchors = np.array([all_anchors_grid[j] for j in self.anch_masks[i]], dtype=np.float32)\n            ref_anchors = np.zeros((len(all_anchors_grid), 4), dtype=np.float32)\n            ref_anchors[:, 2:] = np.array(all_anchors_grid, dtype=np.float32)\n            ref_anchors = torch.from_numpy(ref_anchors)\n            # calculate pred - xywh obj cls\n            fsize = image_size // self.strides[i]\n            grid_x = torch.arange(fsize, dtype=torch.float).repeat(batch, 3, fsize, 1).to(device)\n            grid_y = torch.arange(fsize, dtype=torch.float).repeat(batch, 3, fsize, 1).permute(0, 1, 3, 2).to(device)\n            anchor_w = torch.from_numpy(masked_anchors[:, 0]).repeat(batch, fsize, fsize, 1).permute(0, 3, 1, 2).to(\n                device)\n            anchor_h = torch.from_numpy(masked_anchors[:, 1]).repeat(batch, fsize, fsize, 1).permute(0, 3, 1, 2).to(\n                device)\n\n            self.masked_anchors.append(masked_anchors)\n            self.ref_anchors.append(ref_anchors)\n            self.grid_x.append(grid_x)\n            self.grid_y.append(grid_y)\n            self.anchor_w.append(anchor_w)\n            self.anchor_h.append(anchor_h)\n\n    def build_target(self, pred, labels, batchsize, fsize, n_ch, output_id):\n        # target assignment\n        tgt_mask = torch.zeros(batchsize, self.n_anchors, fsize, fsize, 4 + self.n_classes).to(device=self.device)\n        obj_mask = torch.ones(batchsize, self.n_anchors, fsize, fsize).to(device=self.device)\n        tgt_scale = torch.zeros(batchsize, self.n_anchors, fsize, fsize, 2).to(self.device)\n        target = torch.zeros(batchsize, self.n_anchors, fsize, fsize, n_ch).to(self.device)\n\n        # labels = labels.cpu().data\n        nlabel = (labels.sum(dim=2) > 0).sum(dim=1)  # number of objects\n\n        truth_x_all = (labels[:, :, 2] + labels[:, :, 0]) / (self.strides[output_id] * 2)\n        truth_y_all = (labels[:, :, 3] + labels[:, :, 1]) / (self.strides[output_id] * 2)\n        truth_w_all = (labels[:, :, 2] - labels[:, :, 0]) / self.strides[output_id]\n        truth_h_all = (labels[:, :, 3] - labels[:, :, 1]) / self.strides[output_id]\n        truth_i_all = truth_x_all.to(torch.int16).cpu().numpy()\n        truth_j_all = truth_y_all.to(torch.int16).cpu().numpy()\n\n        for b in range(batchsize):\n            n = int(nlabel[b])\n            if n == 0:\n                continue\n            truth_box = torch.zeros(n, 4).to(self.device)\n            truth_box[:n, 2] = truth_w_all[b, :n]\n            truth_box[:n, 3] = truth_h_all[b, :n]\n            truth_i = truth_i_all[b, :n]\n            truth_j = truth_j_all[b, :n]\n\n            # calculate iou between truth and reference anchors\n            anchor_ious_all = bboxes_iou(truth_box.cpu(), self.ref_anchors[output_id])\n            best_n_all = anchor_ious_all.argmax(dim=1)\n            best_n = best_n_all % 3\n            best_n_mask = ((best_n_all == self.anch_masks[output_id][0]) |\n                           (best_n_all == self.anch_masks[output_id][1]) |\n                           (best_n_all == self.anch_masks[output_id][2]))\n\n            if sum(best_n_mask) == 0:\n                continue\n\n            truth_box[:n, 0] = truth_x_all[b, :n]\n            truth_box[:n, 1] = truth_y_all[b, :n]\n\n            pred_ious = bboxes_iou(pred[b].view(-1, 4), truth_box, xyxy=False)\n            pred_best_iou, _ = pred_ious.max(dim=1)\n            pred_best_iou = (pred_best_iou > self.ignore_thre)\n            pred_best_iou = pred_best_iou.view(pred[b].shape[:3])\n            # set mask to zero (ignore) if pred matches truth\n            obj_mask[b] = ~ pred_best_iou\n\n            for ti in range(best_n.shape[0]):\n                if best_n_mask[ti] == 1:\n                    i, j = truth_i[ti], truth_j[ti]\n                    a = best_n[ti]\n                    obj_mask[b, a, j, i] = 1\n                    tgt_mask[b, a, j, i, :] = 1\n                    target[b, a, j, i, 0] = truth_x_all[b, ti] - truth_x_all[b, ti].to(torch.int16).to(torch.float)\n                    target[b, a, j, i, 1] = truth_y_all[b, ti] - truth_y_all[b, ti].to(torch.int16).to(torch.float)\n                    target[b, a, j, i, 2] = torch.log(\n                        truth_w_all[b, ti] / torch.Tensor(self.masked_anchors[output_id])[best_n[ti], 0] + 1e-16)\n                    target[b, a, j, i, 3] = torch.log(\n                        truth_h_all[b, ti] / torch.Tensor(self.masked_anchors[output_id])[best_n[ti], 1] + 1e-16)\n                    target[b, a, j, i, 4] = 1\n                    target[b, a, j, i, 5 + labels[b, ti, 4].to(torch.int16).cpu().numpy()] = 1\n                    tgt_scale[b, a, j, i, :] = torch.sqrt(2 - truth_w_all[b, ti] * truth_h_all[b, ti] / fsize / fsize)\n        return obj_mask, tgt_mask, tgt_scale, target\n\n    def forward(self, xin, labels=None):\n        loss, loss_xy, loss_wh, loss_obj, loss_cls, loss_l2 = 0, 0, 0, 0, 0, 0\n        for output_id, output in enumerate(xin):\n            batchsize = output.shape[0]\n            fsize = output.shape[2]\n            n_ch = 5 + self.n_classes\n\n            output = output.view(batchsize, self.n_anchors, n_ch, fsize, fsize)\n            output = output.permute(0, 1, 3, 4, 2)  # .contiguous()\n\n            # logistic activation for xy, obj, cls\n            output[..., np.r_[:2, 4:n_ch]] = torch.sigmoid(output[..., np.r_[:2, 4:n_ch]])\n\n            pred = output[..., :4].clone()\n            pred[..., 0] += self.grid_x[output_id]\n            pred[..., 1] += self.grid_y[output_id]\n            pred[..., 2] = torch.exp(pred[..., 2]) * self.anchor_w[output_id]\n            pred[..., 3] = torch.exp(pred[..., 3]) * self.anchor_h[output_id]\n\n            obj_mask, tgt_mask, tgt_scale, target = self.build_target(pred, labels, batchsize, fsize, n_ch, output_id)\n\n            # loss calculation\n            output[..., 4] *= obj_mask\n            output[..., np.r_[0:4, 5:n_ch]] *= tgt_mask\n            output[..., 2:4] *= tgt_scale\n\n            target[..., 4] *= obj_mask\n            target[..., np.r_[0:4, 5:n_ch]] *= tgt_mask\n            target[..., 2:4] *= tgt_scale\n\n            loss_xy += F.binary_cross_entropy(input=output[..., :2], target=target[..., :2],\n                                              weight=tgt_scale * tgt_scale, size_average=False)\n            loss_wh += F.mse_loss(input=output[..., 2:4], target=target[..., 2:4], size_average=False) / 2\n            loss_obj += F.binary_cross_entropy(input=output[..., 4], target=target[..., 4], size_average=False)\n            loss_cls += F.binary_cross_entropy(input=output[..., 5:], target=target[..., 5:], size_average=False)\n            loss_l2 += F.mse_loss(input=output, target=target, size_average=False)\n\n        loss = loss_xy + loss_wh + loss_obj + loss_cls\n\n        return loss, loss_xy, loss_wh, loss_obj, loss_cls, loss_l2\n\n\ndef collate(batch):\n    images = []\n    bboxes = []\n    for img, box in batch:\n        images.append([img])\n        bboxes.append([box])\n    images = np.concatenate(images, axis=0)\n    images = images.transpose(0, 3, 1, 2)\n    images = torch.from_numpy(images).div(255.0)\n    bboxes = np.concatenate(bboxes, axis=0)\n    bboxes = torch.from_numpy(bboxes)\n    return images, bboxes\n\n\ndef train(model, device, config, epochs=5, batch_size=1, save_cp=True, log_step=20, img_scale=0.5):\n    train_dataset = Yolo_dataset(config.train_label, config)\n    val_dataset = Yolo_dataset(config.val_label, config)\n\n    n_train = len(train_dataset)\n    n_val = len(val_dataset)\n\n    train_loader = DataLoader(train_dataset, batch_size=config.batch // config.subdivisions, shuffle=True,\n                              num_workers=8, pin_memory=True, drop_last=True, collate_fn=collate)\n\n    val_loader = DataLoader(val_dataset, batch_size=config.batch // config.subdivisions, shuffle=True, num_workers=8,\n                            pin_memory=True, drop_last=True)\n\n    writer = SummaryWriter(log_dir=config.TRAIN_TENSORBOARD_DIR,\n                           filename_suffix=f\'OPT_{config.TRAIN_OPTIMIZER}_LR_{config.learning_rate}_BS_{config.batch}_Sub_{config.subdivisions}_Size_{config.width}\',\n                           comment=f\'OPT_{config.TRAIN_OPTIMIZER}_LR_{config.learning_rate}_BS_{config.batch}_Sub_{config.subdivisions}_Size_{config.width}\')\n    # writer.add_images(\'legend\',\n    #                   torch.from_numpy(train_dataset.label2colorlegend2(cfg.DATA_CLASSES).transpose([2, 0, 1])).to(\n    #                       device).unsqueeze(0))\n    max_itr = config.TRAIN_EPOCHS * n_train\n    # global_step = cfg.TRAIN_MINEPOCH * n_train\n    global_step = 0\n    logging.info(f\'\'\'Starting training:\n        Epochs:          {epochs}\n        Batch size:      {config.batch}\n        Subdivisions:    {config.subdivisions}\n        Learning rate:   {config.learning_rate}\n        Training size:   {n_train}\n        Validation size: {n_val}\n        Checkpoints:     {save_cp}\n        Device:          {device.type}\n        Images size:     {config.width}\n        Optimizer:       {config.TRAIN_OPTIMIZER}\n        Dataset classes: {config.classes}\n        Train label path:{config.train_label}\n        Pretrained:\n    \'\'\')\n\n    # learning rate setup\n    def burnin_schedule(i):\n        if i < config.burn_in:\n            factor = pow(i / config.burn_in, 4)\n        elif i < config.steps[0]:\n            factor = 1.0\n        elif i < config.steps[1]:\n            factor = 0.1\n        else:\n            factor = 0.01\n        return factor\n\n    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate / config.batch, betas=(0.9, 0.999), eps=1e-08)\n    scheduler = optim.lr_scheduler.LambdaLR(optimizer, burnin_schedule)\n\n    criterion = Yolo_loss(device=device, batch=config.batch // config.subdivisions,n_classes=config.classes)\n    # scheduler = ReduceLROnPlateau(optimizer, mode=\'max\', verbose=True, patience=6, min_lr=1e-7)\n    # scheduler = CosineAnnealingWarmRestarts(optimizer, 0.001, 1e-6, 20)\n\n    model.train()\n    for epoch in range(epochs):\n        #model.train()\n        epoch_loss = 0\n        epoch_step = 0\n\n        with tqdm(total=n_train, desc=f\'Epoch {epoch + 1}/{epochs}\', unit=\'img\', ncols=50) as pbar:\n            for i, batch in enumerate(train_loader):\n                global_step += 1\n                epoch_step += 1\n                images = batch[0]\n                bboxes = batch[1]\n\n                images = images.to(device=device, dtype=torch.float32)\n                bboxes = bboxes.to(device=device)\n\n                bboxes_pred = model(images)\n                loss, loss_xy, loss_wh, loss_obj, loss_cls, loss_l2 = criterion(bboxes_pred, bboxes)\n                # loss = loss / config.subdivisions\n                loss.backward()\n\n                epoch_loss += loss.item()\n\n                if global_step  % config.subdivisions == 0:\n                    optimizer.step()\n                    scheduler.step()\n                    model.zero_grad()\n\n                if global_step % (log_step * config.subdivisions) == 0:\n                    writer.add_scalar(\'train/Loss\', loss.item(), global_step)\n                    writer.add_scalar(\'train/loss_xy\', loss_xy.item(), global_step)\n                    writer.add_scalar(\'train/loss_wh\', loss_wh.item(), global_step)\n                    writer.add_scalar(\'train/loss_obj\', loss_obj.item(), global_step)\n                    writer.add_scalar(\'train/loss_cls\', loss_cls.item(), global_step)\n                    writer.add_scalar(\'train/loss_l2\', loss_l2.item(), global_step)\n                    writer.add_scalar(\'lr\', scheduler.get_lr()[0] * config.batch, global_step)\n                    pbar.set_postfix(**{\'loss (batch)\': loss.item(), \'loss_xy\': loss_xy.item(),\n                                        \'loss_wh\': loss_wh.item(),\n                                        \'loss_obj\': loss_obj.item(),\n                                        \'loss_cls\': loss_cls.item(),\n                                        \'loss_l2\': loss_l2.item(),\n                                        \'lr\': scheduler.get_lr()[0] * config.batch\n                                        })\n                    logging.debug(\'Train step_{}: loss : {},loss xy : {},loss wh : {},\'\n                                  \'loss obj : {}\xef\xbc\x8closs cls : {},loss l2 : {},lr : {}\'\n                                  .format(global_step, loss.item(), loss_xy.item(),\n                                          loss_wh.item(), loss_obj.item(),\n                                          loss_cls.item(), loss_l2.item(),\n                                          scheduler.get_lr()[0] * config.batch))\n\n                pbar.update(images.shape[0])\n\n            if save_cp:\n                try:\n                    os.mkdir(config.checkpoints)\n                    logging.info(\'Created checkpoint directory\')\n                except OSError:\n                    pass\n                torch.save(model.state_dict(), os.path.join(config.checkpoints, f\'Yolov4_epoch{epoch + 1}.pth\'))\n                logging.info(f\'Checkpoint {epoch + 1} saved !\')\n\n    writer.close()\n\n\ndef get_args(**kwargs):\n    cfg = kwargs\n    parser = argparse.ArgumentParser(description=\'Train the Model on images and target masks\',\n                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    # parser.add_argument(\'-b\', \'--batch-size\', metavar=\'B\', type=int, nargs=\'?\', default=2,\n    #                     help=\'Batch size\', dest=\'batchsize\')\n    parser.add_argument(\'-l\', \'--learning-rate\', metavar=\'LR\', type=float, nargs=\'?\', default=0.001,\n                        help=\'Learning rate\', dest=\'learning_rate\')\n    parser.add_argument(\'-f\', \'--load\', dest=\'load\', type=str, default=None,\n                        help=\'Load model from a .pth file\')\n    parser.add_argument(\'-g\', \'--gpu\', metavar=\'G\', type=str, default=\'-1\',\n                        help=\'GPU\', dest=\'gpu\')\n    parser.add_argument(\'-dir\', \'--data-dir\', type=str, default=None,\n                        help=\'dataset dir\', dest=\'dataset_dir\')\n    parser.add_argument(\'-pretrained\',type=str,default=None,help=\'pretrained yolov4.conv.137\')\n    parser.add_argument(\'-classes\',type=int,default=80,help=\'dataset classes\')\n    parser.add_argument(\'-train_label_path\',dest=\'train_label\',type=str,default=\'train.txt\',help=""train label path"")\n    args = vars(parser.parse_args())\n\n    for k in args.keys():\n        cfg[k] = args.get(k)\n    return edict(cfg)\n\n\ndef init_logger(log_file=None, log_dir=None, log_level=logging.INFO, mode=\'w\', stdout=True):\n    """"""\n    log_dir: \xe6\x97\xa5\xe5\xbf\x97\xe6\x96\x87\xe4\xbb\xb6\xe7\x9a\x84\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xe8\xb7\xaf\xe5\xbe\x84\n    mode: \'a\', append; \'w\', \xe8\xa6\x86\xe7\x9b\x96\xe5\x8e\x9f\xe6\x96\x87\xe4\xbb\xb6\xe5\x86\x99\xe5\x85\xa5.\n    """"""\n    import datetime\n    def get_date_str():\n        now = datetime.datetime.now()\n        return now.strftime(\'%Y-%m-%d_%H-%M-%S\')\n\n    fmt = \'%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s: %(message)s\'\n    if log_dir is None:\n        log_dir = \'~/temp/log/\'\n    if log_file is None:\n        log_file = \'log_\' + get_date_str() + \'.txt\'\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n    log_file = os.path.join(log_dir, log_file)\n    # \xe6\xad\xa4\xe5\xa4\x84\xe4\xb8\x8d\xe8\x83\xbd\xe4\xbd\xbf\xe7\x94\xa8logging\xe8\xbe\x93\xe5\x87\xba\n    print(\'log file path:\' + log_file)\n\n    logging.basicConfig(level=logging.DEBUG,\n                        format=fmt,\n                        filename=log_file,\n                        filemode=mode)\n\n    if stdout:\n        console = logging.StreamHandler(stream=sys.stdout)\n        console.setLevel(log_level)\n        formatter = logging.Formatter(fmt)\n        console.setFormatter(formatter)\n        logging.getLogger(\'\').addHandler(console)\n\n    return logging\n\n\nif __name__ == ""__main__"":\n    logging = init_logger(log_dir=\'log\')\n    cfg = get_args(**Cfg)\n    os.environ[""CUDA_VISIBLE_DEVICES""] = cfg.gpu\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    logging.info(f\'Using device {device}\')\n\n    model = Yolov4(cfg.pretrained,n_classes=cfg.classes)\n\n    if torch.cuda.device_count() > 1:\n        model = torch.nn.DataParallel(model)\n    model.to(device=device)\n\n    try:\n        train(model=model,\n              config=cfg,\n              epochs=cfg.TRAIN_EPOCHS,\n              device=device, )\n    except KeyboardInterrupt:\n        torch.save(model.state_dict(), \'INTERRUPTED.pth\')\n        logging.info(\'Saved interrupt\')\n        try:\n            sys.exit(0)\n        except SystemExit:\n            os._exit(0)\n'"
tool/__init__.py,0,b''
tool/camera.py,1,"b'# -*- coding: utf-8 -*-\n\'\'\'\n@Time          : 2020/04/26 15:48\n@Author        : Tianxiaomo\n@File          : camera.py\n@Noice         :\n@Modificattion :\n    @Author    :\n    @Time      :\n    @Detail    :\n\n\'\'\'\nfrom __future__ import division\nimport cv2\nfrom tool.darknet2pytorch import Darknet\nimport argparse\nfrom tool.utils import *\nfrom tool.torch_utils import *\n\n\ndef arg_parse():\n    """"""\n    Parse arguements to the detect module\n\n    """"""\n\n    parser = argparse.ArgumentParser(description=\'YOLO v3 Cam Demo\')\n    parser.add_argument(""--confidence"", dest=""confidence"", help=""Object Confidence to filter predictions"", default=0.25)\n    parser.add_argument(""--nms_thresh"", dest=""nms_thresh"", help=""NMS Threshhold"", default=0.4)\n    parser.add_argument(""--reso"", dest=\'reso\', help=\n    ""Input resolution of the network. Increase to increase accuracy. Decrease to increase speed"",\n                        default=""160"", type=str)\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    cfgfile = ""cfg/yolov4.cfg""\n    weightsfile = ""weight/yolov4.weights""\n\n    args = arg_parse()\n    confidence = float(args.confidence)\n    nms_thesh = float(args.nms_thresh)\n    CUDA = torch.cuda.is_available()\n    num_classes = 80\n    bbox_attrs = 5 + num_classes\n    class_names = load_class_names(""data/coco.names"")\n\n    model = Darknet(cfgfile)\n    model.load_weights(weightsfile)\n\n    if CUDA:\n        model.cuda()\n\n    model.eval()\n    cap = cv2.VideoCapture(0)\n\n    assert cap.isOpened(), \'Cannot capture source\'\n\n    frames = 0\n    start = time.time()\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if ret:\n            sized = cv2.resize(frame, (model.width, model.height))\n            sized = cv2.cvtColor(sized, cv2.COLOR_BGR2RGB)\n            boxes = do_detect(model, sized, 0.5, 0.4, CUDA)\n\n            orig_im = plot_boxes_cv2(frame, boxes, class_names=class_names)\n\n            cv2.imshow(""frame"", orig_im)\n            key = cv2.waitKey(1)\n            if key & 0xFF == ord(\'q\'):\n                break\n            frames += 1\n            print(""FPS of the video is {:5.2f}"".format(frames / (time.time() - start)))\n        else:\n            break\n'"
tool/coco_annotation.py,0,"b'# -*- coding: utf-8 -*-\r\n\'\'\'\r\n@Time          : 2020/05/08 11:45\r\n@Author        : Tianxiaomo\r\n@File          : coco_annotatin.py\r\n@Noice         :\r\n@Modificattion :\r\n    @Author    :\r\n    @Time      :\r\n    @Detail    :\r\n\r\n\'\'\'\r\nimport json\r\nfrom collections import defaultdict\r\nfrom tqdm import tqdm\r\nimport os\r\n\r\n""""""hyper parameters""""""\r\njson_file_path = \'E:/Dataset/coco2017/annotations_trainval2017/annotations/instances_val2017.json\'\r\nimages_dir_path = \'mscoco2017/train2017/\'\r\noutput_path = \'../data/val.txt\'\r\n\r\n""""""load json file""""""\r\nname_box_id = defaultdict(list)\r\nid_name = dict()\r\nwith open(json_file_path, encoding=\'utf-8\') as f:\r\n    data = json.load(f)\r\n\r\n""""""generate labels""""""\r\nimages = data[\'images\']\r\nannotations = data[\'annotations\']\r\nfor ant in tqdm(annotations):\r\n    id = ant[\'image_id\']\r\n    name = os.path.join(images_dir_path, images[id][\'file_name\'])\r\n    cat = ant[\'category_id\']\r\n\r\n    if cat >= 1 and cat <= 11:\r\n        cat = cat - 1\r\n    elif cat >= 13 and cat <= 25:\r\n        cat = cat - 2\r\n    elif cat >= 27 and cat <= 28:\r\n        cat = cat - 3\r\n    elif cat >= 31 and cat <= 44:\r\n        cat = cat - 5\r\n    elif cat >= 46 and cat <= 65:\r\n        cat = cat - 6\r\n    elif cat == 67:\r\n        cat = cat - 7\r\n    elif cat == 70:\r\n        cat = cat - 9\r\n    elif cat >= 72 and cat <= 82:\r\n        cat = cat - 10\r\n    elif cat >= 84 and cat <= 90:\r\n        cat = cat - 11\r\n\r\n    name_box_id[name].append([ant[\'bbox\'], cat])\r\n\r\n""""""write to txt""""""\r\nwith open(output_path, \'w\') as f:\r\n    for key in tqdm(name_box_id.keys()):\r\n        f.write(key)\r\n        box_infos = name_box_id[key]\r\n        for info in box_infos:\r\n            x_min = int(info[0][0])\r\n            y_min = int(info[0][1])\r\n            x_max = x_min + int(info[0][2])\r\n            y_max = y_min + int(info[0][3])\r\n\r\n            box_info = "" %d,%d,%d,%d,%d"" % (\r\n                x_min, y_min, x_max, y_max, int(info[1]))\r\n            f.write(box_info)\r\n        f.write(\'\\n\')\r\n'"
tool/config.py,9,"b'import torch\nfrom tool.torch_utils import convert2cpu\n\n\ndef parse_cfg(cfgfile):\n    blocks = []\n    fp = open(cfgfile, \'r\')\n    block = None\n    line = fp.readline()\n    while line != \'\':\n        line = line.rstrip()\n        if line == \'\' or line[0] == \'#\':\n            line = fp.readline()\n            continue\n        elif line[0] == \'[\':\n            if block:\n                blocks.append(block)\n            block = dict()\n            block[\'type\'] = line.lstrip(\'[\').rstrip(\']\')\n            # set default value\n            if block[\'type\'] == \'convolutional\':\n                block[\'batch_normalize\'] = 0\n        else:\n            key, value = line.split(\'=\')\n            key = key.strip()\n            if key == \'type\':\n                key = \'_type\'\n            value = value.strip()\n            block[key] = value\n        line = fp.readline()\n\n    if block:\n        blocks.append(block)\n    fp.close()\n    return blocks\n\n\ndef print_cfg(blocks):\n    print(\'layer     filters    size              input                output\');\n    prev_width = 416\n    prev_height = 416\n    prev_filters = 3\n    out_filters = []\n    out_widths = []\n    out_heights = []\n    ind = -2\n    for block in blocks:\n        ind = ind + 1\n        if block[\'type\'] == \'net\':\n            prev_width = int(block[\'width\'])\n            prev_height = int(block[\'height\'])\n            continue\n        elif block[\'type\'] == \'convolutional\':\n            filters = int(block[\'filters\'])\n            kernel_size = int(block[\'size\'])\n            stride = int(block[\'stride\'])\n            is_pad = int(block[\'pad\'])\n            pad = (kernel_size - 1) // 2 if is_pad else 0\n            width = (prev_width + 2 * pad - kernel_size) // stride + 1\n            height = (prev_height + 2 * pad - kernel_size) // stride + 1\n            print(\'%5d %-6s %4d  %d x %d / %d   %3d x %3d x%4d   ->   %3d x %3d x%4d\' % (\n                ind, \'conv\', filters, kernel_size, kernel_size, stride, prev_width, prev_height, prev_filters, width,\n                height, filters))\n            prev_width = width\n            prev_height = height\n            prev_filters = filters\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block[\'type\'] == \'maxpool\':\n            pool_size = int(block[\'size\'])\n            stride = int(block[\'stride\'])\n            width = prev_width // stride\n            height = prev_height // stride\n            print(\'%5d %-6s       %d x %d / %d   %3d x %3d x%4d   ->   %3d x %3d x%4d\' % (\n                ind, \'max\', pool_size, pool_size, stride, prev_width, prev_height, prev_filters, width, height,\n                filters))\n            prev_width = width\n            prev_height = height\n            prev_filters = filters\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block[\'type\'] == \'avgpool\':\n            width = 1\n            height = 1\n            print(\'%5d %-6s                   %3d x %3d x%4d   ->  %3d\' % (\n                ind, \'avg\', prev_width, prev_height, prev_filters, prev_filters))\n            prev_width = width\n            prev_height = height\n            prev_filters = filters\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block[\'type\'] == \'softmax\':\n            print(\'%5d %-6s                                    ->  %3d\' % (ind, \'softmax\', prev_filters))\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block[\'type\'] == \'cost\':\n            print(\'%5d %-6s                                     ->  %3d\' % (ind, \'cost\', prev_filters))\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block[\'type\'] == \'reorg\':\n            stride = int(block[\'stride\'])\n            filters = stride * stride * prev_filters\n            width = prev_width // stride\n            height = prev_height // stride\n            print(\'%5d %-6s             / %d   %3d x %3d x%4d   ->   %3d x %3d x%4d\' % (\n                ind, \'reorg\', stride, prev_width, prev_height, prev_filters, width, height, filters))\n            prev_width = width\n            prev_height = height\n            prev_filters = filters\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block[\'type\'] == \'upsample\':\n            stride = int(block[\'stride\'])\n            filters = prev_filters\n            width = prev_width * stride\n            height = prev_height * stride\n            print(\'%5d %-6s           * %d   %3d x %3d x%4d   ->   %3d x %3d x%4d\' % (\n                ind, \'upsample\', stride, prev_width, prev_height, prev_filters, width, height, filters))\n            prev_width = width\n            prev_height = height\n            prev_filters = filters\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block[\'type\'] == \'route\':\n            layers = block[\'layers\'].split(\',\')\n            layers = [int(i) if int(i) > 0 else int(i) + ind for i in layers]\n            if len(layers) == 1:\n                print(\'%5d %-6s %d\' % (ind, \'route\', layers[0]))\n                prev_width = out_widths[layers[0]]\n                prev_height = out_heights[layers[0]]\n                prev_filters = out_filters[layers[0]]\n            elif len(layers) == 2:\n                print(\'%5d %-6s %d %d\' % (ind, \'route\', layers[0], layers[1]))\n                prev_width = out_widths[layers[0]]\n                prev_height = out_heights[layers[0]]\n                assert (prev_width == out_widths[layers[1]])\n                assert (prev_height == out_heights[layers[1]])\n                prev_filters = out_filters[layers[0]] + out_filters[layers[1]]\n            elif len(layers) == 4:\n                print(\'%5d %-6s %d %d %d %d\' % (ind, \'route\', layers[0], layers[1], layers[2], layers[3]))\n                prev_width = out_widths[layers[0]]\n                prev_height = out_heights[layers[0]]\n                assert (prev_width == out_widths[layers[1]] == out_widths[layers[2]] == out_widths[layers[3]])\n                assert (prev_height == out_heights[layers[1]] == out_heights[layers[2]] == out_heights[layers[3]])\n                prev_filters = out_filters[layers[0]] + out_filters[layers[1]] + out_filters[layers[2]] + out_filters[\n                    layers[3]]\n            else:\n                print(""route error !!! {} {} {}"".format(sys._getframe().f_code.co_filename,\n                                                        sys._getframe().f_code.co_name, sys._getframe().f_lineno))\n\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block[\'type\'] in [\'region\', \'yolo\']:\n            print(\'%5d %-6s\' % (ind, \'detection\'))\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block[\'type\'] == \'shortcut\':\n            from_id = int(block[\'from\'])\n            from_id = from_id if from_id > 0 else from_id + ind\n            print(\'%5d %-6s %d\' % (ind, \'shortcut\', from_id))\n            prev_width = out_widths[from_id]\n            prev_height = out_heights[from_id]\n            prev_filters = out_filters[from_id]\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block[\'type\'] == \'connected\':\n            filters = int(block[\'output\'])\n            print(\'%5d %-6s                            %d  ->  %3d\' % (ind, \'connected\', prev_filters, filters))\n            prev_filters = filters\n            out_widths.append(1)\n            out_heights.append(1)\n            out_filters.append(prev_filters)\n        else:\n            print(\'unknown type %s\' % (block[\'type\']))\n\n\ndef load_conv(buf, start, conv_model):\n    num_w = conv_model.weight.numel()\n    num_b = conv_model.bias.numel()\n    conv_model.bias.data.copy_(torch.from_numpy(buf[start:start + num_b]));\n    start = start + num_b\n    conv_model.weight.data.copy_(torch.from_numpy(buf[start:start + num_w]).reshape(conv_model.weight.data.shape));\n    start = start + num_w\n    return start\n\n\ndef save_conv(fp, conv_model):\n    if conv_model.bias.is_cuda:\n        convert2cpu(conv_model.bias.data).numpy().tofile(fp)\n        convert2cpu(conv_model.weight.data).numpy().tofile(fp)\n    else:\n        conv_model.bias.data.numpy().tofile(fp)\n        conv_model.weight.data.numpy().tofile(fp)\n\n\ndef load_conv_bn(buf, start, conv_model, bn_model):\n    num_w = conv_model.weight.numel()\n    num_b = bn_model.bias.numel()\n    bn_model.bias.data.copy_(torch.from_numpy(buf[start:start + num_b]));\n    start = start + num_b\n    bn_model.weight.data.copy_(torch.from_numpy(buf[start:start + num_b]));\n    start = start + num_b\n    bn_model.running_mean.copy_(torch.from_numpy(buf[start:start + num_b]));\n    start = start + num_b\n    bn_model.running_var.copy_(torch.from_numpy(buf[start:start + num_b]));\n    start = start + num_b\n    conv_model.weight.data.copy_(torch.from_numpy(buf[start:start + num_w]).reshape(conv_model.weight.data.shape));\n    start = start + num_w\n    return start\n\n\ndef save_conv_bn(fp, conv_model, bn_model):\n    if bn_model.bias.is_cuda:\n        convert2cpu(bn_model.bias.data).numpy().tofile(fp)\n        convert2cpu(bn_model.weight.data).numpy().tofile(fp)\n        convert2cpu(bn_model.running_mean).numpy().tofile(fp)\n        convert2cpu(bn_model.running_var).numpy().tofile(fp)\n        convert2cpu(conv_model.weight.data).numpy().tofile(fp)\n    else:\n        bn_model.bias.data.numpy().tofile(fp)\n        bn_model.weight.data.numpy().tofile(fp)\n        bn_model.running_mean.numpy().tofile(fp)\n        bn_model.running_var.numpy().tofile(fp)\n        conv_model.weight.data.numpy().tofile(fp)\n\n\ndef load_fc(buf, start, fc_model):\n    num_w = fc_model.weight.numel()\n    num_b = fc_model.bias.numel()\n    fc_model.bias.data.copy_(torch.from_numpy(buf[start:start + num_b]));\n    start = start + num_b\n    fc_model.weight.data.copy_(torch.from_numpy(buf[start:start + num_w]));\n    start = start + num_w\n    return start\n\n\ndef save_fc(fp, fc_model):\n    fc_model.bias.data.numpy().tofile(fp)\n    fc_model.weight.data.numpy().tofile(fp)\n\n\nif __name__ == \'__main__\':\n    import sys\n\n    blocks = parse_cfg(\'cfg/yolo.cfg\')\n    if len(sys.argv) == 2:\n        blocks = parse_cfg(sys.argv[1])\n    print_cfg(blocks)\n'"
tool/darknet2onnx.py,2,"b'import sys\nimport torch\nfrom tool.darknet2pytorch import Darknet\n\n\ndef fransform_to_onnx(cfgfile, weightfile, batch_size=1):\n    model = Darknet(cfgfile)\n\n    model.print_network()\n    model.load_weights(weightfile)\n    print(\'Loading weights from %s... Done!\' % (weightfile))\n\n    # model.cuda()\n\n    x = torch.randn((batch_size, 3, model.height, model.width), requires_grad=True)  # .cuda()\n\n    onnx_file_name = ""yolov4_{}_3_{}_{}.onnx"".format(batch_size, model.height, model.width)\n\n    # Export the model\n    print(\'Export the onnx model ...\')\n    torch.onnx.export(model,\n                      x,\n                      onnx_file_name,\n                      export_params=True,\n                      opset_version=11,\n                      do_constant_folding=True,\n                      # input_names=[\'input\'], output_names=[\'output_1\', \'output_2\', \'output_3\'],\n                      dynamic_axes=None)\n\n    print(\'Onnx model exporting done\')\n    return onnx_file_name\n\n\nif __name__ == \'__main__\':\n    if len(sys.argv) == 3:\n        cfgfile = sys.argv[1]\n        weightfile = sys.argv[2]\n        fransform_to_onnx(cfgfile, weightfile)\n    elif len(sys.argv) == 4:\n        cfgfile = sys.argv[1]\n        weightfile = sys.argv[2]\n        batch_size = int(sys.argv[3])\n        fransform_to_onnx(cfgfile, weightfile, batch_size)\n    else:\n        print(\'Please execute this script this way:\\n\')\n        print(\'  python darknet2onnx.py <cfgFile> <weightFile>\')\n        print(\'or\')\n        print(\'  python darknet2onnx.py <cfgFile> <weightFile> <batchSize>\')\n'"
tool/darknet2pytorch.py,8,"b'import torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom tool.region_loss import RegionLoss\nfrom tool.yolo_layer import YoloLayer\nfrom tool.config import *\n\n\nclass Mish(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        x = x * (torch.tanh(torch.nn.functional.softplus(x)))\n        return x\n\n\nclass MaxPoolDark(nn.Module):\n    def __init__(self, size=2, stride=1):\n        super(MaxPoolDark, self).__init__()\n        self.size = size\n        self.stride = stride\n\n    def forward(self, x):\n        \'\'\'\n        darknet output_size = (input_size + p - k) / s +1\n        p : padding = k - 1\n        k : size\n        s : stride\n        torch output_size = (input_size + 2*p -k) / s +1\n        p : padding = k//2\n        \'\'\'\n        p = self.size // 2\n        if ((x.shape[2] - 1) // self.stride) != ((x.shape[2] + 2 * p - self.size) // self.stride):\n            padding1 = (self.size - 1) // 2\n            padding2 = padding1 + 1\n        else:\n            padding1 = (self.size - 1) // 2\n            padding2 = padding1\n        if ((x.shape[3] - 1) // self.stride) != ((x.shape[3] + 2 * p - self.size) // self.stride):\n            padding3 = (self.size - 1) // 2\n            padding4 = padding3 + 1\n        else:\n            padding3 = (self.size - 1) // 2\n            padding4 = padding3\n        x = F.max_pool2d(F.pad(x, (padding3, padding4, padding1, padding2), mode=\'replicate\'),\n                         self.size, stride=self.stride)\n        return x\n\n\nclass Upsample_expand(nn.Module):\n    def __init__(self, stride=2):\n        super(Upsample_expand, self).__init__()\n        self.stride = stride\n\n    def forward(self, x):\n        stride = self.stride\n        assert (x.data.dim() == 4)\n        B = x.data.size(0)\n        C = x.data.size(1)\n        H = x.data.size(2)\n        W = x.data.size(3)\n        ws = stride\n        hs = stride\n        x = x.view(B, C, H, 1, W, 1).expand(B, C, H, stride, W, stride).contiguous().view(B, C, H * stride, W * stride)\n        return x\n\n\nclass Upsample_interpolate(nn.Module):\n    def __init__(self, stride):\n        super(Upsample_interpolate, self).__init__()\n        self.stride = stride\n\n    def forward(self, x):\n\n        x_numpy = x.cpu().detach().numpy()\n        H = x_numpy.shape[2]\n        W = x_numpy.shape[3]\n\n        H = H * self.stride\n        W = W * self.stride\n\n        out = F.interpolate(x, size=(H, W), mode=\'nearest\')\n        return out\n\n\nclass Reorg(nn.Module):\n    def __init__(self, stride=2):\n        super(Reorg, self).__init__()\n        self.stride = stride\n\n    def forward(self, x):\n        stride = self.stride\n        assert (x.data.dim() == 4)\n        B = x.data.size(0)\n        C = x.data.size(1)\n        H = x.data.size(2)\n        W = x.data.size(3)\n        assert (H % stride == 0)\n        assert (W % stride == 0)\n        ws = stride\n        hs = stride\n        x = x.view(B, C, H / hs, hs, W / ws, ws).transpose(3, 4).contiguous()\n        x = x.view(B, C, H / hs * W / ws, hs * ws).transpose(2, 3).contiguous()\n        x = x.view(B, C, hs * ws, H / hs, W / ws).transpose(1, 2).contiguous()\n        x = x.view(B, hs * ws * C, H / hs, W / ws)\n        return x\n\n\nclass GlobalAvgPool2d(nn.Module):\n    def __init__(self):\n        super(GlobalAvgPool2d, self).__init__()\n\n    def forward(self, x):\n        N = x.data.size(0)\n        C = x.data.size(1)\n        H = x.data.size(2)\n        W = x.data.size(3)\n        x = F.avg_pool2d(x, (H, W))\n        x = x.view(N, C)\n        return x\n\n\n# for route and shortcut\nclass EmptyModule(nn.Module):\n    def __init__(self):\n        super(EmptyModule, self).__init__()\n\n    def forward(self, x):\n        return x\n\n\n# support route shortcut and reorg\nclass Darknet(nn.Module):\n    def __init__(self, cfgfile):\n        super(Darknet, self).__init__()\n\n        self.blocks = parse_cfg(cfgfile)\n        self.width = int(self.blocks[0][\'width\'])\n        self.height = int(self.blocks[0][\'height\'])\n        \n        self.models = self.create_network(self.blocks)  # merge conv, bn,leaky\n        self.loss = self.models[len(self.models) - 1]\n\n        if self.blocks[(len(self.blocks) - 1)][\'type\'] == \'region\':\n            self.anchors = self.loss.anchors\n            self.num_anchors = self.loss.num_anchors\n            self.anchor_step = self.loss.anchor_step\n            self.num_classes = self.loss.num_classes\n\n        self.header = torch.IntTensor([0, 0, 0, 0])\n        self.seen = 0\n\n    def forward(self, x):\n        ind = -2\n        self.loss = None\n        outputs = dict()\n        out_boxes = []\n        for block in self.blocks:\n            ind = ind + 1\n            # if ind > 0:\n            #    return x\n\n            if block[\'type\'] == \'net\':\n                continue\n            elif block[\'type\'] in [\'convolutional\', \'maxpool\', \'reorg\', \'upsample\', \'avgpool\', \'softmax\', \'connected\']:\n                x = self.models[ind](x)\n                outputs[ind] = x\n            elif block[\'type\'] == \'route\':\n                layers = block[\'layers\'].split(\',\')\n                layers = [int(i) if int(i) > 0 else int(i) + ind for i in layers]\n                if len(layers) == 1:\n                    x = outputs[layers[0]]\n                    outputs[ind] = x\n                elif len(layers) == 2:\n                    x1 = outputs[layers[0]]\n                    x2 = outputs[layers[1]]\n                    x = torch.cat((x1, x2), 1)\n                    outputs[ind] = x\n                elif len(layers) == 4:\n                    x1 = outputs[layers[0]]\n                    x2 = outputs[layers[1]]\n                    x3 = outputs[layers[2]]\n                    x4 = outputs[layers[3]]\n                    x = torch.cat((x1, x2, x3, x4), 1)\n                    outputs[ind] = x\n                else:\n                    print(""rounte number > 2 ,is {}"".format(len(layers)))\n\n            elif block[\'type\'] == \'shortcut\':\n                from_layer = int(block[\'from\'])\n                activation = block[\'activation\']\n                from_layer = from_layer if from_layer > 0 else from_layer + ind\n                x1 = outputs[from_layer]\n                x2 = outputs[ind - 1]\n                x = x1 + x2\n                if activation == \'leaky\':\n                    x = F.leaky_relu(x, 0.1, inplace=True)\n                elif activation == \'relu\':\n                    x = F.relu(x, inplace=True)\n                outputs[ind] = x\n            elif block[\'type\'] == \'region\':\n                continue\n                if self.loss:\n                    self.loss = self.loss + self.models[ind](x)\n                else:\n                    self.loss = self.models[ind](x)\n                outputs[ind] = None\n            elif block[\'type\'] == \'yolo\':\n                if self.training:\n                    pass\n                else:\n                    boxes = self.models[ind](x)\n                    out_boxes.append(boxes)\n            elif block[\'type\'] == \'cost\':\n                continue\n            else:\n                print(\'unknown type %s\' % (block[\'type\']))\n        if self.training:\n            return loss\n        else:\n            return out_boxes\n\n    def print_network(self):\n        print_cfg(self.blocks)\n\n    def create_network(self, blocks):\n        models = nn.ModuleList()\n\n        prev_filters = 3\n        out_filters = []\n        prev_stride = 1\n        out_strides = []\n        conv_id = 0\n        for block in blocks:\n            if block[\'type\'] == \'net\':\n                prev_filters = int(block[\'channels\'])\n                continue\n            elif block[\'type\'] == \'convolutional\':\n                conv_id = conv_id + 1\n                batch_normalize = int(block[\'batch_normalize\'])\n                filters = int(block[\'filters\'])\n                kernel_size = int(block[\'size\'])\n                stride = int(block[\'stride\'])\n                is_pad = int(block[\'pad\'])\n                pad = (kernel_size - 1) // 2 if is_pad else 0\n                activation = block[\'activation\']\n                model = nn.Sequential()\n                if batch_normalize:\n                    model.add_module(\'conv{0}\'.format(conv_id),\n                                     nn.Conv2d(prev_filters, filters, kernel_size, stride, pad, bias=False))\n                    model.add_module(\'bn{0}\'.format(conv_id), nn.BatchNorm2d(filters))\n                    # model.add_module(\'bn{0}\'.format(conv_id), BN2d(filters))\n                else:\n                    model.add_module(\'conv{0}\'.format(conv_id),\n                                     nn.Conv2d(prev_filters, filters, kernel_size, stride, pad))\n                if activation == \'leaky\':\n                    model.add_module(\'leaky{0}\'.format(conv_id), nn.LeakyReLU(0.1, inplace=True))\n                elif activation == \'relu\':\n                    model.add_module(\'relu{0}\'.format(conv_id), nn.ReLU(inplace=True))\n                elif activation == \'mish\':\n                    model.add_module(\'mish{0}\'.format(conv_id), Mish())\n                else:\n                    print(""convalution havn\'t activate {}"".format(activation))\n\n                prev_filters = filters\n                out_filters.append(prev_filters)\n                prev_stride = stride * prev_stride\n                out_strides.append(prev_stride)\n                models.append(model)\n            elif block[\'type\'] == \'maxpool\':\n                pool_size = int(block[\'size\'])\n                stride = int(block[\'stride\'])\n                if stride == 1 and pool_size % 2:\n                    # You can use Maxpooldark instead, here is convenient to convert onnx.\n                    # Example: [maxpool] size=3 stride=1\n                    model = nn.MaxPool2d(kernel_size=pool_size, stride=stride, padding=pool_size // 2)\n                elif stride == pool_size:\n                    # You can use Maxpooldark instead, here is convenient to convert onnx.\n                    # Example: [maxpool] size=2 stride=2\n                    model = nn.MaxPool2d(kernel_size=pool_size, stride=stride, padding=0)\n                else:\n                    model = MaxPoolDark(pool_size, stride)\n                out_filters.append(prev_filters)\n                prev_stride = stride * prev_stride\n                out_strides.append(prev_stride)\n                models.append(model)\n            elif block[\'type\'] == \'avgpool\':\n                model = GlobalAvgPool2d()\n                out_filters.append(prev_filters)\n                models.append(model)\n            elif block[\'type\'] == \'softmax\':\n                model = nn.Softmax()\n                out_strides.append(prev_stride)\n                out_filters.append(prev_filters)\n                models.append(model)\n            elif block[\'type\'] == \'cost\':\n                if block[\'_type\'] == \'sse\':\n                    model = nn.MSELoss(size_average=True)\n                elif block[\'_type\'] == \'L1\':\n                    model = nn.L1Loss(size_average=True)\n                elif block[\'_type\'] == \'smooth\':\n                    model = nn.SmoothL1Loss(size_average=True)\n                out_filters.append(1)\n                out_strides.append(prev_stride)\n                models.append(model)\n            elif block[\'type\'] == \'reorg\':\n                stride = int(block[\'stride\'])\n                prev_filters = stride * stride * prev_filters\n                out_filters.append(prev_filters)\n                prev_stride = prev_stride * stride\n                out_strides.append(prev_stride)\n                models.append(Reorg(stride))\n            elif block[\'type\'] == \'upsample\':\n                stride = int(block[\'stride\'])\n                out_filters.append(prev_filters)\n                prev_stride = prev_stride // stride\n                out_strides.append(prev_stride)\n\n                models.append(Upsample_expand(stride))\n                # models.append(Upsample_interpolate(stride))\n\n            elif block[\'type\'] == \'route\':\n                layers = block[\'layers\'].split(\',\')\n                ind = len(models)\n                layers = [int(i) if int(i) > 0 else int(i) + ind for i in layers]\n                if len(layers) == 1:\n                    prev_filters = out_filters[layers[0]]\n                    prev_stride = out_strides[layers[0]]\n                elif len(layers) == 2:\n                    assert (layers[0] == ind - 1)\n                    prev_filters = out_filters[layers[0]] + out_filters[layers[1]]\n                    prev_stride = out_strides[layers[0]]\n                elif len(layers) == 4:\n                    assert (layers[0] == ind - 1)\n                    prev_filters = out_filters[layers[0]] + out_filters[layers[1]] + out_filters[layers[2]] + \\\n                                   out_filters[layers[3]]\n                    prev_stride = out_strides[layers[0]]\n                else:\n                    print(""route error!!!"")\n\n                out_filters.append(prev_filters)\n                out_strides.append(prev_stride)\n                models.append(EmptyModule())\n            elif block[\'type\'] == \'shortcut\':\n                ind = len(models)\n                prev_filters = out_filters[ind - 1]\n                out_filters.append(prev_filters)\n                prev_stride = out_strides[ind - 1]\n                out_strides.append(prev_stride)\n                models.append(EmptyModule())\n            elif block[\'type\'] == \'connected\':\n                filters = int(block[\'output\'])\n                if block[\'activation\'] == \'linear\':\n                    model = nn.Linear(prev_filters, filters)\n                elif block[\'activation\'] == \'leaky\':\n                    model = nn.Sequential(\n                        nn.Linear(prev_filters, filters),\n                        nn.LeakyReLU(0.1, inplace=True))\n                elif block[\'activation\'] == \'relu\':\n                    model = nn.Sequential(\n                        nn.Linear(prev_filters, filters),\n                        nn.ReLU(inplace=True))\n                prev_filters = filters\n                out_filters.append(prev_filters)\n                out_strides.append(prev_stride)\n                models.append(model)\n            elif block[\'type\'] == \'region\':\n                loss = RegionLoss()\n                anchors = block[\'anchors\'].split(\',\')\n                loss.anchors = [float(i) for i in anchors]\n                loss.num_classes = int(block[\'classes\'])\n                loss.num_anchors = int(block[\'num\'])\n                loss.anchor_step = len(loss.anchors) // loss.num_anchors\n                loss.object_scale = float(block[\'object_scale\'])\n                loss.noobject_scale = float(block[\'noobject_scale\'])\n                loss.class_scale = float(block[\'class_scale\'])\n                loss.coord_scale = float(block[\'coord_scale\'])\n                out_filters.append(prev_filters)\n                out_strides.append(prev_stride)\n                models.append(loss)\n            elif block[\'type\'] == \'yolo\':\n                yolo_layer = YoloLayer()\n                anchors = block[\'anchors\'].split(\',\')\n                anchor_mask = block[\'mask\'].split(\',\')\n                yolo_layer.anchor_mask = [int(i) for i in anchor_mask]\n                yolo_layer.anchors = [float(i) for i in anchors]\n                yolo_layer.num_classes = int(block[\'classes\'])\n                yolo_layer.num_anchors = int(block[\'num\'])\n                yolo_layer.anchor_step = len(yolo_layer.anchors) // yolo_layer.num_anchors\n                yolo_layer.stride = prev_stride\n                # yolo_layer.object_scale = float(block[\'object_scale\'])\n                # yolo_layer.noobject_scale = float(block[\'noobject_scale\'])\n                # yolo_layer.class_scale = float(block[\'class_scale\'])\n                # yolo_layer.coord_scale = float(block[\'coord_scale\'])\n                out_filters.append(prev_filters)\n                out_strides.append(prev_stride)\n                models.append(yolo_layer)\n            else:\n                print(\'unknown type %s\' % (block[\'type\']))\n\n        return models\n\n    def load_weights(self, weightfile):\n        fp = open(weightfile, \'rb\')\n        header = np.fromfile(fp, count=5, dtype=np.int32)\n        self.header = torch.from_numpy(header)\n        self.seen = self.header[3]\n        buf = np.fromfile(fp, dtype=np.float32)\n        fp.close()\n\n        start = 0\n        ind = -2\n        for block in self.blocks:\n            if start >= buf.size:\n                break\n            ind = ind + 1\n            if block[\'type\'] == \'net\':\n                continue\n            elif block[\'type\'] == \'convolutional\':\n                model = self.models[ind]\n                batch_normalize = int(block[\'batch_normalize\'])\n                if batch_normalize:\n                    start = load_conv_bn(buf, start, model[0], model[1])\n                else:\n                    start = load_conv(buf, start, model[0])\n            elif block[\'type\'] == \'connected\':\n                model = self.models[ind]\n                if block[\'activation\'] != \'linear\':\n                    start = load_fc(buf, start, model[0])\n                else:\n                    start = load_fc(buf, start, model)\n            elif block[\'type\'] == \'maxpool\':\n                pass\n            elif block[\'type\'] == \'reorg\':\n                pass\n            elif block[\'type\'] == \'upsample\':\n                pass\n            elif block[\'type\'] == \'route\':\n                pass\n            elif block[\'type\'] == \'shortcut\':\n                pass\n            elif block[\'type\'] == \'region\':\n                pass\n            elif block[\'type\'] == \'yolo\':\n                pass\n            elif block[\'type\'] == \'avgpool\':\n                pass\n            elif block[\'type\'] == \'softmax\':\n                pass\n            elif block[\'type\'] == \'cost\':\n                pass\n            else:\n                print(\'unknown type %s\' % (block[\'type\']))\n\n    # def save_weights(self, outfile, cutoff=0):\n    #     if cutoff <= 0:\n    #         cutoff = len(self.blocks) - 1\n    #\n    #     fp = open(outfile, \'wb\')\n    #     self.header[3] = self.seen\n    #     header = self.header\n    #     header.numpy().tofile(fp)\n    #\n    #     ind = -1\n    #     for blockId in range(1, cutoff + 1):\n    #         ind = ind + 1\n    #         block = self.blocks[blockId]\n    #         if block[\'type\'] == \'convolutional\':\n    #             model = self.models[ind]\n    #             batch_normalize = int(block[\'batch_normalize\'])\n    #             if batch_normalize:\n    #                 save_conv_bn(fp, model[0], model[1])\n    #             else:\n    #                 save_conv(fp, model[0])\n    #         elif block[\'type\'] == \'connected\':\n    #             model = self.models[ind]\n    #             if block[\'activation\'] != \'linear\':\n    #                 save_fc(fc, model)\n    #             else:\n    #                 save_fc(fc, model[0])\n    #         elif block[\'type\'] == \'maxpool\':\n    #             pass\n    #         elif block[\'type\'] == \'reorg\':\n    #             pass\n    #         elif block[\'type\'] == \'upsample\':\n    #             pass\n    #         elif block[\'type\'] == \'route\':\n    #             pass\n    #         elif block[\'type\'] == \'shortcut\':\n    #             pass\n    #         elif block[\'type\'] == \'region\':\n    #             pass\n    #         elif block[\'type\'] == \'yolo\':\n    #             pass\n    #         elif block[\'type\'] == \'avgpool\':\n    #             pass\n    #         elif block[\'type\'] == \'softmax\':\n    #             pass\n    #         elif block[\'type\'] == \'cost\':\n    #             pass\n    #         else:\n    #             print(\'unknown type %s\' % (block[\'type\']))\n    #     fp.close()\n'"
tool/onnx2tensorflow.py,0,"b'import sys\nimport onnx\nfrom onnx_tf.backend import prepare\n\n\n# tensorflow >=2.0\n# 1: Thanks:github:https://github.com/onnx/onnx-tensorflow\n# 2: Run git clone https://github.com/onnx/onnx-tensorflow.git && cd onnx-tensorflow\n#    Run pip install -e .\n# Note:\n#    Errors will occur when using ""pip install onnx-tf"", at least for me,\n#    it is recommended to use source code installation\ndef transform_to_tensorflow(onnx_input_path, pb_output_path):\n    onnx_model = onnx.load(onnx_input_path)  # load onnx model\n    tf_exp = prepare(onnx_model)  # prepare tf representation\n    tf_exp.export_graph(pb_output_path)  # export the model\n\n\nif __name__ == \'__main__\':\n    if len(sys.argv) == 1:\n        sys.argv.append(\'../weight/yolov4_1_3_608_608.onnx\')  # use:darknet2onnx.py\n        sys.argv.append(\'../weight/yolov4.pb\')  # use:onnx2tensorflow.py\n    if len(sys.argv) == 3:\n        onnxfile = sys.argv[1]\n        tfpb_outfile = sys.argv[2]\n        transform_to_tensorflow(onnxfile, tfpb_outfile)\n    else:\n        print(\'Please execute this script this way:\\n\')\n        print(\'  python onnx2tensorflow.py <onnxfile> <tfpboutfile>\')\n'"
tool/region_loss.py,29,"b""import torch.nn as nn\nimport torch.nn.functional as F\nfrom tool.torch_utils import *\n\n\ndef build_targets(pred_boxes, target, anchors, num_anchors, num_classes, nH, nW, noobject_scale, object_scale,\n                  sil_thresh, seen):\n    nB = target.size(0)\n    nA = num_anchors\n    nC = num_classes\n    anchor_step = len(anchors) / num_anchors\n    conf_mask = torch.ones(nB, nA, nH, nW) * noobject_scale\n    coord_mask = torch.zeros(nB, nA, nH, nW)\n    cls_mask = torch.zeros(nB, nA, nH, nW)\n    tx = torch.zeros(nB, nA, nH, nW)\n    ty = torch.zeros(nB, nA, nH, nW)\n    tw = torch.zeros(nB, nA, nH, nW)\n    th = torch.zeros(nB, nA, nH, nW)\n    tconf = torch.zeros(nB, nA, nH, nW)\n    tcls = torch.zeros(nB, nA, nH, nW)\n\n    nAnchors = nA * nH * nW\n    nPixels = nH * nW\n    for b in range(nB):\n        cur_pred_boxes = pred_boxes[b * nAnchors:(b + 1) * nAnchors].t()\n        cur_ious = torch.zeros(nAnchors)\n        for t in range(50):\n            if target[b][t * 5 + 1] == 0:\n                break\n            gx = target[b][t * 5 + 1] * nW\n            gy = target[b][t * 5 + 2] * nH\n            gw = target[b][t * 5 + 3] * nW\n            gh = target[b][t * 5 + 4] * nH\n            cur_gt_boxes = torch.FloatTensor([gx, gy, gw, gh]).repeat(nAnchors, 1).t()\n            cur_ious = torch.max(cur_ious, bbox_ious(cur_pred_boxes, cur_gt_boxes, x1y1x2y2=False))\n        conf_mask[b][cur_ious > sil_thresh] = 0\n    if seen < 12800:\n        if anchor_step == 4:\n            tx = torch.FloatTensor(anchors).view(nA, anchor_step).index_select(1, torch.LongTensor([2])).view(1, nA, 1,\n                                                                                                              1).repeat(\n                nB, 1, nH, nW)\n            ty = torch.FloatTensor(anchors).view(num_anchors, anchor_step).index_select(1, torch.LongTensor([2])).view(\n                1, nA, 1, 1).repeat(nB, 1, nH, nW)\n        else:\n            tx.fill_(0.5)\n            ty.fill_(0.5)\n        tw.zero_()\n        th.zero_()\n        coord_mask.fill_(1)\n\n    nGT = 0\n    nCorrect = 0\n    for b in range(nB):\n        for t in range(50):\n            if target[b][t * 5 + 1] == 0:\n                break\n            nGT = nGT + 1\n            best_iou = 0.0\n            best_n = -1\n            min_dist = 10000\n            gx = target[b][t * 5 + 1] * nW\n            gy = target[b][t * 5 + 2] * nH\n            gi = int(gx)\n            gj = int(gy)\n            gw = target[b][t * 5 + 3] * nW\n            gh = target[b][t * 5 + 4] * nH\n            gt_box = [0, 0, gw, gh]\n            for n in range(nA):\n                aw = anchors[anchor_step * n]\n                ah = anchors[anchor_step * n + 1]\n                anchor_box = [0, 0, aw, ah]\n                iou = bbox_iou(anchor_box, gt_box, x1y1x2y2=False)\n                if anchor_step == 4:\n                    ax = anchors[anchor_step * n + 2]\n                    ay = anchors[anchor_step * n + 3]\n                    dist = pow(((gi + ax) - gx), 2) + pow(((gj + ay) - gy), 2)\n                if iou > best_iou:\n                    best_iou = iou\n                    best_n = n\n                elif anchor_step == 4 and iou == best_iou and dist < min_dist:\n                    best_iou = iou\n                    best_n = n\n                    min_dist = dist\n\n            gt_box = [gx, gy, gw, gh]\n            pred_box = pred_boxes[b * nAnchors + best_n * nPixels + gj * nW + gi]\n\n            coord_mask[b][best_n][gj][gi] = 1\n            cls_mask[b][best_n][gj][gi] = 1\n            conf_mask[b][best_n][gj][gi] = object_scale\n            tx[b][best_n][gj][gi] = target[b][t * 5 + 1] * nW - gi\n            ty[b][best_n][gj][gi] = target[b][t * 5 + 2] * nH - gj\n            tw[b][best_n][gj][gi] = math.log(gw / anchors[anchor_step * best_n])\n            th[b][best_n][gj][gi] = math.log(gh / anchors[anchor_step * best_n + 1])\n            iou = bbox_iou(gt_box, pred_box, x1y1x2y2=False)  # best_iou\n            tconf[b][best_n][gj][gi] = iou\n            tcls[b][best_n][gj][gi] = target[b][t * 5]\n            if iou > 0.5:\n                nCorrect = nCorrect + 1\n\n    return nGT, nCorrect, coord_mask, conf_mask, cls_mask, tx, ty, tw, th, tconf, tcls\n\n\nclass RegionLoss(nn.Module):\n    def __init__(self, num_classes=0, anchors=[], num_anchors=1):\n        super(RegionLoss, self).__init__()\n        self.num_classes = num_classes\n        self.anchors = anchors\n        self.num_anchors = num_anchors\n        self.anchor_step = len(anchors) / num_anchors\n        self.coord_scale = 1\n        self.noobject_scale = 1\n        self.object_scale = 5\n        self.class_scale = 1\n        self.thresh = 0.6\n        self.seen = 0\n\n    def forward(self, output, target):\n        # output : BxAs*(4+1+num_classes)*H*W\n        t0 = time.time()\n        nB = output.data.size(0)\n        nA = self.num_anchors\n        nC = self.num_classes\n        nH = output.data.size(2)\n        nW = output.data.size(3)\n\n        output = output.view(nB, nA, (5 + nC), nH, nW)\n        x = F.sigmoid(output.index_select(2, Variable(torch.cuda.LongTensor([0]))).view(nB, nA, nH, nW))\n        y = F.sigmoid(output.index_select(2, Variable(torch.cuda.LongTensor([1]))).view(nB, nA, nH, nW))\n        w = output.index_select(2, Variable(torch.cuda.LongTensor([2]))).view(nB, nA, nH, nW)\n        h = output.index_select(2, Variable(torch.cuda.LongTensor([3]))).view(nB, nA, nH, nW)\n        conf = F.sigmoid(output.index_select(2, Variable(torch.cuda.LongTensor([4]))).view(nB, nA, nH, nW))\n        cls = output.index_select(2, Variable(torch.linspace(5, 5 + nC - 1, nC).long().cuda()))\n        cls = cls.view(nB * nA, nC, nH * nW).transpose(1, 2).contiguous().view(nB * nA * nH * nW, nC)\n        t1 = time.time()\n\n        pred_boxes = torch.cuda.FloatTensor(4, nB * nA * nH * nW)\n        grid_x = torch.linspace(0, nW - 1, nW).repeat(nH, 1).repeat(nB * nA, 1, 1).view(nB * nA * nH * nW).cuda()\n        grid_y = torch.linspace(0, nH - 1, nH).repeat(nW, 1).t().repeat(nB * nA, 1, 1).view(nB * nA * nH * nW).cuda()\n        anchor_w = torch.Tensor(self.anchors).view(nA, self.anchor_step).index_select(1, torch.LongTensor([0])).cuda()\n        anchor_h = torch.Tensor(self.anchors).view(nA, self.anchor_step).index_select(1, torch.LongTensor([1])).cuda()\n        anchor_w = anchor_w.repeat(nB, 1).repeat(1, 1, nH * nW).view(nB * nA * nH * nW)\n        anchor_h = anchor_h.repeat(nB, 1).repeat(1, 1, nH * nW).view(nB * nA * nH * nW)\n        pred_boxes[0] = x.data + grid_x\n        pred_boxes[1] = y.data + grid_y\n        pred_boxes[2] = torch.exp(w.data) * anchor_w\n        pred_boxes[3] = torch.exp(h.data) * anchor_h\n        pred_boxes = convert2cpu(pred_boxes.transpose(0, 1).contiguous().view(-1, 4))\n        t2 = time.time()\n\n        nGT, nCorrect, coord_mask, conf_mask, cls_mask, tx, ty, tw, th, tconf, tcls = build_targets(pred_boxes,\n                                                                                                    target.data,\n                                                                                                    self.anchors, nA,\n                                                                                                    nC, \\\n                                                                                                    nH, nW,\n                                                                                                    self.noobject_scale,\n                                                                                                    self.object_scale,\n                                                                                                    self.thresh,\n                                                                                                    self.seen)\n        cls_mask = (cls_mask == 1)\n        nProposals = int((conf > 0.25).sum().data[0])\n\n        tx = Variable(tx.cuda())\n        ty = Variable(ty.cuda())\n        tw = Variable(tw.cuda())\n        th = Variable(th.cuda())\n        tconf = Variable(tconf.cuda())\n        tcls = Variable(tcls.view(-1)[cls_mask].long().cuda())\n\n        coord_mask = Variable(coord_mask.cuda())\n        conf_mask = Variable(conf_mask.cuda().sqrt())\n        cls_mask = Variable(cls_mask.view(-1, 1).repeat(1, nC).cuda())\n        cls = cls[cls_mask].view(-1, nC)\n\n        t3 = time.time()\n\n        loss_x = self.coord_scale * nn.MSELoss(size_average=False)(x * coord_mask, tx * coord_mask) / 2.0\n        loss_y = self.coord_scale * nn.MSELoss(size_average=False)(y * coord_mask, ty * coord_mask) / 2.0\n        loss_w = self.coord_scale * nn.MSELoss(size_average=False)(w * coord_mask, tw * coord_mask) / 2.0\n        loss_h = self.coord_scale * nn.MSELoss(size_average=False)(h * coord_mask, th * coord_mask) / 2.0\n        loss_conf = nn.MSELoss(size_average=False)(conf * conf_mask, tconf * conf_mask) / 2.0\n        loss_cls = self.class_scale * nn.CrossEntropyLoss(size_average=False)(cls, tcls)\n        loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls\n        t4 = time.time()\n        if False:\n            print('-----------------------------------')\n            print('        activation : %f' % (t1 - t0))\n            print(' create pred_boxes : %f' % (t2 - t1))\n            print('     build targets : %f' % (t3 - t2))\n            print('       create loss : %f' % (t4 - t3))\n            print('             total : %f' % (t4 - t0))\n        print('%d: nGT %d, recall %d, proposals %d, loss: x %f, y %f, w %f, h %f, conf %f, cls %f, total %f' % (\n        self.seen, nGT, nCorrect, nProposals, loss_x.data[0], loss_y.data[0], loss_w.data[0], loss_h.data[0],\n        loss_conf.data[0], loss_cls.data[0], loss.data[0]))\n        return loss\n"""
tool/torch_utils.py,32,"b'import sys\nimport os\nimport time\nimport math\nimport torch\nimport numpy as np\nfrom PIL import Image, ImageDraw, ImageFont\nfrom torch.autograd import Variable\n\nimport itertools\nimport struct  # get_image_size\nimport imghdr  # get_image_size\n\nfrom tool import utils \n\n\ndef bbox_ious(boxes1, boxes2, x1y1x2y2=True):\n    if x1y1x2y2:\n        mx = torch.min(boxes1[0], boxes2[0])\n        Mx = torch.max(boxes1[2], boxes2[2])\n        my = torch.min(boxes1[1], boxes2[1])\n        My = torch.max(boxes1[3], boxes2[3])\n        w1 = boxes1[2] - boxes1[0]\n        h1 = boxes1[3] - boxes1[1]\n        w2 = boxes2[2] - boxes2[0]\n        h2 = boxes2[3] - boxes2[1]\n    else:\n        mx = torch.min(boxes1[0] - boxes1[2] / 2.0, boxes2[0] - boxes2[2] / 2.0)\n        Mx = torch.max(boxes1[0] + boxes1[2] / 2.0, boxes2[0] + boxes2[2] / 2.0)\n        my = torch.min(boxes1[1] - boxes1[3] / 2.0, boxes2[1] - boxes2[3] / 2.0)\n        My = torch.max(boxes1[1] + boxes1[3] / 2.0, boxes2[1] + boxes2[3] / 2.0)\n        w1 = boxes1[2]\n        h1 = boxes1[3]\n        w2 = boxes2[2]\n        h2 = boxes2[3]\n    uw = Mx - mx\n    uh = My - my\n    cw = w1 + w2 - uw\n    ch = h1 + h2 - uh\n    mask = ((cw <= 0) + (ch <= 0) > 0)\n    area1 = w1 * h1\n    area2 = w2 * h2\n    carea = cw * ch\n    carea[mask] = 0\n    uarea = area1 + area2 - carea\n    return carea / uarea\n\n\ndef nms(boxes, nms_thresh):\n    if len(boxes) == 0:\n        return boxes\n\n    det_confs = torch.zeros(len(boxes))\n    for i in range(len(boxes)):\n        det_confs[i] = 1 - boxes[i][4]\n\n    _, sortIds = torch.sort(det_confs)\n    out_boxes = []\n    for i in range(len(boxes)):\n        box_i = boxes[sortIds[i]]\n        if box_i[4] > 0:\n            out_boxes.append(box_i)\n            for j in range(i + 1, len(boxes)):\n                box_j = boxes[sortIds[j]]\n                if bbox_iou(box_i, box_j, x1y1x2y2=False) > nms_thresh:\n                    # print(box_i, box_j, bbox_iou(box_i, box_j, x1y1x2y2=False))\n                    box_j[4] = 0\n    return out_boxes\n\n\ndef convert2cpu(gpu_matrix):\n    return torch.FloatTensor(gpu_matrix.size()).copy_(gpu_matrix)\n\n\ndef convert2cpu_long(gpu_matrix):\n    return torch.LongTensor(gpu_matrix.size()).copy_(gpu_matrix)\n\n\ndef yolo_forward(output, conf_thresh, num_classes, anchors, num_anchors, only_objectness=1,\n                              validation=False):\n    # Output would be invalid if it does not satisfy this assert\n    # assert (output.size(1) == (5 + num_classes) * num_anchors)\n\n    # print(output.size())\n\n    # Slice the second dimension (channel) of output into:\n    # [ 1, 1, 1, 1, 1, num_classes, 1, 1, 1, 1, 1, num_classes, 1, 1, 1, 1, 1, num_classes ]\n    #\n    list_of_slices = []\n\n    for i in range(num_anchors):\n        begin = i * (5 + num_classes)\n        end = (i + 1) * (5 + num_classes)\n        \n        for j in range(5):\n            list_of_slices.append(output[:, begin + j : begin + j + 1])\n\n        list_of_slices.append(output[:, begin + 5 : end])\n\n    # Apply sigmoid(), exp() and softmax() to slices\n    # [ 1, 1,     1, 1,     1,      num_classes, 1, 1, 1, 1, 1, num_classes, 1, 1, 1, 1, 1, num_classes ]\n    #   sigmid()  exp()  sigmoid()  softmax()\n    for i in range(num_anchors):\n        begin = i * (5 + 1)\n\n        # print(list_of_slices[begin].size())\n        \n        list_of_slices[begin] = torch.sigmoid(list_of_slices[begin])\n        list_of_slices[begin + 1] = torch.sigmoid(list_of_slices[begin + 1])\n        \n        list_of_slices[begin + 2] = torch.exp(list_of_slices[begin + 2])\n        list_of_slices[begin + 3] = torch.exp(list_of_slices[begin + 3])\n\n        list_of_slices[begin + 4] = torch.sigmoid(list_of_slices[begin + 4])\n\n        list_of_slices[begin + 5] = torch.nn.Softmax(dim=1)(list_of_slices[begin + 5])\n\n    # Prepare C-x, C-y, P-w, P-h (None of them are torch related)\n    batch = output.size(0)\n    H = output.size(2)\n    W = output.size(3)\n    grid_x = np.expand_dims(np.expand_dims(np.expand_dims(np.linspace(0, W - 1, W), axis=0).repeat(H, 0), axis=0), axis=0)\n    grid_y = np.expand_dims(np.expand_dims(np.expand_dims(np.linspace(0, H - 1, H), axis=1).repeat(W, 1), axis=0), axis=0)\n\n    anchor_w = []\n    anchor_h = []\n    for i in range(num_anchors):\n        anchor_w.append(anchors[i * 2])\n        anchor_h.append(anchors[i * 2 + 1])\n\n    device = None\n    cuda_check = output.is_cuda\n    if cuda_check:\n        device = output.get_device()\n\n    # Apply C-x, C-y, P-w, P-h to slices\n    for i in range(num_anchors):\n        begin = i * (5 + 1)\n        \n        list_of_slices[begin] += torch.tensor(grid_x, device=device)\n        list_of_slices[begin + 1] += torch.tensor(grid_y, device=device)\n        \n        list_of_slices[begin + 2] *= anchor_w[i]\n        list_of_slices[begin + 3] *= anchor_h[i]\n\n\n    ########################################\n    #   Figure out bboxes from slices     #\n    ########################################\n\n    bx_list = []\n    by_list = []\n    bw_list = []\n    bh_list = []\n\n    det_confs_list = []\n    cls_confs_list = []\n\n    for i in range(num_anchors):\n        begin = i * (5 + 1)\n\n        bx_list.append(list_of_slices[begin])\n        by_list.append(list_of_slices[begin + 1])\n        bw_list.append(list_of_slices[begin + 2])\n        bh_list.append(list_of_slices[begin + 3])\n\n        # Shape: [batch, 1, H, W]\n        det_confs = list_of_slices[begin + 4]\n\n        # Shape: [batch, num_classes, H, W]\n        cls_confs = list_of_slices[begin + 5]\n\n        det_confs_list.append(det_confs)\n        cls_confs_list.append(cls_confs)\n    \n    # Shape: [batch, num_anchors, H, W]\n    bx = torch.cat(bx_list, dim=1)\n    by = torch.cat(by_list, dim=1)\n    bw = torch.cat(bw_list, dim=1)\n    bh = torch.cat(bh_list, dim=1)\n\n    # normalize coordinates to [0, 1]\n    bx = bx / W\n    by = by / H\n    bw = bw / W\n    bh = bh / H\n\n    # Shape: [batch, num_anchors * H * W] \n    det_confs = torch.cat(det_confs_list, dim=1).view(batch, num_anchors * H * W)\n\n    # Shape: [batch, num_anchors, num_classes, H * W] \n    cls_confs = torch.cat(cls_confs_list, dim=1).view(batch, num_anchors, num_classes, H * W)\n    # Shape: [batch, num_anchors * H * W, num_classes] \n    cls_confs = cls_confs.permute(0, 1, 3, 2).reshape(batch, num_anchors * H * W, num_classes)\n\n    # Shape: [batch, num_anchors * H * W, 1]\n    bx = bx.view(batch, num_anchors * H * W, 1)\n    by = by.view(batch, num_anchors * H * W, 1)\n    bw = bw.view(batch, num_anchors * H * W, 1)\n    bh = bh.view(batch, num_anchors * H * W, 1)\n\n    # Shape: [batch, num_anchors * h * w, 4]\n    boxes = torch.cat((bx, by, bw, bh), dim=2).clamp(-10.0, 10.0)\n\n    # Shape: [batch, num_anchors * h * w, num_classes, 4]\n    # boxes = boxes.view(N, num_anchors * H * W, 1, 4).expand(N, num_anchors * H * W, num_classes, 4)\n    \n\n    return  boxes, cls_confs, det_confs\n\n\n\ndef do_detect(model, img, conf_thresh, n_classes, nms_thresh, use_cuda=1):\n    model.eval()\n    t0 = time.time()\n\n    if isinstance(img, Image.Image):\n        width = img.width\n        height = img.height\n        img = torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes()))\n        img = img.view(height, width, 3).transpose(0, 1).transpose(0, 2).contiguous()\n        img = img.view(1, 3, height, width)\n        img = img.float().div(255.0)\n    elif type(img) == np.ndarray and len(img.shape) == 3:  # cv2 image\n        img = torch.from_numpy(img.transpose(2, 0, 1)).float().div(255.0).unsqueeze(0)\n    elif type(img) == np.ndarray and len(img.shape) == 4:\n        img = torch.from_numpy(img.transpose(0, 3, 1, 2)).float().div(255.0)\n    else:\n        print(""unknow image type"")\n        exit(-1)\n\n    if use_cuda:\n        img = img.cuda()\n    img = torch.autograd.Variable(img)\n    \n    t1 = time.time()\n\n    boxes_and_confs = model(img)\n\n    # print(boxes_and_confs)\n    output = []\n    \n    for i in range(len(boxes_and_confs)):\n        output.append([])\n        output[-1].append(boxes_and_confs[i][0].cpu().detach().numpy())\n        output[-1].append(boxes_and_confs[i][1].cpu().detach().numpy())\n        output[-1].append(boxes_and_confs[i][2].cpu().detach().numpy())\n\n    t2 = time.time()\n\n    print(\'-----------------------------------\')\n    print(\'          Preprocess : %f\' % (t1 - t0))\n    print(\'     Model Inference : %f\' % (t2 - t1))\n    print(\'-----------------------------------\')\n\n    \'\'\'\n    for i in range(len(boxes_and_confs)):\n        output.append(boxes_and_confs[i].cpu().detach().numpy())\n    \'\'\'\n\n    return utils.post_processing(img, conf_thresh, n_classes, nms_thresh, output)\n\n'"
tool/utils.py,0,"b'import sys\nimport os\nimport time\nimport math\nimport numpy as np\nfrom PIL import Image, ImageDraw, ImageFont\n\nimport itertools\nimport struct  # get_image_size\nimport imghdr  # get_image_size\n\n\ndef sigmoid(x):\n    return 1.0 / (np.exp(-x) + 1.)\n\n\ndef softmax(x):\n    x = np.exp(x - np.expand_dims(np.max(x, axis=1), axis=1))\n    x = x / np.expand_dims(x.sum(axis=1), axis=1)\n    return x\n\n\ndef bbox_iou(box1, box2, x1y1x2y2=True):\n    \n    # print(\'iou box1:\', box1)\n    # print(\'iou box2:\', box2)\n\n    if x1y1x2y2:\n        mx = min(box1[0], box2[0])\n        Mx = max(box1[2], box2[2])\n        my = min(box1[1], box2[1])\n        My = max(box1[3], box2[3])\n        w1 = box1[2] - box1[0]\n        h1 = box1[3] - box1[1]\n        w2 = box2[2] - box2[0]\n        h2 = box2[3] - box2[1]\n    else:\n        w1 = box1[2]\n        h1 = box1[3]\n        w2 = box2[2]\n        h2 = box2[3]\n\n        mx = min(box1[0], box2[0])\n        Mx = max(box1[0] + w1, box2[0] + w2)\n        my = min(box1[1], box2[1])\n        My = max(box1[1] + h1, box2[1] + h2)\n    uw = Mx - mx\n    uh = My - my\n    cw = w1 + w2 - uw\n    ch = h1 + h2 - uh\n    carea = 0\n    if cw <= 0 or ch <= 0:\n        return 0.0\n\n    area1 = w1 * h1\n    area2 = w2 * h2\n    carea = cw * ch\n    uarea = area1 + area2 - carea\n    return carea / uarea\n\n\n\ndef get_region_boxes(boxes, cls_confs, det_confs, conf_thresh):\n    \n    ########################################\n    #   Figure out bboxes from slices     #\n    ########################################\n\n    t1 = time.time()\n    all_boxes = []\n    for b in range(boxes.shape[0]):\n        l_boxes = []\n        # Shape: [batch, num_anchors * H * W] -> [num_anchors * H * W]\n        # print(det_confs.shape)\n        det_conf = det_confs[b, :]\n        # print(det_conf.shape)\n        argwhere = np.argwhere(det_conf > conf_thresh)\n \n        det_conf = det_conf[argwhere].flatten()\n        max_cls_conf = cls_confs[b, argwhere].max(axis=2).flatten()\n        max_cls_id = cls_confs[b, argwhere].argmax(axis=2).flatten()\n\n        bcx = boxes[b, argwhere, 0]\n        bcy = boxes[b, argwhere, 1]\n        bw = boxes[b, argwhere, 2]\n        bh = boxes[b, argwhere, 3]\n\n        for i in range(bcx.shape[0]):\n            # print(max_cls_conf[i])\n            l_box = [bcx[i], bcy[i], bw[i], bh[i], det_conf[i], max_cls_conf[i], max_cls_id[i]]\n            l_boxes.append(l_box)\n\n        all_boxes.append(l_boxes)\n    t2 = time.time()\n\n    if False:\n        print(\'---------------------------------\')\n        print(\'      boxes: %f\' % (t2 - t1))\n        print(\'---------------------------------\')\n    \n    \n    return all_boxes\n\n\ndef nms(boxes, nms_thresh):\n    if len(boxes) == 0:\n        return boxes\n\n    det_confs = np.zeros(len(boxes))\n    for i in range(len(boxes)):\n        det_confs[i] = 1 - boxes[i][4]\n\n    sortIds = np.argsort(det_confs)\n    out_boxes = []\n\n    for i in range(len(boxes)):\n        box_i = boxes[sortIds[i]]\n        if box_i[4] > 0:\n            out_boxes.append(box_i)\n            for j in range(i + 1, len(boxes)):\n                box_j = boxes[sortIds[j]]\n                if bbox_iou(box_i, box_j, x1y1x2y2=False) > nms_thresh:\n                    # print(box_i, box_j, bbox_iou(box_i, box_j, x1y1x2y2=False))\n                    box_j[4] = 0\n    \n    return out_boxes\n\n\n\ndef plot_boxes_cv2(img, boxes, savename=None, class_names=None, color=None):\n    import cv2\n    colors = np.array([[1, 0, 1], [0, 0, 1], [0, 1, 1], [0, 1, 0], [1, 1, 0], [1, 0, 0]], dtype=np.float32)\n\n    def get_color(c, x, max_val):\n        ratio = float(x) / max_val * 5\n        i = int(math.floor(ratio))\n        j = int(math.ceil(ratio))\n        ratio = ratio - i\n        r = (1 - ratio) * colors[i][c] + ratio * colors[j][c]\n        return int(r * 255)\n\n    width = img.shape[1]\n    height = img.shape[0]\n    for i in range(len(boxes)):\n        box = boxes[i]\n        x1 = int((box[0] - box[2] / 2.0) * width)\n        y1 = int((box[1] - box[3] / 2.0) * height)\n        x2 = int((box[0] + box[2] / 2.0) * width)\n        y2 = int((box[1] + box[3] / 2.0) * height)\n\n        if color:\n            rgb = color\n        else:\n            rgb = (255, 0, 0)\n        if len(box) >= 7 and class_names:\n            cls_conf = box[5]\n            cls_id = box[6]\n            print(\'%s: %f\' % (class_names[cls_id], cls_conf))\n            classes = len(class_names)\n            offset = cls_id * 123457 % classes\n            red = get_color(2, offset, classes)\n            green = get_color(1, offset, classes)\n            blue = get_color(0, offset, classes)\n            if color is None:\n                rgb = (red, green, blue)\n            img = cv2.putText(img, class_names[cls_id], (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 1.2, rgb, 1)\n        img = cv2.rectangle(img, (x1, y1), (x2, y2), rgb, 1)\n    if savename:\n        print(""save plot results to %s"" % savename)\n        cv2.imwrite(savename, img)\n    return img\n\n\ndef plot_boxes(img, boxes, savename=None, class_names=None):\n    colors = np.array([[1, 0, 1], [0, 0, 1], [0, 1, 1], [0, 1, 0], [1, 1, 0], [1, 0, 0]], dtype=np.float32)\n\n    def get_color(c, x, max_val):\n        ratio = float(x) / max_val * 5\n        i = int(math.floor(ratio))\n        j = int(math.ceil(ratio))\n        ratio = ratio - i\n        r = (1 - ratio) * colors[i][c] + ratio * colors[j][c]\n        return int(r * 255)\n\n    width = img.width\n    height = img.height\n    draw = ImageDraw.Draw(img)\n    for i in range(len(boxes)):\n        box = boxes[i]\n        x1 = (box[0] - box[2] / 2.0) * width\n        y1 = (box[1] - box[3] / 2.0) * height\n        x2 = (box[0] + box[2] / 2.0) * width\n        y2 = (box[1] + box[3] / 2.0) * height\n\n        rgb = (255, 0, 0)\n        if len(box) >= 7 and class_names:\n            cls_conf = box[5]\n            cls_id = box[6]\n            print(\'%s: %f\' % (class_names[cls_id], cls_conf))\n            classes = len(class_names)\n            offset = cls_id * 123457 % classes\n            red = get_color(2, offset, classes)\n            green = get_color(1, offset, classes)\n            blue = get_color(0, offset, classes)\n            rgb = (red, green, blue)\n            draw.text((x1, y1), class_names[cls_id], fill=rgb)\n        draw.rectangle([x1, y1, x2, y2], outline=rgb)\n    if savename:\n        print(""save plot results to %s"" % savename)\n        img.save(savename)\n    return img\n\n\ndef read_truths(lab_path):\n    if not os.path.exists(lab_path):\n        return np.array([])\n    if os.path.getsize(lab_path):\n        truths = np.loadtxt(lab_path)\n        truths = truths.reshape(truths.size / 5, 5)  # to avoid single truth problem\n        return truths\n    else:\n        return np.array([])\n\n\ndef load_class_names(namesfile):\n    class_names = []\n    with open(namesfile, \'r\') as fp:\n        lines = fp.readlines()\n    for line in lines:\n        line = line.rstrip()\n        class_names.append(line)\n    return class_names\n\n\ndef post_processing(img, conf_thresh, n_classes, nms_thresh, output):\n\n    # anchors = [12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401]\n    # num_anchors = 9\n    # anchor_masks = [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n    # strides = [8, 16, 32]\n    # anchor_step = len(anchors) // num_anchors\n\n    boxes = []  \n    t1 = time.time()\n    for i in range(len(output)):\n        boxes.append(get_region_boxes(output[i][0], output[i][1], output[i][2], conf_thresh))\n    t2 = time.time()\n\n    if img.shape[0] > 1:\n        bboxs_for_imgs = [\n            boxes[0][index] + boxes[1][index] + boxes[2][index]\n            for index in range(img.shape[0])]\n        # \xe5\x88\x86\xe5\x88\xab\xe5\xaf\xb9\xe6\xaf\x8f\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe8\xbf\x9b\xe8\xa1\x8cnms\n        t3 = time.time()\n        boxes = [nms(bboxs, nms_thresh) for bboxs in bboxs_for_imgs]\n    else:\n        boxes = boxes[0][0] + boxes[1][0] + boxes[2][0]\n        t3 = time.time()\n        boxes = nms(boxes, nms_thresh)\n    t4 = time.time()\n\n    print(\'-----------------------------------\')\n    print(\'     get_region_boxes : %f\' % (t2 - t1))\n    print(\'                  nms : %f\' % (t4 - t3))\n    print(\'   post process total : %f\' % (t4 - t1))\n    print(\'-----------------------------------\')\n    return boxes\n'"
tool/utils_boost.py,0,"b'import sys\nimport os\nimport time\nimport math\nimport numpy as np\nfrom PIL import Image, ImageDraw, ImageFont\n\nimport itertools\nimport struct  # get_image_size\nimport imghdr  # get_image_size\n\n\ndef sigmoid(x):\n    return 1.0 / (np.exp(-x) + 1.)\n\n\ndef softmax(x):\n    x = np.exp(x - np.expand_dims(np.max(x, axis=1), axis=1))\n    x = x / np.expand_dims(x.sum(axis=1), axis=1)\n    return x\n\n\ndef bbox_iou(box1, box2):\n    \n    # print(\'iou box1:\', box1)\n    # print(\'iou box2:\', box2)\n\n    w1 = box1[2]\n    h1 = box1[3]\n    w2 = box2[2]\n    h2 = box2[3]\n\n    mx = min(box1[0], box2[0])\n    Mx = max(box1[0] + w1, box2[0] + w2)\n    my = min(box1[1], box2[1])\n    My = max(box1[1] + h1, box2[1] + h2)\n        \n    uw = Mx - mx\n    uh = My - my\n    cw = w1 + w2 - uw\n    ch = h1 + h2 - uh\n    carea = 0\n    if cw <= 0 or ch <= 0:\n        return 0.0\n\n    area1 = w1 * h1\n    area2 = w2 * h2\n    carea = cw * ch\n    uarea = area1 + area2 - carea\n    return carea / uarea\n\n\n\ndef get_region_boxes(boxes, cls_confs, det_confs, conf_thresh):\n    \n    ########################################\n    #   Figure out bboxes from slices     #\n    ########################################\n\n    # boxes:     [batch, num_anchors * H * W]\n    # det_confs: [batch, num_anchors * H * W]\n    # cls_confs: [batch, num_anchors * H * W, num_classes]\n\n    t1 = time.time()\n    all_boxes = []\n    for b in range(boxes.shape[0]):\n        # l_boxes = []\n        # Shape: [batch, num_anchors * H * W] -> [num_anchors * H * W]\n        # print(det_confs.shape)\n        det_confs = det_confs[b, :]\n        # print(det_conf.shape)\n        argwhere = np.argwhere(det_confs > conf_thresh)\n \n        det_confs = det_confs[argwhere].flatten()\n        max_cls_confs = cls_confs[b, argwhere].max(axis=2).flatten()\n        max_cls_ids = cls_confs[b, argwhere].argmax(axis=2).flatten()\n        bboxes = boxes[b, argwhere, :].reshape(-1, 4)\n        \'\'\'\n        print(det_confs.shape)\n        print(max_cls_confs.shape)\n        print(max_cls_ids.shape)\n        print(bboxes.shape)\n        \'\'\'\n\n        all_boxes.append([bboxes, det_confs, max_cls_confs, max_cls_ids])\n    t2 = time.time()\n\n    if False:\n        print(\'---------------------------------\')\n        print(\'      boxes: %f\' % (t2 - t1))\n        print(\'---------------------------------\')\n    \n    return all_boxes\n\n\ndef nms(bboxes_data, nms_thresh):\n    \n    bboxes, det_confs, max_cls_confs, max_cls_ids = bboxes_data\n    \'\'\'\n    if bboxes.shape[0] == 0:\n        return []\n    \'\'\'\n\n    det_confs_reversed = 1 - det_confs\n    sortIds = np.argsort(det_confs_reversed)\n\n    # print(sortIds)\n    bboxes = bboxes[sortIds]\n    det_confs = det_confs[sortIds]\n    max_cls_confs = max_cls_confs[sortIds]\n    max_cls_ids = max_cls_ids[sortIds]\n\n    validity = np.zeros(det_confs.shape, dtype=np.int32)\n    validity = 1 - validity\n    \n    out_boxes = []\n\n    for i in range(bboxes.shape[0]):\n\n        bbox_i = bboxes[i]\n        max_cls_conf_i = max_cls_confs[i]\n        max_cls_ids_i = max_cls_ids[i]\n        \n        if validity[i] > 0:\n            out_boxes.append([bbox_i, max_cls_conf_i, max_cls_ids_i])\n\n            for j in range(i + 1, bboxes.shape[0]):\n                bbox_j = bboxes[j]\n\n                if bbox_iou(bbox_i, bbox_j) > nms_thresh:\n                    validity[j] = 0\n    \'\'\'\n    for out_box in out_boxes:\n        print(out_box)\n    \'\'\'\n    return out_boxes\n\n\n\ndef plot_boxes_cv2(img, boxes, savename=None, class_names=None, color=None):\n    import cv2\n    colors = np.array([[1, 0, 1], [0, 0, 1], [0, 1, 1], [0, 1, 0], [1, 1, 0], [1, 0, 0]], dtype=np.float32)\n\n    def get_color(c, x, max_val):\n        ratio = float(x) / max_val * 5\n        i = int(math.floor(ratio))\n        j = int(math.ceil(ratio))\n        ratio = ratio - i\n        r = (1 - ratio) * colors[i][c] + ratio * colors[j][c]\n        return int(r * 255)\n\n    width = img.shape[1]\n    height = img.shape[0]\n    for i in range(len(boxes)):\n        box = boxes[i]\n        x1 = int((box[0] - box[2] / 2.0) * width)\n        y1 = int((box[1] - box[3] / 2.0) * height)\n        x2 = int((box[0] + box[2] / 2.0) * width)\n        y2 = int((box[1] + box[3] / 2.0) * height)\n\n        if color:\n            rgb = color\n        else:\n            rgb = (255, 0, 0)\n        if len(box) >= 7 and class_names:\n            cls_conf = box[5]\n            cls_id = box[6]\n            print(\'%s: %f\' % (class_names[cls_id], cls_conf))\n            classes = len(class_names)\n            offset = cls_id * 123457 % classes\n            red = get_color(2, offset, classes)\n            green = get_color(1, offset, classes)\n            blue = get_color(0, offset, classes)\n            if color is None:\n                rgb = (red, green, blue)\n            img = cv2.putText(img, class_names[cls_id], (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 1.2, rgb, 1)\n        img = cv2.rectangle(img, (x1, y1), (x2, y2), rgb, 1)\n    if savename:\n        print(""save plot results to %s"" % savename)\n        cv2.imwrite(savename, img)\n    return img\n\n\n\ndef plot_boxes(img, boxes, savename=None, class_names=None):\n    colors = np.array([[1, 0, 1], [0, 0, 1], [0, 1, 1], [0, 1, 0], [1, 1, 0], [1, 0, 0]], dtype=np.float32)\n\n    def get_color(c, x, max_val):\n        ratio = float(x) / max_val * 5\n        i = int(math.floor(ratio))\n        j = int(math.ceil(ratio))\n        ratio = ratio - i\n        r = (1 - ratio) * colors[i][c] + ratio * colors[j][c]\n        return int(r * 255)\n\n    width = img.width\n    height = img.height\n    draw = ImageDraw.Draw(img)\n    for i in range(len(boxes)):\n        box = boxes[i]\n        x1 = (box[0] - box[2] / 2.0) * width\n        y1 = (box[1] - box[3] / 2.0) * height\n        x2 = (box[0] + box[2] / 2.0) * width\n        y2 = (box[1] + box[3] / 2.0) * height\n\n        rgb = (255, 0, 0)\n        if len(box) >= 7 and class_names:\n            cls_conf = box[5]\n            cls_id = box[6]\n            print(\'%s: %f\' % (class_names[cls_id], cls_conf))\n            classes = len(class_names)\n            offset = cls_id * 123457 % classes\n            red = get_color(2, offset, classes)\n            green = get_color(1, offset, classes)\n            blue = get_color(0, offset, classes)\n            rgb = (red, green, blue)\n            draw.text((x1, y1), class_names[cls_id], fill=rgb)\n        draw.rectangle([x1, y1, x2, y2], outline=rgb)\n    if savename:\n        print(""save plot results to %s"" % savename)\n        img.save(savename)\n    return img\n\n\ndef read_truths(lab_path):\n    if not os.path.exists(lab_path):\n        return np.array([])\n    if os.path.getsize(lab_path):\n        truths = np.loadtxt(lab_path)\n        truths = truths.reshape(truths.size / 5, 5)  # to avoid single truth problem\n        return truths\n    else:\n        return np.array([])\n\n\ndef load_class_names(namesfile):\n    class_names = []\n    with open(namesfile, \'r\') as fp:\n        lines = fp.readlines()\n    for line in lines:\n        line = line.rstrip()\n        class_names.append(line)\n    return class_names\n\n\ndef post_processing(img, conf_thresh, n_classes, nms_thresh, output):\n\n    # anchors = [12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401]\n    # num_anchors = 9\n    # anchor_masks = [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n    # strides = [8, 16, 32]\n    # anchor_step = len(anchors) // num_anchors\n\n    boxes = []  \n    t1 = time.time()\n    for i in range(len(output)):\n        boxes.append(get_region_boxes(output[i][0], output[i][1], output[i][2], conf_thresh))\n    t2 = time.time()\n        \n    bboxes_data_list = []\n    \n    for index in range(img.shape[0]):\n        \n        bboxes = np.concatenate((boxes[0][index][0], boxes[1][index][0], boxes[2][index][0]), axis=0)\n\n        det_confs = np.concatenate((boxes[0][index][1], boxes[1][index][1], boxes[2][index][1]), axis=0)\n\n        max_cls_confs = np.concatenate((boxes[0][index][2], boxes[1][index][2], boxes[2][index][2]), axis=0)\n\n        max_cls_ids = np.concatenate((boxes[0][index][3], boxes[1][index][3], boxes[2][index][3]), axis=0)\n\n        print(\'Shapes of combined arrays from all three net outputs: \')\n        print(bboxes.shape)\n        print(det_confs.shape)\n        print(max_cls_confs.shape)\n        print(max_cls_ids.shape)\n\n        bboxes_data_list.append([bboxes, det_confs, max_cls_confs, max_cls_ids])\n    \n    t3 = time.time()\n    # \xe5\x88\x86\xe5\x88\xab\xe5\xaf\xb9\xe6\xaf\x8f\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe8\xbf\x9b\xe8\xa1\x8cnms\n    bboxes_to_plot_for_imgs = [nms(bboxes_data, nms_thresh) for bboxes_data in bboxes_data_list]\n        \n    t4 = time.time()\n\n    print(\'-----------------------------------\')\n    print(\'     get_region_boxes : %f\' % (t2 - t1))\n    print(\'                  nms : %f\' % (t4 - t3))\n    print(\'   post process total : %f\' % (t4 - t1))\n    print(\'-----------------------------------\')\n    return bboxes_to_plot_for_imgs\n'"
tool/yolo_layer.py,31,"b""import torch.nn as nn\nimport torch.nn.functional as F\nfrom tool.torch_utils import *\n\n\ndef build_targets(pred_boxes, target, anchors, num_anchors, num_classes, nH, nW, noobject_scale, object_scale,\n                  sil_thresh, seen):\n    nB = target.size(0)\n    nA = num_anchors\n    nC = num_classes\n    anchor_step = len(anchors) / num_anchors\n    conf_mask = torch.ones(nB, nA, nH, nW) * noobject_scale\n    coord_mask = torch.zeros(nB, nA, nH, nW)\n    cls_mask = torch.zeros(nB, nA, nH, nW)\n    tx = torch.zeros(nB, nA, nH, nW)\n    ty = torch.zeros(nB, nA, nH, nW)\n    tw = torch.zeros(nB, nA, nH, nW)\n    th = torch.zeros(nB, nA, nH, nW)\n    tconf = torch.zeros(nB, nA, nH, nW)\n    tcls = torch.zeros(nB, nA, nH, nW)\n\n    nAnchors = nA * nH * nW\n    nPixels = nH * nW\n    for b in range(nB):\n        cur_pred_boxes = pred_boxes[b * nAnchors:(b + 1) * nAnchors].t()\n        cur_ious = torch.zeros(nAnchors)\n        for t in range(50):\n            if target[b][t * 5 + 1] == 0:\n                break\n            gx = target[b][t * 5 + 1] * nW\n            gy = target[b][t * 5 + 2] * nH\n            gw = target[b][t * 5 + 3] * nW\n            gh = target[b][t * 5 + 4] * nH\n            cur_gt_boxes = torch.FloatTensor([gx, gy, gw, gh]).repeat(nAnchors, 1).t()\n            cur_ious = torch.max(cur_ious, bbox_ious(cur_pred_boxes, cur_gt_boxes, x1y1x2y2=False))\n        conf_mask[b][cur_ious > sil_thresh] = 0\n    if seen < 12800:\n        if anchor_step == 4:\n            tx = torch.FloatTensor(anchors).view(nA, anchor_step).index_select(1, torch.LongTensor([2])).view(1, nA, 1,\n                                                                                                              1).repeat(\n                nB, 1, nH, nW)\n            ty = torch.FloatTensor(anchors).view(num_anchors, anchor_step).index_select(1, torch.LongTensor([2])).view(\n                1, nA, 1, 1).repeat(nB, 1, nH, nW)\n        else:\n            tx.fill_(0.5)\n            ty.fill_(0.5)\n        tw.zero_()\n        th.zero_()\n        coord_mask.fill_(1)\n\n    nGT = 0\n    nCorrect = 0\n    for b in range(nB):\n        for t in range(50):\n            if target[b][t * 5 + 1] == 0:\n                break\n            nGT = nGT + 1\n            best_iou = 0.0\n            best_n = -1\n            min_dist = 10000\n            gx = target[b][t * 5 + 1] * nW\n            gy = target[b][t * 5 + 2] * nH\n            gi = int(gx)\n            gj = int(gy)\n            gw = target[b][t * 5 + 3] * nW\n            gh = target[b][t * 5 + 4] * nH\n            gt_box = [0, 0, gw, gh]\n            for n in range(nA):\n                aw = anchors[anchor_step * n]\n                ah = anchors[anchor_step * n + 1]\n                anchor_box = [0, 0, aw, ah]\n                iou = bbox_iou(anchor_box, gt_box, x1y1x2y2=False)\n                if anchor_step == 4:\n                    ax = anchors[anchor_step * n + 2]\n                    ay = anchors[anchor_step * n + 3]\n                    dist = pow(((gi + ax) - gx), 2) + pow(((gj + ay) - gy), 2)\n                if iou > best_iou:\n                    best_iou = iou\n                    best_n = n\n                elif anchor_step == 4 and iou == best_iou and dist < min_dist:\n                    best_iou = iou\n                    best_n = n\n                    min_dist = dist\n\n            gt_box = [gx, gy, gw, gh]\n            pred_box = pred_boxes[b * nAnchors + best_n * nPixels + gj * nW + gi]\n\n            coord_mask[b][best_n][gj][gi] = 1\n            cls_mask[b][best_n][gj][gi] = 1\n            conf_mask[b][best_n][gj][gi] = object_scale\n            tx[b][best_n][gj][gi] = target[b][t * 5 + 1] * nW - gi\n            ty[b][best_n][gj][gi] = target[b][t * 5 + 2] * nH - gj\n            tw[b][best_n][gj][gi] = math.log(gw / anchors[anchor_step * best_n])\n            th[b][best_n][gj][gi] = math.log(gh / anchors[anchor_step * best_n + 1])\n            iou = bbox_iou(gt_box, pred_box, x1y1x2y2=False)  # best_iou\n            tconf[b][best_n][gj][gi] = iou\n            tcls[b][best_n][gj][gi] = target[b][t * 5]\n            if iou > 0.5:\n                nCorrect = nCorrect + 1\n\n    return nGT, nCorrect, coord_mask, conf_mask, cls_mask, tx, ty, tw, th, tconf, tcls\n\n\nclass YoloLayer(nn.Module):\n    ''' Yolo layer\n    model_out: while inference,is post-processing inside or outside the model\n        true:outside\n    '''\n    def __init__(self, anchor_mask=[], num_classes=0, anchors=[], num_anchors=1, stride=32, model_out=False):\n        super(YoloLayer, self).__init__()\n        self.anchor_mask = anchor_mask\n        self.num_classes = num_classes\n        self.anchors = anchors\n        self.num_anchors = num_anchors\n        self.anchor_step = len(anchors) // num_anchors\n        self.coord_scale = 1\n        self.noobject_scale = 1\n        self.object_scale = 5\n        self.class_scale = 1\n        self.thresh = 0.6\n        self.stride = stride\n        self.seen = 0\n\n        self.model_out = model_out\n\n    def forward(self, output, target=None):\n        if self.training:\n            # output : BxAs*(4+1+num_classes)*H*W\n            t0 = time.time()\n            nB = output.data.size(0)\n            nA = self.num_anchors\n            nC = self.num_classes\n            nH = output.data.size(2)\n            nW = output.data.size(3)\n\n            output = output.view(nB, nA, (5 + nC), nH, nW)\n            x = F.sigmoid(output.index_select(2, Variable(torch.cuda.LongTensor([0]))).view(nB, nA, nH, nW))\n            y = F.sigmoid(output.index_select(2, Variable(torch.cuda.LongTensor([1]))).view(nB, nA, nH, nW))\n            w = output.index_select(2, Variable(torch.cuda.LongTensor([2]))).view(nB, nA, nH, nW)\n            h = output.index_select(2, Variable(torch.cuda.LongTensor([3]))).view(nB, nA, nH, nW)\n            conf = F.sigmoid(output.index_select(2, Variable(torch.cuda.LongTensor([4]))).view(nB, nA, nH, nW))\n            cls = output.index_select(2, Variable(torch.linspace(5, 5 + nC - 1, nC).long().cuda()))\n            cls = cls.view(nB * nA, nC, nH * nW).transpose(1, 2).contiguous().view(nB * nA * nH * nW, nC)\n            t1 = time.time()\n\n            pred_boxes = torch.cuda.FloatTensor(4, nB * nA * nH * nW)\n            grid_x = torch.linspace(0, nW - 1, nW).repeat(nH, 1).repeat(nB * nA, 1, 1).view(nB * nA * nH * nW).cuda()\n            grid_y = torch.linspace(0, nH - 1, nH).repeat(nW, 1).t().repeat(nB * nA, 1, 1).view(\n                nB * nA * nH * nW).cuda()\n            anchor_w = torch.Tensor(self.anchors).view(nA, self.anchor_step).index_select(1,\n                                                                                          torch.LongTensor([0])).cuda()\n            anchor_h = torch.Tensor(self.anchors).view(nA, self.anchor_step).index_select(1,\n                                                                                          torch.LongTensor([1])).cuda()\n            anchor_w = anchor_w.repeat(nB, 1).repeat(1, 1, nH * nW).view(nB * nA * nH * nW)\n            anchor_h = anchor_h.repeat(nB, 1).repeat(1, 1, nH * nW).view(nB * nA * nH * nW)\n            pred_boxes[0] = x.data + grid_x\n            pred_boxes[1] = y.data + grid_y\n            pred_boxes[2] = torch.exp(w.data) * anchor_w\n            pred_boxes[3] = torch.exp(h.data) * anchor_h\n            pred_boxes = convert2cpu(pred_boxes.transpose(0, 1).contiguous().view(-1, 4))\n            t2 = time.time()\n\n            nGT, nCorrect, coord_mask, conf_mask, cls_mask, tx, ty, tw, th, tconf, tcls = build_targets(pred_boxes,\n                                                                                                        target.data,\n                                                                                                        self.anchors,\n                                                                                                        nA, nC, \\\n                                                                                                        nH, nW,\n                                                                                                        self.noobject_scale,\n                                                                                                        self.object_scale,\n                                                                                                        self.thresh,\n                                                                                                        self.seen)\n            cls_mask = (cls_mask == 1)\n            nProposals = int((conf > 0.25).sum().data[0])\n\n            tx = Variable(tx.cuda())\n            ty = Variable(ty.cuda())\n            tw = Variable(tw.cuda())\n            th = Variable(th.cuda())\n            tconf = Variable(tconf.cuda())\n            tcls = Variable(tcls.view(-1)[cls_mask].long().cuda())\n\n            coord_mask = Variable(coord_mask.cuda())\n            conf_mask = Variable(conf_mask.cuda().sqrt())\n            cls_mask = Variable(cls_mask.view(-1, 1).repeat(1, nC).cuda())\n            cls = cls[cls_mask].view(-1, nC)\n\n            t3 = time.time()\n\n            loss_x = self.coord_scale * nn.MSELoss(size_average=False)(x * coord_mask, tx * coord_mask) / 2.0\n            loss_y = self.coord_scale * nn.MSELoss(size_average=False)(y * coord_mask, ty * coord_mask) / 2.0\n            loss_w = self.coord_scale * nn.MSELoss(size_average=False)(w * coord_mask, tw * coord_mask) / 2.0\n            loss_h = self.coord_scale * nn.MSELoss(size_average=False)(h * coord_mask, th * coord_mask) / 2.0\n            loss_conf = nn.MSELoss(size_average=False)(conf * conf_mask, tconf * conf_mask) / 2.0\n            loss_cls = self.class_scale * nn.CrossEntropyLoss(size_average=False)(cls, tcls)\n            loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls\n            t4 = time.time()\n            if False:\n                print('-----------------------------------')\n                print('        activation : %f' % (t1 - t0))\n                print(' create pred_boxes : %f' % (t2 - t1))\n                print('     build targets : %f' % (t3 - t2))\n                print('       create loss : %f' % (t4 - t3))\n                print('             total : %f' % (t4 - t0))\n            print('%d: nGT %d, recall %d, proposals %d, loss: x %f, y %f, w %f, h %f, conf %f, cls %f, total %f' % (\n            self.seen, nGT, nCorrect, nProposals, loss_x.data[0], loss_y.data[0], loss_w.data[0], loss_h.data[0],\n            loss_conf.data[0], loss_cls.data[0], loss.data[0]))\n            return loss\n        else:\n            if self.model_out:\n                return output\n            else:\n                masked_anchors = []\n                for m in self.anchor_mask:\n                    masked_anchors += self.anchors[m * self.anchor_step:(m + 1) * self.anchor_step]\n                masked_anchors = [anchor / self.stride for anchor in masked_anchors]\n\n                # boxes = get_region_boxes_in_model(output.data, self.thresh, self.num_classes, masked_anchors, len(self.anchor_mask))\n\n                return yolo_forward(output, self.thresh, self.num_classes, masked_anchors, len(self.anchor_mask))\n\n                # return boxes\n"""
