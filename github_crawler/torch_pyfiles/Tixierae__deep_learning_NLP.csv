file_path,api_count,code
CNN_IMDB/imdb_main.py,0,"b'# see companion handout for a theoretical intro: http://www.lix.polytechnique.fr/~anti5662/intro_cnn_lstm_tixier.pdf\n# gets to ~0.895 accuracy on the test set usually within 2 to 4 epochs (~160s per epoch on NVidia TITAN)\n# tested on ubuntu with Python 3, Keras version 1.1.0., tensorflow backend\n\nimport csv\nimport json\nimport numpy as np\n\nfrom gensim.models.word2vec import Word2Vec\n\nfrom keras.models import Model\nfrom keras.callbacks import EarlyStopping\nfrom keras.constraints import maxnorm\nfrom keras.layers import Convolution1D, GlobalMaxPooling1D, Dense, Embedding, Input, Merge, Dropout\n\nprint(\'packages loaded\')\n\npath_to_IMDB = \'\'\nuse_pretrained = True # when using pre-trained embeddings, convergence is faster, and absolute accuracy is slightly greater (the margin is bigger when max_size is small)\ndo_early_stopping = True\nmodel_save = False\nif model_save:\n    name_save = \'imdb_cnn_pre_400_04_04_17\'\n    print(\'model will be saved under name:\',name_save)\n\n# ========== parameter values ==========\n\nmax_features = int(2e4)\nstpwd_thd = 10 # TODO: allow greater values than 1 - should be fixed now\nmax_size = int(4e2)\nword_vector_dim = int(3e2)\ndo_non_static = True\nnb_filters = 200\ndrop_rate = 0.3\nbatch_size = 32\nnb_epoch =  10 # increasing the number of epochs may lead to overfitting when max_size is small (especially since dataset is small in the 1st place)\nmy_optimizer = \'adam\' # proved better than SGD and Adadelta\nmy_patience = 2 # for early stopping strategy\n\nif not use_pretrained:\n    # if the embeddings are initialized randomly, using static mode doesn\'t make sense\n    do_non_static = True\n    print(""not using pre-trained embeddings, overwriting \'do_non_static\' argument"")\n\nprint(\'=== parameter values: ===\')\nprint(\'top\',max_features,\'words used as features\')\nprint(\'top\',stpwd_thd,\'words excluded\')\nprint(\'max size of doc (in words):\',max_size)\nprint(\'dim of word vectors:\',word_vector_dim)\nprint(\'non-static:\',do_non_static)\nprint(\'number of filters applied to each region:\',nb_filters)\nprint(\'dropout rate:\',drop_rate)\nprint(\'batch size:\',batch_size)\nprint(\'number of epochs:\',nb_epoch)\nprint(\'optimizer:\',my_optimizer)\nprint(\'patience:\',my_patience)\nprint(\'=== end parameter values ===\')\n\n# ========== read pre-processed data ==========\n\n# dictionary of word indexes (sorted by decreasing frequency across the corpus)\n# this is a 1-based index - 0 is reserved for zero-padding\nwith open(path_to_IMDB + \'word_to_index.json\', \'r\') as my_file:\n    word_to_index = json.load(my_file)\n\nwith open(path_to_IMDB + \'training.csv\', \'r\') as my_file:\n    reader = csv.reader(my_file, delimiter=\',\')\n    x_train = list(reader)\n\nwith open(path_to_IMDB + \'test.csv\', \'r\') as my_file:\n    reader = csv.reader(my_file, delimiter=\',\')\n    x_test = list(reader)\n\nwith open(path_to_IMDB + \'training_labels.txt\', \'r\') as my_file:\n    y_train = my_file.read().splitlines()\n\nwith open(path_to_IMDB + \'test_labels.txt\', \'r\') as my_file:\n    y_test = my_file.read().splitlines()\n\n# turn lists of strings into lists of integers\nx_train = [[int(elt) for elt in sublist] for sublist in x_train]\nx_test = [[int(elt) for elt in sublist] for sublist in x_test]  \n\ny_train = [int(elt) for elt in y_train]\ny_test = [int(elt) for elt in y_test]\n\nprint(\'data loaded\')\n\n# ========== pruning ==========\n\n# only take into account the \'max_features\' most frequent words\n# disregard the \'stopword_threhsold\' most frequent words\n\nx_train = [[elt for elt in rev if elt<=max_features and elt>=stpwd_thd] for rev in x_train]\nx_test =  [[elt for elt in rev if elt<=max_features and elt>=stpwd_thd] for rev in x_test]\n\nprint(\'pruning done\')\n\n# ========== truncation and padding ==========\n\n# truncate reviews of size larger than \'max_size\' to their \'max_size\' first words\nx_train = [rev[:max_size] for rev in x_train]\nx_test = [rev[:max_size] for rev in x_test]\n\n# pad reviews shorter than \'max_size\' with zeroes\n# the vector of the 0th index will be set to all zeroes (zero padding strategy)\n\nprint(\'padding\',len([elt for elt in x_train if len(elt)<max_size])\n,\'reviews from the training set\')\n\nx_train = [rev+[0]*(max_size-len(rev)) if len(rev)<max_size else rev for rev in x_train]\n\n# sanity check: all reviews should now be of size \'max_size\'\nif max_size == list(set([len(rev) for rev in x_train]))[0]:\n    print(\'1st sanity check passed\')\nelse:\n    print(\'1st sanity check failed !\')\n\nprint(\'padding\',len([elt for elt in x_test if len(elt)<max_size])\n,\'reviews from the test set\')\n\nx_test = [rev+[0]*(max_size-len(rev)) if len(rev)<max_size else rev for rev in x_test]\n\nif max_size == list(set([len(rev) for rev in x_test]))[0]:\n    print(\'2nd sanity check passed\')\nelse:\n    print(\'2nd sanity check failed !\')\n\nprint(\'truncation and padding done\')\n\n# ========== loading pre-trained word vectors ==========\n\n# invert mapping\nindex_to_word = dict((v,k) for k, v in word_to_index.items())\n\n# to display the \'stopwords\'\nprint(\'stopwords are:\',[index_to_word[idx] for idx in range(1,stpwd_thd)])\n\n# convert integer reviews into word reviews\nx_full = x_train + x_test\nx_full_words = [[index_to_word[idx] for idx in rev if idx!=0] for rev in x_full]\nall_words = [word for rev in x_full_words for word in rev]\n\nprint(len(all_words),\'words\')\nprint(len(list(set(all_words))),\'unique words\')\n\nif use_pretrained:\n\n    # initialize word vectors\n    word_vectors = Word2Vec(size=word_vector_dim, min_count=1)\n\n    # create entries for the words in our vocabulary\n    word_vectors.build_vocab(x_full_words)\n\n    # sanity check\n    if len(list(set(all_words))) == len(word_vectors.wv.vocab):\n        print(\'3rd sanity check passed\')\n    else:\n        print(\'3rd sanity check failed !\')\n\n    # fill entries with the pre-trained word vectors\n    path_to_pretrained_wv = \'\'\n    word_vectors.intersect_word2vec_format(path_to_pretrained_wv + \'GoogleNews-vectors-negative300.bin\', binary=True)\n\n    print(\'pre-trained word vectors loaded\')\n\n    # NOTE: in-vocab words without an entry in the binary file are not removed from the vocabulary\n    # instead, their vectors are silently initialized to random values\n\n    # if necessary, we can detect those vectors via their norms which approach zero\n    #norms = [np.linalg.norm(word_vectors[word]) for word in word_vectors.wv.vocab.keys()]\n    #idxs_zero_norms = [idx for idx, norm in enumerate(norms) if norm<0.05]\n    # most of those words are proper nouns, like patton, deneuve, etc.\n    # they don\'t have an entry in the word vectors because we lowercased the text\n    #no_entry_words = [word_vectors.wv.vocab.keys()[idx] for idx in idxs_zero_norms]\n\n    # create numpy array of embeddings\n    embeddings = np.zeros((max_features + 1,word_vector_dim))\n    for word in word_vectors.wv.vocab.keys():\n        idx = word_to_index[word]\n        # word_to_index is 1-based! the 0-th row, used for padding, stays at zero\n        embeddings[idx,] = word_vectors[word]\n\n    print(\'embeddings created\')\n\nelse:\n    print(\'not using pre-trained embeddings\')\n\n# ========== training CNN ==========\n\n#max([max(elt) for elt in x_full])\n\nmy_input = Input(shape=(max_size,), dtype=\'int32\') # for some reason here it is important to let the second argument of shape blank\n\n# NOTE: create embedding tables with dimensions based on max_features, ignoring stopword removal and truncation\n# for instance if initially max_features = 2e4, reviews will be composed of integers from 1 to 2e4\n# but after truncation and stopword removal, the actual length of the vocabulary (number of unique integer values) may be much smaller\n# but if input dim is based on this final voc size, some integers still present (in the range [1,2e4]) won\'t have a row anymore\n# so we create the embedding lookup table based on original max_features value, knowing that the words that have been removed just won\'t be looked up\n\nif use_pretrained:\n    embedding = Embedding(input_dim = max_features + 1, # vocab size, including the 0-th word used for padding\n                          output_dim = word_vector_dim,\n                          #input_length = max_size, # length of input sequences\n                          dropout = drop_rate, # see http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf\n                          #embeddings_constraint = maxnorm(3.),\n                          weights=[embeddings], # we pass our pre-trained embeddings\n                          trainable = do_non_static\n                          ) (my_input)\nelse:\n    embedding = Embedding(input_dim = max_features + 1, # vocab size, including the 0-th word used for padding\n                          output_dim = word_vector_dim,\n                          #input_length = max_size, # length of input sequences\n                          dropout = drop_rate, # see http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf\n                          #embeddings_constraint = maxnorm(3.),\n                          trainable = do_non_static\n                          ) (my_input)\n\n# size of the feature map should be equal to max_size-filter_length+1\nconv_1 = Convolution1D(nb_filter = nb_filters,\n                       filter_length = 4, # region size\n                       activation=\'relu\',\n                       ) (embedding)\n\npooled_conv_1 = GlobalMaxPooling1D() (conv_1)\n\nconv_2 = Convolution1D(nb_filter = nb_filters,\n                       filter_length = 5, # region size\n                       activation=\'relu\',\n                       ) (embedding)\n\npooled_conv_2 = GlobalMaxPooling1D() (conv_2)\n\nconv_3 = Convolution1D(nb_filter = nb_filters,\n                       filter_length = 6, # region size\n                       activation=\'relu\',\n                       ) (embedding)\n\npooled_conv_3 = GlobalMaxPooling1D() (conv_3)\n\nmerge = Merge(mode=\'concat\') ([pooled_conv_1,pooled_conv_2,pooled_conv_3])\n\nmerge_dropped = Dropout(drop_rate) (merge) # adding this layer improved test set accuracy by almost 2%\n\n# we finally project onto a single unit output layer with sigmoi activation\nprob = Dense(output_dim = 1, # dimensionality of the output space\n             activation=\'sigmoid\'#,\n             #W_constraint = maxnorm(3.) # constrain L-2 norm of the weights. Slows up convergence (more epochs needed), but does not improve performance. In most recent version of Keras this argument has been renamed \'kernel_constraint\'\n             ) (merge_dropped)\n\nmodel = Model(my_input, prob)\n\nmodel.compile(loss=\'binary_crossentropy\',\n              optimizer=my_optimizer,\n              metrics=[\'accuracy\'])\n\nprint(\'model compiled\')\n\nearly_stopping = EarlyStopping(monitor=\'val_loss\', # go through epochs as long as loss on validation set decreases\n                               patience = my_patience,\n                               mode = \'min\')\nif do_early_stopping:\n    print(\'using early stopping strategy\')\n    \n    model.fit(x_train, \n              y_train,\n              batch_size = batch_size,\n              nb_epoch = nb_epoch,\n              validation_data = (x_test, y_test),\n              callbacks = [early_stopping])\nelse:\n    \n    model.fit(x_train, \n              y_train,\n              batch_size = batch_size,\n              nb_epoch = nb_epoch,\n              validation_data = (x_test, y_test),\n              )\n\n# persist model to disk\nif model_save:\n    \n    model.save(path_to_IMDB + name_save)\n    \n    print(\'model saved to disk\')\n\n#loss, acc = model.evaluate(x_test, y_test, batch_size = batch_size)\n\n#print(\'final accuracy on test set:\', acc)\n'"
CNN_IMDB/imdb_preprocess.py,0,"b'# goes with the following handout: http://www.lix.polytechnique.fr/~anti5662/intro_cnn_lstm_tixier.pdf\n# data initially taken from: http://ai.stanford.edu/~amaas/data/sentiment/\n\nimport os\nimport re\nimport random\nimport operator\nimport csv\nimport json\nfrom nltk import pos_tag\nfrom collections import Counter\nfrom bs4 import BeautifulSoup\n\npath_root = \'C:\\\\Users\\\\mvazirg\\\\\'\n\npath_to_IMDB = path_root + \'Desktop\\\\IMDB\\\\aclImdb\\\\\'\n\n# ========== read pos/neg reviews from the training and test sets ==========\n\ntrain_test = [\'train\',\'test\']\nneg_pos = [\'neg\',\'pos\']\n\nreviews = []\nlabels = []\n\nfor elt in train_test:\n    print(\'==== going over\', elt, \'set ====\')\n    my_path = path_to_IMDB + elt + \'\\\\\'\n    for p, pol in enumerate(neg_pos):\n        review_names = os.listdir(my_path + pol + \'\\\\\')\n        print(len(review_names), pol, \'reviews found\')\n        counter = 0\n        for review_name in review_names:\n            # read .txt file\n            with open(my_path + pol + \'\\\\\' + review_name, \'r\') as my_file: \n                reviews.append(my_file.read())\n                labels.append(p)\n                if counter % 1e3 == 0:\n                    print(counter, \'/\', len(review_names), \'reviews read\')\n                counter += 1\n\nprint(len(reviews), \'reviews and\', len(labels), \'labels assigned\')\n\n# ========== clean reviews ==========\n\n# regex to match intra-word apostrophes\nregex_ap = re.compile(r""(\\b[\']\\b)|[\\W_]"")\n\ncleaned_reviews = []\ncounter = 0\n\nfor rev in reviews:\n    # remove HTML formatting\n    temp = BeautifulSoup(rev)\n    text = temp.get_text()\n    text = text.lower()\n    # replace punctuation with whitespace. TODO: keep emoticons\n    # note that we exclude apostrophes from our list of punctuation marks: we want to keep don\'t, shouldn\'t etc.\n    text = re.sub(r\'[()\\[\\]{}.,;:!?\\<=>?@^_`~#$%""&*-]\', \' \', text)\n    # remove apostrophes that are not intra-word\n    text = regex_ap.sub(lambda x: (x.group(1) if x.group(1) else \' \'), text)\n    # strip extra white space\n    text = re.sub(\' +\',\' \',text)\n    # strip leading and trailing white space\n    text = text.strip()\n    # tokenize\n    tokens = text.split()\n    # remove single letter tokens (we don\'t remove stopwords as some of them might be useful in determining polarity, like not, but...)\n    tokens = [tok for tok in tokens if len(tok)>1]\n    # POS tag\n    #tagged_tokens = pos_tag(tokens)\n    # convert to lower case words that are not identified as proper nouns\n    #tokens = [token.lower() if tagged_tokens[idx][1]!=\'NNP\' else token for idx,token in enumerate(tokens)]\n    # save\n    cleaned_reviews.append(tokens)\n    if counter % 1e3 == 0:\n        print(counter, \'/\', len(reviews), \'reviews cleaned\')\n    counter += 1\n\n# get list of tokens from all reviews\nall_tokens = [token for sublist in cleaned_reviews for token in sublist]\n\ncounts = dict(Counter(all_tokens))\n\nsorted_counts = sorted(counts.items(), key=operator.itemgetter(1), reverse=True)\n\n# assign to each word an index based on its frequency in the corpus\n# the most frequent word will get index equal to 1\n\nword_to_index = dict([(tuple[0],idx+1) for idx, tuple in enumerate(sorted_counts)])\n\n# save dictionary\nwith open(path_to_IMDB + \'word_to_index.json\', \'w\') as my_file:\n    json.dump(word_to_index, my_file, sort_keys=True, indent=4)\n\n# e.g., \'the\' is the most frequent word, ""don\'t"" is the 86th most recent word\nword_to_index[\'the\']\nword_to_index[""don\'t""]\n\n# transform each review into a list of word indexes\ncleaned_reviews_integers = []\ncounter = 0\n\nfor rev in cleaned_reviews:\n    sublist = []\n    for token in rev:\n        sublist.append(word_to_index[token])\n    cleaned_reviews_integers.append(sublist)\n    if counter % 1e4 == 0:\n        print(counter, \'/\', len(reviews), \'reviews cleaned\')\n    counter += 1\n\n# ========== split training/testing, shuffle, and save to disk ==========\n\nidx_split = 25000\ntraining_rev = cleaned_reviews_integers[:idx_split]\ntraining_labels = labels[:idx_split]\n\ntest_rev = cleaned_reviews_integers[idx_split:]\ntest_labels = labels[idx_split:]\n\nrandom.seed(3272017)\nshuffle_train = random.sample(range(len(training_rev)), len(training_rev))\nshuffle_test = random.sample(range(len(test_rev)), len(test_rev))\n\ntraining_rev = [training_rev[shuffle_train[elt]] for elt in shuffle_train]\ntraining_labels = [training_labels[shuffle_train[elt]] for elt in shuffle_train]\n\ntest_rev = [test_rev[shuffle_test[elt]] for elt in shuffle_test]\ntest_labels = [test_labels[shuffle_test[elt]] for elt in shuffle_test]\n\n# \'wb\' instead of \'w\' ensures that the rows are not separated with blank lines\nwith open(path_to_IMDB + \'training.csv\', \'wb\') as my_file:\n    writer = csv.writer(my_file, quoting=csv.QUOTE_NONE)\n    writer.writerows(training_rev)\n\nwith open(path_to_IMDB + \'training_labels.txt\', \'wb\') as my_file:\n    for label in training_labels:\n        my_file.write(str(label) + \'\\n\')\n \nwith open(path_to_IMDB + \'test.csv\', \'wb\') as my_file:\n    writer = csv.writer(my_file, quoting=csv.QUOTE_NONE)\n    writer.writerows(test_rev)\n\nwith open(path_to_IMDB + \'test_labels.txt\', \'wb\') as my_file:\n    for label in test_labels:\n        my_file.write(str(label) + \'\\n\')\n\nprint(\'all results saved to disk\')'"
CNN_IMDB/imdb_preprocess_new.py,0,"b'import os\nimport re\nimport random\nimport operator\nimport csv\nimport json\nfrom nltk.tokenize import TweetTokenizer\nfrom collections import Counter\nfrom bs4 import BeautifulSoup\n\npath_root = \'C:\\\\Users\\\\mvazirg\\\\\'\npath_to_IMDB = path_root + \'Desktop\\\\IMDB\\\\aclImdb\\\\\'\n\n# ========== read pos/neg reviews from the training and test sets ==========\n\ntrain_test = [\'train\',\'test\']\nneg_pos = [\'neg\',\'pos\']\n\nreviews = []\nlabels = []\n\nfor elt in train_test:\n    print(\'==== going over\', elt, \'set ====\')\n    my_path = path_to_IMDB + elt + \'\\\\\'\n    for p, pol in enumerate(neg_pos):\n        review_names = os.listdir(my_path + pol + \'\\\\\')\n        print(len(review_names), pol, \'reviews found\')\n\n        for counter,review_name in enumerate(review_names):\n            # read .txt file\n            with open(my_path + pol + \'\\\\\' + review_name, \'r\', encoding=\'utf8\') as my_file: \n                reviews.append(my_file.read())\n                labels.append(p)\n                if counter % round(len(review_names)/10) == 0:\n                    print(counter, \'/\', len(review_names), \'reviews read\')\n\nprint(len(reviews), \'reviews and\', len(labels), \'labels assigned\')\n\n# ========== clean reviews ==========\n\ntokenizer = TweetTokenizer()\n\ncleaned_reviews = []\n\nfor counter,rev in enumerate(reviews):\n    # remove HTML formatting\n    temp = BeautifulSoup(rev)\n    text = temp.get_text()\n    # strip extra white space\n    text = re.sub(\' +\',\' \',text)\n    # strip leading and trailing white space\n    text = text.strip()\n    # tokenize\n    tokens = tokenizer.tokenize(text)\n    cleaned_reviews.append(tokens)\n    if counter % round(len(reviews)/10) == 0:\n        print(counter, \'/\', len(reviews), \'reviews cleaned\')\n\n# get list of tokens from all reviews\nall_tokens = [token for sublist in cleaned_reviews for token in sublist]\n\ncounts = dict(Counter(all_tokens))\nsorted_counts = sorted(counts.items(), key=operator.itemgetter(1), reverse=True)\n\n# assign to each word an index based on its frequency in the corpus\n# the most frequent word will get index equal to 1\nword_to_index = dict([(tuple[0],idx+1) for idx, tuple in enumerate(sorted_counts)])\n\n# save dictionary\nwith open(path_to_IMDB + \'word_to_index_new.json\', \'w\') as my_file:\n    json.dump(word_to_index, my_file, sort_keys=True, indent=4)\n\n# e.g., \'the\' is the most frequent word, ""don\'t"" is the 86th most recent word\nword_to_index[\'the\']\nword_to_index[""don\'t""]\n\n# transform each review into a list of word indexes\ncleaned_reviews_integers = []\n\nfor counter,rev in enumerate(cleaned_reviews):\n    sublist = []\n    for token in rev:\n        sublist.append(word_to_index[token])\n    cleaned_reviews_integers.append(sublist)\n    if counter % round(len(cleaned_reviews)/10) == 0:\n        print(counter, \'/\', len(reviews), \'reviews cleaned\')\n\n# ========== split training/testing, shuffle, and save to disk ==========\n\nidx_split = 25000\ntraining_rev = cleaned_reviews_integers[:idx_split]\ntraining_labels = labels[:idx_split]\n\ntest_rev = cleaned_reviews_integers[idx_split:]\ntest_labels = labels[idx_split:]\n\nrandom.seed(3272017)\nshuffle_train = random.sample(range(len(training_rev)), len(training_rev))\nshuffle_test = random.sample(range(len(test_rev)), len(test_rev))\n\ntraining_rev = [training_rev[shuffle_train[elt]] for elt in shuffle_train]\ntraining_labels = [training_labels[shuffle_train[elt]] for elt in shuffle_train]\n\ntest_rev = [test_rev[shuffle_test[elt]] for elt in shuffle_test]\ntest_labels = [test_labels[shuffle_test[elt]] for elt in shuffle_test]\n\n# \'wb\' instead of \'w\' ensures that the rows are not separated with blank lines\nwith open(path_to_IMDB + \'training_new.csv\',\'w\',newline=\'\') as my_file:\n    writer = csv.writer(my_file, quoting=csv.QUOTE_NONE)\n    writer.writerows(training_rev)\n\nwith open(path_to_IMDB + \'training_labels_new.txt\',\'w\',newline=\'\') as my_file:\n    for label in training_labels:\n        my_file.write(str(label) + \'\\n\')\n\nwith open(path_to_IMDB + \'test_new.csv\',\'w\',newline=\'\') as my_file:\n    writer = csv.writer(my_file, quoting=csv.QUOTE_NONE)\n    writer.writerows(test_rev)\n\nwith open(path_to_IMDB + \'test_labels_new.txt\',\'w\',newline=\'\') as my_file:\n    for label in test_labels:\n        my_file.write(str(label) + \'\\n\')\n\nprint(\'all results saved to disk\')'"
CNN_MNIST/mnist_cnn.py,0,"b'# initially based on https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py\n\n# gets to ~0.9940 accuracy on test set after 12 epochs (~30s per epoch on NVidia GTX TITAN, 1060s per epoch on Intel i7)\n# tested on ubuntu with Python 3, Keras version 1.2.2, tensorflow backend\n\nfrom keras.datasets import mnist\nfrom keras.layers import Dense, Dropout, Flatten, Input, Convolution2D, MaxPooling2D, Merge\n\nfrom keras.utils import np_utils\nfrom keras.models import Model\n\n#import matplotlib.pyplot as plt\nimport numpy as np\n\nbatch_size = 64\nnum_classes = 10\nnb_epochs = 12\nnb_filters = 64\nmy_optimizer = \'adadelta\' # proved better than \'adam\' in my experiments\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, shuffled and split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# 60,000 training examples - each is of size 28 by 28\n# Pixel values are 0 to 255. 0=white, 255=black \n# plot first 4 images\n#plt.subplot(221)\n#plt.imshow(x_train[0], cmap=plt.get_cmap(\'gray_r\'))\n#plt.subplot(222)\n#plt.imshow(x_train[1], cmap=plt.get_cmap(\'gray_r\'))\n#plt.subplot(223)\n#plt.imshow(x_train[2], cmap=plt.get_cmap(\'gray_r\'))\n#plt.subplot(224)\n#plt.imshow(x_train[3], cmap=plt.get_cmap(\'gray_r\'))\n# show the plot\n#plt.show()\n\n# In 2D, ""channels_last"" assumes (rows, cols, channels) while ""channels_first"" assumes (channels, rows, cols). \n\n# there is only one channel here (levels of grey), so we need to create a dim here\n# for color pictures we would have 3: RGB\n\n# option \'channels last\'\nx_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\nx_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\ninput_shape = (img_rows, img_cols, 1)\t\n\nx_train = x_train.astype(\'float32\')\nx_test = x_test.astype(\'float32\')\n\nmy_max = np.amax(x_train)\n\nx_train /= my_max # divide by 255 (max value) to have all values between 0 and 1\nx_test /= my_max\n\nprint(\'x_train shape:\', x_train.shape)\nprint(x_train.shape[0], \'train samples\')\nprint(x_test.shape[0], \'test samples\')\n\n# transforms integers labels into one-hot flags of length ncol\ny_train = np_utils.to_categorical(y_train, num_classes)\ny_test = np_utils.to_categorical(y_test, num_classes)\n\nmy_input = Input(shape=input_shape, dtype=\'float32\')\n\nconv_1 = Convolution2D(nb_filters, 3, 3, # region size is (3, 3)\n                       border_mode = \'valid\',\n\t\t\t\t\t   activation = \'relu\', \n                       #input_shape=input_shape\n\t\t\t\t\t  ) (my_input)\n# output is of dim  [(w - f + 2p) / s] + 1, where w is input size, f is filter size, s is stride, and p is amount of zero padding\n# [28 - 3 + 2*0 / 1] + 1 all theory is in here: http://cs231n.github.io/convolutional-networks/\n\npooled_conv_1 = MaxPooling2D(pool_size=(2,2)) (conv_1)\npooled_conv_1_dropped = Dropout(0.2) (pooled_conv_1)\n\nconv_11 = Convolution2D(nb_filters, 3, 3,\n                       border_mode = \'valid\',\n\t\t\t\t\t   activation = \'relu\', \n                       #input_shape=input_shape\n\t\t\t\t\t  ) (pooled_conv_1_dropped)\npooled_conv_11 = MaxPooling2D(pool_size=(2,2)) (conv_11)\npooled_conv_11_dropped = Dropout(0.2) (pooled_conv_11)\n\npooled_conv_11_dropped_flat = Flatten()(pooled_conv_11_dropped)\n\n# increasing the number of different filter sizes proved better than increasing depth of each individually in my experiments\n\n# ====\n\nconv_2 = Convolution2D(nb_filters, 4, 4, \n                       border_mode = \'valid\',\n\t\t\t\t\t   activation = \'relu\', \n                       #input_shape=input_shape\n\t\t\t\t\t  ) (my_input)\npooled_conv_2 = MaxPooling2D(pool_size=(2,2)) (conv_2)\npooled_conv_2_dropped = Dropout(0.2) (pooled_conv_2)\n\nconv_22 = Convolution2D(nb_filters, 4, 4, \n                       border_mode = \'valid\',\n\t\t\t\t\t   activation = \'relu\', \n                       #input_shape=input_shape\n\t\t\t\t\t  ) (pooled_conv_2_dropped)\npooled_conv_22 = MaxPooling2D(pool_size=(2,2)) (conv_22)\npooled_conv_22_dropped = Dropout(0.2) (pooled_conv_22)\n\npooled_conv_22_dropped_flat = Flatten()(pooled_conv_22_dropped)\n\n# ====\n\nconv_3 = Convolution2D(nb_filters, 5, 5, \n                       border_mode = \'valid\',\n\t\t\t\t\t   activation = \'relu\', \n                       #input_shape=input_shape\n\t\t\t\t\t  ) (my_input)\npooled_conv_3 = MaxPooling2D(pool_size=(2,2)) (conv_3)\npooled_conv_3_dropped = Dropout(0.2) (pooled_conv_3)\n\nconv_33 = Convolution2D(nb_filters, 2, 2, \n                       border_mode = \'valid\',\n\t\t\t\t\t   activation = \'relu\', \n                       #input_shape=input_shape\n\t\t\t\t\t  ) (pooled_conv_3_dropped)\npooled_conv_33 = MaxPooling2D(pool_size=(2,2)) (conv_33)\npooled_conv_33_dropped = Dropout(0.2) (pooled_conv_33)\n\npooled_conv_33_dropped_flat = Flatten()(pooled_conv_33_dropped)\n\n# ====\n\n# ====\n\nconv_4 = Convolution2D(nb_filters, 6, 6, \n                       border_mode = \'valid\',\n\t\t\t\t\t   activation = \'relu\', \n                       #input_shape=input_shape\n\t\t\t\t\t  ) (my_input)\npooled_conv_4 = MaxPooling2D(pool_size=(2,2)) (conv_4)\npooled_conv_4_dropped = Dropout(0.2) (pooled_conv_4)\n\nconv_44 = Convolution2D(nb_filters, 6, 6, \n                       border_mode = \'valid\',\n\t\t\t\t\t   activation = \'relu\', \n                       #input_shape=input_shape\n\t\t\t\t\t  ) (pooled_conv_4_dropped)\npooled_conv_44 = MaxPooling2D(pool_size=(2,2)) (conv_44)\npooled_conv_44_dropped = Dropout(0.2) (pooled_conv_44)\n\npooled_conv_44_dropped_flat = Flatten()(pooled_conv_44_dropped)\n\n# ====\n\nmerge = Merge(mode=\'concat\') ([pooled_conv_11_dropped_flat,pooled_conv_22_dropped_flat,pooled_conv_33_dropped_flat,pooled_conv_44_dropped_flat])\nmerge_dropped = Dropout(0.2) (merge)\n\ndense = Dense(128,\n             activation=\'relu\'\n\t\t\t) (merge_dropped)\ndense_dropped = Dropout(0.2) (dense)\n\nprob = Dense(output_dim = num_classes, # we output a prob distribution over the classes\n             activation=\'softmax\'\n\t\t\t) (dense_dropped)\n\nmodel = Model(my_input, prob)\n\nprint([layer.output_shape for layer in model.layers])\n\nmodel.compile(loss=\'categorical_crossentropy\',\n\t\t\t  optimizer=my_optimizer,\n\t\t\t  metrics=[\'accuracy\'])\n\nmodel.fit(x_train, \n\t\t  y_train, \n\t\t  batch_size = batch_size, \n\t\t  nb_epoch = nb_epochs,\n\t\t  validation_data = (x_test, y_test)\n\t\t )\n\t\t  '"
HAN/AttentionWithContext.py,0,"b'# this script was taken from https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2\n\nimport keras.backend as K\nfrom keras.layers import Layer\nfrom keras import initializers, regularizers, constraints\n\ndef dot_product(x, kernel):\n    """"""\n    https://github.com/richliao/textClassifier/issues/13#issuecomment-377323318\n    Wrapper for dot product operation, in order to be compatible with both\n    Theano and Tensorflow\n    Args:\n        x (): input\n        kernel (): weights\n    Returns:\n    """"""\n    if K.backend() == \'tensorflow\':\n        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n    else:\n        return K.dot(x, kernel)\n    \n\nclass AttentionWithContext(Layer):\n    """"""\n    Attention operation, with a context/query vector, for temporal data.\n    Supports Masking.\n    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n    ""Hierarchical Attention Networks for Document Classification""\n    by using a context vector to assist the attention\n    # Input shape\n        3D tensor with shape: `(samples, steps, features)`.\n    # Output shape\n        2D tensor with shape: `(samples, features)`.\n    \n    How to use:\n    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n    The dimensions are inferred based on the output shape of the RNN.\n    \n    Note: The layer has been tested with Keras 2.0.6\n    \n    Example:\n        model.add(LSTM(64, return_sequences=True))\n        model.add(AttentionWithContext())\n        # next add a Dense layer (for classification/regression) or whatever...\n    """"""\n    \n    def __init__(self, return_coefficients=False,\n                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n                 W_constraint=None, u_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.return_coefficients = return_coefficients\n        self.init = initializers.get(\'glorot_uniform\')\n        \n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.u_regularizer = regularizers.get(u_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n        \n        self.W_constraint = constraints.get(W_constraint)\n        self.u_constraint = constraints.get(u_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n        \n        self.bias = bias\n        super(AttentionWithContext, self).__init__(**kwargs)\n    \n    def build(self, input_shape):\n        assert len(input_shape) == 3\n        \n        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n                                 initializer=self.init,\n                                 name=\'{}_W\'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        if self.bias:\n            self.b = self.add_weight((input_shape[-1],),\n                                     initializer=\'zero\',\n                                     name=\'{}_b\'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        \n        self.u = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name=\'{}_u\'.format(self.name),\n                                 regularizer=self.u_regularizer,\n                                 constraint=self.u_constraint)\n        \n        super(AttentionWithContext, self).build(input_shape)\n    \n    def compute_mask(self, input, input_mask=None):\n        # do not pass the mask to the next layers\n        return None\n    \n    def call(self, x, mask=None):\n        uit = dot_product(x, self.W)\n        \n        if self.bias:\n            uit += self.b\n        \n        uit = K.tanh(uit)\n        ait = dot_product(uit, self.u)\n        \n        a = K.exp(ait)\n        \n        # apply mask after the exp. will be re-normalized next\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in theano\n            a *= K.cast(mask, K.floatx())\n        \n        # in some cases especially in the early stages of training the sum may be almost zero\n        # and this results in NaN\'s. A workaround is to add a very small positive number \xce\xb5 to the sum.\n        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n        \n        a = K.expand_dims(a)\n        weighted_input = x * a\n        \n        if self.return_coefficients:\n            return [K.sum(weighted_input, axis=1), a]\n        else:\n            return K.sum(weighted_input, axis=1)\n    \n    \n    \n    def compute_output_shape(self, input_shape):\n        if self.return_coefficients:\n            return [(input_shape[0], input_shape[-1]), (input_shape[0], input_shape[-1], 1)]\n        else:\n            return input_shape[0], input_shape[-1]\n\n'"
HAN/CyclicalLearningRate.py,0,"b'# this script was taken from https://github.com/bckenstler/CLR\n\nfrom keras.callbacks import *\n\nclass CyclicLR(Callback):\n    """"""This callback implements a cyclical learning rate policy (CLR).\n    The method cycles the learning rate between two boundaries with\n    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n    The amplitude of the cycle can be scaled on a per-iteration or \n    per-cycle basis.\n    This class has three built-in policies, as put forth in the paper.\n    ""triangular"":\n        A basic triangular cycle w/ no amplitude scaling.\n    ""triangular2"":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    ""exp_range"":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n        cycle iteration.\n    For more detail, please see paper.\n    \n    # Example\n        ```python\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., mode=\'triangular\')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n    \n    Class also supports custom scaling functions:\n        ```python\n            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., scale_fn=clr_fn,\n                                scale_mode=\'cycle\')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```    \n    # Arguments\n        base_lr: initial learning rate which is the\n            lower boundary in the cycle.\n        max_lr: upper boundary in the cycle. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore \n            max_lr may not actually be reached depending on\n            scaling function.\n        step_size: number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch.\n        mode: one of {triangular, triangular2, exp_range}.\n            Default \'triangular\'.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n        gamma: constant in \'exp_range\' scaling function:\n            gamma**(cycle iterations)\n        scale_fn: Custom scaling policy defined by a single\n            argument lambda function, where \n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored \n        scale_mode: {\'cycle\', \'iterations\'}.\n            Defines whether scale_fn is evaluated on \n            cycle number or cycle iterations (training\n            iterations since start of cycle). Default is \'cycle\'.\n    """"""\n\n    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode=\'triangular\',\n                 gamma=1., scale_fn=None, scale_mode=\'cycle\'):\n        super(CyclicLR, self).__init__()\n\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == \'triangular\':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = \'cycle\'\n            elif self.mode == \'triangular2\':\n                self.scale_fn = lambda x: 1/(2.**(x-1))\n                self.scale_mode = \'cycle\'\n            elif self.mode == \'exp_range\':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = \'iterations\'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        #self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        """"""Resets cycle iterations.\n        Optional boundary/step size adjustment.\n        """"""\n        if new_base_lr != None:\n            self.base_lr = new_base_lr\n        if new_max_lr != None:\n            self.max_lr = new_max_lr\n        if new_step_size != None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n        \n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n        if self.scale_mode == \'cycle\':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n        \n    def on_train_begin(self, logs={}):\n        #logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())        \n            \n    def on_batch_end(self, epoch, logs=None):\n        \n        #logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n\n        #self.history.setdefault(\'lr\', []).append(K.get_value(self.model.optimizer.lr))\n        #self.history.setdefault(\'iterations\', []).append(self.trn_iterations)\n\n        #for k, v in logs.items():\n        #    self.history.setdefault(k, []).append(v)\n        \n        K.set_value(self.model.optimizer.lr, self.clr())\n'"
HAN/CyclicalMomentum.py,0,"b'from keras.callbacks import *\n\nclass CyclicMT(Callback):\n    def __init__(self, base_mt=0.85, max_mt=0.95, step_size=2000., mode=\'triangular\',\n                 gamma=1., scale_fn=None, scale_mode=\'cycle\'):\n        super(CyclicMT, self).__init__()\n\n        self.base_mt = base_mt\n        self.max_mt = max_mt\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == \'triangular\':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = \'cycle\'\n            elif self.mode == \'triangular2\':\n                self.scale_fn = lambda x: 1/(2.**(x-1))\n                self.scale_mode = \'cycle\'\n            elif self.mode == \'exp_range\':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = \'iterations\'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.cmt_iterations = 0.\n        self.trn_iterations = 0.\n        #self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_mt=None, new_max_mt=None,\n               new_step_size=None):\n        """"""Resets cycle iterations.\n        Optional boundary/step size adjustment.\n        """"""\n        if new_base_mt != None:\n            self.base_mt = new_base_mt\n        if new_max_mt != None:\n            self.max_mt = new_max_mt\n        if new_step_size != None:\n            self.step_size = new_step_size\n        self.cmt_iterations = 0.\n        \n    def cmt(self):\n        cycle = np.floor(1+self.cmt_iterations/(2*self.step_size))\n        x = np.abs(self.cmt_iterations/self.step_size - 2*cycle + 1)\n        if self.scale_mode == \'cycle\':\n            return self.base_mt + (-self.max_mt+self.base_mt)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_mt + (-self.max_mt+self.base_mt)*np.maximum(0, (1-x))*self.scale_fn(self.cmt_iterations)\n        \n    def on_train_begin(self, logs={}):\n        #logs = logs or {}\n\n        if self.cmt_iterations == 0:\n            K.set_value(self.model.optimizer.momentum, self.base_mt)\n        else:\n            K.set_value(self.model.optimizer.momentum, self.cmt())        \n            \n    def on_batch_end(self, epoch, logs=None):\n        \n        #logs = logs or {}\n        self.trn_iterations += 1\n        self.cmt_iterations += 1\n\n        #self.history.setdefault(\'mt\', []).append(K.get_value(self.model.optimizer.momentum))\n        #self.history.setdefault(\'iterations\', []).append(self.trn_iterations)\n\n        #for k, v in logs.items():\n        #    self.history.setdefault(k, []).append(v)\n        \n        K.set_value(self.model.optimizer.momentum, self.cmt())\n'"
HAN/han_my_functions.py,0,"b""import re\nimport time\nimport numpy as np\nfrom random import shuffle\nfrom sklearn.metrics import confusion_matrix\n\nfrom keras import backend as K\nfrom keras.utils import to_categorical\nfrom keras.callbacks import Callback\nfrom keras.layers import Bidirectional, GRU, CuDNNGRU\n\ndef bidir_gru(my_seq,n_units,my_is_GPU):\n    '''\n    just a convenient wrapper for bidirectional RNN with GRU units\n    # regardless of whether training is done on GPU, can be loaded on CPU\n    # see: https://github.com/keras-team/keras/pull/9112\n    '''\n    if my_is_GPU:\n        return Bidirectional(CuDNNGRU(units=n_units,\n                                      return_sequences=True),\n                             merge_mode='concat', weights=None)(my_seq)\n    else:\n        return Bidirectional(GRU(units=n_units,\n                                 activation='tanh', \n                                 dropout=0.0,\n                                 recurrent_dropout=0.0,\n                                 implementation=1,\n                                 return_sequences=True,\n                                 reset_after=True,\n                                 recurrent_activation='sigmoid'),\n                             merge_mode='concat', weights=None)(my_seq)\n\ndef atoi(text):\n    return int(text) if text.isdigit() else text\n\ndef natural_keys(text):\n    return [atoi(c) for c in re.split('(\\d+)', text)]\n\ndef read_batches(my_batch_names,my_path_to_batches,do_shuffle,do_train,my_max_doc_size_overall,my_max_sent_size_overall,my_n_cats):\n\n    while True:\n        \n        my_batch_names_docs = [elt for elt in my_batch_names if '_doc_' in elt]\n        my_batch_names_labels = [elt for elt in my_batch_names if '_label_' in elt]\n        \n        # make sure each list is sorted the same way\n        my_batch_names_docs.sort(key=natural_keys)\n        my_batch_names_labels.sort(key=natural_keys)\n        \n        assert len(my_batch_names_docs) == len(my_batch_names_labels), 'doc and label # batches differ!'\n        \n        if do_shuffle:\n            my_idxs = list(range(len(my_batch_names_docs)))\n            shuffle(my_idxs)\n            my_batch_names_docs = [my_batch_names_docs[my_idx] for my_idx in my_idxs]\n            my_batch_names_labels = [my_batch_names_labels[my_idx] for my_idx in my_idxs]\n        \n        for (bnd,bnl) in zip(my_batch_names_docs,my_batch_names_labels):\n            \n            with open(my_path_to_batches + bnd, 'r', encoding='utf8') as file:\n                docs = file.read().splitlines()\n            \n            with open(my_path_to_batches + bnl, 'r', encoding='utf8') as file:\n                label_list = file.read().splitlines()\n            \n            doc_list = []\n            for doc in docs:\n                doc = doc.split('##S_S##') # split doc into sentences\n                doc = [elt.strip().split() for elt in doc] # split sentences into words\n                doc_list.append(doc)\n            \n            max_doc_size = min(max([len(d) for d in doc_list]),my_max_doc_size_overall)\n            max_sent_size = min(max([len(s) for d in doc_list for s in d]),my_max_sent_size_overall)\n            \n            # padding (doc level) - add or remove sentences\n            doc_list = [d+[[0]*max_sent_size]*(max_doc_size-len(d)) if len(d)<max_doc_size else d[:max_doc_size] for d in doc_list] \n            # padding (sent level) - add or remove words\n            doc_list = [[s+[0]*(max_sent_size-len(s)) if len(s)<max_sent_size else s[:max_sent_size] for s in d] for d in doc_list] \n            doc_array = np.array(doc_list,dtype='int64')\n            label_array = np.array(label_list,dtype='int64')\n            label_array = to_categorical(label_array,num_classes=my_n_cats)\n            \n            if do_train:\n                yield(doc_array,label_array)\n            else:\n                yield(doc_array)\n\nclass PerClassAccHistory(Callback):\n    '''\n    a note about the confusion matrix:\n    Cij = nb of obs known to be in group i and predicted to be in group j. So:\n    - the nb of right predictions is given by the diagonal\n    - the total nb of observations for a group is given by summing the corresponding row\n    - the total nb of predictions for a group is given by summing the corresponding col\n    accuracy is (nb of correct preds)/(total nb of preds)\n    # https://developers.google.com/machine-learning/crash-course/classification/accuracy\n    '''\n    def __init__(self, my_n_cats, my_rd, my_n_steps):\n        self.my_n_cats = my_n_cats\n        self.my_rd = my_rd\n        self.my_n_steps = my_n_steps\n\n    def on_train_begin(self, logs={}):\n        self.per_class_accuracy = []\n     \n    def on_epoch_end(self, epoch, logs={}):\n        cmat = np.zeros(shape=(self.my_n_cats,self.my_n_cats))\n        for repeat in range(self.my_n_steps):\n            docs, labels = self.my_rd.__next__()\n            preds_floats = self.model.predict(docs)\n            y_pred = np.argmax(np.array(preds_floats),axis=1)\n            y_true = np.argmax(labels,axis=1)\n            cmat = np.add(cmat,confusion_matrix(y_true, y_pred))\n            if repeat % round(self.my_n_steps/5) == 0:\n                print(repeat)\n        accs = list(np.round(1e2*cmat.diagonal()/cmat.sum(axis=0),2))\n        self.per_class_accuracy.append(accs)\n\nclass TimeHistory(Callback):\n    def on_train_begin(self, logs={}):\n        self.times = []\n    \n    def on_epoch_begin(self, batch, logs={}):\n        self.epoch_time_start = time()\n    \n    def on_epoch_end(self, batch, logs={}):\n        self.times.append(round(time() - self.epoch_time_start,2))\n\nclass LossHistory(Callback):\n    '''\n    records the average loss on the full *training* set so far\n    the loss returned by logs is just that of the current batch!\n    '''\n    def on_train_begin(self, logs={}):\n        self.losses = []\n        self.loss_avg = []\n\n    def on_batch_end(self, batch, logs={}):\n        self.losses.append(round(float(logs.get('loss')),6))\n        self.loss_avg.append(round(np.mean(self.losses,dtype=np.float64),6))\n    \n    def on_epoch_end(self, batch, logs={}):\n        self.losses = []\n    \nclass AccHistory(Callback):\n    '''\n    records the average accuracy on the full *training* set so far\n    the accuracy returned by logs is just that of the current batch!\n    '''\n    def on_train_begin(self, logs={}):\n        self.accs = []\n        self.acc_avg = []\n\n    def on_batch_end(self, batch, logs={}):\n        self.accs.append(round(1e2*float(logs.get('acc')),4))\n        self.acc_avg.append(round(np.mean(self.accs,dtype=np.float64),4))\n    \n    def on_epoch_end(self, batch, logs={}):\n        self.accs = []\n\nclass LRHistory(Callback):\n    ''' records the current learning rate'''\n    def on_train_begin(self, logs={}):\n       self.lrs = []\n    \n    def on_batch_end(self, batch, logs={}):\n        my_lr = K.eval(self.model.optimizer.lr)\n        self.lrs.append(my_lr)\n\nclass MTHistory(Callback):\n    ''' records the current momentum'''\n    def on_train_begin(self, logs={}):\n       self.mts = []\n    \n    def on_batch_end(self, batch, logs={}):\n        my_mt = K.eval(self.model.optimizer.momentum)\n        self.mts.append(my_mt)"""
HAN/preprocessing.py,0,"b'\'\'\'\nTODO update description!\nthis script implements the following preprocessing steps:\n    - split training set into training (90%) and validation (10%)\n    - tokenize docs into sentences and sentences into words\n    - count the number of occurrences of each word\n    - replace the unfrequent words with a special token\n    - turn word docs into integer docs, sorted by increasing # of sents (for bucketing)\n    - learn word2vec embeddings from training + validation sets\n\nline by line processing is used, so datasets that don\'t fit in RAM can still be treated\n\nExample of one line of the input files (Amazon review dataset format):\n\n""1"",""mens ultrasheer"",""This model may be ok for sedentary types, but I\'m active \nand get around alot in my job - consistently found these stockings rolled up down \nby my ankles! Not Good!! Solution: go with the standard compression stocking, \n20-30, stock #114622. Excellent support, stays up and gives me what I need. \nBoth pair of these also tore as I struggled to pull them up all the time. \nGood riddance/bad investment!""\n\nwarnings: \n    - input files should be named \'train.csv\' and \'test.csv\', be in format above\n    - last line of each file should be blank!\n    - all available cores are used by default (hardcode \'n_jobs\' to change)\n    - all generated files should be deleted before re-running the script in the same directory\n\ntested on Python 3.5 and 3.6 with gensim 3.2.0\ntakes ~2.17 hours to process full Amazon dataset (3.65M docs, 1.6GB) using 8 cores @2.4GHz\n\n\'\'\'\n\nimport os\nimport re\nimport json\nimport nltk\nimport math\nimport random\nimport operator\nimport multiprocessing\nimport numpy as np\n\nfrom time import time\nfrom random import shuffle\nfrom functools import partial\nfrom collections import Counter\nfrom multiprocessing import Pool\nfrom nltk import sent_tokenize, word_tokenize\nfrom gensim.models.word2vec import LineSentence, Word2Vec, FAST_VERSION \n\n# draft commands below about using Stanford CoreNLP tokenizer instead of NLTK\n# from subprocess import call, Popen\n# path_to_corenlp = \'H:\\\\stanford-corenlp-full-2018-02-27\\\\\'\n# my_command = \'java -mx300m -classpath ""*"" edu.stanford.nlp.process.PTBTokenizer\' + path_to_tmp + \'sample.txt \' + \'> \' + path_to_tmp + \'sample_tok.txt\'\n# os.chdir(path_to_corenlp)\n# my_command = \'java -mx300m -classpath ""*"" edu.stanford.nlp.process.DocumentPreprocessor doc.txt > sents.txt -tokenizePerLine=true -tokenizeNLs=false\'\n# Popen(my_command, shell=True, cwd=path_to_corenlp)\n# my_command = \'java -mx300m -classpath ""*"" edu.stanford.nlp.process.PTBTokenizer sents.txt > tokens.txt -preserveLines\'\n# Popen(my_command, shell=True, cwd=path_to_corenlp)\n\n# ============================== PATHS & PARAMETERS ==============================\n\ndataset_name = \'amazon_review_full_csv\' \none_based_labels = True # Remember to change this when changing dataset!E.g., should be False for IMDB.  \n\nis_GPU = True # just a flag to select the right paths for the machine\n\nif is_GPU:\n    path_to_data = \'/home/antoine/Desktop/TextClassificationDatasets/\' + dataset_name + \'/\'\n    path_to_batches = path_to_data + \'batches/\'\nelse:\n    path_to_data = \'H:\\\\\' + dataset_name +\'\\\\\'\n    path_to_batches = path_to_data + \'batches\\\\\'\n\nttv_l = [\'train_new\',\'test\',\'val\']\n\nl_o = 0.1 # percentage/100 of samples to leave out for validation\nidx_mf = 2 # most freq. token mapped to 2, 0 reserved for padding, 1 for the out-of-vocab token\noov_idx = 1\nmin_count = 5\nword_vector_dim = 200\nbatch_size = 128\nprepend_title = True\n\nn_jobs = multiprocessing.cpu_count()\n\n# ============================== FUNCTIONS ==============================\n\n# \'atoi\' and \'natural_keys\' functions taken from: https://stackoverflow.com/questions/5967500/how-to-correctly-sort-a-string-with-a-number-inside\ndef atoi(text):\n    return int(text) if text.isdigit() else text\n\ndef natural_keys(text):\n    return [ atoi(c) for c in re.split(\'(\\d+)\', text) ]\n\n# function \'my_split\' below taken from: https://stackoverflow.com/questions/2130016/splitting-a-list-into-n-parts-of-approximately-equal-length/2135920#2135920\ndef my_split(a, n):\n    k, m = divmod(len(a), n)\n    return (a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))\n\ndef create_assignments(path_to_data,ttv_l,n_jobs):\n    \'\'\'\n    - TODO update description\n    - compute the byte offset position of each line, and equally splits them to \'n_jobs\' lists\n    - parameters:\n        - path_to_data: where the files live\n        - \'ttv_l\': names of the files to process, without extension. E.g., [\'train_new\',\'test\',\'val\']\n                   note that the extension of all files should be \'.csv\'\n        - \'n_jobs\': number of cores that will be used later on\n    \'\'\'\n    assignments = dict()\n    my_sizes = dict()\n    for ttv in ttv_l:\n        print(\'==== going over\',ttv, \'set ====\')\n        my_tells = []\n        with open(path_to_data + ttv + \'.csv\', \'r\', encoding=\'utf8\') as infile:\n            for line in iter(infile.readline, \'\'):\n                it = infile.tell()\n                my_tells.append(it)\n        \n        my_tells.insert(0,0) # first line starts at byte 0\n        my_tells = my_tells[:-1] # last offset position corresponds to empty line!\n        print(\'file features\',len(my_tells),\'lines\')\n        my_sizes[ttv] = len(my_tells)\n        \n        print(\'creating assignments for\', n_jobs, \'cores\')\n        all_idxs = list(my_split(my_tells,n_jobs))\n        \n        assert len([elt for sublist in all_idxs for elt in sublist]) == len(my_tells), \'size mismatch!\'\n    \n        # prepend job identifier (integer in [0,n_jobs]) to each sublist\n        all_idxs = [[elt_idx] + elt for elt_idx,elt in enumerate(all_idxs)]\n        \n        assignments[ttv] = all_idxs\n        \n    return assignments, my_sizes\n\ndef tokenize_text(ttv,idxs):\n    \'\'\'\n    - TODO update description\n    - this function:\n        - splits each line into label, title, and doc\n        - tokenizes the doc into sentences and each sentence into words (title is disregarded)\n        - creates a word count dict from the training set\n        - creates a separate file containing labels (e.g., review scores) (0-based index)\n    - parameters:\n        - \'ttv\': in [\'train\',\'test\',\'val\']\n        - \'idxs\': list of byte offset of the lines that should be processed\n                  the 1st elt of \'idxs\' is the job identifier (for parallel processing)\n    \'\'\'\n    labels = []\n    job_nb = idxs[0]\n    idxs = idxs[1:]\n    tk_ct = dict()\n    \n    with open(path_to_data + ttv + \'.csv\', \'r\', encoding=\'utf8\') as infile:\n        with open(path_to_data + ttv + \'_tokenized_\'+ str(job_nb) + \'.csv\', \'a\', encoding=\'utf8\') as outfile:\n            for idx in idxs:\n                infile.seek(idx)\n                line = infile.readline()\n                line_split = line.split(\'"",""\')\n                label = re.sub(\'""\',\'\',line_split[0])\n                if one_based_labels:\n                    labels.append(int(label)-1) # keras \'to_categorical\' requires 0-based index\n                else:\n                    labels.append(int(label))\n                if prepend_title:\n                    txt = line_split[1] + \'. \' + line_split[2][:-1]  # prepend title to review\n                else:\n                    txt = line_split[2][:-1]\n                if txt[-1] == \'""\':\n                    txt = txt[:-1]\n                txt = re.sub(r""([\\w/\'+$\\s-]+|[^\\w/\'+$\\s-]+)\\s*"", r""\\1 "", txt) # turns \'one.two\' into \'one . two\' taken from: https://stackoverflow.com/questions/27810884/python-how-do-i-separate-punctuation-from-words-by-white-space-leaving-only-on?rq=1\n                sents = sent_tokenize(txt)\n                txt_to_write = []\n                for sent in sents:\n                    words = word_tokenize(sent)\n                    words = [word.lower() for word in words] # not earlier as case may be necessary for tokenization\n                    if ttv == \'train_new\': # vocab should be built from training set only!\n                        for word in words:\n                            tk_ct[word] = tk_ct.get(word, 0) + 1 # increment, creating key if it doesn\'t already exist\n                    txt_to_write.append(\' \'.join(words)) # separate tokens with white space\n                txt_to_write = \' ##S_S## \'.join(txt_to_write) + \'\\n\' # separate sentences with \'##S_S##\'\n                outfile.write(txt_to_write)\n    \n    if ttv == \'train_new\':\n        with open(path_to_data + \'word_counts_\' + str(job_nb) + \'.json\', \'w\', encoding=\'utf8\') as my_file:\n            json.dump(tk_ct, my_file, sort_keys=True, indent=4)\n    \n    with open(path_to_data + ttv + \'_labels_\'+ str(job_nb) + \'.csv\', \'a\', encoding=\'utf8\') as outfile:\n        for label in labels:\n            outfile.write(str(label) + \'\\n\')\n            \ndef conv_to_ints(ttv,vocab,idxs):\n    \'\'\'\n    - converts word docs to integer docs\n    - parameters:\n        - \'ttv\': in [\'train\',\'test\',\'val\']\n        - vocab: dict mapping words to integers\n        - \'idxs\': list of byte offset positions of the lines that should be processed\n                  the 1st elt of \'idxs\' is the job identifier (for parallel processing)\n    \'\'\'\n    job_nb = idxs[0]\n    idxs = idxs[1:]\n    \n    with open(path_to_data + ttv + \'_tokenized.csv\', \'r\', encoding=\'utf-8\') as infile:\n        with open(path_to_data + ttv + \'_int_\'+ str(job_nb) + \'.csv\', \'a\', encoding=\'utf8\') as outfile:\n            for idx in idxs:\n                infile.seek(idx)\n                line = infile.readline()\n                ints = [elt if elt==\'##S_S##\' else vocab[elt] if elt in vocab else oov_idx for elt in line.split()] \n                to_write = \' \'.join([str(elt) for elt in ints]) + \'\\n\'\n                outfile.write(to_write)\n\ndef get_size(ttv,idxs):\n    \'\'\'\n    - computes the size of each line in the file (# of sents) and prepends it with byte offset position\n    - parameters:\n        - \'ttv\': in [\'train\',\'test\',\'val\']\n        - \'idxs\': list of byte offset positions of the lines that should be processed\n                  the 1st elt of \'idxs\' is the job identifier (for parallel processing)\n    \'\'\'\n    job_nb = idxs[0]\n    idxs = idxs[1:]\n    \n    with open(path_to_data + ttv + \'_int.csv\', \'r\', encoding=\'utf-8\') as infile:\n        with open(path_to_data + ttv + \'_size_\'+ str(job_nb) + \'.csv\', \'a\', encoding=\'utf8\') as outfile:\n            for idx in idxs:\n                infile.seek(idx)\n                line = infile.readline()\n                to_write = str(idx) + \' \' + str(len(line.split(\'##S_S##\'))) + \'\\n\'\n                outfile.write(to_write)\n\ndef write_batch_label(my_assignment,batch_size,my_ttv):\n    my_batch_number = my_assignment[0]\n    my_assignment = my_assignment[1:]\n    \n    batch_lines = []\n    write_batch = False\n    for my_counter, label in enumerate(my_assignment,1):\n        batch_lines.append(label)\n        \n        if my_counter % batch_size == 0:\n            write_batch = True\n        elif my_counter == len(my_assignment): # the current batch must be the last one, and include the last lines in the file\n           write_batch = True\n        # for debugging:\n        #if write_batch:\n        #    print(\'writing batch number\',my_batch_number,len(batch_lines))\n        #    my_batch_number += 1\n        #    batch_lines = [] \n        #    write_batch = False\n          \n        if write_batch:\n            with open(path_to_batches + my_ttv + \'_label_batch_\' + str(my_batch_number) + \'.txt\', \'a\', encoding=\'utf8\') as outfile:\n                for line in batch_lines:\n                    outfile.write(line + \'\\n\')\n            my_batch_number += 1\n            batch_lines = [] \n            write_batch = False\n\ndef write_batch_doc(my_assignment,batch_size,my_ttv):\n    my_batch_number = my_assignment[0]\n    my_assignment = my_assignment[1:]\n    batch_lines = []\n    write_batch = False\n\n    with open(path_to_data + my_ttv + \'_int.csv\', \'r\', encoding=\'utf8\') as infile:\n        for my_counter,offset in enumerate(my_assignment,1): \n            infile.seek(offset)\n            line = infile.readline()\n            batch_lines.append(line)\n            \n            if my_counter % batch_size == 0:\n                write_batch = True\n            elif my_counter == len(my_assignment): # the current batch must be the last one, and include the last lines in the file\n               write_batch = True\n            \n            if write_batch:\n                with open(path_to_batches + my_ttv + \'_doc_batch_\' + str(my_batch_number) + \'.txt\', \'a\', encoding=\'utf8\') as outfile:\n                    for line in batch_lines:\n                        outfile.write(line)\n                my_batch_number += 1\n                batch_lines = [] \n                write_batch = False\n\n# ============================== MAIN ==============================\n\ndef main():\n    beginning_of_time = time()\n    \n    print(\'warning,\',n_jobs,\'core(s) will be used!\')\n    \n    nltk.download(\'punkt\')\n    \n    print(\'creating subfolder to write batch files of size:\',batch_size)\n    os.mkdir(path_to_batches)\n    \n    random.seed(7232018)\n        \n    # save byte offset positions of all lines in \'train.csv\'\n    my_tells = []\n    with open(path_to_data + \'train.csv\', \'r\', encoding=\'utf8\') as infile:\n        for line in iter(infile.readline, \'\'):\n            it = infile.tell()\n            my_tells.append(it)\n    \n    my_tells.insert(0,0) # first line starts at byte 0\n    my_tells = my_tells[:-1] # last offset position corresponds to empty line!\n    n_train = len(my_tells)\n    n_val = round(l_o*n_train)\n    \n    print(\'training set features\',n_train,\'lines\')\n    print(\'leaving\',n_val,\'samples out for validation (\',l_o*100,\'%)\')\n    \n    val_idxs = list(np.random.choice(np.array(my_tells),size=n_val,replace=False))\n    val_idxs.sort() # important that it stays sorted by increasing order!\n    \n    train_new_idxs = list(set(my_tells).difference(set(val_idxs)))\n    train_new_idxs.sort() # important that it stays sorted by increasing order!\n    \n    assert n_train == len(train_new_idxs) + n_val, \'file size mismatch!\'\n    \n    print(\'creating new training set as ""train_new.csv""\')\n    \n    with open(path_to_data + \'train.csv\', \'r\', encoding=\'utf8\') as train_old:\n        with open(path_to_data + \'train_new.csv\', \'a\', encoding=\'utf8\') as train_new:\n            for it in train_new_idxs:\n                train_old.seek(it)\n                line = train_old.readline()\n                train_new.write(line)\n    \n    print(\'creating validation set as ""val.csv""\')\n    \n    with open(path_to_data + \'train.csv\', \'r\', encoding=\'utf8\') as train_old:\n        with open(path_to_data + \'val.csv\', \'a\', encoding=\'utf8\') as val:\n            for it in val_idxs:\n                train_old.seek(it)\n                line = train_old.readline()\n                val.write(line)  \n    \n    assignments, my_sizes = create_assignments(path_to_data,ttv_l,n_jobs)\n    print(\'assignments created\')\n    \n    print(\'***** doing 1st pass to tokenize and get word counts *****\')\n    \n    for ttv in ttv_l:\n        print(\'==== going over\', ttv, \'set ====\')\n        start = time()      \n        all_idxs = assignments[ttv]\n        tokenize_text_partial = partial(tokenize_text, ttv)\n        print(\'using\', n_jobs, \'cores\')\n        pool = Pool(processes=n_jobs)\n        pool.map(tokenize_text_partial, all_idxs)\n        pool.close()\n        pool.join()\n        \n        print(ttv, \'done in\', round(time() - start,2))\n    \n    print(\'==== combining results ====\')\n    \n    file_list = os.listdir(path_to_data)\n    file_list.sort(key=natural_keys)\n    \n    dict_names = [elt for elt in file_list if re.search(\'counts_[0-9]+\', elt)]\n    assert len(dict_names) == len(all_idxs), \'number of dictionaries does not match number of jobs!\'  \n    \n    my_counter_full = Counter()\n    for my_file_name in dict_names:\n        with open(path_to_data + my_file_name, \'r\', encoding=\'utf8\') as my_file:\n            tmp = json.load(my_file)\n            my_counter_full = my_counter_full + Counter(tmp)\n    \n    with open(path_to_data + \'word_counts.json\', \'w\', encoding=\'utf8\') as my_file:\n        json.dump(dict(my_counter_full), my_file, sort_keys=True, indent=4)\n    \n    print(\'word count dictionaries combined and saved to disk\')\n    \n    tok_names_full = []\n    label_names_full = []\n    for ttv in ttv_l:\n        tok_names = [elt for elt in file_list if re.search(ttv + \'_tokenized_[0-9]+\', elt)]\n        label_names = [elt for elt in file_list if re.search(ttv + \'_labels_[0-9]+\', elt)]\n        assert len(tok_names) == len(all_idxs), \'number of tokenized files does not match number of jobs!\'\n        assert len(label_names) == len(all_idxs), \'number of label files does not match number of jobs!\'\n    \n        tok_names_full += tok_names\n        label_names_full += label_names\n        \n        with open(path_to_data + ttv + \'_tokenized.csv\', \'a\', encoding=\'utf8\') as outfile:\n            for my_file_name in tok_names:\n                with open(path_to_data + my_file_name, \'r\', encoding=\'utf8\') as infile:\n                    for line in infile:\n                        outfile.write(line)\n        \n        with open(path_to_data + ttv + \'_labels.csv\', \'a\', encoding=\'utf8\') as outfile:\n            for my_file_name in label_names:\n                with open(path_to_data + my_file_name, \'r\', encoding=\'utf8\') as infile:\n                    for line in infile:\n                        outfile.write(line)\n    \n    print(\'tokenized and label files combined and saved to disk\')      \n    \n    print(\'deleting temporary files\')\n    [os.remove(path_to_data + elt) for elt in dict_names + tok_names_full + label_names_full]\n    \n    print(\'tokens appearing less than\',min_count,\'time(s) will be replaced by special token\')\n    \n    # assign to each word an index based on its frequency in the corpus\n    sorted_counts = sorted(my_counter_full.items(), key=operator.itemgetter(1), reverse=True)\n    \n    word_to_index = dict([(my_t[0],idx) for idx,my_t in enumerate(sorted_counts,idx_mf) if my_t[1]>min_count])\n    with open(path_to_data + \'vocab.json\', \'w\', encoding=\'utf8\') as my_file:\n        json.dump(word_to_index, my_file, sort_keys=True, indent=4)\n    \n    print(\'vocab created and saved to disk\')\n    \n    print(\'***** doing 2nd pass to convert textual docs to integer docs *****\')\n    \n    print(\'updating assignments\')\n    t_names = [elt + \'_tokenized\' for elt in ttv_l]\n    assignments, my_sizes = create_assignments(path_to_data,t_names,n_jobs)\n    \n    for ttv_idx,ttv in enumerate(ttv_l):\n        print(\'==== going over\', ttv, \'set ====\')\n        start = time()      \n        all_idxs = assignments[t_names[ttv_idx]]\n        conv_to_ints_partial = partial(conv_to_ints, ttv, word_to_index)\n        print(\'using\', n_jobs, \'cores\')\n        pool = Pool(processes=n_jobs)\n        pool.map(conv_to_ints_partial, all_idxs)\n        pool.close()\n        pool.join()\n\n        print(ttv, \'done in\', round(time() - start,2))\n    \n    print(\'==== combining results ====\')\n    \n    file_list = os.listdir(path_to_data)\n    file_list.sort(key=natural_keys)\n    \n    int_names_full = []\n    for ttv in ttv_l:\n        int_names = [elt for elt in file_list if re.search(ttv + \'_int_[0-9]+\', elt)]\n        assert len(int_names) == len(all_idxs), \'number of integer files does not match number of jobs!\'\n        int_names_full += int_names\n        \n        with open(path_to_data + ttv + \'_int.csv\', \'a\', encoding=\'utf8\') as outfile:\n            for my_file_name in int_names:\n                with open(path_to_data + my_file_name, \'r\', encoding=\'utf8\') as infile:\n                    for line in infile:\n                        outfile.write(line)\n    \n    print(\'integer files combined and saved to disk\')      \n    \n    print(\'deleting temporary files\')\n    [os.remove(path_to_data + elt) for elt in int_names_full]\n\n    print(\'***** doing 3rd pass to compute doc size and sort files *****\')\n\n    print(\'updating assignments\')\n    int_names = [elt + \'_int\' for elt in ttv_l]\n    assignments, my_sizes = create_assignments(path_to_data,int_names,n_jobs)\n    \n    for ttv_idx,ttv in enumerate(ttv_l):\n        print(\'==== going over\', ttv, \'set ====\')\n        start = time()      \n        all_idxs = assignments[int_names[ttv_idx]]\n        \n        get_size_partial = partial(get_size,ttv)\n        print(\'using\', n_jobs, \'cores\')\n        pool = Pool(processes=n_jobs)\n        pool.map(get_size_partial, all_idxs)\n        pool.close()\n        pool.join()\n            \n        print(ttv, \'done in\', round(time() - start,2))\n    \n    print(\'==== combining results ====\')\n    \n    file_list = os.listdir(path_to_data)\n    file_list.sort(key=natural_keys)\n    \n    size_names_full = []\n    for ttv in ttv_l:\n        size_names = [elt for elt in file_list if re.search(ttv + \'_size_[0-9]+\', elt)]\n        assert len(size_names) == len(all_idxs), \'number of size files does not match number of jobs!\'\n        size_names_full += size_names\n        \n        with open(path_to_data + ttv + \'_size.csv\', \'a\', encoding=\'utf8\') as outfile:\n            for my_file_name in size_names:\n                with open(path_to_data + my_file_name, \'r\', encoding=\'utf8\') as infile:\n                    for line in infile:\n                        outfile.write(line)\n    \n    print(\'size files combined and saved to disk\')      \n    \n    print(\'deleting temporary files\')\n    [os.remove(path_to_data + elt) for elt in size_names_full]\n    \n    print(\'sorting integer and label files by increasing doc size (# of sents) and writing batches\')\n    \n    label_counts = Counter()\n    for ttv in ttv_l:\n        n_lines = int(my_sizes[ttv + \'_int\'])\n        print(\'==== going over\', ttv, \'set ====\')\n        # here we exceptionally load entire file to RAM, but each line is very small (just position and size)\n        with open(path_to_data + ttv + \'_size.csv\', \'r\', encoding=\'utf8\') as file:\n            sizes = file.read().splitlines() # contains byte offset position and size (nb of sents) for each line in the file\n    \n        size_lists = [[int(eltt) for eltt in elt.split()] for elt in sizes]\n        size_lists = [[idx] + elt for idx,elt in enumerate(size_lists)] # prepend line idx (needed to ensure matching with labels)\n        size_cats = list(set([elt[2] for elt in size_lists]))\n                \n        sorted_size_lists = []\n        for size_cat in size_cats:\n            sub_size_lists = [elt for elt in size_lists if elt[2]==size_cat]\n            shuffle(sub_size_lists) # shuffle within each size category\n            sorted_size_lists.append(sub_size_lists)\n        \n        sorted_size_lists = [elt for subl in sorted_size_lists for elt in subl] # flatten\n        sorted_line_idxs = [elt[0] for elt in sorted_size_lists] # for labels, later on\n        sorted_offsets = [elt[1] for elt in sorted_size_lists]\n        \n        with open(path_to_data + ttv + \'_int_sorted.csv\', \'a\', encoding=\'utf8\') as outfile:\n            with open(path_to_data + ttv + \'_int.csv\', \'r\', encoding=\'utf8\') as infile:\n                for offset in sorted_offsets:\n                    infile.seek(offset)\n                    line = infile.readline()\n                    outfile.write(line)\n        \n        my_assignments = list(my_split(sorted_offsets,n_jobs)) # this does not destroy the ordering\n        \n        # prepend number of batches that can be built from previous assignments to each assignment so as to preserve batch ordering\n        cumul_b = 0\n        new_assignments = []\n        for ass in my_assignments:\n            new_assignments.append([cumul_b] + ass) \n            cumul_b += math.ceil(len(ass)/batch_size) # compute nb of batches that can be obtained from that sublist\n        \n        write_batch_doc_partial = partial(write_batch_doc,\n                                          batch_size=batch_size,\n                                          my_ttv=ttv)\n        print(\'using\', n_jobs, \'cores\')\n        start = time()\n        pool = Pool(processes=n_jobs)\n        pool.map(write_batch_doc_partial, new_assignments)\n        pool.close()\n        pool.join()\n\n        print(\'doc batches written in\',round(time() - start,2))\n        \n        # here we again exceptionally load entire file to RAM, but again each line is very small (just label)\n        with open(path_to_data + ttv + \'_labels.csv\', \'r\', encoding=\'utf8\') as file:\n            labels = file.read().splitlines()\n        \n        assert len(sizes) == len(labels), \'size and label files have diff. # of rows!\'\n        assert len(labels) == n_lines, \'# of lines and # of labels differ!\'\n                \n        label_counts = label_counts + Counter(labels)\n        \n        labels_sorted = [labels[idx] for idx in sorted_line_idxs]\n        with open(path_to_data + ttv + \'_labels_sorted.csv\', \'a\', encoding=\'utf8\') as file:\n            for label in labels_sorted:\n                file.write(str(label) + \'\\n\')\n        \n        # for debugging: labels_sorted = [1,4,2,3,1,2,4,2,3,3,2,2,1,1,4,3,2,1,2,3,4,3]\n        # this strategy returns slightly more batches than math.ceil(len(labels_sorted)/n_jobs)\n        my_assignments = list(my_split(labels_sorted,n_jobs)) # this does not destroy the ordering\n        \n        # prepend number of batches that can be built from previous assignments to each assignment so as to preserve batch ordering\n        cumul_b = 0\n        new_assignments = []\n        for ass in my_assignments:\n            new_assignments.append([cumul_b] + ass)\n            cumul_b += math.ceil(len(ass)/batch_size) # compute nb of batches that can be obtained from that sublist\n\n        write_batch_label_partial = partial(write_batch_label,\n                                            batch_size=batch_size,\n                                            my_ttv=ttv)\n        \n        print(\'using\', n_jobs, \'cores\')\n        start = time()      \n        pool = Pool(processes=n_jobs)\n        pool.map(write_batch_label_partial, new_assignments)\n        pool.close()\n        pool.join()\n        \n        print(\'label batches written in\',round(time() - start,2))\n    \n    print(\'label distribution\',label_counts)\n    print(\'# of labels=\',sum(dict(label_counts).values()))\n        \n    with open(path_to_data + \'label_distributions.json\', \'w\', encoding=\'utf8\') as my_file:\n        json.dump(label_counts, my_file, sort_keys=True, indent=4)\n     \n    print(\'creating data for word2vec from new training set and validation set\')\n    # one sentence = one line\n    \n    with open(path_to_data + \'train_new_int_sorted.csv\', \'r\', encoding=\'utf8\') as infile:\n        with open(path_to_data + \'for_w2v.txt\', \'a\', encoding=\'utf8\') as outfile:\n            for line in infile:\n                lines = line.split(\'##S_S##\')\n                lines = [elt.strip() for elt in lines]\n                for l in lines:\n                    outfile.write(l + \'\\n\')\n\n    with open(path_to_data + \'val_int_sorted.csv\', \'r\', encoding=\'utf8\') as infile:\n        with open(path_to_data + \'for_w2v.txt\', \'a\', encoding=\'utf8\') as outfile:\n            for line in infile:\n                lines = line.split(\'##S_S##\')\n                lines = [elt.strip() for elt in lines]\n                for l in lines:\n                    outfile.write(l + \'\\n\')\n    \n    print(\'initializing sentence iterable\')\n    my_sents = LineSentence(path_to_data + \'for_w2v.txt\')\n    \n    print(\'learning word vectors\')\n    assert FAST_VERSION > -1, \'not using cython, word2vec will be too slow!\'\n    print(\'using\', n_jobs, \'cores\')\n    \n    wv = Word2Vec(sentences=my_sents, \n                  size=word_vector_dim, \n                  min_count=1, # because we have already preprocessed the data\n                  workers=n_jobs,\n                  iter=10,\n                  sg=1,\n                  max_vocab_size=2e6) # every 10M words require ~1GB of RAM\n    \n    wv.save(path_to_data + \'word_vectors.kv\') # \'kv\' extension stands for \'keyed vectors\'\n    print(\'word vectors saved to disk\')\n    \n    # just for sanity check\n    with open(path_to_data + \'vocab.json\', \'r\', encoding=\'utf8\') as my_file:\n        vocab = json.load(my_file)\n    \n    print(\'length of word2vec vocab (train+val):\',len(wv.wv.vocab))\n    print(\'length of vocab (train):\',len(vocab))\n    assert len(wv.wv.vocab) == len(vocab) + 1, \'word2vec vocab mismatch!\' # +1 because vocab doesn\'t contain the special out-of-vocab token\n    \n    print(\'everything done in\', round(time() - beginning_of_time,2))    \n \n#==============================================================================\n\nif __name__ == ""__main__"":\n    main()\n'"
skipgram/preprocessing.py,0,"b'import re\nimport json\nimport operator\nfrom bs4 import BeautifulSoup\nfrom collections import Counter\nfrom nltk.tokenize import TweetTokenizer\n\npath_read = \'./data/\'\npath_write = \'./data/\'\n\nmin_freq = 5 # retain the words appearing at least this number of times\noov_token = 0 # for out-of-vocabulary words\nverbosity = 5 # the greater, the more counter increments are printed\n\n# ========== read and clean reviews ==========\n\nwith open(path_read + \'imdb_reviews.txt\',\'r\',encoding=\'utf-8\') as file:\n    reviews = file.readlines()\n\ntokenizer = TweetTokenizer()\n\ncleaned_reviews = []\n\nfor counter,rev in enumerate(reviews):\n    rev = rev.lower()\n    temp = BeautifulSoup(rev)\n    text = temp.get_text() # remove HTML formatting\n    text = re.sub(\' +\',\' \',text) # strip extra white space\n    text = text.strip() # strip leading and trailing white space\n    tokens = tokenizer.tokenize(text) # tokenize\n    cleaned_reviews.append(tokens)\n    if counter % round(len(reviews)/verbosity) == 0:\n        print(counter, \'/\', len(reviews), \'reviews cleaned\')\n\n# ========== build vocab ==========\n\ntokens = [token for sublist in cleaned_reviews for token in sublist]\n\ncounts = dict(Counter(tokens))\n\n# only retain the words that appear at least \'min_freq\' times\ncounts = {k:v for k,v in counts.items() if v>=min_freq}\n\nwith open(path_write + \'counts.json\', \'w\') as file:\n    json.dump(counts, file, sort_keys=True, indent=4)\n\nprint(\'counts saved to disk\')\n\nsorted_counts = sorted(counts.items(), key=operator.itemgetter(1), reverse=True)\n\n# assign to each word an index based on its frequency in the corpus\n# the most frequent word will get index equal to 1\n# 0 is reserved for out-of-vocabulary words\nword_to_index = dict([(my_tuple[0],idx) for idx,my_tuple in enumerate(sorted_counts,1)])\n\n# examples\nword_to_index[\'the\']\nword_to_index[""don\'t""]\n\nwith open(path_write + \'vocab.json\', \'w\') as file:\n    json.dump(word_to_index, file, sort_keys=True, indent=4)\n\nprint(\'vocab saved to disk\')\n\n# ========== transform each review into a list of word indexes ==========\n\nreviews_ints = []\n\nfor i,rev in enumerate(cleaned_reviews):\n    sublist = []\n    for token in rev:\n        if token in word_to_index:\n            sublist.append(word_to_index[token])\n        else:\n            sublist.append(oov_token)\n    reviews_ints.append(sublist)\n    if i % round(len(cleaned_reviews)/verbosity) == 0:\n        print(i, \'/\', len(reviews), \'reviews converted to ints\')\n\n\nwith open(path_write + \'doc_ints.txt\', \'w\') as file:\n    for rev in reviews_ints:\n        file.write(\' \'.join([str(elt) for elt in rev]) + \'\\n\')\n\nprint(\'reviews saved to disk\')\n'"
NMT/code/grid_search.py,2,"b""import sys\nimport json\n\nimport torch\nfrom torch.utils import data\n\n# = = = = = = = = = = =\n\npath_to_model = './code/'\nsys.path.insert(0, path_to_model)\nfrom model import seq2seqModel\n\npath_to_data = './data/'\npath_to_save = './models/'\n\n# = = = = = = = = = = =\n\nclass Dataset(data.Dataset):\n  def __init__(self, pairs):\n        self.pairs = pairs\n\n  def __len__(self):\n        return len(self.pairs) # total nb of observations\n\n  def __getitem__(self, idx):\n        source, target = self.pairs[idx] # one observation\n        return torch.LongTensor(source), torch.LongTensor(target)\n\n\ndef load_pairs(train_or_test):\n    with open(path_to_data + 'pairs_' + train_or_test + '_ints.txt', 'r', encoding='utf-8') as file:\n        pairs_tmp = file.read().splitlines()\n    pairs_tmp = [elt.split('\\t') for elt in pairs_tmp]\n    pairs_tmp = [[[int(eltt) for eltt in elt[0].split()],[int(eltt) for eltt in \\\n                  elt[1].split()]] for elt in pairs_tmp]\n    return pairs_tmp\n\n\n# = = = = = = = = = = =\n\npairs_train = load_pairs('train')\npairs_test = load_pairs('test')\n\nwith open(path_to_data + 'vocab_source.json','r') as file:\n    vocab_source = json.load(file) # word -> index\n\nwith open(path_to_data + 'vocab_target.json','r') as file:\n    vocab_target = json.load(file) # word -> index\n\nvocab_target_inv = {v:k for k,v in vocab_target.items()} # index -> word\n\nprint('data loaded')\n    \ntraining_set = Dataset(pairs_train)\ntest_set = Dataset(pairs_test)\n\nprint('data prepared')\n\nnum_layers = 1\nbidirectional = False\n\nfor att_strategy in ['concat','general','dot','none']:\n    \n    hidden_dim_s = 30\n    \n    if bidirectional:\n        if att_strategy == 'dot':\n            hidden_dim_t = 2*hidden_dim_s\n        else:\n            hidden_dim_t = hidden_dim_s\n    else:\n        hidden_dim_t = hidden_dim_s  \n\n    model = seq2seqModel(vocab_s = vocab_source,\n                         source_language = 'english',\n                         vocab_t_inv = vocab_target_inv,\n                         embedding_dim_s = 40,\n                         embedding_dim_t = 40,\n                         hidden_dim_s = hidden_dim_s,\n                         hidden_dim_t = hidden_dim_t,\n                         hidden_dim_att = 20,\n                         num_layers = num_layers,\n                         bidirectional = bidirectional,\n                         att_strategy = att_strategy,\n                         padding_token = 0,\n                         oov_token = 1,\n                         sos_token = 2,\n                         eos_token = 3,\n                         max_size = 30, # for the decoder, in prediction mode\n                         dropout = 0)\n\n    model.fit(training_set, test_set, lr=0.1, batch_size=64, n_epochs=200, patience=10, my_optimizer='SGD')\n\n    model_name = '_'.join([att_strategy, str(num_layers), str(bidirectional)])\n    model.save(path_to_save, model_name)\n"""
NMT/code/model.py,30,"b'import json\nimport numpy as np\nfrom time import time\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils import data\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom nltk.translate.bleu_score import corpus_bleu\n\'\'\'\nhyp = [\'It\', \'is\', \'a\', \'guide\', \'to\', \'action\', \'which\', \'ensures\', \'that\', \'the\', \'military\', \'always\', \'obeys\', \'the\', \'commands\', \'of\', \'the\', \'party\']\nref = [\'It\', \'is\', \'a\', \'guide\', \'to\', \'action\', \'that\', \'ensures\', \'that\', \'the\', \'military\', \'will\', \'forever\', \'heed\', \'Party\', \'commands\']\nlist_of_references = [[ref]]\nhypotheses = [hyp]\nprint(corpus_bleu(list_of_references, hypotheses))\n\'\'\'\n\ntest_sents = [\'I am a student.\',\n              \'I have a red car.\',  # adj-verb inversion\n              \'Kids love playing video games.\',\n              \'This river is full of fish.\', # plein vs pleine (accord)\n              \'The fridge is full of food.\',\n              \'The cat fell asleep on the mat.\',\n              \'my brother likes pizza.\', # pizza is translated to \'la pizza\'\n              \'I did not mean to hurt you\', # translation of mean in context\n              \'She is so mean\',\n              \'Help me pick out a tie to go with this suit!\', # more involved sentences\n              ""I can\'t help but smoking weed"",\n              \'The kids were playing hide and seek\',\n              \'The cat fell asleep in front of the fireplace\']\n\nfrom tqdm import tqdm\n\nfrom nltk import word_tokenize\n\nclass Encoder(nn.Module):\n    \'\'\'\n    to be passed the entire source sequence at once\n    we use padding_idx in nn.Embedding so that the padding vector does not receive gradient and stays always at zero\n    https://pytorch.org/docs/stable/nn.html#{embedding,dropout,gru}\n    \'\'\'\n    \n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, bidirectional, padding_idx, dropout):\n        super(Encoder, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx)\n        self.dropout = nn.Dropout(dropout)\n        self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers, dropout=dropout, bidirectional=bidirectional)\n    \n    def forward(self, input):\n        word_vectors = self.embedding(input)\n        word_vectors = self.dropout(word_vectors)\n        hs,_ = self.rnn(word_vectors) # (seq,batch,hidden_dim or hidden_dim*2). We are only interested in the first element of the output (annotations of the top layer of the stacking RNN)\n        \n        return hs\n\n\nclass seq2seqAtt(nn.Module):\n    \'\'\'\n    returns context vector given current target annotation (decoder) and all source annotations (encoder)\n    strategy can be \'dot\', \'general\', or \'concat\', as in subsection 3.1 of Luong et al. 2015\n    (see: https://arxiv.org/pdf/1508.04025.pdf)\n    \'\'\'\n    \n    def __init__(self, hidden_dim, hidden_dim_s, hidden_dim_t, strategy, bidirectional):\n        super(seq2seqAtt, self).__init__()\n        assert strategy in [\'dot\',\'general\',\'concat\'], ""\'strategy\' must be in [\'dot\',\'general\',\'concat\']""\n        \n        if bidirectional:\n            hidden_dim_s = hidden_dim_s*2\n        \n        if strategy == \'dot\':\n            assert hidden_dim_s == hidden_dim_t, ""with \'dot\' strategy, source and target hidden dims must be equal!""\n            \n        elif strategy == \'general\':\n            self.ff_general = nn.Linear(hidden_dim_t, hidden_dim_s)\n            \n        elif strategy == \'concat\':\n            self.ff_concat = nn.Linear(hidden_dim_s+hidden_dim_t, hidden_dim)\n            self.ff_score = nn.Linear(hidden_dim, 1, bias=False) # dot product with trainable vector\n        \n        self.strategy = strategy\n        \n    def forward(self, target_h, source_hs):\n        \n        if self.strategy in [\'dot\',\'general\']:\n            source_hs = source_hs.permute(1,0,2) # (seq,batch,hidden_dim_s) -> (batch,seq,hidden_dim_s)\n        \n        if self.strategy == \'dot\':\n            # with this strategy, no trainable parameters are involved\n            # here, feat = hidden_dim_t = hidden_dim_s\n            target_h = target_h.permute(1,2,0) # (1,batch,feat) -> (batch,feat,1)\n            dot_product = torch.matmul(source_hs, target_h) # (batch,seq,feat) * (batch,feat,1) -> (batch,seq,1)\n            scores = dot_product.permute(1,0,2) # -> (seq,batch,1)\n            \n        elif self.strategy == \'general\':\n            target_h = target_h.permute(1,0,2) # (1,batch,hidden_dim_t) -> (batch,1,hidden_dim_t)\n            output = self.ff_general(target_h) #  -> (batch,1,hidden_dim_s)\n            output = output.permute(0,2,1) # -> (batch,hidden_dim_s,1)\n            dot_product = torch.matmul(source_hs, output) # (batch,seq,hidden_dim_s) * (batch,hidden_dim_s,1) -> (batch,seq,1)\n            scores = dot_product.permute(1,0,2) # -> (seq,batch,1)\n            \n        elif self.strategy == \'concat\':\n            target_h_rep = target_h.repeat(source_hs.size(0),1,1) # (1,batch,hidden_dim_s) -> (seq,batch,hidden_dim_s)\n            concat_output = self.ff_concat(torch.cat((target_h_rep,source_hs),-1)) # (seq,batch,hidden_dim_s+hidden_dim_t) -> (seq,batch,hidden_dim)\n            scores = self.ff_score(torch.tanh(concat_output)) # -> (seq,batch,1)\n            source_hs = source_hs.permute(1,0,2) # (seq,batch,hidden_dim_s) -> (batch,seq,hidden_dim_s)\n                \n        scores = scores.squeeze(dim=2) # (seq,batch,1) -> (seq,batch). We specify a dimension, because we don\'t want to squeeze the batch dim in case batch size is equal to 1\n        norm_scores = torch.softmax(scores,0) # sequence-wise normalization\n        source_hs_p = source_hs.permute((2,1,0)) # (batch,seq,hidden_dim_s) -> (hidden_dim_s,seq,batch)\n        weighted_source_hs = (norm_scores * source_hs_p) # (seq,batch) * (hidden_dim_s,seq,batch) -> (hidden_dim_s,seq,batch) (we use broadcasting here - the * operator checks from right to left that the dimensions match)\n        ct = torch.sum(weighted_source_hs.permute((1,2,0)),0,keepdim=True) # (hidden_dim_s,seq,batch) -> (seq,batch,hidden_dim_s) -> (1,batch,hidden_dim_s); we need keepdim as sum squeezes by default \n        \n        return ct\n\n\nclass Decoder(nn.Module):\n    \'\'\'to be used one timestep at a time\n       https://pytorch.org/docs/stable/nn.html#gru\'\'\'\n    \n    def __init__(self, vocab_size, embedding_dim, hidden_dim_t, hidden_dim_s, num_layers, bidirectional, padding_idx, dropout):\n        super(Decoder, self).__init__()\n        \n        if bidirectional:\n            hidden_dim_s = hidden_dim_s*2\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx)\n        self.dropout_1 = nn.Dropout(dropout)\n        self.rnn = nn.GRU(embedding_dim, hidden_dim_t, num_layers, dropout=dropout)\n        self.ff_concat = nn.Linear(hidden_dim_s + hidden_dim_t, hidden_dim_t)\n        self.final = nn.Linear(hidden_dim_t, vocab_size)\n    \n    def forward(self, input, source_context, h):\n        word_vector = self.embedding(input) # (1,batch) -> (1,batch,embedding_dim)\n        word_vector = self.dropout_1(word_vector)\n        h_top, h_all = self.rnn(word_vector, h) # output is (1,batch,hidden_dim_t), (num_layers,batch,hidden_dim_t); ([0]: top stacking RNN layer for all timesteps, [1]: all stacking RNN layers for the last timestep)\n        h_tilde = torch.tanh(self.ff_concat(torch.cat((source_context, h_top), -1))) # (1,batch,hidden_dim_s+hidden_dim_t) -> (1,batch,hidden_dim_t). This corresponds to Eq. 5 in Luong et al. 2015\n        prediction = self.final(h_tilde) # (1,batch,feat) -> (1,batch,vocab) note that the prediction is not normalized at this time (it is just a vector of logits)\n        \n        return prediction, h_all\n\n\nclass seq2seqModel(nn.Module):\n    \'\'\'full seq2seq model a la Luong et al. 2015\'\'\'\n    \n    ARGS = [\'vocab_s\', \'source_language\', \'vocab_t_inv\', \'embedding_dim_s\', \'embedding_dim_t\',\n            \'hidden_dim_s\', \'hidden_dim_t\', \'hidden_dim_att\', \'num_layers\', \'bidirectional\',\n            \'att_strategy\', \'padding_token\', \'oov_token\', \'sos_token\', \'eos_token\', \n            \'max_size\',\'dropout\']\n    \n    def __init__(self, vocab_s, source_language, vocab_t_inv, embedding_dim_s, embedding_dim_t, hidden_dim_s, hidden_dim_t, hidden_dim_att, num_layers, bidirectional, att_strategy, padding_token, oov_token, sos_token, eos_token, max_size, dropout):\n        super(seq2seqModel, self).__init__()\n        self.vocab_s = vocab_s\n        self.source_language = source_language\n        self.vocab_t_inv = vocab_t_inv\n        self.embedding_dim_s = embedding_dim_s\n        self.embedding_dim_t = embedding_dim_t\n        self.hidden_dim_s = hidden_dim_s\n        self.hidden_dim_t = hidden_dim_t\n        self.hidden_dim_att = hidden_dim_att\n        self.num_layers = num_layers\n        self.bidirectional = bidirectional # applies to the encoder only\n        self.att_strategy = att_strategy # should be in [\'none\',\'dot\',\'general\',\'concat\']\n        self.padding_token = padding_token\n        self.oov_token = oov_token\n        self.sos_token = sos_token\n        self.eos_token = eos_token\n        self.max_size = max_size # only used when decoding\n        self.dropout = dropout\n        self.logs = dict()\n        \n        self.max_source_idx = max(list(vocab_s.values()))\n        print(\'max source index\',self.max_source_idx)\n        print(\'source vocab size\',len(vocab_s))\n        \n        self.max_target_idx = max([int(elt) for elt in list(vocab_t_inv.keys())])\n        print(\'max target index\',self.max_target_idx)\n        print(\'target vocab size\',len(vocab_t_inv))\n        \n        self.device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n        \n        self.encoder = Encoder(self.max_source_idx+1, self.embedding_dim_s, self.hidden_dim_s, self.num_layers, self.bidirectional, self.padding_token, self.dropout).to(self.device)\n        \n        self.decoder = Decoder(self.max_target_idx+1, self.embedding_dim_t, self.hidden_dim_t, self.hidden_dim_s, self.num_layers, self.bidirectional, self.padding_token, self.dropout).to(self.device)\n        \n        if not self.att_strategy == \'none\':\n            self.att_mech = seq2seqAtt(self.hidden_dim_att, self.hidden_dim_s, self.hidden_dim_t, self.att_strategy, self.bidirectional).to(self.device)\n    \n    def my_pad(self, my_list):\n        \'\'\'my_list looks like: [(seq_s_1,seq_t_1),...,(seq_s_n,seq_t_n)], where n is batch size.\n        Each tuple is a pair of source and target sequences\n        the <eos> token is appended to each sequence before padding\n        https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_sequence\'\'\'\n        \n        batch_source = pad_sequence([torch.cat((elt[0], torch.LongTensor([self.eos_token]))) for elt in my_list], batch_first=True, padding_value=self.padding_token)\n        batch_target = pad_sequence([torch.cat((elt[1], torch.LongTensor([self.eos_token]))) for elt in my_list], batch_first=True, padding_value=self.padding_token)\n        \n        batch_source = batch_source.transpose(1,0).to(self.device) # (batch,seq) -> (seq,batch) (for RNN)\n        batch_target = batch_target.transpose(1,0).to(self.device)\n        \n        return batch_source, batch_target\n        \n    \n    def forward(self, input, max_size, is_prod):\n        \n        if is_prod: \n            input = input.unsqueeze(1) # (seq) -> (seq,1) 1D input. In production/API mode, we receive just one sentence as input\n        \n        current_batch_size = input.size(1)\n    \n        source_hs = self.encoder(input) # (seq,batch) or (seq,1) -> (seq,batch,hidden_dim_s) or (seq,1,hidden_dim_s)\n        \n        # = = = decoder part (one timestep at a time)  = = =\n        \n        # = init =\n        target_h_top = torch.zeros(size=(1, current_batch_size, self.hidden_dim_t)).to(self.device) # (1,batch,hidden_dim_t)\n        target_h_all = torch.zeros(size=(self.num_layers, current_batch_size, self.hidden_dim_t)).to(self.device) # (num_layers,batch,hidden_dim_t)\n        target_input = torch.LongTensor([self.sos_token]).repeat(current_batch_size).unsqueeze(0).to(self.device) # (1,batch)\n        # = / init =\n        \n        pos = 0 # counter for the nb of decoding steps that have been performed\n        eos_counter = 0\n        logits = []\n        \n        while True:\n            \n            if not self.att_strategy == \'none\':\n                source_context = self.att_mech(target_h_top, source_hs) # (1,batch,hidden_dim_s)\n            else:\n                source_context = source_hs[-1,:,:].unsqueeze(0) # (1,batch,hidden_dim_s) last hidden state of encoder\n            \n            prediction, target_h_all = self.decoder(target_input, source_context, target_h_all) # (1,batch,vocab), (num_layers,batch,hidden_dim_t)\n            \n            logits.append(prediction) # we keep the logits to compute the loss later on\n            \n            target_input = prediction.argmax(-1)\n            \n            eos_counter += torch.sum(target_input==self.eos_token).item()\n            \n            pos += 1\n            if pos>=max_size or (eos_counter == current_batch_size and is_prod):\n                break\n        \n        to_return = torch.cat(logits,0) # list of tensors of shape (batch,vocab) -> (seq,batch,vocab)\n        \n        if is_prod:\n            to_return = to_return.squeeze(dim=1) # (seq,vocab)\n        \n        return to_return\n    \n    \n    def fit(self, trainingDataset, testDataset, lr, batch_size, n_epochs, patience, my_optimizer):\n        \n        parameters = [p for p in self.parameters() if p.requires_grad]\n        \n        if my_optimizer == \'adam\':\n            optimizer = optim.Adam(parameters, lr=lr)\n        elif my_optimizer == \'SGD\':\n            optimizer = optim.SGD(parameters, lr=lr) # https://pytorch.org/docs/stable/optim.html#torch.optim.SGD\n            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\'min\',\n                                                             factor=0.1, patience=5, \n                                                             verbose=True, threshold=0.1) # https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.ReduceLROnPlateau\n        \n        criterion = torch.nn.CrossEntropyLoss(ignore_index=self.padding_token) # the softmax is inside the loss!\n        \n        # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n        # we pass a collate function to perform padding on the fly, within each batch\n        # this is better than truncation/padding at the dataset level\n        # TODO bucketing\n        train_loader = data.DataLoader(trainingDataset, batch_size=batch_size, \n                                       shuffle=True, collate_fn=self.my_pad) # returns (batch,seq)\n        \n        test_loader = data.DataLoader(testDataset, batch_size=512, collate_fn=self.my_pad)\n        \n        tdqm_dict_keys = [\'loss\', \'test loss\', \'test BLEU\']\n        tdqm_dict = dict(zip(tdqm_dict_keys,[0.0]*len(tdqm_dict_keys)))\n        \n        patience_counter = 1\n        patience_loss = 99999\n        it_times = []\n        \n        # my fake code\n        #for p in self.parameters():\n        #    if not p.requires_grad:\n        #         print(p.name, p.data)\n        \n        for epoch in range(n_epochs):\n            \n            logs_epoch = {\'loss\':[],\'test_loss\':[],\'test_BLEU\':[]}\n            \n            with tqdm(total=len(train_loader), unit_scale=True, postfix=tdqm_dict, desc=""Epoch : %i/%i"" % (epoch, n_epochs-1), ncols=100) as pbar:\n                \n                for loader_idx, loader in enumerate([train_loader, test_loader]):\n                    \n                    total_loss = 0\n                    total_bleu = 0\n                    \n                    if loader_idx == 0:\n                        self.train()\n                    else:\n                        self.eval() # deactivate dropout etc.\n                    \n                    for i, (batch_source, batch_target) in enumerate(loader):\n                        \n                        start_time = time()\n                        \n                        is_prod = len(batch_source.shape)==1 # if False, 2D input (seq,batch)\n                        \n                        if is_prod:\n                            max_size = self.max_size\n                            self.eval()\n                        else:\n                            max_size = batch_target.size(0) # in train mode, no need to continue generating after the length of the longest ground truth sequence has been exceeded\n                        \n                        unnormalized_logits = self.forward(batch_source, max_size, is_prod) # (seq,batch,vocab)\n                        \n                        # = = loss = = \n                        batch_loss = criterion(unnormalized_logits.flatten(end_dim=1), batch_target.flatten())\n                        # with flatten, (see https://pytorch.org/docs/stable/nn.html#flatten), we turn unnormalized_logits into (seq*batch,vocab). We compare it with (seq*batch,1)\n                        total_loss += batch_loss.item()\n                        updated_loss = total_loss/(i+1)\n                        tdqm_dict[tdqm_dict_keys[loader_idx]] = updated_loss\n                        \n                        # = = BLEU = =\n                        if loader_idx == 1:\n                            refs = self.for_bleu(batch_target, True)\n                            hyps = self.for_bleu(unnormalized_logits, False)\n                            batch_bleu = corpus_bleu(refs, hyps)\n                            total_bleu += batch_bleu\n                            updated_bleu = total_bleu/(i+1)\n                            tdqm_dict[\'test BLEU\'] = round(100*updated_bleu,2)\n                        \n                        pbar.set_postfix(tdqm_dict)\n                        \n                        if loader_idx == 0:\n                            optimizer.zero_grad() # flush gradient attributes\n                            batch_loss.backward() # compute gradients\n                            optimizer.step() # update parameters of the model\n                            pbar.update(1)\n                        \n                        it_times.append(round(time() - start_time,2))\n                        \n                        \n                    # save loss, test loss, and test BLEU, at the end of each epoch\n                    if loader_idx == 0:\n                        logs_epoch[\'loss\'].append(round(updated_loss,4))\n                    \n                    else:\n                       logs_epoch[\'test_loss\'].append(round(updated_loss,4))\n                       logs_epoch[\'test_BLEU\'].append(round(100*updated_bleu,2))\n            \n            \n            self.logs[\'epoch_\' + str(epoch+1)] = logs_epoch\n            \n            if total_loss > patience_loss:\n                patience_counter += 1\n            else:\n                patience_loss = total_loss\n                patience_counter = 1 # reset\n            \n            if patience_counter>patience:\n                break\n            \n            if my_optimizer == \'SGD\':\n                scheduler.step(total_loss)\n            \n            self.test_toy(test_sents) \n        \n        self.logs[\'avg_time_it\'] = round(np.mean(it_times),4)\n        self.logs[\'n_its\'] = len(it_times)\n       \n        \n    def sourceNl_to_ints(self, source_nl):\n        \'\'\'converts natural language source sentence to source integers\'\'\'\n        source_nl_clean = source_nl.lower().replace(""\'"",\' \').replace(\'-\',\' \')\n        source_nl_clean_tok = word_tokenize(source_nl_clean, self.source_language)\n        source_ints = [int(self.vocab_s[elt]) if elt in self.vocab_s else \\\n                       self.oov_token for elt in source_nl_clean_tok]\n        \n        source_ints = torch.LongTensor(source_ints).to(self.device)\n        return source_ints \n    \n    def targetInts_to_nl(self, target_ints):\n        \'\'\'converts integer target sentence to target natural language\'\'\'\n        return [\'<PAD>\' if elt==self.padding_token else \'<OOV>\' if elt==self.oov_token \\\n                else \'<EOS>\' if elt==self.eos_token else \'<SOS>\' if elt==self.sos_token\\\n                else self.vocab_t_inv[elt] for elt in target_ints]\n    \n    def predict(self, source_nl):\n        source_ints = self.sourceNl_to_ints(source_nl)\n        logits = self.forward(source_ints, self.max_size, True) # (seq) -> (<=max_size,vocab)\n        target_ints = logits.argmax(-1).squeeze() # (<=max_size,1) -> (<=max_size)\n        target_ints_list = target_ints.tolist()\n        if not isinstance(target_ints_list, list):\n            target_ints_list = [target_ints_list]\n        target_nl = self.targetInts_to_nl(target_ints_list)\n        return \' \'.join(target_nl)\n    \n    def test_toy(self, source_sents):\n        for elt in source_sents:\n            print(\'= = = = = \\n\',\'%s -> %s\' % (elt, self.predict(elt)))\n    \n    def for_bleu(self, logits_or_ints, is_ref):\n        if not is_ref:\n            # here, logits_or_ints contains the logits\n            targets_ints = logits_or_ints.argmax(-1).permute(1,0) # (seq,batch,vocab) -> (seq,batch) -> (batch,seq)\n        else:\n            # here, we directly have the indexes (integers), as (seq,batch). We just turn it into (batch,seq)\n            targets_ints = logits_or_ints.permute(1,0)\n        \n        sents = [self.targetInts_to_nl(elt.tolist()) for elt in targets_ints] # (batch,seq)\n        # remove all words after the first occurrence of \'<EOS>\'\n        sents = [elt[:elt.index(\'<EOS>\')+1] if \'<EOS>\' in elt else elt for elt in sents]\n        if is_ref:\n            sents = [[elt] for elt in sents] # BLEU expects references to be a list of lists of lists\n        return sents\n\n    def save(self, path_to_save, model_name):\n        attrs = {attr:getattr(self,attr) for attr in self.ARGS}\n        attrs[\'state_dict\'] = self.state_dict()\n        torch.save(attrs, path_to_save + model_name + \'.pt\')\n        \n        args_save = [elt for elt in self.ARGS if \'vocab\' not in elt]\n        params = {attr:getattr(self,attr) for attr in args_save}\n        \n        with open(path_to_save + \'params_\' + model_name + \'.json\', \'w\') as file:\n            json.dump(params, file, indent=4)\n        \n        with open(path_to_save + \'logs_\' + model_name + \'.json\', \'w\') as file:\n            json.dump(self.logs, file, indent=4)\n    \n    @classmethod # a class method does not see the inside of the class (a static method does not take self as first argument)\n    def load(cls,path_to_file):\n        attrs = torch.load(path_to_file, map_location=lambda storage, loc: storage) # allows loading on CPU a model trained on GPU, see https://discuss.pytorch.org/t/on-a-cpu-device-how-to-load-checkpoint-saved-on-gpu-device/349/6\n        state_dict = attrs.pop(\'state_dict\')\n        new = cls(**attrs) # * list and ** names (dict) see args and kwargs\n        new.load_state_dict(state_dict)\n        return new\n'"
NMT/code/preprocessing.py,0,"b'import json\nimport operator\nimport numpy as np\nfrom collections import Counter\nfrom nltk import word_tokenize\n\n# = = = = = = = = = = = = = = = = = = = = =\n\ndef get_vocab(source_or_target,pairs):\n    \'\'\'pairs is of the form [[source_doc_1,target_doc_1],...,[source_doc_n,target_doc_n]]\'\'\'\n    if source_or_target == \'source\':\n        my_idx = 0\n    elif source_or_target == \'target\':\n        my_idx = 1\n    \n    all_docs = [elt[my_idx] for elt in pairs]\n    all_tokens = [elt for sublist in all_docs for elt in sublist] # flatten\n    counts = dict(Counter(all_tokens)) # get token frequencies\n    sorted_counts = sorted(counts.items(), key=operator.itemgetter(1), \\\n                           reverse=True) # assign to each word an index based on its frequency\n    word_to_idx = dict([(my_t[0],idx) for idx,my_t in enumerate(sorted_counts,idx_mf) \\\n                        if my_t[1]>=min_count]) # works because \'sorted_counts\' is sorted in decreasing order!\n    return word_to_idx\n\ndef convert_to_ints(pairs):\n    pairs_int = []\n    for pair in pairs:\n        source = [str(word_to_idx_source[elt]) if elt in word_to_idx_source else \\\n                  oov_token for elt in pair[0]] #+ [eos_token]\n        target = [str(word_to_idx_target[elt]) if elt in word_to_idx_target else \\\n                  oov_token for elt in pair[1]]\n        target = target #+ [eos_token]\n        pairs_int.append([source,target])\n    \n    return pairs_int\n\n# = = = = = = = = = = = = = = = = = = = = =\n\npath_to_data = \'./data/\'\n\nmin_count = 5\npadding_token = \'0\'\noov_token = \'1\'\nsos_token = \'2\' # start of sentence, only needed for the target sentence\neos_token = \'3\' # end of sentence, only needed for the target sentence\nidx_mf = 4\n\n# = = = = = = = = = = = = = = = = = = = = =\n\nwith open(path_to_data + \'eng_fr.txt\', \'r\', encoding=\'utf-8\') as file:\n    pairs = file.read().splitlines()\n    \npairs = [elt.lower().replace(""\'"",\' \').replace(\'-\',\' \').split(\'\\t\')[:2] for \\\n         elt in pairs] # apostrophes and dashes are removed to lower the nb of unique words\n\npairs = [[word_tokenize(elt[0],\'english\'),word_tokenize(elt[1],\'french\')] for elt in pairs]\n\nsource_lens = [len(elt[0]) for elt in pairs]\ntarget_lens = [len(elt[1]) for elt in pairs]\n\nprint(\'source sentence size: min: %s, max: %s, mean: %s\' % (min(source_lens),max(source_lens),np.mean(source_lens)))\nprint(\'target sentence size: min: %s, max: %s, mean: %s\'% (min(target_lens),max(target_lens),np.mean(target_lens)))\n\n# split train/test\ntest_idxs = np.random.choice(range(len(pairs)),size=int(0.2*len(pairs)),replace=False)\ntrain_idxs = list(set(range(len(pairs))).difference(set(test_idxs)))\nassert len(test_idxs) + len(train_idxs) == len(pairs)\n\npairs_train = [pairs[idx] for idx in train_idxs]\npairs_test = [pairs[idx] for idx in test_idxs]\n\n# create dicts from the training set\nword_to_idx_source = get_vocab(\'source\',pairs_train)\nword_to_idx_target = get_vocab(\'target\',pairs_train)\n\nwith open(path_to_data + \'vocab_source.json\', \'w\') as file:\n    json.dump(word_to_idx_source, file, sort_keys=True, indent=4)\n\nwith open(path_to_data + \'vocab_target.json\', \'w\') as file:\n    json.dump(word_to_idx_target, file, sort_keys=True, indent=4)\n\n# transform into indexes\npairs_train_ints = convert_to_ints(pairs_train)\npairs_test_ints = convert_to_ints(pairs_test)\n\n# save to disk\nwith open(path_to_data + \'pairs_train_ints.txt\', \'w\') as file:\n    for elt in pairs_train_ints:\n        to_write = \' \'.join(elt[0]) + \'\\t\' + \' \'.join(elt[1]) + \'\\n\'\n        file.write(to_write)\n\nwith open(path_to_data + \'pairs_test_ints.txt\', \'w\') as file:\n    for elt in pairs_test_ints:\n        to_write = \' \'.join(elt[0]) + \'\\t\' + \' \'.join(elt[1]) + \'\\n\'\n        file.write(to_write)\n\n'"
