file_path,api_count,code
demo.py,3,"b'import os\nimport argparse\nimport torch\n\nfrom torchvision import transforms\nfrom models.fast_scnn import get_fast_scnn\nfrom PIL import Image\nfrom utils.visualize import get_color_pallete\n\nparser = argparse.ArgumentParser(\n    description=\'Predict segmentation result from a given image\')\nparser.add_argument(\'--model\', type=str, default=\'fast_scnn\',\n                    help=\'model name (default: fast_scnn)\')\nparser.add_argument(\'--dataset\', type=str, default=\'citys\',\n                    help=\'dataset name (default: citys)\')\nparser.add_argument(\'--weights-folder\', default=\'./weights\',\n                    help=\'Directory for saving checkpoint models\')\nparser.add_argument(\'--input-pic\', type=str,\n                    default=\'./datasets/citys/leftImg8bit/test/berlin/berlin_000000_000019_leftImg8bit.png\',\n                    help=\'path to the input picture\')\nparser.add_argument(\'--outdir\', default=\'./test_result\', type=str,\n                    help=\'path to save the predict result\')\n\nparser.add_argument(\'--cpu\', dest=\'cpu\', action=\'store_true\')\nparser.set_defaults(cpu=False)\n\nargs = parser.parse_args()\n\n\ndef demo():\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    # output folder\n    if not os.path.exists(args.outdir):\n        os.makedirs(args.outdir)\n\n    # image transform\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n    ])\n    image = Image.open(args.input_pic).convert(\'RGB\')\n    image = transform(image).unsqueeze(0).to(device)\n    model = get_fast_scnn(args.dataset, pretrained=True, root=args.weights_folder, map_cpu=args.cpu).to(device)\n    print(\'Finished loading model!\')\n    model.eval()\n    with torch.no_grad():\n        outputs = model(image)\n    pred = torch.argmax(outputs[0], 1).squeeze(0).cpu().data.numpy()\n    mask = get_color_pallete(pred, args.dataset)\n    outname = os.path.splitext(os.path.split(args.input_pic)[-1])[0] + \'.png\'\n    mask.save(os.path.join(args.outdir, outname))\n\n\nif __name__ == \'__main__\':\n    demo()\n'"
eval.py,2,"b""import os\nimport torch\nimport torch.utils.data as data\n\nfrom torchvision import transforms\nfrom data_loader import get_segmentation_dataset\nfrom models.fast_scnn import get_fast_scnn\nfrom utils.metric import SegmentationMetric\nfrom utils.visualize import get_color_pallete\n\nfrom train import parse_args\n\n\nclass Evaluator(object):\n    def __init__(self, args):\n        self.args = args\n        # output folder\n        self.outdir = 'test_result'\n        if not os.path.exists(self.outdir):\n            os.makedirs(self.outdir)\n        # image transform\n        input_transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize([.485, .456, .406], [.229, .224, .225]),\n        ])\n        # dataset and dataloader\n        val_dataset = get_segmentation_dataset(args.dataset, split='val', mode='testval',\n                                               transform=input_transform)\n        self.val_loader = data.DataLoader(dataset=val_dataset,\n                                          batch_size=1,\n                                          shuffle=False)\n        # create network\n        self.model = get_fast_scnn(args.dataset, aux=args.aux, pretrained=True, root=args.save_folder).to(args.device)\n        print('Finished loading model!')\n\n        self.metric = SegmentationMetric(val_dataset.num_class)\n\n    def eval(self):\n        self.model.eval()\n        for i, (image, label) in enumerate(self.val_loader):\n            image = image.to(self.args.device)\n\n            outputs = self.model(image)\n\n            pred = torch.argmax(outputs[0], 1)\n            pred = pred.cpu().data.numpy()\n            label = label.numpy()\n\n            self.metric.update(pred, label)\n            pixAcc, mIoU = self.metric.get()\n            print('Sample %d, validation pixAcc: %.3f%%, mIoU: %.3f%%' % (i + 1, pixAcc * 100, mIoU * 100))\n\n            predict = pred.squeeze(0)\n            mask = get_color_pallete(predict, self.args.dataset)\n            mask.save(os.path.join(self.outdir, 'seg_{}.png'.format(i)))\n\n\nif __name__ == '__main__':\n    args = parse_args()\n    evaluator = Evaluator(args)\n    print('Testing model: ', args.model)\n    evaluator.eval()\n"""
train.py,9,"b'import os\nimport argparse\nimport time\nimport shutil\n\nimport torch\nimport torch.utils.data as data\nimport torch.backends.cudnn as cudnn\n\nfrom torchvision import transforms\nfrom data_loader import get_segmentation_dataset\nfrom models.fast_scnn import get_fast_scnn\nfrom utils.loss import MixSoftmaxCrossEntropyLoss, MixSoftmaxCrossEntropyOHEMLoss\nfrom utils.lr_scheduler import LRScheduler\nfrom utils.metric import SegmentationMetric\n\n\ndef parse_args():\n    """"""Training Options for Segmentation Experiments""""""\n    parser = argparse.ArgumentParser(description=\'Fast-SCNN on PyTorch\')\n    # model and dataset\n    parser.add_argument(\'--model\', type=str, default=\'fast_scnn\',\n                        help=\'model name (default: fast_scnn)\')\n    parser.add_argument(\'--dataset\', type=str, default=\'citys\',\n                        help=\'dataset name (default: citys)\')\n    parser.add_argument(\'--base-size\', type=int, default=1024,\n                        help=\'base image size\')\n    parser.add_argument(\'--crop-size\', type=int, default=768,\n                        help=\'crop image size\')\n    parser.add_argument(\'--train-split\', type=str, default=\'train\',\n                        help=\'dataset train split (default: train)\')\n    # training hyper params\n    parser.add_argument(\'--aux\', action=\'store_true\', default=False,\n                        help=\'Auxiliary loss\')\n    parser.add_argument(\'--aux-weight\', type=float, default=0.4,\n                        help=\'auxiliary loss weight\')\n    parser.add_argument(\'--epochs\', type=int, default=160, metavar=\'N\',\n                        help=\'number of epochs to train (default: 100)\')\n    parser.add_argument(\'--start_epoch\', type=int, default=0,\n                        metavar=\'N\', help=\'start epochs (default:0)\')\n    parser.add_argument(\'--batch-size\', type=int, default=2,\n                        metavar=\'N\', help=\'input batch size for training (default: 12)\')\n    parser.add_argument(\'--lr\', type=float, default=1e-2, metavar=\'LR\',\n                        help=\'learning rate (default: 1e-2)\')\n    parser.add_argument(\'--momentum\', type=float, default=0.9,\n                        metavar=\'M\', help=\'momentum (default: 0.9)\')\n    parser.add_argument(\'--weight-decay\', type=float, default=1e-4,\n                        metavar=\'M\', help=\'w-decay (default: 1e-4)\')\n    # checking point\n    parser.add_argument(\'--resume\', type=str, default=None,\n                        help=\'put the path to resuming file if needed\')\n    parser.add_argument(\'--save-folder\', default=\'./weights\',\n                        help=\'Directory for saving checkpoint models\')\n    # evaluation only\n    parser.add_argument(\'--eval\', action=\'store_true\', default=False,\n                        help=\'evaluation only\')\n    parser.add_argument(\'--no-val\', action=\'store_true\', default=True,\n                        help=\'skip validation during training\')\n    # the parser\n    args = parser.parse_args()\n    device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n    cudnn.benchmark = True\n    args.device = device\n    print(args)\n    return args\n\n\nclass Trainer(object):\n    def __init__(self, args):\n        self.args = args\n        # image transform\n        input_transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize([.485, .456, .406], [.229, .224, .225]),\n        ])\n        # dataset and dataloader\n        data_kwargs = {\'transform\': input_transform, \'base_size\': args.base_size, \'crop_size\': args.crop_size}\n        train_dataset = get_segmentation_dataset(args.dataset, split=args.train_split, mode=\'train\', **data_kwargs)\n        val_dataset = get_segmentation_dataset(args.dataset, split=\'val\', mode=\'val\', **data_kwargs)\n        self.train_loader = data.DataLoader(dataset=train_dataset,\n                                            batch_size=args.batch_size,\n                                            shuffle=True,\n                                            drop_last=True)\n        self.val_loader = data.DataLoader(dataset=val_dataset,\n                                          batch_size=1,\n                                          shuffle=False)\n\n        # create network\n        self.model = get_fast_scnn(dataset=args.dataset, aux=args.aux)\n        if torch.cuda.device_count() > 1:\n            self.model = torch.nn.DataParallel(self.model, device_ids=[0, 1, 2])\n        self.model.to(args.device)\n\n        # resume checkpoint if needed\n        if args.resume:\n            if os.path.isfile(args.resume):\n                name, ext = os.path.splitext(args.resume)\n                assert ext == \'.pkl\' or \'.pth\', \'Sorry only .pth and .pkl files supported.\'\n                print(\'Resuming training, loading {}...\'.format(args.resume))\n                self.model.load_state_dict(torch.load(args.resume, map_location=lambda storage, loc: storage))\n\n        # create criterion\n        self.criterion = MixSoftmaxCrossEntropyOHEMLoss(aux=args.aux, aux_weight=args.aux_weight,\n                                                        ignore_index=-1).to(args.device)\n\n        # optimizer\n        self.optimizer = torch.optim.SGD(self.model.parameters(),\n                                         lr=args.lr,\n                                         momentum=args.momentum,\n                                         weight_decay=args.weight_decay)\n\n        # lr scheduling\n        self.lr_scheduler = LRScheduler(mode=\'poly\', base_lr=args.lr, nepochs=args.epochs,\n                                        iters_per_epoch=len(self.train_loader), power=0.9)\n\n        # evaluation metrics\n        self.metric = SegmentationMetric(train_dataset.num_class)\n\n        self.best_pred = 0.0\n\n    def train(self):\n        cur_iters = 0\n        start_time = time.time()\n        for epoch in range(self.args.start_epoch, self.args.epochs):\n            self.model.train()\n\n            for i, (images, targets) in enumerate(self.train_loader):\n                cur_lr = self.lr_scheduler(cur_iters)\n                for param_group in self.optimizer.param_groups:\n                    param_group[\'lr\'] = cur_lr\n\n                images = images.to(self.args.device)\n                targets = targets.to(self.args.device)\n\n                outputs = self.model(images)\n                loss = self.criterion(outputs, targets)\n\n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n\n                cur_iters += 1\n                if cur_iters % 10 == 0:\n                    print(\'Epoch: [%2d/%2d] Iter [%4d/%4d] || Time: %4.4f sec || lr: %.8f || Loss: %.4f\' % (\n                        epoch, args.epochs, i + 1, len(self.train_loader),\n                        time.time() - start_time, cur_lr, loss.item()))\n\n            if self.args.no_val:\n                # save every epoch\n                save_checkpoint(self.model, self.args, is_best=False)\n            else:\n                self.validation(epoch)\n\n        save_checkpoint(self.model, self.args, is_best=False)\n\n    def validation(self, epoch):\n        is_best = False\n        self.metric.reset()\n        self.model.eval()\n        for i, (image, target) in enumerate(self.val_loader):\n            image = image.to(self.args.device)\n\n            outputs = self.model(image)\n            pred = torch.argmax(outputs[0], 1)\n            pred = pred.cpu().data.numpy()\n            self.metric.update(pred, target.numpy())\n            pixAcc, mIoU = self.metric.get()\n            print(\'Epoch %d, Sample %d, validation pixAcc: %.3f%%, mIoU: %.3f%%\' % (\n                epoch, i + 1, pixAcc * 100, mIoU * 100))\n\n        new_pred = (pixAcc + mIoU) / 2\n        if new_pred > self.best_pred:\n            is_best = True\n            self.best_pred = new_pred\n        save_checkpoint(self.model, self.args, is_best)\n\n\ndef save_checkpoint(model, args, is_best=False):\n    """"""Save Checkpoint""""""\n    directory = os.path.expanduser(args.save_folder)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n    filename = \'{}_{}.pth\'.format(args.model, args.dataset)\n    save_path = os.path.join(directory, filename)\n    torch.save(model.state_dict(), save_path)\n    if is_best:\n        best_filename = \'{}_{}_best_model.pth\'.format(args.model, args.dataset)\n        best_filename = os.path.join(directory, best_filename)\n        shutil.copyfile(filename, best_filename)\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    trainer = Trainer(args)\n    if args.eval:\n        print(\'Evaluation model: \', args.resume)\n        trainer.validation(args.start_epoch)\n    else:\n        print(\'Starting Epoch: %d, Total Epochs: %d\' % (args.start_epoch, args.epochs))\n        trainer.train()\n'"
data_loader/__init__.py,0,"b'from .cityscapes import CitySegmentation\n\ndatasets = {\n    \'citys\': CitySegmentation,\n}\n\n\ndef get_segmentation_dataset(name, **kwargs):\n    """"""Segmentation Datasets""""""\n    return datasets[name.lower()](**kwargs)\n'"
data_loader/cityscapes.py,3,"b'""""""Cityscapes Dataloader""""""\nimport os\nimport random\nimport numpy as np\nimport torch\nimport torch.utils.data as data\n\nfrom PIL import Image, ImageOps, ImageFilter\n\n__all__ = [\'CitySegmentation\']\n\n\nclass CitySegmentation(data.Dataset):\n    """"""Cityscapes Semantic Segmentation Dataset.\n\n    Parameters\n    ----------\n    root : string\n        Path to Cityscapes folder. Default is \'./datasets/citys\'\n    split: string\n        \'train\', \'val\' or \'test\'\n    transform : callable, optional\n        A function that transforms the image\n    Examples\n    --------\n    >>> from torchvision import transforms\n    >>> import torch.utils.data as data\n    >>> # Transforms for Normalization\n    >>> input_transform = transforms.Compose([\n    >>>     transforms.ToTensor(),\n    >>>     transforms.Normalize((.485, .456, .406), (.229, .224, .225)),\n    >>> ])\n    >>> # Create Dataset\n    >>> trainset = CitySegmentation(split=\'train\', transform=input_transform)\n    >>> # Create Training Loader\n    >>> train_data = data.DataLoader(\n    >>>     trainset, 4, shuffle=True,\n    >>>     num_workers=4)\n    """"""\n    BASE_DIR = \'cityscapes\'\n    NUM_CLASS = 19\n\n    def __init__(self, root=\'./datasets/citys\', split=\'train\', mode=None, transform=None,\n                 base_size=520, crop_size=480, **kwargs):\n        super(CitySegmentation, self).__init__()\n        self.root = root\n        self.split = split\n        self.mode = mode if mode is not None else split\n        self.transform = transform\n        self.base_size = base_size\n        self.crop_size = crop_size\n        self.images, self.mask_paths = _get_city_pairs(self.root, self.split)\n        assert (len(self.images) == len(self.mask_paths))\n        if len(self.images) == 0:\n            raise RuntimeError(""Found 0 images in subfolders of: "" + self.root + ""\\n"")\n        self.valid_classes = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22,\n                              23, 24, 25, 26, 27, 28, 31, 32, 33]\n        self._key = np.array([-1, -1, -1, -1, -1, -1,\n                              -1, -1, 0, 1, -1, -1,\n                              2, 3, 4, -1, -1, -1,\n                              5, -1, 6, 7, 8, 9,\n                              10, 11, 12, 13, 14, 15,\n                              -1, -1, 16, 17, 18])\n        self._mapping = np.array(range(-1, len(self._key) - 1)).astype(\'int32\')\n\n    def _class_to_index(self, mask):\n        values = np.unique(mask)\n        for value in values:\n            assert (value in self._mapping)\n        index = np.digitize(mask.ravel(), self._mapping, right=True)\n        return self._key[index].reshape(mask.shape)\n\n    def __getitem__(self, index):\n        img = Image.open(self.images[index]).convert(\'RGB\')\n        if self.mode == \'test\':\n            if self.transform is not None:\n                img = self.transform(img)\n            return img, os.path.basename(self.images[index])\n        mask = Image.open(self.mask_paths[index])\n        # synchrosized transform\n        if self.mode == \'train\':\n            img, mask = self._sync_transform(img, mask)\n        elif self.mode == \'val\':\n            img, mask = self._val_sync_transform(img, mask)\n        else:\n            assert self.mode == \'testval\'\n            img, mask = self._img_transform(img), self._mask_transform(mask)\n        # general resize, normalize and toTensor\n        if self.transform is not None:\n            img = self.transform(img)\n        return img, mask\n\n    def _val_sync_transform(self, img, mask):\n        outsize = self.crop_size\n        short_size = outsize\n        w, h = img.size\n        if w > h:\n            oh = short_size\n            ow = int(1.0 * w * oh / h)\n        else:\n            ow = short_size\n            oh = int(1.0 * h * ow / w)\n        img = img.resize((ow, oh), Image.BILINEAR)\n        mask = mask.resize((ow, oh), Image.NEAREST)\n        # center crop\n        w, h = img.size\n        x1 = int(round((w - outsize) / 2.))\n        y1 = int(round((h - outsize) / 2.))\n        img = img.crop((x1, y1, x1 + outsize, y1 + outsize))\n        mask = mask.crop((x1, y1, x1 + outsize, y1 + outsize))\n        # final transform\n        img, mask = self._img_transform(img), self._mask_transform(mask)\n        return img, mask\n\n    def _sync_transform(self, img, mask):\n        # random mirror\n        if random.random() < 0.5:\n            img = img.transpose(Image.FLIP_LEFT_RIGHT)\n            mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n        crop_size = self.crop_size\n        # random scale (short edge)\n        short_size = random.randint(int(self.base_size * 0.5), int(self.base_size * 2.0))\n        w, h = img.size\n        if h > w:\n            ow = short_size\n            oh = int(1.0 * h * ow / w)\n        else:\n            oh = short_size\n            ow = int(1.0 * w * oh / h)\n        img = img.resize((ow, oh), Image.BILINEAR)\n        mask = mask.resize((ow, oh), Image.NEAREST)\n        # pad crop\n        if short_size < crop_size:\n            padh = crop_size - oh if oh < crop_size else 0\n            padw = crop_size - ow if ow < crop_size else 0\n            img = ImageOps.expand(img, border=(0, 0, padw, padh), fill=0)\n            mask = ImageOps.expand(mask, border=(0, 0, padw, padh), fill=0)\n        # random crop crop_size\n        w, h = img.size\n        x1 = random.randint(0, w - crop_size)\n        y1 = random.randint(0, h - crop_size)\n        img = img.crop((x1, y1, x1 + crop_size, y1 + crop_size))\n        mask = mask.crop((x1, y1, x1 + crop_size, y1 + crop_size))\n        # gaussian blur as in PSP\n        if random.random() < 0.5:\n            img = img.filter(ImageFilter.GaussianBlur(\n                radius=random.random()))\n        # final transform\n        img, mask = self._img_transform(img), self._mask_transform(mask)\n        return img, mask\n\n    def _img_transform(self, img):\n        return np.array(img)\n\n    def _mask_transform(self, mask):\n        target = self._class_to_index(np.array(mask).astype(\'int32\'))\n        return torch.LongTensor(np.array(target).astype(\'int32\'))\n\n    def __len__(self):\n        return len(self.images)\n\n    @property\n    def num_class(self):\n        """"""Number of categories.""""""\n        return self.NUM_CLASS\n\n    @property\n    def pred_offset(self):\n        return 0\n\n\ndef _get_city_pairs(folder, split=\'train\'):\n    def get_path_pairs(img_folder, mask_folder):\n        img_paths = []\n        mask_paths = []\n        for root, _, files in os.walk(img_folder):\n            for filename in files:\n                if filename.endswith("".png""):\n                    imgpath = os.path.join(root, filename)\n                    foldername = os.path.basename(os.path.dirname(imgpath))\n                    maskname = filename.replace(\'leftImg8bit\', \'gtFine_labelIds\')\n                    maskpath = os.path.join(mask_folder, foldername, maskname)\n                    if os.path.isfile(imgpath) and os.path.isfile(maskpath):\n                        img_paths.append(imgpath)\n                        mask_paths.append(maskpath)\n                    else:\n                        print(\'cannot find the mask or image:\', imgpath, maskpath)\n        print(\'Found {} images in the folder {}\'.format(len(img_paths), img_folder))\n        return img_paths, mask_paths\n\n    if split in (\'train\', \'val\'):\n        img_folder = os.path.join(folder, \'leftImg8bit/\' + split)\n        mask_folder = os.path.join(folder, \'gtFine/\' + split)\n        img_paths, mask_paths = get_path_pairs(img_folder, mask_folder)\n        return img_paths, mask_paths\n    else:\n        assert split == \'trainval\'\n        print(\'trainval set\')\n        train_img_folder = os.path.join(folder, \'leftImg8bit/train\')\n        train_mask_folder = os.path.join(folder, \'gtFine/train\')\n        val_img_folder = os.path.join(folder, \'leftImg8bit/val\')\n        val_mask_folder = os.path.join(folder, \'gtFine/val\')\n        train_img_paths, train_mask_paths = get_path_pairs(train_img_folder, train_mask_folder)\n        val_img_paths, val_mask_paths = get_path_pairs(val_img_folder, val_mask_folder)\n        img_paths = train_img_paths + val_img_paths\n        mask_paths = train_mask_paths + val_mask_paths\n    return img_paths, mask_paths\n\n\nif __name__ == \'__main__\':\n    dataset = CitySegmentation()\n    img, label = dataset[0]\n'"
models/__init__.py,0,b''
models/fast_scnn.py,6,"b'###########################################################################\n# Created by: Tramac\n# Date: 2019-03-25\n# Copyright (c) 2017\n###########################################################################\n\n""""""Fast Segmentation Convolutional Neural Network""""""\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n__all__ = [\'FastSCNN\', \'get_fast_scnn\']\n\n\nclass FastSCNN(nn.Module):\n    def __init__(self, num_classes, aux=False, **kwargs):\n        super(FastSCNN, self).__init__()\n        self.aux = aux\n        self.learning_to_downsample = LearningToDownsample(32, 48, 64)\n        self.global_feature_extractor = GlobalFeatureExtractor(64, [64, 96, 128], 128, 6, [3, 3, 3])\n        self.feature_fusion = FeatureFusionModule(64, 128, 128)\n        self.classifier = Classifer(128, num_classes)\n        if self.aux:\n            self.auxlayer = nn.Sequential(\n                nn.Conv2d(64, 32, 3, padding=1, bias=False),\n                nn.BatchNorm2d(32),\n                nn.ReLU(True),\n                nn.Dropout(0.1),\n                nn.Conv2d(32, num_classes, 1)\n            )\n\n    def forward(self, x):\n        size = x.size()[2:]\n        higher_res_features = self.learning_to_downsample(x)\n        x = self.global_feature_extractor(higher_res_features)\n        x = self.feature_fusion(higher_res_features, x)\n        x = self.classifier(x)\n        outputs = []\n        x = F.interpolate(x, size, mode=\'bilinear\', align_corners=True)\n        outputs.append(x)\n        if self.aux:\n            auxout = self.auxlayer(higher_res_features)\n            auxout = F.interpolate(auxout, size, mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout)\n        return tuple(outputs)\n\n\nclass _ConvBNReLU(nn.Module):\n    """"""Conv-BN-ReLU""""""\n\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=0, **kwargs):\n        super(_ConvBNReLU, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True)\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass _DSConv(nn.Module):\n    """"""Depthwise Separable Convolutions""""""\n\n    def __init__(self, dw_channels, out_channels, stride=1, **kwargs):\n        super(_DSConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(dw_channels, dw_channels, 3, stride, 1, groups=dw_channels, bias=False),\n            nn.BatchNorm2d(dw_channels),\n            nn.ReLU(True),\n            nn.Conv2d(dw_channels, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True)\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass _DWConv(nn.Module):\n    def __init__(self, dw_channels, out_channels, stride=1, **kwargs):\n        super(_DWConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(dw_channels, out_channels, 3, stride, 1, groups=dw_channels, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True)\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass LinearBottleneck(nn.Module):\n    """"""LinearBottleneck used in MobileNetV2""""""\n\n    def __init__(self, in_channels, out_channels, t=6, stride=2, **kwargs):\n        super(LinearBottleneck, self).__init__()\n        self.use_shortcut = stride == 1 and in_channels == out_channels\n        self.block = nn.Sequential(\n            # pw\n            _ConvBNReLU(in_channels, in_channels * t, 1),\n            # dw\n            _DWConv(in_channels * t, in_channels * t, stride),\n            # pw-linear\n            nn.Conv2d(in_channels * t, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels)\n        )\n\n    def forward(self, x):\n        out = self.block(x)\n        if self.use_shortcut:\n            out = x + out\n        return out\n\n\nclass PyramidPooling(nn.Module):\n    """"""Pyramid pooling module""""""\n\n    def __init__(self, in_channels, out_channels, **kwargs):\n        super(PyramidPooling, self).__init__()\n        inter_channels = int(in_channels / 4)\n        self.conv1 = _ConvBNReLU(in_channels, inter_channels, 1, **kwargs)\n        self.conv2 = _ConvBNReLU(in_channels, inter_channels, 1, **kwargs)\n        self.conv3 = _ConvBNReLU(in_channels, inter_channels, 1, **kwargs)\n        self.conv4 = _ConvBNReLU(in_channels, inter_channels, 1, **kwargs)\n        self.out = _ConvBNReLU(in_channels * 2, out_channels, 1)\n\n    def pool(self, x, size):\n        avgpool = nn.AdaptiveAvgPool2d(size)\n        return avgpool(x)\n\n    def upsample(self, x, size):\n        return F.interpolate(x, size, mode=\'bilinear\', align_corners=True)\n\n    def forward(self, x):\n        size = x.size()[2:]\n        feat1 = self.upsample(self.conv1(self.pool(x, 1)), size)\n        feat2 = self.upsample(self.conv2(self.pool(x, 2)), size)\n        feat3 = self.upsample(self.conv3(self.pool(x, 3)), size)\n        feat4 = self.upsample(self.conv4(self.pool(x, 6)), size)\n        x = torch.cat([x, feat1, feat2, feat3, feat4], dim=1)\n        x = self.out(x)\n        return x\n\n\nclass LearningToDownsample(nn.Module):\n    """"""Learning to downsample module""""""\n\n    def __init__(self, dw_channels1=32, dw_channels2=48, out_channels=64, **kwargs):\n        super(LearningToDownsample, self).__init__()\n        self.conv = _ConvBNReLU(3, dw_channels1, 3, 2)\n        self.dsconv1 = _DSConv(dw_channels1, dw_channels2, 2)\n        self.dsconv2 = _DSConv(dw_channels2, out_channels, 2)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.dsconv1(x)\n        x = self.dsconv2(x)\n        return x\n\n\nclass GlobalFeatureExtractor(nn.Module):\n    """"""Global feature extractor module""""""\n\n    def __init__(self, in_channels=64, block_channels=(64, 96, 128),\n                 out_channels=128, t=6, num_blocks=(3, 3, 3), **kwargs):\n        super(GlobalFeatureExtractor, self).__init__()\n        self.bottleneck1 = self._make_layer(LinearBottleneck, in_channels, block_channels[0], num_blocks[0], t, 2)\n        self.bottleneck2 = self._make_layer(LinearBottleneck, block_channels[0], block_channels[1], num_blocks[1], t, 2)\n        self.bottleneck3 = self._make_layer(LinearBottleneck, block_channels[1], block_channels[2], num_blocks[2], t, 1)\n        self.ppm = PyramidPooling(block_channels[2], out_channels)\n\n    def _make_layer(self, block, inplanes, planes, blocks, t=6, stride=1):\n        layers = []\n        layers.append(block(inplanes, planes, t, stride))\n        for i in range(1, blocks):\n            layers.append(block(planes, planes, t, 1))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.bottleneck1(x)\n        x = self.bottleneck2(x)\n        x = self.bottleneck3(x)\n        x = self.ppm(x)\n        return x\n\n\nclass FeatureFusionModule(nn.Module):\n    """"""Feature fusion module""""""\n\n    def __init__(self, highter_in_channels, lower_in_channels, out_channels, scale_factor=4, **kwargs):\n        super(FeatureFusionModule, self).__init__()\n        self.scale_factor = scale_factor\n        self.dwconv = _DWConv(lower_in_channels, out_channels, 1)\n        self.conv_lower_res = nn.Sequential(\n            nn.Conv2d(out_channels, out_channels, 1),\n            nn.BatchNorm2d(out_channels)\n        )\n        self.conv_higher_res = nn.Sequential(\n            nn.Conv2d(highter_in_channels, out_channels, 1),\n            nn.BatchNorm2d(out_channels)\n        )\n        self.relu = nn.ReLU(True)\n\n    def forward(self, higher_res_feature, lower_res_feature):\n        lower_res_feature = F.interpolate(lower_res_feature, scale_factor=4, mode=\'bilinear\', align_corners=True)\n        lower_res_feature = self.dwconv(lower_res_feature)\n        lower_res_feature = self.conv_lower_res(lower_res_feature)\n\n        higher_res_feature = self.conv_higher_res(higher_res_feature)\n        out = higher_res_feature + lower_res_feature\n        return self.relu(out)\n\n\nclass Classifer(nn.Module):\n    """"""Classifer""""""\n\n    def __init__(self, dw_channels, num_classes, stride=1, **kwargs):\n        super(Classifer, self).__init__()\n        self.dsconv1 = _DSConv(dw_channels, dw_channels, stride)\n        self.dsconv2 = _DSConv(dw_channels, dw_channels, stride)\n        self.conv = nn.Sequential(\n            nn.Dropout(0.1),\n            nn.Conv2d(dw_channels, num_classes, 1)\n        )\n\n    def forward(self, x):\n        x = self.dsconv1(x)\n        x = self.dsconv2(x)\n        x = self.conv(x)\n        return x\n\n\ndef get_fast_scnn(dataset=\'citys\', pretrained=False, root=\'./weights\', map_cpu=False, **kwargs):\n    acronyms = {\n        \'pascal_voc\': \'voc\',\n        \'pascal_aug\': \'voc\',\n        \'ade20k\': \'ade\',\n        \'coco\': \'coco\',\n        \'citys\': \'citys\',\n    }\n    from data_loader import datasets\n    model = FastSCNN(datasets[dataset].NUM_CLASS, **kwargs)\n    if pretrained:\n        if(map_cpu):\n            model.load_state_dict(torch.load(os.path.join(root, \'fast_scnn_%s.pth\' % acronyms[dataset]), map_location=\'cpu\'))\n        else:\n            model.load_state_dict(torch.load(os.path.join(root, \'fast_scnn_%s.pth\' % acronyms[dataset])))\n    return model\n\n\nif __name__ == \'__main__\':\n    img = torch.randn(2, 3, 256, 512)\n    model = get_fast_scnn(\'citys\')\n    outputs = model(img)\n'"
utils/__init__.py,0,b''
utils/loss.py,6,"b'""""""Custom losses.""""""\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\nfrom torch.autograd import Variable\n\n__all__ = [\'MixSoftmaxCrossEntropyLoss\', \'MixSoftmaxCrossEntropyOHEMLoss\']\n\n\nclass MixSoftmaxCrossEntropyLoss(nn.CrossEntropyLoss):\n    def __init__(self, aux=True, aux_weight=0.2, ignore_label=-1, **kwargs):\n        super(MixSoftmaxCrossEntropyLoss, self).__init__(ignore_index=ignore_label)\n        self.aux = aux\n        self.aux_weight = aux_weight\n\n    def _aux_forward(self, *inputs, **kwargs):\n        *preds, target = tuple(inputs)\n\n        loss = super(MixSoftmaxCrossEntropyLoss, self).forward(preds[0], target)\n        for i in range(1, len(preds)):\n            aux_loss = super(MixSoftmaxCrossEntropyLoss, self).forward(preds[i], target)\n            loss += self.aux_weight * aux_loss\n        return loss\n\n    def forward(self, *inputs, **kwargs):\n        preds, target = tuple(inputs)\n        inputs = tuple(list(preds) + [target])\n        if self.aux:\n            return self._aux_forward(*inputs)\n        else:\n            return super(MixSoftmaxCrossEntropyLoss, self).forward(*inputs)\n\n\nclass SoftmaxCrossEntropyOHEMLoss(nn.Module):\n    def __init__(self, ignore_label=-1, thresh=0.7, min_kept=256, use_weight=True, **kwargs):\n        super(SoftmaxCrossEntropyOHEMLoss, self).__init__()\n        self.ignore_label = ignore_label\n        self.thresh = float(thresh)\n        self.min_kept = int(min_kept)\n        if use_weight:\n            print(""w/ class balance"")\n            weight = torch.FloatTensor([0.8373, 0.918, 0.866, 1.0345, 1.0166, 0.9969, 0.9754,\n                                        1.0489, 0.8786, 1.0023, 0.9539, 0.9843, 1.1116, 0.9037, 1.0865, 1.0955,\n                                        1.0865, 1.1529, 1.0507])\n            self.criterion = torch.nn.CrossEntropyLoss(weight=weight, ignore_index=ignore_label)\n        else:\n            print(""w/o class balance"")\n            self.criterion = torch.nn.CrossEntropyLoss(ignore_index=ignore_label)\n\n    def forward(self, predict, target, weight=None):\n        assert not target.requires_grad\n        assert predict.dim() == 4\n        assert target.dim() == 3\n        assert predict.size(0) == target.size(0), ""{0} vs {1} "".format(predict.size(0), target.size(0))\n        assert predict.size(2) == target.size(1), ""{0} vs {1} "".format(predict.size(2), target.size(1))\n        assert predict.size(3) == target.size(2), ""{0} vs {1} "".format(predict.size(3), target.size(3))\n\n        n, c, h, w = predict.size()\n        input_label = target.data.cpu().numpy().ravel().astype(np.int32)\n        x = np.rollaxis(predict.data.cpu().numpy(), 1).reshape((c, -1))\n        input_prob = np.exp(x - x.max(axis=0).reshape((1, -1)))\n        input_prob /= input_prob.sum(axis=0).reshape((1, -1))\n\n        valid_flag = input_label != self.ignore_label\n        valid_inds = np.where(valid_flag)[0]\n        label = input_label[valid_flag]\n        num_valid = valid_flag.sum()\n        if self.min_kept >= num_valid:\n            print(\'Labels: {}\'.format(num_valid))\n        elif num_valid > 0:\n            prob = input_prob[:, valid_flag]\n            pred = prob[label, np.arange(len(label), dtype=np.int32)]\n            threshold = self.thresh\n            if self.min_kept > 0:\n                index = pred.argsort()\n                threshold_index = index[min(len(index), self.min_kept) - 1]\n                if pred[threshold_index] > self.thresh:\n                    threshold = pred[threshold_index]\n            kept_flag = pred <= threshold\n            valid_inds = valid_inds[kept_flag]\n\n        label = input_label[valid_inds].copy()\n        input_label.fill(self.ignore_label)\n        input_label[valid_inds] = label\n        valid_flag_new = input_label != self.ignore_label\n        # print(np.sum(valid_flag_new))\n        target = Variable(torch.from_numpy(input_label.reshape(target.size())).long().cuda())\n\n        return self.criterion(predict, target)\n\n\nclass MixSoftmaxCrossEntropyOHEMLoss(SoftmaxCrossEntropyOHEMLoss):\n    def __init__(self, aux=False, aux_weight=0.2, ignore_index=-1, **kwargs):\n        super(MixSoftmaxCrossEntropyOHEMLoss, self).__init__(ignore_label=ignore_index, **kwargs)\n        self.aux = aux\n        self.aux_weight = aux_weight\n\n    def _aux_forward(self, *inputs, **kwargs):\n        *preds, target = tuple(inputs)\n\n        loss = super(MixSoftmaxCrossEntropyOHEMLoss, self).forward(preds[0], target)\n        for i in range(1, len(preds)):\n            aux_loss = super(MixSoftmaxCrossEntropyOHEMLoss, self).forward(preds[i], target)\n            loss += self.aux_weight * aux_loss\n        return loss\n\n    def forward(self, *inputs, **kwargs):\n        preds, target = tuple(inputs)\n        inputs = tuple(list(preds) + [target])\n        if self.aux:\n            return self._aux_forward(*inputs)\n        else:\n            return super(MixSoftmaxCrossEntropyOHEMLoss, self).forward(*inputs)\n'"
utils/lr_scheduler.py,0,"b'""""""Popular Learning Rate Schedulers""""""\nfrom __future__ import division\nimport math\n\n\nclass LRScheduler(object):\n    r""""""Learning Rate Scheduler\n\n    Parameters\n    ----------\n    mode : str\n        Modes for learning rate scheduler.\n        Currently it supports \'constant\', \'step\', \'linear\', \'poly\' and \'cosine\'.\n    base_lr : float\n        Base learning rate, i.e. the starting learning rate.\n    target_lr : float\n        Target learning rate, i.e. the ending learning rate.\n        With constant mode target_lr is ignored.\n    niters : int\n        Number of iterations to be scheduled.\n    nepochs : int\n        Number of epochs to be scheduled.\n    iters_per_epoch : int\n        Number of iterations in each epoch.\n    offset : int\n        Number of iterations before this scheduler.\n    power : float\n        Power parameter of poly scheduler.\n    step_iter : list\n        A list of iterations to decay the learning rate.\n    step_epoch : list\n        A list of epochs to decay the learning rate.\n    step_factor : float\n        Learning rate decay factor.\n    """"""\n\n    def __init__(self, mode, base_lr=0.01, target_lr=0, niters=0, nepochs=0, iters_per_epoch=0,\n                 offset=0, power=2, step_iter=None, step_epoch=None, step_factor=0.1):\n        super(LRScheduler, self).__init__()\n        assert (mode in [\'constant\', \'step\', \'linear\', \'poly\', \'cosine\'])\n\n        self.mode = mode\n        if mode == \'step\':\n            assert (step_iter is not None or step_epoch is not None)\n        self.base_lr = base_lr\n        self.target_lr = target_lr\n        if self.mode == \'constant\':\n            self.target_lr = self.base_lr\n\n        self.niters = niters\n        self.step = step_iter\n        epoch_iters = nepochs * iters_per_epoch\n        if epoch_iters > 0:\n            self.niters = epoch_iters\n            if step_epoch is not None:\n                self.step = [s * iters_per_epoch for s in step_epoch]\n\n        self.offset = offset\n        self.power = power\n        self.step_factor = step_factor\n\n    def __call__(self, num_update):\n        self.update(num_update)\n        return self.learning_rate\n\n    def update(self, num_update):\n        N = self.niters - 1\n        T = num_update - self.offset\n        T = min(max(0, T), N)\n\n        if self.mode == \'constant\':\n            factor = 0\n        elif self.mode == \'linear\':\n            factor = 1 - T / N\n        elif self.mode == \'poly\':\n            factor = pow(1 - T / N, self.power)\n        elif self.mode == \'cosine\':\n            factor = (1 + math.cos(math.pi * T / N)) / 2\n        elif self.mode == \'step\':\n            if self.step is not None:\n                count = sum([1 for s in self.step if s <= T])\n                factor = pow(self.step_factor, count)\n            else:\n                factor = 1\n        else:\n            raise NotImplementedError\n\n        if self.mode == \'step\':\n            self.learning_rate = self.base_lr * factor\n        else:\n            self.learning_rate = self.target_lr + (self.base_lr - self.target_lr) * factor\n\n\nif __name__ == \'__main__\':\n    lr_scheduler = LRScheduler(mode=\'poly\', base_lr=0.01, nepochs=60,\n                               iters_per_epoch=176, power=0.9)\n    for i in range(60 * 176):\n        lr = lr_scheduler(i)\n        print(lr)\n'"
utils/metric.py,0,"b'from __future__ import division\n\nimport threading\nimport numpy as np\n\n__all__ = [\'SegmentationMetric\', \'batch_pix_accuracy\', \'batch_intersection_union\',\n           \'pixelAccuracy\', \'intersectionAndUnion\', \'hist_info\', \'compute_score\']\n\n""""""Evaluation Metrics for Semantic Segmentation""""""\n\n\nclass SegmentationMetric(object):\n    """"""Computes pixAcc and mIoU metric scores\n    """"""\n\n    def __init__(self, nclass):\n        super(SegmentationMetric, self).__init__()\n        self.nclass = nclass\n        self.lock = threading.Lock()\n        self.reset()\n\n    def update(self, preds, labels):\n        """"""Updates the internal evaluation result.\n\n        Parameters\n        ----------\n        labels : \'NumpyArray\' or list of `NumpyArray`\n            The labels of the data.\n        preds : \'NumpyArray\' or list of `NumpyArray`\n            Predicted values.\n        """"""\n        if isinstance(preds, np.ndarray):\n            self.evaluate_worker(preds, labels)\n        elif isinstance(preds, (list, tuple)):\n            threads = [threading.Thread(target=self.evaluate_worker, args=(pred, label), )\n                       for (pred, label) in zip(preds, labels)]\n            for thread in threads:\n                thread.start()\n            for thread in threads:\n                thread.join()\n\n    def get(self):\n        """"""Gets the current evaluation result.\n\n        Returns\n        -------\n        metrics : tuple of float\n            pixAcc and mIoU\n        """"""\n        pixAcc = 1.0 * self.total_correct / (np.spacing(1) + self.total_label)\n        IoU = 1.0 * self.total_inter / (np.spacing(1) + self.total_union)\n        # It has same result with np.nanmean() when all class exist\n        mIoU = IoU.mean()\n        return pixAcc, mIoU\n\n    def evaluate_worker(self, pred, label):\n        correct, labeled = batch_pix_accuracy(pred, label)\n        inter, union = batch_intersection_union(pred, label, self.nclass)\n        with self.lock:\n            self.total_correct += correct\n            self.total_label += labeled\n            self.total_inter += inter\n            self.total_union += union\n\n    def reset(self):\n        """"""Resets the internal evaluation result to initial state.""""""\n        self.total_inter = 0\n        self.total_union = 0\n        self.total_correct = 0\n        self.total_label = 0\n\n\ndef batch_pix_accuracy(predict, target):\n    """"""PixAcc""""""\n    # inputs are numpy array, output 4D, target 3D\n    assert predict.shape == target.shape\n    predict = predict.astype(\'int64\') + 1\n    target = target.astype(\'int64\') + 1\n\n    pixel_labeled = np.sum(target > 0)\n    pixel_correct = np.sum((predict == target) * (target > 0))\n    assert pixel_correct <= pixel_labeled, ""Correct area should be smaller than Labeled""\n    return pixel_correct, pixel_labeled\n\n\ndef batch_intersection_union(predict, target, nclass):\n    """"""mIoU""""""\n    # inputs are numpy array, output 4D, target 3D\n    assert predict.shape == target.shape\n    mini = 1\n    maxi = nclass\n    nbins = nclass\n    predict = predict.astype(\'int64\') + 1\n    target = target.astype(\'int64\') + 1\n\n    predict = predict * (target > 0).astype(predict.dtype)\n    intersection = predict * (predict == target)\n    # areas of intersection and union\n    # element 0 in intersection occur the main difference from np.bincount. set boundary to -1 is necessary.\n    area_inter, _ = np.histogram(intersection, bins=nbins, range=(mini, maxi))\n    area_pred, _ = np.histogram(predict, bins=nbins, range=(mini, maxi))\n    area_lab, _ = np.histogram(target, bins=nbins, range=(mini, maxi))\n    area_union = area_pred + area_lab - area_inter\n    assert (area_inter <= area_union).all(), ""Intersection area should be smaller than Union area""\n    return area_inter, area_union\n\n\ndef pixelAccuracy(imPred, imLab):\n    """"""\n    This function takes the prediction and label of a single image, returns pixel-wise accuracy\n    To compute over many images do:\n    for i = range(Nimages):\n         (pixel_accuracy[i], pixel_correct[i], pixel_labeled[i]) = \\\n            pixelAccuracy(imPred[i], imLab[i])\n    mean_pixel_accuracy = 1.0 * np.sum(pixel_correct) / (np.spacing(1) + np.sum(pixel_labeled))\n    """"""\n    # Remove classes from unlabeled pixels in gt image.\n    # We should not penalize detections in unlabeled portions of the image.\n    pixel_labeled = np.sum(imLab >= 0)\n    pixel_correct = np.sum((imPred == imLab) * (imLab >= 0))\n    pixel_accuracy = 1.0 * pixel_correct / pixel_labeled\n    return (pixel_accuracy, pixel_correct, pixel_labeled)\n\n\ndef intersectionAndUnion(imPred, imLab, numClass):\n    """"""\n    This function takes the prediction and label of a single image,\n    returns intersection and union areas for each class\n    To compute over many images do:\n    for i in range(Nimages):\n        (area_intersection[:,i], area_union[:,i]) = intersectionAndUnion(imPred[i], imLab[i])\n    IoU = 1.0 * np.sum(area_intersection, axis=1) / np.sum(np.spacing(1)+area_union, axis=1)\n    """"""\n    # Remove classes from unlabeled pixels in gt image.\n    # We should not penalize detections in unlabeled portions of the image.\n    imPred = imPred * (imLab >= 0)\n\n    # Compute area intersection:\n    intersection = imPred * (imPred == imLab)\n    (area_intersection, _) = np.histogram(intersection, bins=numClass, range=(1, numClass))\n\n    # Compute area union:\n    (area_pred, _) = np.histogram(imPred, bins=numClass, range=(1, numClass))\n    (area_lab, _) = np.histogram(imLab, bins=numClass, range=(1, numClass))\n    area_union = area_pred + area_lab - area_intersection\n    return (area_intersection, area_union)\n\n\ndef hist_info(pred, label, num_cls):\n    assert pred.shape == label.shape\n    k = (label >= 0) & (label < num_cls)\n    labeled = np.sum(k)\n    correct = np.sum((pred[k] == label[k]))\n\n    return np.bincount(num_cls * label[k].astype(int) + pred[k], minlength=num_cls ** 2).reshape(num_cls,\n                                                                                                 num_cls), labeled, correct\n\n\ndef compute_score(hist, correct, labeled):\n    iu = np.diag(hist) / (hist.sum(1) + hist.sum(0) - np.diag(hist))\n    # print(\'right\')\n    # print(iu)\n    mean_IU = np.nanmean(iu)\n    mean_IU_no_back = np.nanmean(iu[1:])\n    freq = hist.sum(1) / hist.sum()\n    freq_IU = (iu[freq > 0] * freq[freq > 0]).sum()\n    mean_pixel_acc = correct / labeled\n\n    return iu, mean_IU, mean_IU_no_back, mean_pixel_acc\n'"
utils/visualize.py,0,"b'""""""Visualization Utils""""""\nfrom PIL import Image\n\n__all__ = [\'get_color_pallete\']\n\n\ndef get_color_pallete(npimg, dataset=\'citys\'):\n    """"""Visualize image.\n\n    Parameters\n    ----------\n    npimg : numpy.ndarray\n        Single channel image with shape `H, W, 1`.\n    dataset : str, default: \'pascal_voc\'\n        The dataset that model pretrained on. (\'pascal_voc\', \'ade20k\')\n    Returns\n    -------\n    out_img : PIL.Image\n        Image with color pallete\n    """"""\n    # recovery boundary\n    if dataset in (\'pascal_voc\', \'pascal_aug\'):\n        npimg[npimg == -1] = 255\n    # put colormap\n    if dataset == \'ade20k\':\n        npimg = npimg + 1\n        out_img = Image.fromarray(npimg.astype(\'uint8\'))\n        out_img.putpalette(adepallete)\n        return out_img\n    elif dataset == \'citys\':\n        out_img = Image.fromarray(npimg.astype(\'uint8\'))\n        out_img.putpalette(cityspallete)\n        return out_img\n    out_img = Image.fromarray(npimg.astype(\'uint8\'))\n    out_img.putpalette(vocpallete)\n    return out_img\n\n\ndef _getvocpallete(num_cls):\n    n = num_cls\n    pallete = [0] * (n * 3)\n    for j in range(0, n):\n        lab = j\n        pallete[j * 3 + 0] = 0\n        pallete[j * 3 + 1] = 0\n        pallete[j * 3 + 2] = 0\n        i = 0\n        while (lab > 0):\n            pallete[j * 3 + 0] |= (((lab >> 0) & 1) << (7 - i))\n            pallete[j * 3 + 1] |= (((lab >> 1) & 1) << (7 - i))\n            pallete[j * 3 + 2] |= (((lab >> 2) & 1) << (7 - i))\n            i = i + 1\n            lab >>= 3\n    return pallete\n\n\nvocpallete = _getvocpallete(256)\n\nadepallete = [\n    0, 0, 0, 120, 120, 120, 180, 120, 120, 6, 230, 230, 80, 50, 50, 4, 200, 3, 120, 120, 80, 140, 140, 140, 204,\n    5, 255, 230, 230, 230, 4, 250, 7, 224, 5, 255, 235, 255, 7, 150, 5, 61, 120, 120, 70, 8, 255, 51, 255, 6, 82,\n    143, 255, 140, 204, 255, 4, 255, 51, 7, 204, 70, 3, 0, 102, 200, 61, 230, 250, 255, 6, 51, 11, 102, 255, 255,\n    7, 71, 255, 9, 224, 9, 7, 230, 220, 220, 220, 255, 9, 92, 112, 9, 255, 8, 255, 214, 7, 255, 224, 255, 184, 6,\n    10, 255, 71, 255, 41, 10, 7, 255, 255, 224, 255, 8, 102, 8, 255, 255, 61, 6, 255, 194, 7, 255, 122, 8, 0, 255,\n    20, 255, 8, 41, 255, 5, 153, 6, 51, 255, 235, 12, 255, 160, 150, 20, 0, 163, 255, 140, 140, 140, 250, 10, 15,\n    20, 255, 0, 31, 255, 0, 255, 31, 0, 255, 224, 0, 153, 255, 0, 0, 0, 255, 255, 71, 0, 0, 235, 255, 0, 173, 255,\n    31, 0, 255, 11, 200, 200, 255, 82, 0, 0, 255, 245, 0, 61, 255, 0, 255, 112, 0, 255, 133, 255, 0, 0, 255, 163,\n    0, 255, 102, 0, 194, 255, 0, 0, 143, 255, 51, 255, 0, 0, 82, 255, 0, 255, 41, 0, 255, 173, 10, 0, 255, 173, 255,\n    0, 0, 255, 153, 255, 92, 0, 255, 0, 255, 255, 0, 245, 255, 0, 102, 255, 173, 0, 255, 0, 20, 255, 184, 184, 0,\n    31, 255, 0, 255, 61, 0, 71, 255, 255, 0, 204, 0, 255, 194, 0, 255, 82, 0, 10, 255, 0, 112, 255, 51, 0, 255, 0,\n    194, 255, 0, 122, 255, 0, 255, 163, 255, 153, 0, 0, 255, 10, 255, 112, 0, 143, 255, 0, 82, 0, 255, 163, 255,\n    0, 255, 235, 0, 8, 184, 170, 133, 0, 255, 0, 255, 92, 184, 0, 255, 255, 0, 31, 0, 184, 255, 0, 214, 255, 255,\n    0, 112, 92, 255, 0, 0, 224, 255, 112, 224, 255, 70, 184, 160, 163, 0, 255, 153, 0, 255, 71, 255, 0, 255, 0,\n    163, 255, 204, 0, 255, 0, 143, 0, 255, 235, 133, 255, 0, 255, 0, 235, 245, 0, 255, 255, 0, 122, 255, 245, 0,\n    10, 190, 212, 214, 255, 0, 0, 204, 255, 20, 0, 255, 255, 255, 0, 0, 153, 255, 0, 41, 255, 0, 255, 204, 41, 0,\n    255, 41, 255, 0, 173, 0, 255, 0, 245, 255, 71, 0, 255, 122, 0, 255, 0, 255, 184, 0, 92, 255, 184, 255, 0, 0,\n    133, 255, 255, 214, 0, 25, 194, 194, 102, 255, 0, 92, 0, 255]\n\ncityspallete = [\n    128, 64, 128,\n    244, 35, 232,\n    70, 70, 70,\n    102, 102, 156,\n    190, 153, 153,\n    153, 153, 153,\n    250, 170, 30,\n    220, 220, 0,\n    107, 142, 35,\n    152, 251, 152,\n    0, 130, 180,\n    220, 20, 60,\n    255, 0, 0,\n    0, 0, 142,\n    0, 0, 70,\n    0, 60, 100,\n    0, 80, 100,\n    0, 0, 230,\n    119, 11, 32,\n]\n'"
