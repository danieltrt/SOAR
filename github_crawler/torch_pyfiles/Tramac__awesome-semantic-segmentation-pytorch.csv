file_path,api_count,code
core/__init__.py,0,"b'from . import nn, models, utils, data'"
scripts/demo.py,3,"b'import os\nimport sys\nimport argparse\nimport torch\n\ncur_path = os.path.abspath(os.path.dirname(__file__))\nroot_path = os.path.split(cur_path)[0]\nsys.path.append(root_path)\n\nfrom torchvision import transforms\nfrom PIL import Image\nfrom core.utils.visualize import get_color_pallete\nfrom core.models import get_model\n\nparser = argparse.ArgumentParser(\n    description=\'Predict segmentation result from a given image\')\nparser.add_argument(\'--model\', type=str, default=\'fcn32s_vgg16_voc\',\n                    help=\'model name (default: fcn32_vgg16)\')\nparser.add_argument(\'--dataset\', type=str, default=\'pascal_aug\', choices=[\'pascal_voc/pascal_aug/ade20k/citys\'],\n                    help=\'dataset name (default: pascal_voc)\')\nparser.add_argument(\'--save-folder\', default=\'~/.torch/models\',\n                    help=\'Directory for saving checkpoint models\')\nparser.add_argument(\'--input-pic\', type=str, default=\'../datasets/voc/VOC2012/JPEGImages/2007_000032.jpg\',\n                    help=\'path to the input picture\')\nparser.add_argument(\'--outdir\', default=\'./eval\', type=str,\n                    help=\'path to save the predict result\')\nargs = parser.parse_args()\n\n\ndef demo(config):\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    # output folder\n    if not os.path.exists(config.outdir):\n        os.makedirs(config.outdir)\n\n    # image transform\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n    ])\n    image = Image.open(config.input_pic).convert(\'RGB\')\n    images = transform(image).unsqueeze(0).to(device)\n\n    model = get_model(args.model, pretrained=True, root=args.save_folder).to(device)\n    print(\'Finished loading model!\')\n\n    model.eval()\n    with torch.no_grad():\n        output = model(images)\n\n    pred = torch.argmax(output[0], 1).squeeze(0).cpu().data.numpy()\n    mask = get_color_pallete(pred, args.dataset)\n    outname = os.path.splitext(os.path.split(args.input_pic)[-1])[0] + \'.png\'\n    mask.save(os.path.join(args.outdir, outname))\n\n\nif __name__ == \'__main__\':\n    demo(args)\n'"
scripts/eval.py,10,"b'from __future__ import print_function\n\nimport os\nimport sys\n\ncur_path = os.path.abspath(os.path.dirname(__file__))\nroot_path = os.path.split(cur_path)[0]\nsys.path.append(root_path)\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport torch.backends.cudnn as cudnn\n\nfrom torchvision import transforms\nfrom core.data.dataloader import get_segmentation_dataset\nfrom core.models.model_zoo import get_segmentation_model\nfrom core.utils.score import SegmentationMetric\nfrom core.utils.visualize import get_color_pallete\nfrom core.utils.logger import setup_logger\nfrom core.utils.distributed import synchronize, get_rank, make_data_sampler, make_batch_data_sampler\n\nfrom train import parse_args\n\n\nclass Evaluator(object):\n    def __init__(self, args):\n        self.args = args\n        self.device = torch.device(args.device)\n\n        # image transform\n        input_transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize([.485, .456, .406], [.229, .224, .225]),\n        ])\n\n        # dataset and dataloader\n        val_dataset = get_segmentation_dataset(args.dataset, split=\'val\', mode=\'testval\', transform=input_transform)\n        val_sampler = make_data_sampler(val_dataset, False, args.distributed)\n        val_batch_sampler = make_batch_data_sampler(val_sampler, images_per_batch=1)\n        self.val_loader = data.DataLoader(dataset=val_dataset,\n                                          batch_sampler=val_batch_sampler,\n                                          num_workers=args.workers,\n                                          pin_memory=True)\n\n        # create network\n        BatchNorm2d = nn.SyncBatchNorm if args.distributed else nn.BatchNorm2d\n        self.model = get_segmentation_model(model=args.model, dataset=args.dataset, backbone=args.backbone,\n                                            aux=args.aux, pretrained=True, pretrained_base=False,\n                                            local_rank=args.local_rank,\n                                            norm_layer=BatchNorm2d).to(self.device)\n        if args.distributed:\n            self.model = nn.parallel.DistributedDataParallel(self.model,\n                device_ids=[args.local_rank], output_device=args.local_rank)\n        self.model.to(self.device)\n\n        self.metric = SegmentationMetric(val_dataset.num_class)\n\n    def eval(self):\n        self.metric.reset()\n        self.model.eval()\n        if self.args.distributed:\n            model = self.model.module\n        else:\n            model = self.model\n        logger.info(""Start validation, Total sample: {:d}"".format(len(self.val_loader)))\n        for i, (image, target, filename) in enumerate(self.val_loader):\n            image = image.to(self.device)\n            target = target.to(self.device)\n\n            with torch.no_grad():\n                outputs = model(image)\n            self.metric.update(outputs[0], target)\n            pixAcc, mIoU = self.metric.get()\n            logger.info(""Sample: {:d}, validation pixAcc: {:.3f}, mIoU: {:.3f}"".format(\n                i + 1, pixAcc * 100, mIoU * 100))\n\n            if self.args.save_pred:\n                pred = torch.argmax(outputs[0], 1)\n                pred = pred.cpu().data.numpy()\n\n                predict = pred.squeeze(0)\n                mask = get_color_pallete(predict, self.args.dataset)\n                mask.save(os.path.join(outdir, os.path.splitext(filename[0])[0] + \'.png\'))\n        synchronize()\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    num_gpus = int(os.environ[""WORLD_SIZE""]) if ""WORLD_SIZE"" in os.environ else 1\n    args.distributed = num_gpus > 1\n    if not args.no_cuda and torch.cuda.is_available():\n        cudnn.benchmark = True\n        args.device = ""cuda""\n    else:\n        args.distributed = False\n        args.device = ""cpu""\n    if args.distributed:\n        torch.cuda.set_device(args.local_rank)\n        torch.distributed.init_process_group(backend=""nccl"", init_method=""env://"")\n        synchronize()\n\n    # TODO: optim code\n    args.save_pred = True\n    if args.save_pred:\n        outdir = \'../runs/pred_pic/{}_{}_{}\'.format(args.model, args.backbone, args.dataset)\n        if not os.path.exists(outdir):\n            os.makedirs(outdir)\n\n    logger = setup_logger(""semantic_segmentation"", args.log_dir, get_rank(),\n                          filename=\'{}_{}_{}_log.txt\'.format(args.model, args.backbone, args.dataset), mode=\'a+\')\n\n    evaluator = Evaluator(args)\n    evaluator.eval()\n    torch.cuda.empty_cache()\n'"
scripts/train.py,13,"b'import argparse\nimport time\nimport datetime\nimport os\nimport shutil\nimport sys\n\ncur_path = os.path.abspath(os.path.dirname(__file__))\nroot_path = os.path.split(cur_path)[0]\nsys.path.append(root_path)\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport torch.backends.cudnn as cudnn\n\nfrom torchvision import transforms\nfrom core.data.dataloader import get_segmentation_dataset\nfrom core.models.model_zoo import get_segmentation_model\nfrom core.utils.loss import get_segmentation_loss\nfrom core.utils.distributed import *\nfrom core.utils.logger import setup_logger\nfrom core.utils.lr_scheduler import WarmupPolyLR\nfrom core.utils.score import SegmentationMetric\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'Semantic Segmentation Training With Pytorch\')\n    # model and dataset\n    parser.add_argument(\'--model\', type=str, default=\'fcn\',\n                        choices=[\'fcn32s\', \'fcn16s\', \'fcn8s\',\n                                 \'fcn\', \'psp\', \'deeplabv3\', \'deeplabv3_plus\',\n                                 \'danet\', \'denseaspp\', \'bisenet\',\n                                 \'encnet\', \'dunet\', \'icnet\',\n                                 \'enet\', \'ocnet\', \'ccnet\', \'psanet\',\n                                 \'cgnet\', \'espnet\', \'lednet\', \'dfanet\'],\n                        help=\'model name (default: fcn32s)\')\n    parser.add_argument(\'--backbone\', type=str, default=\'resnet50\',\n                        choices=[\'vgg16\', \'resnet18\', \'resnet50\',\n                                 \'resnet101\', \'resnet152\', \'densenet121\',\n                                 \'densenet161\', \'densenet169\', \'densenet201\'],\n                        help=\'backbone name (default: vgg16)\')\n    parser.add_argument(\'--dataset\', type=str, default=\'pascal_voc\',\n                        choices=[\'pascal_voc\', \'pascal_aug\', \'ade20k\',\n                                 \'citys\', \'sbu\'],\n                        help=\'dataset name (default: pascal_voc)\')\n    parser.add_argument(\'--base-size\', type=int, default=520,\n                        help=\'base image size\')\n    parser.add_argument(\'--crop-size\', type=int, default=480,\n                        help=\'crop image size\')\n    parser.add_argument(\'--workers\', \'-j\', type=int, default=4,\n                        metavar=\'N\', help=\'dataloader threads\')\n    # training hyper params\n    parser.add_argument(\'--jpu\', action=\'store_true\', default=False,\n                        help=\'JPU\')\n    parser.add_argument(\'--use-ohem\', type=bool, default=False,\n                        help=\'OHEM Loss for cityscapes dataset\')\n    parser.add_argument(\'--aux\', action=\'store_true\', default=False,\n                        help=\'Auxiliary loss\')\n    parser.add_argument(\'--aux-weight\', type=float, default=0.4,\n                        help=\'auxiliary loss weight\')\n    parser.add_argument(\'--batch-size\', type=int, default=4, metavar=\'N\',\n                        help=\'input batch size for training (default: 8)\')\n    parser.add_argument(\'--start_epoch\', type=int, default=0,\n                        metavar=\'N\', help=\'start epochs (default:0)\')\n    parser.add_argument(\'--epochs\', type=int, default=50, metavar=\'N\',\n                        help=\'number of epochs to train (default: 50)\')\n    parser.add_argument(\'--lr\', type=float, default=1e-4, metavar=\'LR\',\n                        help=\'learning rate (default: 1e-4)\')\n    parser.add_argument(\'--momentum\', type=float, default=0.9, metavar=\'M\',\n                        help=\'momentum (default: 0.9)\')\n    parser.add_argument(\'--weight-decay\', type=float, default=1e-4, metavar=\'M\',\n                        help=\'w-decay (default: 5e-4)\')\n    parser.add_argument(\'--warmup-iters\', type=int, default=0,\n                        help=\'warmup iters\')\n    parser.add_argument(\'--warmup-factor\', type=float, default=1.0 / 3,\n                        help=\'lr = warmup_factor * lr\')\n    parser.add_argument(\'--warmup-method\', type=str, default=\'linear\',\n                        help=\'method of warmup\')\n    # cuda setting\n    parser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                        help=\'disables CUDA training\')\n    parser.add_argument(\'--local_rank\', type=int, default=0)\n    # checkpoint and log\n    parser.add_argument(\'--resume\', type=str, default=None,\n                        help=\'put the path to resuming file if needed\')\n    parser.add_argument(\'--save-dir\', default=\'~/.torch/models\',\n                        help=\'Directory for saving checkpoint models\')\n    parser.add_argument(\'--save-epoch\', type=int, default=10,\n                        help=\'save model every checkpoint-epoch\')\n    parser.add_argument(\'--log-dir\', default=\'../runs/logs/\',\n                        help=\'Directory for saving checkpoint models\')\n    parser.add_argument(\'--log-iter\', type=int, default=10,\n                        help=\'print log every log-iter\')\n    # evaluation only\n    parser.add_argument(\'--val-epoch\', type=int, default=1,\n                        help=\'run validation every val-epoch\')\n    parser.add_argument(\'--skip-val\', action=\'store_true\', default=False,\n                        help=\'skip validation during training\')\n    args = parser.parse_args()\n\n    # default settings for epochs, batch_size and lr\n    if args.epochs is None:\n        epoches = {\n            \'coco\': 30,\n            \'pascal_aug\': 80,\n            \'pascal_voc\': 50,\n            \'pcontext\': 80,\n            \'ade20k\': 160,\n            \'citys\': 120,\n            \'sbu\': 160,\n        }\n        args.epochs = epoches[args.dataset.lower()]\n    if args.lr is None:\n        lrs = {\n            \'coco\': 0.004,\n            \'pascal_aug\': 0.001,\n            \'pascal_voc\': 0.0001,\n            \'pcontext\': 0.001,\n            \'ade20k\': 0.01,\n            \'citys\': 0.01,\n            \'sbu\': 0.001,\n        }\n        args.lr = lrs[args.dataset.lower()] / 8 * args.batch_size\n    return args\n\n\nclass Trainer(object):\n    def __init__(self, args):\n        self.args = args\n        self.device = torch.device(args.device)\n\n        # image transform\n        input_transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize([.485, .456, .406], [.229, .224, .225]),\n        ])\n        # dataset and dataloader\n        data_kwargs = {\'transform\': input_transform, \'base_size\': args.base_size, \'crop_size\': args.crop_size}\n        train_dataset = get_segmentation_dataset(args.dataset, split=\'train\', mode=\'train\', **data_kwargs)\n        val_dataset = get_segmentation_dataset(args.dataset, split=\'val\', mode=\'val\', **data_kwargs)\n        args.iters_per_epoch = len(train_dataset) // (args.num_gpus * args.batch_size)\n        args.max_iters = args.epochs * args.iters_per_epoch\n\n        train_sampler = make_data_sampler(train_dataset, shuffle=True, distributed=args.distributed)\n        train_batch_sampler = make_batch_data_sampler(train_sampler, args.batch_size, args.max_iters)\n        val_sampler = make_data_sampler(val_dataset, False, args.distributed)\n        val_batch_sampler = make_batch_data_sampler(val_sampler, args.batch_size)\n\n        self.train_loader = data.DataLoader(dataset=train_dataset,\n                                            batch_sampler=train_batch_sampler,\n                                            num_workers=args.workers,\n                                            pin_memory=True)\n        self.val_loader = data.DataLoader(dataset=val_dataset,\n                                          batch_sampler=val_batch_sampler,\n                                          num_workers=args.workers,\n                                          pin_memory=True)\n\n        # create network\n        BatchNorm2d = nn.SyncBatchNorm if args.distributed else nn.BatchNorm2d\n        self.model = get_segmentation_model(model=args.model, dataset=args.dataset, backbone=args.backbone,\n                                            aux=args.aux, jpu=args.jpu, norm_layer=BatchNorm2d).to(self.device)\n\n        # resume checkpoint if needed\n        if args.resume:\n            if os.path.isfile(args.resume):\n                name, ext = os.path.splitext(args.resume)\n                assert ext == \'.pkl\' or \'.pth\', \'Sorry only .pth and .pkl files supported.\'\n                print(\'Resuming training, loading {}...\'.format(args.resume))\n                self.model.load_state_dict(torch.load(args.resume, map_location=lambda storage, loc: storage))\n\n        # create criterion\n        self.criterion = get_segmentation_loss(args.model, use_ohem=args.use_ohem, aux=args.aux,\n                                               aux_weight=args.aux_weight, ignore_index=-1).to(self.device)\n\n        # optimizer, for model just includes pretrained, head and auxlayer\n        params_list = list()\n        if hasattr(self.model, \'pretrained\'):\n            params_list.append({\'params\': self.model.pretrained.parameters(), \'lr\': args.lr})\n        if hasattr(self.model, \'exclusive\'):\n            for module in self.model.exclusive:\n                params_list.append({\'params\': getattr(self.model, module).parameters(), \'lr\': args.lr * 10})\n        self.optimizer = torch.optim.SGD(params_list,\n                                         lr=args.lr,\n                                         momentum=args.momentum,\n                                         weight_decay=args.weight_decay)\n\n        # lr scheduling\n        self.lr_scheduler = WarmupPolyLR(self.optimizer,\n                                         max_iters=args.max_iters,\n                                         power=0.9,\n                                         warmup_factor=args.warmup_factor,\n                                         warmup_iters=args.warmup_iters,\n                                         warmup_method=args.warmup_method)\n\n        if args.distributed:\n            self.model = nn.parallel.DistributedDataParallel(self.model, device_ids=[args.local_rank],\n                                                             output_device=args.local_rank)\n\n        # evaluation metrics\n        self.metric = SegmentationMetric(train_dataset.num_class)\n\n        self.best_pred = 0.0\n\n    def train(self):\n        save_to_disk = get_rank() == 0\n        epochs, max_iters = self.args.epochs, self.args.max_iters\n        log_per_iters, val_per_iters = self.args.log_iter, self.args.val_epoch * self.args.iters_per_epoch\n        save_per_iters = self.args.save_epoch * self.args.iters_per_epoch\n        start_time = time.time()\n        logger.info(\'Start training, Total Epochs: {:d} = Total Iterations {:d}\'.format(epochs, max_iters))\n\n        self.model.train()\n        for iteration, (images, targets, _) in enumerate(self.train_loader):\n            iteration = iteration + 1\n            self.lr_scheduler.step()\n\n            images = images.to(self.device)\n            targets = targets.to(self.device)\n\n            outputs = self.model(images)\n            loss_dict = self.criterion(outputs, targets)\n\n            losses = sum(loss for loss in loss_dict.values())\n\n            # reduce losses over all GPUs for logging purposes\n            loss_dict_reduced = reduce_loss_dict(loss_dict)\n            losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n\n            self.optimizer.zero_grad()\n            losses.backward()\n            self.optimizer.step()\n\n            eta_seconds = ((time.time() - start_time) / iteration) * (max_iters - iteration)\n            eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n\n            if iteration % log_per_iters == 0 and save_to_disk:\n                logger.info(\n                    ""Iters: {:d}/{:d} || Lr: {:.6f} || Loss: {:.4f} || Cost Time: {} || Estimated Time: {}"".format(\n                        iteration, max_iters, self.optimizer.param_groups[0][\'lr\'], losses_reduced.item(),\n                        str(datetime.timedelta(seconds=int(time.time() - start_time))), eta_string))\n\n            if iteration % save_per_iters == 0 and save_to_disk:\n                save_checkpoint(self.model, self.args, is_best=False)\n\n            if not self.args.skip_val and iteration % val_per_iters == 0:\n                self.validation()\n                self.model.train()\n\n        save_checkpoint(self.model, self.args, is_best=False)\n        total_training_time = time.time() - start_time\n        total_training_str = str(datetime.timedelta(seconds=total_training_time))\n        logger.info(\n            ""Total training time: {} ({:.4f}s / it)"".format(\n                total_training_str, total_training_time / max_iters))\n\n    def validation(self):\n        # total_inter, total_union, total_correct, total_label = 0, 0, 0, 0\n        is_best = False\n        self.metric.reset()\n        if self.args.distributed:\n            model = self.model.module\n        else:\n            model = self.model\n        torch.cuda.empty_cache()  # TODO check if it helps\n        model.eval()\n        for i, (image, target, filename) in enumerate(self.val_loader):\n            image = image.to(self.device)\n            target = target.to(self.device)\n\n            with torch.no_grad():\n                outputs = model(image)\n            self.metric.update(outputs[0], target)\n            pixAcc, mIoU = self.metric.get()\n            logger.info(""Sample: {:d}, Validation pixAcc: {:.3f}, mIoU: {:.3f}"".format(i + 1, pixAcc, mIoU))\n\n        new_pred = (pixAcc + mIoU) / 2\n        if new_pred > self.best_pred:\n            is_best = True\n            self.best_pred = new_pred\n        save_checkpoint(self.model, self.args, is_best)\n        synchronize()\n\n\ndef save_checkpoint(model, args, is_best=False):\n    """"""Save Checkpoint""""""\n    directory = os.path.expanduser(args.save_dir)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n    filename = \'{}_{}_{}.pth\'.format(args.model, args.backbone, args.dataset)\n    filename = os.path.join(directory, filename)\n\n    if args.distributed:\n        model = model.module\n    torch.save(model.state_dict(), filename)\n    if is_best:\n        best_filename = \'{}_{}_{}_best_model.pth\'.format(args.model, args.backbone, args.dataset)\n        best_filename = os.path.join(directory, best_filename)\n        shutil.copyfile(filename, best_filename)\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n\n    # reference maskrcnn-benchmark\n    num_gpus = int(os.environ[""WORLD_SIZE""]) if ""WORLD_SIZE"" in os.environ else 1\n    args.num_gpus = num_gpus\n    args.distributed = num_gpus > 1\n    if not args.no_cuda and torch.cuda.is_available():\n        cudnn.benchmark = True\n        args.device = ""cuda""\n    else:\n        args.distributed = False\n        args.device = ""cpu""\n    if args.distributed:\n        torch.cuda.set_device(args.local_rank)\n        torch.distributed.init_process_group(backend=""nccl"", init_method=""env://"")\n        synchronize()\n    args.lr = args.lr * num_gpus\n\n    logger = setup_logger(""semantic_segmentation"", args.log_dir, get_rank(), filename=\'{}_{}_{}_log.txt\'.format(\n        args.model, args.backbone, args.dataset))\n    logger.info(""Using {} GPUs"".format(num_gpus))\n    logger.info(args)\n\n    trainer = Trainer(args)\n    trainer.train()\n    torch.cuda.empty_cache()\n'"
tests/test_model.py,8,"b'""""""Model overfitting test""""""\nimport argparse\nimport time\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport numpy as np\n\nfrom torchvision import transforms\nfrom core.models.model_zoo import get_segmentation_model\nfrom core.utils.loss import MixSoftmaxCrossEntropyLoss, EncNetLoss, ICNetLoss\nfrom core.utils.lr_scheduler import LRScheduler\nfrom core.utils.score import hist_info, compute_score\nfrom core.utils.visualize import get_color_pallete\nfrom PIL import Image\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'Semantic Segmentation Overfitting Test\')\n    # model\n    parser.add_argument(\'--model\', type=str, default=\'ocnet\',\n                        choices=[\'fcn32s/fcn16s/fcn8s/fcn/psp/deeplabv3/danet/denseaspp/bisenet/encnet/dunet/icnet/enet/ocnet\'],\n                        help=\'model name (default: fcn32s)\')\n    parser.add_argument(\'--backbone\', type=str, default=\'resnet50\',\n                        choices=[\'vgg16/resnet18/resnet50/resnet101/resnet152/densenet121/161/169/201\'],\n                        help=\'backbone name (default: vgg16)\')\n    parser.add_argument(\'--dataset\', type=str, default=\'pascal_voc\',\n                        choices=[\'pascal_voc/pascal_aug/ade20k/citys/sbu\'],\n                        help=\'dataset name (default: pascal_voc)\')\n    parser.add_argument(\'--epochs\', type=int, default=100, metavar=\'N\',\n                        help=\'number of epochs to train (default: 60)\')\n    parser.add_argument(\'--lr\', type=float, default=1e-3, metavar=\'LR\',\n                        help=\'learning rate (default: 1e-3)\')\n    parser.add_argument(\'--momentum\', type=float, default=0.9, metavar=\'M\',\n                        help=\'momentum (default: 0.9)\')\n    parser.add_argument(\'--weight-decay\', type=float, default=1e-4, metavar=\'M\',\n                        help=\'w-decay (default: 5e-4)\')\n    args = parser.parse_args()\n    device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n    cudnn.benchmark = True\n    args.device = device\n    print(args)\n    return args\n\n\nclass VOCSegmentation(object):\n    def __init__(self):\n        super(VOCSegmentation, self).__init__()\n        self.img = Image.open(\'test_img.jpg\').convert(\'RGB\')\n        self.mask = Image.open(\'test_mask.png\')\n\n        self.img = self.img.resize((504, 368), Image.BILINEAR)\n        self.mask = self.mask.resize((504, 368), Image.NEAREST)\n\n    def get(self):\n        img, mask = self._img_transform(self.img), self._mask_transform(self.mask)\n        return img, mask\n\n    def _img_transform(self, img):\n        input_transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize([.485, .456, .406], [.229, .224, .225])])\n        img = input_transform(img)\n        img = img.unsqueeze(0)\n\n        # For adaptive pooling\n        # img = torch.cat([img, img], dim=0)\n        return img\n\n    def _mask_transform(self, mask):\n        target = np.array(mask).astype(\'int32\')\n        target[target == 255] = -1\n        target = torch.from_numpy(target).long()\n        target = target.unsqueeze(0)\n\n        # For adaptive pooling\n        # target = torch.cat([target, target], dim=0)\n        return target\n\n\nclass Trainer(object):\n    def __init__(self, args):\n        self.args = args\n\n        self.img, self.target = VOCSegmentation().get()\n\n        self.model = get_segmentation_model(model=args.model, dataset=args.dataset, backbone=args.backbone,\n                                            aux=False, norm_layer=nn.BatchNorm2d).to(args.device)\n\n        self.criterion = MixSoftmaxCrossEntropyLoss(False, 0., ignore_label=-1).to(args.device)\n\n        # for EncNet\n        # self.criterion = EncNetLoss(nclass=21, ignore_label=-1).to(args.device)\n        # for ICNet\n        # self.criterion = ICNetLoss(nclass=21, ignore_index=-1).to(args.device)\n\n        self.optimizer = torch.optim.Adam(self.model.parameters(),\n                                          lr=args.lr,\n                                          weight_decay=args.weight_decay)\n        self.lr_scheduler = LRScheduler(mode=\'poly\', base_lr=args.lr, nepochs=args.epochs,\n                                        iters_per_epoch=1, power=0.9)\n\n    def train(self):\n        self.model.train()\n        start_time = time.time()\n        for epoch in range(self.args.epochs):\n            cur_lr = self.lr_scheduler(epoch)\n            for param_group in self.optimizer.param_groups:\n                param_group[\'lr\'] = cur_lr\n\n            images = self.img.to(self.args.device)\n            targets = self.target.to(self.args.device)\n\n            outputs = self.model(images)\n            loss = self.criterion(outputs, targets)\n\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n\n            pred = torch.argmax(outputs[0], 1).cpu().data.numpy()\n            mask = get_color_pallete(pred.squeeze(0), self.args.dataset)\n            save_pred(self.args, epoch, mask)\n            hist, labeled, correct = hist_info(pred, targets.numpy(), 21)\n            _, mIoU, _, pixAcc = compute_score(hist, correct, labeled)\n\n            print(\'Epoch: [%2d/%2d] || Time: %4.4f sec || lr: %.8f || Loss: %.4f || pixAcc: %.3f || mIoU: %.3f\' % (\n                epoch, self.args.epochs, time.time() - start_time, cur_lr, loss.item(), pixAcc, mIoU))\n\n\ndef save_pred(args, epoch, mask):\n    directory = ""runs/%s/"" % (args.model)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n    filename = directory + \'{}_epoch_{}.png\'.format(args.model, epoch + 1)\n    mask.save(filename)\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    trainer = Trainer(args)\n    print(\'Test model: \', args.model)\n    trainer.train()\n'"
tests/test_module.py,8,"b""import core\nimport torch\nimport numpy as np\n\nfrom torch.autograd import Variable\n\nEPS = 1e-3\nATOL = 1e-3\n\n\ndef _assert_tensor_close(a, b, atol=ATOL, rtol=EPS):\n    npa, npb = a.cpu().numpy(), b.cpu().numpy()\n    assert np.allclose(npa, npb, rtol=rtol, atol=atol), \\\n        'Tensor close check failed\\n{}\\n{}\\nadiff={}, rdiff={}'.format(\n            a, b, np.abs(npa - npb).max(), np.abs((npa - npb) / np.fmax(npa, 1e-5)).max())\n\n\ndef testSyncBN():\n    def _check_batchnorm_result(bn1, bn2, input, is_train, cuda=False):\n        def _find_bn(module):\n            for m in module.modules():\n                if isinstance(m, (torch.nn.BatchNorm1d, torch.nn.BatchNorm2d,\n                                  core.nn.SyncBatchNorm)):\n                    return m\n\n        def _syncParameters(bn1, bn2):\n            bn1.reset_parameters()\n            bn2.reset_parameters()\n            if bn1.affine and bn2.affine:\n                bn2.weight.data.copy_(bn1.weight.data)\n                bn2.bias.data.copy_(bn1.bias.data)\n                bn2.running_mean.copy_(bn1.running_mean)\n                bn2.running_var.copy_(bn1.running_var)\n\n        bn1.train(mode=is_train)\n        bn2.train(mode=is_train)\n\n        if cuda:\n            input = input.cuda()\n        # using the same values for gamma and beta\n        _syncParameters(_find_bn(bn1), _find_bn(bn2))\n\n        input1 = Variable(input.clone().detach(), requires_grad=True)\n        input2 = Variable(input.clone().detach(), requires_grad=True)\n        if is_train:\n            bn1.train()\n            bn2.train()\n            output1 = bn1(input1)\n            output2 = bn2(input2)\n        else:\n            bn1.eval()\n            bn2.eval()\n            with torch.no_grad():\n                output1 = bn1(input1)\n                output2 = bn2(input2)\n        # assert forwarding\n        # _assert_tensor_close(input1.data, input2.data)\n        _assert_tensor_close(output1.data, output2.data)\n        if not is_train:\n            return\n        (output1 ** 2).sum().backward()\n        (output2 ** 2).sum().backward()\n        _assert_tensor_close(_find_bn(bn1).bias.grad.data, _find_bn(bn2).bias.grad.data)\n        _assert_tensor_close(_find_bn(bn1).weight.grad.data, _find_bn(bn2).weight.grad.data)\n        _assert_tensor_close(input1.grad.data, input2.grad.data)\n        _assert_tensor_close(_find_bn(bn1).running_mean, _find_bn(bn2).running_mean)\n        # _assert_tensor_close(_find_bn(bn1).running_var, _find_bn(bn2).running_var)\n\n    bn = torch.nn.BatchNorm2d(10).cuda().double()\n    sync_bn = core.nn.SyncBatchNorm(10, inplace=True, sync=True).cuda().double()\n    sync_bn = torch.nn.DataParallel(sync_bn).cuda()\n    # check with unsync version\n    # _check_batchnorm_result(bn, sync_bn, torch.rand(2, 1, 2, 2).double(), True, cuda=True)\n    for i in range(10):\n        print(i)\n        _check_batchnorm_result(bn, sync_bn, torch.rand(16, 10, 16, 16).double(), True, cuda=True)\n        # _check_batchnorm_result(bn, sync_bn, torch.rand(16, 10, 16, 16).double(), False, cuda=True)\n\n\nif __name__ == '__main__':\n    import nose\n\n    nose.runmodule()\n"""
core/data/__init__.py,0,b''
core/models/__init__.py,0,"b'""""""Model Zoo""""""\nfrom .model_zoo import get_model, get_model_list'"
core/models/bisenet.py,6,"b'""""""Bilateral Segmentation Network""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom core.models.base_models.resnet import resnet18\nfrom core.nn import _ConvBNReLU\n\n__all__ = [\'BiSeNet\', \'get_bisenet\', \'get_bisenet_resnet18_citys\']\n\n\nclass BiSeNet(nn.Module):\n    def __init__(self, nclass, backbone=\'resnet18\', aux=False, jpu=False, pretrained_base=True, **kwargs):\n        super(BiSeNet, self).__init__()\n        self.aux = aux\n        self.spatial_path = SpatialPath(3, 128, **kwargs)\n        self.context_path = ContextPath(backbone, pretrained_base, **kwargs)\n        self.ffm = FeatureFusion(256, 256, 4, **kwargs)\n        self.head = _BiSeHead(256, 64, nclass, **kwargs)\n        if aux:\n            self.auxlayer1 = _BiSeHead(128, 256, nclass, **kwargs)\n            self.auxlayer2 = _BiSeHead(128, 256, nclass, **kwargs)\n\n        self.__setattr__(\'exclusive\',\n                         [\'spatial_path\', \'context_path\', \'ffm\', \'head\', \'auxlayer1\', \'auxlayer2\'] if aux else [\n                             \'spatial_path\', \'context_path\', \'ffm\', \'head\'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        spatial_out = self.spatial_path(x)\n        context_out = self.context_path(x)\n        fusion_out = self.ffm(spatial_out, context_out[-1])\n        outputs = []\n        x = self.head(fusion_out)\n        x = F.interpolate(x, size, mode=\'bilinear\', align_corners=True)\n        outputs.append(x)\n\n        if self.aux:\n            auxout1 = self.auxlayer1(context_out[0])\n            auxout1 = F.interpolate(auxout1, size, mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout1)\n            auxout2 = self.auxlayer2(context_out[1])\n            auxout2 = F.interpolate(auxout2, size, mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout2)\n        return tuple(outputs)\n\n\nclass _BiSeHead(nn.Module):\n    def __init__(self, in_channels, inter_channels, nclass, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(_BiSeHead, self).__init__()\n        self.block = nn.Sequential(\n            _ConvBNReLU(in_channels, inter_channels, 3, 1, 1, norm_layer=norm_layer),\n            nn.Dropout(0.1),\n            nn.Conv2d(inter_channels, nclass, 1)\n        )\n\n    def forward(self, x):\n        x = self.block(x)\n        return x\n\n\nclass SpatialPath(nn.Module):\n    """"""Spatial path""""""\n\n    def __init__(self, in_channels, out_channels, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(SpatialPath, self).__init__()\n        inter_channels = 64\n        self.conv7x7 = _ConvBNReLU(in_channels, inter_channels, 7, 2, 3, norm_layer=norm_layer)\n        self.conv3x3_1 = _ConvBNReLU(inter_channels, inter_channels, 3, 2, 1, norm_layer=norm_layer)\n        self.conv3x3_2 = _ConvBNReLU(inter_channels, inter_channels, 3, 2, 1, norm_layer=norm_layer)\n        self.conv1x1 = _ConvBNReLU(inter_channels, out_channels, 1, 1, 0, norm_layer=norm_layer)\n\n    def forward(self, x):\n        x = self.conv7x7(x)\n        x = self.conv3x3_1(x)\n        x = self.conv3x3_2(x)\n        x = self.conv1x1(x)\n\n        return x\n\n\nclass _GlobalAvgPooling(nn.Module):\n    def __init__(self, in_channels, out_channels, norm_layer, **kwargs):\n        super(_GlobalAvgPooling, self).__init__()\n        self.gap = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            norm_layer(out_channels),\n            nn.ReLU(True)\n        )\n\n    def forward(self, x):\n        size = x.size()[2:]\n        pool = self.gap(x)\n        out = F.interpolate(pool, size, mode=\'bilinear\', align_corners=True)\n        return out\n\n\nclass AttentionRefinmentModule(nn.Module):\n    def __init__(self, in_channels, out_channels, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(AttentionRefinmentModule, self).__init__()\n        self.conv3x3 = _ConvBNReLU(in_channels, out_channels, 3, 1, 1, norm_layer=norm_layer)\n        self.channel_attention = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            _ConvBNReLU(out_channels, out_channels, 1, 1, 0, norm_layer=norm_layer),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.conv3x3(x)\n        attention = self.channel_attention(x)\n        x = x * attention\n        return x\n\n\nclass ContextPath(nn.Module):\n    def __init__(self, backbone=\'resnet18\', pretrained_base=True, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(ContextPath, self).__init__()\n        if backbone == \'resnet18\':\n            pretrained = resnet18(pretrained=pretrained_base, **kwargs)\n        else:\n            raise RuntimeError(\'unknown backbone: {}\'.format(backbone))\n        self.conv1 = pretrained.conv1\n        self.bn1 = pretrained.bn1\n        self.relu = pretrained.relu\n        self.maxpool = pretrained.maxpool\n        self.layer1 = pretrained.layer1\n        self.layer2 = pretrained.layer2\n        self.layer3 = pretrained.layer3\n        self.layer4 = pretrained.layer4\n\n        inter_channels = 128\n        self.global_context = _GlobalAvgPooling(512, inter_channels, norm_layer)\n\n        self.arms = nn.ModuleList(\n            [AttentionRefinmentModule(512, inter_channels, norm_layer, **kwargs),\n             AttentionRefinmentModule(256, inter_channels, norm_layer, **kwargs)]\n        )\n        self.refines = nn.ModuleList(\n            [_ConvBNReLU(inter_channels, inter_channels, 3, 1, 1, norm_layer=norm_layer),\n             _ConvBNReLU(inter_channels, inter_channels, 3, 1, 1, norm_layer=norm_layer)]\n        )\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)\n\n        context_blocks = []\n        context_blocks.append(x)\n        x = self.layer2(x)\n        context_blocks.append(x)\n        c3 = self.layer3(x)\n        context_blocks.append(c3)\n        c4 = self.layer4(c3)\n        context_blocks.append(c4)\n        context_blocks.reverse()\n\n        global_context = self.global_context(c4)\n        last_feature = global_context\n        context_outputs = []\n        for i, (feature, arm, refine) in enumerate(zip(context_blocks[:2], self.arms, self.refines)):\n            feature = arm(feature)\n            feature += last_feature\n            last_feature = F.interpolate(feature, size=context_blocks[i + 1].size()[2:],\n                                         mode=\'bilinear\', align_corners=True)\n            last_feature = refine(last_feature)\n            context_outputs.append(last_feature)\n\n        return context_outputs\n\n\nclass FeatureFusion(nn.Module):\n    def __init__(self, in_channels, out_channels, reduction=1, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(FeatureFusion, self).__init__()\n        self.conv1x1 = _ConvBNReLU(in_channels, out_channels, 1, 1, 0, norm_layer=norm_layer, **kwargs)\n        self.channel_attention = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            _ConvBNReLU(out_channels, out_channels // reduction, 1, 1, 0, norm_layer=norm_layer),\n            _ConvBNReLU(out_channels // reduction, out_channels, 1, 1, 0, norm_layer=norm_layer),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x1, x2):\n        fusion = torch.cat([x1, x2], dim=1)\n        out = self.conv1x1(fusion)\n        attention = self.channel_attention(out)\n        out = out + out * attention\n        return out\n\n\ndef get_bisenet(dataset=\'citys\', backbone=\'resnet18\', pretrained=False, root=\'~/.torch/models\',\n                pretrained_base=True, **kwargs):\n    acronyms = {\n        \'pascal_voc\': \'pascal_voc\',\n        \'pascal_aug\': \'pascal_aug\',\n        \'ade20k\': \'ade\',\n        \'coco\': \'coco\',\n        \'citys\': \'citys\',\n    }\n    from ..data.dataloader import datasets\n    model = BiSeNet(datasets[dataset].NUM_CLASS, backbone=backbone, pretrained_base=pretrained_base, **kwargs)\n    if pretrained:\n        from .model_store import get_model_file\n        device = torch.device(kwargs[\'local_rank\'])\n        model.load_state_dict(torch.load(get_model_file(\'bisenet_%s_%s\' % (backbone, acronyms[dataset]), root=root),\n                              map_location=device))\n    return model\n\n\ndef get_bisenet_resnet18_citys(**kwargs):\n    return get_bisenet(\'citys\', \'resnet18\', **kwargs)\n\n\nif __name__ == \'__main__\':\n    img = torch.randn(2, 3, 224, 224)\n    model = BiSeNet(19, backbone=\'resnet18\')\n    print(model.exclusive)\n'"
core/models/ccnet.py,6,"b'""""""Criss-Cross Network""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom core.nn import CrissCrossAttention\nfrom .segbase import SegBaseModel\nfrom .fcn import _FCNHead\n\n__all__ = [\'CCNet\', \'get_ccnet\', \'get_ccnet_resnet50_citys\', \'get_ccnet_resnet101_citys\',\n           \'get_ccnet_resnet152_citys\', \'get_ccnet_resnet50_ade\', \'get_ccnet_resnet101_ade\',\n           \'get_ccnet_resnet152_ade\']\n\n\nclass CCNet(SegBaseModel):\n    r""""""CCNet\n\n    Parameters\n    ----------\n    nclass : int\n        Number of categories for the training dataset.\n    backbone : string\n        Pre-trained dilated backbone network type (default:\'resnet50\'; \'resnet50\',\n        \'resnet101\' or \'resnet152\').\n    norm_layer : object\n        Normalization layer used in backbone network (default: :class:`nn.BatchNorm`;\n        for Synchronized Cross-GPU BachNormalization).\n    aux : bool\n        Auxiliary loss.\n\n    Reference:\n        Zilong Huang, et al. ""CCNet: Criss-Cross Attention for Semantic Segmentation.""\n        arXiv preprint arXiv:1811.11721 (2018).\n    """"""\n\n    def __init__(self, nclass, backbone=\'resnet50\', aux=False, pretrained_base=True, **kwargs):\n        super(CCNet, self).__init__(nclass, aux, backbone, pretrained_base=pretrained_base, **kwargs)\n        self.head = _CCHead(nclass, **kwargs)\n        if aux:\n            self.auxlayer = _FCNHead(1024, nclass, **kwargs)\n\n        self.__setattr__(\'exclusive\', [\'head\', \'auxlayer\'] if aux else [\'head\'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        _, _, c3, c4 = self.base_forward(x)\n        outputs = list()\n        x = self.head(c4)\n        x = F.interpolate(x, size, mode=\'bilinear\', align_corners=True)\n        outputs.append(x)\n\n        if self.aux:\n            auxout = self.auxlayer(c3)\n            auxout = F.interpolate(auxout, size, mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout)\n        return tuple(outputs)\n\n\nclass _CCHead(nn.Module):\n    def __init__(self, nclass, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(_CCHead, self).__init__()\n        self.rcca = _RCCAModule(2048, 512, norm_layer, **kwargs)\n        self.out = nn.Conv2d(512, nclass, 1)\n\n    def forward(self, x):\n        x = self.rcca(x)\n        x = self.out(x)\n        return x\n\n\nclass _RCCAModule(nn.Module):\n    def __init__(self, in_channels, out_channels, norm_layer, **kwargs):\n        super(_RCCAModule, self).__init__()\n        inter_channels = in_channels // 4\n        self.conva = nn.Sequential(\n            nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n            norm_layer(inter_channels),\n            nn.ReLU(True))\n        self.cca = CrissCrossAttention(inter_channels)\n        self.convb = nn.Sequential(\n            nn.Conv2d(inter_channels, inter_channels, 3, padding=1, bias=False),\n            norm_layer(inter_channels),\n            nn.ReLU(True))\n\n        self.bottleneck = nn.Sequential(\n            nn.Conv2d(in_channels + inter_channels, out_channels, 3, padding=1, bias=False),\n            norm_layer(out_channels),\n            nn.Dropout2d(0.1))\n\n    def forward(self, x, recurrence=1):\n        out = self.conva(x)\n        for i in range(recurrence):\n            out = self.cca(out)\n        out = self.convb(out)\n        out = torch.cat([x, out], dim=1)\n        out = self.bottleneck(out)\n\n        return out\n\n\ndef get_ccnet(dataset=\'pascal_voc\', backbone=\'resnet50\', pretrained=False, root=\'~/.torch/models\',\n              pretrained_base=True, **kwargs):\n    acronyms = {\n        \'pascal_voc\': \'pascal_voc\',\n        \'pascal_aug\': \'pascal_aug\',\n        \'ade20k\': \'ade\',\n        \'coco\': \'coco\',\n        \'citys\': \'citys\',\n    }\n    from ..data.dataloader import datasets\n    model = CCNet(datasets[dataset].NUM_CLASS, backbone=backbone, pretrained_base=pretrained_base, **kwargs)\n    if pretrained:\n        from .model_store import get_model_file\n        device = torch.device(kwargs[\'local_rank\'])\n        model.load_state_dict(torch.load(get_model_file(\'ccnet_%s_%s\' % (backbone, acronyms[dataset]), root=root),\n                              map_location=device))\n    return model\n\n\ndef get_ccnet_resnet50_citys(**kwargs):\n    return get_ccnet(\'citys\', \'resnet50\', **kwargs)\n\n\ndef get_ccnet_resnet101_citys(**kwargs):\n    return get_ccnet(\'citys\', \'resnet101\', **kwargs)\n\n\ndef get_ccnet_resnet152_citys(**kwargs):\n    return get_ccnet(\'citys\', \'resnet152\', **kwargs)\n\n\ndef get_ccnet_resnet50_ade(**kwargs):\n    return get_ccnet(\'ade20k\', \'resnet50\', **kwargs)\n\n\ndef get_ccnet_resnet101_ade(**kwargs):\n    return get_ccnet(\'ade20k\', \'resnet101\', **kwargs)\n\n\ndef get_ccnet_resnet152_ade(**kwargs):\n    return get_ccnet(\'ade20k\', \'resnet152\', **kwargs)\n\n\nif __name__ == \'__main__\':\n    model = get_ccnet_resnet50_citys()\n    img = torch.randn(1, 3, 480, 480)\n    outputs = model(img)\n'"
core/models/cgnet.py,9,"b'""""""Context Guided Network for Semantic Segmentation""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom core.nn import _ConvBNPReLU, _BNPReLU\n\n__all__ = [\'CGNet\', \'get_cgnet\', \'get_cgnet_citys\']\n\n\nclass CGNet(nn.Module):\n    r""""""CGNet\n\n    Parameters\n    ----------\n    nclass : int\n        Number of categories for the training dataset.\n    norm_layer : object\n        Normalization layer used in backbone network (default: :class:`nn.BatchNorm`;\n        for Synchronized Cross-GPU BachNormalization).\n    aux : bool\n        Auxiliary loss.\n\n    Reference:\n        Tianyi Wu, et al. ""CGNet: A Light-weight Context Guided Network for Semantic Segmentation.""\n        arXiv preprint arXiv:1811.08201 (2018).\n    """"""\n\n    def __init__(self, nclass, backbone=\'\', aux=False, jpu=False, pretrained_base=True, M=3, N=21, **kwargs):\n        super(CGNet, self).__init__()\n        # stage 1\n        self.stage1_0 = _ConvBNPReLU(3, 32, 3, 2, 1, **kwargs)\n        self.stage1_1 = _ConvBNPReLU(32, 32, 3, 1, 1, **kwargs)\n        self.stage1_2 = _ConvBNPReLU(32, 32, 3, 1, 1, **kwargs)\n\n        self.sample1 = _InputInjection(1)\n        self.sample2 = _InputInjection(2)\n        self.bn_prelu1 = _BNPReLU(32 + 3, **kwargs)\n\n        # stage 2\n        self.stage2_0 = ContextGuidedBlock(32 + 3, 64, dilation=2, reduction=8, down=True, residual=False, **kwargs)\n        self.stage2 = nn.ModuleList()\n        for i in range(0, M - 1):\n            self.stage2.append(ContextGuidedBlock(64, 64, dilation=2, reduction=8, **kwargs))\n        self.bn_prelu2 = _BNPReLU(128 + 3, **kwargs)\n\n        # stage 3\n        self.stage3_0 = ContextGuidedBlock(128 + 3, 128, dilation=4, reduction=16, down=True, residual=False, **kwargs)\n        self.stage3 = nn.ModuleList()\n        for i in range(0, N - 1):\n            self.stage3.append(ContextGuidedBlock(128, 128, dilation=4, reduction=16, **kwargs))\n        self.bn_prelu3 = _BNPReLU(256, **kwargs)\n\n        self.head = nn.Sequential(\n            nn.Dropout2d(0.1, False),\n            nn.Conv2d(256, nclass, 1))\n\n        self.__setattr__(\'exclusive\', [\'stage1_0\', \'stage1_1\', \'stage1_2\', \'sample1\', \'sample2\',\n                                       \'bn_prelu1\', \'stage2_0\', \'stage2\', \'bn_prelu2\', \'stage3_0\',\n                                       \'stage3\', \'bn_prelu3\', \'head\'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        # stage1\n        out0 = self.stage1_0(x)\n        out0 = self.stage1_1(out0)\n        out0 = self.stage1_2(out0)\n\n        inp1 = self.sample1(x)\n        inp2 = self.sample2(x)\n\n        # stage 2\n        out0_cat = self.bn_prelu1(torch.cat([out0, inp1], dim=1))\n        out1_0 = self.stage2_0(out0_cat)\n        for i, layer in enumerate(self.stage2):\n            if i == 0:\n                out1 = layer(out1_0)\n            else:\n                out1 = layer(out1)\n        out1_cat = self.bn_prelu2(torch.cat([out1, out1_0, inp2], dim=1))\n\n        # stage 3\n        out2_0 = self.stage3_0(out1_cat)\n        for i, layer in enumerate(self.stage3):\n            if i == 0:\n                out2 = layer(out2_0)\n            else:\n                out2 = layer(out2)\n        out2_cat = self.bn_prelu3(torch.cat([out2_0, out2], dim=1))\n\n        outputs = []\n        out = self.head(out2_cat)\n        out = F.interpolate(out, size, mode=\'bilinear\', align_corners=True)\n        outputs.append(out)\n        return tuple(outputs)\n\n\nclass _ChannelWiseConv(nn.Module):\n    def __init__(self, in_channels, out_channels, dilation=1, **kwargs):\n        super(_ChannelWiseConv, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, 3, 1, dilation, dilation, groups=in_channels, bias=False)\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\nclass _FGlo(nn.Module):\n    def __init__(self, in_channels, reduction=16, **kwargs):\n        super(_FGlo, self).__init__()\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(in_channels, in_channels // reduction),\n            nn.ReLU(True),\n            nn.Linear(in_channels // reduction, in_channels),\n            nn.Sigmoid())\n\n    def forward(self, x):\n        n, c, _, _ = x.size()\n        out = self.gap(x).view(n, c)\n        out = self.fc(out).view(n, c, 1, 1)\n        return x * out\n\n\nclass _InputInjection(nn.Module):\n    def __init__(self, ratio):\n        super(_InputInjection, self).__init__()\n        self.pool = nn.ModuleList()\n        for i in range(0, ratio):\n            self.pool.append(nn.AvgPool2d(3, 2, 1))\n\n    def forward(self, x):\n        for pool in self.pool:\n            x = pool(x)\n        return x\n\n\nclass _ConcatInjection(nn.Module):\n    def __init__(self, in_channels, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(_ConcatInjection, self).__init__()\n        self.bn = norm_layer(in_channels)\n        self.prelu = nn.PReLU(in_channels)\n\n    def forward(self, x1, x2):\n        out = torch.cat([x1, x2], dim=1)\n        out = self.bn(out)\n        out = self.prelu(out)\n        return out\n\n\nclass ContextGuidedBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, dilation=2, reduction=16, down=False,\n                 residual=True, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(ContextGuidedBlock, self).__init__()\n        inter_channels = out_channels // 2 if not down else out_channels\n        if down:\n            self.conv = _ConvBNPReLU(in_channels, inter_channels, 3, 2, 1, norm_layer=norm_layer, **kwargs)\n            self.reduce = nn.Conv2d(inter_channels * 2, out_channels, 1, bias=False)\n        else:\n            self.conv = _ConvBNPReLU(in_channels, inter_channels, 1, 1, 0, norm_layer=norm_layer, **kwargs)\n        self.f_loc = _ChannelWiseConv(inter_channels, inter_channels, **kwargs)\n        self.f_sur = _ChannelWiseConv(inter_channels, inter_channels, dilation, **kwargs)\n        self.bn = norm_layer(inter_channels * 2)\n        self.prelu = nn.PReLU(inter_channels * 2)\n        self.f_glo = _FGlo(out_channels, reduction, **kwargs)\n        self.down = down\n        self.residual = residual\n\n    def forward(self, x):\n        out = self.conv(x)\n        loc = self.f_loc(out)\n        sur = self.f_sur(out)\n\n        joi_feat = torch.cat([loc, sur], dim=1)\n        joi_feat = self.prelu(self.bn(joi_feat))\n        if self.down:\n            joi_feat = self.reduce(joi_feat)\n\n        out = self.f_glo(joi_feat)\n        if self.residual:\n            out = out + x\n\n        return out\n\n\ndef get_cgnet(dataset=\'citys\', backbone=\'\', pretrained=False, root=\'~/.torch/models\', pretrained_base=True, **kwargs):\n    acronyms = {\n        \'pascal_voc\': \'pascal_voc\',\n        \'pascal_aug\': \'pascal_aug\',\n        \'ade20k\': \'ade\',\n        \'coco\': \'coco\',\n        \'citys\': \'citys\',\n    }\n    from core.data.dataloader import datasets\n    model = CGNet(datasets[dataset].NUM_CLASS, backbone=backbone, pretrained_base=pretrained_base, **kwargs)\n    if pretrained:\n        from .model_store import get_model_file\n        device = torch.device(kwargs[\'local_rank\'])\n        model.load_state_dict(torch.load(get_model_file(\'cgnet_%s\' % (acronyms[dataset]), root=root),\n                              map_location=device))\n    return model\n\n\ndef get_cgnet_citys(**kwargs):\n    return get_cgnet(\'citys\', \'\', **kwargs)\n\n\nif __name__ == \'__main__\':\n    model = get_cgnet_citys()\n    print(model)\n'"
core/models/danet.py,12,"b'""""""Dual Attention Network""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .segbase import SegBaseModel\n\n__all__ = [\'DANet\', \'get_danet\', \'get_danet_resnet50_citys\',\n           \'get_danet_resnet101_citys\', \'get_danet_resnet152_citys\']\n\n\nclass DANet(SegBaseModel):\n    r""""""Pyramid Scene Parsing Network\n\n    Parameters\n    ----------\n    nclass : int\n        Number of categories for the training dataset.\n    backbone : string\n        Pre-trained dilated backbone network type (default:\'resnet50\'; \'resnet50\',\n        \'resnet101\' or \'resnet152\').\n    norm_layer : object\n        Normalization layer used in backbone network (default: :class:`mxnet.gluon.nn.BatchNorm`;\n        for Synchronized Cross-GPU BachNormalization).\n    aux : bool\n        Auxiliary loss.\n    Reference:\n        Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang,and Hanqing Lu.\n        ""Dual Attention Network for Scene Segmentation."" *CVPR*, 2019\n    """"""\n\n    def __init__(self, nclass, backbone=\'resnet50\', aux=True, pretrained_base=True, **kwargs):\n        super(DANet, self).__init__(nclass, aux, backbone, pretrained_base=pretrained_base, **kwargs)\n        self.head = _DAHead(2048, nclass, aux, **kwargs)\n\n        self.__setattr__(\'exclusive\', [\'head\'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        _, _, c3, c4 = self.base_forward(x)\n        outputs = []\n        x = self.head(c4)\n        x0 = F.interpolate(x[0], size, mode=\'bilinear\', align_corners=True)\n        outputs.append(x0)\n\n        if self.aux:\n            x1 = F.interpolate(x[1], size, mode=\'bilinear\', align_corners=True)\n            x2 = F.interpolate(x[2], size, mode=\'bilinear\', align_corners=True)\n            outputs.append(x1)\n            outputs.append(x2)\n        return outputs\n\n\nclass _PositionAttentionModule(nn.Module):\n    """""" Position attention module""""""\n\n    def __init__(self, in_channels, **kwargs):\n        super(_PositionAttentionModule, self).__init__()\n        self.conv_b = nn.Conv2d(in_channels, in_channels // 8, 1)\n        self.conv_c = nn.Conv2d(in_channels, in_channels // 8, 1)\n        self.conv_d = nn.Conv2d(in_channels, in_channels, 1)\n        self.alpha = nn.Parameter(torch.zeros(1))\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x):\n        batch_size, _, height, width = x.size()\n        feat_b = self.conv_b(x).view(batch_size, -1, height * width).permute(0, 2, 1)\n        feat_c = self.conv_c(x).view(batch_size, -1, height * width)\n        attention_s = self.softmax(torch.bmm(feat_b, feat_c))\n        feat_d = self.conv_d(x).view(batch_size, -1, height * width)\n        feat_e = torch.bmm(feat_d, attention_s.permute(0, 2, 1)).view(batch_size, -1, height, width)\n        out = self.alpha * feat_e + x\n\n        return out\n\n\nclass _ChannelAttentionModule(nn.Module):\n    """"""Channel attention module""""""\n\n    def __init__(self, **kwargs):\n        super(_ChannelAttentionModule, self).__init__()\n        self.beta = nn.Parameter(torch.zeros(1))\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x):\n        batch_size, _, height, width = x.size()\n        feat_a = x.view(batch_size, -1, height * width)\n        feat_a_transpose = x.view(batch_size, -1, height * width).permute(0, 2, 1)\n        attention = torch.bmm(feat_a, feat_a_transpose)\n        attention_new = torch.max(attention, dim=-1, keepdim=True)[0].expand_as(attention) - attention\n        attention = self.softmax(attention_new)\n\n        feat_e = torch.bmm(attention, feat_a).view(batch_size, -1, height, width)\n        out = self.beta * feat_e + x\n\n        return out\n\n\nclass _DAHead(nn.Module):\n    def __init__(self, in_channels, nclass, aux=True, norm_layer=nn.BatchNorm2d, norm_kwargs=None, **kwargs):\n        super(_DAHead, self).__init__()\n        self.aux = aux\n        inter_channels = in_channels // 4\n        self.conv_p1 = nn.Sequential(\n            nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n            norm_layer(inter_channels, **({} if norm_kwargs is None else norm_kwargs)),\n            nn.ReLU(True)\n        )\n        self.conv_c1 = nn.Sequential(\n            nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n            norm_layer(inter_channels, **({} if norm_kwargs is None else norm_kwargs)),\n            nn.ReLU(True)\n        )\n        self.pam = _PositionAttentionModule(inter_channels, **kwargs)\n        self.cam = _ChannelAttentionModule(**kwargs)\n        self.conv_p2 = nn.Sequential(\n            nn.Conv2d(inter_channels, inter_channels, 3, padding=1, bias=False),\n            norm_layer(inter_channels, **({} if norm_kwargs is None else norm_kwargs)),\n            nn.ReLU(True)\n        )\n        self.conv_c2 = nn.Sequential(\n            nn.Conv2d(inter_channels, inter_channels, 3, padding=1, bias=False),\n            norm_layer(inter_channels, **({} if norm_kwargs is None else norm_kwargs)),\n            nn.ReLU(True)\n        )\n        self.out = nn.Sequential(\n            nn.Dropout(0.1),\n            nn.Conv2d(inter_channels, nclass, 1)\n        )\n        if aux:\n            self.conv_p3 = nn.Sequential(\n                nn.Dropout(0.1),\n                nn.Conv2d(inter_channels, nclass, 1)\n            )\n            self.conv_c3 = nn.Sequential(\n                nn.Dropout(0.1),\n                nn.Conv2d(inter_channels, nclass, 1)\n            )\n\n    def forward(self, x):\n        feat_p = self.conv_p1(x)\n        feat_p = self.pam(feat_p)\n        feat_p = self.conv_p2(feat_p)\n\n        feat_c = self.conv_c1(x)\n        feat_c = self.cam(feat_c)\n        feat_c = self.conv_c2(feat_c)\n\n        feat_fusion = feat_p + feat_c\n\n        outputs = []\n        fusion_out = self.out(feat_fusion)\n        outputs.append(fusion_out)\n        if self.aux:\n            p_out = self.conv_p3(feat_p)\n            c_out = self.conv_c3(feat_c)\n            outputs.append(p_out)\n            outputs.append(c_out)\n\n        return tuple(outputs)\n\n\ndef get_danet(dataset=\'citys\', backbone=\'resnet50\', pretrained=False,\n              root=\'~/.torch/models\', pretrained_base=True, **kwargs):\n    r""""""Dual Attention Network\n\n    Parameters\n    ----------\n    dataset : str, default pascal_voc\n        The dataset that model pretrained on. (pascal_voc, ade20k)\n    pretrained : bool or str\n        Boolean value controls whether to load the default pretrained weights for model.\n        String value represents the hashtag for a certain version of pretrained weights.\n    root : str, default \'~/.torch/models\'\n        Location for keeping the model parameters.\n    pretrained_base : bool or str, default True\n        This will load pretrained backbone network, that was trained on ImageNet.\n    Examples\n    --------\n    >>> model = get_danet(dataset=\'pascal_voc\', backbone=\'resnet50\', pretrained=False)\n    >>> print(model)\n    """"""\n    acronyms = {\n        \'pascal_voc\': \'pascal_voc\',\n        \'pascal_aug\': \'pascal_aug\',\n        \'ade20k\': \'ade\',\n        \'coco\': \'coco\',\n        \'citys\': \'citys\',\n    }\n    from ..data.dataloader import datasets\n    model = DANet(datasets[dataset].NUM_CLASS, backbone=backbone, pretrained_base=pretrained_base, **kwargs)\n    if pretrained:\n        from .model_store import get_model_file\n        device = torch.device(kwargs[\'local_rank\'])\n        model.load_state_dict(torch.load(get_model_file(\'danet_%s_%s\' % (backbone, acronyms[dataset]), root=root),\n                              map_location=device))\n    return model\n\n\ndef get_danet_resnet50_citys(**kwargs):\n    return get_danet(\'citys\', \'resnet50\', **kwargs)\n\n\ndef get_danet_resnet101_citys(**kwargs):\n    return get_danet(\'citys\', \'resnet101\', **kwargs)\n\n\ndef get_danet_resnet152_citys(**kwargs):\n    return get_danet(\'citys\', \'resnet152\', **kwargs)\n\n\nif __name__ == \'__main__\':\n    img = torch.randn(2, 3, 480, 480)\n    model = get_danet_resnet50_citys()\n    outputs = model(img)\n'"
core/models/deeplabv3.py,6,"b'""""""Pyramid Scene Parsing Network""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .segbase import SegBaseModel\nfrom .fcn import _FCNHead\n\n__all__ = [\'DeepLabV3\', \'get_deeplabv3\', \'get_deeplabv3_resnet50_voc\', \'get_deeplabv3_resnet101_voc\',\n           \'get_deeplabv3_resnet152_voc\', \'get_deeplabv3_resnet50_ade\', \'get_deeplabv3_resnet101_ade\',\n           \'get_deeplabv3_resnet152_ade\']\n\n\nclass DeepLabV3(SegBaseModel):\n    r""""""DeepLabV3\n\n    Parameters\n    ----------\n    nclass : int\n        Number of categories for the training dataset.\n    backbone : string\n        Pre-trained dilated backbone network type (default:\'resnet50\'; \'resnet50\',\n        \'resnet101\' or \'resnet152\').\n    norm_layer : object\n        Normalization layer used in backbone network (default: :class:`nn.BatchNorm`;\n        for Synchronized Cross-GPU BachNormalization).\n    aux : bool\n        Auxiliary loss.\n\n    Reference:\n        Chen, Liang-Chieh, et al. ""Rethinking atrous convolution for semantic image segmentation.""\n        arXiv preprint arXiv:1706.05587 (2017).\n    """"""\n\n    def __init__(self, nclass, backbone=\'resnet50\', aux=False, pretrained_base=True, **kwargs):\n        super(DeepLabV3, self).__init__(nclass, aux, backbone, pretrained_base=pretrained_base, **kwargs)\n        self.head = _DeepLabHead(nclass, **kwargs)\n        if self.aux:\n            self.auxlayer = _FCNHead(1024, nclass, **kwargs)\n\n        self.__setattr__(\'exclusive\', [\'head\', \'auxlayer\'] if aux else [\'head\'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        _, _, c3, c4 = self.base_forward(x)\n        outputs = []\n        x = self.head(c4)\n        x = F.interpolate(x, size, mode=\'bilinear\', align_corners=True)\n        outputs.append(x)\n\n        if self.aux:\n            auxout = self.auxlayer(c3)\n            auxout = F.interpolate(auxout, size, mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout)\n        return tuple(outputs)\n\n\nclass _DeepLabHead(nn.Module):\n    def __init__(self, nclass, norm_layer=nn.BatchNorm2d, norm_kwargs=None, **kwargs):\n        super(_DeepLabHead, self).__init__()\n        self.aspp = _ASPP(2048, [12, 24, 36], norm_layer=norm_layer, norm_kwargs=norm_kwargs, **kwargs)\n        self.block = nn.Sequential(\n            nn.Conv2d(256, 256, 3, padding=1, bias=False),\n            norm_layer(256, **({} if norm_kwargs is None else norm_kwargs)),\n            nn.ReLU(True),\n            nn.Dropout(0.1),\n            nn.Conv2d(256, nclass, 1)\n        )\n\n    def forward(self, x):\n        x = self.aspp(x)\n        return self.block(x)\n\n\nclass _ASPPConv(nn.Module):\n    def __init__(self, in_channels, out_channels, atrous_rate, norm_layer, norm_kwargs):\n        super(_ASPPConv, self).__init__()\n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, padding=atrous_rate, dilation=atrous_rate, bias=False),\n            norm_layer(out_channels, **({} if norm_kwargs is None else norm_kwargs)),\n            nn.ReLU(True)\n        )\n\n    def forward(self, x):\n        return self.block(x)\n\n\nclass _AsppPooling(nn.Module):\n    def __init__(self, in_channels, out_channels, norm_layer, norm_kwargs, **kwargs):\n        super(_AsppPooling, self).__init__()\n        self.gap = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            norm_layer(out_channels, **({} if norm_kwargs is None else norm_kwargs)),\n            nn.ReLU(True)\n        )\n\n    def forward(self, x):\n        size = x.size()[2:]\n        pool = self.gap(x)\n        out = F.interpolate(pool, size, mode=\'bilinear\', align_corners=True)\n        return out\n\n\nclass _ASPP(nn.Module):\n    def __init__(self, in_channels, atrous_rates, norm_layer, norm_kwargs, **kwargs):\n        super(_ASPP, self).__init__()\n        out_channels = 256\n        self.b0 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            norm_layer(out_channels, **({} if norm_kwargs is None else norm_kwargs)),\n            nn.ReLU(True)\n        )\n\n        rate1, rate2, rate3 = tuple(atrous_rates)\n        self.b1 = _ASPPConv(in_channels, out_channels, rate1, norm_layer, norm_kwargs)\n        self.b2 = _ASPPConv(in_channels, out_channels, rate2, norm_layer, norm_kwargs)\n        self.b3 = _ASPPConv(in_channels, out_channels, rate3, norm_layer, norm_kwargs)\n        self.b4 = _AsppPooling(in_channels, out_channels, norm_layer=norm_layer, norm_kwargs=norm_kwargs)\n\n        self.project = nn.Sequential(\n            nn.Conv2d(5 * out_channels, out_channels, 1, bias=False),\n            norm_layer(out_channels, **({} if norm_kwargs is None else norm_kwargs)),\n            nn.ReLU(True),\n            nn.Dropout(0.5)\n        )\n\n    def forward(self, x):\n        feat1 = self.b0(x)\n        feat2 = self.b1(x)\n        feat3 = self.b2(x)\n        feat4 = self.b3(x)\n        feat5 = self.b4(x)\n        x = torch.cat((feat1, feat2, feat3, feat4, feat5), dim=1)\n        x = self.project(x)\n        return x\n\n\ndef get_deeplabv3(dataset=\'pascal_voc\', backbone=\'resnet50\', pretrained=False, root=\'~/.torch/models\',\n                  pretrained_base=True, **kwargs):\n    acronyms = {\n        \'pascal_voc\': \'pascal_voc\',\n        \'pascal_aug\': \'pascal_aug\',\n        \'ade20k\': \'ade\',\n        \'coco\': \'coco\',\n        \'citys\': \'citys\',\n    }\n    from ..data.dataloader import datasets\n    model = DeepLabV3(datasets[dataset].NUM_CLASS, backbone=backbone, pretrained_base=pretrained_base, **kwargs)\n    if pretrained:\n        from .model_store import get_model_file\n        device = torch.device(kwargs[\'local_rank\'])\n        model.load_state_dict(torch.load(get_model_file(\'deeplabv3_%s_%s\' % (backbone, acronyms[dataset]), root=root),\n                              map_location=device))\n    return model\n\n\ndef get_deeplabv3_resnet50_voc(**kwargs):\n    return get_deeplabv3(\'pascal_voc\', \'resnet50\', **kwargs)\n\n\ndef get_deeplabv3_resnet101_voc(**kwargs):\n    return get_deeplabv3(\'pascal_voc\', \'resnet101\', **kwargs)\n\n\ndef get_deeplabv3_resnet152_voc(**kwargs):\n    return get_deeplabv3(\'pascal_voc\', \'resnet152\', **kwargs)\n\n\ndef get_deeplabv3_resnet50_ade(**kwargs):\n    return get_deeplabv3(\'ade20k\', \'resnet50\', **kwargs)\n\n\ndef get_deeplabv3_resnet101_ade(**kwargs):\n    return get_deeplabv3(\'ade20k\', \'resnet101\', **kwargs)\n\n\ndef get_deeplabv3_resnet152_ade(**kwargs):\n    return get_deeplabv3(\'ade20k\', \'resnet152\', **kwargs)\n\n\nif __name__ == \'__main__\':\n    model = get_deeplabv3_resnet50_voc()\n    img = torch.randn(2, 3, 480, 480)\n    output = model(img)\n'"
core/models/deeplabv3_plus.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .base_models.xception import get_xception\nfrom .deeplabv3 import _ASPP\nfrom .fcn import _FCNHead\nfrom ..nn import _ConvBNReLU\n\n__all__ = [\'DeepLabV3Plus\', \'get_deeplabv3_plus\', \'get_deeplabv3_plus_xception_voc\']\n\n\nclass DeepLabV3Plus(nn.Module):\n    r""""""DeepLabV3Plus\n    Parameters\n    ----------\n    nclass : int\n        Number of categories for the training dataset.\n    backbone : string\n        Pre-trained dilated backbone network type (default:\'xception\').\n    norm_layer : object\n        Normalization layer used in backbone network (default: :class:`nn.BatchNorm`;\n        for Synchronized Cross-GPU BachNormalization).\n    aux : bool\n        Auxiliary loss.\n\n    Reference:\n        Chen, Liang-Chieh, et al. ""Encoder-Decoder with Atrous Separable Convolution for Semantic\n        Image Segmentation.""\n    """"""\n\n    def __init__(self, nclass, backbone=\'xception\', aux=True, pretrained_base=True, dilated=True, **kwargs):\n        super(DeepLabV3Plus, self).__init__()\n        self.aux = aux\n        self.nclass = nclass\n        output_stride = 8 if dilated else 32\n\n        self.pretrained = get_xception(pretrained=pretrained_base, output_stride=output_stride, **kwargs)\n\n        # deeplabv3 plus\n        self.head = _DeepLabHead(nclass, **kwargs)\n        if aux:\n            self.auxlayer = _FCNHead(728, nclass, **kwargs)\n\n    def base_forward(self, x):\n        # Entry flow\n        x = self.pretrained.conv1(x)\n        x = self.pretrained.bn1(x)\n        x = self.pretrained.relu(x)\n\n        x = self.pretrained.conv2(x)\n        x = self.pretrained.bn2(x)\n        x = self.pretrained.relu(x)\n\n        x = self.pretrained.block1(x)\n        # add relu here\n        x = self.pretrained.relu(x)\n        low_level_feat = x\n\n        x = self.pretrained.block2(x)\n        x = self.pretrained.block3(x)\n\n        # Middle flow\n        x = self.pretrained.midflow(x)\n        mid_level_feat = x\n\n        # Exit flow\n        x = self.pretrained.block20(x)\n        x = self.pretrained.relu(x)\n        x = self.pretrained.conv3(x)\n        x = self.pretrained.bn3(x)\n        x = self.pretrained.relu(x)\n\n        x = self.pretrained.conv4(x)\n        x = self.pretrained.bn4(x)\n        x = self.pretrained.relu(x)\n\n        x = self.pretrained.conv5(x)\n        x = self.pretrained.bn5(x)\n        x = self.pretrained.relu(x)\n        return low_level_feat, mid_level_feat, x\n\n    def forward(self, x):\n        size = x.size()[2:]\n        c1, c3, c4 = self.base_forward(x)\n        outputs = list()\n        x = self.head(c4, c1)\n        x = F.interpolate(x, size, mode=\'bilinear\', align_corners=True)\n        outputs.append(x)\n        if self.aux:\n            auxout = self.auxlayer(c3)\n            auxout = F.interpolate(auxout, size, mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout)\n        return tuple(outputs)\n\n\nclass _DeepLabHead(nn.Module):\n    def __init__(self, nclass, c1_channels=128, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(_DeepLabHead, self).__init__()\n        self.aspp = _ASPP(2048, [12, 24, 36], norm_layer=norm_layer, **kwargs)\n        self.c1_block = _ConvBNReLU(c1_channels, 48, 3, padding=1, norm_layer=norm_layer)\n        self.block = nn.Sequential(\n            _ConvBNReLU(304, 256, 3, padding=1, norm_layer=norm_layer),\n            nn.Dropout(0.5),\n            _ConvBNReLU(256, 256, 3, padding=1, norm_layer=norm_layer),\n            nn.Dropout(0.1),\n            nn.Conv2d(256, nclass, 1))\n\n    def forward(self, x, c1):\n        size = c1.size()[2:]\n        c1 = self.c1_block(c1)\n        x = self.aspp(x)\n        x = F.interpolate(x, size, mode=\'bilinear\', align_corners=True)\n        return self.block(torch.cat([x, c1], dim=1))\n\n\ndef get_deeplabv3_plus(dataset=\'pascal_voc\', backbone=\'xception\', pretrained=False, root=\'~/.torch/models\',\n                       pretrained_base=True, **kwargs):\n    acronyms = {\n        \'pascal_voc\': \'pascal_voc\',\n        \'pascal_aug\': \'pascal_aug\',\n        \'ade20k\': \'ade\',\n        \'coco\': \'coco\',\n        \'citys\': \'citys\',\n    }\n    from ..data.dataloader import datasets\n    model = DeepLabV3Plus(datasets[dataset].NUM_CLASS, backbone=backbone, pretrained_base=pretrained_base, **kwargs)\n    if pretrained:\n        from .model_store import get_model_file\n        device = torch.device(kwargs[\'local_rank\'])\n        model.load_state_dict(\n            torch.load(get_model_file(\'deeplabv3_plus_%s_%s\' % (backbone, acronyms[dataset]), root=root),\n                map_location=device))\n    return model\n\n\ndef get_deeplabv3_plus_xception_voc(**kwargs):\n    return get_deeplabv3_plus(\'pascal_voc\', \'xception\', **kwargs)\n\n\nif __name__ == \'__main__\':\n    model = get_deeplabv3_plus_xception_voc()\n'"
core/models/denseaspp.py,10,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .base_models.densenet import *\nfrom .fcn import _FCNHead\n\n__all__ = [\'DenseASPP\', \'get_denseaspp\', \'get_denseaspp_densenet121_citys\',\n           \'get_denseaspp_densenet161_citys\', \'get_denseaspp_densenet169_citys\', \'get_denseaspp_densenet201_citys\']\n\n\nclass DenseASPP(nn.Module):\n    def __init__(self, nclass, backbone=\'densenet121\', aux=False, jpu=False,\n                 pretrained_base=True, dilate_scale=8, **kwargs):\n        super(DenseASPP, self).__init__()\n        self.nclass = nclass\n        self.aux = aux\n        self.dilate_scale = dilate_scale\n        if backbone == \'densenet121\':\n            self.pretrained = dilated_densenet121(dilate_scale, pretrained=pretrained_base, **kwargs)\n        elif backbone == \'densenet161\':\n            self.pretrained = dilated_densenet161(dilate_scale, pretrained=pretrained_base, **kwargs)\n        elif backbone == \'densenet169\':\n            self.pretrained = dilated_densenet169(dilate_scale, pretrained=pretrained_base, **kwargs)\n        elif backbone == \'densenet201\':\n            self.pretrained = dilated_densenet201(dilate_scale, pretrained=pretrained_base, **kwargs)\n        else:\n            raise RuntimeError(\'unknown backbone: {}\'.format(backbone))\n        in_channels = self.pretrained.num_features\n\n        self.head = _DenseASPPHead(in_channels, nclass)\n\n        if aux:\n            self.auxlayer = _FCNHead(in_channels, nclass, **kwargs)\n\n        self.__setattr__(\'exclusive\', [\'head\', \'auxlayer\'] if aux else [\'head\'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        features = self.pretrained.features(x)\n        if self.dilate_scale > 8:\n            features = F.interpolate(features, scale_factor=2, mode=\'bilinear\', align_corners=True)\n        outputs = []\n        x = self.head(features)\n        x = F.interpolate(x, size, mode=\'bilinear\', align_corners=True)\n        outputs.append(x)\n\n        if self.aux:\n            auxout = self.auxlayer(features)\n            auxout = F.interpolate(auxout, size, mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout)\n        return tuple(outputs)\n\n\nclass _DenseASPPHead(nn.Module):\n    def __init__(self, in_channels, nclass, norm_layer=nn.BatchNorm2d, norm_kwargs=None, **kwargs):\n        super(_DenseASPPHead, self).__init__()\n        self.dense_aspp_block = _DenseASPPBlock(in_channels, 256, 64, norm_layer, norm_kwargs)\n        self.block = nn.Sequential(\n            nn.Dropout(0.1),\n            nn.Conv2d(in_channels + 5 * 64, nclass, 1)\n        )\n\n    def forward(self, x):\n        x = self.dense_aspp_block(x)\n        return self.block(x)\n\n\nclass _DenseASPPConv(nn.Sequential):\n    def __init__(self, in_channels, inter_channels, out_channels, atrous_rate,\n                 drop_rate=0.1, norm_layer=nn.BatchNorm2d, norm_kwargs=None):\n        super(_DenseASPPConv, self).__init__()\n        self.add_module(\'conv1\', nn.Conv2d(in_channels, inter_channels, 1)),\n        self.add_module(\'bn1\', norm_layer(inter_channels, **({} if norm_kwargs is None else norm_kwargs))),\n        self.add_module(\'relu1\', nn.ReLU(True)),\n        self.add_module(\'conv2\', nn.Conv2d(inter_channels, out_channels, 3, dilation=atrous_rate, padding=atrous_rate)),\n        self.add_module(\'bn2\', norm_layer(out_channels, **({} if norm_kwargs is None else norm_kwargs))),\n        self.add_module(\'relu2\', nn.ReLU(True)),\n        self.drop_rate = drop_rate\n\n    def forward(self, x):\n        features = super(_DenseASPPConv, self).forward(x)\n        if self.drop_rate > 0:\n            features = F.dropout(features, p=self.drop_rate, training=self.training)\n        return features\n\n\nclass _DenseASPPBlock(nn.Module):\n    def __init__(self, in_channels, inter_channels1, inter_channels2,\n                 norm_layer=nn.BatchNorm2d, norm_kwargs=None):\n        super(_DenseASPPBlock, self).__init__()\n        self.aspp_3 = _DenseASPPConv(in_channels, inter_channels1, inter_channels2, 3, 0.1,\n                                     norm_layer, norm_kwargs)\n        self.aspp_6 = _DenseASPPConv(in_channels + inter_channels2 * 1, inter_channels1, inter_channels2, 6, 0.1,\n                                     norm_layer, norm_kwargs)\n        self.aspp_12 = _DenseASPPConv(in_channels + inter_channels2 * 2, inter_channels1, inter_channels2, 12, 0.1,\n                                      norm_layer, norm_kwargs)\n        self.aspp_18 = _DenseASPPConv(in_channels + inter_channels2 * 3, inter_channels1, inter_channels2, 18, 0.1,\n                                      norm_layer, norm_kwargs)\n        self.aspp_24 = _DenseASPPConv(in_channels + inter_channels2 * 4, inter_channels1, inter_channels2, 24, 0.1,\n                                      norm_layer, norm_kwargs)\n\n    def forward(self, x):\n        aspp3 = self.aspp_3(x)\n        x = torch.cat([aspp3, x], dim=1)\n\n        aspp6 = self.aspp_6(x)\n        x = torch.cat([aspp6, x], dim=1)\n\n        aspp12 = self.aspp_12(x)\n        x = torch.cat([aspp12, x], dim=1)\n\n        aspp18 = self.aspp_18(x)\n        x = torch.cat([aspp18, x], dim=1)\n\n        aspp24 = self.aspp_24(x)\n        x = torch.cat([aspp24, x], dim=1)\n\n        return x\n\n\ndef get_denseaspp(dataset=\'citys\', backbone=\'densenet121\', pretrained=False,\n                  root=\'~/.torch/models\', pretrained_base=True, **kwargs):\n    r""""""DenseASPP\n\n    Parameters\n    ----------\n    dataset : str, default citys\n        The dataset that model pretrained on. (pascal_voc, ade20k)\n    pretrained : bool or str\n        Boolean value controls whether to load the default pretrained weights for model.\n        String value represents the hashtag for a certain version of pretrained weights.\n    root : str, default \'~/.torch/models\'\n        Location for keeping the model parameters.\n    pretrained_base : bool or str, default True\n        This will load pretrained backbone network, that was trained on ImageNet.\n    Examples\n    --------\n    >>> model = get_denseaspp(dataset=\'citys\', backbone=\'densenet121\', pretrained=False)\n    >>> print(model)\n    """"""\n    acronyms = {\n        \'pascal_voc\': \'pascal_voc\',\n        \'pascal_aug\': \'pascal_aug\',\n        \'ade20k\': \'ade\',\n        \'coco\': \'coco\',\n        \'citys\': \'citys\',\n    }\n    from ..data.dataloader import datasets\n    model = DenseASPP(datasets[dataset].NUM_CLASS, backbone=backbone, pretrained_base=pretrained_base, **kwargs)\n    if pretrained:\n        from .model_store import get_model_file\n        device = torch.device(kwargs[\'local_rank\'])\n        model.load_state_dict(torch.load(get_model_file(\'denseaspp_%s_%s\' % (backbone, acronyms[dataset]), root=root),\n                              map_location=device))\n    return model\n\n\ndef get_denseaspp_densenet121_citys(**kwargs):\n    return get_denseaspp(\'citys\', \'densenet121\', **kwargs)\n\n\ndef get_denseaspp_densenet161_citys(**kwargs):\n    return get_denseaspp(\'citys\', \'densenet161\', **kwargs)\n\n\ndef get_denseaspp_densenet169_citys(**kwargs):\n    return get_denseaspp(\'citys\', \'densenet169\', **kwargs)\n\n\ndef get_denseaspp_densenet201_citys(**kwargs):\n    return get_denseaspp(\'citys\', \'densenet201\', **kwargs)\n\n\nif __name__ == \'__main__\':\n    img = torch.randn(2, 3, 480, 480)\n    model = get_denseaspp_densenet121_citys()\n    outputs = model(img)\n'"
core/models/dfanet.py,10,"b'"""""" Deep Feature Aggregation""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom core.models.base_models import Enc, FCAttention, get_xception_a\nfrom core.nn import _ConvBNReLU\n\n__all__ = [\'DFANet\', \'get_dfanet\', \'get_dfanet_citys\']\n\n\nclass DFANet(nn.Module):\n    def __init__(self, nclass, backbone=\'\', aux=False, jpu=False, pretrained_base=False, **kwargs):\n        super(DFANet, self).__init__()\n        self.pretrained = get_xception_a(pretrained_base, **kwargs)\n\n        self.enc2_2 = Enc(240, 48, 4, **kwargs)\n        self.enc3_2 = Enc(144, 96, 6, **kwargs)\n        self.enc4_2 = Enc(288, 192, 4, **kwargs)\n        self.fca_2 = FCAttention(192, **kwargs)\n\n        self.enc2_3 = Enc(240, 48, 4, **kwargs)\n        self.enc3_3 = Enc(144, 96, 6, **kwargs)\n        self.enc3_4 = Enc(288, 192, 4, **kwargs)\n        self.fca_3 = FCAttention(192, **kwargs)\n\n        self.enc2_1_reduce = _ConvBNReLU(48, 32, 1, **kwargs)\n        self.enc2_2_reduce = _ConvBNReLU(48, 32, 1, **kwargs)\n        self.enc2_3_reduce = _ConvBNReLU(48, 32, 1, **kwargs)\n        self.conv_fusion = _ConvBNReLU(32, 32, 1, **kwargs)\n\n        self.fca_1_reduce = _ConvBNReLU(192, 32, 1, **kwargs)\n        self.fca_2_reduce = _ConvBNReLU(192, 32, 1, **kwargs)\n        self.fca_3_reduce = _ConvBNReLU(192, 32, 1, **kwargs)\n        self.conv_out = nn.Conv2d(32, nclass, 1)\n\n        self.__setattr__(\'exclusive\', [\'enc2_2\', \'enc3_2\', \'enc4_2\', \'fca_2\', \'enc2_3\', \'enc3_3\', \'enc3_4\', \'fca_3\',\n                                       \'enc2_1_reduce\', \'enc2_2_reduce\', \'enc2_3_reduce\', \'conv_fusion\', \'fca_1_reduce\',\n                                       \'fca_2_reduce\', \'fca_3_reduce\', \'conv_out\'])\n\n    def forward(self, x):\n        # backbone\n        stage1_conv1 = self.pretrained.conv1(x)\n        stage1_enc2 = self.pretrained.enc2(stage1_conv1)\n        stage1_enc3 = self.pretrained.enc3(stage1_enc2)\n        stage1_enc4 = self.pretrained.enc4(stage1_enc3)\n        stage1_fca = self.pretrained.fca(stage1_enc4)\n        stage1_out = F.interpolate(stage1_fca, scale_factor=4, mode=\'bilinear\', align_corners=True)\n\n        # stage2\n        stage2_enc2 = self.enc2_2(torch.cat([stage1_enc2, stage1_out], dim=1))\n        stage2_enc3 = self.enc3_2(torch.cat([stage1_enc3, stage2_enc2], dim=1))\n        stage2_enc4 = self.enc4_2(torch.cat([stage1_enc4, stage2_enc3], dim=1))\n        stage2_fca = self.fca_2(stage2_enc4)\n        stage2_out = F.interpolate(stage2_fca, scale_factor=4, mode=\'bilinear\', align_corners=True)\n\n        # stage3\n        stage3_enc2 = self.enc2_3(torch.cat([stage2_enc2, stage2_out], dim=1))\n        stage3_enc3 = self.enc3_3(torch.cat([stage2_enc3, stage3_enc2], dim=1))\n        stage3_enc4 = self.enc3_4(torch.cat([stage2_enc4, stage3_enc3], dim=1))\n        stage3_fca = self.fca_3(stage3_enc4)\n\n        stage1_enc2_decoder = self.enc2_1_reduce(stage1_enc2)\n        stage2_enc2_docoder = F.interpolate(self.enc2_2_reduce(stage2_enc2), scale_factor=2,\n                                            mode=\'bilinear\', align_corners=True)\n        stage3_enc2_decoder = F.interpolate(self.enc2_3_reduce(stage3_enc2), scale_factor=4,\n                                            mode=\'bilinear\', align_corners=True)\n        fusion = stage1_enc2_decoder + stage2_enc2_docoder + stage3_enc2_decoder\n        fusion = self.conv_fusion(fusion)\n\n        stage1_fca_decoder = F.interpolate(self.fca_1_reduce(stage1_fca), scale_factor=4,\n                                           mode=\'bilinear\', align_corners=True)\n        stage2_fca_decoder = F.interpolate(self.fca_2_reduce(stage2_fca), scale_factor=8,\n                                           mode=\'bilinear\', align_corners=True)\n        stage3_fca_decoder = F.interpolate(self.fca_3_reduce(stage3_fca), scale_factor=16,\n                                           mode=\'bilinear\', align_corners=True)\n        fusion = fusion + stage1_fca_decoder + stage2_fca_decoder + stage3_fca_decoder\n\n        outputs = list()\n        out = self.conv_out(fusion)\n        out = F.interpolate(out, scale_factor=4, mode=\'bilinear\', align_corners=True)\n        outputs.append(out)\n\n        return tuple(outputs)\n\n\ndef get_dfanet(dataset=\'citys\', backbone=\'\', pretrained=False, root=\'~/.torch/models\',\n               pretrained_base=True, **kwargs):\n    acronyms = {\n        \'pascal_voc\': \'pascal_voc\',\n        \'pascal_aug\': \'pascal_aug\',\n        \'ade20k\': \'ade\',\n        \'coco\': \'coco\',\n        \'citys\': \'citys\',\n    }\n    from ..data.dataloader import datasets\n    model = DFANet(datasets[dataset].NUM_CLASS, backbone=backbone, pretrained_base=pretrained_base, **kwargs)\n    if pretrained:\n        from .model_store import get_model_file\n        device = torch.device(kwargs[\'local_rank\'])\n        model.load_state_dict(torch.load(get_model_file(\'dfanet_%s\' % (acronyms[dataset]), root=root),\n                              map_location=device))\n    return model\n\n\ndef get_dfanet_citys(**kwargs):\n    return get_dfanet(\'citys\', **kwargs)\n\n\nif __name__ == \'__main__\':\n    model = get_dfanet_citys()\n'"
core/models/dunet.py,6,"b'""""""Decoders Matter for Semantic Segmentation""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .segbase import SegBaseModel\nfrom .fcn import _FCNHead\n\n__all__ = [\'DUNet\', \'get_dunet\', \'get_dunet_resnet50_pascal_voc\',\n           \'get_dunet_resnet101_pascal_voc\', \'get_dunet_resnet152_pascal_voc\']\n\n\n# The model may be wrong because lots of details missing in paper.\nclass DUNet(SegBaseModel):\n    """"""Decoders Matter for Semantic Segmentation\n\n    Reference:\n        Zhi Tian, Tong He, Chunhua Shen, and Youliang Yan.\n        ""Decoders Matter for Semantic Segmentation:\n        Data-Dependent Decoding Enables Flexible Feature Aggregation."" CVPR, 2019\n    """"""\n\n    def __init__(self, nclass, backbone=\'resnet50\', aux=True, pretrained_base=True, **kwargs):\n        super(DUNet, self).__init__(nclass, aux, backbone, pretrained_base=pretrained_base, **kwargs)\n        self.head = _DUHead(2144, **kwargs)\n        self.dupsample = DUpsampling(256, nclass, scale_factor=8, **kwargs)\n        if aux:\n            self.auxlayer = _FCNHead(1024, 256, **kwargs)\n            self.aux_dupsample = DUpsampling(256, nclass, scale_factor=8, **kwargs)\n\n        self.__setattr__(\'exclusive\',\n                         [\'dupsample\', \'head\', \'auxlayer\', \'aux_dupsample\'] if aux else [\'dupsample\', \'head\'])\n\n    def forward(self, x):\n        c1, c2, c3, c4 = self.base_forward(x)\n        outputs = []\n        x = self.head(c2, c3, c4)\n        x = self.dupsample(x)\n        outputs.append(x)\n\n        if self.aux:\n            auxout = self.auxlayer(c3)\n            auxout = self.aux_dupsample(auxout)\n            outputs.append(auxout)\n        return tuple(outputs)\n\n\nclass FeatureFused(nn.Module):\n    """"""Module for fused features""""""\n\n    def __init__(self, inter_channels=48, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(FeatureFused, self).__init__()\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(512, inter_channels, 1, bias=False),\n            norm_layer(inter_channels),\n            nn.ReLU(True)\n        )\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(1024, inter_channels, 1, bias=False),\n            norm_layer(inter_channels),\n            nn.ReLU(True)\n        )\n\n    def forward(self, c2, c3, c4):\n        size = c4.size()[2:]\n        c2 = self.conv2(F.interpolate(c2, size, mode=\'bilinear\', align_corners=True))\n        c3 = self.conv3(F.interpolate(c3, size, mode=\'bilinear\', align_corners=True))\n        fused_feature = torch.cat([c4, c3, c2], dim=1)\n        return fused_feature\n\n\nclass _DUHead(nn.Module):\n    def __init__(self, in_channels, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(_DUHead, self).__init__()\n        self.fuse = FeatureFused(norm_layer=norm_layer, **kwargs)\n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels, 256, 3, padding=1, bias=False),\n            norm_layer(256),\n            nn.ReLU(True),\n            nn.Conv2d(256, 256, 3, padding=1, bias=False),\n            norm_layer(256),\n            nn.ReLU(True)\n        )\n\n    def forward(self, c2, c3, c4):\n        fused_feature = self.fuse(c2, c3, c4)\n        out = self.block(fused_feature)\n        return out\n\n\nclass DUpsampling(nn.Module):\n    """"""DUsampling module""""""\n\n    def __init__(self, in_channels, out_channels, scale_factor=2, **kwargs):\n        super(DUpsampling, self).__init__()\n        self.scale_factor = scale_factor\n        self.conv_w = nn.Conv2d(in_channels, out_channels * scale_factor * scale_factor, 1, bias=False)\n\n    def forward(self, x):\n        x = self.conv_w(x)\n        n, c, h, w = x.size()\n\n        # N, C, H, W --> N, W, H, C\n        x = x.permute(0, 3, 2, 1).contiguous()\n\n        # N, W, H, C --> N, W, H * scale, C // scale\n        x = x.view(n, w, h * self.scale_factor, c // self.scale_factor)\n\n        # N, W, H * scale, C // scale --> N, H * scale, W, C // scale\n        x = x.permute(0, 2, 1, 3).contiguous()\n\n        # N, H * scale, W, C // scale --> N, H * scale, W * scale, C // (scale ** 2)\n        x = x.view(n, h * self.scale_factor, w * self.scale_factor, c // (self.scale_factor * self.scale_factor))\n\n        # N, H * scale, W * scale, C // (scale ** 2) -- > N, C // (scale ** 2), H * scale, W * scale\n        x = x.permute(0, 3, 1, 2)\n\n        return x\n\n\ndef get_dunet(dataset=\'pascal_voc\', backbone=\'resnet50\', pretrained=False,\n              root=\'~/.torch/models\', pretrained_base=True, **kwargs):\n    acronyms = {\n        \'pascal_voc\': \'pascal_voc\',\n        \'pascal_aug\': \'pascal_aug\',\n        \'ade20k\': \'ade\',\n        \'coco\': \'coco\',\n        \'citys\': \'citys\',\n    }\n    from ..data.dataloader import datasets\n    model = DUNet(datasets[dataset].NUM_CLASS, backbone=backbone, pretrained_base=pretrained_base, **kwargs)\n    if pretrained:\n        from .model_store import get_model_file\n        device = torch.device(kwargs[\'local_rank\'])\n        model.load_state_dict(torch.load(get_model_file(\'dunet_%s_%s\' % (backbone, acronyms[dataset]), root=root),\n                              map_location=device))\n    return model\n\n\ndef get_dunet_resnet50_pascal_voc(**kwargs):\n    return get_dunet(\'pascal_voc\', \'resnet50\', **kwargs)\n\n\ndef get_dunet_resnet101_pascal_voc(**kwargs):\n    return get_dunet(\'pascal_voc\', \'resnet101\', **kwargs)\n\n\ndef get_dunet_resnet152_pascal_voc(**kwargs):\n    return get_dunet(\'pascal_voc\', \'resnet152\', **kwargs)\n\n\nif __name__ == \'__main__\':\n    img = torch.randn(2, 3, 256, 256)\n    model = get_dunet_resnet50_pascal_voc()\n    outputs = model(img)\n'"
core/models/encnet.py,8,"b'""""""Context Encoding for Semantic Segmentation""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .segbase import SegBaseModel\nfrom .fcn import _FCNHead\n\n__all__ = [\'EncNet\', \'EncModule\', \'get_encnet\', \'get_encnet_resnet50_ade\',\n           \'get_encnet_resnet101_ade\', \'get_encnet_resnet152_ade\']\n\n\nclass EncNet(SegBaseModel):\n    def __init__(self, nclass, backbone=\'resnet50\', aux=True, se_loss=True, lateral=False,\n                 pretrained_base=True, **kwargs):\n        super(EncNet, self).__init__(nclass, aux, backbone, pretrained_base=pretrained_base, **kwargs)\n        self.head = _EncHead(2048, nclass, se_loss=se_loss, lateral=lateral, **kwargs)\n        if aux:\n            self.auxlayer = _FCNHead(1024, nclass, **kwargs)\n\n        self.__setattr__(\'exclusive\', [\'head\', \'auxlayer\'] if aux else [\'head\'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        features = self.base_forward(x)\n\n        x = list(self.head(*features))\n        x[0] = F.interpolate(x[0], size, mode=\'bilinear\', align_corners=True)\n        if self.aux:\n            auxout = self.auxlayer(features[2])\n            auxout = F.interpolate(auxout, size, mode=\'bilinear\', align_corners=True)\n            x.append(auxout)\n        return tuple(x)\n\n\nclass _EncHead(nn.Module):\n    def __init__(self, in_channels, nclass, se_loss=True, lateral=True,\n                 norm_layer=nn.BatchNorm2d, norm_kwargs=None, **kwargs):\n        super(_EncHead, self).__init__()\n        self.lateral = lateral\n        self.conv5 = nn.Sequential(\n            nn.Conv2d(in_channels, 512, 3, padding=1, bias=False),\n            norm_layer(512, **({} if norm_kwargs is None else norm_kwargs)),\n            nn.ReLU(True)\n        )\n        if lateral:\n            self.connect = nn.ModuleList([\n                nn.Sequential(\n                    nn.Conv2d(512, 512, 1, bias=False),\n                    norm_layer(512, **({} if norm_kwargs is None else norm_kwargs)),\n                    nn.ReLU(True)),\n                nn.Sequential(\n                    nn.Conv2d(1024, 512, 1, bias=False),\n                    norm_layer(512, **({} if norm_kwargs is None else norm_kwargs)),\n                    nn.ReLU(True)),\n            ])\n            self.fusion = nn.Sequential(\n                nn.Conv2d(3 * 512, 512, 3, padding=1, bias=False),\n                norm_layer(512, **({} if norm_kwargs is None else norm_kwargs)),\n                nn.ReLU(True)\n            )\n        self.encmodule = EncModule(512, nclass, ncodes=32, se_loss=se_loss,\n                                   norm_layer=norm_layer, norm_kwargs=norm_kwargs, **kwargs)\n        self.conv6 = nn.Sequential(\n            nn.Dropout(0.1, False),\n            nn.Conv2d(512, nclass, 1)\n        )\n\n    def forward(self, *inputs):\n        feat = self.conv5(inputs[-1])\n        if self.lateral:\n            c2 = self.connect[0](inputs[1])\n            c3 = self.connect[1](inputs[2])\n            feat = self.fusion(torch.cat([feat, c2, c3], 1))\n        outs = list(self.encmodule(feat))\n        outs[0] = self.conv6(outs[0])\n        return tuple(outs)\n\n\nclass EncModule(nn.Module):\n    def __init__(self, in_channels, nclass, ncodes=32, se_loss=True,\n                 norm_layer=nn.BatchNorm2d, norm_kwargs=None, **kwargs):\n        super(EncModule, self).__init__()\n        self.se_loss = se_loss\n        self.encoding = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels, 1, bias=False),\n            norm_layer(in_channels, **({} if norm_kwargs is None else norm_kwargs)),\n            nn.ReLU(True),\n            Encoding(D=in_channels, K=ncodes),\n            nn.BatchNorm1d(ncodes),\n            nn.ReLU(True),\n            Mean(dim=1)\n        )\n        self.fc = nn.Sequential(\n            nn.Linear(in_channels, in_channels),\n            nn.Sigmoid()\n        )\n        if self.se_loss:\n            self.selayer = nn.Linear(in_channels, nclass)\n\n    def forward(self, x):\n        en = self.encoding(x)\n        b, c, _, _ = x.size()\n        gamma = self.fc(en)\n        y = gamma.view(b, c, 1, 1)\n        outputs = [F.relu_(x + x * y)]\n        if self.se_loss:\n            outputs.append(self.selayer(en))\n        return tuple(outputs)\n\n\nclass Encoding(nn.Module):\n    def __init__(self, D, K):\n        super(Encoding, self).__init__()\n        # init codewords and smoothing factor\n        self.D, self.K = D, K\n        self.codewords = nn.Parameter(torch.Tensor(K, D), requires_grad=True)\n        self.scale = nn.Parameter(torch.Tensor(K), requires_grad=True)\n        self.reset_params()\n\n    def reset_params(self):\n        std1 = 1. / ((self.K * self.D) ** (1 / 2))\n        self.codewords.data.uniform_(-std1, std1)\n        self.scale.data.uniform_(-1, 0)\n\n    def forward(self, X):\n        # input X is a 4D tensor\n        assert (X.size(1) == self.D)\n        B, D = X.size(0), self.D\n        if X.dim() == 3:\n            # BxDxN -> BxNxD\n            X = X.transpose(1, 2).contiguous()\n        elif X.dim() == 4:\n            # BxDxHxW -> Bx(HW)xD\n            X = X.view(B, D, -1).transpose(1, 2).contiguous()\n        else:\n            raise RuntimeError(\'Encoding Layer unknown input dims!\')\n        # assignment weights BxNxK\n        A = F.softmax(self.scale_l2(X, self.codewords, self.scale), dim=2)\n        # aggregate\n        E = self.aggregate(A, X, self.codewords)\n        return E\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'(\' \\\n               + \'N x\' + str(self.D) + \'=>\' + str(self.K) + \'x\' \\\n               + str(self.D) + \')\'\n\n    @staticmethod\n    def scale_l2(X, C, S):\n        S = S.view(1, 1, C.size(0), 1)\n        X = X.unsqueeze(2).expand(X.size(0), X.size(1), C.size(0), C.size(1))\n        C = C.unsqueeze(0).unsqueeze(0)\n        SL = S * (X - C)\n        SL = SL.pow(2).sum(3)\n        return SL\n\n    @staticmethod\n    def aggregate(A, X, C):\n        A = A.unsqueeze(3)\n        X = X.unsqueeze(2).expand(X.size(0), X.size(1), C.size(0), C.size(1))\n        C = C.unsqueeze(0).unsqueeze(0)\n        E = A * (X - C)\n        E = E.sum(1)\n        return E\n\n\nclass Mean(nn.Module):\n    def __init__(self, dim, keep_dim=False):\n        super(Mean, self).__init__()\n        self.dim = dim\n        self.keep_dim = keep_dim\n\n    def forward(self, input):\n        return input.mean(self.dim, self.keep_dim)\n\n\ndef get_encnet(dataset=\'pascal_voc\', backbone=\'resnet50\', pretrained=False, root=\'~/.torch/models\',\n               pretrained_base=True, **kwargs):\n    acronyms = {\n        \'pascal_voc\': \'pascal_voc\',\n        \'pascal_aug\': \'pascal_aug\',\n        \'ade20k\': \'ade\',\n        \'coco\': \'coco\',\n        \'citys\': \'citys\',\n    }\n    from ..data.dataloader import datasets\n    model = EncNet(datasets[dataset].NUM_CLASS, backbone=backbone, pretrained_base=pretrained_base, **kwargs)\n    if pretrained:\n        from .model_store import get_model_file\n        device = torch.device(kwargs[\'local_rank\'])\n        model.load_state_dict(torch.load(get_model_file(\'encnet_%s_%s\' % (backbone, acronyms[dataset]), root=root),\n                              map_location=device))\n    return model\n\n\ndef get_encnet_resnet50_ade(**kwargs):\n    return get_encnet(\'ade20k\', \'resnet50\', **kwargs)\n\n\ndef get_encnet_resnet101_ade(**kwargs):\n    return get_encnet(\'ade20k\', \'resnet101\', **kwargs)\n\n\ndef get_encnet_resnet152_ade(**kwargs):\n    return get_encnet(\'ade20k\', \'resnet152\', **kwargs)\n\n\nif __name__ == \'__main__\':\n    img = torch.randn(2, 3, 224, 224)\n    model = get_encnet_resnet50_ade()\n    outputs = model(img)\n'"
core/models/enet.py,5,"b'""""""Efficient Neural Network""""""\nimport torch\nimport torch.nn as nn\n\n__all__ = [\'ENet\', \'get_enet\', \'get_enet_citys\']\n\n\nclass ENet(nn.Module):\n    """"""Efficient Neural Network""""""\n\n    def __init__(self, nclass, backbone=\'\', aux=False, jpu=False, pretrained_base=None, **kwargs):\n        super(ENet, self).__init__()\n        self.initial = InitialBlock(13, **kwargs)\n\n        self.bottleneck1_0 = Bottleneck(16, 16, 64, downsampling=True, **kwargs)\n        self.bottleneck1_1 = Bottleneck(64, 16, 64, **kwargs)\n        self.bottleneck1_2 = Bottleneck(64, 16, 64, **kwargs)\n        self.bottleneck1_3 = Bottleneck(64, 16, 64, **kwargs)\n        self.bottleneck1_4 = Bottleneck(64, 16, 64, **kwargs)\n\n        self.bottleneck2_0 = Bottleneck(64, 32, 128, downsampling=True, **kwargs)\n        self.bottleneck2_1 = Bottleneck(128, 32, 128, **kwargs)\n        self.bottleneck2_2 = Bottleneck(128, 32, 128, dilation=2, **kwargs)\n        self.bottleneck2_3 = Bottleneck(128, 32, 128, asymmetric=True, **kwargs)\n        self.bottleneck2_4 = Bottleneck(128, 32, 128, dilation=4, **kwargs)\n        self.bottleneck2_5 = Bottleneck(128, 32, 128, **kwargs)\n        self.bottleneck2_6 = Bottleneck(128, 32, 128, dilation=8, **kwargs)\n        self.bottleneck2_7 = Bottleneck(128, 32, 128, asymmetric=True, **kwargs)\n        self.bottleneck2_8 = Bottleneck(128, 32, 128, dilation=16, **kwargs)\n\n        self.bottleneck3_1 = Bottleneck(128, 32, 128, **kwargs)\n        self.bottleneck3_2 = Bottleneck(128, 32, 128, dilation=2, **kwargs)\n        self.bottleneck3_3 = Bottleneck(128, 32, 128, asymmetric=True, **kwargs)\n        self.bottleneck3_4 = Bottleneck(128, 32, 128, dilation=4, **kwargs)\n        self.bottleneck3_5 = Bottleneck(128, 32, 128, **kwargs)\n        self.bottleneck3_6 = Bottleneck(128, 32, 128, dilation=8, **kwargs)\n        self.bottleneck3_7 = Bottleneck(128, 32, 128, asymmetric=True, **kwargs)\n        self.bottleneck3_8 = Bottleneck(128, 32, 128, dilation=16, **kwargs)\n\n        self.bottleneck4_0 = UpsamplingBottleneck(128, 16, 64, **kwargs)\n        self.bottleneck4_1 = Bottleneck(64, 16, 64, **kwargs)\n        self.bottleneck4_2 = Bottleneck(64, 16, 64, **kwargs)\n\n        self.bottleneck5_0 = UpsamplingBottleneck(64, 4, 16, **kwargs)\n        self.bottleneck5_1 = Bottleneck(16, 4, 16, **kwargs)\n\n        self.fullconv = nn.ConvTranspose2d(16, nclass, 2, 2, bias=False)\n\n        self.__setattr__(\'exclusive\', [\'bottleneck1_0\', \'bottleneck1_1\', \'bottleneck1_2\', \'bottleneck1_3\',\n                                       \'bottleneck1_4\', \'bottleneck2_0\', \'bottleneck2_1\', \'bottleneck2_2\',\n                                       \'bottleneck2_3\', \'bottleneck2_4\', \'bottleneck2_5\', \'bottleneck2_6\',\n                                       \'bottleneck2_7\', \'bottleneck2_8\', \'bottleneck3_1\', \'bottleneck3_2\',\n                                       \'bottleneck3_3\', \'bottleneck3_4\', \'bottleneck3_5\', \'bottleneck3_6\',\n                                       \'bottleneck3_7\', \'bottleneck3_8\', \'bottleneck4_0\', \'bottleneck4_1\',\n                                       \'bottleneck4_2\', \'bottleneck5_0\', \'bottleneck5_1\', \'fullconv\'])\n\n    def forward(self, x):\n        # init\n        x = self.initial(x)\n\n        # stage 1\n        x, max_indices1 = self.bottleneck1_0(x)\n        x = self.bottleneck1_1(x)\n        x = self.bottleneck1_2(x)\n        x = self.bottleneck1_3(x)\n        x = self.bottleneck1_4(x)\n\n        # stage 2\n        x, max_indices2 = self.bottleneck2_0(x)\n        x = self.bottleneck2_1(x)\n        x = self.bottleneck2_2(x)\n        x = self.bottleneck2_3(x)\n        x = self.bottleneck2_4(x)\n        x = self.bottleneck2_5(x)\n        x = self.bottleneck2_6(x)\n        x = self.bottleneck2_7(x)\n        x = self.bottleneck2_8(x)\n\n        # stage 3\n        x = self.bottleneck3_1(x)\n        x = self.bottleneck3_2(x)\n        x = self.bottleneck3_3(x)\n        x = self.bottleneck3_4(x)\n        x = self.bottleneck3_6(x)\n        x = self.bottleneck3_7(x)\n        x = self.bottleneck3_8(x)\n\n        # stage 4\n        x = self.bottleneck4_0(x, max_indices2)\n        x = self.bottleneck4_1(x)\n        x = self.bottleneck4_2(x)\n\n        # stage 5\n        x = self.bottleneck5_0(x, max_indices1)\n        x = self.bottleneck5_1(x)\n\n        # out\n        x = self.fullconv(x)\n        return tuple([x])\n\n\nclass InitialBlock(nn.Module):\n    """"""ENet initial block""""""\n\n    def __init__(self, out_channels, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(InitialBlock, self).__init__()\n        self.conv = nn.Conv2d(3, out_channels, 3, 2, 1, bias=False)\n        self.maxpool = nn.MaxPool2d(2, 2)\n        self.bn = norm_layer(out_channels + 3)\n        self.act = nn.PReLU()\n\n    def forward(self, x):\n        x_conv = self.conv(x)\n        x_pool = self.maxpool(x)\n        x = torch.cat([x_conv, x_pool], dim=1)\n        x = self.bn(x)\n        x = self.act(x)\n        return x\n\n\nclass Bottleneck(nn.Module):\n    """"""Bottlenecks include regular, asymmetric, downsampling, dilated""""""\n\n    def __init__(self, in_channels, inter_channels, out_channels, dilation=1, asymmetric=False,\n                 downsampling=False, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(Bottleneck, self).__init__()\n        self.downsamping = downsampling\n        if downsampling:\n            self.maxpool = nn.MaxPool2d(2, 2, return_indices=True)\n            self.conv_down = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, 1, bias=False),\n                norm_layer(out_channels)\n            )\n\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, inter_channels, 1, bias=False),\n            norm_layer(inter_channels),\n            nn.PReLU()\n        )\n\n        if downsampling:\n            self.conv2 = nn.Sequential(\n                nn.Conv2d(inter_channels, inter_channels, 2, stride=2, bias=False),\n                norm_layer(inter_channels),\n                nn.PReLU()\n            )\n        else:\n            if asymmetric:\n                self.conv2 = nn.Sequential(\n                    nn.Conv2d(inter_channels, inter_channels, (5, 1), padding=(2, 0), bias=False),\n                    nn.Conv2d(inter_channels, inter_channels, (1, 5), padding=(0, 2), bias=False),\n                    norm_layer(inter_channels),\n                    nn.PReLU()\n                )\n            else:\n                self.conv2 = nn.Sequential(\n                    nn.Conv2d(inter_channels, inter_channels, 3, dilation=dilation, padding=dilation, bias=False),\n                    norm_layer(inter_channels),\n                    nn.PReLU()\n                )\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(inter_channels, out_channels, 1, bias=False),\n            norm_layer(out_channels),\n            nn.Dropout2d(0.1)\n        )\n        self.act = nn.PReLU()\n\n    def forward(self, x):\n        identity = x\n        if self.downsamping:\n            identity, max_indices = self.maxpool(identity)\n            identity = self.conv_down(identity)\n\n        out = self.conv1(x)\n        out = self.conv2(out)\n        out = self.conv3(out)\n        out = self.act(out + identity)\n\n        if self.downsamping:\n            return out, max_indices\n        else:\n            return out\n\n\nclass UpsamplingBottleneck(nn.Module):\n    """"""upsampling Block""""""\n\n    def __init__(self, in_channels, inter_channels, out_channels, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(UpsamplingBottleneck, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            norm_layer(out_channels)\n        )\n        self.upsampling = nn.MaxUnpool2d(2)\n\n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels, inter_channels, 1, bias=False),\n            norm_layer(inter_channels),\n            nn.PReLU(),\n            nn.ConvTranspose2d(inter_channels, inter_channels, 2, 2, bias=False),\n            norm_layer(inter_channels),\n            nn.PReLU(),\n            nn.Conv2d(inter_channels, out_channels, 1, bias=False),\n            norm_layer(out_channels),\n            nn.Dropout2d(0.1)\n        )\n        self.act = nn.PReLU()\n\n    def forward(self, x, max_indices):\n        out_up = self.conv(x)\n        out_up = self.upsampling(out_up, max_indices)\n\n        out_ext = self.block(x)\n        out = self.act(out_up + out_ext)\n        return out\n\n\ndef get_enet(dataset=\'citys\', backbone=\'\', pretrained=False, root=\'~/.torch/models\', pretrained_base=True, **kwargs):\n    acronyms = {\n        \'pascal_voc\': \'pascal_voc\',\n        \'pascal_aug\': \'pascal_aug\',\n        \'ade20k\': \'ade\',\n        \'coco\': \'coco\',\n        \'citys\': \'citys\',\n    }\n    from core.data.dataloader import datasets\n    model = ENet(datasets[dataset].NUM_CLASS, backbone=backbone, pretrained_base=pretrained_base, **kwargs)\n    if pretrained:\n        from .model_store import get_model_file\n        device = torch.device(kwargs[\'local_rank\'])\n        model.load_state_dict(torch.load(get_model_file(\'enet_%s\' % (acronyms[dataset]), root=root),\n                              map_location=device))\n    return model\n\n\ndef get_enet_citys(**kwargs):\n    return get_enet(\'citys\', \'\', **kwargs)\n\n\nif __name__ == \'__main__\':\n    img = torch.randn(1, 3, 512, 512)\n    model = get_enet_citys()\n    output = model(img)\n'"
core/models/espnet.py,8,"b'""ESPNetv2: A Light-weight, Power Efficient, and General Purpose for Semantic Segmentation""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom core.models.base_models import eespnet, EESP\nfrom core.nn import _ConvBNPReLU, _BNPReLU\n\n\nclass ESPNetV2(nn.Module):\n    r""""""ESPNetV2\n\n    Parameters\n    ----------\n    nclass : int\n        Number of categories for the training dataset.\n    backbone : string\n        Pre-trained dilated backbone network type (default:\'resnet50\'; \'resnet50\',\n        \'resnet101\' or \'resnet152\').\n    norm_layer : object\n        Normalization layer used in backbone network (default: :class:`nn.BatchNorm`;\n        for Synchronized Cross-GPU BachNormalization).\n    aux : bool\n        Auxiliary loss.\n\n    Reference:\n        Sachin Mehta, et al. ""ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network.""\n        arXiv preprint arXiv:1811.11431 (2018).\n    """"""\n\n    def __init__(self, nclass, backbone=\'\', aux=False, jpu=False, pretrained_base=False, **kwargs):\n        super(ESPNetV2, self).__init__()\n        self.pretrained = eespnet(pretrained=pretrained_base, **kwargs)\n        self.proj_L4_C = _ConvBNPReLU(256, 128, 1, **kwargs)\n        self.pspMod = nn.Sequential(\n            EESP(256, 128, stride=1, k=4, r_lim=7, **kwargs),\n            _PSPModule(128, 128, **kwargs))\n        self.project_l3 = nn.Sequential(\n            nn.Dropout2d(0.1),\n            nn.Conv2d(128, nclass, 1, bias=False))\n        self.act_l3 = _BNPReLU(nclass, **kwargs)\n        self.project_l2 = _ConvBNPReLU(64 + nclass, nclass, 1, **kwargs)\n        self.project_l1 = nn.Sequential(\n            nn.Dropout2d(0.1),\n            nn.Conv2d(32 + nclass, nclass, 1, bias=False))\n\n        self.aux = aux\n\n        self.__setattr__(\'exclusive\', [\'proj_L4_C\', \'pspMod\', \'project_l3\', \'act_l3\', \'project_l2\', \'project_l1\'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        out_l1, out_l2, out_l3, out_l4 = self.pretrained(x, seg=True)\n        out_l4_proj = self.proj_L4_C(out_l4)\n        up_l4_to_l3 = F.interpolate(out_l4_proj, scale_factor=2, mode=\'bilinear\', align_corners=True)\n        merged_l3_upl4 = self.pspMod(torch.cat([out_l3, up_l4_to_l3], 1))\n        proj_merge_l3_bef_act = self.project_l3(merged_l3_upl4)\n        proj_merge_l3 = self.act_l3(proj_merge_l3_bef_act)\n        out_up_l3 = F.interpolate(proj_merge_l3, scale_factor=2, mode=\'bilinear\', align_corners=True)\n        merge_l2 = self.project_l2(torch.cat([out_l2, out_up_l3], 1))\n        out_up_l2 = F.interpolate(merge_l2, scale_factor=2, mode=\'bilinear\', align_corners=True)\n        merge_l1 = self.project_l1(torch.cat([out_l1, out_up_l2], 1))\n\n        outputs = list()\n        merge1_l1 = F.interpolate(merge_l1, scale_factor=2, mode=\'bilinear\', align_corners=True)\n        outputs.append(merge1_l1)\n        if self.aux:\n            # different from paper\n            auxout = F.interpolate(proj_merge_l3_bef_act, size, mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout)\n\n        return tuple(outputs)\n\n\n# different from PSPNet\nclass _PSPModule(nn.Module):\n    def __init__(self, in_channels, out_channels=1024, sizes=(1, 2, 4, 8), **kwargs):\n        super(_PSPModule, self).__init__()\n        self.stages = nn.ModuleList(\n            [nn.Conv2d(in_channels, in_channels, 3, 1, 1, groups=in_channels, bias=False) for _ in sizes])\n        self.project = _ConvBNPReLU(in_channels * (len(sizes) + 1), out_channels, 1, 1, **kwargs)\n\n    def forward(self, x):\n        size = x.size()[2:]\n        feats = [x]\n        for stage in self.stages:\n            x = F.avg_pool2d(x, kernel_size=3, stride=2, padding=1)\n            upsampled = F.interpolate(stage(x), size, mode=\'bilinear\', align_corners=True)\n            feats.append(upsampled)\n        return self.project(torch.cat(feats, dim=1))\n\n\ndef get_espnet(dataset=\'pascal_voc\', backbone=\'\', pretrained=False, root=\'~/.torch/models\',\n               pretrained_base=False, **kwargs):\n    acronyms = {\n        \'pascal_voc\': \'pascal_voc\',\n        \'pascal_aug\': \'pascal_aug\',\n        \'ade20k\': \'ade\',\n        \'coco\': \'coco\',\n        \'citys\': \'citys\',\n    }\n    from core.data.dataloader import datasets\n    model = ESPNetV2(datasets[dataset].NUM_CLASS, backbone=backbone, pretrained_base=pretrained_base, **kwargs)\n    if pretrained:\n        from .model_store import get_model_file\n        device = torch.device(kwargs[\'local_rank\'])\n        model.load_state_dict(torch.load(get_model_file(\'espnet_%s_%s\' % (backbone, acronyms[dataset]), root=root),\n                              map_location=device))\n    return model\n\n\ndef get_espnet_citys(**kwargs):\n    return get_espnet(\'citys\', **kwargs)\n\n\nif __name__ == \'__main__\':\n    model = get_espnet_citys()\n'"
core/models/fcn.py,8,"b'import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .base_models.vgg import vgg16\n\n__all__ = [\'get_fcn32s\', \'get_fcn16s\', \'get_fcn8s\',\n           \'get_fcn32s_vgg16_voc\', \'get_fcn16s_vgg16_voc\', \'get_fcn8s_vgg16_voc\']\n\n\nclass FCN32s(nn.Module):\n    """"""There are some difference from original fcn""""""\n\n    def __init__(self, nclass, backbone=\'vgg16\', aux=False, pretrained_base=True,\n                 norm_layer=nn.BatchNorm2d, **kwargs):\n        super(FCN32s, self).__init__()\n        self.aux = aux\n        if backbone == \'vgg16\':\n            self.pretrained = vgg16(pretrained=pretrained_base).features\n        else:\n            raise RuntimeError(\'unknown backbone: {}\'.format(backbone))\n        self.head = _FCNHead(512, nclass, norm_layer)\n        if aux:\n            self.auxlayer = _FCNHead(512, nclass, norm_layer)\n\n        self.__setattr__(\'exclusive\', [\'head\', \'auxlayer\'] if aux else [\'head\'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        pool5 = self.pretrained(x)\n\n        outputs = []\n        out = self.head(pool5)\n        out = F.interpolate(out, size, mode=\'bilinear\', align_corners=True)\n        outputs.append(out)\n\n        if self.aux:\n            auxout = self.auxlayer(pool5)\n            auxout = F.interpolate(auxout, size, mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout)\n\n        return tuple(outputs)\n\n\nclass FCN16s(nn.Module):\n    def __init__(self, nclass, backbone=\'vgg16\', aux=False, pretrained_base=True, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(FCN16s, self).__init__()\n        self.aux = aux\n        if backbone == \'vgg16\':\n            self.pretrained = vgg16(pretrained=pretrained_base).features\n        else:\n            raise RuntimeError(\'unknown backbone: {}\'.format(backbone))\n        self.pool4 = nn.Sequential(*self.pretrained[:24])\n        self.pool5 = nn.Sequential(*self.pretrained[24:])\n        self.head = _FCNHead(512, nclass, norm_layer)\n        self.score_pool4 = nn.Conv2d(512, nclass, 1)\n        if aux:\n            self.auxlayer = _FCNHead(512, nclass, norm_layer)\n\n        self.__setattr__(\'exclusive\', [\'head\', \'score_pool4\', \'auxlayer\'] if aux else [\'head\', \'score_pool4\'])\n\n    def forward(self, x):\n        pool4 = self.pool4(x)\n        pool5 = self.pool5(pool4)\n\n        outputs = []\n        score_fr = self.head(pool5)\n\n        score_pool4 = self.score_pool4(pool4)\n\n        upscore2 = F.interpolate(score_fr, score_pool4.size()[2:], mode=\'bilinear\', align_corners=True)\n        fuse_pool4 = upscore2 + score_pool4\n\n        out = F.interpolate(fuse_pool4, x.size()[2:], mode=\'bilinear\', align_corners=True)\n        outputs.append(out)\n\n        if self.aux:\n            auxout = self.auxlayer(pool5)\n            auxout = F.interpolate(auxout, x.size()[2:], mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout)\n\n        return tuple(outputs)\n\n\nclass FCN8s(nn.Module):\n    def __init__(self, nclass, backbone=\'vgg16\', aux=False, pretrained_base=True, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(FCN8s, self).__init__()\n        self.aux = aux\n        if backbone == \'vgg16\':\n            self.pretrained = vgg16(pretrained=pretrained_base).features\n        else:\n            raise RuntimeError(\'unknown backbone: {}\'.format(backbone))\n        self.pool3 = nn.Sequential(*self.pretrained[:17])\n        self.pool4 = nn.Sequential(*self.pretrained[17:24])\n        self.pool5 = nn.Sequential(*self.pretrained[24:])\n        self.head = _FCNHead(512, nclass, norm_layer)\n        self.score_pool3 = nn.Conv2d(256, nclass, 1)\n        self.score_pool4 = nn.Conv2d(512, nclass, 1)\n        if aux:\n            self.auxlayer = _FCNHead(512, nclass, norm_layer)\n\n        self.__setattr__(\'exclusive\',\n                         [\'head\', \'score_pool3\', \'score_pool4\', \'auxlayer\'] if aux else [\'head\', \'score_pool3\',\n                                                                                         \'score_pool4\'])\n\n    def forward(self, x):\n        pool3 = self.pool3(x)\n        pool4 = self.pool4(pool3)\n        pool5 = self.pool5(pool4)\n\n        outputs = []\n        score_fr = self.head(pool5)\n\n        score_pool4 = self.score_pool4(pool4)\n        score_pool3 = self.score_pool3(pool3)\n\n        upscore2 = F.interpolate(score_fr, score_pool4.size()[2:], mode=\'bilinear\', align_corners=True)\n        fuse_pool4 = upscore2 + score_pool4\n\n        upscore_pool4 = F.interpolate(fuse_pool4, score_pool3.size()[2:], mode=\'bilinear\', align_corners=True)\n        fuse_pool3 = upscore_pool4 + score_pool3\n\n        out = F.interpolate(fuse_pool3, x.size()[2:], mode=\'bilinear\', align_corners=True)\n        outputs.append(out)\n\n        if self.aux:\n            auxout = self.auxlayer(pool5)\n            auxout = F.interpolate(auxout, x.size()[2:], mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout)\n\n        return tuple(outputs)\n\n\nclass _FCNHead(nn.Module):\n    def __init__(self, in_channels, channels, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(_FCNHead, self).__init__()\n        inter_channels = in_channels // 4\n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n            norm_layer(inter_channels),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.1),\n            nn.Conv2d(inter_channels, channels, 1)\n        )\n\n    def forward(self, x):\n        return self.block(x)\n\n\ndef get_fcn32s(dataset=\'pascal_voc\', backbone=\'vgg16\', pretrained=False, root=\'~/.torch/models\',\n               pretrained_base=True, **kwargs):\n    acronyms = {\n        \'pascal_voc\': \'pascal_voc\',\n        \'pascal_aug\': \'pascal_aug\',\n        \'ade20k\': \'ade\',\n        \'coco\': \'coco\',\n        \'citys\': \'citys\',\n    }\n    from ..data.dataloader import datasets\n    model = FCN32s(datasets[dataset].NUM_CLASS, backbone=backbone, pretrained_base=pretrained_base, **kwargs)\n    if pretrained:\n        from .model_store import get_model_file\n        device = torch.device(kwargs[\'local_rank\'])\n        model.load_state_dict(torch.load(get_model_file(\'fcn32s_%s_%s\' % (backbone, acronyms[dataset]), root=root),\n                              map_location=device))\n    return model\n\n\ndef get_fcn16s(dataset=\'pascal_voc\', backbone=\'vgg16\', pretrained=False, root=\'~/.torch/models\',\n               pretrained_base=True, **kwargs):\n    acronyms = {\n        \'pascal_voc\': \'pascal_voc\',\n        \'pascal_aug\': \'pascal_aug\',\n        \'ade20k\': \'ade\',\n        \'coco\': \'coco\',\n        \'citys\': \'citys\',\n    }\n    from ..data.dataloader import datasets\n    model = FCN16s(datasets[dataset].NUM_CLASS, backbone=backbone, pretrained_base=pretrained_base, **kwargs)\n    if pretrained:\n        from .model_store import get_model_file\n        device = torch.device(kwargs[\'local_rank\'])\n        model.load_state_dict(torch.load(get_model_file(\'fcn16s_%s_%s\' % (backbone, acronyms[dataset]), root=root),\n                              map_location=device))\n    return model\n\n\ndef get_fcn8s(dataset=\'pascal_voc\', backbone=\'vgg16\', pretrained=False, root=\'~/.torch/models\',\n              pretrained_base=True, **kwargs):\n    acronyms = {\n        \'pascal_voc\': \'pascal_voc\',\n        \'pascal_aug\': \'pascal_aug\',\n        \'ade20k\': \'ade\',\n        \'coco\': \'coco\',\n        \'citys\': \'citys\',\n    }\n    from ..data.dataloader import datasets\n    model = FCN8s(datasets[dataset].NUM_CLASS, backbone=backbone, pretrained_base=pretrained_base, **kwargs)\n    if pretrained:\n        from .model_store import get_model_file\n        device = torch.device(kwargs[\'local_rank\'])\n        model.load_state_dict(torch.load(get_model_file(\'fcn8s_%s_%s\' % (backbone, acronyms[dataset]), root=root),\n                              map_location=device))\n    return model\n\n\ndef get_fcn32s_vgg16_voc(**kwargs):\n    return get_fcn32s(\'pascal_voc\', \'vgg16\', **kwargs)\n\n\ndef get_fcn16s_vgg16_voc(**kwargs):\n    return get_fcn16s(\'pascal_voc\', \'vgg16\', **kwargs)\n\n\ndef get_fcn8s_vgg16_voc(**kwargs):\n    return get_fcn8s(\'pascal_voc\', \'vgg16\', **kwargs)\n\n\nif __name__ == \'__main__\':\n    model = FCN16s(21)\n    print(model)\n'"
core/models/fcnv2.py,4,"b'""""""Fully Convolutional Network with Stride of 8""""""\nfrom __future__ import division\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .segbase import SegBaseModel\n\n__all__ = [\'FCN\', \'get_fcn\', \'get_fcn_resnet50_voc\',\n           \'get_fcn_resnet101_voc\', \'get_fcn_resnet152_voc\']\n\n\nclass FCN(SegBaseModel):\n    def __init__(self, nclass, backbone=\'resnet50\', aux=True, pretrained_base=True, **kwargs):\n        super(FCN, self).__init__(nclass, aux, backbone, pretrained_base=pretrained_base, **kwargs)\n        self.head = _FCNHead(2048, nclass, **kwargs)\n        if aux:\n            self.auxlayer = _FCNHead(1024, nclass, **kwargs)\n\n        self.__setattr__(\'exclusive\', [\'head\', \'auxlayer\'] if aux else [\'head\'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        _, _, c3, c4 = self.base_forward(x)\n\n        outputs = []\n        x = self.head(c4)\n        x = F.interpolate(x, size, mode=\'bilinear\', align_corners=True)\n        outputs.append(x)\n        if self.aux:\n            auxout = self.auxlayer(c3)\n            auxout = F.interpolate(auxout, size, mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout)\n        return tuple(outputs)\n\n\nclass _FCNHead(nn.Module):\n    def __init__(self, in_channels, channels, norm_layer=nn.BatchNorm2d, norm_kwargs=None, **kwargs):\n        super(_FCNHead, self).__init__()\n        inter_channels = in_channels // 4\n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n            norm_layer(inter_channels, **({} if norm_kwargs is None else norm_kwargs)),\n            nn.ReLU(True),\n            nn.Dropout(0.1),\n            nn.Conv2d(inter_channels, channels, 1)\n        )\n\n    def forward(self, x):\n        return self.block(x)\n\n\ndef get_fcn(dataset=\'pascal_voc\', backbone=\'resnet50\', pretrained=False, root=\'~/.torch/models\',\n            pretrained_base=True, **kwargs):\n    acronyms = {\n        \'pascal_voc\': \'pascal_voc\',\n        \'pascal_aug\': \'pascal_aug\',\n        \'ade20k\': \'ade\',\n        \'coco\': \'coco\',\n        \'citys\': \'citys\',\n    }\n    from ..data.dataloader import datasets\n    model = FCN(datasets[dataset].NUM_CLASS, backbone=backbone, pretrained_base=pretrained_base, **kwargs)\n    if pretrained:\n        from .model_store import get_model_file\n        device = torch.device(kwargs[\'local_rank\'])\n        model.load_state_dict(torch.load(get_model_file(\'fcn_%s_%s\' % (backbone, acronyms[dataset]), root=root),\n                              map_location=device))\n    return model\n\n\ndef get_fcn_resnet50_voc(**kwargs):\n    return get_fcn(\'pascal_voc\', \'resnet50\', **kwargs)\n\n\ndef get_fcn_resnet101_voc(**kwargs):\n    return get_fcn(\'pascal_voc\', \'resnet101\', **kwargs)\n\n\ndef get_fcn_resnet152_voc(**kwargs):\n    return get_fcn(\'pascal_voc\', \'resnet152\', **kwargs)\n'"
core/models/hrnet.py,2,"b'""""""High-Resolution Representations for Semantic Segmentation""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass HRNet(nn.Module):\n    """"""HRNet\n\n        Parameters\n    ----------\n    nclass : int\n        Number of categories for the training dataset.\n    backbone : string\n        Pre-trained dilated backbone network type (default:\'resnet50\'; \'resnet50\',\n        \'resnet101\' or \'resnet152\').\n    norm_layer : object\n        Normalization layer used in backbone network (default: :class:`nn.BatchNorm`;\n        for Synchronized Cross-GPU BachNormalization).\n    aux : bool\n        Auxiliary loss.\n    Reference:\n        Ke Sun. ""High-Resolution Representations for Labeling Pixels and Regions.""\n        arXiv preprint arXiv:1904.04514 (2019).\n    """"""\n    def __init__(self, nclass, backbone=\'\', aux=False, pretrained_base=False, **kwargs):\n        super(HRNet, self).__init__()\n\n    def forward(self, x):\n        pass'"
core/models/icnet.py,5,"b'""""""Image Cascade Network""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .segbase import SegBaseModel\n\n__all__ = [\'ICNet\', \'get_icnet\', \'get_icnet_resnet50_citys\',\n           \'get_icnet_resnet101_citys\', \'get_icnet_resnet152_citys\']\n\n\nclass ICNet(SegBaseModel):\n    """"""Image Cascade Network""""""\n\n    def __init__(self, nclass, backbone=\'resnet50\', aux=False, jpu=False, pretrained_base=True, **kwargs):\n        super(ICNet, self).__init__(nclass, aux, backbone, pretrained_base=pretrained_base, **kwargs)\n        self.conv_sub1 = nn.Sequential(\n            _ConvBNReLU(3, 32, 3, 2, **kwargs),\n            _ConvBNReLU(32, 32, 3, 2, **kwargs),\n            _ConvBNReLU(32, 64, 3, 2, **kwargs)\n        )\n\n        self.ppm = PyramidPoolingModule()\n\n        self.head = _ICHead(nclass, **kwargs)\n\n        self.__setattr__(\'exclusive\', [\'conv_sub1\', \'head\'])\n\n    def forward(self, x):\n        # sub 1\n        x_sub1 = self.conv_sub1(x)\n\n        # sub 2\n        x_sub2 = F.interpolate(x, scale_factor=0.5, mode=\'bilinear\', align_corners=True)\n        _, x_sub2, _, _ = self.base_forward(x_sub2)\n\n        # sub 4\n        x_sub4 = F.interpolate(x, scale_factor=0.25, mode=\'bilinear\', align_corners=True)\n        _, _, _, x_sub4 = self.base_forward(x_sub4)\n        # add PyramidPoolingModule\n        x_sub4 = self.ppm(x_sub4)   \n        outputs = self.head(x_sub1, x_sub2, x_sub4)\n\n        return tuple(outputs)\n\nclass PyramidPoolingModule(nn.Module):\n    def __init__(self, pyramids=[1,2,3,6]):\n        super(PyramidPoolingModule, self).__init__()\n        self.pyramids = pyramids\n\n    def forward(self, input):\n        feat = input\n        height, width = input.shape[2:]\n        for bin_size in self.pyramids:\n            x = F.adaptive_avg_pool2d(input, output_size=bin_size)\n            x = F.interpolate(x, size=(height, width), mode=\'bilinear\', align_corners=True)\n            feat  = feat + x\n        return feat\n\nclass _ICHead(nn.Module):\n    def __init__(self, nclass, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(_ICHead, self).__init__()\n        #self.cff_12 = CascadeFeatureFusion(512, 64, 128, nclass, norm_layer, **kwargs)\n        self.cff_12 = CascadeFeatureFusion(128, 64, 128, nclass, norm_layer, **kwargs)  \n        self.cff_24 = CascadeFeatureFusion(2048, 512, 128, nclass, norm_layer, **kwargs)\n\n        self.conv_cls = nn.Conv2d(128, nclass, 1, bias=False)\n\n    def forward(self, x_sub1, x_sub2, x_sub4):\n        outputs = list()\n        x_cff_24, x_24_cls = self.cff_24(x_sub4, x_sub2)\n        outputs.append(x_24_cls)\n        #x_cff_12, x_12_cls = self.cff_12(x_sub2, x_sub1)\n        x_cff_12, x_12_cls = self.cff_12(x_cff_24, x_sub1)  \n        outputs.append(x_12_cls)\n        \n        up_x2 = F.interpolate(x_cff_12, scale_factor=2, mode=\'bilinear\', align_corners=True)\n        up_x2 = self.conv_cls(up_x2)\n        outputs.append(up_x2)\n        up_x8 = F.interpolate(up_x2, scale_factor=4, mode=\'bilinear\', align_corners=True)\n        outputs.append(up_x8)\n        # 1 -> 1/4 -> 1/8 -> 1/16\n        outputs.reverse()\n\n        return outputs\n\n\nclass _ConvBNReLU(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, dilation=1,\n                 groups=1, norm_layer=nn.BatchNorm2d, bias=False, **kwargs):\n        super(_ConvBNReLU, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n        self.bn = norm_layer(out_channels)\n        self.relu = nn.ReLU(True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n\nclass CascadeFeatureFusion(nn.Module):\n    """"""CFF Unit""""""\n\n    def __init__(self, low_channels, high_channels, out_channels, nclass, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(CascadeFeatureFusion, self).__init__()\n        self.conv_low = nn.Sequential(\n            nn.Conv2d(low_channels, out_channels, 3, padding=2, dilation=2, bias=False),\n            norm_layer(out_channels)\n        )\n        self.conv_high = nn.Sequential(\n            nn.Conv2d(high_channels, out_channels, 1, bias=False),\n            norm_layer(out_channels)\n        )\n        self.conv_low_cls = nn.Conv2d(out_channels, nclass, 1, bias=False)\n\n    def forward(self, x_low, x_high):\n        x_low = F.interpolate(x_low, size=x_high.size()[2:], mode=\'bilinear\', align_corners=True)\n        x_low = self.conv_low(x_low)\n        x_high = self.conv_high(x_high)\n        x = x_low + x_high\n        x = F.relu(x, inplace=True)\n        x_low_cls = self.conv_low_cls(x_low)\n\n        return x, x_low_cls\n\n\ndef get_icnet(dataset=\'citys\', backbone=\'resnet50\', pretrained=False, root=\'~/.torch/models\',\n              pretrained_base=True, **kwargs):\n    acronyms = {\n        \'pascal_voc\': \'pascal_voc\',\n        \'pascal_aug\': \'pascal_aug\',\n        \'ade20k\': \'ade\',\n        \'coco\': \'coco\',\n        \'citys\': \'citys\',\n    }\n    from ..data.dataloader import datasets\n    model = ICNet(datasets[dataset].NUM_CLASS, backbone=backbone, pretrained_base=pretrained_base, **kwargs)\n    if pretrained:\n        from .model_store import get_model_file\n        device = torch.device(kwargs[\'local_rank\'])\n        model.load_state_dict(torch.load(get_model_file(\'icnet_%s_%s\' % (backbone, acronyms[dataset]), root=root),\n                              map_location=device))\n    return model\n\n\ndef get_icnet_resnet50_citys(**kwargs):\n    return get_icnet(\'citys\', \'resnet50\', **kwargs)\n\n\ndef get_icnet_resnet101_citys(**kwargs):\n    return get_icnet(\'citys\', \'resnet101\', **kwargs)\n\n\ndef get_icnet_resnet152_citys(**kwargs):\n    return get_icnet(\'citys\', \'resnet152\', **kwargs)\n\n\nif __name__ == \'__main__\':\n    img = torch.randn(1, 3, 256, 256)\n    model = get_icnet_resnet50_citys()\n    outputs = model(img)\n'"
core/models/lednet.py,7,"b'""""""LEDNet: A Lightweight Encoder-Decoder Network for Real-time Semantic Segmentation""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom core.nn import _ConvBNReLU\n\n__all__ = [\'LEDNet\', \'get_lednet\', \'get_lednet_citys\']\n\nclass LEDNet(nn.Module):\n    r""""""LEDNet\n\n    Parameters\n    ----------\n    nclass : int\n        Number of categories for the training dataset.\n    backbone : string\n        Pre-trained dilated backbone network type (default:\'resnet50\'; \'resnet50\',\n        \'resnet101\' or \'resnet152\').\n    norm_layer : object\n        Normalization layer used in backbone network (default: :class:`nn.BatchNorm`;\n        for Synchronized Cross-GPU BachNormalization).\n    aux : bool\n        Auxiliary loss.\n\n    Reference:\n        Yu Wang, et al. ""LEDNet: A Lightweight Encoder-Decoder Network for Real-Time Semantic Segmentation.""\n        arXiv preprint arXiv:1905.02423 (2019).\n    """"""\n\n    def __init__(self, nclass, backbone=\'\', aux=False, jpu=False, pretrained_base=True, **kwargs):\n        super(LEDNet, self).__init__()\n        self.encoder = nn.Sequential(\n            Downsampling(3, 32),\n            SSnbt(32, **kwargs), SSnbt(32, **kwargs), SSnbt(32, **kwargs),\n            Downsampling(32, 64),\n            SSnbt(64, **kwargs), SSnbt(64, **kwargs),\n            Downsampling(64, 128),\n            SSnbt(128, **kwargs),\n            SSnbt(128, 2, **kwargs),\n            SSnbt(128, 5, **kwargs),\n            SSnbt(128, 9, **kwargs),\n            SSnbt(128, 2, **kwargs),\n            SSnbt(128, 5, **kwargs),\n            SSnbt(128, 9, **kwargs),\n            SSnbt(128, 17, **kwargs),\n        )\n        self.decoder = APNModule(128, nclass)\n\n        self.__setattr__(\'exclusive\', [\'encoder\', \'decoder\'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        x = self.encoder(x)\n        x = self.decoder(x)\n        outputs = list()\n        x = F.interpolate(x, size, mode=\'bilinear\', align_corners=True)\n        outputs.append(x)\n\n        return tuple(outputs)\n\n\nclass Downsampling(nn.Module):\n    def __init__(self, in_channels, out_channels, **kwargs):\n        super(Downsampling, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels // 2, 3, 2, 2, bias=False)\n        self.conv2 = nn.Conv2d(in_channels, out_channels // 2, 3, 2, 2, bias=False)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=1)\n\n    def forward(self, x):\n        x1 = self.conv1(x)\n        x1 = self.pool(x1)\n\n        x2 = self.conv2(x)\n        x2 = self.pool(x2)\n\n        return torch.cat([x1, x2], dim=1)\n\n\nclass SSnbt(nn.Module):\n    def __init__(self, in_channels, dilation=1, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(SSnbt, self).__init__()\n        inter_channels = in_channels // 2\n        self.branch1 = nn.Sequential(\n            nn.Conv2d(inter_channels, inter_channels, (3, 1), padding=(1, 0), bias=False),\n            nn.ReLU(True),\n            nn.Conv2d(inter_channels, inter_channels, (1, 3), padding=(0, 1), bias=False),\n            norm_layer(inter_channels),\n            nn.ReLU(True),\n            nn.Conv2d(inter_channels, inter_channels, (3, 1), padding=(dilation, 0), dilation=(dilation, 1),\n                      bias=False),\n            nn.ReLU(True),\n            nn.Conv2d(inter_channels, inter_channels, (1, 3), padding=(0, dilation), dilation=(1, dilation),\n                      bias=False),\n            norm_layer(inter_channels),\n            nn.ReLU(True))\n\n        self.branch2 = nn.Sequential(\n            nn.Conv2d(inter_channels, inter_channels, (1, 3), padding=(0, 1), bias=False),\n            nn.ReLU(True),\n            nn.Conv2d(inter_channels, inter_channels, (3, 1), padding=(1, 0), bias=False),\n            norm_layer(inter_channels),\n            nn.ReLU(True),\n            nn.Conv2d(inter_channels, inter_channels, (1, 3), padding=(0, dilation), dilation=(1, dilation),\n                      bias=False),\n            nn.ReLU(True),\n            nn.Conv2d(inter_channels, inter_channels, (3, 1), padding=(dilation, 0), dilation=(dilation, 1),\n                      bias=False),\n            norm_layer(inter_channels),\n            nn.ReLU(True))\n\n        self.relu = nn.ReLU(True)\n\n    @staticmethod\n    def channel_shuffle(x, groups):\n        n, c, h, w = x.size()\n\n        channels_per_group = c // groups\n        x = x.view(n, groups, channels_per_group, h, w)\n        x = torch.transpose(x, 1, 2).contiguous()\n        x = x.view(n, -1, h, w)\n\n        return x\n\n    def forward(self, x):\n        # channels split\n        x1, x2 = x.split(x.size(1) // 2, 1)\n\n        x1 = self.branch1(x1)\n        x2 = self.branch2(x2)\n\n        out = torch.cat([x1, x2], dim=1)\n        out = self.relu(out + x)\n        out = self.channel_shuffle(out, groups=2)\n\n        return out\n\n\nclass APNModule(nn.Module):\n    def __init__(self, in_channels, nclass, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(APNModule, self).__init__()\n        self.conv1 = _ConvBNReLU(in_channels, in_channels, 3, 2, 1, norm_layer=norm_layer)\n        self.conv2 = _ConvBNReLU(in_channels, in_channels, 5, 2, 2, norm_layer=norm_layer)\n        self.conv3 = _ConvBNReLU(in_channels, in_channels, 7, 2, 3, norm_layer=norm_layer)\n        self.level1 = _ConvBNReLU(in_channels, nclass, 1, norm_layer=norm_layer)\n        self.level2 = _ConvBNReLU(in_channels, nclass, 1, norm_layer=norm_layer)\n        self.level3 = _ConvBNReLU(in_channels, nclass, 1, norm_layer=norm_layer)\n        self.level4 = _ConvBNReLU(in_channels, nclass, 1, norm_layer=norm_layer)\n        self.level5 = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            _ConvBNReLU(in_channels, nclass, 1))\n\n    def forward(self, x):\n        w, h = x.size()[2:]\n        branch3 = self.conv1(x)\n        branch2 = self.conv2(branch3)\n        branch1 = self.conv3(branch2)\n\n        out = self.level1(branch1)\n        out = F.interpolate(out, ((w + 3) // 4, (h + 3) // 4), mode=\'bilinear\', align_corners=True)\n        out = self.level2(branch2) + out\n        out = F.interpolate(out, ((w + 1) // 2, (h + 1) // 2), mode=\'bilinear\', align_corners=True)\n        out = self.level3(branch3) + out\n        out = F.interpolate(out, (w, h), mode=\'bilinear\', align_corners=True)\n        out = self.level4(x) * out\n        out = self.level5(x) + out\n        return out\n\n\ndef get_lednet(dataset=\'citys\', backbone=\'\', pretrained=False, root=\'~/.torch/models\',\n               pretrained_base=True, **kwargs):\n    acronyms = {\n        \'pascal_voc\': \'pascal_voc\',\n        \'pascal_aug\': \'pascal_aug\',\n        \'ade20k\': \'ade\',\n        \'coco\': \'coco\',\n        \'citys\': \'citys\',\n    }\n    from ..data.dataloader import datasets\n    model = LEDNet(datasets[dataset].NUM_CLASS, backbone=backbone, pretrained_base=pretrained_base, **kwargs)\n    if pretrained:\n        from .model_store import get_model_file\n        device = torch.device(kwargs[\'local_rank\'])\n        model.load_state_dict(torch.load(get_model_file(\'lednet_%s\' % (acronyms[dataset]), root=root),\n                              map_location=device))\n    return model\n\n\ndef get_lednet_citys(**kwargs):\n    return get_lednet(\'citys\', **kwargs)\n\n\nif __name__ == \'__main__\':\n    model = get_lednet_citys()\n'"
core/models/model_store.py,0,"b'""""""Model store which provides pretrained models.""""""\nfrom __future__ import print_function\n\nimport os\nimport zipfile\n\nfrom ..utils.download import download, check_sha1\n\n__all__ = [\'get_model_file\', \'get_resnet_file\']\n\n_model_sha1 = {name: checksum for checksum, name in [\n    (\'25c4b50959ef024fcc050213a06b614899f94b3d\', \'resnet50\'),\n    (\'2a57e44de9c853fa015b172309a1ee7e2d0e4e2a\', \'resnet101\'),\n    (\'0d43d698c66aceaa2bc0309f55efdd7ff4b143af\', \'resnet152\'),\n]}\n\nencoding_repo_url = \'https://hangzh.s3.amazonaws.com/\'\n_url_format = \'{repo_url}encoding/models/{file_name}.zip\'\n\n\ndef short_hash(name):\n    if name not in _model_sha1:\n        raise ValueError(\'Pretrained model for {name} is not available.\'.format(name=name))\n    return _model_sha1[name][:8]\n\n\ndef get_resnet_file(name, root=\'~/.torch/models\'):\n    file_name = \'{name}-{short_hash}\'.format(name=name, short_hash=short_hash(name))\n    root = os.path.expanduser(root)\n\n    file_path = os.path.join(root, file_name + \'.pth\')\n    sha1_hash = _model_sha1[name]\n    if os.path.exists(file_path):\n        if check_sha1(file_path, sha1_hash):\n            return file_path\n        else:\n            print(\'Mismatch in the content of model file {} detected.\' +\n                  \' Downloading again.\'.format(file_path))\n    else:\n        print(\'Model file {} is not found. Downloading.\'.format(file_path))\n\n    if not os.path.exists(root):\n        os.makedirs(root)\n\n    zip_file_path = os.path.join(root, file_name + \'.zip\')\n    repo_url = os.environ.get(\'ENCODING_REPO\', encoding_repo_url)\n    if repo_url[-1] != \'/\':\n        repo_url = repo_url + \'/\'\n    download(_url_format.format(repo_url=repo_url, file_name=file_name),\n             path=zip_file_path,\n             overwrite=True)\n    with zipfile.ZipFile(zip_file_path) as zf:\n        zf.extractall(root)\n    os.remove(zip_file_path)\n\n    if check_sha1(file_path, sha1_hash):\n        return file_path\n    else:\n        raise ValueError(\'Downloaded file has different hash. Please try again.\')\n\n\ndef get_model_file(name, root=\'~/.torch/models\'):\n    root = os.path.expanduser(root)\n    file_path = os.path.join(root, name + \'.pth\')\n    if os.path.exists(file_path):\n        return file_path\n    else:\n        raise ValueError(\'Model file is not found. Downloading or trainning.\')\n'"
core/models/model_zoo.py,0,"b'""""""Model store which handles pretrained models """"""\nfrom .fcn import *\nfrom .fcnv2 import *\nfrom .pspnet import *\nfrom .deeplabv3 import *\nfrom .deeplabv3_plus import *\nfrom .danet import *\nfrom .denseaspp import *\nfrom .bisenet import *\nfrom .encnet import *\nfrom .dunet import *\nfrom .icnet import *\nfrom .enet import *\nfrom .ocnet import *\nfrom .ccnet import *\nfrom .psanet import *\nfrom .cgnet import *\nfrom .espnet import *\nfrom .lednet import *\nfrom .dfanet import *\n\n__all__ = [\'get_model\', \'get_model_list\', \'get_segmentation_model\']\n\n_models = {\n    \'fcn32s_vgg16_voc\': get_fcn32s_vgg16_voc,\n    \'fcn16s_vgg16_voc\': get_fcn16s_vgg16_voc,\n    \'fcn8s_vgg16_voc\': get_fcn8s_vgg16_voc,\n    \'fcn_resnet50_voc\': get_fcn_resnet50_voc,\n    \'fcn_resnet101_voc\': get_fcn_resnet101_voc,\n    \'fcn_resnet152_voc\': get_fcn_resnet152_voc,\n    \'psp_resnet50_voc\': get_psp_resnet50_voc,\n    \'psp_resnet50_ade\': get_psp_resnet50_ade,\n    \'psp_resnet101_voc\': get_psp_resnet101_voc,\n    \'psp_resnet101_ade\': get_psp_resnet101_ade,\n    \'psp_resnet101_citys\': get_psp_resnet101_citys,\n    \'psp_resnet101_coco\': get_psp_resnet101_coco,\n    \'deeplabv3_resnet50_voc\': get_deeplabv3_resnet50_voc,\n    \'deeplabv3_resnet101_voc\': get_deeplabv3_resnet101_voc,\n    \'deeplabv3_resnet152_voc\': get_deeplabv3_resnet152_voc,\n    \'deeplabv3_resnet50_ade\': get_deeplabv3_resnet50_ade,\n    \'deeplabv3_resnet101_ade\': get_deeplabv3_resnet101_ade,\n    \'deeplabv3_resnet152_ade\': get_deeplabv3_resnet152_ade,\n    \'deeplabv3_plus_xception_voc\': get_deeplabv3_plus_xception_voc,\n    \'danet_resnet50_ciyts\': get_danet_resnet50_citys,\n    \'danet_resnet101_citys\': get_danet_resnet101_citys,\n    \'danet_resnet152_citys\': get_danet_resnet152_citys,\n    \'denseaspp_densenet121_citys\': get_denseaspp_densenet121_citys,\n    \'denseaspp_densenet161_citys\': get_denseaspp_densenet161_citys,\n    \'denseaspp_densenet169_citys\': get_denseaspp_densenet169_citys,\n    \'denseaspp_densenet201_citys\': get_denseaspp_densenet201_citys,\n    \'bisenet_resnet18_citys\': get_bisenet_resnet18_citys,\n    \'encnet_resnet50_ade\': get_encnet_resnet50_ade,\n    \'encnet_resnet101_ade\': get_encnet_resnet101_ade,\n    \'encnet_resnet152_ade\': get_encnet_resnet152_ade,\n    \'dunet_resnet50_pascal_voc\': get_dunet_resnet50_pascal_voc,\n    \'dunet_resnet101_pascal_voc\': get_dunet_resnet101_pascal_voc,\n    \'dunet_resnet152_pascal_voc\': get_dunet_resnet152_pascal_voc,\n    \'icnet_resnet50_citys\': get_icnet_resnet50_citys,\n    \'icnet_resnet101_citys\': get_icnet_resnet101_citys,\n    \'icnet_resnet152_citys\': get_icnet_resnet152_citys,\n    \'enet_citys\': get_enet_citys,\n    \'base_ocnet_resnet101_citys\': get_base_ocnet_resnet101_citys,\n    \'pyramid_ocnet_resnet101_citys\': get_pyramid_ocnet_resnet101_citys,\n    \'asp_ocnet_resnet101_citys\': get_asp_ocnet_resnet101_citys,\n    \'ccnet_resnet50_citys\': get_ccnet_resnet50_citys,\n    \'ccnet_resnet101_citys\': get_ccnet_resnet101_citys,\n    \'ccnet_resnet152_citys\': get_ccnet_resnet152_citys,\n    \'ccnet_resnet50_ade\': get_ccnet_resnet50_ade,\n    \'ccnet_resnet101_ade\': get_ccnet_resnet101_ade,\n    \'ccnet_resnet152_ade\': get_ccnet_resnet152_ade,\n    \'psanet_resnet50_voc\': get_psanet_resnet50_voc,\n    \'psanet_resnet101_voc\': get_psanet_resnet101_voc,\n    \'psanet_resnet152_voc\': get_psanet_resnet152_voc,\n    \'psanet_resnet50_citys\': get_psanet_resnet50_citys,\n    \'psanet_resnet101_citys\': get_psanet_resnet101_citys,\n    \'psanet_resnet152_citys\': get_psanet_resnet152_citys,\n    \'cgnet_citys\': get_cgnet_citys,\n    \'espnet_citys\': get_espnet_citys,\n    \'lednet_citys\': get_lednet_citys,\n    \'dfanet_citys\': get_dfanet_citys,\n}\n\n\ndef get_model(name, **kwargs):\n    name = name.lower()\n    if name not in _models:\n        err_str = \'""%s"" is not among the following model list:\\n\\t\' % (name)\n        err_str += \'%s\' % (\'\\n\\t\'.join(sorted(_models.keys())))\n        raise ValueError(err_str)\n    net = _models[name](**kwargs)\n    return net\n\n\ndef get_model_list():\n    return _models.keys()\n\n\ndef get_segmentation_model(model, **kwargs):\n    models = {\n        \'fcn32s\': get_fcn32s,\n        \'fcn16s\': get_fcn16s,\n        \'fcn8s\': get_fcn8s,\n        \'fcn\': get_fcn,\n        \'psp\': get_psp,\n        \'deeplabv3\': get_deeplabv3,\n        \'deeplabv3_plus\': get_deeplabv3_plus,\n        \'danet\': get_danet,\n        \'denseaspp\': get_denseaspp,\n        \'bisenet\': get_bisenet,\n        \'encnet\': get_encnet,\n        \'dunet\': get_dunet,\n        \'icnet\': get_icnet,\n        \'enet\': get_enet,\n        \'ocnet\': get_ocnet,\n        \'ccnet\': get_ccnet,\n        \'psanet\': get_psanet,\n        \'cgnet\': get_cgnet,\n        \'espnet\': get_espnet,\n        \'lednet\': get_lednet,\n        \'dfanet\': get_dfanet,\n    }\n    return models[model](**kwargs)\n'"
core/models/ocnet.py,14,"b'"""""" Object Context Network for Scene Parsing""""""\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nfrom .segbase import SegBaseModel\r\nfrom .fcn import _FCNHead\r\n\r\n__all__ = [\'OCNet\', \'get_ocnet\', \'get_base_ocnet_resnet101_citys\',\r\n           \'get_pyramid_ocnet_resnet101_citys\', \'get_asp_ocnet_resnet101_citys\']\r\n\r\n\r\nclass OCNet(SegBaseModel):\r\n    r""""""OCNet\r\n\r\n    Parameters\r\n    ----------\r\n    nclass : int\r\n        Number of categories for the training dataset.\r\n    backbone : string\r\n        Pre-trained dilated backbone network type (default:\'resnet50\'; \'resnet50\',\r\n        \'resnet101\' or \'resnet152\').\r\n    norm_layer : object\r\n        Normalization layer used in backbone network (default: :class:`nn.BatchNorm`;\r\n        for Synchronized Cross-GPU BachNormalization).\r\n    aux : bool\r\n        Auxiliary loss.\r\n    Reference:\r\n        Yuhui Yuan, Jingdong Wang. ""OCNet: Object Context Network for Scene Parsing.""\r\n        arXiv preprint arXiv:1809.00916 (2018).\r\n    """"""\r\n\r\n    def __init__(self, nclass, backbone=\'resnet101\', oc_arch=\'base\', aux=False, pretrained_base=True, **kwargs):\r\n        super(OCNet, self).__init__(nclass, aux, backbone, pretrained_base=pretrained_base, **kwargs)\r\n        self.head = _OCHead(nclass, oc_arch, **kwargs)\r\n        if self.aux:\r\n            self.auxlayer = _FCNHead(1024, nclass, **kwargs)\r\n\r\n        self.__setattr__(\'exclusive\', [\'head\', \'auxlayer\'] if aux else [\'head\'])\r\n\r\n    def forward(self, x):\r\n        size = x.size()[2:]\r\n        _, _, c3, c4 = self.base_forward(x)\r\n        outputs = []\r\n        x = self.head(c4)\r\n        x = F.interpolate(x, size, mode=\'bilinear\', align_corners=True)\r\n        outputs.append(x)\r\n\r\n        if self.aux:\r\n            auxout = self.auxlayer(c3)\r\n            auxout = F.interpolate(auxout, size, mode=\'bilinear\', align_corners=True)\r\n            outputs.append(auxout)\r\n        return tuple(outputs)\r\n\r\n\r\nclass _OCHead(nn.Module):\r\n    def __init__(self, nclass, oc_arch, norm_layer=nn.BatchNorm2d, **kwargs):\r\n        super(_OCHead, self).__init__()\r\n        if oc_arch == \'base\':\r\n            self.context = nn.Sequential(\r\n                nn.Conv2d(2048, 512, 3, 1, padding=1, bias=False),\r\n                norm_layer(512),\r\n                nn.ReLU(True),\r\n                BaseOCModule(512, 512, 256, 256, scales=([1]), norm_layer=norm_layer, **kwargs))\r\n        elif oc_arch == \'pyramid\':\r\n            self.context = nn.Sequential(\r\n                nn.Conv2d(2048, 512, 3, 1, padding=1, bias=False),\r\n                norm_layer(512),\r\n                nn.ReLU(True),\r\n                PyramidOCModule(512, 512, 256, 512, scales=([1, 2, 3, 6]), norm_layer=norm_layer, **kwargs))\r\n        elif oc_arch == \'asp\':\r\n            self.context = ASPOCModule(2048, 512, 256, 512, norm_layer=norm_layer, **kwargs)\r\n        else:\r\n            raise ValueError(""Unknown OC architecture!"")\r\n\r\n        self.out = nn.Conv2d(512, nclass, 1)\r\n\r\n    def forward(self, x):\r\n        x = self.context(x)\r\n        return self.out(x)\r\n\r\n\r\nclass BaseAttentionBlock(nn.Module):\r\n    """"""The basic implementation for self-attention block/non-local block.""""""\r\n\r\n    def __init__(self, in_channels, out_channels, key_channels, value_channels,\r\n                 scale=1, norm_layer=nn.BatchNorm2d, **kwargs):\r\n        super(BaseAttentionBlock, self).__init__()\r\n        self.scale = scale\r\n        self.key_channels = key_channels\r\n        self.value_channels = value_channels\r\n        if scale > 1:\r\n            self.pool = nn.MaxPool2d(scale)\r\n\r\n        self.f_value = nn.Conv2d(in_channels, value_channels, 1)\r\n        self.f_key = nn.Sequential(\r\n            nn.Conv2d(in_channels, key_channels, 1),\r\n            norm_layer(key_channels),\r\n            nn.ReLU(True)\r\n        )\r\n        self.f_query = self.f_key\r\n        self.W = nn.Conv2d(value_channels, out_channels, 1)\r\n        nn.init.constant_(self.W.weight, 0)\r\n        nn.init.constant_(self.W.bias, 0)\r\n\r\n    def forward(self, x):\r\n        batch_size, c, w, h = x.size()\r\n        if self.scale > 1:\r\n            x = self.pool(x)\r\n\r\n        value = self.f_value(x).view(batch_size, self.value_channels, -1).permute(0, 2, 1)\r\n        query = self.f_query(x).view(batch_size, self.key_channels, -1).permute(0, 2, 1)\r\n        key = self.f_key(x).view(batch_size, self.key_channels, -1)\r\n\r\n        sim_map = torch.bmm(query, key) * (self.key_channels ** -.5)\r\n        sim_map = F.softmax(sim_map, dim=-1)\r\n\r\n        context = torch.bmm(sim_map, value).permute(0, 2, 1).contiguous()\r\n        context = context.view(batch_size, self.value_channels, *x.size()[2:])\r\n        context = self.W(context)\r\n        if self.scale > 1:\r\n            context = F.interpolate(context, size=(w, h), mode=\'bilinear\', align_corners=True)\r\n\r\n        return context\r\n\r\n\r\nclass BaseOCModule(nn.Module):\r\n    """"""Base-OC""""""\r\n\r\n    def __init__(self, in_channels, out_channels, key_channels, value_channels,\r\n                 scales=([1]), norm_layer=nn.BatchNorm2d, concat=True, **kwargs):\r\n        super(BaseOCModule, self).__init__()\r\n        self.stages = nn.ModuleList([\r\n            BaseAttentionBlock(in_channels, out_channels, key_channels, value_channels, scale, norm_layer, **kwargs)\r\n            for scale in scales])\r\n        in_channels = in_channels * 2 if concat else in_channels\r\n        self.project = nn.Sequential(\r\n            nn.Conv2d(in_channels, out_channels, 1),\r\n            norm_layer(out_channels),\r\n            nn.ReLU(True),\r\n            nn.Dropout2d(0.05)\r\n        )\r\n        self.concat = concat\r\n\r\n    def forward(self, x):\r\n        priors = [stage(x) for stage in self.stages]\r\n        context = priors[0]\r\n        for i in range(1, len(priors)):\r\n            context += priors[i]\r\n        if self.concat:\r\n            context = torch.cat([context, x], 1)\r\n        out = self.project(context)\r\n        return out\r\n\r\n\r\nclass PyramidAttentionBlock(nn.Module):\r\n    """"""The basic implementation for pyramid self-attention block/non-local block""""""\r\n\r\n    def __init__(self, in_channels, out_channels, key_channels, value_channels,\r\n                 scale=1, norm_layer=nn.BatchNorm2d, **kwargs):\r\n        super(PyramidAttentionBlock, self).__init__()\r\n        self.scale = scale\r\n        self.value_channels = value_channels\r\n        self.key_channels = key_channels\r\n\r\n        self.f_value = nn.Conv2d(in_channels, value_channels, 1)\r\n        self.f_key = nn.Sequential(\r\n            nn.Conv2d(in_channels, key_channels, 1),\r\n            norm_layer(key_channels),\r\n            nn.ReLU(True)\r\n        )\r\n        self.f_query = self.f_key\r\n        self.W = nn.Conv2d(value_channels, out_channels, 1)\r\n        nn.init.constant_(self.W.weight, 0)\r\n        nn.init.constant_(self.W.bias, 0)\r\n\r\n    def forward(self, x):\r\n        batch_size, c, w, h = x.size()\r\n\r\n        local_x = list()\r\n        local_y = list()\r\n        step_w, step_h = w // self.scale, h // self.scale\r\n        for i in range(self.scale):\r\n            for j in range(self.scale):\r\n                start_x, start_y = step_w * i, step_h * j\r\n                end_x, end_y = min(start_x + step_w, w), min(start_y + step_h, h)\r\n                if i == (self.scale - 1):\r\n                    end_x = w\r\n                if j == (self.scale - 1):\r\n                    end_y = h\r\n                local_x += [start_x, end_x]\r\n                local_y += [start_y, end_y]\r\n\r\n        value = self.f_value(x)\r\n        query = self.f_query(x)\r\n        key = self.f_key(x)\r\n\r\n        local_list = list()\r\n        local_block_cnt = (self.scale ** 2) * 2\r\n        for i in range(0, local_block_cnt, 2):\r\n            value_local = value[:, :, local_x[i]:local_x[i + 1], local_y[i]:local_y[i + 1]]\r\n            query_local = query[:, :, local_x[i]:local_x[i + 1], local_y[i]:local_y[i + 1]]\r\n            key_local = key[:, :, local_x[i]:local_x[i + 1], local_y[i]:local_y[i + 1]]\r\n\r\n            w_local, h_local = value_local.size(2), value_local.size(3)\r\n            value_local = value_local.contiguous().view(batch_size, self.value_channels, -1).permute(0, 2, 1)\r\n            query_local = query_local.contiguous().view(batch_size, self.key_channels, -1).permute(0, 2, 1)\r\n            key_local = key_local.contiguous().view(batch_size, self.key_channels, -1)\r\n\r\n            sim_map = torch.bmm(query_local, key_local) * (self.key_channels ** -.5)\r\n            sim_map = F.softmax(sim_map, dim=-1)\r\n\r\n            context_local = torch.bmm(sim_map, value_local).permute(0, 2, 1).contiguous()\r\n            context_local = context_local.view(batch_size, self.value_channels, w_local, h_local)\r\n            local_list.append(context_local)\r\n\r\n        context_list = list()\r\n        for i in range(0, self.scale):\r\n            row_tmp = list()\r\n            for j in range(self.scale):\r\n                row_tmp.append(local_list[j + i * self.scale])\r\n            context_list.append(torch.cat(row_tmp, 3))\r\n\r\n        context = torch.cat(context_list, 2)\r\n        context = self.W(context)\r\n\r\n        return context\r\n\r\n\r\nclass PyramidOCModule(nn.Module):\r\n    """"""Pyramid-OC""""""\r\n\r\n    def __init__(self, in_channels, out_channels, key_channels, value_channels,\r\n                 scales=([1]), norm_layer=nn.BatchNorm2d, **kwargs):\r\n        super(PyramidOCModule, self).__init__()\r\n        self.stages = nn.ModuleList([\r\n            PyramidAttentionBlock(in_channels, out_channels, key_channels, value_channels, scale, norm_layer, **kwargs)\r\n            for scale in scales])\r\n        self.up_dr = nn.Sequential(\r\n            nn.Conv2d(in_channels, in_channels * len(scales), 1),\r\n            norm_layer(in_channels * len(scales)),\r\n            nn.ReLU(True)\r\n        )\r\n        self.project = nn.Sequential(\r\n            nn.Conv2d(in_channels * len(scales) * 2, out_channels, 1),\r\n            norm_layer(out_channels),\r\n            nn.ReLU(True),\r\n            nn.Dropout2d(0.05)\r\n        )\r\n\r\n    def forward(self, x):\r\n        priors = [stage(x) for stage in self.stages]\r\n        context = [self.up_dr(x)]\r\n        for i in range(len(priors)):\r\n            context += [priors[i]]\r\n        context = torch.cat(context, 1)\r\n        out = self.project(context)\r\n        return out\r\n\r\n\r\nclass ASPOCModule(nn.Module):\r\n    """"""ASP-OC""""""\r\n\r\n    def __init__(self, in_channels, out_channels, key_channels, value_channels,\r\n                 atrous_rates=(12, 24, 36), norm_layer=nn.BatchNorm2d, **kwargs):\r\n        super(ASPOCModule, self).__init__()\r\n        self.context = nn.Sequential(\r\n            nn.Conv2d(in_channels, out_channels, 3, padding=1),\r\n            norm_layer(out_channels),\r\n            nn.ReLU(True),\r\n            BaseOCModule(out_channels, out_channels, key_channels, value_channels, ([2]), norm_layer, False, **kwargs))\r\n\r\n        rate1, rate2, rate3 = tuple(atrous_rates)\r\n        self.b1 = nn.Sequential(\r\n            nn.Conv2d(in_channels, out_channels, 3, padding=rate1, dilation=rate1, bias=False),\r\n            norm_layer(out_channels),\r\n            nn.ReLU(True))\r\n        self.b2 = nn.Sequential(\r\n            nn.Conv2d(in_channels, out_channels, 3, padding=rate2, dilation=rate2, bias=False),\r\n            norm_layer(out_channels),\r\n            nn.ReLU(True))\r\n        self.b3 = nn.Sequential(\r\n            nn.Conv2d(in_channels, out_channels, 3, padding=rate3, dilation=rate3, bias=False),\r\n            norm_layer(out_channels),\r\n            nn.ReLU(True))\r\n        self.b4 = nn.Sequential(\r\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\r\n            norm_layer(out_channels),\r\n            nn.ReLU(True))\r\n\r\n        self.project = nn.Sequential(\r\n            nn.Conv2d(out_channels * 5, out_channels, 1, bias=False),\r\n            norm_layer(out_channels),\r\n            nn.ReLU(True),\r\n            nn.Dropout2d(0.1)\r\n        )\r\n\r\n    def forward(self, x):\r\n        feat1 = self.context(x)\r\n        feat2 = self.b1(x)\r\n        feat3 = self.b2(x)\r\n        feat4 = self.b3(x)\r\n        feat5 = self.b4(x)\r\n        out = torch.cat((feat1, feat2, feat3, feat4, feat5), dim=1)\r\n        out = self.project(out)\r\n        return out\r\n\r\n\r\ndef get_ocnet(dataset=\'citys\', backbone=\'resnet50\', oc_arch=\'base\', pretrained=False, root=\'~/.torch/models\',\r\n              pretrained_base=True, **kwargs):\r\n    acronyms = {\r\n        \'pascal_voc\': \'pascal_voc\',\r\n        \'pascal_aug\': \'pascal_aug\',\r\n        \'ade20k\': \'ade\',\r\n        \'coco\': \'coco\',\r\n        \'citys\': \'citys\',\r\n    }\r\n    from ..data.dataloader import datasets\r\n    model = OCNet(datasets[dataset].NUM_CLASS, backbone=backbone, oc_arch=oc_arch,\r\n                  pretrained_base=pretrained_base, **kwargs)\r\n    if pretrained:\r\n        from .model_store import get_model_file\r\n        device = torch.device(kwargs[\'local_rank\'])\r\n        model.load_state_dict(torch.load(get_model_file(\'%s_ocnet_%s_%s\' % (\r\n            oc_arch, backbone, acronyms[dataset]), root=root),\r\n            map_location=device))\r\n    return model\r\n\r\n\r\ndef get_base_ocnet_resnet101_citys(**kwargs):\r\n    return get_ocnet(\'citys\', \'resnet101\', \'base\', **kwargs)\r\n\r\n\r\ndef get_pyramid_ocnet_resnet101_citys(**kwargs):\r\n    return get_ocnet(\'citys\', \'resnet101\', \'pyramid\', **kwargs)\r\n\r\n\r\ndef get_asp_ocnet_resnet101_citys(**kwargs):\r\n    return get_ocnet(\'citys\', \'resnet101\', \'asp\', **kwargs)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    img = torch.randn(1, 3, 256, 256)\r\n    model = get_asp_ocnet_resnet101_citys()\r\n    outputs = model(img)\r\n'"
core/models/psanet.py,8,"b'""""""Point-wise Spatial Attention Network""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom core.nn import _ConvBNReLU\nfrom core.models.segbase import SegBaseModel\nfrom core.models.fcn import _FCNHead\n\n__all__ = [\'PSANet\', \'get_psanet\', \'get_psanet_resnet50_voc\', \'get_psanet_resnet101_voc\',\n           \'get_psanet_resnet152_voc\', \'get_psanet_resnet50_citys\', \'get_psanet_resnet101_citys\',\n           \'get_psanet_resnet152_citys\']\n\n\nclass PSANet(SegBaseModel):\n    r""""""PSANet\n\n    Parameters\n    ----------\n    nclass : int\n        Number of categories for the training dataset.\n    backbone : string\n        Pre-trained dilated backbone network type (default:\'resnet50\'; \'resnet50\',\n        \'resnet101\' or \'resnet152\').\n    norm_layer : object\n        Normalization layer used in backbone network (default: :class:`nn.BatchNorm`;\n        for Synchronized Cross-GPU BachNormalization).\n    aux : bool\n        Auxiliary loss.\n\n    Reference:\n        Hengshuang Zhao, et al. ""PSANet: Point-wise Spatial Attention Network for Scene Parsing.""\n        ECCV-2018.\n    """"""\n\n    def __init__(self, nclass, backbone=\'resnet\', aux=False, pretrained_base=True, **kwargs):\n        super(PSANet, self).__init__(nclass, aux, backbone, pretrained_base=pretrained_base, **kwargs)\n        self.head = _PSAHead(nclass, **kwargs)\n        if aux:\n            self.auxlayer = _FCNHead(1024, nclass, **kwargs)\n\n        self.__setattr__(\'exclusive\', [\'head\', \'auxlayer\'] if aux else [\'head\'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        _, _, c3, c4 = self.base_forward(x)\n        outputs = list()\n        x = self.head(c4)\n        x = F.interpolate(x, size, mode=\'bilinear\', align_corners=True)\n        outputs.append(x)\n\n        if self.aux:\n            auxout = self.auxlayer(c3)\n            auxout = F.interpolate(auxout, size, mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout)\n        return tuple(outputs)\n\n\nclass _PSAHead(nn.Module):\n    def __init__(self, nclass, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(_PSAHead, self).__init__()\n        # psa_out_channels = crop_size // 8 ** 2\n        self.psa = _PointwiseSpatialAttention(2048, 3600, norm_layer)\n\n        self.conv_post = _ConvBNReLU(1024, 2048, 1, norm_layer=norm_layer)\n        self.project = nn.Sequential(\n            _ConvBNReLU(4096, 512, 3, padding=1, norm_layer=norm_layer),\n            nn.Dropout2d(0.1, False),\n            nn.Conv2d(512, nclass, 1))\n\n    def forward(self, x):\n        global_feature = self.psa(x)\n        out = self.conv_post(global_feature)\n        out = torch.cat([x, out], dim=1)\n        out = self.project(out)\n\n        return out\n\n\nclass _PointwiseSpatialAttention(nn.Module):\n    def __init__(self, in_channels, out_channels, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(_PointwiseSpatialAttention, self).__init__()\n        reduced_channels = 512\n        self.collect_attention = _AttentionGeneration(in_channels, reduced_channels, out_channels, norm_layer)\n        self.distribute_attention = _AttentionGeneration(in_channels, reduced_channels, out_channels, norm_layer)\n\n    def forward(self, x):\n        collect_fm = self.collect_attention(x)\n        distribute_fm = self.distribute_attention(x)\n        psa_fm = torch.cat([collect_fm, distribute_fm], dim=1)\n        return psa_fm\n\n\nclass _AttentionGeneration(nn.Module):\n    def __init__(self, in_channels, reduced_channels, out_channels, norm_layer, **kwargs):\n        super(_AttentionGeneration, self).__init__()\n        self.conv_reduce = _ConvBNReLU(in_channels, reduced_channels, 1, norm_layer=norm_layer)\n        self.attention = nn.Sequential(\n            _ConvBNReLU(reduced_channels, reduced_channels, 1, norm_layer=norm_layer),\n            nn.Conv2d(reduced_channels, out_channels, 1, bias=False))\n\n        self.reduced_channels = reduced_channels\n\n    def forward(self, x):\n        reduce_x = self.conv_reduce(x)\n        attention = self.attention(reduce_x)\n        n, c, h, w = attention.size()\n        attention = attention.view(n, c, -1)\n        reduce_x = reduce_x.view(n, self.reduced_channels, -1)\n        fm = torch.bmm(reduce_x, torch.softmax(attention, dim=1))\n        fm = fm.view(n, self.reduced_channels, h, w)\n\n        return fm\n\n\ndef get_psanet(dataset=\'pascal_voc\', backbone=\'resnet50\', pretrained=False, root=\'~/.torch/models\',\n               pretrained_base=True, **kwargs):\n    acronyms = {\n        \'pascal_voc\': \'pascal_voc\',\n        \'pascal_aug\': \'pascal_aug\',\n        \'ade20k\': \'ade\',\n        \'coco\': \'coco\',\n        \'citys\': \'citys\',\n    }\n    from core.data.dataloader import datasets\n    model = PSANet(datasets[dataset].NUM_CLASS, backbone=backbone, pretrained_base=pretrained_base, **kwargs)\n    if pretrained:\n        from .model_store import get_model_file\n        device = torch.device(kwargs[\'local_rank\'])\n        model.load_state_dict(torch.load(get_model_file(\'deeplabv3_%s_%s\' % (backbone, acronyms[dataset]), root=root),\n                              map_location=device))\n    return model\n\n\ndef get_psanet_resnet50_voc(**kwargs):\n    return get_psanet(\'pascal_voc\', \'resnet50\', **kwargs)\n\n\ndef get_psanet_resnet101_voc(**kwargs):\n    return get_psanet(\'pascal_voc\', \'resnet101\', **kwargs)\n\n\ndef get_psanet_resnet152_voc(**kwargs):\n    return get_psanet(\'pascal_voc\', \'resnet152\', **kwargs)\n\n\ndef get_psanet_resnet50_citys(**kwargs):\n    return get_psanet(\'citys\', \'resnet50\', **kwargs)\n\n\ndef get_psanet_resnet101_citys(**kwargs):\n    return get_psanet(\'citys\', \'resnet101\', **kwargs)\n\n\ndef get_psanet_resnet152_citys(**kwargs):\n    return get_psanet(\'citys\', \'resnet152\', **kwargs)\n\n\nif __name__ == \'__main__\':\n    model = get_psanet_resnet50_voc()\n    img = torch.randn(1, 3, 480, 480)\n    output = model(img)\n'"
core/models/psanet_old.py,11,"b'""""""Point-wise Spatial Attention Network""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom core.nn import CollectAttention, DistributeAttention\nfrom .segbase import SegBaseModel\nfrom .fcn import _FCNHead\n\n__all__ = [\'PSANet\', \'get_psanet\', \'get_psanet_resnet50_voc\', \'get_psanet_resnet101_voc\',\n           \'get_psanet_resnet152_voc\', \'get_psanet_resnet50_citys\', \'get_psanet_resnet101_citys\',\n           \'get_psanet_resnet152_citys\']\n\n\nclass PSANet(SegBaseModel):\n    r""""""PSANet\n\n    Parameters\n    ----------\n    nclass : int\n        Number of categories for the training dataset.\n    backbone : string\n        Pre-trained dilated backbone network type (default:\'resnet50\'; \'resnet50\',\n        \'resnet101\' or \'resnet152\').\n    norm_layer : object\n        Normalization layer used in backbone network (default: :class:`nn.BatchNorm`;\n        for Synchronized Cross-GPU BachNormalization).\n    aux : bool\n        Auxiliary loss.\n\n    Reference:\n        Hengshuang Zhao, et al. ""PSANet: Point-wise Spatial Attention Network for Scene Parsing.""\n        ECCV-2018.\n    """"""\n\n    def __init__(self, nclass, backbone=\'resnet\', aux=False, pretrained_base=True, **kwargs):\n        super(PSANet, self).__init__(nclass, aux, backbone, pretrained_base, **kwargs)\n        self.head = _PSAHead(nclass, **kwargs)\n        if aux:\n            self.auxlayer = _FCNHead(1024, nclass, **kwargs)\n\n        self.__setattr__(\'exclusive\', [\'head\', \'auxlayer\'] if aux else [\'head\'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        _, _, c3, c4 = self.base_forward(x)\n        outputs = list()\n        x = self.head(c4)\n        x = F.interpolate(x, size, mode=\'bilinear\', align_corners=True)\n        outputs.append(x)\n\n        if self.aux:\n            auxout = self.auxlayer(c3)\n            auxout = F.interpolate(auxout, size, mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout)\n        return tuple(outputs)\n\n\nclass _PSAHead(nn.Module):\n    def __init__(self, nclass, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(_PSAHead, self).__init__()\n        self.collect = _CollectModule(2048, 512, 60, 60, norm_layer, **kwargs)\n        self.distribute = _DistributeModule(2048, 512, 60, 60, norm_layer, **kwargs)\n\n        self.conv_post = nn.Sequential(\n            nn.Conv2d(1024, 2048, 1, bias=False),\n            norm_layer(2048),\n            nn.ReLU(True))\n        self.project = nn.Sequential(\n            nn.Conv2d(4096, 512, 3, padding=1, bias=False),\n            norm_layer(512),\n            nn.ReLU(True),\n            nn.Conv2d(512, nclass, 1)\n        )\n\n    def forward(self, x):\n        global_feature_collect = self.collect(x)\n        global_feature_distribute = self.distribute(x)\n\n        global_feature = torch.cat([global_feature_collect, global_feature_distribute], dim=1)\n        out = self.conv_post(global_feature)\n        out = F.interpolate(out, scale_factor=2, mode=\'bilinear\', align_corners=True)\n        out = torch.cat([x, out], dim=1)\n        out = self.project(out)\n\n        return out\n\n\nclass _CollectModule(nn.Module):\n    def __init__(self, in_channels, reduced_channels, feat_w, feat_h, norm_layer, **kwargs):\n        super(_CollectModule, self).__init__()\n        self.conv_reduce = nn.Sequential(\n            nn.Conv2d(in_channels, reduced_channels, 1, bias=False),\n            norm_layer(reduced_channels),\n            nn.ReLU(True))\n        self.conv_adaption = nn.Sequential(\n            nn.Conv2d(reduced_channels, reduced_channels, 1, bias=False),\n            norm_layer(reduced_channels),\n            nn.ReLU(True),\n            nn.Conv2d(reduced_channels, (feat_w - 1) * (feat_h), 1, bias=False))\n        self.collect_attention = CollectAttention()\n\n        self.reduced_channels = reduced_channels\n        self.feat_w = feat_w\n        self.feat_h = feat_h\n\n    def forward(self, x):\n        x = self.conv_reduce(x)\n        # shrink\n        x_shrink = F.interpolate(x, scale_factor=1 / 2, mode=\'bilinear\', align_corners=True)\n        x_adaption = self.conv_adaption(x_shrink)\n        ca = self.collect_attention(x_adaption)\n        global_feature_collect_list = list()\n        for i in range(x_shrink.shape[0]):\n            x_shrink_i = x_shrink[i].view(self.reduced_channels, -1)\n            ca_i = ca[i].view(ca.shape[1], -1)\n            global_feature_collect_list.append(\n                torch.mm(x_shrink_i, ca_i).view(1, self.reduced_channels, self.feat_h // 2, self.feat_w // 2))\n        global_feature_collect = torch.cat(global_feature_collect_list)\n\n        return global_feature_collect\n\n\nclass _DistributeModule(nn.Module):\n    def __init__(self, in_channels, reduced_channels, feat_w, feat_h, norm_layer, **kwargs):\n        super(_DistributeModule, self).__init__()\n        self.conv_reduce = nn.Sequential(\n            nn.Conv2d(in_channels, reduced_channels, 1, bias=False),\n            norm_layer(reduced_channels),\n            nn.ReLU(True))\n        self.conv_adaption = nn.Sequential(\n            nn.Conv2d(reduced_channels, reduced_channels, 1, bias=False),\n            norm_layer(reduced_channels),\n            nn.ReLU(True),\n            nn.Conv2d(reduced_channels, (feat_w - 1) * (feat_h), 1, bias=False))\n        self.distribute_attention = DistributeAttention()\n\n        self.reduced_channels = reduced_channels\n        self.feat_w = feat_w\n        self.feat_h = feat_h\n\n    def forward(self, x):\n        x = self.conv_reduce(x)\n        x_shrink = F.interpolate(x, scale_factor=1 / 2, mode=\'bilinear\', align_corners=True)\n        x_adaption = self.conv_adaption(x_shrink)\n        da = self.distribute_attention(x_adaption)\n        global_feature_distribute_list = list()\n        for i in range(x_shrink.shape[0]):\n            x_shrink_i = x_shrink[i].view(self.reduced_channels, -1)\n            da_i = da[i].view(da.shape[1], -1)\n            global_feature_distribute_list.append(\n                torch.mm(x_shrink_i, da_i).view(1, self.reduced_channels, self.feat_h // 2, self.feat_w // 2))\n        global_feature_distribute = torch.cat(global_feature_distribute_list)\n\n        return global_feature_distribute\n\n\ndef get_psanet(dataset=\'pascal_voc\', backbone=\'resnet50\', pretrained=False, root=\'~/.torch/models\',\n               pretrained_base=True, **kwargs):\n    acronyms = {\n        \'pascal_voc\': \'pascal_voc\',\n        \'pascal_aug\': \'pascal_aug\',\n        \'ade20k\': \'ade\',\n        \'coco\': \'coco\',\n        \'citys\': \'citys\',\n    }\n    from ..data.dataloader import datasets\n    model = PSANet(datasets[dataset].NUM_CLASS, backbone=backbone, pretrained_base=pretrained_base, **kwargs)\n    if pretrained:\n        from .model_store import get_model_file\n        device = torch.device(kwargs[\'local_rank\'])\n        model.load_state_dict(torch.load(get_model_file(\'deeplabv3_%s_%s\' % (backbone, acronyms[dataset]), root=root),\n                              map_location=device))\n    return model\n\n\ndef get_psanet_resnet50_voc(**kwargs):\n    return get_psanet(\'pascal_voc\', \'resnet50\', **kwargs)\n\n\ndef get_psanet_resnet101_voc(**kwargs):\n    return get_psanet(\'pascal_voc\', \'resnet101\', **kwargs)\n\n\ndef get_psanet_resnet152_voc(**kwargs):\n    return get_psanet(\'pascal_voc\', \'resnet152\', **kwargs)\n\n\ndef get_psanet_resnet50_citys(**kwargs):\n    return get_psanet(\'citys\', \'resnet50\', **kwargs)\n\n\ndef get_psanet_resnet101_citys(**kwargs):\n    return get_psanet(\'citys\', \'resnet101\', **kwargs)\n\n\ndef get_psanet_resnet152_citys(**kwargs):\n    return get_psanet(\'citys\', \'resnet152\', **kwargs)\n\n\nif __name__ == \'__main__\':\n    model = get_psanet_resnet50_voc()\n    img = torch.randn(1, 3, 480, 480)\n    output = model(img)\n'"
core/models/pspnet.py,6,"b'""""""Pyramid Scene Parsing Network""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .segbase import SegBaseModel\nfrom .fcn import _FCNHead\n\n__all__ = [\'PSPNet\', \'get_psp\', \'get_psp_resnet50_voc\', \'get_psp_resnet50_ade\', \'get_psp_resnet101_voc\',\n           \'get_psp_resnet101_ade\', \'get_psp_resnet101_citys\', \'get_psp_resnet101_coco\']\n\n\nclass PSPNet(SegBaseModel):\n    r""""""Pyramid Scene Parsing Network\n\n    Parameters\n    ----------\n    nclass : int\n        Number of categories for the training dataset.\n    backbone : string\n        Pre-trained dilated backbone network type (default:\'resnet50\'; \'resnet50\',\n        \'resnet101\' or \'resnet152\').\n    norm_layer : object\n        Normalization layer used in backbone network (default: :class:`nn.BatchNorm`;\n        for Synchronized Cross-GPU BachNormalization).\n    aux : bool\n        Auxiliary loss.\n\n    Reference:\n        Zhao, Hengshuang, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia.\n        ""Pyramid scene parsing network."" *CVPR*, 2017\n    """"""\n\n    def __init__(self, nclass, backbone=\'resnet50\', aux=False, pretrained_base=True, **kwargs):\n        super(PSPNet, self).__init__(nclass, aux, backbone, pretrained_base=pretrained_base, **kwargs)\n        self.head = _PSPHead(nclass, **kwargs)\n        if self.aux:\n            self.auxlayer = _FCNHead(1024, nclass, **kwargs)\n\n        self.__setattr__(\'exclusive\', [\'head\', \'auxlayer\'] if aux else [\'head\'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        _, _, c3, c4 = self.base_forward(x)\n        outputs = []\n        x = self.head(c4)\n        x = F.interpolate(x, size, mode=\'bilinear\', align_corners=True)\n        outputs.append(x)\n\n        if self.aux:\n            auxout = self.auxlayer(c3)\n            auxout = F.interpolate(auxout, size, mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout)\n        return tuple(outputs)\n\n\ndef _PSP1x1Conv(in_channels, out_channels, norm_layer, norm_kwargs):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, 1, bias=False),\n        norm_layer(out_channels, **({} if norm_kwargs is None else norm_kwargs)),\n        nn.ReLU(True)\n    )\n\n\nclass _PyramidPooling(nn.Module):\n    def __init__(self, in_channels, **kwargs):\n        super(_PyramidPooling, self).__init__()\n        out_channels = int(in_channels / 4)\n        self.avgpool1 = nn.AdaptiveAvgPool2d(1)\n        self.avgpool2 = nn.AdaptiveAvgPool2d(2)\n        self.avgpool3 = nn.AdaptiveAvgPool2d(3)\n        self.avgpool4 = nn.AdaptiveAvgPool2d(6)\n        self.conv1 = _PSP1x1Conv(in_channels, out_channels, **kwargs)\n        self.conv2 = _PSP1x1Conv(in_channels, out_channels, **kwargs)\n        self.conv3 = _PSP1x1Conv(in_channels, out_channels, **kwargs)\n        self.conv4 = _PSP1x1Conv(in_channels, out_channels, **kwargs)\n\n    def forward(self, x):\n        size = x.size()[2:]\n        feat1 = F.interpolate(self.conv1(self.avgpool1(x)), size, mode=\'bilinear\', align_corners=True)\n        feat2 = F.interpolate(self.conv2(self.avgpool2(x)), size, mode=\'bilinear\', align_corners=True)\n        feat3 = F.interpolate(self.conv3(self.avgpool3(x)), size, mode=\'bilinear\', align_corners=True)\n        feat4 = F.interpolate(self.conv4(self.avgpool4(x)), size, mode=\'bilinear\', align_corners=True)\n        return torch.cat([x, feat1, feat2, feat3, feat4], dim=1)\n\n\nclass _PSPHead(nn.Module):\n    def __init__(self, nclass, norm_layer=nn.BatchNorm2d, norm_kwargs=None, **kwargs):\n        super(_PSPHead, self).__init__()\n        self.psp = _PyramidPooling(2048, norm_layer=norm_layer, norm_kwargs=norm_kwargs)\n        self.block = nn.Sequential(\n            nn.Conv2d(4096, 512, 3, padding=1, bias=False),\n            norm_layer(512, **({} if norm_kwargs is None else norm_kwargs)),\n            nn.ReLU(True),\n            nn.Dropout(0.1),\n            nn.Conv2d(512, nclass, 1)\n        )\n\n    def forward(self, x):\n        x = self.psp(x)\n        return self.block(x)\n\n\ndef get_psp(dataset=\'pascal_voc\', backbone=\'resnet50\', pretrained=False, root=\'~/.torch/models\',\n            pretrained_base=True, **kwargs):\n    r""""""Pyramid Scene Parsing Network\n\n    Parameters\n    ----------\n    dataset : str, default pascal_voc\n        The dataset that model pretrained on. (pascal_voc, ade20k)\n    pretrained : bool or str\n        Boolean value controls whether to load the default pretrained weights for model.\n        String value represents the hashtag for a certain version of pretrained weights.\n    root : str, default \'~/.torch/models\'\n        Location for keeping the model parameters.\n    pretrained_base : bool or str, default True\n        This will load pretrained backbone network, that was trained on ImageNet.\n    Examples\n    --------\n    >>> model = get_psp(dataset=\'pascal_voc\', backbone=\'resnet50\', pretrained=False)\n    >>> print(model)\n    """"""\n    acronyms = {\n        \'pascal_voc\': \'pascal_voc\',\n        \'pascal_aug\': \'pascal_aug\',\n        \'ade20k\': \'ade\',\n        \'coco\': \'coco\',\n        \'citys\': \'citys\',\n    }\n    from ..data.dataloader import datasets\n    model = PSPNet(datasets[dataset].NUM_CLASS, backbone=backbone, pretrained_base=pretrained_base, **kwargs)\n    if pretrained:\n        from .model_store import get_model_file\n        device = torch.device(kwargs[\'local_rank\'])\n        model.load_state_dict(torch.load(get_model_file(\'psp_%s_%s\' % (backbone, acronyms[dataset]), root=root),\n                              map_location=device))\n    return model\n\n\ndef get_psp_resnet50_voc(**kwargs):\n    return get_psp(\'pascal_voc\', \'resnet50\', **kwargs)\n\n\ndef get_psp_resnet50_ade(**kwargs):\n    return get_psp(\'ade20k\', \'resnet50\', **kwargs)\n\n\ndef get_psp_resnet101_voc(**kwargs):\n    return get_psp(\'pascal_voc\', \'resnet101\', **kwargs)\n\n\ndef get_psp_resnet101_ade(**kwargs):\n    return get_psp(\'ade20k\', \'resnet101\', **kwargs)\n\n\ndef get_psp_resnet101_citys(**kwargs):\n    return get_psp(\'citys\', \'resnet101\', **kwargs)\n\n\ndef get_psp_resnet101_coco(**kwargs):\n    return get_psp(\'coco\', \'resnet101\', **kwargs)\n\n\nif __name__ == \'__main__\':\n    model = get_psp_resnet50_voc()\n    img = torch.randn(4, 3, 480, 480)\n    output = model(img)\n'"
core/models/segbase.py,1,"b'""""""Base Model for Semantic Segmentation""""""\nimport torch.nn as nn\n\nfrom ..nn import JPU\nfrom .base_models.resnetv1b import resnet50_v1s, resnet101_v1s, resnet152_v1s\n\n__all__ = [\'SegBaseModel\']\n\n\nclass SegBaseModel(nn.Module):\n    r""""""Base Model for Semantic Segmentation\n\n    Parameters\n    ----------\n    backbone : string\n        Pre-trained dilated backbone network type (default:\'resnet50\'; \'resnet50\',\n        \'resnet101\' or \'resnet152\').\n    """"""\n\n    def __init__(self, nclass, aux, backbone=\'resnet50\', jpu=False, pretrained_base=True, **kwargs):\n        super(SegBaseModel, self).__init__()\n        dilated = False if jpu else True\n        self.aux = aux\n        self.nclass = nclass\n        if backbone == \'resnet50\':\n            self.pretrained = resnet50_v1s(pretrained=pretrained_base, dilated=dilated, **kwargs)\n        elif backbone == \'resnet101\':\n            self.pretrained = resnet101_v1s(pretrained=pretrained_base, dilated=dilated, **kwargs)\n        elif backbone == \'resnet152\':\n            self.pretrained = resnet152_v1s(pretrained=pretrained_base, dilated=dilated, **kwargs)\n        else:\n            raise RuntimeError(\'unknown backbone: {}\'.format(backbone))\n\n        self.jpu = JPU([512, 1024, 2048], width=512, **kwargs) if jpu else None\n\n    def base_forward(self, x):\n        """"""forwarding pre-trained network""""""\n        x = self.pretrained.conv1(x)\n        x = self.pretrained.bn1(x)\n        x = self.pretrained.relu(x)\n        x = self.pretrained.maxpool(x)\n        c1 = self.pretrained.layer1(x)\n        c2 = self.pretrained.layer2(c1)\n        c3 = self.pretrained.layer3(c2)\n        c4 = self.pretrained.layer4(c3)\n\n        if self.jpu:\n            return self.jpu(c1, c2, c3, c4)\n        else:\n            return c1, c2, c3, c4\n\n    def evaluate(self, x):\n        """"""evaluating network with inputs and targets""""""\n        return self.forward(x)[0]\n\n    def demo(self, x):\n        pred = self.forward(x)\n        if self.aux:\n            pred = pred[0]\n        return pred\n'"
core/nn/__init__.py,0,"b'""""""Seg NN Modules""""""\n# from .sync_bn.syncbn import *\n# from .syncbn import *\nfrom .ca_block import *\nfrom .psa_block import *\nfrom .jpu import *\nfrom .basic import *'"
core/nn/basic.py,4,"b'""""""Basic Module for Semantic Segmentation""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n__all__ = [\'_ConvBNPReLU\', \'_ConvBN\', \'_BNPReLU\', \'_ConvBNReLU\', \'_DepthwiseConv\', \'InvertedResidual\']\n\n\nclass _ConvBNReLU(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0,\n                 dilation=1, groups=1, relu6=False, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(_ConvBNReLU, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias=False)\n        self.bn = norm_layer(out_channels)\n        self.relu = nn.ReLU6(True) if relu6 else nn.ReLU(True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n\nclass _ConvBNPReLU(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0,\n                 dilation=1, groups=1, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(_ConvBNPReLU, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias=False)\n        self.bn = norm_layer(out_channels)\n        self.prelu = nn.PReLU(out_channels)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.prelu(x)\n        return x\n\n\nclass _ConvBN(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0,\n                 dilation=1, groups=1, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(_ConvBN, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias=False)\n        self.bn = norm_layer(out_channels)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n\n\nclass _BNPReLU(nn.Module):\n    def __init__(self, out_channels, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(_BNPReLU, self).__init__()\n        self.bn = norm_layer(out_channels)\n        self.prelu = nn.PReLU(out_channels)\n\n    def forward(self, x):\n        x = self.bn(x)\n        x = self.prelu(x)\n        return x\n\n\n# -----------------------------------------------------------------\n#                      For PSPNet\n# -----------------------------------------------------------------\nclass _PSPModule(nn.Module):\n    def __init__(self, in_channels, sizes=(1, 2, 3, 6), **kwargs):\n        super(_PSPModule, self).__init__()\n        out_channels = int(in_channels / 4)\n        self.avgpools = nn.ModuleList()\n        self.convs = nn.ModuleList()\n        for size in sizes:\n            self.avgpool.append(nn.AdaptiveAvgPool2d(size))\n            self.convs.append(_ConvBNReLU(in_channels, out_channels, 1, **kwargs))\n\n    def forward(self, x):\n        size = x.size()[2:]\n        feats = [x]\n        for (avgpool, conv) in enumerate(zip(self.avgpools, self.convs)):\n            feats.append(F.interpolate(conv(avgpool(x)), size, mode=\'bilinear\', align_corners=True))\n        return torch.cat(feats, dim=1)\n\n\n# -----------------------------------------------------------------\n#                      For MobileNet\n# -----------------------------------------------------------------\nclass _DepthwiseConv(nn.Module):\n    """"""conv_dw in MobileNet""""""\n\n    def __init__(self, in_channels, out_channels, stride, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(_DepthwiseConv, self).__init__()\n        self.conv = nn.Sequential(\n            _ConvBNReLU(in_channels, in_channels, 3, stride, 1, groups=in_channels, norm_layer=norm_layer),\n            _ConvBNReLU(in_channels, out_channels, 1, norm_layer=norm_layer))\n\n    def forward(self, x):\n        return self.conv(x)\n\n\n# -----------------------------------------------------------------\n#                      For MobileNetV2\n# -----------------------------------------------------------------\nclass InvertedResidual(nn.Module):\n    def __init__(self, in_channels, out_channels, stride, expand_ratio, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(InvertedResidual, self).__init__()\n        assert stride in [1, 2]\n        self.use_res_connect = stride == 1 and in_channels == out_channels\n\n        layers = list()\n        inter_channels = int(round(in_channels * expand_ratio))\n        if expand_ratio != 1:\n            # pw\n            layers.append(_ConvBNReLU(in_channels, inter_channels, 1, relu6=True, norm_layer=norm_layer))\n        layers.extend([\n            # dw\n            _ConvBNReLU(inter_channels, inter_channels, 3, stride, 1,\n                        groups=inter_channels, relu6=True, norm_layer=norm_layer),\n            # pw-linear\n            nn.Conv2d(inter_channels, out_channels, 1, bias=False),\n            norm_layer(out_channels)])\n        self.conv = nn.Sequential(*layers)\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nif __name__ == \'__main__\':\n    x = torch.randn(1, 32, 64, 64)\n    model = InvertedResidual(32, 64, 2, 1)\n    out = model(x)\n'"
core/nn/ca_block.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.autograd.function import once_differentiable\nfrom core.nn import _C\n\n__all__ = [\'CrissCrossAttention\', \'ca_weight\', \'ca_map\']\n\n\nclass _CAWeight(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, t, f):\n        weight = _C.ca_forward(t, f)\n\n        ctx.save_for_backward(t, f)\n\n        return weight\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, dw):\n        t, f = ctx.saved_tensors\n\n        dt, df = _C.ca_backward(dw, t, f)\n        return dt, df\n\n\nclass _CAMap(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, weight, g):\n        out = _C.ca_map_forward(weight, g)\n\n        ctx.save_for_backward(weight, g)\n\n        return out\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, dout):\n        weight, g = ctx.saved_tensors\n\n        dw, dg = _C.ca_map_backward(dout, weight, g)\n\n        return dw, dg\n\n\nca_weight = _CAWeight.apply\nca_map = _CAMap.apply\n\n\nclass CrissCrossAttention(nn.Module):\n    """"""Criss-Cross Attention Module""""""\n\n    def __init__(self, in_channels):\n        super(CrissCrossAttention, self).__init__()\n        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, 1)\n        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, 1)\n        self.value_conv = nn.Conv2d(in_channels, in_channels, 1)\n        self.gamma = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x):\n        proj_query = self.query_conv(x)\n        proj_key = self.key_conv(x)\n        proj_value = self.value_conv(x)\n\n        energy = ca_weight(proj_query, proj_key)\n        attention = F.softmax(energy, 1)\n        out = ca_map(attention, proj_value)\n        out = self.gamma * out + x\n\n        return out\n'"
core/nn/jpu.py,4,"b'""""""Joint Pyramid Upsampling""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n__all__ = [\'JPU\']\n\n\nclass SeparableConv2d(nn.Module):\n    def __init__(self, inplanes, planes, kernel_size=3, stride=1, padding=1,\n                 dilation=1, bias=False, norm_layer=nn.BatchNorm2d):\n        super(SeparableConv2d, self).__init__()\n        self.conv = nn.Conv2d(inplanes, inplanes, kernel_size, stride, padding, dilation, groups=inplanes, bias=bias)\n        self.bn = norm_layer(inplanes)\n        self.pointwise = nn.Conv2d(inplanes, planes, 1, bias=bias)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.pointwise(x)\n        return x\n\n\n# copy from: https://github.com/wuhuikai/FastFCN/blob/master/encoding/nn/customize.py\nclass JPU(nn.Module):\n    def __init__(self, in_channels, width=512, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(JPU, self).__init__()\n\n        self.conv5 = nn.Sequential(\n            nn.Conv2d(in_channels[-1], width, 3, padding=1, bias=False),\n            norm_layer(width),\n            nn.ReLU(True))\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(in_channels[-2], width, 3, padding=1, bias=False),\n            norm_layer(width),\n            nn.ReLU(True))\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(in_channels[-3], width, 3, padding=1, bias=False),\n            norm_layer(width),\n            nn.ReLU(True))\n\n        self.dilation1 = nn.Sequential(\n            SeparableConv2d(3 * width, width, 3, padding=1, dilation=1, bias=False),\n            norm_layer(width),\n            nn.ReLU(True))\n        self.dilation2 = nn.Sequential(\n            SeparableConv2d(3 * width, width, 3, padding=2, dilation=2, bias=False),\n            norm_layer(width),\n            nn.ReLU(True))\n        self.dilation3 = nn.Sequential(\n            SeparableConv2d(3 * width, width, 3, padding=4, dilation=4, bias=False),\n            norm_layer(width),\n            nn.ReLU(True))\n        self.dilation4 = nn.Sequential(\n            SeparableConv2d(3 * width, width, 3, padding=8, dilation=8, bias=False),\n            norm_layer(width),\n            nn.ReLU(True))\n\n    def forward(self, *inputs):\n        feats = [self.conv5(inputs[-1]), self.conv4(inputs[-2]), self.conv3(inputs[-3])]\n        size = feats[-1].size()[2:]\n        feats[-2] = F.interpolate(feats[-2], size, mode=\'bilinear\', align_corners=True)\n        feats[-3] = F.interpolate(feats[-3], size, mode=\'bilinear\', align_corners=True)\n        feat = torch.cat(feats, dim=1)\n        feat = torch.cat([self.dilation1(feat), self.dilation2(feat), self.dilation3(feat), self.dilation4(feat)],\n                         dim=1)\n\n        return inputs[0], inputs[1], inputs[2], feat\n'"
core/nn/psa_block.py,4,"b'import torch\nimport torch.nn as nn\n\nfrom torch.autograd.function import once_differentiable\nfrom core.nn import _C\n\n__all__ = [\'CollectAttention\', \'DistributeAttention\', \'psa_collect\', \'psa_distribute\']\n\n\nclass _PSACollect(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, hc):\n        out = _C.psa_forward(hc, 1)\n\n        ctx.save_for_backward(hc)\n\n        return out\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, dout):\n        hc = ctx.saved_tensors\n\n        dhc = _C.psa_backward(dout, hc[0], 1)\n\n        return dhc\n\n\nclass _PSADistribute(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, hc):\n        out = _C.psa_forward(hc, 2)\n\n        ctx.save_for_backward(hc)\n\n        return out\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, dout):\n        hc = ctx.saved_tensors\n\n        dhc = _C.psa_backward(dout, hc[0], 2)\n\n        return dhc\n\n\npsa_collect = _PSACollect.apply\npsa_distribute = _PSADistribute.apply\n\n\nclass CollectAttention(nn.Module):\n    """"""Collect Attention Generation Module""""""\n\n    def __init__(self):\n        super(CollectAttention, self).__init__()\n\n    def forward(self, x):\n        out = psa_collect(x)\n        return out\n\n\nclass DistributeAttention(nn.Module):\n    """"""Distribute Attention Generation Module""""""\n\n    def __init__(self):\n        super(DistributeAttention, self).__init__()\n\n    def forward(self, x):\n        out = psa_distribute(x)\n        return out\n'"
core/nn/setup.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n# !/usr/bin/env python\n# reference: https://github.com/facebookresearch/maskrcnn-benchmark/blob/90c226cf10e098263d1df28bda054a5f22513b4f/setup.py\n\nimport os\nimport glob\nimport torch\n\nfrom setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CppExtension, CUDAExtension, CUDA_HOME\n\nrequirements = [""torch""]\n\n\ndef get_extension():\n    this_dir = os.path.dirname(os.path.abspath(__file__))\n    extensions_dir = os.path.join(this_dir, ""csrc"")\n\n    main_file = glob.glob(os.path.join(extensions_dir, ""*.cpp""))\n    source_cpu = glob.glob(os.path.join(extensions_dir, ""cpu"", ""*.cpp""))\n    source_cuda = glob.glob(os.path.join(extensions_dir, ""cuda"", ""*.cu""))\n\n    sources = main_file + source_cpu\n    extension = CppExtension\n\n    define_macros = []\n\n    if (torch.cuda.is_available() and CUDA_HOME is not None) or os.getenv(""FORCE_CUDA"", ""0"") == ""1"":\n        extension = CUDAExtension\n        sources += source_cuda\n        define_macros += [(""WITH_CUDA"", None)]\n\n    sources = [os.path.join(extensions_dir, s) for s in sources]\n\n    include_dirs = [extensions_dir]\n\n    ext_modules = [\n        extension(\n            ""._C"",\n            sources,\n            include_dirs=include_dirs,\n            define_macros=define_macros,\n        )\n    ]\n\n    return ext_modules\n\n\nsetup(\n    name=""semantic_segmentation"",\n    version=""0.1"",\n    author=""tramac"",\n    description=""semantic segmentation in pytorch"",\n    ext_modules=get_extension(),\n    cmdclass={""build_ext"": BuildExtension}\n)'"
core/nn/syncbn.py,6,"b'# Adopt from https://github.com/zhanghang1989/PyTorch-Encoding/blob/master/encoding/nn/syncbn.py\n""""""Synchronized Cross-GPU Batch Normalization Module""""""\nimport warnings\nimport torch\nimport torch.cuda.comm as comm\n\nfrom queue import Queue\nfrom torch.autograd import Function\nfrom torch.nn.modules.batchnorm import _BatchNorm\nfrom torch.autograd.function import once_differentiable\nfrom core.nn import _C\n\n__all__ = [\'SyncBatchNorm\', \'BatchNorm1d\', \'BatchNorm2d\', \'BatchNorm3d\']\n\n\nclass _SyncBatchNorm(Function):\n    @classmethod\n    def forward(cls, ctx, x, gamma, beta, running_mean, running_var,\n                extra, sync=True, training=True, momentum=0.1, eps=1e-05,\n                activation=""none"", slope=0.01):\n        # save context\n        cls._parse_extra(ctx, extra)\n        ctx.sync = sync\n        ctx.training = training\n        ctx.momentum = momentum\n        ctx.eps = eps\n        ctx.activation = activation\n        ctx.slope = slope\n        assert activation == \'none\'\n\n        # continous inputs\n        x = x.contiguous()\n        gamma = gamma.contiguous()\n        beta = beta.contiguous()\n\n        if ctx.training:\n            _ex, _exs = _C.expectation_forward(x)\n\n            if ctx.sync:\n                if ctx.is_master:\n                    _ex, _exs = [_ex.unsqueeze(0)], [_exs.unsqueeze(0)]\n                    for _ in range(ctx.master_queue.maxsize):\n                        _ex_w, _exs_w = ctx.master_queue.get()\n                        ctx.master_queue.task_done()\n                        _ex.append(_ex_w.unsqueeze(0))\n                        _exs.append(_exs_w.unsqueeze(0))\n\n                    _ex = comm.gather(_ex).mean(0)\n                    _exs = comm.gather(_exs).mean(0)\n\n                    tensors = comm.broadcast_coalesced((_ex, _exs), [_ex.get_device()] + ctx.worker_ids)\n                    for ts, queue in zip(tensors[1:], ctx.worker_queues):\n                        queue.put(ts)\n                else:\n                    ctx.master_queue.put((_ex, _exs))\n                    _ex, _exs = ctx.worker_queue.get()\n                    ctx.worker_queue.task_done()\n\n            # Update running stats\n            _var = _exs - _ex ** 2\n            running_mean.mul_((1 - ctx.momentum)).add_(ctx.momentum * _ex)\n            running_var.mul_((1 - ctx.momentum)).add_(ctx.momentum * _var)\n\n            # Mark in-place modified tensors\n            ctx.mark_dirty(running_mean, running_var)\n        else:\n            _ex, _var = running_mean.contiguous(), running_var.contiguous()\n            _exs = _var + _ex ** 2\n\n        # BN forward\n        y = _C.batchnorm_forward(x, _ex, _exs, gamma, beta, ctx.eps)\n\n        # Output\n        ctx.save_for_backward(x, _ex, _exs, gamma, beta)\n        return y\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, dz):\n        x, _ex, _exs, gamma, beta = ctx.saved_tensors\n        dz = dz.contiguous()\n\n        # BN backward\n        dx, _dex, _dexs, dgamma, dbeta = _C.batchnorm_backward(dz, x, _ex, _exs, gamma, beta, ctx.eps)\n\n        if ctx.training:\n            if ctx.sync:\n                if ctx.is_master:\n                    _dex, _dexs = [_dex.unsqueeze(0)], [_dexs.unsqueeze(0)]\n                    for _ in range(ctx.master_queue.maxsize):\n                        _dex_w, _dexs_w = ctx.master_queue.get()\n                        ctx.master_queue.task_done()\n                        _dex.append(_dex_w.unsqueeze(0))\n                        _dexs.append(_dexs_w.unsqueeze(0))\n\n                    _dex = comm.gather(_dex).mean(0)\n                    _dexs = comm.gather(_dexs).mean(0)\n\n                    tensors = comm.broadcast_coalesced((_dex, _dexs), [_dex.get_device()] + ctx.worker_ids)\n                    for ts, queue in zip(tensors[1:], ctx.worker_queues):\n                        queue.put(ts)\n                else:\n                    ctx.master_queue.put((_dex, _dexs))\n                    _dex, _dexs = ctx.worker_queue.get()\n                    ctx.worker_queue.task_done()\n\n            dx_ = _C.expectation_backward(x, _dex, _dexs)\n            dx = dx + dx_\n\n        return dx, dgamma, dbeta, None, None, None, None, None, None, None, None, None\n\n    @staticmethod\n    def _parse_extra(ctx, extra):\n        ctx.is_master = extra[""is_master""]\n        if ctx.is_master:\n            ctx.master_queue = extra[""master_queue""]\n            ctx.worker_queues = extra[""worker_queues""]\n            ctx.worker_ids = extra[""worker_ids""]\n        else:\n            ctx.master_queue = extra[""master_queue""]\n            ctx.worker_queue = extra[""worker_queue""]\n\n\nsyncbatchnorm = _SyncBatchNorm.apply\n\n\nclass SyncBatchNorm(_BatchNorm):\n    """"""Cross-GPU Synchronized Batch normalization (SyncBN)\n\n    Parameters:\n        num_features: num_features from an expected input of\n            size batch_size x num_features x height x width\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Default: 0.1\n        sync: a boolean value that when set to ``True``, synchronize across\n            different gpus. Default: ``True``\n        activation : str\n            Name of the activation functions, one of: `leaky_relu` or `none`.\n        slope : float\n            Negative slope for the `leaky_relu` activation.\n\n    Shape:\n        - Input: :math:`(N, C, H, W)`\n        - Output: :math:`(N, C, H, W)` (same shape as input)\n    Reference:\n        .. [1] Ioffe, Sergey, and Christian Szegedy. ""Batch normalization: Accelerating deep network training by reducing internal covariate shift."" *ICML 2015*\n        .. [2] Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang, Ambrish Tyagi, and Amit Agrawal. ""Context Encoding for Semantic Segmentation."" *CVPR 2018*\n    Examples:\n        >>> m = SyncBatchNorm(100)\n        >>> net = torch.nn.DataParallel(m)\n        >>> output = net(input)\n    """"""\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, sync=True, activation=\'none\', slope=0.01):\n        super(SyncBatchNorm, self).__init__(num_features, eps=eps, momentum=momentum, affine=True)\n        self.activation = activation\n        self.slope = slope\n        self.devices = list(range(torch.cuda.device_count()))\n        self.sync = sync if len(self.devices) > 1 else False\n        # Initialize queues\n        self.worker_ids = self.devices[1:]\n        self.master_queue = Queue(len(self.worker_ids))\n        self.worker_queues = [Queue(1) for _ in self.worker_ids]\n\n    def forward(self, x):\n        # resize the input to (B, C, -1)\n        input_shape = x.size()\n        x = x.view(input_shape[0], self.num_features, -1)\n        if x.get_device() == self.devices[0]:\n            # Master mode\n            extra = {\n                ""is_master"": True,\n                ""master_queue"": self.master_queue,\n                ""worker_queues"": self.worker_queues,\n                ""worker_ids"": self.worker_ids\n            }\n        else:\n            # Worker mode\n            extra = {\n                ""is_master"": False,\n                ""master_queue"": self.master_queue,\n                ""worker_queue"": self.worker_queues[self.worker_ids.index(x.get_device())]\n            }\n\n        return syncbatchnorm(x, self.weight, self.bias, self.running_mean, self.running_var,\n                             extra, self.sync, self.training, self.momentum, self.eps,\n                             self.activation, self.slope).view(input_shape)\n\n    def extra_repr(self):\n        if self.activation == \'none\':\n            return \'sync={}\'.format(self.sync)\n        else:\n            return \'sync={}, act={}, slope={}\'.format(\n                self.sync, self.activation, self.slope)\n\n\nclass BatchNorm1d(SyncBatchNorm):\n    """"""BatchNorm1d is deprecated in favor of :class:`core.nn.sync_bn.SyncBatchNorm`.""""""\n\n    def __init__(self, *args, **kwargs):\n        warnings.warn(""core.nn.sync_bn.{} is now deprecated in favor of core.nn.sync_bn.{}.""\n                      .format(\'BatchNorm1d\', SyncBatchNorm.__name__), DeprecationWarning)\n        super(BatchNorm1d, self).__init__(*args, **kwargs)\n\n\nclass BatchNorm2d(SyncBatchNorm):\n    """"""BatchNorm1d is deprecated in favor of :class:`core.nn.sync_bn.SyncBatchNorm`.""""""\n\n    def __init__(self, *args, **kwargs):\n        warnings.warn(""core.nn.sync_bn.{} is now deprecated in favor of core.nn.sync_bn.{}.""\n                      .format(\'BatchNorm2d\', SyncBatchNorm.__name__), DeprecationWarning)\n        super(BatchNorm2d, self).__init__(*args, **kwargs)\n\n\nclass BatchNorm3d(SyncBatchNorm):\n    """"""BatchNorm1d is deprecated in favor of :class:`core.nn.sync_bn.SyncBatchNorm`.""""""\n\n    def __init__(self, *args, **kwargs):\n        warnings.warn(""core.nn.sync_bn.{} is now deprecated in favor of core.nn.sync_bn.{}.""\n                      .format(\'BatchNorm3d\', SyncBatchNorm.__name__), DeprecationWarning)\n        super(BatchNorm3d, self).__init__(*args, **kwargs)\n'"
core/utils/__init__.py,0,"b'""""""Utility functions.""""""\nfrom __future__ import absolute_import\n\nfrom .download import download, check_sha1\nfrom .filesystem import makedirs, try_import_pycocotools\n'"
core/utils/distributed.py,18,"b'""""""\nThis file contains primitives for multi-gpu communication.\nThis is useful when doing distributed training.\n""""""\nimport math\nimport pickle\nimport torch\nimport torch.utils.data as data\nimport torch.distributed as dist\n\nfrom torch.utils.data.sampler import Sampler, BatchSampler\n\n__all__ = [\'get_world_size\', \'get_rank\', \'synchronize\', \'is_main_process\',\n           \'all_gather\', \'make_data_sampler\', \'make_batch_data_sampler\',\n           \'reduce_dict\', \'reduce_loss_dict\']\n\n\n# reference: https://github.com/facebookresearch/maskrcnn-benchmark/blob/master/maskrcnn_benchmark/utils/comm.py\ndef get_world_size():\n    if not dist.is_available():\n        return 1\n    if not dist.is_initialized():\n        return 1\n    return dist.get_world_size()\n\n\ndef get_rank():\n    if not dist.is_available():\n        return 0\n    if not dist.is_initialized():\n        return 0\n    return dist.get_rank()\n\n\ndef is_main_process():\n    return get_rank() == 0\n\n\ndef synchronize():\n    """"""\n    Helper function to synchronize (barrier) among all processes when\n    using distributed training\n    """"""\n    if not dist.is_available():\n        return\n    if not dist.is_initialized():\n        return\n    world_size = dist.get_world_size()\n    if world_size == 1:\n        return\n    dist.barrier()\n\n\ndef all_gather(data):\n    """"""\n    Run all_gather on arbitrary picklable data (not necessarily tensors)\n    Args:\n        data: any picklable object\n    Returns:\n        list[data]: list of data gathered from each rank\n    """"""\n    world_size = get_world_size()\n    if world_size == 1:\n        return [data]\n\n    # serialized to a Tensor\n    buffer = pickle.dumps(data)\n    storage = torch.ByteStorage.from_buffer(buffer)\n    tensor = torch.ByteTensor(storage).to(""cuda"")\n\n    # obtain Tensor size of each rank\n    local_size = torch.IntTensor([tensor.numel()]).to(""cuda"")\n    size_list = [torch.IntTensor([0]).to(""cuda"") for _ in range(world_size)]\n    dist.all_gather(size_list, local_size)\n    size_list = [int(size.item()) for size in size_list]\n    max_size = max(size_list)\n\n    # receiving Tensor from all ranks\n    # we pad the tensor because torch all_gather does not support\n    # gathering tensors of different shapes\n    tensor_list = []\n    for _ in size_list:\n        tensor_list.append(torch.ByteTensor(size=(max_size,)).to(""cuda""))\n    if local_size != max_size:\n        padding = torch.ByteTensor(size=(max_size - local_size,)).to(""cuda"")\n        tensor = torch.cat((tensor, padding), dim=0)\n    dist.all_gather(tensor_list, tensor)\n\n    data_list = []\n    for size, tensor in zip(size_list, tensor_list):\n        buffer = tensor.cpu().numpy().tobytes()[:size]\n        data_list.append(pickle.loads(buffer))\n\n    return data_list\n\n\ndef reduce_dict(input_dict, average=True):\n    """"""\n    Args:\n        input_dict (dict): all the values will be reduced\n        average (bool): whether to do average or sum\n    Reduce the values in the dictionary from all processes so that process with rank\n    0 has the averaged results. Returns a dict with the same fields as\n    input_dict, after reduction.\n    """"""\n    world_size = get_world_size()\n    if world_size < 2:\n        return input_dict\n    with torch.no_grad():\n        names = []\n        values = []\n        # sort the keys so that they are consistent across processes\n        for k in sorted(input_dict.keys()):\n            names.append(k)\n            values.append(input_dict[k])\n        values = torch.stack(values, dim=0)\n        dist.reduce(values, dst=0)\n        if dist.get_rank() == 0 and average:\n            # only main process gets accumulated, so only divide by\n            # world_size in this case\n            values /= world_size\n        reduced_dict = {k: v for k, v in zip(names, values)}\n    return reduced_dict\n\n\ndef reduce_loss_dict(loss_dict):\n    """"""\n    Reduce the loss dictionary from all processes so that process with rank\n    0 has the averaged results. Returns a dict with the same fields as\n    loss_dict, after reduction.\n    """"""\n    world_size = get_world_size()\n    if world_size < 2:\n        return loss_dict\n    with torch.no_grad():\n        loss_names = []\n        all_losses = []\n        for k in sorted(loss_dict.keys()):\n            loss_names.append(k)\n            all_losses.append(loss_dict[k])\n        all_losses = torch.stack(all_losses, dim=0)\n        dist.reduce(all_losses, dst=0)\n        if dist.get_rank() == 0:\n            # only main process gets accumulated, so only divide by\n            # world_size in this case\n            all_losses /= world_size\n        reduced_losses = {k: v for k, v in zip(loss_names, all_losses)}\n    return reduced_losses\n\n\ndef make_data_sampler(dataset, shuffle, distributed):\n    if distributed:\n        return DistributedSampler(dataset, shuffle=shuffle)\n    if shuffle:\n        sampler = data.sampler.RandomSampler(dataset)\n    else:\n        sampler = data.sampler.SequentialSampler(dataset)\n    return sampler\n\n\ndef make_batch_data_sampler(sampler, images_per_batch, num_iters=None, start_iter=0):\n    batch_sampler = data.sampler.BatchSampler(sampler, images_per_batch, drop_last=True)\n    if num_iters is not None:\n        batch_sampler = IterationBasedBatchSampler(batch_sampler, num_iters, start_iter)\n    return batch_sampler\n\n\n# Code is copy-pasted from https://github.com/facebookresearch/maskrcnn-benchmark/blob/master/maskrcnn_benchmark/data/samplers/distributed.py\nclass DistributedSampler(Sampler):\n    """"""Sampler that restricts data loading to a subset of the dataset.\n    It is especially useful in conjunction with\n    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSampler instance as a DataLoader sampler,\n    and load a subset of the original dataset that is exclusive to it.\n    .. note::\n        Dataset is assumed to be of constant size.\n    Arguments:\n        dataset: Dataset used for sampling.\n        num_replicas (optional): Number of processes participating in\n            distributed training.\n        rank (optional): Rank of the current process within num_replicas.\n    """"""\n\n    def __init__(self, dataset, num_replicas=None, rank=None, shuffle=True):\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(""Requires distributed package to be available"")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(""Requires distributed package to be available"")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.num_samples = int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))\n        self.total_size = self.num_samples * self.num_replicas\n        self.shuffle = shuffle\n\n    def __iter__(self):\n        if self.shuffle:\n            # deterministically shuffle based on epoch\n            g = torch.Generator()\n            g.manual_seed(self.epoch)\n            indices = torch.randperm(len(self.dataset), generator=g).tolist()\n        else:\n            indices = torch.arange(len(self.dataset)).tolist()\n\n        # add extra samples to make it evenly divisible\n        indices += indices[: (self.total_size - len(indices))]\n        assert len(indices) == self.total_size\n\n        # subsample\n        offset = self.num_samples * self.rank\n        indices = indices[offset: offset + self.num_samples]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch\n\n\nclass IterationBasedBatchSampler(BatchSampler):\n    """"""\n    Wraps a BatchSampler, resampling from it until\n    a specified number of iterations have been sampled\n    """"""\n\n    def __init__(self, batch_sampler, num_iterations, start_iter=0):\n        self.batch_sampler = batch_sampler\n        self.num_iterations = num_iterations\n        self.start_iter = start_iter\n\n    def __iter__(self):\n        iteration = self.start_iter\n        while iteration <= self.num_iterations:\n            # if the underlying sampler has a set_epoch method, like\n            # DistributedSampler, used for making each process see\n            # a different split of the dataset, then set it\n            if hasattr(self.batch_sampler.sampler, ""set_epoch""):\n                self.batch_sampler.sampler.set_epoch(iteration)\n            for batch in self.batch_sampler:\n                iteration += 1\n                if iteration > self.num_iterations:\n                    break\n                yield batch\n\n    def __len__(self):\n        return self.num_iterations\n\n\nif __name__ == \'__main__\':\n    pass\n'"
core/utils/download.py,0,"b'""""""Download files with progress bar.""""""\nimport os\nimport hashlib\nimport requests\nfrom tqdm import tqdm\n\ndef check_sha1(filename, sha1_hash):\n    """"""Check whether the sha1 hash of the file content matches the expected hash.\n    Parameters\n    ----------\n    filename : str\n        Path to the file.\n    sha1_hash : str\n        Expected sha1 hash in hexadecimal digits.\n    Returns\n    -------\n    bool\n        Whether the file content matches the expected hash.\n    """"""\n    sha1 = hashlib.sha1()\n    with open(filename, \'rb\') as f:\n        while True:\n            data = f.read(1048576)\n            if not data:\n                break\n            sha1.update(data)\n\n    sha1_file = sha1.hexdigest()\n    l = min(len(sha1_file), len(sha1_hash))\n    return sha1.hexdigest()[0:l] == sha1_hash[0:l]\n\ndef download(url, path=None, overwrite=False, sha1_hash=None):\n    """"""Download an given URL\n    Parameters\n    ----------\n    url : str\n        URL to download\n    path : str, optional\n        Destination path to store downloaded file. By default stores to the\n        current directory with same name as in url.\n    overwrite : bool, optional\n        Whether to overwrite destination file if already exists.\n    sha1_hash : str, optional\n        Expected sha1 hash in hexadecimal digits. Will ignore existing file when hash is specified\n        but doesn\'t match.\n    Returns\n    -------\n    str\n        The file path of the downloaded file.\n    """"""\n    if path is None:\n        fname = url.split(\'/\')[-1]\n    else:\n        path = os.path.expanduser(path)\n        if os.path.isdir(path):\n            fname = os.path.join(path, url.split(\'/\')[-1])\n        else:\n            fname = path\n\n    if overwrite or not os.path.exists(fname) or (sha1_hash and not check_sha1(fname, sha1_hash)):\n        dirname = os.path.dirname(os.path.abspath(os.path.expanduser(fname)))\n        if not os.path.exists(dirname):\n            os.makedirs(dirname)\n\n        print(\'Downloading %s from %s...\'%(fname, url))\n        r = requests.get(url, stream=True)\n        if r.status_code != 200:\n            raise RuntimeError(""Failed downloading url %s""%url)\n        total_length = r.headers.get(\'content-length\')\n        with open(fname, \'wb\') as f:\n            if total_length is None: # no content length header\n                for chunk in r.iter_content(chunk_size=1024):\n                    if chunk: # filter out keep-alive new chunks\n                        f.write(chunk)\n            else:\n                total_length = int(total_length)\n                for chunk in tqdm(r.iter_content(chunk_size=1024),\n                                  total=int(total_length / 1024. + 0.5),\n                                  unit=\'KB\', unit_scale=False, dynamic_ncols=True):\n                    f.write(chunk)\n\n        if sha1_hash and not check_sha1(fname, sha1_hash):\n            raise UserWarning(\'File {} is downloaded but the content hash does not match. \' \\\n                              \'The repo may be outdated or download may be incomplete. \' \\\n                              \'If the ""repo_url"" is overridden, consider switching to \' \\\n                              \'the default repo.\'.format(fname))\n\n    return fname'"
core/utils/filesystem.py,0,"b'""""""Filesystem utility functions.""""""\nfrom __future__ import absolute_import\nimport os\nimport errno\n\n\ndef makedirs(path):\n    """"""Create directory recursively if not exists.\n    Similar to `makedir -p`, you can skip checking existence before this function.\n    Parameters\n    ----------\n    path : str\n        Path of the desired dir\n    """"""\n    try:\n        os.makedirs(path)\n    except OSError as exc:\n        if exc.errno != errno.EEXIST:\n            raise\n\n\ndef try_import(package, message=None):\n    """"""Try import specified package, with custom message support.\n    Parameters\n    ----------\n    package : str\n        The name of the targeting package.\n    message : str, default is None\n        If not None, this function will raise customized error message when import error is found.\n    Returns\n    -------\n    module if found, raise ImportError otherwise\n    """"""\n    try:\n        return __import__(package)\n    except ImportError as e:\n        if not message:\n            raise e\n        raise ImportError(message)\n\n\ndef try_import_cv2():\n    """"""Try import cv2 at runtime.\n    Returns\n    -------\n    cv2 module if found. Raise ImportError otherwise\n    """"""\n    msg = ""cv2 is required, you can install by package manager, e.g. \'apt-get\', \\\n        or `pip install opencv-python --user` (note that this is unofficial PYPI package).""\n    return try_import(\'cv2\', msg)\n\n\ndef import_try_install(package, extern_url=None):\n    """"""Try import the specified package.\n    If the package not installed, try use pip to install and import if success.\n    Parameters\n    ----------\n    package : str\n        The name of the package trying to import.\n    extern_url : str or None, optional\n        The external url if package is not hosted on PyPI.\n        For example, you can install a package using:\n         ""pip install git+http://github.com/user/repo/tarball/master/egginfo=xxx"".\n        In this case, you can pass the url to the extern_url.\n    Returns\n    -------\n    <class \'Module\'>\n        The imported python module.\n    """"""\n    try:\n        return __import__(package)\n    except ImportError:\n        try:\n            from pip import main as pipmain\n        except ImportError:\n            from pip._internal import main as pipmain\n\n        # trying to install package\n        url = package if extern_url is None else extern_url\n        pipmain([\'install\', \'--user\', url])  # will raise SystemExit Error if fails\n\n        # trying to load again\n        try:\n            return __import__(package)\n        except ImportError:\n            import sys\n            import site\n            user_site = site.getusersitepackages()\n            if user_site not in sys.path:\n                sys.path.append(user_site)\n            return __import__(package)\n    return __import__(package)\n\n\n""""""Import helper for pycocotools""""""\n\n\n# NOTE: for developers\n# please do not import any pycocotools in __init__ because we are trying to lazy\n# import pycocotools to avoid install it for other users who may not use it.\n# only import when you actually use it\n\n\ndef try_import_pycocotools():\n    """"""Tricks to optionally install and import pycocotools""""""\n    # first we can try import pycocotools\n    try:\n        import pycocotools as _\n    except ImportError:\n        import os\n        # we need to install pycootools, which is a bit tricky\n        # pycocotools sdist requires Cython, numpy(already met)\n        import_try_install(\'cython\')\n        # pypi pycocotools is not compatible with windows\n        win_url = \'git+https://github.com/zhreshold/cocoapi.git#subdirectory=PythonAPI\'\n        try:\n            if os.name == \'nt\':\n                import_try_install(\'pycocotools\', win_url)\n            else:\n                import_try_install(\'pycocotools\')\n        except ImportError:\n            faq = \'cocoapi FAQ\'\n            raise ImportError(\'Cannot import or install pycocotools, please refer to %s.\' % faq)\n'"
core/utils/logger.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport logging\nimport os\nimport sys\n\n__all__ = [\'setup_logger\']\n\n\n# reference from: https://github.com/facebookresearch/maskrcnn-benchmark/blob/master/maskrcnn_benchmark/utils/logger.py\ndef setup_logger(name, save_dir, distributed_rank, filename=""log.txt"", mode=\'w\'):\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG)\n    # don\'t log results for the non-master process\n    if distributed_rank > 0:\n        return logger\n    ch = logging.StreamHandler(stream=sys.stdout)\n    ch.setLevel(logging.DEBUG)\n    formatter = logging.Formatter(""%(asctime)s %(name)s %(levelname)s: %(message)s"")\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n\n    if save_dir:\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n        fh = logging.FileHandler(os.path.join(save_dir, filename), mode=mode)  # \'a+\' for add, \'w\' for overwrite\n        fh.setLevel(logging.DEBUG)\n        fh.setFormatter(formatter)\n        logger.addHandler(fh)\n\n    return logger\n'"
core/utils/loss.py,11,"b'""""""Custom losses.""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.autograd import Variable\n\n__all__ = [\'MixSoftmaxCrossEntropyLoss\', \'MixSoftmaxCrossEntropyOHEMLoss\',\n           \'EncNetLoss\', \'ICNetLoss\', \'get_segmentation_loss\']\n\n\n# TODO: optim function\nclass MixSoftmaxCrossEntropyLoss(nn.CrossEntropyLoss):\n    def __init__(self, aux=True, aux_weight=0.2, ignore_index=-1, **kwargs):\n        super(MixSoftmaxCrossEntropyLoss, self).__init__(ignore_index=ignore_index)\n        self.aux = aux\n        self.aux_weight = aux_weight\n\n    def _aux_forward(self, *inputs, **kwargs):\n        *preds, target = tuple(inputs)\n\n        loss = super(MixSoftmaxCrossEntropyLoss, self).forward(preds[0], target)\n        for i in range(1, len(preds)):\n            aux_loss = super(MixSoftmaxCrossEntropyLoss, self).forward(preds[i], target)\n            loss += self.aux_weight * aux_loss\n        return loss\n\n    def forward(self, *inputs, **kwargs):\n        preds, target = tuple(inputs)\n        inputs = tuple(list(preds) + [target])\n        if self.aux:\n            return dict(loss=self._aux_forward(*inputs))\n        else:\n            return dict(loss=super(MixSoftmaxCrossEntropyLoss, self).forward(*inputs))\n\n\n# reference: https://github.com/zhanghang1989/PyTorch-Encoding/blob/master/encoding/nn/loss.py\nclass EncNetLoss(nn.CrossEntropyLoss):\n    """"""2D Cross Entropy Loss with SE Loss""""""\n\n    def __init__(self, se_loss=True, se_weight=0.2, nclass=19, aux=False,\n                 aux_weight=0.4, weight=None, ignore_index=-1, **kwargs):\n        super(EncNetLoss, self).__init__(weight, None, ignore_index)\n        self.se_loss = se_loss\n        self.aux = aux\n        self.nclass = nclass\n        self.se_weight = se_weight\n        self.aux_weight = aux_weight\n        self.bceloss = nn.BCELoss(weight)\n\n    def forward(self, *inputs):\n        preds, target = tuple(inputs)\n        inputs = tuple(list(preds) + [target])\n        if not self.se_loss and not self.aux:\n            return super(EncNetLoss, self).forward(*inputs)\n        elif not self.se_loss:\n            pred1, pred2, target = tuple(inputs)\n            loss1 = super(EncNetLoss, self).forward(pred1, target)\n            loss2 = super(EncNetLoss, self).forward(pred2, target)\n            return dict(loss=loss1 + self.aux_weight * loss2)\n        elif not self.aux:\n            pred, se_pred, target = tuple(inputs)\n            se_target = self._get_batch_label_vector(target, nclass=self.nclass).type_as(pred)\n            loss1 = super(EncNetLoss, self).forward(pred, target)\n            loss2 = self.bceloss(torch.sigmoid(se_pred), se_target)\n            return dict(loss=loss1 + self.se_weight * loss2)\n        else:\n            pred1, se_pred, pred2, target = tuple(inputs)\n            se_target = self._get_batch_label_vector(target, nclass=self.nclass).type_as(pred1)\n            loss1 = super(EncNetLoss, self).forward(pred1, target)\n            loss2 = super(EncNetLoss, self).forward(pred2, target)\n            loss3 = self.bceloss(torch.sigmoid(se_pred), se_target)\n            return dict(loss=loss1 + self.aux_weight * loss2 + self.se_weight * loss3)\n\n    @staticmethod\n    def _get_batch_label_vector(target, nclass):\n        # target is a 3D Variable BxHxW, output is 2D BxnClass\n        batch = target.size(0)\n        tvect = Variable(torch.zeros(batch, nclass))\n        for i in range(batch):\n            hist = torch.histc(target[i].cpu().data.float(),\n                               bins=nclass, min=0,\n                               max=nclass - 1)\n            vect = hist > 0\n            tvect[i] = vect\n        return tvect\n\n\n# TODO: optim function\nclass ICNetLoss(nn.CrossEntropyLoss):\n    """"""Cross Entropy Loss for ICNet""""""\n\n    def __init__(self, nclass, aux_weight=0.4, ignore_index=-1, **kwargs):\n        super(ICNetLoss, self).__init__(ignore_index=ignore_index)\n        self.nclass = nclass\n        self.aux_weight = aux_weight\n\n    def forward(self, *inputs):\n        preds, target = tuple(inputs)\n        inputs = tuple(list(preds) + [target])\n\n        pred, pred_sub4, pred_sub8, pred_sub16, target = tuple(inputs)\n        # [batch, W, H] -> [batch, 1, W, H]\n        target = target.unsqueeze(1).float()\n        target_sub4 = F.interpolate(target, pred_sub4.size()[2:], mode=\'bilinear\', align_corners=True).squeeze(1).long()\n        target_sub8 = F.interpolate(target, pred_sub8.size()[2:], mode=\'bilinear\', align_corners=True).squeeze(1).long()\n        target_sub16 = F.interpolate(target, pred_sub16.size()[2:], mode=\'bilinear\', align_corners=True).squeeze(\n            1).long()\n        loss1 = super(ICNetLoss, self).forward(pred_sub4, target_sub4)\n        loss2 = super(ICNetLoss, self).forward(pred_sub8, target_sub8)\n        loss3 = super(ICNetLoss, self).forward(pred_sub16, target_sub16)\n        return dict(loss=loss1 + loss2 * self.aux_weight + loss3 * self.aux_weight)\n\n\nclass OhemCrossEntropy2d(nn.Module):\n    def __init__(self, ignore_index=-1, thresh=0.7, min_kept=100000, use_weight=True, **kwargs):\n        super(OhemCrossEntropy2d, self).__init__()\n        self.ignore_index = ignore_index\n        self.thresh = float(thresh)\n        self.min_kept = int(min_kept)\n        if use_weight:\n            weight = torch.FloatTensor([0.8373, 0.918, 0.866, 1.0345, 1.0166, 0.9969, 0.9754,\n                                        1.0489, 0.8786, 1.0023, 0.9539, 0.9843, 1.1116, 0.9037, 1.0865, 1.0955,\n                                        1.0865, 1.1529, 1.0507])\n            self.criterion = torch.nn.CrossEntropyLoss(weight=weight, ignore_index=ignore_index)\n        else:\n            self.criterion = torch.nn.CrossEntropyLoss(ignore_index=ignore_index)\n\n    def forward(self, pred, target):\n        n, c, h, w = pred.size()\n        target = target.view(-1)\n        valid_mask = target.ne(self.ignore_index)\n        target = target * valid_mask.long()\n        num_valid = valid_mask.sum()\n\n        prob = F.softmax(pred, dim=1)\n        prob = prob.transpose(0, 1).reshape(c, -1)\n\n        if self.min_kept > num_valid:\n            print(""Lables: {}"".format(num_valid))\n        elif num_valid > 0:\n            prob = prob.masked_fill_(1 - valid_mask, 1)\n            mask_prob = prob[target, torch.arange(len(target), dtype=torch.long)]\n            threshold = self.thresh\n            if self.min_kept > 0:\n                index = mask_prob.argsort()\n                threshold_index = index[min(len(index), self.min_kept) - 1]\n                if mask_prob[threshold_index] > self.thresh:\n                    threshold = mask_prob[threshold_index]\n            kept_mask = mask_prob.le(threshold)\n            valid_mask = valid_mask * kept_mask\n            target = target * kept_mask.long()\n\n        target = target.masked_fill_(1 - valid_mask, self.ignore_index)\n        target = target.view(n, h, w)\n\n        return self.criterion(pred, target)\n\n\nclass MixSoftmaxCrossEntropyOHEMLoss(OhemCrossEntropy2d):\n    def __init__(self, aux=False, aux_weight=0.4, weight=None, ignore_index=-1, **kwargs):\n        super(MixSoftmaxCrossEntropyOHEMLoss, self).__init__(ignore_index=ignore_index)\n        self.aux = aux\n        self.aux_weight = aux_weight\n        self.bceloss = nn.BCELoss(weight)\n\n    def _aux_forward(self, *inputs, **kwargs):\n        *preds, target = tuple(inputs)\n\n        loss = super(MixSoftmaxCrossEntropyOHEMLoss, self).forward(preds[0], target)\n        for i in range(1, len(preds)):\n            aux_loss = super(MixSoftmaxCrossEntropyOHEMLoss, self).forward(preds[i], target)\n            loss += self.aux_weight * aux_loss\n        return loss\n\n    def forward(self, *inputs):\n        preds, target = tuple(inputs)\n        inputs = tuple(list(preds) + [target])\n        if self.aux:\n            return dict(loss=self._aux_forward(*inputs))\n        else:\n            return dict(loss=super(MixSoftmaxCrossEntropyOHEMLoss, self).forward(*inputs))\n\n\ndef get_segmentation_loss(model, use_ohem=False, **kwargs):\n    if use_ohem:\n        return MixSoftmaxCrossEntropyOHEMLoss(**kwargs)\n\n    model = model.lower()\n    if model == \'encnet\':\n        return EncNetLoss(**kwargs)\n    elif model == \'icnet\':\n        return ICNetLoss(**kwargs)\n    else:\n        return MixSoftmaxCrossEntropyLoss(**kwargs)\n'"
core/utils/lr_scheduler.py,4,"b'""""""Popular Learning Rate Schedulers""""""\nfrom __future__ import division\nimport math\nimport torch\n\nfrom bisect import bisect_right\n\n__all__ = [\'LRScheduler\', \'WarmupMultiStepLR\', \'WarmupPolyLR\']\n\n\nclass LRScheduler(object):\n    r""""""Learning Rate Scheduler\n\n    Parameters\n    ----------\n    mode : str\n        Modes for learning rate scheduler.\n        Currently it supports \'constant\', \'step\', \'linear\', \'poly\' and \'cosine\'.\n    base_lr : float\n        Base learning rate, i.e. the starting learning rate.\n    target_lr : float\n        Target learning rate, i.e. the ending learning rate.\n        With constant mode target_lr is ignored.\n    niters : int\n        Number of iterations to be scheduled.\n    nepochs : int\n        Number of epochs to be scheduled.\n    iters_per_epoch : int\n        Number of iterations in each epoch.\n    offset : int\n        Number of iterations before this scheduler.\n    power : float\n        Power parameter of poly scheduler.\n    step_iter : list\n        A list of iterations to decay the learning rate.\n    step_epoch : list\n        A list of epochs to decay the learning rate.\n    step_factor : float\n        Learning rate decay factor.\n    """"""\n\n    def __init__(self, mode, base_lr=0.01, target_lr=0, niters=0, nepochs=0, iters_per_epoch=0,\n                 offset=0, power=0.9, step_iter=None, step_epoch=None, step_factor=0.1, warmup_epochs=0):\n        super(LRScheduler, self).__init__()\n        assert (mode in [\'constant\', \'step\', \'linear\', \'poly\', \'cosine\'])\n\n        if mode == \'step\':\n            assert (step_iter is not None or step_epoch is not None)\n        self.niters = niters\n        self.step = step_iter\n        epoch_iters = nepochs * iters_per_epoch\n        if epoch_iters > 0:\n            self.niters = epoch_iters\n            if step_epoch is not None:\n                self.step = [s * iters_per_epoch for s in step_epoch]\n\n        self.step_factor = step_factor\n        self.base_lr = base_lr\n        self.target_lr = base_lr if mode == \'constant\' else target_lr\n        self.offset = offset\n        self.power = power\n        self.warmup_iters = warmup_epochs * iters_per_epoch\n        self.mode = mode\n\n    def __call__(self, optimizer, num_update):\n        self.update(num_update)\n        assert self.learning_rate >= 0\n        self._adjust_learning_rate(optimizer, self.learning_rate)\n\n    def update(self, num_update):\n        N = self.niters - 1\n        T = num_update - self.offset\n        T = min(max(0, T), N)\n\n        if self.mode == \'constant\':\n            factor = 0\n        elif self.mode == \'linear\':\n            factor = 1 - T / N\n        elif self.mode == \'poly\':\n            factor = pow(1 - T / N, self.power)\n        elif self.mode == \'cosine\':\n            factor = (1 + math.cos(math.pi * T / N)) / 2\n        elif self.mode == \'step\':\n            if self.step is not None:\n                count = sum([1 for s in self.step if s <= T])\n                factor = pow(self.step_factor, count)\n            else:\n                factor = 1\n        else:\n            raise NotImplementedError\n\n        # warm up lr schedule\n        if self.warmup_iters > 0 and T < self.warmup_iters:\n            factor = factor * 1.0 * T / self.warmup_iters\n\n        if self.mode == \'step\':\n            self.learning_rate = self.base_lr * factor\n        else:\n            self.learning_rate = self.target_lr + (self.base_lr - self.target_lr) * factor\n\n    def _adjust_learning_rate(self, optimizer, lr):\n        optimizer.param_groups[0][\'lr\'] = lr\n        # enlarge the lr at the head\n        for i in range(1, len(optimizer.param_groups)):\n            optimizer.param_groups[i][\'lr\'] = lr * 10\n\n\n# separating MultiStepLR with WarmupLR\n# but the current LRScheduler design doesn\'t allow it\n# reference: https://github.com/facebookresearch/maskrcnn-benchmark/blob/master/maskrcnn_benchmark/solver/lr_scheduler.py\nclass WarmupMultiStepLR(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(self, optimizer, milestones, gamma=0.1, warmup_factor=1.0 / 3,\n                 warmup_iters=500, warmup_method=""linear"", last_epoch=-1):\n        super(WarmupMultiStepLR, self).__init__(optimizer, last_epoch)\n        if not list(milestones) == sorted(milestones):\n            raise ValueError(\n                ""Milestones should be a list of"" "" increasing integers. Got {}"", milestones)\n        if warmup_method not in (""constant"", ""linear""):\n            raise ValueError(\n                ""Only \'constant\' or \'linear\' warmup_method accepted got {}"".format(warmup_method))\n\n        self.milestones = milestones\n        self.gamma = gamma\n        self.warmup_factor = warmup_factor\n        self.warmup_iters = warmup_iters\n        self.warmup_method = warmup_method\n\n    def get_lr(self):\n        warmup_factor = 1\n        if self.last_epoch < self.warmup_iters:\n            if self.warmup_method == \'constant\':\n                warmup_factor = self.warmup_factor\n            elif self.warmup_factor == \'linear\':\n                alpha = float(self.last_epoch) / self.warmup_iters\n                warmup_factor = self.warmup_factor * (1 - alpha) + alpha\n        return [base_lr * warmup_factor * self.gamma ** bisect_right(self.milestones, self.last_epoch)\n                for base_lr in self.base_lrs]\n\n\nclass WarmupPolyLR(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(self, optimizer, target_lr=0, max_iters=0, power=0.9, warmup_factor=1.0 / 3,\n                 warmup_iters=500, warmup_method=\'linear\', last_epoch=-1):\n        if warmup_method not in (""constant"", ""linear""):\n            raise ValueError(\n                ""Only \'constant\' or \'linear\' warmup_method accepted ""\n                ""got {}"".format(warmup_method))\n\n        self.target_lr = target_lr\n        self.max_iters = max_iters\n        self.power = power\n        self.warmup_factor = warmup_factor\n        self.warmup_iters = warmup_iters\n        self.warmup_method = warmup_method\n\n        super(WarmupPolyLR, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        N = self.max_iters - self.warmup_iters\n        T = self.last_epoch - self.warmup_iters\n        if self.last_epoch < self.warmup_iters:\n            if self.warmup_method == \'constant\':\n                warmup_factor = self.warmup_factor\n            elif self.warmup_method == \'linear\':\n                alpha = float(self.last_epoch) / self.warmup_iters\n                warmup_factor = self.warmup_factor * (1 - alpha) + alpha\n            else:\n                raise ValueError(""Unknown warmup type."")\n            return [self.target_lr + (base_lr - self.target_lr) * warmup_factor for base_lr in self.base_lrs]\n        factor = pow(1 - T / N, self.power)\n        return [self.target_lr + (base_lr - self.target_lr) * factor for base_lr in self.base_lrs]\n\n\nif __name__ == \'__main__\':\n    import torch\n    import torch.nn as nn\n\n    model = nn.Conv2d(16, 16, 3, 1, 1)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n    lr_scheduler = WarmupPolyLR(optimizer, niters=1000)\n'"
core/utils/parallel.py,11,"b'""""""Utils for Semantic Segmentation""""""\nimport threading\nimport torch\nimport torch.cuda.comm as comm\nfrom torch.nn.parallel.data_parallel import DataParallel\nfrom torch.nn.parallel._functions import Broadcast\nfrom torch.autograd import Function\n\n__all__ = [\'DataParallelModel\', \'DataParallelCriterion\']\n\n\nclass Reduce(Function):\n    @staticmethod\n    def forward(ctx, *inputs):\n        ctx.target_gpus = [inputs[i].get_device() for i in range(len(inputs))]\n        inputs = sorted(inputs, key=lambda i: i.get_device())\n        return comm.reduce_add(inputs)\n\n    @staticmethod\n    def backward(ctx, gradOutputs):\n        return Broadcast.apply(ctx.target_gpus, gradOutputs)\n\n\nclass DataParallelModel(DataParallel):\n    """"""Data parallelism\n\n    Hide the difference of single/multiple GPUs to the user.\n    In the forward pass, the module is replicated on each device,\n    and each replica handles a portion of the input. During the backwards\n    pass, gradients from each replica are summed into the original module.\n\n    The batch size should be larger than the number of GPUs used.\n\n    Parameters\n    ----------\n    module : object\n        Network to be parallelized.\n    sync : bool\n        enable synchronization (default: False).\n    Inputs:\n        - **inputs**: list of input\n    Outputs:\n        - **outputs**: list of output\n    Example::\n        >>> net = DataParallelModel(model, device_ids=[0, 1, 2])\n        >>> output = net(input_var)  # input_var can be on any device, including CPU\n    """"""\n\n    def gather(self, outputs, output_device):\n        return outputs\n\n    def replicate(self, module, device_ids):\n        modules = super(DataParallelModel, self).replicate(module, device_ids)\n        return modules\n\n\n# Reference: https://github.com/zhanghang1989/PyTorch-Encoding/blob/master/encoding/parallel.py\nclass DataParallelCriterion(DataParallel):\n    """"""\n    Calculate loss in multiple-GPUs, which balance the memory usage for\n    Semantic Segmentation.\n\n    The targets are splitted across the specified devices by chunking in\n    the batch dimension. Please use together with :class:`encoding.parallel.DataParallelModel`.\n\n    Example::\n        >>> net = DataParallelModel(model, device_ids=[0, 1, 2])\n        >>> criterion = DataParallelCriterion(criterion, device_ids=[0, 1, 2])\n        >>> y = net(x)\n        >>> loss = criterion(y, target)\n    """"""\n\n    def forward(self, inputs, *targets, **kwargs):\n        # the inputs should be the outputs of DataParallelModel\n        if not self.device_ids:\n            return self.module(inputs, *targets, **kwargs)\n        targets, kwargs = self.scatter(targets, kwargs, self.device_ids)\n        if len(self.device_ids) == 1:\n            return self.module(inputs, *targets[0], **kwargs[0])\n        replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\n        outputs = criterion_parallel_apply(replicas, inputs, targets, kwargs)\n        return Reduce.apply(*outputs) / len(outputs)\n\n\ndef get_a_var(obj):\n    if isinstance(obj, torch.Tensor):\n        return obj\n\n    if isinstance(obj, list) or isinstance(obj, tuple):\n        for result in map(get_a_var, obj):\n            if isinstance(result, torch.Tensor):\n                return result\n\n    if isinstance(obj, dict):\n        for result in map(get_a_var, obj.items()):\n            if isinstance(result, torch.Tensor):\n                return result\n    return None\n\n\ndef criterion_parallel_apply(modules, inputs, targets, kwargs_tup=None, devices=None):\n    r""""""Applies each `module` in :attr:`modules` in parallel on arguments\n    contained in :attr:`inputs` (positional), attr:\'targets\' (positional) and :attr:`kwargs_tup` (keyword)\n    on each of :attr:`devices`.\n\n    Args:\n        modules (Module): modules to be parallelized\n        inputs (tensor): inputs to the modules\n        targets (tensor): targets to the modules\n        devices (list of int or torch.device): CUDA devices\n    :attr:`modules`, :attr:`inputs`, :attr:\'targets\' :attr:`kwargs_tup` (if given), and\n    :attr:`devices` (if given) should all have same length. Moreover, each\n    element of :attr:`inputs` can either be a single object as the only argument\n    to a module, or a collection of positional arguments.\n    """"""\n    assert len(modules) == len(inputs)\n    assert len(targets) == len(inputs)\n    if kwargs_tup is not None:\n        assert len(modules) == len(kwargs_tup)\n    else:\n        kwargs_tup = ({},) * len(modules)\n    if devices is not None:\n        assert len(modules) == len(devices)\n    else:\n        devices = [None] * len(modules)\n    lock = threading.Lock()\n    results = {}\n    grad_enabled = torch.is_grad_enabled()\n\n    def _worker(i, module, input, target, kwargs, device=None):\n        torch.set_grad_enabled(grad_enabled)\n        if device is None:\n            device = get_a_var(input).get_device()\n        try:\n            with torch.cuda.device(device):\n                output = module(*(list(input) + target), **kwargs)\n            with lock:\n                results[i] = output\n        except Exception as e:\n            with lock:\n                results[i] = e\n\n    if len(modules) > 1:\n        threads = [threading.Thread(target=_worker,\n                                    args=(i, module, input, target, kwargs, device))\n                   for i, (module, input, target, kwargs, device) in\n                   enumerate(zip(modules, inputs, targets, kwargs_tup, devices))]\n\n        for thread in threads:\n            thread.start()\n        for thread in threads:\n            thread.join()\n    else:\n        _worker(0, modules[0], inputs[0], targets[0], kwargs_tup[0], devices[0])\n\n    outputs = []\n    for i in range(len(inputs)):\n        output = results[i]\n        if isinstance(output, Exception):\n            raise output\n        outputs.append(output)\n    return outputs\n'"
core/utils/score.py,11,"b'""""""Evaluation Metrics for Semantic Segmentation""""""\nimport torch\nimport numpy as np\n\n__all__ = [\'SegmentationMetric\', \'batch_pix_accuracy\', \'batch_intersection_union\',\n           \'pixelAccuracy\', \'intersectionAndUnion\', \'hist_info\', \'compute_score\']\n\n\nclass SegmentationMetric(object):\n    """"""Computes pixAcc and mIoU metric scores\n    """"""\n\n    def __init__(self, nclass):\n        super(SegmentationMetric, self).__init__()\n        self.nclass = nclass\n        self.reset()\n\n    def update(self, preds, labels):\n        """"""Updates the internal evaluation result.\n\n        Parameters\n        ----------\n        labels : \'NumpyArray\' or list of `NumpyArray`\n            The labels of the data.\n        preds : \'NumpyArray\' or list of `NumpyArray`\n            Predicted values.\n        """"""\n\n        def evaluate_worker(self, pred, label):\n            correct, labeled = batch_pix_accuracy(pred, label)\n            inter, union = batch_intersection_union(pred, label, self.nclass)\n\n            self.total_correct += correct\n            self.total_label += labeled\n            if self.total_inter.device != inter.device:\n                self.total_inter = self.total_inter.to(inter.device)\n                self.total_union = self.total_union.to(union.device)\n            self.total_inter += inter\n            self.total_union += union\n\n        if isinstance(preds, torch.Tensor):\n            evaluate_worker(self, preds, labels)\n        elif isinstance(preds, (list, tuple)):\n            for (pred, label) in zip(preds, labels):\n                evaluate_worker(self, pred, label)\n\n    def get(self):\n        """"""Gets the current evaluation result.\n\n        Returns\n        -------\n        metrics : tuple of float\n            pixAcc and mIoU\n        """"""\n        pixAcc = 1.0 * self.total_correct / (2.220446049250313e-16 + self.total_label)  # remove np.spacing(1)\n        IoU = 1.0 * self.total_inter / (2.220446049250313e-16 + self.total_union)\n        mIoU = IoU.mean().item()\n        return pixAcc, mIoU\n\n    def reset(self):\n        """"""Resets the internal evaluation result to initial state.""""""\n        self.total_inter = torch.zeros(self.nclass)\n        self.total_union = torch.zeros(self.nclass)\n        self.total_correct = 0\n        self.total_label = 0\n\n\n# pytorch version\ndef batch_pix_accuracy(output, target):\n    """"""PixAcc""""""\n    # inputs are numpy array, output 4D, target 3D\n    predict = torch.argmax(output.long(), 1) + 1\n    target = target.long() + 1\n\n    pixel_labeled = torch.sum(target > 0).item()\n    pixel_correct = torch.sum((predict == target) * (target > 0)).item()\n    assert pixel_correct <= pixel_labeled, ""Correct area should be smaller than Labeled""\n    return pixel_correct, pixel_labeled\n\n\ndef batch_intersection_union(output, target, nclass):\n    """"""mIoU""""""\n    # inputs are numpy array, output 4D, target 3D\n    mini = 1\n    maxi = nclass\n    nbins = nclass\n    predict = torch.argmax(output, 1) + 1\n    target = target.float() + 1\n\n    predict = predict.float() * (target > 0).float()\n    intersection = predict * (predict == target).float()\n    # areas of intersection and union\n    # element 0 in intersection occur the main difference from np.bincount. set boundary to -1 is necessary.\n    area_inter = torch.histc(intersection.cpu(), bins=nbins, min=mini, max=maxi)\n    area_pred = torch.histc(predict.cpu(), bins=nbins, min=mini, max=maxi)\n    area_lab = torch.histc(target.cpu(), bins=nbins, min=mini, max=maxi)\n    area_union = area_pred + area_lab - area_inter\n    assert torch.sum(area_inter > area_union).item() == 0, ""Intersection area should be smaller than Union area""\n    return area_inter.float(), area_union.float()\n\n\ndef pixelAccuracy(imPred, imLab):\n    """"""\n    This function takes the prediction and label of a single image, returns pixel-wise accuracy\n    To compute over many images do:\n    for i = range(Nimages):\n         (pixel_accuracy[i], pixel_correct[i], pixel_labeled[i]) = \\\n            pixelAccuracy(imPred[i], imLab[i])\n    mean_pixel_accuracy = 1.0 * np.sum(pixel_correct) / (np.spacing(1) + np.sum(pixel_labeled))\n    """"""\n    # Remove classes from unlabeled pixels in gt image.\n    # We should not penalize detections in unlabeled portions of the image.\n    pixel_labeled = np.sum(imLab >= 0)\n    pixel_correct = np.sum((imPred == imLab) * (imLab >= 0))\n    pixel_accuracy = 1.0 * pixel_correct / pixel_labeled\n    return (pixel_accuracy, pixel_correct, pixel_labeled)\n\n\ndef intersectionAndUnion(imPred, imLab, numClass):\n    """"""\n    This function takes the prediction and label of a single image,\n    returns intersection and union areas for each class\n    To compute over many images do:\n    for i in range(Nimages):\n        (area_intersection[:,i], area_union[:,i]) = intersectionAndUnion(imPred[i], imLab[i])\n    IoU = 1.0 * np.sum(area_intersection, axis=1) / np.sum(np.spacing(1)+area_union, axis=1)\n    """"""\n    # Remove classes from unlabeled pixels in gt image.\n    # We should not penalize detections in unlabeled portions of the image.\n    imPred = imPred * (imLab >= 0)\n\n    # Compute area intersection:\n    intersection = imPred * (imPred == imLab)\n    (area_intersection, _) = np.histogram(intersection, bins=numClass, range=(1, numClass))\n\n    # Compute area union:\n    (area_pred, _) = np.histogram(imPred, bins=numClass, range=(1, numClass))\n    (area_lab, _) = np.histogram(imLab, bins=numClass, range=(1, numClass))\n    area_union = area_pred + area_lab - area_intersection\n    return (area_intersection, area_union)\n\n\ndef hist_info(pred, label, num_cls):\n    assert pred.shape == label.shape\n    k = (label >= 0) & (label < num_cls)\n    labeled = np.sum(k)\n    correct = np.sum((pred[k] == label[k]))\n\n    return np.bincount(num_cls * label[k].astype(int) + pred[k], minlength=num_cls ** 2).reshape(num_cls,\n                                                                                                 num_cls), labeled, correct\n\n\ndef compute_score(hist, correct, labeled):\n    iu = np.diag(hist) / (hist.sum(1) + hist.sum(0) - np.diag(hist))\n    mean_IU = np.nanmean(iu)\n    mean_IU_no_back = np.nanmean(iu[1:])\n    freq = hist.sum(1) / hist.sum()\n    freq_IU = (iu[freq > 0] * freq[freq > 0]).sum()\n    mean_pixel_acc = correct / labeled\n\n    return iu, mean_IU, mean_IU_no_back, mean_pixel_acc\n'"
core/utils/visualize.py,0,"b'import os\nimport numpy as np\nfrom PIL import Image\n\n__all__ = [\'get_color_pallete\', \'print_iou\', \'set_img_color\',\n           \'show_prediction\', \'show_colorful_images\', \'save_colorful_images\']\n\n\ndef print_iou(iu, mean_pixel_acc, class_names=None, show_no_back=False):\n    n = iu.size\n    lines = []\n    for i in range(n):\n        if class_names is None:\n            cls = \'Class %d:\' % (i + 1)\n        else:\n            cls = \'%d %s\' % (i + 1, class_names[i])\n        # lines.append(\'%-8s: %.3f%%\' % (cls, iu[i] * 100))\n    mean_IU = np.nanmean(iu)\n    mean_IU_no_back = np.nanmean(iu[1:])\n    if show_no_back:\n        lines.append(\'mean_IU: %.3f%% || mean_IU_no_back: %.3f%% || mean_pixel_acc: %.3f%%\' % (\n            mean_IU * 100, mean_IU_no_back * 100, mean_pixel_acc * 100))\n    else:\n        lines.append(\'mean_IU: %.3f%% || mean_pixel_acc: %.3f%%\' % (mean_IU * 100, mean_pixel_acc * 100))\n    lines.append(\'=================================================\')\n    line = ""\\n"".join(lines)\n\n    print(line)\n\n\ndef set_img_color(img, label, colors, background=0, show255=False):\n    for i in range(len(colors)):\n        if i != background:\n            img[np.where(label == i)] = colors[i]\n    if show255:\n        img[np.where(label == 255)] = 255\n\n    return img\n\n\ndef show_prediction(img, pred, colors, background=0):\n    im = np.array(img, np.uint8)\n    set_img_color(im, pred, colors, background)\n    out = np.array(im)\n\n    return out\n\n\ndef show_colorful_images(prediction, palettes):\n    im = Image.fromarray(palettes[prediction.astype(\'uint8\').squeeze()])\n    im.show()\n\n\ndef save_colorful_images(prediction, filename, output_dir, palettes):\n    \'\'\'\n    :param prediction: [B, H, W, C]\n    \'\'\'\n    im = Image.fromarray(palettes[prediction.astype(\'uint8\').squeeze()])\n    fn = os.path.join(output_dir, filename)\n    out_dir = os.path.split(fn)[0]\n    if not os.path.exists(out_dir):\n        os.mkdir(out_dir)\n    im.save(fn)\n\n\ndef get_color_pallete(npimg, dataset=\'pascal_voc\'):\n    """"""Visualize image.\n\n    Parameters\n    ----------\n    npimg : numpy.ndarray\n        Single channel image with shape `H, W, 1`.\n    dataset : str, default: \'pascal_voc\'\n        The dataset that model pretrained on. (\'pascal_voc\', \'ade20k\')\n    Returns\n    -------\n    out_img : PIL.Image\n        Image with color pallete\n    """"""\n    # recovery boundary\n    if dataset in (\'pascal_voc\', \'pascal_aug\'):\n        npimg[npimg == -1] = 255\n    # put colormap\n    if dataset == \'ade20k\':\n        npimg = npimg + 1\n        out_img = Image.fromarray(npimg.astype(\'uint8\'))\n        out_img.putpalette(adepallete)\n        return out_img\n    elif dataset == \'citys\':\n        out_img = Image.fromarray(npimg.astype(\'uint8\'))\n        out_img.putpalette(cityspallete)\n        return out_img\n    out_img = Image.fromarray(npimg.astype(\'uint8\'))\n    out_img.putpalette(vocpallete)\n    return out_img\n\n\ndef _getvocpallete(num_cls):\n    n = num_cls\n    pallete = [0] * (n * 3)\n    for j in range(0, n):\n        lab = j\n        pallete[j * 3 + 0] = 0\n        pallete[j * 3 + 1] = 0\n        pallete[j * 3 + 2] = 0\n        i = 0\n        while (lab > 0):\n            pallete[j * 3 + 0] |= (((lab >> 0) & 1) << (7 - i))\n            pallete[j * 3 + 1] |= (((lab >> 1) & 1) << (7 - i))\n            pallete[j * 3 + 2] |= (((lab >> 2) & 1) << (7 - i))\n            i = i + 1\n            lab >>= 3\n    return pallete\n\n\nvocpallete = _getvocpallete(256)\n\nadepallete = [\n    0, 0, 0, 120, 120, 120, 180, 120, 120, 6, 230, 230, 80, 50, 50, 4, 200, 3, 120, 120, 80, 140, 140, 140, 204,\n    5, 255, 230, 230, 230, 4, 250, 7, 224, 5, 255, 235, 255, 7, 150, 5, 61, 120, 120, 70, 8, 255, 51, 255, 6, 82,\n    143, 255, 140, 204, 255, 4, 255, 51, 7, 204, 70, 3, 0, 102, 200, 61, 230, 250, 255, 6, 51, 11, 102, 255, 255,\n    7, 71, 255, 9, 224, 9, 7, 230, 220, 220, 220, 255, 9, 92, 112, 9, 255, 8, 255, 214, 7, 255, 224, 255, 184, 6,\n    10, 255, 71, 255, 41, 10, 7, 255, 255, 224, 255, 8, 102, 8, 255, 255, 61, 6, 255, 194, 7, 255, 122, 8, 0, 255,\n    20, 255, 8, 41, 255, 5, 153, 6, 51, 255, 235, 12, 255, 160, 150, 20, 0, 163, 255, 140, 140, 140, 250, 10, 15,\n    20, 255, 0, 31, 255, 0, 255, 31, 0, 255, 224, 0, 153, 255, 0, 0, 0, 255, 255, 71, 0, 0, 235, 255, 0, 173, 255,\n    31, 0, 255, 11, 200, 200, 255, 82, 0, 0, 255, 245, 0, 61, 255, 0, 255, 112, 0, 255, 133, 255, 0, 0, 255, 163,\n    0, 255, 102, 0, 194, 255, 0, 0, 143, 255, 51, 255, 0, 0, 82, 255, 0, 255, 41, 0, 255, 173, 10, 0, 255, 173, 255,\n    0, 0, 255, 153, 255, 92, 0, 255, 0, 255, 255, 0, 245, 255, 0, 102, 255, 173, 0, 255, 0, 20, 255, 184, 184, 0,\n    31, 255, 0, 255, 61, 0, 71, 255, 255, 0, 204, 0, 255, 194, 0, 255, 82, 0, 10, 255, 0, 112, 255, 51, 0, 255, 0,\n    194, 255, 0, 122, 255, 0, 255, 163, 255, 153, 0, 0, 255, 10, 255, 112, 0, 143, 255, 0, 82, 0, 255, 163, 255,\n    0, 255, 235, 0, 8, 184, 170, 133, 0, 255, 0, 255, 92, 184, 0, 255, 255, 0, 31, 0, 184, 255, 0, 214, 255, 255,\n    0, 112, 92, 255, 0, 0, 224, 255, 112, 224, 255, 70, 184, 160, 163, 0, 255, 153, 0, 255, 71, 255, 0, 255, 0,\n    163, 255, 204, 0, 255, 0, 143, 0, 255, 235, 133, 255, 0, 255, 0, 235, 245, 0, 255, 255, 0, 122, 255, 245, 0,\n    10, 190, 212, 214, 255, 0, 0, 204, 255, 20, 0, 255, 255, 255, 0, 0, 153, 255, 0, 41, 255, 0, 255, 204, 41, 0,\n    255, 41, 255, 0, 173, 0, 255, 0, 245, 255, 71, 0, 255, 122, 0, 255, 0, 255, 184, 0, 92, 255, 184, 255, 0, 0,\n    133, 255, 255, 214, 0, 25, 194, 194, 102, 255, 0, 92, 0, 255]\n\ncityspallete = [\n    128, 64, 128,\n    244, 35, 232,\n    70, 70, 70,\n    102, 102, 156,\n    190, 153, 153,\n    153, 153, 153,\n    250, 170, 30,\n    220, 220, 0,\n    107, 142, 35,\n    152, 251, 152,\n    0, 130, 180,\n    220, 20, 60,\n    255, 0, 0,\n    0, 0, 142,\n    0, 0, 70,\n    0, 60, 100,\n    0, 80, 100,\n    0, 0, 230,\n    119, 11, 32,\n]\n'"
core/data/dataloader/__init__.py,0,"b'""""""\nThis module provides data loaders and transformers for popular vision datasets.\n""""""\nfrom .mscoco import COCOSegmentation\nfrom .cityscapes import CitySegmentation\nfrom .ade import ADE20KSegmentation\nfrom .pascal_voc import VOCSegmentation\nfrom .pascal_aug import VOCAugSegmentation\nfrom .sbu_shadow import SBUSegmentation\n\ndatasets = {\n    \'ade20k\': ADE20KSegmentation,\n    \'pascal_voc\': VOCSegmentation,\n    \'pascal_aug\': VOCAugSegmentation,\n    \'coco\': COCOSegmentation,\n    \'citys\': CitySegmentation,\n    \'sbu\': SBUSegmentation,\n}\n\n\ndef get_segmentation_dataset(name, **kwargs):\n    """"""Segmentation Datasets""""""\n    return datasets[name.lower()](**kwargs)\n'"
core/data/dataloader/ade.py,2,"b'""""""Pascal ADE20K Semantic Segmentation Dataset.""""""\nimport os\nimport torch\nimport numpy as np\n\nfrom PIL import Image\nfrom .segbase import SegmentationDataset\n\n\nclass ADE20KSegmentation(SegmentationDataset):\n    """"""ADE20K Semantic Segmentation Dataset.\n\n    Parameters\n    ----------\n    root : string\n        Path to ADE20K folder. Default is \'./datasets/ade\'\n    split: string\n        \'train\', \'val\' or \'test\'\n    transform : callable, optional\n        A function that transforms the image\n    Examples\n    --------\n    >>> from torchvision import transforms\n    >>> import torch.utils.data as data\n    >>> # Transforms for Normalization\n    >>> input_transform = transforms.Compose([\n    >>>     transforms.ToTensor(),\n    >>>     transforms.Normalize((.485, .456, .406), (.229, .224, .225)),\n    >>> ])\n    >>> # Create Dataset\n    >>> trainset = ADE20KSegmentation(split=\'train\', transform=input_transform)\n    >>> # Create Training Loader\n    >>> train_data = data.DataLoader(\n    >>>     trainset, 4, shuffle=True,\n    >>>     num_workers=4)\n    """"""\n    BASE_DIR = \'ADEChallengeData2016\'\n    NUM_CLASS = 150\n\n    def __init__(self, root=\'../datasets/ade\', split=\'test\', mode=None, transform=None, **kwargs):\n        super(ADE20KSegmentation, self).__init__(root, split, mode, transform, **kwargs)\n        root = os.path.join(root, self.BASE_DIR)\n        assert os.path.exists(root), ""Please setup the dataset using ../datasets/ade20k.py""\n        self.images, self.masks = _get_ade20k_pairs(root, split)\n        assert (len(self.images) == len(self.masks))\n        if len(self.images) == 0:\n            raise RuntimeError(""Found 0 images in subfolders of:"" + root + ""\\n"")\n        print(\'Found {} images in the folder {}\'.format(len(self.images), root))\n\n    def __getitem__(self, index):\n        img = Image.open(self.images[index]).convert(\'RGB\')\n        if self.mode == \'test\':\n            img = self._img_transform(img)\n            if self.transform is not None:\n                img = self.transform(img)\n            return img, os.path.basename(self.images[index])\n        mask = Image.open(self.masks[index])\n        # synchrosized transform\n        if self.mode == \'train\':\n            img, mask = self._sync_transform(img, mask)\n        elif self.mode == \'val\':\n            img, mask = self._val_sync_transform(img, mask)\n        else:\n            assert self.mode == \'testval\'\n            img, mask = self._img_transform(img), self._mask_transform(mask)\n        # general resize, normalize and to Tensor\n        if self.transform is not None:\n            img = self.transform(img)\n        return img, mask, os.path.basename(self.images[index])\n\n    def _mask_transform(self, mask):\n        return torch.LongTensor(np.array(mask).astype(\'int32\') - 1)\n\n    def __len__(self):\n        return len(self.images)\n\n    @property\n    def pred_offset(self):\n        return 1\n\n    @property\n    def classes(self):\n        """"""Category names.""""""\n        return (""wall"", ""building, edifice"", ""sky"", ""floor, flooring"", ""tree"",\n                ""ceiling"", ""road, route"", ""bed"", ""windowpane, window"", ""grass"",\n                ""cabinet"", ""sidewalk, pavement"",\n                ""person, individual, someone, somebody, mortal, soul"",\n                ""earth, ground"", ""door, double door"", ""table"", ""mountain, mount"",\n                ""plant, flora, plant life"", ""curtain, drape, drapery, mantle, pall"",\n                ""chair"", ""car, auto, automobile, machine, motorcar"",\n                ""water"", ""painting, picture"", ""sofa, couch, lounge"", ""shelf"",\n                ""house"", ""sea"", ""mirror"", ""rug, carpet, carpeting"", ""field"", ""armchair"",\n                ""seat"", ""fence, fencing"", ""desk"", ""rock, stone"", ""wardrobe, closet, press"",\n                ""lamp"", ""bathtub, bathing tub, bath, tub"", ""railing, rail"", ""cushion"",\n                ""base, pedestal, stand"", ""box"", ""column, pillar"", ""signboard, sign"",\n                ""chest of drawers, chest, bureau, dresser"", ""counter"", ""sand"", ""sink"",\n                ""skyscraper"", ""fireplace, hearth, open fireplace"", ""refrigerator, icebox"",\n                ""grandstand, covered stand"", ""path"", ""stairs, steps"", ""runway"",\n                ""case, display case, showcase, vitrine"",\n                ""pool table, billiard table, snooker table"", ""pillow"",\n                ""screen door, screen"", ""stairway, staircase"", ""river"", ""bridge, span"",\n                ""bookcase"", ""blind, screen"", ""coffee table, cocktail table"",\n                ""toilet, can, commode, crapper, pot, potty, stool, throne"",\n                ""flower"", ""book"", ""hill"", ""bench"", ""countertop"",\n                ""stove, kitchen stove, range, kitchen range, cooking stove"",\n                ""palm, palm tree"", ""kitchen island"",\n                ""computer, computing machine, computing device, data processor, ""\n                ""electronic computer, information processing system"",\n                ""swivel chair"", ""boat"", ""bar"", ""arcade machine"",\n                ""hovel, hut, hutch, shack, shanty"",\n                ""bus, autobus, coach, charabanc, double-decker, jitney, motorbus, ""\n                ""motorcoach, omnibus, passenger vehicle"",\n                ""towel"", ""light, light source"", ""truck, motortruck"", ""tower"",\n                ""chandelier, pendant, pendent"", ""awning, sunshade, sunblind"",\n                ""streetlight, street lamp"", ""booth, cubicle, stall, kiosk"",\n                ""television receiver, television, television set, tv, tv set, idiot ""\n                ""box, boob tube, telly, goggle box"",\n                ""airplane, aeroplane, plane"", ""dirt track"",\n                ""apparel, wearing apparel, dress, clothes"",\n                ""pole"", ""land, ground, soil"",\n                ""bannister, banister, balustrade, balusters, handrail"",\n                ""escalator, moving staircase, moving stairway"",\n                ""ottoman, pouf, pouffe, puff, hassock"",\n                ""bottle"", ""buffet, counter, sideboard"",\n                ""poster, posting, placard, notice, bill, card"",\n                ""stage"", ""van"", ""ship"", ""fountain"",\n                ""conveyer belt, conveyor belt, conveyer, conveyor, transporter"",\n                ""canopy"", ""washer, automatic washer, washing machine"",\n                ""plaything, toy"", ""swimming pool, swimming bath, natatorium"",\n                ""stool"", ""barrel, cask"", ""basket, handbasket"", ""waterfall, falls"",\n                ""tent, collapsible shelter"", ""bag"", ""minibike, motorbike"", ""cradle"",\n                ""oven"", ""ball"", ""food, solid food"", ""step, stair"", ""tank, storage tank"",\n                ""trade name, brand name, brand, marque"", ""microwave, microwave oven"",\n                ""pot, flowerpot"", ""animal, animate being, beast, brute, creature, fauna"",\n                ""bicycle, bike, wheel, cycle"", ""lake"",\n                ""dishwasher, dish washer, dishwashing machine"",\n                ""screen, silver screen, projection screen"",\n                ""blanket, cover"", ""sculpture"", ""hood, exhaust hood"", ""sconce"", ""vase"",\n                ""traffic light, traffic signal, stoplight"", ""tray"",\n                ""ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, ""\n                ""dustbin, trash barrel, trash bin"",\n                ""fan"", ""pier, wharf, wharfage, dock"", ""crt screen"",\n                ""plate"", ""monitor, monitoring device"", ""bulletin board, notice board"",\n                ""shower"", ""radiator"", ""glass, drinking glass"", ""clock"", ""flag"")\n\n\ndef _get_ade20k_pairs(folder, mode=\'train\'):\n    img_paths = []\n    mask_paths = []\n    if mode == \'train\':\n        img_folder = os.path.join(folder, \'images/training\')\n        mask_folder = os.path.join(folder, \'annotations/training\')\n    else:\n        img_folder = os.path.join(folder, \'images/validation\')\n        mask_folder = os.path.join(folder, \'annotations/validation\')\n    for filename in os.listdir(img_folder):\n        basename, _ = os.path.splitext(filename)\n        if filename.endswith("".jpg""):\n            imgpath = os.path.join(img_folder, filename)\n            maskname = basename + \'.png\'\n            maskpath = os.path.join(mask_folder, maskname)\n            if os.path.isfile(maskpath):\n                img_paths.append(imgpath)\n                mask_paths.append(maskpath)\n            else:\n                print(\'cannot find the mask:\', maskpath)\n\n    return img_paths, mask_paths\n\n\nif __name__ == \'__main__\':\n    train_dataset = ADE20KSegmentation()\n'"
core/data/dataloader/cityscapes.py,2,"b'""""""Prepare Cityscapes dataset""""""\nimport os\nimport torch\nimport numpy as np\n\nfrom PIL import Image\nfrom .segbase import SegmentationDataset\n\n\nclass CitySegmentation(SegmentationDataset):\n    """"""Cityscapes Semantic Segmentation Dataset.\n\n    Parameters\n    ----------\n    root : string\n        Path to Cityscapes folder. Default is \'./datasets/citys\'\n    split: string\n        \'train\', \'val\' or \'test\'\n    transform : callable, optional\n        A function that transforms the image\n    Examples\n    --------\n    >>> from torchvision import transforms\n    >>> import torch.utils.data as data\n    >>> # Transforms for Normalization\n    >>> input_transform = transforms.Compose([\n    >>>     transforms.ToTensor(),\n    >>>     transforms.Normalize((.485, .456, .406), (.229, .224, .225)),\n    >>> ])\n    >>> # Create Dataset\n    >>> trainset = CitySegmentation(split=\'train\', transform=input_transform)\n    >>> # Create Training Loader\n    >>> train_data = data.DataLoader(\n    >>>     trainset, 4, shuffle=True,\n    >>>     num_workers=4)\n    """"""\n    BASE_DIR = \'cityscapes\'\n    NUM_CLASS = 19\n\n    def __init__(self, root=\'../datasets/citys\', split=\'train\', mode=None, transform=None, **kwargs):\n        super(CitySegmentation, self).__init__(root, split, mode, transform, **kwargs)\n        # self.root = os.path.join(root, self.BASE_DIR)\n        assert os.path.exists(self.root), ""Please setup the dataset using ../datasets/cityscapes.py""\n        self.images, self.mask_paths = _get_city_pairs(self.root, self.split)\n        assert (len(self.images) == len(self.mask_paths))\n        if len(self.images) == 0:\n            raise RuntimeError(""Found 0 images in subfolders of:"" + root + ""\\n"")\n        self.valid_classes = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22,\n                              23, 24, 25, 26, 27, 28, 31, 32, 33]\n        self._key = np.array([-1, -1, -1, -1, -1, -1,\n                              -1, -1, 0, 1, -1, -1,\n                              2, 3, 4, -1, -1, -1,\n                              5, -1, 6, 7, 8, 9,\n                              10, 11, 12, 13, 14, 15,\n                              -1, -1, 16, 17, 18])\n        self._mapping = np.array(range(-1, len(self._key) - 1)).astype(\'int32\')\n\n    def _class_to_index(self, mask):\n        # assert the value\n        values = np.unique(mask)\n        for value in values:\n            assert (value in self._mapping)\n        index = np.digitize(mask.ravel(), self._mapping, right=True)\n        return self._key[index].reshape(mask.shape)\n\n    def __getitem__(self, index):\n        img = Image.open(self.images[index]).convert(\'RGB\')\n        if self.mode == \'test\':\n            if self.transform is not None:\n                img = self.transform(img)\n            return img, os.path.basename(self.images[index])\n        mask = Image.open(self.mask_paths[index])\n        # synchrosized transform\n        if self.mode == \'train\':\n            img, mask = self._sync_transform(img, mask)\n        elif self.mode == \'val\':\n            img, mask = self._val_sync_transform(img, mask)\n        else:\n            assert self.mode == \'testval\'\n            img, mask = self._img_transform(img), self._mask_transform(mask)\n        # general resize, normalize and toTensor\n        if self.transform is not None:\n            img = self.transform(img)\n        return img, mask, os.path.basename(self.images[index])\n\n    def _mask_transform(self, mask):\n        target = self._class_to_index(np.array(mask).astype(\'int32\'))\n        return torch.LongTensor(np.array(target).astype(\'int32\'))\n\n    def __len__(self):\n        return len(self.images)\n\n    @property\n    def pred_offset(self):\n        return 0\n\n\ndef _get_city_pairs(folder, split=\'train\'):\n    def get_path_pairs(img_folder, mask_folder):\n        img_paths = []\n        mask_paths = []\n        for root, _, files in os.walk(img_folder):\n            for filename in files:\n                if filename.endswith(\'.png\'):\n                    imgpath = os.path.join(root, filename)\n                    foldername = os.path.basename(os.path.dirname(imgpath))\n                    maskname = filename.replace(\'leftImg8bit\', \'gtFine_labelIds\')\n                    maskpath = os.path.join(mask_folder, foldername, maskname)\n                    if os.path.isfile(imgpath) and os.path.isfile(maskpath):\n                        img_paths.append(imgpath)\n                        mask_paths.append(maskpath)\n                    else:\n                        print(\'cannot find the mask or image:\', imgpath, maskpath)\n        print(\'Found {} images in the folder {}\'.format(len(img_paths), img_folder))\n        return img_paths, mask_paths\n\n    if split in (\'train\', \'val\'):\n        img_folder = os.path.join(folder, \'leftImg8bit/\' + split)\n        mask_folder = os.path.join(folder, \'gtFine/\' + split)\n        img_paths, mask_paths = get_path_pairs(img_folder, mask_folder)\n        return img_paths, mask_paths\n    else:\n        assert split == \'trainval\'\n        print(\'trainval set\')\n        train_img_folder = os.path.join(folder, \'leftImg8bit/train\')\n        train_mask_folder = os.path.join(folder, \'gtFine/train\')\n        val_img_folder = os.path.join(folder, \'leftImg8bit/val\')\n        val_mask_folder = os.path.join(folder, \'gtFine/val\')\n        train_img_paths, train_mask_paths = get_path_pairs(train_img_folder, train_mask_folder)\n        val_img_paths, val_mask_paths = get_path_pairs(val_img_folder, val_mask_folder)\n        img_paths = train_img_paths + val_img_paths\n        mask_paths = train_mask_paths + val_mask_paths\n    return img_paths, mask_paths\n\n\nif __name__ == \'__main__\':\n    dataset = CitySegmentation()\n'"
core/data/dataloader/lip_parsing.py,1,"b'""""""Look into Person Dataset""""""\nimport os\nimport torch\nimport numpy as np\n\nfrom PIL import Image\nfrom core.data.dataloader.segbase import SegmentationDataset\n\n\nclass LIPSegmentation(SegmentationDataset):\n    """"""Look into person parsing dataset """"""\n\n    BASE_DIR = \'LIP\'\n    NUM_CLASS = 20\n\n    def __init__(self, root=\'../datasets/LIP\', split=\'train\', mode=None, transform=None, **kwargs):\n        super(LIPSegmentation, self).__init__(root, split, mode, transform, **kwargs)\n        _trainval_image_dir = os.path.join(root, \'TrainVal_images\')\n        _testing_image_dir = os.path.join(root, \'Testing_images\')\n        _trainval_mask_dir = os.path.join(root, \'TrainVal_parsing_annotations\')\n        if split == \'train\':\n            _image_dir = os.path.join(_trainval_image_dir, \'train_images\')\n            _mask_dir = os.path.join(_trainval_mask_dir, \'train_segmentations\')\n            _split_f = os.path.join(_trainval_image_dir, \'train_id.txt\')\n        elif split == \'val\':\n            _image_dir = os.path.join(_trainval_image_dir, \'val_images\')\n            _mask_dir = os.path.join(_trainval_mask_dir, \'val_segmentations\')\n            _split_f = os.path.join(_trainval_image_dir, \'val_id.txt\')\n        elif split == \'test\':\n            _image_dir = os.path.join(_testing_image_dir, \'testing_images\')\n            _split_f = os.path.join(_testing_image_dir, \'test_id.txt\')\n        else:\n            raise RuntimeError(\'Unknown dataset split.\')\n\n        self.images = []\n        self.masks = []\n        with open(os.path.join(_split_f), \'r\') as lines:\n            for line in lines:\n                _image = os.path.join(_image_dir, line.rstrip(\'\\n\') + \'.jpg\')\n                assert os.path.isfile(_image)\n                self.images.append(_image)\n                if split != \'test\':\n                    _mask = os.path.join(_mask_dir, line.rstrip(\'\\n\') + \'.png\')\n                    assert os.path.isfile(_mask)\n                    self.masks.append(_mask)\n\n        if split != \'test\':\n            assert (len(self.images) == len(self.masks))\n        print(\'Found {} {} images in the folder {}\'.format(len(self.images), split, root))\n\n    def __getitem__(self, index):\n        img = Image.open(self.images[index]).convert(\'RGB\')\n        if self.mode == \'test\':\n            img = self._img_transform(img)\n            if self.transform is not None:\n                img = self.transform(img)\n            return img, os.path.basename(self.images[index])\n        mask = Image.open(self.masks[index])\n        # synchronized transform\n        if self.mode == \'train\':\n            img, mask = self._sync_transform(img, mask)\n        elif self.mode == \'val\':\n            img, mask = self._val_sync_transform(img, mask)\n        else:\n            assert self.mode == \'testval\'\n            img, mask = self._img_transform(img), self._mask_transform(mask)\n        # general resize, normalize and toTensor\n        if self.transform is not None:\n            img = self.transform(img)\n\n        return img, mask, os.path.basename(self.images[index])\n\n    def __len__(self):\n        return len(self.images)\n\n    def _mask_transform(self, mask):\n        target = np.array(mask).astype(\'int32\')\n        return torch.from_numpy(target).long()\n\n    @property\n    def classes(self):\n        """"""Category name.""""""\n        return (\'background\', \'hat\', \'hair\', \'glove\', \'sunglasses\', \'upperclothes\',\n                \'dress\', \'coat\', \'socks\', \'pants\', \'jumpsuits\', \'scarf\', \'skirt\',\n                \'face\', \'leftArm\', \'rightArm\', \'leftLeg\', \'rightLeg\', \'leftShoe\',\n                \'rightShoe\')\n\n\nif __name__ == \'__main__\':\n    dataset = LIPSegmentation(base_size=280, crop_size=256)'"
core/data/dataloader/mscoco.py,2,"b'""""""MSCOCO Semantic Segmentation pretraining for VOC.""""""\nimport os\nimport pickle\nimport torch\nimport numpy as np\n\nfrom tqdm import trange\nfrom PIL import Image\nfrom .segbase import SegmentationDataset\n\n\nclass COCOSegmentation(SegmentationDataset):\n    """"""COCO Semantic Segmentation Dataset for VOC Pre-training.\n\n    Parameters\n    ----------\n    root : string\n        Path to ADE20K folder. Default is \'./datasets/coco\'\n    split: string\n        \'train\', \'val\' or \'test\'\n    transform : callable, optional\n        A function that transforms the image\n    Examples\n    --------\n    >>> from torchvision import transforms\n    >>> import torch.utils.data as data\n    >>> # Transforms for Normalization\n    >>> input_transform = transforms.Compose([\n    >>>     transforms.ToTensor(),\n    >>>     transforms.Normalize((.485, .456, .406), (.229, .224, .225)),\n    >>> ])\n    >>> # Create Dataset\n    >>> trainset = COCOSegmentation(split=\'train\', transform=input_transform)\n    >>> # Create Training Loader\n    >>> train_data = data.DataLoader(\n    >>>     trainset, 4, shuffle=True,\n    >>>     num_workers=4)\n    """"""\n    CAT_LIST = [0, 5, 2, 16, 9, 44, 6, 3, 17, 62, 21, 67, 18, 19, 4,\n                1, 64, 20, 63, 7, 72]\n    NUM_CLASS = 21\n\n    def __init__(self, root=\'../datasets/coco\', split=\'train\', mode=None, transform=None, **kwargs):\n        super(COCOSegmentation, self).__init__(root, split, mode, transform, **kwargs)\n        # lazy import pycocotools\n        from pycocotools.coco import COCO\n        from pycocotools import mask\n        if split == \'train\':\n            print(\'train set\')\n            ann_file = os.path.join(root, \'annotations/instances_train2017.json\')\n            ids_file = os.path.join(root, \'annotations/train_ids.mx\')\n            self.root = os.path.join(root, \'train2017\')\n        else:\n            print(\'val set\')\n            ann_file = os.path.join(root, \'annotations/instances_val2017.json\')\n            ids_file = os.path.join(root, \'annotations/val_ids.mx\')\n            self.root = os.path.join(root, \'val2017\')\n        self.coco = COCO(ann_file)\n        self.coco_mask = mask\n        if os.path.exists(ids_file):\n            with open(ids_file, \'rb\') as f:\n                self.ids = pickle.load(f)\n        else:\n            ids = list(self.coco.imgs.keys())\n            self.ids = self._preprocess(ids, ids_file)\n        self.transform = transform\n\n    def __getitem__(self, index):\n        coco = self.coco\n        img_id = self.ids[index]\n        img_metadata = coco.loadImgs(img_id)[0]\n        path = img_metadata[\'file_name\']\n        img = Image.open(os.path.join(self.root, path)).convert(\'RGB\')\n        cocotarget = coco.loadAnns(coco.getAnnIds(imgIds=img_id))\n        mask = Image.fromarray(self._gen_seg_mask(\n            cocotarget, img_metadata[\'height\'], img_metadata[\'width\']))\n        # synchrosized transform\n        if self.mode == \'train\':\n            img, mask = self._sync_transform(img, mask)\n        elif self.mode == \'val\':\n            img, mask = self._val_sync_transform(img, mask)\n        else:\n            assert self.mode == \'testval\'\n            img, mask = self._img_transform(img), self._mask_transform(mask)\n        # general resize, normalize and toTensor\n        if self.transform is not None:\n            img = self.transform(img)\n        return img, mask, os.path.basename(self.ids[index])\n\n    def _mask_transform(self, mask):\n        return torch.LongTensor(np.array(mask).astype(\'int32\'))\n\n    def _gen_seg_mask(self, target, h, w):\n        mask = np.zeros((h, w), dtype=np.uint8)\n        coco_mask = self.coco_mask\n        for instance in target:\n            rle = coco_mask.frPyObjects(instance[\'Segmentation\'], h, w)\n            m = coco_mask.decode(rle)\n            cat = instance[\'category_id\']\n            if cat in self.CAT_LIST:\n                c = self.CAT_LIST.index(cat)\n            else:\n                continue\n            if len(m.shape) < 3:\n                mask[:, :] += (mask == 0) * (m * c)\n            else:\n                mask[:, :] += (mask == 0) * (((np.sum(m, axis=2)) > 0) * c).astype(np.uint8)\n        return mask\n\n    def _preprocess(self, ids, ids_file):\n        print(""Preprocessing mask, this will take a while."" + \\\n              ""But don\'t worry, it only run once for each split."")\n        tbar = trange(len(ids))\n        new_ids = []\n        for i in tbar:\n            img_id = ids[i]\n            cocotarget = self.coco.loadAnns(self.coco.getAnnIds(imgIds=img_id))\n            img_metadata = self.coco.loadImgs(img_id)[0]\n            mask = self._gen_seg_mask(cocotarget, img_metadata[\'height\'], img_metadata[\'width\'])\n            # more than 1k pixels\n            if (mask > 0).sum() > 1000:\n                new_ids.append(img_id)\n            tbar.set_description(\'Doing: {}/{}, got {} qualified images\'. \\\n                                 format(i, len(ids), len(new_ids)))\n        print(\'Found number of qualified images: \', len(new_ids))\n        with open(ids_file, \'wb\') as f:\n            pickle.dump(new_ids, f)\n        return new_ids\n\n    @property\n    def classes(self):\n        """"""Category names.""""""\n        return (\'background\', \'airplane\', \'bicycle\', \'bird\', \'boat\', \'bottle\',\n                \'bus\', \'car\', \'cat\', \'chair\', \'cow\', \'diningtable\', \'dog\', \'horse\',\n                \'motorcycle\', \'person\', \'potted-plant\', \'sheep\', \'sofa\', \'train\',\n                \'tv\')\n'"
core/data/dataloader/pascal_aug.py,2,"b'""""""Pascal Augmented VOC Semantic Segmentation Dataset.""""""\nimport os\nimport torch\nimport scipy.io as sio\nimport numpy as np\n\nfrom PIL import Image\nfrom .segbase import SegmentationDataset\n\n\nclass VOCAugSegmentation(SegmentationDataset):\n    """"""Pascal VOC Augmented Semantic Segmentation Dataset.\n\n    Parameters\n    ----------\n    root : string\n        Path to VOCdevkit folder. Default is \'./datasets/voc\'\n    split: string\n        \'train\', \'val\' or \'test\'\n    transform : callable, optional\n        A function that transforms the image\n    Examples\n    --------\n    >>> from torchvision import transforms\n    >>> import torch.utils.data as data\n    >>> # Transforms for Normalization\n    >>> input_transform = transforms.Compose([\n    >>>     transforms.ToTensor(),\n    >>>     transforms.Normalize([.485, .456, .406], [.229, .224, .225]),\n    >>> ])\n    >>> # Create Dataset\n    >>> trainset = VOCAugSegmentation(split=\'train\', transform=input_transform)\n    >>> # Create Training Loader\n    >>> train_data = data.DataLoader(\n    >>>     trainset, 4, shuffle=True,\n    >>>     num_workers=4)\n    """"""\n    BASE_DIR = \'VOCaug/dataset/\'\n    NUM_CLASS = 21\n\n    def __init__(self, root=\'../datasets/voc\', split=\'train\', mode=None, transform=None, **kwargs):\n        super(VOCAugSegmentation, self).__init__(root, split, mode, transform, **kwargs)\n        # train/val/test splits are pre-cut\n        _voc_root = os.path.join(root, self.BASE_DIR)\n        _mask_dir = os.path.join(_voc_root, \'cls\')\n        _image_dir = os.path.join(_voc_root, \'img\')\n        if split == \'train\':\n            _split_f = os.path.join(_voc_root, \'trainval.txt\')\n        elif split == \'val\':\n            _split_f = os.path.join(_voc_root, \'val.txt\')\n        else:\n            raise RuntimeError(\'Unknown dataset split: {}\'.format(split))\n\n        self.images = []\n        self.masks = []\n        with open(os.path.join(_split_f), ""r"") as lines:\n            for line in lines:\n                _image = os.path.join(_image_dir, line.rstrip(\'\\n\') + "".jpg"")\n                assert os.path.isfile(_image)\n                self.images.append(_image)\n                _mask = os.path.join(_mask_dir, line.rstrip(\'\\n\') + "".mat"")\n                assert os.path.isfile(_mask)\n                self.masks.append(_mask)\n\n        assert (len(self.images) == len(self.masks))\n        print(\'Found {} images in the folder {}\'.format(len(self.images), _voc_root))\n\n    def __getitem__(self, index):\n        img = Image.open(self.images[index]).convert(\'RGB\')\n        target = self._load_mat(self.masks[index])\n        # synchrosized transform\n        if self.mode == \'train\':\n            img, target = self._sync_transform(img, target)\n        elif self.mode == \'val\':\n            img, target = self._val_sync_transform(img, target)\n        else:\n            raise RuntimeError(\'unknown mode for dataloader: {}\'.format(self.mode))\n        # general resize, normalize and toTensor\n        if self.transform is not None:\n            img = self.transform(img)\n        return img, target, os.path.basename(self.images[index])\n\n    def _mask_transform(self, mask):\n        return torch.LongTensor(np.array(mask).astype(\'int32\'))\n\n    def _load_mat(self, filename):\n        mat = sio.loadmat(filename, mat_dtype=True, squeeze_me=True, struct_as_record=False)\n        mask = mat[\'GTcls\'].Segmentation\n        return Image.fromarray(mask)\n\n    def __len__(self):\n        return len(self.images)\n\n    @property\n    def classes(self):\n        """"""Category names.""""""\n        return (\'background\', \'airplane\', \'bicycle\', \'bird\', \'boat\', \'bottle\',\n                \'bus\', \'car\', \'cat\', \'chair\', \'cow\', \'diningtable\', \'dog\', \'horse\',\n                \'motorcycle\', \'person\', \'potted-plant\', \'sheep\', \'sofa\', \'train\',\n                \'tv\')\n\n\nif __name__ == \'__main__\':\n    dataset = VOCAugSegmentation()'"
core/data/dataloader/pascal_voc.py,2,"b'""""""Pascal VOC Semantic Segmentation Dataset.""""""\nimport os\nimport torch\nimport numpy as np\n\nfrom PIL import Image\nfrom .segbase import SegmentationDataset\n\n\nclass VOCSegmentation(SegmentationDataset):\n    """"""Pascal VOC Semantic Segmentation Dataset.\n\n    Parameters\n    ----------\n    root : string\n        Path to VOCdevkit folder. Default is \'./datasets/VOCdevkit\'\n    split: string\n        \'train\', \'val\' or \'test\'\n    transform : callable, optional\n        A function that transforms the image\n    Examples\n    --------\n    >>> from torchvision import transforms\n    >>> import torch.utils.data as data\n    >>> # Transforms for Normalization\n    >>> input_transform = transforms.Compose([\n    >>>     transforms.ToTensor(),\n    >>>     transforms.Normalize([.485, .456, .406], [.229, .224, .225]),\n    >>> ])\n    >>> # Create Dataset\n    >>> trainset = VOCSegmentation(split=\'train\', transform=input_transform)\n    >>> # Create Training Loader\n    >>> train_data = data.DataLoader(\n    >>>     trainset, 4, shuffle=True,\n    >>>     num_workers=4)\n    """"""\n    BASE_DIR = \'VOC2012\'\n    NUM_CLASS = 21\n\n    def __init__(self, root=\'../datasets/voc\', split=\'train\', mode=None, transform=None, **kwargs):\n        super(VOCSegmentation, self).__init__(root, split, mode, transform, **kwargs)\n        _voc_root = os.path.join(root, self.BASE_DIR)\n        _mask_dir = os.path.join(_voc_root, \'SegmentationClass\')\n        _image_dir = os.path.join(_voc_root, \'JPEGImages\')\n        # train/val/test splits are pre-cut\n        _splits_dir = os.path.join(_voc_root, \'ImageSets/Segmentation\')\n        if split == \'train\':\n            _split_f = os.path.join(_splits_dir, \'train.txt\')\n        elif split == \'val\':\n            _split_f = os.path.join(_splits_dir, \'val.txt\')\n        elif split == \'test\':\n            _split_f = os.path.join(_splits_dir, \'test.txt\')\n        else:\n            raise RuntimeError(\'Unknown dataset split.\')\n\n        self.images = []\n        self.masks = []\n        with open(os.path.join(_split_f), ""r"") as lines:\n            for line in lines:\n                _image = os.path.join(_image_dir, line.rstrip(\'\\n\') + "".jpg"")\n                assert os.path.isfile(_image)\n                self.images.append(_image)\n                if split != \'test\':\n                    _mask = os.path.join(_mask_dir, line.rstrip(\'\\n\') + "".png"")\n                    assert os.path.isfile(_mask)\n                    self.masks.append(_mask)\n\n        if split != \'test\':\n            assert (len(self.images) == len(self.masks))\n        print(\'Found {} images in the folder {}\'.format(len(self.images), _voc_root))\n\n    def __getitem__(self, index):\n        img = Image.open(self.images[index]).convert(\'RGB\')\n        if self.mode == \'test\':\n            img = self._img_transform(img)\n            if self.transform is not None:\n                img = self.transform(img)\n            return img, os.path.basename(self.images[index])\n        mask = Image.open(self.masks[index])\n        # synchronized transform\n        if self.mode == \'train\':\n            img, mask = self._sync_transform(img, mask)\n        elif self.mode == \'val\':\n            img, mask = self._val_sync_transform(img, mask)\n        else:\n            assert self.mode == \'testval\'\n            img, mask = self._img_transform(img), self._mask_transform(mask)\n        # general resize, normalize and toTensor\n        if self.transform is not None:\n            img = self.transform(img)\n\n        return img, mask, os.path.basename(self.images[index])\n\n    def __len__(self):\n        return len(self.images)\n\n    def _mask_transform(self, mask):\n        target = np.array(mask).astype(\'int32\')\n        target[target == 255] = -1\n        return torch.from_numpy(target).long()\n\n    @property\n    def classes(self):\n        """"""Category names.""""""\n        return (\'background\', \'airplane\', \'bicycle\', \'bird\', \'boat\', \'bottle\',\n                \'bus\', \'car\', \'cat\', \'chair\', \'cow\', \'diningtable\', \'dog\', \'horse\',\n                \'motorcycle\', \'person\', \'potted-plant\', \'sheep\', \'sofa\', \'train\',\n                \'tv\')\n\n\nif __name__ == \'__main__\':\n    dataset = VOCSegmentation()'"
core/data/dataloader/sbu_shadow.py,1,"b'""""""SBU Shadow  Segmentation Dataset.""""""\nimport os\nimport torch\nimport numpy as np\n\nfrom PIL import Image\nfrom .segbase import SegmentationDataset\n\n\nclass SBUSegmentation(SegmentationDataset):\n    """"""SBU Shadow Segmentation Dataset\n    """"""\n    NUM_CLASS = 2\n\n    def __init__(self, root=\'../datasets/sbu\', split=\'train\', mode=None, transform=None, **kwargs):\n        super(SBUSegmentation, self).__init__(root, split, mode, transform, **kwargs)\n        assert os.path.exists(self.root)\n        self.images, self.masks = _get_sbu_pairs(self.root, self.split)\n        assert (len(self.images) == len(self.masks))\n        if len(self.images) == 0:\n            raise RuntimeError(""Found 0 images in subfolders of:"" + root + ""\\n"")\n\n    def __getitem__(self, index):\n        img = Image.open(self.images[index]).convert(\'RGB\')\n        if self.mode == \'test\':\n            if self.transform is not None:\n                img = self.transform(img)\n            return img, os.path.basename(self.images[index])\n        mask = Image.open(self.masks[index])\n        # synchrosized transform\n        if self.mode == \'train\':\n            img, mask = self._sync_transform(img, mask)\n        elif self.mode == \'val\':\n            img, mask = self._val_sync_transform(img, mask)\n        else:\n            assert self.mode == \'testval\'\n            img, mask = self._img_transform(img), self._mask_transform(mask)\n        # general resize, normalize and toTensor\n        if self.transform is not None:\n            img = self.transform(img)\n        return img, mask, os.path.basename(self.images[index])\n\n    def _mask_transform(self, mask):\n        target = np.array(mask).astype(\'int32\')\n        target[target > 0] = 1\n        return torch.from_numpy(target).long()\n\n    def __len__(self):\n        return len(self.images)\n\n    @property\n    def pred_offset(self):\n        return 0\n\n\ndef _get_sbu_pairs(folder, split=\'train\'):\n    def get_path_pairs(img_folder, mask_folder):\n        img_paths = []\n        mask_paths = []\n        for root, _, files in os.walk(img_folder):\n            print(root)\n            for filename in files:\n                if filename.endswith(\'.jpg\'):\n                    imgpath = os.path.join(root, filename)\n                    maskname = filename.replace(\'.jpg\', \'.png\')\n                    maskpath = os.path.join(mask_folder, maskname)\n                    if os.path.isfile(imgpath) and os.path.isfile(maskpath):\n                        img_paths.append(imgpath)\n                        mask_paths.append(maskpath)\n                    else:\n                        print(\'cannot find the mask or image:\', imgpath, maskpath)\n        print(\'Found {} images in the folder {}\'.format(len(img_paths), img_folder))\n        return img_paths, mask_paths\n\n    if split == \'train\':\n        img_folder = os.path.join(folder, \'SBUTrain4KRecoveredSmall/ShadowImages\')\n        mask_folder = os.path.join(folder, \'SBUTrain4KRecoveredSmall/ShadowMasks\')\n        img_paths, mask_paths = get_path_pairs(img_folder, mask_folder)\n    else:\n        assert split in (\'val\', \'test\')\n        img_folder = os.path.join(folder, \'SBU-Test/ShadowImages\')\n        mask_folder = os.path.join(folder, \'SBU-Test/ShadowMasks\')\n        img_paths, mask_paths = get_path_pairs(img_folder, mask_folder)\n    return img_paths, mask_paths\n\n\nif __name__ == \'__main__\':\n    dataset = SBUSegmentation(base_size=280, crop_size=256)'"
core/data/dataloader/segbase.py,0,"b'""""""Base segmentation dataset""""""\nimport random\nimport numpy as np\n\nfrom PIL import Image, ImageOps, ImageFilter\n\n__all__ = [\'SegmentationDataset\']\n\n\nclass SegmentationDataset(object):\n    """"""Segmentation Base Dataset""""""\n\n    def __init__(self, root, split, mode, transform, base_size=520, crop_size=480):\n        super(SegmentationDataset, self).__init__()\n        self.root = root\n        self.transform = transform\n        self.split = split\n        self.mode = mode if mode is not None else split\n        self.base_size = base_size\n        self.crop_size = crop_size\n\n    def _val_sync_transform(self, img, mask):\n        outsize = self.crop_size\n        short_size = outsize\n        w, h = img.size\n        if w > h:\n            oh = short_size\n            ow = int(1.0 * w * oh / h)\n        else:\n            ow = short_size\n            oh = int(1.0 * h * ow / w)\n        img = img.resize((ow, oh), Image.BILINEAR)\n        mask = mask.resize((ow, oh), Image.NEAREST)\n        # center crop\n        w, h = img.size\n        x1 = int(round((w - outsize) / 2.))\n        y1 = int(round((h - outsize) / 2.))\n        img = img.crop((x1, y1, x1 + outsize, y1 + outsize))\n        mask = mask.crop((x1, y1, x1 + outsize, y1 + outsize))\n        # final transform\n        img, mask = self._img_transform(img), self._mask_transform(mask)\n        return img, mask\n\n    def _sync_transform(self, img, mask):\n        # random mirror\n        if random.random() < 0.5:\n            img = img.transpose(Image.FLIP_LEFT_RIGHT)\n            mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n        crop_size = self.crop_size\n        # random scale (short edge)\n        short_size = random.randint(int(self.base_size * 0.5), int(self.base_size * 2.0))\n        w, h = img.size\n        if h > w:\n            ow = short_size\n            oh = int(1.0 * h * ow / w)\n        else:\n            oh = short_size\n            ow = int(1.0 * w * oh / h)\n        img = img.resize((ow, oh), Image.BILINEAR)\n        mask = mask.resize((ow, oh), Image.NEAREST)\n        # pad crop\n        if short_size < crop_size:\n            padh = crop_size - oh if oh < crop_size else 0\n            padw = crop_size - ow if ow < crop_size else 0\n            img = ImageOps.expand(img, border=(0, 0, padw, padh), fill=0)\n            mask = ImageOps.expand(mask, border=(0, 0, padw, padh), fill=0)\n        # random crop crop_size\n        w, h = img.size\n        x1 = random.randint(0, w - crop_size)\n        y1 = random.randint(0, h - crop_size)\n        img = img.crop((x1, y1, x1 + crop_size, y1 + crop_size))\n        mask = mask.crop((x1, y1, x1 + crop_size, y1 + crop_size))\n        # gaussian blur as in PSP\n        if random.random() < 0.5:\n            img = img.filter(ImageFilter.GaussianBlur(radius=random.random()))\n        # final transform\n        img, mask = self._img_transform(img), self._mask_transform(mask)\n        return img, mask\n\n    def _img_transform(self, img):\n        return np.array(img)\n\n    def _mask_transform(self, mask):\n        return np.array(mask).astype(\'int32\')\n\n    @property\n    def num_class(self):\n        """"""Number of categories.""""""\n        return self.NUM_CLASS\n\n    @property\n    def pred_offset(self):\n        return 0\n'"
core/data/dataloader/utils.py,1,"b'import os\nimport hashlib\nimport errno\nimport tarfile\nfrom six.moves import urllib\nfrom torch.utils.model_zoo import tqdm\n\ndef gen_bar_updater():\n    pbar = tqdm(total=None)\n\n    def bar_update(count, block_size, total_size):\n        if pbar.total is None and total_size:\n            pbar.total = total_size\n        progress_bytes = count * block_size\n        pbar.update(progress_bytes - pbar.n)\n\n    return bar_update\n\ndef check_integrity(fpath, md5=None):\n    if md5 is None:\n        return True\n    if not os.path.isfile(fpath):\n        return False\n    md5o = hashlib.md5()\n    with open(fpath, \'rb\') as f:\n        # read in 1MB chunks\n        for chunk in iter(lambda: f.read(1024 * 1024), b\'\'):\n            md5o.update(chunk)\n    md5c = md5o.hexdigest()\n    if md5c != md5:\n        return False\n    return True\n\ndef makedir_exist_ok(dirpath):\n    try:\n        os.makedirs(dirpath)\n    except OSError as e:\n        if e.errno == errno.EEXIST:\n            pass\n        else:\n            pass\n\ndef download_url(url, root, filename=None, md5=None):\n    """"""Download a file from a url and place it in root.""""""\n    root = os.path.expanduser(root)\n    if not filename:\n        filename = os.path.basename(url)\n    fpath = os.path.join(root, filename)\n\n    makedir_exist_ok(root)\n\n    # downloads file\n    if os.path.isfile(fpath) and check_integrity(fpath, md5):\n        print(\'Using downloaded and verified file: \' + fpath)\n    else:\n        try:\n            print(\'Downloading \' + url + \' to \' + fpath)\n            urllib.request.urlretrieve(url, fpath, reporthook=gen_bar_updater())\n        except OSError:\n            if url[:5] == \'https\':\n                url = url.replace(\'https:\', \'http:\')\n                print(\'Failed download. Trying https -> http instead.\'\n                      \' Downloading \' + url + \' to \' + fpath)\n                urllib.request.urlretrieve(url, fpath, reporthook=gen_bar_updater())\n\ndef download_extract(url, root, filename, md5):\n    download_url(url, root, filename, md5)\n    with tarfile.open(os.path.join(root, filename), ""r"") as tar:\n        tar.extractall(path=root)'"
core/data/downloader/__init__.py,0,b''
core/data/downloader/ade20k.py,0,"b'""""""Prepare ADE20K dataset""""""\nimport os\nimport sys\nimport argparse\nimport zipfile\n\n# TODO: optim code\ncur_path = os.path.abspath(os.path.dirname(__file__))\nroot_path = os.path.split(os.path.split(os.path.split(cur_path)[0])[0])[0]\nsys.path.append(root_path)\n\nfrom core.utils import download, makedirs\n\n_TARGET_DIR = os.path.expanduser(\'~/.torch/datasets/ade\')\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\'Initialize ADE20K dataset.\',\n        epilog=\'Example: python setup_ade20k.py\',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'--download-dir\', default=None, help=\'dataset directory on disk\')\n    args = parser.parse_args()\n    return args\n\n\ndef download_ade(path, overwrite=False):\n    _AUG_DOWNLOAD_URLS = [\n        (\'http://data.csail.mit.edu/places/ADEchallenge/ADEChallengeData2016.zip\',\n         \'219e1696abb36c8ba3a3afe7fb2f4b4606a897c7\'),\n        (\n            \'http://data.csail.mit.edu/places/ADEchallenge/release_test.zip\',\n            \'e05747892219d10e9243933371a497e905a4860c\'), ]\n    download_dir = os.path.join(path, \'downloads\')\n    makedirs(download_dir)\n    for url, checksum in _AUG_DOWNLOAD_URLS:\n        filename = download(url, path=download_dir, overwrite=overwrite, sha1_hash=checksum)\n        # extract\n        with zipfile.ZipFile(filename, ""r"") as zip_ref:\n            zip_ref.extractall(path=path)\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    makedirs(os.path.expanduser(\'~/.torch/datasets\'))\n    if args.download_dir is not None:\n        if os.path.isdir(_TARGET_DIR):\n            os.remove(_TARGET_DIR)\n        # make symlink\n        os.symlink(args.download_dir, _TARGET_DIR)\n    download_ade(_TARGET_DIR, overwrite=False)\n'"
core/data/downloader/cityscapes.py,0,"b'""""""Prepare Cityscapes dataset""""""\nimport os\nimport sys\nimport argparse\nimport zipfile\n\n# TODO: optim code\ncur_path = os.path.abspath(os.path.dirname(__file__))\nroot_path = os.path.split(os.path.split(os.path.split(cur_path)[0])[0])[0]\nsys.path.append(root_path)\n\nfrom core.utils import download, makedirs, check_sha1\n\n_TARGET_DIR = os.path.expanduser(\'~/.torch/datasets/citys\')\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\'Initialize ADE20K dataset.\',\n        epilog=\'Example: python prepare_cityscapes.py\',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'--download-dir\', default=None, help=\'dataset directory on disk\')\n    args = parser.parse_args()\n    return args\n\n\ndef download_city(path, overwrite=False):\n    _CITY_DOWNLOAD_URLS = [\n        (\'gtFine_trainvaltest.zip\', \'99f532cb1af174f5fcc4c5bc8feea8c66246ddbc\'),\n        (\'leftImg8bit_trainvaltest.zip\', \'2c0b77ce9933cc635adda307fbba5566f5d9d404\')]\n    download_dir = os.path.join(path, \'downloads\')\n    makedirs(download_dir)\n    for filename, checksum in _CITY_DOWNLOAD_URLS:\n        if not check_sha1(filename, checksum):\n            raise UserWarning(\'File {} is downloaded but the content hash does not match. \' \\\n                              \'The repo may be outdated or download may be incomplete. \' \\\n                              \'If the ""repo_url"" is overridden, consider switching to \' \\\n                              \'the default repo.\'.format(filename))\n        # extract\n        with zipfile.ZipFile(filename, ""r"") as zip_ref:\n            zip_ref.extractall(path=path)\n        print(""Extracted"", filename)\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    makedirs(os.path.expanduser(\'~/.torch/datasets\'))\n    if args.download_dir is not None:\n        if os.path.isdir(_TARGET_DIR):\n            os.remove(_TARGET_DIR)\n        # make symlink\n        os.symlink(args.download_dir, _TARGET_DIR)\n    else:\n        download_city(_TARGET_DIR, overwrite=False)\n'"
core/data/downloader/mscoco.py,0,"b'""""""Prepare MS COCO datasets""""""\nimport os\nimport sys\nimport argparse\nimport zipfile\n\n# TODO: optim code\ncur_path = os.path.abspath(os.path.dirname(__file__))\nroot_path = os.path.split(os.path.split(os.path.split(cur_path)[0])[0])[0]\nsys.path.append(root_path)\n\nfrom core.utils import download, makedirs, try_import_pycocotools\n\n_TARGET_DIR = os.path.expanduser(\'~/.torch/datasets/coco\')\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\'Initialize MS COCO dataset.\',\n        epilog=\'Example: python mscoco.py --download-dir ~/mscoco\',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'--download-dir\', type=str, default=\'~/mscoco/\', help=\'dataset directory on disk\')\n    parser.add_argument(\'--no-download\', action=\'store_true\', help=\'disable automatic download if set\')\n    parser.add_argument(\'--overwrite\', action=\'store_true\',\n                        help=\'overwrite downloaded files if set, in case they are corrupted\')\n    args = parser.parse_args()\n    return args\n\n\ndef download_coco(path, overwrite=False):\n    _DOWNLOAD_URLS = [\n        (\'http://images.cocodataset.org/zips/train2017.zip\',\n         \'10ad623668ab00c62c096f0ed636d6aff41faca5\'),\n        (\'http://images.cocodataset.org/annotations/annotations_trainval2017.zip\',\n         \'8551ee4bb5860311e79dace7e79cb91e432e78b3\'),\n        (\'http://images.cocodataset.org/zips/val2017.zip\',\n         \'4950dc9d00dbe1c933ee0170f5797584351d2a41\'),\n        # (\'http://images.cocodataset.org/annotations/stuff_annotations_trainval2017.zip\',\n        # \'46cdcf715b6b4f67e980b529534e79c2edffe084\'),\n        # test2017.zip, for those who want to attend the competition.\n        # (\'http://images.cocodataset.org/zips/test2017.zip\',\n        #  \'4e443f8a2eca6b1dac8a6c57641b67dd40621a49\'),\n    ]\n    makedirs(path)\n    for url, checksum in _DOWNLOAD_URLS:\n        filename = download(url, path=path, overwrite=overwrite, sha1_hash=checksum)\n        # extract\n        with zipfile.ZipFile(filename) as zf:\n            zf.extractall(path=path)\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    path = os.path.expanduser(args.download_dir)\n    if not os.path.isdir(path) or not os.path.isdir(os.path.join(path, \'train2017\')) \\\n            or not os.path.isdir(os.path.join(path, \'val2017\')) \\\n            or not os.path.isdir(os.path.join(path, \'annotations\')):\n        if args.no_download:\n            raise ValueError((\'{} is not a valid directory, make sure it is present.\'\n                              \' Or you should not disable ""--no-download"" to grab it\'.format(path)))\n        else:\n            download_coco(path, overwrite=args.overwrite)\n\n    # make symlink\n    makedirs(os.path.expanduser(\'~/.torch/datasets\'))\n    if os.path.isdir(_TARGET_DIR):\n        os.remove(_TARGET_DIR)\n    os.symlink(path, _TARGET_DIR)\n    try_import_pycocotools()\n'"
core/data/downloader/pascal_voc.py,0,"b'""""""Prepare PASCAL VOC datasets""""""\nimport os\nimport sys\nimport shutil\nimport argparse\nimport tarfile\n\n# TODO: optim code\ncur_path = os.path.abspath(os.path.dirname(__file__))\nroot_path = os.path.split(os.path.split(os.path.split(cur_path)[0])[0])[0]\nsys.path.append(root_path)\n\nfrom core.utils import download, makedirs\n\n_TARGET_DIR = os.path.expanduser(\'~/.torch/datasets/voc\')\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\'Initialize PASCAL VOC dataset.\',\n        epilog=\'Example: python pascal_voc.py --download-dir ~/VOCdevkit\',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'--download-dir\', type=str, default=\'~/VOCdevkit/\', help=\'dataset directory on disk\')\n    parser.add_argument(\'--no-download\', action=\'store_true\', help=\'disable automatic download if set\')\n    parser.add_argument(\'--overwrite\', action=\'store_true\',\n                        help=\'overwrite downloaded files if set, in case they are corrupted\')\n    args = parser.parse_args()\n    return args\n\n\n#####################################################################################\n# Download and extract VOC datasets into ``path``\n\ndef download_voc(path, overwrite=False):\n    _DOWNLOAD_URLS = [\n        (\'http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\',\n         \'34ed68851bce2a36e2a223fa52c661d592c66b3c\'),\n        (\'http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\',\n         \'41a8d6e12baa5ab18ee7f8f8029b9e11805b4ef1\'),\n        (\'http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\',\n         \'4e443f8a2eca6b1dac8a6c57641b67dd40621a49\')]\n    makedirs(path)\n    for url, checksum in _DOWNLOAD_URLS:\n        filename = download(url, path=path, overwrite=overwrite, sha1_hash=checksum)\n        # extract\n        with tarfile.open(filename) as tar:\n            tar.extractall(path=path)\n\n\n#####################################################################################\n# Download and extract the VOC augmented segmentation dataset into ``path``\n\ndef download_aug(path, overwrite=False):\n    _AUG_DOWNLOAD_URLS = [\n        (\'http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/semantic_contours/benchmark.tgz\',\n         \'7129e0a480c2d6afb02b517bb18ac54283bfaa35\')]\n    makedirs(path)\n    for url, checksum in _AUG_DOWNLOAD_URLS:\n        filename = download(url, path=path, overwrite=overwrite, sha1_hash=checksum)\n        # extract\n        with tarfile.open(filename) as tar:\n            tar.extractall(path=path)\n            shutil.move(os.path.join(path, \'benchmark_RELEASE\'),\n                        os.path.join(path, \'VOCaug\'))\n            filenames = [\'VOCaug/dataset/train.txt\', \'VOCaug/dataset/val.txt\']\n            # generate trainval.txt\n            with open(os.path.join(path, \'VOCaug/dataset/trainval.txt\'), \'w\') as outfile:\n                for fname in filenames:\n                    fname = os.path.join(path, fname)\n                    with open(fname) as infile:\n                        for line in infile:\n                            outfile.write(line)\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    path = os.path.expanduser(args.download_dir)\n    if not os.path.isfile(path) or not os.path.isdir(os.path.join(path, \'VOC2007\')) \\\n            or not os.path.isdir(os.path.join(path, \'VOC2012\')):\n        if args.no_download:\n            raise ValueError((\'{} is not a valid directory, make sure it is present.\'\n                              \' Or you should not disable ""--no-download"" to grab it\'.format(path)))\n        else:\n            download_voc(path, overwrite=args.overwrite)\n            shutil.move(os.path.join(path, \'VOCdevkit\', \'VOC2007\'), os.path.join(path, \'VOC2007\'))\n            shutil.move(os.path.join(path, \'VOCdevkit\', \'VOC2012\'), os.path.join(path, \'VOC2012\'))\n            shutil.rmtree(os.path.join(path, \'VOCdevkit\'))\n\n    if not os.path.isdir(os.path.join(path, \'VOCaug\')):\n        if args.no_download:\n            raise ValueError((\'{} is not a valid directory, make sure it is present.\'\n                              \' Or you should not disable ""--no-download"" to grab it\'.format(path)))\n        else:\n            download_aug(path, overwrite=args.overwrite)\n\n    # make symlink\n    makedirs(os.path.expanduser(\'~/.torch/datasets\'))\n    if os.path.isdir(_TARGET_DIR):\n        os.remove(_TARGET_DIR)\n    os.symlink(path, _TARGET_DIR)\n'"
core/data/downloader/sbu_shadow.py,0,"b'""""""Prepare SBU Shadow datasets""""""\nimport os\nimport sys\nimport argparse\nimport zipfile\n\n# TODO: optim code\ncur_path = os.path.abspath(os.path.dirname(__file__))\nroot_path = os.path.split(os.path.split(os.path.split(cur_path)[0])[0])[0]\nsys.path.append(root_path)\n\nfrom core.utils import download, makedirs\n\n_TARGET_DIR = os.path.expanduser(\'~/.torch/datasets/sbu\')\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\'Initialize SBU Shadow dataset.\',\n        epilog=\'Example: python sbu_shadow.py --download-dir ~/SBU-shadow\',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'--download-dir\', type=str, default=None, help=\'dataset directory on disk\')\n    parser.add_argument(\'--no-download\', action=\'store_true\', help=\'disable automatic download if set\')\n    parser.add_argument(\'--overwrite\', action=\'store_true\',\n                        help=\'overwrite downloaded files if set, in case they are corrupted\')\n    args = parser.parse_args()\n    return args\n\n\n#####################################################################################\n# Download and extract SBU shadow datasets into ``path``\n\ndef download_sbu(path, overwrite=False):\n    _DOWNLOAD_URLS = [\n        (\'http://www3.cs.stonybrook.edu/~cvl/content/datasets/shadow_db/SBU-shadow.zip\'),\n    ]\n    download_dir = os.path.join(path, \'downloads\')\n    makedirs(download_dir)\n    for url in _DOWNLOAD_URLS:\n        filename = download(url, path=path, overwrite=overwrite)\n        # extract\n        with zipfile.ZipFile(filename, ""r"") as zf:\n            zf.extractall(path=path)\n        print(""Extracted"", filename)\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    makedirs(os.path.expanduser(\'~/.torch/datasets\'))\n    if args.download_dir is not None:\n        if os.path.isdir(_TARGET_DIR):\n            os.remove(_TARGET_DIR)\n        # make symlink\n        os.symlink(args.download_dir, _TARGET_DIR)\n    else:\n        download_sbu(_TARGET_DIR, overwrite=False)\n'"
core/models/base_models/__init__.py,0,b'from .densenet import *\nfrom .resnet import *\nfrom .resnetv1b import *\nfrom .vgg import *\nfrom .eespnet import *\nfrom .xception import *\n'
core/models/base_models/densenet.py,9,"b'import re\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\n\nfrom collections import OrderedDict\n\n__all__ = [\'DenseNet\', \'densenet121\', \'densenet161\', \'densenet169\', \'densenet201\',\n           \'dilated_densenet121\', \'dilated_densenet161\', \'dilated_densenet169\', \'dilated_densenet201\']\n\nmodel_urls = {\n    \'densenet121\': \'https://download.pytorch.org/models/densenet121-a639ec97.pth\',\n    \'densenet169\': \'https://download.pytorch.org/models/densenet169-b2777c0a.pth\',\n    \'densenet201\': \'https://download.pytorch.org/models/densenet201-c1103571.pth\',\n    \'densenet161\': \'https://download.pytorch.org/models/densenet161-8d451a50.pth\',\n}\n\n\nclass _DenseLayer(nn.Sequential):\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, dilation=1, norm_layer=nn.BatchNorm2d):\n        super(_DenseLayer, self).__init__()\n        self.add_module(\'norm1\', norm_layer(num_input_features)),\n        self.add_module(\'relu1\', nn.ReLU(True)),\n        self.add_module(\'conv1\', nn.Conv2d(num_input_features, bn_size * growth_rate, 1, 1, bias=False)),\n        self.add_module(\'norm2\', norm_layer(bn_size * growth_rate)),\n        self.add_module(\'relu2\', nn.ReLU(True)),\n        self.add_module(\'conv2\', nn.Conv2d(bn_size * growth_rate, growth_rate, 3, 1, dilation, dilation, bias=False)),\n        self.drop_rate = drop_rate\n\n    def forward(self, x):\n        new_features = super(_DenseLayer, self).forward(x)\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n        return torch.cat([x, new_features], 1)\n\n\nclass _DenseBlock(nn.Sequential):\n    def __init__(self, num_layers, num_input_features, bn_size,\n                 growth_rate, drop_rate, dilation=1, norm_layer=nn.BatchNorm2d):\n        super(_DenseBlock, self).__init__()\n        for i in range(num_layers):\n            layer = _DenseLayer(num_input_features + i * growth_rate,\n                                growth_rate, bn_size, drop_rate, dilation, norm_layer)\n            self.add_module(\'denselayer%d\' % (i + 1), layer)\n\n\nclass _Transition(nn.Sequential):\n    def __init__(self, num_input_features, num_output_features, norm_layer=nn.BatchNorm2d):\n        super(_Transition, self).__init__()\n        self.add_module(\'norm\', norm_layer(num_input_features))\n        self.add_module(\'relu\', nn.ReLU(True))\n        self.add_module(\'conv\', nn.Conv2d(num_input_features, num_output_features, 1, 1, bias=False))\n        self.add_module(\'pool\', nn.AvgPool2d(2, 2))\n\n\n# Net\nclass DenseNet(nn.Module):\n\n    def __init__(self, growth_rate=12, block_config=(6, 12, 24, 16), num_init_features=64,\n                 bn_size=4, drop_rate=0, num_classes=1000, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(DenseNet, self).__init__()\n\n        # First convolution\n        self.features = nn.Sequential(OrderedDict([\n            (\'conv0\', nn.Conv2d(3, num_init_features, 7, 2, 3, bias=False)),\n            (\'norm0\', norm_layer(num_init_features)),\n            (\'relu0\', nn.ReLU(True)),\n            (\'pool0\', nn.MaxPool2d(3, 2, 1)),\n        ]))\n\n        # Each denseblock\n        num_features = num_init_features\n        for i, num_layers in enumerate(block_config):\n            block = _DenseBlock(num_layers, num_features, bn_size, growth_rate, drop_rate, norm_layer=norm_layer)\n            self.features.add_module(\'denseblock%d\' % (i + 1), block)\n            num_features = num_features + num_layers * growth_rate\n            if i != len(block_config) - 1:\n                trans = _Transition(num_features, num_features // 2, norm_layer=norm_layer)\n                self.features.add_module(\'transition%d\' % (i + 1), trans)\n                num_features = num_features // 2\n        self.num_features = num_features\n\n        # Final batch norm\n        self.features.add_module(\'norm5\', norm_layer(num_features))\n\n        # Linear layer\n        self.classifier = nn.Linear(num_features, num_classes)\n\n        # Official init from torch repo.\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        features = self.features(x)\n        out = F.relu(features, True)\n        out = F.adaptive_avg_pool2d(out, (1, 1)).view(features.size(0), -1)\n        out = self.classifier(out)\n        return out\n\n\nclass DilatedDenseNet(DenseNet):\n    def __init__(self, growth_rate=12, block_config=(6, 12, 24, 16), num_init_features=64,\n                 bn_size=4, drop_rate=0, num_classes=1000, dilate_scale=8, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(DilatedDenseNet, self).__init__(growth_rate, block_config, num_init_features,\n                                              bn_size, drop_rate, num_classes, norm_layer)\n        assert (dilate_scale == 8 or dilate_scale == 16), ""dilate_scale can only set as 8 or 16""\n        from functools import partial\n        if dilate_scale == 8:\n            self.features.denseblock3.apply(partial(self._conv_dilate, dilate=2))\n            self.features.denseblock4.apply(partial(self._conv_dilate, dilate=4))\n            del self.features.transition2.pool\n            del self.features.transition3.pool\n        elif dilate_scale == 16:\n            self.features.denseblock4.apply(partial(self._conv_dilate, dilate=2))\n            del self.features.transition3.pool\n\n    def _conv_dilate(self, m, dilate):\n        classname = m.__class__.__name__\n        if classname.find(\'Conv\') != -1:\n            if m.kernel_size == (3, 3):\n                m.padding = (dilate, dilate)\n                m.dilation = (dilate, dilate)\n\n\n# Specification\ndensenet_spec = {121: (64, 32, [6, 12, 24, 16]),\n                 161: (96, 48, [6, 12, 36, 24]),\n                 169: (64, 32, [6, 12, 32, 32]),\n                 201: (64, 32, [6, 12, 48, 32])}\n\n\n# Constructor\ndef get_densenet(num_layers, pretrained=False, **kwargs):\n    r""""""Densenet-BC model from the\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`_ paper.\n\n    Parameters\n    ----------\n    num_layers : int\n        Number of layers for the variant of densenet. Options are 121, 161, 169, 201.\n    pretrained : bool or str\n        Boolean value controls whether to load the default pretrained weights for model.\n        String value represents the hashtag for a certain version of pretrained weights.\n    root : str, default $TORCH_HOME/models\n        Location for keeping the model parameters.\n    """"""\n    num_init_features, growth_rate, block_config = densenet_spec[num_layers]\n    model = DenseNet(growth_rate, block_config, num_init_features, **kwargs)\n    if pretrained:\n        # \'.\'s are no longer allowed in module names, but pervious _DenseLayer\n        # has keys \'norm.1\', \'relu.1\', \'conv.1\', \'norm.2\', \'relu.2\', \'conv.2\'.\n        # They are also in the checkpoints in model_urls. This pattern is used\n        # to find such keys.\n        pattern = re.compile(\n            r\'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$\')\n        state_dict = model_zoo.load_url(model_urls[\'densenet%d\' % num_layers])\n        for key in list(state_dict.keys()):\n            res = pattern.match(key)\n            if res:\n                new_key = res.group(1) + res.group(2)\n                state_dict[new_key] = state_dict[key]\n                del state_dict[key]\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef get_dilated_densenet(num_layers, dilate_scale, pretrained=False, **kwargs):\n    num_init_features, growth_rate, block_config = densenet_spec[num_layers]\n    model = DilatedDenseNet(growth_rate, block_config, num_init_features, dilate_scale=dilate_scale)\n    if pretrained:\n        # \'.\'s are no longer allowed in module names, but pervious _DenseLayer\n        # has keys \'norm.1\', \'relu.1\', \'conv.1\', \'norm.2\', \'relu.2\', \'conv.2\'.\n        # They are also in the checkpoints in model_urls. This pattern is used\n        # to find such keys.\n        pattern = re.compile(\n            r\'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$\')\n        state_dict = model_zoo.load_url(model_urls[\'densenet%d\' % num_layers])\n        for key in list(state_dict.keys()):\n            res = pattern.match(key)\n            if res:\n                new_key = res.group(1) + res.group(2)\n                state_dict[new_key] = state_dict[key]\n                del state_dict[key]\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef densenet121(**kwargs):\n    return get_densenet(121, **kwargs)\n\n\ndef densenet161(**kwargs):\n    return get_densenet(161, **kwargs)\n\n\ndef densenet169(**kwargs):\n    return get_densenet(169, **kwargs)\n\n\ndef densenet201(**kwargs):\n    return get_densenet(201, **kwargs)\n\n\ndef dilated_densenet121(dilate_scale, **kwargs):\n    return get_dilated_densenet(121, dilate_scale, **kwargs)\n\n\ndef dilated_densenet161(dilate_scale, **kwargs):\n    return get_dilated_densenet(161, dilate_scale, **kwargs)\n\n\ndef dilated_densenet169(dilate_scale, **kwargs):\n    return get_dilated_densenet(169, dilate_scale, **kwargs)\n\n\ndef dilated_densenet201(dilate_scale, **kwargs):\n    return get_dilated_densenet(201, dilate_scale, **kwargs)\n\n\nif __name__ == \'__main__\':\n    img = torch.randn(2, 3, 224, 224)\n    model = dilated_densenet121(8)\n    outputs = model(img)\n'"
core/models/base_models/eespnet.py,5,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom core.nn import _ConvBNPReLU, _ConvBN, _BNPReLU\n\n__all__ = [\'EESP\', \'EESPNet\', \'eespnet\']\n\n\nclass EESP(nn.Module):\n\n    def __init__(self, in_channels, out_channels, stride=1, k=4, r_lim=7, down_method=\'esp\', norm_layer=nn.BatchNorm2d):\n        super(EESP, self).__init__()\n        self.stride = stride\n        n = int(out_channels / k)\n        n1 = out_channels - (k - 1) * n\n        assert down_method in [\'avg\', \'esp\'], \'One of these is suppported (avg or esp)\'\n        assert n == n1, ""n(={}) and n1(={}) should be equal for Depth-wise Convolution "".format(n, n1)\n        self.proj_1x1 = _ConvBNPReLU(in_channels, n, 1, stride=1, groups=k, norm_layer=norm_layer)\n\n        map_receptive_ksize = {3: 1, 5: 2, 7: 3, 9: 4, 11: 5, 13: 6, 15: 7, 17: 8}\n        self.k_sizes = list()\n        for i in range(k):\n            ksize = int(3 + 2 * i)\n            ksize = ksize if ksize <= r_lim else 3\n            self.k_sizes.append(ksize)\n        self.k_sizes.sort()\n        self.spp_dw = nn.ModuleList()\n        for i in range(k):\n            dilation = map_receptive_ksize[self.k_sizes[i]]\n            self.spp_dw.append(nn.Conv2d(n, n, 3, stride, dilation, dilation=dilation, groups=n, bias=False))\n        self.conv_1x1_exp = _ConvBN(out_channels, out_channels, 1, 1, groups=k, norm_layer=norm_layer)\n        self.br_after_cat = _BNPReLU(out_channels, norm_layer)\n        self.module_act = nn.PReLU(out_channels)\n        self.downAvg = True if down_method == \'avg\' else False\n\n    def forward(self, x):\n        output1 = self.proj_1x1(x)\n        output = [self.spp_dw[0](output1)]\n        for k in range(1, len(self.spp_dw)):\n            out_k = self.spp_dw[k](output1)\n            out_k = out_k + output[k - 1]\n            output.append(out_k)\n        expanded = self.conv_1x1_exp(self.br_after_cat(torch.cat(output, 1)))\n        del output\n        if self.stride == 2 and self.downAvg:\n            return expanded\n\n        if expanded.size() == x.size():\n            expanded = expanded + x\n\n        return self.module_act(expanded)\n\n\nclass DownSampler(nn.Module):\n\n    def __init__(self, in_channels, out_channels, k=4, r_lim=9, reinf=True, inp_reinf=3, norm_layer=None):\n        super(DownSampler, self).__init__()\n        channels_diff = out_channels - in_channels\n        self.eesp = EESP(in_channels, channels_diff, stride=2, k=k,\n                         r_lim=r_lim, down_method=\'avg\', norm_layer=norm_layer)\n        self.avg = nn.AvgPool2d(kernel_size=3, padding=1, stride=2)\n        if reinf:\n            self.inp_reinf = nn.Sequential(\n                _ConvBNPReLU(inp_reinf, inp_reinf, 3, 1, 1),\n                _ConvBN(inp_reinf, out_channels, 1, 1))\n        self.act = nn.PReLU(out_channels)\n\n    def forward(self, x, x2=None):\n        avg_out = self.avg(x)\n        eesp_out = self.eesp(x)\n        output = torch.cat([avg_out, eesp_out], 1)\n        if x2 is not None:\n            w1 = avg_out.size(2)\n            while True:\n                x2 = F.avg_pool2d(x2, kernel_size=3, padding=1, stride=2)\n                w2 = x2.size(2)\n                if w2 == w1:\n                    break\n            output = output + self.inp_reinf(x2)\n\n        return self.act(output)\n\n\nclass EESPNet(nn.Module):\n    def __init__(self, num_classes=1000, scale=1, reinf=True, norm_layer=nn.BatchNorm2d):\n        super(EESPNet, self).__init__()\n        inp_reinf = 3 if reinf else None\n        reps = [0, 3, 7, 3]\n        r_lim = [13, 11, 9, 7, 5]\n        K = [4] * len(r_lim)\n\n        # set out_channels\n        base, levels, base_s = 32, 5, 0\n        out_channels = [base] * levels\n        for i in range(levels):\n            if i == 0:\n                base_s = int(base * scale)\n                base_s = math.ceil(base_s / K[0]) * K[0]\n                out_channels[i] = base if base_s > base else base_s\n            else:\n                out_channels[i] = base_s * pow(2, i)\n        if scale <= 1.5:\n            out_channels.append(1024)\n        elif scale in [1.5, 2]:\n            out_channels.append(1280)\n        else:\n            raise ValueError(""Unknown scale value."")\n\n        self.level1 = _ConvBNPReLU(3, out_channels[0], 3, 2, 1, norm_layer=norm_layer)\n\n        self.level2_0 = DownSampler(out_channels[0], out_channels[1], k=K[0], r_lim=r_lim[0],\n                                    reinf=reinf, inp_reinf=inp_reinf, norm_layer=norm_layer)\n\n        self.level3_0 = DownSampler(out_channels[1], out_channels[2], k=K[1], r_lim=r_lim[1],\n                                    reinf=reinf, inp_reinf=inp_reinf, norm_layer=norm_layer)\n        self.level3 = nn.ModuleList()\n        for i in range(reps[1]):\n            self.level3.append(EESP(out_channels[2], out_channels[2], k=K[2], r_lim=r_lim[2],\n                                    norm_layer=norm_layer))\n\n        self.level4_0 = DownSampler(out_channels[2], out_channels[3], k=K[2], r_lim=r_lim[2],\n                                    reinf=reinf, inp_reinf=inp_reinf, norm_layer=norm_layer)\n        self.level4 = nn.ModuleList()\n        for i in range(reps[2]):\n            self.level4.append(EESP(out_channels[3], out_channels[3], k=K[3], r_lim=r_lim[3],\n                                    norm_layer=norm_layer))\n\n        self.level5_0 = DownSampler(out_channels[3], out_channels[4], k=K[3], r_lim=r_lim[3],\n                                    reinf=reinf, inp_reinf=inp_reinf, norm_layer=norm_layer)\n        self.level5 = nn.ModuleList()\n        for i in range(reps[2]):\n            self.level5.append(EESP(out_channels[4], out_channels[4], k=K[4], r_lim=r_lim[4],\n                                    norm_layer=norm_layer))\n\n        self.level5.append(_ConvBNPReLU(out_channels[4], out_channels[4], 3, 1, 1,\n                                        groups=out_channels[4], norm_layer=norm_layer))\n        self.level5.append(_ConvBNPReLU(out_channels[4], out_channels[5], 1, 1, 0,\n                                        groups=K[4], norm_layer=norm_layer))\n\n        self.fc = nn.Linear(out_channels[5], num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, std=0.001)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, x, seg=True):\n        out_l1 = self.level1(x)\n\n        out_l2 = self.level2_0(out_l1, x)\n\n        out_l3_0 = self.level3_0(out_l2, x)\n        for i, layer in enumerate(self.level3):\n            if i == 0:\n                out_l3 = layer(out_l3_0)\n            else:\n                out_l3 = layer(out_l3)\n\n        out_l4_0 = self.level4_0(out_l3, x)\n        for i, layer in enumerate(self.level4):\n            if i == 0:\n                out_l4 = layer(out_l4_0)\n            else:\n                out_l4 = layer(out_l4)\n\n        if not seg:\n            out_l5_0 = self.level5_0(out_l4)  # down-sampled\n            for i, layer in enumerate(self.level5):\n                if i == 0:\n                    out_l5 = layer(out_l5_0)\n                else:\n                    out_l5 = layer(out_l5)\n\n            output_g = F.adaptive_avg_pool2d(out_l5, output_size=1)\n            output_g = F.dropout(output_g, p=0.2, training=self.training)\n            output_1x1 = output_g.view(output_g.size(0), -1)\n\n            return self.fc(output_1x1)\n        return out_l1, out_l2, out_l3, out_l4\n\n\ndef eespnet(pretrained=False, **kwargs):\n    model = EESPNet(**kwargs)\n    if pretrained:\n        raise ValueError(""Don\'t support pretrained"")\n    return model\n\n\nif __name__ == \'__main__\':\n    img = torch.randn(1, 3, 224, 224)\n    model = eespnet()\n    out = model(img)\n'"
core/models/base_models/hrnet.py,2,"b""import torch\nimport torch.nn as nn\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, norm_layer=nn.BatchNorm2d):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, 3, stride, padding=1, bias=False)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(True)\n        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, norm_layer=nn.BatchNorm2d):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n        self.bn1 = norm_layer(planes)\n        self.conv2 = nn.Conv2d(planes, planes, 3, stride, 1, bias=False)\n        self.bn2 = norm_layer(planes)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass HighResolutionModule(nn.Module):\n    def __init__(self, num_branches, blocks, num_blocks, num_inchannels, num_channels,\n                 fuse_method, multi_scale_output=True, norm_layer=nn.BatchNorm2d):\n        super(HighResolutionModule, self).__init__()\n        assert num_branches == len(num_blocks)\n        assert num_branches == len(num_channels)\n        assert num_branches == len(num_inchannels)\n\n        self.num_inchannels = num_inchannels\n        self.fuse_method = fuse_method\n        self.num_branches = num_branches\n        self.multi_scale_output = multi_scale_output\n\n        self.branches = self._make_branches(num_branches, blocks, num_blocks, num_channels, norm_layer=norm_layer)\n        self.fuse_layers = self._make_fuse_layers(norm_layer)\n        self.relu = nn.ReLU(True)\n\n    def _make_one_branch(self, branch_index, block, num_blocks, num_channels,\n                         stride=1, norm_layer=nn.BatchNorm2d):\n        downsample = None\n        if stride != 1 or self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.num_inchannels[branch_index], num_channels[branch_index] * block.expansion,\n                          1, stride, bias=False),\n                norm_layer(num_channels[branch_index] * block.expansion))\n\n        layers = list()\n        layers.append(block(self.num_inchannels[branch_index], num_channels[branch_index],\n                            stride, downsample, norm_layer=norm_layer))\n        self.num_inchannels[branch_index] = num_channels[branch_index] * block.expansion\n        for i in range(1, num_blocks[branch_index]):\n            layers.append(block(self.num_inchannels[branch_index], num_channels[branch_index], norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def _make_branches(self, num_branches, block, num_blocks, num_channels, norm_layer=nn.BatchNorm2d):\n        branches = list()\n        for i in range(num_branches):\n            branches.append(\n                self._make_one_branch(i, block, num_blocks, num_channels, norm_layer=norm_layer))\n\n        return nn.ModuleList(branches)\n\n    def _make_fuse_layers(self, norm_layer=nn.BatchNorm2d):\n        if self.num_branches == 1:\n            return None\n\n        num_branches = self.num_branches\n        num_inchannels = self.num_inchannels\n        fuse_layers = []\n        for i in range(num_branches if self.multi_scale_output else 1):\n            fuse_layer = list()\n            for j in range(num_branches):\n                if j > i:\n                    fuse_layer.append(nn.Sequential(\n                        nn.Conv2d(num_inchannels[j], num_inchannels[i], 1, bias=False),\n                        norm_layer(num_inchannels[i]),\n                        nn.Upsample(scale_factor=2 ** (j - i), mode='nearest')))\n                elif j == i:\n                    fuse_layer.append(None)\n                else:\n                    conv3x3s = list()\n                    for k in range(i - j):\n                        if k == i - j - 1:\n                            num_outchannels_conv3x3 = num_inchannels[i]\n                            conv3x3s.append(nn.Sequential(\n                                nn.Conv2d(num_inchannels[j], num_outchannels_conv3x3, 3, 2, 1, bias=False),\n                                norm_layer(num_outchannels_conv3x3)))\n                        else:\n                            num_outchannels_conv3x3 = num_inchannels[j]\n                            conv3x3s.append(nn.Sequential(\n                                nn.Conv2d(num_inchannels[j], num_outchannels_conv3x3, 3, 2, 1, bias=False),\n                                norm_layer(num_outchannels_conv3x3),\n                                nn.ReLU(False)))\n                    fuse_layer.append(nn.Sequential(*conv3x3s))\n            fuse_layers.append(nn.ModuleList(fuse_layer))\n\n        return nn.ModuleList(fuse_layers)\n\n    def get_num_inchannels(self):\n        return self.num_inchannels\n\n    def forward(self, x):\n        if self.num_branches == 1:\n            return [self.branches[0](x[0])]\n\n        for i in range(self.num_branches):\n            x[i] = self.branches[i](x[i])\n\n        x_fuse = list()\n        for i in range(len(self.fuse_layers)):\n            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\n            for j in range(1, self.num_branches):\n                if i == j:\n                    y = y + x[j]\n                else:\n                    y = y + self.fuse_layers[i][j](x[j])\n            x_fuse.append(self.relu(y))\n\n        return x_fuse\n\n\nclass HighResolutionNet(nn.Module):\n    def __init__(self, blocks, num_channels, num_modules, num_branches, num_blocks,\n                 fuse_method, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(HighResolutionNet, self).__init__()\n        self.num_branches = num_branches\n\n        # deep stem\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(3, 64, 3, 2, 1, bias=False),\n            norm_layer(64),\n            nn.ReLU(True),\n            nn.Conv2d(64, 64, 3, 2, 1, bias=False),\n            norm_layer(64),\n            nn.ReLU(True))\n\n        self.layer1 = self._make_layer(Bottleneck, 64, 64, 4, norm_layer=norm_layer)\n\n        # stage 2\n        num_channel, block = num_channels[0], blocks[0]\n        channels = [channel * block.expansion for channel in num_channel]\n        self.transition1 = self._make_transition_layer([256], channels, norm_layer)\n        self.stage2, pre_stage_channels = self._make_stage(num_modules[0], num_branches[0],\n                                                           num_blocks[0], channels, block,\n                                                           fuse_method[0], channels,\n                                                           norm_layer=norm_layer)\n\n        # stage 3\n        num_channel, block = num_channels[1], blocks[1]\n        channels = [channel * block.expansion for channel in num_channel]\n        self.transition1 = self._make_transition_layer(pre_stage_channels, channels, norm_layer)\n        self.stage3, pre_stage_channels = self._make_stage(num_modules[1], num_branches[1],\n                                                           num_blocks[1], channels, block,\n                                                           fuse_method[1], channels,\n                                                           norm_layer=norm_layer)\n\n        # stage 4\n        num_channel, block = num_channels[2], blocks[2]\n        channels = [channel * block.expansion for channel in num_channel]\n        self.transition1 = self._make_transition_layer(pre_stage_channels, channels, norm_layer)\n        self.stage4, pre_stage_channels = self._make_stage(num_modules[2], num_branches[2],\n                                                           num_blocks[2], channels, block,\n                                                           fuse_method[2], channels,\n                                                           norm_layer=norm_layer)\n\n        self.incre_modules, self.downsamp_modules, self.final_layer = self._make_head(pre_stage_channels, norm_layer)\n\n        self.classifier = nn.Linear(2048, 1000)\n\n    def _make_layer(self, block, inplanes, planes, blocks, stride=1, norm_layer=nn.BatchNorm2d):\n        downsample = None\n        if stride != 1 or inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(inplanes, planes * block.expansion, 1, stride, bias=False),\n                norm_layer(planes * block.expansion))\n\n        layers = list()\n        layers.append(block(inplanes, planes, stride, downsample=downsample, norm_layer=norm_layer))\n        inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(inplanes, planes, norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def _make_transition_layer(self, num_channels_pre_layer, num_channels_cur_layer, norm_layer=nn.BatchNorm2d):\n        num_branches_cur = len(num_channels_cur_layer)\n        num_branches_pre = len(num_channels_pre_layer)\n\n        transition_layers = list()\n        for i in range(num_branches_cur):\n            if i < num_branches_pre:\n                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n                    transition_layers.append(nn.Sequential(\n                        nn.Conv2d(num_channels_pre_layer[i], num_channels_cur_layer[i], 3, padding=1, bias=False),\n                        norm_layer(num_channels_cur_layer[i]),\n                        nn.ReLU(True)))\n                else:\n                    transition_layers.append(None)\n            else:\n                conv3x3s = list()\n                for j in range(i + 1 - num_branches_pre):\n                    in_channels = num_channels_pre_layer[-1]\n                    out_channels = num_channels_cur_layer[i] if j == i - num_branches_pre else in_channels\n                    conv3x3s.append(nn.Sequential(\n                        nn.Conv2d(in_channels, out_channels, 3, 2, 1, bias=False),\n                        norm_layer(out_channels),\n                        nn.ReLU(True)))\n                transition_layers.append(nn.Sequential(*conv3x3s))\n\n        return nn.ModuleList(transition_layers)\n\n    def _make_stage(self, num_modules, num_branches, num_blocks, num_channels, block,\n                    fuse_method, num_inchannels, multi_scale_output=True, norm_layer=nn.BatchNorm2d):\n        modules = list()\n        for i in range(num_modules):\n            # multi_scale_output is only used last module\n            if not multi_scale_output and i == num_modules - 1:\n                reset_multi_scale_output = False\n            else:\n                reset_multi_scale_output = True\n\n            modules.append(HighResolutionModule(num_branches, block, num_blocks, num_inchannels, num_channels,\n                                                fuse_method, reset_multi_scale_output, norm_layer=norm_layer))\n            num_inchannels = modules[-1].get_num_inchannels()\n\n        return nn.Sequential(*modules), num_inchannels\n\n    def _make_head(self, pre_stage_channels, norm_layer=nn.BatchNorm2d):\n        head_block = Bottleneck\n        head_channels = [32, 64, 128, 256]\n\n        # Increasing the #channels on each resolution\n        # from C, 2C, 4C, 8C to 128, 256, 512, 1024\n        incre_modules = list()\n        for i, channels in enumerate(pre_stage_channels):\n            incre_module = self._make_layer(head_block, channels, head_channels[i], 1)\n            incre_modules.append(incre_module)\n        incre_modules = nn.ModuleList(incre_modules)\n\n        # downsampling modules\n        downsamp_modules = []\n        for i in range(len(pre_stage_channels) - 1):\n            in_channels = head_channels[i] * head_block.expansion\n            out_channels = head_channels[i + 1] * head_block.expansion\n\n            downsamp_module = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, 3, 2, 1),\n                norm_layer(out_channels),\n                nn.ReLU(True))\n\n            downsamp_modules.append(downsamp_module)\n        downsamp_modules = nn.ModuleList(downsamp_modules)\n\n        final_layer = nn.Sequential(\n            nn.Conv2d(head_channels[3] * head_block.expansion, 2048, 1),\n            norm_layer(2048),\n            nn.ReLU(True))\n\n        return incre_modules, downsamp_modules, final_layer\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.layer1(x)\n\n        x_list = list()\n        for i in range(self.num_branches[0]):\n            if self.transition1[i] is not None:\n                tmp = self.transition1[i](x)\n                print(tmp.size())\n                x_list.append(self.transition1[i](x))\n            else:\n                x_list.append(x)\n        y_list = self.stage2(x_list)\n\n        x_list = []\n        for i in range(self.num_branches[1]):\n            if self.transition2[i] is not None:\n                x_list.append(self.transition2[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage3(x_list)\n\n        x_list = []\n        for i in range(self.num_branches[2]):\n            if self.transition3[i] is not None:\n                x_list.append(self.transition3[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage4(x_list)\n\n        # Classification Head\n        y = self.incre_modules[0](y_list[0])\n        for i in range(len(self.downsamp_modules)):\n            y = self.incre_modules[i + 1](y_list[i + 1]) + self.downsamp_modules[i](y)\n\n        y = self.final_layer(y)\n\n        y = F.avg_pool2d(y, kernel_size=y.size()\n        [2:]).view(y.size(0), -1)\n\n        y = self.classifier(y)\n\n        return y\n\n\nblocks = [BasicBlock, BasicBlock, BasicBlock]\nnum_modules = [1, 1, 1]\nnum_branches = [2, 3, 4]\nnum_blocks = [[4, 4], [4, 4, 4], [4, 4, 4, 4]]\nnum_channels = [[256, 256], [32, 64, 128], [32, 64, 128, 256]]\nfuse_method = ['sum', 'sum', 'sum']\n\nif __name__ == '__main__':\n    img = torch.randn(1, 3, 256, 256)\n    model = HighResolutionNet(blocks, num_channels, num_modules, num_branches, num_blocks, fuse_method)\n    output = model(img)\n"""
core/models/base_models/mobilenetv2.py,1,"b'""""""MobileNet and MobileNetV2.""""""\nimport torch\nimport torch.nn as nn\n\nfrom core.nn import _ConvBNReLU, _DepthwiseConv, InvertedResidual\n\n__all__ = [\'MobileNet\', \'MobileNetV2\', \'get_mobilenet\', \'get_mobilenet_v2\',\n           \'mobilenet1_0\', \'mobilenet_v2_1_0\', \'mobilenet0_75\', \'mobilenet_v2_0_75\',\n           \'mobilenet0_5\', \'mobilenet_v2_0_5\', \'mobilenet0_25\', \'mobilenet_v2_0_25\']\n\n\nclass MobileNet(nn.Module):\n    def __init__(self, num_classes=1000, multiplier=1.0, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(MobileNet, self).__init__()\n        conv_dw_setting = [\n            [64, 1, 1],\n            [128, 2, 2],\n            [256, 2, 2],\n            [512, 6, 2],\n            [1024, 2, 2]]\n        input_channels = int(32 * multiplier) if multiplier > 1.0 else 32\n        features = [_ConvBNReLU(3, input_channels, 3, 2, 1, norm_layer=norm_layer)]\n\n        for c, n, s in conv_dw_setting:\n            out_channels = int(c * multiplier)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(_DepthwiseConv(input_channels, out_channels, stride, norm_layer))\n                input_channels = out_channels\n        features.append(nn.AdaptiveAvgPool2d(1))\n        self.features = nn.Sequential(*features)\n\n        self.classifier = nn.Linear(int(1024 * multiplier), num_classes)\n\n        # weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x.view(x.size(0), x.size(1)))\n        return x\n\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, num_classes=1000, multiplier=1.0, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(MobileNetV2, self).__init__()\n        inverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1]]\n        # building first layer\n        input_channels = int(32 * multiplier) if multiplier > 1.0 else 32\n        last_channels = int(1280 * multiplier) if multiplier > 1.0 else 1280\n        features = [_ConvBNReLU(3, input_channels, 3, 2, 1, relu6=True, norm_layer=norm_layer)]\n\n        # building inverted residual blocks\n        for t, c, n, s in inverted_residual_setting:\n            out_channels = int(c * multiplier)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(InvertedResidual(input_channels, out_channels, stride, t, norm_layer))\n                input_channels = out_channels\n\n        # building last several layers\n        features.append(_ConvBNReLU(input_channels, last_channels, 1, relu6=True, norm_layer=norm_layer))\n        features.append(nn.AdaptiveAvgPool2d(1))\n        self.features = nn.Sequential(*features)\n\n        self.classifier = nn.Sequential(\n            nn.Dropout2d(0.2),\n            nn.Linear(last_channels, num_classes))\n\n        # weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x.view(x.size(0), x.size(1)))\n        return x\n\n\n# Constructor\ndef get_mobilenet(multiplier=1.0, pretrained=False, root=\'~/.torch/models\', **kwargs):\n    model = MobileNet(multiplier=multiplier, **kwargs)\n\n    if pretrained:\n        raise ValueError(""Not support pretrained"")\n    return model\n\n\ndef get_mobilenet_v2(multiplier=1.0, pretrained=False, root=\'~/.torch/models\', **kwargs):\n    model = MobileNetV2(multiplier=multiplier, **kwargs)\n\n    if pretrained:\n        raise ValueError(""Not support pretrained"")\n    return model\n\n\ndef mobilenet1_0(**kwargs):\n    return get_mobilenet(1.0, **kwargs)\n\n\ndef mobilenet_v2_1_0(**kwargs):\n    return get_mobilenet_v2(1.0, **kwargs)\n\n\ndef mobilenet0_75(**kwargs):\n    return get_mobilenet(0.75, **kwargs)\n\n\ndef mobilenet_v2_0_75(**kwargs):\n    return get_mobilenet_v2(0.75, **kwargs)\n\n\ndef mobilenet0_5(**kwargs):\n    return get_mobilenet(0.5, **kwargs)\n\n\ndef mobilenet_v2_0_5(**kwargs):\n    return get_mobilenet_v2(0.5, **kwargs)\n\n\ndef mobilenet0_25(**kwargs):\n    return get_mobilenet(0.25, **kwargs)\n\n\ndef mobilenet_v2_0_25(**kwargs):\n    return get_mobilenet_v2(0.25, **kwargs)\n\n\nif __name__ == \'__main__\':\n    model = mobilenet0_5()\n'"
core/models/base_models/resnet.py,8,"b'import torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\n           \'resnet152\']\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    """"""1x1 convolution""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, norm_layer=nn.BatchNorm2d):\n        super(BasicBlock, self).__init__()\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, norm_layer=nn.BatchNorm2d):\n        super(Bottleneck, self).__init__()\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, planes)\n        self.bn1 = norm_layer(planes)\n        self.conv2 = conv3x3(planes, planes, stride)\n        self.bn2 = norm_layer(planes)\n        self.conv3 = conv1x1(planes, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False, norm_layer=nn.BatchNorm2d):\n        super(ResNet, self).__init__()\n        self.inplanes = 64\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = norm_layer(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0], norm_layer=norm_layer)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, norm_layer=norm_layer)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, norm_layer=norm_layer)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, norm_layer=norm_layer)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, norm_layer=nn.BatchNorm2d):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet18\']))\n    return model\n\n\ndef resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet34\']))\n    return model\n\n\ndef resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\']))\n    return model\n\n\ndef resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet101\']))\n    return model\n\n\ndef resnet152(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet152\']))\n    return model\n\n\nif __name__ == \'__main__\':\n    import torch\n    img = torch.randn(4, 3, 224, 224)\n    model = resnet50(True)\n    output = model(img)'"
core/models/base_models/resnetv1b.py,11,"b'import torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\n__all__ = [\'ResNetV1b\', \'resnet18_v1b\', \'resnet34_v1b\', \'resnet50_v1b\',\n           \'resnet101_v1b\', \'resnet152_v1b\', \'resnet152_v1s\', \'resnet101_v1s\', \'resnet50_v1s\']\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\nclass BasicBlockV1b(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None,\n                 previous_dilation=1, norm_layer=nn.BatchNorm2d):\n        super(BasicBlockV1b, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, 3, stride,\n                               dilation, dilation, bias=False)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(True)\n        self.conv2 = nn.Conv2d(planes, planes, 3, 1, previous_dilation,\n                               dilation=previous_dilation, bias=False)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass BottleneckV1b(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None,\n                 previous_dilation=1, norm_layer=nn.BatchNorm2d):\n        super(BottleneckV1b, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n        self.bn1 = norm_layer(planes)\n        self.conv2 = nn.Conv2d(planes, planes, 3, stride,\n                               dilation, dilation, bias=False)\n        self.bn2 = norm_layer(planes)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNetV1b(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000, dilated=True, deep_stem=False,\n                 zero_init_residual=False, norm_layer=nn.BatchNorm2d):\n        self.inplanes = 128 if deep_stem else 64\n        super(ResNetV1b, self).__init__()\n        if deep_stem:\n            self.conv1 = nn.Sequential(\n                nn.Conv2d(3, 64, 3, 2, 1, bias=False),\n                norm_layer(64),\n                nn.ReLU(True),\n                nn.Conv2d(64, 64, 3, 1, 1, bias=False),\n                norm_layer(64),\n                nn.ReLU(True),\n                nn.Conv2d(64, 128, 3, 1, 1, bias=False)\n            )\n        else:\n            self.conv1 = nn.Conv2d(3, 64, 7, 2, 3, bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(True)\n        self.maxpool = nn.MaxPool2d(3, 2, 1)\n        self.layer1 = self._make_layer(block, 64, layers[0], norm_layer=norm_layer)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, norm_layer=norm_layer)\n        if dilated:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2, norm_layer=norm_layer)\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4, norm_layer=norm_layer)\n        else:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=2, norm_layer=norm_layer)\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=2, norm_layer=norm_layer)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, BottleneckV1b):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlockV1b):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, norm_layer=nn.BatchNorm2d):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion, 1, stride, bias=False),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        if dilation in (1, 2):\n            layers.append(block(self.inplanes, planes, stride, dilation=1, downsample=downsample,\n                                previous_dilation=dilation, norm_layer=norm_layer))\n        elif dilation == 4:\n            layers.append(block(self.inplanes, planes, stride, dilation=2, downsample=downsample,\n                                previous_dilation=dilation, norm_layer=norm_layer))\n        else:\n            raise RuntimeError(""=> unknown dilation size: {}"".format(dilation))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dilation=dilation,\n                                previous_dilation=dilation, norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef resnet18_v1b(pretrained=False, **kwargs):\n    model = ResNetV1b(BasicBlockV1b, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        old_dict = model_zoo.load_url(model_urls[\'resnet18\'])\n        model_dict = model.state_dict()\n        old_dict = {k: v for k, v in old_dict.items() if (k in model_dict)}\n        model_dict.update(old_dict)\n        model.load_state_dict(model_dict)\n    return model\n\n\ndef resnet34_v1b(pretrained=False, **kwargs):\n    model = ResNetV1b(BasicBlockV1b, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        old_dict = model_zoo.load_url(model_urls[\'resnet34\'])\n        model_dict = model.state_dict()\n        old_dict = {k: v for k, v in old_dict.items() if (k in model_dict)}\n        model_dict.update(old_dict)\n        model.load_state_dict(model_dict)\n    return model\n\n\ndef resnet50_v1b(pretrained=False, **kwargs):\n    model = ResNetV1b(BottleneckV1b, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        old_dict = model_zoo.load_url(model_urls[\'resnet50\'])\n        model_dict = model.state_dict()\n        old_dict = {k: v for k, v in old_dict.items() if (k in model_dict)}\n        model_dict.update(old_dict)\n        model.load_state_dict(model_dict)\n    return model\n\n\ndef resnet101_v1b(pretrained=False, **kwargs):\n    model = ResNetV1b(BottleneckV1b, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        old_dict = model_zoo.load_url(model_urls[\'resnet101\'])\n        model_dict = model.state_dict()\n        old_dict = {k: v for k, v in old_dict.items() if (k in model_dict)}\n        model_dict.update(old_dict)\n        model.load_state_dict(model_dict)\n    return model\n\n\ndef resnet152_v1b(pretrained=False, **kwargs):\n    model = ResNetV1b(BottleneckV1b, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        old_dict = model_zoo.load_url(model_urls[\'resnet152\'])\n        model_dict = model.state_dict()\n        old_dict = {k: v for k, v in old_dict.items() if (k in model_dict)}\n        model_dict.update(old_dict)\n        model.load_state_dict(model_dict)\n    return model\n\n\ndef resnet50_v1s(pretrained=False, root=\'~/.torch/models\', **kwargs):\n    model = ResNetV1b(BottleneckV1b, [3, 4, 6, 3], deep_stem=True, **kwargs)\n    if pretrained:\n        from ..model_store import get_resnet_file\n        model.load_state_dict(torch.load(get_resnet_file(\'resnet50\', root=root)), strict=False)\n    return model\n\n\ndef resnet101_v1s(pretrained=False, root=\'~/.torch/models\', **kwargs):\n    model = ResNetV1b(BottleneckV1b, [3, 4, 23, 3], deep_stem=True, **kwargs)\n    if pretrained:\n        from ..model_store import get_resnet_file\n        model.load_state_dict(torch.load(get_resnet_file(\'resnet101\', root=root)), strict=False)\n    return model\n\n\ndef resnet152_v1s(pretrained=False, root=\'~/.torch/models\', **kwargs):\n    model = ResNetV1b(BottleneckV1b, [3, 8, 36, 3], deep_stem=True, **kwargs)\n    if pretrained:\n        from ..model_store import get_resnet_file\n        model.load_state_dict(torch.load(get_resnet_file(\'resnet152\', root=root)), strict=False)\n    return model\n\n\nif __name__ == \'__main__\':\n    import torch\n\n    img = torch.randn(4, 3, 224, 224)\n    model = resnet50_v1b(True)\n    output = model(img)\n'"
core/models/base_models/resnext.py,4,"b'import torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\n__all__ = [\'ResNext\', \'resnext50_32x4d\', \'resnext101_32x8d\']\n\nmodel_urls = {\n    \'resnext50_32x4d\': \'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\',\n    \'resnext101_32x8d\': \'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth\',\n}\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None, **kwargs):\n        super(Bottleneck, self).__init__()\n        width = int(planes * (base_width / 64.)) * groups\n\n        self.conv1 = nn.Conv2d(inplanes, width, 1, bias=False)\n        self.bn1 = norm_layer(width)\n        self.conv2 = nn.Conv2d(width, width, 3, stride, dilation, dilation, groups, bias=False)\n        self.bn2 = norm_layer(width)\n        self.conv3 = nn.Conv2d(width, planes * self.expansion, 1, bias=False)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNext(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False, groups=1,\n                 width_per_group=64, dilated=False, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(ResNext, self).__init__()\n        self.inplanes = 64\n        self.groups = groups\n        self.base_width = width_per_group\n\n        self.conv1 = nn.Conv2d(3, self.inplanes, 7, 2, 3, bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(True)\n        self.maxpool = nn.MaxPool2d(3, 2, 1)\n\n        self.layer1 = self._make_layer(block, 64, layers[0], norm_layer=norm_layer)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, norm_layer=norm_layer)\n        if dilated:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2, norm_layer=norm_layer)\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4, norm_layer=norm_layer)\n        else:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=2, norm_layer=norm_layer)\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=2, norm_layer=norm_layer)\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, norm_layer=nn.BatchNorm2d):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion, 1, stride, bias=False),\n                norm_layer(planes * block.expansion)\n            )\n\n        layers = list()\n        if dilation in (1, 2):\n            layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n                                self.base_width, norm_layer=norm_layer))\n        elif dilation == 4:\n            layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n                                self.base_width, dilation=2, norm_layer=norm_layer))\n        else:\n            raise RuntimeError(""=> unknown dilation size: {}"".format(dilation))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups=self.groups, base_width=self.base_width,\n                                dilation=dilation, norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef resnext50_32x4d(pretrained=False, **kwargs):\n    kwargs[\'groups\'] = 32\n    kwargs[\'width_per_group\'] = 4\n    model = ResNext(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        state_dict = model_zoo.load_url(model_urls[\'resnext50_32x4d\'])\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef resnext101_32x8d(pretrained=False, **kwargs):\n    kwargs[\'groups\'] = 32\n    kwargs[\'width_per_group\'] = 8\n    model = ResNext(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        state_dict = model_zoo.load_url(model_urls[\'resnext101_32x8d\'])\n        model.load_state_dict(state_dict)\n    return model\n\n\nif __name__ == \'__main__\':\n    model = resnext101_32x8d()\n'"
core/models/base_models/vgg.py,11,"b'import torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\n__all__ = [\n    \'VGG\', \'vgg11\', \'vgg11_bn\', \'vgg13\', \'vgg13_bn\', \'vgg16\', \'vgg16_bn\',\n    \'vgg19_bn\', \'vgg19\',\n]\n\nmodel_urls = {\n    \'vgg11\': \'https://download.pytorch.org/models/vgg11-bbd30ac9.pth\',\n    \'vgg13\': \'https://download.pytorch.org/models/vgg13-c768596a.pth\',\n    \'vgg16\': \'https://download.pytorch.org/models/vgg16-397923af.pth\',\n    \'vgg19\': \'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\',\n    \'vgg11_bn\': \'https://download.pytorch.org/models/vgg11_bn-6002323d.pth\',\n    \'vgg13_bn\': \'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth\',\n    \'vgg16_bn\': \'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\',\n    \'vgg19_bn\': \'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth\',\n}\n\n\nclass VGG(nn.Module):\n    def __init__(self, features, num_classes=1000, init_weights=True):\n        super(VGG, self).__init__()\n        self.features = features\n        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, num_classes)\n        )\n        if init_weights:\n            self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)\n\n\ndef make_layers(cfg, batch_norm=False):\n    layers = []\n    in_channels = 3\n    for v in cfg:\n        if v == \'M\':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += (conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True))\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = v\n    return nn.Sequential(*layers)\n\n\ncfg = {\n    \'A\': [64, \'M\', 128, \'M\', 256, 256, \'M\', 512, 512, \'M\', 512, 512, \'M\'],\n    \'B\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, \'M\', 512, 512, \'M\', 512, 512, \'M\'],\n    \'D\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, \'M\', 512, 512, 512, \'M\', 512, 512, 512, \'M\'],\n    \'E\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, 256, \'M\', 512, 512, 512, 512, \'M\', 512, 512, 512, 512, \'M\'],\n}\n\n\ndef vgg11(pretrained=False, **kwargs):\n    """"""VGG 11-layer model (configuration ""A"")\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    if pretrained:\n        kwargs[\'init_weights\'] = False\n    model = VGG(make_layers(cfg[\'A\']), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg11\']))\n    return model\n\n\ndef vgg11_bn(pretrained=False, **kwargs):\n    """"""VGG 11-layer model (configuration ""A"") with batch normalization\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    if pretrained:\n        kwargs[\'init_weights\'] = False\n    model = VGG(make_layers(cfg[\'A\'], batch_norm=True), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg11_bn\']))\n    return model\n\n\ndef vgg13(pretrained=False, **kwargs):\n    """"""VGG 13-layer model (configuration ""B"")\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    if pretrained:\n        kwargs[\'init_weights\'] = False\n    model = VGG(make_layers(cfg[\'B\']), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg13\']))\n    return model\n\n\ndef vgg13_bn(pretrained=False, **kwargs):\n    """"""VGG 13-layer model (configuration ""B"") with batch normalization\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    if pretrained:\n        kwargs[\'init_weights\'] = False\n    model = VGG(make_layers(cfg[\'B\'], batch_norm=True), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg13_bn\']))\n    return model\n\n\ndef vgg16(pretrained=False, **kwargs):\n    """"""VGG 16-layer model (configuration ""D"")\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    if pretrained:\n        kwargs[\'init_weights\'] = False\n    model = VGG(make_layers(cfg[\'D\']), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg16\']))\n    return model\n\n\ndef vgg16_bn(pretrained=False, **kwargs):\n    """"""VGG 16-layer model (configuration ""D"") with batch normalization\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    if pretrained:\n        kwargs[\'init_weights\'] = False\n    model = VGG(make_layers(cfg[\'D\'], batch_norm=True), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg16_bn\']))\n    return model\n\n\ndef vgg19(pretrained=False, **kwargs):\n    """"""VGG 19-layer model (configuration ""E"")\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    if pretrained:\n        kwargs[\'init_weights\'] = False\n    model = VGG(make_layers(cfg[\'E\']), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg19\']))\n    return model\n\n\ndef vgg19_bn(pretrained=False, **kwargs):\n    """"""VGG 19-layer model (configuration \'E\') with batch normalization\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    if pretrained:\n        kwargs[\'init_weights\'] = False\n    model = VGG(make_layers(cfg[\'E\'], batch_norm=True), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg19_bn\']))\n    return model\n\n\nif __name__ == \'__main__\':\n    img = torch.randn((4, 3, 480, 480))\n    model = vgg16(pretrained=False)\n    out = model(img)\n'"
core/models/base_models/xception.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n__all__ = [\'Enc\', \'FCAttention\', \'Xception65\', \'Xception71\', \'get_xception\', \'get_xception_71\', \'get_xception_a\']\n\n\nclass SeparableConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1, bias=False, norm_layer=None):\n        super(SeparableConv2d, self).__init__()\n        self.kernel_size = kernel_size\n        self.dilation = dilation\n\n        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size, stride, 0, dilation, groups=in_channels,\n                               bias=bias)\n        self.bn = norm_layer(in_channels)\n        self.pointwise = nn.Conv2d(in_channels, out_channels, 1, bias=bias)\n\n    def forward(self, x):\n        x = self.fix_padding(x, self.kernel_size, self.dilation)\n        x = self.conv1(x)\n        x = self.bn(x)\n        x = self.pointwise(x)\n\n        return x\n\n    def fix_padding(self, x, kernel_size, dilation):\n        kernel_size_effective = kernel_size + (kernel_size - 1) * (dilation - 1)\n        pad_total = kernel_size_effective - 1\n        pad_beg = pad_total // 2\n        pad_end = pad_total - pad_beg\n        padded_inputs = F.pad(x, (pad_beg, pad_end, pad_beg, pad_end))\n        return padded_inputs\n\n\nclass Block(nn.Module):\n    def __init__(self, in_channels, out_channels, reps, stride=1, dilation=1, norm_layer=None,\n                 start_with_relu=True, grow_first=True, is_last=False):\n        super(Block, self).__init__()\n        if out_channels != in_channels or stride != 1:\n            self.skip = nn.Conv2d(in_channels, out_channels, 1, stride, bias=False)\n            self.skipbn = norm_layer(out_channels)\n        else:\n            self.skip = None\n        self.relu = nn.ReLU(True)\n        rep = list()\n        filters = in_channels\n        if grow_first:\n            if start_with_relu:\n                rep.append(self.relu)\n            rep.append(SeparableConv2d(in_channels, out_channels, 3, 1, dilation, norm_layer=norm_layer))\n            rep.append(norm_layer(out_channels))\n            filters = out_channels\n        for i in range(reps - 1):\n            if grow_first or start_with_relu:\n                rep.append(self.relu)\n            rep.append(SeparableConv2d(filters, filters, 3, 1, dilation, norm_layer=norm_layer))\n            rep.append(norm_layer(filters))\n        if not grow_first:\n            rep.append(self.relu)\n            rep.append(SeparableConv2d(in_channels, out_channels, 3, 1, dilation, norm_layer=norm_layer))\n        if stride != 1:\n            rep.append(self.relu)\n            rep.append(SeparableConv2d(out_channels, out_channels, 3, stride, norm_layer=norm_layer))\n            rep.append(norm_layer(out_channels))\n        elif is_last:\n            rep.append(self.relu)\n            rep.append(SeparableConv2d(out_channels, out_channels, 3, 1, dilation, norm_layer=norm_layer))\n            rep.append(norm_layer(out_channels))\n        self.rep = nn.Sequential(*rep)\n\n    def forward(self, x):\n        out = self.rep(x)\n        if self.skip is not None:\n            skip = self.skipbn(self.skip(x))\n        else:\n            skip = x\n        out = out + skip\n        return out\n\n\nclass Xception65(nn.Module):\n    """"""Modified Aligned Xception\n    """"""\n\n    def __init__(self, num_classes=1000, output_stride=32, norm_layer=nn.BatchNorm2d):\n        super(Xception65, self).__init__()\n        if output_stride == 32:\n            entry_block3_stride = 2\n            exit_block20_stride = 2\n            middle_block_dilation = 1\n            exit_block_dilations = (1, 1)\n        elif output_stride == 16:\n            entry_block3_stride = 2\n            exit_block20_stride = 1\n            middle_block_dilation = 1\n            exit_block_dilations = (1, 2)\n        elif output_stride == 8:\n            entry_block3_stride = 1\n            exit_block20_stride = 1\n            middle_block_dilation = 2\n            exit_block_dilations = (2, 4)\n        else:\n            raise NotImplementedError\n        # Entry flow\n        self.conv1 = nn.Conv2d(3, 32, 3, 2, 1, bias=False)\n        self.bn1 = norm_layer(32)\n        self.relu = nn.ReLU(True)\n\n        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1, bias=False)\n        self.bn2 = norm_layer(64)\n\n        self.block1 = Block(64, 128, reps=2, stride=2, norm_layer=norm_layer, start_with_relu=False)\n        self.block2 = Block(128, 256, reps=2, stride=2, norm_layer=norm_layer, start_with_relu=False, grow_first=True)\n        self.block3 = Block(256, 728, reps=2, stride=entry_block3_stride, norm_layer=norm_layer,\n                            start_with_relu=True, grow_first=True, is_last=True)\n\n        # Middle flow\n        midflow = list()\n        for i in range(4, 20):\n            midflow.append(Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, norm_layer=norm_layer,\n                                 start_with_relu=True, grow_first=True))\n        self.midflow = nn.Sequential(*midflow)\n\n        # Exit flow\n        self.block20 = Block(728, 1024, reps=2, stride=exit_block20_stride, dilation=exit_block_dilations[0],\n                             norm_layer=norm_layer, start_with_relu=True, grow_first=False, is_last=True)\n        self.conv3 = SeparableConv2d(1024, 1536, 3, 1, dilation=exit_block_dilations[1], norm_layer=norm_layer)\n        self.bn3 = norm_layer(1536)\n        self.conv4 = SeparableConv2d(1536, 1536, 3, stride=1, dilation=exit_block_dilations[1], norm_layer=norm_layer)\n        self.bn4 = norm_layer(1536)\n        self.conv5 = SeparableConv2d(1536, 2048, 3, 1, dilation=exit_block_dilations[1], norm_layer=norm_layer)\n        self.bn5 = norm_layer(2048)\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(2048, num_classes)\n\n    def forward(self, x):\n        # Entry flow\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n\n        x = self.block1(x)\n        x = self.relu(x)\n        # c1 = x\n        x = self.block2(x)\n        # c2 = x\n        x = self.block3(x)\n\n        # Middle flow\n        x = self.midflow(x)\n        # c3 = x\n\n        # Exit flow\n        x = self.block20(x)\n        x = self.relu(x)\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.relu(x)\n\n        x = self.conv4(x)\n        x = self.bn4(x)\n        x = self.relu(x)\n\n        x = self.conv5(x)\n        x = self.bn5(x)\n        x = self.relu(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\nclass Xception71(nn.Module):\n    """"""Modified Aligned Xception\n    """"""\n\n    def __init__(self, num_classes=1000, output_stride=32, norm_layer=nn.BatchNorm2d):\n        super(Xception71, self).__init__()\n        if output_stride == 32:\n            entry_block3_stride = 2\n            exit_block20_stride = 2\n            middle_block_dilation = 1\n            exit_block_dilations = (1, 1)\n        elif output_stride == 16:\n            entry_block3_stride = 2\n            exit_block20_stride = 1\n            middle_block_dilation = 1\n            exit_block_dilations = (1, 2)\n        elif output_stride == 8:\n            entry_block3_stride = 1\n            exit_block20_stride = 1\n            middle_block_dilation = 2\n            exit_block_dilations = (2, 4)\n        else:\n            raise NotImplementedError\n        # Entry flow\n        self.conv1 = nn.Conv2d(3, 32, 3, 2, 1, bias=False)\n        self.bn1 = norm_layer(32)\n        self.relu = nn.ReLU(True)\n\n        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1, bias=False)\n        self.bn2 = norm_layer(64)\n\n        self.block1 = Block(64, 128, reps=2, stride=2, norm_layer=norm_layer, start_with_relu=False)\n        self.block2 = nn.Sequential(\n            Block(128, 256, reps=2, stride=2, norm_layer=norm_layer, start_with_relu=False, grow_first=True),\n            Block(256, 728, reps=2, stride=2, norm_layer=norm_layer, start_with_relu=False, grow_first=True))\n        self.block3 = Block(728, 728, reps=2, stride=entry_block3_stride, norm_layer=norm_layer,\n                            start_with_relu=True, grow_first=True, is_last=True)\n\n        # Middle flow\n        midflow = list()\n        for i in range(4, 20):\n            midflow.append(Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, norm_layer=norm_layer,\n                                 start_with_relu=True, grow_first=True))\n        self.midflow = nn.Sequential(*midflow)\n\n        # Exit flow\n        self.block20 = Block(728, 1024, reps=2, stride=exit_block20_stride, dilation=exit_block_dilations[0],\n                             norm_layer=norm_layer, start_with_relu=True, grow_first=False, is_last=True)\n        self.conv3 = SeparableConv2d(1024, 1536, 3, 1, dilation=exit_block_dilations[1], norm_layer=norm_layer)\n        self.bn3 = norm_layer(1536)\n        self.conv4 = SeparableConv2d(1536, 1536, 3, stride=1, dilation=exit_block_dilations[1], norm_layer=norm_layer)\n        self.bn4 = norm_layer(1536)\n        self.conv5 = SeparableConv2d(1536, 2048, 3, 1, dilation=exit_block_dilations[1], norm_layer=norm_layer)\n        self.bn5 = norm_layer(2048)\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(2048, num_classes)\n\n    def forward(self, x):\n        # Entry flow\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n\n        x = self.block1(x)\n        x = self.relu(x)\n        # c1 = x\n        x = self.block2(x)\n        # c2 = x\n        x = self.block3(x)\n\n        # Middle flow\n        x = self.midflow(x)\n        # c3 = x\n\n        # Exit flow\n        x = self.block20(x)\n        x = self.relu(x)\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.relu(x)\n\n        x = self.conv4(x)\n        x = self.bn4(x)\n        x = self.relu(x)\n\n        x = self.conv5(x)\n        x = self.bn5(x)\n        x = self.relu(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\n# -------------------------------------------------\n#                   For DFANet\n# -------------------------------------------------\nclass BlockA(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, dilation=1, norm_layer=None, start_with_relu=True):\n        super(BlockA, self).__init__()\n        if out_channels != in_channels or stride != 1:\n            self.skip = nn.Conv2d(in_channels, out_channels, 1, stride, bias=False)\n            self.skipbn = norm_layer(out_channels)\n        else:\n            self.skip = None\n        self.relu = nn.ReLU(True)\n        rep = list()\n        inter_channels = out_channels // 4\n\n        if start_with_relu:\n            rep.append(self.relu)\n        rep.append(SeparableConv2d(in_channels, inter_channels, 3, 1, dilation, norm_layer=norm_layer))\n        rep.append(norm_layer(inter_channels))\n\n        rep.append(self.relu)\n        rep.append(SeparableConv2d(inter_channels, inter_channels, 3, 1, dilation, norm_layer=norm_layer))\n        rep.append(norm_layer(inter_channels))\n\n        if stride != 1:\n            rep.append(self.relu)\n            rep.append(SeparableConv2d(inter_channels, out_channels, 3, stride, norm_layer=norm_layer))\n            rep.append(norm_layer(out_channels))\n        else:\n            rep.append(self.relu)\n            rep.append(SeparableConv2d(inter_channels, out_channels, 3, 1, norm_layer=norm_layer))\n            rep.append(norm_layer(out_channels))\n        self.rep = nn.Sequential(*rep)\n\n    def forward(self, x):\n        out = self.rep(x)\n        if self.skip is not None:\n            skip = self.skipbn(self.skip(x))\n        else:\n            skip = x\n        out = out + skip\n        return out\n\n\nclass Enc(nn.Module):\n    def __init__(self, in_channels, out_channels, blocks, norm_layer=None):\n        super(Enc, self).__init__()\n        block = list()\n        block.append(BlockA(in_channels, out_channels, 2, norm_layer=norm_layer))\n        for i in range(blocks - 1):\n            block.append(BlockA(out_channels, out_channels, 1, norm_layer=norm_layer))\n        self.block = nn.Sequential(*block)\n\n    def forward(self, x):\n        return self.block(x)\n\n\nclass FCAttention(nn.Module):\n    def __init__(self, in_channels, norm_layer=None):\n        super(FCAttention, self).__init__()\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(in_channels, 1000)\n        self.conv = nn.Sequential(\n            nn.Conv2d(1000, in_channels, 1, bias=False),\n            norm_layer(in_channels),\n            nn.ReLU(True))\n\n    def forward(self, x):\n        n, c, _, _ = x.size()\n        att = self.avgpool(x).view(n, c)\n        att = self.fc(att).view(n, 1000, 1, 1)\n        att = self.conv(att)\n        return x * att.expand_as(x)\n\n\nclass XceptionA(nn.Module):\n    def __init__(self, num_classes=1000, norm_layer=nn.BatchNorm2d):\n        super(XceptionA, self).__init__()\n        self.conv1 = nn.Sequential(nn.Conv2d(3, 8, 3, 2, 1, bias=False),\n                                   norm_layer(8),\n                                   nn.ReLU(True))\n\n        self.enc2 = Enc(8, 48, 4, norm_layer=norm_layer)\n        self.enc3 = Enc(48, 96, 6, norm_layer=norm_layer)\n        self.enc4 = Enc(96, 192, 4, norm_layer=norm_layer)\n\n        self.fca = FCAttention(192, norm_layer=norm_layer)\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(192, num_classes)\n\n    def forward(self, x):\n        x = self.conv1(x)\n\n        x = self.enc2(x)\n        x = self.enc3(x)\n        x = self.enc4(x)\n        x = self.fca(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\n# Constructor\ndef get_xception(pretrained=False, root=\'~/.torch/models\', **kwargs):\n    model = Xception65(**kwargs)\n    if pretrained:\n        from ..model_store import get_model_file\n        model.load_state_dict(torch.load(get_model_file(\'xception\', root=root)))\n    return model\n\n\ndef get_xception_71(pretrained=False, root=\'~/.torch/models\', **kwargs):\n    model = Xception71(**kwargs)\n    if pretrained:\n        from ..model_store import get_model_file\n        model.load_state_dict(torch.load(get_model_file(\'xception71\', root=root)))\n    return model\n\n\ndef get_xception_a(pretrained=False, root=\'~/.torch/models\', **kwargs):\n    model = XceptionA(**kwargs)\n    if pretrained:\n        from ..model_store import get_model_file\n        model.load_state_dict(torch.load(get_model_file(\'xception_a\', root=root)))\n    return model\n\n\nif __name__ == \'__main__\':\n    model = get_xception_a()\n'"
core/nn/sync_bn/__init__.py,0,b''
core/nn/sync_bn/functions.py,3,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Hang Zhang\n## Email: zhanghang0704@gmail.com\n## Copyright (c) 2018\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n""""""Synchronized Cross-GPU Batch Normalization functions""""""\nimport torch.cuda.comm as comm\n\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\nfrom core.nn.sync_bn import lib\n\n__all__ = [\'syncbatchnorm\', \'inp_syncbatchnorm\']\n\n\nclass syncbatchnorm_(Function):\n    @classmethod\n    def forward(cls, ctx, x, gamma, beta, running_mean, running_var,\n                extra, sync=True, training=True, momentum=0.1, eps=1e-05,\n                activation=""none"", slope=0.01):\n        # save context\n        cls._parse_extra(ctx, extra)\n        ctx.sync = sync\n        ctx.training = training\n        ctx.momentum = momentum\n        ctx.eps = eps\n        ctx.activation = activation\n        ctx.slope = slope\n        assert activation == \'none\'\n\n        # continous inputs\n        x = x.contiguous()\n        gamma = gamma.contiguous()\n        beta = beta.contiguous()\n\n        if ctx.training:\n            if x.is_cuda:\n                _ex, _exs = lib.gpu.expectation_forward(x)\n            else:\n                raise NotImplemented\n\n            if ctx.sync:\n                if ctx.is_master:\n                    _ex, _exs = [_ex.unsqueeze(0)], [_exs.unsqueeze(0)]\n                    for _ in range(ctx.master_queue.maxsize):\n                        _ex_w, _exs_w = ctx.master_queue.get()\n                        ctx.master_queue.task_done()\n                        _ex.append(_ex_w.unsqueeze(0))\n                        _exs.append(_exs_w.unsqueeze(0))\n\n                    _ex = comm.gather(_ex).mean(0)\n                    _exs = comm.gather(_exs).mean(0)\n\n                    tensors = comm.broadcast_coalesced((_ex, _exs), [_ex.get_device()] + ctx.worker_ids)\n                    for ts, queue in zip(tensors[1:], ctx.worker_queues):\n                        queue.put(ts)\n                else:\n                    ctx.master_queue.put((_ex, _exs))\n                    _ex, _exs = ctx.worker_queue.get()\n                    ctx.worker_queue.task_done()\n\n            # Update running stats\n            _var = _exs - _ex ** 2\n            running_mean.mul_((1 - ctx.momentum)).add_(ctx.momentum * _ex)\n            running_var.mul_((1 - ctx.momentum)).add_(ctx.momentum * _var)\n\n            # Mark in-place modified tensors\n            ctx.mark_dirty(running_mean, running_var)\n        else:\n            _ex, _var = running_mean.contiguous(), running_var.contiguous()\n            _exs = _var + _ex ** 2\n\n        # BN forward\n        if x.is_cuda:\n            y = lib.gpu.batchnorm_forward(x, _ex, _exs, gamma, beta, ctx.eps)\n        else:\n            y = lib.cpu.batchnorm_forward(x, _ex, _exs, gamma, beta, ctx.eps)\n\n        # Output\n        ctx.save_for_backward(x, _ex, _exs, gamma, beta)\n        return y\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, dz):\n        x, _ex, _exs, gamma, beta = ctx.saved_tensors\n        dz = dz.contiguous()\n\n        # BN backward\n        if dz.is_cuda:\n            dx, _dex, _dexs, dgamma, dbeta = lib.gpu.batchnorm_backward(dz, x, _ex, _exs, gamma, beta, ctx.eps)\n        else:\n            raise NotImplemented\n\n        if ctx.training:\n            if ctx.sync:\n                if ctx.is_master:\n                    _dex, _dexs = [_dex.unsqueeze(0)], [_dexs.unsqueeze(0)]\n                    for _ in range(ctx.master_queue.maxsize):\n                        _dex_w, _dexs_w = ctx.master_queue.get()\n                        ctx.master_queue.task_done()\n                        _dex.append(_dex_w.unsqueeze(0))\n                        _dexs.append(_dexs_w.unsqueeze(0))\n\n                    _dex = comm.gather(_dex).mean(0)\n                    _dexs = comm.gather(_dexs).mean(0)\n\n                    tensors = comm.broadcast_coalesced((_dex, _dexs), [_dex.get_device()] + ctx.worker_ids)\n                    for ts, queue in zip(tensors[1:], ctx.worker_queues):\n                        queue.put(ts)\n                else:\n                    ctx.master_queue.put((_dex, _dexs))\n                    _dex, _dexs = ctx.worker_queue.get()\n                    ctx.worker_queue.task_done()\n\n            if x.is_cuda:\n                dx_ = lib.gpu.expectation_backward(x, _dex, _dexs)\n            else:\n                raise NotImplemented\n            dx = dx + dx_\n\n        return dx, dgamma, dbeta, None, None, None, None, None, None, None, None, None\n\n    @staticmethod\n    def _parse_extra(ctx, extra):\n        ctx.is_master = extra[""is_master""]\n        if ctx.is_master:\n            ctx.master_queue = extra[""master_queue""]\n            ctx.worker_queues = extra[""worker_queues""]\n            ctx.worker_ids = extra[""worker_ids""]\n        else:\n            ctx.master_queue = extra[""master_queue""]\n            ctx.worker_queue = extra[""worker_queue""]\n\n\ndef _act_forward(ctx, x):\n    if ctx.activation.lower() == ""leaky_relu"":\n        if x.is_cuda:\n            lib.gpu.leaky_relu_forward(x, ctx.slope)\n        else:\n            raise NotImplemented\n    else:\n        assert ctx.activation == \'none\'\n\n\ndef _act_backward(ctx, x, dx):\n    if ctx.activation.lower() == ""leaky_relu"":\n        if x.is_cuda:\n            lib.gpu.leaky_relu_backward(x, dx, ctx.slope)\n        else:\n            raise NotImplemented\n    else:\n        assert ctx.activation == \'none\'\n\n\nclass inp_syncbatchnorm_(Function):\n    @classmethod\n    def forward(cls, ctx, x, gamma, beta, running_mean, running_var,\n                extra, sync=True, training=True, momentum=0.1, eps=1e-5,\n                activation=\'none\', slope=0.01):\n        # save context\n        cls._parse_extra(ctx, extra)\n        ctx.sync = sync\n        ctx.training = training\n        ctx.momentum = momentum\n        ctx.eps = eps\n        ctx.activation = activation\n        ctx.slope = slope\n\n        # continous inputs\n        x = x.contiguous()\n        gamma = gamma.contiguous()\n        beta = beta.contiguous()\n\n        if ctx.training:\n            if x.is_cuda:\n                _ex, _exs = lib.gpu.expectation_forward(x)\n            else:\n                raise NotImplemented\n\n            if ctx.sync:\n                if ctx.is_master:\n                    _ex, _exs = [_ex.unsqueeze(0)], [_exs.unsqueeze(0)]\n                    for _ in range(ctx.master_queue.maxsize):\n                        _ex_w, _exs_w = ctx.master_queue.get()\n                        ctx.master_queue.task_done()\n                        _ex.append(_ex_w.unsqueeze(0))\n                        _exs.append(_exs_w.unsuqeeze(0))\n\n                    _ex = comm.gather(_ex).mean(0)\n                    _exs = comm.gather(_exs).mean(0)\n\n                    tensors = comm.broadcast_coalesced((_ex, _exs), [_ex.get_device()] + ctx.worker_ids)\n                    for ts, queue in zip(tensors[1:], ctx.worker_queues):\n                        queue.put(ts)\n                else:\n                    ctx.master_queue.put((_ex, _exs))\n                    _ex, _exs = ctx.worker_queue.get()\n                    ctx.worker_queue.task_done()\n\n            # Update running stats\n            _var = _exs - _ex ** 2\n            running_mean.mul_((1 - ctx.momentum)).add_(ctx.momentum * _ex)\n            running_var.mul_((1 - ctx.momentum)).add_(ctx.momentum * _var)\n\n            # Mark in-place modified tensors\n            ctx.mark_dirty(x, running_mean, running_var)\n        else:\n            _ex, _var = running_mean.contiguous(), running_var.contiguous()\n            _exs = _var + _ex ** 2\n            ctx.mark_dirty(x)\n\n        # BN forward + activation\n        if x.is_cuda:\n            lib.gpu.batchnorm_inp_forward(x, _ex, _exs, gamma, beta, ctx.eps)\n        else:\n            raise NotImplemented\n\n        _act_forward(ctx, x)\n\n        # Output\n        ctx.save_for_backward(x, _ex, _exs, gamma, beta)\n        return x\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, dz):\n        z, _ex, _exs, gamma, beta = ctx.saved_tensors\n        dz = dz.contiguous()\n\n        # Undo activation\n        _act_backward(ctx, z, dz)\n\n        # BN backward\n        if dz.is_cuda:\n            dx, _dex, _dexs, dgamma, dbeta = lib.gpu.batchnorm_inp_backward(dz, z, _ex, _exs, gamma, beta, ctx.eps)\n        else:\n            raise NotImplemented\n\n        if ctx.training:\n            if ctx.sync:\n                if ctx.is_master:\n                    _dex, _dexs = [_dex.unsqueeze(0)], [_dexs.unsqueeze(0)]\n                    for _ in range(ctx.master_queue.maxsize):\n                        _dex_w, _dexs_w = ctx.master_queue.get()\n                        ctx.master_queue.task_done()\n                        _dex.append(_dex_w.unsqueeze(0))\n                        _dexs.append(_dexs_w.unsqueeze(0))\n\n                    _dex = comm.gather(_dex).mean(0)\n                    _dexs = comm.gather(_dexs).mean(0)\n\n                    tensors = comm.broadcast_coalesced((_dex, _dexs), [_dex.get_device()] + ctx.worker_ids)\n                    for ts, queue in zip(tensors[1:], ctx.worker_queues):\n                        queue.put(ts)\n                else:\n                    ctx.master_queue.put((_dex, _dexs))\n                    _dex, _dexs = ctx.worker_queue.get()\n                    ctx.worker_queue.task_done()\n\n            if z.is_cuda:\n                lib.gpu.expectation_inp_backward(dx, z, _dex, _dexs, _ex, _exs, gamma, beta, ctx.eps)\n            else:\n                raise NotImplemented\n\n        return dx, dgamma, dbeta, None, None, None, None, None, None, None, None, None\n\n    @staticmethod\n    def _parse_extra(ctx, extra):\n        ctx.is_master = extra[""is_master""]\n        if ctx.is_master:\n            ctx.master_queue = extra[""master_queue""]\n            ctx.worker_queues = extra[""worker_queues""]\n            ctx.worker_ids = extra[""worker_ids""]\n        else:\n            ctx.master_queue = extra[""master_queue""]\n            ctx.worker_queue = extra[""worker_queue""]\n\n\nsyncbatchnorm = syncbatchnorm_.apply\ninp_syncbatchnorm = inp_syncbatchnorm_.apply\n'"
core/nn/sync_bn/syncbn.py,3,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Hang Zhang\n## ECE Department, Rutgers University\n## Email: zhang.hang@rutgers.edu\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n""""""Synchronized Cross-GPU Batch Normalization Module""""""\nimport warnings\nimport torch\n\nfrom torch.nn.modules.batchnorm import _BatchNorm\nfrom queue import Queue\nfrom .functions import *\n\n__all__ = [\'SyncBatchNorm\', \'BatchNorm1d\', \'BatchNorm2d\', \'BatchNorm3d\']\n\n\n# Adopt from https://github.com/zhanghang1989/PyTorch-Encoding/blob/master/encoding/nn/syncbn.py\nclass SyncBatchNorm(_BatchNorm):\n    """"""Cross-GPU Synchronized Batch normalization (SyncBN)\n\n    Parameters:\n        num_features: num_features from an expected input of\n            size batch_size x num_features x height x width\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Default: 0.1\n        sync: a boolean value that when set to ``True``, synchronize across\n            different gpus. Default: ``True``\n        activation : str\n            Name of the activation functions, one of: `leaky_relu` or `none`.\n        slope : float\n            Negative slope for the `leaky_relu` activation.\n\n    Shape:\n        - Input: :math:`(N, C, H, W)`\n        - Output: :math:`(N, C, H, W)` (same shape as input)\n    Reference:\n        .. [1] Ioffe, Sergey, and Christian Szegedy. ""Batch normalization: Accelerating deep network training by reducing internal covariate shift."" *ICML 2015*\n        .. [2] Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang, Ambrish Tyagi, and Amit Agrawal. ""Context Encoding for Semantic Segmentation."" *CVPR 2018*\n    Examples:\n        >>> m = SyncBatchNorm(100)\n        >>> net = torch.nn.DataParallel(m)\n        >>> output = net(input)\n    """"""\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, sync=True, activation=\'none\', slope=0.01, inplace=True):\n        super(SyncBatchNorm, self).__init__(num_features, eps=eps, momentum=momentum, affine=True)\n        self.activation = activation\n        self.inplace = False if activation == \'none\' else inplace\n        self.slope = slope\n        self.devices = list(range(torch.cuda.device_count()))\n        self.sync = sync if len(self.devices) > 1 else False\n        # Initialize queues\n        self.worker_ids = self.devices[1:]\n        self.master_queue = Queue(len(self.worker_ids))\n        self.worker_queues = [Queue(1) for _ in self.worker_ids]\n\n    def forward(self, x):\n        # resize the input to (B, C, -1)\n        input_shape = x.size()\n        x = x.view(input_shape[0], self.num_features, -1)\n        if x.get_device() == self.devices[0]:\n            # Master mode\n            extra = {\n                ""is_master"": True,\n                ""master_queue"": self.master_queue,\n                ""worker_queues"": self.worker_queues,\n                ""worker_ids"": self.worker_ids\n            }\n        else:\n            # Worker mode\n            extra = {\n                ""is_master"": False,\n                ""master_queue"": self.master_queue,\n                ""worker_queue"": self.worker_queues[self.worker_ids.index(x.get_device())]\n            }\n        if self.inplace:\n            return inp_syncbatchnorm(x, self.weight, self.bias, self.running_mean, self.running_var,\n                                     extra, self.sync, self.training, self.momentum, self.eps,\n                                     self.activation, self.slope).view(input_shape)\n        else:\n            return syncbatchnorm(x, self.weight, self.bias, self.running_mean, self.running_var,\n                                 extra, self.sync, self.training, self.momentum, self.eps,\n                                 self.activation, self.slope).view(input_shape)\n\n    def extra_repr(self):\n        if self.activation == \'none\':\n            return \'sync={}\'.format(self.sync)\n        else:\n            return \'sync={}, act={}, slope={}, inplace={}\'.format(\n                self.sync, self.activation, self.slope, self.inplace)\n\n\nclass BatchNorm1d(SyncBatchNorm):\n    """"""BatchNorm1d is deprecated in favor of :class:`core.nn.sync_bn.SyncBatchNorm`.""""""\n\n    def __init__(self, *args, **kwargs):\n        warnings.warn(""core.nn.sync_bn.{} is now deprecated in favor of core.nn.sync_bn.{}.""\n                      .format(\'BatchNorm1d\', SyncBatchNorm.__name__), DeprecationWarning)\n        super(BatchNorm1d, self).__init__(*args, **kwargs)\n\n\nclass BatchNorm2d(SyncBatchNorm):\n    """"""BatchNorm1d is deprecated in favor of :class:`core.nn.sync_bn.SyncBatchNorm`.""""""\n\n    def __init__(self, *args, **kwargs):\n        warnings.warn(""core.nn.sync_bn.{} is now deprecated in favor of core.nn.sync_bn.{}.""\n                      .format(\'BatchNorm2d\', SyncBatchNorm.__name__), DeprecationWarning)\n        super(BatchNorm2d, self).__init__(*args, **kwargs)\n\n\nclass BatchNorm3d(SyncBatchNorm):\n    """"""BatchNorm1d is deprecated in favor of :class:`core.nn.sync_bn.SyncBatchNorm`.""""""\n\n    def __init__(self, *args, **kwargs):\n        warnings.warn(""core.nn.sync_bn.{} is now deprecated in favor of core.nn.sync_bn.{}.""\n                      .format(\'BatchNorm3d\', SyncBatchNorm.__name__), DeprecationWarning)\n        super(BatchNorm3d, self).__init__(*args, **kwargs)\n'"
core/nn/sync_bn/lib/__init__.py,2,"b'import os\nimport torch\nfrom torch.utils.cpp_extension import load\n\ncwd = os.path.dirname(os.path.realpath(__file__))\ncpu_path = os.path.join(cwd, \'cpu\')\ngpu_path = os.path.join(cwd, \'gpu\')\n\ncpu = load(\'sync_cpu\', [\n    os.path.join(cpu_path, \'operator.cpp\'),\n    os.path.join(cpu_path, \'syncbn_cpu.cpp\'),\n], build_directory=cpu_path, verbose=False)\n\nif torch.cuda.is_available():\n    gpu = load(\'sync_gpu\', [\n        os.path.join(gpu_path, \'operator.cpp\'),\n        os.path.join(gpu_path, \'activation_kernel.cu\'),\n        os.path.join(gpu_path, \'syncbn_kernel.cu\'),\n    ], extra_cuda_cflags=[""--expt-extended-lambda""],\n               build_directory=gpu_path, verbose=False)\n'"
core/nn/sync_bn/lib/cpu/setup.py,1,"b""from setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CppExtension\n\nsetup(\n    name='syncbn_cpu',\n    ext_modules=[\n        CppExtension('syncbn_cpu', [\n            'operator.cpp',\n            'syncbn_cpu.cpp',\n        ]),\n    ],\n    cmdclass={\n        'build_ext': BuildExtension\n    })\n"""
core/nn/sync_bn/lib/gpu/__init__.py,0,b''
core/nn/sync_bn/lib/gpu/setup.py,1,"b""from setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\nsetup(\n    name='syncbn_gpu',\n    ext_modules=[\n        CUDAExtension('sync_gpu', [\n            'operator.cpp',\n            'activation_kernel.cu',\n            'syncbn_kernel.cu',\n            ]),\n    ],\n    cmdclass={\n        'build_ext': BuildExtension\n    })"""
