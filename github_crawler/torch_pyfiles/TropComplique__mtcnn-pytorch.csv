file_path,api_count,code
extract_weights_from_caffe_models.py,0,"b'import caffe\nimport numpy as np\n\n""""""\nThe purpose of this script is to convert pretrained weights taken from\nofficial implementation here:\nhttps://github.com/kpzhang93/MTCNN_face_detection_alignment/tree/master/code/codes/MTCNNv2\nto required format.\n\nIn a nutshell, it just renames and transposes some of the weights.\nYou don\'t have to use this script because weights are already in `src/weights`.\n""""""\n\n\ndef get_all_weights(net):\n    all_weights = {}\n    for p in net.params:\n        if \'conv\' in p:\n            name = \'features.\' + p\n            if \'-\' in p:\n                s = list(p)\n                s[-2] = \'_\'\n                s = \'\'.join(s)\n                all_weights[s + \'.weight\'] = net.params[p][0].data\n                all_weights[s + \'.bias\'] = net.params[p][1].data\n            elif len(net.params[p][0].data.shape) == 4:\n                all_weights[name + \'.weight\'] = net.params[p][0].data.transpose((0, 1, 3, 2))\n                all_weights[name + \'.bias\'] = net.params[p][1].data\n            else:\n                all_weights[name + \'.weight\'] = net.params[p][0].data\n                all_weights[name + \'.bias\'] = net.params[p][1].data\n        elif \'prelu\' in p.lower():\n            all_weights[\'features.\' + p.lower() + \'.weight\'] = net.params[p][0].data\n    return all_weights\n\n\n# P-Net\nnet = caffe.Net(\'caffe_models/det1.prototxt\', \'caffe_models/det1.caffemodel\', caffe.TEST)\nnp.save(\'src/weights/pnet.npy\', get_all_weights(net))\n\n# R-Net\nnet = caffe.Net(\'caffe_models/det2.prototxt\', \'caffe_models/det2.caffemodel\', caffe.TEST)\nnp.save(\'src/weights/rnet.npy\', get_all_weights(net))\n\n# O-Net\nnet = caffe.Net(\'caffe_models/det3.prototxt\', \'caffe_models/det3.caffemodel\', caffe.TEST)\nnp.save(\'src/weights/onet.npy\', get_all_weights(net))\n'"
src/__init__.py,0,b'from .visualization_utils import show_bboxes\nfrom .detector import detect_faces\n'
src/box_utils.py,0,"b'import numpy as np\nfrom PIL import Image\n\n\ndef nms(boxes, overlap_threshold=0.5, mode=\'union\'):\n    """"""Non-maximum suppression.\n\n    Arguments:\n        boxes: a float numpy array of shape [n, 5],\n            where each row is (xmin, ymin, xmax, ymax, score).\n        overlap_threshold: a float number.\n        mode: \'union\' or \'min\'.\n\n    Returns:\n        list with indices of the selected boxes\n    """"""\n\n    # if there are no boxes, return the empty list\n    if len(boxes) == 0:\n        return []\n\n    # list of picked indices\n    pick = []\n\n    # grab the coordinates of the bounding boxes\n    x1, y1, x2, y2, score = [boxes[:, i] for i in range(5)]\n\n    area = (x2 - x1 + 1.0)*(y2 - y1 + 1.0)\n    ids = np.argsort(score)  # in increasing order\n\n    while len(ids) > 0:\n\n        # grab index of the largest value\n        last = len(ids) - 1\n        i = ids[last]\n        pick.append(i)\n\n        # compute intersections\n        # of the box with the largest score\n        # with the rest of boxes\n\n        # left top corner of intersection boxes\n        ix1 = np.maximum(x1[i], x1[ids[:last]])\n        iy1 = np.maximum(y1[i], y1[ids[:last]])\n\n        # right bottom corner of intersection boxes\n        ix2 = np.minimum(x2[i], x2[ids[:last]])\n        iy2 = np.minimum(y2[i], y2[ids[:last]])\n\n        # width and height of intersection boxes\n        w = np.maximum(0.0, ix2 - ix1 + 1.0)\n        h = np.maximum(0.0, iy2 - iy1 + 1.0)\n\n        # intersections\' areas\n        inter = w * h\n        if mode == \'min\':\n            overlap = inter/np.minimum(area[i], area[ids[:last]])\n        elif mode == \'union\':\n            # intersection over union (IoU)\n            overlap = inter/(area[i] + area[ids[:last]] - inter)\n\n        # delete all boxes where overlap is too big\n        ids = np.delete(\n            ids,\n            np.concatenate([[last], np.where(overlap > overlap_threshold)[0]])\n        )\n\n    return pick\n\n\ndef convert_to_square(bboxes):\n    """"""Convert bounding boxes to a square form.\n\n    Arguments:\n        bboxes: a float numpy array of shape [n, 5].\n\n    Returns:\n        a float numpy array of shape [n, 5],\n            squared bounding boxes.\n    """"""\n\n    square_bboxes = np.zeros_like(bboxes)\n    x1, y1, x2, y2 = [bboxes[:, i] for i in range(4)]\n    h = y2 - y1 + 1.0\n    w = x2 - x1 + 1.0\n    max_side = np.maximum(h, w)\n    square_bboxes[:, 0] = x1 + w*0.5 - max_side*0.5\n    square_bboxes[:, 1] = y1 + h*0.5 - max_side*0.5\n    square_bboxes[:, 2] = square_bboxes[:, 0] + max_side - 1.0\n    square_bboxes[:, 3] = square_bboxes[:, 1] + max_side - 1.0\n    return square_bboxes\n\n\ndef calibrate_box(bboxes, offsets):\n    """"""Transform bounding boxes to be more like true bounding boxes.\n    \'offsets\' is one of the outputs of the nets.\n\n    Arguments:\n        bboxes: a float numpy array of shape [n, 5].\n        offsets: a float numpy array of shape [n, 4].\n\n    Returns:\n        a float numpy array of shape [n, 5].\n    """"""\n    x1, y1, x2, y2 = [bboxes[:, i] for i in range(4)]\n    w = x2 - x1 + 1.0\n    h = y2 - y1 + 1.0\n    w = np.expand_dims(w, 1)\n    h = np.expand_dims(h, 1)\n\n    # this is what happening here:\n    # tx1, ty1, tx2, ty2 = [offsets[:, i] for i in range(4)]\n    # x1_true = x1 + tx1*w\n    # y1_true = y1 + ty1*h\n    # x2_true = x2 + tx2*w\n    # y2_true = y2 + ty2*h\n    # below is just more compact form of this\n\n    # are offsets always such that\n    # x1 < x2 and y1 < y2 ?\n\n    translation = np.hstack([w, h, w, h])*offsets\n    bboxes[:, 0:4] = bboxes[:, 0:4] + translation\n    return bboxes\n\n\ndef get_image_boxes(bounding_boxes, img, size=24):\n    """"""Cut out boxes from the image.\n\n    Arguments:\n        bounding_boxes: a float numpy array of shape [n, 5].\n        img: an instance of PIL.Image.\n        size: an integer, size of cutouts.\n\n    Returns:\n        a float numpy array of shape [n, 3, size, size].\n    """"""\n\n    num_boxes = len(bounding_boxes)\n    width, height = img.size\n\n    [dy, edy, dx, edx, y, ey, x, ex, w, h] = correct_bboxes(bounding_boxes, width, height)\n    img_boxes = np.zeros((num_boxes, 3, size, size), \'float32\')\n\n    for i in range(num_boxes):\n        img_box = np.zeros((h[i], w[i], 3), \'uint8\')\n\n        img_array = np.asarray(img, \'uint8\')\n        img_box[dy[i]:(edy[i] + 1), dx[i]:(edx[i] + 1), :] =\\\n            img_array[y[i]:(ey[i] + 1), x[i]:(ex[i] + 1), :]\n\n        # resize\n        img_box = Image.fromarray(img_box)\n        img_box = img_box.resize((size, size), Image.BILINEAR)\n        img_box = np.asarray(img_box, \'float32\')\n\n        img_boxes[i, :, :, :] = _preprocess(img_box)\n\n    return img_boxes\n\n\ndef correct_bboxes(bboxes, width, height):\n    """"""Crop boxes that are too big and get coordinates\n    with respect to cutouts.\n\n    Arguments:\n        bboxes: a float numpy array of shape [n, 5],\n            where each row is (xmin, ymin, xmax, ymax, score).\n        width: a float number.\n        height: a float number.\n\n    Returns:\n        dy, dx, edy, edx: a int numpy arrays of shape [n],\n            coordinates of the boxes with respect to the cutouts.\n        y, x, ey, ex: a int numpy arrays of shape [n],\n            corrected ymin, xmin, ymax, xmax.\n        h, w: a int numpy arrays of shape [n],\n            just heights and widths of boxes.\n\n        in the following order:\n            [dy, edy, dx, edx, y, ey, x, ex, w, h].\n    """"""\n\n    x1, y1, x2, y2 = [bboxes[:, i] for i in range(4)]\n    w, h = x2 - x1 + 1.0,  y2 - y1 + 1.0\n    num_boxes = bboxes.shape[0]\n\n    # \'e\' stands for end\n    # (x, y) -> (ex, ey)\n    x, y, ex, ey = x1, y1, x2, y2\n\n    # we need to cut out a box from the image.\n    # (x, y, ex, ey) are corrected coordinates of the box\n    # in the image.\n    # (dx, dy, edx, edy) are coordinates of the box in the cutout\n    # from the image.\n    dx, dy = np.zeros((num_boxes,)), np.zeros((num_boxes,))\n    edx, edy = w.copy() - 1.0, h.copy() - 1.0\n\n    # if box\'s bottom right corner is too far right\n    ind = np.where(ex > width - 1.0)[0]\n    edx[ind] = w[ind] + width - 2.0 - ex[ind]\n    ex[ind] = width - 1.0\n\n    # if box\'s bottom right corner is too low\n    ind = np.where(ey > height - 1.0)[0]\n    edy[ind] = h[ind] + height - 2.0 - ey[ind]\n    ey[ind] = height - 1.0\n\n    # if box\'s top left corner is too far left\n    ind = np.where(x < 0.0)[0]\n    dx[ind] = 0.0 - x[ind]\n    x[ind] = 0.0\n\n    # if box\'s top left corner is too high\n    ind = np.where(y < 0.0)[0]\n    dy[ind] = 0.0 - y[ind]\n    y[ind] = 0.0\n\n    return_list = [dy, edy, dx, edx, y, ey, x, ex, w, h]\n    return_list = [i.astype(\'int32\') for i in return_list]\n\n    return return_list\n\n\ndef _preprocess(img):\n    """"""Preprocessing step before feeding the network.\n\n    Arguments:\n        img: a float numpy array of shape [h, w, c].\n\n    Returns:\n        a float numpy array of shape [1, c, h, w].\n    """"""\n    img = img.transpose((2, 0, 1))\n    img = np.expand_dims(img, 0)\n    img = (img - 127.5)*0.0078125\n    return img\n'"
src/detector.py,3,"b'import numpy as np\nimport torch\nfrom torch.autograd import Variable\nfrom .get_nets import PNet, RNet, ONet\nfrom .box_utils import nms, calibrate_box, get_image_boxes, convert_to_square\nfrom .first_stage import run_first_stage\n\n\ndef detect_faces(image, min_face_size=20.0,\n                 thresholds=[0.6, 0.7, 0.8],\n                 nms_thresholds=[0.7, 0.7, 0.7]):\n    """"""\n    Arguments:\n        image: an instance of PIL.Image.\n        min_face_size: a float number.\n        thresholds: a list of length 3.\n        nms_thresholds: a list of length 3.\n\n    Returns:\n        two float numpy arrays of shapes [n_boxes, 4] and [n_boxes, 10],\n        bounding boxes and facial landmarks.\n    """"""\n\n    # LOAD MODELS\n    pnet = PNet()\n    rnet = RNet()\n    onet = ONet()\n    onet.eval()\n\n    # BUILD AN IMAGE PYRAMID\n    width, height = image.size\n    min_length = min(height, width)\n\n    min_detection_size = 12\n    factor = 0.707  # sqrt(0.5)\n\n    # scales for scaling the image\n    scales = []\n\n    # scales the image so that\n    # minimum size that we can detect equals to\n    # minimum face size that we want to detect\n    m = min_detection_size/min_face_size\n    min_length *= m\n\n    factor_count = 0\n    while min_length > min_detection_size:\n        scales.append(m*factor**factor_count)\n        min_length *= factor\n        factor_count += 1\n\n    # STAGE 1\n\n    # it will be returned\n    bounding_boxes = []\n\n    # run P-Net on different scales\n    for s in scales:\n        boxes = run_first_stage(image, pnet, scale=s, threshold=thresholds[0])\n        bounding_boxes.append(boxes)\n\n    # collect boxes (and offsets, and scores) from different scales\n    bounding_boxes = [i for i in bounding_boxes if i is not None]\n    bounding_boxes = np.vstack(bounding_boxes)\n\n    keep = nms(bounding_boxes[:, 0:5], nms_thresholds[0])\n    bounding_boxes = bounding_boxes[keep]\n\n    # use offsets predicted by pnet to transform bounding boxes\n    bounding_boxes = calibrate_box(bounding_boxes[:, 0:5], bounding_boxes[:, 5:])\n    # shape [n_boxes, 5]\n\n    bounding_boxes = convert_to_square(bounding_boxes)\n    bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n\n    # STAGE 2\n\n    img_boxes = get_image_boxes(bounding_boxes, image, size=24)\n    img_boxes = Variable(torch.FloatTensor(img_boxes), volatile=True)\n    output = rnet(img_boxes)\n    offsets = output[0].data.numpy()  # shape [n_boxes, 4]\n    probs = output[1].data.numpy()  # shape [n_boxes, 2]\n\n    keep = np.where(probs[:, 1] > thresholds[1])[0]\n    bounding_boxes = bounding_boxes[keep]\n    bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))\n    offsets = offsets[keep]\n\n    keep = nms(bounding_boxes, nms_thresholds[1])\n    bounding_boxes = bounding_boxes[keep]\n    bounding_boxes = calibrate_box(bounding_boxes, offsets[keep])\n    bounding_boxes = convert_to_square(bounding_boxes)\n    bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n\n    # STAGE 3\n\n    img_boxes = get_image_boxes(bounding_boxes, image, size=48)\n    if len(img_boxes) == 0: \n        return [], []\n    img_boxes = Variable(torch.FloatTensor(img_boxes), volatile=True)\n    output = onet(img_boxes)\n    landmarks = output[0].data.numpy()  # shape [n_boxes, 10]\n    offsets = output[1].data.numpy()  # shape [n_boxes, 4]\n    probs = output[2].data.numpy()  # shape [n_boxes, 2]\n\n    keep = np.where(probs[:, 1] > thresholds[2])[0]\n    bounding_boxes = bounding_boxes[keep]\n    bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))\n    offsets = offsets[keep]\n    landmarks = landmarks[keep]\n\n    # compute landmark points\n    width = bounding_boxes[:, 2] - bounding_boxes[:, 0] + 1.0\n    height = bounding_boxes[:, 3] - bounding_boxes[:, 1] + 1.0\n    xmin, ymin = bounding_boxes[:, 0], bounding_boxes[:, 1]\n    landmarks[:, 0:5] = np.expand_dims(xmin, 1) + np.expand_dims(width, 1)*landmarks[:, 0:5]\n    landmarks[:, 5:10] = np.expand_dims(ymin, 1) + np.expand_dims(height, 1)*landmarks[:, 5:10]\n\n    bounding_boxes = calibrate_box(bounding_boxes, offsets)\n    keep = nms(bounding_boxes, nms_thresholds[2], mode=\'min\')\n    bounding_boxes = bounding_boxes[keep]\n    landmarks = landmarks[keep]\n\n    return bounding_boxes, landmarks\n'"
src/first_stage.py,2,"b'import torch\nfrom torch.autograd import Variable\nimport math\nfrom PIL import Image\nimport numpy as np\nfrom .box_utils import nms, _preprocess\n\n\ndef run_first_stage(image, net, scale, threshold):\n    """"""Run P-Net, generate bounding boxes, and do NMS.\n\n    Arguments:\n        image: an instance of PIL.Image.\n        net: an instance of pytorch\'s nn.Module, P-Net.\n        scale: a float number,\n            scale width and height of the image by this number.\n        threshold: a float number,\n            threshold on the probability of a face when generating\n            bounding boxes from predictions of the net.\n\n    Returns:\n        a float numpy array of shape [n_boxes, 9],\n            bounding boxes with scores and offsets (4 + 1 + 4).\n    """"""\n\n    # scale the image and convert it to a float array\n    width, height = image.size\n    sw, sh = math.ceil(width*scale), math.ceil(height*scale)\n    img = image.resize((sw, sh), Image.BILINEAR)\n    img = np.asarray(img, \'float32\')\n\n    img = Variable(torch.FloatTensor(_preprocess(img)), volatile=True)\n    output = net(img)\n    probs = output[1].data.numpy()[0, 1, :, :]\n    offsets = output[0].data.numpy()\n    # probs: probability of a face at each sliding window\n    # offsets: transformations to true bounding boxes\n\n    boxes = _generate_bboxes(probs, offsets, scale, threshold)\n    if len(boxes) == 0:\n        return None\n\n    keep = nms(boxes[:, 0:5], overlap_threshold=0.5)\n    return boxes[keep]\n\n\ndef _generate_bboxes(probs, offsets, scale, threshold):\n    """"""Generate bounding boxes at places\n    where there is probably a face.\n\n    Arguments:\n        probs: a float numpy array of shape [n, m].\n        offsets: a float numpy array of shape [1, 4, n, m].\n        scale: a float number,\n            width and height of the image were scaled by this number.\n        threshold: a float number.\n\n    Returns:\n        a float numpy array of shape [n_boxes, 9]\n    """"""\n\n    # applying P-Net is equivalent, in some sense, to\n    # moving 12x12 window with stride 2\n    stride = 2\n    cell_size = 12\n\n    # indices of boxes where there is probably a face\n    inds = np.where(probs > threshold)\n\n    if inds[0].size == 0:\n        return np.array([])\n\n    # transformations of bounding boxes\n    tx1, ty1, tx2, ty2 = [offsets[0, i, inds[0], inds[1]] for i in range(4)]\n    # they are defined as:\n    # w = x2 - x1 + 1\n    # h = y2 - y1 + 1\n    # x1_true = x1 + tx1*w\n    # x2_true = x2 + tx2*w\n    # y1_true = y1 + ty1*h\n    # y2_true = y2 + ty2*h\n\n    offsets = np.array([tx1, ty1, tx2, ty2])\n    score = probs[inds[0], inds[1]]\n\n    # P-Net is applied to scaled images\n    # so we need to rescale bounding boxes back\n    bounding_boxes = np.vstack([\n        np.round((stride*inds[1] + 1.0)/scale),\n        np.round((stride*inds[0] + 1.0)/scale),\n        np.round((stride*inds[1] + 1.0 + cell_size)/scale),\n        np.round((stride*inds[0] + 1.0 + cell_size)/scale),\n        score, offsets\n    ])\n    # why one is added?\n\n    return bounding_boxes.T\n'"
src/get_nets.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict\nimport numpy as np\n\n\nclass Flatten(nn.Module):\n\n    def __init__(self):\n        super(Flatten, self).__init__()\n\n    def forward(self, x):\n        """"""\n        Arguments:\n            x: a float tensor with shape [batch_size, c, h, w].\n        Returns:\n            a float tensor with shape [batch_size, c*h*w].\n        """"""\n\n        # without this pretrained model isn\'t working\n        x = x.transpose(3, 2).contiguous()\n\n        return x.view(x.size(0), -1)\n\n\nclass PNet(nn.Module):\n\n    def __init__(self):\n\n        super(PNet, self).__init__()\n\n        # suppose we have input with size HxW, then\n        # after first layer: H - 2,\n        # after pool: ceil((H - 2)/2),\n        # after second conv: ceil((H - 2)/2) - 2,\n        # after last conv: ceil((H - 2)/2) - 4,\n        # and the same for W\n\n        self.features = nn.Sequential(OrderedDict([\n            (\'conv1\', nn.Conv2d(3, 10, 3, 1)),\n            (\'prelu1\', nn.PReLU(10)),\n            (\'pool1\', nn.MaxPool2d(2, 2, ceil_mode=True)),\n\n            (\'conv2\', nn.Conv2d(10, 16, 3, 1)),\n            (\'prelu2\', nn.PReLU(16)),\n\n            (\'conv3\', nn.Conv2d(16, 32, 3, 1)),\n            (\'prelu3\', nn.PReLU(32))\n        ]))\n\n        self.conv4_1 = nn.Conv2d(32, 2, 1, 1)\n        self.conv4_2 = nn.Conv2d(32, 4, 1, 1)\n\n        weights = np.load(\'src/weights/pnet.npy\')[()]\n        for n, p in self.named_parameters():\n            p.data = torch.FloatTensor(weights[n])\n\n    def forward(self, x):\n        """"""\n        Arguments:\n            x: a float tensor with shape [batch_size, 3, h, w].\n        Returns:\n            b: a float tensor with shape [batch_size, 4, h\', w\'].\n            a: a float tensor with shape [batch_size, 2, h\', w\'].\n        """"""\n        x = self.features(x)\n        a = self.conv4_1(x)\n        b = self.conv4_2(x)\n        a = F.softmax(a)\n        return b, a\n\n\nclass RNet(nn.Module):\n\n    def __init__(self):\n\n        super(RNet, self).__init__()\n\n        self.features = nn.Sequential(OrderedDict([\n            (\'conv1\', nn.Conv2d(3, 28, 3, 1)),\n            (\'prelu1\', nn.PReLU(28)),\n            (\'pool1\', nn.MaxPool2d(3, 2, ceil_mode=True)),\n\n            (\'conv2\', nn.Conv2d(28, 48, 3, 1)),\n            (\'prelu2\', nn.PReLU(48)),\n            (\'pool2\', nn.MaxPool2d(3, 2, ceil_mode=True)),\n\n            (\'conv3\', nn.Conv2d(48, 64, 2, 1)),\n            (\'prelu3\', nn.PReLU(64)),\n\n            (\'flatten\', Flatten()),\n            (\'conv4\', nn.Linear(576, 128)),\n            (\'prelu4\', nn.PReLU(128))\n        ]))\n\n        self.conv5_1 = nn.Linear(128, 2)\n        self.conv5_2 = nn.Linear(128, 4)\n\n        weights = np.load(\'src/weights/rnet.npy\')[()]\n        for n, p in self.named_parameters():\n            p.data = torch.FloatTensor(weights[n])\n\n    def forward(self, x):\n        """"""\n        Arguments:\n            x: a float tensor with shape [batch_size, 3, h, w].\n        Returns:\n            b: a float tensor with shape [batch_size, 4].\n            a: a float tensor with shape [batch_size, 2].\n        """"""\n        x = self.features(x)\n        a = self.conv5_1(x)\n        b = self.conv5_2(x)\n        a = F.softmax(a)\n        return b, a\n\n\nclass ONet(nn.Module):\n\n    def __init__(self):\n\n        super(ONet, self).__init__()\n\n        self.features = nn.Sequential(OrderedDict([\n            (\'conv1\', nn.Conv2d(3, 32, 3, 1)),\n            (\'prelu1\', nn.PReLU(32)),\n            (\'pool1\', nn.MaxPool2d(3, 2, ceil_mode=True)),\n\n            (\'conv2\', nn.Conv2d(32, 64, 3, 1)),\n            (\'prelu2\', nn.PReLU(64)),\n            (\'pool2\', nn.MaxPool2d(3, 2, ceil_mode=True)),\n\n            (\'conv3\', nn.Conv2d(64, 64, 3, 1)),\n            (\'prelu3\', nn.PReLU(64)),\n            (\'pool3\', nn.MaxPool2d(2, 2, ceil_mode=True)),\n\n            (\'conv4\', nn.Conv2d(64, 128, 2, 1)),\n            (\'prelu4\', nn.PReLU(128)),\n\n            (\'flatten\', Flatten()),\n            (\'conv5\', nn.Linear(1152, 256)),\n            (\'drop5\', nn.Dropout(0.25)),\n            (\'prelu5\', nn.PReLU(256)),\n        ]))\n\n        self.conv6_1 = nn.Linear(256, 2)\n        self.conv6_2 = nn.Linear(256, 4)\n        self.conv6_3 = nn.Linear(256, 10)\n\n        weights = np.load(\'src/weights/onet.npy\')[()]\n        for n, p in self.named_parameters():\n            p.data = torch.FloatTensor(weights[n])\n\n    def forward(self, x):\n        """"""\n        Arguments:\n            x: a float tensor with shape [batch_size, 3, h, w].\n        Returns:\n            c: a float tensor with shape [batch_size, 10].\n            b: a float tensor with shape [batch_size, 4].\n            a: a float tensor with shape [batch_size, 2].\n        """"""\n        x = self.features(x)\n        a = self.conv6_1(x)\n        b = self.conv6_2(x)\n        c = self.conv6_3(x)\n        a = F.softmax(a)\n        return c, b, a\n'"
src/visualization_utils.py,0,"b'from PIL import ImageDraw\n\n\ndef show_bboxes(img, bounding_boxes, facial_landmarks=[]):\n    """"""Draw bounding boxes and facial landmarks.\n\n    Arguments:\n        img: an instance of PIL.Image.\n        bounding_boxes: a float numpy array of shape [n, 5].\n        facial_landmarks: a float numpy array of shape [n, 10].\n\n    Returns:\n        an instance of PIL.Image.\n    """"""\n\n    img_copy = img.copy()\n    draw = ImageDraw.Draw(img_copy)\n\n    for b in bounding_boxes:\n        draw.rectangle([\n            (b[0], b[1]), (b[2], b[3])\n        ], outline=\'white\')\n\n    for p in facial_landmarks:\n        for i in range(5):\n            draw.ellipse([\n                (p[i] - 1.0, p[i + 5] - 1.0),\n                (p[i] + 1.0, p[i + 5] + 1.0)\n            ], outline=\'blue\')\n\n    return img_copy\n'"
