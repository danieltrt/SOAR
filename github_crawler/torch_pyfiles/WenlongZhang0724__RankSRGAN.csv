file_path,api_count,code
codes/test.py,0,"b""import os.path as osp\nimport logging\nimport time\nimport argparse\nfrom collections import OrderedDict\n\nimport options.options as option\nimport utils.util as util\nfrom data.util import bgr2ycbcr\nfrom data import create_dataset, create_dataloader\nfrom models import create_model\n\n#### options\nparser = argparse.ArgumentParser()\nparser.add_argument('-opt', type=str, required=True, help='Path to options YMAL file.')\nopt = option.parse(parser.parse_args().opt, is_train=False)\nopt = option.dict_to_nonedict(opt)\n\nutil.mkdirs(\n    (path for key, path in opt['path'].items()\n     if not key == 'experiments_root' and 'pretrain_model' not in key and 'resume' not in key))\nutil.setup_logger('base', opt['path']['log'], 'test_' + opt['name'], level=logging.INFO,\n                  screen=True, tofile=True)\nlogger = logging.getLogger('base')\nlogger.info(option.dict2str(opt))\n\n#### Create test dataset and dataloader\ntest_loaders = []\nfor phase, dataset_opt in sorted(opt['datasets'].items()):\n    test_set = create_dataset(dataset_opt)\n    test_loader = create_dataloader(test_set, dataset_opt)\n    logger.info('Number of test images in [{:s}]: {:d}'.format(dataset_opt['name'], len(test_set)))\n    test_loaders.append(test_loader)\n\nmodel = create_model(opt)\nfor test_loader in test_loaders:\n    test_set_name = test_loader.dataset.opt['name']\n    logger.info('\\nTesting [{:s}]...'.format(test_set_name))\n    test_start_time = time.time()\n    dataset_dir = osp.join(opt['path']['results_root'], test_set_name)\n    util.mkdir(dataset_dir)\n\n    test_results = OrderedDict()\n    test_results['psnr'] = []\n    test_results['ssim'] = []\n    test_results['psnr_y'] = []\n    test_results['ssim_y'] = []\n\n    for data in test_loader:\n        need_GT = False if test_loader.dataset.opt['dataroot_GT'] is None else True\n        model.feed_data(data, need_GT=need_GT)\n        img_path = data['GT_path'][0] if need_GT else data['LQ_path'][0]\n        img_name = osp.splitext(osp.basename(img_path))[0]\n\n        model.test()\n        visuals = model.get_current_visuals(need_GT=need_GT)\n\n        sr_img = util.tensor2img(visuals['rlt'])  # uint8\n\n        # save images\n        suffix = opt['suffix']\n        if suffix:\n            save_img_path = osp.join(dataset_dir, img_name + suffix + '.png')\n        else:\n            save_img_path = osp.join(dataset_dir, img_name + '.png')\n        util.save_img(sr_img, save_img_path)\n\n        # calculate PSNR and SSIM\n        if need_GT:\n            gt_img = util.tensor2img(visuals['GT'])\n            sr_img, gt_img = util.crop_border([sr_img, gt_img], opt['scale'])\n            psnr = util.calculate_psnr(sr_img, gt_img)\n            ssim = util.calculate_ssim(sr_img, gt_img)\n            test_results['psnr'].append(psnr)\n            test_results['ssim'].append(ssim)\n\n            if gt_img.shape[2] == 3:  # RGB image\n                sr_img_y = bgr2ycbcr(sr_img / 255., only_y=True)\n                gt_img_y = bgr2ycbcr(gt_img / 255., only_y=True)\n\n                psnr_y = util.calculate_psnr(sr_img_y * 255, gt_img_y * 255)\n                ssim_y = util.calculate_ssim(sr_img_y * 255, gt_img_y * 255)\n                test_results['psnr_y'].append(psnr_y)\n                test_results['ssim_y'].append(ssim_y)\n                logger.info(\n                    '{:20s} - PSNR: {:.6f} dB; SSIM: {:.6f}; PSNR_Y: {:.6f} dB; SSIM_Y: {:.6f}.'.\n                    format(img_name, psnr, ssim, psnr_y, ssim_y))\n            else:\n                logger.info('{:20s} - PSNR: {:.6f} dB; SSIM: {:.6f}.'.format(img_name, psnr, ssim))\n        else:\n            logger.info(img_name)\n\n    if need_GT:  # metrics\n        # Average PSNR/SSIM results\n        ave_psnr = sum(test_results['psnr']) / len(test_results['psnr'])\n        ave_ssim = sum(test_results['ssim']) / len(test_results['ssim'])\n        logger.info(\n            '----Average PSNR/SSIM results for {}----\\n\\tPSNR: {:.6f} dB; SSIM: {:.6f}\\n'.format(\n                test_set_name, ave_psnr, ave_ssim))\n        if test_results['psnr_y'] and test_results['ssim_y']:\n            ave_psnr_y = sum(test_results['psnr_y']) / len(test_results['psnr_y'])\n            ave_ssim_y = sum(test_results['ssim_y']) / len(test_results['ssim_y'])\n            logger.info(\n                '----Y channel, average PSNR/SSIM----\\n\\tPSNR_Y: {:.6f} dB; SSIM_Y: {:.6f}\\n'.\n                format(ave_psnr_y, ave_ssim_y))\n"""
codes/train.py,12,"b'import os\nimport math\nimport argparse\nimport random\nimport logging\n\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom data.data_sampler import DistIterSampler\n\nimport options.options as option\nfrom utils import util\nfrom data import create_dataloader, create_dataset\nfrom models import create_model\n\n\ndef init_dist(backend=\'nccl\', **kwargs):\n    """"""initialization for distributed training""""""\n    if mp.get_start_method(allow_none=True) != \'spawn\':\n        mp.set_start_method(\'spawn\')\n    rank = int(os.environ[\'RANK\'])\n    num_gpus = torch.cuda.device_count()\n    torch.cuda.set_device(rank % num_gpus)\n    dist.init_process_group(backend=backend, **kwargs)\n\n\ndef main():\n    #### options\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-opt\', type=str, help=\'Path to option YAML file.\')\n    parser.add_argument(\'--launcher\', choices=[\'none\', \'pytorch\'], default=\'none\',\n                        help=\'job launcher\')\n    parser.add_argument(\'--local_rank\', type=int, default=0)\n    args = parser.parse_args()\n    opt = option.parse(args.opt, is_train=True)\n\n    #### distributed training settings\n    if args.launcher == \'none\':  # disabled distributed training\n        opt[\'dist\'] = False\n        rank = -1\n        print(\'Disabled distributed training.\')\n    else:\n        opt[\'dist\'] = True\n        init_dist()\n        world_size = torch.distributed.get_world_size()\n        rank = torch.distributed.get_rank()\n\n    #### loading resume state if exists\n    if opt[\'path\'].get(\'resume_state\', None):\n        # distributed resuming: all load into default GPU\n        device_id = torch.cuda.current_device()\n        resume_state = torch.load(opt[\'path\'][\'resume_state\'],\n                                  map_location=lambda storage, loc: storage.cuda(device_id))\n        option.check_resume(opt, resume_state[\'iter\'])  # check resume options\n    else:\n        resume_state = None\n\n    #### mkdir and loggers\n    if rank <= 0:  # normal training (rank -1) OR distributed training (rank 0)\n        if resume_state is None:\n            util.mkdir_and_rename(\n                opt[\'path\'][\'experiments_root\'])  # rename experiment folder if exists\n            util.mkdirs((path for key, path in opt[\'path\'].items() if not key == \'experiments_root\'\n                         and \'pretrain_model\' not in key and \'resume\' not in key))\n\n        # config loggers. Before it, the log will not work\n        util.setup_logger(\'base\', opt[\'path\'][\'log\'], \'train_\' + opt[\'name\'], level=logging.INFO,\n                          screen=True, tofile=True)\n        logger = logging.getLogger(\'base\')\n        logger.info(option.dict2str(opt))\n        # tensorboard logger\n        if opt[\'use_tb_logger\'] and \'debug\' not in opt[\'name\']:\n            version = float(torch.__version__[0:3])\n            if version >= 1.1:  # PyTorch 1.1\n                from torch.utils.tensorboard import SummaryWriter\n            else:\n                logger.info(\n                    \'You are using PyTorch {}. Tensorboard will use [tensorboardX]\'.format(version))\n                from tensorboardX import SummaryWriter\n            tb_logger = SummaryWriter(log_dir=\'../tb_logger/\' + opt[\'name\'])\n    else:\n        util.setup_logger(\'base\', opt[\'path\'][\'log\'], \'train\', level=logging.INFO, screen=True)\n        logger = logging.getLogger(\'base\')\n\n    # convert to NoneDict, which returns None for missing keys\n    opt = option.dict_to_nonedict(opt)\n\n    #### random seed\n    seed = opt[\'train\'][\'manual_seed\']\n    if seed is None:\n        seed = random.randint(1, 10000)\n    if rank <= 0:\n        logger.info(\'Random seed: {}\'.format(seed))\n    util.set_random_seed(seed)\n\n    torch.backends.cudnn.benchmark = True\n    # torch.backends.cudnn.deterministic = True\n\n    #### create train and val dataloader\n    dataset_ratio = 200  # enlarge the size of each epoch\n    for phase, dataset_opt in opt[\'datasets\'].items():\n        if phase == \'train\':\n            train_set = create_dataset(dataset_opt)\n            train_size = int(math.ceil(len(train_set) / dataset_opt[\'batch_size\']))\n            total_iters = int(opt[\'train\'][\'niter\'])\n            total_epochs = int(math.ceil(total_iters / train_size))\n            if opt[\'dist\']:\n                train_sampler = DistIterSampler(train_set, world_size, rank, dataset_ratio)\n                total_epochs = int(math.ceil(total_iters / (train_size * dataset_ratio)))\n            else:\n                train_sampler = None\n            train_loader = create_dataloader(train_set, dataset_opt, opt, train_sampler)\n            if rank <= 0:\n                logger.info(\'Number of train images: {:,d}, iters: {:,d}\'.format(\n                    len(train_set), train_size))\n                logger.info(\'Total epochs needed: {:d} for iters {:,d}\'.format(\n                    total_epochs, total_iters))\n        elif phase == \'val\':\n            val_set = create_dataset(dataset_opt)\n            val_loader = create_dataloader(val_set, dataset_opt, opt, None)\n            if rank <= 0:\n                logger.info(\'Number of val images in [{:s}]: {:d}\'.format(\n                    dataset_opt[\'name\'], len(val_set)))\n        else:\n            raise NotImplementedError(\'Phase [{:s}] is not recognized.\'.format(phase))\n    assert train_loader is not None\n\n    #### create model\n    model = create_model(opt)\n\n    #### resume training\n    if resume_state:\n        logger.info(\'Resuming training from epoch: {}, iter: {}.\'.format(\n            resume_state[\'epoch\'], resume_state[\'iter\']))\n\n        start_epoch = resume_state[\'epoch\']\n        current_step = resume_state[\'iter\']\n        model.resume_training(resume_state)  # handle optimizers and schedulers\n    else:\n        current_step = 0\n        start_epoch = 0\n\n    #### training\n    logger.info(\'Start training from epoch: {:d}, iter: {:d}\'.format(start_epoch, current_step))\n    for epoch in range(start_epoch, total_epochs + 1):\n        if opt[\'dist\']:\n            train_sampler.set_epoch(epoch)\n        for _, train_data in enumerate(train_loader):\n            current_step += 1\n            if current_step > total_iters:\n                break\n            #### update learning rate\n            model.update_learning_rate(current_step, warmup_iter=opt[\'train\'][\'warmup_iter\'])\n\n            #### training\n            model.feed_data(train_data)\n            model.optimize_parameters(current_step)\n\n            #### log\n            if current_step % opt[\'logger\'][\'print_freq\'] == 0:\n                logs = model.get_current_log()\n                message = \'[epoch:{:3d}, iter:{:8,d}, lr:(\'.format(epoch, current_step)\n                for v in model.get_current_learning_rate():\n                    message += \'{:.3e},\'.format(v)\n                message += \')] \'\n                for k, v in logs.items():\n                    message += \'{:s}: {:.4e} \'.format(k, v)\n                    # tensorboard logger\n                    if opt[\'use_tb_logger\'] and \'debug\' not in opt[\'name\']:\n                        if rank <= 0:\n                            tb_logger.add_scalar(k, v, current_step)\n                if rank <= 0:\n                    logger.info(message)\n            #### validation\n            if opt[\'datasets\'].get(\'val\', None) and current_step % opt[\'train\'][\'val_freq\'] == 0:\n                if rank <= 0:  # image restoration validation\n                    # does not support multi-GPU validation\n                    pbar = util.ProgressBar(len(val_loader))\n                    avg_psnr = 0.\n                    idx = 0\n                    for val_data in val_loader:\n                        idx += 1\n                        img_name = os.path.splitext(os.path.basename(val_data[\'LQ_path\'][0]))[0]\n                        img_dir = os.path.join(opt[\'path\'][\'val_images\'], img_name)\n                        util.mkdir(img_dir)\n\n                        model.feed_data(val_data)\n                        model.test()\n\n                        visuals = model.get_current_visuals()\n                        sr_img = util.tensor2img(visuals[\'rlt\'])  # uint8\n                        gt_img = util.tensor2img(visuals[\'GT\'])  # uint8\n\n                        # Save SR images for reference\n                        save_img_path = os.path.join(img_dir,\n                                                     \'{:s}_{:d}.png\'.format(img_name, current_step))\n                        util.save_img(sr_img, save_img_path)\n\n                        # calculate PSNR\n                        sr_img, gt_img = util.crop_border([sr_img, gt_img], opt[\'scale\'])\n                        avg_psnr += util.calculate_psnr(sr_img, gt_img)\n                        pbar.update(\'Test {}\'.format(img_name))\n\n                    avg_psnr = avg_psnr / idx\n\n                    # log\n                    logger.info(\'# Validation # PSNR: {:.4e}\'.format(avg_psnr))\n                    # tensorboard logger\n                    if opt[\'use_tb_logger\'] and \'debug\' not in opt[\'name\']:\n                        tb_logger.add_scalar(\'psnr\', avg_psnr, current_step)\n\n            #### save models and training states\n            if current_step % opt[\'logger\'][\'save_checkpoint_freq\'] == 0:\n                if rank <= 0:\n                    logger.info(\'Saving models and training states.\')\n                    model.save(current_step)\n                    model.save_training_state(epoch, current_step)\n\n    if rank <= 0:\n        logger.info(\'Saving the final model.\')\n        model.save(\'latest\')\n        logger.info(\'End of training.\')\n        tb_logger.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
codes/train_niqe.py,12,"b'import os\nimport math\nimport argparse\nimport random\nimport logging\n\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom data.data_sampler import DistIterSampler\n\nimport options.options as option\nfrom utils import util\nfrom data import create_dataloader, create_dataset\nfrom models import create_model\nimport matlab\nimport matlab.engine\n\ndef init_dist(backend=\'nccl\', **kwargs):\n    """"""initialization for distributed training""""""\n    if mp.get_start_method(allow_none=True) != \'spawn\':\n        mp.set_start_method(\'spawn\')\n    rank = int(os.environ[\'RANK\'])\n    num_gpus = torch.cuda.device_count()\n    torch.cuda.set_device(rank % num_gpus)\n    dist.init_process_group(backend=backend, **kwargs)\n\ndef main():\n    #### options\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-opt\', type=str, help=\'Path to option YAML file.\')\n    parser.add_argument(\'--launcher\', choices=[\'none\', \'pytorch\'], default=\'none\',\n                        help=\'job launcher\')\n    parser.add_argument(\'--local_rank\', type=int, default=0)\n    args = parser.parse_args()\n    opt = option.parse(args.opt, is_train=True)\n    perceptualmetric_path = os.getcwd() + \'/utils/perceptualmetric\'\n\n    #### distributed training settings\n    if args.launcher == \'none\':  # disabled distributed training\n        opt[\'dist\'] = False\n        rank = -1\n        print(\'Disabled distributed training.\')\n    else:\n        opt[\'dist\'] = True\n        init_dist()\n        world_size = torch.distributed.get_world_size()\n        rank = torch.distributed.get_rank()\n\n    #### loading resume state if exists\n    if opt[\'path\'].get(\'resume_state\', None):\n        # distributed resuming: all load into default GPU\n        device_id = torch.cuda.current_device()\n        resume_state = torch.load(opt[\'path\'][\'resume_state\'],\n                                  map_location=lambda storage, loc: storage.cuda(device_id))\n        option.check_resume(opt, resume_state[\'iter\'])  # check resume options\n    else:\n        resume_state = None\n\n    #### mkdir and loggers\n    if rank <= 0:  # normal training (rank -1) OR distributed training (rank 0)\n        if resume_state is None:\n            util.mkdir_and_rename(\n                opt[\'path\'][\'experiments_root\'])  # rename experiment folder if exists\n            util.mkdirs((path for key, path in opt[\'path\'].items() if not key == \'experiments_root\'\n                         and \'pretrain_model\' not in key and \'resume\' not in key))\n\n        # config loggers. Before it, the log will not work\n        util.setup_logger(\'base\', opt[\'path\'][\'log\'], \'train_\' + opt[\'name\'], level=logging.INFO,\n                          screen=True, tofile=True)\n        logger = logging.getLogger(\'base\')\n        logger.info(option.dict2str(opt))\n        # tensorboard logger\n        if opt[\'use_tb_logger\'] and \'debug\' not in opt[\'name\']:\n            version = float(torch.__version__[0:3])\n            if version >= 1.1:  # PyTorch 1.1\n                from torch.utils.tensorboard import SummaryWriter\n            else:\n                logger.info(\n                    \'You are using PyTorch {}. Tensorboard will use [tensorboardX]\'.format(version))\n                from tensorboardX import SummaryWriter\n            tb_logger = SummaryWriter(log_dir=\'../tb_logger/\' + opt[\'name\'])\n    else:\n        util.setup_logger(\'base\', opt[\'path\'][\'log\'], \'train\', level=logging.INFO, screen=True)\n        logger = logging.getLogger(\'base\')\n\n    # convert to NoneDict, which returns None for missing keys\n    opt = option.dict_to_nonedict(opt)\n\n    #### random seed\n    seed = opt[\'train\'][\'manual_seed\']\n    if seed is None:\n        seed = random.randint(1, 10000)\n    if rank <= 0:\n        logger.info(\'Random seed: {}\'.format(seed))\n    util.set_random_seed(seed)\n\n    torch.backends.cudnn.benchmark = True\n    # torch.backends.cudnn.deterministic = True\n\n    #### create train and val dataloader\n    dataset_ratio = 200  # enlarge the size of each epoch\n    for phase, dataset_opt in opt[\'datasets\'].items():\n        if phase == \'train\':\n            train_set = create_dataset(dataset_opt)\n            train_size = int(math.ceil(len(train_set) / dataset_opt[\'batch_size\']))\n            total_iters = int(opt[\'train\'][\'niter\'])\n            total_epochs = int(math.ceil(total_iters / train_size))\n            if opt[\'dist\']:\n                train_sampler = DistIterSampler(train_set, world_size, rank, dataset_ratio)\n                total_epochs = int(math.ceil(total_iters / (train_size * dataset_ratio)))\n            else:\n                train_sampler = None\n            train_loader = create_dataloader(train_set, dataset_opt, opt, train_sampler)\n            if rank <= 0:\n                logger.info(\'Number of train images: {:,d}, iters: {:,d}\'.format(\n                    len(train_set), train_size))\n                logger.info(\'Total epochs needed: {:d} for iters {:,d}\'.format(\n                    total_epochs, total_iters))\n        elif phase == \'val\':\n            val_set = create_dataset(dataset_opt)\n            val_loader = create_dataloader(val_set, dataset_opt, opt, None)\n            if rank <= 0:\n                logger.info(\'Number of val images in [{:s}]: {:d}\'.format(\n                    dataset_opt[\'name\'], len(val_set)))\n        else:\n            raise NotImplementedError(\'Phase [{:s}] is not recognized.\'.format(phase))\n    assert train_loader is not None\n\n    #### create model\n    model = create_model(opt)\n\n    #### resume training\n    if resume_state:\n        logger.info(\'Resuming training from epoch: {}, iter: {}.\'.format(\n            resume_state[\'epoch\'], resume_state[\'iter\']))\n\n        start_epoch = resume_state[\'epoch\']\n        current_step = resume_state[\'iter\']\n        model.resume_training(resume_state)  # handle optimizers and schedulers\n    else:\n        current_step = 0\n        start_epoch = 0\n\n    #### create python-matlab interface\n    eng = matlab.engine.connect_matlab()\n    print(\'matlab process name: \',matlab.engine.find_matlab())\n    eng.addpath(perceptualmetric_path)\n\n    #### training\n    logger.info(\'Start training from epoch: {:d}, iter: {:d}\'.format(start_epoch, current_step))\n    for epoch in range(start_epoch, total_epochs + 1):\n        if opt[\'dist\']:\n            train_sampler.set_epoch(epoch)\n        for _, train_data in enumerate(train_loader):\n            current_step += 1\n            if current_step > total_iters:\n                break\n            #### update learning rate\n            model.update_learning_rate(current_step, warmup_iter=opt[\'train\'][\'warmup_iter\'])\n\n            #### training\n            model.feed_data(train_data)\n            model.optimize_parameters(current_step)\n\n            #### log\n            if current_step % opt[\'logger\'][\'print_freq\'] == 0:\n                logs = model.get_current_log()\n                message = \'[epoch:{:3d}, iter:{:8,d}, lr:(\'.format(epoch, current_step)\n                for v in model.get_current_learning_rate():\n                    message += \'{:.3e},\'.format(v)\n                message += \')] \'\n                for k, v in logs.items():\n                    message += \'{:s}: {:.4e} \'.format(k, v)\n                    # tensorboard logger\n                    if opt[\'use_tb_logger\'] and \'debug\' not in opt[\'name\']:\n                        if rank <= 0:\n                            tb_logger.add_scalar(k, v, current_step)\n                if rank <= 0:\n                    logger.info(message)\n            #### validation\n            if opt[\'datasets\'].get(\'val\', None) and current_step % opt[\'train\'][\'val_freq\'] == 0:\n                if rank <= 0:  # image restoration validation\n                    # does not support multi-GPU validation\n                    pbar = util.ProgressBar(len(val_loader))\n                    avg_psnr = 0.\n                    avg_niqe = 0\n\n                    idx = 0\n                    for val_data in val_loader:\n                        idx += 1\n                        img_name = os.path.splitext(os.path.basename(val_data[\'LQ_path\'][0]))[0]\n                        img_dir = os.path.join(opt[\'path\'][\'val_images\'], img_name)\n                        util.mkdir(img_dir)\n\n                        model.feed_data(val_data)\n                        model.test()\n\n                        visuals = model.get_current_visuals()\n                        sr_img = util.tensor2img(visuals[\'rlt\'])  # uint8\n                        gt_img = util.tensor2img(visuals[\'GT\'])  # uint8\n\n                        # Save SR images for reference\n                        save_img_path = os.path.join(img_dir,\n                                                     \'{:s}_{:d}.png\'.format(img_name, current_step))\n                        util.save_img(sr_img, save_img_path)\n\n                        # calculate PSNR\n                        sr_img, gt_img = util.crop_border([sr_img, gt_img], opt[\'scale\'])\n                        avg_psnr += util.calculate_psnr(sr_img, gt_img)\n                        pbar.update(\'Test {}\'.format(img_name))\n                        # calculate NIQE\n                        avg_niqe +=  eng.calc_NIQE(save_img_path,4)\n\n                    avg_psnr = avg_psnr / idx\n                    avg_niqe = avg_niqe / idx\n\n                    # log\n                    logger.info(\'# Validation # PSNR: {:.4e}\'.format(avg_psnr))\n                    logger.info(\'# Validation # NIQE: {:.4e}\'.format(avg_niqe))\n\n                    # tensorboard logger\n                    if opt[\'use_tb_logger\'] and \'debug\' not in opt[\'name\']:\n                        tb_logger.add_scalar(\'psnr\', avg_psnr, current_step)\n\n                    if opt[\'use_tb_logger\'] and \'debug\' not in opt[\'name\']:\n                        tb_logger.add_scalar(\'niqe\', avg_niqe, current_step)\n\n            #### save models and training states\n            if current_step % opt[\'logger\'][\'save_checkpoint_freq\'] == 0:\n                if rank <= 0:\n                    logger.info(\'Saving models and training states.\')\n                    model.save(current_step)\n                    model.save_training_state(epoch, current_step)\n\n    if rank <= 0:\n        logger.info(\'Saving the final model.\')\n        model.save(\'latest\')\n        logger.info(\'End of training.\')\n        tb_logger.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
codes/train_rank.py,12,"b'import os\nimport math\nimport argparse\nimport random\nimport logging\n\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom data.data_sampler import DistIterSampler\n\nimport options.options as option\nfrom utils import util\nfrom data import create_dataloader, create_dataset\nfrom models import create_model\nfrom utils.rank_test import rank_pair_test\n\n\ndef init_dist(backend=\'nccl\', **kwargs):\n    """"""initialization for distributed training""""""\n    if mp.get_start_method(allow_none=True) != \'spawn\':\n        mp.set_start_method(\'spawn\')\n    rank = int(os.environ[\'RANK\'])\n    num_gpus = torch.cuda.device_count()\n    torch.cuda.set_device(rank % num_gpus)\n    dist.init_process_group(backend=backend, **kwargs)\n\n\ndef main():\n    #### options\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-opt\', type=str, help=\'Path to option YAML file.\')\n    parser.add_argument(\'--launcher\', choices=[\'none\', \'pytorch\'], default=\'none\',\n                        help=\'job launcher\')\n    parser.add_argument(\'--local_rank\', type=int, default=0)\n    args = parser.parse_args()\n    opt = option.parse(args.opt, is_train=True)\n    label_path = opt[\'datasets\'][\'val\'][\'dataroot_label_file\']\n\n    #### distributed training settings\n    if args.launcher == \'none\':  # disabled distributed training\n        opt[\'dist\'] = False\n        rank = -1\n        print(\'Disabled distributed training.\')\n    else:\n        opt[\'dist\'] = True\n        init_dist()\n        world_size = torch.distributed.get_world_size()\n        rank = torch.distributed.get_rank()\n\n    #### loading resume state if exists\n    if opt[\'path\'].get(\'resume_state\', None):\n        # distributed resuming: all load into default GPU\n        device_id = torch.cuda.current_device()\n        resume_state = torch.load(opt[\'path\'][\'resume_state\'],\n                                  map_location=lambda storage, loc: storage.cuda(device_id))\n        option.check_resume(opt, resume_state[\'iter\'])  # check resume options\n    else:\n        resume_state = None\n\n    #### mkdir and loggers\n    if rank <= 0:  # normal training (rank -1) OR distributed training (rank 0)\n        if resume_state is None:\n            util.mkdir_and_rename(\n                opt[\'path\'][\'experiments_root\'])  # rename experiment folder if exists\n            util.mkdirs((path for key, path in opt[\'path\'].items() if not key == \'experiments_root\'\n                         and \'pretrain_model\' not in key and \'resume\' not in key))\n\n        # config loggers. Before it, the log will not work\n        util.setup_logger(\'base\', opt[\'path\'][\'log\'], \'train_\' + opt[\'name\'], level=logging.INFO,\n                          screen=True, tofile=True)\n        logger = logging.getLogger(\'base\')\n        logger.info(option.dict2str(opt))\n        # tensorboard logger\n        if opt[\'use_tb_logger\'] and \'debug\' not in opt[\'name\']:\n            version = float(torch.__version__[0:3])\n            if version >= 1.1:  # PyTorch 1.1\n                from torch.utils.tensorboard import SummaryWriter\n            else:\n                logger.info(\n                    \'You are using PyTorch {}. Tensorboard will use [tensorboardX]\'.format(version))\n                from tensorboardX import SummaryWriter\n            tb_logger = SummaryWriter(log_dir=\'../tb_logger/\' + opt[\'name\'])\n    else:\n        util.setup_logger(\'base\', opt[\'path\'][\'log\'], \'train\', level=logging.INFO, screen=True)\n        logger = logging.getLogger(\'base\')\n\n    # convert to NoneDict, which returns None for missing keys\n    opt = option.dict_to_nonedict(opt)\n\n    #### random seed\n    seed = opt[\'train\'][\'manual_seed\']\n    if seed is None:\n        seed = random.randint(1, 10000)\n    if rank <= 0:\n        logger.info(\'Random seed: {}\'.format(seed))\n    util.set_random_seed(seed)\n\n    torch.backends.cudnn.benchmark = True\n    # torch.backends.cudnn.deterministic = True\n\n    #### create train and val dataloader\n    dataset_ratio = 200  # enlarge the size of each epoch\n    for phase, dataset_opt in opt[\'datasets\'].items():\n        if phase == \'train\':\n            train_set = create_dataset(dataset_opt)\n            train_size = int(math.ceil(len(train_set) / dataset_opt[\'batch_size\']))\n            total_iters = int(opt[\'train\'][\'niter\'])\n            total_epochs = int(math.ceil(total_iters / train_size))\n            if opt[\'dist\']:\n                train_sampler = DistIterSampler(train_set, world_size, rank, dataset_ratio)\n                total_epochs = int(math.ceil(total_iters / (train_size * dataset_ratio)))\n            else:\n                train_sampler = None\n            train_loader = create_dataloader(train_set, dataset_opt, opt, train_sampler)\n            if rank <= 0:\n                logger.info(\'Number of train images: {:,d}, iters: {:,d}\'.format(\n                    len(train_set), train_size))\n                logger.info(\'Total epochs needed: {:d} for iters {:,d}\'.format(\n                    total_epochs, total_iters))\n        elif phase == \'val\':\n            val_set = create_dataset(dataset_opt,is_train = False)\n            val_loader = create_dataloader(val_set, dataset_opt, opt, None)\n            if rank <= 0:\n                logger.info(\'Number of val images in [{:s}]: {:d}\'.format(\n                    dataset_opt[\'name\'], len(val_set)))\n        else:\n            raise NotImplementedError(\'Phase [{:s}] is not recognized.\'.format(phase))\n    assert train_loader is not None\n\n    #### create model\n    model = create_model(opt)\n\n    #### resume training\n    if resume_state:\n        logger.info(\'Resuming training from epoch: {}, iter: {}.\'.format(\n            resume_state[\'epoch\'], resume_state[\'iter\']))\n\n        start_epoch = resume_state[\'epoch\']\n        current_step = resume_state[\'iter\']\n        model.resume_training(resume_state)  # handle optimizers and schedulers\n    else:\n        current_step = 0\n        start_epoch = 0\n\n    #### training\n    logger.info(\'Start training from epoch: {:d}, iter: {:d}\'.format(start_epoch, current_step))\n    for epoch in range(start_epoch, total_epochs + 1):\n        if opt[\'dist\']:\n            train_sampler.set_epoch(epoch)\n        for _, train_data in enumerate(train_loader):\n            current_step += 1\n            if current_step > total_iters:\n                break\n            #### update learning rate\n            model.update_learning_rate(current_step, warmup_iter=opt[\'train\'][\'warmup_iter\'])\n\n            #### training\n            model.feed_data(train_data)\n            model.optimize_parameters(current_step)\n\n            #### log\n            if current_step % opt[\'logger\'][\'print_freq\'] == 0:\n                logs = model.get_current_log()\n                message = \'[epoch:{:3d}, iter:{:8,d}, lr:(\'.format(epoch, current_step)\n                for v in model.get_current_learning_rate():\n                    message += \'{:.3e},\'.format(v)\n                message += \')] \'\n                for k, v in logs.items():\n                    message += \'{:s}: {:.4e} \'.format(k, v)\n                    # tensorboard logger\n                    if opt[\'use_tb_logger\'] and \'debug\' not in opt[\'name\']:\n                        if rank <= 0:\n                            tb_logger.add_scalar(k, v, current_step)\n                if rank <= 0:\n                    logger.info(message)\n            #### validation\n            if opt[\'datasets\'].get(\'val\', None) and current_step % opt[\'train\'][\'val_freq\'] == 0:\n                if rank <= 0:  #\n                    # does not support multi-GPU validation\n                    pbar = util.ProgressBar(len(val_loader))\n                    idx = 0\n                    for val_data in val_loader:\n                        idx += 1\n                        img_name = os.path.splitext(os.path.basename(val_data[\'img1_path\'][0]))[0]\n                        img_dir = os.path.join(opt[\'path\'][\'val_images\'], str(current_step))\n                        util.mkdir(img_dir)\n                        f = open(os.path.join(img_dir, \'predict_score.txt\'), \'a\')\n\n                        model.feed_data(val_data)\n                        model.test()\n\n                        visuals = model.get_current_visuals()\n                        predict_score1 = visuals[\'predict_score1\'].numpy()\n                        # Save predict scores\n                        f.write(\'%s  %f\\n\' % (img_name + \'.png\', predict_score1))\n                        f.close()\n                        pbar.update(\'Test {}\'.format(img_name))\n\n                    # calculate accuracy\n                    aligned_pair_accuracy, accuracy_esrganbig, accuracy_srganbig = rank_pair_test(\\\n                        os.path.join(img_dir, \'predict_score.txt\'), label_path)\n\n                    # log\n                    logger.info(\n                        \'# Validation # Accuracy: {:.4e}, Accuracy_pair1_class1: {:.4e}, Accuracy_pair1_class2: {:.4e} \'.format(\n                            aligned_pair_accuracy, accuracy_esrganbig, accuracy_srganbig))\n                    logger_val = logging.getLogger(\'val\')  # validation logger\n                    logger_val.info(\n                        \'<epoch:{:3d}, iter:{:8,d}> Accuracy: {:.4e}, Accuracy_pair1_class1: {:.4e}, Accuracy_pair1_class2: {:.4e} \'.format(\n                            epoch, current_step, aligned_pair_accuracy, accuracy_esrganbig, accuracy_srganbig))\n\n                    # tensorboard logger\n                    if opt[\'use_tb_logger\'] and \'debug\' not in opt[\'name\']:\n                        tb_logger.add_scalar(\'Accuracy\', aligned_pair_accuracy, current_step)\n                        tb_logger.add_scalar(\'Accuracy_pair1_class1\', accuracy_esrganbig, current_step)\n                        tb_logger.add_scalar(\'Accuracy_pair1_class2\', accuracy_srganbig, current_step)\n            #### save models and training states\n            if current_step % opt[\'logger\'][\'save_checkpoint_freq\'] == 0:\n                if rank <= 0:\n                    logger.info(\'Saving models and training states.\')\n                    model.save(current_step)\n                    model.save_training_state(epoch, current_step)\n\n    if rank <= 0:\n        logger.info(\'Saving the final model.\')\n        model.save(\'latest\')\n        logger.info(\'End of training.\')\n        tb_logger.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
codes/data/LQGT_dataset.py,3,"b'import random\nimport numpy as np\nimport cv2\nimport lmdb\nimport torch\nimport torch.utils.data as data\nimport data.util as util\n\nclass LQGTDataset(data.Dataset):\n    """"""\n    Read LQ (Low Quality, e.g. LR (Low Resolution), blurry, etc) and GT image pairs.\n    If only GT images are provided, generate LQ images on-the-fly.\n    """"""\n\n    def __init__(self, opt):\n        super(LQGTDataset, self).__init__()\n        self.opt = opt\n        self.data_type = self.opt[\'data_type\']\n        self.paths_LQ, self.paths_GT = None, None\n        self.sizes_LQ, self.sizes_GT = None, None\n        self.LQ_env, self.GT_env = None, None  # environments for lmdb\n\n        self.paths_GT, self.sizes_GT = util.get_image_paths(self.data_type, opt[\'dataroot_GT\'])\n        self.paths_LQ, self.sizes_LQ = util.get_image_paths(self.data_type, opt[\'dataroot_LQ\'])\n        assert self.paths_GT, \'Error: GT path is empty.\'\n        if self.paths_LQ and self.paths_GT:\n            assert len(self.paths_LQ) == len(\n                self.paths_GT\n            ), \'GT and LQ datasets have different number of images - {}, {}.\'.format(\n                len(self.paths_LQ), len(self.paths_GT))\n        self.random_scale_list = [1]\n\n    def _init_lmdb(self):\n        # https://github.com/chainer/chainermn/issues/129\n        self.GT_env = lmdb.open(self.opt[\'dataroot_GT\'], readonly=True, lock=False, readahead=False,\n                                meminit=False)\n        self.LQ_env = lmdb.open(self.opt[\'dataroot_LQ\'], readonly=True, lock=False, readahead=False,\n                                meminit=False)\n\n    def __getitem__(self, index):\n        if self.data_type == \'lmdb\' and (self.GT_env is None or self.LQ_env is None):\n            self._init_lmdb()\n        GT_path, LQ_path = None, None\n        scale = self.opt[\'scale\']\n        GT_size = self.opt[\'GT_size\']\n\n        # get GT image\n        GT_path = self.paths_GT[index]\n        resolution = [int(s) for s in self.sizes_GT[index].split(\'_\')\n                      ] if self.data_type == \'lmdb\' else None\n        img_GT = util.read_img(self.GT_env, GT_path, resolution)\n        if self.opt[\'phase\'] != \'train\':  # modcrop in the validation / test phase\n            img_GT = util.modcrop(img_GT, scale)\n        if self.opt[\'color\']:  # change color space if necessary\n            img_GT = util.channel_convert(img_GT.shape[2], self.opt[\'color\'], [img_GT])[0]\n\n        # get LQ image\n        if self.paths_LQ:\n            LQ_path = self.paths_LQ[index]\n            resolution = [int(s) for s in self.sizes_LQ[index].split(\'_\')\n                          ] if self.data_type == \'lmdb\' else None\n            img_LQ = util.read_img(self.LQ_env, LQ_path, resolution)\n        else:  # down-sampling on-the-fly\n            # randomly scale during training\n            if self.opt[\'phase\'] == \'train\':\n                random_scale = random.choice(self.random_scale_list)\n                H_s, W_s, _ = img_GT.shape\n\n                def _mod(n, random_scale, scale, thres):\n                    rlt = int(n * random_scale)\n                    rlt = (rlt // scale) * scale\n                    return thres if rlt < thres else rlt\n\n                H_s = _mod(H_s, random_scale, scale, GT_size)\n                W_s = _mod(W_s, random_scale, scale, GT_size)\n                img_GT = cv2.resize(img_GT, (W_s, H_s), interpolation=cv2.INTER_LINEAR)\n                if img_GT.ndim == 2:\n                    img_GT = cv2.cvtColor(img_GT, cv2.COLOR_GRAY2BGR)\n\n            H, W, _ = img_GT.shape\n            # using matlab imresize\n            img_LQ = util.imresize_np(img_GT, 1 / scale, True)\n            if img_LQ.ndim == 2:\n                img_LQ = np.expand_dims(img_LQ, axis=2)\n\n        if self.opt[\'phase\'] == \'train\':\n            # if the image size is too small\n            H, W, _ = img_GT.shape\n            if H < GT_size or W < GT_size:\n                img_GT = cv2.resize(img_GT, (GT_size, GT_size), interpolation=cv2.INTER_LINEAR)\n                # using matlab imresize\n                img_LQ = util.imresize_np(img_GT, 1 / scale, True)\n                if img_LQ.ndim == 2:\n                    img_LQ = np.expand_dims(img_LQ, axis=2)\n\n            H, W, C = img_LQ.shape\n            LQ_size = GT_size // scale\n\n            # randomly crop\n            rnd_h = random.randint(0, max(0, H - LQ_size))\n            rnd_w = random.randint(0, max(0, W - LQ_size))\n            img_LQ = img_LQ[rnd_h:rnd_h + LQ_size, rnd_w:rnd_w + LQ_size, :]\n            rnd_h_GT, rnd_w_GT = int(rnd_h * scale), int(rnd_w * scale)\n            img_GT = img_GT[rnd_h_GT:rnd_h_GT + GT_size, rnd_w_GT:rnd_w_GT + GT_size, :]\n\n            # augmentation - flip, rotate\n            img_LQ, img_GT = util.augment([img_LQ, img_GT], self.opt[\'use_flip\'],\n                                          self.opt[\'use_rot\'])\n\n        if self.opt[\'color\']:  # change color space if necessary\n            img_LQ = util.channel_convert(C, self.opt[\'color\'],\n                                          [img_LQ])[0]  # TODO during val no definition\n\n        # BGR to RGB, HWC to CHW, numpy to tensor\n        if img_GT.shape[2] == 3:\n            img_GT = img_GT[:, :, [2, 1, 0]]\n            img_LQ = img_LQ[:, :, [2, 1, 0]]\n        img_GT = torch.from_numpy(np.ascontiguousarray(np.transpose(img_GT, (2, 0, 1)))).float()\n        img_LQ = torch.from_numpy(np.ascontiguousarray(np.transpose(img_LQ, (2, 0, 1)))).float()\n\n        if LQ_path is None:\n            LQ_path = GT_path\n        return {\'LQ\': img_LQ, \'GT\': img_GT, \'LQ_path\': LQ_path, \'GT_path\': GT_path}\n\n    def __len__(self):\n        return len(self.paths_GT)\n'"
codes/data/LQ_dataset.py,2,"b""import numpy as np\nimport lmdb\nimport torch\nimport torch.utils.data as data\nimport data.util as util\n\n\nclass LQDataset(data.Dataset):\n    '''Read LQ images only in the test phase.'''\n\n    def __init__(self, opt):\n        super(LQDataset, self).__init__()\n        self.opt = opt\n        self.data_type = self.opt['data_type']\n        self.paths_LQ, self.paths_GT = None, None\n        self.LQ_env = None  # environment for lmdb\n\n        self.paths_LQ, self.sizes_LQ = util.get_image_paths(self.data_type, opt['dataroot_LQ'])\n        assert self.paths_LQ, 'Error: LQ paths are empty.'\n\n    def _init_lmdb(self):\n        self.LQ_env = lmdb.open(self.opt['dataroot_LQ'], readonly=True, lock=False, readahead=False,\n                                meminit=False)\n\n    def __getitem__(self, index):\n        if self.data_type == 'lmdb' and self.LQ_env is None:\n            self._init_lmdb()\n        LQ_path = None\n\n        # get LQ image\n        LQ_path = self.paths_LQ[index]\n        resolution = [int(s) for s in self.sizes_LQ[index].split('_')\n                      ] if self.data_type == 'lmdb' else None\n        img_LQ = util.read_img(self.LQ_env, LQ_path, resolution)\n        H, W, C = img_LQ.shape\n\n        if self.opt['color']:  # change color space if necessary\n            img_LQ = util.channel_convert(C, self.opt['color'], [img_LQ])[0]\n\n        # BGR to RGB, HWC to CHW, numpy to tensor\n        if img_LQ.shape[2] == 3:\n            img_LQ = img_LQ[:, :, [2, 1, 0]]\n        img_LQ = torch.from_numpy(np.ascontiguousarray(np.transpose(img_LQ, (2, 0, 1)))).float()\n\n        return {'LQ': img_LQ, 'LQ_path': LQ_path}\n\n    def __len__(self):\n        return len(self.paths_LQ)\n"""
codes/data/Rank_IMIM_Pair_dataset.py,7,"b""import os.path\nimport random\nimport cv2\nimport numpy as np\nimport torch\nimport torch.utils.data as data\nimport data.util as util\nfrom itertools import combinations\nfrom scipy.special import comb\n\n\nclass RANK_IMIM_Pair_Dataset(data.Dataset):\n    '''\n    Read LR and HR image pair.\n    If only HR image is provided, generate LR image on-the-fly.\n    The pair is ensured by 'sorted' function, so please check the name convention.\n    '''\n\n    def name(self):\n        return 'RANK_IMIM_Pair_Dataset'\n\n    def __init__(self, opt, is_train):\n        super(RANK_IMIM_Pair_Dataset, self).__init__()\n        self.opt = opt\n\n        self.is_train = is_train\n\n        # read image list from lmdb or image files\n\n        self.paths_img1, self.sizes_GT = util.get_image_paths(opt['data_type'], opt['dataroot_img1'])\n        self.paths_img2, self.sizes_GT = util.get_image_paths(opt['data_type'], opt['dataroot_img2'])\n        self.paths_img3, self.sizes_GT = util.get_image_paths(opt['data_type'], opt['dataroot_img3'])\n\n        self.img_env1 = None\n        self.img_env2 = None\n        self.img_env3 = None\n\n        self.label_path = opt['dataroot_label_file']\n\n        # get image label scores\n        self.label = {}\n        f = open(self.label_path, 'r')\n        for line in f.readlines():\n            line = line.strip().split()\n            self.label[line[0]] = line[1]\n        f.close()\n\n        assert self.paths_img1, 'Error: img1 paths are empty.'\n\n        # self.random_scale_list = [1, 0.9, 0.8, 0.7, 0.6, 0.5]\n        self.random_scale_list = None\n\n    def __getitem__(self, index):\n\n        if self.is_train:\n            # get img1 and img1 label score\n            # choice = random.choice(['img1_img2','img1_img2','img1_img2','img1_img3','img2_img3']) #Oversampling for hard sample\n            choice = random.choice(['img1_img2', 'img1_img3', 'img2_img3'])\n\n            # print(choice)\n\n            if choice == 'img1_img2':\n                img1_path = self.paths_img1[index]\n                img1 = util.read_img(self.img_env1, img1_path)\n                img2_path = self.paths_img2[index]\n                img2 = util.read_img(self.img_env2, img2_path)\n            elif choice == 'img1_img3':\n                img1_path = self.paths_img1[index]\n                img1 = util.read_img(self.img_env1, img1_path)\n                img2_path = self.paths_img3[index]\n                img2 = util.read_img(self.img_env3, img2_path)\n\n            elif choice == 'img2_img3':\n                img1_path = self.paths_img2[index]\n                img1 = util.read_img(self.img_env2, img1_path)\n                img2_path = self.paths_img3[index]\n                img2 = util.read_img(self.img_env3, img2_path)\n\n\n            img1_name = img1_path.split('/')[-1]\n            img1_score = np.array(float(self.label[img1_name]), dtype='float')\n            img1_score = img1_score.reshape(1)\n\n            img2_name = img2_path.split('/')[-1]\n            img2_score = np.array(float(self.label[img2_name]), dtype='float')\n            img2_score = img2_score.reshape(1)\n\n            if img1.shape[2] == 3:\n                img1 = img1[:, :, [2, 1, 0]]\n            img1 = torch.from_numpy(np.ascontiguousarray(np.transpose(img1, (2, 0, 1)))).float()\n            img1_score = torch.from_numpy(img1_score).float()\n\n            if img2.shape[2] == 3:\n                img2 = img2[:, :, [2, 1, 0]]\n            img2 = torch.from_numpy(np.ascontiguousarray(np.transpose(img2, (2, 0, 1)))).float()\n            img2_score = torch.from_numpy(img2_score).float()\n\n            # print('img1:'+img1_name,' & ','img2:'+img2_name)\n\n        else:\n            # get img1\n            img1_path = self.paths_img1[index]\n            img1 = util.read_img(self.img_env1, img1_path)\n\n            img1_name = img1_path.split('/')[-1]\n            img1_score = np.array(float(self.label[img1_name]), dtype='float')\n            img1_score = img1_score.reshape(1)\n\n            if img1.shape[2] == 3:\n                img1 = img1[:, :, [2, 1, 0]]\n            img1 = torch.from_numpy(np.ascontiguousarray(np.transpose(img1, (2, 0, 1)))).float()\n            img1_score = torch.from_numpy(img1_score).float()\n            # print('img1:'+img1_name)\n\n            # not useful\n            img2_path = img1_path\n            img2 = img1\n            img2_score = img1_score\n\n        # exit()\n\n        return {'img1': img1, 'img2': img2, 'img1_path': img1_path, 'img2_path': img2_path, 'score1': img1_score,\n                'score2': img2_score}\n\n    def __len__(self):\n        return len(self.paths_img1)\n"""
codes/data/__init__.py,4,"b'""""""create dataset and dataloader""""""\nimport logging\nimport torch\nimport torch.utils.data\n\n\ndef create_dataloader(dataset, dataset_opt, opt=None, sampler=None):\n    phase = dataset_opt[\'phase\']\n    if phase == \'train\':\n        if opt[\'dist\']:\n            world_size = torch.distributed.get_world_size()\n            num_workers = dataset_opt[\'n_workers\']\n            assert dataset_opt[\'batch_size\'] % world_size == 0\n            batch_size = dataset_opt[\'batch_size\'] // world_size\n            shuffle = False\n        else:\n            num_workers = dataset_opt[\'n_workers\'] * len(opt[\'gpu_ids\'])\n            batch_size = dataset_opt[\'batch_size\']\n            shuffle = True\n        return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,\n                                           num_workers=num_workers, sampler=sampler, drop_last=True,\n                                           pin_memory=False)\n    else:\n        return torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1,\n                                           pin_memory=False)\n\ndef create_dataset(dataset_opt, is_train = True):\n    mode = dataset_opt[\'mode\']\n    # datasets for image restoration\n    if mode == \'LQ\':\n        from data.LQ_dataset import LQDataset as D\n    elif mode == \'LQGT\':\n        from data.LQGT_dataset import LQGTDataset as D\n    elif mode == \'RANK_IMIM_Pair\':\n        from data.Rank_IMIM_Pair_dataset import RANK_IMIM_Pair_Dataset as D\n    else:\n        raise NotImplementedError(\'Dataset [{:s}] is not recognized.\'.format(mode))\n    if \'RANK_IMIM_Pair\' in mode:\n        dataset = D(dataset_opt, is_train = is_train)\n    else:\n        dataset = D(dataset_opt)\n    logger = logging.getLogger(\'base\')\n    logger.info(\'Dataset [{:s} - {:s}] is created.\'.format(dataset.__class__.__name__,\n                                                           dataset_opt[\'name\']))\n    return dataset\n'"
codes/data/data_sampler.py,6,"b'""""""\nModified from torch.utils.data.distributed.DistributedSampler\nSupport enlarging the dataset for *iteration-oriented* training, for saving time when restart the\ndataloader after each epoch\n""""""\nimport math\nimport torch\nfrom torch.utils.data.sampler import Sampler\nimport torch.distributed as dist\n\n\nclass DistIterSampler(Sampler):\n    """"""Sampler that restricts data loading to a subset of the dataset.\n\n    It is especially useful in conjunction with\n    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSampler instance as a DataLoader sampler,\n    and load a subset of the original dataset that is exclusive to it.\n\n    .. note::\n        Dataset is assumed to be of constant size.\n\n    Arguments:\n        dataset: Dataset used for sampling.\n        num_replicas (optional): Number of processes participating in\n            distributed training.\n        rank (optional): Rank of the current process within num_replicas.\n    """"""\n\n    def __init__(self, dataset, num_replicas=None, rank=None, ratio=100):\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(""Requires distributed package to be available"")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(""Requires distributed package to be available"")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.num_samples = int(math.ceil(len(self.dataset) * ratio / self.num_replicas))\n        self.total_size = self.num_samples * self.num_replicas\n\n    def __iter__(self):\n        # deterministically shuffle based on epoch\n        g = torch.Generator()\n        g.manual_seed(self.epoch)\n        indices = torch.randperm(self.total_size, generator=g).tolist()\n\n        dsize = len(self.dataset)\n        indices = [v % dsize for v in indices]\n\n        # subsample\n        indices = indices[self.rank:self.total_size:self.num_replicas]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch\n'"
codes/data/util.py,25,"b'import os\nimport math\nimport pickle\nimport random\nimport numpy as np\nimport glob\nimport torch\nimport cv2\n\n####################\n# Files & IO\n####################\n\n###################### get image path list ######################\nIMG_EXTENSIONS = [\'.jpg\', \'.JPG\', \'.jpeg\', \'.JPEG\', \'.png\', \'.PNG\', \'.ppm\', \'.PPM\', \'.bmp\', \'.BMP\']\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\n\ndef _get_paths_from_images(path):\n    """"""get image path list from image folder""""""\n    assert os.path.isdir(path), \'{:s} is not a valid directory\'.format(path)\n    images = []\n    for dirpath, _, fnames in sorted(os.walk(path)):\n        for fname in sorted(fnames):\n            if is_image_file(fname):\n                img_path = os.path.join(dirpath, fname)\n                images.append(img_path)\n    assert images, \'{:s} has no valid image file\'.format(path)\n    return images\n\n\ndef _get_paths_from_lmdb(dataroot):\n    """"""get image path list from lmdb meta info""""""\n    meta_info = pickle.load(open(os.path.join(dataroot, \'meta_info.pkl\'), \'rb\'))\n    paths = meta_info[\'keys\']\n    sizes = meta_info[\'resolution\']\n    if len(sizes) == 1:\n        sizes = sizes * len(paths)\n    return paths, sizes\n\n\ndef get_image_paths(data_type, dataroot):\n    """"""get image path list\n    support lmdb or image files""""""\n    paths, sizes = None, None\n    if dataroot is not None:\n        if data_type == \'lmdb\':\n            paths, sizes = _get_paths_from_lmdb(dataroot)\n        elif data_type == \'img\':\n            paths = sorted(_get_paths_from_images(dataroot))\n        else:\n            raise NotImplementedError(\'data_type [{:s}] is not recognized.\'.format(data_type))\n    return paths, sizes\n\n\ndef glob_file_list(root):\n    return sorted(glob.glob(os.path.join(root, \'*\')))\n\n\n###################### read images ######################\ndef _read_img_lmdb(env, key, size):\n    """"""read image from lmdb with key (w/ and w/o fixed size)\n    size: (C, H, W) tuple""""""\n    with env.begin(write=False) as txn:\n        buf = txn.get(key.encode(\'ascii\'))\n    img_flat = np.frombuffer(buf, dtype=np.uint8)\n    C, H, W = size\n    img = img_flat.reshape(H, W, C)\n    return img\n\n\ndef read_img(env, path, size=None):\n    """"""read image by cv2 or from lmdb\n    return: Numpy float32, HWC, BGR, [0,1]""""""\n    if env is None:  # img\n        img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n    else:\n        img = _read_img_lmdb(env, path, size)\n    img = img.astype(np.float32) / 255.\n    if img.ndim == 2:\n        img = np.expand_dims(img, axis=2)\n    # some images have 4 channels\n    if img.shape[2] > 3:\n        img = img[:, :, :3]\n    return img\n\n\ndef read_img_seq(path):\n    """"""Read a sequence of images from a given folder path\n    Args:\n        path (list/str): list of image paths/image folder path\n\n    Returns:\n        imgs (Tensor): size (T, C, H, W), RGB, [0, 1]\n    """"""\n    if type(path) is list:\n        img_path_l = path\n    else:\n        img_path_l = sorted(glob.glob(os.path.join(path, \'*\')))\n    img_l = [read_img(None, v) for v in img_path_l]\n    # stack to Torch tensor\n    imgs = np.stack(img_l, axis=0)\n    imgs = imgs[:, :, :, [2, 1, 0]]\n    imgs = torch.from_numpy(np.ascontiguousarray(np.transpose(imgs, (0, 3, 1, 2)))).float()\n    return imgs\n\n\ndef index_generation(crt_i, max_n, N, padding=\'reflection\'):\n    """"""Generate an index list for reading N frames from a sequence of images\n    Args:\n        crt_i (int): current center index\n        max_n (int): max number of the sequence of images (calculated from 1)\n        N (int): reading N frames\n        padding (str): padding mode, one of replicate | reflection | new_info | circle\n            Example: crt_i = 0, N = 5\n            replicate: [0, 0, 0, 1, 2]\n            reflection: [2, 1, 0, 1, 2]\n            new_info: [4, 3, 0, 1, 2]\n            circle: [3, 4, 0, 1, 2]\n\n    Returns:\n        return_l (list [int]): a list of indexes\n    """"""\n    max_n = max_n - 1\n    n_pad = N // 2\n    return_l = []\n\n    for i in range(crt_i - n_pad, crt_i + n_pad + 1):\n        if i < 0:\n            if padding == \'replicate\':\n                add_idx = 0\n            elif padding == \'reflection\':\n                add_idx = -i\n            elif padding == \'new_info\':\n                add_idx = (crt_i + n_pad) + (-i)\n            elif padding == \'circle\':\n                add_idx = N + i\n            else:\n                raise ValueError(\'Wrong padding mode\')\n        elif i > max_n:\n            if padding == \'replicate\':\n                add_idx = max_n\n            elif padding == \'reflection\':\n                add_idx = max_n * 2 - i\n            elif padding == \'new_info\':\n                add_idx = (crt_i - n_pad) - (i - max_n)\n            elif padding == \'circle\':\n                add_idx = i - N\n            else:\n                raise ValueError(\'Wrong padding mode\')\n        else:\n            add_idx = i\n        return_l.append(add_idx)\n    return return_l\n\n\n####################\n# image processing\n# process on numpy image\n####################\n\n\ndef augment(img_list, hflip=True, rot=True):\n    """"""horizontal flip OR rotate (0, 90, 180, 270 degrees)""""""\n    hflip = hflip and random.random() < 0.5\n    vflip = rot and random.random() < 0.5\n    rot90 = rot and random.random() < 0.5\n\n    def _augment(img):\n        if hflip:\n            img = img[:, ::-1, :]\n        if vflip:\n            img = img[::-1, :, :]\n        if rot90:\n            img = img.transpose(1, 0, 2)\n        return img\n\n    return [_augment(img) for img in img_list]\n\n\ndef augment_flow(img_list, flow_list, hflip=True, rot=True):\n    """"""horizontal flip OR rotate (0, 90, 180, 270 degrees) with flows""""""\n    hflip = hflip and random.random() < 0.5\n    vflip = rot and random.random() < 0.5\n    rot90 = rot and random.random() < 0.5\n\n    def _augment(img):\n        if hflip:\n            img = img[:, ::-1, :]\n        if vflip:\n            img = img[::-1, :, :]\n        if rot90:\n            img = img.transpose(1, 0, 2)\n        return img\n\n    def _augment_flow(flow):\n        if hflip:\n            flow = flow[:, ::-1, :]\n            flow[:, :, 0] *= -1\n        if vflip:\n            flow = flow[::-1, :, :]\n            flow[:, :, 1] *= -1\n        if rot90:\n            flow = flow.transpose(1, 0, 2)\n            flow = flow[:, :, [1, 0]]\n        return flow\n\n    rlt_img_list = [_augment(img) for img in img_list]\n    rlt_flow_list = [_augment_flow(flow) for flow in flow_list]\n\n    return rlt_img_list, rlt_flow_list\n\n\ndef channel_convert(in_c, tar_type, img_list):\n    """"""conversion among BGR, gray and y""""""\n    if in_c == 3 and tar_type == \'gray\':  # BGR to gray\n        gray_list = [cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) for img in img_list]\n        return [np.expand_dims(img, axis=2) for img in gray_list]\n    elif in_c == 3 and tar_type == \'y\':  # BGR to y\n        y_list = [bgr2ycbcr(img, only_y=True) for img in img_list]\n        return [np.expand_dims(img, axis=2) for img in y_list]\n    elif in_c == 1 and tar_type == \'RGB\':  # gray/y to BGR\n        return [cv2.cvtColor(img, cv2.COLOR_GRAY2BGR) for img in img_list]\n    else:\n        return img_list\n\n\ndef rgb2ycbcr(img, only_y=True):\n    """"""same as matlab rgb2ycbcr\n    only_y: only return Y channel\n    Input:\n        uint8, [0, 255]\n        float, [0, 1]\n    """"""\n    in_img_type = img.dtype\n    img.astype(np.float32)\n    if in_img_type != np.uint8:\n        img *= 255.\n    # convert\n    if only_y:\n        rlt = np.dot(img, [65.481, 128.553, 24.966]) / 255.0 + 16.0\n    else:\n        rlt = np.matmul(img, [[65.481, -37.797, 112.0], [128.553, -74.203, -93.786],\n                              [24.966, 112.0, -18.214]]) / 255.0 + [16, 128, 128]\n    if in_img_type == np.uint8:\n        rlt = rlt.round()\n    else:\n        rlt /= 255.\n    return rlt.astype(in_img_type)\n\n\ndef bgr2ycbcr(img, only_y=True):\n    """"""bgr version of rgb2ycbcr\n    only_y: only return Y channel\n    Input:\n        uint8, [0, 255]\n        float, [0, 1]\n    """"""\n    in_img_type = img.dtype\n    img.astype(np.float32)\n    if in_img_type != np.uint8:\n        img *= 255.\n    # convert\n    if only_y:\n        rlt = np.dot(img, [24.966, 128.553, 65.481]) / 255.0 + 16.0\n    else:\n        rlt = np.matmul(img, [[24.966, 112.0, -18.214], [128.553, -74.203, -93.786],\n                              [65.481, -37.797, 112.0]]) / 255.0 + [16, 128, 128]\n    if in_img_type == np.uint8:\n        rlt = rlt.round()\n    else:\n        rlt /= 255.\n    return rlt.astype(in_img_type)\n\n\ndef ycbcr2rgb(img):\n    """"""same as matlab ycbcr2rgb\n    Input:\n        uint8, [0, 255]\n        float, [0, 1]\n    """"""\n    in_img_type = img.dtype\n    img.astype(np.float32)\n    if in_img_type != np.uint8:\n        img *= 255.\n    # convert\n    rlt = np.matmul(img, [[0.00456621, 0.00456621, 0.00456621], [0, -0.00153632, 0.00791071],\n                          [0.00625893, -0.00318811, 0]]) * 255.0 + [-222.921, 135.576, -276.836]\n    if in_img_type == np.uint8:\n        rlt = rlt.round()\n    else:\n        rlt /= 255.\n    return rlt.astype(in_img_type)\n\n\ndef modcrop(img_in, scale):\n    """"""img_in: Numpy, HWC or HW""""""\n    img = np.copy(img_in)\n    if img.ndim == 2:\n        H, W = img.shape\n        H_r, W_r = H % scale, W % scale\n        img = img[:H - H_r, :W - W_r]\n    elif img.ndim == 3:\n        H, W, C = img.shape\n        H_r, W_r = H % scale, W % scale\n        img = img[:H - H_r, :W - W_r, :]\n    else:\n        raise ValueError(\'Wrong img ndim: [{:d}].\'.format(img.ndim))\n    return img\n\n\n####################\n# Functions\n####################\n\n\n# matlab \'imresize\' function, now only support \'bicubic\'\ndef cubic(x):\n    absx = torch.abs(x)\n    absx2 = absx**2\n    absx3 = absx**3\n    return (1.5 * absx3 - 2.5 * absx2 + 1) * (\n        (absx <= 1).type_as(absx)) + (-0.5 * absx3 + 2.5 * absx2 - 4 * absx + 2) * ((\n            (absx > 1) * (absx <= 2)).type_as(absx))\n\n\ndef calculate_weights_indices(in_length, out_length, scale, kernel, kernel_width, antialiasing):\n    if (scale < 1) and (antialiasing):\n        # Use a modified kernel to simultaneously interpolate and antialias- larger kernel width\n        kernel_width = kernel_width / scale\n\n    # Output-space coordinates\n    x = torch.linspace(1, out_length, out_length)\n\n    # Input-space coordinates. Calculate the inverse mapping such that 0.5\n    # in output space maps to 0.5 in input space, and 0.5+scale in output\n    # space maps to 1.5 in input space.\n    u = x / scale + 0.5 * (1 - 1 / scale)\n\n    # What is the left-most pixel that can be involved in the computation?\n    left = torch.floor(u - kernel_width / 2)\n\n    # What is the maximum number of pixels that can be involved in the\n    # computation?  Note: it\'s OK to use an extra pixel here; if the\n    # corresponding weights are all zero, it will be eliminated at the end\n    # of this function.\n    P = math.ceil(kernel_width) + 2\n\n    # The indices of the input pixels involved in computing the k-th output\n    # pixel are in row k of the indices matrix.\n    indices = left.view(out_length, 1).expand(out_length, P) + torch.linspace(0, P - 1, P).view(\n        1, P).expand(out_length, P)\n\n    # The weights used to compute the k-th output pixel are in row k of the\n    # weights matrix.\n    distance_to_center = u.view(out_length, 1).expand(out_length, P) - indices\n    # apply cubic kernel\n    if (scale < 1) and (antialiasing):\n        weights = scale * cubic(distance_to_center * scale)\n    else:\n        weights = cubic(distance_to_center)\n    # Normalize the weights matrix so that each row sums to 1.\n    weights_sum = torch.sum(weights, 1).view(out_length, 1)\n    weights = weights / weights_sum.expand(out_length, P)\n\n    # If a column in weights is all zero, get rid of it. only consider the first and last column.\n    weights_zero_tmp = torch.sum((weights == 0), 0)\n    if not math.isclose(weights_zero_tmp[0], 0, rel_tol=1e-6):\n        indices = indices.narrow(1, 1, P - 2)\n        weights = weights.narrow(1, 1, P - 2)\n    if not math.isclose(weights_zero_tmp[-1], 0, rel_tol=1e-6):\n        indices = indices.narrow(1, 0, P - 2)\n        weights = weights.narrow(1, 0, P - 2)\n    weights = weights.contiguous()\n    indices = indices.contiguous()\n    sym_len_s = -indices.min() + 1\n    sym_len_e = indices.max() - in_length\n    indices = indices + sym_len_s - 1\n    return weights, indices, int(sym_len_s), int(sym_len_e)\n\n\ndef imresize(img, scale, antialiasing=True):\n    # Now the scale should be the same for H and W\n    # input: img: CHW RGB [0,1]\n    # output: CHW RGB [0,1] w/o round\n\n    in_C, in_H, in_W = img.size()\n    _, out_H, out_W = in_C, math.ceil(in_H * scale), math.ceil(in_W * scale)\n    kernel_width = 4\n    kernel = \'cubic\'\n\n    # Return the desired dimension order for performing the resize.  The\n    # strategy is to perform the resize first along the dimension with the\n    # smallest scale factor.\n    # Now we do not support this.\n\n    # get weights and indices\n    weights_H, indices_H, sym_len_Hs, sym_len_He = calculate_weights_indices(\n        in_H, out_H, scale, kernel, kernel_width, antialiasing)\n    weights_W, indices_W, sym_len_Ws, sym_len_We = calculate_weights_indices(\n        in_W, out_W, scale, kernel, kernel_width, antialiasing)\n    # process H dimension\n    # symmetric copying\n    img_aug = torch.FloatTensor(in_C, in_H + sym_len_Hs + sym_len_He, in_W)\n    img_aug.narrow(1, sym_len_Hs, in_H).copy_(img)\n\n    sym_patch = img[:, :sym_len_Hs, :]\n    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n    img_aug.narrow(1, 0, sym_len_Hs).copy_(sym_patch_inv)\n\n    sym_patch = img[:, -sym_len_He:, :]\n    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n    img_aug.narrow(1, sym_len_Hs + in_H, sym_len_He).copy_(sym_patch_inv)\n\n    out_1 = torch.FloatTensor(in_C, out_H, in_W)\n    kernel_width = weights_H.size(1)\n    for i in range(out_H):\n        idx = int(indices_H[i][0])\n        out_1[0, i, :] = img_aug[0, idx:idx + kernel_width, :].transpose(0, 1).mv(weights_H[i])\n        out_1[1, i, :] = img_aug[1, idx:idx + kernel_width, :].transpose(0, 1).mv(weights_H[i])\n        out_1[2, i, :] = img_aug[2, idx:idx + kernel_width, :].transpose(0, 1).mv(weights_H[i])\n\n    # process W dimension\n    # symmetric copying\n    out_1_aug = torch.FloatTensor(in_C, out_H, in_W + sym_len_Ws + sym_len_We)\n    out_1_aug.narrow(2, sym_len_Ws, in_W).copy_(out_1)\n\n    sym_patch = out_1[:, :, :sym_len_Ws]\n    inv_idx = torch.arange(sym_patch.size(2) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(2, inv_idx)\n    out_1_aug.narrow(2, 0, sym_len_Ws).copy_(sym_patch_inv)\n\n    sym_patch = out_1[:, :, -sym_len_We:]\n    inv_idx = torch.arange(sym_patch.size(2) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(2, inv_idx)\n    out_1_aug.narrow(2, sym_len_Ws + in_W, sym_len_We).copy_(sym_patch_inv)\n\n    out_2 = torch.FloatTensor(in_C, out_H, out_W)\n    kernel_width = weights_W.size(1)\n    for i in range(out_W):\n        idx = int(indices_W[i][0])\n        out_2[0, :, i] = out_1_aug[0, :, idx:idx + kernel_width].mv(weights_W[i])\n        out_2[1, :, i] = out_1_aug[1, :, idx:idx + kernel_width].mv(weights_W[i])\n        out_2[2, :, i] = out_1_aug[2, :, idx:idx + kernel_width].mv(weights_W[i])\n\n    return out_2\n\n\ndef imresize_np(img, scale, antialiasing=True):\n    # Now the scale should be the same for H and W\n    # input: img: Numpy, HWC BGR [0,1]\n    # output: HWC BGR [0,1] w/o round\n    img = torch.from_numpy(img)\n\n    in_H, in_W, in_C = img.size()\n    _, out_H, out_W = in_C, math.ceil(in_H * scale), math.ceil(in_W * scale)\n    kernel_width = 4\n    kernel = \'cubic\'\n\n    # Return the desired dimension order for performing the resize.  The\n    # strategy is to perform the resize first along the dimension with the\n    # smallest scale factor.\n    # Now we do not support this.\n\n    # get weights and indices\n    weights_H, indices_H, sym_len_Hs, sym_len_He = calculate_weights_indices(\n        in_H, out_H, scale, kernel, kernel_width, antialiasing)\n    weights_W, indices_W, sym_len_Ws, sym_len_We = calculate_weights_indices(\n        in_W, out_W, scale, kernel, kernel_width, antialiasing)\n    # process H dimension\n    # symmetric copying\n    img_aug = torch.FloatTensor(in_H + sym_len_Hs + sym_len_He, in_W, in_C)\n    img_aug.narrow(0, sym_len_Hs, in_H).copy_(img)\n\n    sym_patch = img[:sym_len_Hs, :, :]\n    inv_idx = torch.arange(sym_patch.size(0) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(0, inv_idx)\n    img_aug.narrow(0, 0, sym_len_Hs).copy_(sym_patch_inv)\n\n    sym_patch = img[-sym_len_He:, :, :]\n    inv_idx = torch.arange(sym_patch.size(0) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(0, inv_idx)\n    img_aug.narrow(0, sym_len_Hs + in_H, sym_len_He).copy_(sym_patch_inv)\n\n    out_1 = torch.FloatTensor(out_H, in_W, in_C)\n    kernel_width = weights_H.size(1)\n    for i in range(out_H):\n        idx = int(indices_H[i][0])\n        out_1[i, :, 0] = img_aug[idx:idx + kernel_width, :, 0].transpose(0, 1).mv(weights_H[i])\n        out_1[i, :, 1] = img_aug[idx:idx + kernel_width, :, 1].transpose(0, 1).mv(weights_H[i])\n        out_1[i, :, 2] = img_aug[idx:idx + kernel_width, :, 2].transpose(0, 1).mv(weights_H[i])\n\n    # process W dimension\n    # symmetric copying\n    out_1_aug = torch.FloatTensor(out_H, in_W + sym_len_Ws + sym_len_We, in_C)\n    out_1_aug.narrow(1, sym_len_Ws, in_W).copy_(out_1)\n\n    sym_patch = out_1[:, :sym_len_Ws, :]\n    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n    out_1_aug.narrow(1, 0, sym_len_Ws).copy_(sym_patch_inv)\n\n    sym_patch = out_1[:, -sym_len_We:, :]\n    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n    out_1_aug.narrow(1, sym_len_Ws + in_W, sym_len_We).copy_(sym_patch_inv)\n\n    out_2 = torch.FloatTensor(out_H, out_W, in_C)\n    kernel_width = weights_W.size(1)\n    for i in range(out_W):\n        idx = int(indices_W[i][0])\n        out_2[:, i, 0] = out_1_aug[:, idx:idx + kernel_width, 0].mv(weights_W[i])\n        out_2[:, i, 1] = out_1_aug[:, idx:idx + kernel_width, 1].mv(weights_W[i])\n        out_2[:, i, 2] = out_1_aug[:, idx:idx + kernel_width, 2].mv(weights_W[i])\n\n    return out_2.numpy()\n\n\nif __name__ == \'__main__\':\n    # test imresize function\n    # read images\n    img = cv2.imread(\'test.png\')\n    img = img * 1.0 / 255\n    img = torch.from_numpy(np.transpose(img[:, :, [2, 1, 0]], (2, 0, 1))).float()\n    # imresize\n    scale = 1 / 4\n    import time\n    total_time = 0\n    for i in range(10):\n        start_time = time.time()\n        rlt = imresize(img, scale, antialiasing=True)\n        use_time = time.time() - start_time\n        total_time += use_time\n    print(\'average time: {}\'.format(total_time / 10))\n\n    import torchvision.utils\n    torchvision.utils.save_image((rlt * 255).round() / 255, \'rlt.png\', nrow=1, padding=0,\n                                 normalize=False)\n'"
codes/data_scripts/create_lmdb.py,0,"b'""""""Create lmdb files for [General images (291 images/DIV2K) | Vimeo90K | REDS] training datasets""""""\n\nimport sys\nimport os.path as osp\nimport glob\nimport pickle\nfrom multiprocessing import Pool\nimport numpy as np\nimport lmdb\nimport cv2\n\nsys.path.append(osp.dirname(osp.dirname(osp.abspath(__file__))))\nimport data.util as data_util  # noqa: E402\nimport utils.util as util  # noqa: E402\n\n\ndef main():\n    dataset = \'DIV2K_demo\'  # vimeo90K | REDS | general (e.g., DIV2K, 291) | DIV2K_demo |test\n    mode = \'GT\'  # used for vimeo90k and REDS datasets\n    # vimeo90k: GT | LR | flow\n    # REDS: train_sharp, train_sharp_bicubic, train_blur_bicubic, train_blur, train_blur_comp\n    #       train_sharp_flowx4\n    if dataset == \'vimeo90k\':\n        vimeo90k(mode)\n    elif dataset == \'REDS\':\n        REDS(mode)\n    elif dataset == \'general\':\n        opt = {}\n        opt[\'img_folder\'] = \'../../datasets/DIV2K/DIV2K800_sub\'\n        opt[\'lmdb_save_path\'] = \'../../datasets/DIV2K/DIV2K800_sub.lmdb\'\n        opt[\'name\'] = \'DIV2K800_sub_GT\'\n        general_image_folder(opt)\n    elif dataset == \'DIV2K_demo\':\n        opt = {}\n        ## GT\n        opt[\'img_folder\'] = \'../../datasets/DIV2K/DIV2K800_sub\'\n        opt[\'lmdb_save_path\'] = \'../../datasets/DIV2K/DIV2K800_sub.lmdb\'\n        opt[\'name\'] = \'DIV2K800_sub_GT\'\n        general_image_folder(opt)\n        ## LR\n        opt[\'img_folder\'] = \'../../datasets/DIV2K/DIV2K800_sub_bicLRx4\'\n        opt[\'lmdb_save_path\'] = \'../../datasets/DIV2K/DIV2K800_sub_bicLRx4.lmdb\'\n        opt[\'name\'] = \'DIV2K800_sub_bicLRx4\'\n        general_image_folder(opt)\n    elif dataset == \'test\':\n        test_lmdb(\'../../datasets/REDS/train_sharp_wval.lmdb\', \'REDS\')\n\n\ndef read_image_worker(path, key):\n    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n    return (key, img)\n\n\ndef general_image_folder(opt):\n    """"""Create lmdb for general image folders\n    Users should define the keys, such as: \'0321_s035\' for DIV2K sub-images\n    If all the images have the same resolution, it will only store one copy of resolution info.\n        Otherwise, it will store every resolution info.\n    """"""\n    #### configurations\n    read_all_imgs = False  # whether real all images to memory with multiprocessing\n    # Set False for use limited memory\n    BATCH = 5000  # After BATCH images, lmdb commits, if read_all_imgs = False\n    n_thread = 40\n    ########################################################\n    img_folder = opt[\'img_folder\']\n    lmdb_save_path = opt[\'lmdb_save_path\']\n    meta_info = {\'name\': opt[\'name\']}\n    if not lmdb_save_path.endswith(\'.lmdb\'):\n        raise ValueError(""lmdb_save_path must end with \\\'lmdb\\\'."")\n    if osp.exists(lmdb_save_path):\n        print(\'Folder [{:s}] already exists. Exit...\'.format(lmdb_save_path))\n        sys.exit(1)\n\n    #### read all the image paths to a list\n    print(\'Reading image path list ...\')\n    all_img_list = sorted(glob.glob(osp.join(img_folder, \'*\')))\n    keys = []\n    for img_path in all_img_list:\n        keys.append(osp.splitext(osp.basename(img_path))[0])\n\n    if read_all_imgs:\n        #### read all images to memory (multiprocessing)\n        dataset = {}  # store all image data. list cannot keep the order, use dict\n        print(\'Read images with multiprocessing, #thread: {} ...\'.format(n_thread))\n        pbar = util.ProgressBar(len(all_img_list))\n\n        def mycallback(arg):\n            \'\'\'get the image data and update pbar\'\'\'\n            key = arg[0]\n            dataset[key] = arg[1]\n            pbar.update(\'Reading {}\'.format(key))\n\n        pool = Pool(n_thread)\n        for path, key in zip(all_img_list, keys):\n            pool.apply_async(read_image_worker, args=(path, key), callback=mycallback)\n        pool.close()\n        pool.join()\n        print(\'Finish reading {} images.\\nWrite lmdb...\'.format(len(all_img_list)))\n\n    #### create lmdb environment\n    data_size_per_img = cv2.imread(all_img_list[0], cv2.IMREAD_UNCHANGED).nbytes\n    print(\'data size per image is: \', data_size_per_img)\n    data_size = data_size_per_img * len(all_img_list)\n    env = lmdb.open(lmdb_save_path, map_size=data_size * 10)\n\n    #### write data to lmdb\n    pbar = util.ProgressBar(len(all_img_list))\n    txn = env.begin(write=True)\n    resolutions = []\n    for idx, (path, key) in enumerate(zip(all_img_list, keys)):\n        pbar.update(\'Write {}\'.format(key))\n        key_byte = key.encode(\'ascii\')\n        data = dataset[key] if read_all_imgs else cv2.imread(path, cv2.IMREAD_UNCHANGED)\n        if data.ndim == 2:\n            H, W = data.shape\n            C = 1\n        else:\n            H, W, C = data.shape\n        txn.put(key_byte, data)\n        resolutions.append(\'{:d}_{:d}_{:d}\'.format(C, H, W))\n        if not read_all_imgs and idx % BATCH == 0:\n            txn.commit()\n            txn = env.begin(write=True)\n    txn.commit()\n    env.close()\n    print(\'Finish writing lmdb.\')\n\n    #### create meta information\n    # check whether all the images are the same size\n    assert len(keys) == len(resolutions)\n    if len(set(resolutions)) <= 1:\n        meta_info[\'resolution\'] = [resolutions[0]]\n        meta_info[\'keys\'] = keys\n        print(\'All images have the same resolution. Simplify the meta info.\')\n    else:\n        meta_info[\'resolution\'] = resolutions\n        meta_info[\'keys\'] = keys\n        print(\'Not all images have the same resolution. Save meta info for each image.\')\n\n    pickle.dump(meta_info, open(osp.join(lmdb_save_path, \'meta_info.pkl\'), ""wb""))\n    print(\'Finish creating lmdb meta info.\')\n\n\ndef vimeo90k(mode):\n    """"""Create lmdb for the Vimeo90K dataset, each image with a fixed size\n    GT: [3, 256, 448]\n        Now only need the 4th frame, e.g., 00001_0001_4\n    LR: [3, 64, 112]\n        1st - 7th frames, e.g., 00001_0001_1, ..., 00001_0001_7\n    key:\n        Use the folder and subfolder names, w/o the frame index, e.g., 00001_0001\n\n    flow: downsampled flow: [3, 360, 320], keys: 00001_0001_4_[p3, p2, p1, n1, n2, n3]\n        Each flow is calculated with GT images by PWCNet and then downsampled by 1/4\n        Flow map is quantized by mmcv and saved in png format\n    """"""\n    #### configurations\n    read_all_imgs = False  # whether real all images to memory with multiprocessing\n    # Set False for use limited memory\n    BATCH = 5000  # After BATCH images, lmdb commits, if read_all_imgs = False\n    if mode == \'GT\':\n        img_folder = \'../../datasets/vimeo90k/vimeo_septuplet/sequences\'\n        lmdb_save_path = \'../../datasets/vimeo90k/vimeo90k_train_GT.lmdb\'\n        txt_file = \'../../datasets/vimeo90k/vimeo_septuplet/sep_trainlist.txt\'\n        H_dst, W_dst = 256, 448\n    elif mode == \'LR\':\n        img_folder = \'../../datasets/vimeo90k/vimeo_septuplet_matlabLRx4/sequences\'\n        lmdb_save_path = \'../../datasets/vimeo90k/vimeo90k_train_LR7frames.lmdb\'\n        txt_file = \'../../datasets/vimeo90k/vimeo_septuplet/sep_trainlist.txt\'\n        H_dst, W_dst = 64, 112\n    elif mode == \'flow\':\n        img_folder = \'../../datasets/vimeo90k/vimeo_septuplet/sequences_flowx4\'\n        lmdb_save_path = \'../../datasets/vimeo90k/vimeo90k_train_flowx4.lmdb\'\n        txt_file = \'../../datasets/vimeo90k/vimeo_septuplet/sep_trainlist.txt\'\n        H_dst, W_dst = 128, 112\n    else:\n        raise ValueError(\'Wrong dataset mode: {}\'.format(mode))\n    n_thread = 40\n    ########################################################\n    if not lmdb_save_path.endswith(\'.lmdb\'):\n        raise ValueError(""lmdb_save_path must end with \\\'lmdb\\\'."")\n    if osp.exists(lmdb_save_path):\n        print(\'Folder [{:s}] already exists. Exit...\'.format(lmdb_save_path))\n        sys.exit(1)\n\n    #### read all the image paths to a list\n    print(\'Reading image path list ...\')\n    with open(txt_file) as f:\n        train_l = f.readlines()\n        train_l = [v.strip() for v in train_l]\n    all_img_list = []\n    keys = []\n    for line in train_l:\n        folder = line.split(\'/\')[0]\n        sub_folder = line.split(\'/\')[1]\n        all_img_list.extend(glob.glob(osp.join(img_folder, folder, sub_folder, \'*\')))\n        if mode == \'flow\':\n            for j in range(1, 4):\n                keys.append(\'{}_{}_4_n{}\'.format(folder, sub_folder, j))\n                keys.append(\'{}_{}_4_p{}\'.format(folder, sub_folder, j))\n        else:\n            for j in range(7):\n                keys.append(\'{}_{}_{}\'.format(folder, sub_folder, j + 1))\n    all_img_list = sorted(all_img_list)\n    keys = sorted(keys)\n    if mode == \'GT\':  # only read the 4th frame for the GT mode\n        print(\'Only keep the 4th frame.\')\n        all_img_list = [v for v in all_img_list if v.endswith(\'im4.png\')]\n        keys = [v for v in keys if v.endswith(\'_4\')]\n\n    if read_all_imgs:\n        #### read all images to memory (multiprocessing)\n        dataset = {}  # store all image data. list cannot keep the order, use dict\n        print(\'Read images with multiprocessing, #thread: {} ...\'.format(n_thread))\n        pbar = util.ProgressBar(len(all_img_list))\n\n        def mycallback(arg):\n            """"""get the image data and update pbar""""""\n            key = arg[0]\n            dataset[key] = arg[1]\n            pbar.update(\'Reading {}\'.format(key))\n\n        pool = Pool(n_thread)\n        for path, key in zip(all_img_list, keys):\n            pool.apply_async(read_image_worker, args=(path, key), callback=mycallback)\n        pool.close()\n        pool.join()\n        print(\'Finish reading {} images.\\nWrite lmdb...\'.format(len(all_img_list)))\n\n    #### write data to lmdb\n    data_size_per_img = cv2.imread(all_img_list[0], cv2.IMREAD_UNCHANGED).nbytes\n    print(\'data size per image is: \', data_size_per_img)\n    data_size = data_size_per_img * len(all_img_list)\n    env = lmdb.open(lmdb_save_path, map_size=data_size * 10)\n    txn = env.begin(write=True)\n    pbar = util.ProgressBar(len(all_img_list))\n    for idx, (path, key) in enumerate(zip(all_img_list, keys)):\n        pbar.update(\'Write {}\'.format(key))\n        key_byte = key.encode(\'ascii\')\n        data = dataset[key] if read_all_imgs else cv2.imread(path, cv2.IMREAD_UNCHANGED)\n        if \'flow\' in mode:\n            H, W = data.shape\n            assert H == H_dst and W == W_dst, \'different shape.\'\n        else:\n            H, W, C = data.shape\n            assert H == H_dst and W == W_dst and C == 3, \'different shape.\'\n        txn.put(key_byte, data)\n        if not read_all_imgs and idx % BATCH == 0:\n            txn.commit()\n            txn = env.begin(write=True)\n    txn.commit()\n    env.close()\n    print(\'Finish writing lmdb.\')\n\n    #### create meta information\n    meta_info = {}\n    if mode == \'GT\':\n        meta_info[\'name\'] = \'Vimeo90K_train_GT\'\n    elif mode == \'LR\':\n        meta_info[\'name\'] = \'Vimeo90K_train_LR\'\n    elif mode == \'flow\':\n        meta_info[\'name\'] = \'Vimeo90K_train_flowx4\'\n    channel = 1 if \'flow\' in mode else 3\n    meta_info[\'resolution\'] = \'{}_{}_{}\'.format(channel, H_dst, W_dst)\n    key_set = set()\n    for key in keys:\n        if mode == \'flow\':\n            a, b, _, _ = key.split(\'_\')\n        else:\n            a, b, _ = key.split(\'_\')\n        key_set.add(\'{}_{}\'.format(a, b))\n    meta_info[\'keys\'] = list(key_set)\n    pickle.dump(meta_info, open(osp.join(lmdb_save_path, \'meta_info.pkl\'), ""wb""))\n    print(\'Finish creating lmdb meta info.\')\n\n\ndef REDS(mode):\n    """"""Create lmdb for the REDS dataset, each image with a fixed size\n    GT: [3, 720, 1280], key: 000_00000000\n    LR: [3, 180, 320], key: 000_00000000\n    key: 000_00000000\n\n    flow: downsampled flow: [3, 360, 320], keys: 000_00000005_[p2, p1, n1, n2]\n        Each flow is calculated with the GT images by PWCNet and then downsampled by 1/4\n        Flow map is quantized by mmcv and saved in png format\n    """"""\n    #### configurations\n    read_all_imgs = False  # whether real all images to memory with multiprocessing\n    # Set False for use limited memory\n    BATCH = 5000  # After BATCH images, lmdb commits, if read_all_imgs = False\n    if mode == \'train_sharp\':\n        img_folder = \'../../datasets/REDS/train_sharp\'\n        lmdb_save_path = \'../../datasets/REDS/train_sharp_wval.lmdb\'\n        H_dst, W_dst = 720, 1280\n    elif mode == \'train_sharp_bicubic\':\n        img_folder = \'../../datasets/REDS/train_sharp_bicubic\'\n        lmdb_save_path = \'../../datasets/REDS/train_sharp_bicubic_wval.lmdb\'\n        H_dst, W_dst = 180, 320\n    elif mode == \'train_blur_bicubic\':\n        img_folder = \'../../datasets/REDS/train_blur_bicubic\'\n        lmdb_save_path = \'../../datasets/REDS/train_blur_bicubic_wval.lmdb\'\n        H_dst, W_dst = 180, 320\n    elif mode == \'train_blur\':\n        img_folder = \'../../datasets/REDS/train_blur\'\n        lmdb_save_path = \'../../datasets/REDS/train_blur_wval.lmdb\'\n        H_dst, W_dst = 720, 1280\n    elif mode == \'train_blur_comp\':\n        img_folder = \'../../datasets/REDS/train_blur_comp\'\n        lmdb_save_path = \'../../datasets/REDS/train_blur_comp_wval.lmdb\'\n        H_dst, W_dst = 720, 1280\n    elif mode == \'train_sharp_flowx4\':\n        img_folder = \'../../datasets/REDS/train_sharp_flowx4\'\n        lmdb_save_path = \'../../datasets/REDS/train_sharp_flowx4.lmdb\'\n        H_dst, W_dst = 360, 320\n    n_thread = 40\n    ########################################################\n    if not lmdb_save_path.endswith(\'.lmdb\'):\n        raise ValueError(""lmdb_save_path must end with \\\'lmdb\\\'."")\n    if osp.exists(lmdb_save_path):\n        print(\'Folder [{:s}] already exists. Exit...\'.format(lmdb_save_path))\n        sys.exit(1)\n\n    #### read all the image paths to a list\n    print(\'Reading image path list ...\')\n    all_img_list = data_util._get_paths_from_images(img_folder)\n    keys = []\n    for img_path in all_img_list:\n        split_rlt = img_path.split(\'/\')\n        folder = split_rlt[-2]\n        img_name = split_rlt[-1].split(\'.png\')[0]\n        keys.append(folder + \'_\' + img_name)\n\n    if read_all_imgs:\n        #### read all images to memory (multiprocessing)\n        dataset = {}  # store all image data. list cannot keep the order, use dict\n        print(\'Read images with multiprocessing, #thread: {} ...\'.format(n_thread))\n        pbar = util.ProgressBar(len(all_img_list))\n\n        def mycallback(arg):\n            \'\'\'get the image data and update pbar\'\'\'\n            key = arg[0]\n            dataset[key] = arg[1]\n            pbar.update(\'Reading {}\'.format(key))\n\n        pool = Pool(n_thread)\n        for path, key in zip(all_img_list, keys):\n            pool.apply_async(read_image_worker, args=(path, key), callback=mycallback)\n        pool.close()\n        pool.join()\n        print(\'Finish reading {} images.\\nWrite lmdb...\'.format(len(all_img_list)))\n\n    #### create lmdb environment\n    data_size_per_img = cv2.imread(all_img_list[0], cv2.IMREAD_UNCHANGED).nbytes\n    print(\'data size per image is: \', data_size_per_img)\n    data_size = data_size_per_img * len(all_img_list)\n    env = lmdb.open(lmdb_save_path, map_size=data_size * 10)\n\n    #### write data to lmdb\n    pbar = util.ProgressBar(len(all_img_list))\n    txn = env.begin(write=True)\n    for idx, (path, key) in enumerate(zip(all_img_list, keys)):\n        pbar.update(\'Write {}\'.format(key))\n        key_byte = key.encode(\'ascii\')\n        data = dataset[key] if read_all_imgs else cv2.imread(path, cv2.IMREAD_UNCHANGED)\n        if \'flow\' in mode:\n            H, W = data.shape\n            assert H == H_dst and W == W_dst, \'different shape.\'\n        else:\n            H, W, C = data.shape\n            assert H == H_dst and W == W_dst and C == 3, \'different shape.\'\n        txn.put(key_byte, data)\n        if not read_all_imgs and idx % BATCH == 0:\n            txn.commit()\n            txn = env.begin(write=True)\n    txn.commit()\n    env.close()\n    print(\'Finish writing lmdb.\')\n\n    #### create meta information\n    meta_info = {}\n    meta_info[\'name\'] = \'REDS_{}_wval\'.format(mode)\n    channel = 1 if \'flow\' in mode else 3\n    meta_info[\'resolution\'] = \'{}_{}_{}\'.format(channel, H_dst, W_dst)\n    meta_info[\'keys\'] = keys\n    pickle.dump(meta_info, open(osp.join(lmdb_save_path, \'meta_info.pkl\'), ""wb""))\n    print(\'Finish creating lmdb meta info.\')\n\n\ndef test_lmdb(dataroot, dataset=\'REDS\'):\n    env = lmdb.open(dataroot, readonly=True, lock=False, readahead=False, meminit=False)\n    meta_info = pickle.load(open(osp.join(dataroot, \'meta_info.pkl\'), ""rb""))\n    print(\'Name: \', meta_info[\'name\'])\n    print(\'Resolution: \', meta_info[\'resolution\'])\n    print(\'# keys: \', len(meta_info[\'keys\']))\n    # read one image\n    if dataset == \'vimeo90k\':\n        key = \'00001_0001_4\'\n    else:\n        key = \'000_00000000\'\n    print(\'Reading {} for test.\'.format(key))\n    with env.begin(write=False) as txn:\n        buf = txn.get(key.encode(\'ascii\'))\n    img_flat = np.frombuffer(buf, dtype=np.uint8)\n    C, H, W = [int(s) for s in meta_info[\'resolution\'].split(\'_\')]\n    img = img_flat.reshape(H, W, C)\n    cv2.imwrite(\'test.png\', img)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
codes/data_scripts/extract_subimages.py,0,"b'""""""A multi-thread tool to crop large images to sub-images for faster IO.""""""\nimport os\nimport os.path as osp\nimport sys\nfrom multiprocessing import Pool\nimport numpy as np\nimport cv2\nfrom PIL import Image\nsys.path.append(osp.dirname(osp.dirname(osp.abspath(__file__))))\nfrom utils.util import ProgressBar  # noqa: E402\nimport data.util as data_util  # noqa: E402\n\n\ndef main():\n    mode = \'pair\'  # single (one input folder) | pair (extract corresponding GT and LR pairs)\n    opt = {}\n    opt[\'n_thread\'] = 20\n    opt[\'compression_level\'] = 3  # 3 is the default value in cv2\n    # CV_IMWRITE_PNG_COMPRESSION from 0 to 9. A higher value means a smaller size and longer\n    # compression time. If read raw images during training, use 0 for faster IO speed.\n    if mode == \'single\':\n        opt[\'input_folder\'] = \'../../datasets/DIV2K/DIV2K_train_HR\'\n        opt[\'save_folder\'] = \'../../datasets/DIV2K/DIV2K800_sub\'\n        opt[\'crop_sz\'] = 480  # the size of each sub-image\n        opt[\'step\'] = 240  # step of the sliding crop window\n        opt[\'thres_sz\'] = 48  # size threshold\n        extract_signle(opt)\n    elif mode == \'pair\':\n        GT_folder = \'../../datasets/DIV2K/DIV2K_train_HR\'\n        LR_folder = \'../../datasets/DIV2K/DIV2K_train_LR_bicubic/X4\'\n        save_GT_folder = \'../../datasets/DIV2K/DIV2K800_sub\'\n        save_LR_folder = \'../../datasets/DIV2K/DIV2K800_sub_bicLRx4\'\n        scale_ratio = 4\n        crop_sz = 480  # the size of each sub-image (GT)\n        step = 240  # step of the sliding crop window (GT)\n        thres_sz = 48  # size threshold\n        ########################################################################\n        # check that all the GT and LR images have correct scale ratio\n        img_GT_list = data_util._get_paths_from_images(GT_folder)\n        img_LR_list = data_util._get_paths_from_images(LR_folder)\n        assert len(img_GT_list) == len(img_LR_list), \'different length of GT_folder and LR_folder.\'\n        for path_GT, path_LR in zip(img_GT_list, img_LR_list):\n            img_GT = Image.open(path_GT)\n            img_LR = Image.open(path_LR)\n            w_GT, h_GT = img_GT.size\n            w_LR, h_LR = img_LR.size\n            assert w_GT / w_LR == scale_ratio, \'GT width [{:d}] is not {:d}X as LR weight [{:d}] for {:s}.\'.format(  # noqa: E501\n                w_GT, scale_ratio, w_LR, path_GT)\n            assert w_GT / w_LR == scale_ratio, \'GT width [{:d}] is not {:d}X as LR weight [{:d}] for {:s}.\'.format(  # noqa: E501\n                w_GT, scale_ratio, w_LR, path_GT)\n        # check crop size, step and threshold size\n        assert crop_sz % scale_ratio == 0, \'crop size is not {:d}X multiplication.\'.format(\n            scale_ratio)\n        assert step % scale_ratio == 0, \'step is not {:d}X multiplication.\'.format(scale_ratio)\n        assert thres_sz % scale_ratio == 0, \'thres_sz is not {:d}X multiplication.\'.format(\n            scale_ratio)\n        print(\'process GT...\')\n        opt[\'input_folder\'] = GT_folder\n        opt[\'save_folder\'] = save_GT_folder\n        opt[\'crop_sz\'] = crop_sz\n        opt[\'step\'] = step\n        opt[\'thres_sz\'] = thres_sz\n        extract_signle(opt)\n        print(\'process LR...\')\n        opt[\'input_folder\'] = LR_folder\n        opt[\'save_folder\'] = save_LR_folder\n        opt[\'crop_sz\'] = crop_sz // scale_ratio\n        opt[\'step\'] = step // scale_ratio\n        opt[\'thres_sz\'] = thres_sz // scale_ratio\n        extract_signle(opt)\n        assert len(data_util._get_paths_from_images(save_GT_folder)) == len(\n            data_util._get_paths_from_images(\n                save_LR_folder)), \'different length of save_GT_folder and save_LR_folder.\'\n    else:\n        raise ValueError(\'Wrong mode.\')\n\n\ndef extract_signle(opt):\n    input_folder = opt[\'input_folder\']\n    save_folder = opt[\'save_folder\']\n    if not osp.exists(save_folder):\n        os.makedirs(save_folder)\n        print(\'mkdir [{:s}] ...\'.format(save_folder))\n    else:\n        print(\'Folder [{:s}] already exists. Exit...\'.format(save_folder))\n        sys.exit(1)\n    img_list = data_util._get_paths_from_images(input_folder)\n\n    def update(arg):\n        pbar.update(arg)\n\n    pbar = ProgressBar(len(img_list))\n\n    pool = Pool(opt[\'n_thread\'])\n    for path in img_list:\n        pool.apply_async(worker, args=(path, opt), callback=update)\n    pool.close()\n    pool.join()\n    print(\'All subprocesses done.\')\n\n\ndef worker(path, opt):\n    crop_sz = opt[\'crop_sz\']\n    step = opt[\'step\']\n    thres_sz = opt[\'thres_sz\']\n    img_name = osp.basename(path)\n    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n\n    n_channels = len(img.shape)\n    if n_channels == 2:\n        h, w = img.shape\n    elif n_channels == 3:\n        h, w, c = img.shape\n    else:\n        raise ValueError(\'Wrong image shape - {}\'.format(n_channels))\n\n    h_space = np.arange(0, h - crop_sz + 1, step)\n    if h - (h_space[-1] + crop_sz) > thres_sz:\n        h_space = np.append(h_space, h - crop_sz)\n    w_space = np.arange(0, w - crop_sz + 1, step)\n    if w - (w_space[-1] + crop_sz) > thres_sz:\n        w_space = np.append(w_space, w - crop_sz)\n\n    index = 0\n    for x in h_space:\n        for y in w_space:\n            index += 1\n            if n_channels == 2:\n                crop_img = img[x:x + crop_sz, y:y + crop_sz]\n            else:\n                crop_img = img[x:x + crop_sz, y:y + crop_sz, :]\n            crop_img = np.ascontiguousarray(crop_img)\n            cv2.imwrite(\n                osp.join(opt[\'save_folder\'],\n                         img_name.replace(\'.png\', \'_s{:03d}.png\'.format(index))), crop_img,\n                [cv2.IMWRITE_PNG_COMPRESSION, opt[\'compression_level\']])\n    return \'Processing {:s} ...\'.format(img_name)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
codes/data_scripts/generate_mod_LR_bic.py,0,"b'import os\nimport sys\nimport cv2\nimport numpy as np\n\ntry:\n    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n    from data.util import imresize_np\nexcept ImportError:\n    pass\n\n\ndef generate_mod_LR_bic():\n    # set parameters\n    up_scale = 4\n    mod_scale = 4\n    # set data dir\n    sourcedir = \'/data/datasets/img\'\n    savedir = \'/data/datasets/mod\'\n\n    saveHRpath = os.path.join(savedir, \'HR\', \'x\' + str(mod_scale))\n    saveLRpath = os.path.join(savedir, \'LR\', \'x\' + str(up_scale))\n    saveBicpath = os.path.join(savedir, \'Bic\', \'x\' + str(up_scale))\n\n    if not os.path.isdir(sourcedir):\n        print(\'Error: No source data found\')\n        exit(0)\n    if not os.path.isdir(savedir):\n        os.mkdir(savedir)\n\n    if not os.path.isdir(os.path.join(savedir, \'HR\')):\n        os.mkdir(os.path.join(savedir, \'HR\'))\n    if not os.path.isdir(os.path.join(savedir, \'LR\')):\n        os.mkdir(os.path.join(savedir, \'LR\'))\n    if not os.path.isdir(os.path.join(savedir, \'Bic\')):\n        os.mkdir(os.path.join(savedir, \'Bic\'))\n\n    if not os.path.isdir(saveHRpath):\n        os.mkdir(saveHRpath)\n    else:\n        print(\'It will cover \' + str(saveHRpath))\n\n    if not os.path.isdir(saveLRpath):\n        os.mkdir(saveLRpath)\n    else:\n        print(\'It will cover \' + str(saveLRpath))\n\n    if not os.path.isdir(saveBicpath):\n        os.mkdir(saveBicpath)\n    else:\n        print(\'It will cover \' + str(saveBicpath))\n\n    filepaths = [f for f in os.listdir(sourcedir) if f.endswith(\'.png\')]\n    num_files = len(filepaths)\n\n    # prepare data with augementation\n    for i in range(num_files):\n        filename = filepaths[i]\n        print(\'No.{} -- Processing {}\'.format(i, filename))\n        # read image\n        image = cv2.imread(os.path.join(sourcedir, filename))\n\n        width = int(np.floor(image.shape[1] / mod_scale))\n        height = int(np.floor(image.shape[0] / mod_scale))\n        # modcrop\n        if len(image.shape) == 3:\n            image_HR = image[0:mod_scale * height, 0:mod_scale * width, :]\n        else:\n            image_HR = image[0:mod_scale * height, 0:mod_scale * width]\n        # LR\n        image_LR = imresize_np(image_HR, 1 / up_scale, True)\n        # bic\n        image_Bic = imresize_np(image_LR, up_scale, True)\n\n        cv2.imwrite(os.path.join(saveHRpath, filename), image_HR)\n        cv2.imwrite(os.path.join(saveLRpath, filename), image_LR)\n        cv2.imwrite(os.path.join(saveBicpath, filename), image_Bic)\n\n\nif __name__ == ""__main__"":\n    generate_mod_LR_bic()\n'"
codes/data_scripts/rename.py,0,"b'import os\nimport glob\n\n\ndef main():\n    folder = \'../../datasets/DIV2K/DIV2K_train_LR_bicubic/X4\'\n    DIV2K(folder)\n    print(\'Finished.\')\n\n\ndef DIV2K(path):\n    img_path_l = glob.glob(os.path.join(path, \'*\'))\n    for img_path in img_path_l:\n        new_path = img_path.replace(\'x2\', \'\').replace(\'x3\', \'\').replace(\'x4\', \'\').replace(\'x8\', \'\')\n        os.rename(img_path, new_path)\n\n\nif __name__ == ""__main__"":\n    main()'"
codes/data_scripts/test_dataloader.py,0,"b'import sys\nimport os.path as osp\nimport math\nimport torchvision.utils\n\nsys.path.append(osp.dirname(osp.dirname(osp.abspath(__file__))))\nfrom data import create_dataloader, create_dataset  # noqa: E402\nfrom utils import util  # noqa: E402\n\n\ndef main():\n    dataset = \'DIV2K800_sub\'  # REDS | Vimeo90K | DIV2K800_sub\n    opt = {}\n    opt[\'dist\'] = False\n    opt[\'gpu_ids\'] = [0]\n    if dataset == \'REDS\':\n        opt[\'name\'] = \'test_REDS\'\n        opt[\'dataroot_GT\'] = \'../../datasets/REDS/train_sharp_wval.lmdb\'\n        opt[\'dataroot_LQ\'] = \'../../datasets/REDS/train_sharp_bicubic_wval.lmdb\'\n        opt[\'mode\'] = \'REDS\'\n        opt[\'N_frames\'] = 5\n        opt[\'phase\'] = \'train\'\n        opt[\'use_shuffle\'] = True\n        opt[\'n_workers\'] = 8\n        opt[\'batch_size\'] = 16\n        opt[\'GT_size\'] = 256\n        opt[\'LQ_size\'] = 64\n        opt[\'scale\'] = 4\n        opt[\'use_flip\'] = True\n        opt[\'use_rot\'] = True\n        opt[\'interval_list\'] = [1]\n        opt[\'random_reverse\'] = False\n        opt[\'border_mode\'] = False\n        opt[\'cache_keys\'] = None\n        opt[\'data_type\'] = \'lmdb\'  # img | lmdb | mc\n    elif dataset == \'Vimeo90K\':\n        opt[\'name\'] = \'test_Vimeo90K\'\n        opt[\'dataroot_GT\'] = \'../../datasets/vimeo90k/vimeo90k_train_GT.lmdb\'\n        opt[\'dataroot_LQ\'] = \'../../datasets/vimeo90k/vimeo90k_train_LR7frames.lmdb\'\n        opt[\'mode\'] = \'Vimeo90K\'\n        opt[\'N_frames\'] = 7\n        opt[\'phase\'] = \'train\'\n        opt[\'use_shuffle\'] = True\n        opt[\'n_workers\'] = 8\n        opt[\'batch_size\'] = 16\n        opt[\'GT_size\'] = 256\n        opt[\'LQ_size\'] = 64\n        opt[\'scale\'] = 4\n        opt[\'use_flip\'] = True\n        opt[\'use_rot\'] = True\n        opt[\'interval_list\'] = [1]\n        opt[\'random_reverse\'] = False\n        opt[\'border_mode\'] = False\n        opt[\'cache_keys\'] = None\n        opt[\'data_type\'] = \'lmdb\'  # img | lmdb | mc\n    elif dataset == \'DIV2K800_sub\':\n        opt[\'name\'] = \'DIV2K800\'\n        opt[\'dataroot_GT\'] = \'../../datasets/DIV2K/DIV2K800_sub.lmdb\'\n        opt[\'dataroot_LQ\'] = \'../../datasets/DIV2K/DIV2K800_sub_bicLRx4.lmdb\'\n        opt[\'mode\'] = \'LQGT\'\n        opt[\'phase\'] = \'train\'\n        opt[\'use_shuffle\'] = True\n        opt[\'n_workers\'] = 8\n        opt[\'batch_size\'] = 16\n        opt[\'GT_size\'] = 128\n        opt[\'scale\'] = 4\n        opt[\'use_flip\'] = True\n        opt[\'use_rot\'] = True\n        opt[\'color\'] = \'RGB\'\n        opt[\'data_type\'] = \'lmdb\'  # img | lmdb\n    else:\n        raise ValueError(\'Please implement by yourself.\')\n\n    util.mkdir(\'tmp\')\n    train_set = create_dataset(opt)\n    train_loader = create_dataloader(train_set, opt, opt, None)\n    nrow = int(math.sqrt(opt[\'batch_size\']))\n    padding = 2 if opt[\'phase\'] == \'train\' else 0\n\n    print(\'start...\')\n    for i, data in enumerate(train_loader):\n        if i > 5:\n            break\n        print(i)\n        if dataset == \'REDS\' or dataset == \'Vimeo90K\':\n            LQs = data[\'LQs\']\n        else:\n            LQ = data[\'LQ\']\n        GT = data[\'GT\']\n\n        if dataset == \'REDS\' or dataset == \'Vimeo90K\':\n            for j in range(LQs.size(1)):\n                torchvision.utils.save_image(LQs[:, j, :, :, :],\n                                             \'tmp/LQ_{:03d}_{}.png\'.format(i, j), nrow=nrow,\n                                             padding=padding, normalize=False)\n        else:\n            torchvision.utils.save_image(LQ, \'tmp/LQ_{:03d}.png\'.format(i), nrow=nrow,\n                                         padding=padding, normalize=False)\n        torchvision.utils.save_image(GT, \'tmp/GT_{:03d}.png\'.format(i), nrow=nrow, padding=padding,\n                                     normalize=False)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
codes/metrics/calculate_PSNR_SSIM.py,0,"b""'''\ncalculate the PSNR and SSIM.\nsame as MATLAB's results\n'''\nimport os\nimport math\nimport numpy as np\nimport cv2\nimport glob\n\n\ndef main():\n    # Configurations\n\n    # GT - Ground-truth;\n    # Gen: Generated / Restored / Recovered images\n    folder_GT = '/mnt/SSD/xtwang/BasicSR_datasets/val_set5/Set5'\n    folder_Gen = '/home/xtwang/Projects/BasicSR/results/RRDB_PSNR_x4/set5'\n\n    crop_border = 4\n    suffix = ''  # suffix for Gen images\n    test_Y = False  # True: test Y channel only; False: test RGB channels\n\n    PSNR_all = []\n    SSIM_all = []\n    img_list = sorted(glob.glob(folder_GT + '/*'))\n\n    if test_Y:\n        print('Testing Y channel.')\n    else:\n        print('Testing RGB channels.')\n\n    for i, img_path in enumerate(img_list):\n        base_name = os.path.splitext(os.path.basename(img_path))[0]\n        im_GT = cv2.imread(img_path) / 255.\n        im_Gen = cv2.imread(os.path.join(folder_Gen, base_name + suffix + '.png')) / 255.\n\n        if test_Y and im_GT.shape[2] == 3:  # evaluate on Y channel in YCbCr color space\n            im_GT_in = bgr2ycbcr(im_GT)\n            im_Gen_in = bgr2ycbcr(im_Gen)\n        else:\n            im_GT_in = im_GT\n            im_Gen_in = im_Gen\n\n        # crop borders\n        if im_GT_in.ndim == 3:\n            cropped_GT = im_GT_in[crop_border:-crop_border, crop_border:-crop_border, :]\n            cropped_Gen = im_Gen_in[crop_border:-crop_border, crop_border:-crop_border, :]\n        elif im_GT_in.ndim == 2:\n            cropped_GT = im_GT_in[crop_border:-crop_border, crop_border:-crop_border]\n            cropped_Gen = im_Gen_in[crop_border:-crop_border, crop_border:-crop_border]\n        else:\n            raise ValueError('Wrong image dimension: {}. Should be 2 or 3.'.format(im_GT_in.ndim))\n\n        # calculate PSNR and SSIM\n        PSNR = calculate_psnr(cropped_GT * 255, cropped_Gen * 255)\n\n        SSIM = calculate_ssim(cropped_GT * 255, cropped_Gen * 255)\n        print('{:3d} - {:25}. \\tPSNR: {:.6f} dB, \\tSSIM: {:.6f}'.format(\n            i + 1, base_name, PSNR, SSIM))\n        PSNR_all.append(PSNR)\n        SSIM_all.append(SSIM)\n    print('Average: PSNR: {:.6f} dB, SSIM: {:.6f}'.format(\n        sum(PSNR_all) / len(PSNR_all),\n        sum(SSIM_all) / len(SSIM_all)))\n\n\ndef calculate_psnr(img1, img2):\n    # img1 and img2 have range [0, 255]\n    img1 = img1.astype(np.float64)\n    img2 = img2.astype(np.float64)\n    mse = np.mean((img1 - img2)**2)\n    if mse == 0:\n        return float('inf')\n    return 20 * math.log10(255.0 / math.sqrt(mse))\n\n\ndef ssim(img1, img2):\n    C1 = (0.01 * 255)**2\n    C2 = (0.03 * 255)**2\n\n    img1 = img1.astype(np.float64)\n    img2 = img2.astype(np.float64)\n    kernel = cv2.getGaussianKernel(11, 1.5)\n    window = np.outer(kernel, kernel.transpose())\n\n    mu1 = cv2.filter2D(img1, -1, window)[5:-5, 5:-5]  # valid\n    mu2 = cv2.filter2D(img2, -1, window)[5:-5, 5:-5]\n    mu1_sq = mu1**2\n    mu2_sq = mu2**2\n    mu1_mu2 = mu1 * mu2\n    sigma1_sq = cv2.filter2D(img1**2, -1, window)[5:-5, 5:-5] - mu1_sq\n    sigma2_sq = cv2.filter2D(img2**2, -1, window)[5:-5, 5:-5] - mu2_sq\n    sigma12 = cv2.filter2D(img1 * img2, -1, window)[5:-5, 5:-5] - mu1_mu2\n\n    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) *\n                                                            (sigma1_sq + sigma2_sq + C2))\n    return ssim_map.mean()\n\n\ndef calculate_ssim(img1, img2):\n    '''calculate SSIM\n    the same outputs as MATLAB's\n    img1, img2: [0, 255]\n    '''\n    if not img1.shape == img2.shape:\n        raise ValueError('Input images must have the same dimensions.')\n    if img1.ndim == 2:\n        return ssim(img1, img2)\n    elif img1.ndim == 3:\n        if img1.shape[2] == 3:\n            ssims = []\n            for i in range(3):\n                ssims.append(ssim(img1, img2))\n            return np.array(ssims).mean()\n        elif img1.shape[2] == 1:\n            return ssim(np.squeeze(img1), np.squeeze(img2))\n    else:\n        raise ValueError('Wrong input image dimensions.')\n\n\ndef bgr2ycbcr(img, only_y=True):\n    '''same as matlab rgb2ycbcr\n    only_y: only return Y channel\n    Input:\n        uint8, [0, 255]\n        float, [0, 1]\n    '''\n    in_img_type = img.dtype\n    img.astype(np.float32)\n    if in_img_type != np.uint8:\n        img *= 255.\n    # convert\n    if only_y:\n        rlt = np.dot(img, [24.966, 128.553, 65.481]) / 255.0 + 16.0\n    else:\n        rlt = np.matmul(img, [[24.966, 112.0, -18.214], [128.553, -74.203, -93.786],\n                              [65.481, -37.797, 112.0]]) / 255.0 + [16, 128, 128]\n    if in_img_type == np.uint8:\n        rlt = rlt.round()\n    else:\n        rlt /= 255.\n    return rlt.astype(in_img_type)\n\n\nif __name__ == '__main__':\n    main()\n"""
codes/models/RankSRGAN_model.py,18,"b""import logging\nfrom collections import OrderedDict\nimport torch\nimport torch.nn as nn\nfrom torch.nn.parallel import DataParallel, DistributedDataParallel\nimport models.networks as networks\nimport models.lr_scheduler as lr_scheduler\nfrom .base_model import BaseModel\nfrom models.loss import GANLoss\n\nlogger = logging.getLogger('base')\n\nclass SRGANModel(BaseModel):\n    def __init__(self, opt):\n        super(SRGANModel, self).__init__(opt)\n        if opt['dist']:\n            self.rank = torch.distributed.get_rank()\n        else:\n            self.rank = -1  # non dist training\n        train_opt = opt['train']\n\n        # define networks and load pretrained models\n        self.netG = networks.define_G(opt).to(self.device)\n        if opt['dist']:\n            self.netG = DistributedDataParallel(self.netG, device_ids=[torch.cuda.current_device()])\n        else:\n            self.netG = DataParallel(self.netG)\n        if self.is_train:\n            self.netD = networks.define_D(opt).to(self.device)\n            if opt['dist']:\n                self.netD = DistributedDataParallel(self.netD,\n                                                    device_ids=[torch.cuda.current_device()])\n            else:\n                self.netD = DataParallel(self.netD)\n\n            self.netG.train()\n            self.netD.train()\n\n        # define losses, optimizer and scheduler\n        if self.is_train:\n            # G pixel loss\n            if train_opt['pixel_weight'] > 0:\n                l_pix_type = train_opt['pixel_criterion']\n                if l_pix_type == 'l1':\n                    self.cri_pix = nn.L1Loss().to(self.device)\n                elif l_pix_type == 'l2':\n                    self.cri_pix = nn.MSELoss().to(self.device)\n                else:\n                    raise NotImplementedError('Loss type [{:s}] not recognized.'.format(l_pix_type))\n                self.l_pix_w = train_opt['pixel_weight']\n            else:\n                logger.info('Remove pixel loss.')\n                self.cri_pix = None\n\n            # G feature loss\n            if train_opt['feature_weight'] > 0:\n                l_fea_type = train_opt['feature_criterion']\n                if l_fea_type == 'l1':\n                    self.cri_fea = nn.L1Loss().to(self.device)\n                elif l_fea_type == 'l2':\n                    self.cri_fea = nn.MSELoss().to(self.device)\n                else:\n                    raise NotImplementedError('Loss type [{:s}] not recognized.'.format(l_fea_type))\n                self.l_fea_w = train_opt['feature_weight']\n            else:\n                logger.info('Remove feature loss.')\n                self.cri_fea = None\n            if self.cri_fea:  # load VGG perceptual loss\n                self.netF = networks.define_F(opt, use_bn=False).to(self.device)\n                if opt['dist']:\n                    self.netF = DistributedDataParallel(self.netF,\n                                                        device_ids=[torch.cuda.current_device()])\n                else:\n                    self.netF = DataParallel(self.netF)\n\n            # G Rank-content loss\n            if train_opt['R_weight'] > 0:\n                self.l_R_w = train_opt['R_weight'] # load rank-content loss\n                self.R_bias = train_opt['R_bias']\n                self.netR = networks.define_R(opt).to(self.device)\n                if opt['dist']:\n                    self.netR = DistributedDataParallel(self.netR,\n                                                        device_ids=[torch.cuda.current_device()])\n                else:\n                    self.netR = DataParallel(self.netR)\n            else:\n                logger.info('Remove rank-content loss.')\n\n            # GD gan loss\n            self.cri_gan = GANLoss(train_opt['gan_type'], 1.0, 0.0).to(self.device)\n            self.l_gan_w = train_opt['gan_weight']\n            # D_update_ratio and D_init_iters\n            self.D_update_ratio = train_opt['D_update_ratio'] if train_opt['D_update_ratio'] else 1\n            self.D_init_iters = train_opt['D_init_iters'] if train_opt['D_init_iters'] else 0\n\n            # optimizers\n            # G\n            wd_G = train_opt['weight_decay_G'] if train_opt['weight_decay_G'] else 0\n            optim_params = []\n            for k, v in self.netG.named_parameters():  # can optimize for a part of the model\n                if v.requires_grad:\n                    optim_params.append(v)\n                else:\n                    if self.rank <= 0:\n                        logger.warning('Params [{:s}] will not optimize.'.format(k))\n            self.optimizer_G = torch.optim.Adam(optim_params, lr=train_opt['lr_G'],\n                                                weight_decay=wd_G,\n                                                betas=(train_opt['beta1_G'], train_opt['beta2_G']))\n            self.optimizers.append(self.optimizer_G)\n            # D\n            wd_D = train_opt['weight_decay_D'] if train_opt['weight_decay_D'] else 0\n            self.optimizer_D = torch.optim.Adam(self.netD.parameters(), lr=train_opt['lr_D'],\n                                                weight_decay=wd_D,\n                                                betas=(train_opt['beta1_D'], train_opt['beta2_D']))\n            self.optimizers.append(self.optimizer_D)\n\n            # schedulers\n            if train_opt['lr_scheme'] == 'MultiStepLR':\n                for optimizer in self.optimizers:\n                    self.schedulers.append(\n                        lr_scheduler.MultiStepLR_Restart(optimizer, train_opt['lr_steps'],\n                                                         restarts=train_opt['restarts'],\n                                                         weights=train_opt['restart_weights'],\n                                                         gamma=train_opt['lr_gamma'],\n                                                         clear_state=train_opt['clear_state']))\n            elif train_opt['lr_scheme'] == 'CosineAnnealingLR_Restart':\n                for optimizer in self.optimizers:\n                    self.schedulers.append(\n                        lr_scheduler.CosineAnnealingLR_Restart(\n                            optimizer, train_opt['T_period'], eta_min=train_opt['eta_min'],\n                            restarts=train_opt['restarts'], weights=train_opt['restart_weights']))\n            else:\n                raise NotImplementedError('MultiStepLR learning rate scheme is enough.')\n\n            self.log_dict = OrderedDict()\n\n        self.print_network()  # print network\n        self.load()  # load G and D if needed\n\n    def feed_data(self, data, need_GT=True):\n        self.var_L = data['LQ'].to(self.device)  # LQ\n        if need_GT:\n            self.var_H = data['GT'].to(self.device)  # GT\n            input_ref = data['ref'] if 'ref' in data else data['GT']\n            self.var_ref = input_ref.to(self.device)\n\n    def optimize_parameters(self, step):\n        # G\n        for p in self.netD.parameters():\n            p.requires_grad = False\n\n        self.optimizer_G.zero_grad()\n        self.fake_H = self.netG(self.var_L)\n\n        l_g_total = 0\n        if step % self.D_update_ratio == 0 and step > self.D_init_iters:\n            if self.cri_pix:  # pixel loss\n                l_g_pix = self.l_pix_w * self.cri_pix(self.fake_H, self.var_H)\n                l_g_total += l_g_pix\n            if self.cri_fea:  # feature loss\n                real_fea = self.netF(self.var_H).detach()\n                fake_fea = self.netF(self.fake_H)\n                l_g_fea = self.l_fea_w * self.cri_fea(fake_fea, real_fea)\n                l_g_total += l_g_fea\n\n            pred_g_fake = self.netD(self.fake_H)\n            if self.opt['train']['gan_type'] == 'gan':\n                l_g_gan = self.l_gan_w * self.cri_gan(pred_g_fake, True)\n            elif self.opt['train']['gan_type'] == 'ragan':\n                pred_d_real = self.netD(self.var_ref).detach()\n                l_g_gan = self.l_gan_w * (\n                    self.cri_gan(pred_d_real - torch.mean(pred_g_fake), False) +\n                    self.cri_gan(pred_g_fake - torch.mean(pred_d_real), True)) / 2\n            l_g_total += l_g_gan\n\n            if self.l_R_w > 0:     # rank-content loss\n                l_g_rank = self.netR(self.fake_H)\n                l_g_rank = torch.sigmoid(l_g_rank-self.R_bias)\n                l_g_rank = torch.sum(l_g_rank)\n                l_g_rank = self.l_R_w*l_g_rank\n                l_g_total += l_g_rank\n\n            l_g_total.backward()\n            self.optimizer_G.step()\n\n        # D\n        for p in self.netD.parameters():\n            p.requires_grad = True\n\n        self.optimizer_D.zero_grad()\n        l_d_total = 0\n        pred_d_real = self.netD(self.var_ref)\n        pred_d_fake = self.netD(self.fake_H.detach())  # detach to avoid BP to G\n        if self.opt['train']['gan_type'] == 'gan':\n            l_d_real = self.cri_gan(pred_d_real, True)\n            l_d_fake = self.cri_gan(pred_d_fake, False)\n            l_d_total = l_d_real + l_d_fake\n        elif self.opt['train']['gan_type'] == 'ragan':\n            l_d_real = self.cri_gan(pred_d_real - torch.mean(pred_d_fake), True)\n            l_d_fake = self.cri_gan(pred_d_fake - torch.mean(pred_d_real), False)\n            l_d_total = (l_d_real + l_d_fake) / 2\n\n        l_d_total.backward()\n        self.optimizer_D.step()\n\n        # set log\n        if step % self.D_update_ratio == 0 and step > self.D_init_iters:\n            if self.cri_pix:\n                self.log_dict['l_g_pix'] = l_g_pix.item()\n            if self.cri_fea:\n                self.log_dict['l_g_fea'] = l_g_fea.item()\n            self.log_dict['l_g_gan'] = l_g_gan.item()\n            self.log_dict['l_g_rank'] = l_g_rank.item()\n\n        self.log_dict['l_d_real'] = l_d_real.item()\n        self.log_dict['l_d_fake'] = l_d_fake.item()\n        self.log_dict['D_real'] = torch.mean(pred_d_real.detach())\n        self.log_dict['D_fake'] = torch.mean(pred_d_fake.detach())\n\n    def test(self):\n        self.netG.eval()\n        with torch.no_grad():\n            self.fake_H = self.netG(self.var_L)\n        self.netG.train()\n\n    def get_current_log(self):\n        return self.log_dict\n\n    def get_current_visuals(self, need_GT=True):\n        out_dict = OrderedDict()\n        out_dict['LQ'] = self.var_L.detach()[0].float().cpu()\n        out_dict['rlt'] = self.fake_H.detach()[0].float().cpu()\n        if need_GT:\n            out_dict['GT'] = self.var_H.detach()[0].float().cpu()\n        return out_dict\n\n    def print_network(self):\n        # Generator\n        s, n = self.get_network_description(self.netG)\n        if isinstance(self.netG, nn.DataParallel) or isinstance(self.netG, DistributedDataParallel):\n            net_struc_str = '{} - {}'.format(self.netG.__class__.__name__,\n                                             self.netG.module.__class__.__name__)\n        else:\n            net_struc_str = '{}'.format(self.netG.__class__.__name__)\n        if self.rank <= 0:\n            logger.info('Network G structure: {}, with parameters: {:,d}'.format(net_struc_str, n))\n            logger.info(s)\n        if self.is_train:\n            # Discriminator\n            s, n = self.get_network_description(self.netD)\n            if isinstance(self.netD, nn.DataParallel) or isinstance(self.netD,\n                                                                    DistributedDataParallel):\n                net_struc_str = '{} - {}'.format(self.netD.__class__.__name__,\n                                                 self.netD.module.__class__.__name__)\n            else:\n                net_struc_str = '{}'.format(self.netD.__class__.__name__)\n            if self.rank <= 0:\n                logger.info('Network D structure: {}, with parameters: {:,d}'.format(\n                    net_struc_str, n))\n                logger.info(s)\n\n            if self.cri_fea:  # F, Perceptual Network\n                s, n = self.get_network_description(self.netF)\n                if isinstance(self.netF, nn.DataParallel) or isinstance(\n                        self.netF, DistributedDataParallel):\n                    net_struc_str = '{} - {}'.format(self.netF.__class__.__name__,\n                                                     self.netF.module.__class__.__name__)\n                else:\n                    net_struc_str = '{}'.format(self.netF.__class__.__name__)\n                if self.rank <= 0:\n                    logger.info('Network F structure: {}, with parameters: {:,d}'.format(\n                        net_struc_str, n))\n                    logger.info(s)\n\n            if self.l_R_w:  # R, Ranker Network\n                s, n = self.get_network_description(self.netR)\n                if isinstance(self.netR, nn.DataParallel) or isinstance(\n                        self.netR, DistributedDataParallel):\n                    net_struc_str = '{} - {}'.format(self.netR.__class__.__name__,\n                                                     self.netR.module.__class__.__name__)\n                else:\n                    net_struc_str = '{}'.format(self.netR.__class__.__name__)\n                if self.rank <= 0:\n                    logger.info('Network Ranker structure: {}, with parameters: {:,d}'.format(\n                        net_struc_str, n))\n                    logger.info(s)\n    def load(self):\n        load_path_G = self.opt['path']['pretrain_model_G']\n        if load_path_G is not None:\n            logger.info('Loading model for G [{:s}] ...'.format(load_path_G))\n            self.load_network(load_path_G, self.netG, self.opt['path']['strict_load'])\n\n        load_path_D = self.opt['path']['pretrain_model_D']\n        if self.opt['is_train'] and load_path_D is not None:\n            logger.info('Loading model for D [{:s}] ...'.format(load_path_D))\n            self.load_network(load_path_D, self.netD, self.opt['path']['strict_load'])\n        load_path_R = self.opt['path']['pretrain_model_R']\n        if load_path_R is not None:\n            logger.info('Loading model for R [{:s}] ...'.format(load_path_R))\n            self.load_network(load_path_R, self.netR, self.opt['path']['strict_load'])\n\n    def save(self, iter_step):\n        self.save_network(self.netG, 'G', iter_step)\n        self.save_network(self.netD, 'D', iter_step)\n"""
codes/models/Ranker_model.py,8,"b""import os\nfrom collections import OrderedDict\nimport logging\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import lr_scheduler\nfrom torch.nn.parallel import DataParallel, DistributedDataParallel\n\nimport models.networks as networks\nfrom .base_model import BaseModel\n\nlogger = logging.getLogger('base')\n\n\nclass Ranker_Model(BaseModel):\n    def name(self):\n        return 'Ranker_Model'\n\n    def __init__(self, opt):\n        super(Ranker_Model, self).__init__(opt)\n\n        if opt['dist']:\n            self.rank = torch.distributed.get_rank()\n        else:\n            self.rank = -1  # non dist training\n        train_opt = opt['train']\n\n        # define networks and load pretrained models\n        self.netR = networks.define_R(opt).to(self.device)\n        if opt['dist']:\n            self.netR = DistributedDataParallel(self.netR, device_ids=[torch.cuda.current_device()])\n        else:\n            self.netR = DataParallel(self.netR)\n        self.load()\n\n        if self.is_train:\n            self.netR.train()\n\n            # loss\n            self.RankLoss = nn.MarginRankingLoss(margin=0.5)\n            self.RankLoss.to(self.device)\n            self.L2Loss = nn.L1Loss()\n            self.L2Loss.to(self.device)\n            # optimizers\n            self.optimizers = []\n            wd_R = train_opt['weight_decay_R'] if train_opt['weight_decay_R'] else 0\n            optim_params = []\n            for k, v in self.netR.named_parameters():  # can optimize for a part of the model\n                if v.requires_grad:\n                    optim_params.append(v)\n                else:\n                    print('WARNING: params [%s] will not optimize.' % k)\n            self.optimizer_R = torch.optim.Adam(optim_params, lr=train_opt['lr_R'], weight_decay=wd_R)\n            print('Weight_decay:%f' % wd_R)\n            self.optimizers.append(self.optimizer_R)\n\n            # schedulers\n            self.schedulers = []\n            if train_opt['lr_scheme'] == 'MultiStepLR':\n                for optimizer in self.optimizers:\n                    self.schedulers.append(lr_scheduler.MultiStepLR(optimizer, \\\n                                                                    train_opt['lr_steps'], train_opt['lr_gamma']))\n            else:\n                raise NotImplementedError('MultiStepLR learning rate scheme is enough.')\n\n            self.log_dict = OrderedDict()\n\n        print('---------- Model initialized ------------------')\n        self.print_network()\n        print('-----------------------------------------------')\n\n    def feed_data(self, data, need_img2=True):\n        # input img1\n        self.input_img1 = data['img1'].to(self.device)\n\n        # label score1\n        self.label_score1 = data['score1'].to(self.device)\n\n        if need_img2:\n            # input img2\n            self.input_img2 = data['img2'].to(self.device)\n\n            # label score2\n            self.label_score2 = data['score2'].to(self.device)\n\n            # rank label\n            self.label = self.label_score1 >= self.label_score2  # get a ByteTensor\n            # transfer into FloatTensor\n            self.label = self.label.float()\n            self.label = (self.label - 0.5) * 2\n\n\n    def optimize_parameters(self, step):\n        self.optimizer_R.zero_grad()\n        self.predict_score1 = self.netR(self.input_img1)\n        self.predict_score2 = self.netR(self.input_img2)\n\n\n        self.predict_score1 = torch.clamp(self.predict_score1, min=-5, max=5)\n        self.predict_score2 = torch.clamp(self.predict_score2, min=-5, max=5)\n\n        l_rank = self.RankLoss(self.predict_score1, self.predict_score2, self.label)\n\n        l_rank.backward()\n        self.optimizer_R.step()\n\n        # set log\n        self.log_dict['l_rank'] = l_rank.item()\n\n    def test(self):\n        self.netR.eval()\n        self.predict_score1 = self.netR(self.input_img1)\n        self.netR.train()\n\n    def get_current_log(self):\n        return self.log_dict\n\n    def get_current_visuals(self, need_HR=True):\n        out_dict = OrderedDict()  # ............................\n        out_dict['predict_score1'] = self.predict_score1.data[0].float().cpu()\n\n        return out_dict\n\n    def print_network(self):\n        s, n = self.get_network_description(self.netR)\n        if isinstance(self.netR, nn.DataParallel):\n            net_struc_str = '{} - {}'.format(self.netR.__class__.__name__,\n                                             self.netR.module.__class__.__name__)\n        else:\n            net_struc_str = '{}'.format(self.netR.__class__.__name__)\n        logger.info('Network R structure: {}, with parameters: {:,d}'.format(net_struc_str, n))\n        logger.info(s)\n\n    def load(self):\n\n        load_path_R = self.opt['path']['pretrain_model_R']\n        if load_path_R is not None:\n            logger.info('Loading pretrained model for R [{:s}] ...'.format(load_path_R))\n            self.load_network(load_path_R, self.netR)\n    def save(self, iter_step):\n        self.save_network(self.netR, 'R', iter_step)\n"""
codes/models/SRGAN_model.py,16,"b""import logging\nfrom collections import OrderedDict\nimport torch\nimport torch.nn as nn\nfrom torch.nn.parallel import DataParallel, DistributedDataParallel\nimport models.networks as networks\nimport models.lr_scheduler as lr_scheduler\nfrom .base_model import BaseModel\nfrom models.loss import GANLoss\n\nlogger = logging.getLogger('base')\n\n\nclass SRGANModel(BaseModel):\n    def __init__(self, opt):\n        super(SRGANModel, self).__init__(opt)\n        if opt['dist']:\n            self.rank = torch.distributed.get_rank()\n        else:\n            self.rank = -1  # non dist training\n        train_opt = opt['train']\n\n        # define networks and load pretrained models\n        self.netG = networks.define_G(opt).to(self.device)\n        if opt['dist']:\n            self.netG = DistributedDataParallel(self.netG, device_ids=[torch.cuda.current_device()])\n        else:\n            self.netG = DataParallel(self.netG)\n        if self.is_train:\n            self.netD = networks.define_D(opt).to(self.device)\n            if opt['dist']:\n                self.netD = DistributedDataParallel(self.netD,\n                                                    device_ids=[torch.cuda.current_device()])\n            else:\n                self.netD = DataParallel(self.netD)\n\n            self.netG.train()\n            self.netD.train()\n\n        # define losses, optimizer and scheduler\n        if self.is_train:\n            # G pixel loss\n            if train_opt['pixel_weight'] > 0:\n                l_pix_type = train_opt['pixel_criterion']\n                if l_pix_type == 'l1':\n                    self.cri_pix = nn.L1Loss().to(self.device)\n                elif l_pix_type == 'l2':\n                    self.cri_pix = nn.MSELoss().to(self.device)\n                else:\n                    raise NotImplementedError('Loss type [{:s}] not recognized.'.format(l_pix_type))\n                self.l_pix_w = train_opt['pixel_weight']\n            else:\n                logger.info('Remove pixel loss.')\n                self.cri_pix = None\n\n            # G feature loss\n            if train_opt['feature_weight'] > 0:\n                l_fea_type = train_opt['feature_criterion']\n                if l_fea_type == 'l1':\n                    self.cri_fea = nn.L1Loss().to(self.device)\n                elif l_fea_type == 'l2':\n                    self.cri_fea = nn.MSELoss().to(self.device)\n                else:\n                    raise NotImplementedError('Loss type [{:s}] not recognized.'.format(l_fea_type))\n                self.l_fea_w = train_opt['feature_weight']\n            else:\n                logger.info('Remove feature loss.')\n                self.cri_fea = None\n            if self.cri_fea:  # load VGG perceptual loss\n                self.netF = networks.define_F(opt, use_bn=False).to(self.device)\n                if opt['dist']:\n                    pass  # do not need to use DistributedDataParallel for netF\n                else:\n                    self.netF = DataParallel(self.netF)\n\n            # GD gan loss\n            self.cri_gan = GANLoss(train_opt['gan_type'], 1.0, 0.0).to(self.device)\n            self.l_gan_w = train_opt['gan_weight']\n            # D_update_ratio and D_init_iters\n            self.D_update_ratio = train_opt['D_update_ratio'] if train_opt['D_update_ratio'] else 1\n            self.D_init_iters = train_opt['D_init_iters'] if train_opt['D_init_iters'] else 0\n\n            # optimizers\n            # G\n            wd_G = train_opt['weight_decay_G'] if train_opt['weight_decay_G'] else 0\n            optim_params = []\n            for k, v in self.netG.named_parameters():  # can optimize for a part of the model\n                if v.requires_grad:\n                    optim_params.append(v)\n                else:\n                    if self.rank <= 0:\n                        logger.warning('Params [{:s}] will not optimize.'.format(k))\n            self.optimizer_G = torch.optim.Adam(optim_params, lr=train_opt['lr_G'],\n                                                weight_decay=wd_G,\n                                                betas=(train_opt['beta1_G'], train_opt['beta2_G']))\n            self.optimizers.append(self.optimizer_G)\n            # D\n            wd_D = train_opt['weight_decay_D'] if train_opt['weight_decay_D'] else 0\n            self.optimizer_D = torch.optim.Adam(self.netD.parameters(), lr=train_opt['lr_D'],\n                                                weight_decay=wd_D,\n                                                betas=(train_opt['beta1_D'], train_opt['beta2_D']))\n            self.optimizers.append(self.optimizer_D)\n\n            # schedulers\n            if train_opt['lr_scheme'] == 'MultiStepLR':\n                for optimizer in self.optimizers:\n                    self.schedulers.append(\n                        lr_scheduler.MultiStepLR_Restart(optimizer, train_opt['lr_steps'],\n                                                         restarts=train_opt['restarts'],\n                                                         weights=train_opt['restart_weights'],\n                                                         gamma=train_opt['lr_gamma'],\n                                                         clear_state=train_opt['clear_state']))\n            elif train_opt['lr_scheme'] == 'CosineAnnealingLR_Restart':\n                for optimizer in self.optimizers:\n                    self.schedulers.append(\n                        lr_scheduler.CosineAnnealingLR_Restart(\n                            optimizer, train_opt['T_period'], eta_min=train_opt['eta_min'],\n                            restarts=train_opt['restarts'], weights=train_opt['restart_weights']))\n            else:\n                raise NotImplementedError('MultiStepLR learning rate scheme is enough.')\n\n            self.log_dict = OrderedDict()\n\n        self.print_network()  # print network\n        self.load()  # load G and D if needed\n\n    def feed_data(self, data, need_GT=True):\n        self.var_L = data['LQ'].to(self.device)  # LQ\n        if need_GT:\n            self.var_H = data['GT'].to(self.device)  # GT\n            input_ref = data['ref'] if 'ref' in data else data['GT']\n            self.var_ref = input_ref.to(self.device)\n\n    def optimize_parameters(self, step):\n        # G\n        for p in self.netD.parameters():\n            p.requires_grad = False\n\n        self.optimizer_G.zero_grad()\n        self.fake_H = self.netG(self.var_L)\n\n        l_g_total = 0\n        if step % self.D_update_ratio == 0 and step > self.D_init_iters:\n            if self.cri_pix:  # pixel loss\n                l_g_pix = self.l_pix_w * self.cri_pix(self.fake_H, self.var_H)\n                l_g_total += l_g_pix\n            if self.cri_fea:  # feature loss\n                real_fea = self.netF(self.var_H).detach()\n                fake_fea = self.netF(self.fake_H)\n                l_g_fea = self.l_fea_w * self.cri_fea(fake_fea, real_fea)\n                l_g_total += l_g_fea\n\n            if self.opt['train']['gan_type'] == 'gan':\n                pred_g_fake = self.netD(self.fake_H)\n                l_g_gan = self.l_gan_w * self.cri_gan(pred_g_fake, True)\n            elif self.opt['train']['gan_type'] == 'ragan':\n                pred_d_real = self.netD(self.var_ref).detach()\n                pred_g_fake = self.netD(self.fake_H)\n                l_g_gan = self.l_gan_w * (\n                    self.cri_gan(pred_d_real - torch.mean(pred_g_fake), False) +\n                    self.cri_gan(pred_g_fake - torch.mean(pred_d_real), True)) / 2\n            l_g_total += l_g_gan\n\n            l_g_total.backward()\n            self.optimizer_G.step()\n\n        # D\n        for p in self.netD.parameters():\n            p.requires_grad = True\n\n        self.optimizer_D.zero_grad()\n        if self.opt['train']['gan_type'] == 'gan':\n            # need to forward and backward separately, since batch norm statistics differ\n            # real\n            pred_d_real = self.netD(self.var_ref)\n            l_d_real = self.cri_gan(pred_d_real, True)\n            l_d_real.backward()\n            # fake\n            pred_d_fake = self.netD(self.fake_H.detach())  # detach to avoid BP to G\n            l_d_fake = self.cri_gan(pred_d_fake, False)\n            l_d_fake.backward()\n        elif self.opt['train']['gan_type'] == 'ragan':\n            # pred_d_real = self.netD(self.var_ref)\n            # pred_d_fake = self.netD(self.fake_H.detach())  # detach to avoid BP to G\n            # l_d_real = self.cri_gan(pred_d_real - torch.mean(pred_d_fake), True)\n            # l_d_fake = self.cri_gan(pred_d_fake - torch.mean(pred_d_real), False)\n            # l_d_total = (l_d_real + l_d_fake) / 2\n            # l_d_total.backward()\n            pred_d_fake = self.netD(self.fake_H.detach()).detach()\n            pred_d_real = self.netD(self.var_ref)\n            l_d_real = self.cri_gan(pred_d_real - torch.mean(pred_d_fake), True) * 0.5\n            l_d_real.backward()\n            pred_d_fake = self.netD(self.fake_H.detach())\n            l_d_fake = self.cri_gan(pred_d_fake - torch.mean(pred_d_real.detach()), False) * 0.5\n            l_d_fake.backward()\n        self.optimizer_D.step()\n\n        # set log\n        if step % self.D_update_ratio == 0 and step > self.D_init_iters:\n            if self.cri_pix:\n                self.log_dict['l_g_pix'] = l_g_pix.item()\n            if self.cri_fea:\n                self.log_dict['l_g_fea'] = l_g_fea.item()\n            self.log_dict['l_g_gan'] = l_g_gan.item()\n\n        self.log_dict['l_d_real'] = l_d_real.item()\n        self.log_dict['l_d_fake'] = l_d_fake.item()\n        self.log_dict['D_real'] = torch.mean(pred_d_real.detach())\n        self.log_dict['D_fake'] = torch.mean(pred_d_fake.detach())\n\n    def test(self):\n        self.netG.eval()\n        with torch.no_grad():\n            self.fake_H = self.netG(self.var_L)\n        self.netG.train()\n\n    def get_current_log(self):\n        return self.log_dict\n\n    def get_current_visuals(self, need_GT=True):\n        out_dict = OrderedDict()\n        out_dict['LQ'] = self.var_L.detach()[0].float().cpu()\n        out_dict['rlt'] = self.fake_H.detach()[0].float().cpu()\n        if need_GT:\n            out_dict['GT'] = self.var_H.detach()[0].float().cpu()\n        return out_dict\n\n    def print_network(self):\n        # Generator\n        s, n = self.get_network_description(self.netG)\n        if isinstance(self.netG, nn.DataParallel) or isinstance(self.netG, DistributedDataParallel):\n            net_struc_str = '{} - {}'.format(self.netG.__class__.__name__,\n                                             self.netG.module.__class__.__name__)\n        else:\n            net_struc_str = '{}'.format(self.netG.__class__.__name__)\n        if self.rank <= 0:\n            logger.info('Network G structure: {}, with parameters: {:,d}'.format(net_struc_str, n))\n            logger.info(s)\n        if self.is_train:\n            # Discriminator\n            s, n = self.get_network_description(self.netD)\n            if isinstance(self.netD, nn.DataParallel) or isinstance(self.netD,\n                                                                    DistributedDataParallel):\n                net_struc_str = '{} - {}'.format(self.netD.__class__.__name__,\n                                                 self.netD.module.__class__.__name__)\n            else:\n                net_struc_str = '{}'.format(self.netD.__class__.__name__)\n            if self.rank <= 0:\n                logger.info('Network D structure: {}, with parameters: {:,d}'.format(\n                    net_struc_str, n))\n                logger.info(s)\n\n            if self.cri_fea:  # F, Perceptual Network\n                s, n = self.get_network_description(self.netF)\n                if isinstance(self.netF, nn.DataParallel) or isinstance(\n                        self.netF, DistributedDataParallel):\n                    net_struc_str = '{} - {}'.format(self.netF.__class__.__name__,\n                                                     self.netF.module.__class__.__name__)\n                else:\n                    net_struc_str = '{}'.format(self.netF.__class__.__name__)\n                if self.rank <= 0:\n                    logger.info('Network F structure: {}, with parameters: {:,d}'.format(\n                        net_struc_str, n))\n                    logger.info(s)\n\n    def load(self):\n        load_path_G = self.opt['path']['pretrain_model_G']\n        if load_path_G is not None:\n            logger.info('Loading model for G [{:s}] ...'.format(load_path_G))\n            self.load_network(load_path_G, self.netG, self.opt['path']['strict_load'])\n        load_path_D = self.opt['path']['pretrain_model_D']\n        if self.opt['is_train'] and load_path_D is not None:\n            logger.info('Loading model for D [{:s}] ...'.format(load_path_D))\n            self.load_network(load_path_D, self.netD, self.opt['path']['strict_load'])\n\n    def save(self, iter_step):\n        self.save_network(self.netG, 'G', iter_step)\n        self.save_network(self.netD, 'D', iter_step)\n"""
codes/models/SR_model.py,9,"b""import logging\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.parallel import DataParallel, DistributedDataParallel\nimport models.networks as networks\nimport models.lr_scheduler as lr_scheduler\nfrom .base_model import BaseModel\nfrom models.loss import CharbonnierLoss\n\nlogger = logging.getLogger('base')\n\n\nclass SRModel(BaseModel):\n    def __init__(self, opt):\n        super(SRModel, self).__init__(opt)\n\n        if opt['dist']:\n            self.rank = torch.distributed.get_rank()\n        else:\n            self.rank = -1  # non dist training\n        train_opt = opt['train']\n\n        # define network and load pretrained models\n        self.netG = networks.define_G(opt).to(self.device)\n        if opt['dist']:\n            self.netG = DistributedDataParallel(self.netG, device_ids=[torch.cuda.current_device()])\n        else:\n            self.netG = DataParallel(self.netG)\n        # print network\n        self.print_network()\n        self.load()\n\n        if self.is_train:\n            self.netG.train()\n\n            # loss\n            loss_type = train_opt['pixel_criterion']\n            if loss_type == 'l1':\n                self.cri_pix = nn.L1Loss().to(self.device)\n            elif loss_type == 'l2':\n                self.cri_pix = nn.MSELoss().to(self.device)\n            elif loss_type == 'cb':\n                self.cri_pix = CharbonnierLoss().to(self.device)\n            else:\n                raise NotImplementedError('Loss type [{:s}] is not recognized.'.format(loss_type))\n            self.l_pix_w = train_opt['pixel_weight']\n\n            # optimizers\n            wd_G = train_opt['weight_decay_G'] if train_opt['weight_decay_G'] else 0\n            optim_params = []\n            for k, v in self.netG.named_parameters():  # can optimize for a part of the model\n                if v.requires_grad:\n                    optim_params.append(v)\n                else:\n                    if self.rank <= 0:\n                        logger.warning('Params [{:s}] will not optimize.'.format(k))\n            self.optimizer_G = torch.optim.Adam(optim_params, lr=train_opt['lr_G'],\n                                                weight_decay=wd_G,\n                                                betas=(train_opt['beta1'], train_opt['beta2']))\n            self.optimizers.append(self.optimizer_G)\n\n            # schedulers\n            if train_opt['lr_scheme'] == 'MultiStepLR':\n                for optimizer in self.optimizers:\n                    self.schedulers.append(\n                        lr_scheduler.MultiStepLR_Restart(optimizer, train_opt['lr_steps'],\n                                                         restarts=train_opt['restarts'],\n                                                         weights=train_opt['restart_weights'],\n                                                         gamma=train_opt['lr_gamma'],\n                                                         clear_state=train_opt['clear_state']))\n            elif train_opt['lr_scheme'] == 'CosineAnnealingLR_Restart':\n                for optimizer in self.optimizers:\n                    self.schedulers.append(\n                        lr_scheduler.CosineAnnealingLR_Restart(\n                            optimizer, train_opt['T_period'], eta_min=train_opt['eta_min'],\n                            restarts=train_opt['restarts'], weights=train_opt['restart_weights']))\n            else:\n                raise NotImplementedError('MultiStepLR learning rate scheme is enough.')\n\n            self.log_dict = OrderedDict()\n\n    def feed_data(self, data, need_GT=True):\n        self.var_L = data['LQ'].to(self.device)  # LQ\n        if need_GT:\n            self.real_H = data['GT'].to(self.device)  # GT\n\n    def optimize_parameters(self, step):\n        self.optimizer_G.zero_grad()\n        self.fake_H = self.netG(self.var_L)\n        l_pix = self.l_pix_w * self.cri_pix(self.fake_H, self.real_H)\n        l_pix.backward()\n        self.optimizer_G.step()\n\n        # set log\n        self.log_dict['l_pix'] = l_pix.item()\n\n    def test(self):\n        self.netG.eval()\n        with torch.no_grad():\n            self.fake_H = self.netG(self.var_L)\n        self.netG.train()\n\n    def test_x8(self):\n        # from https://github.com/thstkdgus35/EDSR-PyTorch\n        self.netG.eval()\n\n        def _transform(v, op):\n            # if self.precision != 'single': v = v.float()\n            v2np = v.data.cpu().numpy()\n            if op == 'v':\n                tfnp = v2np[:, :, :, ::-1].copy()\n            elif op == 'h':\n                tfnp = v2np[:, :, ::-1, :].copy()\n            elif op == 't':\n                tfnp = v2np.transpose((0, 1, 3, 2)).copy()\n\n            ret = torch.Tensor(tfnp).to(self.device)\n            # if self.precision == 'half': ret = ret.half()\n\n            return ret\n\n        lr_list = [self.var_L]\n        for tf in 'v', 'h', 't':\n            lr_list.extend([_transform(t, tf) for t in lr_list])\n        with torch.no_grad():\n            sr_list = [self.netG(aug) for aug in lr_list]\n        for i in range(len(sr_list)):\n            if i > 3:\n                sr_list[i] = _transform(sr_list[i], 't')\n            if i % 4 > 1:\n                sr_list[i] = _transform(sr_list[i], 'h')\n            if (i % 4) % 2 == 1:\n                sr_list[i] = _transform(sr_list[i], 'v')\n\n        output_cat = torch.cat(sr_list, dim=0)\n        self.fake_H = output_cat.mean(dim=0, keepdim=True)\n        self.netG.train()\n\n    def get_current_log(self):\n        return self.log_dict\n\n    def get_current_visuals(self, need_GT=True):\n        out_dict = OrderedDict()\n        out_dict['LQ'] = self.var_L.detach()[0].float().cpu()\n        out_dict['rlt'] = self.fake_H.detach()[0].float().cpu()\n        if need_GT:\n            out_dict['GT'] = self.real_H.detach()[0].float().cpu()\n        return out_dict\n\n    def print_network(self):\n        s, n = self.get_network_description(self.netG)\n        if isinstance(self.netG, nn.DataParallel) or isinstance(self.netG, DistributedDataParallel):\n            net_struc_str = '{} - {}'.format(self.netG.__class__.__name__,\n                                             self.netG.module.__class__.__name__)\n        else:\n            net_struc_str = '{}'.format(self.netG.__class__.__name__)\n        if self.rank <= 0:\n            logger.info('Network G structure: {}, with parameters: {:,d}'.format(net_struc_str, n))\n            logger.info(s)\n\n    def load(self):\n        load_path_G = self.opt['path']['pretrain_model_G']\n        if load_path_G is not None:\n            logger.info('Loading model for G [{:s}] ...'.format(load_path_G))\n            self.load_network(load_path_G, self.netG, self.opt['path']['strict_load'])\n\n    def save(self, iter_label):\n        self.save_network(self.netG, 'G', iter_label)\n"""
codes/models/__init__.py,0,"b""import logging\nlogger = logging.getLogger('base')\n\n\ndef create_model(opt):\n    model = opt['model']\n    # image restoration\n    if model == 'sr':  # PSNR-oriented super resolution\n        from .SR_model import SRModel as M\n    elif model == 'srgan':  # GAN-based super resolution, SRGAN / ESRGAN\n        from .SRGAN_model import SRGANModel as M\n    elif model == 'ranksrgan':  # GAN-based super resolution\n        from .RankSRGAN_model import SRGANModel as M\n    # Ranker\n    elif model == 'rank':\n        from .Ranker_model import Ranker_Model as M\n    else:\n        raise NotImplementedError('Model [{:s}] not recognized.'.format(model))\n    m = M(opt)\n    logger.info('Model [{:s}] is created.'.format(m.__class__.__name__))\n    return m\n"""
codes/models/base_model.py,6,"b'import os\nfrom collections import OrderedDict\nimport torch\nimport torch.nn as nn\nfrom torch.nn.parallel import DistributedDataParallel\n\n\nclass BaseModel():\n    def __init__(self, opt):\n        self.opt = opt\n        self.device = torch.device(\'cuda\' if opt[\'gpu_ids\'] is not None else \'cpu\')\n        self.is_train = opt[\'is_train\']\n        self.schedulers = []\n        self.optimizers = []\n\n    def feed_data(self, data):\n        pass\n\n    def optimize_parameters(self):\n        pass\n\n    def get_current_visuals(self):\n        pass\n\n    def get_current_losses(self):\n        pass\n\n    def print_network(self):\n        pass\n\n    def save(self, label):\n        pass\n\n    def load(self):\n        pass\n\n    def _set_lr(self, lr_groups_l):\n        """"""Set learning rate for warmup\n        lr_groups_l: list for lr_groups. each for a optimizer""""""\n        for optimizer, lr_groups in zip(self.optimizers, lr_groups_l):\n            for param_group, lr in zip(optimizer.param_groups, lr_groups):\n                param_group[\'lr\'] = lr\n\n    def _get_init_lr(self):\n        """"""Get the initial lr, which is set by the scheduler""""""\n        init_lr_groups_l = []\n        for optimizer in self.optimizers:\n            init_lr_groups_l.append([v[\'initial_lr\'] for v in optimizer.param_groups])\n        return init_lr_groups_l\n\n    def update_learning_rate(self, cur_iter, warmup_iter=-1):\n        for scheduler in self.schedulers:\n            scheduler.step()\n        # set up warm-up learning rate\n        if cur_iter < warmup_iter:\n            # get initial lr for each group\n            init_lr_g_l = self._get_init_lr()\n            # modify warming-up learning rates\n            warm_up_lr_l = []\n            for init_lr_g in init_lr_g_l:\n                warm_up_lr_l.append([v / warmup_iter * cur_iter for v in init_lr_g])\n            # set learning rate\n            self._set_lr(warm_up_lr_l)\n\n    def get_current_learning_rate(self):\n        return [param_group[\'lr\'] for param_group in self.optimizers[0].param_groups]\n\n    def get_network_description(self, network):\n        """"""Get the string and total parameters of the network""""""\n        if isinstance(network, nn.DataParallel) or isinstance(network, DistributedDataParallel):\n            network = network.module\n        return str(network), sum(map(lambda x: x.numel(), network.parameters()))\n\n    def save_network(self, network, network_label, iter_label):\n        save_filename = \'{}_{}.pth\'.format(iter_label, network_label)\n        save_path = os.path.join(self.opt[\'path\'][\'models\'], save_filename)\n        if isinstance(network, nn.DataParallel) or isinstance(network, DistributedDataParallel):\n            network = network.module\n        state_dict = network.state_dict()\n        for key, param in state_dict.items():\n            state_dict[key] = param.cpu()\n        torch.save(state_dict, save_path)\n\n    def load_network(self, load_path, network, strict=True):\n        if isinstance(network, nn.DataParallel) or isinstance(network, DistributedDataParallel):\n            network = network.module\n        load_net = torch.load(load_path)\n        load_net_clean = OrderedDict()  # remove unnecessary \'module.\'\n        for k, v in load_net.items():\n            if k.startswith(\'module.\'):\n                load_net_clean[k[7:]] = v\n            else:\n                load_net_clean[k] = v\n        network.load_state_dict(load_net_clean, strict=strict)\n\n    def save_training_state(self, epoch, iter_step):\n        """"""Save training state during training, which will be used for resuming""""""\n        state = {\'epoch\': epoch, \'iter\': iter_step, \'schedulers\': [], \'optimizers\': []}\n        for s in self.schedulers:\n            state[\'schedulers\'].append(s.state_dict())\n        for o in self.optimizers:\n            state[\'optimizers\'].append(o.state_dict())\n        save_filename = \'{}.state\'.format(iter_step)\n        save_path = os.path.join(self.opt[\'path\'][\'training_state\'], save_filename)\n        torch.save(state, save_path)\n\n    def resume_training(self, resume_state):\n        """"""Resume the optimizers and schedulers for training""""""\n        resume_optimizers = resume_state[\'optimizers\']\n        resume_schedulers = resume_state[\'schedulers\']\n        assert len(resume_optimizers) == len(self.optimizers), \'Wrong lengths of optimizers\'\n        assert len(resume_schedulers) == len(self.schedulers), \'Wrong lengths of schedulers\'\n        for i, o in enumerate(resume_optimizers):\n            self.optimizers[i].load_state_dict(o)\n        for i, s in enumerate(resume_schedulers):\n            self.schedulers[i].load_state_dict(s)\n'"
codes/models/loss.py,7,"b'import torch\nimport torch.nn as nn\n\n\nclass CharbonnierLoss(nn.Module):\n    """"""Charbonnier Loss (L1)""""""\n\n    def __init__(self, eps=1e-6):\n        super(CharbonnierLoss, self).__init__()\n        self.eps = eps\n\n    def forward(self, x, y):\n        diff = x - y\n        loss = torch.sum(torch.sqrt(diff * diff + self.eps))\n        return loss\n\n\n# Define GAN loss: [vanilla | lsgan | wgan-gp]\nclass GANLoss(nn.Module):\n    def __init__(self, gan_type, real_label_val=1.0, fake_label_val=0.0):\n        super(GANLoss, self).__init__()\n        self.gan_type = gan_type.lower()\n        self.real_label_val = real_label_val\n        self.fake_label_val = fake_label_val\n\n        if self.gan_type == \'gan\' or self.gan_type == \'ragan\':\n            self.loss = nn.BCEWithLogitsLoss()\n        elif self.gan_type == \'lsgan\':\n            self.loss = nn.MSELoss()\n        elif self.gan_type == \'wgan-gp\':\n\n            def wgan_loss(input, target):\n                # target is boolean\n                return -1 * input.mean() if target else input.mean()\n\n            self.loss = wgan_loss\n        else:\n            raise NotImplementedError(\'GAN type [{:s}] is not found\'.format(self.gan_type))\n\n    def get_target_label(self, input, target_is_real):\n        if self.gan_type == \'wgan-gp\':\n            return target_is_real\n        if target_is_real:\n            return torch.empty_like(input).fill_(self.real_label_val)\n        else:\n            return torch.empty_like(input).fill_(self.fake_label_val)\n\n    def forward(self, input, target_is_real):\n        target_label = self.get_target_label(input, target_is_real)\n        loss = self.loss(input, target_label)\n        return loss\n\n\nclass GradientPenaltyLoss(nn.Module):\n    def __init__(self, device=torch.device(\'cpu\')):\n        super(GradientPenaltyLoss, self).__init__()\n        self.register_buffer(\'grad_outputs\', torch.Tensor())\n        self.grad_outputs = self.grad_outputs.to(device)\n\n    def get_grad_outputs(self, input):\n        if self.grad_outputs.size() != input.size():\n            self.grad_outputs.resize_(input.size()).fill_(1.0)\n        return self.grad_outputs\n\n    def forward(self, interp, interp_crit):\n        grad_outputs = self.get_grad_outputs(interp_crit)\n        grad_interp = torch.autograd.grad(outputs=interp_crit, inputs=interp,\n                                          grad_outputs=grad_outputs, create_graph=True,\n                                          retain_graph=True, only_inputs=True)[0]\n        grad_interp = grad_interp.view(grad_interp.size(0), -1)\n        grad_interp_norm = grad_interp.norm(2, dim=1)\n\n        loss = ((grad_interp_norm - 1)**2).mean()\n        return loss\n'"
codes/models/lr_scheduler.py,2,"b'import math\nfrom collections import Counter\nfrom collections import defaultdict\nimport torch\nfrom torch.optim.lr_scheduler import _LRScheduler\n\n\nclass MultiStepLR_Restart(_LRScheduler):\n    def __init__(self, optimizer, milestones, restarts=None, weights=None, gamma=0.1,\n                 clear_state=False, last_epoch=-1):\n        self.milestones = Counter(milestones)\n        self.gamma = gamma\n        self.clear_state = clear_state\n        self.restarts = restarts if restarts else [0]\n        self.restarts = [v + 1 for v in self.restarts]\n        self.restart_weights = weights if weights else [1]\n        assert len(self.restarts) == len(\n            self.restart_weights), \'restarts and their weights do not match.\'\n        super(MultiStepLR_Restart, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        if self.last_epoch in self.restarts:\n            if self.clear_state:\n                self.optimizer.state = defaultdict(dict)\n            weight = self.restart_weights[self.restarts.index(self.last_epoch)]\n            return [group[\'initial_lr\'] * weight for group in self.optimizer.param_groups]\n        if self.last_epoch not in self.milestones:\n            return [group[\'lr\'] for group in self.optimizer.param_groups]\n        return [\n            group[\'lr\'] * self.gamma**self.milestones[self.last_epoch]\n            for group in self.optimizer.param_groups\n        ]\n\n\nclass CosineAnnealingLR_Restart(_LRScheduler):\n    def __init__(self, optimizer, T_period, restarts=None, weights=None, eta_min=0, last_epoch=-1):\n        self.T_period = T_period\n        self.T_max = self.T_period[0]  # current T period\n        self.eta_min = eta_min\n        self.restarts = restarts if restarts else [0]\n        self.restarts = [v + 1 for v in self.restarts]\n        self.restart_weights = weights if weights else [1]\n        self.last_restart = 0\n        assert len(self.restarts) == len(\n            self.restart_weights), \'restarts and their weights do not match.\'\n        super(CosineAnnealingLR_Restart, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        if self.last_epoch == 0:\n            return self.base_lrs\n        elif self.last_epoch in self.restarts:\n            self.last_restart = self.last_epoch\n            self.T_max = self.T_period[self.restarts.index(self.last_epoch) + 1]\n            weight = self.restart_weights[self.restarts.index(self.last_epoch)]\n            return [group[\'initial_lr\'] * weight for group in self.optimizer.param_groups]\n        elif (self.last_epoch - self.last_restart - 1 - self.T_max) % (2 * self.T_max) == 0:\n            return [\n                group[\'lr\'] + (base_lr - self.eta_min) * (1 - math.cos(math.pi / self.T_max)) / 2\n                for base_lr, group in zip(self.base_lrs, self.optimizer.param_groups)\n            ]\n        return [(1 + math.cos(math.pi * (self.last_epoch - self.last_restart) / self.T_max)) /\n                (1 + math.cos(math.pi * ((self.last_epoch - self.last_restart) - 1) / self.T_max)) *\n                (group[\'lr\'] - self.eta_min) + self.eta_min\n                for group in self.optimizer.param_groups]\n\n\nif __name__ == ""__main__"":\n    optimizer = torch.optim.Adam([torch.zeros(3, 64, 3, 3)], lr=2e-4, weight_decay=0,\n                                 betas=(0.9, 0.99))\n    ##############################\n    # MultiStepLR_Restart\n    ##############################\n    ## Original\n    lr_steps = [200000, 400000, 600000, 800000]\n    restarts = None\n    restart_weights = None\n\n    ## two\n    lr_steps = [100000, 200000, 300000, 400000, 490000, 600000, 700000, 800000, 900000, 990000]\n    restarts = [500000]\n    restart_weights = [1]\n\n    ## four\n    lr_steps = [\n        50000, 100000, 150000, 200000, 240000, 300000, 350000, 400000, 450000, 490000, 550000,\n        600000, 650000, 700000, 740000, 800000, 850000, 900000, 950000, 990000\n    ]\n    restarts = [250000, 500000, 750000]\n    restart_weights = [1, 1, 1]\n\n    scheduler = MultiStepLR_Restart(optimizer, lr_steps, restarts, restart_weights, gamma=0.5,\n                                    clear_state=False)\n\n    ##############################\n    # Cosine Annealing Restart\n    ##############################\n    ## two\n    T_period = [500000, 500000]\n    restarts = [500000]\n    restart_weights = [1]\n\n    ## four\n    T_period = [250000, 250000, 250000, 250000]\n    restarts = [250000, 500000, 750000]\n    restart_weights = [1, 1, 1]\n\n    scheduler = CosineAnnealingLR_Restart(optimizer, T_period, eta_min=1e-7, restarts=restarts,\n                                          weights=restart_weights)\n\n    ##############################\n    # Draw figure\n    ##############################\n    N_iter = 1000000\n    lr_l = list(range(N_iter))\n    for i in range(N_iter):\n        scheduler.step()\n        current_lr = optimizer.param_groups[0][\'lr\']\n        lr_l[i] = current_lr\n\n    import matplotlib as mpl\n    from matplotlib import pyplot as plt\n    import matplotlib.ticker as mtick\n    mpl.style.use(\'default\')\n    import seaborn\n    seaborn.set(style=\'whitegrid\')\n    seaborn.set_context(\'paper\')\n\n    plt.figure(1)\n    plt.subplot(111)\n    plt.ticklabel_format(style=\'sci\', axis=\'x\', scilimits=(0, 0))\n    plt.title(\'Title\', fontsize=16, color=\'k\')\n    plt.plot(list(range(N_iter)), lr_l, linewidth=1.5, label=\'learning rate scheme\')\n    legend = plt.legend(loc=\'upper right\', shadow=False)\n    ax = plt.gca()\n    labels = ax.get_xticks().tolist()\n    for k, v in enumerate(labels):\n        labels[k] = str(int(v / 1000)) + \'K\'\n    ax.set_xticklabels(labels)\n    ax.yaxis.set_major_formatter(mtick.FormatStrFormatter(\'%.1e\'))\n\n    ax.set_ylabel(\'Learning rate\')\n    ax.set_xlabel(\'Iteration\')\n    fig = plt.gcf()\n    plt.show()\n'"
codes/models/networks.py,1,"b""import torch\nimport models.archs.SRResNet_arch as SRResNet_arch\nimport models.archs.discriminator_vgg_arch as SRGAN_arch\nimport models.archs.RRDBNet_arch as RRDBNet_arch\nimport models.archs.RankSRGAN_arch as RankSRGAN_arch\n\n\n# Generator\ndef define_G(opt):\n    opt_net = opt['network_G']\n    which_model = opt_net['which_model_G']\n\n    # image restoration\n    if which_model == 'MSRResNet':\n        netG = SRResNet_arch.MSRResNet(in_nc=opt_net['in_nc'], out_nc=opt_net['out_nc'],\n                                       nf=opt_net['nf'], nb=opt_net['nb'], upscale=opt_net['scale'])\n    elif which_model == 'RRDBNet':\n        netG = RRDBNet_arch.RRDBNet(in_nc=opt_net['in_nc'], out_nc=opt_net['out_nc'],\n                                    nf=opt_net['nf'], nb=opt_net['nb'])\n    elif which_model == 'SRResNet':\n        netG = RankSRGAN_arch.SRResNet(in_nc=opt_net['in_nc'], out_nc=opt_net['out_nc'],\n                                       nf=opt_net['nf'], nb=opt_net['nb'], upscale=opt_net['scale'])\n    else:\n        raise NotImplementedError('Generator model [{:s}] not recognized'.format(which_model))\n\n    return netG\n\n\n# Discriminator\ndef define_D(opt):\n    opt_net = opt['network_D']\n    which_model = opt_net['which_model_D']\n\n    if which_model == 'discriminator_vgg_128':\n        netD = SRGAN_arch.Discriminator_VGG_128(in_nc=opt_net['in_nc'], nf=opt_net['nf'])\n    elif which_model == 'discriminator_vgg_296':\n        netD = RankSRGAN_arch.Discriminator_VGG_296(in_nc=opt_net['in_nc'], nf=opt_net['nf'])\n    else:\n        raise NotImplementedError('Discriminator model [{:s}] not recognized'.format(which_model))\n    return netD\n\n# Define network used for perceptual loss\ndef define_F(opt, use_bn=False):\n    gpu_ids = opt['gpu_ids']\n    device = torch.device('cuda' if gpu_ids else 'cpu')\n    # PyTorch pretrained VGG19-54, before ReLU.\n    if use_bn:\n        feature_layer = 49\n    else:\n        feature_layer = 34\n    netF = SRGAN_arch.VGGFeatureExtractor(feature_layer=feature_layer, use_bn=use_bn,\n                                          use_input_norm=True, device=device)\n    netF.eval()  # No need to train\n    return netF\n\n# Define network used for rank-content loss\ndef define_R(opt):\n    opt_net = opt['network_R']\n    which_model = opt_net['which_model_R']\n\n    if which_model == 'Ranker_VGG12':\n        netR = RankSRGAN_arch.Ranker_VGG12_296(in_nc=opt_net['in_nc'], nf=opt_net['nf'])\n    else:\n        raise NotImplementedError('Ranker model [{:s}] is not recognized'.format(which_model))\n\n    return netR\n"""
codes/options/__init__.py,0,b''
codes/options/options.py,0,"b""import os\nimport os.path as osp\nimport logging\nimport yaml\nfrom utils.util import OrderedYaml\nLoader, Dumper = OrderedYaml()\n\n\ndef parse(opt_path, is_train=True):\n    with open(opt_path, mode='r') as f:\n        opt = yaml.load(f, Loader=Loader)\n    # export CUDA_VISIBLE_DEVICES\n    gpu_list = ','.join(str(x) for x in opt['gpu_ids'])\n    os.environ['CUDA_VISIBLE_DEVICES'] = gpu_list\n    print('export CUDA_VISIBLE_DEVICES=' + gpu_list)\n\n    opt['is_train'] = is_train\n    if opt['distortion'] == 'sr':\n        scale = opt['scale']\n\n    # datasets\n    for phase, dataset in opt['datasets'].items():\n        phase = phase.split('_')[0]\n        dataset['phase'] = phase\n        if opt['distortion'] == 'sr':\n            dataset['scale'] = scale\n        is_lmdb = False\n        if dataset.get('dataroot_GT', None) is not None:\n            dataset['dataroot_GT'] = osp.expanduser(dataset['dataroot_GT'])\n            if dataset['dataroot_GT'].endswith('lmdb'):\n                is_lmdb = True\n        if dataset.get('dataroot_LQ', None) is not None:\n            dataset['dataroot_LQ'] = osp.expanduser(dataset['dataroot_LQ'])\n            if dataset['dataroot_LQ'].endswith('lmdb'):\n                is_lmdb = True\n        dataset['data_type'] = 'lmdb' if is_lmdb else 'img'\n        if dataset['mode'].endswith('mc'):  # for memcached\n            dataset['data_type'] = 'mc'\n            dataset['mode'] = dataset['mode'].replace('_mc', '')\n\n    # path\n    for key, path in opt['path'].items():\n        if path and key in opt['path'] and key != 'strict_load':\n            opt['path'][key] = osp.expanduser(path)\n    opt['path']['root'] = osp.abspath(osp.join(__file__, osp.pardir, osp.pardir, osp.pardir))\n    if is_train:\n        experiments_root = osp.join(opt['path']['root'], 'experiments', opt['name'])\n        opt['path']['experiments_root'] = experiments_root\n        opt['path']['models'] = osp.join(experiments_root, 'models')\n        opt['path']['training_state'] = osp.join(experiments_root, 'training_state')\n        opt['path']['log'] = experiments_root\n        opt['path']['val_images'] = osp.join(experiments_root, 'val_images')\n\n        # change some options for debug mode\n        if 'debug' in opt['name']:\n            opt['train']['val_freq'] = 8\n            opt['logger']['print_freq'] = 1\n            opt['logger']['save_checkpoint_freq'] = 8\n    else:  # test\n        results_root = osp.join(opt['path']['root'], 'results', opt['name'])\n        opt['path']['results_root'] = results_root\n        opt['path']['log'] = results_root\n\n    # network\n    if opt['distortion'] == 'sr':\n        opt['network_G']['scale'] = scale\n\n    return opt\n\n\ndef dict2str(opt, indent_l=1):\n    '''dict to string for logger'''\n    msg = ''\n    for k, v in opt.items():\n        if isinstance(v, dict):\n            msg += ' ' * (indent_l * 2) + k + ':[\\n'\n            msg += dict2str(v, indent_l + 1)\n            msg += ' ' * (indent_l * 2) + ']\\n'\n        else:\n            msg += ' ' * (indent_l * 2) + k + ': ' + str(v) + '\\n'\n    return msg\n\n\nclass NoneDict(dict):\n    def __missing__(self, key):\n        return None\n\n\n# convert to NoneDict, which return None for missing key.\ndef dict_to_nonedict(opt):\n    if isinstance(opt, dict):\n        new_opt = dict()\n        for key, sub_opt in opt.items():\n            new_opt[key] = dict_to_nonedict(sub_opt)\n        return NoneDict(**new_opt)\n    elif isinstance(opt, list):\n        return [dict_to_nonedict(sub_opt) for sub_opt in opt]\n    else:\n        return opt\n\n\ndef check_resume(opt, resume_iter):\n    '''Check resume states and pretrain_model paths'''\n    logger = logging.getLogger('base')\n    if opt['path']['resume_state']:\n        if opt['path'].get('pretrain_model_G', None) is not None or opt['path'].get(\n                'pretrain_model_D', None) is not None:\n            logger.warning('pretrain_model path will be ignored when resuming training.')\n\n        opt['path']['pretrain_model_G'] = osp.join(opt['path']['models'],\n                                                   '{}_G.pth'.format(resume_iter))\n        logger.info('Set [pretrain_model_G] to ' + opt['path']['pretrain_model_G'])\n        if 'gan' in opt['model']:\n            opt['path']['pretrain_model_D'] = osp.join(opt['path']['models'],\n                                                       '{}_D.pth'.format(resume_iter))\n            logger.info('Set [pretrain_model_D] to ' + opt['path']['pretrain_model_D'])\n"""
codes/scripts/arch_util.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\n\n\ndef initialize_weights(net_l, scale=1):\n    if not isinstance(net_l, list):\n        net_l = [net_l]\n    for net in net_l:\n        for m in net.modules():\n            if isinstance(m, nn.Conv2d):\n                init.kaiming_normal_(m.weight, a=0, mode=\'fan_in\')\n                m.weight.data *= scale  # for residual block\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                init.kaiming_normal_(m.weight, a=0, mode=\'fan_in\')\n                m.weight.data *= scale\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                init.constant_(m.weight, 1)\n                init.constant_(m.bias.data, 0.0)\n\n\ndef make_layer(block, n_layers):\n    layers = []\n    for _ in range(n_layers):\n        layers.append(block())\n    return nn.Sequential(*layers)\n\n\nclass ResidualBlock_noBN(nn.Module):\n    \'\'\'Residual block w/o BN\n    ---Conv-ReLU-Conv-+-\n     |________________|\n    \'\'\'\n\n    def __init__(self, nf=64):\n        super(ResidualBlock_noBN, self).__init__()\n        self.conv1 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n        self.conv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n\n        # initialization\n        initialize_weights([self.conv1, self.conv2], 0.1)\n\n    def forward(self, x):\n        identity = x\n        out = F.relu(self.conv1(x), inplace=True)\n        out = self.conv2(out)\n        return identity + out\n\n\ndef flow_warp(x, flow, interp_mode=\'bilinear\', padding_mode=\'zeros\'):\n    """"""Warp an image or feature map with optical flow\n    Args:\n        x (Tensor): size (N, C, H, W)\n        flow (Tensor): size (N, H, W, 2), normal value\n        interp_mode (str): \'nearest\' or \'bilinear\'\n        padding_mode (str): \'zeros\' or \'border\' or \'reflection\'\n\n    Returns:\n        Tensor: warped image or feature map\n    """"""\n    assert x.size()[-2:] == flow.size()[1:3]\n    B, C, H, W = x.size()\n    # mesh grid\n    grid_y, grid_x = torch.meshgrid(torch.arange(0, H), torch.arange(0, W))\n    grid = torch.stack((grid_x, grid_y), 2).float()  # W(x), H(y), 2\n    grid.requires_grad = False\n    grid = grid.type_as(x)\n    vgrid = grid + flow\n    # scale grid to [-1,1]\n    vgrid_x = 2.0 * vgrid[:, :, :, 0] / max(W - 1, 1) - 1.0\n    vgrid_y = 2.0 * vgrid[:, :, :, 1] / max(H - 1, 1) - 1.0\n    vgrid_scaled = torch.stack((vgrid_x, vgrid_y), dim=3)\n    output = F.grid_sample(x, vgrid_scaled, mode=interp_mode, padding_mode=padding_mode)\n    return output\n'"
codes/scripts/block.py,6,"b""from collections import OrderedDict\nimport torch\nimport torch.nn as nn\n\n####################\n# Basic blocks\n####################\n\n\ndef act(act_type, inplace=True, neg_slope=0.2, n_prelu=1):\n    # helper selecting activation\n    # neg_slope: for leakyrelu and init of prelu\n    # n_prelu: for p_relu num_parameters\n    act_type = act_type.lower()\n    if act_type == 'relu':\n        layer = nn.ReLU(inplace)\n    elif act_type == 'leakyrelu':\n        layer = nn.LeakyReLU(neg_slope, inplace)\n    elif act_type == 'prelu':\n        layer = nn.PReLU(num_parameters=n_prelu, init=neg_slope)\n    else:\n        raise NotImplementedError('activation layer [{:s}] is not found'.format(act_type))\n    return layer\n\n\ndef norm(norm_type, nc):\n    # helper selecting normalization layer\n    norm_type = norm_type.lower()\n    if norm_type == 'batch':\n        layer = nn.BatchNorm2d(nc, affine=True)\n    elif norm_type == 'instance':\n        layer = nn.InstanceNorm2d(nc, affine=False)\n    else:\n        raise NotImplementedError('normalization layer [{:s}] is not found'.format(norm_type))\n    return layer\n\n\ndef pad(pad_type, padding):\n    # helper selecting padding layer\n    # if padding is 'zero', do by conv layers\n    pad_type = pad_type.lower()\n    if padding == 0:\n        return None\n    if pad_type == 'reflect':\n        layer = nn.ReflectionPad2d(padding)\n    elif pad_type == 'replicate':\n        layer = nn.ReplicationPad2d(padding)\n    else:\n        raise NotImplementedError('padding layer [{:s}] is not implemented'.format(pad_type))\n    return layer\n\n\ndef get_valid_padding(kernel_size, dilation):\n    kernel_size = kernel_size + (kernel_size - 1) * (dilation - 1)\n    padding = (kernel_size - 1) // 2\n    return padding\n\n\nclass ConcatBlock(nn.Module):\n    # Concat the output of a submodule to its input\n    def __init__(self, submodule):\n        super(ConcatBlock, self).__init__()\n        self.sub = submodule\n\n    def forward(self, x):\n        output = torch.cat((x, self.sub(x)), dim=1)\n        return output\n\n    def __repr__(self):\n        tmpstr = 'Identity .. \\n|'\n        modstr = self.sub.__repr__().replace('\\n', '\\n|')\n        tmpstr = tmpstr + modstr\n        return tmpstr\n\n\nclass ShortcutBlock(nn.Module):\n    #Elementwise sum the output of a submodule to its input\n    def __init__(self, submodule):\n        super(ShortcutBlock, self).__init__()\n        self.sub = submodule\n\n    def forward(self, x):\n        output = x + self.sub(x)\n        return output\n\n    def __repr__(self):\n        tmpstr = 'Identity + \\n|'\n        modstr = self.sub.__repr__().replace('\\n', '\\n|')\n        tmpstr = tmpstr + modstr\n        return tmpstr\n\n\ndef sequential(*args):\n    # Flatten Sequential. It unwraps nn.Sequential.\n    if len(args) == 1:\n        if isinstance(args[0], OrderedDict):\n            raise NotImplementedError('sequential does not support OrderedDict input.')\n        return args[0]  # No sequential is needed.\n    modules = []\n    for module in args:\n        if isinstance(module, nn.Sequential):\n            for submodule in module.children():\n                modules.append(submodule)\n        elif isinstance(module, nn.Module):\n            modules.append(module)\n    return nn.Sequential(*modules)\n\n\ndef conv_block(in_nc, out_nc, kernel_size, stride=1, dilation=1, groups=1, bias=True, \\\n               pad_type='zero', norm_type=None, act_type='relu', mode='CNA'):\n    '''\n    Conv layer with padding, normalization, activation\n    mode: CNA --> Conv -> Norm -> Act\n        NAC --> Norm -> Act --> Conv (Identity Mappings in Deep Residual Networks, ECCV16)\n    '''\n    assert mode in ['CNA', 'NAC', 'CNAC'], 'Wong conv mode [{:s}]'.format(mode)\n    padding = get_valid_padding(kernel_size, dilation)\n    p = pad(pad_type, padding) if pad_type and pad_type != 'zero' else None\n    padding = padding if pad_type == 'zero' else 0\n\n    c = nn.Conv2d(in_nc, out_nc, kernel_size=kernel_size, stride=stride, padding=padding, \\\n            dilation=dilation, bias=bias, groups=groups)\n    a = act(act_type) if act_type else None\n    if 'CNA' in mode:\n        n = norm(norm_type, out_nc) if norm_type else None\n        return sequential(p, c, n, a)\n    elif mode == 'NAC':\n        if norm_type is None and act_type is not None:\n            a = act(act_type, inplace=False)\n            # Important!\n            # input----ReLU(inplace)----Conv--+----output\n            #        |________________________|\n            # inplace ReLU will modify the input, therefore wrong output\n        n = norm(norm_type, in_nc) if norm_type else None\n        return sequential(n, a, p, c)\n\n\n####################\n# Useful blocks\n####################\n\n\nclass ResNetBlock(nn.Module):\n    '''\n    ResNet Block, 3-3 style\n    with extra residual scaling used in EDSR\n    (Enhanced Deep Residual Networks for Single Image Super-Resolution, CVPRW 17)\n    '''\n\n    def __init__(self, in_nc, mid_nc, out_nc, kernel_size=3, stride=1, dilation=1, groups=1, \\\n            bias=True, pad_type='zero', norm_type=None, act_type='relu', mode='CNA', res_scale=1):\n        super(ResNetBlock, self).__init__()\n        conv0 = conv_block(in_nc, mid_nc, kernel_size, stride, dilation, groups, bias, pad_type, \\\n            norm_type, act_type, mode)\n        if mode == 'CNA':\n            act_type = None\n        if mode == 'CNAC':  # Residual path: |-CNAC-|\n            act_type = None\n            norm_type = None\n        conv1 = conv_block(mid_nc, out_nc, kernel_size, stride, dilation, groups, bias, pad_type, \\\n            norm_type, act_type, mode)\n        # if in_nc != out_nc:\n        #     self.project = conv_block(in_nc, out_nc, 1, stride, dilation, 1, bias, pad_type, \\\n        #         None, None)\n        #     print('Need a projecter in ResNetBlock.')\n        # else:\n        #     self.project = lambda x:x\n        self.res = sequential(conv0, conv1)\n        self.res_scale = res_scale\n\n    def forward(self, x):\n        res = self.res(x).mul(self.res_scale)\n        return x + res\n\n\nclass ResidualDenseBlock_5C(nn.Module):\n    '''\n    Residual Dense Block\n    style: 5 convs\n    The core module of paper: (Residual Dense Network for Image Super-Resolution, CVPR 18)\n    '''\n\n    def __init__(self, nc, kernel_size=3, gc=32, stride=1, bias=True, pad_type='zero', \\\n            norm_type=None, act_type='leakyrelu', mode='CNA'):\n        super(ResidualDenseBlock_5C, self).__init__()\n        # gc: growth channel, i.e. intermediate channels\n        self.conv1 = conv_block(nc, gc, kernel_size, stride, bias=bias, pad_type=pad_type, \\\n            norm_type=norm_type, act_type=act_type, mode=mode)\n        self.conv2 = conv_block(nc+gc, gc, kernel_size, stride, bias=bias, pad_type=pad_type, \\\n            norm_type=norm_type, act_type=act_type, mode=mode)\n        self.conv3 = conv_block(nc+2*gc, gc, kernel_size, stride, bias=bias, pad_type=pad_type, \\\n            norm_type=norm_type, act_type=act_type, mode=mode)\n        self.conv4 = conv_block(nc+3*gc, gc, kernel_size, stride, bias=bias, pad_type=pad_type, \\\n            norm_type=norm_type, act_type=act_type, mode=mode)\n        if mode == 'CNA':\n            last_act = None\n        else:\n            last_act = act_type\n        self.conv5 = conv_block(nc+4*gc, nc, 3, stride, bias=bias, pad_type=pad_type, \\\n            norm_type=norm_type, act_type=last_act, mode=mode)\n\n    def forward(self, x):\n        x1 = self.conv1(x)\n        x2 = self.conv2(torch.cat((x, x1), 1))\n        x3 = self.conv3(torch.cat((x, x1, x2), 1))\n        x4 = self.conv4(torch.cat((x, x1, x2, x3), 1))\n        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))\n        return x5.mul(0.2) + x\n\n\nclass RRDB(nn.Module):\n    '''\n    Residual in Residual Dense Block\n    (ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks)\n    '''\n\n    def __init__(self, nc, kernel_size=3, gc=32, stride=1, bias=True, pad_type='zero', \\\n            norm_type=None, act_type='leakyrelu', mode='CNA'):\n        super(RRDB, self).__init__()\n        self.RDB1 = ResidualDenseBlock_5C(nc, kernel_size, gc, stride, bias, pad_type, \\\n            norm_type, act_type, mode)\n        self.RDB2 = ResidualDenseBlock_5C(nc, kernel_size, gc, stride, bias, pad_type, \\\n            norm_type, act_type, mode)\n        self.RDB3 = ResidualDenseBlock_5C(nc, kernel_size, gc, stride, bias, pad_type, \\\n            norm_type, act_type, mode)\n\n    def forward(self, x):\n        out = self.RDB1(x)\n        out = self.RDB2(out)\n        out = self.RDB3(out)\n        return out.mul(0.2) + x\n\n\n####################\n# Upsampler\n####################\n\n\ndef pixelshuffle_block(in_nc, out_nc, upscale_factor=2, kernel_size=3, stride=1, bias=True, \\\n                        pad_type='zero', norm_type=None, act_type='relu'):\n    '''\n    Pixel shuffle layer\n    (Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional\n    Neural Network, CVPR17)\n    '''\n    conv = conv_block(in_nc, out_nc * (upscale_factor ** 2), kernel_size, stride, bias=bias, \\\n                        pad_type=pad_type, norm_type=None, act_type=None)\n    pixel_shuffle = nn.PixelShuffle(upscale_factor)\n\n    n = norm(norm_type, out_nc) if norm_type else None\n    a = act(act_type) if act_type else None\n    return sequential(conv, pixel_shuffle, n, a)\n\n\ndef upconv_blcok(in_nc, out_nc, upscale_factor=2, kernel_size=3, stride=1, bias=True, \\\n                pad_type='zero', norm_type=None, act_type='relu', mode='nearest'):\n    # Up conv\n    # described in https://distill.pub/2016/deconv-checkerboard/\n    upsample = nn.Upsample(scale_factor=upscale_factor, mode=mode)\n    conv = conv_block(in_nc, out_nc, kernel_size, stride, bias=bias, \\\n                        pad_type=pad_type, norm_type=norm_type, act_type=act_type)\n    return sequential(upsample, conv)\n"""
codes/scripts/calparameters.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchsummary import summary\nfrom collections import OrderedDict\nimport torch\nimport torch.nn as nn\nfrom . import block as B\n\nclass Discriminator_VGG_128(nn.Module):\n    def __init__(self, in_nc = 3, base_nf = 64, norm_type=\'batch\', act_type=\'leakyrelu\', mode=\'CNA\'):\n        super(Discriminator_VGG_128, self).__init__()\n        # features\n        # hxw, c\n        # 128, 64\n\n        conv0 = B.conv_block(in_nc, base_nf, kernel_size=3, norm_type=None, act_type=act_type, \\\n            mode=mode)\n        conv1 = B.conv_block(base_nf, base_nf, kernel_size=4, stride=2, norm_type=norm_type, \\\n            act_type=act_type, mode=mode)\n        # 64, 64\n        conv2 = B.conv_block(base_nf, base_nf*2, kernel_size=3, stride=1, norm_type=norm_type, \\\n            act_type=act_type, mode=mode)\n        conv3 = B.conv_block(base_nf*2, base_nf*2, kernel_size=4, stride=2, norm_type=norm_type, \\\n            act_type=act_type, mode=mode)\n        # 32, 128\n        conv4 = B.conv_block(base_nf*2, base_nf*4, kernel_size=3, stride=1, norm_type=norm_type, \\\n            act_type=act_type, mode=mode)\n        conv5 = B.conv_block(base_nf*4, base_nf*4, kernel_size=4, stride=2, norm_type=norm_type, \\\n            act_type=act_type, mode=mode)\n        # 16, 256\n        conv6 = B.conv_block(base_nf*4, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n            act_type=act_type, mode=mode)\n        conv7 = B.conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n            act_type=act_type, mode=mode)\n        # 8, 512\n        conv8 = B.conv_block(base_nf*8, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n            act_type=act_type, mode=mode)\n        conv9 = B.conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n            act_type=act_type, mode=mode)\n        # 4, 512\n        self.features = B.sequential(conv0, conv1, conv2, conv3, conv4, conv5, conv6, conv7, conv8,\\\n            conv9)\n\n        # classifier\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 9 * 9, 100), nn.LeakyReLU(0.2, True), nn.Linear(100, 1)) # patch400  12, patch 256  8  200  6 296 9\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"") # PyTorch v0.4.0\nmodel = Discriminator_VGG_128().to(device)\n\nsummary(model, (1, 28, 28))'"
codes/scripts/create_lmdb.py,0,"b'import sys\nimport os.path\nimport glob\nimport pickle\nimport lmdb\nimport cv2\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom utils.progress_bar import ProgressBar\n\n# configurations\nimg_folder = \'/sdd/BasicSR_datasets/DIV2K800/DIV2K800/*\'  # glob matching pattern\nlmdb_save_path = \'/sdd/BasicSR_datasets/DIV2K800/DIV2K800.lmdb\'  # must end with .lmdb\n\nimg_list = sorted(glob.glob(img_folder))\ndataset = []\ndata_size = 0\n\nprint(\'Read images...\')\npbar = ProgressBar(len(img_list))\nfor i, v in enumerate(img_list):\n    pbar.update(\'Read {}\'.format(v))\n    img = cv2.imread(v, cv2.IMREAD_UNCHANGED)\n    dataset.append(img)\n    data_size += img.nbytes\nenv = lmdb.open(lmdb_save_path, map_size=data_size * 10)\nprint(\'Finish reading {} images.\\nWrite lmdb...\'.format(len(img_list)))\n\npbar = ProgressBar(len(img_list))\nwith env.begin(write=True) as txn:  # txn is a Transaction object\n    for i, v in enumerate(img_list):\n        pbar.update(\'Write {}\'.format(v))\n        base_name = os.path.splitext(os.path.basename(v))[0]\n        key = base_name.encode(\'ascii\')\n        data = dataset[i]\n        if dataset[i].ndim == 2:\n            H, W = dataset[i].shape\n            C = 1\n        else:\n            H, W, C = dataset[i].shape\n        meta_key = (base_name + \'.meta\').encode(\'ascii\')\n        meta = \'{:d}, {:d}, {:d}\'.format(H, W, C)\n        # The encode is only essential in Python 3\n        txn.put(key, data)\n        txn.put(meta_key, meta.encode(\'ascii\'))\nprint(\'Finish writing lmdb.\')\n\n# create keys cache\nkeys_cache_file = os.path.join(lmdb_save_path, \'_keys_cache.p\')\nenv = lmdb.open(lmdb_save_path, readonly=True, lock=False, readahead=False, meminit=False)\nwith env.begin(write=False) as txn:\n    print(\'Create lmdb keys cache: {}\'.format(keys_cache_file))\n    keys = [key.decode(\'ascii\') for key, _ in txn.cursor()]\n    pickle.dump(keys, open(keys_cache_file, ""wb""))\nprint(\'Finish creating lmdb keys cache.\')\n'"
codes/scripts/extract_subimgs_single.py,0,"b""import os\nimport os.path\nfrom multiprocessing import Pool\nimport time\nimport numpy as np\nimport cv2\n\n\ndef main():\n    GT_dir = '/media/sdc/wlzhang/data/DIV2K_train_HR'\n    save_GT_dir = '/media/sdc/wlzhang/data/DIV2K800_sub2'\n    n_thread = 20\n\n    print('Parent process %s.' % os.getpid())\n    start = time.time()\n\n    p = Pool(n_thread)\n    # read all files to a list\n    all_files = []\n    for root, _, fnames in sorted(os.walk(GT_dir)):\n        full_path = [os.path.join(root, x) for x in fnames]\n        all_files.extend(full_path)\n    # cut into subtasks\n    def chunkify(lst, n):  # for non-continuous chunks\n        return [lst[i::n] for i in range(n)]\n\n    sub_lists = chunkify(all_files, n_thread)\n    # call workers\n    for i in range(n_thread):\n        p.apply_async(worker, args=(sub_lists[i], save_GT_dir))\n    print('Waiting for all subprocesses done...')\n    p.close()\n    p.join()\n    end = time.time()\n    print('All subprocesses done. Using time {} sec.'.format(end - start))\n\n\ndef worker(GT_paths, save_GT_dir):\n    crop_sz = 480\n    step = 240\n    thres_sz = 48\n\n    for GT_path in GT_paths:\n        base_name = os.path.basename(GT_path)\n        print(base_name, os.getpid())\n        img_GT = cv2.imread(GT_path, cv2.IMREAD_UNCHANGED)\n\n        n_channels = len(img_GT.shape)\n        if n_channels == 2:\n            h, w = img_GT.shape\n        elif n_channels == 3:\n            h, w, c = img_GT.shape\n        else:\n            raise ValueError('Wrong image shape - {}'.format(n_channels))\n\n        h_space = np.arange(0, h - crop_sz + 1, step)\n        if h - (h_space[-1] + crop_sz) > thres_sz:\n            h_space = np.append(h_space, h - crop_sz)\n        w_space = np.arange(0, w - crop_sz + 1, step)\n        if w - (w_space[-1] + crop_sz) > thres_sz:\n            w_space = np.append(w_space, w - crop_sz)\n        index = 0\n        for x in h_space:\n            for y in w_space:\n                index += 1\n                if n_channels == 2:\n                    crop_img = img_GT[x:x + crop_sz, y:y + crop_sz]\n                else:\n                    crop_img = img_GT[x:x + crop_sz, y:y + crop_sz, :]\n\n                crop_img = np.ascontiguousarray(crop_img)\n                index_str = '{:03d}'.format(index)\n                # var = np.var(crop_img / 255)\n                # if var > 0.008:\n                #     print(index_str, var)\n                cv2.imwrite(os.path.join(save_GT_dir, base_name.replace('.png', \\\n                    '_s'+index_str+'.png')), crop_img, [cv2.IMWRITE_PNG_COMPRESSION, 0])\n\n\nif __name__ == '__main__':\n    main()\n"""
codes/scripts/transfer.py,7,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchsummary import summary\nfrom collections import OrderedDict\nimport torch\nimport torch.nn as nn\nimport block as B\nimport os\nimport math\nimport functools\nimport arch_util as arch_util\n\nclass SRResNet(nn.Module):\n    def __init__(self, in_nc=3, out_nc=3, nf=64, nb=16, upscale=4, norm_type= None , act_type=\'relu\', \\\n            mode=\'CNA\', res_scale=1, upsample_mode=\'pixelshuffle\'):\n        super(SRResNet, self).__init__()\n        n_upscale = int(math.log(upscale, 2))\n        if upscale == 3:\n            n_upscale = 1\n\n        fea_conv = B.conv_block(in_nc, nf, kernel_size=3, norm_type=None, act_type=None)\n        resnet_blocks = [B.ResNetBlock(nf, nf, nf, norm_type=norm_type, act_type=act_type,\\\n            mode=mode, res_scale=res_scale) for _ in range(nb)]\n        LR_conv = B.conv_block(nf, nf, kernel_size=3, norm_type=norm_type, act_type=None, mode=mode)\n\n        if upsample_mode == \'upconv\':\n            upsample_block = B.upconv_blcok\n        elif upsample_mode == \'pixelshuffle\':\n            upsample_block = B.pixelshuffle_block\n        else:\n            raise NotImplementedError(\'upsample mode [{:s}] is not found\'.format(upsample_mode))\n        if upscale == 3:\n            upsampler = upsample_block(nf, nf, 3, act_type=act_type)\n        else:\n            upsampler = [upsample_block(nf, nf, act_type=act_type) for _ in range(n_upscale)]\n        HR_conv0 = B.conv_block(nf, nf, kernel_size=3, norm_type=None, act_type=act_type)\n        HR_conv1 = B.conv_block(nf, out_nc, kernel_size=3, norm_type=None, act_type=None)\n\n        self.model = B.sequential(fea_conv, B.ShortcutBlock(B.sequential(*resnet_blocks, LR_conv)),\\\n            *upsampler, HR_conv0, HR_conv1)\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\nclass mmsrSRResNet(nn.Module):\n    \'\'\' modified SRResNet\'\'\'\n\n    def __init__(self, in_nc=3, out_nc=3, nf=64, nb=16, upscale=4):\n        super(mmsrSRResNet, self).__init__()\n        self.upscale = upscale\n\n        self.conv_first = nn.Conv2d(in_nc, nf, 3, 1, 1, bias=True)\n        basic_block = functools.partial(arch_util.ResidualBlock_noBN, nf=nf)\n        self.recon_trunk = arch_util.make_layer(basic_block, nb)\n        self.LRconv = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n\n        # upsampling\n        if self.upscale == 2:\n            self.upconv1 = nn.Conv2d(nf, nf * 4, 3, 1, 1, bias=True)\n            self.pixel_shuffle = nn.PixelShuffle(2)\n        elif self.upscale == 3:\n            self.upconv1 = nn.Conv2d(nf, nf * 9, 3, 1, 1, bias=True)\n            self.pixel_shuffle = nn.PixelShuffle(3)\n        elif self.upscale == 4:\n            self.upconv1 = nn.Conv2d(nf, nf * 4, 3, 1, 1, bias=True)\n            self.upconv2 = nn.Conv2d(nf, nf * 4, 3, 1, 1, bias=True)\n            self.pixel_shuffle = nn.PixelShuffle(2)\n\n        self.HRconv = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n        self.conv_last = nn.Conv2d(nf, out_nc, 3, 1, 1, bias=True)\n\n        # activation function\n        self.relu = nn.ReLU(inplace=True)\n        # self.relu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n\n        # initialization\n        arch_util.initialize_weights([self.conv_first, self.upconv1, self.HRconv, self.conv_last],\n                                     0.1)\n        if self.upscale == 4:\n            arch_util.initialize_weights(self.upconv2, 0.1)\n\n    def forward(self, x):\n        fea = self.conv_first(x)\n        out = self.recon_trunk(fea)\n        out = self.LRconv(out) \n\n        if self.upscale == 4:\n            out = self.relu(self.pixel_shuffle(self.upconv1(out+fea)))\n            out = self.relu(self.pixel_shuffle(self.upconv2(out)))\n        elif self.upscale == 3 or self.upscale == 2:\n            out = self.relu(self.pixel_shuffle(self.upconv1(out+fea)))\n\n        out = self.conv_last(self.relu(self.HRconv(out)))\n\n        return out\n\n\ndef transfer_network(load_path, network,ordereddict, strict=True):\n    load_net = torch.load(load_path)\n    load_net_dict = OrderedDict()  # remove unnecessary \'module.\'\n    load_net_clean = OrderedDict()  # remove unnecessary \'module.\'\n    load_model_key = []\n    for k, v in load_net.items():\n        load_net_dict[k] = v\n        load_model_key.append(k)\n\n    i = 0\n    for param_tensor in model2.state_dict():\n            load_net_clean[param_tensor] = load_net_dict[load_model_key[i]]\n            print(\'-------\')\n            print(param_tensor)\n            print(load_model_key[i])\n            i=i+1\n    print(i)\n\n    torch.save(load_net_clean, \'/home/wlzhang/mmsr/experiments/pretrained_models/mmsr_SRResNet_pretrain.pth\')\n    network.load_state_dict(load_net_clean, strict=strict)\n\nnet_old = SRResNet()\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"") # PyTorch v0.4.0\nmodel = net_old.to(device)\n\n# print(""Model\'s state_dict:"")\n# for param_tensor in model.state_dict():\n#     print(param_tensor, ""\\t"", model.state_dict()[param_tensor].size())\n\nnet_new = mmsrSRResNet()\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"") # PyTorch v0.4.0\nmodel2 = net_new.to(device)\n\nprint(""Model2\'s state_dict:"")\nordereddict = []\nfor param_tensor in model2.state_dict():\n    ordereddict.append(param_tensor)\n    # print(param_tensor, ""\\t"", model2.state_dict()[param_tensor].size())\n\n# print(""key state_dict:"")\n# print(ordereddict)\n\ntransfer_network(\'/home/wlzhang/mmsr/experiments/pretrained_models/SRResNet_bicx4_in3nf64nb16.pth\', net_new, ordereddict)\n\n\n# print(""key:"")\n# print(ordereddict)\n# summary(model, (3, 296, 296))'"
codes/scripts/transfer_params_MSRResNet.py,2,"b""import os.path as osp\nimport sys\nimport torch\ntry:\n    sys.path.append(osp.dirname(osp.dirname(osp.abspath(__file__))))\n    import models.archs.SRResNet_arch as SRResNet_arch\nexcept ImportError:\n    pass\n\npretrained_net = torch.load('../../experiments/pretrained_models/MSRResNetx4.pth')\ncrt_model = SRResNet_arch.MSRResNet(in_nc=3, out_nc=3, nf=64, nb=16, upscale=3)\ncrt_net = crt_model.state_dict()\n\nfor k, v in crt_net.items():\n    if k in pretrained_net and 'upconv1' not in k:\n        crt_net[k] = pretrained_net[k]\n        print('replace ... ', k)\n\n# x4 -> x3\ncrt_net['upconv1.weight'][0:256, :, :, :] = pretrained_net['upconv1.weight'] / 2\ncrt_net['upconv1.weight'][256:512, :, :, :] = pretrained_net['upconv1.weight'] / 2\ncrt_net['upconv1.weight'][512:576, :, :, :] = pretrained_net['upconv1.weight'][0:64, :, :, :] / 2\ncrt_net['upconv1.bias'][0:256] = pretrained_net['upconv1.bias'] / 2\ncrt_net['upconv1.bias'][256:512] = pretrained_net['upconv1.bias'] / 2\ncrt_net['upconv1.bias'][512:576] = pretrained_net['upconv1.bias'][0:64] / 2\n\ntorch.save(crt_net, '../../experiments/pretrained_models/MSRResNetx3_ini.pth')\n"""
codes/utils/__init__.py,0,b''
codes/utils/rank_test.py,0,"b""\ndef rank_pair_test(predict_file ,label_file):\n    predict_score = {}\n    label_score = {}\n    f1 = open(predict_file ,'r')\n    f2 = open(label_file ,'r')\n\n    for line in f1.readlines():\n        line = line.strip().split()\n        img_name = line[0]\n        img_score = line[1]\n        predict_score[img_name] = float(img_score)\n\n    for line in f2.readlines():\n        line = line.strip().split()\n        img_name = line[0]\n        img_score = line[1]\n        label_score[img_name] = float(img_score)\n\n    keys_list = list(predict_score.keys())\n    keys_list.sort()\n\n    cursor = keys_list[0].split('_')[0]\n    class_num = 0\n    for key in keys_list:\n        if cursor == key.split('_')[0]:\n            class_num += 1\n        else:\n            break\n    count = 0\n    positive = 0\n    for idx in range(0 ,len(keys_list) ,class_num):\n        for i in range(idx ,idx +class_num):\n            for j in range( i +1 ,idx +class_num):\n\n                real_rank = 1 if label_score[keys_list[i]] >= label_score[keys_list[j]] else -1\n\n                predict_rank = 1 if predict_score[keys_list[i]] >= predict_score[keys_list[j]] else -1\n\n                count += 1\n                if real_rank == predict_rank:\n                    positive += 1\n\n    # print('%d/%d ' %(positive ,count))\n    accuracy = positive /count\n    # print('Aligned Pair Accuracy: %f ' %accuracy)\n\n    count1 = 1\n    count2 = 1\n    positive1 = 0\n    positive2 = 0\n\n    for idx in range(0 ,len(keys_list) ,class_num):\n\n        i = idx\n        j = i+ 1\n        real_rank = 1 if label_score[keys_list[i]] >= label_score[keys_list[j]] else -1\n\n        predict_rank = 1 if predict_score[keys_list[i]] >= predict_score[keys_list[j]] else -1\n\n        count += 1\n        if real_rank == 1:\n            count1 += 1\n            if real_rank == predict_rank:\n                positive1 += 1\n        if real_rank == -1:\n            count2 += 1\n            if real_rank == predict_rank:\n                positive2 += 1\n\n    # print('%d/%d' % (positive1, count1))\n    accuracy_esrganbig1 = positive1 / count1\n    # print('accuracy_esrganbig: %f' % accuracy_esrganbig1)\n    #\n    # print('%d/%d' % (positive2, count2))\n    accuracy_srganbig1 = positive2 / count2\n    # print('accuracy_srganbig: %f' % accuracy_srganbig1)\n    count1 = 1\n    count2 = 1\n    positive1 = 0\n    positive2 = 0\n    for idx in range(0, len(keys_list), class_num):\n\n        i = idx\n        j = i + 2\n        real_rank = 1 if label_score[keys_list[i]] >= label_score[keys_list[j]] else -1\n\n        predict_rank = 1 if predict_score[keys_list[i]] >= predict_score[keys_list[j]] else -1\n\n        count += 1\n        if real_rank == 1:\n            count1 += 1\n            if real_rank == predict_rank:\n                positive1 += 1\n        if real_rank == -1:\n            count2 += 1\n            if real_rank == predict_rank:\n                positive2 += 1\n\n    # print('%d/%d' % (positive1, count1))\n    # accuracy_esrganbig = positive1 / count1\n    # print('accuracy2: %f' % accuracy_esrganbig)\n    #\n    # print('%d/%d' % (positive2, count2))\n    # accuracy_srganbig = positive2 / count2\n    # print('accuracy2: %f' % accuracy_srganbig)\n\n    count1 = 1\n    count2 = 1\n    positive1 = 0\n    positive2 = 0\n\n    for idx in range(0, len(keys_list), class_num):\n\n        i = idx + 1\n        j = i + 1\n        real_rank = 1 if label_score[keys_list[i]] >= label_score[keys_list[j]] else -1\n        predict_rank = 1 if predict_score[keys_list[i]] >= predict_score[keys_list[j]] else -1\n\n        count += 1\n        if real_rank == 1:\n            count1 += 1\n            if real_rank == predict_rank:\n                positive1 += 1\n        if real_rank == -1:\n            count2 += 1\n            if real_rank == predict_rank:\n                positive2 += 1\n\n    # print('%d/%d' % (positive1, count1))\n    # accuracy_esrganbig = positive1 / count1\n    # print('accuracy3: %f' % accuracy_esrganbig)\n\n    # print('%d/%d' % (positive2, count2))\n    # accuracy_srganbig = positive2 / count2\n    # print('accuracy3: %f' % accuracy_srganbig)\n\n    return accuracy, accuracy_esrganbig1, accuracy_srganbig1"""
codes/utils/util.py,11,"b'import os\nimport sys\nimport time\nimport math\nimport torch.nn.functional as F\nfrom datetime import datetime\nimport random\nimport logging\nfrom collections import OrderedDict\nimport numpy as np\nimport cv2\nimport torch\nfrom torchvision.utils import make_grid\nfrom shutil import get_terminal_size\n\nimport yaml\ntry:\n    from yaml import CLoader as Loader, CDumper as Dumper\nexcept ImportError:\n    from yaml import Loader, Dumper\n\n\ndef OrderedYaml():\n    \'\'\'yaml orderedDict support\'\'\'\n    _mapping_tag = yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG\n\n    def dict_representer(dumper, data):\n        return dumper.represent_dict(data.items())\n\n    def dict_constructor(loader, node):\n        return OrderedDict(loader.construct_pairs(node))\n\n    Dumper.add_representer(OrderedDict, dict_representer)\n    Loader.add_constructor(_mapping_tag, dict_constructor)\n    return Loader, Dumper\n\n\n####################\n# miscellaneous\n####################\n\n\ndef get_timestamp():\n    return datetime.now().strftime(\'%y%m%d-%H%M%S\')\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n\ndef mkdirs(paths):\n    if isinstance(paths, str):\n        mkdir(paths)\n    else:\n        for path in paths:\n            mkdir(path)\n\n\ndef mkdir_and_rename(path):\n    if os.path.exists(path):\n        new_name = path + \'_archived_\' + get_timestamp()\n        print(\'Path already exists. Rename it to [{:s}]\'.format(new_name))\n        logger = logging.getLogger(\'base\')\n        logger.info(\'Path already exists. Rename it to [{:s}]\'.format(new_name))\n        os.rename(path, new_name)\n    os.makedirs(path)\n\n\ndef set_random_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef setup_logger(logger_name, root, phase, level=logging.INFO, screen=False, tofile=False):\n    \'\'\'set up logger\'\'\'\n    lg = logging.getLogger(logger_name)\n    formatter = logging.Formatter(\'%(asctime)s.%(msecs)03d - %(levelname)s: %(message)s\',\n                                  datefmt=\'%y-%m-%d %H:%M:%S\')\n    lg.setLevel(level)\n    if tofile:\n        log_file = os.path.join(root, phase + \'_{}.log\'.format(get_timestamp()))\n        fh = logging.FileHandler(log_file, mode=\'w\')\n        fh.setFormatter(formatter)\n        lg.addHandler(fh)\n    if screen:\n        sh = logging.StreamHandler()\n        sh.setFormatter(formatter)\n        lg.addHandler(sh)\n\n\n####################\n# image convert\n####################\ndef crop_border(img_list, crop_border):\n    """"""Crop borders of images\n    Args:\n        img_list (list [Numpy]): HWC\n        crop_border (int): crop border for each end of height and weight\n\n    Returns:\n        (list [Numpy]): cropped image list\n    """"""\n    if crop_border == 0:\n        return img_list\n    else:\n        return [v[crop_border:-crop_border, crop_border:-crop_border] for v in img_list]\n\n\ndef tensor2img(tensor, out_type=np.uint8, min_max=(0, 1)):\n    \'\'\'\n    Converts a torch Tensor into an image Numpy array\n    Input: 4D(B,(3/1),H,W), 3D(C,H,W), or 2D(H,W), any range, RGB channel order\n    Output: 3D(H,W,C) or 2D(H,W), [0,255], np.uint8 (default)\n    \'\'\'\n    tensor = tensor.squeeze().float().cpu().clamp_(*min_max)  # clamp\n    tensor = (tensor - min_max[0]) / (min_max[1] - min_max[0])  # to range [0,1]\n    n_dim = tensor.dim()\n    if n_dim == 4:\n        n_img = len(tensor)\n        img_np = make_grid(tensor, nrow=int(math.sqrt(n_img)), normalize=False).numpy()\n        img_np = np.transpose(img_np[[2, 1, 0], :, :], (1, 2, 0))  # HWC, BGR\n    elif n_dim == 3:\n        img_np = tensor.numpy()\n        img_np = np.transpose(img_np[[2, 1, 0], :, :], (1, 2, 0))  # HWC, BGR\n    elif n_dim == 2:\n        img_np = tensor.numpy()\n    else:\n        raise TypeError(\n            \'Only support 4D, 3D and 2D tensor. But received with dimension: {:d}\'.format(n_dim))\n    if out_type == np.uint8:\n        img_np = (img_np * 255.0).round()\n        # Important. Unlike matlab, numpy.unit8() WILL NOT round by default.\n    return img_np.astype(out_type)\n\n\ndef save_img(img, img_path, mode=\'RGB\'):\n    cv2.imwrite(img_path, img)\n\n\ndef DUF_downsample(x, scale=4):\n    """"""Downsamping with Gaussian kernel used in the DUF official code\n\n    Args:\n        x (Tensor, [B, T, C, H, W]): frames to be downsampled.\n        scale (int): downsampling factor: 2 | 3 | 4.\n    """"""\n\n    assert scale in [2, 3, 4], \'Scale [{}] is not supported\'.format(scale)\n\n    def gkern(kernlen=13, nsig=1.6):\n        import scipy.ndimage.filters as fi\n        inp = np.zeros((kernlen, kernlen))\n        # set element at the middle to one, a dirac delta\n        inp[kernlen // 2, kernlen // 2] = 1\n        # gaussian-smooth the dirac, resulting in a gaussian filter mask\n        return fi.gaussian_filter(inp, nsig)\n\n    B, T, C, H, W = x.size()\n    x = x.view(-1, 1, H, W)\n    pad_w, pad_h = 6 + scale * 2, 6 + scale * 2  # 6 is the pad of the gaussian filter\n    r_h, r_w = 0, 0\n    if scale == 3:\n        r_h = 3 - (H % 3)\n        r_w = 3 - (W % 3)\n    x = F.pad(x, [pad_w, pad_w + r_w, pad_h, pad_h + r_h], \'reflect\')\n\n    gaussian_filter = torch.from_numpy(gkern(13, 0.4 * scale)).type_as(x).unsqueeze(0).unsqueeze(0)\n    x = F.conv2d(x, gaussian_filter, stride=scale)\n    x = x[:, :, 2:-2, 2:-2]\n    x = x.view(B, T, C, x.size(2), x.size(3))\n    return x\n\n\ndef single_forward(model, inp):\n    """"""PyTorch model forward (single test), it is just a simple warpper\n    Args:\n        model (PyTorch model)\n        inp (Tensor): inputs defined by the model\n\n    Returns:\n        output (Tensor): outputs of the model. float, in CPU\n    """"""\n    with torch.no_grad():\n        model_output = model(inp)\n        if isinstance(model_output, list) or isinstance(model_output, tuple):\n            output = model_output[0]\n        else:\n            output = model_output\n    output = output.data.float().cpu()\n    return output\n\n\ndef flipx4_forward(model, inp):\n    """"""Flip testing with X4 self ensemble, i.e., normal, flip H, flip W, flip H and W\n    Args:\n        model (PyTorch model)\n        inp (Tensor): inputs defined by the model\n\n    Returns:\n        output (Tensor): outputs of the model. float, in CPU\n    """"""\n    # normal\n    output_f = single_forward(model, inp)\n\n    # flip W\n    output = single_forward(model, torch.flip(inp, (-1, )))\n    output_f = output_f + torch.flip(output, (-1, ))\n    # flip H\n    output = single_forward(model, torch.flip(inp, (-2, )))\n    output_f = output_f + torch.flip(output, (-2, ))\n    # flip both H and W\n    output = single_forward(model, torch.flip(inp, (-2, -1)))\n    output_f = output_f + torch.flip(output, (-2, -1))\n\n    return output_f / 4\n\n\n####################\n# metric\n####################\n\n\ndef calculate_psnr(img1, img2):\n    # img1 and img2 have range [0, 255]\n    img1 = img1.astype(np.float64)\n    img2 = img2.astype(np.float64)\n    mse = np.mean((img1 - img2)**2)\n    if mse == 0:\n        return float(\'inf\')\n    return 20 * math.log10(255.0 / math.sqrt(mse))\n\n\ndef ssim(img1, img2):\n    C1 = (0.01 * 255)**2\n    C2 = (0.03 * 255)**2\n\n    img1 = img1.astype(np.float64)\n    img2 = img2.astype(np.float64)\n    kernel = cv2.getGaussianKernel(11, 1.5)\n    window = np.outer(kernel, kernel.transpose())\n\n    mu1 = cv2.filter2D(img1, -1, window)[5:-5, 5:-5]  # valid\n    mu2 = cv2.filter2D(img2, -1, window)[5:-5, 5:-5]\n    mu1_sq = mu1**2\n    mu2_sq = mu2**2\n    mu1_mu2 = mu1 * mu2\n    sigma1_sq = cv2.filter2D(img1**2, -1, window)[5:-5, 5:-5] - mu1_sq\n    sigma2_sq = cv2.filter2D(img2**2, -1, window)[5:-5, 5:-5] - mu2_sq\n    sigma12 = cv2.filter2D(img1 * img2, -1, window)[5:-5, 5:-5] - mu1_mu2\n\n    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) *\n                                                            (sigma1_sq + sigma2_sq + C2))\n    return ssim_map.mean()\n\n\ndef calculate_ssim(img1, img2):\n    \'\'\'calculate SSIM\n    the same outputs as MATLAB\'s\n    img1, img2: [0, 255]\n    \'\'\'\n    if not img1.shape == img2.shape:\n        raise ValueError(\'Input images must have the same dimensions.\')\n    if img1.ndim == 2:\n        return ssim(img1, img2)\n    elif img1.ndim == 3:\n        if img1.shape[2] == 3:\n            ssims = []\n            for i in range(3):\n                ssims.append(ssim(img1, img2))\n            return np.array(ssims).mean()\n        elif img1.shape[2] == 1:\n            return ssim(np.squeeze(img1), np.squeeze(img2))\n    else:\n        raise ValueError(\'Wrong input image dimensions.\')\n\n\nclass ProgressBar(object):\n    \'\'\'A progress bar which can print the progress\n    modified from https://github.com/hellock/cvbase/blob/master/cvbase/progress.py\n    \'\'\'\n\n    def __init__(self, task_num=0, bar_width=50, start=True):\n        self.task_num = task_num\n        max_bar_width = self._get_max_bar_width()\n        self.bar_width = (bar_width if bar_width <= max_bar_width else max_bar_width)\n        self.completed = 0\n        if start:\n            self.start()\n\n    def _get_max_bar_width(self):\n        terminal_width, _ = get_terminal_size()\n        max_bar_width = min(int(terminal_width * 0.6), terminal_width - 50)\n        if max_bar_width < 10:\n            print(\'terminal width is too small ({}), please consider widen the terminal for better \'\n                  \'progressbar visualization\'.format(terminal_width))\n            max_bar_width = 10\n        return max_bar_width\n\n    def start(self):\n        if self.task_num > 0:\n            sys.stdout.write(\'[{}] 0/{}, elapsed: 0s, ETA:\\n{}\\n\'.format(\n                \' \' * self.bar_width, self.task_num, \'Start...\'))\n        else:\n            sys.stdout.write(\'completed: 0, elapsed: 0s\')\n        sys.stdout.flush()\n        self.start_time = time.time()\n\n    def update(self, msg=\'In progress...\'):\n        self.completed += 1\n        elapsed = time.time() - self.start_time\n        fps = self.completed / elapsed\n        if self.task_num > 0:\n            percentage = self.completed / float(self.task_num)\n            eta = int(elapsed * (1 - percentage) / percentage + 0.5)\n            mark_width = int(self.bar_width * percentage)\n            bar_chars = \'>\' * mark_width + \'-\' * (self.bar_width - mark_width)\n            sys.stdout.write(\'\\033[2F\')  # cursor up 2 lines\n            sys.stdout.write(\'\\033[J\')  # clean the output (remove extra chars since last display)\n            sys.stdout.write(\'[{}] {}/{}, {:.1f} task/s, elapsed: {}s, ETA: {:5}s\\n{}\\n\'.format(\n                bar_chars, self.completed, self.task_num, fps, int(elapsed + 0.5), eta, msg))\n        else:\n            sys.stdout.write(\'completed: {}, elapsed: {}s, {:.1f} tasks/s\'.format(\n                self.completed, int(elapsed + 0.5), fps))\n        sys.stdout.flush()\n'"
datasets/generate_rankdataset/move_valid.py,0,"b'import os, random, shutil\nLevel1_patchsave_path = \'/home/wlzhang/RankSRGAN/data/Rank_dataset_test/DF2K_train_patch_esrgan/\';\nLevel2_patchsave_path = \'/home/wlzhang/RankSRGAN/data/Rank_dataset_test/DF2K_train_patch_srgan/\';\nLevel3_patchsave_path = \'/home/wlzhang/RankSRGAN/data/Rank_dataset_test/DF2K_train_patch_srres/\';\n\nLevel1_valid_patchsave_path = \'/home/wlzhang/RankSRGAN/data/Rank_dataset_test/DF2K_valid_patch_esrgan/\';\nLevel2_valid_patchsave_path = \'/home/wlzhang/RankSRGAN/data/Rank_dataset_test/DF2K_valid_patch_srgan/\';\nLevel3_valid_patchsave_path = \'/home/wlzhang/RankSRGAN/data/Rank_dataset_test/DF2K_valid_patch_srres/\';\n\nif not os.path.exists(Level1_valid_patchsave_path):\n\tos.makedirs(Level1_valid_patchsave_path)\nelse:\n\tprint(\'exists\')\n\nif not os.path.exists(Level2_valid_patchsave_path):\n\tos.makedirs(Level2_valid_patchsave_path)\nelse:\n\tprint(\'exists\')\n\nif not os.path.exists(Level3_valid_patchsave_path):\n\tos.makedirs(Level3_valid_patchsave_path)\nelse:\n\tprint(\'exists\')\n\npathDir = os.listdir(Level1_patchsave_path)    #\xe5\x8f\x96\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe5\x8e\x9f\xe5\xa7\x8b\xe8\xb7\xaf\xe5\xbe\x84\nfilenumber=len(pathDir)\nrate=0.1\npicknumber=int(filenumber*rate)\n\nsample = random.sample(pathDir, picknumber)\n\n\nfor name in sample:\n\n\tname = """".join(name)\n\tname = name.split(\'_\')\n\tprint(name[0])\n\n\tshutil.move(Level1_patchsave_path+name[0]+\'_esrgan.png\', Level1_valid_patchsave_path)\n\tshutil.move(Level2_patchsave_path+name[0]+\'_srgan.png\', Level2_valid_patchsave_path)\n\tshutil.move(Level3_patchsave_path+name[0]+\'_srres.png\', Level3_valid_patchsave_path)\n'"
codes/models/archs/RRDBNet_arch.py,6,"b""import functools\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport models.archs.arch_util as arch_util\n\n\nclass ResidualDenseBlock_5C(nn.Module):\n    def __init__(self, nf=64, gc=32, bias=True):\n        super(ResidualDenseBlock_5C, self).__init__()\n        # gc: growth channel, i.e. intermediate channels\n        self.conv1 = nn.Conv2d(nf, gc, 3, 1, 1, bias=bias)\n        self.conv2 = nn.Conv2d(nf + gc, gc, 3, 1, 1, bias=bias)\n        self.conv3 = nn.Conv2d(nf + 2 * gc, gc, 3, 1, 1, bias=bias)\n        self.conv4 = nn.Conv2d(nf + 3 * gc, gc, 3, 1, 1, bias=bias)\n        self.conv5 = nn.Conv2d(nf + 4 * gc, nf, 3, 1, 1, bias=bias)\n        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n\n        # initialization\n        arch_util.initialize_weights([self.conv1, self.conv2, self.conv3, self.conv4, self.conv5],\n                                     0.1)\n\n    def forward(self, x):\n        x1 = self.lrelu(self.conv1(x))\n        x2 = self.lrelu(self.conv2(torch.cat((x, x1), 1)))\n        x3 = self.lrelu(self.conv3(torch.cat((x, x1, x2), 1)))\n        x4 = self.lrelu(self.conv4(torch.cat((x, x1, x2, x3), 1)))\n        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))\n        return x5 * 0.2 + x\n\n\nclass RRDB(nn.Module):\n    '''Residual in Residual Dense Block'''\n\n    def __init__(self, nf, gc=32):\n        super(RRDB, self).__init__()\n        self.RDB1 = ResidualDenseBlock_5C(nf, gc)\n        self.RDB2 = ResidualDenseBlock_5C(nf, gc)\n        self.RDB3 = ResidualDenseBlock_5C(nf, gc)\n\n    def forward(self, x):\n        out = self.RDB1(x)\n        out = self.RDB2(out)\n        out = self.RDB3(out)\n        return out * 0.2 + x\n\n\nclass RRDBNet(nn.Module):\n    def __init__(self, in_nc, out_nc, nf, nb, gc=32):\n        super(RRDBNet, self).__init__()\n        RRDB_block_f = functools.partial(RRDB, nf=nf, gc=gc)\n\n        self.conv_first = nn.Conv2d(in_nc, nf, 3, 1, 1, bias=True)\n        self.RRDB_trunk = arch_util.make_layer(RRDB_block_f, nb)\n        self.trunk_conv = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n        #### upsampling\n        self.upconv1 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n        self.upconv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n        self.HRconv = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n        self.conv_last = nn.Conv2d(nf, out_nc, 3, 1, 1, bias=True)\n\n        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n\n    def forward(self, x):\n        fea = self.conv_first(x)\n        trunk = self.trunk_conv(self.RRDB_trunk(fea))\n        fea = fea + trunk\n\n        fea = self.lrelu(self.upconv1(F.interpolate(fea, scale_factor=2, mode='nearest')))\n        fea = self.lrelu(self.upconv2(F.interpolate(fea, scale_factor=2, mode='nearest')))\n        out = self.conv_last(self.lrelu(self.HRconv(fea)))\n\n        return out\n"""
codes/models/archs/RankSRGAN_arch.py,1,"b'import functools\nimport torch.nn as nn\nimport models.archs.arch_util as arch_util\n\n####################\n# Generator for RankSRGAN\n####################\nclass SRResNet(nn.Module):\n\n    def __init__(self, in_nc=3, out_nc=3, nf=64, nb=16, upscale=4):\n        super(SRResNet, self).__init__()\n        self.upscale = upscale\n\n        self.conv_first = nn.Conv2d(in_nc, nf, 3, 1, 1, bias=True)\n        basic_block = functools.partial(arch_util.ResidualBlock_noBN, nf=nf)\n        self.recon_trunk = arch_util.make_layer(basic_block, nb)\n        self.LRconv = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n\n        # upsampling\n        if self.upscale == 2:\n            self.upconv1 = nn.Conv2d(nf, nf * 4, 3, 1, 1, bias=True)\n            self.pixel_shuffle = nn.PixelShuffle(2)\n        elif self.upscale == 3:\n            self.upconv1 = nn.Conv2d(nf, nf * 9, 3, 1, 1, bias=True)\n            self.pixel_shuffle = nn.PixelShuffle(3)\n        elif self.upscale == 4:\n            self.upconv1 = nn.Conv2d(nf, nf * 4, 3, 1, 1, bias=True)\n            self.upconv2 = nn.Conv2d(nf, nf * 4, 3, 1, 1, bias=True)\n            self.pixel_shuffle = nn.PixelShuffle(2)\n\n        self.HRconv = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n        self.conv_last = nn.Conv2d(nf, out_nc, 3, 1, 1, bias=True)\n\n        # activation function\n        self.relu = nn.ReLU(inplace=True)\n\n        # initialization\n        arch_util.initialize_weights([self.conv_first, self.upconv1, self.HRconv, self.conv_last],\n                                     0.1)\n        if self.upscale == 4:\n            arch_util.initialize_weights(self.upconv2, 0.1)\n\n    def forward(self, x):\n        fea = self.conv_first(x)\n        out = self.recon_trunk(fea)\n        out = self.LRconv(out)\n\n        if self.upscale == 4:\n            out = self.relu(self.pixel_shuffle(self.upconv1(out+fea)))\n            out = self.relu(self.pixel_shuffle(self.upconv2(out)))\n        elif self.upscale == 3 or self.upscale == 2:\n            out = self.relu(self.pixel_shuffle(self.upconv1(out+fea)))\n\n        out = self.conv_last(self.relu(self.HRconv(out)))\n\n        return out\n\n####################\n# Discriminator with patchsize 296 for RankSRGAN\n####################\nclass Discriminator_VGG_296(nn.Module):\n    def __init__(self, in_nc, nf):\n        super(Discriminator_VGG_296, self).__init__()\n        # [64, 128, 128]\n        self.conv0_0 = nn.Conv2d(in_nc, nf, 3, 1, 1, bias=True)\n        self.conv0_1 = nn.Conv2d(nf, nf, 4, 2, 1, bias=False)\n        self.bn0_1 = nn.BatchNorm2d(nf, affine=True)\n        # [64, 64, 64]\n        self.conv1_0 = nn.Conv2d(nf, nf * 2, 3, 1, 1, bias=False)\n        self.bn1_0 = nn.BatchNorm2d(nf * 2, affine=True)\n        self.conv1_1 = nn.Conv2d(nf * 2, nf * 2, 4, 2, 1, bias=False)\n        self.bn1_1 = nn.BatchNorm2d(nf * 2, affine=True)\n        # [128, 32, 32]\n        self.conv2_0 = nn.Conv2d(nf * 2, nf * 4, 3, 1, 1, bias=False)\n        self.bn2_0 = nn.BatchNorm2d(nf * 4, affine=True)\n        self.conv2_1 = nn.Conv2d(nf * 4, nf * 4, 4, 2, 1, bias=False)\n        self.bn2_1 = nn.BatchNorm2d(nf * 4, affine=True)\n        # [256, 16, 16]\n        self.conv3_0 = nn.Conv2d(nf * 4, nf * 8, 3, 1, 1, bias=False)\n        self.bn3_0 = nn.BatchNorm2d(nf * 8, affine=True)\n        self.conv3_1 = nn.Conv2d(nf * 8, nf * 8, 4, 2, 1, bias=False)\n        self.bn3_1 = nn.BatchNorm2d(nf * 8, affine=True)\n        # [512, 8, 8]\n        self.conv4_0 = nn.Conv2d(nf * 8, nf * 8, 3, 1, 1, bias=False)\n        self.bn4_0 = nn.BatchNorm2d(nf * 8, affine=True)\n        self.conv4_1 = nn.Conv2d(nf * 8, nf * 8, 4, 2, 1, bias=False)\n        self.bn4_1 = nn.BatchNorm2d(nf * 8, affine=True)\n\n        self.linear1 = nn.Linear(512 * 9 * 9, 100)\n        self.linear2 = nn.Linear(100, 1)\n\n        # activation function\n        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n\n    def forward(self, x):\n        fea = self.lrelu(self.conv0_0(x))\n        fea = self.lrelu(self.bn0_1(self.conv0_1(fea)))\n\n        fea = self.lrelu(self.bn1_0(self.conv1_0(fea)))\n        fea = self.lrelu(self.bn1_1(self.conv1_1(fea)))\n\n        fea = self.lrelu(self.bn2_0(self.conv2_0(fea)))\n        fea = self.lrelu(self.bn2_1(self.conv2_1(fea)))\n\n        fea = self.lrelu(self.bn3_0(self.conv3_0(fea)))\n        fea = self.lrelu(self.bn3_1(self.conv3_1(fea)))\n\n        fea = self.lrelu(self.bn4_0(self.conv4_0(fea)))\n        fea = self.lrelu(self.bn4_1(self.conv4_1(fea)))\n\n        fea = fea.view(fea.size(0), -1)\n        fea = self.lrelu(self.linear1(fea))\n        out = self.linear2(fea)\n        return out\n\n\n####################\n# Ranker\n####################\n\nclass Ranker_VGG12_296(nn.Module):\n    def __init__(self, in_nc, nf):\n        super(Ranker_VGG12_296, self).__init__()\n        # [64, 128, 128]\n        self.conv0_0 = nn.Conv2d(in_nc, nf, 3, 1, 1, bias=True)\n        self.conv0_1 = nn.Conv2d(nf, nf, 4, 2, 1, bias=True)\n        self.bn0_1 = nn.BatchNorm2d(nf, affine=True)\n        # [64, 64, 64]\n        self.conv1_0 = nn.Conv2d(nf, nf * 2, 3, 1, 1, bias=True)\n        self.bn1_0 = nn.BatchNorm2d(nf * 2, affine=True)\n        self.conv1_1 = nn.Conv2d(nf * 2, nf * 2, 4, 2, 1, bias=True)\n        self.bn1_1 = nn.BatchNorm2d(nf * 2, affine=True)\n        # [128, 32, 32]\n        self.conv2_0 = nn.Conv2d(nf * 2, nf * 4, 3, 1, 1, bias=True)\n        self.bn2_0 = nn.BatchNorm2d(nf * 4, affine=True)\n        self.conv2_1 = nn.Conv2d(nf * 4, nf * 4, 4, 2, 1, bias=True)\n        self.bn2_1 = nn.BatchNorm2d(nf * 4, affine=True)\n        # [256, 16, 16]\n        self.conv3_0 = nn.Conv2d(nf * 4, nf * 8, 3, 1, 1, bias=True)\n        self.bn3_0 = nn.BatchNorm2d(nf * 8, affine=True)\n        self.conv3_1 = nn.Conv2d(nf * 8, nf * 8, 4, 2, 1, bias=True)\n        self.bn3_1 = nn.BatchNorm2d(nf * 8, affine=True)\n        # [512, 8, 8]\n        self.conv4_0 = nn.Conv2d(nf * 8, nf * 8, 3, 1, 1, bias=True)\n        self.bn4_0 = nn.BatchNorm2d(nf * 8, affine=True)\n        self.conv4_1 = nn.Conv2d(nf * 8, nf * 8, 4, 2, 1, bias=True)\n        self.bn4_1 = nn.BatchNorm2d(nf * 8, affine=True)\n\n        # classifier\n        self.classifier = nn.Sequential(\n            nn.Linear(512, 100),\n            nn.LeakyReLU(0.2, True),\n            nn.Linear(100, 1)\n        )\n\n        # activation function\n        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n\n    def forward(self, x):\n        fea = self.lrelu(self.conv0_0(x))\n        fea = self.lrelu(self.bn0_1(self.conv0_1(fea)))\n\n        fea = self.lrelu(self.bn1_0(self.conv1_0(fea)))\n        fea = self.lrelu(self.bn1_1(self.conv1_1(fea)))\n\n        fea = self.lrelu(self.bn2_0(self.conv2_0(fea)))\n        fea = self.lrelu(self.bn2_1(self.conv2_1(fea)))\n\n        fea = self.lrelu(self.bn3_0(self.conv3_0(fea)))\n        fea = self.lrelu(self.bn3_1(self.conv3_1(fea)))\n\n        fea = self.lrelu(self.bn4_0(self.conv4_0(fea)))\n        fea = self.lrelu(self.bn4_1(self.conv4_1(fea)))\n\n        fea = nn.AvgPool2d(fea.size()[2])(fea)\n        fea = fea.view(fea.size(0), -1)\n        out = self.classifier(fea)\n        return out\n\n'"
codes/models/archs/SRResNet_arch.py,2,"b""import functools\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport models.archs.arch_util as arch_util\n\n\nclass MSRResNet(nn.Module):\n    ''' modified SRResNet'''\n\n    def __init__(self, in_nc=3, out_nc=3, nf=64, nb=16, upscale=4):\n        super(MSRResNet, self).__init__()\n        self.upscale = upscale\n\n        self.conv_first = nn.Conv2d(in_nc, nf, 3, 1, 1, bias=True)\n        basic_block = functools.partial(arch_util.ResidualBlock_noBN, nf=nf)\n        self.recon_trunk = arch_util.make_layer(basic_block, nb)\n\n        # upsampling\n        if self.upscale == 2:\n            self.upconv1 = nn.Conv2d(nf, nf * 4, 3, 1, 1, bias=True)\n            self.pixel_shuffle = nn.PixelShuffle(2)\n        elif self.upscale == 3:\n            self.upconv1 = nn.Conv2d(nf, nf * 9, 3, 1, 1, bias=True)\n            self.pixel_shuffle = nn.PixelShuffle(3)\n        elif self.upscale == 4:\n            self.upconv1 = nn.Conv2d(nf, nf * 4, 3, 1, 1, bias=True)\n            self.upconv2 = nn.Conv2d(nf, nf * 4, 3, 1, 1, bias=True)\n            self.pixel_shuffle = nn.PixelShuffle(2)\n\n        self.HRconv = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n        self.conv_last = nn.Conv2d(nf, out_nc, 3, 1, 1, bias=True)\n\n        # activation function\n        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n\n        # initialization\n        arch_util.initialize_weights([self.conv_first, self.upconv1, self.HRconv, self.conv_last],\n                                     0.1)\n        if self.upscale == 4:\n            arch_util.initialize_weights(self.upconv2, 0.1)\n\n    def forward(self, x):\n        fea = self.lrelu(self.conv_first(x))\n        out = self.recon_trunk(fea)\n\n        if self.upscale == 4:\n            out = self.lrelu(self.pixel_shuffle(self.upconv1(out)))\n            out = self.lrelu(self.pixel_shuffle(self.upconv2(out)))\n        elif self.upscale == 3 or self.upscale == 2:\n            out = self.lrelu(self.pixel_shuffle(self.upconv1(out)))\n\n        out = self.conv_last(self.lrelu(self.HRconv(out)))\n        base = F.interpolate(x, scale_factor=self.upscale, mode='bilinear', align_corners=False)\n        out += base\n        return out\n"""
codes/models/archs/__init__.py,0,b''
codes/models/archs/arch_util.py,3,"b""import torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\n\n\ndef initialize_weights(net_l, scale=1):\n    if not isinstance(net_l, list):\n        net_l = [net_l]\n    for net in net_l:\n        for m in net.modules():\n            if isinstance(m, nn.Conv2d):\n                init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n                m.weight.data *= scale  # for residual block\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n                m.weight.data *= scale\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                init.constant_(m.weight, 1)\n                init.constant_(m.bias.data, 0.0)\n\n\ndef make_layer(block, n_layers):\n    layers = []\n    for _ in range(n_layers):\n        layers.append(block())\n    return nn.Sequential(*layers)\n\n\nclass ResidualBlock_noBN(nn.Module):\n    '''Residual block w/o BN\n    ---Conv-ReLU-Conv-+-\n     |________________|\n    '''\n\n    def __init__(self, nf=64):\n        super(ResidualBlock_noBN, self).__init__()\n        self.conv1 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n        self.conv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n\n        # initialization\n        initialize_weights([self.conv1, self.conv2], 0.1)\n\n    def forward(self, x):\n        identity = x\n        out = F.relu(self.conv1(x), inplace=True)\n        out = self.conv2(out)\n        return identity + out\n\n"""
codes/models/archs/discriminator_vgg_arch.py,4,"b""import torch\nimport torch.nn as nn\nimport torchvision\n\n\nclass Discriminator_VGG_128(nn.Module):\n    def __init__(self, in_nc, nf):\n        super(Discriminator_VGG_128, self).__init__()\n        # [64, 128, 128]\n        self.conv0_0 = nn.Conv2d(in_nc, nf, 3, 1, 1, bias=True)\n        self.conv0_1 = nn.Conv2d(nf, nf, 4, 2, 1, bias=False)\n        self.bn0_1 = nn.BatchNorm2d(nf, affine=True)\n        # [64, 64, 64]\n        self.conv1_0 = nn.Conv2d(nf, nf * 2, 3, 1, 1, bias=False)\n        self.bn1_0 = nn.BatchNorm2d(nf * 2, affine=True)\n        self.conv1_1 = nn.Conv2d(nf * 2, nf * 2, 4, 2, 1, bias=False)\n        self.bn1_1 = nn.BatchNorm2d(nf * 2, affine=True)\n        # [128, 32, 32]\n        self.conv2_0 = nn.Conv2d(nf * 2, nf * 4, 3, 1, 1, bias=False)\n        self.bn2_0 = nn.BatchNorm2d(nf * 4, affine=True)\n        self.conv2_1 = nn.Conv2d(nf * 4, nf * 4, 4, 2, 1, bias=False)\n        self.bn2_1 = nn.BatchNorm2d(nf * 4, affine=True)\n        # [256, 16, 16]\n        self.conv3_0 = nn.Conv2d(nf * 4, nf * 8, 3, 1, 1, bias=False)\n        self.bn3_0 = nn.BatchNorm2d(nf * 8, affine=True)\n        self.conv3_1 = nn.Conv2d(nf * 8, nf * 8, 4, 2, 1, bias=False)\n        self.bn3_1 = nn.BatchNorm2d(nf * 8, affine=True)\n        # [512, 8, 8]\n        self.conv4_0 = nn.Conv2d(nf * 8, nf * 8, 3, 1, 1, bias=False)\n        self.bn4_0 = nn.BatchNorm2d(nf * 8, affine=True)\n        self.conv4_1 = nn.Conv2d(nf * 8, nf * 8, 4, 2, 1, bias=False)\n        self.bn4_1 = nn.BatchNorm2d(nf * 8, affine=True)\n\n        self.linear1 = nn.Linear(512 * 4 * 4, 100)\n        self.linear2 = nn.Linear(100, 1)\n\n        # activation function\n        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n\n    def forward(self, x):\n        fea = self.lrelu(self.conv0_0(x))\n        fea = self.lrelu(self.bn0_1(self.conv0_1(fea)))\n\n        fea = self.lrelu(self.bn1_0(self.conv1_0(fea)))\n        fea = self.lrelu(self.bn1_1(self.conv1_1(fea)))\n\n        fea = self.lrelu(self.bn2_0(self.conv2_0(fea)))\n        fea = self.lrelu(self.bn2_1(self.conv2_1(fea)))\n\n        fea = self.lrelu(self.bn3_0(self.conv3_0(fea)))\n        fea = self.lrelu(self.bn3_1(self.conv3_1(fea)))\n\n        fea = self.lrelu(self.bn4_0(self.conv4_0(fea)))\n        fea = self.lrelu(self.bn4_1(self.conv4_1(fea)))\n\n        fea = fea.view(fea.size(0), -1)\n        fea = self.lrelu(self.linear1(fea))\n        out = self.linear2(fea)\n        return out\n\n\nclass VGGFeatureExtractor(nn.Module):\n    def __init__(self, feature_layer=34, use_bn=False, use_input_norm=True,\n                 device=torch.device('cpu')):\n        super(VGGFeatureExtractor, self).__init__()\n        self.use_input_norm = use_input_norm\n        if use_bn:\n            model = torchvision.models.vgg19_bn(pretrained=True)\n        else:\n            model = torchvision.models.vgg19(pretrained=True)\n        if self.use_input_norm:\n            mean = torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n            # [0.485 - 1, 0.456 - 1, 0.406 - 1] if input in range [-1, 1]\n            std = torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n            # [0.229 * 2, 0.224 * 2, 0.225 * 2] if input in range [-1, 1]\n            self.register_buffer('mean', mean)\n            self.register_buffer('std', std)\n        self.features = nn.Sequential(*list(model.features.children())[:(feature_layer + 1)])\n        # No need to BP to variable\n        for k, v in self.features.named_parameters():\n            v.requires_grad = False\n\n    def forward(self, x):\n        # Assume input range is [0, 1]\n        if self.use_input_norm:\n            x = (x - self.mean) / self.std\n        output = self.features(x)\n        return output\n"""
