file_path,api_count,code
base/__init__.py,0,b'from .base_trainer import BaseTrainer\nfrom .base_dataset import BaseDataSet'
base/base_dataset.py,1,"b'# -*- coding: utf-8 -*-\n# @Time    : 2019/12/4 13:12\n# @Author  : zhoujun\nimport copy\nfrom torch.utils.data import Dataset\nfrom data_loader.modules import *\n\n\nclass BaseDataSet(Dataset):\n\n    def __init__(self, data_path: str, img_mode, pre_processes, filter_keys, ignore_tags, transform=None,\n                 target_transform=None):\n        assert img_mode in [\'RGB\', \'BRG\', \'GRAY\']\n        self.ignore_tags = ignore_tags\n        self.data_list = self.load_data(data_path)\n        item_keys = [\'img_path\', \'img_name\', \'text_polys\', \'texts\', \'ignore_tags\']\n        for item in item_keys:\n            assert item in self.data_list[0], \'data_list from load_data must contains {}\'.format(item_keys)\n        self.img_mode = img_mode\n        self.filter_keys = filter_keys\n        self.transform = transform\n        self.target_transform = target_transform\n        self._init_pre_processes(pre_processes)\n\n    def _init_pre_processes(self, pre_processes):\n        self.aug = []\n        if pre_processes is not None:\n            for aug in pre_processes:\n                if \'args\' not in aug:\n                    args = {}\n                else:\n                    args = aug[\'args\']\n                if isinstance(args, dict):\n                    cls = eval(aug[\'type\'])(**args)\n                else:\n                    cls = eval(aug[\'type\'])(args)\n                self.aug.append(cls)\n\n    def load_data(self, data_path: str) -> list:\n        """"""\n        \xe6\x8a\x8a\xe6\x95\xb0\xe6\x8d\xae\xe5\x8a\xa0\xe8\xbd\xbd\xe4\xb8\xba\xe4\xb8\x80\xe4\xb8\xaalist\xef\xbc\x9a\n        :params data_path: \xe5\xad\x98\xe5\x82\xa8\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xe6\x88\x96\xe8\x80\x85\xe6\x96\x87\xe4\xbb\xb6\n        return a dict ,\xe5\x8c\x85\xe5\x90\xab\xe4\xba\x86\xef\xbc\x8c\'img_path\',\'img_name\',\'text_polys\',\'texts\',\'ignore_tags\'\n        """"""\n        raise NotImplementedError\n\n    def apply_pre_processes(self, data):\n        for aug in self.aug:\n            data = aug(data)\n        return data\n\n    def __getitem__(self, index):\n        try:\n            data = copy.deepcopy(self.data_list[index])\n            im = cv2.imread(data[\'img_path\'], 1 if self.img_mode != \'GRAY\' else 0)\n            if self.img_mode == \'RGB\':\n                im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n            data[\'img\'] = im\n            data[\'shape\'] = [im.shape[0], im.shape[1]]\n            data = self.apply_pre_processes(data)\n\n            if self.transform:\n                data[\'img\'] = self.transform(data[\'img\'])\n            data[\'text_polys\'] = data[\'text_polys\'].tolist()\n            if len(self.filter_keys):\n                data_dict = {}\n                for k, v in data.items():\n                    if k not in self.filter_keys:\n                        data_dict[k] = v\n                return data_dict\n            else:\n                return data\n        except:\n            return self.__getitem__(np.random.randint(self.__len__()))\n\n    def __len__(self):\n        return len(self.data_list)\n'"
base/base_trainer.py,18,"b'# -*- coding: utf-8 -*-\n# @Time    : 2019/8/23 21:50\n# @Author  : zhoujun\n\nimport os\nimport pathlib\nimport shutil\nfrom pprint import pformat\n\nimport anyconfig\nimport torch\n\nfrom utils import setup_logger\n\n\nclass BaseTrainer:\n    def __init__(self, config, model, criterion):\n        config[\'trainer\'][\'output_dir\'] = os.path.join(str(pathlib.Path(os.path.abspath(__name__)).parent),\n                                                       config[\'trainer\'][\'output_dir\'])\n        config[\'name\'] = config[\'name\'] + \'_\' + model.name\n        self.save_dir = os.path.join(config[\'trainer\'][\'output_dir\'], config[\'name\'])\n        self.checkpoint_dir = os.path.join(self.save_dir, \'checkpoint\')\n\n        if config[\'trainer\'][\'resume_checkpoint\'] == \'\' and config[\'trainer\'][\'finetune_checkpoint\'] == \'\':\n            shutil.rmtree(self.save_dir, ignore_errors=True)\n        if not os.path.exists(self.checkpoint_dir):\n            os.makedirs(self.checkpoint_dir)\n\n        self.global_step = 0\n        self.start_epoch = 0\n        self.config = config\n        self.model = model\n        self.criterion = criterion\n        # logger and tensorboard\n        self.tensorboard_enable = self.config[\'trainer\'][\'tensorboard\']\n        self.epochs = self.config[\'trainer\'][\'epochs\']\n        self.log_iter = self.config[\'trainer\'][\'log_iter\']\n        if config[\'local_rank\'] == 0:\n            anyconfig.dump(config, os.path.join(self.save_dir, \'config.yaml\'))\n            self.logger = setup_logger(os.path.join(self.save_dir, \'train.log\'))\n            self.logger_info(pformat(self.config))\n\n        # device\n        torch.manual_seed(self.config[\'trainer\'][\'seed\'])  # \xe4\xb8\xbaCPU\xe8\xae\xbe\xe7\xbd\xae\xe9\x9a\x8f\xe6\x9c\xba\xe7\xa7\x8d\xe5\xad\x90\n        if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n            self.with_cuda = True\n            torch.backends.cudnn.benchmark = True\n            self.device = torch.device(""cuda"")\n            torch.cuda.manual_seed(self.config[\'trainer\'][\'seed\'])  # \xe4\xb8\xba\xe5\xbd\x93\xe5\x89\x8dGPU\xe8\xae\xbe\xe7\xbd\xae\xe9\x9a\x8f\xe6\x9c\xba\xe7\xa7\x8d\xe5\xad\x90\n            torch.cuda.manual_seed_all(self.config[\'trainer\'][\'seed\'])  # \xe4\xb8\xba\xe6\x89\x80\xe6\x9c\x89GPU\xe8\xae\xbe\xe7\xbd\xae\xe9\x9a\x8f\xe6\x9c\xba\xe7\xa7\x8d\xe5\xad\x90\n        else:\n            self.with_cuda = False\n            self.device = torch.device(""cpu"")\n        self.logger_info(\'train with device {} and pytorch {}\'.format(self.device, torch.__version__))\n        # metrics\n        self.metrics = {\'recall\': 0, \'precision\': 0, \'hmean\': 0, \'train_loss\': float(\'inf\'),\'best_model_epoch\':0}\n\n        self.optimizer = self._initialize(\'optimizer\', torch.optim, model.parameters())\n\n        # resume or finetune\n        if self.config[\'trainer\'][\'resume_checkpoint\'] != \'\':\n            self._laod_checkpoint(self.config[\'trainer\'][\'resume_checkpoint\'], resume=True)\n        elif self.config[\'trainer\'][\'finetune_checkpoint\'] != \'\':\n            self._laod_checkpoint(self.config[\'trainer\'][\'finetune_checkpoint\'], resume=False)\n\n        if self.config[\'lr_scheduler\'][\'type\'] != \'WarmupPolyLR\':\n            self.scheduler = self._initialize(\'lr_scheduler\', torch.optim.lr_scheduler, self.optimizer)\n\n        self.model.to(self.device)\n\n        if self.tensorboard_enable and config[\'local_rank\'] == 0:\n            from torch.utils.tensorboard import SummaryWriter\n            self.writer = SummaryWriter(self.save_dir)\n            try:\n                # add graph\n                in_channels = 3 if config[\'dataset\'][\'train\'][\'dataset\'][\'args\'][\'img_mode\'] != \'GRAY\' else 1\n                dummy_input = torch.zeros(1, in_channels, 640, 640).to(self.device)\n                self.writer.add_graph(self.model, dummy_input)\n                torch.cuda.empty_cache()\n            except:\n                import traceback\n                self.logger.error(traceback.format_exc())\n                self.logger.warn(\'add graph to tensorboard failed\')\n        # \xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe8\xae\xad\xe7\xbb\x83\n        if torch.cuda.device_count() > 1:\n            local_rank = config[\'local_rank\']\n            self.model = torch.nn.parallel.DistributedDataParallel(self.model, device_ids=[local_rank], output_device=local_rank, broadcast_buffers=False,\n                                                                   find_unused_parameters=True)\n        # make inverse Normalize\n        self.UN_Normalize = False\n        for t in self.config[\'dataset\'][\'train\'][\'dataset\'][\'args\'][\'transforms\']:\n            if t[\'type\'] == \'Normalize\':\n                self.normalize_mean = t[\'args\'][\'mean\']\n                self.normalize_std = t[\'args\'][\'std\']\n                self.UN_Normalize = True\n\n    def train(self):\n        """"""\n        Full training logic\n        """"""\n        for epoch in range(self.start_epoch + 1, self.epochs + 1):\n            if self.config[\'distributed\']:\n                self.train_loader.sampler.set_epoch(epoch)\n            self.epoch_result = self._train_epoch(epoch)\n            if self.config[\'lr_scheduler\'][\'type\'] != \'WarmupPolyLR\':\n                self.scheduler.step()\n            self._on_epoch_finish()\n        if self.config[\'local_rank\'] == 0 and self.tensorboard_enable:\n            self.writer.close()\n        self._on_train_finish()\n\n    def _train_epoch(self, epoch):\n        """"""\n        Training logic for an epoch\n\n        :param epoch: Current epoch number\n        """"""\n        raise NotImplementedError\n\n    def _eval(self, epoch):\n        """"""\n        eval logic for an epoch\n\n        :param epoch: Current epoch number\n        """"""\n        raise NotImplementedError\n\n    def _on_epoch_finish(self):\n        raise NotImplementedError\n\n    def _on_train_finish(self):\n        raise NotImplementedError\n\n    def _save_checkpoint(self, epoch, file_name, save_best=False):\n        """"""\n        Saving checkpoints\n\n        :param epoch: current epoch number\n        :param log: logging information of the epoch\n        :param save_best: if True, rename the saved checkpoint to \'model_best.pth.tar\'\n        """"""\n        state_dict = self.model.module.state_dict() if self.config[\'distributed\'] else self.model.state_dict()\n        state = {\n            \'epoch\': epoch,\n            \'global_step\': self.global_step,\n            \'state_dict\': state_dict,\n            \'optimizer\': self.optimizer.state_dict(),\n            \'scheduler\': self.scheduler.state_dict(),\n            \'config\': self.config,\n            \'metrics\': self.metrics\n        }\n        filename = os.path.join(self.checkpoint_dir, file_name)\n        torch.save(state, filename)\n        if save_best:\n            shutil.copy(filename, os.path.join(self.checkpoint_dir, \'model_best.pth\'))\n            self.logger_info(""Saving current best: {}"".format(os.path.join(self.checkpoint_dir, \'model_best.pth\')))\n        else:\n            self.logger_info(""Saving checkpoint: {}"".format(filename))\n\n    def _laod_checkpoint(self, checkpoint_path, resume):\n        """"""\n        Resume from saved checkpoints\n        :param checkpoint_path: Checkpoint path to be resumed\n        """"""\n        self.logger_info(""Loading checkpoint: {} ..."".format(checkpoint_path))\n        checkpoint = torch.load(checkpoint_path, map_location=torch.device(\'cpu\'))\n        self.model.load_state_dict(checkpoint[\'state_dict\'], strict=resume)\n        if resume:\n            self.global_step = checkpoint[\'global_step\']\n            self.start_epoch = checkpoint[\'epoch\']\n            self.config[\'lr_scheduler\'][\'args\'][\'last_epoch\'] = self.start_epoch\n            # self.scheduler.load_state_dict(checkpoint[\'scheduler\'])\n            self.optimizer.load_state_dict(checkpoint[\'optimizer\'])\n            if \'metrics\' in checkpoint:\n                self.metrics = checkpoint[\'metrics\']\n            if self.with_cuda:\n                for state in self.optimizer.state.values():\n                    for k, v in state.items():\n                        if isinstance(v, torch.Tensor):\n                            state[k] = v.to(self.device)\n            self.logger_info(""resume from checkpoint {} (epoch {})"".format(checkpoint_path, self.start_epoch))\n        else:\n            self.logger_info(""finetune from checkpoint {}"".format(checkpoint_path))\n\n    def _initialize(self, name, module, *args, **kwargs):\n        module_name = self.config[name][\'type\']\n        module_args = self.config[name][\'args\']\n        assert all([k not in module_args for k in kwargs]), \'Overwriting kwargs given in config file is not allowed\'\n        module_args.update(kwargs)\n        return getattr(module, module_name)(*args, **module_args)\n\n    def inverse_normalize(self, batch_img):\n        if self.UN_Normalize:\n            batch_img[:, 0, :, :] = batch_img[:, 0, :, :] * self.normalize_std[0] + self.normalize_mean[0]\n            batch_img[:, 1, :, :] = batch_img[:, 1, :, :] * self.normalize_std[1] + self.normalize_mean[1]\n            batch_img[:, 2, :, :] = batch_img[:, 2, :, :] * self.normalize_std[2] + self.normalize_mean[2]\n\n    def logger_info(self, s):\n        if self.config[\'local_rank\'] == 0:\n            self.logger.info(s)\n'"
data_loader/__init__.py,4,"b'# -*- coding: utf-8 -*-\n# @Time    : 2019/8/23 21:52\n# @Author  : zhoujun\nimport copy\n\nimport PIL\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\n\n\ndef get_dataset(data_path, module_name, transform, dataset_args):\n    """"""\n    \xe8\x8e\xb7\xe5\x8f\x96\xe8\xae\xad\xe7\xbb\x83dataset\n    :param data_path: dataset\xe6\x96\x87\xe4\xbb\xb6\xe5\x88\x97\xe8\xa1\xa8\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x96\x87\xe4\xbb\xb6\xe5\x86\x85\xe4\xbb\xa5\xe5\xa6\x82\xe4\xb8\x8b\xe6\xa0\xbc\xe5\xbc\x8f\xe5\xad\x98\xe5\x82\xa8 \xe2\x80\x98path/to/img\\tlabel\xe2\x80\x99\n    :param module_name: \xe6\x89\x80\xe4\xbd\xbf\xe7\x94\xa8\xe7\x9a\x84\xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89dataset\xe5\x90\x8d\xe7\xa7\xb0\xef\xbc\x8c\xe7\x9b\xae\xe5\x89\x8d\xe5\x8f\xaa\xe6\x94\xaf\xe6\x8c\x81data_loaders.ImageDataset\n    :param transform: \xe8\xaf\xa5\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe4\xbd\xbf\xe7\x94\xa8\xe7\x9a\x84transforms\n    :param dataset_args: module_name\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\n    :return: \xe5\xa6\x82\xe6\x9e\x9cdata_path\xe5\x88\x97\xe8\xa1\xa8\xe4\xb8\x8d\xe4\xb8\xba\xe7\xa9\xba\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9e\xe5\xaf\xb9\xe4\xba\x8e\xe7\x9a\x84ConcatDataset\xe5\xaf\xb9\xe8\xb1\xa1\xef\xbc\x8c\xe5\x90\xa6\xe5\x88\x99None\n    """"""\n    from . import dataset\n    s_dataset = getattr(dataset, module_name)(transform=transform, data_path=data_path,\n                                              **dataset_args)\n    return s_dataset\n\n\ndef get_transforms(transforms_config):\n    tr_list = []\n    for item in transforms_config:\n        if \'args\' not in item:\n            args = {}\n        else:\n            args = item[\'args\']\n        cls = getattr(transforms, item[\'type\'])(**args)\n        tr_list.append(cls)\n    tr_list = transforms.Compose(tr_list)\n    return tr_list\n\n\nclass ICDARCollectFN():\n    def __init__(self, *args, **kwargs):\n        pass\n\n    def __call__(self, batch):\n        data_dict = {}\n        to_tensor_keys = []\n        for sample in batch:\n            for k, v in sample.items():\n                if k not in data_dict:\n                    data_dict[k] = []\n                if isinstance(v, (np.ndarray, torch.Tensor, PIL.Image.Image)):\n                    if k not in to_tensor_keys:\n                        to_tensor_keys.append(k)\n                data_dict[k].append(v)\n        for k in to_tensor_keys:\n            data_dict[k] = torch.stack(data_dict[k], 0)\n        return data_dict\n\n\ndef get_dataloader(module_config, distributed=False):\n    if module_config is None:\n        return None\n    config = copy.deepcopy(module_config)\n    dataset_args = config[\'dataset\'][\'args\']\n    if \'transforms\' in dataset_args:\n        img_transfroms = get_transforms(dataset_args.pop(\'transforms\'))\n    else:\n        img_transfroms = None\n    # \xe5\x88\x9b\xe5\xbb\xba\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n    dataset_name = config[\'dataset\'][\'type\']\n    data_path = dataset_args.pop(\'data_path\')\n    if data_path == None:\n        return None\n\n    data_path = [x for x in data_path if x is not None]\n    if len(data_path) == 0:\n        return None\n    if \'collate_fn\' not in config[\'loader\'] or config[\'loader\'][\'collate_fn\'] is None or len(config[\'loader\'][\'collate_fn\']) == 0:\n        config[\'loader\'][\'collate_fn\'] = None\n    else:\n        config[\'loader\'][\'collate_fn\'] = eval(config[\'loader\'][\'collate_fn\'])()\n\n    _dataset = get_dataset(data_path=data_path, module_name=dataset_name, transform=img_transfroms, dataset_args=dataset_args)\n    sampler = None\n    if distributed:\n        from torch.utils.data.distributed import DistributedSampler\n        # 3\xef\xbc\x89\xe4\xbd\xbf\xe7\x94\xa8DistributedSampler\n        sampler = DistributedSampler(_dataset)\n        config[\'loader\'][\'shuffle\'] = False\n        config[\'loader\'][\'pin_memory\'] = True\n    loader = DataLoader(dataset=_dataset, sampler=sampler, **config[\'loader\'])\n    return loader\n'"
data_loader/dataset.py,3,"b'# -*- coding: utf-8 -*-\n# @Time    : 2019/8/23 21:54\n# @Author  : zhoujun\nimport pathlib\nimport os\nimport cv2\nimport numpy as np\nimport scipy.io as sio\nfrom tqdm.auto import tqdm\n\nfrom base import BaseDataSet\nfrom utils import order_points_clockwise, get_datalist, load,expand_polygon\n\n\nclass ICDAR2015Dataset(BaseDataSet):\n    def __init__(self, data_path: str, img_mode, pre_processes, filter_keys, ignore_tags, transform=None, **kwargs):\n        super().__init__(data_path, img_mode, pre_processes, filter_keys, ignore_tags, transform)\n\n    def load_data(self, data_path: str) -> list:\n        data_list = get_datalist(data_path)\n        t_data_list = []\n        for img_path, label_path in data_list:\n            data = self._get_annotation(label_path)\n            if len(data[\'text_polys\']) > 0:\n                item = {\'img_path\': img_path, \'img_name\': pathlib.Path(img_path).stem}\n                item.update(data)\n                t_data_list.append(item)\n            else:\n                print(\'there is no suit bbox in {}\'.format(label_path))\n        return t_data_list\n\n    def _get_annotation(self, label_path: str) -> dict:\n        boxes = []\n        texts = []\n        ignores = []\n        with open(label_path, encoding=\'utf-8\', mode=\'r\') as f:\n            for line in f.readlines():\n                params = line.strip().strip(\'\\ufeff\').strip(\'\\xef\\xbb\\xbf\').split(\',\')\n                try:\n                    box = order_points_clockwise(np.array(list(map(float, params[:8]))).reshape(-1, 2))\n                    if cv2.contourArea(box) > 0:\n                        boxes.append(box)\n                        label = params[8]\n                        texts.append(label)\n                        ignores.append(label in self.ignore_tags)\n                except:\n                    print(\'load label failed on {}\'.format(label_path))\n        data = {\n            \'text_polys\': np.array(boxes),\n            \'texts\': texts,\n            \'ignore_tags\': ignores,\n        }\n        return data\n\n\nclass DetDataset(BaseDataSet):\n    def __init__(self, data_path: str, img_mode, pre_processes, filter_keys, ignore_tags, transform=None, **kwargs):\n        self.load_char_annotation = kwargs[\'load_char_annotation\']\n        self.expand_one_char = kwargs[\'expand_one_char\']\n        super().__init__(data_path, img_mode, pre_processes, filter_keys, ignore_tags, transform)\n\n    def load_data(self, data_path: str) -> list:\n        """"""\n        \xe4\xbb\x8ejson\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\xe8\xaf\xbb\xe5\x8f\x96\xe5\x87\xba \xe6\x96\x87\xe6\x9c\xac\xe8\xa1\x8c\xe7\x9a\x84\xe5\x9d\x90\xe6\xa0\x87\xe5\x92\x8cgt\xef\xbc\x8c\xe5\xad\x97\xe7\xac\xa6\xe7\x9a\x84\xe5\x9d\x90\xe6\xa0\x87\xe5\x92\x8cgt\n        :param data_path:\n        :return:\n        """"""\n        data_list = []\n        for path in data_path:\n            content = load(path)\n            for gt in tqdm(content[\'data_list\'], desc=\'read file {}\'.format(path)):\n                img_path = os.path.join(content[\'data_root\'], gt[\'img_name\'])\n                polygons = []\n                texts = []\n                illegibility_list = []\n                language_list = []\n                for annotation in gt[\'annotations\']:\n                    if len(annotation[\'polygon\']) == 0 or len(annotation[\'text\']) == 0:\n                        continue\n                    if len(annotation[\'text\']) > 1 and self.expand_one_char:\n                        annotation[\'polygon\'] = expand_polygon(annotation[\'polygon\'])\n                    polygons.append(annotation[\'polygon\'])\n                    texts.append(annotation[\'text\'])\n                    illegibility_list.append(annotation[\'illegibility\'])\n                    language_list.append(annotation[\'language\'])\n                    if self.load_char_annotation:\n                        for char_annotation in annotation[\'chars\']:\n                            if len(char_annotation[\'polygon\']) == 0 or len(char_annotation[\'char\']) == 0:\n                                continue\n                            polygons.append(char_annotation[\'polygon\'])\n                            texts.append(char_annotation[\'char\'])\n                            illegibility_list.append(char_annotation[\'illegibility\'])\n                            language_list.append(char_annotation[\'language\'])\n                data_list.append({\'img_path\': img_path, \'img_name\': gt[\'img_name\'], \'text_polys\': np.array(polygons),\n                                  \'texts\': texts, \'ignore_tags\': illegibility_list})\n        return data_list\n\n\nclass SynthTextDataset(BaseDataSet):\n    def __init__(self, data_path: str, img_mode, pre_processes, filter_keys, transform=None, **kwargs):\n        self.transform = transform\n        self.dataRoot = pathlib.Path(data_path)\n        if not self.dataRoot.exists():\n            raise FileNotFoundError(\'Dataset folder is not exist.\')\n\n        self.targetFilePath = self.dataRoot / \'gt.mat\'\n        if not self.targetFilePath.exists():\n            raise FileExistsError(\'Target file is not exist.\')\n        targets = {}\n        sio.loadmat(self.targetFilePath, targets, squeeze_me=True, struct_as_record=False,\n                    variable_names=[\'imnames\', \'wordBB\', \'txt\'])\n\n        self.imageNames = targets[\'imnames\']\n        self.wordBBoxes = targets[\'wordBB\']\n        self.transcripts = targets[\'txt\']\n        super().__init__(data_path, img_mode, pre_processes, filter_keys, transform)\n\n    def load_data(self, data_path: str) -> list:\n        t_data_list = []\n        for imageName, wordBBoxes, texts in zip(self.imageNames, self.wordBBoxes, self.transcripts):\n            item = {}\n            wordBBoxes = np.expand_dims(wordBBoxes, axis=2) if (wordBBoxes.ndim == 2) else wordBBoxes\n            _, _, numOfWords = wordBBoxes.shape\n            text_polys = wordBBoxes.reshape([8, numOfWords], order=\'F\').T  # num_words * 8\n            text_polys = text_polys.reshape(numOfWords, 4, 2)  # num_of_words * 4 * 2\n            transcripts = [word for line in texts for word in line.split()]\n            if numOfWords != len(transcripts):\n                continue\n            item[\'img_path\'] = str(self.dataRoot / imageName)\n            item[\'img_name\'] = (self.dataRoot / imageName).stem\n            item[\'text_polys\'] = text_polys\n            item[\'texts\'] = transcripts\n            item[\'ignore_tags\'] = [x in self.ignore_tags for x in transcripts]\n            t_data_list.append(item)\n        return t_data_list\n\n\nif __name__ == \'__main__\':\n    import torch\n    import anyconfig\n    from torch.utils.data import DataLoader\n    from torchvision import transforms\n\n    from utils import parse_config, show_img, plt, draw_bbox\n\n    config = anyconfig.load(\'config/icdar2015_resnet18_FPN_DBhead_polyLR.yaml\')\n    config = parse_config(config)\n    dataset_args = config[\'dataset\'][\'train\'][\'dataset\'][\'args\']\n    # dataset_args.pop(\'data_path\')\n    # data_list = [(r\'E:/zj/dataset/icdar2015/train/img/img_15.jpg\', \'E:/zj/dataset/icdar2015/train/gt/gt_img_15.txt\')]\n    train_data = ICDAR2015Dataset(data_path=dataset_args.pop(\'data_path\'), transform=transforms.ToTensor(),\n                                  **dataset_args)\n    train_loader = DataLoader(dataset=train_data, batch_size=1, shuffle=True, num_workers=0)\n    for i, data in enumerate(tqdm(train_loader)):\n        # img = data[\'img\']\n        # shrink_label = data[\'shrink_map\']\n        # threshold_label = data[\'threshold_map\']\n        #\n        # print(threshold_label.shape, threshold_label.shape, img.shape)\n        # show_img(img[0].numpy().transpose(1, 2, 0), title=\'img\')\n        # show_img((shrink_label[0].to(torch.float)).numpy(), title=\'shrink_label\')\n        # show_img((threshold_label[0].to(torch.float)).numpy(), title=\'threshold_label\')\n        # img = draw_bbox(img[0].numpy().transpose(1, 2, 0),np.array(data[\'text_polys\']))\n        # show_img(img, title=\'draw_bbox\')\n        # plt.show()\n        pass\n'"
models/__init__.py,0,"b""# -*- coding: utf-8 -*-\n# @Time    : 2019/8/23 21:55\n# @Author  : zhoujun\nfrom .model import Model\nfrom .losses import build_loss\n\n__all__ = ['build_loss', 'build_model']\nsupport_model = ['Model']\n\n\ndef build_model(model_name, **kwargs):\n    assert model_name in support_model, f'all support model is {support_model}'\n    model = eval(model_name)(kwargs)\n    return model\n"""
models/basic.py,0,"b""# -*- coding: utf-8 -*-\n# @Time    : 2019/12/6 11:19\n# @Author  : zhoujun\nfrom torch import nn\n\n\nclass ConvBnRelu(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', inplace=True):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation,\n                              groups=groups, bias=bias, padding_mode=padding_mode)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=inplace)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n"""
models/model.py,4,"b'# -*- coding: utf-8 -*-\n# @Time    : 2019/8/23 21:57\n# @Author  : zhoujun\nfrom addict import Dict\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom models.backbone import build_backbone\nfrom models.neck import build_neck\nfrom models.head import build_head\n\n\nclass Model(nn.Module):\n    def __init__(self, model_config: dict):\n        """"""\n        PANnet\n        :param model_config: \xe6\xa8\xa1\xe5\x9e\x8b\xe9\x85\x8d\xe7\xbd\xae\n        """"""\n        super().__init__()\n        model_config = Dict(model_config)\n        backbone_type = model_config.backbone.pop(\'type\')\n        neck_type = model_config.neck.pop(\'type\')\n        head_type = model_config.head.pop(\'type\')\n        self.backbone = build_backbone(backbone_type, **model_config.backbone)\n        self.neck = build_neck(neck_type, in_channels=self.backbone.out_channels, **model_config.neck)\n        self.head = build_head(head_type, in_channels=self.neck.out_channels, **model_config.head)\n        self.name = f\'{backbone_type}_{neck_type}_{head_type}\'\n\n    def forward(self, x):\n        _, _, H, W = x.size()\n        backbone_out = self.backbone(x)\n        neck_out = self.neck(backbone_out)\n        y = self.head(neck_out)\n        y = F.interpolate(y, size=(H, W), mode=\'bilinear\', align_corners=True)\n        return y\n\n\nif __name__ == \'__main__\':\n    import torch\n\n    device = torch.device(\'cpu\')\n    x = torch.zeros(2, 3, 640, 640).to(device)\n\n    model_config = {\n        \'backbone\': {\'type\': \'resnest50\', \'pretrained\': True, ""in_channels"": 3},\n        \'neck\': {\'type\': \'FPN\', \'inner_channels\': 256},  # \xe5\x88\x86\xe5\x89\xb2\xe5\xa4\xb4\xef\xbc\x8cFPN or FPEM_FFM\n        \'head\': {\'type\': \'DBHead\', \'out_channels\': 2, \'k\': 50},\n    }\n    model = Model(model_config=model_config).to(device)\n    import time\n\n    tic = time.time()\n    y = model(x)\n    print(time.time() - tic)\n    print(y.shape)\n    print(model.name)\n    print(model)\n    # torch.save(model.state_dict(), \'PAN.pth\')\n'"
post_processing/__init__.py,0,"b""# -*- coding: utf-8 -*-\n# @Time    : 2019/12/5 15:17\n# @Author  : zhoujun\n\nfrom .seg_detector_representer import SegDetectorRepresenter\n\n\ndef get_post_processing(config):\n    try:\n        cls = eval(config['type'])(**config['args'])\n        return cls\n    except:\n        return None"""
post_processing/seg_detector_representer.py,0,"b""import cv2\nimport numpy as np\nimport pyclipper\nfrom shapely.geometry import Polygon\n\n\nclass SegDetectorRepresenter():\n    def __init__(self, thresh=0.3, box_thresh=0.7, max_candidates=1000, unclip_ratio=1.5):\n        self.min_size = 3\n        self.thresh = thresh\n        self.box_thresh = box_thresh\n        self.max_candidates = max_candidates\n        self.unclip_ratio = unclip_ratio\n\n    def __call__(self, batch, pred, is_output_polygon=False):\n        '''\n        batch: (image, polygons, ignore_tags\n        batch: a dict produced by dataloaders.\n            image: tensor of shape (N, C, H, W).\n            polygons: tensor of shape (N, K, 4, 2), the polygons of objective regions.\n            ignore_tags: tensor of shape (N, K), indicates whether a region is ignorable or not.\n            shape: the original shape of images.\n            filename: the original filenames of images.\n        pred:\n            binary: text region segmentation map, with shape (N, H, W)\n            thresh: [if exists] thresh hold prediction with shape (N, H, W)\n            thresh_binary: [if exists] binarized with threshhold, (N, H, W)\n        '''\n        pred = pred[:, 0, :, :]\n        segmentation = self.binarize(pred)\n        boxes_batch = []\n        scores_batch = []\n        for batch_index in range(pred.size(0)):\n            height, width = batch['shape'][batch_index]\n            if is_output_polygon:\n                boxes, scores = self.polygons_from_bitmap(pred[batch_index], segmentation[batch_index], width, height)\n            else:\n                boxes, scores = self.boxes_from_bitmap(pred[batch_index], segmentation[batch_index], width, height)\n            boxes_batch.append(boxes)\n            scores_batch.append(scores)\n        return boxes_batch, scores_batch\n\n    def binarize(self, pred):\n        return pred > self.thresh\n\n    def polygons_from_bitmap(self, pred, _bitmap, dest_width, dest_height):\n        '''\n        _bitmap: single map with shape (H, W),\n            whose values are binarized as {0, 1}\n        '''\n\n        assert len(_bitmap.shape) == 2\n        bitmap = _bitmap.cpu().numpy()  # The first channel\n        pred = pred.cpu().detach().numpy()\n        height, width = bitmap.shape\n        boxes = []\n        scores = []\n\n        contours, _ = cv2.findContours((bitmap * 255).astype(np.uint8), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n\n        for contour in contours[:self.max_candidates]:\n            epsilon = 0.005 * cv2.arcLength(contour, True)\n            approx = cv2.approxPolyDP(contour, epsilon, True)\n            points = approx.reshape((-1, 2))\n            if points.shape[0] < 4:\n                continue\n            # _, sside = self.get_mini_boxes(contour)\n            # if sside < self.min_size:\n            #     continue\n            score = self.box_score_fast(pred, contour.squeeze(1))\n            if self.box_thresh > score:\n                continue\n\n            if points.shape[0] > 2:\n                box = self.unclip(points, unclip_ratio=self.unclip_ratio)\n                if len(box) > 1:\n                    continue\n            else:\n                continue\n            box = box.reshape(-1, 2)\n            _, sside = self.get_mini_boxes(box.reshape((-1, 1, 2)))\n            if sside < self.min_size + 2:\n                continue\n\n            if not isinstance(dest_width, int):\n                dest_width = dest_width.item()\n                dest_height = dest_height.item()\n\n            box[:, 0] = np.clip(np.round(box[:, 0] / width * dest_width), 0, dest_width)\n            box[:, 1] = np.clip(np.round(box[:, 1] / height * dest_height), 0, dest_height)\n            boxes.append(box)\n            scores.append(score)\n        return boxes, scores\n\n    def boxes_from_bitmap(self, pred, _bitmap, dest_width, dest_height):\n        '''\n        _bitmap: single map with shape (H, W),\n            whose values are binarized as {0, 1}\n        '''\n\n        assert len(_bitmap.shape) == 2\n        bitmap = _bitmap.cpu().numpy()  # The first channel\n        pred = pred.cpu().detach().numpy()\n        height, width = bitmap.shape\n        contours, _ = cv2.findContours((bitmap * 255).astype(np.uint8), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n        num_contours = min(len(contours), self.max_candidates)\n        boxes = np.zeros((num_contours, 4, 2), dtype=np.int16)\n        scores = np.zeros((num_contours,), dtype=np.float32)\n\n        for index in range(num_contours):\n            contour = contours[index].squeeze(1)\n            points, sside = self.get_mini_boxes(contour)\n            if sside < self.min_size:\n                continue\n            points = np.array(points)\n            score = self.box_score_fast(pred, contour)\n            if self.box_thresh > score:\n                continue\n\n            box = self.unclip(points, unclip_ratio=self.unclip_ratio).reshape(-1, 1, 2)\n            box, sside = self.get_mini_boxes(box)\n            if sside < self.min_size + 2:\n                continue\n            box = np.array(box)\n            if not isinstance(dest_width, int):\n                dest_width = dest_width.item()\n                dest_height = dest_height.item()\n\n            box[:, 0] = np.clip(np.round(box[:, 0] / width * dest_width), 0, dest_width)\n            box[:, 1] = np.clip(np.round(box[:, 1] / height * dest_height), 0, dest_height)\n            boxes[index, :, :] = box.astype(np.int16)\n            scores[index] = score\n        return boxes, scores\n\n    def unclip(self, box, unclip_ratio=1.5):\n        poly = Polygon(box)\n        distance = poly.area * unclip_ratio / poly.length\n        offset = pyclipper.PyclipperOffset()\n        offset.AddPath(box, pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)\n        expanded = np.array(offset.Execute(distance))\n        return expanded\n\n    def get_mini_boxes(self, contour):\n        bounding_box = cv2.minAreaRect(contour)\n        points = sorted(list(cv2.boxPoints(bounding_box)), key=lambda x: x[0])\n\n        index_1, index_2, index_3, index_4 = 0, 1, 2, 3\n        if points[1][1] > points[0][1]:\n            index_1 = 0\n            index_4 = 1\n        else:\n            index_1 = 1\n            index_4 = 0\n        if points[3][1] > points[2][1]:\n            index_2 = 2\n            index_3 = 3\n        else:\n            index_2 = 3\n            index_3 = 2\n\n        box = [points[index_1], points[index_2], points[index_3], points[index_4]]\n        return box, min(bounding_box[1])\n\n    def box_score_fast(self, bitmap, _box):\n        h, w = bitmap.shape[:2]\n        box = _box.copy()\n        xmin = np.clip(np.floor(box[:, 0].min()).astype(np.int), 0, w - 1)\n        xmax = np.clip(np.ceil(box[:, 0].max()).astype(np.int), 0, w - 1)\n        ymin = np.clip(np.floor(box[:, 1].min()).astype(np.int), 0, h - 1)\n        ymax = np.clip(np.ceil(box[:, 1].max()).astype(np.int), 0, h - 1)\n\n        mask = np.zeros((ymax - ymin + 1, xmax - xmin + 1), dtype=np.uint8)\n        box[:, 0] = box[:, 0] - xmin\n        box[:, 1] = box[:, 1] - ymin\n        cv2.fillPoly(mask, box.reshape(1, -1, 2).astype(np.int32), 1)\n        return cv2.mean(bitmap[ymin:ymax + 1, xmin:xmax + 1], mask)[0]\n"""
tools/__init__.py,0,b'# -*- coding: utf-8 -*-\n# @Time    : 2019/12/8 13:14\n# @Author  : zhoujun'
tools/eval.py,8,"b'# -*- coding: utf-8 -*-\n# @Time    : 2018/6/11 15:54\n# @Author  : zhoujun\nimport os\nimport sys\n\nproject = \'DBNet.pytorch\'  # \xe5\xb7\xa5\xe4\xbd\x9c\xe9\xa1\xb9\xe7\x9b\xae\xe6\xa0\xb9\xe7\x9b\xae\xe5\xbd\x95\nsys.path.append(os.getcwd().split(project)[0] + project)\n\nimport argparse\nimport time\nimport torch\nfrom tqdm.auto import tqdm\n\n\nclass EVAL():\n    def __init__(self, model_path, gpu_id=0):\n        from models import build_model\n        from data_loader import get_dataloader\n        from post_processing import get_post_processing\n        from utils import get_metric\n        self.gpu_id = gpu_id\n        if self.gpu_id is not None and isinstance(self.gpu_id, int) and torch.cuda.is_available():\n            self.device = torch.device(""cuda:%s"" % self.gpu_id)\n            torch.backends.cudnn.benchmark = True\n        else:\n            self.device = torch.device(""cpu"")\n        checkpoint = torch.load(model_path, map_location=torch.device(\'cpu\'))\n        config = checkpoint[\'config\']\n        config[\'arch\'][\'backbone\'][\'pretrained\'] = False\n\n        self.validate_loader = get_dataloader(config[\'dataset\'][\'validate\'], config[\'distributed\'])\n\n        self.model = build_model(config[\'arch\'].pop(\'type\'), **config[\'arch\'])\n        self.model.load_state_dict(checkpoint[\'state_dict\'])\n        self.model.to(self.device)\n\n        self.post_process = get_post_processing(config[\'post_processing\'])\n        self.metric_cls = get_metric(config[\'metric\'])\n\n    def eval(self):\n        self.model.eval()\n        # torch.cuda.empty_cache()  # speed up evaluating after training finished\n        raw_metrics = []\n        total_frame = 0.0\n        total_time = 0.0\n        for i, batch in tqdm(enumerate(self.validate_loader), total=len(self.validate_loader), desc=\'test model\'):\n            with torch.no_grad():\n                # \xe6\x95\xb0\xe6\x8d\xae\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xbd\xac\xe6\x8d\xa2\xe5\x92\x8c\xe4\xb8\xa2\xe5\x88\xb0gpu\n                for key, value in batch.items():\n                    if value is not None:\n                        if isinstance(value, torch.Tensor):\n                            batch[key] = value.to(self.device)\n                start = time.time()\n                preds = self.model(batch[\'img\'])\n                boxes, scores = self.post_process(batch, preds,is_output_polygon=self.metric_cls.is_output_polygon)\n                total_frame += batch[\'img\'].size()[0]\n                total_time += time.time() - start\n                raw_metric = self.metric_cls.validate_measure(batch, (boxes, scores))\n                raw_metrics.append(raw_metric)\n        metrics = self.metric_cls.gather_measure(raw_metrics)\n        print(\'FPS:{}\'.format(total_frame / total_time))\n        return metrics[\'recall\'].avg, metrics[\'precision\'].avg, metrics[\'fmeasure\'].avg\n\n\ndef init_args():\n    parser = argparse.ArgumentParser(description=\'DBNet.pytorch\')\n    parser.add_argument(\'--model_path\', required=False,default=\'output/DBNet_resnet18_FPN_DBHead/checkpoint/1.pth\', type=str)\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \'__main__\':\n    args = init_args()\n    eval = EVAL(args.model_path)\n    result = eval.eval()\n    print(result)\n'"
tools/predict.py,8,"b'# -*- coding: utf-8 -*-\n# @Time    : 2019/8/24 12:06\n# @Author  : zhoujun\n\nimport os\nimport sys\n\nproject = \'DBNet.pytorch\'  # \xe5\xb7\xa5\xe4\xbd\x9c\xe9\xa1\xb9\xe7\x9b\xae\xe6\xa0\xb9\xe7\x9b\xae\xe5\xbd\x95\nsys.path.append(os.getcwd().split(project)[0] + project)\nimport time\nimport math\nimport cv2\nimport torch\n\nfrom data_loader import get_transforms\nfrom models import build_model\nfrom post_processing import get_post_processing\n\n\ndef resize_image(img, short_size):\n    height, width, _ = img.shape\n    if height < width:\n        new_height = short_size\n        new_width = new_height / height * width\n    else:\n        new_width = short_size\n        new_height = new_width / width * height\n    new_height = int(round(new_height / 32) * 32)\n    new_width = int(round(new_width / 32) * 32)\n    resized_img = cv2.resize(img, (new_width, new_height))\n    return resized_img\n\n\nclass Pytorch_model:\n    def __init__(self, model_path, post_p_thre=0.7, gpu_id=None):\n        \'\'\'\n        \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96pytorch\xe6\xa8\xa1\xe5\x9e\x8b\n        :param model_path: \xe6\xa8\xa1\xe5\x9e\x8b\xe5\x9c\xb0\xe5\x9d\x80(\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x98\xaf\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe6\x88\x96\xe8\x80\x85\xe5\x8f\x82\xe6\x95\xb0\xe5\x92\x8c\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe4\xb8\x80\xe8\xb5\xb7\xe4\xbf\x9d\xe5\xad\x98\xe7\x9a\x84\xe6\x96\x87\xe4\xbb\xb6)\n        :param gpu_id: \xe5\x9c\xa8\xe5\x93\xaa\xe4\xb8\x80\xe5\x9d\x97gpu\xe4\xb8\x8a\xe8\xbf\x90\xe8\xa1\x8c\n        \'\'\'\n        self.gpu_id = gpu_id\n\n        if self.gpu_id is not None and isinstance(self.gpu_id, int) and torch.cuda.is_available():\n            self.device = torch.device(""cuda:%s"" % self.gpu_id)\n        else:\n            self.device = torch.device(""cpu"")\n        print(\'device:\', self.device)\n        checkpoint = torch.load(model_path, map_location=self.device)\n\n        config = checkpoint[\'config\']\n        config[\'arch\'][\'backbone\'][\'pretrained\'] = False\n        self.model = build_model(config[\'arch\'].pop(\'type\'), **config[\'arch\'])\n        self.post_process = get_post_processing(config[\'post_processing\'])\n        self.post_process.box_thresh = post_p_thre\n        self.img_mode = config[\'dataset\'][\'train\'][\'dataset\'][\'args\'][\'img_mode\']\n        self.model.load_state_dict(checkpoint[\'state_dict\'])\n        self.model.to(self.device)\n        self.model.eval()\n\n        self.transform = []\n        for t in config[\'dataset\'][\'train\'][\'dataset\'][\'args\'][\'transforms\']:\n            if t[\'type\'] in [\'ToTensor\', \'Normalize\']:\n                self.transform.append(t)\n        self.transform = get_transforms(self.transform)\n\n    def predict(self, img_path: str, is_output_polygon=False, short_size: int = 1024):\n        \'\'\'\n        \xe5\xaf\xb9\xe4\xbc\xa0\xe5\x85\xa5\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f\xe8\xbf\x9b\xe8\xa1\x8c\xe9\xa2\x84\xe6\xb5\x8b\xef\xbc\x8c\xe6\x94\xaf\xe6\x8c\x81\xe5\x9b\xbe\xe5\x83\x8f\xe5\x9c\xb0\xe5\x9d\x80,opecv \xe8\xaf\xbb\xe5\x8f\x96\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c\xe5\x81\x8f\xe6\x85\xa2\n        :param img_path: \xe5\x9b\xbe\xe5\x83\x8f\xe5\x9c\xb0\xe5\x9d\x80\n        :param is_numpy:\n        :return:\n        \'\'\'\n        assert os.path.exists(img_path), \'file is not exists\'\n        img = cv2.imread(img_path, 1 if self.img_mode != \'GRAY\' else 0)\n        if self.img_mode == \'RGB\':\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        h, w = img.shape[:2]\n        img = resize_image(img, short_size)\n        # \xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe7\x94\xb1(w,h)\xe5\x8f\x98\xe4\xb8\xba(1,img_channel,h,w)\n        tensor = self.transform(img)\n        tensor = tensor.unsqueeze_(0)\n\n        tensor = tensor.to(self.device)\n        batch = {\'shape\': [(h, w)]}\n        with torch.no_grad():\n            if str(self.device).__contains__(\'cuda\'):\n                torch.cuda.synchronize(self.device)\n            start = time.time()\n            preds = self.model(tensor)\n            if str(self.device).__contains__(\'cuda\'):\n                torch.cuda.synchronize(self.device)\n            box_list, score_list = self.post_process(batch, preds, is_output_polygon=is_output_polygon)\n            box_list, score_list = box_list[0], score_list[0]\n            if len(box_list) > 0:\n                if is_output_polygon:\n                    idx = [x.sum() > 0 for x in box_list]\n                    box_list = [box_list[i] for i, v in enumerate(idx) if v]\n                    score_list = [score_list[i] for i, v in enumerate(idx) if v]\n                else:\n                    idx = box_list.reshape(box_list.shape[0], -1).sum(axis=1) > 0  # \xe5\x8e\xbb\xe6\x8e\x89\xe5\x85\xa8\xe4\xb8\xba0\xe7\x9a\x84\xe6\xa1\x86\n                    box_list, score_list = box_list[idx], score_list[idx]\n            else:\n                box_list, score_list = [], []\n            t = time.time() - start\n        return preds[0, 0, :, :].detach().cpu().numpy(), box_list, score_list, t\n\n\ndef save_depoly(model, input, save_path):\n    traced_script_model = torch.jit.trace(model, input)\n    traced_script_model.save(save_path)\n\n\ndef init_args():\n    import argparse\n    parser = argparse.ArgumentParser(description=\'DBNet.pytorch\')\n    parser.add_argument(\'--model_path\', default=\'model_best.pth\', type=str)\n    parser.add_argument(\'--input_folder\', default=\'./test/input\', type=str, help=\'img path for predict\')\n    parser.add_argument(\'--output_folder\', default=\'./test/output\', type=str, help=\'img path for output\')\n    parser.add_argument(\'--thre\', default=0.3, help=\'the thresh of post_processing\')\n    parser.add_argument(\'--polygon\', action=\'store_true\', help=\'output polygon or box\')\n    parser.add_argument(\'--show\', action=\'store_true\', help=\'show result\')\n    parser.add_argument(\'--save_resut\', action=\'store_true\', help=\'save box and score to txt file\')\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \'__main__\':\n    import pathlib\n    from tqdm import tqdm\n    import matplotlib.pyplot as plt\n    from utils.util import show_img, draw_bbox, save_result, get_file_list\n\n    args = init_args()\n    print(args)\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = str(\'0\')\n    # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe7\xbd\x91\xe7\xbb\x9c\n    model = Pytorch_model(args.model_path, post_p_thre=args.thre, gpu_id=0)\n    img_folder = pathlib.Path(args.input_folder)\n    for img_path in tqdm(get_file_list(args.input_folder, p_postfix=[\'.jpg\'])):\n        preds, boxes_list, score_list, t = model.predict(img_path, is_output_polygon=args.polygon)\n        img = draw_bbox(cv2.imread(img_path)[:, :, ::-1], boxes_list)\n        if args.show:\n            show_img(preds)\n            show_img(img, title=os.path.basename(img_path))\n            plt.show()\n        # \xe4\xbf\x9d\xe5\xad\x98\xe7\xbb\x93\xe6\x9e\x9c\xe5\x88\xb0\xe8\xb7\xaf\xe5\xbe\x84\n        os.makedirs(args.output_folder, exist_ok=True)\n        img_path = pathlib.Path(img_path)\n        output_path = os.path.join(args.output_folder, img_path.stem + \'_result.jpg\')\n        pred_path = os.path.join(args.output_folder, img_path.stem + \'_pred.jpg\')\n        cv2.imwrite(output_path, img[:, :, ::-1])\n        cv2.imwrite(pred_path, preds * 255)\n        save_result(output_path.replace(\'_result.jpg\', \'.txt\'), boxes_list, score_list, args.polygon)\n'"
tools/train.py,3,"b'# -*- coding: utf-8 -*-\n# @Time    : 2019/8/23 22:00\n# @Author  : zhoujun\n\nfrom __future__ import print_function\n\nimport argparse\nimport os\n\nimport anyconfig\n\n\ndef init_args():\n    parser = argparse.ArgumentParser(description=\'DBNet.pytorch\')\n    parser.add_argument(\'--config_file\', default=\'config/open_dataset_resnet18_FPN_DBhead_polyLR.yaml\', type=str)\n    parser.add_argument(\'--local_rank\', dest=\'local_rank\', default=0, type=int, help=\'Use distributed training\')\n\n    args = parser.parse_args()\n    return args\n\n\ndef main(config):\n    import torch\n    from models import build_model, build_loss\n    from data_loader import get_dataloader\n    from trainer import Trainer\n    from post_processing import get_post_processing\n    from utils import get_metric\n    if torch.cuda.device_count() > 1:\n        torch.cuda.set_device(args.local_rank)\n        torch.distributed.init_process_group(backend=""nccl"", init_method=""env://"", world_size=torch.cuda.device_count(), rank=args.local_rank)\n        config[\'distributed\'] = True\n    else:\n        config[\'distributed\'] = False\n    config[\'local_rank\'] = args.local_rank\n\n    train_loader = get_dataloader(config[\'dataset\'][\'train\'], config[\'distributed\'])\n    assert train_loader is not None\n    if \'validate\' in config[\'dataset\']:\n        validate_loader = get_dataloader(config[\'dataset\'][\'validate\'], False)\n    else:\n        validate_loader = None\n\n    criterion = build_loss(config[\'loss\'].pop(\'type\'), **config[\'loss\']).cuda()\n\n    config[\'arch\'][\'backbone\'][\'in_channels\'] = 3 if config[\'dataset\'][\'train\'][\'dataset\'][\'args\'][\'img_mode\'] != \'GRAY\' else 1\n    config[\'arch\'][\'backbone\'][\'pretrained\'] = False\n    model = build_model(config[\'arch\'][\'type\'], **config[\'arch\'])\n\n    post_p = get_post_processing(config[\'post_processing\'])\n    metric = get_metric(config[\'metric\'])\n\n    trainer = Trainer(config=config,\n                      model=model,\n                      criterion=criterion,\n                      train_loader=train_loader,\n                      post_process=post_p,\n                      metric_cls=metric,\n                      validate_loader=validate_loader)\n    trainer.train()\n\n\nif __name__ == \'__main__\':\n    import sys\n\n    project = \'DBNet.pytorch\'  # \xe5\xb7\xa5\xe4\xbd\x9c\xe9\xa1\xb9\xe7\x9b\xae\xe6\xa0\xb9\xe7\x9b\xae\xe5\xbd\x95\n    sys.path.append(os.getcwd().split(project)[0] + project)\n\n    from utils import parse_config\n\n    args = init_args()\n    assert os.path.exists(args.config_file)\n    config = anyconfig.load(open(args.config_file, \'rb\'))\n    if \'base\' in config:\n        config = parse_config(config)\n    main(config)\n'"
trainer/__init__.py,0,b'# -*- coding: utf-8 -*-\n# @Time    : 2019/8/23 21:58\n# @Author  : zhoujun\nfrom .trainer import Trainer'
trainer/trainer.py,6,"b""# -*- coding: utf-8 -*-\n# @Time    : 2019/8/23 21:58\n# @Author  : zhoujun\nimport time\n\nimport torch\nimport torchvision.utils as vutils\nfrom tqdm import tqdm\n\nfrom base import BaseTrainer\nfrom utils import WarmupPolyLR, runningScore, cal_text_score\n\n\nclass Trainer(BaseTrainer):\n    def __init__(self, config, model, criterion, train_loader, validate_loader, metric_cls, post_process=None):\n        super(Trainer, self).__init__(config, model, criterion)\n        self.show_images_iter = self.config['trainer']['show_images_iter']\n        self.train_loader = train_loader\n        if validate_loader is None:\n            assert post_process is not None and metric_cls is not None\n        self.validate_loader = validate_loader\n        self.post_process = post_process\n        self.metric_cls = metric_cls\n        self.train_loader_len = len(train_loader)\n        if self.config['lr_scheduler']['type'] == 'WarmupPolyLR':\n            warmup_iters = config['lr_scheduler']['args']['warmup_epoch'] * self.train_loader_len\n            if self.start_epoch > 1:\n                self.config['lr_scheduler']['args']['last_epoch'] = (self.start_epoch - 1) * self.train_loader_len\n            self.scheduler = WarmupPolyLR(self.optimizer, max_iters=self.epochs * self.train_loader_len,\n                                          warmup_iters=warmup_iters, **config['lr_scheduler']['args'])\n        if self.validate_loader is not None:\n            self.logger_info(\n                'train dataset has {} samples,{} in dataloader, validate dataset has {} samples,{} in dataloader'.format(\n                    len(self.train_loader.dataset), self.train_loader_len, len(self.validate_loader.dataset), len(self.validate_loader)))\n        else:\n            self.logger_info('train dataset has {} samples,{} in dataloader'.format(len(self.train_loader.dataset), self.train_loader_len))\n\n    def _train_epoch(self, epoch):\n        self.model.train()\n        epoch_start = time.time()\n        batch_start = time.time()\n        train_loss = 0.\n        running_metric_text = runningScore(2)\n        lr = self.optimizer.param_groups[0]['lr']\n\n        for i, batch in enumerate(self.train_loader):\n            if i >= self.train_loader_len:\n                break\n            self.global_step += 1\n            lr = self.optimizer.param_groups[0]['lr']\n\n            # \xe6\x95\xb0\xe6\x8d\xae\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xbd\xac\xe6\x8d\xa2\xe5\x92\x8c\xe4\xb8\xa2\xe5\x88\xb0gpu\n            for key, value in batch.items():\n                if value is not None:\n                    if isinstance(value, torch.Tensor):\n                        batch[key] = value.to(self.device)\n            cur_batch_size = batch['img'].size()[0]\n\n            preds = self.model(batch['img'])\n            loss_dict = self.criterion(preds, batch)\n            # backward\n            self.optimizer.zero_grad()\n            loss_dict['loss'].backward()\n            self.optimizer.step()\n            if self.config['lr_scheduler']['type'] == 'WarmupPolyLR':\n                self.scheduler.step()\n            # acc iou\n            score_shrink_map = cal_text_score(preds[:, 0, :, :], batch['shrink_map'], batch['shrink_mask'], running_metric_text,\n                                              thred=self.config['post_processing']['args']['thresh'])\n\n            # loss \xe5\x92\x8c acc \xe8\xae\xb0\xe5\xbd\x95\xe5\x88\xb0\xe6\x97\xa5\xe5\xbf\x97\n            loss_str = 'loss: {:.4f}, '.format(loss_dict['loss'].item())\n            for idx, (key, value) in enumerate(loss_dict.items()):\n                loss_dict[key] = value.item()\n                if key == 'loss':\n                    continue\n                loss_str += '{}: {:.4f}'.format(key, loss_dict[key])\n                if idx < len(loss_dict) - 1:\n                    loss_str += ', '\n\n            train_loss += loss_dict['loss']\n            acc = score_shrink_map['Mean Acc']\n            iou_shrink_map = score_shrink_map['Mean IoU']\n\n            if self.global_step % self.log_iter == 0:\n                batch_time = time.time() - batch_start\n                self.logger_info(\n                    '[{}/{}], [{}/{}], global_step: {}, speed: {:.1f} samples/sec, acc: {:.4f}, iou_shrink_map: {:.4f}, {}, lr:{:.6}, time:{:.2f}'.format(\n                        epoch, self.epochs, i + 1, self.train_loader_len, self.global_step, self.log_iter * cur_batch_size / batch_time, acc,\n                        iou_shrink_map, loss_str, lr, batch_time))\n                batch_start = time.time()\n\n            if self.tensorboard_enable and self.config['local_rank'] == 0:\n                # write tensorboard\n                for key, value in loss_dict.items():\n                    self.writer.add_scalar('TRAIN/LOSS/{}'.format(key), value, self.global_step)\n                self.writer.add_scalar('TRAIN/ACC_IOU/acc', acc, self.global_step)\n                self.writer.add_scalar('TRAIN/ACC_IOU/iou_shrink_map', iou_shrink_map, self.global_step)\n                self.writer.add_scalar('TRAIN/lr', lr, self.global_step)\n                if self.global_step % self.show_images_iter == 0:\n                    # show images on tensorboard\n                    self.inverse_normalize(batch['img'])\n                    self.writer.add_images('TRAIN/imgs', batch['img'], self.global_step)\n                    # shrink_labels and threshold_labels\n                    shrink_labels = batch['shrink_map']\n                    threshold_labels = batch['threshold_map']\n                    shrink_labels[shrink_labels <= 0.5] = 0\n                    shrink_labels[shrink_labels > 0.5] = 1\n                    show_label = torch.cat([shrink_labels, threshold_labels])\n                    show_label = vutils.make_grid(show_label.unsqueeze(1), nrow=cur_batch_size, normalize=False, padding=20, pad_value=1)\n                    self.writer.add_image('TRAIN/gt', show_label, self.global_step)\n                    # model output\n                    show_pred = []\n                    for kk in range(preds.shape[1]):\n                        show_pred.append(preds[:, kk, :, :])\n                    show_pred = torch.cat(show_pred)\n                    show_pred = vutils.make_grid(show_pred.unsqueeze(1), nrow=cur_batch_size, normalize=False, padding=20, pad_value=1)\n                    self.writer.add_image('TRAIN/preds', show_pred, self.global_step)\n        return {'train_loss': train_loss / self.train_loader_len, 'lr': lr, 'time': time.time() - epoch_start,\n                'epoch': epoch}\n\n    def _eval(self, epoch):\n        self.model.eval()\n        # torch.cuda.empty_cache()  # speed up evaluating after training finished\n        raw_metrics = []\n        total_frame = 0.0\n        total_time = 0.0\n        for i, batch in tqdm(enumerate(self.validate_loader), total=len(self.validate_loader), desc='test model'):\n            with torch.no_grad():\n                # \xe6\x95\xb0\xe6\x8d\xae\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xbd\xac\xe6\x8d\xa2\xe5\x92\x8c\xe4\xb8\xa2\xe5\x88\xb0gpu\n                for key, value in batch.items():\n                    if value is not None:\n                        if isinstance(value, torch.Tensor):\n                            batch[key] = value.to(self.device)\n                start = time.time()\n                preds = self.model(batch['img'])\n                boxes, scores = self.post_process(batch, preds,is_output_polygon=self.metric_cls.is_output_polygon)\n                total_frame += batch['img'].size()[0]\n                total_time += time.time() - start\n                raw_metric = self.metric_cls.validate_measure(batch, (boxes, scores))\n                raw_metrics.append(raw_metric)\n        metrics = self.metric_cls.gather_measure(raw_metrics)\n        self.logger_info('FPS:{}'.format(total_frame / total_time))\n        return metrics['recall'].avg, metrics['precision'].avg, metrics['fmeasure'].avg\n\n    def _on_epoch_finish(self):\n        self.logger_info('[{}/{}], train_loss: {:.4f}, time: {:.4f}, lr: {}'.format(\n            self.epoch_result['epoch'], self.epochs, self.epoch_result['train_loss'], self.epoch_result['time'],\n            self.epoch_result['lr']))\n        net_save_path = '{}/model_latest.pth'.format(self.checkpoint_dir)\n\n        if self.config['local_rank'] == 0:\n            save_best = False\n            if self.validate_loader is not None and self.metric_cls is not None:  # \xe4\xbd\xbf\xe7\x94\xa8f1\xe4\xbd\x9c\xe4\xb8\xba\xe6\x9c\x80\xe4\xbc\x98\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x8c\x87\xe6\xa0\x87\n                recall, precision, hmean = self._eval(self.epoch_result['epoch'])\n\n                if self.tensorboard_enable:\n                    self.writer.add_scalar('EVAL/recall', recall, self.global_step)\n                    self.writer.add_scalar('EVAL/precision', precision, self.global_step)\n                    self.writer.add_scalar('EVAL/hmean', hmean, self.global_step)\n                self.logger_info('test: recall: {:.6f}, precision: {:.6f}, f1: {:.6f}'.format(recall, precision, hmean))\n\n                if hmean >= self.metrics['hmean']:\n                    save_best = True\n                    self.metrics['train_loss'] = self.epoch_result['train_loss']\n                    self.metrics['hmean'] = hmean\n                    self.metrics['precision'] = precision\n                    self.metrics['recall'] = recall\n                    self.metrics['best_model_epoch'] = self.epoch_result['epoch']\n            else:\n                if self.epoch_result['train_loss'] <= self.metrics['train_loss']:\n                    save_best = True\n                    self.metrics['train_loss'] = self.epoch_result['train_loss']\n                    self.metrics['best_model_epoch'] = self.epoch_result['epoch']\n            best_str = 'current best, '\n            for k, v in self.metrics.items():\n                best_str += '{}: {:.6f}, '.format(k, v)\n            self.logger_info(best_str)\n            self._save_checkpoint(self.epoch_result['epoch'], net_save_path, save_best)\n\n    def _on_train_finish(self):\n        for k, v in self.metrics.items():\n            self.logger_info('{}:{}'.format(k, v))\n        self.logger_info('finish train')\n"""
utils/__init__.py,0,b'# -*- coding: utf-8 -*-\n# @Time    : 2019/8/23 21:58\n# @Author  : zhoujun\nfrom .util import *\nfrom .metrics import *\nfrom .schedulers import *\nfrom .cal_recall.script import  cal_recall_precison_f1\nfrom .ocr_metric import get_metric'
utils/compute_mean_std.py,0,"b'# -*- coding: utf-8 -*-\n# @Time    : 2019/12/7 14:46\n# @Author  : zhoujun\n\nimport numpy as np\nimport cv2\nimport os\nimport random\nfrom tqdm import tqdm\n# calculate means and std\ntrain_txt_path = \'./train_val_list.txt\'\n\nCNum = 10000  # \xe6\x8c\x91\xe9\x80\x89\xe5\xa4\x9a\xe5\xb0\x91\xe5\x9b\xbe\xe7\x89\x87\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xae\xa1\xe7\xae\x97\n\nimg_h, img_w = 640, 640\nimgs = np.zeros([img_w, img_h, 3, 1])\nmeans, stdevs = [], []\n\nwith open(train_txt_path, \'r\') as f:\n    lines = f.readlines()\n    random.shuffle(lines)  # shuffle , \xe9\x9a\x8f\xe6\x9c\xba\xe6\x8c\x91\xe9\x80\x89\xe5\x9b\xbe\xe7\x89\x87\n\n    for i in tqdm(range(CNum)):\n        img_path = lines[i].split(\'\\t\')[0]\n\n        img = cv2.imread(img_path)\n        img = cv2.resize(img, (img_h, img_w))\n        img = img[:, :, :, np.newaxis]\n\n        imgs = np.concatenate((imgs, img), axis=3)\n#         print(i)\n\nimgs = imgs.astype(np.float32) / 255.\n\nfor i in tqdm(range(3)):\n    pixels = imgs[:, :, i, :].ravel()  # \xe6\x8b\x89\xe6\x88\x90\xe4\xb8\x80\xe8\xa1\x8c\n    means.append(np.mean(pixels))\n    stdevs.append(np.std(pixels))\n\n# cv2 \xe8\xaf\xbb\xe5\x8f\x96\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f\xe6\xa0\xbc\xe5\xbc\x8f\xe4\xb8\xbaBGR\xef\xbc\x8cPIL/Skimage\xe8\xaf\xbb\xe5\x8f\x96\xe5\x88\xb0\xe7\x9a\x84\xe9\x83\xbd\xe6\x98\xafRGB\xe4\xb8\x8d\xe7\x94\xa8\xe8\xbd\xac\nmeans.reverse()  # BGR --> RGB\nstdevs.reverse()\n\nprint(""normMean = {}"".format(means))\nprint(""normStd = {}"".format(stdevs))\nprint(\'transforms.Normalize(normMean = {}, normStd = {})\'.format(means, stdevs))'"
utils/make_trainfile.py,0,"b""# -*- coding: utf-8 -*-\n# @Time    : 2019/8/24 12:06\n# @Author  : zhoujun\nimport os\nimport glob\nimport pathlib\n\ndata_path = r'E:\\zj\\dataset\\icdar2015\\test'\n# data_path/img \xe5\xad\x98\xe6\x94\xbe\xe5\x9b\xbe\xe7\x89\x87\n# data_path/gt \xe5\xad\x98\xe6\x94\xbe\xe6\xa0\x87\xe7\xad\xbe\xe6\x96\x87\xe4\xbb\xb6\n\nf_w = open(os.path.join(data_path, 'test.txt'), 'w', encoding='utf8')\nfor img_path in glob.glob(data_path + '/img/*.jpg', recursive=True):\n    d = pathlib.Path(img_path)\n    label_path = os.path.join(data_path, 'gt', ('gt_' + str(d.stem) + '.txt'))\n    if os.path.exists(img_path) and os.path.exists(label_path):\n        print(img_path, label_path)\n    else:\n        print('\xe4\xb8\x8d\xe5\xad\x98\xe5\x9c\xa8', img_path, label_path)\n    f_w.write('{}\\t{}\\n'.format(img_path, label_path))\nf_w.close()"""
utils/metrics.py,0,"b'# Adapted from score written by wkentaro\n# https://github.com/wkentaro/pytorch-fcn/blob/master/torchfcn/utils.py\n\nimport numpy as np\n\n\nclass runningScore(object):\n\n    def __init__(self, n_classes):\n        self.n_classes = n_classes\n        self.confusion_matrix = np.zeros((n_classes, n_classes))\n\n    def _fast_hist(self, label_true, label_pred, n_class):\n        mask = (label_true >= 0) & (label_true < n_class)\n\n        if np.sum((label_pred[mask] < 0)) > 0:\n            print(label_pred[label_pred < 0])\n        hist = np.bincount(n_class * label_true[mask].astype(int) +\n                           label_pred[mask], minlength=n_class ** 2).reshape(n_class, n_class)\n        return hist\n\n    def update(self, label_trues, label_preds):\n        # print label_trues.dtype, label_preds.dtype\n        for lt, lp in zip(label_trues, label_preds):\n            try:\n                self.confusion_matrix += self._fast_hist(lt.flatten(), lp.flatten(), self.n_classes)\n            except:\n                pass\n\n    def get_scores(self):\n        """"""Returns accuracy score evaluation result.\n            - overall accuracy\n            - mean accuracy\n            - mean IU\n            - fwavacc\n        """"""\n        hist = self.confusion_matrix\n        acc = np.diag(hist).sum() / (hist.sum() + 0.0001)\n        acc_cls = np.diag(hist) / (hist.sum(axis=1) + 0.0001)\n        acc_cls = np.nanmean(acc_cls)\n        iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist) + 0.0001)\n        mean_iu = np.nanmean(iu)\n        freq = hist.sum(axis=1) / (hist.sum() + 0.0001)\n        fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n        cls_iu = dict(zip(range(self.n_classes), iu))\n\n        return {\'Overall Acc\': acc,\n                \'Mean Acc\': acc_cls,\n                \'FreqW Acc\': fwavacc,\n                \'Mean IoU\': mean_iu, }, cls_iu\n\n    def reset(self):\n        self.confusion_matrix = np.zeros((self.n_classes, self.n_classes))\n'"
utils/schedulers.py,3,"b'""""""Popular Learning Rate Schedulers""""""\nfrom __future__ import division\n\nimport math\nfrom bisect import bisect_right\n\nimport torch\n\n__all__ = [\'LRScheduler\', \'WarmupMultiStepLR\', \'WarmupPolyLR\']\n\n\nclass LRScheduler(object):\n    r""""""Learning Rate Scheduler\n    Parameters\n    ----------\n    mode : str\n        Modes for learning rate scheduler.\n        Currently it supports \'constant\', \'step\', \'linear\', \'poly\' and \'cosine\'.\n    base_lr : float\n        Base learning rate, i.e. the starting learning rate.\n    target_lr : float\n        Target learning rate, i.e. the ending learning rate.\n        With constant mode target_lr is ignored.\n    niters : int\n        Number of iterations to be scheduled.\n    nepochs : int\n        Number of epochs to be scheduled.\n    iters_per_epoch : int\n        Number of iterations in each epoch.\n    offset : int\n        Number of iterations before this scheduler.\n    power : float\n        Power parameter of poly scheduler.\n    step_iter : list\n        A list of iterations to decay the learning rate.\n    step_epoch : list\n        A list of epochs to decay the learning rate.\n    step_factor : float\n        Learning rate decay factor.\n    """"""\n\n    def __init__(self, mode, base_lr=0.01, target_lr=0, niters=0, nepochs=0, iters_per_epoch=0,\n                 offset=0, power=0.9, step_iter=None, step_epoch=None, step_factor=0.1, warmup_epochs=0):\n        super(LRScheduler, self).__init__()\n        assert (mode in [\'constant\', \'step\', \'linear\', \'poly\', \'cosine\'])\n\n        if mode == \'step\':\n            assert (step_iter is not None or step_epoch is not None)\n        self.niters = niters\n        self.step = step_iter\n        epoch_iters = nepochs * iters_per_epoch\n        if epoch_iters > 0:\n            self.niters = epoch_iters\n            if step_epoch is not None:\n                self.step = [s * iters_per_epoch for s in step_epoch]\n\n        self.step_factor = step_factor\n        self.base_lr = base_lr\n        self.target_lr = base_lr if mode == \'constant\' else target_lr\n        self.offset = offset\n        self.power = power\n        self.warmup_iters = warmup_epochs * iters_per_epoch\n        self.mode = mode\n\n    def __call__(self, optimizer, num_update):\n        self.update(num_update)\n        assert self.learning_rate >= 0\n        self._adjust_learning_rate(optimizer, self.learning_rate)\n\n    def update(self, num_update):\n        N = self.niters - 1\n        T = num_update - self.offset\n        T = min(max(0, T), N)\n\n        if self.mode == \'constant\':\n            factor = 0\n        elif self.mode == \'linear\':\n            factor = 1 - T / N\n        elif self.mode == \'poly\':\n            factor = pow(1 - T / N, self.power)\n        elif self.mode == \'cosine\':\n            factor = (1 + math.cos(math.pi * T / N)) / 2\n        elif self.mode == \'step\':\n            if self.step is not None:\n                count = sum([1 for s in self.step if s <= T])\n                factor = pow(self.step_factor, count)\n            else:\n                factor = 1\n        else:\n            raise NotImplementedError\n\n        # warm up lr schedule\n        if self.warmup_iters > 0 and T < self.warmup_iters:\n            factor = factor * 1.0 * T / self.warmup_iters\n\n        if self.mode == \'step\':\n            self.learning_rate = self.base_lr * factor\n        else:\n            self.learning_rate = self.target_lr + (self.base_lr - self.target_lr) * factor\n\n    def _adjust_learning_rate(self, optimizer, lr):\n        optimizer.param_groups[0][\'lr\'] = lr\n        # enlarge the lr at the head\n        for i in range(1, len(optimizer.param_groups)):\n            optimizer.param_groups[i][\'lr\'] = lr * 10\n\n\n# separating MultiStepLR with WarmupLR\n# but the current LRScheduler design doesn\'t allow it\n# reference: https://github.com/facebookresearch/maskrcnn-benchmark/blob/master/maskrcnn_benchmark/solver/lr_scheduler.py\nclass WarmupMultiStepLR(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(self, optimizer, milestones, gamma=0.1, warmup_factor=1.0 / 3,\n                 warmup_iters=500, warmup_method=""linear"", last_epoch=-1, **kwargs):\n        super(WarmupMultiStepLR, self).__init__(optimizer, last_epoch)\n        if not list(milestones) == sorted(milestones):\n            raise ValueError(\n                ""Milestones should be a list of"" "" increasing integers. Got {}"", milestones)\n        if warmup_method not in (""constant"", ""linear""):\n            raise ValueError(\n                ""Only \'constant\' or \'linear\' warmup_method accepted got {}"".format(warmup_method))\n\n        self.milestones = milestones\n        self.gamma = gamma\n        self.warmup_factor = warmup_factor\n        self.warmup_iters = warmup_iters\n        self.warmup_method = warmup_method\n\n    def get_lr(self):\n        warmup_factor = 1\n        if self.last_epoch < self.warmup_iters:\n            if self.warmup_method == \'constant\':\n                warmup_factor = self.warmup_factor\n            elif self.warmup_factor == \'linear\':\n                alpha = float(self.last_epoch) / self.warmup_iters\n                warmup_factor = self.warmup_factor * (1 - alpha) + alpha\n        return [base_lr * warmup_factor * self.gamma ** bisect_right(self.milestones, self.last_epoch)\n                for base_lr in self.base_lrs]\n\n\nclass WarmupPolyLR(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(self, optimizer, target_lr=0, max_iters=0, power=0.9, warmup_factor=1.0 / 3,\n                 warmup_iters=500, warmup_method=\'linear\', last_epoch=-1, **kwargs):\n        if warmup_method not in (""constant"", ""linear""):\n            raise ValueError(\n                ""Only \'constant\' or \'linear\' warmup_method accepted ""\n                ""got {}"".format(warmup_method))\n\n        self.target_lr = target_lr\n        self.max_iters = max_iters\n        self.power = power\n        self.warmup_factor = warmup_factor\n        self.warmup_iters = warmup_iters\n        self.warmup_method = warmup_method\n\n        super(WarmupPolyLR, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        N = self.max_iters - self.warmup_iters\n        T = self.last_epoch - self.warmup_iters\n        if self.last_epoch < self.warmup_iters:\n            if self.warmup_method == \'constant\':\n                warmup_factor = self.warmup_factor\n            elif self.warmup_method == \'linear\':\n                alpha = float(self.last_epoch) / self.warmup_iters\n                warmup_factor = self.warmup_factor * (1 - alpha) + alpha\n            else:\n                raise ValueError(""Unknown warmup type."")\n            return [self.target_lr + (base_lr - self.target_lr) * warmup_factor for base_lr in self.base_lrs]\n        factor = pow(1 - T / N, self.power)\n        return [self.target_lr + (base_lr - self.target_lr) * factor for base_lr in self.base_lrs]\n\n\nif __name__ == \'__main__\':\n    import torch\n    from torchvision.models import resnet18\n\n    max_iter = 600 * 63\n    model = resnet18()\n    op = torch.optim.SGD(model.parameters(), 1e-3)\n    sc = WarmupPolyLR(op, max_iters=max_iter, power=0.9, warmup_iters=3 * 63, warmup_method=\'constant\')\n    lr = []\n    for i in range(max_iter):\n        sc.step()\n        print(i, sc.last_epoch, sc.get_lr()[0])\n        lr.append(sc.get_lr()[0])\n    from matplotlib import pyplot as plt\n\n    plt.plot(list(range(max_iter)), lr)\n    plt.show()\n'"
utils/util.py,0,"b'# -*- coding: utf-8 -*-\n# @Time    : 2019/8/23 21:59\n# @Author  : zhoujun\nimport json\nimport pathlib\nimport time\nimport os\nimport glob\nfrom natsort import natsorted\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef get_file_list(folder_path: str, p_postfix: list = None, sub_dir: bool = True) -> list:\n    """"""\n    \xe8\x8e\xb7\xe5\x8f\x96\xe6\x89\x80\xe7\xbb\x99\xe6\x96\x87\xe4\xbb\xb6\xe7\x9b\xae\xe5\xbd\x95\xe9\x87\x8c\xe7\x9a\x84\xe6\x8c\x87\xe5\xae\x9a\xe5\x90\x8e\xe7\xbc\x80\xe7\x9a\x84\xe6\x96\x87\xe4\xbb\xb6,\xe8\xaf\xbb\xe5\x8f\x96\xe6\x96\x87\xe4\xbb\xb6\xe5\x88\x97\xe8\xa1\xa8\xe7\x9b\xae\xe5\x89\x8d\xe4\xbd\xbf\xe7\x94\xa8\xe7\x9a\x84\xe6\x98\xaf os.walk \xe5\x92\x8c os.listdir \xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xa4\xe4\xb8\xaa\xe7\x9b\xae\xe5\x89\x8d\xe6\xaf\x94 pathlib \xe5\xbf\xab\xe5\xbe\x88\xe5\xa4\x9a\n    :param filder_path: \xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xe5\x90\x8d\xe7\xa7\xb0\n    :param p_postfix: \xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8e\xe7\xbc\x80,\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\xba [.*]\xe5\xb0\x86\xe8\xbf\x94\xe5\x9b\x9e\xe5\x85\xa8\xe9\x83\xa8\xe6\x96\x87\xe4\xbb\xb6\n    :param sub_dir: \xe6\x98\xaf\xe5\x90\xa6\xe6\x90\x9c\xe7\xb4\xa2\xe5\xad\x90\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\n    :return: \xe8\x8e\xb7\xe5\x8f\x96\xe5\x88\xb0\xe7\x9a\x84\xe6\x8c\x87\xe5\xae\x9a\xe7\xb1\xbb\xe5\x9e\x8b\xe7\x9a\x84\xe6\x96\x87\xe4\xbb\xb6\xe5\x88\x97\xe8\xa1\xa8\n    """"""\n    assert os.path.exists(folder_path) and os.path.isdir(folder_path)\n    if p_postfix is None:\n        p_postfix = [\'.jpg\']\n    if isinstance(p_postfix, str):\n        p_postfix = [p_postfix]\n    file_list = [x for x in glob.glob(folder_path + \'/**/*.*\', recursive=True) if\n                 os.path.splitext(x)[-1] in p_postfix or \'.*\' in p_postfix]\n    return natsorted(file_list)\n\n\ndef setup_logger(log_file_path: str = None):\n    import logging\n    logging._warn_preinit_stderr = 0\n    logger = logging.getLogger(\'DBNet.pytorch\')\n    formatter = logging.Formatter(\'%(asctime)s %(name)s %(levelname)s: %(message)s\')\n    ch = logging.StreamHandler()\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n    if log_file_path is not None:\n        file_handle = logging.FileHandler(log_file_path)\n        file_handle.setFormatter(formatter)\n        logger.addHandler(file_handle)\n    logger.setLevel(logging.DEBUG)\n    return logger\n\n\n# --exeTime\ndef exe_time(func):\n    def newFunc(*args, **args2):\n        t0 = time.time()\n        back = func(*args, **args2)\n        print(""{} cost {:.3f}s"".format(func.__name__, time.time() - t0))\n        return back\n\n    return newFunc\n\n\ndef load(file_path: str):\n    file_path = pathlib.Path(file_path)\n    func_dict = {\'.txt\': _load_txt, \'.json\': _load_json, \'.list\': _load_txt}\n    assert file_path.suffix in func_dict\n    return func_dict[file_path.suffix](file_path)\n\n\ndef _load_txt(file_path: str):\n    with open(file_path, \'r\', encoding=\'utf8\') as f:\n        content = [x.strip().strip(\'\\ufeff\').strip(\'\\xef\\xbb\\xbf\') for x in f.readlines()]\n    return content\n\n\ndef _load_json(file_path: str):\n    with open(file_path, \'r\', encoding=\'utf8\') as f:\n        content = json.load(f)\n    return content\n\n\ndef save(data, file_path):\n    file_path = pathlib.Path(file_path)\n    func_dict = {\'.txt\': _save_txt, \'.json\': _save_json}\n    assert file_path.suffix in func_dict\n    return func_dict[file_path.suffix](data, file_path)\n\n\ndef _save_txt(data, file_path):\n    """"""\n    \xe5\xb0\x86\xe4\xb8\x80\xe4\xb8\xaalist\xe7\x9a\x84\xe6\x95\xb0\xe7\xbb\x84\xe5\x86\x99\xe5\x85\xa5txt\xe6\x96\x87\xe4\xbb\xb6\xe9\x87\x8c\n    :param data:\n    :param file_path:\n    :return:\n    """"""\n    if not isinstance(data, list):\n        data = [data]\n    with open(file_path, mode=\'w\', encoding=\'utf8\') as f:\n        f.write(\'\\n\'.join(data))\n\n\ndef _save_json(data, file_path):\n    with open(file_path, \'w\', encoding=\'utf-8\') as json_file:\n        json.dump(data, json_file, ensure_ascii=False, indent=4)\n\n\ndef show_img(imgs: np.ndarray, title=\'img\'):\n    color = (len(imgs.shape) == 3 and imgs.shape[-1] == 3)\n    imgs = np.expand_dims(imgs, axis=0)\n    for i, img in enumerate(imgs):\n        plt.figure()\n        plt.title(\'{}_{}\'.format(title, i))\n        plt.imshow(img, cmap=None if color else \'gray\')\n    plt.show()\n\n\ndef draw_bbox(img_path, result, color=(255, 0, 0), thickness=2):\n    if isinstance(img_path, str):\n        img_path = cv2.imread(img_path)\n        # img_path = cv2.cvtColor(img_path, cv2.COLOR_BGR2RGB)\n    img_path = img_path.copy()\n    for point in result:\n        point = point.astype(int)\n        cv2.polylines(img_path, [point], True, color, thickness)\n    return img_path\n\n\ndef cal_text_score(texts, gt_texts, training_masks, running_metric_text, thred=0.5):\n    training_masks = training_masks.data.cpu().numpy()\n    pred_text = texts.data.cpu().numpy() * training_masks\n    pred_text[pred_text <= thred] = 0\n    pred_text[pred_text > thred] = 1\n    pred_text = pred_text.astype(np.int32)\n    gt_text = gt_texts.data.cpu().numpy() * training_masks\n    gt_text = gt_text.astype(np.int32)\n    running_metric_text.update(gt_text, pred_text)\n    score_text, _ = running_metric_text.get_scores()\n    return score_text\n\n\ndef order_points_clockwise(pts):\n    rect = np.zeros((4, 2), dtype=""float32"")\n    s = pts.sum(axis=1)\n    rect[0] = pts[np.argmin(s)]\n    rect[2] = pts[np.argmax(s)]\n    diff = np.diff(pts, axis=1)\n    rect[1] = pts[np.argmin(diff)]\n    rect[3] = pts[np.argmax(diff)]\n    return rect\n\n\ndef order_points_clockwise_list(pts):\n    pts = pts.tolist()\n    pts.sort(key=lambda x: (x[1], x[0]))\n    pts[:2] = sorted(pts[:2], key=lambda x: x[0])\n    pts[2:] = sorted(pts[2:], key=lambda x: -x[0])\n    pts = np.array(pts)\n    return pts\n\n\ndef get_datalist(train_data_path):\n    """"""\n    \xe8\x8e\xb7\xe5\x8f\x96\xe8\xae\xad\xe7\xbb\x83\xe5\x92\x8c\xe9\xaa\x8c\xe8\xaf\x81\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xaelist\n    :param train_data_path: \xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84dataset\xe6\x96\x87\xe4\xbb\xb6\xe5\x88\x97\xe8\xa1\xa8\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x96\x87\xe4\xbb\xb6\xe5\x86\x85\xe4\xbb\xa5\xe5\xa6\x82\xe4\xb8\x8b\xe6\xa0\xbc\xe5\xbc\x8f\xe5\xad\x98\xe5\x82\xa8 \xe2\x80\x98path/to/img\\tlabel\xe2\x80\x99\n    :return:\n    """"""\n    train_data = []\n    for p in train_data_path:\n        with open(p, \'r\', encoding=\'utf-8\') as f:\n            for line in f.readlines():\n                line = line.strip(\'\\n\').replace(\'.jpg \', \'.jpg\\t\').split(\'\\t\')\n                if len(line) > 1:\n                    img_path = pathlib.Path(line[0].strip(\' \'))\n                    label_path = pathlib.Path(line[1].strip(\' \'))\n                    if img_path.exists() and img_path.stat().st_size > 0 and label_path.exists() and label_path.stat().st_size > 0:\n                        train_data.append((str(img_path), str(label_path)))\n    return train_data\n\n\ndef parse_config(config: dict) -> dict:\n    import anyconfig\n    base_file_list = config.pop(\'base\')\n    base_config = {}\n    for base_file in base_file_list:\n        tmp_config = anyconfig.load(open(base_file, \'rb\'))\n        if \'base\' in tmp_config:\n            tmp_config = parse_config(tmp_config)\n        anyconfig.merge(tmp_config, base_config)\n        base_config = tmp_config\n    anyconfig.merge(base_config, config)\n    return base_config\n\n\ndef save_result(result_path, box_list, score_list, is_output_polygon):\n    if is_output_polygon:\n        with open(result_path, \'wt\') as res:\n            for i, box in enumerate(box_list):\n                box = box.reshape(-1).tolist()\n                result = "","".join([str(int(x)) for x in box])\n                score = score_list[i]\n                res.write(result + \',\' + str(score) + ""\\n"")\n    else:\n        with open(result_path, \'wt\') as res:\n            for i, box in enumerate(box_list):\n                score = score_list[i]\n                box = box.reshape(-1).tolist()\n                result = "","".join([str(int(x)) for x in box])\n                res.write(result + \',\' + str(score) + ""\\n"")\n\n\ndef expand_polygon(polygon):\n    """"""\n    \xe5\xaf\xb9\xe5\x8f\xaa\xe6\x9c\x89\xe4\xb8\x80\xe4\xb8\xaa\xe5\xad\x97\xe7\xac\xa6\xe7\x9a\x84\xe6\xa1\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x89\xa9\xe5\x85\x85\n    """"""\n    (x, y), (w, h), angle = cv2.minAreaRect(np.float32(polygon))\n    if angle < -45:\n        w, h = h, w\n        angle += 90\n    new_w = w + h\n    box = ((x, y), (new_w, h), angle)\n    points = cv2.boxPoints(box)\n    return order_points_clockwise(points)\n\n\nif __name__ == \'__main__\':\n    img = np.zeros((1, 3, 640, 640))\n    show_img(img[0][0])\n    plt.show()\n'"
data_loader/modules/__init__.py,0,"b'# -*- coding: utf-8 -*-\n# @Time    : 2019/12/4 10:53\n# @Author  : zhoujun\nfrom .iaa_augment import IaaAugment\nfrom .augment import *\nfrom .random_crop_data import EastRandomCropData,PSERandomCrop\nfrom .make_border_map import MakeBorderMap\nfrom .make_shrink_map import MakeShrinkMap\n'"
data_loader/modules/augment.py,0,"b'# -*- coding: utf-8 -*-\n# @Time    : 2019/8/23 21:52\n# @Author  : zhoujun\n\nimport math\nimport numbers\nimport random\n\nimport cv2\nimport numpy as np\nfrom skimage.util import random_noise\n\n\nclass RandomNoise:\n    def __init__(self, random_rate):\n        self.random_rate = random_rate\n\n    def __call__(self, data: dict):\n        """"""\n        \xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe5\x8a\xa0\xe5\x99\xaa\xe5\xa3\xb0\n        :param data: {\'img\':,\'text_polys\':,\'texts\':,\'ignore_tags\':}\n        :return:\n        """"""\n        if random.random() > self.random_rate:\n            return data\n        data[\'img\'] = (random_noise(data[\'img\'], mode=\'gaussian\', clip=True) * 255).astype(im.dtype)\n        return data\n\n\nclass RandomScale:\n    def __init__(self, scales, random_rate):\n        """"""\n        :param scales: \xe5\xb0\xba\xe5\xba\xa6\n        :param ramdon_rate: \xe9\x9a\x8f\xe6\x9c\xba\xe7\xb3\xbb\xe6\x95\xb0\n        :return:\n        """"""\n        self.random_rate = random_rate\n        self.scales = scales\n\n    def __call__(self, data: dict) -> dict:\n        """"""\n        \xe4\xbb\x8escales\xe4\xb8\xad\xe9\x9a\x8f\xe6\x9c\xba\xe9\x80\x89\xe6\x8b\xa9\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb0\xba\xe5\xba\xa6\xef\xbc\x8c\xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe7\xbc\xa9\xe6\x94\xbe\n        :param data: {\'img\':,\'text_polys\':,\'texts\':,\'ignore_tags\':}\n        :return:\n        """"""\n        if random.random() > self.random_rate:\n            return data\n        im = data[\'img\']\n        text_polys = data[\'text_polys\']\n\n        tmp_text_polys = text_polys.copy()\n        rd_scale = float(np.random.choice(self.scales))\n        im = cv2.resize(im, dsize=None, fx=rd_scale, fy=rd_scale)\n        tmp_text_polys *= rd_scale\n\n        data[\'img\'] = im\n        data[\'text_polys\'] = tmp_text_polys\n        return data\n\n\nclass RandomRotateImgBox:\n    def __init__(self, degrees, random_rate, same_size=False):\n        """"""\n        :param degrees: \xe8\xa7\x92\xe5\xba\xa6\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe5\x80\xbc\xe6\x88\x96\xe8\x80\x85list\n        :param ramdon_rate: \xe9\x9a\x8f\xe6\x9c\xba\xe7\xb3\xbb\xe6\x95\xb0\n        :param same_size: \xe6\x98\xaf\xe5\x90\xa6\xe4\xbf\x9d\xe6\x8c\x81\xe5\x92\x8c\xe5\x8e\x9f\xe5\x9b\xbe\xe4\xb8\x80\xe6\xa0\xb7\xe5\xa4\xa7\n        :return:\n        """"""\n        if isinstance(degrees, numbers.Number):\n            if degrees < 0:\n                raise ValueError(""If degrees is a single number, it must be positive."")\n            degrees = (-degrees, degrees)\n        elif isinstance(degrees, list) or isinstance(degrees, tuple) or isinstance(degrees, np.ndarray):\n            if len(degrees) != 2:\n                raise ValueError(""If degrees is a sequence, it must be of len 2."")\n            degrees = degrees\n        else:\n            raise Exception(\'degrees must in Number or list or tuple or np.ndarray\')\n        self.degrees = degrees\n        self.same_size = same_size\n        self.random_rate = random_rate\n\n    def __call__(self, data: dict) -> dict:\n        """"""\n        \xe4\xbb\x8escales\xe4\xb8\xad\xe9\x9a\x8f\xe6\x9c\xba\xe9\x80\x89\xe6\x8b\xa9\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb0\xba\xe5\xba\xa6\xef\xbc\x8c\xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe7\xbc\xa9\xe6\x94\xbe\n        :param data: {\'img\':,\'text_polys\':,\'texts\':,\'ignore_tags\':}\n        :return:\n        """"""\n        if random.random() > self.random_rate:\n            return data\n        im = data[\'img\']\n        text_polys = data[\'text_polys\']\n\n        # ---------------------- \xe6\x97\x8b\xe8\xbd\xac\xe5\x9b\xbe\xe5\x83\x8f ----------------------\n        w = im.shape[1]\n        h = im.shape[0]\n        angle = np.random.uniform(self.degrees[0], self.degrees[1])\n\n        if self.same_size:\n            nw = w\n            nh = h\n        else:\n            # \xe8\xa7\x92\xe5\xba\xa6\xe5\x8f\x98\xe5\xbc\xa7\xe5\xba\xa6\n            rangle = np.deg2rad(angle)\n            # \xe8\xae\xa1\xe7\xae\x97\xe6\x97\x8b\xe8\xbd\xac\xe4\xb9\x8b\xe5\x90\x8e\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84w, h\n            nw = (abs(np.sin(rangle) * h) + abs(np.cos(rangle) * w))\n            nh = (abs(np.cos(rangle) * h) + abs(np.sin(rangle) * w))\n        # \xe6\x9e\x84\xe9\x80\xa0\xe4\xbb\xbf\xe5\xb0\x84\xe7\x9f\xa9\xe9\x98\xb5\n        rot_mat = cv2.getRotationMatrix2D((nw * 0.5, nh * 0.5), angle, 1)\n        # \xe8\xae\xa1\xe7\xae\x97\xe5\x8e\x9f\xe5\x9b\xbe\xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\xe5\x88\xb0\xe6\x96\xb0\xe5\x9b\xbe\xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\xe7\x9a\x84\xe5\x81\x8f\xe7\xa7\xbb\xe9\x87\x8f\n        rot_move = np.dot(rot_mat, np.array([(nw - w) * 0.5, (nh - h) * 0.5, 0]))\n        # \xe6\x9b\xb4\xe6\x96\xb0\xe4\xbb\xbf\xe5\xb0\x84\xe7\x9f\xa9\xe9\x98\xb5\n        rot_mat[0, 2] += rot_move[0]\n        rot_mat[1, 2] += rot_move[1]\n        # \xe4\xbb\xbf\xe5\xb0\x84\xe5\x8f\x98\xe6\x8d\xa2\n        rot_img = cv2.warpAffine(im, rot_mat, (int(math.ceil(nw)), int(math.ceil(nh))), flags=cv2.INTER_LANCZOS4)\n\n        # ---------------------- \xe7\x9f\xab\xe6\xad\xa3bbox\xe5\x9d\x90\xe6\xa0\x87 ----------------------\n        # rot_mat\xe6\x98\xaf\xe6\x9c\x80\xe7\xbb\x88\xe7\x9a\x84\xe6\x97\x8b\xe8\xbd\xac\xe7\x9f\xa9\xe9\x98\xb5\n        # \xe8\x8e\xb7\xe5\x8f\x96\xe5\x8e\x9f\xe5\xa7\x8bbbox\xe7\x9a\x84\xe5\x9b\x9b\xe4\xb8\xaa\xe4\xb8\xad\xe7\x82\xb9\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe5\xb0\x86\xe8\xbf\x99\xe5\x9b\x9b\xe4\xb8\xaa\xe7\x82\xb9\xe8\xbd\xac\xe6\x8d\xa2\xe5\x88\xb0\xe6\x97\x8b\xe8\xbd\xac\xe5\x90\x8e\xe7\x9a\x84\xe5\x9d\x90\xe6\xa0\x87\xe7\xb3\xbb\xe4\xb8\x8b\n        rot_text_polys = list()\n        for bbox in text_polys:\n            point1 = np.dot(rot_mat, np.array([bbox[0, 0], bbox[0, 1], 1]))\n            point2 = np.dot(rot_mat, np.array([bbox[1, 0], bbox[1, 1], 1]))\n            point3 = np.dot(rot_mat, np.array([bbox[2, 0], bbox[2, 1], 1]))\n            point4 = np.dot(rot_mat, np.array([bbox[3, 0], bbox[3, 1], 1]))\n            rot_text_polys.append([point1, point2, point3, point4])\n        data[\'img\'] = rot_img\n        data[\'text_polys\'] = np.array(rot_text_polys)\n        return data\n\n\nclass RandomResize:\n    def __init__(self, size, random_rate, keep_ratio=False):\n        """"""\n        :param input_size: resize\xe5\xb0\xba\xe5\xaf\xb8,\xe6\x95\xb0\xe5\xad\x97\xe6\x88\x96\xe8\x80\x85list\xe7\x9a\x84\xe5\xbd\xa2\xe5\xbc\x8f\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\xbalist\xe5\xbd\xa2\xe5\xbc\x8f\xef\xbc\x8c\xe5\xb0\xb1\xe6\x98\xaf[w,h]\n        :param ramdon_rate: \xe9\x9a\x8f\xe6\x9c\xba\xe7\xb3\xbb\xe6\x95\xb0\n        :param keep_ratio: \xe6\x98\xaf\xe5\x90\xa6\xe4\xbf\x9d\xe6\x8c\x81\xe9\x95\xbf\xe5\xae\xbd\xe6\xaf\x94\n        :return:\n        """"""\n        if isinstance(size, numbers.Number):\n            if size < 0:\n                raise ValueError(""If input_size is a single number, it must be positive."")\n            size = (size, size)\n        elif isinstance(size, list) or isinstance(size, tuple) or isinstance(size, np.ndarray):\n            if len(size) != 2:\n                raise ValueError(""If input_size is a sequence, it must be of len 2."")\n            size = (size[0], size[1])\n        else:\n            raise Exception(\'input_size must in Number or list or tuple or np.ndarray\')\n        self.size = size\n        self.keep_ratio = keep_ratio\n        self.random_rate = random_rate\n\n    def __call__(self, data: dict) -> dict:\n        """"""\n        \xe4\xbb\x8escales\xe4\xb8\xad\xe9\x9a\x8f\xe6\x9c\xba\xe9\x80\x89\xe6\x8b\xa9\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb0\xba\xe5\xba\xa6\xef\xbc\x8c\xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe7\xbc\xa9\xe6\x94\xbe\n        :param data: {\'img\':,\'text_polys\':,\'texts\':,\'ignore_tags\':}\n        :return:\n        """"""\n        if random.random() > self.random_rate:\n            return data\n        im = data[\'img\']\n        text_polys = data[\'text_polys\']\n\n        if self.keep_ratio:\n            # \xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe7\x9f\xad\xe8\xbe\xb9pad\xe5\x88\xb0\xe5\x92\x8c\xe9\x95\xbf\xe8\xbe\xb9\xe4\xb8\x80\xe6\xa0\xb7\n            h, w, c = im.shape\n            max_h = max(h, self.size[0])\n            max_w = max(w, self.size[1])\n            im_padded = np.zeros((max_h, max_w, c), dtype=np.uint8)\n            im_padded[:h, :w] = im.copy()\n            im = im_padded\n        text_polys = text_polys.astype(np.float32)\n        h, w, _ = im.shape\n        im = cv2.resize(im, self.size)\n        w_scale = self.size[0] / float(w)\n        h_scale = self.size[1] / float(h)\n        text_polys[:, :, 0] *= w_scale\n        text_polys[:, :, 1] *= h_scale\n\n        data[\'img\'] = im\n        data[\'text_polys\'] = text_polys\n        return data\n\n\ndef resize_image(img, short_size):\n    height, width, _ = img.shape\n    if height < width:\n        new_height = short_size\n        new_width = new_height / height * width\n    else:\n        new_width = short_size\n        new_height = new_width / width * height\n    new_height = int(round(new_height / 32) * 32)\n    new_width = int(round(new_width / 32) * 32)\n    resized_img = cv2.resize(img, (new_width, new_height))\n    return resized_img, (new_width / width, new_height / height)\n\n\nclass ResizeShortSize:\n    def __init__(self, short_size, resize_text_polys=True):\n        """"""\n        :param size: resize\xe5\xb0\xba\xe5\xaf\xb8,\xe6\x95\xb0\xe5\xad\x97\xe6\x88\x96\xe8\x80\x85list\xe7\x9a\x84\xe5\xbd\xa2\xe5\xbc\x8f\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\xbalist\xe5\xbd\xa2\xe5\xbc\x8f\xef\xbc\x8c\xe5\xb0\xb1\xe6\x98\xaf[w,h]\n        :return:\n        """"""\n        self.short_size = short_size\n        self.resize_text_polys = resize_text_polys\n\n    def __call__(self, data: dict) -> dict:\n        """"""\n        \xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe7\xbc\xa9\xe6\x94\xbe\n        :param data: {\'img\':,\'text_polys\':,\'texts\':,\'ignore_tags\':}\n        :return:\n        """"""\n        im = data[\'img\']\n        text_polys = data[\'text_polys\']\n\n        h, w, _ = im.shape\n        short_edge = min(h, w)\n        if short_edge < self.short_size:\n            # \xe4\xbf\x9d\xe8\xaf\x81\xe7\x9f\xad\xe8\xbe\xb9 >= short_size\n            scale = self.short_size / short_edge\n            im = cv2.resize(im, dsize=None, fx=scale, fy=scale)\n            scale = (scale, scale)\n            # im, scale = resize_image(im, self.short_size)\n            if self.resize_text_polys:\n                # text_polys *= scale\n                text_polys[:, 0] *= scale[0]\n                text_polys[:, 1] *= scale[1]\n\n        data[\'img\'] = im\n        data[\'text_polys\'] = text_polys\n        return data\n\n\nclass HorizontalFlip:\n    def __init__(self, random_rate):\n        """"""\n\n        :param random_rate: \xe9\x9a\x8f\xe6\x9c\xba\xe7\xb3\xbb\xe6\x95\xb0\n        """"""\n        self.random_rate = random_rate\n\n    def __call__(self, data: dict) -> dict:\n        """"""\n        \xe4\xbb\x8escales\xe4\xb8\xad\xe9\x9a\x8f\xe6\x9c\xba\xe9\x80\x89\xe6\x8b\xa9\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb0\xba\xe5\xba\xa6\xef\xbc\x8c\xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe7\xbc\xa9\xe6\x94\xbe\n        :param data: {\'img\':,\'text_polys\':,\'texts\':,\'ignore_tags\':}\n        :return:\n        """"""\n        if random.random() > self.random_rate:\n            return data\n        im = data[\'img\']\n        text_polys = data[\'text_polys\']\n\n        flip_text_polys = text_polys.copy()\n        flip_im = cv2.flip(im, 1)\n        h, w, _ = flip_im.shape\n        flip_text_polys[:, :, 0] = w - flip_text_polys[:, :, 0]\n\n        data[\'img\'] = flip_im\n        data[\'text_polys\'] = flip_text_polys\n        return data\n\n\nclass VerticallFlip:\n    def __init__(self, random_rate):\n        """"""\n\n        :param random_rate: \xe9\x9a\x8f\xe6\x9c\xba\xe7\xb3\xbb\xe6\x95\xb0\n        """"""\n        self.random_rate = random_rate\n\n    def __call__(self, data: dict) -> dict:\n        """"""\n        \xe4\xbb\x8escales\xe4\xb8\xad\xe9\x9a\x8f\xe6\x9c\xba\xe9\x80\x89\xe6\x8b\xa9\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb0\xba\xe5\xba\xa6\xef\xbc\x8c\xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe7\xbc\xa9\xe6\x94\xbe\n        :param data: {\'img\':,\'text_polys\':,\'texts\':,\'ignore_tags\':}\n        :return:\n        """"""\n        if random.random() > self.random_rate:\n            return data\n        im = data[\'img\']\n        text_polys = data[\'text_polys\']\n\n        flip_text_polys = text_polys.copy()\n        flip_im = cv2.flip(im, 0)\n        h, w, _ = flip_im.shape\n        flip_text_polys[:, :, 1] = h - flip_text_polys[:, :, 1]\n        data[\'img\'] = flip_im\n        data[\'text_polys\'] = flip_text_polys\n        return data\n'"
data_loader/modules/iaa_augment.py,0,"b""# -*- coding: utf-8 -*-\n# @Time    : 2019/12/4 18:06\n# @Author  : zhoujun\nimport numpy as np\nimport imgaug\nimport imgaug.augmenters as iaa\n\nclass AugmenterBuilder(object):\n    def __init__(self):\n        pass\n\n    def build(self, args, root=True):\n        if args is None or len(args) == 0:\n            return None\n        elif isinstance(args, list):\n            if root:\n                sequence = [self.build(value, root=False) for value in args]\n                return iaa.Sequential(sequence)\n            else:\n                return getattr(iaa, args[0])(*[self.to_tuple_if_list(a) for a in args[1:]])\n        elif isinstance(args, dict):\n            cls = getattr(iaa, args['type'])\n            return cls(**{k: self.to_tuple_if_list(v) for k, v in args['args'].items()})\n        else:\n            raise RuntimeError('unknown augmenter arg: ' + str(args))\n\n    def to_tuple_if_list(self, obj):\n        if isinstance(obj, list):\n            return tuple(obj)\n        return obj\n\n\nclass IaaAugment():\n    def __init__(self, augmenter_args):\n        self.augmenter_args = augmenter_args\n        self.augmenter = AugmenterBuilder().build(self.augmenter_args)\n\n    def __call__(self, data):\n        image = data['img']\n        shape = image.shape\n\n        if self.augmenter:\n            aug = self.augmenter.to_deterministic()\n            data['img'] = aug.augment_image(image)\n            data = self.may_augment_annotation(aug, data, shape)\n        return data\n\n    def may_augment_annotation(self, aug, data, shape):\n        if aug is None:\n            return data\n\n        line_polys = []\n        for poly in data['text_polys']:\n            new_poly = self.may_augment_poly(aug, shape, poly)\n            line_polys.append(new_poly)\n        data['text_polys'] = np.array(line_polys)\n        return data\n\n    def may_augment_poly(self, aug, img_shape, poly):\n        keypoints = [imgaug.Keypoint(p[0], p[1]) for p in poly]\n        keypoints = aug.augment_keypoints(\n            [imgaug.KeypointsOnImage(keypoints, shape=img_shape)])[0].keypoints\n        poly = [(p.x, p.y) for p in keypoints]\n        return poly\n"""
data_loader/modules/make_border_map.py,0,"b'import cv2\nimport numpy as np\nnp.seterr(divide=\'ignore\',invalid=\'ignore\')\nimport pyclipper\nfrom shapely.geometry import Polygon\n\n\nclass MakeBorderMap():\n    def __init__(self, shrink_ratio=0.4, thresh_min=0.3, thresh_max=0.7):\n        self.shrink_ratio = shrink_ratio\n        self.thresh_min = thresh_min\n        self.thresh_max = thresh_max\n\n    def __call__(self, data: dict) -> dict:\n        """"""\n        \xe4\xbb\x8escales\xe4\xb8\xad\xe9\x9a\x8f\xe6\x9c\xba\xe9\x80\x89\xe6\x8b\xa9\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb0\xba\xe5\xba\xa6\xef\xbc\x8c\xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe7\xbc\xa9\xe6\x94\xbe\n        :param data: {\'img\':,\'text_polys\':,\'texts\':,\'ignore_tags\':}\n        :return:\n        """"""\n        im = data[\'img\']\n        text_polys = data[\'text_polys\']\n        ignore_tags = data[\'ignore_tags\']\n\n        canvas = np.zeros(im.shape[:2], dtype=np.float32)\n        mask = np.zeros(im.shape[:2], dtype=np.float32)\n\n        for i in range(len(text_polys)):\n            if ignore_tags[i]:\n                continue\n            self.draw_border_map(text_polys[i], canvas, mask=mask)\n        canvas = canvas * (self.thresh_max - self.thresh_min) + self.thresh_min\n\n        data[\'threshold_map\'] = canvas\n        data[\'threshold_mask\'] = mask\n        return data\n\n    def draw_border_map(self, polygon, canvas, mask):\n        polygon = np.array(polygon)\n        assert polygon.ndim == 2\n        assert polygon.shape[1] == 2\n\n        polygon_shape = Polygon(polygon)\n        if polygon_shape.area <= 0:\n            return\n        distance = polygon_shape.area * (1 - np.power(self.shrink_ratio, 2)) / polygon_shape.length\n        subject = [tuple(l) for l in polygon]\n        padding = pyclipper.PyclipperOffset()\n        padding.AddPath(subject, pyclipper.JT_ROUND,\n                        pyclipper.ET_CLOSEDPOLYGON)\n\n        padded_polygon = np.array(padding.Execute(distance)[0])\n        cv2.fillPoly(mask, [padded_polygon.astype(np.int32)], 1.0)\n\n        xmin = padded_polygon[:, 0].min()\n        xmax = padded_polygon[:, 0].max()\n        ymin = padded_polygon[:, 1].min()\n        ymax = padded_polygon[:, 1].max()\n        width = xmax - xmin + 1\n        height = ymax - ymin + 1\n\n        polygon[:, 0] = polygon[:, 0] - xmin\n        polygon[:, 1] = polygon[:, 1] - ymin\n\n        xs = np.broadcast_to(\n            np.linspace(0, width - 1, num=width).reshape(1, width), (height, width))\n        ys = np.broadcast_to(\n            np.linspace(0, height - 1, num=height).reshape(height, 1), (height, width))\n\n        distance_map = np.zeros(\n            (polygon.shape[0], height, width), dtype=np.float32)\n        for i in range(polygon.shape[0]):\n            j = (i + 1) % polygon.shape[0]\n            absolute_distance = self.distance(xs, ys, polygon[i], polygon[j])\n            distance_map[i] = np.clip(absolute_distance / distance, 0, 1)\n        distance_map = distance_map.min(axis=0)\n\n        xmin_valid = min(max(0, xmin), canvas.shape[1] - 1)\n        xmax_valid = min(max(0, xmax), canvas.shape[1] - 1)\n        ymin_valid = min(max(0, ymin), canvas.shape[0] - 1)\n        ymax_valid = min(max(0, ymax), canvas.shape[0] - 1)\n        canvas[ymin_valid:ymax_valid + 1, xmin_valid:xmax_valid + 1] = np.fmax(\n            1 - distance_map[\n                ymin_valid - ymin:ymax_valid - ymax + height,\n                xmin_valid - xmin:xmax_valid - xmax + width],\n            canvas[ymin_valid:ymax_valid + 1, xmin_valid:xmax_valid + 1])\n\n    def distance(self, xs, ys, point_1, point_2):\n        \'\'\'\n        compute the distance from point to a line\n        ys: coordinates in the first axis\n        xs: coordinates in the second axis\n        point_1, point_2: (x, y), the end of the line\n        \'\'\'\n        height, width = xs.shape[:2]\n        square_distance_1 = np.square(xs - point_1[0]) + np.square(ys - point_1[1])\n        square_distance_2 = np.square(xs - point_2[0]) + np.square(ys - point_2[1])\n        square_distance = np.square(point_1[0] - point_2[0]) + np.square(point_1[1] - point_2[1])\n\n        cosin = (square_distance - square_distance_1 - square_distance_2) / (2 * np.sqrt(square_distance_1 * square_distance_2))\n        square_sin = 1 - np.square(cosin)\n        square_sin = np.nan_to_num(square_sin)\n\n        result = np.sqrt(square_distance_1 * square_distance_2 * square_sin / square_distance)\n        result[cosin < 0] = np.sqrt(np.fmin(square_distance_1, square_distance_2))[cosin < 0]\n        # self.extend_line(point_1, point_2, result)\n        return result\n\n    def extend_line(self, point_1, point_2, result):\n        ex_point_1 = (int(round(point_1[0] + (point_1[0] - point_2[0]) * (1 + self.shrink_ratio))),\n                      int(round(point_1[1] + (point_1[1] - point_2[1]) * (1 + self.shrink_ratio))))\n        cv2.line(result, tuple(ex_point_1), tuple(point_1), 4096.0, 1, lineType=cv2.LINE_AA, shift=0)\n        ex_point_2 = (int(round(point_2[0] + (point_2[0] - point_1[0]) * (1 + self.shrink_ratio))),\n                      int(round(point_2[1] + (point_2[1] - point_1[1]) * (1 + self.shrink_ratio))))\n        cv2.line(result, tuple(ex_point_2), tuple(point_2), 4096.0, 1, lineType=cv2.LINE_AA, shift=0)\n        return ex_point_1, ex_point_2\n'"
data_loader/modules/make_shrink_map.py,0,"b'import numpy as np\nimport cv2\n\n\ndef shrink_polygon_py(polygon, shrink_ratio):\n    """"""\n    \xe5\xaf\xb9\xe6\xa1\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe7\xbc\xa9\xe6\x94\xbe\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9e\xe5\x8e\xbb\xe7\x9a\x84\xe6\xaf\x94\xe4\xbe\x8b\xe4\xb8\xba1/shrink_ratio \xe5\x8d\xb3\xe5\x8f\xaf\n    """"""\n    cx = polygon[:, 0].mean()\n    cy = polygon[:, 1].mean()\n    polygon[:, 0] = cx + (polygon[:, 0] - cx) * shrink_ratio\n    polygon[:, 1] = cy + (polygon[:, 1] - cy) * shrink_ratio\n    return polygon\n\n\ndef shrink_polygon_pyclipper(polygon, shrink_ratio):\n    from shapely.geometry import Polygon\n    import pyclipper\n    polygon_shape = Polygon(polygon)\n    distance = polygon_shape.area * (1 - np.power(shrink_ratio, 2)) / polygon_shape.length\n    subject = [tuple(l) for l in polygon]\n    padding = pyclipper.PyclipperOffset()\n    padding.AddPath(subject, pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)\n    shrinked = padding.Execute(-distance)\n    if shrinked == []:\n        shrinked = np.array(shrinked)\n    else:\n        shrinked = np.array(shrinked[0]).reshape(-1, 2)\n    return shrinked\n\n\nclass MakeShrinkMap():\n    r\'\'\'\n    Making binary mask from detection data with ICDAR format.\n    Typically following the process of class `MakeICDARData`.\n    \'\'\'\n\n    def __init__(self, min_text_size=8, shrink_ratio=0.4, shrink_type=\'pyclipper\'):\n        shrink_func_dict = {\'py\': shrink_polygon_py, \'pyclipper\': shrink_polygon_pyclipper}\n        self.shrink_func = shrink_func_dict[shrink_type]\n        self.min_text_size = min_text_size\n        self.shrink_ratio = shrink_ratio\n\n    def __call__(self, data: dict) -> dict:\n        """"""\n        \xe4\xbb\x8escales\xe4\xb8\xad\xe9\x9a\x8f\xe6\x9c\xba\xe9\x80\x89\xe6\x8b\xa9\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb0\xba\xe5\xba\xa6\xef\xbc\x8c\xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe7\xbc\xa9\xe6\x94\xbe\n        :param data: {\'img\':,\'text_polys\':,\'texts\':,\'ignore_tags\':}\n        :return:\n        """"""\n        image = data[\'img\']\n        text_polys = data[\'text_polys\']\n        ignore_tags = data[\'ignore_tags\']\n\n        h, w = image.shape[:2]\n        text_polys, ignore_tags = self.validate_polygons(text_polys, ignore_tags, h, w)\n        gt = np.zeros((h, w), dtype=np.float32)\n        mask = np.ones((h, w), dtype=np.float32)\n        for i in range(len(text_polys)):\n            polygon = text_polys[i]\n            height = max(polygon[:, 1]) - min(polygon[:, 1])\n            width = max(polygon[:, 0]) - min(polygon[:, 0])\n            if ignore_tags[i] or min(height, width) < self.min_text_size:\n                cv2.fillPoly(mask, polygon.astype(np.int32)[np.newaxis, :, :], 0)\n                ignore_tags[i] = True\n            else:\n                shrinked = self.shrink_func(polygon, self.shrink_ratio)\n                if shrinked.size == 0:\n                    cv2.fillPoly(mask, polygon.astype(np.int32)[np.newaxis, :, :], 0)\n                    ignore_tags[i] = True\n                    continue\n                cv2.fillPoly(gt, [shrinked.astype(np.int32)], 1)\n\n        data[\'shrink_map\'] = gt\n        data[\'shrink_mask\'] = mask\n        return data\n\n    def validate_polygons(self, polygons, ignore_tags, h, w):\n        \'\'\'\n        polygons (numpy.array, required): of shape (num_instances, num_points, 2)\n        \'\'\'\n        if len(polygons) == 0:\n            return polygons, ignore_tags\n        assert len(polygons) == len(ignore_tags)\n        for polygon in polygons:\n            polygon[:, 0] = np.clip(polygon[:, 0], 0, w - 1)\n            polygon[:, 1] = np.clip(polygon[:, 1], 0, h - 1)\n\n        for i in range(len(polygons)):\n            area = self.polygon_area(polygons[i])\n            if abs(area) < 1:\n                ignore_tags[i] = True\n            if area > 0:\n                polygons[i] = polygons[i][::-1, :]\n        return polygons, ignore_tags\n\n    def polygon_area(self, polygon):\n        return cv2.contourArea(polygon)\n        # edge = 0\n        # for i in range(polygon.shape[0]):\n        #     next_index = (i + 1) % polygon.shape[0]\n        #     edge += (polygon[next_index, 0] - polygon[i, 0]) * (polygon[next_index, 1] - polygon[i, 1])\n        #\n        # return edge / 2.\n\n\nif __name__ == \'__main__\':\n    from shapely.geometry import Polygon\n    import pyclipper\n\n    polygon = np.array([[0, 0], [100, 10], [100, 100], [10, 90]])\n    a = shrink_polygon_py(polygon, 0.4)\n    print(a)\n    print(shrink_polygon_py(a, 1 / 0.4))\n    b = shrink_polygon_pyclipper(polygon, 0.4)\n    print(b)\n    poly = Polygon(b)\n    distance = poly.area * 1.5 / poly.length\n    offset = pyclipper.PyclipperOffset()\n    offset.AddPath(b, pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)\n    expanded = np.array(offset.Execute(distance))\n    bounding_box = cv2.minAreaRect(expanded)\n    points = cv2.boxPoints(bounding_box)\n    print(points)\n'"
data_loader/modules/random_crop_data.py,0,"b'import random\n\nimport cv2\nimport numpy as np\n\n\n# random crop algorithm similar to https://github.com/argman/EAST\nclass EastRandomCropData():\n    def __init__(self, size=(640, 640), max_tries=50, min_crop_side_ratio=0.1, require_original_image=False, keep_ratio=True):\n        self.size = size\n        self.max_tries = max_tries\n        self.min_crop_side_ratio = min_crop_side_ratio\n        self.require_original_image = require_original_image\n        self.keep_ratio = keep_ratio\n\n    def __call__(self, data: dict) -> dict:\n        """"""\n        \xe4\xbb\x8escales\xe4\xb8\xad\xe9\x9a\x8f\xe6\x9c\xba\xe9\x80\x89\xe6\x8b\xa9\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb0\xba\xe5\xba\xa6\xef\xbc\x8c\xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe7\xbc\xa9\xe6\x94\xbe\n        :param data: {\'img\':,\'text_polys\':,\'texts\':,\'ignore_tags\':}\n        :return:\n        """"""\n        im = data[\'img\']\n        text_polys = data[\'text_polys\']\n        ignore_tags = data[\'ignore_tags\']\n        texts = data[\'texts\']\n        all_care_polys = [text_polys[i] for i, tag in enumerate(ignore_tags) if not tag]\n        # \xe8\xae\xa1\xe7\xae\x97crop\xe5\x8c\xba\xe5\x9f\x9f\n        crop_x, crop_y, crop_w, crop_h = self.crop_area(im, all_care_polys)\n        # crop \xe5\x9b\xbe\xe7\x89\x87 \xe4\xbf\x9d\xe6\x8c\x81\xe6\xaf\x94\xe4\xbe\x8b\xe5\xa1\xab\xe5\x85\x85\n        scale_w = self.size[0] / crop_w\n        scale_h = self.size[1] / crop_h\n        scale = min(scale_w, scale_h)\n        h = int(crop_h * scale)\n        w = int(crop_w * scale)\n        if self.keep_ratio:\n            if len(im.shape) == 3:\n                padimg = np.zeros((self.size[1], self.size[0], im.shape[2]), im.dtype)\n            else:\n                padimg = np.zeros((self.size[1], self.size[0]), im.dtype)\n            padimg[:h, :w] = cv2.resize(im[crop_y:crop_y + crop_h, crop_x:crop_x + crop_w], (w, h))\n            img = padimg\n        else:\n            img = cv2.resize(im[crop_y:crop_y + crop_h, crop_x:crop_x + crop_w], tuple(self.size))\n        # crop \xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\n        text_polys_crop = []\n        ignore_tags_crop = []\n        texts_crop = []\n        for poly, text, tag in zip(text_polys, texts, ignore_tags):\n            poly = ((poly - (crop_x, crop_y)) * scale).tolist()\n            if not self.is_poly_outside_rect(poly, 0, 0, w, h):\n                text_polys_crop.append(poly)\n                ignore_tags_crop.append(tag)\n                texts_crop.append(text)\n        data[\'img\'] = img\n        data[\'text_polys\'] = np.float32(text_polys_crop)\n        data[\'ignore_tags\'] = ignore_tags_crop\n        data[\'texts\'] = texts_crop\n        return data\n\n    def is_poly_in_rect(self, poly, x, y, w, h):\n        poly = np.array(poly)\n        if poly[:, 0].min() < x or poly[:, 0].max() > x + w:\n            return False\n        if poly[:, 1].min() < y or poly[:, 1].max() > y + h:\n            return False\n        return True\n\n    def is_poly_outside_rect(self, poly, x, y, w, h):\n        poly = np.array(poly)\n        if poly[:, 0].max() < x or poly[:, 0].min() > x + w:\n            return True\n        if poly[:, 1].max() < y or poly[:, 1].min() > y + h:\n            return True\n        return False\n\n    def split_regions(self, axis):\n        regions = []\n        min_axis = 0\n        for i in range(1, axis.shape[0]):\n            if axis[i] != axis[i - 1] + 1:\n                region = axis[min_axis:i]\n                min_axis = i\n                regions.append(region)\n        return regions\n\n    def random_select(self, axis, max_size):\n        xx = np.random.choice(axis, size=2)\n        xmin = np.min(xx)\n        xmax = np.max(xx)\n        xmin = np.clip(xmin, 0, max_size - 1)\n        xmax = np.clip(xmax, 0, max_size - 1)\n        return xmin, xmax\n\n    def region_wise_random_select(self, regions, max_size):\n        selected_index = list(np.random.choice(len(regions), 2))\n        selected_values = []\n        for index in selected_index:\n            axis = regions[index]\n            xx = int(np.random.choice(axis, size=1))\n            selected_values.append(xx)\n        xmin = min(selected_values)\n        xmax = max(selected_values)\n        return xmin, xmax\n\n    def crop_area(self, im, text_polys):\n        h, w = im.shape[:2]\n        h_array = np.zeros(h, dtype=np.int32)\n        w_array = np.zeros(w, dtype=np.int32)\n        for points in text_polys:\n            points = np.round(points, decimals=0).astype(np.int32)\n            minx = np.min(points[:, 0])\n            maxx = np.max(points[:, 0])\n            w_array[minx:maxx] = 1\n            miny = np.min(points[:, 1])\n            maxy = np.max(points[:, 1])\n            h_array[miny:maxy] = 1\n        # ensure the cropped area not across a text\n        h_axis = np.where(h_array == 0)[0]\n        w_axis = np.where(w_array == 0)[0]\n\n        if len(h_axis) == 0 or len(w_axis) == 0:\n            return 0, 0, w, h\n\n        h_regions = self.split_regions(h_axis)\n        w_regions = self.split_regions(w_axis)\n\n        for i in range(self.max_tries):\n            if len(w_regions) > 1:\n                xmin, xmax = self.region_wise_random_select(w_regions, w)\n            else:\n                xmin, xmax = self.random_select(w_axis, w)\n            if len(h_regions) > 1:\n                ymin, ymax = self.region_wise_random_select(h_regions, h)\n            else:\n                ymin, ymax = self.random_select(h_axis, h)\n\n            if xmax - xmin < self.min_crop_side_ratio * w or ymax - ymin < self.min_crop_side_ratio * h:\n                # area too small\n                continue\n            num_poly_in_rect = 0\n            for poly in text_polys:\n                if not self.is_poly_outside_rect(poly, xmin, ymin, xmax - xmin, ymax - ymin):\n                    num_poly_in_rect += 1\n                    break\n\n            if num_poly_in_rect > 0:\n                return xmin, ymin, xmax - xmin, ymax - ymin\n\n        return 0, 0, w, h\n\n\nclass PSERandomCrop():\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, data):\n        imgs = data[\'imgs\']\n\n        h, w = imgs[0].shape[0:2]\n        th, tw = self.size\n        if w == tw and h == th:\n            return imgs\n\n        # label\xe4\xb8\xad\xe5\xad\x98\xe5\x9c\xa8\xe6\x96\x87\xe6\x9c\xac\xe5\xae\x9e\xe4\xbe\x8b\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\x94\xe6\x8c\x89\xe7\x85\xa7\xe6\xa6\x82\xe7\x8e\x87\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xa3\x81\xe5\x89\xaa\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8threshold_label_map\xe6\x8e\xa7\xe5\x88\xb6\n        if np.max(imgs[2]) > 0 and random.random() > 3 / 8:\n            # \xe6\x96\x87\xe6\x9c\xac\xe5\xae\x9e\xe4\xbe\x8b\xe7\x9a\x84\xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92\xe7\x82\xb9\n            tl = np.min(np.where(imgs[2] > 0), axis=1) - self.size\n            tl[tl < 0] = 0\n            # \xe6\x96\x87\xe6\x9c\xac\xe5\xae\x9e\xe4\xbe\x8b\xe7\x9a\x84\xe5\x8f\xb3\xe4\xb8\x8b\xe8\xa7\x92\xe7\x82\xb9\n            br = np.max(np.where(imgs[2] > 0), axis=1) - self.size\n            br[br < 0] = 0\n            # \xe4\xbf\x9d\xe8\xaf\x81\xe9\x80\x89\xe5\x88\xb0\xe5\x8f\xb3\xe4\xb8\x8b\xe8\xa7\x92\xe7\x82\xb9\xe6\x97\xb6\xef\xbc\x8c\xe6\x9c\x89\xe8\xb6\xb3\xe5\xa4\x9f\xe7\x9a\x84\xe8\xb7\x9d\xe7\xa6\xbb\xe8\xbf\x9b\xe8\xa1\x8ccrop\n            br[0] = min(br[0], h - th)\n            br[1] = min(br[1], w - tw)\n\n            for _ in range(50000):\n                i = random.randint(tl[0], br[0])\n                j = random.randint(tl[1], br[1])\n                # \xe4\xbf\x9d\xe8\xaf\x81shrink_label_map\xe6\x9c\x89\xe6\x96\x87\xe6\x9c\xac\n                if imgs[1][i:i + th, j:j + tw].sum() <= 0:\n                    continue\n                else:\n                    break\n        else:\n            i = random.randint(0, h - th)\n            j = random.randint(0, w - tw)\n\n        # return i, j, th, tw\n        for idx in range(len(imgs)):\n            if len(imgs[idx].shape) == 3:\n                imgs[idx] = imgs[idx][i:i + th, j:j + tw, :]\n            else:\n                imgs[idx] = imgs[idx][i:i + th, j:j + tw]\n        data[\'imgs\'] = imgs\n        return data\n'"
models/backbone/MobilenetV3.py,1,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nclass HSwish(nn.Module):\n    def forward(self, x):\n        out = x * F.relu6(x + 3, inplace=True) / 6\n        return out\n\n\nclass HardSigmoid(nn.Module):\n    def __init__(self, slope=.2, offset=.5):\n        super().__init__()\n        self.slope = slope\n        self.offset = offset\n\n    def forward(self, x):\n        x = (self.slope * x) + self.offset\n        x = F.threshold(-x, -1, -1)\n        x = F.threshold(-x, 0, 0)\n        return x\n\n\nclass ConvBNACT(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, groups=1, act=None):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n                              stride=stride, padding=padding, groups=groups,\n                              bias=False)\n        self.bn = nn.BatchNorm2d(out_channels)\n        if act == \'relu\':\n            self.act = nn.ReLU()\n        elif act == \'hard_swish\':\n            self.act = HSwish()\n        elif act is None:\n            self.act = None\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        if self.act is not None:\n            x = self.act(x)\n        return x\n\n\nclass SEBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, ratio=4):\n        super().__init__()\n        num_mid_filter = out_channels // ratio\n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=num_mid_filter, kernel_size=1, bias=True)\n        self.relu1 = nn.ReLU()\n        self.conv2 = nn.Conv2d(in_channels=num_mid_filter, kernel_size=1, out_channels=out_channels, bias=True)\n        self.relu2 = HardSigmoid()\n\n    def forward(self, x):\n        attn = self.pool(x)\n        attn = self.conv1(attn)\n        attn = self.relu1(attn)\n        attn = self.conv2(attn)\n        attn = self.relu2(attn)\n        return x * attn\n\n\nclass ResidualUnit(nn.Module):\n    def __init__(self, num_in_filter, num_mid_filter, num_out_filter, stride, kernel_size, act=None, use_se=False):\n        super().__init__()\n        self.conv0 = ConvBNACT(in_channels=num_in_filter, out_channels=num_mid_filter, kernel_size=1, stride=1,\n                               padding=0, act=act)\n\n        self.conv1 = ConvBNACT(in_channels=num_mid_filter, out_channels=num_mid_filter, kernel_size=kernel_size,\n                               stride=stride,\n                               padding=int((kernel_size - 1) // 2), act=act, groups=num_mid_filter)\n        if use_se:\n            self.se = SEBlock(in_channels=num_mid_filter, out_channels=num_mid_filter)\n        else:\n            self.se = None\n\n        self.conv2 = ConvBNACT(in_channels=num_mid_filter, out_channels=num_out_filter, kernel_size=1, stride=1,\n                               padding=0)\n        self.not_add = num_in_filter != num_out_filter or stride != 1\n\n    def forward(self, x):\n        y = self.conv0(x)\n        y = self.conv1(y)\n        if self.se is not None:\n            y = self.se(y)\n        y = self.conv2(y)\n        if not self.not_add:\n            y = x + y\n        return y\n\n\nclass MobileNetV3(nn.Module):\n    def __init__(self, in_channels=3, **kwargs):\n        """"""\n        the MobilenetV3 backbone network for detection module.\n        Args:\n            params(dict): the super parameters for build network\n        """"""\n        super().__init__()\n        self.scale = kwargs.get(\'scale\', 0.5)\n        model_name = kwargs.get(\'model_name\', \'large\')\n        self.inplanes = 16\n        if model_name == ""large"":\n            self.cfg = [\n                # k, exp, c,  se,     nl,  s,\n                [3, 16, 16, False, \'relu\', 1],\n                [3, 64, 24, False, \'relu\', 2],\n                [3, 72, 24, False, \'relu\', 1],\n                [5, 72, 40, True, \'relu\', 2],\n                [5, 120, 40, True, \'relu\', 1],\n                [5, 120, 40, True, \'relu\', 1],\n                [3, 240, 80, False, \'hard_swish\', 2],\n                [3, 200, 80, False, \'hard_swish\', 1],\n                [3, 184, 80, False, \'hard_swish\', 1],\n                [3, 184, 80, False, \'hard_swish\', 1],\n                [3, 480, 112, True, \'hard_swish\', 1],\n                [3, 672, 112, True, \'hard_swish\', 1],\n                [5, 672, 160, True, \'hard_swish\', 2],\n                [5, 960, 160, True, \'hard_swish\', 1],\n                [5, 960, 160, True, \'hard_swish\', 1],\n            ]\n            self.cls_ch_squeeze = 960\n            self.cls_ch_expand = 1280\n        elif model_name == ""small"":\n            self.cfg = [\n                # k, exp, c,  se,     nl,  s,\n                [3, 16, 16, True, \'relu\', 2],\n                [3, 72, 24, False, \'relu\', 2],\n                [3, 88, 24, False, \'relu\', 1],\n                [5, 96, 40, True, \'hard_swish\', 2],\n                [5, 240, 40, True, \'hard_swish\', 1],\n                [5, 240, 40, True, \'hard_swish\', 1],\n                [5, 120, 48, True, \'hard_swish\', 1],\n                [5, 144, 48, True, \'hard_swish\', 1],\n                [5, 288, 96, True, \'hard_swish\', 2],\n                [5, 576, 96, True, \'hard_swish\', 1],\n                [5, 576, 96, True, \'hard_swish\', 1],\n            ]\n            self.cls_ch_squeeze = 576\n            self.cls_ch_expand = 1280\n        else:\n            raise NotImplementedError(""mode["" + model_name +\n                                      ""_model] is not implemented!"")\n\n        supported_scale = [0.35, 0.5, 0.75, 1.0, 1.25]\n        assert self.scale in supported_scale, \\\n            ""supported scale are {} but input scale is {}"".format(supported_scale, self.scale)\n\n        scale = self.scale\n        inplanes = self.inplanes\n        cfg = self.cfg\n        cls_ch_squeeze = self.cls_ch_squeeze\n        # conv1\n        self.conv1 = ConvBNACT(in_channels=in_channels,\n                               out_channels=self.make_divisible(inplanes * scale),\n                               kernel_size=3,\n                               stride=2,\n                               padding=1,\n                               groups=1,\n                               act=\'hard_swish\')\n        i = 0\n        inplanes = self.make_divisible(inplanes * scale)\n        self.stages = nn.ModuleList()\n        block_list = []\n        self.out_channels = []\n        for layer_cfg in cfg:\n            if layer_cfg[5] == 2 and i > 2:\n                self.out_channels.append(inplanes)\n                self.stages.append(nn.Sequential(*block_list))\n                block_list = []\n            block = ResidualUnit(num_in_filter=inplanes,\n                                 num_mid_filter=self.make_divisible(scale * layer_cfg[1]),\n                                 num_out_filter=self.make_divisible(scale * layer_cfg[2]),\n                                 act=layer_cfg[4],\n                                 stride=layer_cfg[5],\n                                 kernel_size=layer_cfg[0],\n                                 use_se=layer_cfg[3])\n            block_list.append(block)\n            inplanes = self.make_divisible(scale * layer_cfg[2])\n            i += 1\n        self.stages.append(nn.Sequential(*block_list))\n        self.conv2 = ConvBNACT(\n            in_channels=inplanes,\n            out_channels=self.make_divisible(scale * cls_ch_squeeze),\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            groups=1,\n            act=\'hard_swish\')\n        self.out_channels.append(self.make_divisible(scale * cls_ch_squeeze))\n\n    def make_divisible(self, v, divisor=8, min_value=None):\n        if min_value is None:\n            min_value = divisor\n        new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n        if new_v < 0.9 * v:\n            new_v += divisor\n        return new_v\n\n    def forward(self, x):\n        x = self.conv1(x)\n        out = []\n        for stage in self.stages:\n            x = stage(x)\n            out.append(x)\n        out[-1] = self.conv2(out[-1])\n        return out\n'"
models/backbone/__init__.py,0,"b""# -*- coding: utf-8 -*-\n# @Time    : 2019/8/23 21:54\n# @Author  : zhoujun\n__all__ = ['build_backbone']\n\nfrom .resnet import *\nfrom .resnest import *\nfrom .shufflenetv2 import *\nfrom .MobilenetV3 import MobileNetV3\n\nsupport_backbone = ['resnet18', 'deformable_resnet18', 'deformable_resnet50',\n                    'resnet50', 'resnet34', 'resnet101', 'resnet152',\n                    'resnest50', 'resnest101', 'resnest200', 'resnest269',\n                    'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0',\n                    'MobileNetV3']\n\n\ndef build_backbone(backbone_name, **kwargs):\n    assert backbone_name in support_backbone, f'all support backbone is {support_backbone}'\n    backbone = eval(backbone_name)(**kwargs)\n    return backbone\n"""
models/backbone/resnet.py,8,"b'import torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\n\nBatchNorm2d = nn.BatchNorm2d\n\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\', \'deformable_resnet18\', \'deformable_resnet50\',\n           \'resnet152\']\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\ndef constant_init(module, constant, bias=0):\n    nn.init.constant_(module.weight, constant)\n    if hasattr(module, \'bias\'):\n        nn.init.constant_(module.bias, bias)\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dcn=None):\n        super(BasicBlock, self).__init__()\n        self.with_dcn = dcn is not None\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.with_modulated_dcn = False\n        if not self.with_dcn:\n            self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, bias=False)\n        else:\n            from torchvision.ops import DeformConv2d\n            deformable_groups = dcn.get(\'deformable_groups\', 1)\n            offset_channels = 18\n            self.conv2_offset = nn.Conv2d(planes, deformable_groups * offset_channels, kernel_size=3, padding=1)\n            self.conv2 = DeformConv2d(planes, planes, kernel_size=3, padding=1, bias=False)\n        self.bn2 = BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        # out = self.conv2(out)\n        if not self.with_dcn:\n            out = self.conv2(out)\n        else:\n            offset = self.conv2_offset(out)\n            out = self.conv2(out, offset)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dcn=None):\n        super(Bottleneck, self).__init__()\n        self.with_dcn = dcn is not None\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = BatchNorm2d(planes)\n        self.with_modulated_dcn = False\n        if not self.with_dcn:\n            self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        else:\n            deformable_groups = dcn.get(\'deformable_groups\', 1)\n            from torchvision.ops import DeformConv2d\n            offset_channels = 18\n            self.conv2_offset = nn.Conv2d(planes, deformable_groups * offset_channels, stride=stride, kernel_size=3, padding=1)\n            self.conv2 = DeformConv2d(planes, planes, kernel_size=3, padding=1, stride=stride, bias=False)\n        self.bn2 = BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.dcn = dcn\n        self.with_dcn = dcn is not None\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        # out = self.conv2(out)\n        if not self.with_dcn:\n            out = self.conv2(out)\n        else:\n            offset = self.conv2_offset(out)\n            out = self.conv2(out, offset)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, in_channels=3, dcn=None):\n        self.dcn = dcn\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.out_channels = []\n        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dcn=dcn)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dcn=dcn)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dcn=dcn)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n        if self.dcn is not None:\n            for m in self.modules():\n                if isinstance(m, Bottleneck) or isinstance(m, BasicBlock):\n                    if hasattr(m, \'conv2_offset\'):\n                        constant_init(m.conv2_offset, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dcn=None):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, dcn=dcn))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dcn=dcn))\n        self.out_channels.append(planes * block.expansion)\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x2 = self.layer1(x)\n        x3 = self.layer2(x2)\n        x4 = self.layer3(x3)\n        x5 = self.layer4(x4)\n\n        return x2, x3, x4, x5\n\n\ndef resnet18(pretrained=True, **kwargs):\n    """"""Constructs a ResNet-18 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        assert kwargs[\'in_channels\'] == 3, \'in_channels must be 3 whem pretrained is True\'\n        print(\'load from imagenet\')\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet18\']), strict=False)\n    return model\n\n\ndef deformable_resnet18(pretrained=True, **kwargs):\n    """"""Constructs a ResNet-18 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], dcn=dict(deformable_groups=1), **kwargs)\n    if pretrained:\n        assert kwargs[\'in_channels\'] == 3, \'in_channels must be 3 whem pretrained is True\'\n        print(\'load from imagenet\')\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet18\']), strict=False)\n    return model\n\n\ndef resnet34(pretrained=True, **kwargs):\n    """"""Constructs a ResNet-34 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        assert kwargs[\'in_channels\'] == 3, \'in_channels must be 3 whem pretrained is True\'\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet34\']), strict=False)\n    return model\n\n\ndef resnet50(pretrained=True, **kwargs):\n    """"""Constructs a ResNet-50 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        assert kwargs[\'in_channels\'] == 3, \'in_channels must be 3 whem pretrained is True\'\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\']), strict=False)\n    return model\n\n\ndef deformable_resnet50(pretrained=True, **kwargs):\n    """"""Constructs a ResNet-50 model with deformable conv.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], dcn=dict(deformable_groups=1), **kwargs)\n    if pretrained:\n        assert kwargs[\'in_channels\'] == 3, \'in_channels must be 3 whem pretrained is True\'\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\']), strict=False)\n    return model\n\n\ndef resnet101(pretrained=True, **kwargs):\n    """"""Constructs a ResNet-101 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        assert kwargs[\'in_channels\'] == 3, \'in_channels must be 3 whem pretrained is True\'\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet101\']), strict=False)\n    return model\n\n\ndef resnet152(pretrained=True, **kwargs):\n    """"""Constructs a ResNet-152 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        assert kwargs[\'in_channels\'] == 3, \'in_channels must be 3 whem pretrained is True\'\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet152\']), strict=False)\n    return model\n\n\nif __name__ == \'__main__\':\n    import torch\n\n    x = torch.zeros(2, 3, 640, 640)\n    net = deformable_resnet50(pretrained=False)\n    y = net(x)\n    for u in y:\n        print(u.shape)\n\n    print(net.out_channels)\n'"
models/backbone/shufflenetv2.py,6,"b'# -*- coding: utf-8 -*-\n# @Time    : 2019/11/1 15:31\n# @Author  : zhoujun\n\nimport torch\nimport torch.nn as nn\nfrom torchvision.models.utils import load_state_dict_from_url\n\n__all__ = [\n    \'ShuffleNetV2\', \'shufflenet_v2_x0_5\', \'shufflenet_v2_x1_0\',\n    \'shufflenet_v2_x1_5\', \'shufflenet_v2_x2_0\'\n]\n\nmodel_urls = {\n    \'shufflenetv2_x0.5\': \'https://download.pytorch.org/models/shufflenetv2_x0.5-f707e7126e.pth\',\n    \'shufflenetv2_x1.0\': \'https://download.pytorch.org/models/shufflenetv2_x1-5666bf0f80.pth\',\n    \'shufflenetv2_x1.5\': None,\n    \'shufflenetv2_x2.0\': None,\n}\n\n\ndef channel_shuffle(x, groups):\n    batchsize, num_channels, height, width = x.data.size()\n    channels_per_group = num_channels // groups\n\n    # reshape\n    x = x.view(batchsize, groups,\n               channels_per_group, height, width)\n\n    x = torch.transpose(x, 1, 2).contiguous()\n\n    # flatten\n    x = x.view(batchsize, -1, height, width)\n\n    return x\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride):\n        super(InvertedResidual, self).__init__()\n\n        if not (1 <= stride <= 3):\n            raise ValueError(\'illegal stride value\')\n        self.stride = stride\n\n        branch_features = oup // 2\n        assert (self.stride != 1) or (inp == branch_features << 1)\n\n        if self.stride > 1:\n            self.branch1 = nn.Sequential(\n                self.depthwise_conv(inp, inp, kernel_size=3, stride=self.stride, padding=1),\n                nn.BatchNorm2d(inp),\n                nn.Conv2d(inp, branch_features, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(branch_features),\n                nn.ReLU(inplace=True),\n            )\n\n        self.branch2 = nn.Sequential(\n            nn.Conv2d(inp if (self.stride > 1) else branch_features,\n                      branch_features, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(branch_features),\n            nn.ReLU(inplace=True),\n            self.depthwise_conv(branch_features, branch_features, kernel_size=3, stride=self.stride, padding=1),\n            nn.BatchNorm2d(branch_features),\n            nn.Conv2d(branch_features, branch_features, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(branch_features),\n            nn.ReLU(inplace=True),\n        )\n\n    @staticmethod\n    def depthwise_conv(i, o, kernel_size, stride=1, padding=0, bias=False):\n        return nn.Conv2d(i, o, kernel_size, stride, padding, bias=bias, groups=i)\n\n    def forward(self, x):\n        if self.stride == 1:\n            x1, x2 = x.chunk(2, dim=1)\n            out = torch.cat((x1, self.branch2(x2)), dim=1)\n        else:\n            out = torch.cat((self.branch1(x), self.branch2(x)), dim=1)\n\n        out = channel_shuffle(out, 2)\n\n        return out\n\n\nclass ShuffleNetV2(nn.Module):\n    def __init__(self, stages_repeats, stages_out_channels, in_channels=3, **kwargs):\n        super(ShuffleNetV2, self).__init__()\n        self.out_channels = []\n        if len(stages_repeats) != 3:\n            raise ValueError(\'expected stages_repeats as list of 3 positive ints\')\n        if len(stages_out_channels) != 5:\n            raise ValueError(\'expected stages_out_channels as list of 5 positive ints\')\n        self._stage_out_channels = stages_out_channels\n\n        output_channels = self._stage_out_channels[0]\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, output_channels, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(output_channels),\n            nn.ReLU(inplace=True),\n        )\n        input_channels = output_channels\n\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.out_channels.append(input_channels)\n        stage_names = [\'stage{}\'.format(i) for i in [2, 3, 4]]\n        for name, repeats, output_channels in zip(\n                stage_names, stages_repeats, self._stage_out_channels[1:]):\n            seq = [InvertedResidual(input_channels, output_channels, 2)]\n            for i in range(repeats - 1):\n                seq.append(InvertedResidual(output_channels, output_channels, 1))\n            setattr(self, name, nn.Sequential(*seq))\n            input_channels = output_channels\n            self.out_channels.append(input_channels)\n        output_channels = self._stage_out_channels[-1]\n        self.conv5 = nn.Sequential(\n            nn.Conv2d(input_channels, output_channels, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(output_channels),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        x = self.conv1(x)\n        c2 = self.maxpool(x)\n        c3 = self.stage2(c2)\n        c4 = self.stage3(c3)\n        c5 = self.stage4(c4)\n        # c5 = self.conv5(c5)\n        return c2, c3, c4, c5\n\n\ndef _shufflenetv2(arch, pretrained, progress, *args, **kwargs):\n    model = ShuffleNetV2(*args, **kwargs)\n\n    if pretrained:\n        model_url = model_urls[arch]\n        if model_url is None:\n            raise NotImplementedError(\'pretrained {} is not supported as of now\'.format(arch))\n        else:\n            assert kwargs[\'in_channels\'] == 3, \'in_channels must be 3 whem pretrained is True\'\n            state_dict = load_state_dict_from_url(model_url, progress=progress)\n            model.load_state_dict(state_dict, strict=False)\n\n    return model\n\n\ndef shufflenet_v2_x0_5(pretrained=False, progress=True, **kwargs):\n    """"""\n    Constructs a ShuffleNetV2 with 0.5x output channels, as described in\n    `""ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design""\n    <https://arxiv.org/abs/1807.11164>`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return _shufflenetv2(\'shufflenetv2_x0.5\', pretrained, progress,\n                         [4, 8, 4], [24, 48, 96, 192, 1024], **kwargs)\n\n\ndef shufflenet_v2_x1_0(pretrained=False, progress=True, **kwargs):\n    """"""\n    Constructs a ShuffleNetV2 with 1.0x output channels, as described in\n    `""ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design""\n    <https://arxiv.org/abs/1807.11164>`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return _shufflenetv2(\'shufflenetv2_x1.0\', pretrained, progress,\n                         [4, 8, 4], [24, 116, 232, 464, 1024], **kwargs)\n\n\ndef shufflenet_v2_x1_5(pretrained=False, progress=True, **kwargs):\n    """"""\n    Constructs a ShuffleNetV2 with 1.5x output channels, as described in\n    `""ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design""\n    <https://arxiv.org/abs/1807.11164>`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return _shufflenetv2(\'shufflenetv2_x1.5\', pretrained, progress,\n                         [4, 8, 4], [24, 176, 352, 704, 1024], **kwargs)\n\n\ndef shufflenet_v2_x2_0(pretrained=False, progress=True, **kwargs):\n    """"""\n    Constructs a ShuffleNetV2 with 2.0x output channels, as described in\n    `""ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design""\n    <https://arxiv.org/abs/1807.11164>`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return _shufflenetv2(\'shufflenetv2_x2.0\', pretrained, progress,\n                         [4, 8, 4], [24, 244, 488, 976, 2048], **kwargs)\n'"
models/head/ConvHead.py,0,"b'# -*- coding: utf-8 -*-\n# @Time    : 2019/12/4 14:54\n# @Author  : zhoujun\nimport torch\nfrom torch import nn\n\n\nclass ConvHead(nn.Module):\n    def __init__(self, in_channels, out_channels,**kwargs):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.conv(x)'"
models/head/DBHead.py,3,"b""# -*- coding: utf-8 -*-\n# @Time    : 2019/12/4 14:54\n# @Author  : zhoujun\nimport torch\nfrom torch import nn\n\nclass DBHead(nn.Module):\n    def __init__(self, in_channels, out_channels, k = 50):\n        super().__init__()\n        self.k = k\n        self.binarize = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels // 4, 3, padding=1),\n            nn.BatchNorm2d(in_channels // 4),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(in_channels // 4, in_channels // 4, 2, 2),\n            nn.BatchNorm2d(in_channels // 4),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(in_channels // 4, 1, 2, 2),\n            nn.Sigmoid())\n        self.binarize.apply(self.weights_init)\n\n        self.thresh = self._init_thresh(in_channels)\n        self.thresh.apply(self.weights_init)\n\n    def forward(self, x):\n        shrink_maps = self.binarize(x)\n        threshold_maps = self.thresh(x)\n        if self.training:\n            binary_maps = self.step_function(shrink_maps, threshold_maps)\n            y = torch.cat((shrink_maps, threshold_maps, binary_maps), dim=1)\n        else:\n            y = torch.cat((shrink_maps, threshold_maps), dim=1)\n        return y\n\n    def weights_init(self, m):\n        classname = m.__class__.__name__\n        if classname.find('Conv') != -1:\n            nn.init.kaiming_normal_(m.weight.data)\n        elif classname.find('BatchNorm') != -1:\n            m.weight.data.fill_(1.)\n            m.bias.data.fill_(1e-4)\n\n    def _init_thresh(self, inner_channels, serial=False, smooth=False, bias=False):\n        in_channels = inner_channels\n        if serial:\n            in_channels += 1\n        self.thresh = nn.Sequential(\n            nn.Conv2d(in_channels, inner_channels // 4, 3, padding=1, bias=bias),\n            nn.BatchNorm2d(inner_channels // 4),\n            nn.ReLU(inplace=True),\n            self._init_upsample(inner_channels // 4, inner_channels // 4, smooth=smooth, bias=bias),\n            nn.BatchNorm2d(inner_channels // 4),\n            nn.ReLU(inplace=True),\n            self._init_upsample(inner_channels // 4, 1, smooth=smooth, bias=bias),\n            nn.Sigmoid())\n        return self.thresh\n\n    def _init_upsample(self, in_channels, out_channels, smooth=False, bias=False):\n        if smooth:\n            inter_out_channels = out_channels\n            if out_channels == 1:\n                inter_out_channels = in_channels\n            module_list = [\n                nn.Upsample(scale_factor=2, mode='nearest'),\n                nn.Conv2d(in_channels, inter_out_channels, 3, 1, 1, bias=bias)]\n            if out_channels == 1:\n                module_list.append(nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=1, bias=True))\n            return nn.Sequential(module_list)\n        else:\n            return nn.ConvTranspose2d(in_channels, out_channels, 2, 2)\n\n    def step_function(self, x, y):\n        return torch.reciprocal(1 + torch.exp(-self.k * (x - y)))\n"""
models/head/__init__.py,0,"b""# -*- coding: utf-8 -*-\n# @Time    : 2020/6/5 11:35\n# @Author  : zhoujun\n__all__ = ['build_head']\nfrom .DBHead import DBHead\nfrom .ConvHead import ConvHead\n\nsupport_head = ['ConvHead', 'DBHead']\n\n\ndef build_head(head_name, **kwargs):\n    assert head_name in support_head, f'all support head is {support_head}'\n    head = eval(head_name)(**kwargs)\n    return head"""
models/losses/DB_loss.py,0,"b'# -*- coding: utf-8 -*-\n# @Time    : 2019/8/23 21:56\n# @Author  : zhoujun\nfrom torch import nn\n\nfrom models.losses.basic_loss import BalanceCrossEntropyLoss, MaskL1Loss, DiceLoss\n\n\nclass DBLoss(nn.Module):\n    def __init__(self, alpha=1.0, beta=10, ohem_ratio=3, reduction=\'mean\', eps=1e-6):\n        """"""\n        Implement PSE Loss.\n        :param alpha: binary_map loss \xe5\x89\x8d\xe9\x9d\xa2\xe7\x9a\x84\xe7\xb3\xbb\xe6\x95\xb0\n        :param beta: threshold_map loss \xe5\x89\x8d\xe9\x9d\xa2\xe7\x9a\x84\xe7\xb3\xbb\xe6\x95\xb0\n        :param ohem_ratio: OHEM\xe7\x9a\x84\xe6\xaf\x94\xe4\xbe\x8b\n        :param reduction: \'mean\' or \'sum\'\xe5\xaf\xb9 batch\xe9\x87\x8c\xe7\x9a\x84loss \xe7\xae\x97\xe5\x9d\x87\xe5\x80\xbc\xe6\x88\x96\xe6\xb1\x82\xe5\x92\x8c\n        """"""\n        super().__init__()\n        assert reduction in [\'mean\', \'sum\'], "" reduction must in [\'mean\',\'sum\']""\n        self.alpha = alpha\n        self.beta = beta\n        self.bce_loss = BalanceCrossEntropyLoss(negative_ratio=ohem_ratio)\n        self.dice_loss = DiceLoss(eps=eps)\n        self.l1_loss = MaskL1Loss(eps=eps)\n        self.ohem_ratio = ohem_ratio\n        self.reduction = reduction\n\n    def forward(self, pred, batch):\n        shrink_maps = pred[:, 0, :, :]\n        threshold_maps = pred[:, 1, :, :]\n        binary_maps = pred[:, 2, :, :]\n\n        loss_shrink_maps = self.bce_loss(shrink_maps, batch[\'shrink_map\'], batch[\'shrink_mask\'])\n        loss_threshold_maps = self.l1_loss(threshold_maps, batch[\'threshold_map\'], batch[\'threshold_mask\'])\n        metrics = dict(loss_shrink_maps=loss_shrink_maps, loss_threshold_maps=loss_threshold_maps)\n        if pred.size()[1] > 2:\n            loss_binary_maps = self.dice_loss(binary_maps, batch[\'shrink_map\'], batch[\'shrink_mask\'])\n            metrics[\'loss_binary_maps\'] = loss_binary_maps\n            loss_all = self.alpha * loss_shrink_maps + self.beta * loss_threshold_maps + loss_binary_maps\n            metrics[\'loss\'] = loss_all\n        else:\n            metrics[\'loss\'] = loss_shrink_maps\n        return metrics\n'"
models/losses/__init__.py,0,"b""# -*- coding: utf-8 -*-\n# @Time    : 2020/6/5 11:36\n# @Author  : zhoujun\n__all__ = ['build_loss']\n\nfrom .DB_loss import DBLoss\n\nsupport_loss = ['DBLoss']\n\n\ndef build_loss(loss_name, **kwargs):\n    assert loss_name in support_loss, f'all support loss is {support_loss}'\n    criterion = eval(loss_name)(**kwargs)\n    return criterion\n"""
models/losses/basic_loss.py,10,"b""# -*- coding: utf-8 -*-\n# @Time    : 2019/12/4 14:39\n# @Author  : zhoujun\nimport torch\nimport torch.nn as nn\n\n\nclass BalanceCrossEntropyLoss(nn.Module):\n    '''\n    Balanced cross entropy loss.\n    Shape:\n        - Input: :math:`(N, 1, H, W)`\n        - GT: :math:`(N, 1, H, W)`, same shape as the input\n        - Mask: :math:`(N, H, W)`, same spatial shape as the input\n        - Output: scalar.\n\n    Examples::\n\n        >>> m = nn.Sigmoid()\n        >>> loss = nn.BCELoss()\n        >>> input = torch.randn(3, requires_grad=True)\n        >>> target = torch.empty(3).random_(2)\n        >>> output = loss(m(input), target)\n        >>> output.backward()\n    '''\n\n    def __init__(self, negative_ratio=3.0, eps=1e-6):\n        super(BalanceCrossEntropyLoss, self).__init__()\n        self.negative_ratio = negative_ratio\n        self.eps = eps\n\n    def forward(self,\n                pred: torch.Tensor,\n                gt: torch.Tensor,\n                mask: torch.Tensor,\n                return_origin=False):\n        '''\n        Args:\n            pred: shape :math:`(N, 1, H, W)`, the prediction of network\n            gt: shape :math:`(N, 1, H, W)`, the target\n            mask: shape :math:`(N, H, W)`, the mask indicates positive regions\n        '''\n        positive = (gt * mask).byte()\n        negative = ((1 - gt) * mask).byte()\n        positive_count = int(positive.float().sum())\n        negative_count = min(int(negative.float().sum()), int(positive_count * self.negative_ratio))\n        loss = nn.functional.binary_cross_entropy(pred, gt, reduction='none')\n        positive_loss = loss * positive.float()\n        negative_loss = loss * negative.float()\n        # negative_loss, _ = torch.topk(negative_loss.view(-1).contiguous(), negative_count)\n        negative_loss, _ = negative_loss.view(-1).topk(negative_count)\n\n        balance_loss = (positive_loss.sum() + negative_loss.sum()) / (positive_count + negative_count + self.eps)\n\n        if return_origin:\n            return balance_loss, loss\n        return balance_loss\n\n\nclass DiceLoss(nn.Module):\n    '''\n    Loss function from https://arxiv.org/abs/1707.03237,\n    where iou computation is introduced heatmap manner to measure the\n    diversity bwtween tow heatmaps.\n    '''\n\n    def __init__(self, eps=1e-6):\n        super(DiceLoss, self).__init__()\n        self.eps = eps\n\n    def forward(self, pred: torch.Tensor, gt, mask, weights=None):\n        '''\n        pred: one or two heatmaps of shape (N, 1, H, W),\n            the losses of tow heatmaps are added together.\n        gt: (N, 1, H, W)\n        mask: (N, H, W)\n        '''\n        return self._compute(pred, gt, mask, weights)\n\n    def _compute(self, pred, gt, mask, weights):\n        if pred.dim() == 4:\n            pred = pred[:, 0, :, :]\n            gt = gt[:, 0, :, :]\n        assert pred.shape == gt.shape\n        assert pred.shape == mask.shape\n        if weights is not None:\n            assert weights.shape == mask.shape\n            mask = weights * mask\n        intersection = (pred * gt * mask).sum()\n\n        union = (pred * mask).sum() + (gt * mask).sum() + self.eps\n        loss = 1 - 2.0 * intersection / union\n        assert loss <= 1\n        return loss\n\n\nclass MaskL1Loss(nn.Module):\n    def __init__(self, eps=1e-6):\n        super(MaskL1Loss, self).__init__()\n        self.eps = eps\n\n    def forward(self, pred: torch.Tensor, gt, mask):\n        loss = (torch.abs(pred - gt) * mask).sum() / (mask.sum() + self.eps)\n        return loss\n"""
models/neck/FPEM_FFM.py,2,"b'# -*- coding: utf-8 -*-\n# @Time    : 2019/9/13 10:29\n# @Author  : zhoujun\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom models.basic import ConvBnRelu\n\n\nclass FPEM_FFM(nn.Module):\n    def __init__(self, in_channels, inner_channels=128, fpem_repeat=2, **kwargs):\n        """"""\n        PANnet\n        :param in_channels: \xe5\x9f\xba\xe7\xa1\x80\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\n        """"""\n        super().__init__()\n        self.conv_out = inner_channels\n        inplace = True\n        # reduce layers\n        self.reduce_conv_c2 = ConvBnRelu(in_channels[0], inner_channels, kernel_size=1, inplace=inplace)\n        self.reduce_conv_c3 = ConvBnRelu(in_channels[1], inner_channels, kernel_size=1, inplace=inplace)\n        self.reduce_conv_c4 = ConvBnRelu(in_channels[2], inner_channels, kernel_size=1, inplace=inplace)\n        self.reduce_conv_c5 = ConvBnRelu(in_channels[3], inner_channels, kernel_size=1, inplace=inplace)\n        self.fpems = nn.ModuleList()\n        for i in range(fpem_repeat):\n            self.fpems.append(FPEM(self.conv_out))\n        self.out_channels = self.conv_out * 4\n\n    def forward(self, x):\n        c2, c3, c4, c5 = x\n        # reduce channel\n        c2 = self.reduce_conv_c2(c2)\n        c3 = self.reduce_conv_c3(c3)\n        c4 = self.reduce_conv_c4(c4)\n        c5 = self.reduce_conv_c5(c5)\n\n        # FPEM\n        for i, fpem in enumerate(self.fpems):\n            c2, c3, c4, c5 = fpem(c2, c3, c4, c5)\n            if i == 0:\n                c2_ffm = c2\n                c3_ffm = c3\n                c4_ffm = c4\n                c5_ffm = c5\n            else:\n                c2_ffm += c2\n                c3_ffm += c3\n                c4_ffm += c4\n                c5_ffm += c5\n\n        # FFM\n        c5 = F.interpolate(c5_ffm, c2_ffm.size()[-2:])\n        c4 = F.interpolate(c4_ffm, c2_ffm.size()[-2:])\n        c3 = F.interpolate(c3_ffm, c2_ffm.size()[-2:])\n        Fy = torch.cat([c2_ffm, c3, c4, c5], dim=1)\n        return Fy\n\n\nclass FPEM(nn.Module):\n    def __init__(self, in_channels=128):\n        super().__init__()\n        self.up_add1 = SeparableConv2d(in_channels, in_channels, 1)\n        self.up_add2 = SeparableConv2d(in_channels, in_channels, 1)\n        self.up_add3 = SeparableConv2d(in_channels, in_channels, 1)\n        self.down_add1 = SeparableConv2d(in_channels, in_channels, 2)\n        self.down_add2 = SeparableConv2d(in_channels, in_channels, 2)\n        self.down_add3 = SeparableConv2d(in_channels, in_channels, 2)\n\n    def forward(self, c2, c3, c4, c5):\n        # up\xe9\x98\xb6\xe6\xae\xb5\n        c4 = self.up_add1(self._upsample_add(c5, c4))\n        c3 = self.up_add2(self._upsample_add(c4, c3))\n        c2 = self.up_add3(self._upsample_add(c3, c2))\n\n        # down \xe9\x98\xb6\xe6\xae\xb5\n        c3 = self.down_add1(self._upsample_add(c3, c2))\n        c4 = self.down_add2(self._upsample_add(c4, c3))\n        c5 = self.down_add3(self._upsample_add(c5, c4))\n        return c2, c3, c4, c5\n\n    def _upsample_add(self, x, y):\n        return F.interpolate(x, size=y.size()[2:]) + y\n\n\nclass SeparableConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(SeparableConv2d, self).__init__()\n\n        self.depthwise_conv = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, padding=1,\n                                        stride=stride, groups=in_channels)\n        self.pointwise_conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.depthwise_conv(x)\n        x = self.pointwise_conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n'"
models/neck/FPN.py,2,"b'# -*- coding: utf-8 -*-\n# @Time    : 2019/9/13 10:29\n# @Author  : zhoujun\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom models.basic import ConvBnRelu\n\n\nclass FPN(nn.Module):\n    def __init__(self, in_channels, inner_channels=256, **kwargs):\n        """"""\n        :param in_channels: \xe5\x9f\xba\xe7\xa1\x80\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\n        :param kwargs:\n        """"""\n        super().__init__()\n        inplace = True\n        self.conv_out = inner_channels\n        inner_channels = inner_channels // 4\n        # reduce layers\n        self.reduce_conv_c2 = ConvBnRelu(in_channels[0], inner_channels, kernel_size=1, inplace=inplace)\n        self.reduce_conv_c3 = ConvBnRelu(in_channels[1], inner_channels, kernel_size=1, inplace=inplace)\n        self.reduce_conv_c4 = ConvBnRelu(in_channels[2], inner_channels, kernel_size=1, inplace=inplace)\n        self.reduce_conv_c5 = ConvBnRelu(in_channels[3], inner_channels, kernel_size=1, inplace=inplace)\n        # Smooth layers\n        self.smooth_p4 = ConvBnRelu(inner_channels, inner_channels, kernel_size=3, padding=1, inplace=inplace)\n        self.smooth_p3 = ConvBnRelu(inner_channels, inner_channels, kernel_size=3, padding=1, inplace=inplace)\n        self.smooth_p2 = ConvBnRelu(inner_channels, inner_channels, kernel_size=3, padding=1, inplace=inplace)\n\n        self.conv = nn.Sequential(\n            nn.Conv2d(self.conv_out, self.conv_out, kernel_size=3, padding=1, stride=1),\n            nn.BatchNorm2d(self.conv_out),\n            nn.ReLU(inplace=inplace)\n        )\n        self.out_channels = self.conv_out\n\n    def forward(self, x):\n        c2, c3, c4, c5 = x\n        # Top-down\n        p5 = self.reduce_conv_c5(c5)\n        p4 = self._upsample_add(p5, self.reduce_conv_c4(c4))\n        p4 = self.smooth_p4(p4)\n        p3 = self._upsample_add(p4, self.reduce_conv_c3(c3))\n        p3 = self.smooth_p3(p3)\n        p2 = self._upsample_add(p3, self.reduce_conv_c2(c2))\n        p2 = self.smooth_p2(p2)\n\n        x = self._upsample_cat(p2, p3, p4, p5)\n        x = self.conv(x)\n        return x\n\n    def _upsample_add(self, x, y):\n        return F.interpolate(x, size=y.size()[2:]) + y\n\n    def _upsample_cat(self, p2, p3, p4, p5):\n        h, w = p2.size()[2:]\n        p3 = F.interpolate(p3, size=(h, w))\n        p4 = F.interpolate(p4, size=(h, w))\n        p5 = F.interpolate(p5, size=(h, w))\n        return torch.cat([p2, p3, p4, p5], dim=1)\n'"
models/neck/__init__.py,0,"b""# -*- coding: utf-8 -*-\n# @Time    : 2020/6/5 11:34\n# @Author  : zhoujun\n__all__ = ['build_neck']\nfrom .FPN import FPN\nfrom .FPEM_FFM import FPEM_FFM\n\nsupport_neck = ['FPN', 'FPEM_FFM']\n\n\ndef build_neck(neck_name, **kwargs):\n    assert neck_name in support_neck, f'all support neck is {support_neck}'\n    neck = eval(neck_name)(**kwargs)\n    return neck\n"""
utils/cal_recall/__init__.py,0,"b""# -*- coding: utf-8 -*-\n# @Time    : 1/16/19 6:40 AM\n# @Author  : zhoujun\nfrom .script import  cal_recall_precison_f1\n__all__ = ['cal_recall_precison_f1']"""
utils/cal_recall/rrc_evaluation_funcs.py,0,"b'#!/usr/bin/env python2\n#encoding: UTF-8\nimport json\nimport sys;sys.path.append(\'./\')\nimport zipfile\nimport re\nimport sys\nimport os\nimport codecs\nimport traceback\nimport numpy as np\nfrom utils import order_points_clockwise\n\ndef print_help():\n    sys.stdout.write(\'Usage: python %s.py -g=<gtFile> -s=<submFile> [-o=<outputFolder> -p=<jsonParams>]\' %sys.argv[0])\n    sys.exit(2)\n    \n\ndef load_zip_file_keys(file,fileNameRegExp=\'\'):\n    """"""\n    Returns an array with the entries of the ZIP file that match with the regular expression.\n    The key\'s are the names or the file or the capturing group definied in the fileNameRegExp\n    """"""\n    try:\n        archive=zipfile.ZipFile(file, mode=\'r\', allowZip64=True)\n    except :\n        raise Exception(\'Error loading the ZIP archive.\')\n\n    pairs = []\n    \n    for name in archive.namelist():\n        addFile = True\n        keyName = name\n        if fileNameRegExp!="""":\n            m = re.match(fileNameRegExp,name)\n            if m == None:\n                addFile = False\n            else:\n                if len(m.groups())>0:\n                    keyName = m.group(1)\n                    \n        if addFile:\n            pairs.append( keyName )\n                \n    return pairs\n    \n\ndef load_zip_file(file,fileNameRegExp=\'\',allEntries=False):\n    """"""\n    Returns an array with the contents (filtered by fileNameRegExp) of a ZIP file.\n    The key\'s are the names or the file or the capturing group definied in the fileNameRegExp\n    allEntries validates that all entries in the ZIP file pass the fileNameRegExp\n    """"""\n    try:\n        archive=zipfile.ZipFile(file, mode=\'r\', allowZip64=True)\n    except :\n        raise Exception(\'Error loading the ZIP archive\')    \n\n    pairs = []\n    for name in archive.namelist():\n        addFile = True\n        keyName = name\n        if fileNameRegExp!="""":\n            m = re.match(fileNameRegExp,name)\n            if m == None:\n                addFile = False\n            else:\n                if len(m.groups())>0:\n                    keyName = m.group(1)\n        \n        if addFile:\n            pairs.append( [ keyName , archive.read(name)] )\n        else:\n            if allEntries:\n                raise Exception(\'ZIP entry not valid: %s\' %name)             \n\n    return dict(pairs)\n\n\ndef load_folder_file(file, fileNameRegExp=\'\', allEntries=False):\n    """"""\n    Returns an array with the contents (filtered by fileNameRegExp) of a ZIP file.\n    The key\'s are the names or the file or the capturing group definied in the fileNameRegExp\n    allEntries validates that all entries in the ZIP file pass the fileNameRegExp\n    """"""\n    pairs = []\n    for name in os.listdir(file):\n        addFile = True\n        keyName = name\n        if fileNameRegExp != """":\n            m = re.match(fileNameRegExp, name)\n            if m == None:\n                addFile = False\n            else:\n                if len(m.groups()) > 0:\n                    keyName = m.group(1)\n\n        if addFile:\n            pairs.append([keyName, open(os.path.join(file,name)).read()])\n        else:\n            if allEntries:\n                raise Exception(\'ZIP entry not valid: %s\' % name)\n\n    return dict(pairs)\n\n\ndef decode_utf8(raw):\n    """"""\n    Returns a Unicode object on success, or None on failure\n    """"""\n    try:\n        raw = codecs.decode(raw,\'utf-8\', \'replace\')\n        #extracts BOM if exists\n        raw = raw.encode(\'utf8\')\n        if raw.startswith(codecs.BOM_UTF8):\n            raw = raw.replace(codecs.BOM_UTF8, \'\', 1)\n        return raw.decode(\'utf-8\')\n    except:\n       return None\n   \ndef validate_lines_in_file(fileName,file_contents,CRLF=True,LTRB=True,withTranscription=False,withConfidence=False,imWidth=0,imHeight=0):\n    """"""\n    This function validates that all lines of the file calling the Line validation function for each line\n    """"""\n    utf8File = decode_utf8(file_contents)\n    if (utf8File is None) :\n        raise Exception(""The file %s is not UTF-8"" %fileName)\n\n    lines = utf8File.split( ""\\r\\n"" if CRLF else ""\\n"" )\n    for line in lines:\n        line = line.replace(""\\r"","""").replace(""\\n"","""")\n        if(line != """"):\n            try:\n                validate_tl_line(line,LTRB,withTranscription,withConfidence,imWidth,imHeight)\n            except Exception as e:\n                raise Exception((""Line in sample not valid. Sample: %s Line: %s Error: %s"" %(fileName,line,str(e))).encode(\'utf-8\', \'replace\'))\n    \n   \n   \ndef validate_tl_line(line,LTRB=True,withTranscription=True,withConfidence=True,imWidth=0,imHeight=0):\n    """"""\n    Validate the format of the line. If the line is not valid an exception will be raised.\n    If maxWidth and maxHeight are specified, all points must be inside the imgage bounds.\n    Posible values are:\n    LTRB=True: xmin,ymin,xmax,ymax[,confidence][,transcription] \n    LTRB=False: x1,y1,x2,y2,x3,y3,x4,y4[,confidence][,transcription] \n    """"""\n    get_tl_line_values(line,LTRB,withTranscription,withConfidence,imWidth,imHeight)\n    \n   \ndef get_tl_line_values(line,LTRB=True,withTranscription=False,withConfidence=False,imWidth=0,imHeight=0):\n    """"""\n    Validate the format of the line. If the line is not valid an exception will be raised.\n    If maxWidth and maxHeight are specified, all points must be inside the imgage bounds.\n    Posible values are:\n    LTRB=True: xmin,ymin,xmax,ymax[,confidence][,transcription] \n    LTRB=False: x1,y1,x2,y2,x3,y3,x4,y4[,confidence][,transcription] \n    Returns values from a textline. Points , [Confidences], [Transcriptions]\n    """"""\n    confidence = 0.0\n    transcription = """";\n    points = []\n    \n    numPoints = 4;\n    \n    if LTRB:\n    \n        numPoints = 4;\n        \n        if withTranscription and withConfidence:\n            m = re.match(r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-1].?[0-9]*)\\s*,(.*)$\',line)\n            if m == None :\n                m = re.match(r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-1].?[0-9]*)\\s*,(.*)$\',line)\n                raise Exception(""Format incorrect. Should be: xmin,ymin,xmax,ymax,confidence,transcription"")\n        elif withConfidence:\n            m = re.match(r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-1].?[0-9]*)\\s*$\',line)\n            if m == None :\n                raise Exception(""Format incorrect. Should be: xmin,ymin,xmax,ymax,confidence"")\n        elif withTranscription:\n            m = re.match(r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,(.*)$\',line)\n            if m == None :\n                raise Exception(""Format incorrect. Should be: xmin,ymin,xmax,ymax,transcription"")\n        else:\n            m = re.match(r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,?\\s*$\',line)\n            if m == None :\n                raise Exception(""Format incorrect. Should be: xmin,ymin,xmax,ymax"")\n            \n        xmin = int(m.group(1))\n        ymin = int(m.group(2))\n        xmax = int(m.group(3))\n        ymax = int(m.group(4))\n        if(xmax<xmin):\n                raise Exception(""Xmax value (%s) not valid (Xmax < Xmin)."" %(xmax))\n        if(ymax<ymin):\n                raise Exception(""Ymax value (%s)  not valid (Ymax < Ymin)."" %(ymax))  \n\n        points = [ float(m.group(i)) for i in range(1, (numPoints+1) ) ]\n        \n        if (imWidth>0 and imHeight>0):\n            validate_point_inside_bounds(xmin,ymin,imWidth,imHeight);\n            validate_point_inside_bounds(xmax,ymax,imWidth,imHeight);\n\n    else:\n        \n        numPoints = 8;\n        \n        if withTranscription and withConfidence:\n            m = re.match(r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-1].?[0-9]*)\\s*,(.*)$\',line)\n            if m == None :\n                raise Exception(""Format incorrect. Should be: x1,y1,x2,y2,x3,y3,x4,y4,confidence,transcription"")\n        elif withConfidence:\n            m = re.match(r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-1].?[0-9]*)\\s*$\',line)\n            if m == None :\n                raise Exception(""Format incorrect. Should be: x1,y1,x2,y2,x3,y3,x4,y4,confidence"")\n        elif withTranscription:\n            m = re.match(r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,(.*)$\',line)\n            if m == None :\n                raise Exception(""Format incorrect. Should be: x1,y1,x2,y2,x3,y3,x4,y4,transcription"")\n        else:\n            m = re.match(r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*$\',line)\n            if m == None :\n                raise Exception(""Format incorrect. Should be: x1,y1,x2,y2,x3,y3,x4,y4"")\n            \n        points = [ float(m.group(i)) for i in range(1, (numPoints+1) ) ]\n\n        points = order_points_clockwise(np.array(points).reshape(-1, 2)).reshape(-1)\n        validate_clockwise_points(points)\n        \n        if (imWidth>0 and imHeight>0):\n            validate_point_inside_bounds(points[0],points[1],imWidth,imHeight);\n            validate_point_inside_bounds(points[2],points[3],imWidth,imHeight);\n            validate_point_inside_bounds(points[4],points[5],imWidth,imHeight);\n            validate_point_inside_bounds(points[6],points[7],imWidth,imHeight);\n            \n    \n    if withConfidence:\n        try:\n            confidence = float(m.group(numPoints+1))\n        except ValueError:\n            raise Exception(""Confidence value must be a float"")       \n            \n    if withTranscription:\n        posTranscription = numPoints + (2 if withConfidence else 1)\n        transcription = m.group(posTranscription)\n        m2 = re.match(r\'^\\s*\\""(.*)\\""\\s*$\',transcription)\n        if m2 != None : #Transcription with double quotes, we extract the value and replace escaped characters\n            transcription = m2.group(1).replace(""\\\\\\\\"", ""\\\\"").replace(""\\\\\\"""", ""\\"""")\n    \n    return points,confidence,transcription\n    \n            \ndef validate_point_inside_bounds(x,y,imWidth,imHeight):\n    if(x<0 or x>imWidth):\n            raise Exception(""X value (%s) not valid. Image dimensions: (%s,%s)"" %(xmin,imWidth,imHeight))\n    if(y<0 or y>imHeight):\n            raise Exception(""Y value (%s)  not valid. Image dimensions: (%s,%s) Sample: %s Line:%s"" %(ymin,imWidth,imHeight))\n\ndef validate_clockwise_points(points):\n    """"""\n    Validates that the points that the 4 points that dlimite a polygon are in clockwise order.\n    """"""\n    \n    if len(points) != 8:\n        raise Exception(""Points list not valid."" + str(len(points)))\n    \n    point = [\n                [int(points[0]) , int(points[1])],\n                [int(points[2]) , int(points[3])],\n                [int(points[4]) , int(points[5])],\n                [int(points[6]) , int(points[7])]\n            ]\n    edge = [\n                ( point[1][0] - point[0][0])*( point[1][1] + point[0][1]),\n                ( point[2][0] - point[1][0])*( point[2][1] + point[1][1]),\n                ( point[3][0] - point[2][0])*( point[3][1] + point[2][1]),\n                ( point[0][0] - point[3][0])*( point[0][1] + point[3][1])\n    ]\n    \n    summatory = edge[0] + edge[1] + edge[2] + edge[3];\n    if summatory>0:\n        raise Exception(""Points are not clockwise. The coordinates of bounding quadrilaterals have to be given in clockwise order. Regarding the correct interpretation of \'clockwise\' remember that the image coordinate system used is the standard one, with the image origin at the upper left, the X axis extending to the right and Y axis extending downwards."")\n\ndef get_tl_line_values_from_file_contents(content,CRLF=True,LTRB=True,withTranscription=False,withConfidence=False,imWidth=0,imHeight=0,sort_by_confidences=True):\n    """"""\n    Returns all points, confindences and transcriptions of a file in lists. Valid line formats:\n    xmin,ymin,xmax,ymax,[confidence],[transcription]\n    x1,y1,x2,y2,x3,y3,x4,y4,[confidence],[transcription]\n    """"""\n    pointsList = []\n    transcriptionsList = []\n    confidencesList = []\n    \n    lines = content.split( ""\\r\\n"" if CRLF else ""\\n"" )\n    for line in lines:\n        line = line.replace(""\\r"","""").replace(""\\n"","""")\n        if(line != """") :\n            points, confidence, transcription = get_tl_line_values(line,LTRB,withTranscription,withConfidence,imWidth,imHeight);\n            pointsList.append(points)\n            transcriptionsList.append(transcription)\n            confidencesList.append(confidence)\n\n    if withConfidence and len(confidencesList)>0 and sort_by_confidences:\n        import numpy as np\n        sorted_ind = np.argsort(-np.array(confidencesList))\n        confidencesList = [confidencesList[i] for i in sorted_ind]\n        pointsList = [pointsList[i] for i in sorted_ind]\n        transcriptionsList = [transcriptionsList[i] for i in sorted_ind]        \n        \n    return pointsList,confidencesList,transcriptionsList\n\ndef main_evaluation(p,default_evaluation_params_fn,validate_data_fn,evaluate_method_fn,show_result=True,per_sample=True):\n    """"""\n    This process validates a method, evaluates it and if it succed generates a ZIP file with a JSON entry for each sample.\n    Params:\n    p: Dictionary of parmeters with the GT/submission locations. If None is passed, the parameters send by the system are used.\n    default_evaluation_params_fn: points to a function that returns a dictionary with the default parameters used for the evaluation\n    validate_data_fn: points to a method that validates the corrct format of the submission\n    evaluate_method_fn: points to a function that evaluated the submission and return a Dictionary with the results\n    """"""\n    evalParams = default_evaluation_params_fn()\n    if \'p\' in p.keys():\n        evalParams.update( p[\'p\'] if isinstance(p[\'p\'], dict) else json.loads(p[\'p\'][1:-1]) )\n\n    resDict={\'calculated\':True,\'Message\':\'\',\'method\':\'{}\',\'per_sample\':\'{}\'}    \n    try:\n        # validate_data_fn(p[\'g\'], p[\'s\'], evalParams)\n        evalData = evaluate_method_fn(p[\'g\'], p[\'s\'], evalParams)\n        resDict.update(evalData)\n        \n    except Exception as e:\n        traceback.print_exc()\n        resDict[\'Message\']= str(e)\n        resDict[\'calculated\']=False\n\n    if \'o\' in p:\n        if not os.path.exists(p[\'o\']):\n            os.makedirs(p[\'o\'])\n\n        resultsOutputname = p[\'o\'] + \'/results.zip\'\n        outZip = zipfile.ZipFile(resultsOutputname, mode=\'w\', allowZip64=True)\n\n        del resDict[\'per_sample\']\n        if \'output_items\' in resDict.keys():\n            del resDict[\'output_items\']\n\n        outZip.writestr(\'method.json\',json.dumps(resDict))\n        \n    if not resDict[\'calculated\']:\n        if show_result:\n            sys.stderr.write(\'Error!\\n\'+ resDict[\'Message\']+\'\\n\\n\')\n        if \'o\' in p:\n            outZip.close()\n        return resDict\n    \n    if \'o\' in p:\n        if per_sample == True:\n            for k,v in evalData[\'per_sample\'].iteritems():\n                outZip.writestr( k + \'.json\',json.dumps(v)) \n\n            if \'output_items\' in evalData.keys():\n                for k, v in evalData[\'output_items\'].iteritems():\n                    outZip.writestr( k,v) \n\n        outZip.close()\n\n    if show_result:\n        sys.stdout.write(""Calculated!"")\n        sys.stdout.write(json.dumps(resDict[\'method\']))\n    \n    return resDict\n\n\ndef main_validation(default_evaluation_params_fn,validate_data_fn):\n    """"""\n    This process validates a method\n    Params:\n    default_evaluation_params_fn: points to a function that returns a dictionary with the default parameters used for the evaluation\n    validate_data_fn: points to a method that validates the corrct format of the submission\n    """"""    \n    try:\n        p = dict([s[1:].split(\'=\') for s in sys.argv[1:]])\n        evalParams = default_evaluation_params_fn()\n        if \'p\' in p.keys():\n            evalParams.update( p[\'p\'] if isinstance(p[\'p\'], dict) else json.loads(p[\'p\'][1:-1]) )\n\n        validate_data_fn(p[\'g\'], p[\'s\'], evalParams)              \n        print(\'SUCCESS\')\n        sys.exit(0)\n    except Exception as e:\n        print(str(e))\n        sys.exit(101)'"
utils/cal_recall/script.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom collections import namedtuple\nfrom . import rrc_evaluation_funcs\nimport Polygon as plg\nimport numpy as np\n\n\ndef default_evaluation_params():\n    """"""\n    default_evaluation_params: Default parameters to use for the validation and evaluation.\n    """"""\n    return {\n        \'IOU_CONSTRAINT\': 0.5,\n        \'AREA_PRECISION_CONSTRAINT\': 0.5,\n        \'GT_SAMPLE_NAME_2_ID\': \'gt_img_([0-9]+).txt\',\n        \'DET_SAMPLE_NAME_2_ID\': \'res_img_([0-9]+).txt\',\n        \'LTRB\': False,  # LTRB:2points(left,top,right,bottom) or 4 points(x1,y1,x2,y2,x3,y3,x4,y4)\n        \'CRLF\': False,  # Lines are delimited by Windows CRLF format\n        \'CONFIDENCES\': False,  # Detections must include confidence value. AP will be calculated\n        \'PER_SAMPLE_RESULTS\': True  # Generate per sample results and produce data for visualization\n    }\n\n\ndef validate_data(gtFilePath, submFilePath, evaluationParams):\n    """"""\n    Method validate_data: validates that all files in the results folder are correct (have the correct name contents).\n                            Validates also that there are no missing files in the folder.\n                            If some error detected, the method raises the error\n    """"""\n    gt = rrc_evaluation_funcs.load_folder_file(gtFilePath, evaluationParams[\'GT_SAMPLE_NAME_2_ID\'])\n\n    subm = rrc_evaluation_funcs.load_folder_file(submFilePath, evaluationParams[\'DET_SAMPLE_NAME_2_ID\'], True)\n\n    # Validate format of GroundTruth\n    for k in gt:\n        rrc_evaluation_funcs.validate_lines_in_file(k, gt[k], evaluationParams[\'CRLF\'], evaluationParams[\'LTRB\'], True)\n\n    # Validate format of results\n    for k in subm:\n        if (k in gt) == False:\n            raise Exception(""The sample %s not present in GT"" % k)\n\n        rrc_evaluation_funcs.validate_lines_in_file(k, subm[k], evaluationParams[\'CRLF\'], evaluationParams[\'LTRB\'],\n                                                    False, evaluationParams[\'CONFIDENCES\'])\n\n\ndef evaluate_method(gtFilePath, submFilePath, evaluationParams):\n    """"""\n    Method evaluate_method: evaluate method and returns the results\n        Results. Dictionary with the following values:\n        - method (required)  Global method metrics. Ex: { \'Precision\':0.8,\'Recall\':0.9 }\n        - samples (optional) Per sample metrics. Ex: {\'sample1\' : { \'Precision\':0.8,\'Recall\':0.9 } , \'sample2\' : { \'Precision\':0.8,\'Recall\':0.9 }\n    """"""\n\n    def polygon_from_points(points):\n        """"""\n        Returns a Polygon object to use with the Polygon2 class from a list of 8 points: x1,y1,x2,y2,x3,y3,x4,y4\n        """"""\n        resBoxes = np.empty([1, 8], dtype=\'int32\')\n        resBoxes[0, 0] = int(points[0])\n        resBoxes[0, 4] = int(points[1])\n        resBoxes[0, 1] = int(points[2])\n        resBoxes[0, 5] = int(points[3])\n        resBoxes[0, 2] = int(points[4])\n        resBoxes[0, 6] = int(points[5])\n        resBoxes[0, 3] = int(points[6])\n        resBoxes[0, 7] = int(points[7])\n        pointMat = resBoxes[0].reshape([2, 4]).T\n        return plg.Polygon(pointMat)\n\n    def rectangle_to_polygon(rect):\n        resBoxes = np.empty([1, 8], dtype=\'int32\')\n        resBoxes[0, 0] = int(rect.xmin)\n        resBoxes[0, 4] = int(rect.ymax)\n        resBoxes[0, 1] = int(rect.xmin)\n        resBoxes[0, 5] = int(rect.ymin)\n        resBoxes[0, 2] = int(rect.xmax)\n        resBoxes[0, 6] = int(rect.ymin)\n        resBoxes[0, 3] = int(rect.xmax)\n        resBoxes[0, 7] = int(rect.ymax)\n\n        pointMat = resBoxes[0].reshape([2, 4]).T\n\n        return plg.Polygon(pointMat)\n\n    def rectangle_to_points(rect):\n        points = [int(rect.xmin), int(rect.ymax), int(rect.xmax), int(rect.ymax), int(rect.xmax), int(rect.ymin),\n                  int(rect.xmin), int(rect.ymin)]\n        return points\n\n    def get_union(pD, pG):\n        areaA = pD.area();\n        areaB = pG.area();\n        return areaA + areaB - get_intersection(pD, pG);\n\n    def get_intersection_over_union(pD, pG):\n        try:\n            return get_intersection(pD, pG) / get_union(pD, pG);\n        except:\n            return 0\n\n    def get_intersection(pD, pG):\n        pInt = pD & pG\n        if len(pInt) == 0:\n            return 0\n        return pInt.area()\n\n    def compute_ap(confList, matchList, numGtCare):\n        correct = 0\n        AP = 0\n        if len(confList) > 0:\n            confList = np.array(confList)\n            matchList = np.array(matchList)\n            sorted_ind = np.argsort(-confList)\n            confList = confList[sorted_ind]\n            matchList = matchList[sorted_ind]\n            for n in range(len(confList)):\n                match = matchList[n]\n                if match:\n                    correct += 1\n                    AP += float(correct) / (n + 1)\n\n            if numGtCare > 0:\n                AP /= numGtCare\n\n        return AP\n\n    perSampleMetrics = {}\n\n    matchedSum = 0\n\n    Rectangle = namedtuple(\'Rectangle\', \'xmin ymin xmax ymax\')\n\n    gt = rrc_evaluation_funcs.load_folder_file(gtFilePath, evaluationParams[\'GT_SAMPLE_NAME_2_ID\'])\n    subm = rrc_evaluation_funcs.load_folder_file(submFilePath, evaluationParams[\'DET_SAMPLE_NAME_2_ID\'], True)\n\n    numGlobalCareGt = 0;\n    numGlobalCareDet = 0;\n\n    arrGlobalConfidences = [];\n    arrGlobalMatches = [];\n\n    for resFile in gt:\n\n        gtFile = gt[resFile]  # rrc_evaluation_funcs.decode_utf8(gt[resFile])\n        recall = 0\n        precision = 0\n        hmean = 0\n\n        detMatched = 0\n\n        iouMat = np.empty([1, 1])\n\n        gtPols = []\n        detPols = []\n\n        gtPolPoints = []\n        detPolPoints = []\n\n        # Array of Ground Truth Polygons\' keys marked as don\'t Care\n        gtDontCarePolsNum = []\n        # Array of Detected Polygons\' matched with a don\'t Care GT\n        detDontCarePolsNum = []\n\n        pairs = []\n        detMatchedNums = []\n\n        arrSampleConfidences = [];\n        arrSampleMatch = [];\n        sampleAP = 0;\n\n        evaluationLog = """"\n\n        pointsList, _, transcriptionsList = rrc_evaluation_funcs.get_tl_line_values_from_file_contents(gtFile,\n                                                                                                       evaluationParams[\n                                                                                                           \'CRLF\'],\n                                                                                                       evaluationParams[\n                                                                                                           \'LTRB\'],\n                                                                                                       True, False)\n        for n in range(len(pointsList)):\n            points = pointsList[n]\n            transcription = transcriptionsList[n]\n            dontCare = transcription == ""###""\n            if evaluationParams[\'LTRB\']:\n                gtRect = Rectangle(*points)\n                gtPol = rectangle_to_polygon(gtRect)\n            else:\n                gtPol = polygon_from_points(points)\n            gtPols.append(gtPol)\n            gtPolPoints.append(points)\n            if dontCare:\n                gtDontCarePolsNum.append(len(gtPols) - 1)\n\n        evaluationLog += ""GT polygons: "" + str(len(gtPols)) + (\n            "" ("" + str(len(gtDontCarePolsNum)) + "" don\'t care)\\n"" if len(gtDontCarePolsNum) > 0 else ""\\n"")\n\n        if resFile in subm:\n\n            detFile = subm[resFile]  # rrc_evaluation_funcs.decode_utf8(subm[resFile])\n\n            pointsList, confidencesList, _ = rrc_evaluation_funcs.get_tl_line_values_from_file_contents(detFile,\n                                                                                                        evaluationParams[\n                                                                                                            \'CRLF\'],\n                                                                                                        evaluationParams[\n                                                                                                            \'LTRB\'],\n                                                                                                        False,\n                                                                                                        evaluationParams[\n                                                                                                            \'CONFIDENCES\'])\n            for n in range(len(pointsList)):\n                points = pointsList[n]\n\n                if evaluationParams[\'LTRB\']:\n                    detRect = Rectangle(*points)\n                    detPol = rectangle_to_polygon(detRect)\n                else:\n                    detPol = polygon_from_points(points)\n                detPols.append(detPol)\n                detPolPoints.append(points)\n                if len(gtDontCarePolsNum) > 0:\n                    for dontCarePol in gtDontCarePolsNum:\n                        dontCarePol = gtPols[dontCarePol]\n                        intersected_area = get_intersection(dontCarePol, detPol)\n                        pdDimensions = detPol.area()\n                        precision = 0 if pdDimensions == 0 else intersected_area / pdDimensions\n                        if (precision > evaluationParams[\'AREA_PRECISION_CONSTRAINT\']):\n                            detDontCarePolsNum.append(len(detPols) - 1)\n                            break\n\n            evaluationLog += ""DET polygons: "" + str(len(detPols)) + (\n                "" ("" + str(len(detDontCarePolsNum)) + "" don\'t care)\\n"" if len(detDontCarePolsNum) > 0 else ""\\n"")\n\n            if len(gtPols) > 0 and len(detPols) > 0:\n                # Calculate IoU and precision matrixs\n                outputShape = [len(gtPols), len(detPols)]\n                iouMat = np.empty(outputShape)\n                gtRectMat = np.zeros(len(gtPols), np.int8)\n                detRectMat = np.zeros(len(detPols), np.int8)\n                for gtNum in range(len(gtPols)):\n                    for detNum in range(len(detPols)):\n                        pG = gtPols[gtNum]\n                        pD = detPols[detNum]\n                        iouMat[gtNum, detNum] = get_intersection_over_union(pD, pG)\n\n                for gtNum in range(len(gtPols)):\n                    for detNum in range(len(detPols)):\n                        if gtRectMat[gtNum] == 0 and detRectMat[\n                            detNum] == 0 and gtNum not in gtDontCarePolsNum and detNum not in detDontCarePolsNum:\n                            if iouMat[gtNum, detNum] > evaluationParams[\'IOU_CONSTRAINT\']:\n                                gtRectMat[gtNum] = 1\n                                detRectMat[detNum] = 1\n                                detMatched += 1\n                                pairs.append({\'gt\': gtNum, \'det\': detNum})\n                                detMatchedNums.append(detNum)\n                                evaluationLog += ""Match GT #"" + str(gtNum) + "" with Det #"" + str(detNum) + ""\\n""\n\n            if evaluationParams[\'CONFIDENCES\']:\n                for detNum in range(len(detPols)):\n                    if detNum not in detDontCarePolsNum:\n                        # we exclude the don\'t care detections\n                        match = detNum in detMatchedNums\n\n                        arrSampleConfidences.append(confidencesList[detNum])\n                        arrSampleMatch.append(match)\n\n                        arrGlobalConfidences.append(confidencesList[detNum]);\n                        arrGlobalMatches.append(match);\n\n        numGtCare = (len(gtPols) - len(gtDontCarePolsNum))\n        numDetCare = (len(detPols) - len(detDontCarePolsNum))\n        if numGtCare == 0:\n            recall = float(1)\n            precision = float(0) if numDetCare > 0 else float(1)\n            sampleAP = precision\n        else:\n            recall = float(detMatched) / numGtCare\n            precision = 0 if numDetCare == 0 else float(detMatched) / numDetCare\n            if evaluationParams[\'CONFIDENCES\'] and evaluationParams[\'PER_SAMPLE_RESULTS\']:\n                sampleAP = compute_ap(arrSampleConfidences, arrSampleMatch, numGtCare)\n\n        hmean = 0 if (precision + recall) == 0 else 2.0 * precision * recall / (precision + recall)\n\n        matchedSum += detMatched\n        numGlobalCareGt += numGtCare\n        numGlobalCareDet += numDetCare\n\n        if evaluationParams[\'PER_SAMPLE_RESULTS\']:\n            perSampleMetrics[resFile] = {\n                \'precision\': precision,\n                \'recall\': recall,\n                \'hmean\': hmean,\n                \'pairs\': pairs,\n                \'AP\': sampleAP,\n                \'iouMat\': [] if len(detPols) > 100 else iouMat.tolist(),\n                \'gtPolPoints\': gtPolPoints,\n                \'detPolPoints\': detPolPoints,\n                \'gtDontCare\': gtDontCarePolsNum,\n                \'detDontCare\': detDontCarePolsNum,\n                \'evaluationParams\': evaluationParams,\n                \'evaluationLog\': evaluationLog\n            }\n\n    # Compute MAP and MAR\n    AP = 0\n    if evaluationParams[\'CONFIDENCES\']:\n        AP = compute_ap(arrGlobalConfidences, arrGlobalMatches, numGlobalCareGt)\n\n    methodRecall = 0 if numGlobalCareGt == 0 else float(matchedSum) / numGlobalCareGt\n    methodPrecision = 0 if numGlobalCareDet == 0 else float(matchedSum) / numGlobalCareDet\n    methodHmean = 0 if methodRecall + methodPrecision == 0 else 2 * methodRecall * methodPrecision / (\n            methodRecall + methodPrecision)\n\n    methodMetrics = {\'precision\': methodPrecision, \'recall\': methodRecall, \'hmean\': methodHmean, \'AP\': AP}\n\n    resDict = {\'calculated\': True, \'Message\': \'\', \'method\': methodMetrics, \'per_sample\': perSampleMetrics}\n\n    return resDict;\n\n\ndef cal_recall_precison_f1(gt_path, result_path, show_result=False):\n    p = {\'g\': gt_path, \'s\': result_path}\n    result = rrc_evaluation_funcs.main_evaluation(p, default_evaluation_params, validate_data, evaluate_method,\n                                                  show_result)\n    return result[\'method\']'"
utils/ocr_metric/__init__.py,0,"b""# -*- coding: utf-8 -*-\n# @Time    : 2019/12/5 15:36\n# @Author  : zhoujun\nfrom .icdar2015 import QuadMetric\n\n\ndef get_metric(config):\n    try:\n        if 'args' not in config:\n            args = {}\n        else:\n            args = config['args']\n        if isinstance(args, dict):\n            cls = eval(config['type'])(**args)\n        else:\n            cls = eval(config['type'])(args)\n        return cls\n    except:\n        return None"""
models/backbone/resnest/__init__.py,0,b'from .resnest import *'
models/backbone/resnest/ablation.py,7,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Hang Zhang\n## Email: zhanghang0704@gmail.com\n## Copyright (c) 2020\n##\n## LICENSE file in the root directory of this source tree \n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n""""""ResNeSt ablation study models""""""\n\nimport torch\nfrom .resnet import ResNet, Bottleneck\n\n__all__ = [\'resnest50_fast_1s1x64d\', \'resnest50_fast_2s1x64d\', \'resnest50_fast_4s1x64d\',\n           \'resnest50_fast_1s2x40d\', \'resnest50_fast_2s2x40d\', \'resnest50_fast_4s2x40d\',\n           \'resnest50_fast_1s4x24d\']\n\n_url_format = \'https://hangzh.s3.amazonaws.com/encoding/models/{}-{}.pth\'\n\n_model_sha256 = {name: checksum for checksum, name in [\n    (\'d8fbf808\', \'resnest50_fast_1s1x64d\'),\n    (\'44938639\', \'resnest50_fast_2s1x64d\'),\n    (\'f74f3fc3\', \'resnest50_fast_4s1x64d\'),\n    (\'32830b84\', \'resnest50_fast_1s2x40d\'),\n    (\'9d126481\', \'resnest50_fast_2s2x40d\'),\n    (\'41d14ed0\', \'resnest50_fast_4s2x40d\'),\n    (\'d4a4f76f\', \'resnest50_fast_1s4x24d\'),\n    ]}\n\ndef short_hash(name):\n    if name not in _model_sha256:\n        raise ValueError(\'Pretrained model for {name} is not available.\'.format(name=name))\n    return _model_sha256[name][:8]\n\nresnest_model_urls = {name: _url_format.format(name, short_hash(name)) for\n    name in _model_sha256.keys()\n}\n\ndef resnest50_fast_1s1x64d(pretrained=False, root=\'~/.encoding/models\', **kwargs):\n    model = ResNet(Bottleneck, [3, 4, 6, 3],\n                   radix=1, groups=1, bottleneck_width=64,\n                   deep_stem=True, stem_width=32, avg_down=True,\n                   avd=True, avd_first=True, **kwargs)\n    if pretrained:\n        model.load_state_dict(torch.hub.load_state_dict_from_url(\n            resnest_model_urls[\'resnest50_fast_1s1x64d\'], progress=True, check_hash=True))\n    return model\n\ndef resnest50_fast_2s1x64d(pretrained=False, root=\'~/.encoding/models\', **kwargs):\n    model = ResNet(Bottleneck, [3, 4, 6, 3],\n                   radix=2, groups=1, bottleneck_width=64,\n                   deep_stem=True, stem_width=32, avg_down=True,\n                   avd=True, avd_first=True, **kwargs)\n    if pretrained:\n        model.load_state_dict(torch.hub.load_state_dict_from_url(\n            resnest_model_urls[\'resnest50_fast_2s1x64d\'], progress=True, check_hash=True))\n    return model\n\ndef resnest50_fast_4s1x64d(pretrained=False, root=\'~/.encoding/models\', **kwargs):\n    model = ResNet(Bottleneck, [3, 4, 6, 3],\n                   radix=4, groups=1, bottleneck_width=64,\n                   deep_stem=True, stem_width=32, avg_down=True,\n                   avd=True, avd_first=True, **kwargs)\n    if pretrained:\n        model.load_state_dict(torch.hub.load_state_dict_from_url(\n            resnest_model_urls[\'resnest50_fast_4s1x64d\'], progress=True, check_hash=True))\n    return model\n\ndef resnest50_fast_1s2x40d(pretrained=False, root=\'~/.encoding/models\', **kwargs):\n    model = ResNet(Bottleneck, [3, 4, 6, 3],\n                   radix=1, groups=2, bottleneck_width=40,\n                   deep_stem=True, stem_width=32, avg_down=True,\n                   avd=True, avd_first=True, **kwargs)\n    if pretrained:\n        model.load_state_dict(torch.hub.load_state_dict_from_url(\n            resnest_model_urls[\'resnest50_fast_1s2x40d\'], progress=True, check_hash=True))\n    return model\n\ndef resnest50_fast_2s2x40d(pretrained=False, root=\'~/.encoding/models\', **kwargs):\n    model = ResNet(Bottleneck, [3, 4, 6, 3],\n                   radix=2, groups=2, bottleneck_width=40,\n                   deep_stem=True, stem_width=32, avg_down=True,\n                   avd=True, avd_first=True, **kwargs)\n    if pretrained:\n        model.load_state_dict(torch.hub.load_state_dict_from_url(\n            resnest_model_urls[\'resnest50_fast_2s2x40d\'], progress=True, check_hash=True))\n    return model\n\ndef resnest50_fast_4s2x40d(pretrained=False, root=\'~/.encoding/models\', **kwargs):\n    model = ResNet(Bottleneck, [3, 4, 6, 3],\n                   radix=4, groups=2, bottleneck_width=40,\n                   deep_stem=True, stem_width=32, avg_down=True,\n                   avd=True, avd_first=True, **kwargs)\n    if pretrained:\n        model.load_state_dict(torch.hub.load_state_dict_from_url(\n            resnest_model_urls[\'resnest50_fast_4s2x40d\'], progress=True, check_hash=True))\n    return model\n\ndef resnest50_fast_1s4x24d(pretrained=False, root=\'~/.encoding/models\', **kwargs):\n    model = ResNet(Bottleneck, [3, 4, 6, 3],\n                   radix=1, groups=4, bottleneck_width=24,\n                   deep_stem=True, stem_width=32, avg_down=True,\n                   avd=True, avd_first=True, **kwargs)\n    if pretrained:\n        model.load_state_dict(torch.hub.load_state_dict_from_url(\n            resnest_model_urls[\'resnest50_fast_1s4x24d\'], progress=True, check_hash=True))\n    return model\n'"
models/backbone/resnest/resnest.py,5,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Hang Zhang\n## Email: zhanghang0704@gmail.com\n## Copyright (c) 2020\n##\n## LICENSE file in the root directory of this source tree \n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n""""""ResNeSt models""""""\n\nimport torch\nfrom models.backbone.resnest.resnet import ResNet, Bottleneck\n\n__all__ = [\'resnest50\', \'resnest101\', \'resnest200\', \'resnest269\']\n\n_url_format = \'https://hangzh.s3.amazonaws.com/encoding/models/{}-{}.pth\'\n\n_model_sha256 = {name: checksum for checksum, name in [\n    (\'528c19ca\', \'resnest50\'),\n    (\'22405ba7\', \'resnest101\'),\n    (\'75117900\', \'resnest200\'),\n    (\'0cc87c48\', \'resnest269\'),\n    ]}\n\ndef short_hash(name):\n    if name not in _model_sha256:\n        raise ValueError(\'Pretrained model for {name} is not available.\'.format(name=name))\n    return _model_sha256[name][:8]\n\nresnest_model_urls = {name: _url_format.format(name, short_hash(name)) for\n    name in _model_sha256.keys()\n}\n\ndef resnest50(pretrained=False, root=\'~/.encoding/models\', **kwargs):\n    model = ResNet(Bottleneck, [3, 4, 6, 3],\n                   radix=2, groups=1, bottleneck_width=64,\n                   deep_stem=True, stem_width=32, avg_down=True,\n                   avd=True, avd_first=False, **kwargs)\n    if pretrained:\n        assert kwargs[\'in_channels\'] == 3, \'in_channels must be 3 whem pretrained is True\'\n        model.load_state_dict(torch.hub.load_state_dict_from_url(\n            resnest_model_urls[\'resnest50\'], progress=True, check_hash=True))\n    return model\n\ndef resnest101(pretrained=False, root=\'~/.encoding/models\', **kwargs):\n    model = ResNet(Bottleneck, [3, 4, 23, 3],\n                   radix=2, groups=1, bottleneck_width=64,\n                   deep_stem=True, stem_width=64, avg_down=True,\n                   avd=True, avd_first=False, **kwargs)\n    if pretrained:\n        assert kwargs[\'in_channels\'] == 3, \'in_channels must be 3 whem pretrained is True\'\n        model.load_state_dict(torch.hub.load_state_dict_from_url(\n            resnest_model_urls[\'resnest101\'], progress=True, check_hash=True))\n    return model\n\ndef resnest200(pretrained=False, root=\'~/.encoding/models\', **kwargs):\n    model = ResNet(Bottleneck, [3, 24, 36, 3],\n                   radix=2, groups=1, bottleneck_width=64,\n                   deep_stem=True, stem_width=64, avg_down=True,\n                   avd=True, avd_first=False, **kwargs)\n    if pretrained:\n        assert kwargs[\'in_channels\'] == 3, \'in_channels must be 3 whem pretrained is True\'\n        model.load_state_dict(torch.hub.load_state_dict_from_url(\n            resnest_model_urls[\'resnest200\'], progress=True, check_hash=True))\n    return model\n\ndef resnest269(pretrained=False, root=\'~/.encoding/models\', **kwargs):\n    model = ResNet(Bottleneck, [3, 30, 48, 8],\n                   radix=2, groups=1, bottleneck_width=64,\n                   deep_stem=True, stem_width=64, avg_down=True,\n                   avd=True, avd_first=False, **kwargs)\n    if pretrained:\n        assert kwargs[\'in_channels\'] == 3, \'in_channels must be 3 whem pretrained is True\'\n        model.load_state_dict(torch.hub.load_state_dict_from_url(\n            resnest_model_urls[\'resnest269\'], progress=True, check_hash=True))\n    return model\n\nif __name__ == \'__main__\':\n    x = torch.zeros(2,3,640,640)\n    net = resnest269(pretrained=False)\n    y = net(x)\n    for u in y:\n        print(u.shape)\n    print(net.out_channels)'"
models/backbone/resnest/resnet.py,2,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Hang Zhang\n## Email: zhanghang0704@gmail.com\n## Copyright (c) 2020\n##\n## LICENSE file in the root directory of this source tree \n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n""""""ResNet variants""""""\nimport math\nimport torch\nimport torch.nn as nn\n\nfrom .splat import SplAtConv2d\n\n__all__ = [\'ResNet\', \'Bottleneck\']\n\nclass DropBlock2D(object):\n    def __init__(self, *args, **kwargs):\n        raise NotImplementedError\n\nclass GlobalAvgPool2d(nn.Module):\n    def __init__(self):\n        """"""Global average pooling over the input\'s spatial dimensions""""""\n        super(GlobalAvgPool2d, self).__init__()\n\n    def forward(self, inputs):\n        return nn.functional.adaptive_avg_pool2d(inputs, 1).view(inputs.size(0), -1)\n\nclass Bottleneck(nn.Module):\n    """"""ResNet Bottleneck\n    """"""\n    # pylint: disable=unused-argument\n    expansion = 4\n    def __init__(self, inplanes, planes, stride=1, downsample=None,\n                 radix=1, cardinality=1, bottleneck_width=64,\n                 avd=False, avd_first=False, dilation=1, is_first=False,\n                 rectified_conv=False, rectify_avg=False,\n                 norm_layer=None, dropblock_prob=0.0, last_gamma=False):\n        super(Bottleneck, self).__init__()\n        group_width = int(planes * (bottleneck_width / 64.)) * cardinality\n        self.conv1 = nn.Conv2d(inplanes, group_width, kernel_size=1, bias=False)\n        self.bn1 = norm_layer(group_width)\n        self.dropblock_prob = dropblock_prob\n        self.radix = radix\n        self.avd = avd and (stride > 1 or is_first)\n        self.avd_first = avd_first\n\n        if self.avd:\n            self.avd_layer = nn.AvgPool2d(3, stride, padding=1)\n            stride = 1\n\n        if dropblock_prob > 0.0:\n            self.dropblock1 = DropBlock2D(dropblock_prob, 3)\n            if radix == 1:\n                self.dropblock2 = DropBlock2D(dropblock_prob, 3)\n            self.dropblock3 = DropBlock2D(dropblock_prob, 3)\n\n        if radix >= 1:\n            self.conv2 = SplAtConv2d(\n                group_width, group_width, kernel_size=3,\n                stride=stride, padding=dilation,\n                dilation=dilation, groups=cardinality, bias=False,\n                radix=radix, rectify=rectified_conv,\n                rectify_avg=rectify_avg,\n                norm_layer=norm_layer,\n                dropblock_prob=dropblock_prob)\n        elif rectified_conv:\n            from rfconv import RFConv2d\n            self.conv2 = RFConv2d(\n                group_width, group_width, kernel_size=3, stride=stride,\n                padding=dilation, dilation=dilation,\n                groups=cardinality, bias=False,\n                average_mode=rectify_avg)\n            self.bn2 = norm_layer(group_width)\n        else:\n            self.conv2 = nn.Conv2d(\n                group_width, group_width, kernel_size=3, stride=stride,\n                padding=dilation, dilation=dilation,\n                groups=cardinality, bias=False)\n            self.bn2 = norm_layer(group_width)\n\n        self.conv3 = nn.Conv2d(\n            group_width, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = norm_layer(planes*4)\n\n        if last_gamma:\n            from torch.nn.init import zeros_\n            zeros_(self.bn3.weight)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.dilation = dilation\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        if self.dropblock_prob > 0.0:\n            out = self.dropblock1(out)\n        out = self.relu(out)\n\n        if self.avd and self.avd_first:\n            out = self.avd_layer(out)\n\n        out = self.conv2(out)\n        if self.radix == 0:\n            out = self.bn2(out)\n            if self.dropblock_prob > 0.0:\n                out = self.dropblock2(out)\n            out = self.relu(out)\n\n        if self.avd and not self.avd_first:\n            out = self.avd_layer(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n        if self.dropblock_prob > 0.0:\n            out = self.dropblock3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass ResNet(nn.Module):\n    """"""ResNet Variants\n\n    Parameters\n    ----------\n    block : Block\n        Class for the residual block. Options are BasicBlockV1, BottleneckV1.\n    layers : list of int\n        Numbers of layers in each block\n    classes : int, default 1000\n        Number of classification classes.\n    dilated : bool, default False\n        Applying dilation strategy to pretrained ResNet yielding a stride-8 model,\n        typically used in Semantic Segmentation.\n    norm_layer : object\n        Normalization layer used in backbone network (default: :class:`mxnet.gluon.nn.BatchNorm`;\n        for Synchronized Cross-GPU BachNormalization).\n\n    Reference:\n\n        - He, Kaiming, et al. ""Deep residual learning for image recognition."" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n\n        - Yu, Fisher, and Vladlen Koltun. ""Multi-scale context aggregation by dilated convolutions.""\n    """"""\n    # pylint: disable=unused-variable\n    def __init__(self, block, layers, radix=1, groups=1, bottleneck_width=64,\n                 num_classes=1000, dilated=False, dilation=1,\n                 deep_stem=False, stem_width=64, avg_down=False,\n                 rectified_conv=False, rectify_avg=False,\n                 avd=False, avd_first=False,\n                 final_drop=0.0, dropblock_prob=0,\n                 last_gamma=False, norm_layer=nn.BatchNorm2d,in_channels=3):\n        self.cardinality = groups\n        self.bottleneck_width = bottleneck_width\n        # ResNet-D params\n        self.inplanes = stem_width*2 if deep_stem else 64\n        self.avg_down = avg_down\n        self.last_gamma = last_gamma\n        # ResNeSt params\n        self.radix = radix\n        self.avd = avd\n        self.avd_first = avd_first\n\n        super(ResNet, self).__init__()\n        self.out_channels = []\n        self.rectified_conv = rectified_conv\n        self.rectify_avg = rectify_avg\n        if rectified_conv:\n            from rfconv import RFConv2d\n            conv_layer = RFConv2d\n        else:\n            conv_layer = nn.Conv2d\n        conv_kwargs = {\'average_mode\': rectify_avg} if rectified_conv else {}\n        if deep_stem:\n            self.conv1 = nn.Sequential(\n                conv_layer(in_channels, stem_width, kernel_size=3, stride=2, padding=1, bias=False, **conv_kwargs),\n                norm_layer(stem_width),\n                nn.ReLU(inplace=True),\n                conv_layer(stem_width, stem_width, kernel_size=3, stride=1, padding=1, bias=False, **conv_kwargs),\n                norm_layer(stem_width),\n                nn.ReLU(inplace=True),\n                conv_layer(stem_width, stem_width*2, kernel_size=3, stride=1, padding=1, bias=False, **conv_kwargs),\n            )\n        else:\n            self.conv1 = conv_layer(in_channels, 64, kernel_size=7, stride=2, padding=3,\n                                   bias=False, **conv_kwargs)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0], norm_layer=norm_layer, is_first=False)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, norm_layer=norm_layer)\n        if dilated or dilation == 4:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=1,\n                                           dilation=2, norm_layer=norm_layer,\n                                           dropblock_prob=dropblock_prob)\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=1,\n                                           dilation=4, norm_layer=norm_layer,\n                                           dropblock_prob=dropblock_prob)\n        elif dilation==2:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n                                           dilation=1, norm_layer=norm_layer,\n                                           dropblock_prob=dropblock_prob)\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=1,\n                                           dilation=2, norm_layer=norm_layer,\n                                           dropblock_prob=dropblock_prob)\n        else:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n                                           norm_layer=norm_layer,\n                                           dropblock_prob=dropblock_prob)\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n                                           norm_layer=norm_layer,\n                                           dropblock_prob=dropblock_prob)\n        self.avgpool = GlobalAvgPool2d()\n        self.drop = nn.Dropout(final_drop) if final_drop > 0.0 else None\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, norm_layer):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, norm_layer=None,\n                    dropblock_prob=0.0, is_first=True):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            down_layers = []\n            if self.avg_down:\n                if dilation == 1:\n                    down_layers.append(nn.AvgPool2d(kernel_size=stride, stride=stride,\n                                                    ceil_mode=True, count_include_pad=False))\n                else:\n                    down_layers.append(nn.AvgPool2d(kernel_size=1, stride=1,\n                                                    ceil_mode=True, count_include_pad=False))\n                down_layers.append(nn.Conv2d(self.inplanes, planes * block.expansion,\n                                             kernel_size=1, stride=1, bias=False))\n            else:\n                down_layers.append(nn.Conv2d(self.inplanes, planes * block.expansion,\n                                             kernel_size=1, stride=stride, bias=False))\n            down_layers.append(norm_layer(planes * block.expansion))\n            downsample = nn.Sequential(*down_layers)\n\n        layers = []\n        if dilation == 1 or dilation == 2:\n            layers.append(block(self.inplanes, planes, stride, downsample=downsample,\n                                radix=self.radix, cardinality=self.cardinality,\n                                bottleneck_width=self.bottleneck_width,\n                                avd=self.avd, avd_first=self.avd_first,\n                                dilation=1, is_first=is_first, rectified_conv=self.rectified_conv,\n                                rectify_avg=self.rectify_avg,\n                                norm_layer=norm_layer, dropblock_prob=dropblock_prob,\n                                last_gamma=self.last_gamma))\n        elif dilation == 4:\n            layers.append(block(self.inplanes, planes, stride, downsample=downsample,\n                                radix=self.radix, cardinality=self.cardinality,\n                                bottleneck_width=self.bottleneck_width,\n                                avd=self.avd, avd_first=self.avd_first,\n                                dilation=2, is_first=is_first, rectified_conv=self.rectified_conv,\n                                rectify_avg=self.rectify_avg,\n                                norm_layer=norm_layer, dropblock_prob=dropblock_prob,\n                                last_gamma=self.last_gamma))\n        else:\n            raise RuntimeError(""=> unknown dilation size: {}"".format(dilation))\n\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes,\n                                radix=self.radix, cardinality=self.cardinality,\n                                bottleneck_width=self.bottleneck_width,\n                                avd=self.avd, avd_first=self.avd_first,\n                                dilation=dilation, rectified_conv=self.rectified_conv,\n                                rectify_avg=self.rectify_avg,\n                                norm_layer=norm_layer, dropblock_prob=dropblock_prob,\n                                last_gamma=self.last_gamma))\n        self.out_channels.append(planes*block.expansion)\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x2 = self.layer1(x)\n        x3 = self.layer2(x2)\n        x4 = self.layer3(x3)\n        x5 = self.layer4(x4)\n\n        return x2, x3, x4, x5\n'"
models/backbone/resnest/splat.py,10,"b'""""""Split-Attention""""""\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.nn import Conv2d, Module, Linear, BatchNorm2d, ReLU\nfrom torch.nn.modules.utils import _pair\n\n__all__ = [\'SplAtConv2d\']\n\nclass SplAtConv2d(Module):\n    """"""Split-Attention Conv2d\n    """"""\n    def __init__(self, in_channels, channels, kernel_size, stride=(1, 1), padding=(0, 0),\n                 dilation=(1, 1), groups=1, bias=True,\n                 radix=2, reduction_factor=4,\n                 rectify=False, rectify_avg=False, norm_layer=None,\n                 dropblock_prob=0.0, **kwargs):\n        super(SplAtConv2d, self).__init__()\n        padding = _pair(padding)\n        self.rectify = rectify and (padding[0] > 0 or padding[1] > 0)\n        self.rectify_avg = rectify_avg\n        inter_channels = max(in_channels*radix//reduction_factor, 32)\n        self.radix = radix\n        self.cardinality = groups\n        self.channels = channels\n        self.dropblock_prob = dropblock_prob\n        if self.rectify:\n            from rfconv import RFConv2d\n            self.conv = RFConv2d(in_channels, channels*radix, kernel_size, stride, padding, dilation,\n                                 groups=groups*radix, bias=bias, average_mode=rectify_avg, **kwargs)\n        else:\n            self.conv = Conv2d(in_channels, channels*radix, kernel_size, stride, padding, dilation,\n                               groups=groups*radix, bias=bias, **kwargs)\n        self.use_bn = norm_layer is not None\n        if self.use_bn:\n            self.bn0 = norm_layer(channels*radix)\n        self.relu = ReLU(inplace=True)\n        self.fc1 = Conv2d(channels, inter_channels, 1, groups=self.cardinality)\n        if self.use_bn:\n            self.bn1 = norm_layer(inter_channels)\n        self.fc2 = Conv2d(inter_channels, channels*radix, 1, groups=self.cardinality)\n        if dropblock_prob > 0.0:\n            self.dropblock = DropBlock2D(dropblock_prob, 3)\n        self.rsoftmax = rSoftMax(radix, groups)\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.use_bn:\n            x = self.bn0(x)\n        if self.dropblock_prob > 0.0:\n            x = self.dropblock(x)\n        x = self.relu(x)\n\n        batch, rchannel = x.shape[:2]\n        if self.radix > 1:\n            if torch.__version__ < \'1.5\':\n                splited = torch.split(x, int(rchannel//self.radix), dim=1)\n            else:\n                splited = torch.split(x, rchannel//self.radix, dim=1)\n            gap = sum(splited)\n        else:\n            gap = x\n        gap = F.adaptive_avg_pool2d(gap, 1)\n        gap = self.fc1(gap)\n\n        if self.use_bn:\n            gap = self.bn1(gap)\n        gap = self.relu(gap)\n\n        atten = self.fc2(gap)\n        atten = self.rsoftmax(atten).view(batch, -1, 1, 1)\n\n        if self.radix > 1:\n            if torch.__version__ < \'1.5\':\n                attens = torch.split(atten, int(rchannel//self.radix), dim=1)\n            else:\n                attens = torch.split(atten, rchannel//self.radix, dim=1)\n            out = sum([att*split for (att, split) in zip(attens, splited)])\n        else:\n            out = atten * x\n        return out.contiguous()\n\nclass rSoftMax(nn.Module):\n    def __init__(self, radix, cardinality):\n        super().__init__()\n        self.radix = radix\n        self.cardinality = cardinality\n\n    def forward(self, x):\n        batch = x.size(0)\n        if self.radix > 1:\n            x = x.view(batch, self.cardinality, self.radix, -1).transpose(1, 2)\n            x = F.softmax(x, dim=1)\n            x = x.reshape(batch, -1)\n        else:\n            x = torch.sigmoid(x)\n        return x\n\n'"
utils/ocr_metric/icdar2015/__init__.py,0,b'# -*- coding: utf-8 -*-\n# @Time    : 2019/12/5 15:36\n# @Author  : zhoujun\n\nfrom .quad_metric import QuadMetric'
utils/ocr_metric/icdar2015/quad_metric.py,0,"b'import numpy as np\n\nfrom .detection.iou import DetectionIoUEvaluator\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n        return self\n\n\nclass QuadMetric():\n    def __init__(self, is_output_polygon=False):\n        self.is_output_polygon = is_output_polygon\n        self.evaluator = DetectionIoUEvaluator(is_output_polygon=is_output_polygon)\n\n    def measure(self, batch, output, box_thresh=0.6):\n        \'\'\'\n        batch: (image, polygons, ignore_tags\n        batch: a dict produced by dataloaders.\n            image: tensor of shape (N, C, H, W).\n            polygons: tensor of shape (N, K, 4, 2), the polygons of objective regions.\n            ignore_tags: tensor of shape (N, K), indicates whether a region is ignorable or not.\n            shape: the original shape of images.\n            filename: the original filenames of images.\n        output: (polygons, ...)\n        \'\'\'\n        results = []\n        gt_polyons_batch = batch[\'text_polys\']\n        ignore_tags_batch = batch[\'ignore_tags\']\n        pred_polygons_batch = np.array(output[0])\n        pred_scores_batch = np.array(output[1])\n        for polygons, pred_polygons, pred_scores, ignore_tags in zip(gt_polyons_batch, pred_polygons_batch, pred_scores_batch, ignore_tags_batch):\n            gt = [dict(points=np.int64(polygons[i]), ignore=ignore_tags[i]) for i in range(len(polygons))]\n            if self.is_output_polygon:\n                pred = [dict(points=pred_polygons[i]) for i in range(len(pred_polygons))]\n            else:\n                pred = []\n                # print(pred_polygons.shape)\n                for i in range(pred_polygons.shape[0]):\n                    if pred_scores[i] >= box_thresh:\n                        # print(pred_polygons[i,:,:].tolist())\n                        pred.append(dict(points=pred_polygons[i, :, :].astype(np.int)))\n                # pred = [dict(points=pred_polygons[i,:,:].tolist()) if pred_scores[i] >= box_thresh for i in range(pred_polygons.shape[0])]\n            results.append(self.evaluator.evaluate_image(gt, pred))\n        return results\n\n    def validate_measure(self, batch, output, box_thresh=0.6):\n        return self.measure(batch, output, box_thresh)\n\n    def evaluate_measure(self, batch, output):\n        return self.measure(batch, output), np.linspace(0, batch[\'image\'].shape[0]).tolist()\n\n    def gather_measure(self, raw_metrics):\n        raw_metrics = [image_metrics\n                       for batch_metrics in raw_metrics\n                       for image_metrics in batch_metrics]\n\n        result = self.evaluator.combine_results(raw_metrics)\n\n        precision = AverageMeter()\n        recall = AverageMeter()\n        fmeasure = AverageMeter()\n\n        precision.update(result[\'precision\'], n=len(raw_metrics))\n        recall.update(result[\'recall\'], n=len(raw_metrics))\n        fmeasure_score = 2 * precision.val * recall.val / (precision.val + recall.val + 1e-8)\n        fmeasure.update(fmeasure_score)\n\n        return {\n            \'precision\': precision,\n            \'recall\': recall,\n            \'fmeasure\': fmeasure\n        }\n'"
utils/ocr_metric/icdar2015/detection/__init__.py,0,b''
utils/ocr_metric/icdar2015/detection/deteval.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport math\nfrom collections import namedtuple\nimport numpy as np\nfrom shapely.geometry import Polygon\n\n\nclass DetectionDetEvalEvaluator(object):\n    def __init__(\n        self,\n        area_recall_constraint=0.8, area_precision_constraint=0.4,\n        ev_param_ind_center_diff_thr=1,\n        mtype_oo_o=1.0, mtype_om_o=0.8, mtype_om_m=1.0\n    ):\n\n\n        self.area_recall_constraint = area_recall_constraint\n        self.area_precision_constraint = area_precision_constraint\n        self.ev_param_ind_center_diff_thr = ev_param_ind_center_diff_thr\n        self.mtype_oo_o = mtype_oo_o\n        self.mtype_om_o = mtype_om_o\n        self.mtype_om_m = mtype_om_m\n\n    def evaluate_image(self, gt, pred):\n\n        def get_union(pD,pG):\n            return Polygon(pD).union(Polygon(pG)).area\n\n        def get_intersection_over_union(pD,pG):\n            return get_intersection(pD, pG) / get_union(pD, pG)\n\n        def get_intersection(pD,pG):\n            return Polygon(pD).intersection(Polygon(pG)).area\n\n        def one_to_one_match(row, col):\n            cont = 0\n            for j in range(len(recallMat[0])):    \n                if recallMat[row,j] >= self.area_recall_constraint and precisionMat[row,j] >= self.area_precision_constraint:\n                    cont = cont +1\n            if (cont != 1):\n                return False\n            cont = 0\n            for i in range(len(recallMat)):    \n                if recallMat[i,col] >= self.area_recall_constraint and precisionMat[i,col] >= self.area_precision_constraint:\n                    cont = cont +1\n            if (cont != 1):\n                return False\n            \n            if recallMat[row,col] >= self.area_recall_constraint and precisionMat[row,col] >= self.area_precision_constraint:\n                return True\n            return False\n        \n        def num_overlaps_gt(gtNum):\n            cont = 0\n            for detNum in range(len(detRects)):\n                if detNum not in detDontCareRectsNum:\n                    if recallMat[gtNum,detNum] > 0 :\n                        cont = cont +1\n            return cont\n\n        def num_overlaps_det(detNum):\n            cont = 0\n            for gtNum in range(len(recallMat)):    \n                if gtNum not in gtDontCareRectsNum:\n                    if recallMat[gtNum,detNum] > 0 :\n                        cont = cont +1\n            return cont\n        \n        def is_single_overlap(row, col):\n            if num_overlaps_gt(row)==1 and num_overlaps_det(col)==1:\n                return True\n            else:\n                return False\n        \n        def one_to_many_match(gtNum):\n            many_sum = 0\n            detRects = []\n            for detNum in range(len(recallMat[0])):    \n                if gtRectMat[gtNum] == 0 and detRectMat[detNum] == 0 and detNum not in detDontCareRectsNum:\n                    if precisionMat[gtNum,detNum] >= self.area_precision_constraint:\n                        many_sum += recallMat[gtNum,detNum]\n                        detRects.append(detNum)\n            if round(many_sum,4) >= self.area_recall_constraint:\n                return True,detRects\n            else:\n                return False,[]         \n        \n        def many_to_one_match(detNum):\n            many_sum = 0\n            gtRects = []\n            for gtNum in range(len(recallMat)):    \n                if gtRectMat[gtNum] == 0 and detRectMat[detNum] == 0 and gtNum not in gtDontCareRectsNum:\n                    if recallMat[gtNum,detNum] >= self.area_recall_constraint:\n                        many_sum += precisionMat[gtNum,detNum]\n                        gtRects.append(gtNum)\n            if round(many_sum,4) >= self.area_precision_constraint:\n                return True,gtRects\n            else:\n                return False,[]\n        \n        def center_distance(r1, r2):\n            return ((np.mean(r1, axis=0) - np.mean(r2, axis=0)) ** 2).sum() ** 0.5\n        \n        def diag(r):\n            r = np.array(r)\n            return ((r[:, 0].max() - r[:, 0].min()) ** 2 + (r[:, 1].max() - r[:, 1].min()) ** 2) ** 0.5\n        \n        perSampleMetrics = {}\n        \n        recall = 0\n        precision = 0\n        hmean = 0        \n        recallAccum = 0.\n        precisionAccum = 0.\n        gtRects = []\n        detRects = []\n        gtPolPoints = []\n        detPolPoints = []\n        gtDontCareRectsNum = []#Array of Ground Truth Rectangles\' keys marked as don\'t Care\n        detDontCareRectsNum = []#Array of Detected Rectangles\' matched with a don\'t Care GT\n        pairs = []\n        evaluationLog = """"\n        \n        recallMat = np.empty([1,1])\n        precisionMat = np.empty([1,1])              \n        \n        for n in range(len(gt)):\n            points = gt[n][\'points\']\n            # transcription = gt[n][\'text\']\n            dontCare = gt[n][\'ignore\']\n\n            if not Polygon(points).is_valid or not Polygon(points).is_simple:\n                continue\n\n            gtRects.append(points)\n            gtPolPoints.append(points)\n            if dontCare:\n                gtDontCareRectsNum.append( len(gtRects)-1 )                 \n        \n        evaluationLog += ""GT rectangles: "" + str(len(gtRects)) + ("" ("" + str(len(gtDontCareRectsNum)) + "" don\'t care)\\n"" if len(gtDontCareRectsNum)>0 else ""\\n"")\n        \n        for n in range(len(pred)):\n            points = pred[n][\'points\']\n\n            if not Polygon(points).is_valid or not Polygon(points).is_simple:\n                continue\n\n            detRect = points\n            detRects.append(detRect)\n            detPolPoints.append(points)\n            if len(gtDontCareRectsNum)>0 :\n                for dontCareRectNum in gtDontCareRectsNum:\n                    dontCareRect = gtRects[dontCareRectNum]\n                    intersected_area = get_intersection(dontCareRect,detRect)\n                    rdDimensions = Polygon(detRect).area\n                    if (rdDimensions==0) :\n                        precision = 0\n                    else:\n                        precision= intersected_area / rdDimensions\n                    if (precision > self.area_precision_constraint):\n                        detDontCareRectsNum.append( len(detRects)-1 )\n                        break\n\n        evaluationLog += ""DET rectangles: "" + str(len(detRects)) + ("" ("" + str(len(detDontCareRectsNum)) + "" don\'t care)\\n"" if len(detDontCareRectsNum)>0 else ""\\n"")\n\n        if len(gtRects)==0:\n            recall = 1\n            precision = 0 if len(detRects)>0 else 1\n\n        if len(detRects)>0:\n            #Calculate recall and precision matrixs\n            outputShape=[len(gtRects),len(detRects)]\n            recallMat = np.empty(outputShape)\n            precisionMat = np.empty(outputShape)\n            gtRectMat = np.zeros(len(gtRects),np.int8)\n            detRectMat = np.zeros(len(detRects),np.int8)\n            for gtNum in range(len(gtRects)):\n                for detNum in range(len(detRects)):\n                    rG = gtRects[gtNum]\n                    rD = detRects[detNum]\n                    intersected_area = get_intersection(rG,rD)\n                    rgDimensions = Polygon(rG).area\n                    rdDimensions = Polygon(rD).area\n                    recallMat[gtNum,detNum] = 0 if rgDimensions==0 else  intersected_area / rgDimensions\n                    precisionMat[gtNum,detNum] = 0 if rdDimensions==0 else intersected_area / rdDimensions\n\n            # Find one-to-one matches\n            evaluationLog += ""Find one-to-one matches\\n""\n            for gtNum in range(len(gtRects)):\n                for detNum in range(len(detRects)):\n                    if gtRectMat[gtNum] == 0 and detRectMat[detNum] == 0 and gtNum not in gtDontCareRectsNum and detNum not in detDontCareRectsNum :\n                        match = one_to_one_match(gtNum, detNum)\n                        if match is True :\n                            #in deteval we have to make other validation before mark as one-to-one\n                            if is_single_overlap(gtNum, detNum) is True :\n                                rG = gtRects[gtNum]\n                                rD = detRects[detNum]\n                                normDist = center_distance(rG, rD);\n                                normDist /= diag(rG) + diag(rD);\n                                normDist *= 2.0;\n                                if normDist < self.ev_param_ind_center_diff_thr:\n                                    gtRectMat[gtNum] = 1\n                                    detRectMat[detNum] = 1\n                                    recallAccum += self.mtype_oo_o\n                                    precisionAccum += self.mtype_oo_o\n                                    pairs.append({\'gt\':gtNum,\'det\':detNum,\'type\':\'OO\'})\n                                    evaluationLog += ""Match GT #"" + str(gtNum) + "" with Det #"" + str(detNum) + ""\\n""\n                                else:\n                                    evaluationLog += ""Match Discarded GT #"" + str(gtNum) + "" with Det #"" + str(detNum) + "" normDist: "" + str(normDist) + "" \\n""\n                            else:\n                                evaluationLog += ""Match Discarded GT #"" + str(gtNum) + "" with Det #"" + str(detNum) + "" not single overlap\\n""\n            # Find one-to-many matches\n            evaluationLog += ""Find one-to-many matches\\n""\n            for gtNum in range(len(gtRects)):\n                if gtNum not in gtDontCareRectsNum:\n                    match,matchesDet = one_to_many_match(gtNum)\n                    if match is True :\n                        evaluationLog += ""num_overlaps_gt="" + str(num_overlaps_gt(gtNum))\n                        #in deteval we have to make other validation before mark as one-to-one\n                        if num_overlaps_gt(gtNum)>=2 :\n                            gtRectMat[gtNum] = 1\n                            recallAccum += (self.mtype_oo_o if len(matchesDet)==1 else self.mtype_om_o)\n                            precisionAccum += (self.mtype_oo_o if len(matchesDet)==1 else self.mtype_om_o*len(matchesDet))\n                            pairs.append({\'gt\':gtNum,\'det\':matchesDet,\'type\': \'OO\' if len(matchesDet)==1 else \'OM\'})\n                            for detNum in matchesDet :\n                                detRectMat[detNum] = 1\n                            evaluationLog += ""Match GT #"" + str(gtNum) + "" with Det #"" + str(matchesDet) + ""\\n""\n                        else:\n                            evaluationLog += ""Match Discarded GT #"" + str(gtNum) + "" with Det #"" + str(matchesDet) + "" not single overlap\\n""    \n\n            # Find many-to-one matches\n            evaluationLog += ""Find many-to-one matches\\n""\n            for detNum in range(len(detRects)):\n                if detNum not in detDontCareRectsNum:\n                    match,matchesGt = many_to_one_match(detNum)\n                    if match is True :\n                        #in deteval we have to make other validation before mark as one-to-one\n                        if num_overlaps_det(detNum)>=2 :                          \n                            detRectMat[detNum] = 1\n                            recallAccum += (self.mtype_oo_o if len(matchesGt)==1 else self.mtype_om_m*len(matchesGt))\n                            precisionAccum += (self.mtype_oo_o if len(matchesGt)==1 else self.mtype_om_m)\n                            pairs.append({\'gt\':matchesGt,\'det\':detNum,\'type\': \'OO\' if len(matchesGt)==1 else \'MO\'})\n                            for gtNum in matchesGt :\n                                gtRectMat[gtNum] = 1\n                            evaluationLog += ""Match GT #"" + str(matchesGt) + "" with Det #"" + str(detNum) + ""\\n""\n                        else:\n                            evaluationLog += ""Match Discarded GT #"" + str(matchesGt) + "" with Det #"" + str(detNum) + "" not single overlap\\n""                                    \n\n            numGtCare = (len(gtRects) - len(gtDontCareRectsNum))\n            if numGtCare == 0:\n                recall = float(1)\n                precision = float(0) if len(detRects)>0 else float(1)\n            else:\n                recall = float(recallAccum) / numGtCare\n                precision =  float(0) if (len(detRects) - len(detDontCareRectsNum))==0 else float(precisionAccum) / (len(detRects) - len(detDontCareRectsNum))\n            hmean = 0 if (precision + recall)==0 else 2.0 * precision * recall / (precision + recall)  \n\n        numGtCare = len(gtRects) - len(gtDontCareRectsNum)\n        numDetCare = len(detRects) - len(detDontCareRectsNum)\n\n        perSampleMetrics = {\n            \'precision\':precision,\n            \'recall\':recall,\n            \'hmean\':hmean,\n            \'pairs\':pairs,\n            \'recallMat\':[] if len(detRects)>100 else recallMat.tolist(),\n            \'precisionMat\':[] if len(detRects)>100 else precisionMat.tolist(),\n            \'gtPolPoints\':gtPolPoints,\n            \'detPolPoints\':detPolPoints,\n            \'gtCare\': numGtCare,\n            \'detCare\': numDetCare,\n            \'gtDontCare\':gtDontCareRectsNum,\n            \'detDontCare\':detDontCareRectsNum,\n            \'recallAccum\':recallAccum,\n            \'precisionAccum\':precisionAccum,\n            \'evaluationLog\': evaluationLog\n        }\n\n        return perSampleMetrics\n\n    def combine_results(self, results):\n        numGt = 0\n        numDet = 0\n        methodRecallSum = 0\n        methodPrecisionSum = 0\n\n        for result in results:\n            numGt += result[\'gtCare\']\n            numDet += result[\'detCare\']\n            methodRecallSum += result[\'recallAccum\']\n            methodPrecisionSum += result[\'precisionAccum\']\n\n        methodRecall = 0 if numGt==0 else methodRecallSum/numGt\n        methodPrecision = 0 if numDet==0 else methodPrecisionSum/numDet\n        methodHmean = 0 if methodRecall + methodPrecision==0 else 2* methodRecall * methodPrecision / (methodRecall + methodPrecision)\n        \n        methodMetrics = {\'precision\':methodPrecision, \'recall\':methodRecall,\'hmean\': methodHmean  }\n\n        return methodMetrics\n\n\nif __name__==\'__main__\':\n    evaluator = DetectionDetEvalEvaluator()\n    gts = [[{\n        \'points\': [(0, 0), (1, 0), (1, 1), (0, 1)],\n        \'text\': 1234,\n        \'ignore\': False,\n    }, {\n        \'points\': [(2, 2), (3, 2), (3, 3), (2, 3)],\n        \'text\': 5678,\n        \'ignore\': True,\n    }]]\n    preds = [[{\n        \'points\': [(0.1, 0.1), (1, 0), (1, 1), (0, 1)],\n        \'text\': 123,\n        \'ignore\': False,\n    }]]\n    results = []\n    for gt, pred in zip(gts, preds):\n        results.append(evaluator.evaluate_image(gt, pred))\n    metrics = evaluator.combine_results(results)\n    print(metrics)\n'"
utils/ocr_metric/icdar2015/detection/icdar2013.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport math\nfrom collections import namedtuple\nimport numpy as np\nfrom shapely.geometry import Polygon\n\n\nclass DetectionICDAR2013Evaluator(object):\n    def __init__(\n        self,\n        area_recall_constraint=0.8, area_precision_constraint=0.4,\n        ev_param_ind_center_diff_thr=1,\n        mtype_oo_o=1.0, mtype_om_o=0.8, mtype_om_m=1.0\n    ):\n\n\n        self.area_recall_constraint = area_recall_constraint\n        self.area_precision_constraint = area_precision_constraint\n        self.ev_param_ind_center_diff_thr = ev_param_ind_center_diff_thr\n        self.mtype_oo_o = mtype_oo_o\n        self.mtype_om_o = mtype_om_o\n        self.mtype_om_m = mtype_om_m\n\n    def evaluate_image(self, gt, pred):\n\n        def get_union(pD,pG):\n            return Polygon(pD).union(Polygon(pG)).area\n\n        def get_intersection_over_union(pD,pG):\n            return get_intersection(pD, pG) / get_union(pD, pG)\n\n        def get_intersection(pD,pG):\n            return Polygon(pD).intersection(Polygon(pG)).area\n\n        def one_to_one_match(row, col):\n            cont = 0\n            for j in range(len(recallMat[0])):    \n                if recallMat[row,j] >= self.area_recall_constraint and precisionMat[row,j] >= self.area_precision_constraint:\n                    cont = cont +1\n            if (cont != 1):\n                return False\n            cont = 0\n            for i in range(len(recallMat)):    \n                if recallMat[i,col] >= self.area_recall_constraint and precisionMat[i,col] >= self.area_precision_constraint:\n                    cont = cont +1\n            if (cont != 1):\n                return False\n            \n            if recallMat[row,col] >= self.area_recall_constraint and precisionMat[row,col] >= self.area_precision_constraint:\n                return True\n            return False\n        \n        def one_to_many_match(gtNum):\n            many_sum = 0\n            detRects = []\n            for detNum in range(len(recallMat[0])):    \n                if gtRectMat[gtNum] == 0 and detRectMat[detNum] == 0 and detNum not in detDontCareRectsNum:\n                    if precisionMat[gtNum,detNum] >= self.area_precision_constraint:\n                        many_sum += recallMat[gtNum,detNum]\n                        detRects.append(detNum)\n            if round(many_sum,4) >= self.area_recall_constraint:\n                return True,detRects\n            else:\n                return False,[]         \n        \n        def many_to_one_match(detNum):\n            many_sum = 0\n            gtRects = []\n            for gtNum in range(len(recallMat)):    \n                if gtRectMat[gtNum] == 0 and detRectMat[detNum] == 0 and gtNum not in gtDontCareRectsNum:\n                    if recallMat[gtNum,detNum] >= self.area_recall_constraint:\n                        many_sum += precisionMat[gtNum,detNum]\n                        gtRects.append(gtNum)\n            if round(many_sum,4) >= self.area_precision_constraint:\n                return True,gtRects\n            else:\n                return False,[]\n        \n        def center_distance(r1, r2):\n            return ((np.mean(r1, axis=0) - np.mean(r2, axis=0)) ** 2).sum() ** 0.5\n        \n        def diag(r):\n            r = np.array(r)\n            return ((r[:, 0].max() - r[:, 0].min()) ** 2 + (r[:, 1].max() - r[:, 1].min()) ** 2) ** 0.5\n        \n        perSampleMetrics = {}\n        \n        recall = 0\n        precision = 0\n        hmean = 0        \n        recallAccum = 0.\n        precisionAccum = 0.\n        gtRects = []\n        detRects = []\n        gtPolPoints = []\n        detPolPoints = []\n        gtDontCareRectsNum = []#Array of Ground Truth Rectangles\' keys marked as don\'t Care\n        detDontCareRectsNum = []#Array of Detected Rectangles\' matched with a don\'t Care GT\n        pairs = []\n        evaluationLog = """"\n        \n        recallMat = np.empty([1,1])\n        precisionMat = np.empty([1,1])              \n        \n        for n in range(len(gt)):\n            points = gt[n][\'points\']\n            # transcription = gt[n][\'text\']\n            dontCare = gt[n][\'ignore\']\n\n            if not Polygon(points).is_valid or not Polygon(points).is_simple:\n                continue\n\n            gtRects.append(points)\n            gtPolPoints.append(points)\n            if dontCare:\n                gtDontCareRectsNum.append( len(gtRects)-1 )                 \n        \n        evaluationLog += ""GT rectangles: "" + str(len(gtRects)) + ("" ("" + str(len(gtDontCareRectsNum)) + "" don\'t care)\\n"" if len(gtDontCareRectsNum)>0 else ""\\n"")\n        \n        for n in range(len(pred)):\n            points = pred[n][\'points\']\n\n            if not Polygon(points).is_valid or not Polygon(points).is_simple:\n                continue\n\n            detRect = points\n            detRects.append(detRect)\n            detPolPoints.append(points)\n            if len(gtDontCareRectsNum)>0 :\n                for dontCareRectNum in gtDontCareRectsNum:\n                    dontCareRect = gtRects[dontCareRectNum]\n                    intersected_area = get_intersection(dontCareRect,detRect)\n                    rdDimensions = Polygon(detRect).area\n                    if (rdDimensions==0) :\n                        precision = 0\n                    else:\n                        precision= intersected_area / rdDimensions\n                    if (precision > self.area_precision_constraint):\n                        detDontCareRectsNum.append( len(detRects)-1 )\n                        break\n\n        evaluationLog += ""DET rectangles: "" + str(len(detRects)) + ("" ("" + str(len(detDontCareRectsNum)) + "" don\'t care)\\n"" if len(detDontCareRectsNum)>0 else ""\\n"")\n\n        if len(gtRects)==0:\n            recall = 1\n            precision = 0 if len(detRects)>0 else 1\n\n        if len(detRects)>0:\n            #Calculate recall and precision matrixs\n            outputShape=[len(gtRects),len(detRects)]\n            recallMat = np.empty(outputShape)\n            precisionMat = np.empty(outputShape)\n            gtRectMat = np.zeros(len(gtRects),np.int8)\n            detRectMat = np.zeros(len(detRects),np.int8)\n            for gtNum in range(len(gtRects)):\n                for detNum in range(len(detRects)):\n                    rG = gtRects[gtNum]\n                    rD = detRects[detNum]\n                    intersected_area = get_intersection(rG,rD)\n                    rgDimensions = Polygon(rG).area\n                    rdDimensions = Polygon(rD).area\n                    recallMat[gtNum,detNum] = 0 if rgDimensions==0 else  intersected_area / rgDimensions\n                    precisionMat[gtNum,detNum] = 0 if rdDimensions==0 else intersected_area / rdDimensions\n\n            # Find one-to-one matches\n            evaluationLog += ""Find one-to-one matches\\n""\n            for gtNum in range(len(gtRects)):\n                for detNum in range(len(detRects)):\n                    if gtRectMat[gtNum] == 0 and detRectMat[detNum] == 0 and gtNum not in gtDontCareRectsNum and detNum not in detDontCareRectsNum :\n                        match = one_to_one_match(gtNum, detNum)\n                        if match is True :\n                            #in deteval we have to make other validation before mark as one-to-one\n                            rG = gtRects[gtNum]\n                            rD = detRects[detNum]\n                            normDist = center_distance(rG, rD);\n                            normDist /= diag(rG) + diag(rD);\n                            normDist *= 2.0;\n                            if normDist < self.ev_param_ind_center_diff_thr:\n                                gtRectMat[gtNum] = 1\n                                detRectMat[detNum] = 1\n                                recallAccum += self.mtype_oo_o\n                                precisionAccum += self.mtype_oo_o\n                                pairs.append({\'gt\':gtNum,\'det\':detNum,\'type\':\'OO\'})\n                                evaluationLog += ""Match GT #"" + str(gtNum) + "" with Det #"" + str(detNum) + ""\\n""\n                            else:\n                                evaluationLog += ""Match Discarded GT #"" + str(gtNum) + "" with Det #"" + str(detNum) + "" normDist: "" + str(normDist) + "" \\n""\n            # Find one-to-many matches\n            evaluationLog += ""Find one-to-many matches\\n""\n            for gtNum in range(len(gtRects)):\n                if gtNum not in gtDontCareRectsNum:\n                    match,matchesDet = one_to_many_match(gtNum)\n                    if match is True :\n                        evaluationLog += ""num_overlaps_gt="" + str(num_overlaps_gt(gtNum))\n                        gtRectMat[gtNum] = 1\n                        recallAccum += (self.mtype_oo_o if len(matchesDet)==1 else self.mtype_om_o)\n                        precisionAccum += (self.mtype_oo_o if len(matchesDet)==1 else self.mtype_om_o*len(matchesDet))\n                        pairs.append({\'gt\':gtNum,\'det\':matchesDet,\'type\': \'OO\' if len(matchesDet)==1 else \'OM\'})\n                        for detNum in matchesDet :\n                            detRectMat[detNum] = 1\n                        evaluationLog += ""Match GT #"" + str(gtNum) + "" with Det #"" + str(matchesDet) + ""\\n""\n\n            # Find many-to-one matches\n            evaluationLog += ""Find many-to-one matches\\n""\n            for detNum in range(len(detRects)):\n                if detNum not in detDontCareRectsNum:\n                    match,matchesGt = many_to_one_match(detNum)\n                    if match is True :\n                        detRectMat[detNum] = 1\n                        recallAccum += (self.mtype_oo_o if len(matchesGt)==1 else self.mtype_om_m*len(matchesGt))\n                        precisionAccum += (self.mtype_oo_o if len(matchesGt)==1 else self.mtype_om_m)\n                        pairs.append({\'gt\':matchesGt,\'det\':detNum,\'type\': \'OO\' if len(matchesGt)==1 else \'MO\'})\n                        for gtNum in matchesGt :\n                            gtRectMat[gtNum] = 1\n                        evaluationLog += ""Match GT #"" + str(matchesGt) + "" with Det #"" + str(detNum) + ""\\n""\n\n            numGtCare = (len(gtRects) - len(gtDontCareRectsNum))\n            if numGtCare == 0:\n                recall = float(1)\n                precision = float(0) if len(detRects)>0 else float(1)\n            else:\n                recall = float(recallAccum) / numGtCare\n                precision =  float(0) if (len(detRects) - len(detDontCareRectsNum))==0 else float(precisionAccum) / (len(detRects) - len(detDontCareRectsNum))\n            hmean = 0 if (precision + recall)==0 else 2.0 * precision * recall / (precision + recall)  \n\n        numGtCare = len(gtRects) - len(gtDontCareRectsNum)\n        numDetCare = len(detRects) - len(detDontCareRectsNum)\n\n        perSampleMetrics = {\n            \'precision\':precision,\n            \'recall\':recall,\n            \'hmean\':hmean,\n            \'pairs\':pairs,\n            \'recallMat\':[] if len(detRects)>100 else recallMat.tolist(),\n            \'precisionMat\':[] if len(detRects)>100 else precisionMat.tolist(),\n            \'gtPolPoints\':gtPolPoints,\n            \'detPolPoints\':detPolPoints,\n            \'gtCare\': numGtCare,\n            \'detCare\': numDetCare,\n            \'gtDontCare\':gtDontCareRectsNum,\n            \'detDontCare\':detDontCareRectsNum,\n            \'recallAccum\':recallAccum,\n            \'precisionAccum\':precisionAccum,\n            \'evaluationLog\': evaluationLog\n        }\n\n        return perSampleMetrics\n\n    def combine_results(self, results):\n        numGt = 0\n        numDet = 0\n        methodRecallSum = 0\n        methodPrecisionSum = 0\n\n        for result in results:\n            numGt += result[\'gtCare\']\n            numDet += result[\'detCare\']\n            methodRecallSum += result[\'recallAccum\']\n            methodPrecisionSum += result[\'precisionAccum\']\n\n        methodRecall = 0 if numGt==0 else methodRecallSum/numGt\n        methodPrecision = 0 if numDet==0 else methodPrecisionSum/numDet\n        methodHmean = 0 if methodRecall + methodPrecision==0 else 2* methodRecall * methodPrecision / (methodRecall + methodPrecision)\n        \n        methodMetrics = {\'precision\':methodPrecision, \'recall\':methodRecall,\'hmean\': methodHmean  }\n\n        return methodMetrics\n\n\nif __name__==\'__main__\':\n    evaluator = DetectionICDAR2013Evaluator()\n    gts = [[{\n        \'points\': [(0, 0), (1, 0), (1, 1), (0, 1)],\n        \'text\': 1234,\n        \'ignore\': False,\n    }, {\n        \'points\': [(2, 2), (3, 2), (3, 3), (2, 3)],\n        \'text\': 5678,\n        \'ignore\': True,\n    }]]\n    preds = [[{\n        \'points\': [(0.1, 0.1), (1, 0), (1, 1), (0, 1)],\n        \'text\': 123,\n        \'ignore\': False,\n    }]]\n    results = []\n    for gt, pred in zip(gts, preds):\n        results.append(evaluator.evaluate_image(gt, pred))\n    metrics = evaluator.combine_results(results)\n    print(metrics)\n'"
utils/ocr_metric/icdar2015/detection/iou.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom collections import namedtuple\nimport numpy as np\nfrom shapely.geometry import Polygon\nimport cv2\n\n\ndef iou_rotate(box_a, box_b, method=\'union\'):\n    rect_a = cv2.minAreaRect(box_a)\n    rect_b = cv2.minAreaRect(box_b)\n    r1 = cv2.rotatedRectangleIntersection(rect_a, rect_b)\n    if r1[0] == 0:\n        return 0\n    else:\n        inter_area = cv2.contourArea(r1[1])\n        area_a = cv2.contourArea(box_a)\n        area_b = cv2.contourArea(box_b)\n        union_area = area_a + area_b - inter_area\n        if union_area == 0 or inter_area == 0:\n            return 0\n        if method == \'union\':\n            iou = inter_area / union_area\n        elif method == \'intersection\':\n            iou = inter_area / min(area_a, area_b)\n        else:\n            raise NotImplementedError\n        return iou\n\n\nclass DetectionIoUEvaluator(object):\n    def __init__(self, is_output_polygon=False, iou_constraint=0.5, area_precision_constraint=0.5):\n        self.is_output_polygon = is_output_polygon\n        self.iou_constraint = iou_constraint\n        self.area_precision_constraint = area_precision_constraint\n\n    def evaluate_image(self, gt, pred):\n\n        def get_union(pD, pG):\n            return Polygon(pD).union(Polygon(pG)).area\n\n        def get_intersection_over_union(pD, pG):\n            return get_intersection(pD, pG) / get_union(pD, pG)\n\n        def get_intersection(pD, pG):\n            return Polygon(pD).intersection(Polygon(pG)).area\n\n        def compute_ap(confList, matchList, numGtCare):\n            correct = 0\n            AP = 0\n            if len(confList) > 0:\n                confList = np.array(confList)\n                matchList = np.array(matchList)\n                sorted_ind = np.argsort(-confList)\n                confList = confList[sorted_ind]\n                matchList = matchList[sorted_ind]\n                for n in range(len(confList)):\n                    match = matchList[n]\n                    if match:\n                        correct += 1\n                        AP += float(correct) / (n + 1)\n\n                if numGtCare > 0:\n                    AP /= numGtCare\n\n            return AP\n\n        perSampleMetrics = {}\n\n        matchedSum = 0\n\n        Rectangle = namedtuple(\'Rectangle\', \'xmin ymin xmax ymax\')\n\n        numGlobalCareGt = 0\n        numGlobalCareDet = 0\n\n        arrGlobalConfidences = []\n        arrGlobalMatches = []\n\n        recall = 0\n        precision = 0\n        hmean = 0\n\n        detMatched = 0\n\n        iouMat = np.empty([1, 1])\n\n        gtPols = []\n        detPols = []\n\n        gtPolPoints = []\n        detPolPoints = []\n\n        # Array of Ground Truth Polygons\' keys marked as don\'t Care\n        gtDontCarePolsNum = []\n        # Array of Detected Polygons\' matched with a don\'t Care GT\n        detDontCarePolsNum = []\n\n        pairs = []\n        detMatchedNums = []\n\n        arrSampleConfidences = []\n        arrSampleMatch = []\n\n        evaluationLog = """"\n\n        for n in range(len(gt)):\n            points = gt[n][\'points\']\n            # transcription = gt[n][\'text\']\n            dontCare = gt[n][\'ignore\']\n\n            if not Polygon(points).is_valid or not Polygon(points).is_simple:\n                continue\n\n            gtPol = points\n            gtPols.append(gtPol)\n            gtPolPoints.append(points)\n            if dontCare:\n                gtDontCarePolsNum.append(len(gtPols) - 1)\n\n        evaluationLog += ""GT polygons: "" + str(len(gtPols)) + ("" ("" + str(len(\n            gtDontCarePolsNum)) + "" don\'t care)\\n"" if len(gtDontCarePolsNum) > 0 else ""\\n"")\n\n        for n in range(len(pred)):\n            points = pred[n][\'points\']\n            if not Polygon(points).is_valid or not Polygon(points).is_simple:\n                continue\n\n            detPol = points\n            detPols.append(detPol)\n            detPolPoints.append(points)\n            if len(gtDontCarePolsNum) > 0:\n                for dontCarePol in gtDontCarePolsNum:\n                    dontCarePol = gtPols[dontCarePol]\n                    intersected_area = get_intersection(dontCarePol, detPol)\n                    pdDimensions = Polygon(detPol).area\n                    precision = 0 if pdDimensions == 0 else intersected_area / pdDimensions\n                    if (precision > self.area_precision_constraint):\n                        detDontCarePolsNum.append(len(detPols) - 1)\n                        break\n\n        evaluationLog += ""DET polygons: "" + str(len(detPols)) + ("" ("" + str(len(\n            detDontCarePolsNum)) + "" don\'t care)\\n"" if len(detDontCarePolsNum) > 0 else ""\\n"")\n\n        if len(gtPols) > 0 and len(detPols) > 0:\n            # Calculate IoU and precision matrixs\n            outputShape = [len(gtPols), len(detPols)]\n            iouMat = np.empty(outputShape)\n            gtRectMat = np.zeros(len(gtPols), np.int8)\n            detRectMat = np.zeros(len(detPols), np.int8)\n            if self.is_output_polygon:\n                for gtNum in range(len(gtPols)):\n                    for detNum in range(len(detPols)):\n                        pG = gtPols[gtNum]\n                        pD = detPols[detNum]\n                        iouMat[gtNum, detNum] = get_intersection_over_union(pD, pG)\n            else:\n                # gtPols = np.float32(gtPols)\n                # detPols = np.float32(detPols)\n                for gtNum in range(len(gtPols)):\n                    for detNum in range(len(detPols)):\n                        pG = np.float32(gtPols[gtNum])\n                        pD = np.float32(detPols[detNum])\n                        iouMat[gtNum, detNum] = iou_rotate(pD, pG)\n            for gtNum in range(len(gtPols)):\n                for detNum in range(len(detPols)):\n                    if gtRectMat[gtNum] == 0 and detRectMat[\n                        detNum] == 0 and gtNum not in gtDontCarePolsNum and detNum not in detDontCarePolsNum:\n                        if iouMat[gtNum, detNum] > self.iou_constraint:\n                            gtRectMat[gtNum] = 1\n                            detRectMat[detNum] = 1\n                            detMatched += 1\n                            pairs.append({\'gt\': gtNum, \'det\': detNum})\n                            detMatchedNums.append(detNum)\n                            evaluationLog += ""Match GT #"" + \\\n                                             str(gtNum) + "" with Det #"" + str(detNum) + ""\\n""\n\n        numGtCare = (len(gtPols) - len(gtDontCarePolsNum))\n        numDetCare = (len(detPols) - len(detDontCarePolsNum))\n        if numGtCare == 0:\n            recall = float(1)\n            precision = float(0) if numDetCare > 0 else float(1)\n        else:\n            recall = float(detMatched) / numGtCare\n            precision = 0 if numDetCare == 0 else float(\n                detMatched) / numDetCare\n\n        hmean = 0 if (precision + recall) == 0 else 2.0 * \\\n                                                    precision * recall / (precision + recall)\n\n        matchedSum += detMatched\n        numGlobalCareGt += numGtCare\n        numGlobalCareDet += numDetCare\n\n        perSampleMetrics = {\n            \'precision\': precision,\n            \'recall\': recall,\n            \'hmean\': hmean,\n            \'pairs\': pairs,\n            \'iouMat\': [] if len(detPols) > 100 else iouMat.tolist(),\n            \'gtPolPoints\': gtPolPoints,\n            \'detPolPoints\': detPolPoints,\n            \'gtCare\': numGtCare,\n            \'detCare\': numDetCare,\n            \'gtDontCare\': gtDontCarePolsNum,\n            \'detDontCare\': detDontCarePolsNum,\n            \'detMatched\': detMatched,\n            \'evaluationLog\': evaluationLog\n        }\n\n        return perSampleMetrics\n\n    def combine_results(self, results):\n        numGlobalCareGt = 0\n        numGlobalCareDet = 0\n        matchedSum = 0\n        for result in results:\n            numGlobalCareGt += result[\'gtCare\']\n            numGlobalCareDet += result[\'detCare\']\n            matchedSum += result[\'detMatched\']\n\n        methodRecall = 0 if numGlobalCareGt == 0 else float(\n            matchedSum) / numGlobalCareGt\n        methodPrecision = 0 if numGlobalCareDet == 0 else float(\n            matchedSum) / numGlobalCareDet\n        methodHmean = 0 if methodRecall + methodPrecision == 0 else 2 * \\\n                                                                    methodRecall * methodPrecision / (\n                                                                            methodRecall + methodPrecision)\n\n        methodMetrics = {\'precision\': methodPrecision,\n                         \'recall\': methodRecall, \'hmean\': methodHmean}\n\n        return methodMetrics\n\n\nif __name__ == \'__main__\':\n    evaluator = DetectionIoUEvaluator()\n    gts = [[{\n        \'points\': [(0, 0), (1, 0), (1, 1), (0, 1)],\n        \'text\': 1234,\n        \'ignore\': False,\n    }, {\n        \'points\': [(2, 2), (3, 2), (3, 3), (2, 3)],\n        \'text\': 5678,\n        \'ignore\': False,\n    }]]\n    preds = [[{\n        \'points\': [(0.1, 0.1), (1, 0), (1, 1), (0, 1)],\n        \'text\': 123,\n        \'ignore\': False,\n    }]]\n    results = []\n    for gt, pred in zip(gts, preds):\n        results.append(evaluator.evaluate_image(gt, pred))\n    metrics = evaluator.combine_results(results)\n    print(metrics)\n'"
utils/ocr_metric/icdar2015/detection/mtwi2018.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport math\nfrom collections import namedtuple\nimport numpy as np\nfrom shapely.geometry import Polygon\n\n\nclass DetectionMTWI2018Evaluator(object):\n    def __init__(\n        self,\n        area_recall_constraint=0.7, area_precision_constraint=0.7,\n        ev_param_ind_center_diff_thr=1,\n    ):\n\n\n        self.area_recall_constraint = area_recall_constraint\n        self.area_precision_constraint = area_precision_constraint\n        self.ev_param_ind_center_diff_thr = ev_param_ind_center_diff_thr\n\n    def evaluate_image(self, gt, pred):\n\n        def get_union(pD,pG):\n            return Polygon(pD).union(Polygon(pG)).area\n\n        def get_intersection_over_union(pD,pG):\n            return get_intersection(pD, pG) / get_union(pD, pG)\n\n        def get_intersection(pD,pG):\n            return Polygon(pD).intersection(Polygon(pG)).area\n\n        def one_to_one_match(row, col):\n            cont = 0\n            for j in range(len(recallMat[0])):    \n                if recallMat[row,j] >= self.area_recall_constraint and precisionMat[row,j] >= self.area_precision_constraint:\n                    cont = cont +1\n            if (cont != 1):\n                return False\n            cont = 0\n            for i in range(len(recallMat)):    \n                if recallMat[i,col] >= self.area_recall_constraint and precisionMat[i,col] >= self.area_precision_constraint:\n                    cont = cont +1\n            if (cont != 1):\n                return False\n            \n            if recallMat[row,col] >= self.area_recall_constraint and precisionMat[row,col] >= self.area_precision_constraint:\n                return True\n            return False\n        \n        def one_to_many_match(gtNum):\n            many_sum = 0\n            detRects = []\n            for detNum in range(len(recallMat[0])):    \n                if gtRectMat[gtNum] == 0 and detRectMat[detNum] == 0 and detNum not in detDontCareRectsNum:\n                    if precisionMat[gtNum,detNum] >= self.area_precision_constraint:\n                        many_sum += recallMat[gtNum,detNum]\n                        detRects.append(detNum)\n            if round(many_sum,4) >= self.area_recall_constraint:\n                return True,detRects\n            else:\n                return False,[]         \n        \n        def many_to_one_match(detNum):\n            many_sum = 0\n            gtRects = []\n            for gtNum in range(len(recallMat)):    \n                if gtRectMat[gtNum] == 0 and detRectMat[detNum] == 0 and gtNum not in gtDontCareRectsNum:\n                    if recallMat[gtNum,detNum] >= self.area_recall_constraint:\n                        many_sum += precisionMat[gtNum,detNum]\n                        gtRects.append(gtNum)\n            if round(many_sum,4) >= self.area_precision_constraint:\n                return True,gtRects\n            else:\n                return False,[]\n        \n        def center_distance(r1, r2):\n            return ((np.mean(r1, axis=0) - np.mean(r2, axis=0)) ** 2).sum() ** 0.5\n        \n        def diag(r):\n            r = np.array(r)\n            return ((r[:, 0].max() - r[:, 0].min()) ** 2 + (r[:, 1].max() - r[:, 1].min()) ** 2) ** 0.5\n        \n        perSampleMetrics = {}\n        \n        recall = 0\n        precision = 0\n        hmean = 0        \n        recallAccum = 0.\n        precisionAccum = 0.\n        gtRects = []\n        detRects = []\n        gtPolPoints = []\n        detPolPoints = []\n        gtDontCareRectsNum = []#Array of Ground Truth Rectangles\' keys marked as don\'t Care\n        detDontCareRectsNum = []#Array of Detected Rectangles\' matched with a don\'t Care GT\n        pairs = []\n        evaluationLog = """"\n        \n        recallMat = np.empty([1,1])\n        precisionMat = np.empty([1,1])              \n        \n        for n in range(len(gt)):\n            points = gt[n][\'points\']\n            # transcription = gt[n][\'text\']\n            dontCare = gt[n][\'ignore\']\n\n            if not Polygon(points).is_valid or not Polygon(points).is_simple:\n                continue\n\n            gtRects.append(points)\n            gtPolPoints.append(points)\n            if dontCare:\n                gtDontCareRectsNum.append( len(gtRects)-1 )                 \n        \n        evaluationLog += ""GT rectangles: "" + str(len(gtRects)) + ("" ("" + str(len(gtDontCareRectsNum)) + "" don\'t care)\\n"" if len(gtDontCareRectsNum)>0 else ""\\n"")\n        \n        for n in range(len(pred)):\n            points = pred[n][\'points\']\n\n            if not Polygon(points).is_valid or not Polygon(points).is_simple:\n                continue\n\n            detRect = points\n            detRects.append(detRect)\n            detPolPoints.append(points)\n            if len(gtDontCareRectsNum)>0 :\n                for dontCareRectNum in gtDontCareRectsNum:\n                    dontCareRect = gtRects[dontCareRectNum]\n                    intersected_area = get_intersection(dontCareRect,detRect)\n                    rdDimensions = Polygon(detRect).area\n                    if (rdDimensions==0) :\n                        precision = 0\n                    else:\n                        precision= intersected_area / rdDimensions\n                    if (precision > 0.5):\n                        detDontCareRectsNum.append( len(detRects)-1 )\n                        break\n\n        evaluationLog += ""DET rectangles: "" + str(len(detRects)) + ("" ("" + str(len(detDontCareRectsNum)) + "" don\'t care)\\n"" if len(detDontCareRectsNum)>0 else ""\\n"")\n\n        if len(gtRects)==0:\n            recall = 1\n            precision = 0 if len(detRects)>0 else 1\n\n        if len(detRects)>0:\n            #Calculate recall and precision matrixs\n            outputShape=[len(gtRects),len(detRects)]\n            recallMat = np.empty(outputShape)\n            precisionMat = np.empty(outputShape)\n            gtRectMat = np.zeros(len(gtRects),np.int8)\n            detRectMat = np.zeros(len(detRects),np.int8)\n            for gtNum in range(len(gtRects)):\n                for detNum in range(len(detRects)):\n                    rG = gtRects[gtNum]\n                    rD = detRects[detNum]\n                    intersected_area = get_intersection(rG,rD)\n                    rgDimensions = Polygon(rG).area\n                    rdDimensions = Polygon(rD).area\n                    recallMat[gtNum,detNum] = 0 if rgDimensions==0 else  intersected_area / rgDimensions\n                    precisionMat[gtNum,detNum] = 0 if rdDimensions==0 else intersected_area / rdDimensions\n\n            # Find one-to-one matches\n            evaluationLog += ""Find one-to-one matches\\n""\n            for gtNum in range(len(gtRects)):\n                for detNum in range(len(detRects)):\n                    if gtRectMat[gtNum] == 0 and detRectMat[detNum] == 0 and gtNum not in gtDontCareRectsNum and detNum not in detDontCareRectsNum :\n                        match = one_to_one_match(gtNum, detNum)\n                        if match is True :\n                            #in deteval we have to make other validation before mark as one-to-one\n                            rG = gtRects[gtNum]\n                            rD = detRects[detNum]\n                            normDist = center_distance(rG, rD);\n                            normDist /= diag(rG) + diag(rD);\n                            normDist *= 2.0;\n                            if normDist < self.ev_param_ind_center_diff_thr:\n                                gtRectMat[gtNum] = 1\n                                detRectMat[detNum] = 1\n                                recallAccum += 1.0\n                                precisionAccum += 1.0\n                                pairs.append({\'gt\':gtNum,\'det\':detNum,\'type\':\'OO\'})\n                                evaluationLog += ""Match GT #"" + str(gtNum) + "" with Det #"" + str(detNum) + ""\\n""\n                            else:\n                                evaluationLog += ""Match Discarded GT #"" + str(gtNum) + "" with Det #"" + str(detNum) + "" normDist: "" + str(normDist) + "" \\n""\n            # Find one-to-many matches\n            evaluationLog += ""Find one-to-many matches\\n""\n            for gtNum in range(len(gtRects)):\n                if gtNum not in gtDontCareRectsNum:\n                    match,matchesDet = one_to_many_match(gtNum)\n                    if match is True :\n                        gtRectMat[gtNum] = 1\n                        recallAccum += 1.0\n                        precisionAccum += len(matchesDet) / (1 + math.log(len(matchesDet)))\n                        pairs.append({\'gt\':gtNum,\'det\':matchesDet,\'type\': \'OO\' if len(matchesDet)==1 else \'OM\'})\n                        for detNum in matchesDet :\n                            detRectMat[detNum] = 1\n                        evaluationLog += ""Match GT #"" + str(gtNum) + "" with Det #"" + str(matchesDet) + ""\\n""\n\n            # Find many-to-one matches\n            evaluationLog += ""Find many-to-one matches\\n""\n            for detNum in range(len(detRects)):\n                if detNum not in detDontCareRectsNum:\n                    match,matchesGt = many_to_one_match(detNum)\n                    if match is True :\n                        detRectMat[detNum] = 1\n                        recallAccum += len(matchesGt) / (1 + math.log(len(matchesGt)))\n                        precisionAccum += 1.0\n                        pairs.append({\'gt\':matchesGt,\'det\':detNum,\'type\': \'OO\' if len(matchesGt)==1 else \'MO\'})\n                        for gtNum in matchesGt :\n                            gtRectMat[gtNum] = 1\n                        evaluationLog += ""Match GT #"" + str(matchesGt) + "" with Det #"" + str(detNum) + ""\\n""\n\n            numGtCare = (len(gtRects) - len(gtDontCareRectsNum))\n            if numGtCare == 0:\n                recall = float(1)\n                precision = float(0) if len(detRects)>0 else float(1)\n            else:\n                recall = float(recallAccum) / numGtCare\n                precision =  float(0) if (len(detRects) - len(detDontCareRectsNum))==0 else float(precisionAccum) / (len(detRects) - len(detDontCareRectsNum))\n            hmean = 0 if (precision + recall)==0 else 2.0 * precision * recall / (precision + recall)  \n\n        numGtCare = len(gtRects) - len(gtDontCareRectsNum)\n        numDetCare = len(detRects) - len(detDontCareRectsNum)\n\n        perSampleMetrics = {\n            \'precision\':precision,\n            \'recall\':recall,\n            \'hmean\':hmean,\n            \'pairs\':pairs,\n            \'recallMat\':[] if len(detRects)>100 else recallMat.tolist(),\n            \'precisionMat\':[] if len(detRects)>100 else precisionMat.tolist(),\n            \'gtPolPoints\':gtPolPoints,\n            \'detPolPoints\':detPolPoints,\n            \'gtCare\': numGtCare,\n            \'detCare\': numDetCare,\n            \'gtDontCare\':gtDontCareRectsNum,\n            \'detDontCare\':detDontCareRectsNum,\n            \'recallAccum\':recallAccum,\n            \'precisionAccum\':precisionAccum,\n            \'evaluationLog\': evaluationLog\n        }\n\n        return perSampleMetrics\n\n    def combine_results(self, results):\n        numGt = 0\n        numDet = 0\n        methodRecallSum = 0\n        methodPrecisionSum = 0\n\n        for result in results:\n            numGt += result[\'gtCare\']\n            numDet += result[\'detCare\']\n            methodRecallSum += result[\'recallAccum\']\n            methodPrecisionSum += result[\'precisionAccum\']\n\n        methodRecall = 0 if numGt==0 else methodRecallSum/numGt\n        methodPrecision = 0 if numDet==0 else methodPrecisionSum/numDet\n        methodHmean = 0 if methodRecall + methodPrecision==0 else 2* methodRecall * methodPrecision / (methodRecall + methodPrecision)\n        \n        methodMetrics = {\'precision\':methodPrecision, \'recall\':methodRecall,\'hmean\': methodHmean  }\n\n        return methodMetrics\n\n\nif __name__==\'__main__\':\n    evaluator = DetectionICDAR2013Evaluator()\n    gts = [[{\n        \'points\': [(0, 0), (1, 0), (1, 1), (0, 1)],\n        \'text\': 1234,\n        \'ignore\': False,\n    }, {\n        \'points\': [(2, 2), (3, 2), (3, 3), (2, 3)],\n        \'text\': 5678,\n        \'ignore\': True,\n    }]]\n    preds = [[{\n        \'points\': [(0.1, 0.1), (1, 0), (1, 1), (0, 1)],\n        \'text\': 123,\n        \'ignore\': False,\n    }]]\n    results = []\n    for gt, pred in zip(gts, preds):\n        results.append(evaluator.evaluate_image(gt, pred))\n    metrics = evaluator.combine_results(results)\n    print(metrics)\n'"
