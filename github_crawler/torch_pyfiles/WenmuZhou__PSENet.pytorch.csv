file_path,api_count,code
config.py,0,"b""# -*- coding: utf-8 -*-\r\n# @Time    : 2019/1/3 17:40\r\n# @Author  : zhoujun\r\n\r\n# data config\r\ntrainroot = '/data2/dataset/ICD15/train'\r\ntestroot = '/data2/dataset/ICD15/test'\r\noutput_dir = 'output/psenet_icd2015_resnet152_4gpu_author_crop_adam_MultiStepLR_authorloss'\r\ndata_shape = 640\r\n\r\n# train config\r\ngpu_id = '2'\r\nworkers = 12\r\nstart_epoch = 0\r\nepochs = 600\r\n\r\ntrain_batch_size = 4\r\n\r\nlr = 1e-4\r\nend_lr = 1e-7\r\nlr_gamma = 0.1\r\nlr_decay_step = [200,400]\r\nweight_decay = 5e-4\r\nwarm_up_epoch = 6\r\nwarm_up_lr = lr * lr_gamma\r\n\r\ndisplay_input_images = False\r\ndisplay_output_images = False\r\ndisplay_interval = 10\r\nshow_images_interval = 50\r\n\r\npretrained = True\r\nrestart_training = True\r\ncheckpoint = ''\r\n\r\n# net config\r\nbackbone = 'resnet152'\r\nLambda = 0.7\r\nn = 6\r\nm = 0.5\r\nOHEM_ratio = 3\r\nscale = 1\r\n# random seed\r\nseed = 2\r\n\r\n\r\ndef print():\r\n    from pprint import pformat\r\n    tem_d = {}\r\n    for k, v in globals().items():\r\n        if not k.startswith('_') and not callable(v):\r\n            tem_d[k] = v\r\n    return pformat(tem_d)\r\n"""
eval.py,1,"b""# -*- coding: utf-8 -*-\n# @Time    : 2018/6/11 15:54\n# @Author  : zhoujun\nimport torch\nimport shutil\nimport numpy as np\nimport config\nimport os\nimport cv2\nfrom tqdm import tqdm\nfrom models import PSENet\nfrom predict import Pytorch_model\nfrom cal_recall.script import cal_recall_precison_f1\nfrom utils import draw_bbox\n\ntorch.backends.cudnn.benchmark = True\n\n\ndef main(model_path, backbone, scale, path, save_path, gpu_id):\n    if os.path.exists(save_path):\n        shutil.rmtree(save_path, ignore_errors=True)\n    if not os.path.exists(save_path):\n        os.makedirs(save_path)\n    save_img_folder = os.path.join(save_path, 'img')\n    if not os.path.exists(save_img_folder):\n        os.makedirs(save_img_folder)\n    save_txt_folder = os.path.join(save_path, 'result')\n    if not os.path.exists(save_txt_folder):\n        os.makedirs(save_txt_folder)\n    img_paths = [os.path.join(path, x) for x in os.listdir(path)]\n    net = PSENet(backbone=backbone, pretrained=False, result_num=config.n)\n    model = Pytorch_model(model_path, net=net, scale=scale, gpu_id=gpu_id)\n    total_frame = 0.0\n    total_time = 0.0\n    for img_path in tqdm(img_paths):\n        img_name = os.path.basename(img_path).split('.')[0]\n        save_name = os.path.join(save_txt_folder, 'res_' + img_name + '.txt')\n        _, boxes_list, t = model.predict(img_path)\n        total_frame += 1\n        total_time += t\n        # img = draw_bbox(img_path, boxes_list, color=(0, 0, 255))\n        # cv2.imwrite(os.path.join(save_img_folder, '{}.jpg'.format(img_name)), img)\n        np.savetxt(save_name, boxes_list.reshape(-1, 8), delimiter=',', fmt='%d')\n    print('fps:{}'.format(total_frame / total_time))\n    return save_txt_folder\n\n\nif __name__ == '__main__':\n    os.environ['CUDA_VISIBLE_DEVICES'] = str('2')\n    backbone = 'resnet152'\n    scale = 4\n    model_path = 'output/psenet_icd2015_resnet152_author_crop_adam_warm_up_myloss/best_r0.714011_p0.708214_f10.711100.pth'\n    data_path = '/data2/dataset/ICD15/test/img'\n    gt_path = '/data2/dataset/ICD15/test/gt'\n    save_path = './result/_scale{}'.format(scale)\n    gpu_id = 0\n    print('backbone:{},scale:{},model_path:{}'.format(backbone,scale,model_path))\n    save_path = main(model_path, backbone, scale, data_path, save_path, gpu_id=gpu_id)\n    result = cal_recall_precison_f1(gt_path=gt_path, result_path=save_path)\n    print(result)\n    # print(cal_recall_precison_f1('/data2/dataset/ICD151/test/gt', '/data1/zj/tensorflow_PSENet/tmp/'))\n"""
predict.py,7,"b'# -*- coding: utf-8 -*-\n# @Time    : 1/4/19 11:14 AM\n# @Author  : zhoujun\nimport torch\nfrom torchvision import transforms\nimport os\nimport cv2\nimport time\nimport numpy as np\n\nfrom pse import decode as pse_decode\n\n\nclass Pytorch_model:\n    def __init__(self, model_path, net, scale, gpu_id=None):\n        \'\'\'\n        \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96pytorch\xe6\xa8\xa1\xe5\x9e\x8b\n        :param model_path: \xe6\xa8\xa1\xe5\x9e\x8b\xe5\x9c\xb0\xe5\x9d\x80(\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x98\xaf\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe6\x88\x96\xe8\x80\x85\xe5\x8f\x82\xe6\x95\xb0\xe5\x92\x8c\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe4\xb8\x80\xe8\xb5\xb7\xe4\xbf\x9d\xe5\xad\x98\xe7\x9a\x84\xe6\x96\x87\xe4\xbb\xb6)\n        :param net: \xe7\xbd\x91\xe7\xbb\x9c\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe5\x9c\xa8model_path\xe4\xb8\xad\xe6\x8c\x87\xe5\xae\x9a\xe7\x9a\x84\xe6\x98\xaf\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe4\xbf\x9d\xe5\xad\x98\xe8\xb7\xaf\xe5\xbe\x84\xef\xbc\x8c\xe5\x88\x99\xe9\x9c\x80\xe8\xa6\x81\xe7\xbb\x99\xe5\x87\xba\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\n        :param img_channel: \xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0: 1,3\n        :param gpu_id: \xe5\x9c\xa8\xe5\x93\xaa\xe4\xb8\x80\xe5\x9d\x97gpu\xe4\xb8\x8a\xe8\xbf\x90\xe8\xa1\x8c\n        \'\'\'\n        self.scale = scale\n        if gpu_id is not None and isinstance(gpu_id, int) and torch.cuda.is_available():\n            self.device = torch.device(""cuda:{}"".format(gpu_id))\n        else:\n            self.device = torch.device(""cpu"")\n        self.net = torch.load(model_path, map_location=self.device)[\'state_dict\']\n        print(\'device:\', self.device)\n\n        if net is not None:\n            # \xe5\xa6\x82\xe6\x9e\x9c\xe7\xbd\x91\xe7\xbb\x9c\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe5\x92\x8c\xe5\x8f\x82\xe6\x95\xb0\xe6\x98\xaf\xe5\x88\x86\xe5\xbc\x80\xe4\xbf\x9d\xe5\xad\x98\xe7\x9a\x84\xef\xbc\x8c\xe5\xb0\xb1\xe6\x89\xa7\xe8\xa1\x8c\xe5\x8f\x82\xe6\x95\xb0\xe5\x8a\xa0\xe8\xbd\xbd\n            net = net.to(self.device)\n            net.scale = scale\n            try:\n                sk = {}\n                for k in self.net:\n                    sk[k[7:]] = self.net[k]\n                net.load_state_dict(sk)\n            except:\n                net.load_state_dict(self.net)\n            self.net = net\n            print(\'load models\')\n        self.net.eval()\n\n    def predict(self, img: str, long_size: int = 2240):\n        \'\'\'\n        \xe5\xaf\xb9\xe4\xbc\xa0\xe5\x85\xa5\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f\xe8\xbf\x9b\xe8\xa1\x8c\xe9\xa2\x84\xe6\xb5\x8b\xef\xbc\x8c\xe6\x94\xaf\xe6\x8c\x81\xe5\x9b\xbe\xe5\x83\x8f\xe5\x9c\xb0\xe5\x9d\x80,opecv \xe8\xaf\xbb\xe5\x8f\x96\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c\xe5\x81\x8f\xe6\x85\xa2\n        :param img: \xe5\x9b\xbe\xe5\x83\x8f\xe5\x9c\xb0\xe5\x9d\x80\n        :param is_numpy:\n        :return:\n        \'\'\'\n        assert os.path.exists(img), \'file is not exists\'\n        img = cv2.imread(img)\n        # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        h, w = img.shape[:2]\n\n        scale = long_size / max(h, w)\n        img = cv2.resize(img, None, fx=scale, fy=scale)\n        # \xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe7\x94\xb1(w,h)\xe5\x8f\x98\xe4\xb8\xba(1,img_channel,h,w)\n        tensor = transforms.ToTensor()(img)\n        tensor = tensor.unsqueeze_(0)\n\n        tensor = tensor.to(self.device)\n        with torch.no_grad():\n            torch.cuda.synchronize()\n            start = time.time()\n            preds = self.net(tensor)\n            preds, boxes_list = pse_decode(preds[0], self.scale)\n            scale = (preds.shape[1] / w, preds.shape[0] / h)\n            # print(scale)\n            # preds, boxes_list = decode(preds,num_pred=-1)\n            if len(boxes_list):\n                boxes_list = boxes_list / scale\n            torch.cuda.synchronize()\n            t = time.time() - start\n        return preds, boxes_list, t\n\n\ndef _get_annotation(label_path):\n    boxes = []\n    with open(label_path, encoding=\'utf-8\', mode=\'r\') as f:\n        for line in f.readlines():\n            params = line.strip().strip(\'\\ufeff\').strip(\'\\xef\\xbb\\xbf\').split(\',\')\n            try:\n                label = params[8]\n                if label == \'*\' or label == \'###\':\n                    continue\n                x1, y1, x2, y2, x3, y3, x4, y4 = list(map(float, params[:8]))\n                boxes.append([[x1, y1], [x2, y2], [x3, y3], [x4, y4]])\n            except:\n                print(\'load label failed on {}\'.format(label_path))\n    return np.array(boxes, dtype=np.float32)\n\n\nif __name__ == \'__main__\':\n    import config\n    from models import PSENet\n    import matplotlib.pyplot as plt\n    from utils.utils import show_img, draw_bbox\n\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = str(\'2\')\n\n    model_path = \'output/psenet_icd2015_resnet152_author_crop_adam_warm_up_myloss/best_r0.714011_p0.708214_f10.711100.pth\'\n\n    # model_path = \'output/psenet_icd2015_new_loss/final.pth\'\n    img_id = 10\n    img_path = \'/data2/dataset/ICD15/test/img/img_{}.jpg\'.format(img_id)\n    label_path = \'/data2/dataset/ICD15/test/gt/gt_img_{}.txt\'.format(img_id)\n    label = _get_annotation(label_path)\n\n    # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe7\xbd\x91\xe7\xbb\x9c\n    net = PSENet(backbone=\'resnet152\', pretrained=False, result_num=config.n)\n    model = Pytorch_model(model_path, net=net, scale=1, gpu_id=0)\n    # for i in range(100):\n    #     models.predict(img_path)\n    preds, boxes_list,t = model.predict(img_path)\n    print(boxes_list)\n    show_img(preds)\n    img = draw_bbox(img_path, boxes_list, color=(0, 0, 255))\n    cv2.imwrite(\'result.jpg\', img)\n    # img = draw_bbox(img, label,color=(0,0,255))\n    show_img(img, color=True)\n\n    plt.show()\n'"
train.py,20,"b'# -*- coding: utf-8 -*-\r\n# @Time    : 2018/6/11 15:54\r\n# @Author  : zhoujun\r\nimport cv2\r\nimport os\r\nimport config\r\n\r\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = config.gpu_id\r\n\r\nimport shutil\r\nimport glob\r\nimport time\r\nimport numpy as np\r\nimport torch\r\nfrom tqdm import tqdm\r\nfrom torch import nn\r\nimport torch.utils.data as Data\r\nfrom torchvision import transforms\r\nimport torchvision.utils as vutils\r\nfrom torch.utils.tensorboard import SummaryWriter\r\n\r\nfrom dataset.data_utils import MyDataset\r\nfrom models import PSENet\r\nfrom models.loss import PSELoss\r\nfrom utils.utils import load_checkpoint, save_checkpoint, setup_logger\r\nfrom pse import decode as pse_decode\r\nfrom cal_recall import cal_recall_precison_f1\r\n\r\n\r\ndef weights_init(m):\r\n    if isinstance(m, nn.Conv2d):\r\n        nn.init.kaiming_normal_(m.weight)\r\n        if m.bias is not None:\r\n            nn.init.constant_(m.bias, 0)\r\n\r\n\r\n# learning rate\xe7\x9a\x84warming up\xe6\x93\x8d\xe4\xbd\x9c\r\ndef adjust_learning_rate(optimizer, epoch):\r\n    """"""Sets the learning rate\r\n    # Adapted from PyTorch Imagenet example:\r\n    # https://github.com/pytorch/examples/blob/master/imagenet/main.py\r\n    """"""\r\n    if epoch < config.warm_up_epoch:\r\n        lr = 1e-6 + (config.lr - 1e-6) * epoch / (config.warm_up_epoch)\r\n    else:\r\n        lr = config.lr * (config.lr_gamma ** (epoch / config.lr_decay_step[0]))\r\n\r\n    for param_group in optimizer.param_groups:\r\n        param_group[\'lr\'] = lr\r\n\r\n    return lr\r\n\r\n\r\ndef train_epoch(net, optimizer, scheduler, train_loader, device, criterion, epoch, all_step, writer, logger):\r\n    net.train()\r\n    train_loss = 0.\r\n    start = time.time()\r\n    scheduler.step()\r\n    # lr = adjust_learning_rate(optimizer, epoch)\r\n    lr = scheduler.get_lr()[0]\r\n    for i, (images, labels, training_mask) in enumerate(train_loader):\r\n        cur_batch = images.size()[0]\r\n        images, labels, training_mask = images.to(device), labels.to(device), training_mask.to(device)\r\n        # Forward\r\n        y1 = net(images)\r\n        loss_c, loss_s, loss = criterion(y1, labels, training_mask)\r\n        # Backward\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        optimizer.step()\r\n        train_loss += loss.item()\r\n\r\n        loss_c = loss_c.item()\r\n        loss_s = loss_s.item()\r\n        loss = loss.item()\r\n        cur_step = epoch * all_step + i\r\n        writer.add_scalar(tag=\'Train/loss_c\', scalar_value=loss_c, global_step=cur_step)\r\n        writer.add_scalar(tag=\'Train/loss_s\', scalar_value=loss_s, global_step=cur_step)\r\n        writer.add_scalar(tag=\'Train/loss\', scalar_value=loss, global_step=cur_step)\r\n        writer.add_scalar(tag=\'Train/lr\', scalar_value=lr, global_step=cur_step)\r\n\r\n        if i % config.display_interval == 0:\r\n            batch_time = time.time() - start\r\n            logger.info(\r\n                \'[{}/{}], [{}/{}], step: {}, {:.3f} samples/sec, batch_loss: {:.4f}, batch_loss_c: {:.4f}, batch_loss_s: {:.4f}, time:{:.4f}, lr:{}\'.format(\r\n                    epoch, config.epochs, i, all_step, cur_step, config.display_interval * cur_batch / batch_time,\r\n                    loss, loss_c, loss_s, batch_time, lr))\r\n            start = time.time()\r\n\r\n        if i % config.show_images_interval == 0:\r\n            if config.display_input_images:\r\n                # show images on tensorboard\r\n                x = vutils.make_grid(images.detach().cpu(), nrow=4, normalize=True, scale_each=True, padding=20)\r\n                writer.add_image(tag=\'input/image\', img_tensor=x, global_step=cur_step)\r\n\r\n                show_label = labels.detach().cpu()\r\n                b, c, h, w = show_label.size()\r\n                show_label = show_label.reshape(b * c, h, w)\r\n                show_label = vutils.make_grid(show_label.unsqueeze(1), nrow=config.n, normalize=False, padding=20,\r\n                                              pad_value=1)\r\n                writer.add_image(tag=\'input/label\', img_tensor=show_label, global_step=cur_step)\r\n\r\n            if config.display_output_images:\r\n                y1 = torch.sigmoid(y1)\r\n                show_y = y1.detach().cpu()\r\n                b, c, h, w = show_y.size()\r\n                show_y = show_y.reshape(b * c, h, w)\r\n                show_y = vutils.make_grid(show_y.unsqueeze(1), nrow=config.n, normalize=False, padding=20, pad_value=1)\r\n                writer.add_image(tag=\'output/preds\', img_tensor=show_y, global_step=cur_step)\r\n    writer.add_scalar(tag=\'Train_epoch/loss\', scalar_value=train_loss / all_step, global_step=epoch)\r\n    return train_loss / all_step, lr\r\n\r\n\r\ndef eval(model, save_path, test_path, device):\r\n    model.eval()\r\n    # torch.cuda.empty_cache()  # speed up evaluating after training finished\r\n    img_path = os.path.join(test_path, \'img\')\r\n    gt_path = os.path.join(test_path, \'gt\')\r\n    if os.path.exists(save_path):\r\n        shutil.rmtree(save_path, ignore_errors=True)\r\n    if not os.path.exists(save_path):\r\n        os.makedirs(save_path)\r\n    long_size = 2240\r\n    # \xe9\xa2\x84\xe6\xb5\x8b\xe6\x89\x80\xe6\x9c\x89\xe6\xb5\x8b\xe8\xaf\x95\xe5\x9b\xbe\xe7\x89\x87\r\n    img_paths = [os.path.join(img_path, x) for x in os.listdir(img_path)]\r\n    for img_path in tqdm(img_paths, desc=\'test models\'):\r\n        img_name = os.path.basename(img_path).split(\'.\')[0]\r\n        save_name = os.path.join(save_path, \'res_\' + img_name + \'.txt\')\r\n\r\n        assert os.path.exists(img_path), \'file is not exists\'\r\n        img = cv2.imread(img_path)\r\n        h, w = img.shape[:2]\r\n        #if max(h, w) > long_size:\r\n        scale = long_size / max(h, w)\r\n        img = cv2.resize(img, None, fx=scale, fy=scale)\r\n        # \xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe7\x94\xb1(w,h)\xe5\x8f\x98\xe4\xb8\xba(1,img_channel,h,w)\r\n        tensor = transforms.ToTensor()(img)\r\n        tensor = tensor.unsqueeze_(0)\r\n        tensor = tensor.to(device)\r\n        with torch.no_grad():\r\n            preds = model(tensor)\r\n            preds, boxes_list = pse_decode(preds[0], config.scale)\r\n            scale = (preds.shape[1] * 1.0 / w, preds.shape[0] * 1.0 / h)\r\n            if len(boxes_list):\r\n                boxes_list = boxes_list / scale\r\n        np.savetxt(save_name, boxes_list.reshape(-1, 8), delimiter=\',\', fmt=\'%d\')\r\n    # \xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xa1\xe7\xae\x97 recall precision f1\r\n    result_dict = cal_recall_precison_f1(gt_path, save_path)\r\n    return result_dict[\'recall\'], result_dict[\'precision\'], result_dict[\'hmean\']\r\n\r\n\r\ndef main():\r\n    if config.output_dir is None:\r\n        config.output_dir = \'output\'\r\n    if config.restart_training:\r\n        shutil.rmtree(config.output_dir, ignore_errors=True)\r\n    if not os.path.exists(config.output_dir):\r\n        os.makedirs(config.output_dir)\r\n\r\n    logger = setup_logger(os.path.join(config.output_dir, \'train_log\'))\r\n    logger.info(config.print())\r\n\r\n    torch.manual_seed(config.seed)  # \xe4\xb8\xbaCPU\xe8\xae\xbe\xe7\xbd\xae\xe9\x9a\x8f\xe6\x9c\xba\xe7\xa7\x8d\xe5\xad\x90\r\n    if config.gpu_id is not None and torch.cuda.is_available():\r\n        torch.backends.cudnn.benchmark = True\r\n        logger.info(\'train with gpu {} and pytorch {}\'.format(config.gpu_id, torch.__version__))\r\n        device = torch.device(""cuda:0"")\r\n        torch.cuda.manual_seed(config.seed)  # \xe4\xb8\xba\xe5\xbd\x93\xe5\x89\x8dGPU\xe8\xae\xbe\xe7\xbd\xae\xe9\x9a\x8f\xe6\x9c\xba\xe7\xa7\x8d\xe5\xad\x90\r\n        torch.cuda.manual_seed_all(config.seed)  # \xe4\xb8\xba\xe6\x89\x80\xe6\x9c\x89GPU\xe8\xae\xbe\xe7\xbd\xae\xe9\x9a\x8f\xe6\x9c\xba\xe7\xa7\x8d\xe5\xad\x90\r\n    else:\r\n        logger.info(\'train with cpu and pytorch {}\'.format(torch.__version__))\r\n        device = torch.device(""cpu"")\r\n\r\n    train_data = MyDataset(config.trainroot, data_shape=config.data_shape, n=config.n, m=config.m,\r\n                           transform=transforms.ToTensor())\r\n    train_loader = Data.DataLoader(dataset=train_data, batch_size=config.train_batch_size, shuffle=True,\r\n                                   num_workers=int(config.workers))\r\n\r\n    writer = SummaryWriter(config.output_dir)\r\n    model = PSENet(backbone=config.backbone, pretrained=config.pretrained, result_num=config.n, scale=config.scale)\r\n    if not config.pretrained and not config.restart_training:\r\n        model.apply(weights_init)\r\n\r\n    num_gpus = torch.cuda.device_count()\r\n    if num_gpus > 1:\r\n        model = nn.DataParallel(model)\r\n    model = model.to(device)\r\n    # dummy_input = torch.autograd.Variable(torch.Tensor(1, 3, 600, 800).to(device))\r\n    # writer.add_graph(models=models, input_to_model=dummy_input)\r\n    criterion = PSELoss(Lambda=config.Lambda, ratio=config.OHEM_ratio, reduction=\'mean\')\r\n    # optimizer = torch.optim.SGD(models.parameters(), lr=config.lr, momentum=0.99)\r\n    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\r\n    if config.checkpoint != \'\' and not config.restart_training:\r\n        start_epoch = load_checkpoint(config.checkpoint, model, logger, device, optimizer)\r\n        start_epoch += 1\r\n        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, config.lr_decay_step, gamma=config.lr_gamma,\r\n                                                         last_epoch=start_epoch)\r\n    else:\r\n        start_epoch = config.start_epoch\r\n        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, config.lr_decay_step, gamma=config.lr_gamma)\r\n\r\n    all_step = len(train_loader)\r\n    logger.info(\'train dataset has {} samples,{} in dataloader\'.format(train_data.__len__(), all_step))\r\n    epoch = 0\r\n    best_model = {\'recall\': 0, \'precision\': 0, \'f1\': 0, \'models\': \'\'}\r\n    try:\r\n        for epoch in range(start_epoch, config.epochs):\r\n            start = time.time()\r\n            train_loss, lr = train_epoch(model, optimizer, scheduler, train_loader, device, criterion, epoch, all_step,\r\n                                         writer, logger)\r\n            logger.info(\'[{}/{}], train_loss: {:.4f}, time: {:.4f}, lr: {}\'.format(\r\n                epoch, config.epochs, train_loss, time.time() - start, lr))\r\n            # net_save_path = \'{}/PSENet_{}_loss{:.6f}.pth\'.format(config.output_dir, epoch,\r\n            #                                                                               train_loss)\r\n            # save_checkpoint(net_save_path, models, optimizer, epoch, logger)\r\n            if (0.3 < train_loss < 0.4 and epoch % 4 == 0) or train_loss < 0.3:\r\n                recall, precision, f1 = eval(model, os.path.join(config.output_dir, \'output\'), config.testroot, device)\r\n                logger.info(\'test: recall: {:.6f}, precision: {:.6f}, f1: {:.6f}\'.format(recall, precision, f1))\r\n\r\n                net_save_path = \'{}/PSENet_{}_loss{:.6f}_r{:.6f}_p{:.6f}_f1{:.6f}.pth\'.format(config.output_dir, epoch,\r\n                                                                                              train_loss,\r\n                                                                                              recall,\r\n                                                                                              precision,\r\n                                                                                              f1)\r\n                save_checkpoint(net_save_path, model, optimizer, epoch, logger)\r\n                if f1 > best_model[\'f1\']:\r\n                    best_path = glob.glob(config.output_dir + \'/Best_*.pth\')\r\n                    for b_path in best_path:\r\n                        if os.path.exists(b_path):\r\n                            os.remove(b_path)\r\n\r\n                    best_model[\'recall\'] = recall\r\n                    best_model[\'precision\'] = precision\r\n                    best_model[\'f1\'] = f1\r\n                    best_model[\'models\'] = net_save_path\r\n\r\n                    best_save_path = \'{}/Best_{}_r{:.6f}_p{:.6f}_f1{:.6f}.pth\'.format(config.output_dir, epoch,\r\n                                                                                      recall,\r\n                                                                                      precision,\r\n                                                                                      f1)\r\n                    if os.path.exists(net_save_path):\r\n                        shutil.copyfile(net_save_path, best_save_path)\r\n                    else:\r\n                        save_checkpoint(best_save_path, model, optimizer, epoch, logger)\r\n\r\n                    pse_path = glob.glob(config.output_dir + \'/PSENet_*.pth\')\r\n                    for p_path in pse_path:\r\n                        if os.path.exists(p_path):\r\n                            os.remove(p_path)\r\n\r\n                writer.add_scalar(tag=\'Test/recall\', scalar_value=recall, global_step=epoch)\r\n                writer.add_scalar(tag=\'Test/precision\', scalar_value=precision, global_step=epoch)\r\n                writer.add_scalar(tag=\'Test/f1\', scalar_value=f1, global_step=epoch)\r\n        writer.close()\r\n    except KeyboardInterrupt:\r\n        save_checkpoint(\'{}/final.pth\'.format(config.output_dir), model, optimizer, epoch, logger)\r\n    finally:\r\n        if best_model[\'models\']:\r\n            logger.info(best_model)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n'"
cal_recall/__init__.py,0,"b""# -*- coding: utf-8 -*-\n# @Time    : 1/16/19 6:40 AM\n# @Author  : zhoujun\nfrom .script import  cal_recall_precison_f1\n__all__ = ['cal_recall_precison_f1']"""
cal_recall/rrc_evaluation_funcs.py,0,"b'#!/usr/bin/env python2\n#encoding: UTF-8\nimport json\nimport sys;sys.path.append(\'./\')\nimport zipfile\nimport re\nimport sys\nimport os\nimport codecs\nimport traceback\nimport importlib\nfrom io import StringIO\n\ndef print_help():\n    sys.stdout.write(\'Usage: python %s.py -g=<gtFile> -s=<submFile> [-o=<outputFolder> -p=<jsonParams>]\' %sys.argv[0])\n    sys.exit(2)\n    \n\ndef load_zip_file_keys(file,fileNameRegExp=\'\'):\n    """"""\n    Returns an array with the entries of the ZIP file that match with the regular expression.\n    The key\'s are the names or the file or the capturing group definied in the fileNameRegExp\n    """"""\n    try:\n        archive=zipfile.ZipFile(file, mode=\'r\', allowZip64=True)\n    except :\n        raise Exception(\'Error loading the ZIP archive.\')\n\n    pairs = []\n    \n    for name in archive.namelist():\n        addFile = True\n        keyName = name\n        if fileNameRegExp!="""":\n            m = re.match(fileNameRegExp,name)\n            if m == None:\n                addFile = False\n            else:\n                if len(m.groups())>0:\n                    keyName = m.group(1)\n                    \n        if addFile:\n            pairs.append( keyName )\n                \n    return pairs\n    \n\ndef load_zip_file(file,fileNameRegExp=\'\',allEntries=False):\n    """"""\n    Returns an array with the contents (filtered by fileNameRegExp) of a ZIP file.\n    The key\'s are the names or the file or the capturing group definied in the fileNameRegExp\n    allEntries validates that all entries in the ZIP file pass the fileNameRegExp\n    """"""\n    try:\n        archive=zipfile.ZipFile(file, mode=\'r\', allowZip64=True)\n    except :\n        raise Exception(\'Error loading the ZIP archive\')    \n\n    pairs = []\n    for name in archive.namelist():\n        addFile = True\n        keyName = name\n        if fileNameRegExp!="""":\n            m = re.match(fileNameRegExp,name)\n            if m == None:\n                addFile = False\n            else:\n                if len(m.groups())>0:\n                    keyName = m.group(1)\n        \n        if addFile:\n            pairs.append( [ keyName , archive.read(name)] )\n        else:\n            if allEntries:\n                raise Exception(\'ZIP entry not valid: %s\' %name)             \n\n    return dict(pairs)\n\n\ndef load_folder_file(file, fileNameRegExp=\'\', allEntries=False):\n    """"""\n    Returns an array with the contents (filtered by fileNameRegExp) of a ZIP file.\n    The key\'s are the names or the file or the capturing group definied in the fileNameRegExp\n    allEntries validates that all entries in the ZIP file pass the fileNameRegExp\n    """"""\n    pairs = []\n    for name in os.listdir(file):\n        addFile = True\n        keyName = name\n        if fileNameRegExp != """":\n            m = re.match(fileNameRegExp, name)\n            if m == None:\n                addFile = False\n            else:\n                if len(m.groups()) > 0:\n                    keyName = m.group(1)\n\n        if addFile:\n            pairs.append([keyName, open(os.path.join(file,name)).read()])\n        else:\n            if allEntries:\n                raise Exception(\'ZIP entry not valid: %s\' % name)\n\n    return dict(pairs)\n\n\ndef decode_utf8(raw):\n    """"""\n    Returns a Unicode object on success, or None on failure\n    """"""\n    try:\n        raw = codecs.decode(raw,\'utf-8\', \'replace\')\n        #extracts BOM if exists\n        raw = raw.encode(\'utf8\')\n        if raw.startswith(codecs.BOM_UTF8):\n            raw = raw.replace(codecs.BOM_UTF8, \'\', 1)\n        return raw.decode(\'utf-8\')\n    except:\n       return None\n   \ndef validate_lines_in_file(fileName,file_contents,CRLF=True,LTRB=True,withTranscription=False,withConfidence=False,imWidth=0,imHeight=0):\n    """"""\n    This function validates that all lines of the file calling the Line validation function for each line\n    """"""\n    utf8File = decode_utf8(file_contents)\n    if (utf8File is None) :\n        raise Exception(""The file %s is not UTF-8"" %fileName)\n\n    lines = utf8File.split( ""\\r\\n"" if CRLF else ""\\n"" )\n    for line in lines:\n        line = line.replace(""\\r"","""").replace(""\\n"","""")\n        if(line != """"):\n            try:\n                validate_tl_line(line,LTRB,withTranscription,withConfidence,imWidth,imHeight)\n            except Exception as e:\n                raise Exception((""Line in sample not valid. Sample: %s Line: %s Error: %s"" %(fileName,line,str(e))).encode(\'utf-8\', \'replace\'))\n    \n   \n   \ndef validate_tl_line(line,LTRB=True,withTranscription=True,withConfidence=True,imWidth=0,imHeight=0):\n    """"""\n    Validate the format of the line. If the line is not valid an exception will be raised.\n    If maxWidth and maxHeight are specified, all points must be inside the imgage bounds.\n    Posible values are:\n    LTRB=True: xmin,ymin,xmax,ymax[,confidence][,transcription] \n    LTRB=False: x1,y1,x2,y2,x3,y3,x4,y4[,confidence][,transcription] \n    """"""\n    get_tl_line_values(line,LTRB,withTranscription,withConfidence,imWidth,imHeight)\n    \n   \ndef get_tl_line_values(line,LTRB=True,withTranscription=False,withConfidence=False,imWidth=0,imHeight=0):\n    """"""\n    Validate the format of the line. If the line is not valid an exception will be raised.\n    If maxWidth and maxHeight are specified, all points must be inside the imgage bounds.\n    Posible values are:\n    LTRB=True: xmin,ymin,xmax,ymax[,confidence][,transcription] \n    LTRB=False: x1,y1,x2,y2,x3,y3,x4,y4[,confidence][,transcription] \n    Returns values from a textline. Points , [Confidences], [Transcriptions]\n    """"""\n    confidence = 0.0\n    transcription = """";\n    points = []\n    \n    numPoints = 4;\n    \n    if LTRB:\n    \n        numPoints = 4;\n        \n        if withTranscription and withConfidence:\n            m = re.match(r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-1].?[0-9]*)\\s*,(.*)$\',line)\n            if m == None :\n                m = re.match(r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-1].?[0-9]*)\\s*,(.*)$\',line)\n                raise Exception(""Format incorrect. Should be: xmin,ymin,xmax,ymax,confidence,transcription"")\n        elif withConfidence:\n            m = re.match(r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-1].?[0-9]*)\\s*$\',line)\n            if m == None :\n                raise Exception(""Format incorrect. Should be: xmin,ymin,xmax,ymax,confidence"")\n        elif withTranscription:\n            m = re.match(r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,(.*)$\',line)\n            if m == None :\n                raise Exception(""Format incorrect. Should be: xmin,ymin,xmax,ymax,transcription"")\n        else:\n            m = re.match(r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,?\\s*$\',line)\n            if m == None :\n                raise Exception(""Format incorrect. Should be: xmin,ymin,xmax,ymax"")\n            \n        xmin = int(m.group(1))\n        ymin = int(m.group(2))\n        xmax = int(m.group(3))\n        ymax = int(m.group(4))\n        if(xmax<xmin):\n                raise Exception(""Xmax value (%s) not valid (Xmax < Xmin)."" %(xmax))\n        if(ymax<ymin):\n                raise Exception(""Ymax value (%s)  not valid (Ymax < Ymin)."" %(ymax))  \n\n        points = [ float(m.group(i)) for i in range(1, (numPoints+1) ) ]\n        \n        if (imWidth>0 and imHeight>0):\n            validate_point_inside_bounds(xmin,ymin,imWidth,imHeight);\n            validate_point_inside_bounds(xmax,ymax,imWidth,imHeight);\n\n    else:\n        \n        numPoints = 8;\n        \n        if withTranscription and withConfidence:\n            m = re.match(r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-1].?[0-9]*)\\s*,(.*)$\',line)\n            if m == None :\n                raise Exception(""Format incorrect. Should be: x1,y1,x2,y2,x3,y3,x4,y4,confidence,transcription"")\n        elif withConfidence:\n            m = re.match(r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-1].?[0-9]*)\\s*$\',line)\n            if m == None :\n                raise Exception(""Format incorrect. Should be: x1,y1,x2,y2,x3,y3,x4,y4,confidence"")\n        elif withTranscription:\n            m = re.match(r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,(.*)$\',line)\n            if m == None :\n                raise Exception(""Format incorrect. Should be: x1,y1,x2,y2,x3,y3,x4,y4,transcription"")\n        else:\n            m = re.match(r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*$\',line)\n            if m == None :\n                raise Exception(""Format incorrect. Should be: x1,y1,x2,y2,x3,y3,x4,y4"")\n            \n        points = [ float(m.group(i)) for i in range(1, (numPoints+1) ) ]\n        \n        validate_clockwise_points(points)\n        \n        if (imWidth>0 and imHeight>0):\n            validate_point_inside_bounds(points[0],points[1],imWidth,imHeight);\n            validate_point_inside_bounds(points[2],points[3],imWidth,imHeight);\n            validate_point_inside_bounds(points[4],points[5],imWidth,imHeight);\n            validate_point_inside_bounds(points[6],points[7],imWidth,imHeight);\n            \n    \n    if withConfidence:\n        try:\n            confidence = float(m.group(numPoints+1))\n        except ValueError:\n            raise Exception(""Confidence value must be a float"")       \n            \n    if withTranscription:\n        posTranscription = numPoints + (2 if withConfidence else 1)\n        transcription = m.group(posTranscription)\n        m2 = re.match(r\'^\\s*\\""(.*)\\""\\s*$\',transcription)\n        if m2 != None : #Transcription with double quotes, we extract the value and replace escaped characters\n            transcription = m2.group(1).replace(""\\\\\\\\"", ""\\\\"").replace(""\\\\\\"""", ""\\"""")\n    \n    return points,confidence,transcription\n    \n            \ndef validate_point_inside_bounds(x,y,imWidth,imHeight):\n    if(x<0 or x>imWidth):\n            raise Exception(""X value (%s) not valid. Image dimensions: (%s,%s)"" %(xmin,imWidth,imHeight))\n    if(y<0 or y>imHeight):\n            raise Exception(""Y value (%s)  not valid. Image dimensions: (%s,%s) Sample: %s Line:%s"" %(ymin,imWidth,imHeight))\n\ndef validate_clockwise_points(points):\n    """"""\n    Validates that the points that the 4 points that dlimite a polygon are in clockwise order.\n    """"""\n    \n    if len(points) != 8:\n        raise Exception(""Points list not valid."" + str(len(points)))\n    \n    point = [\n                [int(points[0]) , int(points[1])],\n                [int(points[2]) , int(points[3])],\n                [int(points[4]) , int(points[5])],\n                [int(points[6]) , int(points[7])]\n            ]\n    edge = [\n                ( point[1][0] - point[0][0])*( point[1][1] + point[0][1]),\n                ( point[2][0] - point[1][0])*( point[2][1] + point[1][1]),\n                ( point[3][0] - point[2][0])*( point[3][1] + point[2][1]),\n                ( point[0][0] - point[3][0])*( point[0][1] + point[3][1])\n    ]\n    \n    summatory = edge[0] + edge[1] + edge[2] + edge[3];\n    if summatory>0:\n        raise Exception(""Points are not clockwise. The coordinates of bounding quadrilaterals have to be given in clockwise order. Regarding the correct interpretation of \'clockwise\' remember that the image coordinate system used is the standard one, with the image origin at the upper left, the X axis extending to the right and Y axis extending downwards."")\n\ndef get_tl_line_values_from_file_contents(content,CRLF=True,LTRB=True,withTranscription=False,withConfidence=False,imWidth=0,imHeight=0,sort_by_confidences=True):\n    """"""\n    Returns all points, confindences and transcriptions of a file in lists. Valid line formats:\n    xmin,ymin,xmax,ymax,[confidence],[transcription]\n    x1,y1,x2,y2,x3,y3,x4,y4,[confidence],[transcription]\n    """"""\n    pointsList = []\n    transcriptionsList = []\n    confidencesList = []\n    \n    lines = content.split( ""\\r\\n"" if CRLF else ""\\n"" )\n    for line in lines:\n        line = line.replace(""\\r"","""").replace(""\\n"","""")\n        if(line != """") :\n            points, confidence, transcription = get_tl_line_values(line,LTRB,withTranscription,withConfidence,imWidth,imHeight);\n            pointsList.append(points)\n            transcriptionsList.append(transcription)\n            confidencesList.append(confidence)\n\n    if withConfidence and len(confidencesList)>0 and sort_by_confidences:\n        import numpy as np\n        sorted_ind = np.argsort(-np.array(confidencesList))\n        confidencesList = [confidencesList[i] for i in sorted_ind]\n        pointsList = [pointsList[i] for i in sorted_ind]\n        transcriptionsList = [transcriptionsList[i] for i in sorted_ind]        \n        \n    return pointsList,confidencesList,transcriptionsList\n\ndef main_evaluation(p,default_evaluation_params_fn,validate_data_fn,evaluate_method_fn,show_result=True,per_sample=True):\n    """"""\n    This process validates a method, evaluates it and if it succed generates a ZIP file with a JSON entry for each sample.\n    Params:\n    p: Dictionary of parmeters with the GT/submission locations. If None is passed, the parameters send by the system are used.\n    default_evaluation_params_fn: points to a function that returns a dictionary with the default parameters used for the evaluation\n    validate_data_fn: points to a method that validates the corrct format of the submission\n    evaluate_method_fn: points to a function that evaluated the submission and return a Dictionary with the results\n    """"""\n    evalParams = default_evaluation_params_fn()\n    if \'p\' in p.keys():\n        evalParams.update( p[\'p\'] if isinstance(p[\'p\'], dict) else json.loads(p[\'p\'][1:-1]) )\n\n    resDict={\'calculated\':True,\'Message\':\'\',\'method\':\'{}\',\'per_sample\':\'{}\'}    \n    try:\n        # validate_data_fn(p[\'g\'], p[\'s\'], evalParams)\n        evalData = evaluate_method_fn(p[\'g\'], p[\'s\'], evalParams)\n        resDict.update(evalData)\n        \n    except Exception as e:\n        traceback.print_exc()\n        resDict[\'Message\']= str(e)\n        resDict[\'calculated\']=False\n\n    if \'o\' in p:\n        if not os.path.exists(p[\'o\']):\n            os.makedirs(p[\'o\'])\n\n        resultsOutputname = p[\'o\'] + \'/results.zip\'\n        outZip = zipfile.ZipFile(resultsOutputname, mode=\'w\', allowZip64=True)\n\n        del resDict[\'per_sample\']\n        if \'output_items\' in resDict.keys():\n            del resDict[\'output_items\']\n\n        outZip.writestr(\'method.json\',json.dumps(resDict))\n        \n    if not resDict[\'calculated\']:\n        if show_result:\n            sys.stderr.write(\'Error!\\n\'+ resDict[\'Message\']+\'\\n\\n\')\n        if \'o\' in p:\n            outZip.close()\n        return resDict\n    \n    if \'o\' in p:\n        if per_sample == True:\n            for k,v in evalData[\'per_sample\'].iteritems():\n                outZip.writestr( k + \'.json\',json.dumps(v)) \n\n            if \'output_items\' in evalData.keys():\n                for k, v in evalData[\'output_items\'].iteritems():\n                    outZip.writestr( k,v) \n\n        outZip.close()\n\n    if show_result:\n        sys.stdout.write(""Calculated!"")\n        sys.stdout.write(json.dumps(resDict[\'method\']))\n    \n    return resDict\n\n\ndef main_validation(default_evaluation_params_fn,validate_data_fn):\n    """"""\n    This process validates a method\n    Params:\n    default_evaluation_params_fn: points to a function that returns a dictionary with the default parameters used for the evaluation\n    validate_data_fn: points to a method that validates the corrct format of the submission\n    """"""    \n    try:\n        p = dict([s[1:].split(\'=\') for s in sys.argv[1:]])\n        evalParams = default_evaluation_params_fn()\n        if \'p\' in p.keys():\n            evalParams.update( p[\'p\'] if isinstance(p[\'p\'], dict) else json.loads(p[\'p\'][1:-1]) )\n\n        validate_data_fn(p[\'g\'], p[\'s\'], evalParams)              \n        print(\'SUCCESS\')\n        sys.exit(0)\n    except Exception as e:\n        print(str(e))\n        sys.exit(101)'"
cal_recall/script.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom collections import namedtuple\nfrom . import rrc_evaluation_funcs\nimport Polygon as plg\nimport numpy as np\n\n\ndef default_evaluation_params():\n    """"""\n    default_evaluation_params: Default parameters to use for the validation and evaluation.\n    """"""\n    return {\n        \'IOU_CONSTRAINT\': 0.5,\n        \'AREA_PRECISION_CONSTRAINT\': 0.5,\n        \'GT_SAMPLE_NAME_2_ID\': \'gt_img_([0-9]+).txt\',\n        \'DET_SAMPLE_NAME_2_ID\': \'res_img_([0-9]+).txt\',\n        \'LTRB\': False,  # LTRB:2points(left,top,right,bottom) or 4 points(x1,y1,x2,y2,x3,y3,x4,y4)\n        \'CRLF\': False,  # Lines are delimited by Windows CRLF format\n        \'CONFIDENCES\': False,  # Detections must include confidence value. AP will be calculated\n        \'PER_SAMPLE_RESULTS\': True  # Generate per sample results and produce data for visualization\n    }\n\n\ndef validate_data(gtFilePath, submFilePath, evaluationParams):\n    """"""\n    Method validate_data: validates that all files in the results folder are correct (have the correct name contents).\n                            Validates also that there are no missing files in the folder.\n                            If some error detected, the method raises the error\n    """"""\n    gt = rrc_evaluation_funcs.load_folder_file(gtFilePath, evaluationParams[\'GT_SAMPLE_NAME_2_ID\'])\n\n    subm = rrc_evaluation_funcs.load_folder_file(submFilePath, evaluationParams[\'DET_SAMPLE_NAME_2_ID\'], True)\n\n    # Validate format of GroundTruth\n    for k in gt:\n        rrc_evaluation_funcs.validate_lines_in_file(k, gt[k], evaluationParams[\'CRLF\'], evaluationParams[\'LTRB\'], True)\n\n    # Validate format of results\n    for k in subm:\n        if (k in gt) == False:\n            raise Exception(""The sample %s not present in GT"" % k)\n\n        rrc_evaluation_funcs.validate_lines_in_file(k, subm[k], evaluationParams[\'CRLF\'], evaluationParams[\'LTRB\'],\n                                                    False, evaluationParams[\'CONFIDENCES\'])\n\n\ndef evaluate_method(gtFilePath, submFilePath, evaluationParams):\n    """"""\n    Method evaluate_method: evaluate method and returns the results\n        Results. Dictionary with the following values:\n        - method (required)  Global method metrics. Ex: { \'Precision\':0.8,\'Recall\':0.9 }\n        - samples (optional) Per sample metrics. Ex: {\'sample1\' : { \'Precision\':0.8,\'Recall\':0.9 } , \'sample2\' : { \'Precision\':0.8,\'Recall\':0.9 }\n    """"""\n\n    def polygon_from_points(points):\n        """"""\n        Returns a Polygon object to use with the Polygon2 class from a list of 8 points: x1,y1,x2,y2,x3,y3,x4,y4\n        """"""\n        resBoxes = np.empty([1, 8], dtype=\'int32\')\n        resBoxes[0, 0] = int(points[0])\n        resBoxes[0, 4] = int(points[1])\n        resBoxes[0, 1] = int(points[2])\n        resBoxes[0, 5] = int(points[3])\n        resBoxes[0, 2] = int(points[4])\n        resBoxes[0, 6] = int(points[5])\n        resBoxes[0, 3] = int(points[6])\n        resBoxes[0, 7] = int(points[7])\n        pointMat = resBoxes[0].reshape([2, 4]).T\n        return plg.Polygon(pointMat)\n\n    def rectangle_to_polygon(rect):\n        resBoxes = np.empty([1, 8], dtype=\'int32\')\n        resBoxes[0, 0] = int(rect.xmin)\n        resBoxes[0, 4] = int(rect.ymax)\n        resBoxes[0, 1] = int(rect.xmin)\n        resBoxes[0, 5] = int(rect.ymin)\n        resBoxes[0, 2] = int(rect.xmax)\n        resBoxes[0, 6] = int(rect.ymin)\n        resBoxes[0, 3] = int(rect.xmax)\n        resBoxes[0, 7] = int(rect.ymax)\n\n        pointMat = resBoxes[0].reshape([2, 4]).T\n\n        return plg.Polygon(pointMat)\n\n    def rectangle_to_points(rect):\n        points = [int(rect.xmin), int(rect.ymax), int(rect.xmax), int(rect.ymax), int(rect.xmax), int(rect.ymin),\n                  int(rect.xmin), int(rect.ymin)]\n        return points\n\n    def get_union(pD, pG):\n        areaA = pD.area();\n        areaB = pG.area();\n        return areaA + areaB - get_intersection(pD, pG);\n\n    def get_intersection_over_union(pD, pG):\n        try:\n            return get_intersection(pD, pG) / get_union(pD, pG);\n        except:\n            return 0\n\n    def get_intersection(pD, pG):\n        pInt = pD & pG\n        if len(pInt) == 0:\n            return 0\n        return pInt.area()\n\n    def compute_ap(confList, matchList, numGtCare):\n        correct = 0\n        AP = 0\n        if len(confList) > 0:\n            confList = np.array(confList)\n            matchList = np.array(matchList)\n            sorted_ind = np.argsort(-confList)\n            confList = confList[sorted_ind]\n            matchList = matchList[sorted_ind]\n            for n in range(len(confList)):\n                match = matchList[n]\n                if match:\n                    correct += 1\n                    AP += float(correct) / (n + 1)\n\n            if numGtCare > 0:\n                AP /= numGtCare\n\n        return AP\n\n    perSampleMetrics = {}\n\n    matchedSum = 0\n\n    Rectangle = namedtuple(\'Rectangle\', \'xmin ymin xmax ymax\')\n\n    gt = rrc_evaluation_funcs.load_folder_file(gtFilePath, evaluationParams[\'GT_SAMPLE_NAME_2_ID\'])\n    subm = rrc_evaluation_funcs.load_folder_file(submFilePath, evaluationParams[\'DET_SAMPLE_NAME_2_ID\'], True)\n\n    numGlobalCareGt = 0;\n    numGlobalCareDet = 0;\n\n    arrGlobalConfidences = [];\n    arrGlobalMatches = [];\n\n    for resFile in gt:\n\n        gtFile = gt[resFile]  # rrc_evaluation_funcs.decode_utf8(gt[resFile])\n        recall = 0\n        precision = 0\n        hmean = 0\n\n        detMatched = 0\n\n        iouMat = np.empty([1, 1])\n\n        gtPols = []\n        detPols = []\n\n        gtPolPoints = []\n        detPolPoints = []\n\n        # Array of Ground Truth Polygons\' keys marked as don\'t Care\n        gtDontCarePolsNum = []\n        # Array of Detected Polygons\' matched with a don\'t Care GT\n        detDontCarePolsNum = []\n\n        pairs = []\n        detMatchedNums = []\n\n        arrSampleConfidences = [];\n        arrSampleMatch = [];\n        sampleAP = 0;\n\n        evaluationLog = """"\n\n        pointsList, _, transcriptionsList = rrc_evaluation_funcs.get_tl_line_values_from_file_contents(gtFile,\n                                                                                                       evaluationParams[\n                                                                                                           \'CRLF\'],\n                                                                                                       evaluationParams[\n                                                                                                           \'LTRB\'],\n                                                                                                       True, False)\n        for n in range(len(pointsList)):\n            points = pointsList[n]\n            transcription = transcriptionsList[n]\n            dontCare = transcription == ""###""\n            if evaluationParams[\'LTRB\']:\n                gtRect = Rectangle(*points)\n                gtPol = rectangle_to_polygon(gtRect)\n            else:\n                gtPol = polygon_from_points(points)\n            gtPols.append(gtPol)\n            gtPolPoints.append(points)\n            if dontCare:\n                gtDontCarePolsNum.append(len(gtPols) - 1)\n\n        evaluationLog += ""GT polygons: "" + str(len(gtPols)) + (\n            "" ("" + str(len(gtDontCarePolsNum)) + "" don\'t care)\\n"" if len(gtDontCarePolsNum) > 0 else ""\\n"")\n\n        if resFile in subm:\n\n            detFile = subm[resFile]  # rrc_evaluation_funcs.decode_utf8(subm[resFile])\n\n            pointsList, confidencesList, _ = rrc_evaluation_funcs.get_tl_line_values_from_file_contents(detFile,\n                                                                                                        evaluationParams[\n                                                                                                            \'CRLF\'],\n                                                                                                        evaluationParams[\n                                                                                                            \'LTRB\'],\n                                                                                                        False,\n                                                                                                        evaluationParams[\n                                                                                                            \'CONFIDENCES\'])\n            for n in range(len(pointsList)):\n                points = pointsList[n]\n\n                if evaluationParams[\'LTRB\']:\n                    detRect = Rectangle(*points)\n                    detPol = rectangle_to_polygon(detRect)\n                else:\n                    detPol = polygon_from_points(points)\n                detPols.append(detPol)\n                detPolPoints.append(points)\n                if len(gtDontCarePolsNum) > 0:\n                    for dontCarePol in gtDontCarePolsNum:\n                        dontCarePol = gtPols[dontCarePol]\n                        intersected_area = get_intersection(dontCarePol, detPol)\n                        pdDimensions = detPol.area()\n                        precision = 0 if pdDimensions == 0 else intersected_area / pdDimensions\n                        if (precision > evaluationParams[\'AREA_PRECISION_CONSTRAINT\']):\n                            detDontCarePolsNum.append(len(detPols) - 1)\n                            break\n\n            evaluationLog += ""DET polygons: "" + str(len(detPols)) + (\n                "" ("" + str(len(detDontCarePolsNum)) + "" don\'t care)\\n"" if len(detDontCarePolsNum) > 0 else ""\\n"")\n\n            if len(gtPols) > 0 and len(detPols) > 0:\n                # Calculate IoU and precision matrixs\n                outputShape = [len(gtPols), len(detPols)]\n                iouMat = np.empty(outputShape)\n                gtRectMat = np.zeros(len(gtPols), np.int8)\n                detRectMat = np.zeros(len(detPols), np.int8)\n                for gtNum in range(len(gtPols)):\n                    for detNum in range(len(detPols)):\n                        pG = gtPols[gtNum]\n                        pD = detPols[detNum]\n                        iouMat[gtNum, detNum] = get_intersection_over_union(pD, pG)\n\n                for gtNum in range(len(gtPols)):\n                    for detNum in range(len(detPols)):\n                        if gtRectMat[gtNum] == 0 and detRectMat[\n                            detNum] == 0 and gtNum not in gtDontCarePolsNum and detNum not in detDontCarePolsNum:\n                            if iouMat[gtNum, detNum] > evaluationParams[\'IOU_CONSTRAINT\']:\n                                gtRectMat[gtNum] = 1\n                                detRectMat[detNum] = 1\n                                detMatched += 1\n                                pairs.append({\'gt\': gtNum, \'det\': detNum})\n                                detMatchedNums.append(detNum)\n                                evaluationLog += ""Match GT #"" + str(gtNum) + "" with Det #"" + str(detNum) + ""\\n""\n\n            if evaluationParams[\'CONFIDENCES\']:\n                for detNum in range(len(detPols)):\n                    if detNum not in detDontCarePolsNum:\n                        # we exclude the don\'t care detections\n                        match = detNum in detMatchedNums\n\n                        arrSampleConfidences.append(confidencesList[detNum])\n                        arrSampleMatch.append(match)\n\n                        arrGlobalConfidences.append(confidencesList[detNum]);\n                        arrGlobalMatches.append(match);\n\n        numGtCare = (len(gtPols) - len(gtDontCarePolsNum))\n        numDetCare = (len(detPols) - len(detDontCarePolsNum))\n        if numGtCare == 0:\n            recall = float(1)\n            precision = float(0) if numDetCare > 0 else float(1)\n            sampleAP = precision\n        else:\n            recall = float(detMatched) / numGtCare\n            precision = 0 if numDetCare == 0 else float(detMatched) / numDetCare\n            if evaluationParams[\'CONFIDENCES\'] and evaluationParams[\'PER_SAMPLE_RESULTS\']:\n                sampleAP = compute_ap(arrSampleConfidences, arrSampleMatch, numGtCare)\n\n        hmean = 0 if (precision + recall) == 0 else 2.0 * precision * recall / (precision + recall)\n\n        matchedSum += detMatched\n        numGlobalCareGt += numGtCare\n        numGlobalCareDet += numDetCare\n\n        if evaluationParams[\'PER_SAMPLE_RESULTS\']:\n            perSampleMetrics[resFile] = {\n                \'precision\': precision,\n                \'recall\': recall,\n                \'hmean\': hmean,\n                \'pairs\': pairs,\n                \'AP\': sampleAP,\n                \'iouMat\': [] if len(detPols) > 100 else iouMat.tolist(),\n                \'gtPolPoints\': gtPolPoints,\n                \'detPolPoints\': detPolPoints,\n                \'gtDontCare\': gtDontCarePolsNum,\n                \'detDontCare\': detDontCarePolsNum,\n                \'evaluationParams\': evaluationParams,\n                \'evaluationLog\': evaluationLog\n            }\n\n    # Compute MAP and MAR\n    AP = 0\n    if evaluationParams[\'CONFIDENCES\']:\n        AP = compute_ap(arrGlobalConfidences, arrGlobalMatches, numGlobalCareGt)\n\n    methodRecall = 0 if numGlobalCareGt == 0 else float(matchedSum) / numGlobalCareGt\n    methodPrecision = 0 if numGlobalCareDet == 0 else float(matchedSum) / numGlobalCareDet\n    methodHmean = 0 if methodRecall + methodPrecision == 0 else 2 * methodRecall * methodPrecision / (\n            methodRecall + methodPrecision)\n\n    methodMetrics = {\'precision\': methodPrecision, \'recall\': methodRecall, \'hmean\': methodHmean, \'AP\': AP}\n\n    resDict = {\'calculated\': True, \'Message\': \'\', \'method\': methodMetrics, \'per_sample\': perSampleMetrics}\n\n    return resDict;\n\n\ndef cal_recall_precison_f1(gt_path, result_path, show_result=False):\n    p = {\'g\': gt_path, \'s\': result_path}\n    result = rrc_evaluation_funcs.main_evaluation(p, default_evaluation_params, validate_data, evaluate_method,\n                                                  show_result)\n    return result[\'method\']\n'"
dataset/__init__.py,0,b'# -*- coding: utf-8 -*-\n# @Time    : 1/17/19 2:09 AM\n# @Author  : zhoujun'
dataset/augment.py,0,"b'# -*- coding: utf-8 -*-\r\n# @Time    : 2019/1/12 13:06\r\n\r\nimport cv2\r\nimport numbers\r\nimport math\r\nimport random\r\nimport numpy as np\r\nfrom skimage.util import random_noise\r\n\r\n\r\ndef show_pic(img, bboxes=None, name=\'pic\'):\r\n    \'\'\'\r\n    \xe8\xbe\x93\xe5\x85\xa5:\r\n        img:\xe5\x9b\xbe\xe5\x83\x8farray\r\n        bboxes:\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89boudning box list, \xe6\xa0\xbc\xe5\xbc\x8f\xe4\xb8\xba[[x_min, y_min, x_max, y_max]....]\r\n        names:\xe6\xaf\x8f\xe4\xb8\xaabox\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe5\x90\x8d\xe7\xa7\xb0\r\n    \'\'\'\r\n    show_img = img.copy()\r\n    if not isinstance(bboxes, np.ndarray):\r\n        bboxes = np.array(bboxes)\r\n    for point in bboxes.astype(np.int):\r\n        cv2.line(show_img, tuple(point[0]), tuple(point[1]), (255, 0, 0), 2)\r\n        cv2.line(show_img, tuple(point[1]), tuple(point[2]), (255, 0, 0), 2)\r\n        cv2.line(show_img, tuple(point[2]), tuple(point[3]), (255, 0, 0), 2)\r\n        cv2.line(show_img, tuple(point[3]), tuple(point[0]), (255, 0, 0), 2)\r\n    # cv2.namedWindow(name, 0)  # 1\xe8\xa1\xa8\xe7\xa4\xba\xe5\x8e\x9f\xe5\x9b\xbe\r\n    # cv2.moveWindow(name, 0, 0)\r\n    # cv2.resizeWindow(name, 1200, 800)  # \xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe5\xa4\xa7\xe5\xb0\x8f\r\n    cv2.imshow(name, show_img)\r\n\r\n\r\n# \xe5\x9b\xbe\xe5\x83\x8f\xe5\x9d\x87\xe4\xb8\xbacv2\xe8\xaf\xbb\xe5\x8f\x96\r\nclass DataAugment():\r\n    def __init__(self):\r\n        pass\r\n\r\n    def add_noise(self, im: np.ndarray):\r\n        """"""\r\n        \xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe5\x8a\xa0\xe5\x99\xaa\xe5\xa3\xb0\r\n        :param img: \xe5\x9b\xbe\xe5\x83\x8farray\r\n        :return: \xe5\x8a\xa0\xe5\x99\xaa\xe5\xa3\xb0\xe5\x90\x8e\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8farray,\xe7\x94\xb1\xe4\xba\x8e\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe5\x83\x8f\xe7\xb4\xa0\xe6\x98\xaf\xe5\x9c\xa8[0,1]\xe4\xb9\x8b\xe9\x97\xb4,\xe6\x89\x80\xe4\xbb\xa5\xe5\xbe\x97\xe4\xb9\x98\xe4\xbb\xa5255\r\n        """"""\r\n        return (random_noise(im, mode=\'gaussian\', clip=True) * 255).astype(im.dtype)\r\n\r\n    def random_scale(self, im: np.ndarray, text_polys: np.ndarray, scales: np.ndarray or list) -> tuple:\r\n        """"""\r\n        \xe4\xbb\x8escales\xe4\xb8\xad\xe9\x9a\x8f\xe6\x9c\xba\xe9\x80\x89\xe6\x8b\xa9\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb0\xba\xe5\xba\xa6\xef\xbc\x8c\xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe7\xbc\xa9\xe6\x94\xbe\r\n        :param im: \xe5\x8e\x9f\xe5\x9b\xbe\r\n        :param text_polys: \xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\r\n        :param scales: \xe5\xb0\xba\xe5\xba\xa6\r\n        :return: \xe7\xbb\x8f\xe8\xbf\x87\xe7\xbc\xa9\xe6\x94\xbe\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe6\x96\x87\xe6\x9c\xac\r\n        """"""\r\n        tmp_text_polys = text_polys.copy()\r\n        rd_scale = float(np.random.choice(scales))\r\n        im = cv2.resize(im, dsize=None, fx=rd_scale, fy=rd_scale)\r\n        tmp_text_polys *= rd_scale\r\n        return im, tmp_text_polys\r\n\r\n    def random_rotate_img_bbox(self, img, text_polys, degrees: numbers.Number or list or tuple or np.ndarray,\r\n                               same_size=False):\r\n        """"""\r\n        \xe4\xbb\x8e\xe7\xbb\x99\xe5\xae\x9a\xe7\x9a\x84\xe8\xa7\x92\xe5\xba\xa6\xe4\xb8\xad\xe9\x80\x89\xe6\x8b\xa9\xe4\xb8\x80\xe4\xb8\xaa\xe8\xa7\x92\xe5\xba\xa6\xef\xbc\x8c\xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x97\x8b\xe8\xbd\xac\r\n        :param img: \xe5\x9b\xbe\xe7\x89\x87\r\n        :param text_polys: \xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\r\n        :param degrees: \xe8\xa7\x92\xe5\xba\xa6\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe5\x80\xbc\xe6\x88\x96\xe8\x80\x85list\r\n        :param same_size: \xe6\x98\xaf\xe5\x90\xa6\xe4\xbf\x9d\xe6\x8c\x81\xe5\x92\x8c\xe5\x8e\x9f\xe5\x9b\xbe\xe4\xb8\x80\xe6\xa0\xb7\xe5\xa4\xa7\r\n        :return: \xe6\x97\x8b\xe8\xbd\xac\xe5\x90\x8e\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe8\xa7\x92\xe5\xba\xa6\r\n        """"""\r\n        if isinstance(degrees, numbers.Number):\r\n            if degrees < 0:\r\n                raise ValueError(""If degrees is a single number, it must be positive."")\r\n            degrees = (-degrees, degrees)\r\n        elif isinstance(degrees, list) or isinstance(degrees, tuple) or isinstance(degrees, np.ndarray):\r\n            if len(degrees) != 2:\r\n                raise ValueError(""If degrees is a sequence, it must be of len 2."")\r\n            degrees = degrees\r\n        else:\r\n            raise Exception(\'degrees must in Number or list or tuple or np.ndarray\')\r\n        # ---------------------- \xe6\x97\x8b\xe8\xbd\xac\xe5\x9b\xbe\xe5\x83\x8f ----------------------\r\n        w = img.shape[1]\r\n        h = img.shape[0]\r\n        angle = np.random.uniform(degrees[0], degrees[1])\r\n\r\n        if same_size:\r\n            nw = w\r\n            nh = h\r\n        else:\r\n            # \xe8\xa7\x92\xe5\xba\xa6\xe5\x8f\x98\xe5\xbc\xa7\xe5\xba\xa6\r\n            rangle = np.deg2rad(angle)\r\n            # \xe8\xae\xa1\xe7\xae\x97\xe6\x97\x8b\xe8\xbd\xac\xe4\xb9\x8b\xe5\x90\x8e\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84w, h\r\n            nw = (abs(np.sin(rangle) * h) + abs(np.cos(rangle) * w))\r\n            nh = (abs(np.cos(rangle) * h) + abs(np.sin(rangle) * w))\r\n        # \xe6\x9e\x84\xe9\x80\xa0\xe4\xbb\xbf\xe5\xb0\x84\xe7\x9f\xa9\xe9\x98\xb5\r\n        rot_mat = cv2.getRotationMatrix2D((nw * 0.5, nh * 0.5), angle, 1)\r\n        # \xe8\xae\xa1\xe7\xae\x97\xe5\x8e\x9f\xe5\x9b\xbe\xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\xe5\x88\xb0\xe6\x96\xb0\xe5\x9b\xbe\xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\xe7\x9a\x84\xe5\x81\x8f\xe7\xa7\xbb\xe9\x87\x8f\r\n        rot_move = np.dot(rot_mat, np.array([(nw - w) * 0.5, (nh - h) * 0.5, 0]))\r\n        # \xe6\x9b\xb4\xe6\x96\xb0\xe4\xbb\xbf\xe5\xb0\x84\xe7\x9f\xa9\xe9\x98\xb5\r\n        rot_mat[0, 2] += rot_move[0]\r\n        rot_mat[1, 2] += rot_move[1]\r\n        # \xe4\xbb\xbf\xe5\xb0\x84\xe5\x8f\x98\xe6\x8d\xa2\r\n        rot_img = cv2.warpAffine(img, rot_mat, (int(math.ceil(nw)), int(math.ceil(nh))), flags=cv2.INTER_LANCZOS4)\r\n\r\n        # ---------------------- \xe7\x9f\xab\xe6\xad\xa3bbox\xe5\x9d\x90\xe6\xa0\x87 ----------------------\r\n        # rot_mat\xe6\x98\xaf\xe6\x9c\x80\xe7\xbb\x88\xe7\x9a\x84\xe6\x97\x8b\xe8\xbd\xac\xe7\x9f\xa9\xe9\x98\xb5\r\n        # \xe8\x8e\xb7\xe5\x8f\x96\xe5\x8e\x9f\xe5\xa7\x8bbbox\xe7\x9a\x84\xe5\x9b\x9b\xe4\xb8\xaa\xe4\xb8\xad\xe7\x82\xb9\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe5\xb0\x86\xe8\xbf\x99\xe5\x9b\x9b\xe4\xb8\xaa\xe7\x82\xb9\xe8\xbd\xac\xe6\x8d\xa2\xe5\x88\xb0\xe6\x97\x8b\xe8\xbd\xac\xe5\x90\x8e\xe7\x9a\x84\xe5\x9d\x90\xe6\xa0\x87\xe7\xb3\xbb\xe4\xb8\x8b\r\n        rot_text_polys = list()\r\n        for bbox in text_polys:\r\n            point1 = np.dot(rot_mat, np.array([bbox[0, 0], bbox[0, 1], 1]))\r\n            point2 = np.dot(rot_mat, np.array([bbox[1, 0], bbox[1, 1], 1]))\r\n            point3 = np.dot(rot_mat, np.array([bbox[2, 0], bbox[2, 1], 1]))\r\n            point4 = np.dot(rot_mat, np.array([bbox[3, 0], bbox[3, 1], 1]))\r\n            rot_text_polys.append([point1, point2, point3, point4])\r\n        return rot_img, np.array(rot_text_polys, dtype=np.float32)\r\n\r\n    def random_crop_img_bboxes(self, im: np.ndarray, text_polys: np.ndarray, max_tries=50) -> tuple:\r\n        """"""\r\n        \xe4\xbb\x8e\xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\xad\xe8\xa3\x81\xe5\x89\xaa\xe5\x87\xba cropsize\xe5\xa4\xa7\xe5\xb0\x8f\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe5\xaf\xb9\xe5\xba\x94\xe5\x8c\xba\xe5\x9f\x9f\xe7\x9a\x84\xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\r\n        :param im: \xe5\x9b\xbe\xe7\x89\x87\r\n        :param text_polys: \xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\r\n        :param max_tries: \xe6\x9c\x80\xe5\xa4\xa7\xe5\xb0\x9d\xe8\xaf\x95\xe6\xac\xa1\xe6\x95\xb0\r\n        :return: \xe8\xa3\x81\xe5\x89\xaa\xe5\x90\x8e\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\r\n        """"""\r\n        h, w, _ = im.shape\r\n        pad_h = h // 10\r\n        pad_w = w // 10\r\n        h_array = np.zeros((h + pad_h * 2), dtype=np.int32)\r\n        w_array = np.zeros((w + pad_w * 2), dtype=np.int32)\r\n        for poly in text_polys:\r\n            poly = np.round(poly, decimals=0).astype(np.int32)  # \xe5\x9b\x9b\xe8\x88\x8d\xe4\xba\x94\xe5\x85\xa5\xe5\x8f\x96\xe6\x95\xb4\r\n            minx = np.min(poly[:, 0])\r\n            maxx = np.max(poly[:, 0])\r\n            w_array[minx + pad_w:maxx + pad_w] = 1  # \xe5\xb0\x86\xe6\x96\x87\xe6\x9c\xac\xe5\x8c\xba\xe5\x9f\x9f\xe7\x9a\x84\xe5\x9c\xa8w_array\xe4\xb8\x8a\xe8\xae\xbe\xe4\xb8\xba1\xef\xbc\x8c\xe8\xa1\xa8\xe7\xa4\xbax\xe8\xbd\xb4\xe6\x96\xb9\xe5\x90\x91\xe4\xb8\x8a\xe8\xbf\x99\xe9\x83\xa8\xe5\x88\x86\xe4\xbd\x8d\xe7\xbd\xae\xe6\x9c\x89\xe6\x96\x87\xe6\x9c\xac\r\n            miny = np.min(poly[:, 1])\r\n            maxy = np.max(poly[:, 1])\r\n            h_array[miny + pad_h:maxy + pad_h] = 1  # \xe5\xb0\x86\xe6\x96\x87\xe6\x9c\xac\xe5\x8c\xba\xe5\x9f\x9f\xe7\x9a\x84\xe5\x9c\xa8h_array\xe4\xb8\x8a\xe8\xae\xbe\xe4\xb8\xba1\xef\xbc\x8c\xe8\xa1\xa8\xe7\xa4\xbay\xe8\xbd\xb4\xe6\x96\xb9\xe5\x90\x91\xe4\xb8\x8a\xe8\xbf\x99\xe9\x83\xa8\xe5\x88\x86\xe4\xbd\x8d\xe7\xbd\xae\xe6\x9c\x89\xe6\x96\x87\xe6\x9c\xac\r\n        # \xe5\x9c\xa8\xe4\xb8\xa4\xe4\xb8\xaa\xe8\xbd\xb4\xe4\xb8\x8a \xe6\x8b\xbf\xe5\x87\xba\xe8\x83\x8c\xe6\x99\xaf\xe4\xbd\x8d\xe7\xbd\xae\xe5\x8e\xbb\xe8\xbf\x9b\xe8\xa1\x8c\xe9\x9a\x8f\xe6\x9c\xba\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\xe9\x80\x89\xe6\x8b\xa9\xef\xbc\x8c\xe9\x81\xbf\xe5\x85\x8d\xe9\x80\x89\xe6\x8b\xa9\xe7\x9a\x84\xe5\x8c\xba\xe5\x9f\x9f\xe7\xa9\xbf\xe8\xbf\x87\xe6\x96\x87\xe6\x9c\xac\r\n        h_axis = np.where(h_array == 0)[0]\r\n        w_axis = np.where(w_array == 0)[0]\r\n        if len(h_axis) == 0 or len(w_axis) == 0:\r\n            # \xe6\x95\xb4\xe5\xbc\xa0\xe5\x9b\xbe\xe5\x85\xa8\xe6\x98\xaf\xe6\x96\x87\xe6\x9c\xac\xe7\x9a\x84\xe6\x83\x85\xe5\x86\xb5\xe4\xb8\x8b\xef\xbc\x8c\xe7\x9b\xb4\xe6\x8e\xa5\xe8\xbf\x94\xe5\x9b\x9e\r\n            return im, text_polys\r\n        for i in range(max_tries):\r\n            xx = np.random.choice(w_axis, size=2)\r\n            # \xe5\xaf\xb9\xe9\x80\x89\xe6\x8b\xa9\xe5\x8c\xba\xe5\x9f\x9f\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xbe\xb9\xe7\x95\x8c\xe6\x8e\xa7\xe5\x88\xb6\r\n            xmin = np.min(xx) - pad_w\r\n            xmax = np.max(xx) - pad_w\r\n            xmin = np.clip(xmin, 0, w - 1)\r\n            xmax = np.clip(xmax, 0, w - 1)\r\n            yy = np.random.choice(h_axis, size=2)\r\n            ymin = np.min(yy) - pad_h\r\n            ymax = np.max(yy) - pad_h\r\n            ymin = np.clip(ymin, 0, h - 1)\r\n            ymax = np.clip(ymax, 0, h - 1)\r\n            if xmax - xmin < 0.1 * w or ymax - ymin < 0.1 * h:\r\n                # \xe9\x80\x89\xe6\x8b\xa9\xe7\x9a\x84\xe5\x8c\xba\xe5\x9f\x9f\xe8\xbf\x87\xe5\xb0\x8f\r\n                # area too small\r\n                continue\r\n            if text_polys.shape[0] != 0:  # \xe8\xbf\x99\xe4\xb8\xaa\xe5\x88\xa4\xe6\x96\xad\xe4\xb8\x8d\xe7\x9f\xa5\xe9\x81\x93\xe5\xb9\xb2\xe5\x95\xa5\xe7\x9a\x84\r\n                poly_axis_in_area = (text_polys[:, :, 0] >= xmin) & (text_polys[:, :, 0] <= xmax) \\\r\n                                    & (text_polys[:, :, 1] >= ymin) & (text_polys[:, :, 1] <= ymax)\r\n                selected_polys = np.where(np.sum(poly_axis_in_area, axis=1) == 4)[0]\r\n            else:\r\n                selected_polys = []\r\n            if len(selected_polys) == 0:\r\n                # \xe5\x8c\xba\xe5\x9f\x9f\xe5\x86\x85\xe6\xb2\xa1\xe6\x9c\x89\xe6\x96\x87\xe6\x9c\xac\r\n                continue\r\n            im = im[ymin:ymax + 1, xmin:xmax + 1, :]\r\n            polys = text_polys[selected_polys]\r\n            # \xe5\x9d\x90\xe6\xa0\x87\xe8\xb0\x83\xe6\x95\xb4\xe5\x88\xb0\xe8\xa3\x81\xe5\x89\xaa\xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\x8a\r\n            polys[:, :, 0] -= xmin\r\n            polys[:, :, 1] -= ymin\r\n            return im, polys\r\n        return im, text_polys\r\n\r\n    def random_crop_image_pse(self, im: np.ndarray, text_polys: np.ndarray, input_size) -> tuple:\r\n        """"""\r\n        \xe4\xbb\x8e\xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\xad\xe8\xa3\x81\xe5\x89\xaa\xe5\x87\xba cropsize\xe5\xa4\xa7\xe5\xb0\x8f\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe5\xaf\xb9\xe5\xba\x94\xe5\x8c\xba\xe5\x9f\x9f\xe7\x9a\x84\xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\r\n        :param im: \xe5\x9b\xbe\xe7\x89\x87\r\n        :param text_polys: \xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\r\n        :param input_size: \xe8\xbe\x93\xe5\x87\xba\xe5\x9b\xbe\xe5\x83\x8f\xe5\xa4\xa7\xe5\xb0\x8f\r\n        :return: \xe8\xa3\x81\xe5\x89\xaa\xe5\x90\x8e\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\r\n        """"""\r\n        h, w, _ = im.shape\r\n        short_edge = min(h, w)\r\n        if short_edge < input_size:\r\n            # \xe4\xbf\x9d\xe8\xaf\x81\xe7\x9f\xad\xe8\xbe\xb9 >= inputsize\r\n            scale = input_size / short_edge\r\n            im = cv2.resize(im, dsize=None, fx=scale, fy=scale)\r\n            text_polys *= scale\r\n            h, w, _ = im.shape\r\n        # \xe8\xae\xa1\xe7\xae\x97\xe9\x9a\x8f\xe6\x9c\xba\xe8\x8c\x83\xe5\x9b\xb4\r\n        w_range = w - input_size\r\n        h_range = h - input_size\r\n        for _ in range(50):\r\n            xmin = random.randint(0, w_range)\r\n            ymin = random.randint(0, h_range)\r\n            xmax = xmin + input_size\r\n            ymax = ymin + input_size\r\n            if text_polys.shape[0] != 0:\r\n                selected_polys = []\r\n                for poly in text_polys:\r\n                    if poly[:, 0].max() < xmin or poly[:, 0].min() > xmax or \\\r\n                            poly[:, 1].max() < ymin or poly[:, 1].min() > ymax:\r\n                        continue\r\n                    # area_p = cv2.contourArea(poly)\r\n                    poly[:, 0] -= xmin\r\n                    poly[:, 1] -= ymin\r\n                    poly[:, 0] = np.clip(poly[:, 0], 0, input_size)\r\n                    poly[:, 1] = np.clip(poly[:, 1], 0, input_size)\r\n                    # rect = cv2.minAreaRect(poly)\r\n                    # area_n = cv2.contourArea(poly)\r\n                    # h1, w1 = rect[1]\r\n                    # if w1 < 10 or h1 < 10 or area_n / area_p < 0.5:\r\n                    #     continue\r\n                    selected_polys.append(poly)\r\n            else:\r\n                selected_polys = []\r\n            # if len(selected_polys) == 0:\r\n                # \xe5\x8c\xba\xe5\x9f\x9f\xe5\x86\x85\xe6\xb2\xa1\xe6\x9c\x89\xe6\x96\x87\xe6\x9c\xac\r\n                # continue\r\n            im = im[ymin:ymax, xmin:xmax, :]\r\n            polys = np.array(selected_polys)\r\n            return im, polys\r\n        return im, text_polys\r\n\r\n    def random_crop_author(self,imgs, img_size):\r\n        h, w = imgs[0].shape[0:2]\r\n        th, tw = img_size\r\n        if w == tw and h == th:\r\n            return imgs\r\n\r\n        # label\xe4\xb8\xad\xe5\xad\x98\xe5\x9c\xa8\xe6\x96\x87\xe6\x9c\xac\xe5\xae\x9e\xe4\xbe\x8b\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\x94\xe6\x8c\x89\xe7\x85\xa7\xe6\xa6\x82\xe7\x8e\x87\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xa3\x81\xe5\x89\xaa\r\n        if np.max(imgs[1][:,:,-1]) > 0 and random.random() > 3.0 / 8.0:\r\n            # \xe6\x96\x87\xe6\x9c\xac\xe5\xae\x9e\xe4\xbe\x8b\xe7\x9a\x84top left\xe7\x82\xb9\r\n            tl = np.min(np.where(imgs[1][:,:,-1] > 0), axis=1) - img_size\r\n            tl[tl < 0] = 0\r\n            # \xe6\x96\x87\xe6\x9c\xac\xe5\xae\x9e\xe4\xbe\x8b\xe7\x9a\x84 bottom right \xe7\x82\xb9\r\n            br = np.max(np.where(imgs[1][:,:,-1] > 0), axis=1) - img_size\r\n            br[br < 0] = 0\r\n            # \xe4\xbf\x9d\xe8\xaf\x81\xe9\x80\x89\xe5\x88\xb0\xe5\x8f\xb3\xe4\xb8\x8b\xe8\xa7\x92\xe7\x82\xb9\xe6\x98\xaf\xef\xbc\x8c\xe6\x9c\x89\xe8\xb6\xb3\xe5\xa4\x9f\xe7\x9a\x84\xe8\xb7\x9d\xe7\xa6\xbb\xe8\xbf\x9b\xe8\xa1\x8ccrop\r\n            br[0] = min(br[0], h - th)\r\n            br[1] = min(br[1], w - tw)\r\n            for _ in range(50000):\r\n                i = random.randint(tl[0], br[0])\r\n                j = random.randint(tl[1], br[1])\r\n                # \xe4\xbf\x9d\xe8\xaf\x81\xe6\x9c\x80\xe5\xb0\x8f\xe7\x9a\x84\xe5\x9b\xbe\xe6\x9c\x89\xe6\x96\x87\xe6\x9c\xac\r\n                if imgs[1][:,:,0][i:i + th, j:j + tw].sum() <= 0:\r\n                    continue\r\n                else:\r\n                    break\r\n        else:\r\n            i = random.randint(0, h - th)\r\n            j = random.randint(0, w - tw)\r\n\r\n        # return i, j, th, tw\r\n        for idx in range(len(imgs)):\r\n            if len(imgs[idx].shape) == 3:\r\n                imgs[idx] = imgs[idx][i:i + th, j:j + tw, :]\r\n            else:\r\n                imgs[idx] = imgs[idx][i:i + th, j:j + tw]\r\n        return imgs\r\n\r\n    def resize(self, im: np.ndarray, text_polys: np.ndarray,\r\n               input_size: numbers.Number or list or tuple or np.ndarray, keep_ratio: bool = False) -> tuple:\r\n        """"""\r\n        \xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\xe8\xbf\x9b\xe8\xa1\x8cresize\r\n        :param im: \xe5\x9b\xbe\xe7\x89\x87\r\n        :param text_polys: \xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\r\n        :param input_size: resize\xe5\xb0\xba\xe5\xaf\xb8,\xe6\x95\xb0\xe5\xad\x97\xe6\x88\x96\xe8\x80\x85list\xe7\x9a\x84\xe5\xbd\xa2\xe5\xbc\x8f\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\xbalist\xe5\xbd\xa2\xe5\xbc\x8f\xef\xbc\x8c\xe5\xb0\xb1\xe6\x98\xaf[w,h]\r\n        :param keep_ratio: \xe6\x98\xaf\xe5\x90\xa6\xe4\xbf\x9d\xe6\x8c\x81\xe9\x95\xbf\xe5\xae\xbd\xe6\xaf\x94\r\n        :return: resize\xe5\x90\x8e\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\r\n        """"""\r\n        if isinstance(input_size, numbers.Number):\r\n            if input_size < 0:\r\n                raise ValueError(""If input_size is a single number, it must be positive."")\r\n            input_size = (input_size, input_size)\r\n        elif isinstance(input_size, list) or isinstance(input_size, tuple) or isinstance(input_size, np.ndarray):\r\n            if len(input_size) != 2:\r\n                raise ValueError(""If input_size is a sequence, it must be of len 2."")\r\n            input_size = (input_size[0], input_size[1])\r\n        else:\r\n            raise Exception(\'input_size must in Number or list or tuple or np.ndarray\')\r\n        if keep_ratio:\r\n            # \xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe7\x9f\xad\xe8\xbe\xb9pad\xe5\x88\xb0\xe5\x92\x8c\xe9\x95\xbf\xe8\xbe\xb9\xe4\xb8\x80\xe6\xa0\xb7\r\n            h, w, c = im.shape\r\n            max_h = max(h, input_size[0])\r\n            max_w = max(w, input_size[1])\r\n            im_padded = np.zeros((max_h, max_w, c), dtype=np.uint8)\r\n            im_padded[:h, :w] = im.copy()\r\n            im = im_padded\r\n        text_polys = text_polys.astype(np.float32)\r\n        h, w, _ = im.shape\r\n        im = cv2.resize(im, input_size)\r\n        w_scale = input_size[0] / float(w)\r\n        h_scale = input_size[1] / float(h)\r\n        text_polys[:, :, 0] *= w_scale\r\n        text_polys[:, :, 1] *= h_scale\r\n        return im, text_polys\r\n\r\n    def horizontal_flip(self, im: np.ndarray, text_polys: np.ndarray) -> tuple:\r\n        """"""\r\n        \xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe6\xb0\xb4\xe5\xb9\xb3\xe7\xbf\xbb\xe8\xbd\xac\r\n        :param im: \xe5\x9b\xbe\xe7\x89\x87\r\n        :param text_polys: \xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\r\n        :return: \xe6\xb0\xb4\xe5\xb9\xb3\xe7\xbf\xbb\xe8\xbd\xac\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\r\n        """"""\r\n        flip_text_polys = text_polys.copy()\r\n        flip_im = cv2.flip(im, 1)\r\n        h, w, _ = flip_im.shape\r\n        flip_text_polys[:, :, 0] = w - flip_text_polys[:, :, 0]\r\n        return flip_im, flip_text_polys\r\n\r\n    def vertical_flip(self, im: np.ndarray, text_polys: np.ndarray) -> tuple:\r\n        """"""\r\n         \xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe7\xab\x96\xe7\x9b\xb4\xe7\xbf\xbb\xe8\xbd\xac\r\n        :param im: \xe5\x9b\xbe\xe7\x89\x87\r\n        :param text_polys: \xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\r\n        :return: \xe7\xab\x96\xe7\x9b\xb4\xe7\xbf\xbb\xe8\xbd\xac\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\r\n        """"""\r\n        flip_text_polys = text_polys.copy()\r\n        flip_im = cv2.flip(im, 0)\r\n        h, w, _ = flip_im.shape\r\n        flip_text_polys[:, :, 1] = h - flip_text_polys[:, :, 1]\r\n        return flip_im, flip_text_polys\r\n\r\n    def test(self, im: np.ndarray, text_polys: np.ndarray):\r\n        print(\'\xe9\x9a\x8f\xe6\x9c\xba\xe5\xb0\xba\xe5\xba\xa6\xe7\xbc\xa9\xe6\x94\xbe\')\r\n        t_im, t_text_polys = self.random_scale(im, text_polys, [0.5, 1, 2, 3])\r\n        print(t_im.shape, t_text_polys.dtype)\r\n        show_pic(t_im, t_text_polys, \'random_scale\')\r\n\r\n        print(\'\xe9\x9a\x8f\xe6\x9c\xba\xe6\x97\x8b\xe8\xbd\xac\')\r\n        t_im, t_text_polys = self.random_rotate_img_bbox(im, text_polys, 10)\r\n        print(t_im.shape, t_text_polys.dtype)\r\n        show_pic(t_im, t_text_polys, \'random_rotate_img_bbox\')\r\n\r\n        print(\'\xe9\x9a\x8f\xe6\x9c\xba\xe8\xa3\x81\xe5\x89\xaa\')\r\n        t_im, t_text_polys = self.random_crop_img_bboxes(im, text_polys)\r\n        print(t_im.shape, t_text_polys.dtype)\r\n        show_pic(t_im, t_text_polys, \'random_crop_img_bboxes\')\r\n\r\n        print(\'\xe6\xb0\xb4\xe5\xb9\xb3\xe7\xbf\xbb\xe8\xbd\xac\')\r\n        t_im, t_text_polys = self.horizontal_flip(im, text_polys)\r\n        print(t_im.shape, t_text_polys.dtype)\r\n        show_pic(t_im, t_text_polys, \'horizontal_flip\')\r\n\r\n        print(\'\xe7\xab\x96\xe7\x9b\xb4\xe7\xbf\xbb\xe8\xbd\xac\')\r\n        t_im, t_text_polys = self.vertical_flip(im, text_polys)\r\n        print(t_im.shape, t_text_polys.dtype)\r\n        show_pic(t_im, t_text_polys, \'vertical_flip\')\r\n        show_pic(im, text_polys, \'vertical_flip_ori\')\r\n\r\n        print(\'\xe5\x8a\xa0\xe5\x99\xaa\xe5\xa3\xb0\')\r\n        t_im = self.add_noise(im)\r\n        print(t_im.shape)\r\n        show_pic(t_im, text_polys, \'add_noise\')\r\n        show_pic(im, text_polys, \'add_noise_ori\')\r\n'"
dataset/augment_img.py,0,"b'# -*- coding: utf-8 -*-\r\n# @Time    : 2019/1/12 13:06\r\n\r\nimport cv2\r\nimport numbers\r\nimport math\r\nimport random\r\nimport numpy as np\r\nfrom skimage.util import random_noise\r\n\r\n\r\ndef show_pic(img, bboxes=None, name=\'pic\'):\r\n    \'\'\'\r\n    \xe8\xbe\x93\xe5\x85\xa5:\r\n        img:\xe5\x9b\xbe\xe5\x83\x8farray\r\n        bboxes:\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89boudning box list, \xe6\xa0\xbc\xe5\xbc\x8f\xe4\xb8\xba[[x_min, y_min, x_max, y_max]....]\r\n        names:\xe6\xaf\x8f\xe4\xb8\xaabox\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe5\x90\x8d\xe7\xa7\xb0\r\n    \'\'\'\r\n    show_img = img.copy()\r\n    if not isinstance(bboxes, np.ndarray):\r\n        bboxes = np.array(bboxes)\r\n    for point in bboxes.astype(np.int):\r\n        cv2.line(show_img, tuple(point[0]), tuple(point[1]), (255, 0, 0), 2)\r\n        cv2.line(show_img, tuple(point[1]), tuple(point[2]), (255, 0, 0), 2)\r\n        cv2.line(show_img, tuple(point[2]), tuple(point[3]), (255, 0, 0), 2)\r\n        cv2.line(show_img, tuple(point[3]), tuple(point[0]), (255, 0, 0), 2)\r\n    # cv2.namedWindow(name, 0)  # 1\xe8\xa1\xa8\xe7\xa4\xba\xe5\x8e\x9f\xe5\x9b\xbe\r\n    # cv2.moveWindow(name, 0, 0)\r\n    # cv2.resizeWindow(name, 1200, 800)  # \xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe5\xa4\xa7\xe5\xb0\x8f\r\n    cv2.imshow(name, show_img)\r\n\r\n\r\n# \xe5\x9b\xbe\xe5\x83\x8f\xe5\x9d\x87\xe4\xb8\xbacv2\xe8\xaf\xbb\xe5\x8f\x96\r\nclass DataAugment():\r\n    def __init__(self):\r\n        pass\r\n\r\n    def add_noise(self, im: np.ndarray):\r\n        """"""\r\n        \xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe5\x8a\xa0\xe5\x99\xaa\xe5\xa3\xb0\r\n        :param img: \xe5\x9b\xbe\xe5\x83\x8farray\r\n        :return: \xe5\x8a\xa0\xe5\x99\xaa\xe5\xa3\xb0\xe5\x90\x8e\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8farray,\xe7\x94\xb1\xe4\xba\x8e\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe5\x83\x8f\xe7\xb4\xa0\xe6\x98\xaf\xe5\x9c\xa8[0,1]\xe4\xb9\x8b\xe9\x97\xb4,\xe6\x89\x80\xe4\xbb\xa5\xe5\xbe\x97\xe4\xb9\x98\xe4\xbb\xa5255\r\n        """"""\r\n        return (random_noise(im, mode=\'gaussian\', clip=True) * 255).astype(im.dtype)\r\n\r\n    def random_scale(self, imgs: list, scales: np.ndarray or list, input_size: int) -> list:\r\n        """"""\r\n        \xe4\xbb\x8escales\xe4\xb8\xad\xe9\x9a\x8f\xe6\x9c\xba\xe9\x80\x89\xe6\x8b\xa9\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb0\xba\xe5\xba\xa6\xef\xbc\x8c\xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe7\xbc\xa9\xe6\x94\xbe\r\n        :param imgs: \xe5\x8e\x9f\xe5\x9b\xbe \xe5\x92\x8c label\r\n        :param scales: \xe5\xb0\xba\xe5\xba\xa6\r\n        :param input_size: \xe5\x9b\xbe\xe7\x89\x87\xe7\x9f\xad\xe8\xbe\xb9\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\r\n        :return: \xe7\xbb\x8f\xe8\xbf\x87\xe7\xbc\xa9\xe6\x94\xbe\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe6\x96\x87\xe6\x9c\xac\r\n        """"""\r\n        rd_scale = float(np.random.choice(scales))\r\n        for idx in range(len(imgs)):\r\n            imgs[idx] = cv2.resize(imgs[idx], dsize=None, fx=rd_scale, fy=rd_scale)\r\n            imgs[idx], _ = self.rescale(imgs[idx], min_side=input_size)\r\n        return imgs\r\n\r\n    def rescale(self, img, min_side):\r\n        h, w = img.shape[:2]\r\n        scale = 1.0\r\n        if min(h, w) < min_side:\r\n            if h <= w:\r\n                scale = 1.0 * min_side / h\r\n            else:\r\n                scale = 1.0 * min_side / w\r\n        img = cv2.resize(img, dsize=None, fx=scale, fy=scale)\r\n        return img\r\n\r\n    def random_rotate_img_bbox(self, imgs, degrees: numbers.Number or list or tuple or np.ndarray,\r\n                               same_size=False):\r\n        """"""\r\n        \xe4\xbb\x8e\xe7\xbb\x99\xe5\xae\x9a\xe7\x9a\x84\xe8\xa7\x92\xe5\xba\xa6\xe4\xb8\xad\xe9\x80\x89\xe6\x8b\xa9\xe4\xb8\x80\xe4\xb8\xaa\xe8\xa7\x92\xe5\xba\xa6\xef\xbc\x8c\xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x97\x8b\xe8\xbd\xac\r\n        :param imgs: \xe5\x8e\x9f\xe5\x9b\xbe \xe5\x92\x8c label\r\n        :param degrees: \xe8\xa7\x92\xe5\xba\xa6\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe5\x80\xbc\xe6\x88\x96\xe8\x80\x85list\r\n        :param same_size: \xe6\x98\xaf\xe5\x90\xa6\xe4\xbf\x9d\xe6\x8c\x81\xe5\x92\x8c\xe5\x8e\x9f\xe5\x9b\xbe\xe4\xb8\x80\xe6\xa0\xb7\xe5\xa4\xa7\r\n        :return: \xe6\x97\x8b\xe8\xbd\xac\xe5\x90\x8e\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe8\xa7\x92\xe5\xba\xa6\r\n        """"""\r\n        if isinstance(degrees, numbers.Number):\r\n            if degrees < 0:\r\n                raise ValueError(""If degrees is a single number, it must be positive."")\r\n            degrees = (-degrees, degrees)\r\n        elif isinstance(degrees, list) or isinstance(degrees, tuple) or isinstance(degrees, np.ndarray):\r\n            if len(degrees) != 2:\r\n                raise ValueError(""If degrees is a sequence, it must be of len 2."")\r\n            degrees = degrees\r\n        else:\r\n            raise Exception(\'degrees must in Number or list or tuple or np.ndarray\')\r\n        # ---------------------- \xe6\x97\x8b\xe8\xbd\xac\xe5\x9b\xbe\xe5\x83\x8f ----------------------\r\n        w = imgs[0].shape[1]\r\n        h = imgs[0].shape[0]\r\n        angle = np.random.uniform(degrees[0], degrees[1])\r\n\r\n        if same_size:\r\n            nw = w\r\n            nh = h\r\n        else:\r\n            # \xe8\xa7\x92\xe5\xba\xa6\xe5\x8f\x98\xe5\xbc\xa7\xe5\xba\xa6\r\n            rangle = np.deg2rad(angle)\r\n            # \xe8\xae\xa1\xe7\xae\x97\xe6\x97\x8b\xe8\xbd\xac\xe4\xb9\x8b\xe5\x90\x8e\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84w, h\r\n            nw = (abs(np.sin(rangle) * h) + abs(np.cos(rangle) * w))\r\n            nh = (abs(np.cos(rangle) * h) + abs(np.sin(rangle) * w))\r\n        # \xe6\x9e\x84\xe9\x80\xa0\xe4\xbb\xbf\xe5\xb0\x84\xe7\x9f\xa9\xe9\x98\xb5\r\n        rot_mat = cv2.getRotationMatrix2D((nw * 0.5, nh * 0.5), angle, 1)\r\n        # \xe8\xae\xa1\xe7\xae\x97\xe5\x8e\x9f\xe5\x9b\xbe\xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\xe5\x88\xb0\xe6\x96\xb0\xe5\x9b\xbe\xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\xe7\x9a\x84\xe5\x81\x8f\xe7\xa7\xbb\xe9\x87\x8f\r\n        rot_move = np.dot(rot_mat, np.array([(nw - w) * 0.5, (nh - h) * 0.5, 0]))\r\n        # \xe6\x9b\xb4\xe6\x96\xb0\xe4\xbb\xbf\xe5\xb0\x84\xe7\x9f\xa9\xe9\x98\xb5\r\n        rot_mat[0, 2] += rot_move[0]\r\n        rot_mat[1, 2] += rot_move[1]\r\n        for idx in range(len(imgs)):\r\n            # \xe4\xbb\xbf\xe5\xb0\x84\xe5\x8f\x98\xe6\x8d\xa2\r\n            imgs[idx] = cv2.warpAffine(imgs[idx], rot_mat, (int(math.ceil(nw)), int(math.ceil(nh))), flags=cv2.INTER_LANCZOS4)\r\n        return imgs\r\n\r\n    def random_crop(self, imgs, img_size):\r\n        h, w = imgs[0].shape[0:2]\r\n        th, tw = img_size\r\n        if w == tw and h == th:\r\n            return imgs\r\n\r\n        # label\xe4\xb8\xad\xe5\xad\x98\xe5\x9c\xa8\xe6\x96\x87\xe6\x9c\xac\xe5\xae\x9e\xe4\xbe\x8b\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\x94\xe6\x8c\x89\xe7\x85\xa7\xe6\xa6\x82\xe7\x8e\x87\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xa3\x81\xe5\x89\xaa\r\n        if np.max(imgs[1][:, :, -1]) > 0 and random.random() > 3.0 / 8.0:\r\n            # \xe6\x96\x87\xe6\x9c\xac\xe5\xae\x9e\xe4\xbe\x8b\xe7\x9a\x84top left\xe7\x82\xb9\r\n            tl = np.min(np.where(imgs[1][:, :, -1] > 0), axis=1) - img_size\r\n            tl[tl < 0] = 0\r\n            # \xe6\x96\x87\xe6\x9c\xac\xe5\xae\x9e\xe4\xbe\x8b\xe7\x9a\x84 bottom right \xe7\x82\xb9\r\n            br = np.max(np.where(imgs[1][:, :, -1] > 0), axis=1) - img_size\r\n            br[br < 0] = 0\r\n            # \xe4\xbf\x9d\xe8\xaf\x81\xe9\x80\x89\xe5\x88\xb0\xe5\x8f\xb3\xe4\xb8\x8b\xe8\xa7\x92\xe7\x82\xb9\xe6\x98\xaf\xef\xbc\x8c\xe6\x9c\x89\xe8\xb6\xb3\xe5\xa4\x9f\xe7\x9a\x84\xe8\xb7\x9d\xe7\xa6\xbb\xe8\xbf\x9b\xe8\xa1\x8ccrop\r\n            br[0] = min(br[0], h - th)\r\n            br[1] = min(br[1], w - tw)\r\n            for _ in range(50000):\r\n                i = random.randint(tl[0], br[0])\r\n                j = random.randint(tl[1], br[1])\r\n                # \xe4\xbf\x9d\xe8\xaf\x81\xe6\x9c\x80\xe5\xb0\x8f\xe7\x9a\x84\xe5\x9b\xbe\xe6\x9c\x89\xe6\x96\x87\xe6\x9c\xac\r\n                if imgs[1][:, :, 0][i:i + th, j:j + tw].sum() <= 0:\r\n                    continue\r\n                else:\r\n                    break\r\n        else:\r\n            i = random.randint(0, h - th)\r\n            j = random.randint(0, w - tw)\r\n\r\n        # return i, j, th, tw\r\n        for idx in range(len(imgs)):\r\n            if len(imgs[idx].shape) == 3:\r\n                imgs[idx] = imgs[idx][i:i + th, j:j + tw, :]\r\n            else:\r\n                imgs[idx] = imgs[idx][i:i + th, j:j + tw]\r\n        return imgs\r\n\r\n\r\n    def horizontal_flip(self, imgs: list) -> list:\r\n        """"""\r\n        \xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe6\xb0\xb4\xe5\xb9\xb3\xe7\xbf\xbb\xe8\xbd\xac\r\n        :param im: \xe5\x9b\xbe\xe7\x89\x87\r\n        :param text_polys: \xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\r\n        :return: \xe6\xb0\xb4\xe5\xb9\xb3\xe7\xbf\xbb\xe8\xbd\xac\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\r\n        """"""\r\n        for idx in range(len(imgs)):\r\n            imgs[idx] = cv2.flip(imgs[idx], 1)\r\n        return imgs\r\n\r\n    def vertical_flip(self, imgs: list) -> list:\r\n        """"""\r\n         \xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe7\xab\x96\xe7\x9b\xb4\xe7\xbf\xbb\xe8\xbd\xac\r\n        :param im: \xe5\x9b\xbe\xe7\x89\x87\r\n        :param text_polys: \xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\r\n        :return: \xe7\xab\x96\xe7\x9b\xb4\xe7\xbf\xbb\xe8\xbd\xac\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\r\n        """"""\r\n        for idx in range(len(imgs)):\r\n            imgs[idx] = cv2.flip(imgs[idx], 0)\r\n        return imgs'"
dataset/data_utils.py,3,"b'# -*- coding: utf-8 -*-\n# @Time    : 2018/6/11 15:54\n# @Author  : zhoujun\n\nimport os\nimport random\nimport pathlib\nimport pyclipper\nfrom torch.utils import data\nimport glob\nimport numpy as np\nimport cv2\nfrom dataset.augment import DataAugment\nfrom utils.utils import draw_bbox\n\ndata_aug = DataAugment()\n\n\ndef check_and_validate_polys(polys, xxx_todo_changeme):\n    \'\'\'\n    check so that the text poly is in the same direction,\n    and also filter some invalid polygons\n    :param polys:\n    :param tags:\n    :return:\n    \'\'\'\n    (h, w) = xxx_todo_changeme\n    if polys.shape[0] == 0:\n        return polys\n    polys[:, :, 0] = np.clip(polys[:, :, 0], 0, w - 1)  # x coord not max w-1, and not min 0\n    polys[:, :, 1] = np.clip(polys[:, :, 1], 0, h - 1)  # y coord not max h-1, and not min 0\n\n    validated_polys = []\n    for poly in polys:\n        p_area = cv2.contourArea(poly)\n        if abs(p_area) < 1:\n            continue\n        validated_polys.append(poly)\n    return np.array(validated_polys)\n\n\ndef generate_rbox(im_size, text_polys, text_tags, training_mask, i, n, m):\n    """"""\n    \xe7\x94\x9f\xe6\x88\x90mask\xe5\x9b\xbe\xef\xbc\x8c\xe7\x99\xbd\xe8\x89\xb2\xe9\x83\xa8\xe5\x88\x86\xe6\x98\xaf\xe6\x96\x87\xe6\x9c\xac\xef\xbc\x8c\xe9\xbb\x91\xe8\x89\xb2\xe6\x98\xaf\xe5\x8c\x97\xe4\xba\xac\n    :param im_size: \xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84h,w\n    :param text_polys: \xe6\xa1\x86\xe7\x9a\x84\xe5\x9d\x90\xe6\xa0\x87\n    :param text_tags: \xe6\xa0\x87\xe6\xb3\xa8\xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\xe6\x98\xaf\xe5\x90\xa6\xe5\x8f\x82\xe4\xb8\x8e\xe8\xae\xad\xe7\xbb\x83\n    :return: \xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84mask\xe5\x9b\xbe\n    """"""\n    h, w = im_size\n    score_map = np.zeros((h, w), dtype=np.uint8)\n    for poly, tag in zip(text_polys, text_tags):\n        poly = poly.astype(np.int)\n        r_i = 1 - (1 - m) * (n - i) / (n - 1)\n        d_i = cv2.contourArea(poly) * (1 - r_i * r_i) / cv2.arcLength(poly, True)\n        pco = pyclipper.PyclipperOffset()\n        # pco.AddPath(pyclipper.scale_to_clipper(poly), pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)\n        # shrinked_poly = np.floor(np.array(pyclipper.scale_from_clipper(pco.Execute(-d_i)))).astype(np.int)\n        pco.AddPath(poly, pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)\n        shrinked_poly = np.array(pco.Execute(-d_i))\n        cv2.fillPoly(score_map, shrinked_poly, 1)\n        # \xe5\x88\xb6\xe4\xbd\x9cmask\n        # rect = cv2.minAreaRect(shrinked_poly)\n        # poly_h, poly_w = rect[1]\n\n        # if min(poly_h, poly_w) < 10:\n        #     cv2.fillPoly(training_mask, shrinked_poly, 0)\n        if tag:\n            cv2.fillPoly(training_mask, shrinked_poly, 0)\n        # \xe9\x97\xad\xe8\xbf\x90\xe7\xae\x97\xe5\xa1\xab\xe5\x85\x85\xe5\x86\x85\xe9\x83\xa8\xe5\xb0\x8f\xe6\xa1\x86\n        # kernel = np.ones((3, 3), np.uint8)\n        # score_map = cv2.morphologyEx(score_map, cv2.MORPH_CLOSE, kernel)\n    return score_map, training_mask\n\n\ndef augmentation(im: np.ndarray, text_polys: np.ndarray, scales: np.ndarray, degrees: int, input_size: int) -> tuple:\n    # the images are rescaled with ratio {0.5, 1.0, 2.0, 3.0} randomly\n    im, text_polys = data_aug.random_scale(im, text_polys, scales)\n    # the images are horizontally fliped and rotated in range [\xe2\x88\x9210\xe2\x97\xa6, 10\xe2\x97\xa6] randomly\n    if random.random() < 0.5:\n        im, text_polys = data_aug.horizontal_flip(im, text_polys)\n    if random.random() < 0.5:\n        im, text_polys = data_aug.random_rotate_img_bbox(im, text_polys, degrees)\n    # 640 \xc3\x97 640 random samples are cropped from the transformed images\n    # im, text_polys = data_aug.random_crop_img_bboxes(im, text_polys)\n\n    # im, text_polys = data_aug.resize(im, text_polys, input_size, keep_ratio=False)\n    # im, text_polys = data_aug.random_crop_image_pse(im, text_polys, input_size)\n\n    return im, text_polys\n\n\ndef image_label(im_fn: str, text_polys: np.ndarray, text_tags: list, n: int, m: float, input_size: int,\n                defrees: int = 10,\n                scales: np.ndarray = np.array([0.5, 1, 2.0, 3.0])) -> tuple:\n    \'\'\'\n    get image\'s corresponding matrix and ground truth\n    return\n    images [512, 512, 3]\n    score  [128, 128, 1]\n    geo    [128, 128, 5]\n    mask   [128, 128, 1]\n    \'\'\'\n\n    im = cv2.imread(im_fn)\n    im = cv2.cvtColor(im,cv2.COLOR_BGR2RGB)\n    h, w, _ = im.shape\n    # \xe6\xa3\x80\xe6\x9f\xa5\xe8\xb6\x8a\xe7\x95\x8c\n    text_polys = check_and_validate_polys(text_polys, (h, w))\n    im, text_polys, = augmentation(im, text_polys, scales, defrees, input_size)\n\n    h, w, _ = im.shape\n    short_edge = min(h, w)\n    if short_edge < input_size:\n        # \xe4\xbf\x9d\xe8\xaf\x81\xe7\x9f\xad\xe8\xbe\xb9 >= inputsize\n        scale = input_size / short_edge\n        im = cv2.resize(im, dsize=None, fx=scale, fy=scale)\n        text_polys *= scale\n\n    # # normal images\n    # im = im.astype(np.float32)\n    # im /= 255.0\n    # im -= np.array((0.485, 0.456, 0.406))\n    # im /= np.array((0.229, 0.224, 0.225))\n\n    h, w, _ = im.shape\n    training_mask = np.ones((h, w), dtype=np.uint8)\n    score_maps = []\n    for i in range(1, n + 1):\n        # s1->sn,\xe7\x94\xb1\xe5\xb0\x8f\xe5\x88\xb0\xe5\xa4\xa7\n        score_map, training_mask = generate_rbox((h, w), text_polys, text_tags, training_mask, i, n, m)\n        score_maps.append(score_map)\n    score_maps = np.array(score_maps, dtype=np.float32)\n    imgs = data_aug.random_crop_author([im, score_maps.transpose((1, 2, 0)),training_mask], (input_size, input_size))\n    return imgs[0], imgs[1].transpose((2, 0, 1)), imgs[2]#im,score_maps,training_mask#\n\n\nclass MyDataset(data.Dataset):\n    def __init__(self, data_dir, data_shape: int = 640, n=6, m=0.5, transform=None, target_transform=None):\n        self.data_list = self.load_data(data_dir)\n        self.data_shape = data_shape\n        self.transform = transform\n        self.target_transform = target_transform\n        self.n = n\n        self.m = m\n\n    def __getitem__(self, index):\n        # print(self.image_list[index])\n        img_path, text_polys, text_tags = self.data_list[index]\n        img, score_maps, training_mask = image_label(img_path, text_polys, text_tags, input_size=self.data_shape,\n                                                     n=self.n,\n                                                     m=self.m)\n        # img = draw_bbox(img,text_polys)\n        if self.transform:\n            img = self.transform(img)\n        if self.target_transform:\n            score_maps = self.target_transform(score_maps)\n            training_mask = self.target_transform(training_mask)\n        return img, score_maps, training_mask\n\n    def load_data(self, data_dir: str) -> list:\n        data_list = []\n        for x in glob.glob(data_dir + \'/img/*.jpg\', recursive=True):\n            d = pathlib.Path(x)\n            label_path = os.path.join(data_dir, \'gt\', (\'gt_\' + str(d.stem) + \'.txt\'))\n            bboxs, text = self._get_annotation(label_path)\n            if len(bboxs) > 0:\n                data_list.append((x, bboxs, text))\n            else:\n                print(\'there is no suit bbox on {}\'.format(label_path))\n        return data_list\n\n    def _get_annotation(self, label_path: str) -> tuple:\n        boxes = []\n        text_tags = []\n        with open(label_path, encoding=\'utf-8\', mode=\'r\') as f:\n            for line in f.readlines():\n                params = line.strip().strip(\'\\ufeff\').strip(\'\\xef\\xbb\\xbf\').split(\',\')\n                try:\n                    label = params[8]\n                    if label == \'*\' or label == \'###\':\n                        text_tags.append(True)\n                    else:\n                        text_tags.append(False)\n                    # if label == \'*\' or label == \'###\':\n                    x1, y1, x2, y2, x3, y3, x4, y4 = list(map(float, params[:8]))\n                    boxes.append([[x1, y1], [x2, y2], [x3, y3], [x4, y4]])\n                except:\n                    print(\'load label failed on {}\'.format(label_path))\n        return np.array(boxes, dtype=np.float32), np.array(text_tags, dtype=np.bool)\n\n    def __len__(self):\n        return len(self.data_list)\n\n    def save_label(self, img_path, label):\n        save_path = img_path.replace(\'img\', \'save\')\n        if not os.path.exists(os.path.split(save_path)[0]):\n            os.makedirs(os.path.split(save_path)[0])\n        img = draw_bbox(img_path, label)\n        cv2.imwrite(save_path, img)\n        return img\n\n\nif __name__ == \'__main__\':\n    import torch\n    import config\n    from utils.utils import show_img\n    from tqdm import tqdm\n    from torch.utils.data import DataLoader\n    import matplotlib.pyplot as plt\n    from torchvision import transforms\n\n    train_data = MyDataset(config.trainroot, data_shape=config.data_shape, n=config.n, m=config.m,\n                           transform=transforms.ToTensor())\n    train_loader = DataLoader(dataset=train_data, batch_size=1, shuffle=False, num_workers=0)\n\n    pbar = tqdm(total=len(train_loader))\n    for i, (img, label, mask) in enumerate(train_loader):\n        print(label.shape)\n        print(img.shape)\n        print(label[0][-1].sum())\n        print(mask[0].shape)\n        # pbar.update(1)\n        show_img((img[0] * mask[0].to(torch.float)).numpy().transpose(1, 2, 0), color=True)\n        show_img(label[0])\n        show_img(mask[0])\n        plt.show()\n\n    pbar.close()\n'"
models/ShuffleNetV2.py,9,"b'import torch\r\nimport torch.nn as nn\r\nimport logging\r\nfrom torchvision.models.utils import load_state_dict_from_url\r\n\r\nlogger = logging.getLogger(\'project\')\r\n\r\n__all__ = [\r\n    \'ShuffleNetV2\', \'shufflenet_v2_x0_5\', \'shufflenet_v2_x1_0\',\r\n    \'shufflenet_v2_x1_5\', \'shufflenet_v2_x2_0\'\r\n]\r\n\r\nmodel_urls = {\r\n    \'shufflenetv2_x0.5\': \'https://download.pytorch.org/models/shufflenetv2_x0.5-f707e7126e.pth\',\r\n    \'shufflenetv2_x1.0\': \'https://download.pytorch.org/models/shufflenetv2_x1-5666bf0f80.pth\',\r\n    \'shufflenetv2_x1.5\': None,\r\n    \'shufflenetv2_x2.0\': None,\r\n}\r\n\r\n\r\ndef channel_shuffle(x, groups):\r\n    batchsize, num_channels, height, width = x.data.size()\r\n    channels_per_group = num_channels // groups\r\n\r\n    # reshape\r\n    x = x.view(batchsize, groups,\r\n               channels_per_group, height, width)\r\n\r\n    x = torch.transpose(x, 1, 2).contiguous()\r\n\r\n    # flatten\r\n    x = x.view(batchsize, -1, height, width)\r\n\r\n    return x\r\n\r\n\r\nclass InvertedResidual(nn.Module):\r\n    def __init__(self, inp, oup, stride):\r\n        super(InvertedResidual, self).__init__()\r\n\r\n        if not (1 <= stride <= 3):\r\n            raise ValueError(\'illegal stride value\')\r\n        self.stride = stride\r\n\r\n        branch_features = oup // 2\r\n        assert (self.stride != 1) or (inp == branch_features << 1)\r\n\r\n        if self.stride > 1:\r\n            self.branch1 = nn.Sequential(\r\n                self.depthwise_conv(inp, inp, kernel_size=3, stride=self.stride, padding=1),\r\n                nn.BatchNorm2d(inp),\r\n                nn.Conv2d(inp, branch_features, kernel_size=1, stride=1, padding=0, bias=False),\r\n                nn.BatchNorm2d(branch_features),\r\n                nn.ReLU(inplace=True),\r\n            )\r\n\r\n        self.branch2 = nn.Sequential(\r\n            nn.Conv2d(inp if (self.stride > 1) else branch_features,\r\n                      branch_features, kernel_size=1, stride=1, padding=0, bias=False),\r\n            nn.BatchNorm2d(branch_features),\r\n            nn.ReLU(inplace=True),\r\n            self.depthwise_conv(branch_features, branch_features, kernel_size=3, stride=self.stride, padding=1),\r\n            nn.BatchNorm2d(branch_features),\r\n            nn.Conv2d(branch_features, branch_features, kernel_size=1, stride=1, padding=0, bias=False),\r\n            nn.BatchNorm2d(branch_features),\r\n            nn.ReLU(inplace=True),\r\n        )\r\n\r\n    @staticmethod\r\n    def depthwise_conv(i, o, kernel_size, stride=1, padding=0, bias=False):\r\n        return nn.Conv2d(i, o, kernel_size, stride, padding, bias=bias, groups=i)\r\n\r\n    def forward(self, x):\r\n        if self.stride == 1:\r\n            x1, x2 = x.chunk(2, dim=1)\r\n            out = torch.cat((x1, self.branch2(x2)), dim=1)\r\n        else:\r\n            out = torch.cat((self.branch1(x), self.branch2(x)), dim=1)\r\n\r\n        out = channel_shuffle(out, 2)\r\n\r\n        return out\r\n\r\n\r\nclass ShuffleNetV2(nn.Module):\r\n    def __init__(self, stages_repeats, stages_out_channels, num_classes=1000):\r\n        super(ShuffleNetV2, self).__init__()\r\n\r\n        if len(stages_repeats) != 3:\r\n            raise ValueError(\'expected stages_repeats as list of 3 positive ints\')\r\n        if len(stages_out_channels) != 5:\r\n            raise ValueError(\'expected stages_out_channels as list of 5 positive ints\')\r\n        self._stage_out_channels = stages_out_channels\r\n\r\n        input_channels = 3\r\n        output_channels = self._stage_out_channels[0]\r\n        self.conv1 = nn.Sequential(\r\n            nn.Conv2d(input_channels, output_channels, 3, 2, 1, bias=False),\r\n            nn.BatchNorm2d(output_channels),\r\n            nn.ReLU(inplace=True),\r\n        )\r\n        input_channels = output_channels\r\n\r\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\r\n\r\n        stage_names = [\'stage{}\'.format(i) for i in [2, 3, 4]]\r\n        for name, repeats, output_channels in zip(\r\n                stage_names, stages_repeats, self._stage_out_channels[1:]):\r\n            seq = [InvertedResidual(input_channels, output_channels, 2)]\r\n            for i in range(repeats - 1):\r\n                seq.append(InvertedResidual(output_channels, output_channels, 1))\r\n            setattr(self, name, nn.Sequential(*seq))\r\n            input_channels = output_channels\r\n\r\n        output_channels = self._stage_out_channels[-1]\r\n        self.conv5 = nn.Sequential(\r\n            nn.Conv2d(input_channels, output_channels, 1, 1, 0, bias=False),\r\n            nn.BatchNorm2d(output_channels),\r\n            nn.ReLU(inplace=True),\r\n        )\r\n\r\n    def forward(self, x):\r\n        x = self.conv1(x)\r\n        c2 = self.maxpool(x)\r\n        c3 = self.stage2(c2)\r\n        c4 = self.stage3(c3)\r\n        c5 = self.stage4(c4)\r\n        # c5 = self.conv5(c5)\r\n        return c2, c3, c4, c5\r\n\r\n\r\ndef _shufflenetv2(arch, pretrained, progress, *args, **kwargs):\r\n    model = ShuffleNetV2(*args, **kwargs)\r\n\r\n    if pretrained:\r\n        model_url = model_urls[arch]\r\n        if model_url is None:\r\n            raise NotImplementedError(\'pretrained {} is not supported as of now\'.format(arch))\r\n        else:\r\n            state_dict = load_state_dict_from_url(model_url, progress=progress)\r\n            model.load_state_dict(state_dict,strict=False)\r\n            logger.info(\'load pretrained models from imagenet\')\r\n    return model\r\n\r\n\r\ndef shufflenet_v2_x0_5(pretrained=False, progress=True, **kwargs):\r\n    """"""\r\n    Constructs a ShuffleNetV2 with 0.5x output channels, as described in\r\n    `""ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design""\r\n    <https://arxiv.org/abs/1807.11164>`_.\r\n\r\n    Args:\r\n        pretrained (bool): If True, returns a models pre-trained on ImageNet\r\n        progress (bool): If True, displays a progress bar of the download to stderr\r\n    """"""\r\n    return _shufflenetv2(\'shufflenetv2_x0.5\', pretrained, progress,\r\n                         [4, 8, 4], [24, 48, 96, 192, 1024], **kwargs)\r\n\r\n\r\ndef shufflenet_v2_x1_0(pretrained=False, progress=True, **kwargs):\r\n    """"""\r\n    Constructs a ShuffleNetV2 with 1.0x output channels, as described in\r\n    `""ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design""\r\n    <https://arxiv.org/abs/1807.11164>`_.\r\n\r\n    Args:\r\n        pretrained (bool): If True, returns a models pre-trained on ImageNet\r\n        progress (bool): If True, displays a progress bar of the download to stderr\r\n    """"""\r\n    return _shufflenetv2(\'shufflenetv2_x1.0\', pretrained, progress,\r\n                         [4, 8, 4], [24, 116, 232, 464, 1024], **kwargs)\r\n\r\n\r\ndef shufflenet_v2_x1_5(pretrained=False, progress=True, **kwargs):\r\n    """"""\r\n    Constructs a ShuffleNetV2 with 1.5x output channels, as described in\r\n    `""ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design""\r\n    <https://arxiv.org/abs/1807.11164>`_.\r\n\r\n    Args:\r\n        pretrained (bool): If True, returns a models pre-trained on ImageNet\r\n        progress (bool): If True, displays a progress bar of the download to stderr\r\n    """"""\r\n    return _shufflenetv2(\'shufflenetv2_x1.5\', pretrained, progress,\r\n                         [4, 8, 4], [24, 176, 352, 704, 1024], **kwargs)\r\n\r\n\r\ndef shufflenet_v2_x2_0(pretrained=False, progress=True, **kwargs):\r\n    """"""\r\n    Constructs a ShuffleNetV2 with 2.0x output channels, as described in\r\n    `""ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design""\r\n    <https://arxiv.org/abs/1807.11164>`_.\r\n\r\n    Args:\r\n        pretrained (bool): If True, returns a models pre-trained on ImageNet\r\n        progress (bool): If True, displays a progress bar of the download to stderr\r\n    """"""\r\n    return _shufflenetv2(\'shufflenetv2_x2.0\', pretrained, progress,\r\n                         [4, 8, 4], [24, 244, 488, 976, 2048], **kwargs)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    import time\r\n\r\n    device = torch.device(\'cpu\')\r\n    net = shufflenet_v2_x1_0(pretrained=True)\r\n    net.eval()\r\n    x = torch.zeros(1, 3, 640, 640).to(device)\r\n    start = time.time()\r\n    y = net(x)\r\n    print(time.time() - start)\r\n    for u in y:\r\n        print(u.shape)\r\n    # torch.save(net.state_dict(), f\'shufflenet_v2_x2_0.pth\')\r\n'"
models/__init__.py,0,b'# -*- coding: utf-8 -*-\r\n# @Time    : 2019/1/2 18:18\r\n# @Author  : zhoujun\r\nfrom .model import PSENet'
models/loss.py,8,"b'# -*- coding: utf-8 -*-\n# @Time    : 3/29/19 11:03 AM\n# @Author  : zhoujun\nimport torch\nfrom torch import nn\nimport numpy as np\n\n\nclass PSELoss(nn.Module):\n    def __init__(self, Lambda, ratio=3, reduction=\'mean\'):\n        """"""Implement PSE Loss.\n        """"""\n        super(PSELoss, self).__init__()\n        assert reduction in [\'mean\', \'sum\'], "" reduction must in [\'mean\',\'sum\']""\n        self.Lambda = Lambda\n        self.ratio = ratio\n        self.reduction = reduction\n\n    def forward(self, outputs, labels, training_masks):\n        texts = outputs[:, -1, :, :]\n        kernels = outputs[:, :-1, :, :]\n        gt_texts = labels[:, -1, :, :]\n        gt_kernels = labels[:, :-1, :, :]\n\n        selected_masks = self.ohem_batch(texts, gt_texts, training_masks)\n        selected_masks = selected_masks.to(outputs.device)\n\n        loss_text = self.dice_loss(texts, gt_texts, selected_masks)\n\n        loss_kernels = []\n        mask0 = torch.sigmoid(texts).data.cpu().numpy()\n        mask1 = training_masks.data.cpu().numpy()\n        selected_masks = ((mask0 > 0.5) & (mask1 > 0.5)).astype(\'float32\')\n        selected_masks = torch.from_numpy(selected_masks).float()\n        selected_masks = selected_masks.to(outputs.device)\n        kernels_num = gt_kernels.size()[1]\n        for i in range(kernels_num):\n            kernel_i = kernels[:, i, :, :]\n            gt_kernel_i = gt_kernels[:, i, :, :]\n            loss_kernel_i = self.dice_loss(kernel_i, gt_kernel_i, selected_masks)\n            loss_kernels.append(loss_kernel_i)\n        loss_kernels = torch.stack(loss_kernels).mean(0)\n        if self.reduction == \'mean\':\n            loss_text = loss_text.mean()\n            loss_kernels = loss_kernels.mean()\n        elif self.reduction == \'sum\':\n            loss_text = loss_text.sum()\n            loss_kernels = loss_kernels.sum()\n\n        loss = self.Lambda * loss_text + (1 - self.Lambda) * loss_kernels\n        return loss_text, loss_kernels, loss\n\n    def dice_loss(self, input, target, mask):\n        input = torch.sigmoid(input)\n\n        input = input.contiguous().view(input.size()[0], -1)\n        target = target.contiguous().view(target.size()[0], -1)\n        mask = mask.contiguous().view(mask.size()[0], -1)\n\n        input = input * mask\n        target = target * mask\n\n        a = torch.sum(input * target, 1)\n        b = torch.sum(input * input, 1) + 0.001\n        c = torch.sum(target * target, 1) + 0.001\n        d = (2 * a) / (b + c)\n        return 1 - d\n\n    def ohem_single(self, score, gt_text, training_mask):\n        pos_num = (int)(np.sum(gt_text > 0.5)) - (int)(np.sum((gt_text > 0.5) & (training_mask <= 0.5)))\n\n        if pos_num == 0:\n            # selected_mask = gt_text.copy() * 0 # may be not good\n            selected_mask = training_mask\n            selected_mask = selected_mask.reshape(1, selected_mask.shape[0], selected_mask.shape[1]).astype(\'float32\')\n            return selected_mask\n\n        neg_num = (int)(np.sum(gt_text <= 0.5))\n        neg_num = (int)(min(pos_num * 3, neg_num))\n\n        if neg_num == 0:\n            selected_mask = training_mask\n            selected_mask = selected_mask.reshape(1, selected_mask.shape[0], selected_mask.shape[1]).astype(\'float32\')\n            return selected_mask\n\n        neg_score = score[gt_text <= 0.5]\n        # \xe5\xb0\x86\xe8\xb4\x9f\xe6\xa0\xb7\xe6\x9c\xac\xe5\xbe\x97\xe5\x88\x86\xe4\xbb\x8e\xe9\xab\x98\xe5\x88\xb0\xe4\xbd\x8e\xe6\x8e\x92\xe5\xba\x8f\n        neg_score_sorted = np.sort(-neg_score)\n        threshold = -neg_score_sorted[neg_num - 1]\n        # \xe9\x80\x89\xe5\x87\xba \xe5\xbe\x97\xe5\x88\x86\xe9\xab\x98\xe7\x9a\x84 \xe8\xb4\x9f\xe6\xa0\xb7\xe6\x9c\xac \xe5\x92\x8c\xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac \xe7\x9a\x84 mask\n        selected_mask = ((score >= threshold) | (gt_text > 0.5)) & (training_mask > 0.5)\n        selected_mask = selected_mask.reshape(1, selected_mask.shape[0], selected_mask.shape[1]).astype(\'float32\')\n        return selected_mask\n\n    def ohem_batch(self, scores, gt_texts, training_masks):\n        scores = scores.data.cpu().numpy()\n        gt_texts = gt_texts.data.cpu().numpy()\n        training_masks = training_masks.data.cpu().numpy()\n\n        selected_masks = []\n        for i in range(scores.shape[0]):\n            selected_masks.append(self.ohem_single(scores[i, :, :], gt_texts[i, :, :], training_masks[i, :, :]))\n\n        selected_masks = np.concatenate(selected_masks, 0)\n        selected_masks = torch.from_numpy(selected_masks).float()\n\n        return selected_masks\n'"
models/mobilenetv3.py,6,"b""# -*- coding: utf-8 -*-\r\n# @Time    : 2019/5/23 15:22\r\n# @Author  : zhoujun\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.nn import init\r\n\r\n\r\nclass hswish(nn.Module):\r\n    def forward(self, x):\r\n        out = x * F.relu6(x + 3, inplace=True) / 6\r\n        return out\r\n\r\n\r\nclass hsigmoid(nn.Module):\r\n    def forward(self, x):\r\n        out = F.relu6(x + 3, inplace=True) / 6\r\n        return out\r\n\r\n\r\nclass SeModule(nn.Module):\r\n    def __init__(self, in_size, reduction=4):\r\n        super(SeModule, self).__init__()\r\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\r\n\r\n        self.se = nn.Sequential(\r\n            nn.Conv2d(in_size, in_size // reduction, kernel_size=1, stride=1, padding=0, bias=False),\r\n            nn.BatchNorm2d(in_size // reduction),\r\n            nn.ReLU(inplace=True),\r\n            nn.Conv2d(in_size // reduction, in_size, kernel_size=1, stride=1, padding=0, bias=False),\r\n            nn.BatchNorm2d(in_size),\r\n            hsigmoid()\r\n        )\r\n\r\n    def forward(self, x):\r\n        return x * self.se(x)\r\n\r\n\r\nclass Block(nn.Module):\r\n    '''expand + depthwise + pointwise'''\r\n\r\n    def __init__(self, kernel_size, in_size, expand_size, out_size, nolinear, semodule, stride):\r\n        super(Block, self).__init__()\r\n        self.stride = stride\r\n        self.se = semodule\r\n\r\n        self.conv1 = nn.Conv2d(in_size, expand_size, kernel_size=1, stride=1, padding=0, bias=False)\r\n        self.bn1 = nn.BatchNorm2d(expand_size)\r\n        self.nolinear1 = nolinear\r\n        self.conv2 = nn.Conv2d(expand_size, expand_size, kernel_size=kernel_size, stride=stride,\r\n                               padding=kernel_size // 2, groups=expand_size, bias=False)\r\n        self.bn2 = nn.BatchNorm2d(expand_size)\r\n        self.nolinear2 = nolinear\r\n        self.conv3 = nn.Conv2d(expand_size, out_size, kernel_size=1, stride=1, padding=0, bias=False)\r\n        self.bn3 = nn.BatchNorm2d(out_size)\r\n\r\n        self.shortcut = nn.Sequential()\r\n        if stride == 1 and in_size != out_size:\r\n            self.shortcut = nn.Sequential(\r\n                nn.Conv2d(in_size, out_size, kernel_size=1, stride=1, padding=0, bias=False),\r\n                nn.BatchNorm2d(out_size),\r\n            )\r\n\r\n    def forward(self, x):\r\n        out = self.nolinear1(self.bn1(self.conv1(x)))\r\n        out = self.nolinear2(self.bn2(self.conv2(out)))\r\n        out = self.bn3(self.conv3(out))\r\n        if self.se != None:\r\n            out = self.se(out)\r\n        out = out + self.shortcut(x) if self.stride == 1 else out\r\n        return out\r\n\r\n\r\nclass MobileNetV3_Large(nn.Module):\r\n    def __init__(self, pretrained):\r\n        super(MobileNetV3_Large, self).__init__()\r\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1, bias=False)\r\n        self.bn1 = nn.BatchNorm2d(16)\r\n        self.hs1 = hswish()\r\n\r\n        self.layer1 = nn.Sequential(\r\n            Block(3, 16, 16, 16, nn.ReLU(inplace=True), None, 1),\r\n            Block(3, 16, 64, 24, nn.ReLU(inplace=True), None, 2),\r\n            Block(3, 24, 72, 24, nn.ReLU(inplace=True), None, 1),\r\n        )\r\n\r\n        self.layer2 = nn.Sequential(\r\n            Block(5, 24, 72, 40, nn.ReLU(inplace=True), SeModule(40), 2),\r\n            Block(5, 40, 120, 40, nn.ReLU(inplace=True), SeModule(40), 1),\r\n            Block(5, 40, 120, 40, nn.ReLU(inplace=True), SeModule(40), 1),\r\n        )\r\n\r\n        self.layer3 = nn.Sequential(\r\n            Block(3, 40, 240, 80, hswish(), None, 2),\r\n            Block(3, 80, 200, 80, hswish(), None, 1),\r\n            Block(3, 80, 184, 80, hswish(), None, 1),\r\n            Block(3, 80, 184, 80, hswish(), None, 1),\r\n            Block(3, 80, 480, 112, hswish(), SeModule(112), 1),\r\n            Block(3, 112, 672, 112, hswish(), SeModule(112), 1),\r\n            Block(5, 112, 672, 160, hswish(), SeModule(160), 1),\r\n        )\r\n        self.layer4 = nn.Sequential(\r\n            Block(5, 160, 672, 160, hswish(), SeModule(160), 2),\r\n            Block(5, 160, 960, 160, hswish(), SeModule(160), 1),\r\n        )\r\n        self.conv2 = nn.Conv2d(160, 960, kernel_size=1, stride=1, padding=0, bias=False)\r\n        self.bn2 = nn.BatchNorm2d(960)\r\n        self.hs2 = hswish()\r\n        self.init_params()\r\n\r\n    def init_params(self):\r\n        for m in self.modules():\r\n            if isinstance(m, nn.Conv2d):\r\n                init.kaiming_normal_(m.weight, mode='fan_out')\r\n                if m.bias is not None:\r\n                    init.constant_(m.bias, 0)\r\n            elif isinstance(m, nn.BatchNorm2d):\r\n                init.constant_(m.weight, 1)\r\n                init.constant_(m.bias, 0)\r\n            elif isinstance(m, nn.Linear):\r\n                init.normal_(m.weight, std=0.001)\r\n                if m.bias is not None:\r\n                    init.constant_(m.bias, 0)\r\n\r\n    def forward(self, x):\r\n        c1 = self.hs1(self.bn1(self.conv1(x)))\r\n        c2 = self.layer1(c1)\r\n        c3 = self.layer2(c2)\r\n        c4 = self.layer3(c3)\r\n        c5 = self.layer4(c4)\r\n        # c5 = self.hs2(self.bn2(self.conv2(c5)))\r\n        return c1, c2, c3, c4, c5\r\n\r\n\r\nclass MobileNetV3_Small(nn.Module):\r\n    def __init__(self, pretrained):\r\n        super(MobileNetV3_Small, self).__init__()\r\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1, bias=False)\r\n        self.bn1 = nn.BatchNorm2d(16)\r\n        self.hs1 = hswish()\r\n\r\n        self.layer1 = Block(3, 16, 16, 16, nn.ReLU(inplace=True), SeModule(16), 2)\r\n        self.layer2 = nn.Sequential(\r\n            Block(3, 16, 72, 24, nn.ReLU(inplace=True), None, 2),\r\n            Block(3, 24, 88, 24, nn.ReLU(inplace=True), None, 1),\r\n        )\r\n\r\n        self.layer3 = nn.Sequential(\r\n            Block(5, 24, 96, 40, hswish(), SeModule(40), 2),\r\n            Block(5, 40, 240, 40, hswish(), SeModule(40), 1),\r\n            Block(5, 40, 240, 40, hswish(), SeModule(40), 1),\r\n            Block(5, 40, 120, 48, hswish(), SeModule(48), 1),\r\n            Block(5, 48, 144, 48, hswish(), SeModule(48), 1),\r\n        )\r\n        self.layer4 = nn.Sequential(\r\n            Block(5, 48, 288, 96, hswish(), SeModule(96), 2),\r\n            Block(5, 96, 576, 96, hswish(), SeModule(96), 1),\r\n            Block(5, 96, 576, 96, hswish(), SeModule(96), 1),\r\n        )\r\n        self.conv2 = nn.Conv2d(96, 576, kernel_size=1, stride=1, padding=0, bias=False)\r\n        self.bn2 = nn.BatchNorm2d(576)\r\n        self.hs2 = hswish()\r\n        self.init_params()\r\n\r\n    def init_params(self):\r\n        for m in self.modules():\r\n            if isinstance(m, nn.Conv2d):\r\n                init.kaiming_normal_(m.weight, mode='fan_out')\r\n                if m.bias is not None:\r\n                    init.constant_(m.bias, 0)\r\n            elif isinstance(m, nn.BatchNorm2d):\r\n                init.constant_(m.weight, 1)\r\n                init.constant_(m.bias, 0)\r\n            elif isinstance(m, nn.Linear):\r\n                init.normal_(m.weight, std=0.001)\r\n                if m.bias is not None:\r\n                    init.constant_(m.bias, 0)\r\n\r\n    def forward(self, x):\r\n        c1 = self.hs1(self.bn1(self.conv1(x)))\r\n        c2 = self.layer1(c1)\r\n        c3 = self.layer2(c2)\r\n        c4 = self.layer3(c3)\r\n        c5 = self.layer4(c4)\r\n        # c5 = self.hs2(self.bn2(self.conv2(c5)))\r\n        return c1, c2, c3, c4, c5\r\n\r\n\r\nif __name__ == '__main__':\r\n    import time\r\n\r\n    device = torch.device('cpu')\r\n    net = MobileNetV3_Large(pretrained=False)\r\n    net.eval()\r\n    x = torch.zeros(1, 3, 608, 800).to(device)\r\n    start = time.time()\r\n    y = net(x)\r\n    print(time.time() - start)\r\n    for u in y:\r\n        print(u.shape)\r\n    torch.save(net.state_dict(), f'MobileNetV3_Large111.pth')\r\n"""
models/model.py,6,"b""# -*- coding: utf-8 -*-\n# @Time    : 2019/1/2 17:29\n# @Author  : zhoujun\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom models.resnet import resnet18, resnet34, resnet50, resnet101, resnet152\nfrom models.mobilenetv3 import MobileNetV3_Large, MobileNetV3_Small\nfrom models.ShuffleNetV2 import shufflenet_v2_x1_0\n\nd = {'resnet18': {'models': resnet18, 'out': [64, 128, 256, 512]},\n     'resnet34': {'models': resnet34, 'out': [64, 128, 256, 512]},\n     'resnet50': {'models': resnet50, 'out': [256, 512, 1024, 2048]},\n     'resnet101': {'models': resnet101, 'out': [256, 512, 1024, 2048]},\n     'resnet152': {'models': resnet152, 'out': [256, 512, 1024, 2048]},\n     'MobileNetV3_Large': {'models': MobileNetV3_Large, 'out': [24, 40, 160, 160]},\n     'MobileNetV3_Small': {'models': MobileNetV3_Small, 'out': [16, 24, 48, 96]},\n     'shufflenetv2': {'models': shufflenet_v2_x1_0, 'out': [24, 116, 232, 464]}}\ninplace = True\n\n\nclass PSENet(nn.Module):\n    def __init__(self, backbone, result_num=6, scale: int = 1, pretrained=False):\n        super(PSENet, self).__init__()\n        assert backbone in d, 'backbone must in: {}'.format(d)\n        self.scale = scale\n        conv_out = 256\n        model, out = d[backbone]['models'], d[backbone]['out']\n        self.backbone = model(pretrained=pretrained)\n        # Reduce channels\n        # Top layer\n        self.toplayer = nn.Sequential(nn.Conv2d(out[3], conv_out, kernel_size=1, stride=1, padding=0),\n                                      nn.BatchNorm2d(conv_out),\n                                      nn.ReLU(inplace=inplace)\n                                      )\n        # Lateral layers\n        self.latlayer1 = nn.Sequential(nn.Conv2d(out[2], conv_out, kernel_size=1, stride=1, padding=0),\n                                       nn.BatchNorm2d(conv_out),\n                                       nn.ReLU(inplace=inplace)\n                                       )\n        self.latlayer2 = nn.Sequential(nn.Conv2d(out[1], conv_out, kernel_size=1, stride=1, padding=0),\n                                       nn.BatchNorm2d(conv_out),\n                                       nn.ReLU(inplace=inplace)\n                                       )\n        self.latlayer3 = nn.Sequential(nn.Conv2d(out[0], conv_out, kernel_size=1, stride=1, padding=0),\n                                       nn.BatchNorm2d(conv_out),\n                                       nn.ReLU(inplace=inplace)\n                                       )\n\n        # Smooth layers\n        self.smooth1 = nn.Sequential(nn.Conv2d(conv_out, conv_out, kernel_size=3, stride=1, padding=1),\n                                     nn.BatchNorm2d(conv_out),\n                                     nn.ReLU(inplace=inplace)\n                                     )\n        self.smooth2 = nn.Sequential(nn.Conv2d(conv_out, conv_out, kernel_size=3, stride=1, padding=1),\n                                     nn.BatchNorm2d(conv_out),\n                                     nn.ReLU(inplace=inplace)\n                                     )\n        self.smooth3 = nn.Sequential(nn.Conv2d(conv_out, conv_out, kernel_size=3, stride=1, padding=1),\n                                     nn.BatchNorm2d(conv_out),\n                                     nn.ReLU(inplace=inplace)\n                                     )\n\n        self.conv = nn.Sequential(\n            nn.Conv2d(conv_out * 4, conv_out, kernel_size=3, padding=1, stride=1),\n            nn.BatchNorm2d(conv_out),\n            nn.ReLU(inplace=inplace)\n        )\n        self.out_conv = nn.Conv2d(conv_out, result_num, kernel_size=1, stride=1)\n\n    def forward(self, input: torch.Tensor):\n        _, _, H, W = input.size()\n        c2, c3, c4, c5 = self.backbone(input)\n        # Top-down\n        p5 = self.toplayer(c5)\n        p4 = self._upsample_add(p5, self.latlayer1(c4))\n        p4 = self.smooth1(p4)\n        p3 = self._upsample_add(p4, self.latlayer2(c3))\n        p3 = self.smooth2(p3)\n        p2 = self._upsample_add(p3, self.latlayer3(c2))\n        p2 = self.smooth3(p2)\n\n        x = self._upsample_cat(p2, p3, p4, p5)\n        x = self.conv(x)\n        x = self.out_conv(x)\n\n        if self.train:\n            x = F.interpolate(x, size=(H, W), mode='bilinear', align_corners=True)\n        else:\n            x = F.interpolate(x, size=(H // self.scale, W // self.scale), mode='bilinear', align_corners=True)\n        return x\n\n    def _upsample_add(self, x, y):\n        return F.interpolate(x, size=y.size()[2:], mode='bilinear', align_corners=False) + y\n\n    def _upsample_cat(self, p2, p3, p4, p5):\n        h, w = p2.size()[2:]\n        p3 = F.interpolate(p3, size=(h, w), mode='bilinear', align_corners=False)\n        p4 = F.interpolate(p4, size=(h, w), mode='bilinear', align_corners=False)\n        p5 = F.interpolate(p5, size=(h, w), mode='bilinear', align_corners=False)\n        return torch.cat([p2, p3, p4, p5], dim=1)\n\n\nif __name__ == '__main__':\n    import time\n\n    device = torch.device('cpu')\n    backbone = 'shufflenetv2'\n    net = PSENet(backbone=backbone, pretrained=False, result_num=6).to(device)\n    net.eval()\n    x = torch.zeros(1, 3, 512, 512).to(device)\n    start = time.time()\n    y = net(x)\n    print(time.time() - start)\n    print(y.shape)\n    # torch.save(net.state_dict(),f'{backbone}.pth')\n"""
models/resnet.py,8,"b'# -*- coding: utf-8 -*-\r\n# @Time    : 2019/1/2 17:30\r\n# @Author  : zhoujun\r\nimport torch\r\nimport torch.nn as nn\r\nimport math\r\nimport logging\r\nimport torch.utils.model_zoo as model_zoo\r\nimport torchvision.models.resnet\r\n\r\nlogger = logging.getLogger(\'project\')\r\n\r\n__all__ = [\'ResNet\', \'resnet50\', \'resnet101\',\r\n           \'resnet152\']\r\n\r\nmodel_urls = {\r\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\r\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\r\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\r\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\r\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\r\n}\r\n\r\n\r\ndef conv3x3(in_planes, out_planes, stride=1):\r\n    """"""3x3 convolution with padding""""""\r\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\r\n                     padding=1, bias=False)\r\n\r\n\r\nclass BasicBlock(nn.Module):\r\n    expansion = 1\r\n\r\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\r\n        super(BasicBlock, self).__init__()\r\n        self.conv1 = conv3x3(inplanes, planes, stride)\r\n        self.bn1 = nn.BatchNorm2d(planes)\r\n        self.relu = nn.ReLU(inplace=True)\r\n        self.conv2 = conv3x3(planes, planes)\r\n        self.bn2 = nn.BatchNorm2d(planes)\r\n        self.downsample = downsample\r\n        self.stride = stride\r\n\r\n    def forward(self, x):\r\n        residual = x\r\n\r\n        out = self.conv1(x)\r\n        out = self.bn1(out)\r\n        out = self.relu(out)\r\n\r\n        out = self.conv2(out)\r\n        out = self.bn2(out)\r\n\r\n        if self.downsample is not None:\r\n            residual = self.downsample(x)\r\n\r\n        out += residual\r\n        out = self.relu(out)\r\n\r\n        return out\r\n\r\n\r\nclass Bottleneck(nn.Module):\r\n    expansion = 4\r\n\r\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\r\n        super(Bottleneck, self).__init__()\r\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\r\n        self.bn1 = nn.BatchNorm2d(planes)\r\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\r\n                               padding=1, bias=False)\r\n        self.bn2 = nn.BatchNorm2d(planes)\r\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\r\n        self.bn3 = nn.BatchNorm2d(planes * 4)\r\n        self.relu = nn.ReLU(inplace=True)\r\n        self.downsample = downsample\r\n        self.stride = stride\r\n\r\n    def forward(self, x):\r\n        residual = x\r\n\r\n        out = self.conv1(x)\r\n        out = self.bn1(out)\r\n        out = self.relu(out)\r\n\r\n        out = self.conv2(out)\r\n        out = self.bn2(out)\r\n        out = self.relu(out)\r\n\r\n        out = self.conv3(out)\r\n        out = self.bn3(out)\r\n\r\n        if self.downsample is not None:\r\n            residual = self.downsample(x)\r\n\r\n        out += residual\r\n        out = self.relu(out)\r\n\r\n        return out\r\n\r\n\r\nclass ResNet(nn.Module):\r\n\r\n    def __init__(self, block, layers):\r\n        self.inplanes = 64\r\n        super(ResNet, self).__init__()\r\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\r\n                               bias=False)\r\n        self.bn1 = nn.BatchNorm2d(64)\r\n        self.relu = nn.ReLU(inplace=True)\r\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\r\n        self.layer1 = self._make_layer(block, 64, layers[0])\r\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\r\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\r\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\r\n\r\n        for m in self.modules():\r\n            if isinstance(m, nn.Conv2d):\r\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\r\n                m.weight.data.normal_(0, math.sqrt(2. / n))\r\n            elif isinstance(m, nn.BatchNorm2d):\r\n                m.weight.data.fill_(1)\r\n                m.bias.data.zero_()\r\n\r\n    def _make_layer(self, block, planes, blocks, stride=1):\r\n        downsample = None\r\n        if stride != 1 or self.inplanes != planes * block.expansion:\r\n            downsample = nn.Sequential(\r\n                nn.Conv2d(self.inplanes, planes * block.expansion,\r\n                          kernel_size=1, stride=stride, bias=False),\r\n                nn.BatchNorm2d(planes * block.expansion),\r\n            )\r\n\r\n        layers = []\r\n        layers.append(block(self.inplanes, planes, stride, downsample))\r\n        self.inplanes = planes * block.expansion\r\n        for i in range(1, blocks):\r\n            layers.append(block(self.inplanes, planes))\r\n\r\n        return nn.Sequential(*layers)\r\n\r\n    def _load_pretrained_model(self, model_url):\r\n        pretrain_dict = model_zoo.load_url(model_url)\r\n        model_dict = {}\r\n        state_dict = self.state_dict()\r\n        for k, v in pretrain_dict.items():\r\n            if k in state_dict:\r\n                model_dict[k] = v\r\n        state_dict.update(model_dict)\r\n        self.load_state_dict(state_dict)\r\n        logger.info(\'load pretrained models from imagenet\')\r\n\r\n    def forward(self, input):\r\n        x = self.conv1(input)\r\n        x = self.bn1(x)\r\n        x = self.relu(x)\r\n        x = self.maxpool(x)\r\n        c2 = self.layer1(x)\r\n        c3 = self.layer2(c2)\r\n        c4 = self.layer3(c3)\r\n        c5 = self.layer4(c4)\r\n        return c2, c3, c4, c5\r\n\r\ndef resnet18(pretrained=False, **kwargs):\r\n    """"""Constructs a ResNet-18 models.\r\n\r\n    Args:\r\n        pretrained (bool): If True, returns a models pre-trained on ImageNet\r\n    """"""\r\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\r\n    if pretrained:\r\n        model._load_pretrained_model(model_urls[\'resnet18\'])\r\n    return model\r\n\r\n\r\ndef resnet34(pretrained=False, **kwargs):\r\n    """"""Constructs a ResNet-34 models.\r\n\r\n    Args:\r\n        pretrained (bool): If True, returns a models pre-trained on ImageNet\r\n    """"""\r\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\r\n    if pretrained:\r\n        model._load_pretrained_model(model_urls[\'resnet34\'])\r\n    return model\r\n\r\ndef resnet50(pretrained=False, **kwargs):\r\n    """"""Constructs a ResNet-50 models.\r\n\r\n    Args:\r\n        pretrained (bool): If True, returns a models pre-trained on ImageNet\r\n    """"""\r\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\r\n    if pretrained:\r\n        model._load_pretrained_model(model_urls[\'resnet50\'])\r\n    return model\r\n\r\n\r\ndef resnet101(pretrained=False, **kwargs):\r\n    """"""Constructs a ResNet-101 models.\r\n\r\n    Args:\r\n        pretrained (bool): If True, returns a models pre-trained on ImageNet\r\n    """"""\r\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\r\n    if pretrained:\r\n        model._load_pretrained_model(model_urls[\'resnet101\'])\r\n    return model\r\n\r\n\r\ndef resnet152(pretrained=False, **kwargs):\r\n    """"""Constructs a ResNet-152 models.\r\n\r\n    Args:\r\n        pretrained (bool): If True, returns a models pre-trained on ImageNet\r\n    """"""\r\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\r\n    if pretrained:\r\n        model._load_pretrained_model(model_urls[\'resnet152\'])\r\n    return model\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    x = torch.zeros(1, 3, 640, 640)\r\n    net = resnet50()\r\n    y = net(x)\r\n    for u in y:\r\n        print(u.shape)\r\n'"
pse/__init__.py,1,"b'import subprocess\nimport os\nimport numpy as np\nimport cv2\nimport torch\n\nBASE_DIR = os.path.dirname(os.path.realpath(__file__))\n\nif subprocess.call([\'make\', \'-C\', BASE_DIR]) != 0:  # return value\n    raise RuntimeError(\'Cannot compile pse: {}\'.format(BASE_DIR))\n\ndef pse_warpper(kernals, min_area=5):\n    \'\'\'\n    reference https://github.com/liuheng92/tensorflow_PSENet/blob/feature_dev/pse\n    :param kernals:\n    :param min_area:\n    :return:\n    \'\'\'\n    from .pse import pse_cpp\n    kernal_num = len(kernals)\n    if not kernal_num:\n        return np.array([]), []\n    kernals = np.array(kernals)\n    label_num, label = cv2.connectedComponents(kernals[0].astype(np.uint8), connectivity=4)\n    label_values = []\n    for label_idx in range(1, label_num):\n        if np.sum(label == label_idx) < min_area:\n            label[label == label_idx] = 0\n            continue\n        label_values.append(label_idx)\n\n    pred = pse_cpp(label, kernals, c=kernal_num)\n\n    return np.array(pred), label_values\n\n\ndef decode(preds, scale, threshold=0.7311):\n    """"""\n    \xe5\x9c\xa8\xe8\xbe\x93\xe5\x87\xba\xe4\xb8\x8a\xe4\xbd\xbf\xe7\x94\xa8sigmoid \xe5\xb0\x86\xe5\x80\xbc\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xba\xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6\xef\xbc\x8c\xe5\xb9\xb6\xe4\xbd\xbf\xe7\x94\xa8\xe9\x98\x88\xe5\x80\xbc\xe6\x9d\xa5\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x96\x87\xe5\xad\x97\xe5\x92\x8c\xe8\x83\x8c\xe6\x99\xaf\xe7\x9a\x84\xe5\x8c\xba\xe5\x88\x86\n    :param preds: \xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\n    :param scale: \xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84scale\n    :param threshold: sigmoid\xe7\x9a\x84\xe9\x98\x88\xe5\x80\xbc\n    :return: \xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\x9b\xbe\xe5\x92\x8c\xe6\x96\x87\xe6\x9c\xac\xe6\xa1\x86\n    """"""\n    preds = torch.sigmoid(preds)\n    preds = preds.detach().cpu().numpy()\n\n    score = preds[-1].astype(np.float32)\n    preds = preds > threshold\n    # preds = preds * preds[-1] # \xe4\xbd\xbf\xe7\x94\xa8\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84kernel\xe4\xbd\x9c\xe4\xb8\xba\xe5\x85\xb6\xe4\xbb\x96\xe5\xb0\x8f\xe5\x9b\xbe\xe7\x9a\x84mask,\xe4\xb8\x8d\xe4\xbd\xbf\xe7\x94\xa8\xe7\x9a\x84\xe8\xaf\x9d\xe6\x95\x88\xe6\x9e\x9c\xe6\x9b\xb4\xe5\xa5\xbd\n    pred, label_values = pse_warpper(preds, 5)\n    bbox_list = []\n    for label_value in label_values:\n        points = np.array(np.where(pred == label_value)).transpose((1, 0))[:, ::-1]\n\n        if points.shape[0] < 800 / (scale * scale):\n            continue\n\n        score_i = np.mean(score[pred == label_value])\n        if score_i < 0.93:\n            continue\n\n        rect = cv2.minAreaRect(points)\n        bbox = cv2.boxPoints(rect)\n        bbox_list.append([bbox[1], bbox[2], bbox[3], bbox[0]])\n    return pred, np.array(bbox_list)\n'"
utils/__init__.py,0,b'# -*- coding: utf-8 -*-\n# @Time    : 3/22/19 11:45 AM\n# @Author  : zhoujun\nfrom .utils import *'
utils/lr_scheduler.py,1,"b'# -*- coding: utf-8 -*-\n# @Time    : 1/19/19 3:37 PM\n# @Author  : zhoujun\nfrom torch.optim.lr_scheduler import MultiStepLR\n\n\nclass WarmupMultiStepLR(MultiStepLR):\n    def __init__(self, optimizer, milestones, gamma=0.1, warmup_factor=1.0 / 3,\n                 warmup_iters=500, last_epoch=-1):\n        self.warmup_factor = warmup_factor\n        self.warmup_iters = warmup_iters\n        super().__init__(optimizer, milestones, gamma, last_epoch)\n\n    def get_lr(self):\n        lr = super().get_lr()\n        if self.last_epoch < self.warmup_iters:\n            alpha = self.last_epoch / self.warmup_iters\n            warmup_factor = self.warmup_factor * (1 - alpha) + alpha\n            return [l * warmup_factor for l in lr]\n        return lr'"
utils/utils.py,2,"b'# -*- coding: utf-8 -*-\n# @Time    : 1/4/19 11:18 AM\n# @Author  : zhoujun\nimport cv2\nimport time\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef show_img(imgs: np.ndarray, color=False):\n    if (len(imgs.shape) == 3 and color) or (len(imgs.shape) == 2 and not color):\n        imgs = np.expand_dims(imgs, axis=0)\n    for img in imgs:\n        plt.figure()\n        plt.imshow(img, cmap=None if color else \'gray\')\n\n\ndef draw_bbox(img_path, result, color=(255, 0, 0),thickness=2):\n    if isinstance(img_path, str):\n        img_path = cv2.imread(img_path)\n        # img_path = cv2.cvtColor(img_path, cv2.COLOR_BGR2RGB)\n    img_path = img_path.copy()\n    for point in result:\n        point = point.astype(int)\n        cv2.line(img_path, tuple(point[0]), tuple(point[1]), color, thickness)\n        cv2.line(img_path, tuple(point[1]), tuple(point[2]), color, thickness)\n        cv2.line(img_path, tuple(point[2]), tuple(point[3]), color, thickness)\n        cv2.line(img_path, tuple(point[3]), tuple(point[0]), color, thickness)\n    return img_path\n\n\ndef setup_logger(log_file_path: str = None):\n    import logging\n    from colorlog import ColoredFormatter\n    logging.basicConfig(filename=log_file_path, format=\'%(asctime)s %(levelname)-8s %(filename)s: %(message)s\',\n                        # \xe5\xae\x9a\xe4\xb9\x89\xe8\xbe\x93\xe5\x87\xbalog\xe7\x9a\x84\xe6\xa0\xbc\xe5\xbc\x8f\n                        datefmt=\'%Y-%m-%d %H:%M:%S\', )\n    """"""Return a logger with a default ColoredFormatter.""""""\n    formatter = ColoredFormatter(""%(asctime)s %(log_color)s%(levelname)-8s %(reset)s %(filename)s: %(message)s"",\n                                 datefmt=\'%Y-%m-%d %H:%M:%S\',\n                                 reset=True,\n                                 log_colors={\n                                     \'DEBUG\': \'blue\',\n                                     \'INFO\': \'green\',\n                                     \'WARNING\': \'yellow\',\n                                     \'ERROR\': \'red\',\n                                     \'CRITICAL\': \'red\',\n                                 })\n\n    logger = logging.getLogger(\'project\')\n    handler = logging.StreamHandler()\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n    logger.setLevel(logging.DEBUG)\n    logger.info(\'logger init finished\')\n    return logger\n\n\ndef save_checkpoint(checkpoint_path, model, optimizer, epoch, logger):\n    state = {\'state_dict\': model.state_dict(),\n             \'optimizer\': optimizer.state_dict(),\n             \'epoch\': epoch}\n    torch.save(state, checkpoint_path)\n    logger.info(\'models saved to %s\' % checkpoint_path)\n\n\ndef load_checkpoint(checkpoint_path, model, logger, device, optimizer=None):\n    state = torch.load(checkpoint_path, map_location=device)\n    model.load_state_dict(state[\'state_dict\'])\n    if optimizer is not None:\n        optimizer.load_state_dict(state[\'optimizer\'])\n    start_epoch = state[\'epoch\']\n    logger.info(\'models loaded from %s\' % checkpoint_path)\n    return start_epoch\n\n\n# --exeTime\ndef exe_time(func):\n    def newFunc(*args, **args2):\n        t0 = time.time()\n        back = func(*args, **args2)\n        print(""{} cost {:.3f}s"".format(func.__name__, time.time() - t0))\n        return back\n    return newFunc\n'"
