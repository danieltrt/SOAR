file_path,api_count,code
cspn_paddle/demo.py,0,"b""'''\n    CSPN module demo\n'''\n\nimport argparse\nimport numpy as np\nimport paddle.fluid as fluid\n\n\nclass CSPN():\n    '''\n        cspn module\n    '''\n    def __init__(self, dim_num, feat_chan, prop_kernel, prop_step):\n        self.dim_num = dim_num\n        self.feat_chan = feat_chan\n        self.prop_kernel = prop_kernel\n        self.prop_step = prop_step\n\n    def cspn(self, guide, feat):\n        '''\n            cspn func\n        '''\n        guide = fluid.layers.abs(guide)\n        guide_num = guide.shape[1] // feat.shape[1]\n        expand_times = [1, guide_num, *([1] * self.dim_num)]\n        layer_name = 'cspn_affinity_propagate'\n        if feat.shape[1] > 1:\n            cspn_feat = list()\n            for channel_ind in range(feat.shape[1]):\n                slice_guide = fluid.layers.slice(\n                    guide, axes=[1], starts=[channel_ind * guide_num],\n                    ends=[(channel_ind + 1) * guide_num])\n                normalizer = fluid.layers.reduce_sum(slice_guide, dim=1, keep_dim=True)\n                normalizer = fluid.layers.expand(normalizer, expand_times=expand_times)\n                slice_guide = fluid.layers.elementwise_div(slice_guide, normalizer)\n                slice_feat = fluid.layers.slice(feat, axes=[1], starts=[channel_ind],\n                                                ends=[channel_ind + 1])\n                for _ in range(self.prop_step):\n                    # gate_weight: normalized guidance, shared across all the channels\n                    slice_feat = fluid.layers.affinity_propagate(\n                        slice_feat, gate_weight=slice_guide, kernel_size=self.prop_kernel,\n                        name=layer_name)\n                cspn_feat.append(slice_feat)\n            cspn_feat = fluid.layers.concat(cspn_feat, axis=1)\n        else:\n            normalizer = fluid.layers.reduce_sum(guide, dim=1, keep_dim=True)\n            normalizer = fluid.layers.expand(normalizer, expand_times=expand_times)\n            guide = fluid.layers.elementwise_div(guide, normalizer)\n            for _ in range(self.prop_step):\n                feat = fluid.layers.affinity_propagate(\n                    feat, gate_weight=guide, kernel_size=self.prop_kernel, name=layer_name)\n            cspn_feat = feat\n        return cspn_feat\n\n    def demo(self, batch_size=3, iter_num=20, map_shape=None):\n        '''\n            func to run demo\n        '''\n        # define net\n        if map_shape is None:\n            map_shape = [48, 64, 128][3 - self.dim_num:]\n        else:\n            assert len(map_shape) == self.dim_num, '{}d map shape is required'.format(self.dim_num)\n        guide_chan = self.prop_kernel ** self.dim_num - 1\n        guide_shape = [self.feat_chan * guide_chan, *map_shape]\n        guide = fluid.layers.data(name='guide', shape=guide_shape)\n        feat_shape = [self.feat_chan, *map_shape]\n        feat = fluid.layers.data(name='feat', shape=feat_shape, stop_gradient=False)\n        cspn_feat = self.cspn(guide, feat)\n        print(cspn_feat)\n        output = fluid.layers.reduce_mean(cspn_feat)\n        # define optim\n        optim = fluid.optimizer.AdamOptimizer()\n        optim.minimize(output)\n        # initialize param\n        place = fluid.CUDAPlace(0)\n        exe = fluid.Executor(place)\n        exe.run(fluid.default_startup_program())\n        # train\n        for i in range(iter_num):\n            guide_data = np.random.rand(batch_size, *guide_shape).astype(np.float32)\n            feat_data = np.random.rand(batch_size, *feat_shape).astype(np.float32)\n            outs = exe.run(feed={'guide': guide_data, 'feat': feat_data}, fetch_list=[output.name])\n            print('iter={:02}  out={:.4f}'.format(i, outs[0][0]))\n\nif __name__ == '__main__':\n    PARSER = argparse.ArgumentParser()\n    PARSER.add_argument('--dimNum', choices=[2, 3], default=3, help='version of cspn module')\n    PARSER.add_argument('--featChan', type=int, default=1, help='#channels of feature')\n    PARSER.add_argument('--propKernel', choices=[3], default=3, help='kernel size for propagation')\n    PARSER.add_argument('--propStep', type=int, default=24, help='#steps to propagate')\n    ARGS = PARSER.parse_args()\n    MODULE = CSPN(\n        dim_num=ARGS.dimNum, feat_chan=ARGS.featChan,\n        prop_kernel=ARGS.propKernel, prop_step=ARGS.propStep)\n    MODULE.demo()\n"""
cspn_pytorch/data_transform.py,17,"b'""""""\nCreated on Thu Feb  1 19:31:56 2018\n@ author:  Xinjing Cheng\n@ email : chengxinjing@baidu.com\n""""""\n\nfrom __future__ import print_function, division\nimport os\nimport torch\nimport pandas as pd\nfrom skimage import io, transform\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nimport math\nimport random\nfrom PIL import Image, ImageOps\nimport numbers\nimport types\nimport scipy.ndimage as ndimage\ntry:\n    import accimage\nexcept ImportError:\n    accimage = None\n\n\'\'\'Set of tranform random routines that takes both input and target as arguments,\nin order to have random but coherent transformations.\ninputs are PIL Image pairs and targets are ndarrays\'\'\'\n\n\'\'\'use torchvision.transform and my own transform:\n    torchvision.transform function list:\n        ""Compose"", ""ToTensor"", ""ToPILImage"", ""Normalize"", ""Resize"", ""Scale"", ""CenterCrop"", ""Pad"",\n        ""Lambda"", ""RandomCrop"", ""RandomHorizontalFlip"", ""RandomVerticalFlip"", ""RandomResizedCrop"",\n        ""RandomSizedCrop"", ""FiveCrop"", ""TenCrop"", ""LinearTransformation"", ""ColorJitter"",\n        ""RandomRotation"", ""Grayscale"", ""RandomGrayscale""\n\n    my own transform function list:\n        ToTensor(without div(255)), ColorNormalize, DepthNormalize, Scale, CenterCropRectangle,\n\'\'\'\n\ndef _is_pil_image(img):\n    if accimage is not None:\n        return isinstance(img, (Image.Image, accimage.Image))\n    else:\n        return isinstance(img, Image.Image)\n\ndef _is_numpy_image(img):\n    return isinstance(img, np.ndarray) and (img.ndim in {2, 3})\n\ndef _is_tensor_image(img):\n    return torch.is_tensor(img) and img.ndimension() == 3\n\ndef to_pil_image(pic, mode=None):\n    """"""Convert a tensor or an ndarray to PIL Image.\n    See :class:`~torchvision.transforms.ToPIlImage` for more details.\n    Args:\n        pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\n        mode (`PIL.Image mode`_): color space and pixel depth of input data (optional).\n    .. _PIL.Image mode: http://pillow.readthedocs.io/en/3.4.x/handbook/concepts.html#modes\n    Returns:\n        PIL Image: Image converted to PIL Image.\n    """"""\n    if not(_is_numpy_image(pic) or _is_tensor_image(pic)):\n        raise TypeError(\'pic should be Tensor or ndarray. Got {}.\'.format(type(pic)))\n\n    npimg = pic\n    if isinstance(pic, torch.FloatTensor):\n        pic = pic.byte()\n    if torch.is_tensor(pic):\n        npimg = np.transpose(pic.numpy(), (1, 2, 0))\n\n    if not isinstance(npimg, np.ndarray):\n        raise TypeError(\'Input pic must be a torch.Tensor or NumPy ndarray, \' +\n                        \'not {}\'.format(type(npimg)))\n\n    if npimg.shape[2] == 1:\n        expected_mode = None\n        npimg = npimg[:, :, 0]\n        if npimg.dtype == np.uint8:\n            expected_mode = \'L\'\n        if npimg.dtype == np.int16:\n            expected_mode = \'I;16\'\n        if npimg.dtype == np.int32:\n            expected_mode = \'I\'\n        elif npimg.dtype == np.float32:\n            expected_mode = \'F\'\n        if mode is not None and mode != expected_mode:\n            raise ValueError(""Incorrect mode ({}) supplied for input type {}. Should be {}""\n                             .format(mode, np.dtype, expected_mode))\n        mode = expected_mode\n\n    elif npimg.shape[2] == 4:\n        permitted_4_channel_modes = [\'RGBA\', \'CMYK\']\n        if mode is not None and mode not in permitted_4_channel_modes:\n            raise ValueError(""Only modes {} are supported for 4D inputs"".format(permitted_4_channel_modes))\n\n        if mode is None and npimg.dtype == np.uint8:\n            mode = \'RGBA\'\n    else:\n        permitted_3_channel_modes = [\'RGB\', \'YCbCr\', \'HSV\']\n        if mode is not None and mode not in permitted_3_channel_modes:\n            raise ValueError(""Only modes {} are supported for 3D inputs"".format(permitted_3_channel_modes))\n        if mode is None and npimg.dtype == np.uint8:\n            mode = \'RGB\'\n\n    if mode is None:\n        raise TypeError(\'Input type {} is not supported\'.format(npimg.dtype))\n\n    return Image.fromarray(npimg, mode=mode)\n\nclass ToPILImage(object):\n    """"""Convert a tensor or an ndarray to PIL Image.\n    Converts a torch.*Tensor of shape C x H x W or a numpy ndarray of shape\n    H x W x C to a PIL Image while preserving the value range.\n    Args:\n        mode (`PIL.Image mode`_): color space and pixel depth of input data (optional).\n            If ``mode`` is ``None`` (default) there are some assumptions made about the input data:\n            1. If the input has 3 channels, the ``mode`` is assumed to be ``RGB``.\n            2. If the input has 4 channels, the ``mode`` is assumed to be ``RGBA``.\n            3. If the input has 1 channel, the ``mode`` is determined by the data type (i,e,\n            ``int``, ``float``, ``short``).\n    .. _PIL.Image mode: http://pillow.readthedocs.io/en/3.4.x/handbook/concepts.html#modes\n    """"""\n    def __init__(self, mode=None):\n        self.mode = mode\n\n    def __call__(self, pic):\n        """"""\n        Args:\n            pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\n        Returns:\n            PIL Image: Image converted to PIL Image.\n        """"""\n        return to_pil_image(pic, self.mode)\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'({0})\'.format(self.mode)\n\n\ndef to_tensor(pic):\n    """"""Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.\n    See ``ToTensor`` for more details.\n    Args:\n        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\n    Returns:\n        Tensor: Converted image.\n    """"""\n    if not(_is_pil_image(pic) or _is_numpy_image(pic)):\n        raise TypeError(\'pic should be PIL Image or ndarray. Got {}\'.format(type(pic)))\n\n    if isinstance(pic, np.ndarray):\n        # handle numpy array\n        img = torch.from_numpy(pic.transpose((2, 0, 1)))\n        # backward compatibility\n        return img.float()\n\n    if accimage is not None and isinstance(pic, accimage.Image):\n        nppic = np.zeros([pic.channels, pic.height, pic.width], dtype=np.float32)\n        pic.copyto(nppic)\n        return torch.from_numpy(nppic)\n\n    # handle PIL Image\n    if pic.mode == \'I\':\n        img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n    elif pic.mode == \'I;16\':\n        img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n    elif pic.mode == \'F\':\n        img = torch.from_numpy(np.array(pic, np.float32, copy=False))\n    else:\n        img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n    # PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK\n    if pic.mode == \'YCbCr\':\n        nchannel = 3\n    elif pic.mode == \'I;16\':\n        nchannel = 1\n    else:\n        nchannel = len(pic.mode)\n#    print (nchannel)\n    img = img.view(pic.size[1], pic.size[0], nchannel)\n    # put it from HWC to CHW format\n    # yikes, this transpose takes 80% of the loading time/CPU\n    img = img.transpose(0, 1).transpose(0, 2).contiguous()\n    if isinstance(img, torch.ByteTensor):\n        return img.float()\n    else:\n        return img\n\n\ndef un_normalize(tensor, mean, std):\n    """"""un_normalize a tensor image with mean and standard deviation.\n    See ``un_normalize`` for more details.\n    Args:\n        tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n        mean (sequence): Sequence of means for each channel.\n        std (sequence): Sequence of standard deviations for each channely.\n    Returns:\n        Tensor: Normalized Tensor image.\n    """"""\n    if not _is_tensor_image(tensor):\n        raise TypeError(\'tensor is not a torch image.\')\n    # TODO: make efficient\n    for t, m, s in zip(tensor, mean, std):\n        t.mul_(s).add_(m)\n    return tensor\n\nclass Un_Normalize(object):\n    """"""Normalize an tensor image with mean and standard deviation.\n    Given mean: ``(M1,...,Mn)`` and std: ``(S1,..,Sn)`` for ``n`` channels, this transform\n    will normalize each channel of the input ``torch.*Tensor`` i.e.\n    ``input[channel] = (input[channel] - mean[channel]) / std[channel]``\n    Args:\n        mean (sequence): Sequence of means for each channel.\n        std (sequence): Sequence of standard deviations for each channel.\n    """"""\n\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        """"""\n        Args:\n            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n        Returns:\n            Tensor: Normalized Tensor image.\n        """"""\n        return un_normalize(tensor, self.mean, self.std)\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'(mean={0}, std={1})\'.format(self.mean, self.std)\n\nclass Compose(object):\n\n    """""" Composes several co_transforms together.\n    For example:\n    >>> co_transforms.Compose([\n    >>>     co_transforms.CenterCrop(10),\n    >>>     co_transforms.ToTensor(),\n    >>>  ])\n    """"""\n    def __init__(self, co_transforms):\n        self.co_transforms = co_transforms\n\n    def __call__(self, input):\n        for _, t in enumerate(self.co_transforms):\n            input = t(input)\n        return input\n\nclass ToTensor(object):\n    """"""Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.\n    Converts a PIL Image or numpy.ndarray (H x W x C) in the range\n    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n    """"""\n\n    def __call__(self, pic):\n        """"""\n        Args:\n            pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\n        Returns:\n            Tensor: Converted image.\n        """"""\n        return to_tensor(pic)\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'()\'\n\n\nclass Crop(object):\n    """"""Crops the given PIL Image to size: [left, right, up, down].\n    Args:\n        left, right, up, down pixel you want to crop in image\n    """"""\n\n    def __init__(self, left, right, up, dowm):\n        self.left = left\n        self.right = right\n        self.up = up\n        self.dowm = dowm\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (PIL Image): Image to be cropped.\n        Returns:\n            PIL Image: Cropped image.\n        """"""\n        if not _is_pil_image(img):\n            raise TypeError(\'img should be PIL Image. Got {}\'.format(type(img)))\n        return img.crop((self.left, self.up, self.right, self.dowm))\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'(size={0})\'.format(self.size)\n\n\n\n\nclass ColorNormalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, input_image):\n        input_image = input_image.clone()\n        print(input_image.size())\n        print(self.mean)\n        print(self.std)\n        for i in range(3):\n            input_image[i] = torch.add(input_image[i], self.mean[i])\n            input_image[i] = torch.div(input_image[i], self.std[i])\n        return input_image\n\nclass DepthNormalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, input_depth):\n        input_depth = input_depth.copy()\n        input_depth = (input_depth - self.mean) / self.std\n        return input_depth\n\ndef resize(img, size, interpolation=Image.BILINEAR):\n    """"""Resize the input PIL Image to the given size.\n    Args:\n        img (PIL Image): Image to be resized.\n        size (sequence or int): Desired output size. If size is a sequence like\n            (h, w), the output size will be matched to this. If size is an int,\n            the smaller edge of the image will be matched to this number maintaing\n            the aspect ratio. i.e, if height > width, then image will be rescaled to\n            (size * height / width, size)\n        interpolation (int, optional): Desired interpolation. Default is\n            ``PIL.Image.BILINEAR``\n    Returns:\n        PIL Image: Resized image.\n    """"""\n    if not _is_pil_image(img):\n        raise TypeError(\'img should be PIL Image. Got {}\'.format(type(img)))\n    if not (isinstance(size, int) or (isinstance(size, collections.Iterable) and len(size) == 2)):\n        raise TypeError(\'Got inappropriate size arg: {}\'.format(size))\n\n    if isinstance(size, int):\n        print(""==========="")\n        w, h = img.size\n        if (w <= h and w == size) or (h <= w and h == size):\n            return img\n        if w < h:\n            ow = size\n            oh = int(size * h / w)\n            return img.resize((ow, oh), interpolation)\n        else:\n            print(interpolation)\n            oh = size\n            ow = int(size * w / h)\n            return img.resize((ow, oh), interpolation)\n    else:\n        return img.resize(size[::-1], interpolation)\n\nclass Resize(object):\n    """"""Resize the input PIL Image to the given size.\n    Args:\n        size (sequence or int): Desired output size. If size is a sequence like\n            (h, w), output size will be matched to this. If size is an int,\n            smaller edge of the image will be matched to this number.\n            i.e, if height > width, then image will be rescaled to\n            (size * height / width, size)\n        interpolation (int, optional): Desired interpolation. Default is\n            ``PIL.Image.BILINEAR``\n    """"""\n\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        assert isinstance(size, int) or (isinstance(size, collections.Iterable) and len(size) == 2)\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (PIL Image): Image to be scaled.\n        Returns:\n            PIL Image: Rescaled image.\n        """"""\n        return resize(img, self.size, self.interpolation)\n\n\nclass Scale(object):\n    #Scales the smaller edge to size\n    def __init__(self, size, interpolation=\'bicubic\'):\n        self.output_size = size\n#        0: Nearest-neighbor\n#        1: Bi-linear (default)\n#        2: Bi-quadratic\n#        3: Bi-cubic\n#        4: Bi-quartic\n#        5: Bi-quintic\n        if interpolation == \'bicubic\':\n            self.order = 3\n        elif interpolation == \'nearest\':\n            self.order = 0\n        else:\n            self.order = 1\n\n    def __call__(self, input_image):\n        h, w = input_image.shape[:2]\n        if isinstance(self.output_size, int):\n            if h > w:\n                new_h, new_w = self.output_size * h / w, self.output_size\n            else:\n                new_h, new_w = self.output_size, self.output_size * w / h\n        else:\n            new_h, new_w = self.output_size, self.output_size\n\n        new_h, new_w = int(new_h), int(new_w)\n        img = transform.resize(input_image, (new_h, new_w), order=self.order)\n        return img\n\nclass CenterCropRectangle(object):\n    #Crop to centered rectangle\n    def __init__(self, height, width):\n        self.width = width\n        self.height = height\n    def __call__(self, input_image):\n        input_image = input_image.copy()\n        h, w = input_image.shape[:2]\n        top = np.int16((h - self.height)/2)\n        left = np.int16((w - self.width)/2)\n        input_image = input_image[top:top+self.height, left:left+self.width]\n        return input_image\n\nclass RandomHorizontalFlip_rgbd(object):\n    """"""Horizontally flip the given PIL Image randomly with a probability of 1.""""""\n\n    def __call__(self, img, depth):\n        """"""\n        Args:\n            img (PIL Image): Image to be flipped.\n        Returns:\n            PIL Image: Horizontally flipped image.\n        """"""\n        if not _is_pil_image(img):\n            raise TypeError(\'img should be PIL Image. Got {}\'.format(type(img)))\n\n        if not _is_pil_image(depth):\n            raise TypeError(\'img should be PIL Image. Got {}\'.format(type(depth)))\n\n        if np.random.uniform() < 0.5:\n            img = img.transpose(Image.FLIP_LEFT_RIGHT)\n            depth = depth.transpose(Image.FLIP_LEFT_RIGHT)\n\n        return img, depth\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'()\'\n\nclass Rotation(object):\n    """"""Rotate the image by angle.\n    Args:\n        degrees (sequence or float or int): Range of degrees to select from.\n            If degrees is a number instead of sequence like (min, max), the range of degrees\n            will be (-degrees, +degrees).\n        resample ({PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC}, optional):\n            An optional resampling filter.\n            See http://pillow.readthedocs.io/en/3.4.x/handbook/concepts.html#filters\n            If omitted, or if the image has mode ""1"" or ""P"", it is set to PIL.Image.NEAREST.\n        expand (bool, optional): Optional expansion flag.\n            If true, expands the output to make it large enough to hold the entire rotated image.\n            If false or omitted, make the output image the same size as the input image.\n            Note that the expand flag assumes rotation around the center and no translation.\n        center (2-tuple, optional): Optional center of rotation.\n            Origin is the upper left corner.\n            Default is the center of the image.\n    """"""\n\n    def __init__(self, degrees, resample=False, expand=False, center=None):\n        self.degrees = degrees\n        self.resample = resample\n        self.expand = expand\n        self.center = center\n\n    def __call__(self, img):\n        """"""\n            img (PIL Image): Image to be rotated.\n        Returns:\n            PIL Image: Rotated image.\n        """"""\n\n        if not _is_pil_image(img):\n            raise TypeError(\'img should be PIL Image. Got {}\'.format(type(img)))\n\n        return img.rotate(self.degrees, self.resample, self.expand, self.center)\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'(degrees={0})\'.format(self.degrees)\n'"
cspn_pytorch/eval.py,8,"b'""""""\nCreated on Fri Feb  2 19:16:42 2018\n\n@ author:  Xinjing Cheng\n@ email : chengxinjing@baidu.com\n""""""\n\nfrom __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.backends.cudnn as cudnn\nimport os\nimport sys\nimport argparse\nfrom torch.autograd import Variable\nimport utils\nimport loss as my_loss\n\nparser = argparse.ArgumentParser(description=\'PyTorch Sparse To Dense Evaluation\')\n\n# net parameters\nparser.add_argument(\'--n_sample\', default=200, type=int, help=\'sampled sparse point number\')\nparser.add_argument(\'--data_set\', default=\'nyudepth\', type=str, help=\'train dataset\')\n\n# optimizer parameters\nparser.add_argument(\'--lr\', default=0.01, type=float, help=\'learning rate\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, help=\'sgd momentum\')\nparser.add_argument(\'--weight_decay\', default=1e-4, type=float, help=\'weight decay (L2 penalty)\')\nparser.add_argument(\'--dampening\', default=0.0, type=float, help=\'dampening for momentum\')\nparser.add_argument(\'--nesterov\', \'-n\', action=\'store_true\', help=\'enables Nesterov momentum\')\n\n# network parameters\nparser.add_argument(\'--cspn_step\', default=24, type=int, help=\'steps of propagation\')\nparser.add_argument(\'--cspn_norm_type\', default=\'8sum\', type=str, help=\'norm type of cspn\')\n\n# batch size\nparser.add_argument(\'--batch_size_eval\', default=1, type=int, help=\'batch size for eval\')\n\n#data directory\nparser.add_argument(\'--save_dir\', default=\'result/base_line\', type=str, help=\'result save directory\')\nparser.add_argument(\'--best_model_dir\', default=\'result/base_line\', type=str, help=\'best model load directory\')\nparser.add_argument(\'--train_list\', default=\'datalist/nyudepth_hdf5_train.csv\', type=str, help=\'train data lists\')\nparser.add_argument(\'--eval_list\', default=\'datalist/nyudepth_hdf5_val.csv\', type=str, help=\'eval data list\')\nparser.add_argument(\'--model\', default=\'base_model\', type=str, help=\'model for net\')\nparser.add_argument(\'--resume\', \'-r\', action=\'store_true\', help=\'resume from checkpoint\')\nparser.add_argument(\'--pretrain\', \'-p\', action=\'store_true\', help=\'load pretrained resnet model\')\n\nargs = parser.parse_args()\n\nsys.path.append(""./models"")\nimport update_model\nif args.model == \'cspn_unet\':\n    if args.data_set==\'nyudepth\':\n        print(""==> evaluating model with cspn and unet on nyudepth"")\n        import torch_resnet_cspn_nyu as model\n    elif args.data_set ==\'kitti\':\n        print(""==> evaluating model with cspn and unet on kitti"")\n        import torch_resnet_cspn_kitti as model\nelse:\n    import torch_resnet as model\n\nuse_cuda = torch.cuda.is_available()\n\n# global variable\nbest_rmse = sys.maxsize  # best test rmse\ncspn_config = {\'step\': args.cspn_step, \'norm_type\': args.cspn_norm_type}\n\n# Data\nprint(\'==> Preparing data..\')\nassert args.data_set in [\'nyudepth\', \'kitti\']\nif args.data_set==\'nyudepth\':\n    import eval_nyu_dataset_loader as dataset_loader\n    valset = dataset_loader.NyuDepthDataset(csv_file=args.eval_list,\n                                            root_dir=\'.\',\n                                            split = \'val\',\n                                            n_sample = args.n_sample,\n                                            input_format=\'hdf5\')\nelif args.data_set ==\'kitti\':\n    import eval_kitti_dataset_loader as dataset_loader\n    valset = dataset_loader.KittiDataset(csv_file=args.eval_list,\n                                         root_dir=\'.\',\n                                         split = \'val\',\n                                         n_sample = args.n_sample,\n                                         input_format=\'hdf5\')\nelse:\n    print(""==> input unknow dataset.."")\n\nvalloader = torch.utils.data.DataLoader(valset,\n                                        batch_size=args.batch_size_eval,\n                                        shuffle=False,\n                                        num_workers=4,\n                                        pin_memory=True,\n                                        drop_last=False)\n# Model\nprint(\'==> Building model..\')\n\nif args.data_set == \'nyudepth\':\n    net = model.resnet50(cspn_config=cspn_config)\nelif args.data_set == \'kitti\':\n    net = model.resnet18(cspn_config=cspn_config)\nelse:\n    print(""==> input unknow dataset.."")\n\nif True:\n    # Load best model checkpoint.\n    print(\'==> Resuming from best model..\')\n    best_model_path = os.path.join(args.best_model_dir, \'best_model.pth\')\n    assert os.path.isdir(args.best_model_dir), \'Error: no checkpoint directory found!\'\n    best_model_dict = torch.load(best_model_path)\n    best_model_dict = update_model.remove_moudle(best_model_dict)\n    net.load_state_dict(update_model.update_model(net, best_model_dict))\n\nif use_cuda:\n    net.cuda()\n    net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n    cudnn.benchmark = True\n\ncriterion = my_loss.Wighted_L1_Loss().cuda()\n\noptimizer = optim.SGD(net.parameters(),\n                      lr=args.lr,\n                      momentum=args.momentum,\n                      weight_decay=args.weight_decay,\n                      nesterov=args.nesterov,\n                      dampening=args.dampening)\n\n# evaluation\ndef val(epoch):\n    net.eval()\n    total_step_val = 0\n    error_sum_val = {\'MSE\':0, \'RMSE\':0, \'ABS_REL\':0, \'LG10\':0, \'MAE\':0,\\\n                     \'DELTA1.02\':0, \'DELTA1.05\':0, \'DELTA1.10\':0, \\\n                     \'DELTA1.25\':0, \'DELTA1.25^2\':0, \'DELTA1.25^3\':0}\n    error_avg = {\'MSE\':0, \'RMSE\':0, \'ABS_REL\':0, \'LG10\':0, \'MAE\':0,\\\n                 \'DELTA1.02\':0, \'DELTA1.05\':0, \'DELTA1.10\':0, \\\n                 \'DELTA1.25\':0, \'DELTA1.25^2\':0, \'DELTA1.25^3\':0}\n    for batch_idx, sample in enumerate(valloader):\n        [inputs, targets, raw_rgb] = [sample[\'rgbd\'] , sample[\'depth\'], sample[\'raw_rgb\']]\n        if use_cuda:\n            inputs, targets = inputs.cuda(), targets.cuda()\n        inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n        outputs = net(inputs)\n        loss = criterion(outputs, targets)\n        targets = targets.data.cpu()\n        outputs = outputs.data.cpu()\n        loss = loss.data.cpu()\n\n        error_result = utils.evaluate_error(gt_depth=targets, pred_depth=outputs)\n\n        total_step_val += args.batch_size_eval\n        error_avg = utils.avg_error(error_sum_val,\n                                    error_result,\n                                    total_step_val,\n                                    args.batch_size_eval)\n        utils.print_error(\'eval_result: step(average)\',\n                          epoch, batch_idx,\n                          loss, error_result, error_avg)\n        utils.save_eval_img(args.data_set, args.best_model_dir, batch_idx,\n                            inputs.data.cpu(), raw_rgb, targets, outputs)\n\n    utils.print_single_error(epoch, batch_idx, loss, error_avg)\n\ndef eval_error():\n    val(0)\n\neval_error()\n\n'"
cspn_pytorch/eval_kitti_dataset_loader.py,7,"b'""""""\nCreated on Thu Feb  1 18:07:52 2018\n\n@author: norbot\n""""""\n\nfrom __future__ import print_function, division\nimport os\nimport torch\nimport pandas as pd\nfrom skimage import io, transform\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nimport data_transform\nfrom PIL import Image, ImageOps\nimport h5py\n\nimagenet_mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\nimagenet_std = np.array([0.229, 0.224, 0.225], dtype=np.float32)\nimagenet_eigval = np.array([0.2175, 0.0188, 0.0045], dtype=np.float32)\nimagenet_eigvec = np.array([[-0.5675,  0.7192,  0.4009],\n                            [-0.5808, -0.0045, -0.8140],\n                            [-0.5836, -0.6948,  0.4203]], dtype=np.float32)\n\nclass KittiDataset(Dataset):\n    # nyu depth dataset\n    def __init__(self, csv_file, root_dir, split, n_sample=500, input_format = \'img\'):\n        """"""\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        """"""\n        self.rgbd_frame = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.split = split\n        self.input_format = input_format\n        self.n_sample = n_sample\n\n    def __len__(self):\n        return len(self.rgbd_frame)\n\n    def __getitem__(self, idx):\n        # read input image\n        if self.input_format == \'img\':\n            rgb_name = os.path.join(self.root_dir,\n                                    self.rgbd_frame.iloc[idx, 0])\n            with open(rgb_name, \'rb\') as fRgb:\n                rgb_image = Image.open(rgb_name).convert(\'RGB\')\n\n            depth_name = os.path.join(self.root_dir,\n                                      self.rgbd_frame.iloc[idx, 1])\n            with open(depth_name, \'rb\') as fDepth:\n                depth_image = Image.open(depth_name)\n\n        # read input hdf5\n        elif self.input_format == \'hdf5\':\n            file_name = os.path.join(self.root_dir,\n                                     self.rgbd_frame.iloc[idx, 0])\n            rgb_h5, depth_h5 = self.load_h5(file_name)\n            rgb_image = Image.fromarray(rgb_h5, mode=\'RGB\')\n            depth_image = Image.fromarray(depth_h5.astype(\'float32\'), mode=\'F\')\n        else:\n            print(\'error: the input format is not supported now!\')\n            return None\n\n        _s = np.random.uniform(1.0, 1.5)\n        degree = np.random.uniform(-5.0, 5.0)\n        if self.split == \'train\':\n            tRgb = data_transform.Compose([data_transform.Crop(10, 1210, 130, 370),\n                                           data_transform.Rotation(degree),\n                                           transforms.ColorJitter(brightness = 0.4, contrast = 0.4, saturation = 0.4),\n                                           transforms.CenterCrop((228, 912)),\n                                           transforms.ToTensor(),\n                                           transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n                                           transforms.ToPILImage()])\n\n            tDepth = data_transform.Compose([data_transform.Crop(10, 1210, 130, 370),\n                                             data_transform.Rotation(degree),\n                                             transforms.CenterCrop((228, 912))])\n            rgb_image = tRgb(rgb_image)\n            depth_image = tDepth(depth_image)\n            if np.random.uniform()<0.5:\n                rgb_image = rgb_image.transpose(Image.FLIP_LEFT_RIGHT)\n                depth_image = depth_image.transpose(Image.FLIP_LEFT_RIGHT)\n\n            rgb_image = transforms.ToTensor()(rgb_image)\n            if self.input_format == \'img\':\n                depth_image = transforms.ToTensor()(depth_image)\n            else:\n                depth_image = data_transform.ToTensor()(depth_image)\n            depth_image = depth_image.div(_s)\n            sparse_image = self.createSparseDepthImage(depth_image, self.n_sample)\n            rgbd_image = torch.cat((rgb_image, sparse_image), 0)\n            sample = {\'rgbd\': rgbd_image, \'depth\': depth_image}\n\n        elif self.split == \'val\':\n            tRgb = data_transform.Compose([data_transform.Crop(10, 1210, 130, 370),\n                                           transforms.CenterCrop((228, 912)),\n                                           transforms.ToTensor(),\n                                           transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n                                           transforms.ToPILImage()])\n\n            tDepth = data_transform.Compose([data_transform.Crop(10, 1210, 130, 370),\n                                             transforms.CenterCrop((228, 912))])\n            rgb_raw = tDepth(rgb_image)\n            rgb_image = tRgb(rgb_image)\n            depth_image = tDepth(depth_image)\n            rgb_image = transforms.ToTensor()(rgb_image)\n            rgb_raw = transforms.ToTensor()(rgb_raw)\n            if self.input_format == \'img\':\n                depth_image = transforms.ToTensor()(depth_image)\n            else:\n                depth_image = data_transform.ToTensor()(depth_image)\n            sparse_image = self.createSparseDepthImage(depth_image, self.n_sample)\n            rgbd_image = torch.cat((rgb_image, sparse_image), 0)\n\n            sample = {\'rgbd\': rgbd_image, \'depth\': depth_image, \'raw_rgb\': rgb_raw }\n\n        return sample\n\n    def createSparseDepthImage(self, depth_image, n_sample):\n        random_mask = torch.zeros(1, depth_image.size(1), depth_image.size(2))\n        n_pixels = depth_image.size(1) * depth_image.size(2)\n        n_valid_pixels = torch.sum(depth_image>0.0001)\n        perc_sample = float(n_sample)/n_valid_pixels.float()\n        random_mask = torch.bernoulli((torch.ones_like(random_mask)*perc_sample))\n        sparse_depth = torch.mul(depth_image, random_mask)\n        return sparse_depth\n\n    def load_h5(self, h5_filename):\n        f = h5py.File(h5_filename, \'r\')\n        rgb = f[\'rgb\'][:].transpose(1,2,0)\n        depth = f[\'depth\'][:]\n        return (rgb, depth)\n\n'"
cspn_pytorch/eval_nyu_dataset_loader.py,7,"b'""""""\nCreated on Thu Feb  1 18:07:52 2018\n\n@author: norbot\n""""""\n\nfrom __future__ import print_function, division\nimport os\nimport torch\nimport pandas as pd\nfrom skimage import io, transform\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nimport data_transform\nfrom PIL import Image, ImageOps\nimport h5py\n\nimagenet_mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\nimagenet_std = np.array([0.229, 0.224, 0.225], dtype=np.float32)\nimagenet_eigval = np.array([0.2175, 0.0188, 0.0045], dtype=np.float32)\nimagenet_eigvec = np.array([[-0.5675,  0.7192,  0.4009],\n                            [-0.5808, -0.0045, -0.8140],\n                            [-0.5836, -0.6948,  0.4203]], dtype=np.float32)\n\n\nclass NyuDepthDataset(Dataset):\n    # nyu depth dataset\n    def __init__(self, csv_file, root_dir, split, n_sample=200, input_format = \'img\'):\n        """"""\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        """"""\n        self.rgbd_frame = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.split = split\n        self.input_format = input_format\n        self.n_sample = n_sample\n\n    def __len__(self):\n        return len(self.rgbd_frame)\n\n    def __getitem__(self, idx):\n        # read input image\n        if self.input_format == \'img\':\n            rgb_name = os.path.join(self.root_dir,\n                                    self.rgbd_frame.iloc[idx, 0])\n            with open(rgb_name, \'rb\') as fRgb:\n                rgb_image = Image.open(rgb_name).convert(\'RGB\')\n\n            depth_name = os.path.join(self.root_dir,\n                                      self.rgbd_frame.iloc[idx, 1])\n            with open(depth_name, \'rb\') as fDepth:\n                depth_image = Image.open(depth_name)\n\n        # read input hdf5\n        elif self.input_format == \'hdf5\':\n            file_name = os.path.join(self.root_dir,\n                                     self.rgbd_frame.iloc[idx, 0])\n            rgb_h5, depth_h5 = self.load_h5(file_name)\n            rgb_image = Image.fromarray(rgb_h5, mode=\'RGB\')\n            depth_image = Image.fromarray(depth_h5.astype(\'float32\'), mode=\'F\')\n        else:\n            print(\'error: the input format is not supported now!\')\n            return None\n\n        _s = np.random.uniform(1.0, 1.5)\n        s = np.int(240*_s)\n        degree = np.random.uniform(-5.0, 5.0)\n        if self.split == \'train\':\n            tRgb = data_transform.Compose([transforms.Resize(s),\n                                           data_transform.Rotation(degree),\n                                           transforms.ColorJitter(brightness = 0.4, contrast = 0.4, saturation = 0.4),\n#                                           data_transform.Lighting(0.1, imagenet_eigval, imagenet_eigvec)])\n                                           transforms.CenterCrop((228, 304)),\n                                           transforms.ToTensor(),\n                                           transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n                                           transforms.ToPILImage()])\n\n            tDepth = data_transform.Compose([transforms.Resize(s),\n                                             data_transform.Rotation(degree),\n                                             transforms.CenterCrop((228, 304))])\n            rgb_image = tRgb(rgb_image)\n            depth_image = tDepth(depth_image)\n            if np.random.uniform()<0.5:\n                rgb_image = rgb_image.transpose(Image.FLIP_LEFT_RIGHT)\n                depth_image = depth_image.transpose(Image.FLIP_LEFT_RIGHT)\n\n            rgb_image = transforms.ToTensor()(rgb_image)\n            if self.input_format == \'img\':\n                depth_image = transforms.ToTensor()(depth_image)\n            else:\n                depth_image = data_transform.ToTensor()(depth_image)\n            depth_image = depth_image.div(_s)\n            sparse_image = self.createSparseDepthImage(depth_image, self.n_sample)\n            rgbd_image = torch.cat((rgb_image, sparse_image), 0)\n            sample = {\'rgbd\': rgbd_image, \'depth\': depth_image}\n\n        elif self.split == \'val\':\n            tRgb = data_transform.Compose([transforms.Resize(240),\n                                           transforms.CenterCrop((228, 304)),\n                                           transforms.ToTensor(),\n                                           transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n                                           transforms.ToPILImage()])\n\n            tDepth = data_transform.Compose([transforms.Resize(240),\n                                             transforms.CenterCrop((228, 304))])\n\n            rgb_raw = tDepth(rgb_image)\n            rgb_image = tRgb(rgb_image)\n            depth_image = tDepth(depth_image)\n            rgb_image = transforms.ToTensor()(rgb_image)\n            rgb_raw = transforms.ToTensor()(rgb_raw)\n            if self.input_format == \'img\':\n                depth_image = transforms.ToTensor()(depth_image)\n            else:\n                depth_image = data_transform.ToTensor()(depth_image)\n            sparse_image = self.createSparseDepthImage(depth_image, self.n_sample)\n            rgbd_image = torch.cat((rgb_image, sparse_image), 0)\n\n            sample = {\'rgbd\': rgbd_image, \'depth\': depth_image, \'raw_rgb\': rgb_raw }\n\n        return sample\n\n    def createSparseDepthImage(self, depth_image, n_sample):\n        random_mask = torch.zeros(1, depth_image.size(1), depth_image.size(2))\n        n_pixels = depth_image.size(1) * depth_image.size(2)\n        n_valid_pixels = torch.sum(depth_image>0.0001)\n        perc_sample = n_sample/n_pixels\n        random_mask = torch.bernoulli(torch.ones_like(random_mask)*perc_sample)\n        sparse_depth = torch.mul(depth_image, random_mask)\n        return sparse_depth\n\n    def load_h5(self, h5_filename):\n        f = h5py.File(h5_filename, \'r\')\n        rgb = f[\'rgb\'][:].transpose(1,2,0)\n        depth = f[\'depth\'][:]\n        return (rgb, depth)\n\n\n'"
cspn_pytorch/kitti_dataset_loader.py,11,"b'#!/usr/bin/env python2\n# -*- coding: utf-8 -*-\n""""""\nCreated on Thu Feb  1 18:07:52 2018\n\n@author: norbot\n""""""\n\nfrom __future__ import print_function, division\nimport os\nimport torch\nimport pandas as pd\nfrom skimage import io, transform\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nimport data_transform\nfrom PIL import Image, ImageOps\nimport h5py\n\nimagenet_mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\nimagenet_std = np.array([0.229, 0.224, 0.225], dtype=np.float32)\nimagenet_eigval = np.array([0.2175, 0.0188, 0.0045], dtype=np.float32)\nimagenet_eigvec = np.array([[-0.5675,  0.7192,  0.4009],\n                            [-0.5808, -0.0045, -0.8140],\n                            [-0.5836, -0.6948,  0.4203]], dtype=np.float32)\n\nclass KittiDataset(Dataset):\n    # nyu depth dataset \n    def __init__(self, csv_file, root_dir, split, n_sample=500, input_format = \'img\'):\n        """"""\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        """"""\n        self.rgbd_frame = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.split = split\n        self.input_format = input_format\n        self.n_sample = n_sample\n    \n    def __len__(self):\n        return len(self.rgbd_frame)\n\n    def __getitem__(self, idx):\n        # read input image\n        if self.input_format == \'img\':\n#            print(\'==> Input Format is image\')\n            rgb_name = os.path.join(self.root_dir,\n                                    self.rgbd_frame.iloc[idx, 0])\n            with open(rgb_name, \'rb\') as fRgb:\n                rgb_image = Image.open(rgb_name).convert(\'RGB\')\n            \n            depth_name = os.path.join(self.root_dir,\n                                      self.rgbd_frame.iloc[idx, 1])\n            with open(depth_name, \'rb\') as fDepth:\n                depth_image = Image.open(depth_name)\n                \n        # read input hdf5\n        elif self.input_format == \'hdf5\':\n#            print(\'==> Input Format is hdf5\')\n            file_name = os.path.join(self.root_dir,\n                                     self.rgbd_frame.iloc[idx, 0])\n            rgb_h5, depth_h5 = self.load_h5(file_name)\n            rgb_image = Image.fromarray(rgb_h5, mode=\'RGB\')\n            depth_image = Image.fromarray(depth_h5.astype(\'float32\'), mode=\'F\')\n#            print(rgb_image.size)\n#            plt.figure()\n#            show_img(rgb_image)\n#            plt.figure()\n#            show_img(depth_image)\n        else:\n            print(\'error: the input format is not supported now!\')\n            return None\n        \n        _s = np.random.uniform(1.0, 1.5)\n        degree = np.random.uniform(-5.0, 5.0)\n        if self.split == \'train\':\n            tRgb = data_transform.Compose([data_transform.Crop(10, 1210, 130, 370),\n                                           data_transform.Rotation(degree),\n                                           transforms.ColorJitter(brightness = 0.4, contrast = 0.4, saturation = 0.4),\n                                           transforms.CenterCrop((228, 912)),\n                                           transforms.ToTensor(),\n                                           transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n                                           transforms.ToPILImage()])\n\n            tDepth = data_transform.Compose([data_transform.Crop(10, 1210, 130, 370),\n                                             data_transform.Rotation(degree),\n                                             transforms.CenterCrop((228, 912))])\n            rgb_image = tRgb(rgb_image)\n            depth_image = tDepth(depth_image)\n            if np.random.uniform()<0.5:\n                rgb_image = rgb_image.transpose(Image.FLIP_LEFT_RIGHT)\n                depth_image = depth_image.transpose(Image.FLIP_LEFT_RIGHT)\n            \n            rgb_image = transforms.ToTensor()(rgb_image)\n            if self.input_format == \'img\':\n                depth_image = transforms.ToTensor()(depth_image)\n            else:\n                depth_image = data_transform.ToTensor()(depth_image)\n            depth_image = depth_image.div(_s)           \n            sparse_image = self.createSparseDepthImage(depth_image, self.n_sample)\n            rgbd_image = torch.cat((rgb_image, sparse_image), 0)\n            \n\n        elif self.split == \'val\':\n            tRgb = data_transform.Compose([data_transform.Crop(10, 1210, 130, 370),\n                                           transforms.CenterCrop((228, 912)),\n                                           transforms.ToTensor(),\n                                           transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n                                           transforms.ToPILImage()])\n\n            tDepth = data_transform.Compose([data_transform.Crop(10, 1210, 130, 370),\n                                             transforms.CenterCrop((228, 912))])            \n            rgb_image = tRgb(rgb_image)\n            depth_image = tDepth(depth_image)\n            rgb_image = transforms.ToTensor()(rgb_image)\n            if self.input_format == \'img\':\n                depth_image = transforms.ToTensor()(depth_image)\n            else:\n                depth_image = data_transform.ToTensor()(depth_image)\n            sparse_image = self.createSparseDepthImage(depth_image, self.n_sample)\n            rgbd_image = torch.cat((rgb_image, sparse_image), 0)\n            \n        sample = {\'rgbd\': rgbd_image, \'depth\': depth_image }\n        \n        return sample\n    \n    def createSparseDepthImage(self, depth_image, n_sample):\n        random_mask = torch.zeros(1, depth_image.size(1), depth_image.size(2))\n        n_pixels = depth_image.size(1) * depth_image.size(2)\n        n_valid_pixels = torch.sum(depth_image>0.0001)\n#        print(\'===> number of total pixels is: %d\\n\' % n_pixels)\n#        print(\'===> number of total valid pixels is: %d\\n\' % n_valid_pixels)\n        perc_sample = float(n_sample)/n_valid_pixels.float()\n#        print(random_mask.type())\n#        print(torch.ones_like(random_mask).type())\n#        print(perc_sample)\n        random_mask = torch.bernoulli((torch.ones_like(random_mask)*perc_sample))\n        sparse_depth = torch.mul(depth_image, random_mask)\n        return sparse_depth\n\n    def load_h5(self, h5_filename):\n        f = h5py.File(h5_filename, \'r\')\n    #    print (f.keys())\n        rgb = f[\'rgb\'][:].transpose(1,2,0)\n        depth = f[\'depth\'][:]\n        return (rgb, depth)\n\n\ndef show_img(image):\n    """"""Show image""""""\n    plt.imshow(image)\n\n\ndef test_load_h5():\n    \n    def load_h5(h5_filename):\n        f = h5py.File(h5_filename, \'r\')\n    #    print (f.keys())\n        rgb = f[\'rgb\'][:].transpose(1,2,0)\n        depth = f[\'depth\'][:]\n        return (rgb, depth)\n    \n    file_name = \'./data/kitti_hdf5/val/11/00466-R.h5\'\n    rgb_h5, depth_h5 = load_h5(file_name)   \n    depth_h5 = depth_h5.astype(\'uint16\')\n    rgb_image = Image.fromarray(rgb_h5, mode=\'RGB\')\n    depth_image = Image.fromarray(depth_h5.astype(\'uint16\'), mode=\'L\')\n#    cv2.imwrite(\'tmp/cv_save_kitti_depth.png\', depth_h5)\n    \n#test_load_h5()\n\n        \ndef test_imgread():\n    # train preprocessing   \n    kitti_dataset = KittiDataset(csv_file=\'data/kitti_hdf5/kitti_hdf5_train.csv\',\n                                       root_dir=\'.\',\n                                       split = \'train\',\n                                       n_sample = 500,\n                                       input_format=\'hdf5\')\n#    kitti_dataset = KittiDataset(csv_file=\'data/nyudepth_v2/nyudepthv2_val.csv\',\n#                                       root_dir=\'.\',\n#                                       split = \'val\',\n#                                       n_sample = 500,\n#                                       input_format=\'hdf5\')    \n    fig = plt.figure()\n    for i in range(len(kitti_dataset)):\n        sample = kitti_dataset[i]\n        rgb = data_transform.ToPILImage()(sample[\'rgbd\'][0:3,:,:])\n        depth = data_transform.ToPILImage()(sample[\'depth\'])\n        sparse_depth = data_transform.ToPILImage()(sample[\'rgbd\'][3,:,:].unsqueeze(0))\n        depth_mask = data_transform.ToPILImage()(torch.sign(sample[\'depth\']))\n        sparse_depth_mask = data_transform.ToPILImage()(sample[\'rgbd\'][3,:,:].unsqueeze(0).sign())\n        print(sample[\'depth\'])\n        invalid_depth = torch.sum(sample[\'rgbd\'][3,:,:].unsqueeze(0).sign() < 0)\n        print(invalid_depth)\n        plt.imsave(""tmp/plt_save_kitit_depth.png"", depth)\n        depth.save((""tmp/pil_save_kitti_depth.png""))\n        rgb.save(""tmp/pil_save_kitti_rgb.png"")\n#        print(sample[\'depth\'].size())\n#        print(torch.sign(sample[\'sparse_depth\']))\n        ax = plt.subplot(5, 4, i + 1)\n        ax.axis(\'off\')\n        show_img(rgb)\n        ax = plt.subplot(5, 4, i + 5)\n        ax.axis(\'off\')\n        show_img(depth)\n        ax = plt.subplot(5, 4, i + 9)\n        ax.axis(\'off\')\n        show_img(depth_mask)\n        ax = plt.subplot(5, 4, i + 13)\n        ax.axis(\'off\')\n        show_img(sparse_depth)\n        ax = plt.subplot(5, 4, i + 17)\n        ax.axis(\'off\')\n        show_img(sparse_depth_mask)\n        if i == 3:\n            plt.show()\n            break\n    \n#test_imgread()\n'"
cspn_pytorch/loss.py,4,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Sun Feb  4 13:06:56 2018\n\n@author: norbot\n""""""\n\nimport torch\nimport torch.nn as nn\n\n\nclass Wighted_L1_Loss(torch.nn.Module):\n    def __init__(self):\n        super(Wighted_L1_Loss, self).__init__()\n\n    def forward(self, pred, label):\n        label_mask = label > 0.0001\n        _pred = pred[label_mask]\n        _label = label[label_mask]\n        n_valid_element = _label.size(0)\n        diff_mat = torch.abs(_pred-_label)\n        loss = torch.sum(diff_mat)/n_valid_element\n        return loss\n'"
cspn_pytorch/lr_scheduler.py,2,"b'import numpy as np\nimport warnings\nfrom torch.optim.optimizer import Optimizer\nimport math\n\nclass ReduceLROnPlateau(object):\n    """"""Reduce learning rate when a metric has stopped improving.\n    Models often benefit from reducing the learning rate by a factor\n    of 2-10 once learning stagnates. This scheduler reads a metrics\n    quantity and if no improvement is seen for a \'patience\' number\n    of epochs, the learning rate is reduced.\n    \n    Args:\n        factor: factor by which the learning rate will\n            be reduced. new_lr = lr * factor\n        patience: number of epochs with no improvement\n            after which learning rate will be reduced.\n        verbose: int. 0: quiet, 1: update messages.\n        mode: one of {min, max}. In `min` mode,\n            lr will be reduced when the quantity\n            monitored has stopped decreasing; in `max`\n            mode it will be reduced when the quantity\n            monitored has stopped increasing.\n        epsilon: threshold for measuring the new optimum,\n            to only focus on significant changes.\n        cooldown: number of epochs to wait before resuming\n            normal operation after lr has been reduced.\n        min_lr: lower bound on the learning rate.\n        \n        \n    Example:\n        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n        >>> scheduler = ReduceLROnPlateau(optimizer, \'min\')\n        >>> for epoch in range(10):\n        >>>     train(...)\n        >>>     val_acc, val_loss = validate(...)\n        >>>     scheduler.step(val_loss, epoch)\n    """"""\n\n    def __init__(self, optimizer, mode=\'min\', factor=0.1, patience=3,\n                 verbose=0, epsilon=1e-4, cooldown=0, min_lr=0.000001):\n        super(ReduceLROnPlateau, self).__init__()\n\n        if factor >= 1.0:\n            raise ValueError(\'ReduceLROnPlateau \'\n                             \'does not support a factor >= 1.0.\')\n        self.factor = factor\n        self.min_lr = min_lr\n        self.epsilon = epsilon\n        self.patience = patience\n        self.verbose = verbose\n        self.cooldown = cooldown\n        self.cooldown_counter = 0  # Cooldown counter.\n        self.monitor_op = None\n        self.wait = 0\n        self.best = 0\n        self.mode = mode\n        assert isinstance(optimizer, Optimizer)\n        self.optimizer = optimizer\n        self._reset()\n\n    def _reset(self):\n        """"""Resets wait counter and cooldown counter.\n        """"""\n        if self.mode not in [\'min\', \'max\']:\n            raise RuntimeError(\'Learning Rate Plateau Reducing mode %s is unknown!\')\n        if self.mode == \'min\' :\n            self.monitor_op = lambda a, b: np.less(a, b - self.epsilon)\n            self.best = np.Inf\n        else:\n            self.monitor_op = lambda a, b: np.greater(a, b + self.epsilon)\n            self.best = -np.Inf\n        self.cooldown_counter = 0\n        self.wait = 0\n        self.lr_epsilon = self.min_lr * 1e-4\n\n    def reset(self):\n        self._reset()\n\n    def step(self, metrics, epoch):\n\n        current = metrics\n        if current is None:\n            warnings.warn(\'Learning Rate Plateau Reducing requires metrics available!\', RuntimeWarning)\n        else:\n            if self.in_cooldown():\n                self.cooldown_counter -= 1\n                self.wait = 0\n\n            if self.monitor_op(current, self.best):\n                self.best = current\n                self.wait = 0\n            elif not self.in_cooldown():\n                if self.wait >= self.patience:\n                    for param_group in self.optimizer.param_groups:\n                        old_lr = float(param_group[\'lr\'])\n                        if old_lr > self.min_lr + self.lr_epsilon:\n                            new_lr = old_lr * self.factor\n                            new_lr = max(new_lr, self.min_lr)\n                            param_group[\'lr\'] = new_lr\n                            if self.verbose > 0:\n                                print(\'\\nEpoch %05d: reducing learning rate to %s.\' % (epoch, new_lr))\n                            self.cooldown_counter = self.cooldown\n                            self.wait = 0\n                self.wait += 1\n                 \n    def in_cooldown(self):\n        return self.cooldown_counter > 0\n'"
cspn_pytorch/nyu_dataset_loader.py,10,"b'#!/usr/bin/env python2\n# -*- coding: utf-8 -*-\n""""""\nCreated on Thu Feb  1 18:07:52 2018\n\n@author: norbot\n""""""\n\nfrom __future__ import print_function, division\nimport os\nimport torch\nimport pandas as pd\nfrom skimage import io, transform\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nimport data_transform\nfrom PIL import Image, ImageOps\nimport h5py\n\nimagenet_mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\nimagenet_std = np.array([0.229, 0.224, 0.225], dtype=np.float32)\nimagenet_eigval = np.array([0.2175, 0.0188, 0.0045], dtype=np.float32)\nimagenet_eigvec = np.array([[-0.5675,  0.7192,  0.4009],\n                            [-0.5808, -0.0045, -0.8140],\n                            [-0.5836, -0.6948,  0.4203]], dtype=np.float32)\n\n\nclass NyuDepthDataset(Dataset):\n    # nyu depth dataset \n    def __init__(self, csv_file, root_dir, split, n_sample=200, input_format = \'img\'):\n        """"""\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        """"""\n        self.rgbd_frame = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.split = split\n        self.input_format = input_format\n        self.n_sample = n_sample\n    \n    def __len__(self):\n        return len(self.rgbd_frame)\n\n    def __getitem__(self, idx):\n        # read input image\n        if self.input_format == \'img\':\n#            print(\'==> Input Format is image\')\n            rgb_name = os.path.join(self.root_dir,\n                                    self.rgbd_frame.iloc[idx, 0])\n            with open(rgb_name, \'rb\') as fRgb:\n                rgb_image = Image.open(rgb_name).convert(\'RGB\')\n            \n            depth_name = os.path.join(self.root_dir,\n                                      self.rgbd_frame.iloc[idx, 1])\n            with open(depth_name, \'rb\') as fDepth:\n                depth_image = Image.open(depth_name)\n                \n        # read input hdf5\n        elif self.input_format == \'hdf5\':\n#            print(\'==> Input Format is hdf5\')\n            file_name = os.path.join(self.root_dir,\n                                     self.rgbd_frame.iloc[idx, 0])\n            rgb_h5, depth_h5 = self.load_h5(file_name)   \n#            print(depth_h5.dtype)\n            rgb_image = Image.fromarray(rgb_h5, mode=\'RGB\')\n            depth_image = Image.fromarray(depth_h5.astype(\'float32\'), mode=\'F\')\n#            plt.figure()\n#            show_img(rgb_image)\n#            plt.figure()\n#            show_img(depth_image)\n        else:\n            print(\'error: the input format is not supported now!\')\n            return None\n        \n        _s = np.random.uniform(1.0, 1.5)\n        s = np.int(240*_s)\n        degree = np.random.uniform(-5.0, 5.0)\n        if self.split == \'train\':\n            tRgb = data_transform.Compose([transforms.Resize(s),\n                                           data_transform.Rotation(degree),\n                                           transforms.ColorJitter(brightness = 0.4, contrast = 0.4, saturation = 0.4),\n#                                           data_transform.Lighting(0.1, imagenet_eigval, imagenet_eigvec)])\n                                           transforms.CenterCrop((228, 304)),\n                                           transforms.ToTensor(),\n                                           transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n                                           transforms.ToPILImage()])\n\n            tDepth = data_transform.Compose([transforms.Resize(s),\n                                             data_transform.Rotation(degree),\n                                             transforms.CenterCrop((228, 304))])\n            rgb_image = tRgb(rgb_image)\n            depth_image = tDepth(depth_image)\n            if np.random.uniform()<0.5:\n                rgb_image = rgb_image.transpose(Image.FLIP_LEFT_RIGHT)\n                depth_image = depth_image.transpose(Image.FLIP_LEFT_RIGHT)\n            \n            rgb_image = transforms.ToTensor()(rgb_image)\n            if self.input_format == \'img\':\n                depth_image = transforms.ToTensor()(depth_image)\n            else:\n                depth_image = data_transform.ToTensor()(depth_image)\n            depth_image = depth_image.div(_s)           \n            sparse_image = self.createSparseDepthImage(depth_image, self.n_sample)\n            rgbd_image = torch.cat((rgb_image, sparse_image), 0)\n\n\n        elif self.split == \'val\':\n            tRgb = data_transform.Compose([transforms.Resize(240),\n                                           transforms.CenterCrop((228, 304)),\n                                           transforms.ToTensor(),\n                                           transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n                                           transforms.ToPILImage()])\n\n            tDepth = data_transform.Compose([transforms.Resize(240),\n                                             transforms.CenterCrop((228, 304))])            \n            rgb_image = tRgb(rgb_image)\n            depth_image = tDepth(depth_image)\n            rgb_image = transforms.ToTensor()(rgb_image)\n            if self.input_format == \'img\':\n                depth_image = transforms.ToTensor()(depth_image)\n            else:\n                depth_image = data_transform.ToTensor()(depth_image)\n            sparse_image = self.createSparseDepthImage(depth_image, self.n_sample)\n            rgbd_image = torch.cat((rgb_image, sparse_image), 0)\n            \n        sample = {\'rgbd\': rgbd_image, \'depth\': depth_image }\n        \n        return sample\n    \n    def createSparseDepthImage(self, depth_image, n_sample):\n        random_mask = torch.zeros(1, depth_image.size(1), depth_image.size(2))\n        n_pixels = depth_image.size(1) * depth_image.size(2)\n        n_valid_pixels = torch.sum(depth_image>0.0001)\n#        print(\'===> number of total pixels is: %d\\n\' % n_pixels)\n#        print(\'===> number of total valid pixels is: %d\\n\' % n_valid_pixels)\n        perc_sample = n_sample/n_pixels\n        random_mask = torch.bernoulli(torch.ones_like(random_mask)*perc_sample)\n        sparse_depth = torch.mul(depth_image, random_mask)\n        return sparse_depth\n\n    def load_h5(self, h5_filename):\n        f = h5py.File(h5_filename, \'r\')\n    #    print (f.keys())\n        rgb = f[\'rgb\'][:].transpose(1,2,0)\n        depth = f[\'depth\'][:]\n        return (rgb, depth)\n\n\ndef show_img(image):\n    """"""Show image""""""\n    plt.imshow(image)\n    \ndef test_imgread():\n\n    # train preprocessing   \n#    nyudepth_dataset = NyuDepthDataset(csv_file=\'data/kitti_hdf5/kitti_hdf5_train.csv\',\n#                                       root_dir=\'.\',\n#                                       split = \'train\',\n#                                       n_sample = 200,\n#                                       input_format=\'hdf5\')\n    nyudepth_dataset = NyuDepthDataset(csv_file=\'data/nyudepth_hdf5/nyudepth_hdf5_val.csv\',\n                                       root_dir=\'.\',\n                                       split = \'val\',\n                                       n_sample = 500,\n                                       input_format=\'hdf5\')    \n    fig = plt.figure()\n    for i in range(len(nyudepth_dataset)):\n        sample = nyudepth_dataset[i]\n        rgb = transforms.ToPILImage()(sample[\'rgbd\'][0:3,:,:])\n        depth = transforms.ToPILImage()(sample[\'depth\'])\n        sparse_depth = transforms.ToPILImage()(sample[\'rgbd\'][3,:,:].unsqueeze(0))\n        depth_mask = transforms.ToPILImage()(torch.sign(sample[\'depth\']))\n        sparse_depth_mask = transforms.ToPILImage()(sample[\'rgbd\'][3,:,:].unsqueeze(0).sign())\n        print(sample[\'rgbd\'][0:3,:,:])\n        invalid_depth = torch.sum(sample[\'rgbd\'][3,:,:].unsqueeze(0).sign() < 0)\n        print(invalid_depth)\n#        print(sample[\'depth\'].size())\n#        print(torch.sign(sample[\'sparse_depth\']))\n        ax = plt.subplot(5, 4, i + 1)\n        ax.axis(\'off\')\n        show_img(rgb)\n        ax = plt.subplot(5, 4, i + 5)\n        ax.axis(\'off\')\n        show_img(depth)\n        ax = plt.subplot(5, 4, i + 9)\n        ax.axis(\'off\')\n        show_img(depth_mask)\n        ax = plt.subplot(5, 4, i + 13)\n        ax.axis(\'off\')\n        show_img(sparse_depth)\n\n        ax = plt.subplot(5, 4, i + 17)\n        ax.axis(\'off\')\n        show_img(sparse_depth_mask)\n        plt.imsave(\'sparse_depth.png\', sparse_depth_mask)\n        if i == 3:\n            plt.show()\n            break\n    \n#test_imgread()'"
cspn_pytorch/train.py,15,"b'#!/usr/bin/env python2\n# -*- coding: utf-8 -*-\n""""""\nCreated on Fri Feb  2 19:16:42 2018\n@author: norbot\n""""""\n\nfrom __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\n\nimport torchvision\nimport torchvision.transforms as transforms\n\nimport os\nimport sys\nimport argparse\n\nfrom torch.autograd import Variable\n\nimport utils\nimport loss as my_loss\nimport lr_scheduler as lrs\nfrom tqdm import tqdm\n\n\nparser = argparse.ArgumentParser(description=\'PyTorch Sparse To Dense Training\')\n\n# net parameters\nparser.add_argument(\'--n_sample\', default=200, type=int, help=\'sampled sparse point number\')\nparser.add_argument(\'--data_set\', default=\'nyudepth\', type=str, help=\'train dataset\')\n\n# optimizer parameters\nparser.add_argument(\'--lr\', default=0.01, type=float, help=\'learning rate\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, help=\'sgd momentum\')\nparser.add_argument(\'--weight_decay\', default=1e-4, type=float, help=\'weight decay (L2 penalty)\')\nparser.add_argument(\'--dampening\', default=0.0, type=float, help=\'dampening for momentum\')\nparser.add_argument(\'--nesterov\', \'-n\', action=\'store_true\', help=\'enables Nesterov momentum\')\nparser.add_argument(\'--num_epoch\', default=40, type=int, help=\'number of epoch for training\')\n\n# network parameters\nparser.add_argument(\'--cspn_step\', default=24, type=int, help=\'steps of propagation\')\nparser.add_argument(\'--cspn_norm_type\', default=\'8sum\', type=str, help=\'norm type of cspn\')\n\n# batch size\nparser.add_argument(\'--batch_size_train\', default=8, type=int, help=\'batch size for training\')\nparser.add_argument(\'--batch_size_eval\', default=1, type=int, help=\'batch size for eval\')\n\n#data directory\nparser.add_argument(\'--save_dir\', default=\'result/base_line\', type=str, help=\'result save directory\')\nparser.add_argument(\'--best_model_dir\', default=\'result/base_line\', type=str, help=\'best model load directory\')\nparser.add_argument(\'--train_list\', default=\'data/nyudepth_hdf5/nyudepth_hdf5_train.csv\', type=str, help=\'train data lists\')\nparser.add_argument(\'--eval_list\', default=\'data/nyudepth_hdf5/nyudepth_hdf5_val.csv\', type=str, help=\'eval data list\')\nparser.add_argument(\'--model\', default=\'base_model\', type=str, help=\'model for net\')\nparser.add_argument(\'--resume\', \'-r\', action=\'store_true\', help=\'resume from checkpoint\')\nparser.add_argument(\'--pretrain\', \'-p\', action=\'store_true\', help=\'load pretrained resnet model\')\n\nargs = parser.parse_args()\n\nsys.path.append(""./models"")\n\nimport update_model\nif args.model == \'cspn_unet\':\n    if args.data_set==\'nyudepth\':\n        print(""==> training model with cspn and unet on nyudepth"")\n        import torch_resnet_cspn_nyu as model\n    elif args.data_set ==\'kitti\':\n        print(""==> training model with cspn and unet on kitti"")\n        import torch_resnet_cspn_kitti as model\nelse:\n    import torch_resnet as model\n\nuse_cuda = torch.cuda.is_available()\n\n\n# global variable\nbest_rmse = sys.maxsize  # best test rmse\ncspn_config = {\'step\': args.cspn_step, \'norm_type\': args.cspn_norm_type}\nstart_epoch = 0 # start from epoch 0 or last checkpoint epoch\n\n\n# Data\nprint(\'==> Preparing data..\')\nif args.data_set==\'nyudepth\':\n    import nyu_dataset_loader as dataset_loader\n    trainset = dataset_loader.NyuDepthDataset(csv_file=args.train_list,\n                                              root_dir=\'.\',\n                                              split = \'train\',\n                                              n_sample = args.n_sample,\n                                              input_format=\'hdf5\')\n    valset = dataset_loader.NyuDepthDataset(csv_file=args.eval_list,\n                                            root_dir=\'.\',\n                                            split = \'val\',\n                                            n_sample = args.n_sample,\n                                            input_format=\'hdf5\')\nelif args.data_set ==\'kitti\':\n    import kitti_dataset_loader as dataset_loader\n    trainset = dataset_loader.KittiDataset(csv_file=args.train_list,\n                                           root_dir=\'.\',\n                                           split = \'train\',\n                                           n_sample = args.n_sample,\n                                           input_format=\'hdf5\')\n    valset = dataset_loader.KittiDataset(csv_file=args.eval_list,\n                                         root_dir=\'.\',\n                                         split = \'val\',\n                                         n_sample = args.n_sample,\n                                         input_format=\'hdf5\')\nelse:\n    print(""==> input unknow dataset.."")\n\ntrainloader = torch.utils.data.DataLoader(trainset,\n                                          batch_size=args.batch_size_train,\n                                          shuffle=True,\n                                          num_workers=2,\n                                          pin_memory=True,\n                                          drop_last=True)\n\nif args.data_set==\'nyudepth\':\n    valloader = torch.utils.data.DataLoader(valset,\n                                            batch_size=args.batch_size_eval,\n                                            shuffle=False,\n                                            num_workers=2,\n                                            pin_memory=True,\n                                            drop_last=True)\nelif args.data_set == \'kitti\':\n    valloader = torch.utils.data.DataLoader(valset,\n                                            batch_size=args.batch_size_eval,\n                                            shuffle=True,\n                                            num_workers=2,\n                                            pin_memory=True,\n                                            drop_last=True)\n# Model\n\nprint(\'==> Prepare results folder and files...\')\nutils.log_file_folder_make_lr(args.save_dir)\nprint(\'==> Building model..\')\n\nif args.data_set == \'nyudepth\':\n    net = model.resnet50(pretrained = args.pretrain,\n                         cspn_config=cspn_config)\nelif args.data_set == \'kitti\':\n    net = model.resnet18(pretrained = args.pretrain,\n                         cspn_config=cspn_config)\nelse:\n    print(""==> input unknow dataset.."")\n\nif args.resume:\n    # Load best model checkpoint.\n    print(\'==> Resuming from best model..\')\n    best_model_path = os.path.join(args.best_model_dir, \'best_model.pth\')\n    print(best_model_path)\n    assert os.path.isdir(args.best_model_dir), \'Error: no checkpoint directory found!\'\n    best_model_dict = torch.load(best_model_path)\n    best_model_dict = update_model.remove_moudle(best_model_dict)\n    net.load_state_dict(update_model.update_model(net, best_model_dict))\n\n\nif use_cuda:\n    net.cuda()\n    assert torch.cuda.device_count() == 1, \'only support single gpu\'\n    net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n    cudnn.benchmark = True\n\n\ncriterion = my_loss.Wighted_L1_Loss().cuda()\noptimizer = optim.SGD(net.parameters(),\n                      lr=args.lr,\n                      momentum=args.momentum,\n                      weight_decay=args.weight_decay,\n                      nesterov=args.nesterov,\n                      dampening=args.dampening)\n\nscheduler = lrs.ReduceLROnPlateau(optimizer, \'min\') # set up scheduler\n\n\n# Training\ndef train(epoch):\n    net.train()\n    total_step_train = 0\n    train_loss = 0.0\n    error_sum_train = {\'MSE\':0, \'RMSE\':0, \'ABS_REL\':0, \'LG10\':0, \'MAE\':0,\\\n                       \'DELTA1.02\':0, \'DELTA1.05\':0, \'DELTA1.10\':0, \\\n                       \'DELTA1.25\':0, \'DELTA1.25^2\':0, \'DELTA1.25^3\':0,}\n\n    tbar = tqdm(trainloader)\n    for batch_idx, sample in enumerate(tbar):\n        [inputs, targets] = [sample[\'rgbd\'] , sample[\'depth\']]\n        if use_cuda:\n            inputs, targets = inputs.cuda(), targets.cuda()\n        optimizer.zero_grad()\n        inputs, targets = Variable(inputs), Variable(targets)\n        outputs = net(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n        error_str = \'Epoch: %d, loss=%.4f\' % (epoch, train_loss / (batch_idx + 1))\n        tbar.set_description(error_str)\n\n        targets = targets.data.cpu()\n        outputs = outputs.data.cpu()\n        error_result = utils.evaluate_error(gt_depth=targets, pred_depth=outputs)\n        total_step_train += args.batch_size_train\n        error_avg = utils.avg_error(error_sum_train,\n                                    error_result,\n                                    total_step_train,\n                                    args.batch_size_train)\n        if batch_idx % 500 == 0:\n            utils.print_error(\'training_result: step(average)\',\n                              epoch,\n                              batch_idx,\n                              loss,\n                              error_result,\n                              error_avg,\n                              print_out=True)\n\n    error_avg = utils.avg_error(error_sum_train,\n                                error_result,\n                                total_step_train,\n                                args.batch_size_train)\n    for param_group in optimizer.param_groups:\n        old_lr = float(param_group[\'lr\'])\n    utils.log_result_lr(args.save_dir, error_avg, epoch, old_lr, False, \'train\')\n\n    tmp_name = ""epoch_%02d.pth"" % (epoch)\n    save_name = os.path.join(args.save_dir, tmp_name)\n    torch.save(net.state_dict(), save_name)\n\n\ndef val(epoch):\n    global best_rmse\n    is_best_model = False\n    net.eval()\n    total_step_val = 0\n    eval_loss = 0.0\n    error_sum_val = {\'MSE\':0, \'RMSE\':0, \'ABS_REL\':0, \'LG10\':0, \'MAE\':0,\\\n                     \'DELTA1.02\':0, \'DELTA1.05\':0, \'DELTA1.10\':0, \\\n                     \'DELTA1.25\':0, \'DELTA1.25^2\':0, \'DELTA1.25^3\':0,}\n\n    tbar = tqdm(valloader)\n    for batch_idx, sample in enumerate(tbar):\n        [inputs, targets] = [sample[\'rgbd\'] , sample[\'depth\']]\n        with torch.no_grad():\n            if use_cuda:\n                inputs, targets = inputs.cuda(), targets.cuda()\n            inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n            outputs = net(inputs)\n        loss = criterion(outputs, targets)\n        targets = targets.data.cpu()\n        outputs = outputs.data.cpu()\n        loss = loss.data.cpu()\n        eval_loss += loss.item()\n        error_str = \'Epoch: %d, loss=%.4f\' % (epoch, eval_loss / (batch_idx + 1))\n        tbar.set_description(error_str)\n\n        error_result = utils.evaluate_error(gt_depth=targets, pred_depth=outputs)\n        total_step_val += args.batch_size_eval\n        error_avg = utils.avg_error(error_sum_val, error_result, total_step_val, args.batch_size_eval)\n\n    utils.print_error(\'eval_result: step(average)\',\n                      epoch, batch_idx, loss,\n                      error_result, error_avg, print_out=True)\n\n    #log best_model\n    if utils.updata_best_model(error_avg, best_rmse):\n        is_best_model = True\n        best_rmse = error_avg[\'RMSE\']\n    for param_group in optimizer.param_groups:\n        old_lr = float(param_group[\'lr\'])\n    utils.log_result_lr(args.save_dir, error_avg, epoch, old_lr, is_best_model, \'eval\')\n\n    # saving best_model\n    if is_best_model:\n        print(\'==> saving best model at epoch %d\' % epoch)\n        best_model_pytorch = os.path.join(args.save_dir, \'best_model.pth\')\n        torch.save(net.state_dict(), best_model_pytorch)\n\n    #updata lr\n    scheduler.step(error_avg[\'MAE\'], epoch)\n\n\ndef train_val():\n    for epoch in range(0, args.num_epoch):\n        train(epoch)\n        val(epoch)\n\nif __name__ == \'__main__\':\n    train_val()\n'"
cspn_pytorch/utils.py,21,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Sat Feb  3 16:27:01 2018\n\n@author: norbot\n""""""\n\nimport torch\nimport math\nimport os\nimport data_transform\nfrom torchvision import transforms\nfrom PIL import Image, ImageOps\nimport matplotlib.pyplot as plt\n\ndef max_of_two(y_over_z, z_over_y):\n    return torch.max(y_over_z, z_over_y)\n\ndef evaluate_error(gt_depth, pred_depth):\n    # for numerical stability\n    depth_mask = gt_depth>0.0001\n    batch_size = gt_depth.size(0)\n    error = {\'MSE\':0, \'RMSE\':0, \'ABS_REL\':0, \'LG10\':0, \'MAE\':0,\\\n             \'DELTA1.02\':0, \'DELTA1.05\':0, \'DELTA1.10\':0, \\\n             \'DELTA1.25\':0, \'DELTA1.25^2\':0, \'DELTA1.25^3\':0,\n             }\n    _pred_depth = pred_depth[depth_mask]\n    _gt_depth   = gt_depth[depth_mask]\n    n_valid_element = _gt_depth.size(0)\n\n    if n_valid_element > 0:\n        diff_mat = torch.abs(_gt_depth-_pred_depth)\n        rel_mat = torch.div(diff_mat, _gt_depth)\n        error[\'MSE\'] = torch.sum(torch.pow(diff_mat, 2))/n_valid_element\n        error[\'RMSE\'] = math.sqrt(error[\'MSE\'])\n        error[\'MAE\'] = torch.sum(diff_mat)/n_valid_element\n        error[\'ABS_REL\'] = torch.sum(rel_mat)/n_valid_element\n        y_over_z = torch.div(_gt_depth, _pred_depth)\n        z_over_y = torch.div(_pred_depth, _gt_depth)\n        max_ratio = max_of_two(y_over_z, z_over_y)\n        error[\'DELTA1.02\'] = torch.sum(max_ratio < 1.02).numpy()/float(n_valid_element)\n        error[\'DELTA1.05\'] = torch.sum(max_ratio < 1.05).numpy()/float(n_valid_element)\n        error[\'DELTA1.10\'] = torch.sum(max_ratio < 1.10).numpy()/float(n_valid_element)\n        error[\'DELTA1.25\'] = torch.sum(max_ratio < 1.25).numpy()/float(n_valid_element)\n        error[\'DELTA1.25^2\'] = torch.sum(max_ratio < 1.25**2).numpy()/float(n_valid_element)\n        error[\'DELTA1.25^3\'] = torch.sum(max_ratio < 1.25**3).numpy()/float(n_valid_element)\n    return error\n\n# avg the error\ndef avg_error(error_sum, error_step, total_step, batch_size):\n    error_avg = {\'MSE\':0, \'RMSE\':0, \'ABS_REL\':0, \'LG10\':0, \'MAE\':0,\\\n                 \'DELTA1.02\':0, \'DELTA1.05\':0, \'DELTA1.10\':0, \\\n                 \'DELTA1.25\':0, \'DELTA1.25^2\':0, \'DELTA1.25^3\':0,}\n    for item, value in error_step.items():\n        error_sum[item] += error_step[item] * batch_size\n        error_avg[item] = error_sum[item]/float(total_step)\n    return error_avg\n\n\n# print error\ndef print_error(split, epoch, step, loss, error, error_avg, print_out=False):\n    format_str = (\'%s ===>\\n\\\n                  Epoch: %d, step: %d, loss=%.4f\\n\\\n                  MSE=%.4f(%.4f)\\tRMSE=%.4f(%.4f)\\tMAE=%.4f(%.4f)\\tABS_REL=%.4f(%.4f)\\n\\\n                  DELTA1.02=%.4f(%.4f)\\tDELTA1.05=%.4f(%.4f)\\tDELTA1.10=%.4f(%.4f)\\n\\\n                  DELTA1.25=%.4f(%.4f)\\tDELTA1.25^2=%.4f(%.4f)\\tDELTA1.25^3=%.4f(%.4f)\\n\')\n    error_str = format_str % (split, epoch, step, loss,\\\n                         error[\'MSE\'], error_avg[\'MSE\'], error[\'RMSE\'], error_avg[\'RMSE\'],\\\n                         error[\'MAE\'], error_avg[\'MAE\'], error[\'ABS_REL\'], error_avg[\'ABS_REL\'],\\\n                         error[\'DELTA1.02\'], error_avg[\'DELTA1.02\'], \\\n                         error[\'DELTA1.05\'], error_avg[\'DELTA1.05\'], \\\n                         error[\'DELTA1.10\'], error_avg[\'DELTA1.10\'], \\\n                         error[\'DELTA1.25\'], error_avg[\'DELTA1.25\'], \\\n                         error[\'DELTA1.25^2\'], error_avg[\'DELTA1.25^2\'], \\\n                         error[\'DELTA1.25^3\'], error_avg[\'DELTA1.25^3\'])\n    if print_out:\n        print(error_str)\n    return error_str\n\n\ndef print_single_error(epoch, step, loss, error):\n    format_str = (\'%s ===>\\n\\\n                  Epoch: %d, step: %d, loss=%.4f\\n\\\n                  MSE=%.4f\\tRMSE=%.4f\\tMAE=%.4f\\tABS_REL=%.4f\\n\\\n                  DELTA1.02=%.4f\\tDELTA1.05=%.4f\\tDELTA1.10=%.4f\\n\\\n                  DELTA1.25=%.4f\\tDELTA1.25^2=%.4f\\tDELTA1.25^3=%.4f\\n\')\n    print (format_str % (\'eval_avg_error\', epoch, step, loss,\\\n                         error[\'MSE\'], error[\'RMSE\'], error[\'MAE\'],  error[\'ABS_REL\'], \\\n                         error[\'DELTA1.02\'], error[\'DELTA1.05\'], error[\'DELTA1.10\'], \\\n                         error[\'DELTA1.25\'], error[\'DELTA1.25^2\'], error[\'DELTA1.25^3\']))\n\n# update_best_model\ndef updata_best_model(error_avg, best_RMSE):\n    if error_avg[\'RMSE\'] < best_RMSE:\n        return True\n    else:\n        return False\n\n# log best_model\ndef log_file_folder_make(save_dir):\n    if not os.path.isdir(save_dir):\n        os.makedirs(save_dir, 0o777)\n\n    train_log_file = os.path.join(save_dir, \'log_train.txt\')\n    train_fd = open(train_log_file, \'w\')\n    train_fd.write(\'epoch\\t bestModel\\t MSE\\t RMSE\\t MAE\\t \\\n                   DELTA1.02\\t DELTA1.05\\t DELTA1.10\\t DELTA1.25\\t \\\n                   DELTA1.25^2\\t DELTA1.25^3\\t ABS_REL\\n\')\n    train_fd.close()\n\n    eval_log_file = os.path.join(save_dir, \'log_eval.txt\')\n    eval_fd = open(eval_log_file, \'w\')\n    eval_fd.write(\'epoch\\t bestModel\\t MSE\\t RMSE\\t MAE\\t \\\n                  DELTA1.02\\t DELTA1.05\\t DELTA1.10\\t \\\n                  DELTA1.25\\t DELTA1.25^2\\t DELTA1.25^3\\t ABS_REL\\n\')\n    eval_fd.close()\n\ndef log_result(save_dir, error_avg, epoch, lr, best_model, split):\n    format_str = (\'%.4f\\t %.4f\\t\\t %.4f\\t %.4f\\t %.4f\\t %.4f\\t %.4f\\t %.4f\\t %.4f\\t %.4f\\t %.4f\\t %.4f\\n\')\n    train_log_file = os.path.join(save_dir, \'log_train.txt\')\n    eval_log_file = os.path.join(save_dir, \'log_eval.txt\')\n    if split == \'train\':\n        train_fd = open(train_log_file, \'a\')\n        train_fd.write(format_str%(epoch, best_model, error_avg[\'MSE\'], error_avg[\'RMSE\'],\\\n                                   error_avg[\'MAE\'], error_avg[\'DELTA1.02\'], error_avg[\'DELTA1.05\'],\\\n                                   error_avg[\'DELTA1.10\'], error_avg[\'DELTA1.25\'], error_avg[\'DELTA1.25^2\'],\\\n                                   error_avg[\'DELTA1.25^3\'], error_avg[\'ABS_REL\']))\n        train_fd.close()\n    elif split == \'eval\':\n        eval_fd = open(eval_log_file, \'a\')\n        eval_fd.write(format_str%(epoch, best_model, error_avg[\'MSE\'], error_avg[\'RMSE\'],\\\n                                  error_avg[\'MAE\'], error_avg[\'DELTA1.02\'], error_avg[\'DELTA1.05\'],\\\n                                  error_avg[\'DELTA1.10\'], error_avg[\'DELTA1.25\'], error_avg[\'DELTA1.25^2\'],\\\n                                  error_avg[\'DELTA1.25^3\'], error_avg[\'ABS_REL\']))\n        eval_fd.close()\n\n# log best_model\ndef log_file_folder_make_lr(save_dir):\n    if not os.path.isdir(save_dir):\n        os.makedirs(save_dir, 0o777)\n    train_log_file = os.path.join(save_dir, \'log_train.txt\')\n    train_fd = open(train_log_file, \'w\')\n    train_fd.write(\'epoch\\t lr\\t bestModel\\t MSE\\t RMSE\\t MAE\\t \\\n                   DELTA1.02\\t DELTA1.05\\t DELTA1.10\\t DELTA1.25\\t \\\n                   DELTA1.25^2\\t DELTA1.25^3\\t ABS_REL\\n\')\n    train_fd.close()\n\n    eval_log_file = os.path.join(save_dir, \'log_eval.txt\')\n    eval_fd = open(eval_log_file, \'w\')\n    eval_fd.write(\'epoch\\t lr\\t bestModel\\t MSE\\t RMSE\\t MAE\\t \\\n                  DELTA1.02\\t DELTA1.05\\t DELTA1.10\\t DELTA1.25\\t \\\n                  DELTA1.25^2\\t DELTA1.25^3\\t ABS_REL\\n\')\n    eval_fd.close()\n\ndef log_result_lr(save_dir, error_avg, epoch, lr, best_model, split):\n    format_str = (\'%.4f\\t %.4f\\t %.4f\\t\\t %.4f\\t %.4f\\t %.4f\\t %.4f\\t %.4f\\t %.4f\\t %.4f\\t %.4f\\t %.4f\\t %.4f\\n\')\n    train_log_file = os.path.join(save_dir, \'log_train.txt\')\n    eval_log_file = os.path.join(save_dir, \'log_eval.txt\')\n    if split == \'train\':\n        train_fd = open(train_log_file, \'a\')\n        train_fd.write(format_str%(epoch, lr, best_model, error_avg[\'MSE\'], error_avg[\'RMSE\'],\\\n                                   error_avg[\'MAE\'], error_avg[\'DELTA1.02\'], error_avg[\'DELTA1.05\'],\\\n                                   error_avg[\'DELTA1.10\'], error_avg[\'DELTA1.25\'], error_avg[\'DELTA1.25^2\'],\\\n                                   error_avg[\'DELTA1.25^3\'], error_avg[\'ABS_REL\']))\n        train_fd.close()\n    elif split == \'eval\':\n        eval_fd = open(eval_log_file, \'a\')\n        eval_fd.write(format_str%(epoch, lr, best_model, error_avg[\'MSE\'], error_avg[\'RMSE\'],\\\n                                  error_avg[\'MAE\'], error_avg[\'DELTA1.02\'], error_avg[\'DELTA1.05\'],\\\n                                  error_avg[\'DELTA1.10\'], error_avg[\'DELTA1.25\'], error_avg[\'DELTA1.25^2\'],\\\n                                  error_avg[\'DELTA1.25^3\'], error_avg[\'ABS_REL\']))\n        eval_fd.close()\n\n\ndef un_normalize(tensor):\n    img_mean = (0.485, 0.456, 0.406)\n    img_std = (0.229, 0.224, 0.225)\n    for t, m, s in zip(tensor, img_mean, img_std):\n        t.mul_(s).add_(m)\n    return tensor\n\ndef save_eval_img(data_set, model_dir, index, input_rgbd, input_rgb, gt_depth, pred_depth):\n    img_save_folder = os.path.join(model_dir, \'eval_result\')\n    if not os.path.isdir(img_save_folder):\n        os.makedirs(img_save_folder, 0o777)\n\n    save_name_rgb = os.path.join(img_save_folder, ""%05d_input.png"" % (index))\n    save_name_gt = os.path.join(img_save_folder, ""%05d_gt.png"" % (index))\n    save_name_pred = os.path.join(img_save_folder, ""%05d_pred.png"" % (index))\n    save_name_sparse_point = os.path.join(img_save_folder, ""%05d_sparse_point.png"" % (index))\n    save_name_sparse_mask = os.path.join(img_save_folder, ""%05d_sparse_mask.png"" % (index))\n    save_rgb = transforms.ToPILImage()(torch.squeeze(input_rgb, 0))\n    save_gt = None\n    save_pred = None\n    if data_set == \'kitti\':\n        save_sparse_point = data_transform.ToPILImage()(input_rgbd[:,3,:,:])\n        save_sparse_mask = data_transform.ToPILImage()(input_rgbd[:,3,:,:].sign())\n        save_gt = data_transform.ToPILImage()(torch.squeeze(gt_depth*1.0, 0))\n        save_pred = data_transform.ToPILImage()(torch.squeeze(pred_depth*1.0, 0))\n        plt.imsave(save_name_rgb, save_rgb)\n        plt.imsave(save_name_gt, save_gt)\n        plt.imsave(save_name_pred, save_pred)\n\n    elif data_set == \'nyudepth\':\n        save_gt = data_transform.ToPILImage()(torch.squeeze(gt_depth*25.5, 0))\n        save_pred = data_transform.ToPILImage()(torch.squeeze(pred_depth*25.5, 0))\n        save_rgb.save(save_name_rgb)\n        save_gt.save(save_name_gt)\n        save_pred.save(save_name_pred)\n\ndef test_eval_error():\n    gt_depth = torch.abs(torch.randn(1,3,4))\n    pred_depth = torch.abs(torch.randn(1,3,4))\n    eval_result = evaluate_error(gt_depth, pred_depth)\n    for item, value in eval_result.items():\n        print((\'%s\\\' value is: %f\') %(item, eval_result[item]))\n'"
cspn_pytorch/models/__init__.py,0,b'from base_model import *\nfrom torch_resnet import *\n#from torch_resnet_ps import *\n'
cspn_pytorch/models/cspn.py,19,"b'""""""\n@author: Xinjing Cheng & Peng Wang\n\n""""""\n\nimport torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\n\nclass Affinity_Propagate(nn.Module):\n\n    def __init__(self,\n                 prop_time,\n                 prop_kernel,\n                 norm_type=\'8sum\'):\n        """"""\n\n        Inputs:\n            prop_time: how many steps for CSPN to perform\n            prop_kernel: the size of kernel (current only support 3x3)\n            way to normalize affinity\n                \'8sum\': normalize using 8 surrounding neighborhood\n                \'8sum_abs\': normalization enforcing affinity to be positive\n                            This will lead the center affinity to be 0\n        """"""\n        super(Affinity_Propagate, self).__init__()\n        self.prop_time = prop_time\n        self.prop_kernel = prop_kernel\n        assert prop_kernel == 3, \'this version only support 8 (3x3 - 1) neighborhood\'\n\n        self.norm_type = norm_type\n        assert norm_type in [\'8sum\', \'8sum_abs\']\n\n        self.in_feature = 1\n        self.out_feature = 1\n\n\n    def forward(self, guidance, blur_depth, sparse_depth=None):\n\n        self.sum_conv = nn.Conv3d(in_channels=8,\n                                  out_channels=1,\n                                  kernel_size=(1, 1, 1),\n                                  stride=1,\n                                  padding=0,\n                                  bias=False)\n        weight = torch.ones(1, 8, 1, 1, 1).cuda()\n        self.sum_conv.weight = nn.Parameter(weight)\n        for param in self.sum_conv.parameters():\n            param.requires_grad = False\n\n        gate_wb, gate_sum = self.affinity_normalization(guidance)\n\n        # pad input and convert to 8 channel 3D features\n        raw_depth_input = blur_depth\n\n        #blur_depht_pad = nn.ZeroPad2d((1,1,1,1))\n        result_depth = blur_depth\n\n        if sparse_depth is not None:\n            sparse_mask = sparse_depth.sign()\n\n        for i in range(self.prop_time):\n            # one propagation\n            spn_kernel = self.prop_kernel\n            result_depth = self.pad_blur_depth(result_depth)\n            neigbor_weighted_sum = self.sum_conv(gate_wb * result_depth)\n            neigbor_weighted_sum = neigbor_weighted_sum.squeeze(1)\n            neigbor_weighted_sum = neigbor_weighted_sum[:, :, 1:-1, 1:-1]\n            result_depth = neigbor_weighted_sum\n\n            if \'8sum\' in self.norm_type:\n                result_depth = (1.0 - gate_sum) * raw_depth_input + result_depth\n            else:\n                raise ValueError(\'unknown norm %s\' % self.norm_type)\n\n            if sparse_depth is not None:\n                result_depth = (1 - sparse_mask) * result_depth + sparse_mask * raw_depth_input\n\n        return result_depth\n\n    def affinity_normalization(self, guidance):\n\n        # normalize features\n        if \'abs\' in self.norm_type:\n            guidance = torch.abs(guidance)\n\n        gate1_wb_cmb = guidance.narrow(1, 0                   , self.out_feature)\n        gate2_wb_cmb = guidance.narrow(1, 1 * self.out_feature, self.out_feature)\n        gate3_wb_cmb = guidance.narrow(1, 2 * self.out_feature, self.out_feature)\n        gate4_wb_cmb = guidance.narrow(1, 3 * self.out_feature, self.out_feature)\n        gate5_wb_cmb = guidance.narrow(1, 4 * self.out_feature, self.out_feature)\n        gate6_wb_cmb = guidance.narrow(1, 5 * self.out_feature, self.out_feature)\n        gate7_wb_cmb = guidance.narrow(1, 6 * self.out_feature, self.out_feature)\n        gate8_wb_cmb = guidance.narrow(1, 7 * self.out_feature, self.out_feature)\n\n        # gate1:left_top, gate2:center_top, gate3:right_top\n        # gate4:left_center,              , gate5: right_center\n        # gate6:left_bottom, gate7: center_bottom, gate8: right_bottm\n\n        # top pad\n        left_top_pad = nn.ZeroPad2d((0,2,0,2))\n        gate1_wb_cmb = left_top_pad(gate1_wb_cmb).unsqueeze(1)\n\n        center_top_pad = nn.ZeroPad2d((1,1,0,2))\n        gate2_wb_cmb = center_top_pad(gate2_wb_cmb).unsqueeze(1)\n\n        right_top_pad = nn.ZeroPad2d((2,0,0,2))\n        gate3_wb_cmb = right_top_pad(gate3_wb_cmb).unsqueeze(1)\n\n        # center pad\n        left_center_pad = nn.ZeroPad2d((0,2,1,1))\n        gate4_wb_cmb = left_center_pad(gate4_wb_cmb).unsqueeze(1)\n\n        right_center_pad = nn.ZeroPad2d((2,0,1,1))\n        gate5_wb_cmb = right_center_pad(gate5_wb_cmb).unsqueeze(1)\n\n        # bottom pad\n        left_bottom_pad = nn.ZeroPad2d((0,2,2,0))\n        gate6_wb_cmb = left_bottom_pad(gate6_wb_cmb).unsqueeze(1)\n\n        center_bottom_pad = nn.ZeroPad2d((1,1,2,0))\n        gate7_wb_cmb = center_bottom_pad(gate7_wb_cmb).unsqueeze(1)\n\n        right_bottm_pad = nn.ZeroPad2d((2,0,2,0))\n        gate8_wb_cmb = right_bottm_pad(gate8_wb_cmb).unsqueeze(1)\n\n        gate_wb = torch.cat((gate1_wb_cmb,gate2_wb_cmb,gate3_wb_cmb,gate4_wb_cmb,\n                             gate5_wb_cmb,gate6_wb_cmb,gate7_wb_cmb,gate8_wb_cmb), 1)\n\n        # normalize affinity using their abs sum\n        gate_wb_abs = torch.abs(gate_wb)\n        abs_weight = self.sum_conv(gate_wb_abs)\n\n        gate_wb = torch.div(gate_wb, abs_weight)\n        gate_sum = self.sum_conv(gate_wb)\n\n        gate_sum = gate_sum.squeeze(1)\n        gate_sum = gate_sum[:, :, 1:-1, 1:-1]\n\n        return gate_wb, gate_sum\n\n\n    def pad_blur_depth(self, blur_depth):\n        # top pad\n        left_top_pad = nn.ZeroPad2d((0,2,0,2))\n        blur_depth_1 = left_top_pad(blur_depth).unsqueeze(1)\n        center_top_pad = nn.ZeroPad2d((1,1,0,2))\n        blur_depth_2 = center_top_pad(blur_depth).unsqueeze(1)\n        right_top_pad = nn.ZeroPad2d((2,0,0,2))\n        blur_depth_3 = right_top_pad(blur_depth).unsqueeze(1)\n\n        # center pad\n        left_center_pad = nn.ZeroPad2d((0,2,1,1))\n        blur_depth_4 = left_center_pad(blur_depth).unsqueeze(1)\n        right_center_pad = nn.ZeroPad2d((2,0,1,1))\n        blur_depth_5 = right_center_pad(blur_depth).unsqueeze(1)\n\n        # bottom pad\n        left_bottom_pad = nn.ZeroPad2d((0,2,2,0))\n        blur_depth_6 = left_bottom_pad(blur_depth).unsqueeze(1)\n        center_bottom_pad = nn.ZeroPad2d((1,1,2,0))\n        blur_depth_7 = center_bottom_pad(blur_depth).unsqueeze(1)\n        right_bottm_pad = nn.ZeroPad2d((2,0,2,0))\n        blur_depth_8 = right_bottm_pad(blur_depth).unsqueeze(1)\n\n        result_depth = torch.cat((blur_depth_1, blur_depth_2, blur_depth_3, blur_depth_4,\n                                  blur_depth_5, blur_depth_6, blur_depth_7, blur_depth_8), 1)\n        return result_depth\n\n\n    def normalize_gate(self, guidance):\n        gate1_x1_g1 = guidance.narrow(1,0,1)\n        gate1_x1_g2 = guidance.narrow(1,1,1)\n        gate1_x1_g1_abs = torch.abs(gate1_x1_g1)\n        gate1_x1_g2_abs = torch.abs(gate1_x1_g2)\n        elesum_gate1_x1 = torch.add(gate1_x1_g1_abs, gate1_x1_g2_abs)\n        gate1_x1_g1_cmb = torch.div(gate1_x1_g1, elesum_gate1_x1)\n        gate1_x1_g2_cmb = torch.div(gate1_x1_g2, elesum_gate1_x1)\n        return gate1_x1_g1_cmb, gate1_x1_g2_cmb\n\n\n    def max_of_4_tensor(self, element1, element2, element3, element4):\n        max_element1_2 = torch.max(element1, element2)\n        max_element3_4 = torch.max(element3, element4)\n        return torch.max(max_element1_2, max_element3_4)\n\n    def max_of_8_tensor(self, element1, element2, element3, element4, element5, element6, element7, element8):\n        max_element1_2 = self.max_of_4_tensor(element1, element2, element3, element4)\n        max_element3_4 = self.max_of_4_tensor(element5, element6, element7, element8)\n        return torch.max(max_element1_2, max_element3_4)\n\n\n'"
cspn_pytorch/models/torch_resnet_cspn_nyu.py,16,"b'0#!/usr/bin/env python2\n# -*- coding: utf-8 -*-\n""""""\nCreated on Sat Feb  3 15:32:49 2018\n\n@author: norbot\n""""""\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torch.utils.model_zoo as model_zoo\nimport cspn as post_process\nfrom torch.autograd import Variable\nimport update_model\nimport torch.nn.functional as F\n\n# memory analyze\nimport gc\n\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\n           \'resnet152\']\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\nmodel_path ={\n    \'resnet18\': \'pretrained/resnet18.pth\',\n    \'resnet50\': \'pretrained/resnet50.pth\'\n}\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\nclass Unpool(nn.Module):\n    # Unpool: 2*2 unpooling with zero padding\n    def __init__(self, num_channels, stride=2):\n        super(Unpool, self).__init__()\n\n        self.num_channels = num_channels\n        self.stride = stride\n\n        # create kernel [1, 0; 0, 0]\n        self.weights = torch.autograd.Variable(torch.zeros(num_channels, 1, stride, stride).cuda()) # currently not compatible with running on CPU\n        self.weights[:,:,0,0] = 1\n\n    def forward(self, x):\n        return F.conv_transpose2d(x, self.weights, stride=self.stride, groups=self.num_channels)\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass UpProj_Block(nn.Module):\n    def __init__(self, in_channels, out_channels, oheight=0, owidth=0):\n        super(UpProj_Block, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=5, stride=1, padding=2, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.sc_conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=5, stride=1, padding=2, bias=False)\n        self.sc_bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.oheight = oheight\n        self.owidth = owidth\n        self._up_pool = Unpool(in_channels)\n\n    def _up_pooling(self, x, scale):\n        oheight = 0\n        owidth = 0\n        if self.oheight == 0 and self.owidth == 0:\n            oheight = scale * x.size(2)\n            owidth = scale * x.size(3)\n            x = self._up_pool(x)\n        else:\n            oheight = self.oheight\n            owidth = self.owidth\n            x = self._up_pool(x)\n        return x\n\n    def forward(self, x):\n        x = self._up_pooling(x, 2)\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        short_cut = self.sc_bn1(self.sc_conv1(x))\n        out += short_cut\n        out = self.relu(out)\n        return out\n\nclass Simple_Gudi_UpConv_Block(nn.Module):\n    def __init__(self, in_channels, out_channels, oheight=0, owidth=0):\n        super(Simple_Gudi_UpConv_Block, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=5, stride=1, padding=2, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.oheight = oheight\n        self.owidth = owidth\n        self._up_pool = Unpool(in_channels)\n\n\n    def _up_pooling(self, x, scale):\n\n        x = self._up_pool(x)\n        if self.oheight !=0 and self.owidth !=0:\n            x = x.narrow(2,0,self.oheight)\n            x = x.narrow(3,0,self.owidth)\n        return x\n\n\n    def forward(self, x):\n        x = self._up_pooling(x, 2)\n        out = self.relu(self.bn1(self.conv1(x)))\n        return out\n\nclass Simple_Gudi_UpConv_Block_Last_Layer(nn.Module):\n    def __init__(self, in_channels, out_channels, oheight=0, owidth=0):\n        super(Simple_Gudi_UpConv_Block_Last_Layer, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.oheight = oheight\n        self.owidth = owidth\n        self._up_pool = Unpool(in_channels)\n\n    def _up_pooling(self, x, scale):\n\n        x = self._up_pool(x)\n        if self.oheight != 0 and self.owidth != 0:\n            x = x.narrow(2, 0, self.oheight)\n            x = x.narrow(3, 0, self.owidth)\n        return x\n\n    def forward(self, x):\n        x = self._up_pooling(x, 2)\n        out = self.conv1(x)\n        return out\n\nclass Gudi_UpProj_Block(nn.Module):\n    def __init__(self, in_channels, out_channels, oheight=0, owidth=0):\n        super(Gudi_UpProj_Block, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=5, stride=1, padding=2, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.sc_conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=5, stride=1, padding=2, bias=False)\n        self.sc_bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.oheight = oheight\n        self.owidth = owidth\n\n    def _up_pooling(self, x, scale):\n\n        x = nn.Upsample(scale_factor=scale, mode=\'nearest\')(x)\n        if self.oheight !=0 and self.owidth !=0:\n            x = x[:,:,0:self.oheight, 0:self.owidth]\n        mask = torch.zeros_like(x)\n        for h in range(0, self.oheight, 2):\n            for w in range(0, self.owidth, 2):\n                mask[:,:,h,w] = 1\n        x = torch.mul(mask, x)\n        return x\n\n    def forward(self, x):\n        x = self._up_pooling(x, 2)\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        short_cut = self.sc_bn1(self.sc_conv1(x))\n        out += short_cut\n        out = self.relu(out)\n        return out\n\n\nclass Gudi_UpProj_Block_Cat(nn.Module):\n    def __init__(self, in_channels, out_channels, oheight=0, owidth=0):\n        super(Gudi_UpProj_Block_Cat, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=5, stride=1, padding=2, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv1_1 = nn.Conv2d(out_channels*2, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1_1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.sc_conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=5, stride=1, padding=2, bias=False)\n        self.sc_bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.oheight = oheight\n        self.owidth = owidth\n        self._up_pool = Unpool(in_channels)\n\n    def _up_pooling(self, x, scale):\n\n        x = self._up_pool(x)\n        if self.oheight !=0 and self.owidth !=0:\n            x = x.narrow(2, 0, self.oheight)\n            x = x.narrow(3, 0, self.owidth)\n        return x\n\n    def forward(self, x, side_input):\n        x = self._up_pooling(x, 2)\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = torch.cat((out, side_input), 1)\n        out = self.relu(self.bn1_1(self.conv1_1(out)))\n        out = self.bn2(self.conv2(out))\n        short_cut = self.sc_bn1(self.sc_conv1(x))\n        out += short_cut\n        out = self.relu(out)\n        return out\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, up_proj_block, cspn_config=None):\n        self.inplanes = 64\n        cspn_config_default = {\'step\': 24, \'kernel\': 3, \'norm_type\': \'8sum\'}\n        if not (cspn_config is None):\n            cspn_config_default.update(cspn_config)\n        print(cspn_config_default)\n\n        super(ResNet, self).__init__()\n        self.conv1_1 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.mid_channel = 256*block.expansion\n        self.conv2 = nn.Conv2d(512*block.expansion, 512*block.expansion, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(512*block.expansion)\n        self.up_proj_layer1 = self._make_up_conv_layer(up_proj_block,\n                                                       self.mid_channel,\n                                                       int(self.mid_channel/2))\n        self.up_proj_layer2 = self._make_up_conv_layer(up_proj_block,\n                                                       int(self.mid_channel/2),\n                                                       int(self.mid_channel/4))\n        self.up_proj_layer3 = self._make_up_conv_layer(up_proj_block,\n                                                       int(self.mid_channel/4),\n                                                       int(self.mid_channel/8))\n        self.up_proj_layer4 = self._make_up_conv_layer(up_proj_block,\n                                                       int(self.mid_channel/8),\n                                                       int(self.mid_channel/16))\n        self.conv3 = nn.Conv2d(128, 1, kernel_size=3, stride=1, padding=1, bias=False)\n        self.post_process_layer = self._make_post_process_layer(cspn_config_default)\n        self.gud_up_proj_layer1 = self._make_gud_up_conv_layer(Gudi_UpProj_Block, 2048, 1024, 15, 19)\n        self.gud_up_proj_layer2 = self._make_gud_up_conv_layer(Gudi_UpProj_Block_Cat, 1024, 512, 29, 38)\n        self.gud_up_proj_layer3 = self._make_gud_up_conv_layer(Gudi_UpProj_Block_Cat, 512, 256, 57, 76)\n        self.gud_up_proj_layer4 = self._make_gud_up_conv_layer(Gudi_UpProj_Block_Cat, 256, 64, 114, 152)\n        self.gud_up_proj_layer5 = self._make_gud_up_conv_layer(Simple_Gudi_UpConv_Block_Last_Layer, 64, 1, 228, 304)\n        self.gud_up_proj_layer6 = self._make_gud_up_conv_layer(Simple_Gudi_UpConv_Block_Last_Layer, 64, 8, 228, 304)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def _make_up_conv_layer(self, up_proj_block, in_channels, out_channels):\n        return up_proj_block(in_channels, out_channels)\n\n    def _make_gud_up_conv_layer(self, up_proj_block, in_channels, out_channels, oheight, owidth):\n        return up_proj_block(in_channels, out_channels, oheight, owidth)\n\n    def _make_post_process_layer(self, cspn_config=None):\n        return post_process.Affinity_Propagate(cspn_config[\'step\'],\n                                               cspn_config[\'kernel\'],\n                                               norm_type=cspn_config[\'norm_type\'])\n\n    def forward(self, x):\n        [batch_size, channel, height, width] = x.size()\n        sparse_depth = x.narrow(1,3,1).clone()\n        x = self.conv1_1(x)\n        skip4 = x\n\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        skip3 = x\n\n        x = self.layer2(x)\n        skip2 = x\n\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.bn2(self.conv2(x))\n        x = self.gud_up_proj_layer1(x)\n        x = self.gud_up_proj_layer2(x, skip2)\n        x = self.gud_up_proj_layer3(x, skip3)\n        x = self.gud_up_proj_layer4(x, skip4)\n\n        guidance = self.gud_up_proj_layer6(x)\n        x= self.gud_up_proj_layer5(x)\n\n        x = self.post_process_layer(guidance, x, sparse_depth)\n        return x\n\n\ndef resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], UpProj_Block, **kwargs)\n    if pretrained:\n        print(\'==> Load pretrained model..\')\n        pretrained_dict = torch.load(model_path[\'resnet18\'])\n        model.load_state_dict(update_model.update_model(model, pretrained_dict))\n    return model\n\n\ndef resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], UpProj_Block, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet34\']))\n    return model\n\n\ndef resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], UpProj_Block, **kwargs)\n    if pretrained:\n        print(\'==> Load pretrained model from \', model_path[\'resnet50\'])\n        pretrained_dict = torch.load(model_path[\'resnet50\'])\n        model.load_state_dict(update_model.update_model(model, pretrained_dict))\n    return model\n\n\ndef resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], UpProj_Block, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet101\']))\n    return model\n\n\ndef resnet152(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], UpProj_Block, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet152\']))\n    return model\n'"
cspn_pytorch/models/update_model.py,2,"b'""""""\nCreated on Mon Feb  5 16:19:25 2018\n\n@author: norbot\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport string\n\n# update pretrained model params according to my model params\ndef update_model(my_model, pretrained_dict):\n    my_model_dict = my_model.state_dict()\n    # 1. filter out unnecessary keys\n    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in my_model_dict}\n    # 2. overwrite entries in the existing state dict\n    my_model_dict.update(pretrained_dict)\n\n    return my_model_dict\n\n# dont know why my offline saved model has \'module.\' in front of all key name\ndef remove_moudle(remove_dict):\n    for k, v in remove_dict.items():\n        if \'module\' in k :\n            print(""==> model dict with addtional module, remove it..."")\n            removed_dict = { k[7:]: v for k, v in remove_dict.items()}\n        else:\n            removed_dict = remove_dict\n        break\n    return removed_dict\n\ndef update_conv_spn_model(out_dict, in_dict):\n    in_dict = {k: v for k, v in in_dict.items() if k in out_dict}\n    return in_dict\n\n'"
