file_path,api_count,code
experiments/ner.py,11,"b'""""""\nImplementation of Bi-directional LSTM-CNNs-CRF model for NER.\n""""""\n\nimport os\nimport sys\nimport gc\nimport json\n\ncurrent_path = os.path.dirname(os.path.realpath(__file__))\nroot_path = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))\nsys.path.append(root_path)\n\nimport time\nimport argparse\n\nimport numpy as np\nimport torch\nfrom torch.optim.adamw import AdamW\nfrom torch.optim import SGD\nfrom torch.nn.utils import clip_grad_norm_\nfrom neuronlp2.io import get_logger, conll03_data, CoNLL03Writer, iterate_data\nfrom neuronlp2.models import BiRecurrentConv, BiVarRecurrentConv, BiRecurrentConvCRF, BiVarRecurrentConvCRF\nfrom neuronlp2.optim import ExponentialScheduler\nfrom neuronlp2 import utils\n\n\ndef evaluate(output_file, scorefile):\n    script = os.path.join(current_path, \'eval/conll03eval.v2\')\n    os.system(""perl %s < %s > %s"" % (script, output_file, scorefile))\n    with open(scorefile, \'r\') as fin:\n        fin.readline()\n        line = fin.readline()\n        fields = line.split("";"")\n        acc = float(fields[0].split("":"")[1].strip()[:-1])\n        precision = float(fields[1].split("":"")[1].strip()[:-1])\n        recall = float(fields[2].split("":"")[1].strip()[:-1])\n        f1 = float(fields[3].split("":"")[1].strip())\n    return acc, precision, recall, f1\n\n\ndef get_optimizer(parameters, optim, learning_rate, lr_decay, amsgrad, weight_decay, warmup_steps):\n    if optim == \'sgd\':\n        optimizer = SGD(parameters, lr=learning_rate, momentum=0.9, weight_decay=weight_decay, nesterov=True)\n    else:\n        optimizer = AdamW(parameters, lr=learning_rate, betas=(0.9, 0.999), eps=1e-8, amsgrad=amsgrad, weight_decay=weight_decay)\n    init_lr = 1e-7\n    scheduler = ExponentialScheduler(optimizer, lr_decay, warmup_steps, init_lr)\n    return optimizer, scheduler\n\n\ndef eval(data, network, writer, outfile, scorefile, device):\n    network.eval()\n    writer.start(outfile)\n    for data in iterate_data(data, 256):\n        words = data[\'WORD\'].to(device)\n        chars = data[\'CHAR\'].to(device)\n        labels = data[\'NER\'].numpy()\n        masks = data[\'MASK\'].to(device)\n        postags = data[\'POS\'].numpy()\n        chunks = data[\'CHUNK\'].numpy()\n        lengths = data[\'LENGTH\'].numpy()\n        preds = network.decode(words, chars, mask=masks, leading_symbolic=conll03_data.NUM_SYMBOLIC_TAGS)\n        writer.write(words.cpu().numpy(), postags, chunks, preds.cpu().numpy(), labels, lengths)\n    writer.close()\n    acc, precision, recall, f1 = evaluate(outfile, scorefile)\n    return acc, precision, recall, f1\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'NER with bi-directional RNN-CNN\')\n    parser.add_argument(\'--config\', type=str, help=\'config file\', required=True)\n    parser.add_argument(\'--num_epochs\', type=int, default=100, help=\'Number of training epochs\')\n    parser.add_argument(\'--batch_size\', type=int, default=16, help=\'Number of sentences in each batch\')\n    parser.add_argument(\'--loss_type\', choices=[\'sentence\', \'token\'], default=\'sentence\', help=\'loss type (default: sentence)\')\n    parser.add_argument(\'--optim\', choices=[\'sgd\', \'adam\'], help=\'type of optimizer\', required=True)\n    parser.add_argument(\'--learning_rate\', type=float, default=0.1, help=\'Learning rate\')\n    parser.add_argument(\'--lr_decay\', type=float, default=0.999995, help=\'Decay rate of learning rate\')\n    parser.add_argument(\'--amsgrad\', action=\'store_true\', help=\'AMS Grad\')\n    parser.add_argument(\'--grad_clip\', type=float, default=0, help=\'max norm for gradient clip (default 0: no clip\')\n    parser.add_argument(\'--warmup_steps\', type=int, default=0, metavar=\'N\', help=\'number of steps to warm up (default: 0)\')\n    parser.add_argument(\'--weight_decay\', type=float, default=0.0, help=\'weight for l2 norm decay\')\n    parser.add_argument(\'--unk_replace\', type=float, default=0., help=\'The rate to replace a singleton word with UNK\')\n    parser.add_argument(\'--embedding\', choices=[\'glove\', \'senna\', \'sskip\', \'polyglot\'], help=\'Embedding for words\', required=True)\n    parser.add_argument(\'--embedding_dict\', help=\'path for embedding dict\')\n    parser.add_argument(\'--train\', help=\'path for training file.\', required=True)\n    parser.add_argument(\'--dev\', help=\'path for dev file.\', required=True)\n    parser.add_argument(\'--test\', help=\'path for test file.\', required=True)\n    parser.add_argument(\'--model_path\', help=\'path for saving model file.\', required=True)\n\n    args = parser.parse_args()\n\n    logger = get_logger(""NER"")\n\n    args.cuda = torch.cuda.is_available()\n    device = torch.device(\'cuda\', 0) if args.cuda else torch.device(\'cpu\')\n    train_path = args.train\n    dev_path = args.dev\n    test_path = args.test\n\n    num_epochs = args.num_epochs\n    batch_size = args.batch_size\n    optim = args.optim\n    learning_rate = args.learning_rate\n    lr_decay = args.lr_decay\n    amsgrad = args.amsgrad\n    warmup_steps = args.warmup_steps\n    weight_decay = args.weight_decay\n    grad_clip = args.grad_clip\n\n    loss_ty_token = args.loss_type == \'token\'\n    unk_replace = args.unk_replace\n\n    model_path = args.model_path\n    model_name = os.path.join(model_path, \'model.pt\')\n    embedding = args.embedding\n    embedding_path = args.embedding_dict\n\n    print(args)\n\n    embedd_dict, embedd_dim = utils.load_embedding_dict(embedding, embedding_path)\n\n    logger.info(""Creating Alphabets"")\n    alphabet_path = os.path.join(model_path, \'alphabets\')\n    word_alphabet, char_alphabet, pos_alphabet, chunk_alphabet, ner_alphabet = conll03_data.create_alphabets(alphabet_path, train_path,\n                                                                                                             data_paths=[dev_path, test_path],\n                                                                                                             embedd_dict=embedd_dict, max_vocabulary_size=50000)\n\n    logger.info(""Word Alphabet Size: %d"" % word_alphabet.size())\n    logger.info(""Character Alphabet Size: %d"" % char_alphabet.size())\n    logger.info(""POS Alphabet Size: %d"" % pos_alphabet.size())\n    logger.info(""Chunk Alphabet Size: %d"" % chunk_alphabet.size())\n    logger.info(""NER Alphabet Size: %d"" % ner_alphabet.size())\n\n    logger.info(""Reading Data"")\n\n    data_train = conll03_data.read_bucketed_data(train_path, word_alphabet, char_alphabet, pos_alphabet, chunk_alphabet, ner_alphabet)\n    num_data = sum(data_train[1])\n    num_labels = ner_alphabet.size()\n\n    data_dev = conll03_data.read_data(dev_path, word_alphabet, char_alphabet, pos_alphabet, chunk_alphabet, ner_alphabet)\n    data_test = conll03_data.read_data(test_path, word_alphabet, char_alphabet, pos_alphabet, chunk_alphabet, ner_alphabet)\n\n    writer = CoNLL03Writer(word_alphabet, char_alphabet, pos_alphabet, chunk_alphabet, ner_alphabet)\n\n    def construct_word_embedding_table():\n        scale = np.sqrt(3.0 / embedd_dim)\n        table = np.empty([word_alphabet.size(), embedd_dim], dtype=np.float32)\n        table[conll03_data.UNK_ID, :] = np.random.uniform(-scale, scale, [1, embedd_dim]).astype(np.float32)\n        oov = 0\n        for word, index in word_alphabet.items():\n            if word in embedd_dict:\n                embedding = embedd_dict[word]\n            elif word.lower() in embedd_dict:\n                embedding = embedd_dict[word.lower()]\n            else:\n                embedding = np.random.uniform(-scale, scale, [1, embedd_dim]).astype(np.float32)\n                oov += 1\n            table[index, :] = embedding\n        print(\'oov: %d\' % oov)\n        return torch.from_numpy(table)\n\n    word_table = construct_word_embedding_table()\n\n    logger.info(""constructing network..."")\n\n    hyps = json.load(open(args.config, \'r\'))\n    json.dump(hyps, open(os.path.join(model_path, \'config.json\'), \'w\'), indent=2)\n    dropout = hyps[\'dropout\']\n    crf = hyps[\'crf\']\n    bigram = hyps[\'bigram\']\n    assert embedd_dim == hyps[\'embedd_dim\']\n    char_dim = hyps[\'char_dim\']\n    mode = hyps[\'rnn_mode\']\n    hidden_size = hyps[\'hidden_size\']\n    out_features = hyps[\'out_features\']\n    num_layers = hyps[\'num_layers\']\n    p_in = hyps[\'p_in\']\n    p_out = hyps[\'p_out\']\n    p_rnn = hyps[\'p_rnn\']\n    activation = hyps[\'activation\']\n\n    if dropout == \'std\':\n        if crf:\n            network = BiRecurrentConvCRF(embedd_dim, word_alphabet.size(), char_dim, char_alphabet.size(), mode, hidden_size, out_features, num_layers,\n                                         num_labels, embedd_word=word_table, p_in=p_in, p_out=p_out, p_rnn=p_rnn, bigram=bigram, activation=activation)\n        else:\n            network = BiRecurrentConv(embedd_dim, word_alphabet.size(), char_dim, char_alphabet.size(), mode, hidden_size, out_features, num_layers,\n                                      num_labels, embedd_word=word_table, p_in=p_in, p_out=p_out, p_rnn=p_rnn, activation=activation)\n    elif dropout == \'variational\':\n        if crf:\n            network = BiVarRecurrentConvCRF(embedd_dim, word_alphabet.size(), char_dim, char_alphabet.size(), mode, hidden_size, out_features, num_layers,\n                                            num_labels, embedd_word=word_table, p_in=p_in, p_out=p_out, p_rnn=p_rnn, bigram=bigram, activation=activation)\n        else:\n            network = BiVarRecurrentConv(embedd_dim, word_alphabet.size(), char_dim, char_alphabet.size(), mode, hidden_size, out_features, num_layers,\n                                         num_labels, embedd_word=word_table, p_in=p_in, p_out=p_out, p_rnn=p_rnn, activation=activation)\n    else:\n        raise ValueError(\'Unkown dropout type: {}\'.format(dropout))\n\n    network = network.to(device)\n\n    optimizer, scheduler = get_optimizer(network.parameters(), optim, learning_rate, lr_decay, amsgrad, weight_decay, warmup_steps)\n    model = ""{}-CNN{}"".format(mode, ""-CRF"" if crf else """")\n    logger.info(""Network: %s, num_layer=%d, hidden=%d, act=%s"" % (model, num_layers, hidden_size, activation))\n    logger.info(""training: l2: %f, (#training data: %d, batch: %d, unk replace: %.2f)"" % (weight_decay, num_data, batch_size, unk_replace))\n    logger.info(""dropout(in, out, rnn): %s(%.2f, %.2f, %s)"" % (dropout, p_in, p_out, p_rnn))\n    print(\'# of Parameters: %d\' % (sum([param.numel() for param in network.parameters()])))\n\n    best_f1 = 0.0\n    best_acc = 0.0\n    best_precision = 0.0\n    best_recall = 0.0\n    test_f1 = 0.0\n    test_acc = 0.0\n    test_precision = 0.0\n    test_recall = 0.0\n    best_epoch = 0\n    patient = 0\n    num_batches = num_data // batch_size + 1\n    result_path = os.path.join(model_path, \'tmp\')\n    if not os.path.exists(result_path):\n        os.makedirs(result_path)\n    for epoch in range(1, num_epochs + 1):\n        start_time = time.time()\n        train_loss = 0.\n        num_insts = 0\n        num_words = 0\n        num_back = 0\n        network.train()\n        lr = scheduler.get_lr()[0]\n        print(\'Epoch %d (%s, lr=%.6f, lr decay=%.6f, amsgrad=%s, l2=%.1e): \' % (epoch, optim, lr, lr_decay, amsgrad, weight_decay))\n        if args.cuda:\n            torch.cuda.empty_cache()\n        gc.collect()\n        for step, data in enumerate(iterate_data(data_train, batch_size, bucketed=True, unk_replace=unk_replace, shuffle=True)):\n            optimizer.zero_grad()\n            words = data[\'WORD\'].to(device)\n            chars = data[\'CHAR\'].to(device)\n            labels = data[\'NER\'].to(device)\n            masks = data[\'MASK\'].to(device)\n\n            nbatch = words.size(0)\n            nwords = masks.sum().item()\n\n            loss_total = network.loss(words, chars, labels, mask=masks).sum()\n            if loss_ty_token:\n                loss = loss_total.div(nwords)\n            else:\n                loss = loss_total.div(nbatch)\n            loss.backward()\n            if grad_clip > 0:\n                clip_grad_norm_(network.parameters(), grad_clip)\n            optimizer.step()\n            scheduler.step()\n\n            with torch.no_grad():\n                num_insts += nbatch\n                num_words += nwords\n                train_loss += loss_total.item()\n\n            # update log\n            if step % 100 == 0:\n                torch.cuda.empty_cache()\n                sys.stdout.write(""\\b"" * num_back)\n                sys.stdout.write("" "" * num_back)\n                sys.stdout.write(""\\b"" * num_back)\n                curr_lr = scheduler.get_lr()[0]\n                log_info = \'[%d/%d (%.0f%%) lr=%.6f] loss: %.4f (%.4f)\' % (step, num_batches, 100. * step / num_batches,\n                                                                           curr_lr, train_loss / num_insts, train_loss / num_words)\n                sys.stdout.write(log_info)\n                sys.stdout.flush()\n                num_back = len(log_info)\n\n        sys.stdout.write(""\\b"" * num_back)\n        sys.stdout.write("" "" * num_back)\n        sys.stdout.write(""\\b"" * num_back)\n        print(\'total: %d (%d), loss: %.4f (%.4f), time: %.2fs\' % (num_insts, num_words, train_loss / num_insts,\n                                                                  train_loss / num_words, time.time() - start_time))\n        print(\'-\' * 100)\n\n        # evaluate performance on dev data\n        with torch.no_grad():\n            outfile = os.path.join(result_path, \'pred_dev%d\' % epoch)\n            scorefile = os.path.join(result_path, ""score_dev%d"" % epoch)\n            acc, precision, recall, f1 = eval(data_dev, network, writer, outfile, scorefile, device)\n            print(\'Dev  acc: %.2f%%, precision: %.2f%%, recall: %.2f%%, F1: %.2f%%\' % (acc, precision, recall, f1))\n            if best_f1 < f1:\n                torch.save(network.state_dict(), model_name)\n                best_f1 = f1\n                best_acc = acc\n                best_precision = precision\n                best_recall = recall\n                best_epoch = epoch\n\n                # evaluate on test data when better performance detected\n                outfile = os.path.join(result_path, \'pred_test%d\' % epoch)\n                scorefile = os.path.join(result_path, ""score_test%d"" % epoch)\n                test_acc, test_precision, test_recall, test_f1 = eval(data_test, network, writer, outfile, scorefile, device)\n                print(\'test acc: %.2f%%, precision: %.2f%%, recall: %.2f%%, F1: %.2f%%\' % (test_acc, test_precision, test_recall, test_f1))\n                patient = 0\n            else:\n                patient += 1\n            print(\'-\' * 100)\n\n            print(""Best dev  acc: %.2f%%, precision: %.2f%%, recall: %.2f%%, F1: %.2f%% (epoch: %d (%d))"" % (best_acc, best_precision, best_recall, best_f1, best_epoch, patient))\n            print(""Best test acc: %.2f%%, precision: %.2f%%, recall: %.2f%%, F1: %.2f%% (epoch: %d (%d))"" % (test_acc, test_precision, test_recall, test_f1, best_epoch, patient))\n            print(\'=\' * 100)\n\n        if patient > 4:\n            logger.info(\'reset optimizer momentums\')\n            scheduler.reset_state()\n            patient = 0\n\n\nif __name__ == \'__main__\':\n    main()\n'"
experiments/parsing.py,17,"b'""""""\nImplementation of Graph-based dependency parsing.\n""""""\n\nimport os\nimport sys\nimport gc\nimport json\n\ncurrent_path = os.path.dirname(os.path.realpath(__file__))\nroot_path = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))\nsys.path.append(root_path)\n\nimport time\nimport argparse\nimport math\nimport numpy as np\nimport torch\nfrom torch.optim.adamw import AdamW\nfrom torch.optim import SGD\nfrom torch.nn.utils import clip_grad_norm_\nfrom neuronlp2.nn.utils import total_grad_norm\nfrom neuronlp2.io import get_logger, conllx_data, conllx_stacked_data, iterate_data\nfrom neuronlp2.models import DeepBiAffine, NeuroMST, StackPtrNet\nfrom neuronlp2.optim import ExponentialScheduler\nfrom neuronlp2 import utils\nfrom neuronlp2.io import CoNLLXWriter\nfrom neuronlp2.tasks import parser\nfrom neuronlp2.nn.utils import freeze_embedding\n\n\ndef get_optimizer(parameters, optim, learning_rate, lr_decay, betas, eps, amsgrad, weight_decay, warmup_steps):\n    if optim == \'sgd\':\n        optimizer = SGD(parameters, lr=learning_rate, momentum=0.9, weight_decay=weight_decay, nesterov=True)\n    else:\n        optimizer = AdamW(parameters, lr=learning_rate, betas=betas, eps=eps, amsgrad=amsgrad, weight_decay=weight_decay)\n    init_lr = 1e-7\n    scheduler = ExponentialScheduler(optimizer, lr_decay, warmup_steps, init_lr)\n    return optimizer, scheduler\n\n\ndef eval(alg, data, network, pred_writer, gold_writer, punct_set, word_alphabet, pos_alphabet, device, beam=1, batch_size=256):\n    network.eval()\n    accum_ucorr = 0.0\n    accum_lcorr = 0.0\n    accum_total = 0\n    accum_ucomlpete = 0.0\n    accum_lcomplete = 0.0\n    accum_ucorr_nopunc = 0.0\n    accum_lcorr_nopunc = 0.0\n    accum_total_nopunc = 0\n    accum_ucomlpete_nopunc = 0.0\n    accum_lcomplete_nopunc = 0.0\n    accum_root_corr = 0.0\n    accum_total_root = 0.0\n    accum_total_inst = 0.0\n    for data in iterate_data(data, batch_size):\n        words = data[\'WORD\'].to(device)\n        chars = data[\'CHAR\'].to(device)\n        postags = data[\'POS\'].to(device)\n        heads = data[\'HEAD\'].numpy()\n        types = data[\'TYPE\'].numpy()\n        lengths = data[\'LENGTH\'].numpy()\n        if alg == \'graph\':\n            masks = data[\'MASK\'].to(device)\n            heads_pred, types_pred = network.decode(words, chars, postags, mask=masks, leading_symbolic=conllx_data.NUM_SYMBOLIC_TAGS)\n        else:\n            masks = data[\'MASK_ENC\'].to(device)\n            heads_pred, types_pred = network.decode(words, chars, postags, mask=masks, beam=beam, leading_symbolic=conllx_data.NUM_SYMBOLIC_TAGS)\n\n        words = words.cpu().numpy()\n        postags = postags.cpu().numpy()\n        pred_writer.write(words, postags, heads_pred, types_pred, lengths, symbolic_root=True)\n        gold_writer.write(words, postags, heads, types, lengths, symbolic_root=True)\n\n        stats, stats_nopunc, stats_root, num_inst = parser.eval(words, postags, heads_pred, types_pred, heads, types,\n                                                                word_alphabet, pos_alphabet, lengths, punct_set=punct_set, symbolic_root=True)\n        ucorr, lcorr, total, ucm, lcm = stats\n        ucorr_nopunc, lcorr_nopunc, total_nopunc, ucm_nopunc, lcm_nopunc = stats_nopunc\n        corr_root, total_root = stats_root\n\n        accum_ucorr += ucorr\n        accum_lcorr += lcorr\n        accum_total += total\n        accum_ucomlpete += ucm\n        accum_lcomplete += lcm\n\n        accum_ucorr_nopunc += ucorr_nopunc\n        accum_lcorr_nopunc += lcorr_nopunc\n        accum_total_nopunc += total_nopunc\n        accum_ucomlpete_nopunc += ucm_nopunc\n        accum_lcomplete_nopunc += lcm_nopunc\n\n        accum_root_corr += corr_root\n        accum_total_root += total_root\n\n        accum_total_inst += num_inst\n\n    print(\'W. Punct: ucorr: %d, lcorr: %d, total: %d, uas: %.2f%%, las: %.2f%%, ucm: %.2f%%, lcm: %.2f%%\' % (\n        accum_ucorr, accum_lcorr, accum_total, accum_ucorr * 100 / accum_total, accum_lcorr * 100 / accum_total,\n        accum_ucomlpete * 100 / accum_total_inst, accum_lcomplete * 100 / accum_total_inst))\n    print(\'Wo Punct: ucorr: %d, lcorr: %d, total: %d, uas: %.2f%%, las: %.2f%%, ucm: %.2f%%, lcm: %.2f%%\' % (\n        accum_ucorr_nopunc, accum_lcorr_nopunc, accum_total_nopunc, accum_ucorr_nopunc * 100 / accum_total_nopunc,\n        accum_lcorr_nopunc * 100 / accum_total_nopunc,\n        accum_ucomlpete_nopunc * 100 / accum_total_inst, accum_lcomplete_nopunc * 100 / accum_total_inst))\n    print(\'Root: corr: %d, total: %d, acc: %.2f%%\' %(accum_root_corr, accum_total_root, accum_root_corr * 100 / accum_total_root))\n    return (accum_ucorr, accum_lcorr, accum_ucomlpete, accum_lcomplete, accum_total), \\\n           (accum_ucorr_nopunc, accum_lcorr_nopunc, accum_ucomlpete_nopunc, accum_lcomplete_nopunc, accum_total_nopunc), \\\n           (accum_root_corr, accum_total_root, accum_total_inst)\n\n\ndef train(args):\n    logger = get_logger(""Parsing"")\n\n    args.cuda = torch.cuda.is_available()\n    device = torch.device(\'cuda\', 0) if args.cuda else torch.device(\'cpu\')\n    train_path = args.train\n    dev_path = args.dev\n    test_path = args.test\n\n    num_epochs = args.num_epochs\n    batch_size = args.batch_size\n    optim = args.optim\n    learning_rate = args.learning_rate\n    lr_decay = args.lr_decay\n    amsgrad = args.amsgrad\n    eps = args.eps\n    betas = (args.beta1, args.beta2)\n    warmup_steps = args.warmup_steps\n    weight_decay = args.weight_decay\n    grad_clip = args.grad_clip\n\n    loss_ty_token = args.loss_type == \'token\'\n    unk_replace = args.unk_replace\n    freeze = args.freeze\n\n    model_path = args.model_path\n    model_name = os.path.join(model_path, \'model.pt\')\n    punctuation = args.punctuation\n\n    word_embedding = args.word_embedding\n    word_path = args.word_path\n    char_embedding = args.char_embedding\n    char_path = args.char_path\n\n    print(args)\n\n    word_dict, word_dim = utils.load_embedding_dict(word_embedding, word_path)\n    char_dict = None\n    if char_embedding != \'random\':\n        char_dict, char_dim = utils.load_embedding_dict(char_embedding, char_path)\n    else:\n        char_dict = None\n        char_dim = None\n\n    logger.info(""Creating Alphabets"")\n    alphabet_path = os.path.join(model_path, \'alphabets\')\n    word_alphabet, char_alphabet, pos_alphabet, type_alphabet = conllx_data.create_alphabets(alphabet_path, train_path,\n                                                                                             data_paths=[dev_path, test_path],\n                                                                                             embedd_dict=word_dict, max_vocabulary_size=200000)\n\n    num_words = word_alphabet.size()\n    num_chars = char_alphabet.size()\n    num_pos = pos_alphabet.size()\n    num_types = type_alphabet.size()\n\n    logger.info(""Word Alphabet Size: %d"" % num_words)\n    logger.info(""Character Alphabet Size: %d"" % num_chars)\n    logger.info(""POS Alphabet Size: %d"" % num_pos)\n    logger.info(""Type Alphabet Size: %d"" % num_types)\n\n    result_path = os.path.join(model_path, \'tmp\')\n    if not os.path.exists(result_path):\n        os.makedirs(result_path)\n\n    punct_set = None\n    if punctuation is not None:\n        punct_set = set(punctuation)\n        logger.info(""punctuations(%d): %s"" % (len(punct_set), \' \'.join(punct_set)))\n\n    def construct_word_embedding_table():\n        scale = np.sqrt(3.0 / word_dim)\n        table = np.empty([word_alphabet.size(), word_dim], dtype=np.float32)\n        table[conllx_data.UNK_ID, :] = np.zeros([1, word_dim]).astype(np.float32) if freeze else np.random.uniform(-scale, scale, [1, word_dim]).astype(np.float32)\n        oov = 0\n        for word, index in word_alphabet.items():\n            if word in word_dict:\n                embedding = word_dict[word]\n            elif word.lower() in word_dict:\n                embedding = word_dict[word.lower()]\n            else:\n                embedding = np.zeros([1, word_dim]).astype(np.float32) if freeze else np.random.uniform(-scale, scale, [1, word_dim]).astype(np.float32)\n                oov += 1\n            table[index, :] = embedding\n        print(\'word OOV: %d\' % oov)\n        return torch.from_numpy(table)\n\n    def construct_char_embedding_table():\n        if char_dict is None:\n            return None\n\n        scale = np.sqrt(3.0 / char_dim)\n        table = np.empty([num_chars, char_dim], dtype=np.float32)\n        table[conllx_data.UNK_ID, :] = np.random.uniform(-scale, scale, [1, char_dim]).astype(np.float32)\n        oov = 0\n        for char, index, in char_alphabet.items():\n            if char in char_dict:\n                embedding = char_dict[char]\n            else:\n                embedding = np.random.uniform(-scale, scale, [1, char_dim]).astype(np.float32)\n                oov += 1\n            table[index, :] = embedding\n        print(\'character OOV: %d\' % oov)\n        return torch.from_numpy(table)\n\n    word_table = construct_word_embedding_table()\n    char_table = construct_char_embedding_table()\n\n    logger.info(""constructing network..."")\n\n    hyps = json.load(open(args.config, \'r\'))\n    json.dump(hyps, open(os.path.join(model_path, \'config.json\'), \'w\'), indent=2)\n    model_type = hyps[\'model\']\n    assert model_type in [\'DeepBiAffine\', \'NeuroMST\', \'StackPtr\']\n    assert word_dim == hyps[\'word_dim\']\n    if char_dim is not None:\n        assert char_dim == hyps[\'char_dim\']\n    else:\n        char_dim = hyps[\'char_dim\']\n    use_pos = hyps[\'pos\']\n    pos_dim = hyps[\'pos_dim\']\n    mode = hyps[\'rnn_mode\']\n    hidden_size = hyps[\'hidden_size\']\n    arc_space = hyps[\'arc_space\']\n    type_space = hyps[\'type_space\']\n    p_in = hyps[\'p_in\']\n    p_out = hyps[\'p_out\']\n    p_rnn = hyps[\'p_rnn\']\n    activation = hyps[\'activation\']\n    prior_order = None\n\n    alg = \'transition\' if model_type == \'StackPtr\' else \'graph\'\n    if model_type == \'DeepBiAffine\':\n        num_layers = hyps[\'num_layers\']\n        network = DeepBiAffine(word_dim, num_words, char_dim, num_chars, pos_dim, num_pos,\n                               mode, hidden_size, num_layers, num_types, arc_space, type_space,\n                               embedd_word=word_table, embedd_char=char_table,\n                               p_in=p_in, p_out=p_out, p_rnn=p_rnn, pos=use_pos, activation=activation)\n    elif model_type == \'NeuroMST\':\n        num_layers = hyps[\'num_layers\']\n        network = NeuroMST(word_dim, num_words, char_dim, num_chars, pos_dim, num_pos,\n                           mode, hidden_size, num_layers, num_types, arc_space, type_space,\n                           embedd_word=word_table, embedd_char=char_table,\n                           p_in=p_in, p_out=p_out, p_rnn=p_rnn, pos=use_pos, activation=activation)\n    elif model_type == \'StackPtr\':\n        encoder_layers = hyps[\'encoder_layers\']\n        decoder_layers = hyps[\'decoder_layers\']\n        num_layers = (encoder_layers, decoder_layers)\n        prior_order = hyps[\'prior_order\']\n        grandPar = hyps[\'grandPar\']\n        sibling = hyps[\'sibling\']\n        network = StackPtrNet(word_dim, num_words, char_dim, num_chars, pos_dim, num_pos, mode, hidden_size,\n                              encoder_layers, decoder_layers, num_types, arc_space, type_space,\n                              embedd_word=word_table, embedd_char=char_table, prior_order=prior_order, activation=activation,\n                              p_in=p_in, p_out=p_out, p_rnn=p_rnn, pos=use_pos, grandPar=grandPar, sibling=sibling)\n    else:\n        raise RuntimeError(\'Unknown model type: %s\' % model_type)\n\n    if freeze:\n        freeze_embedding(network.word_embed)\n\n    network = network.to(device)\n    model = ""{}-{}"".format(model_type, mode)\n    logger.info(""Network: %s, num_layer=%s, hidden=%d, act=%s"" % (model, num_layers, hidden_size, activation))\n    logger.info(""dropout(in, out, rnn): %s(%.2f, %.2f, %s)"" % (\'variational\', p_in, p_out, p_rnn))\n    logger.info(\'# of Parameters: %d\' % (sum([param.numel() for param in network.parameters()])))\n\n    logger.info(""Reading Data"")\n    if alg == \'graph\':\n        data_train = conllx_data.read_bucketed_data(train_path, word_alphabet, char_alphabet, pos_alphabet, type_alphabet, symbolic_root=True)\n        data_dev = conllx_data.read_data(dev_path, word_alphabet, char_alphabet, pos_alphabet, type_alphabet, symbolic_root=True)\n        data_test = conllx_data.read_data(test_path, word_alphabet, char_alphabet, pos_alphabet, type_alphabet, symbolic_root=True)\n    else:\n        data_train = conllx_stacked_data.read_bucketed_data(train_path, word_alphabet, char_alphabet, pos_alphabet, type_alphabet, prior_order=prior_order)\n        data_dev = conllx_stacked_data.read_data(dev_path, word_alphabet, char_alphabet, pos_alphabet, type_alphabet, prior_order=prior_order)\n        data_test = conllx_stacked_data.read_data(test_path, word_alphabet, char_alphabet, pos_alphabet, type_alphabet, prior_order=prior_order)\n    num_data = sum(data_train[1])\n    logger.info(""training: #training data: %d, batch: %d, unk replace: %.2f"" % (num_data, batch_size, unk_replace))\n\n    pred_writer = CoNLLXWriter(word_alphabet, char_alphabet, pos_alphabet, type_alphabet)\n    gold_writer = CoNLLXWriter(word_alphabet, char_alphabet, pos_alphabet, type_alphabet)\n    optimizer, scheduler = get_optimizer(network.parameters(), optim, learning_rate, lr_decay, betas, eps, amsgrad, weight_decay, warmup_steps)\n\n    best_ucorrect = 0.0\n    best_lcorrect = 0.0\n    best_ucomlpete = 0.0\n    best_lcomplete = 0.0\n\n    best_ucorrect_nopunc = 0.0\n    best_lcorrect_nopunc = 0.0\n    best_ucomlpete_nopunc = 0.0\n    best_lcomplete_nopunc = 0.0\n    best_root_correct = 0.0\n    best_total = 0\n    best_total_nopunc = 0\n    best_total_inst = 0\n    best_total_root = 0\n\n    best_epoch = 0\n\n    test_ucorrect = 0.0\n    test_lcorrect = 0.0\n    test_ucomlpete = 0.0\n    test_lcomplete = 0.0\n\n    test_ucorrect_nopunc = 0.0\n    test_lcorrect_nopunc = 0.0\n    test_ucomlpete_nopunc = 0.0\n    test_lcomplete_nopunc = 0.0\n    test_root_correct = 0.0\n    test_total = 0\n    test_total_nopunc = 0\n    test_total_inst = 0\n    test_total_root = 0\n\n    patient = 0\n    beam = args.beam\n    reset = args.reset\n    num_batches = num_data // batch_size + 1\n    if optim == \'adam\':\n        opt_info = \'adam, betas=(%.1f, %.3f), eps=%.1e, amsgrad=%s\' % (betas[0], betas[1], eps, amsgrad)\n    else:\n        opt_info = \'sgd, momentum=0.9, nesterov=True\'\n    for epoch in range(1, num_epochs + 1):\n        start_time = time.time()\n        train_loss = 0.\n        train_arc_loss = 0.\n        train_type_loss = 0.\n        num_insts = 0\n        num_words = 0\n        num_back = 0\n        num_nans = 0\n        network.train()\n        lr = scheduler.get_lr()[0]\n        print(\'Epoch %d (%s, lr=%.6f, lr decay=%.6f, grad clip=%.1f, l2=%.1e): \' % (epoch, opt_info, lr, lr_decay, grad_clip, weight_decay))\n        if args.cuda:\n            torch.cuda.empty_cache()\n        gc.collect()\n        for step, data in enumerate(iterate_data(data_train, batch_size, bucketed=True, unk_replace=unk_replace, shuffle=True)):\n            optimizer.zero_grad()\n            words = data[\'WORD\'].to(device)\n            chars = data[\'CHAR\'].to(device)\n            postags = data[\'POS\'].to(device)\n            heads = data[\'HEAD\'].to(device)\n            nbatch = words.size(0)\n            if alg == \'graph\':\n                types = data[\'TYPE\'].to(device)\n                masks = data[\'MASK\'].to(device)\n                nwords = masks.sum() - nbatch\n                loss_arc, loss_type = network.loss(words, chars, postags, heads, types, mask=masks)\n            else:\n                masks_enc = data[\'MASK_ENC\'].to(device)\n                masks_dec = data[\'MASK_DEC\'].to(device)\n                stacked_heads = data[\'STACK_HEAD\'].to(device)\n                children = data[\'CHILD\'].to(device)\n                siblings = data[\'SIBLING\'].to(device)\n                stacked_types = data[\'STACK_TYPE\'].to(device)\n                nwords = masks_enc.sum() - nbatch\n                loss_arc, loss_type = network.loss(words, chars, postags, heads, stacked_heads, children, siblings, stacked_types,\n                                                   mask_e=masks_enc, mask_d=masks_dec)\n            loss_arc = loss_arc.sum()\n            loss_type = loss_type.sum()\n            loss_total = loss_arc + loss_type\n            if loss_ty_token:\n                loss = loss_total.div(nwords)\n            else:\n                loss = loss_total.div(nbatch)\n            loss.backward()\n            if grad_clip > 0:\n                grad_norm = clip_grad_norm_(network.parameters(), grad_clip)\n            else:\n                grad_norm = total_grad_norm(network.parameters())\n\n            if math.isnan(grad_norm):\n                num_nans += 1\n            else:\n                optimizer.step()\n                scheduler.step()\n\n                with torch.no_grad():\n                    num_insts += nbatch\n                    num_words += nwords\n                    train_loss += loss_total.item()\n                    train_arc_loss += loss_arc.item()\n                    train_type_loss += loss_type.item()\n\n            # update log\n            if step % 100 == 0:\n                torch.cuda.empty_cache()\n                sys.stdout.write(""\\b"" * num_back)\n                sys.stdout.write("" "" * num_back)\n                sys.stdout.write(""\\b"" * num_back)\n                curr_lr = scheduler.get_lr()[0]\n                num_insts = max(num_insts, 1)\n                num_words = max(num_words, 1)\n                log_info = \'[%d/%d (%.0f%%) lr=%.6f (%d)] loss: %.4f (%.4f), arc: %.4f (%.4f), type: %.4f (%.4f)\' % (step, num_batches, 100. * step / num_batches, curr_lr, num_nans,\n                                                                                                                     train_loss / num_insts, train_loss / num_words,\n                                                                                                                     train_arc_loss / num_insts, train_arc_loss / num_words,\n                                                                                                                     train_type_loss / num_insts, train_type_loss / num_words)\n                sys.stdout.write(log_info)\n                sys.stdout.flush()\n                num_back = len(log_info)\n\n        sys.stdout.write(""\\b"" * num_back)\n        sys.stdout.write("" "" * num_back)\n        sys.stdout.write(""\\b"" * num_back)\n        print(\'total: %d (%d), loss: %.4f (%.4f), arc: %.4f (%.4f), type: %.4f (%.4f), time: %.2fs\' % (num_insts, num_words, train_loss / num_insts, train_loss / num_words,\n                                                                                                       train_arc_loss / num_insts, train_arc_loss / num_words,\n                                                                                                       train_type_loss / num_insts, train_type_loss / num_words,\n                                                                                                       time.time() - start_time))\n        print(\'-\' * 125)\n\n        # evaluate performance on dev data\n        with torch.no_grad():\n            pred_filename = os.path.join(result_path, \'pred_dev%d\' % epoch)\n            pred_writer.start(pred_filename)\n            gold_filename = os.path.join(result_path, \'gold_dev%d\' % epoch)\n            gold_writer.start(gold_filename)\n\n            print(\'Evaluating dev:\')\n            dev_stats, dev_stats_nopunct, dev_stats_root = eval(alg, data_dev, network, pred_writer, gold_writer, punct_set, word_alphabet, pos_alphabet, device, beam=beam)\n\n            pred_writer.close()\n            gold_writer.close()\n\n            dev_ucorr, dev_lcorr, dev_ucomlpete, dev_lcomplete, dev_total = dev_stats\n            dev_ucorr_nopunc, dev_lcorr_nopunc, dev_ucomlpete_nopunc, dev_lcomplete_nopunc, dev_total_nopunc = dev_stats_nopunct\n            dev_root_corr, dev_total_root, dev_total_inst = dev_stats_root\n\n            if best_ucorrect_nopunc + best_lcorrect_nopunc < dev_ucorr_nopunc + dev_lcorr_nopunc:\n                best_ucorrect_nopunc = dev_ucorr_nopunc\n                best_lcorrect_nopunc = dev_lcorr_nopunc\n                best_ucomlpete_nopunc = dev_ucomlpete_nopunc\n                best_lcomplete_nopunc = dev_lcomplete_nopunc\n\n                best_ucorrect = dev_ucorr\n                best_lcorrect = dev_lcorr\n                best_ucomlpete = dev_ucomlpete\n                best_lcomplete = dev_lcomplete\n\n                best_root_correct = dev_root_corr\n                best_total = dev_total\n                best_total_nopunc = dev_total_nopunc\n                best_total_root = dev_total_root\n                best_total_inst = dev_total_inst\n\n                best_epoch = epoch\n                patient = 0\n                torch.save(network.state_dict(), model_name)\n\n                pred_filename = os.path.join(result_path, \'pred_test%d\' % epoch)\n                pred_writer.start(pred_filename)\n                gold_filename = os.path.join(result_path, \'gold_test%d\' % epoch)\n                gold_writer.start(gold_filename)\n\n                print(\'Evaluating test:\')\n                test_stats, test_stats_nopunct, test_stats_root = eval(alg, data_test, network, pred_writer, gold_writer, punct_set, word_alphabet, pos_alphabet, device, beam=beam)\n\n                test_ucorrect, test_lcorrect, test_ucomlpete, test_lcomplete, test_total = test_stats\n                test_ucorrect_nopunc, test_lcorrect_nopunc, test_ucomlpete_nopunc, test_lcomplete_nopunc, test_total_nopunc = test_stats_nopunct\n                test_root_correct, test_total_root, test_total_inst = test_stats_root\n\n                pred_writer.close()\n                gold_writer.close()\n            else:\n                patient += 1\n\n            print(\'-\' * 125)\n            print(\'best dev  W. Punct: ucorr: %d, lcorr: %d, total: %d, uas: %.2f%%, las: %.2f%%, ucm: %.2f%%, lcm: %.2f%% (epoch: %d)\' % (\n                best_ucorrect, best_lcorrect, best_total, best_ucorrect * 100 / best_total, best_lcorrect * 100 / best_total,\n                best_ucomlpete * 100 / dev_total_inst, best_lcomplete * 100 / dev_total_inst,\n                best_epoch))\n            print(\'best dev  Wo Punct: ucorr: %d, lcorr: %d, total: %d, uas: %.2f%%, las: %.2f%%, ucm: %.2f%%, lcm: %.2f%% (epoch: %d)\' % (\n                best_ucorrect_nopunc, best_lcorrect_nopunc, best_total_nopunc,\n                best_ucorrect_nopunc * 100 / best_total_nopunc, best_lcorrect_nopunc * 100 / best_total_nopunc,\n                best_ucomlpete_nopunc * 100 / best_total_inst, best_lcomplete_nopunc * 100 / best_total_inst,\n                best_epoch))\n            print(\'best dev  Root: corr: %d, total: %d, acc: %.2f%% (epoch: %d)\' % (\n                best_root_correct, best_total_root, best_root_correct * 100 / best_total_root, best_epoch))\n            print(\'-\' * 125)\n            print(\'best test W. Punct: ucorr: %d, lcorr: %d, total: %d, uas: %.2f%%, las: %.2f%%, ucm: %.2f%%, lcm: %.2f%% (epoch: %d)\' % (\n                test_ucorrect, test_lcorrect, test_total, test_ucorrect * 100 / test_total, test_lcorrect * 100 / test_total,\n                test_ucomlpete * 100 / test_total_inst, test_lcomplete * 100 / test_total_inst,\n                best_epoch))\n            print(\'best test Wo Punct: ucorr: %d, lcorr: %d, total: %d, uas: %.2f%%, las: %.2f%%, ucm: %.2f%%, lcm: %.2f%% (epoch: %d)\' % (\n                test_ucorrect_nopunc, test_lcorrect_nopunc, test_total_nopunc,\n                test_ucorrect_nopunc * 100 / test_total_nopunc, test_lcorrect_nopunc * 100 / test_total_nopunc,\n                test_ucomlpete_nopunc * 100 / test_total_inst, test_lcomplete_nopunc * 100 / test_total_inst,\n                best_epoch))\n            print(\'best test Root: corr: %d, total: %d, acc: %.2f%% (epoch: %d)\' % (\n                test_root_correct, test_total_root, test_root_correct * 100 / test_total_root, best_epoch))\n            print(\'=\' * 125)\n\n            if patient >= reset:\n                logger.info(\'reset optimizer momentums\')\n                network.load_state_dict(torch.load(model_name, map_location=device))\n                scheduler.reset_state()\n                patient = 0\n\n\ndef parse(args):\n    logger = get_logger(""Parsing"")\n    args.cuda = torch.cuda.is_available()\n    device = torch.device(\'cuda\', 0) if args.cuda else torch.device(\'cpu\')\n    test_path = args.test\n\n    model_path = args.model_path\n    model_name = os.path.join(model_path, \'model.pt\')\n    punctuation = args.punctuation\n    print(args)\n\n    logger.info(""Creating Alphabets"")\n    alphabet_path = os.path.join(model_path, \'alphabets\')\n    assert os.path.exists(alphabet_path)\n    word_alphabet, char_alphabet, pos_alphabet, type_alphabet = conllx_data.create_alphabets(alphabet_path, None)\n\n    num_words = word_alphabet.size()\n    num_chars = char_alphabet.size()\n    num_pos = pos_alphabet.size()\n    num_types = type_alphabet.size()\n\n    logger.info(""Word Alphabet Size: %d"" % num_words)\n    logger.info(""Character Alphabet Size: %d"" % num_chars)\n    logger.info(""POS Alphabet Size: %d"" % num_pos)\n    logger.info(""Type Alphabet Size: %d"" % num_types)\n\n    result_path = os.path.join(model_path, \'tmp\')\n    if not os.path.exists(result_path):\n        os.makedirs(result_path)\n\n    punct_set = None\n    if punctuation is not None:\n        punct_set = set(punctuation)\n        logger.info(""punctuations(%d): %s"" % (len(punct_set), \' \'.join(punct_set)))\n\n    logger.info(""loading network..."")\n    hyps = json.load(open(os.path.join(model_path, \'config.json\'), \'r\'))\n    model_type = hyps[\'model\']\n    assert model_type in [\'DeepBiAffine\', \'NeuroMST\', \'StackPtr\']\n    word_dim = hyps[\'word_dim\']\n    char_dim = hyps[\'char_dim\']\n    use_pos = hyps[\'pos\']\n    pos_dim = hyps[\'pos_dim\']\n    mode = hyps[\'rnn_mode\']\n    hidden_size = hyps[\'hidden_size\']\n    arc_space = hyps[\'arc_space\']\n    type_space = hyps[\'type_space\']\n    p_in = hyps[\'p_in\']\n    p_out = hyps[\'p_out\']\n    p_rnn = hyps[\'p_rnn\']\n    activation = hyps[\'activation\']\n    prior_order = None\n\n    alg = \'transition\' if model_type == \'StackPtr\' else \'graph\'\n    if model_type == \'DeepBiAffine\':\n        num_layers = hyps[\'num_layers\']\n        network = DeepBiAffine(word_dim, num_words, char_dim, num_chars, pos_dim, num_pos,\n                               mode, hidden_size, num_layers, num_types, arc_space, type_space,\n                               p_in=p_in, p_out=p_out, p_rnn=p_rnn, pos=use_pos, activation=activation)\n    elif model_type == \'NeuroMST\':\n        num_layers = hyps[\'num_layers\']\n        network = NeuroMST(word_dim, num_words, char_dim, num_chars, pos_dim, num_pos,\n                           mode, hidden_size, num_layers, num_types, arc_space, type_space,\n                           p_in=p_in, p_out=p_out, p_rnn=p_rnn, pos=use_pos, activation=activation)\n    elif model_type == \'StackPtr\':\n        encoder_layers = hyps[\'encoder_layers\']\n        decoder_layers = hyps[\'decoder_layers\']\n        num_layers = (encoder_layers, decoder_layers)\n        prior_order = hyps[\'prior_order\']\n        grandPar = hyps[\'grandPar\']\n        sibling = hyps[\'sibling\']\n        network = StackPtrNet(word_dim, num_words, char_dim, num_chars, pos_dim, num_pos,\n                              mode, hidden_size, encoder_layers, decoder_layers, num_types, arc_space, type_space,\n                              prior_order=prior_order, activation=activation, p_in=p_in, p_out=p_out, p_rnn=p_rnn,\n                              pos=use_pos, grandPar=grandPar, sibling=sibling)\n    else:\n        raise RuntimeError(\'Unknown model type: %s\' % model_type)\n\n    network = network.to(device)\n    network.load_state_dict(torch.load(model_name, map_location=device))\n    model = ""{}-{}"".format(model_type, mode)\n    logger.info(""Network: %s, num_layer=%s, hidden=%d, act=%s"" % (model, num_layers, hidden_size, activation))\n\n    logger.info(""Reading Data"")\n    if alg == \'graph\':\n        data_test = conllx_data.read_data(test_path, word_alphabet, char_alphabet, pos_alphabet, type_alphabet, symbolic_root=True)\n    else:\n        data_test = conllx_stacked_data.read_data(test_path, word_alphabet, char_alphabet, pos_alphabet, type_alphabet, prior_order=prior_order)\n\n    beam = args.beam\n    pred_writer = CoNLLXWriter(word_alphabet, char_alphabet, pos_alphabet, type_alphabet)\n    gold_writer = CoNLLXWriter(word_alphabet, char_alphabet, pos_alphabet, type_alphabet)\n    pred_filename = os.path.join(result_path, \'pred.txt\')\n    pred_writer.start(pred_filename)\n    gold_filename = os.path.join(result_path, \'gold.txt\')\n    gold_writer.start(gold_filename)\n\n    with torch.no_grad():\n        print(\'Parsing...\')\n        start_time = time.time()\n        eval(alg, data_test, network, pred_writer, gold_writer, punct_set, word_alphabet, pos_alphabet, device, beam, batch_size=args.batch_size)\n        print(\'Time: %.2fs\' % (time.time() - start_time))\n\n    pred_writer.close()\n    gold_writer.close()\n\n\nif __name__ == \'__main__\':\n    args_parser = argparse.ArgumentParser(description=\'Tuning with graph-based parsing\')\n    args_parser.add_argument(\'--mode\', choices=[\'train\', \'parse\'], required=True, help=\'processing mode\')\n    args_parser.add_argument(\'--config\', type=str, help=\'config file\')\n    args_parser.add_argument(\'--num_epochs\', type=int, default=200, help=\'Number of training epochs\')\n    args_parser.add_argument(\'--batch_size\', type=int, default=16, help=\'Number of sentences in each batch\')\n    args_parser.add_argument(\'--loss_type\', choices=[\'sentence\', \'token\'], default=\'sentence\', help=\'loss type (default: sentence)\')\n    args_parser.add_argument(\'--optim\', choices=[\'sgd\', \'adam\'], help=\'type of optimizer\')\n    args_parser.add_argument(\'--learning_rate\', type=float, default=0.1, help=\'Learning rate\')\n    args_parser.add_argument(\'--beta1\', type=float, default=0.9, help=\'beta1 of Adam\')\n    args_parser.add_argument(\'--beta2\', type=float, default=0.999, help=\'beta2 of Adam\')\n    args_parser.add_argument(\'--eps\', type=float, default=1e-8, help=\'epsilon for adam or adamax\')\n    args_parser.add_argument(\'--lr_decay\', type=float, default=0.999995, help=\'Decay rate of learning rate\')\n    args_parser.add_argument(\'--amsgrad\', action=\'store_true\', help=\'AMS Grad\')\n    args_parser.add_argument(\'--grad_clip\', type=float, default=0, help=\'max norm for gradient clip (default 0: no clip\')\n    args_parser.add_argument(\'--warmup_steps\', type=int, default=0, metavar=\'N\', help=\'number of steps to warm up (default: 0)\')\n    args_parser.add_argument(\'--reset\', type=int, default=10, help=\'Number of epochs to reset optimizer (default 10)\')\n    args_parser.add_argument(\'--weight_decay\', type=float, default=0.0, help=\'weight for l2 norm decay\')\n    args_parser.add_argument(\'--unk_replace\', type=float, default=0., help=\'The rate to replace a singleton word with UNK\')\n    args_parser.add_argument(\'--freeze\', action=\'store_true\', help=\'frozen the word embedding (disable fine-tuning).\')\n    args_parser.add_argument(\'--punctuation\', nargs=\'+\', type=str, help=\'List of punctuations\')\n    args_parser.add_argument(\'--beam\', type=int, default=1, help=\'Beam size for decoding\')\n    args_parser.add_argument(\'--word_embedding\', choices=[\'glove\', \'senna\', \'sskip\', \'polyglot\'], help=\'Embedding for words\')\n    args_parser.add_argument(\'--word_path\', help=\'path for word embedding dict\')\n    args_parser.add_argument(\'--char_embedding\', choices=[\'random\', \'polyglot\'], help=\'Embedding for characters\')\n    args_parser.add_argument(\'--char_path\', help=\'path for character embedding dict\')\n    args_parser.add_argument(\'--train\', help=\'path for training file.\')\n    args_parser.add_argument(\'--dev\', help=\'path for dev file.\')\n    args_parser.add_argument(\'--test\', help=\'path for test file.\', required=True)\n    args_parser.add_argument(\'--model_path\', help=\'path for saving model file.\', required=True)\n\n    args = args_parser.parse_args()\n    if args.mode == \'train\':\n        train(args)\n    else:\n        parse(args)\n'"
experiments/pos_tagging.py,12,"b'""""""\nImplementation of Bi-directional LSTM-CNNs-CRF model for POS Tagging.\n""""""\n\nimport os\nimport sys\nimport gc\nimport json\n\ncurrent_path = os.path.dirname(os.path.realpath(__file__))\nroot_path = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))\nsys.path.append(root_path)\n\nimport time\nimport argparse\n\nimport numpy as np\nimport torch\nfrom torch.optim.adamw import AdamW\nfrom torch.optim import SGD\nfrom torch.nn.utils import clip_grad_norm_\nfrom neuronlp2.io import get_logger, conllx_data, iterate_data, POSWriter\nfrom neuronlp2.models import BiRecurrentConv, BiVarRecurrentConv, BiRecurrentConvCRF, BiVarRecurrentConvCRF\nfrom neuronlp2.optim import ExponentialScheduler\nfrom neuronlp2 import utils\n\n\ndef get_optimizer(parameters, optim, learning_rate, lr_decay, amsgrad, weight_decay, warmup_steps):\n    if optim == \'sgd\':\n        optimizer = SGD(parameters, lr=learning_rate, momentum=0.9, weight_decay=weight_decay, nesterov=True)\n    else:\n        optimizer = AdamW(parameters, lr=learning_rate, betas=(0.9, 0.999), eps=1e-8, amsgrad=amsgrad, weight_decay=weight_decay)\n    init_lr = 1e-7\n    scheduler = ExponentialScheduler(optimizer, lr_decay, warmup_steps, init_lr)\n    return optimizer, scheduler\n\n\ndef eval(data, network, writer, outfile, device):\n    network.eval()\n    corr = 0\n    total = 0\n    writer.start(outfile)\n    for data in iterate_data(data, 256):\n        words = data[\'WORD\'].to(device)\n        chars = data[\'CHAR\'].to(device)\n        masks = data[\'MASK\'].to(device)\n        postags = data[\'POS\'].to(device)\n        lengths = data[\'LENGTH\'].numpy()\n        preds = network.decode(words, chars, mask=masks, leading_symbolic=conllx_data.NUM_SYMBOLIC_TAGS)\n        corr += torch.eq(preds, postags).float().mul(masks).sum().item()\n        total += masks.sum().item()\n        writer.write(words.cpu().numpy(), preds.cpu().numpy(), postags.cpu().numpy(), lengths)\n    writer.close()\n    return corr, total\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'NER with bi-directional RNN-CNN\')\n    parser.add_argument(\'--config\', type=str, help=\'config file\', required=True)\n    parser.add_argument(\'--num_epochs\', type=int, default=100, help=\'Number of training epochs\')\n    parser.add_argument(\'--batch_size\', type=int, default=16, help=\'Number of sentences in each batch\')\n    parser.add_argument(\'--loss_type\', choices=[\'sentence\', \'token\'], default=\'sentence\', help=\'loss type (default: sentence)\')\n    parser.add_argument(\'--optim\', choices=[\'sgd\', \'adam\'], help=\'type of optimizer\', required=True)\n    parser.add_argument(\'--learning_rate\', type=float, default=0.1, help=\'Learning rate\')\n    parser.add_argument(\'--lr_decay\', type=float, default=0.999995, help=\'Decay rate of learning rate\')\n    parser.add_argument(\'--amsgrad\', action=\'store_true\', help=\'AMS Grad\')\n    parser.add_argument(\'--grad_clip\', type=float, default=0, help=\'max norm for gradient clip (default 0: no clip\')\n    parser.add_argument(\'--warmup_steps\', type=int, default=0, metavar=\'N\', help=\'number of steps to warm up (default: 0)\')\n    parser.add_argument(\'--weight_decay\', type=float, default=0.0, help=\'weight for l2 norm decay\')\n    parser.add_argument(\'--unk_replace\', type=float, default=0., help=\'The rate to replace a singleton word with UNK\')\n    parser.add_argument(\'--embedding\', choices=[\'glove\', \'senna\', \'sskip\', \'polyglot\'], help=\'Embedding for words\', required=True)\n    parser.add_argument(\'--embedding_dict\', help=\'path for embedding dict\')\n    parser.add_argument(\'--train\', help=\'path for training file.\', required=True)\n    parser.add_argument(\'--dev\', help=\'path for dev file.\', required=True)\n    parser.add_argument(\'--test\', help=\'path for test file.\', required=True)\n    parser.add_argument(\'--model_path\', help=\'path for saving model file.\', required=True)\n\n    args = parser.parse_args()\n\n    logger = get_logger(""POS"")\n\n    args.cuda = torch.cuda.is_available()\n    device = torch.device(\'cuda\', 0) if args.cuda else torch.device(\'cpu\')\n    train_path = args.train\n    dev_path = args.dev\n    test_path = args.test\n\n    num_epochs = args.num_epochs\n    batch_size = args.batch_size\n    optim = args.optim\n    learning_rate = args.learning_rate\n    lr_decay = args.lr_decay\n    amsgrad = args.amsgrad\n    warmup_steps = args.warmup_steps\n    weight_decay = args.weight_decay\n    grad_clip = args.grad_clip\n\n    loss_ty_token = args.loss_type == \'token\'\n    unk_replace = args.unk_replace\n\n    model_path = args.model_path\n    model_name = os.path.join(model_path, \'model.pt\')\n    embedding = args.embedding\n    embedding_path = args.embedding_dict\n\n    print(args)\n\n    embedd_dict, embedd_dim = utils.load_embedding_dict(embedding, embedding_path)\n\n    logger.info(""Creating Alphabets"")\n    alphabet_path = os.path.join(model_path, \'alphabets\')\n    word_alphabet, char_alphabet, pos_alphabet, type_alphabet = conllx_data.create_alphabets(alphabet_path, train_path,\n                                                                                             data_paths=[dev_path, test_path],\n                                                                                             embedd_dict=embedd_dict, max_vocabulary_size=50000)\n\n    logger.info(""Word Alphabet Size: %d"" % word_alphabet.size())\n    logger.info(""Character Alphabet Size: %d"" % char_alphabet.size())\n    logger.info(""POS Alphabet Size: %d"" % pos_alphabet.size())\n\n    logger.info(""Reading Data"")\n\n    data_train = conllx_data.read_bucketed_data(train_path, word_alphabet, char_alphabet, pos_alphabet, type_alphabet)\n    num_data = sum(data_train[1])\n    num_labels = pos_alphabet.size()\n\n    data_dev = conllx_data.read_data(dev_path, word_alphabet, char_alphabet, pos_alphabet, type_alphabet)\n    data_test = conllx_data.read_data(test_path, word_alphabet, char_alphabet, pos_alphabet, type_alphabet)\n\n    writer = POSWriter(word_alphabet, char_alphabet, pos_alphabet)\n\n    def construct_word_embedding_table():\n        scale = np.sqrt(3.0 / embedd_dim)\n        table = np.empty([word_alphabet.size(), embedd_dim], dtype=np.float32)\n        table[conllx_data.UNK_ID, :] = np.random.uniform(-scale, scale, [1, embedd_dim]).astype(np.float32)\n        oov = 0\n        for word, index in word_alphabet.items():\n            if word in embedd_dict:\n                embedding = embedd_dict[word]\n            elif word.lower() in embedd_dict:\n                embedding = embedd_dict[word.lower()]\n            else:\n                embedding = np.random.uniform(-scale, scale, [1, embedd_dim]).astype(np.float32)\n                oov += 1\n            table[index, :] = embedding\n        print(\'oov: %d\' % oov)\n        return torch.from_numpy(table)\n\n    word_table = construct_word_embedding_table()\n\n    logger.info(""constructing network..."")\n\n    hyps = json.load(open(args.config, \'r\'))\n    json.dump(hyps, open(os.path.join(model_path, \'config.json\'), \'w\'), indent=2)\n    dropout = hyps[\'dropout\']\n    crf = hyps[\'crf\']\n    bigram = hyps[\'bigram\']\n    assert embedd_dim == hyps[\'embedd_dim\']\n    char_dim = hyps[\'char_dim\']\n    mode = hyps[\'rnn_mode\']\n    hidden_size = hyps[\'hidden_size\']\n    out_features = hyps[\'out_features\']\n    num_layers = hyps[\'num_layers\']\n    p_in = hyps[\'p_in\']\n    p_out = hyps[\'p_out\']\n    p_rnn = hyps[\'p_rnn\']\n    activation = hyps[\'activation\']\n\n    if dropout == \'std\':\n        if crf:\n            network = BiRecurrentConvCRF(embedd_dim, word_alphabet.size(), char_dim, char_alphabet.size(), mode, hidden_size, out_features, num_layers,\n                                         num_labels, embedd_word=word_table, p_in=p_in, p_out=p_out, p_rnn=p_rnn, bigram=bigram, activation=activation)\n        else:\n            network = BiRecurrentConv(embedd_dim, word_alphabet.size(), char_dim, char_alphabet.size(), mode, hidden_size, out_features, num_layers,\n                                      num_labels, embedd_word=word_table, p_in=p_in, p_out=p_out, p_rnn=p_rnn, activation=activation)\n    elif dropout == \'variational\':\n        if crf:\n            network = BiVarRecurrentConvCRF(embedd_dim, word_alphabet.size(), char_dim, char_alphabet.size(), mode, hidden_size, out_features, num_layers,\n                                            num_labels, embedd_word=word_table, p_in=p_in, p_out=p_out, p_rnn=p_rnn, bigram=bigram, activation=activation)\n        else:\n            network = BiVarRecurrentConv(embedd_dim, word_alphabet.size(), char_dim, char_alphabet.size(), mode, hidden_size, out_features, num_layers,\n                                         num_labels, embedd_word=word_table, p_in=p_in, p_out=p_out, p_rnn=p_rnn, activation=activation)\n    else:\n        raise ValueError(\'Unkown dropout type: {}\'.format(dropout))\n\n    network = network.to(device)\n\n    optimizer, scheduler = get_optimizer(network.parameters(), optim, learning_rate, lr_decay, amsgrad, weight_decay, warmup_steps)\n    model = ""{}-CNN{}"".format(mode, ""-CRF"" if crf else """")\n    logger.info(""Network: %s, num_layer=%d, hidden=%d, act=%s"" % (model, num_layers, hidden_size, activation))\n    logger.info(""training: l2: %f, (#training data: %d, batch: %d, unk replace: %.2f)"" % (weight_decay, num_data, batch_size, unk_replace))\n    logger.info(""dropout(in, out, rnn): %s(%.2f, %.2f, %s)"" % (dropout, p_in, p_out, p_rnn))\n    print(\'# of Parameters: %d\' % (sum([param.numel() for param in network.parameters()])))\n\n    best_corr = 0.0\n    best_total = 0.0\n    test_corr = 0.0\n    test_total = 0.0\n    best_epoch = 0\n    patient = 0\n    num_batches = num_data // batch_size + 1\n    result_path = os.path.join(model_path, \'tmp\')\n    if not os.path.exists(result_path):\n        os.makedirs(result_path)\n    for epoch in range(1, num_epochs + 1):\n        start_time = time.time()\n        train_loss = 0.\n        num_insts = 0\n        num_words = 0\n        num_back = 0\n        network.train()\n        lr = scheduler.get_lr()[0]\n        print(\'Epoch %d (%s, lr=%.6f, lr decay=%.6f, amsgrad=%s, l2=%.1e): \' % (epoch, optim, lr, lr_decay, amsgrad, weight_decay))\n        if args.cuda:\n            torch.cuda.empty_cache()\n        gc.collect()\n        for step, data in enumerate(iterate_data(data_train, batch_size, bucketed=True, unk_replace=unk_replace, shuffle=True)):\n            optimizer.zero_grad()\n            words = data[\'WORD\'].to(device)\n            chars = data[\'CHAR\'].to(device)\n            labels = data[\'POS\'].to(device)\n            masks = data[\'MASK\'].to(device)\n\n            nbatch = words.size(0)\n            nwords = masks.sum().item()\n\n            loss_total = network.loss(words, chars, labels, mask=masks).sum()\n            if loss_ty_token:\n                loss = loss_total.div(nwords)\n            else:\n                loss = loss_total.div(nbatch)\n            loss.backward()\n            if grad_clip > 0:\n                clip_grad_norm_(network.parameters(), grad_clip)\n            optimizer.step()\n            scheduler.step()\n\n            with torch.no_grad():\n                num_insts += nbatch\n                num_words += nwords\n                train_loss += loss_total.item()\n\n            # update log\n            if step % 100 == 0:\n                torch.cuda.empty_cache()\n                sys.stdout.write(""\\b"" * num_back)\n                sys.stdout.write("" "" * num_back)\n                sys.stdout.write(""\\b"" * num_back)\n                curr_lr = scheduler.get_lr()[0]\n                log_info = \'[%d/%d (%.0f%%) lr=%.6f] loss: %.4f (%.4f)\' % (step, num_batches, 100. * step / num_batches,\n                                                                           curr_lr, train_loss / num_insts, train_loss / num_words)\n                sys.stdout.write(log_info)\n                sys.stdout.flush()\n                num_back = len(log_info)\n\n        sys.stdout.write(""\\b"" * num_back)\n        sys.stdout.write("" "" * num_back)\n        sys.stdout.write(""\\b"" * num_back)\n        print(\'total: %d (%d), loss: %.4f (%.4f), time: %.2fs\' % (num_insts, num_words, train_loss / num_insts,\n                                                                  train_loss / num_words, time.time() - start_time))\n        print(\'-\' * 100)\n\n        # evaluate performance on dev data\n        with torch.no_grad():\n            outfile = os.path.join(result_path, \'pred_dev%d\' % epoch)\n            dev_corr, dev_total = eval(data_dev, network, writer, outfile, device)\n            print(\'Dev  corr: %d, total: %d, acc: %.2f%%\' % (dev_corr, dev_total, dev_corr * 100 / dev_total))\n            if best_corr < dev_corr:\n                torch.save(network.state_dict(), model_name)\n                best_corr = dev_corr\n                best_total = dev_total\n                best_epoch = epoch\n\n                # evaluate on test data when better performance detected\n                outfile = os.path.join(result_path, \'pred_test%d\' % epoch)\n                test_corr, test_total = eval(data_test, network, writer, outfile, device)\n                print(\'test corr: %d, total: %d, acc: %.2f%%\' % (test_corr, test_total, test_corr * 100 / test_total))\n                patient = 0\n            else:\n                patient += 1\n            print(\'-\' * 100)\n\n            print(""Best dev  corr: %d, total: %d, acc: %.2f%% (epoch: %d (%d))"" % (best_corr, best_total, best_corr * 100 / best_total, best_epoch, patient))\n            print(""Best test corr: %d, total: %d, acc: %.2f%% (epoch: %d (%d))"" % (test_corr, test_total, test_corr * 100 / test_total, best_epoch, patient))\n            print(\'=\' * 100)\n\n        if patient > 4:\n            logger.info(\'reset optimizer momentums\')\n            scheduler.reset_state()\n            patient = 0\n\n\nif __name__ == \'__main__\':\n    main()\n'"
neuronlp2/__init__.py,0,"b'__author__ = \'max\'\n\n__version__ = ""0.2.dev1""\n'"
neuronlp2/utils.py,0,"b'__author__ = \'max\'\n\nfrom collections import OrderedDict\nimport pickle\nimport numpy as np\nfrom gensim.models.word2vec import Word2Vec\nimport gzip\n\nfrom neuronlp2.io.common import DIGIT_RE\n\n\ndef load_embedding_dict(embedding, embedding_path, normalize_digits=True):\n    """"""\n    load word embeddings from file\n    :param embedding:\n    :param embedding_path:\n    :return: embedding dict, embedding dimention, caseless\n    """"""\n    print(""loading embedding: %s from %s"" % (embedding, embedding_path))\n    if embedding == \'word2vec\':\n        # loading word2vec\n        word2vec = Word2Vec.load_word2vec_format(embedding_path, binary=True)\n        embedd_dim = word2vec.vector_size\n        return word2vec, embedd_dim\n    elif embedding == \'glove\':\n        # loading GloVe\n        embedd_dim = -1\n        embedd_dict = OrderedDict()\n        with gzip.open(embedding_path, \'rt\') as file:\n            for line in file:\n                line = line.strip()\n                if len(line) == 0:\n                    continue\n\n                tokens = line.split()\n                if embedd_dim < 0:\n                    embedd_dim = len(tokens) - 1\n                else:\n                    assert (embedd_dim + 1 == len(tokens))\n                embedd = np.empty([1, embedd_dim], dtype=np.float32)\n                embedd[:] = tokens[1:]\n                word = DIGIT_RE.sub(""0"", tokens[0]) if normalize_digits else tokens[0]\n                embedd_dict[word] = embedd\n        return embedd_dict, embedd_dim\n    elif embedding == \'senna\':\n        # loading Senna\n        embedd_dim = -1\n        embedd_dict = OrderedDict()\n        with gzip.open(embedding_path, \'rt\') as file:\n            for line in file:\n                line = line.strip()\n                if len(line) == 0:\n                    continue\n\n                tokens = line.split()\n                if embedd_dim < 0:\n                    embedd_dim = len(tokens) - 1\n                else:\n                    assert (embedd_dim + 1 == len(tokens))\n                embedd = np.empty([1, embedd_dim], dtype=np.float32)\n                embedd[:] = tokens[1:]\n                word = DIGIT_RE.sub(""0"", tokens[0]) if normalize_digits else tokens[0]\n                embedd_dict[word] = embedd\n        return embedd_dict, embedd_dim\n    elif embedding == \'sskip\':\n        embedd_dim = -1\n        embedd_dict = OrderedDict()\n        with gzip.open(embedding_path, \'rt\') as file:\n            # skip the first line\n            file.readline()\n            for line in file:\n                line = line.strip()\n                try:\n                    if len(line) == 0:\n                        continue\n\n                    tokens = line.split()\n                    if len(tokens) < embedd_dim:\n                        continue\n\n                    if embedd_dim < 0:\n                        embedd_dim = len(tokens) - 1\n\n                    embedd = np.empty([1, embedd_dim], dtype=np.float32)\n                    start = len(tokens) - embedd_dim\n                    word = \' \'.join(tokens[0:start])\n                    embedd[:] = tokens[start:]\n                    word = DIGIT_RE.sub(""0"", word) if normalize_digits else word\n                    embedd_dict[word] = embedd\n                except UnicodeDecodeError:\n                    continue\n        return embedd_dict, embedd_dim\n    elif embedding == \'polyglot\':\n        words, embeddings = pickle.load(open(embedding_path, \'rb\'), encoding=\'latin1\')\n        _, embedd_dim = embeddings.shape\n        embedd_dict = OrderedDict()\n        for i, word in enumerate(words):\n            embedd = np.empty([1, embedd_dim], dtype=np.float32)\n            embedd[:] = embeddings[i, :]\n            word = DIGIT_RE.sub(""0"", word) if normalize_digits else word\n            embedd_dict[word] = embedd\n        return embedd_dict, embedd_dim\n\n    else:\n        raise ValueError(""embedding should choose from [word2vec, senna, glove, sskip, polyglot]"")\n'"
neuronlp2/io/__init__.py,0,"b""__author__ = 'max'\n\nfrom neuronlp2.io.alphabet import Alphabet\nfrom neuronlp2.io.instance import *\nfrom neuronlp2.io.logger import get_logger\nfrom neuronlp2.io.writer import CoNLL03Writer, CoNLLXWriter, POSWriter\nfrom neuronlp2.io.utils import get_batch, get_bucketed_batch, iterate_data\nfrom neuronlp2.io import conllx_data, conll03_data, conllx_stacked_data\n"""
neuronlp2/io/alphabet.py,0,"b'__author__ = \'max\'\n\n""""""\nAlphabet maps objects to integer ids. It provides two way mapping from the index to the objects.\n""""""\nimport json\nimport os\nfrom neuronlp2.io.logger import get_logger\n\nclass Alphabet(object):\n    def __init__(self, name, defualt_value=False, keep_growing=True, singleton=False):\n        self.__name = name\n\n        self.instance2index = {}\n        self.instances = []\n        self.default_value = defualt_value\n        self.offset = 1 if self.default_value else 0\n        self.keep_growing = keep_growing\n        self.singletons = set() if singleton else None\n\n        # Index 0 is occupied by default, all else following.\n        self.default_index = 0 if self.default_value else None\n\n        self.next_index = self.offset\n\n        self.logger = get_logger(\'Alphabet\')\n\n    def add(self, instance):\n        if instance not in self.instance2index:\n            self.instances.append(instance)\n            self.instance2index[instance] = self.next_index\n            self.next_index += 1\n\n    def add_singleton(self, id):\n        if self.singletons is None:\n            raise RuntimeError(\'Alphabet %s does not have singleton.\' % self.__name)\n        else:\n            self.singletons.add(id)\n\n    def add_singletons(self, ids):\n        if self.singletons is None:\n            raise RuntimeError(\'Alphabet %s does not have singleton.\' % self.__name)\n        else:\n            self.singletons.update(ids)\n\n    def is_singleton(self, id):\n        if self.singletons is None:\n            raise RuntimeError(\'Alphabet %s does not have singleton.\' % self.__name)\n        else:\n            return id in self.singletons\n\n    def get_index(self, instance):\n        try:\n            return self.instance2index[instance]\n        except KeyError:\n            if self.keep_growing:\n                index = self.next_index\n                self.add(instance)\n                return index\n            else:\n                if self.default_value:\n                    return self.default_index\n                else:\n                    raise KeyError(""instance not found: %s"" % instance)\n\n    def get_instance(self, index):\n        if self.default_value and index == self.default_index:\n            # First index is occupied by the wildcard element.\n            return \'<_UNK>\'\n        else:\n            try:\n                return self.instances[index - self.offset]\n            except IndexError:\n                raise IndexError(\'unknown index: %d\' % index)\n\n    def size(self):\n        return len(self.instances) + self.offset\n\n    def singleton_size(self):\n        return len(self.singletons)\n\n    def items(self):\n        return self.instance2index.items()\n\n    def enumerate_items(self, start):\n        if start < self.offset or start >= self.size():\n            raise IndexError(""Enumerate is allowed between [%d : size of the alphabet)"" % self.offset)\n        return zip(range(start, len(self.instances) + self.offset), self.instances[start - self.offset:])\n\n    def close(self):\n        self.keep_growing = False\n\n    def open(self):\n        self.keep_growing = True\n\n    def get_content(self):\n        if self.singletons is None:\n            return {\'instance2index\': self.instance2index, \'instances\': self.instances}\n        else:\n            return {\'instance2index\': self.instance2index, \'instances\': self.instances,\n                    \'singletions\': list(self.singletons)}\n\n    def __from_json(self, data):\n        self.instances = data[""instances""]\n        self.instance2index = data[""instance2index""]\n        if \'singletions\' in data:\n            self.singletons = set(data[\'singletions\'])\n        else:\n            self.singletons = None\n\n    def save(self, output_directory, name=None):\n        """"""\n        Save both alhpabet records to the given directory.\n        :param output_directory: Directory to save model and weights.\n        :param name: The alphabet saving name, optional.\n        :return:\n        """"""\n        saving_name = name if name else self.__name\n        try:\n            if not os.path.exists(output_directory):\n                os.makedirs(output_directory)\n\n            json.dump(self.get_content(),\n                      open(os.path.join(output_directory, saving_name + "".json""), \'w\'), indent=4)\n        except Exception as e:\n            self.logger.warn(""Alphabet is not saved: %s"" % repr(e))\n\n    def load(self, input_directory, name=None):\n        """"""\n        Load model architecture and weights from the give directory. This allow we use old models even the structure\n        changes.\n        :param input_directory: Directory to save model and weights\n        :return:\n        """"""\n        loading_name = name if name else self.__name\n        self.__from_json(json.load(open(os.path.join(input_directory, loading_name + "".json""))))\n        self.next_index = len(self.instances) + self.offset\n        self.keep_growing = False\n'"
neuronlp2/io/common.py,0,"b'__author__ = \'max\'\n\nimport re\n\nMAX_CHAR_LENGTH = 45\n\n# Regular expressions used to normalize digits.\nDIGIT_RE = re.compile(r""\\d"")\n\nPAD = ""_PAD""\nPAD_POS = ""_PAD_POS""\nPAD_TYPE = ""_<PAD>""\nPAD_CHAR = ""_PAD_CHAR""\nPAD_CHUNK = ""_PAD_CHUNK""\nPAD_NER = ""_PAD_NER""\n\nROOT = ""_ROOT""\nROOT_POS = ""_ROOT_POS""\nROOT_TYPE = ""_<ROOT>""\nROOT_CHAR = ""_ROOT_CHAR""\n\nEND = ""_END""\nEND_POS = ""_END_POS""\nEND_TYPE = ""_<END>""\nEND_CHAR = ""_END_CHAR""\n\nUNK_ID = 0\nPAD_ID_WORD = 1\nPAD_ID_CHAR = 1\nPAD_ID_TAG = 0\n'"
neuronlp2/io/conll03_data.py,16,"b'__author__ = \'max\'\n\nimport os.path\nimport numpy as np\nfrom collections import defaultdict, OrderedDict\nimport torch\n\nfrom neuronlp2.io.reader import CoNLL03Reader\nfrom neuronlp2.io.alphabet import Alphabet\nfrom neuronlp2.io.logger import get_logger\nfrom neuronlp2.io.common import DIGIT_RE, MAX_CHAR_LENGTH, UNK_ID\nfrom neuronlp2.io.common import PAD_CHAR, PAD, PAD_CHUNK, PAD_POS, PAD_NER, PAD_ID_CHAR, PAD_ID_TAG, PAD_ID_WORD\n\n# Special vocabulary symbols - we always put them at the start.\n_START_VOCAB = [PAD,]\nNUM_SYMBOLIC_TAGS = 1\n\n_buckets = [5, 10, 15, 20, 25, 30, 40, 50, 140]\n\n\ndef create_alphabets(alphabet_directory, train_path, data_paths=None, max_vocabulary_size=100000, embedd_dict=None,\n                     min_occurrence=1, normalize_digits=True):\n\n    def expand_vocab():\n        vocab_set = set(vocab_list)\n        for data_path in data_paths:\n            # logger.info(""Processing data: %s"" % data_path)\n            with open(data_path, \'r\') as file:\n                for line in file:\n                    line = line.strip()\n                    if len(line) == 0:\n                        continue\n\n                    tokens = line.split(\' \')\n                    word = DIGIT_RE.sub(""0"", tokens[1]) if normalize_digits else tokens[1]\n                    pos = tokens[2]\n                    chunk = tokens[3]\n                    ner = tokens[4]\n\n                    pos_alphabet.add(pos)\n                    chunk_alphabet.add(chunk)\n                    ner_alphabet.add(ner)\n\n                    if word not in vocab_set and (word in embedd_dict or word.lower() in embedd_dict):\n                        vocab_set.add(word)\n                        vocab_list.append(word)\n\n    logger = get_logger(""Create Alphabets"")\n    word_alphabet = Alphabet(\'word\', defualt_value=True, singleton=True)\n    char_alphabet = Alphabet(\'character\', defualt_value=True)\n    pos_alphabet = Alphabet(\'pos\')\n    chunk_alphabet = Alphabet(\'chunk\')\n    ner_alphabet = Alphabet(\'ner\')\n\n    if not os.path.isdir(alphabet_directory):\n        logger.info(""Creating Alphabets: %s"" % alphabet_directory)\n\n        char_alphabet.add(PAD_CHAR)\n        pos_alphabet.add(PAD_POS)\n        chunk_alphabet.add(PAD_CHUNK)\n        ner_alphabet.add(PAD_NER)\n\n        vocab = defaultdict(int)\n        with open(train_path, \'r\') as file:\n            for line in file:\n                line = line.strip()\n                if len(line) == 0:\n                    continue\n\n                tokens = line.split(\' \')\n                for char in tokens[1]:\n                    char_alphabet.add(char)\n\n                word = DIGIT_RE.sub(""0"", tokens[1]) if normalize_digits else tokens[1]\n                vocab[word] += 1\n\n                pos = tokens[2]\n                pos_alphabet.add(pos)\n\n                chunk = tokens[3]\n                chunk_alphabet.add(chunk)\n\n                ner = tokens[4]\n                ner_alphabet.add(ner)\n\n        # collect singletons\n        singletons = set([word for word, count in vocab.items() if count <= min_occurrence])\n\n        # if a singleton is in pretrained embedding dict, set the count to min_occur + c\n        if embedd_dict is not None:\n            assert isinstance(embedd_dict, OrderedDict)\n            for word in vocab.keys():\n                if word in embedd_dict or word.lower() in embedd_dict:\n                    vocab[word] += min_occurrence\n\n        vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n        logger.info(""Total Vocabulary Size: %d"" % len(vocab_list))\n        logger.info(""Total Singleton Size:  %d"" % len(singletons))\n        vocab_list = [word for word in vocab_list if word in _START_VOCAB or vocab[word] > min_occurrence]\n        logger.info(""Total Vocabulary Size (w.o rare words): %d"" % len(vocab_list))\n\n        if len(vocab_list) > max_vocabulary_size:\n            vocab_list = vocab_list[:max_vocabulary_size]\n\n        if data_paths is not None and embedd_dict is not None:\n            expand_vocab()\n\n        for word in vocab_list:\n            word_alphabet.add(word)\n            if word in singletons:\n                word_alphabet.add_singleton(word_alphabet.get_index(word))\n\n        word_alphabet.save(alphabet_directory)\n        char_alphabet.save(alphabet_directory)\n        pos_alphabet.save(alphabet_directory)\n        chunk_alphabet.save(alphabet_directory)\n        ner_alphabet.save(alphabet_directory)\n    else:\n        word_alphabet.load(alphabet_directory)\n        char_alphabet.load(alphabet_directory)\n        pos_alphabet.load(alphabet_directory)\n        chunk_alphabet.load(alphabet_directory)\n        ner_alphabet.load(alphabet_directory)\n\n    word_alphabet.close()\n    char_alphabet.close()\n    pos_alphabet.close()\n    chunk_alphabet.close()\n    ner_alphabet.close()\n    logger.info(""Word Alphabet Size (Singleton): %d (%d)"" % (word_alphabet.size(), word_alphabet.singleton_size()))\n    logger.info(""Character Alphabet Size: %d"" % char_alphabet.size())\n    logger.info(""POS Alphabet Size: %d"" % pos_alphabet.size())\n    logger.info(""Chunk Alphabet Size: %d"" % chunk_alphabet.size())\n    logger.info(""NER Alphabet Size: %d"" % ner_alphabet.size())\n    return word_alphabet, char_alphabet, pos_alphabet, chunk_alphabet, ner_alphabet\n\n\ndef read_data(source_path: str, word_alphabet: Alphabet, char_alphabet: Alphabet,\n              pos_alphabet: Alphabet, chunk_alphabet: Alphabet, ner_alphabet: Alphabet,\n              max_size=None, normalize_digits=True):\n    data = []\n    max_length = 0\n    max_char_length = 0\n    print(\'Reading data from %s\' % source_path)\n    counter = 0\n    reader = CoNLL03Reader(source_path, word_alphabet, char_alphabet, pos_alphabet, chunk_alphabet, ner_alphabet)\n    inst = reader.getNext(normalize_digits)\n    while inst is not None and (not max_size or counter < max_size):\n        counter += 1\n        if counter % 10000 == 0:\n            print(""reading data: %d"" % counter)\n\n        sent = inst.sentence\n        data.append([sent.word_ids, sent.char_id_seqs, inst.pos_ids, inst.chunk_ids, inst.ner_ids])\n        max_len = max([len(char_seq) for char_seq in sent.char_seqs])\n        if max_char_length < max_len:\n            max_char_length = max_len\n        if max_length < inst.length():\n            max_length = inst.length()\n        inst = reader.getNext(normalize_digits)\n    reader.close()\n    print(""Total number of data: %d"" % counter)\n\n    data_size = len(data)\n    char_length = min(MAX_CHAR_LENGTH, max_char_length)\n    wid_inputs = np.empty([data_size, max_length], dtype=np.int64)\n    cid_inputs = np.empty([data_size, max_length, char_length], dtype=np.int64)\n    pid_inputs = np.empty([data_size, max_length], dtype=np.int64)\n    chid_inputs = np.empty([data_size, max_length], dtype=np.int64)\n    nid_inputs = np.empty([data_size, max_length], dtype=np.int64)\n\n    masks = np.zeros([data_size, max_length], dtype=np.float32)\n    single = np.zeros([data_size, max_length], dtype=np.int64)\n    lengths = np.empty(data_size, dtype=np.int64)\n\n    for i, inst in enumerate(data):\n        wids, cid_seqs, pids, chids, nids = inst\n        inst_size = len(wids)\n        lengths[i] = inst_size\n        # word ids\n        wid_inputs[i, :inst_size] = wids\n        wid_inputs[i, inst_size:] = PAD_ID_WORD\n        for c, cids in enumerate(cid_seqs):\n            cid_inputs[i, c, :len(cids)] = cids\n            cid_inputs[i, c, len(cids):] = PAD_ID_CHAR\n        cid_inputs[i, inst_size:, :] = PAD_ID_CHAR\n        # pos ids\n        pid_inputs[i, :inst_size] = pids\n        pid_inputs[i, inst_size:] = PAD_ID_TAG\n        # chunk ids\n        chid_inputs[i, :inst_size] = chids\n        chid_inputs[i, inst_size:] = PAD_ID_TAG\n        # ner ids\n        nid_inputs[i, :inst_size] = nids\n        nid_inputs[i, inst_size:] = PAD_ID_TAG\n        # masks\n        masks[i, :inst_size] = 1.0\n        for j, wid in enumerate(wids):\n            if word_alphabet.is_singleton(wid):\n                single[i, j] = 1\n\n    words = torch.from_numpy(wid_inputs)\n    chars = torch.from_numpy(cid_inputs)\n    pos = torch.from_numpy(pid_inputs)\n    chunks = torch.from_numpy(chid_inputs)\n    ners = torch.from_numpy(nid_inputs)\n    masks = torch.from_numpy(masks)\n    single = torch.from_numpy(single)\n    lengths = torch.from_numpy(lengths)\n\n    data_tensor = {\'WORD\': words, \'CHAR\': chars, \'POS\': pos, \'CHUNK\': chunks,\n                   \'NER\': ners, \'MASK\': masks, \'SINGLE\': single, \'LENGTH\': lengths}\n    return data_tensor, data_size\n\n\ndef read_bucketed_data(source_path: str, word_alphabet: Alphabet, char_alphabet: Alphabet,\n                       pos_alphabet: Alphabet, chunk_alphabet: Alphabet, ner_alphabet: Alphabet,\n                       max_size=None, normalize_digits=True):\n    data = [[] for _ in _buckets]\n    max_char_length = [0 for _ in _buckets]\n    print(\'Reading data from %s\' % source_path)\n    counter = 0\n    reader = CoNLL03Reader(source_path, word_alphabet, char_alphabet, pos_alphabet, chunk_alphabet, ner_alphabet)\n    inst = reader.getNext(normalize_digits)\n    while inst is not None and (not max_size or counter < max_size):\n        counter += 1\n        if counter % 10000 == 0:\n            print(""reading data: %d"" % counter)\n\n        inst_size = inst.length()\n        sent = inst.sentence\n        for bucket_id, bucket_size in enumerate(_buckets):\n            if inst_size < bucket_size:\n                data[bucket_id].append([sent.word_ids, sent.char_id_seqs, inst.pos_ids, inst.chunk_ids, inst.ner_ids])\n                max_len = max([len(char_seq) for char_seq in sent.char_seqs])\n                if max_char_length[bucket_id] < max_len:\n                    max_char_length[bucket_id] = max_len\n                break\n\n        inst = reader.getNext(normalize_digits)\n    reader.close()\n    print(""Total number of data: %d"" % counter)\n\n    bucket_sizes = [len(data[b]) for b in range(len(_buckets))]\n    data_tensors = []\n    for bucket_id in range(len(_buckets)):\n        bucket_size = bucket_sizes[bucket_id]\n        if bucket_size == 0:\n            data_tensors.append((1, 1))\n            continue\n\n        bucket_length = _buckets[bucket_id]\n        char_length = min(MAX_CHAR_LENGTH, max_char_length[bucket_id])\n        wid_inputs = np.empty([bucket_size, bucket_length], dtype=np.int64)\n        cid_inputs = np.empty([bucket_size, bucket_length, char_length], dtype=np.int64)\n        pid_inputs = np.empty([bucket_size, bucket_length], dtype=np.int64)\n        chid_inputs = np.empty([bucket_size, bucket_length], dtype=np.int64)\n        nid_inputs = np.empty([bucket_size, bucket_length], dtype=np.int64)\n\n        masks = np.zeros([bucket_size, bucket_length], dtype=np.float32)\n        single = np.zeros([bucket_size, bucket_length], dtype=np.int64)\n        lengths = np.empty(bucket_size, dtype=np.int64)\n\n        for i, inst in enumerate(data[bucket_id]):\n            wids, cid_seqs, pids, chids, nids = inst\n            inst_size = len(wids)\n            lengths[i] = inst_size\n            # word ids\n            wid_inputs[i, :inst_size] = wids\n            wid_inputs[i, inst_size:] = PAD_ID_WORD\n            for c, cids in enumerate(cid_seqs):\n                cid_inputs[i, c, :len(cids)] = cids\n                cid_inputs[i, c, len(cids):] = PAD_ID_CHAR\n            cid_inputs[i, inst_size:, :] = PAD_ID_CHAR\n            # pos ids\n            pid_inputs[i, :inst_size] = pids\n            pid_inputs[i, inst_size:] = PAD_ID_TAG\n            # chunk ids\n            chid_inputs[i, :inst_size] = chids\n            chid_inputs[i, inst_size:] = PAD_ID_TAG\n            # ner ids\n            nid_inputs[i, :inst_size] = nids\n            nid_inputs[i, inst_size:] = PAD_ID_TAG\n            # masks\n            masks[i, :inst_size] = 1.0\n            for j, wid in enumerate(wids):\n                if word_alphabet.is_singleton(wid):\n                    single[i, j] = 1\n\n        words = torch.from_numpy(wid_inputs)\n        chars = torch.from_numpy(cid_inputs)\n        pos = torch.from_numpy(pid_inputs)\n        chunks = torch.from_numpy(chid_inputs)\n        ners = torch.from_numpy(nid_inputs)\n        masks = torch.from_numpy(masks)\n        single = torch.from_numpy(single)\n        lengths = torch.from_numpy(lengths)\n\n        data_tensor = {\'WORD\': words, \'CHAR\': chars, \'POS\': pos, \'CHUNK\': chunks,\n                       \'NER\': ners, \'MASK\': masks, \'SINGLE\': single, \'LENGTH\': lengths}\n        data_tensors.append(data_tensor)\n    return data_tensors, bucket_sizes\n'"
neuronlp2/io/conllx_data.py,16,"b'__author__ = \'max\'\n\nimport os.path\nimport numpy as np\nfrom collections import defaultdict, OrderedDict\nimport torch\n\nfrom neuronlp2.io.reader import CoNLLXReader\nfrom neuronlp2.io.alphabet import Alphabet\nfrom neuronlp2.io.logger import get_logger\nfrom neuronlp2.io.common import DIGIT_RE, MAX_CHAR_LENGTH, UNK_ID\nfrom neuronlp2.io.common import PAD_CHAR, PAD, PAD_POS, PAD_TYPE, PAD_ID_CHAR, PAD_ID_TAG, PAD_ID_WORD\nfrom neuronlp2.io.common import ROOT, END, ROOT_CHAR, ROOT_POS, ROOT_TYPE, END_CHAR, END_POS, END_TYPE\n\n# Special vocabulary symbols - we always put them at the start.\n_START_VOCAB = [PAD, ROOT, END]\nNUM_SYMBOLIC_TAGS = 3\n\n_buckets = [10, 15, 20, 25, 30, 35, 40, 50, 60, 70, 140]\n\n\ndef create_alphabets(alphabet_directory, train_path, data_paths=None, max_vocabulary_size=100000, embedd_dict=None,\n                     min_occurrence=1, normalize_digits=True):\n\n    def expand_vocab():\n        vocab_set = set(vocab_list)\n        for data_path in data_paths:\n            # logger.info(""Processing data: %s"" % data_path)\n            with open(data_path, \'r\') as file:\n                for line in file:\n                    line = line.strip()\n                    if len(line) == 0:\n                        continue\n\n                    tokens = line.split(\'\\t\')\n                    for char in tokens[1]:\n                        char_alphabet.add(char)\n\n                    word = DIGIT_RE.sub(""0"", tokens[1]) if normalize_digits else tokens[1]\n                    pos = tokens[4]\n                    type = tokens[7]\n\n                    pos_alphabet.add(pos)\n                    type_alphabet.add(type)\n\n                    if word not in vocab_set and (word in embedd_dict or word.lower() in embedd_dict):\n                        vocab_set.add(word)\n                        vocab_list.append(word)\n\n    logger = get_logger(""Create Alphabets"")\n    word_alphabet = Alphabet(\'word\', defualt_value=True, singleton=True)\n    char_alphabet = Alphabet(\'character\', defualt_value=True)\n    pos_alphabet = Alphabet(\'pos\')\n    type_alphabet = Alphabet(\'type\')\n    if not os.path.isdir(alphabet_directory):\n        logger.info(""Creating Alphabets: %s"" % alphabet_directory)\n\n        char_alphabet.add(PAD_CHAR)\n        pos_alphabet.add(PAD_POS)\n        type_alphabet.add(PAD_TYPE)\n\n        char_alphabet.add(ROOT_CHAR)\n        pos_alphabet.add(ROOT_POS)\n        type_alphabet.add(ROOT_TYPE)\n\n        char_alphabet.add(END_CHAR)\n        pos_alphabet.add(END_POS)\n        type_alphabet.add(END_TYPE)\n\n        vocab = defaultdict(int)\n        with open(train_path, \'r\') as file:\n            for line in file:\n                line = line.strip()\n                if len(line) == 0:\n                    continue\n\n                tokens = line.split(\'\\t\')\n                for char in tokens[1]:\n                    char_alphabet.add(char)\n\n                word = DIGIT_RE.sub(""0"", tokens[1]) if normalize_digits else tokens[1]\n                vocab[word] += 1\n\n                pos = tokens[4]\n                pos_alphabet.add(pos)\n\n                type = tokens[7]\n                type_alphabet.add(type)\n\n        # collect singletons\n        singletons = set([word for word, count in vocab.items() if count <= min_occurrence])\n\n        # if a singleton is in pretrained embedding dict, set the count to min_occur + c\n        if embedd_dict is not None:\n            assert isinstance(embedd_dict, OrderedDict)\n            for word in vocab.keys():\n                if word in embedd_dict or word.lower() in embedd_dict:\n                    vocab[word] += min_occurrence\n\n        vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n        logger.info(""Total Vocabulary Size: %d"" % len(vocab_list))\n        logger.info(""Total Singleton Size:  %d"" % len(singletons))\n        vocab_list = [word for word in vocab_list if word in _START_VOCAB or vocab[word] > min_occurrence]\n        logger.info(""Total Vocabulary Size (w.o rare words): %d"" % len(vocab_list))\n\n        if len(vocab_list) > max_vocabulary_size:\n            vocab_list = vocab_list[:max_vocabulary_size]\n\n        if data_paths is not None and embedd_dict is not None:\n            expand_vocab()\n\n        for word in vocab_list:\n            word_alphabet.add(word)\n            if word in singletons:\n                word_alphabet.add_singleton(word_alphabet.get_index(word))\n\n        word_alphabet.save(alphabet_directory)\n        char_alphabet.save(alphabet_directory)\n        pos_alphabet.save(alphabet_directory)\n        type_alphabet.save(alphabet_directory)\n    else:\n        word_alphabet.load(alphabet_directory)\n        char_alphabet.load(alphabet_directory)\n        pos_alphabet.load(alphabet_directory)\n        type_alphabet.load(alphabet_directory)\n\n    word_alphabet.close()\n    char_alphabet.close()\n    pos_alphabet.close()\n    type_alphabet.close()\n    logger.info(""Word Alphabet Size (Singleton): %d (%d)"" % (word_alphabet.size(), word_alphabet.singleton_size()))\n    logger.info(""Character Alphabet Size: %d"" % char_alphabet.size())\n    logger.info(""POS Alphabet Size: %d"" % pos_alphabet.size())\n    logger.info(""Type Alphabet Size: %d"" % type_alphabet.size())\n    return word_alphabet, char_alphabet, pos_alphabet, type_alphabet\n\n\ndef read_data(source_path: str, word_alphabet: Alphabet, char_alphabet: Alphabet, pos_alphabet: Alphabet, type_alphabet: Alphabet,\n              max_size=None, normalize_digits=True, symbolic_root=False, symbolic_end=False):\n    data = []\n    max_length = 0\n    max_char_length = 0\n    print(\'Reading data from %s\' % source_path)\n    counter = 0\n    reader = CoNLLXReader(source_path, word_alphabet, char_alphabet, pos_alphabet, type_alphabet)\n    inst = reader.getNext(normalize_digits=normalize_digits, symbolic_root=symbolic_root, symbolic_end=symbolic_end)\n    while inst is not None and (not max_size or counter < max_size):\n        counter += 1\n        if counter % 10000 == 0:\n            print(""reading data: %d"" % counter)\n\n        sent = inst.sentence\n        data.append([sent.word_ids, sent.char_id_seqs, inst.pos_ids, inst.heads, inst.type_ids])\n        max_len = max([len(char_seq) for char_seq in sent.char_seqs])\n        if max_char_length < max_len:\n            max_char_length = max_len\n        if max_length < inst.length():\n            max_length = inst.length()\n        inst = reader.getNext(normalize_digits=normalize_digits, symbolic_root=symbolic_root, symbolic_end=symbolic_end)\n    reader.close()\n    print(""Total number of data: %d"" % counter)\n\n    data_size = len(data)\n    char_length = min(MAX_CHAR_LENGTH, max_char_length)\n    wid_inputs = np.empty([data_size, max_length], dtype=np.int64)\n    cid_inputs = np.empty([data_size, max_length, char_length], dtype=np.int64)\n    pid_inputs = np.empty([data_size, max_length], dtype=np.int64)\n    hid_inputs = np.empty([data_size, max_length], dtype=np.int64)\n    tid_inputs = np.empty([data_size, max_length], dtype=np.int64)\n\n    masks = np.zeros([data_size, max_length], dtype=np.float32)\n    single = np.zeros([data_size, max_length], dtype=np.int64)\n    lengths = np.empty(data_size, dtype=np.int64)\n\n    for i, inst in enumerate(data):\n        wids, cid_seqs, pids, hids, tids = inst\n        inst_size = len(wids)\n        lengths[i] = inst_size\n        # word ids\n        wid_inputs[i, :inst_size] = wids\n        wid_inputs[i, inst_size:] = PAD_ID_WORD\n        for c, cids in enumerate(cid_seqs):\n            cid_inputs[i, c, :len(cids)] = cids\n            cid_inputs[i, c, len(cids):] = PAD_ID_CHAR\n        cid_inputs[i, inst_size:, :] = PAD_ID_CHAR\n        # pos ids\n        pid_inputs[i, :inst_size] = pids\n        pid_inputs[i, inst_size:] = PAD_ID_TAG\n        # type ids\n        tid_inputs[i, :inst_size] = tids\n        tid_inputs[i, inst_size:] = PAD_ID_TAG\n        # heads\n        hid_inputs[i, :inst_size] = hids\n        hid_inputs[i, inst_size:] = PAD_ID_TAG\n        # masks\n        masks[i, :inst_size] = 1.0\n        for j, wid in enumerate(wids):\n            if word_alphabet.is_singleton(wid):\n                single[i, j] = 1\n\n    words = torch.from_numpy(wid_inputs)\n    chars = torch.from_numpy(cid_inputs)\n    pos = torch.from_numpy(pid_inputs)\n    heads = torch.from_numpy(hid_inputs)\n    types = torch.from_numpy(tid_inputs)\n    masks = torch.from_numpy(masks)\n    single = torch.from_numpy(single)\n    lengths = torch.from_numpy(lengths)\n\n    data_tensor = {\'WORD\': words, \'CHAR\': chars, \'POS\': pos, \'HEAD\': heads, \'TYPE\': types,\n                   \'MASK\': masks, \'SINGLE\': single, \'LENGTH\': lengths}\n    return data_tensor, data_size\n\n\ndef read_bucketed_data(source_path: str, word_alphabet: Alphabet, char_alphabet: Alphabet, pos_alphabet: Alphabet, type_alphabet: Alphabet,\n                       max_size=None, normalize_digits=True, symbolic_root=False, symbolic_end=False):\n    data = [[] for _ in _buckets]\n    max_char_length = [0 for _ in _buckets]\n    print(\'Reading data from %s\' % source_path)\n    counter = 0\n    reader = CoNLLXReader(source_path, word_alphabet, char_alphabet, pos_alphabet, type_alphabet)\n    inst = reader.getNext(normalize_digits=normalize_digits, symbolic_root=symbolic_root, symbolic_end=symbolic_end)\n    while inst is not None and (not max_size or counter < max_size):\n        counter += 1\n        if counter % 10000 == 0:\n            print(""reading data: %d"" % counter)\n\n        inst_size = inst.length()\n        sent = inst.sentence\n        for bucket_id, bucket_size in enumerate(_buckets):\n            if inst_size < bucket_size:\n                data[bucket_id].append([sent.word_ids, sent.char_id_seqs, inst.pos_ids, inst.heads, inst.type_ids])\n                max_len = max([len(char_seq) for char_seq in sent.char_seqs])\n                if max_char_length[bucket_id] < max_len:\n                    max_char_length[bucket_id] = max_len\n                break\n\n        inst = reader.getNext(normalize_digits=normalize_digits, symbolic_root=symbolic_root, symbolic_end=symbolic_end)\n    reader.close()\n    print(""Total number of data: %d"" % counter)\n\n    bucket_sizes = [len(data[b]) for b in range(len(_buckets))]\n    data_tensors = []\n    for bucket_id in range(len(_buckets)):\n        bucket_size = bucket_sizes[bucket_id]\n        if bucket_size == 0:\n            data_tensors.append((1, 1))\n            continue\n\n        bucket_length = _buckets[bucket_id]\n        char_length = min(MAX_CHAR_LENGTH, max_char_length[bucket_id])\n        wid_inputs = np.empty([bucket_size, bucket_length], dtype=np.int64)\n        cid_inputs = np.empty([bucket_size, bucket_length, char_length], dtype=np.int64)\n        pid_inputs = np.empty([bucket_size, bucket_length], dtype=np.int64)\n        hid_inputs = np.empty([bucket_size, bucket_length], dtype=np.int64)\n        tid_inputs = np.empty([bucket_size, bucket_length], dtype=np.int64)\n\n        masks = np.zeros([bucket_size, bucket_length], dtype=np.float32)\n        single = np.zeros([bucket_size, bucket_length], dtype=np.int64)\n        lengths = np.empty(bucket_size, dtype=np.int64)\n\n        for i, inst in enumerate(data[bucket_id]):\n            wids, cid_seqs, pids, hids, tids = inst\n            inst_size = len(wids)\n            lengths[i] = inst_size\n            # word ids\n            wid_inputs[i, :inst_size] = wids\n            wid_inputs[i, inst_size:] = PAD_ID_WORD\n            for c, cids in enumerate(cid_seqs):\n                cid_inputs[i, c, :len(cids)] = cids\n                cid_inputs[i, c, len(cids):] = PAD_ID_CHAR\n            cid_inputs[i, inst_size:, :] = PAD_ID_CHAR\n            # pos ids\n            pid_inputs[i, :inst_size] = pids\n            pid_inputs[i, inst_size:] = PAD_ID_TAG\n            # type ids\n            tid_inputs[i, :inst_size] = tids\n            tid_inputs[i, inst_size:] = PAD_ID_TAG\n            # heads\n            hid_inputs[i, :inst_size] = hids\n            hid_inputs[i, inst_size:] = PAD_ID_TAG\n            # masks\n            masks[i, :inst_size] = 1.0\n            for j, wid in enumerate(wids):\n                if word_alphabet.is_singleton(wid):\n                    single[i, j] = 1\n\n        words = torch.from_numpy(wid_inputs)\n        chars = torch.from_numpy(cid_inputs)\n        pos = torch.from_numpy(pid_inputs)\n        heads = torch.from_numpy(hid_inputs)\n        types = torch.from_numpy(tid_inputs)\n        masks = torch.from_numpy(masks)\n        single = torch.from_numpy(single)\n        lengths = torch.from_numpy(lengths)\n\n        data_tensor = {\'WORD\': words, \'CHAR\': chars, \'POS\': pos, \'HEAD\': heads, \'TYPE\': types,\n                       \'MASK\': masks, \'SINGLE\': single, \'LENGTH\': lengths}\n        data_tensors.append(data_tensor)\n    return data_tensors, bucket_sizes\n'"
neuronlp2/io/conllx_stacked_data.py,28,"b'__author__ = \'max\'\n\nimport numpy as np\nimport torch\nfrom neuronlp2.io.reader import CoNLLXReader\nfrom neuronlp2.io.conllx_data import _buckets, NUM_SYMBOLIC_TAGS, create_alphabets\nfrom neuronlp2.io.common import DIGIT_RE, MAX_CHAR_LENGTH, UNK_ID\nfrom neuronlp2.io.common import PAD_CHAR, PAD, PAD_POS, PAD_TYPE, PAD_ID_CHAR, PAD_ID_TAG, PAD_ID_WORD\nfrom neuronlp2.io.common import ROOT, END, ROOT_CHAR, ROOT_POS, ROOT_TYPE, END_CHAR, END_POS, END_TYPE\n\n\ndef _obtain_child_index_for_left2right(heads):\n    child_ids = [[] for _ in range(len(heads))]\n    # skip the symbolic root.\n    for child in range(1, len(heads)):\n        head = heads[child]\n        child_ids[head].append(child)\n    return child_ids\n\n\ndef _obtain_child_index_for_inside_out(heads):\n    child_ids = [[] for _ in range(len(heads))]\n    for head in range(len(heads)):\n        # first find left children inside-out\n        for child in reversed(range(1, head)):\n            if heads[child] == head:\n                child_ids[head].append(child)\n        # second find right children inside-out\n        for child in range(head + 1, len(heads)):\n            if heads[child] == head:\n                child_ids[head].append(child)\n    return child_ids\n\n\ndef _obtain_child_index_for_depth(heads, reverse):\n    def calc_depth(head):\n        children = child_ids[head]\n        max_depth = 0\n        for child in children:\n            depth = calc_depth(child)\n            child_with_depth[head].append((child, depth))\n            max_depth = max(max_depth, depth + 1)\n        child_with_depth[head] = sorted(child_with_depth[head], key=lambda x: x[1], reverse=reverse)\n        return max_depth\n\n    child_ids = _obtain_child_index_for_left2right(heads)\n    child_with_depth = [[] for _ in range(len(heads))]\n    calc_depth(0)\n    return [[child for child, depth in child_with_depth[head]] for head in range(len(heads))]\n\n\ndef _generate_stack_inputs(heads, types, prior_order):\n    if prior_order == \'deep_first\':\n        child_ids = _obtain_child_index_for_depth(heads, True)\n    elif prior_order == \'shallow_first\':\n        child_ids = _obtain_child_index_for_depth(heads, False)\n    elif prior_order == \'left2right\':\n        child_ids = _obtain_child_index_for_left2right(heads)\n    elif prior_order == \'inside_out\':\n        child_ids = _obtain_child_index_for_inside_out(heads)\n    else:\n        raise ValueError(\'Unknown prior order: %s\' % prior_order)\n\n    stacked_heads = []\n    children = []\n    siblings = []\n    stacked_types = []\n    skip_connect = []\n    prev = [0 for _ in range(len(heads))]\n    sibs = [0 for _ in range(len(heads))]\n    stack = [0]\n    position = 1\n    while len(stack) > 0:\n        head = stack[-1]\n        stacked_heads.append(head)\n        siblings.append(sibs[head])\n        child_id = child_ids[head]\n        skip_connect.append(prev[head])\n        prev[head] = position\n        if len(child_id) == 0:\n            children.append(head)\n            sibs[head] = 0\n            stacked_types.append(PAD_ID_TAG)\n            stack.pop()\n        else:\n            child = child_id.pop(0)\n            children.append(child)\n            sibs[head] = child\n            stack.append(child)\n            stacked_types.append(types[child])\n        position += 1\n\n    return stacked_heads, children, siblings, stacked_types, skip_connect\n\n\ndef read_data(source_path, word_alphabet, char_alphabet, pos_alphabet, type_alphabet,\n              max_size=None, normalize_digits=True, prior_order=\'inside_out\'):\n    data = []\n    max_length = 0\n    max_char_length = 0\n    print(\'Reading data from %s\' % source_path)\n    counter = 0\n    reader = CoNLLXReader(source_path, word_alphabet, char_alphabet, pos_alphabet, type_alphabet)\n    inst = reader.getNext(normalize_digits=normalize_digits, symbolic_root=True, symbolic_end=False)\n    while inst is not None and (not max_size or counter < max_size):\n        counter += 1\n        if counter % 10000 == 0:\n            print(""reading data: %d"" % counter)\n\n        sent = inst.sentence\n        stacked_heads, children, siblings, stacked_types, skip_connect = _generate_stack_inputs(inst.heads, inst.type_ids, prior_order)\n        data.append([sent.word_ids, sent.char_id_seqs, inst.pos_ids, inst.heads, inst.type_ids, stacked_heads, children, siblings, stacked_types, skip_connect])\n        max_len = max([len(char_seq) for char_seq in sent.char_seqs])\n        if max_char_length < max_len:\n            max_char_length = max_len\n        if max_length < inst.length():\n            max_length = inst.length()\n        inst = reader.getNext(normalize_digits=normalize_digits, symbolic_root=True, symbolic_end=False)\n    reader.close()\n    print(""Total number of data: %d"" % counter)\n\n    data_size = len(data)\n    char_length = min(MAX_CHAR_LENGTH, max_char_length)\n    wid_inputs = np.empty([data_size, max_length], dtype=np.int64)\n    cid_inputs = np.empty([data_size, max_length, char_length], dtype=np.int64)\n    pid_inputs = np.empty([data_size, max_length], dtype=np.int64)\n    hid_inputs = np.empty([data_size, max_length], dtype=np.int64)\n    tid_inputs = np.empty([data_size, max_length], dtype=np.int64)\n\n    masks_e = np.zeros([data_size, max_length], dtype=np.float32)\n    single = np.zeros([data_size, max_length], dtype=np.int64)\n    lengths = np.empty(data_size, dtype=np.int64)\n\n    stack_hid_inputs = np.empty([data_size, 2 * max_length - 1], dtype=np.int64)\n    chid_inputs = np.empty([data_size, 2 * max_length - 1], dtype=np.int64)\n    ssid_inputs = np.empty([data_size, 2 * max_length - 1], dtype=np.int64)\n    stack_tid_inputs = np.empty([data_size, 2 * max_length - 1], dtype=np.int64)\n    skip_connect_inputs = np.empty([data_size, 2 * max_length - 1], dtype=np.int64)\n\n    masks_d = np.zeros([data_size, 2 * max_length - 1], dtype=np.float32)\n\n    for i, inst in enumerate(data):\n        wids, cid_seqs, pids, hids, tids, stack_hids, chids, ssids, stack_tids, skip_ids = inst\n        inst_size = len(wids)\n        lengths[i] = inst_size\n        # word ids\n        wid_inputs[i, :inst_size] = wids\n        wid_inputs[i, inst_size:] = PAD_ID_WORD\n        for c, cids in enumerate(cid_seqs):\n            cid_inputs[i, c, :len(cids)] = cids\n            cid_inputs[i, c, len(cids):] = PAD_ID_CHAR\n        cid_inputs[i, inst_size:, :] = PAD_ID_CHAR\n        # pos ids\n        pid_inputs[i, :inst_size] = pids\n        pid_inputs[i, inst_size:] = PAD_ID_TAG\n        # type ids\n        tid_inputs[i, :inst_size] = tids\n        tid_inputs[i, inst_size:] = PAD_ID_TAG\n        # heads\n        hid_inputs[i, :inst_size] = hids\n        hid_inputs[i, inst_size:] = PAD_ID_TAG\n        # masks_e\n        masks_e[i, :inst_size] = 1.0\n        for j, wid in enumerate(wids):\n            if word_alphabet.is_singleton(wid):\n                single[i, j] = 1\n\n        inst_size_decoder = 2 * inst_size - 1\n        # stacked heads\n        stack_hid_inputs[i, :inst_size_decoder] = stack_hids\n        stack_hid_inputs[i, inst_size_decoder:] = PAD_ID_TAG\n        # children\n        chid_inputs[i, :inst_size_decoder] = chids\n        chid_inputs[i, inst_size_decoder:] = PAD_ID_TAG\n        # siblings\n        ssid_inputs[i, :inst_size_decoder] = ssids\n        ssid_inputs[i, inst_size_decoder:] = PAD_ID_TAG\n        # stacked types\n        stack_tid_inputs[i, :inst_size_decoder] = stack_tids\n        stack_tid_inputs[i, inst_size_decoder:] = PAD_ID_TAG\n        # skip connects\n        skip_connect_inputs[i, :inst_size_decoder] = skip_ids\n        skip_connect_inputs[i, inst_size_decoder:] = PAD_ID_TAG\n        # masks_d\n        masks_d[i, :inst_size_decoder] = 1.0\n\n    words = torch.from_numpy(wid_inputs)\n    chars = torch.from_numpy(cid_inputs)\n    pos = torch.from_numpy(pid_inputs)\n    heads = torch.from_numpy(hid_inputs)\n    types = torch.from_numpy(tid_inputs)\n    masks_e = torch.from_numpy(masks_e)\n    single = torch.from_numpy(single)\n    lengths = torch.from_numpy(lengths)\n\n    stacked_heads = torch.from_numpy(stack_hid_inputs)\n    children = torch.from_numpy(chid_inputs)\n    siblings = torch.from_numpy(ssid_inputs)\n    stacked_types = torch.from_numpy(stack_tid_inputs)\n    skip_connect = torch.from_numpy(skip_connect_inputs)\n    masks_d = torch.from_numpy(masks_d)\n\n    data_tensor = {\'WORD\': words, \'CHAR\': chars, \'POS\': pos, \'HEAD\': heads, \'TYPE\': types, \'MASK_ENC\': masks_e,\n                   \'SINGLE\': single, \'LENGTH\': lengths, \'STACK_HEAD\': stacked_heads, \'CHILD\': children,\n                   \'SIBLING\': siblings, \'STACK_TYPE\': stacked_types, \'SKIP_CONNECT\': skip_connect, \'MASK_DEC\': masks_d}\n    return data_tensor, data_size\n\n\ndef read_bucketed_data(source_path, word_alphabet, char_alphabet, pos_alphabet, type_alphabet,\n                       max_size=None, normalize_digits=True, prior_order=\'inside_out\'):\n    data = [[] for _ in _buckets]\n    max_char_length = [0 for _ in _buckets]\n    print(\'Reading data from %s\' % source_path)\n    counter = 0\n    reader = CoNLLXReader(source_path, word_alphabet, char_alphabet, pos_alphabet, type_alphabet)\n    inst = reader.getNext(normalize_digits=normalize_digits, symbolic_root=True, symbolic_end=False)\n    while inst is not None and (not max_size or counter < max_size):\n        counter += 1\n        if counter % 10000 == 0:\n            print(""reading data: %d"" % counter)\n\n        inst_size = inst.length()\n        sent = inst.sentence\n        for bucket_id, bucket_size in enumerate(_buckets):\n            if inst_size < bucket_size:\n                stacked_heads, children, siblings, stacked_types, skip_connect = _generate_stack_inputs(inst.heads, inst.type_ids, prior_order)\n                data[bucket_id].append([sent.word_ids, sent.char_id_seqs, inst.pos_ids, inst.heads, inst.type_ids, stacked_heads, children, siblings, stacked_types, skip_connect])\n                max_len = max([len(char_seq) for char_seq in sent.char_seqs])\n                if max_char_length[bucket_id] < max_len:\n                    max_char_length[bucket_id] = max_len\n                break\n\n        inst = reader.getNext(normalize_digits=normalize_digits, symbolic_root=True, symbolic_end=False)\n    reader.close()\n    print(""Total number of data: %d"" % counter)\n\n    bucket_sizes = [len(data[b]) for b in range(len(_buckets))]\n    data_tensors = []\n    for bucket_id in range(len(_buckets)):\n        bucket_size = bucket_sizes[bucket_id]\n        if bucket_size == 0:\n            data_tensors.append((1, 1))\n            continue\n\n        bucket_length = _buckets[bucket_id]\n        char_length = min(MAX_CHAR_LENGTH, max_char_length[bucket_id])\n        wid_inputs = np.empty([bucket_size, bucket_length], dtype=np.int64)\n        cid_inputs = np.empty([bucket_size, bucket_length, char_length], dtype=np.int64)\n        pid_inputs = np.empty([bucket_size, bucket_length], dtype=np.int64)\n        hid_inputs = np.empty([bucket_size, bucket_length], dtype=np.int64)\n        tid_inputs = np.empty([bucket_size, bucket_length], dtype=np.int64)\n\n        masks_e = np.zeros([bucket_size, bucket_length], dtype=np.float32)\n        single = np.zeros([bucket_size, bucket_length], dtype=np.int64)\n        lengths = np.empty(bucket_size, dtype=np.int64)\n\n        stack_hid_inputs = np.empty([bucket_size, 2 * bucket_length - 1], dtype=np.int64)\n        chid_inputs = np.empty([bucket_size, 2 * bucket_length - 1], dtype=np.int64)\n        ssid_inputs = np.empty([bucket_size, 2 * bucket_length - 1], dtype=np.int64)\n        stack_tid_inputs = np.empty([bucket_size, 2 * bucket_length - 1], dtype=np.int64)\n        skip_connect_inputs = np.empty([bucket_size, 2 * bucket_length - 1], dtype=np.int64)\n\n        masks_d = np.zeros([bucket_size, 2 * bucket_length - 1], dtype=np.float32)\n\n        for i, inst in enumerate(data[bucket_id]):\n            wids, cid_seqs, pids, hids, tids, stack_hids, chids, ssids, stack_tids, skip_ids = inst\n            inst_size = len(wids)\n            lengths[i] = inst_size\n            # word ids\n            wid_inputs[i, :inst_size] = wids\n            wid_inputs[i, inst_size:] = PAD_ID_WORD\n            for c, cids in enumerate(cid_seqs):\n                cid_inputs[i, c, :len(cids)] = cids\n                cid_inputs[i, c, len(cids):] = PAD_ID_CHAR\n            cid_inputs[i, inst_size:, :] = PAD_ID_CHAR\n            # pos ids\n            pid_inputs[i, :inst_size] = pids\n            pid_inputs[i, inst_size:] = PAD_ID_TAG\n            # type ids\n            tid_inputs[i, :inst_size] = tids\n            tid_inputs[i, inst_size:] = PAD_ID_TAG\n            # heads\n            hid_inputs[i, :inst_size] = hids\n            hid_inputs[i, inst_size:] = PAD_ID_TAG\n            # masks_e\n            masks_e[i, :inst_size] = 1.0\n            for j, wid in enumerate(wids):\n                if word_alphabet.is_singleton(wid):\n                    single[i, j] = 1\n\n            inst_size_decoder = 2 * inst_size - 1\n            # stacked heads\n            stack_hid_inputs[i, :inst_size_decoder] = stack_hids\n            stack_hid_inputs[i, inst_size_decoder:] = PAD_ID_TAG\n            # children\n            chid_inputs[i, :inst_size_decoder] = chids\n            chid_inputs[i, inst_size_decoder:] = PAD_ID_TAG\n            # siblings\n            ssid_inputs[i, :inst_size_decoder] = ssids\n            ssid_inputs[i, inst_size_decoder:] = PAD_ID_TAG\n            # stacked types\n            stack_tid_inputs[i, :inst_size_decoder] = stack_tids\n            stack_tid_inputs[i, inst_size_decoder:] = PAD_ID_TAG\n            # skip connects\n            skip_connect_inputs[i, :inst_size_decoder] = skip_ids\n            skip_connect_inputs[i, inst_size_decoder:] = PAD_ID_TAG\n            # masks_d\n            masks_d[i, :inst_size_decoder] = 1.0\n\n        words = torch.from_numpy(wid_inputs)\n        chars = torch.from_numpy(cid_inputs)\n        pos = torch.from_numpy(pid_inputs)\n        heads = torch.from_numpy(hid_inputs)\n        types = torch.from_numpy(tid_inputs)\n        masks_e = torch.from_numpy(masks_e)\n        single = torch.from_numpy(single)\n        lengths = torch.from_numpy(lengths)\n\n        stacked_heads = torch.from_numpy(stack_hid_inputs)\n        children = torch.from_numpy(chid_inputs)\n        siblings = torch.from_numpy(ssid_inputs)\n        stacked_types = torch.from_numpy(stack_tid_inputs)\n        skip_connect = torch.from_numpy(skip_connect_inputs)\n        masks_d = torch.from_numpy(masks_d)\n\n        data_tensor = {\'WORD\': words, \'CHAR\': chars, \'POS\': pos, \'HEAD\': heads, \'TYPE\': types, \'MASK_ENC\': masks_e,\n                       \'SINGLE\': single, \'LENGTH\': lengths, \'STACK_HEAD\': stacked_heads, \'CHILD\': children,\n                       \'SIBLING\': siblings, \'STACK_TYPE\': stacked_types, \'SKIP_CONNECT\': skip_connect, \'MASK_DEC\': masks_d}\n        data_tensors.append(data_tensor)\n\n    return data_tensors, bucket_sizes\n'"
neuronlp2/io/instance.py,0,"b""__author__ = 'max'\n\n__all__ = ['Sentence', 'DependencyInstance', 'NERInstance']\n\n\nclass Sentence(object):\n    def __init__(self, words, word_ids, char_seqs, char_id_seqs):\n        self.words = words\n        self.word_ids = word_ids\n        self.char_seqs = char_seqs\n        self.char_id_seqs = char_id_seqs\n\n    def length(self):\n        return len(self.words)\n\n\nclass DependencyInstance(object):\n    def __init__(self, sentence, postags, pos_ids, heads, types, type_ids):\n        self.sentence = sentence\n        self.postags = postags\n        self.pos_ids = pos_ids\n        self.heads = heads\n        self.types = types\n        self.type_ids = type_ids\n\n    def length(self):\n        return self.sentence.length()\n\n\nclass NERInstance(object):\n    def __init__(self, sentence, postags, pos_ids, chunk_tags, chunk_ids, ner_tags, ner_ids):\n        self.sentence = sentence\n        self.postags = postags\n        self.pos_ids = pos_ids\n        self.chunk_tags = chunk_tags\n        self.chunk_ids = chunk_ids\n        self.ner_tags = ner_tags\n        self.ner_ids = ner_ids\n\n    def length(self):\n        return self.sentence.length()"""
neuronlp2/io/logger.py,0,"b""_author__ = 'max'\n\nimport logging\nimport sys\n\n\ndef get_logger(name, level=logging.INFO, handler=sys.stdout,\n               formatter='%(asctime)s - %(name)s - %(levelname)s - %(message)s'):\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.INFO)\n    formatter = logging.Formatter(formatter)\n    stream_handler = logging.StreamHandler(handler)\n    stream_handler.setLevel(level)\n    stream_handler.setFormatter(formatter)\n    logger.addHandler(stream_handler)\n\n    return logger\n"""
neuronlp2/io/reader.py,0,"b'__author__ = \'max\'\n\nfrom neuronlp2.io.instance import DependencyInstance, NERInstance\nfrom neuronlp2.io.instance import Sentence\nfrom neuronlp2.io.common import ROOT, ROOT_POS, ROOT_CHAR, ROOT_TYPE, END, END_POS, END_CHAR, END_TYPE\nfrom neuronlp2.io.common import DIGIT_RE, MAX_CHAR_LENGTH\n\n\nclass CoNLLXReader(object):\n    def __init__(self, file_path, word_alphabet, char_alphabet, pos_alphabet, type_alphabet):\n        self.__source_file = open(file_path, \'r\')\n        self.__word_alphabet = word_alphabet\n        self.__char_alphabet = char_alphabet\n        self.__pos_alphabet = pos_alphabet\n        self.__type_alphabet = type_alphabet\n\n    def close(self):\n        self.__source_file.close()\n\n    def getNext(self, normalize_digits=True, symbolic_root=False, symbolic_end=False):\n        line = self.__source_file.readline()\n        # skip multiple blank lines.\n        while len(line) > 0 and len(line.strip()) == 0:\n            line = self.__source_file.readline()\n        if len(line) == 0:\n            return None\n\n        lines = []\n        while len(line.strip()) > 0:\n            line = line.strip()\n            lines.append(line.split(\'\\t\'))\n            line = self.__source_file.readline()\n\n        length = len(lines)\n        if length == 0:\n            return None\n\n        words = []\n        word_ids = []\n        char_seqs = []\n        char_id_seqs = []\n        postags = []\n        pos_ids = []\n        types = []\n        type_ids = []\n        heads = []\n\n        if symbolic_root:\n            words.append(ROOT)\n            word_ids.append(self.__word_alphabet.get_index(ROOT))\n            char_seqs.append([ROOT_CHAR, ])\n            char_id_seqs.append([self.__char_alphabet.get_index(ROOT_CHAR), ])\n            postags.append(ROOT_POS)\n            pos_ids.append(self.__pos_alphabet.get_index(ROOT_POS))\n            types.append(ROOT_TYPE)\n            type_ids.append(self.__type_alphabet.get_index(ROOT_TYPE))\n            heads.append(0)\n\n        for tokens in lines:\n            chars = []\n            char_ids = []\n            for char in tokens[1]:\n                chars.append(char)\n                char_ids.append(self.__char_alphabet.get_index(char))\n            if len(chars) > MAX_CHAR_LENGTH:\n                chars = chars[:MAX_CHAR_LENGTH]\n                char_ids = char_ids[:MAX_CHAR_LENGTH]\n            char_seqs.append(chars)\n            char_id_seqs.append(char_ids)\n\n            word = DIGIT_RE.sub(""0"", tokens[1]) if normalize_digits else tokens[1]\n            pos = tokens[4]\n            head = int(tokens[6])\n            type = tokens[7]\n\n            words.append(word)\n            word_ids.append(self.__word_alphabet.get_index(word))\n\n            postags.append(pos)\n            pos_ids.append(self.__pos_alphabet.get_index(pos))\n\n            types.append(type)\n            type_ids.append(self.__type_alphabet.get_index(type))\n\n            heads.append(head)\n\n        if symbolic_end:\n            words.append(END)\n            word_ids.append(self.__word_alphabet.get_index(END))\n            char_seqs.append([END_CHAR, ])\n            char_id_seqs.append([self.__char_alphabet.get_index(END_CHAR), ])\n            postags.append(END_POS)\n            pos_ids.append(self.__pos_alphabet.get_index(END_POS))\n            types.append(END_TYPE)\n            type_ids.append(self.__type_alphabet.get_index(END_TYPE))\n            heads.append(0)\n\n        return DependencyInstance(Sentence(words, word_ids, char_seqs, char_id_seqs), postags, pos_ids, heads, types, type_ids)\n\n\nclass CoNLL03Reader(object):\n    def __init__(self, file_path, word_alphabet, char_alphabet, pos_alphabet, chunk_alphabet, ner_alphabet):\n        self.__source_file = open(file_path, \'r\')\n        self.__word_alphabet = word_alphabet\n        self.__char_alphabet = char_alphabet\n        self.__pos_alphabet = pos_alphabet\n        self.__chunk_alphabet = chunk_alphabet\n        self.__ner_alphabet = ner_alphabet\n\n    def close(self):\n        self.__source_file.close()\n\n    def getNext(self, normalize_digits=True):\n        line = self.__source_file.readline()\n        # skip multiple blank lines.\n        while len(line) > 0 and len(line.strip()) == 0:\n            line = self.__source_file.readline()\n        if len(line) == 0:\n            return None\n\n        lines = []\n        while len(line.strip()) > 0:\n            line = line.strip()\n            lines.append(line.split(\' \'))\n            line = self.__source_file.readline()\n\n        length = len(lines)\n        if length == 0:\n            return None\n\n        words = []\n        word_ids = []\n        char_seqs = []\n        char_id_seqs = []\n        postags = []\n        pos_ids = []\n        chunk_tags = []\n        chunk_ids = []\n        ner_tags = []\n        ner_ids = []\n\n        for tokens in lines:\n            chars = []\n            char_ids = []\n            for char in tokens[1]:\n                chars.append(char)\n                char_ids.append(self.__char_alphabet.get_index(char))\n            if len(chars) > MAX_CHAR_LENGTH:\n                chars = chars[:MAX_CHAR_LENGTH]\n                char_ids = char_ids[:MAX_CHAR_LENGTH]\n            char_seqs.append(chars)\n            char_id_seqs.append(char_ids)\n\n            word = DIGIT_RE.sub(""0"", tokens[1]) if normalize_digits else tokens[1]\n            pos = tokens[2]\n            chunk = tokens[3]\n            ner = tokens[4]\n\n            words.append(word)\n            word_ids.append(self.__word_alphabet.get_index(word))\n\n            postags.append(pos)\n            pos_ids.append(self.__pos_alphabet.get_index(pos))\n\n            chunk_tags.append(chunk)\n            chunk_ids.append(self.__chunk_alphabet.get_index(chunk))\n\n            ner_tags.append(ner)\n            ner_ids.append(self.__ner_alphabet.get_index(ner))\n\n        return NERInstance(Sentence(words, word_ids, char_seqs, char_id_seqs),\n                           postags, pos_ids, chunk_tags, chunk_ids, ner_tags, ner_ids)\n'"
neuronlp2/io/utils.py,4,"b""__author__ = 'max'\n\nimport numpy as np\nimport torch\n\n\ndef get_batch(data, batch_size, unk_replace=0.):\n    data, data_size = data\n    batch_size = min(data_size, batch_size)\n    index = torch.randperm(data_size).long()[:batch_size]\n\n    lengths = data['LENGTH'][index]\n    max_length = lengths.max().item()\n    words = data['WORD']\n    single = data['SINGLE']\n    words = words[index, :max_length]\n    single = single[index, :max_length]\n    if unk_replace:\n        ones = single.new_ones(batch_size, max_length)\n        noise = single.new_empty(batch_size, max_length).bernoulli_(unk_replace).long()\n        words = words * (ones - single * noise)\n\n    stack_keys = ['STACK_HEAD', 'CHILD', 'SIBLING', 'STACK_TYPE', 'SKIP_CONNECT', 'MASK_DEC']\n    exclude_keys = set(['SINGLE', 'WORD', 'LENGTH'] + stack_keys)\n    stack_keys = set(stack_keys)\n    batch = {'WORD': words, 'LENGTH': lengths}\n    batch.update({key: field[index, :max_length] for key, field in data.items() if key not in exclude_keys})\n    batch.update({key: field[index, :2 * max_length - 1] for key, field in data.items() if key in stack_keys})\n    return batch\n\n\ndef get_bucketed_batch(data, batch_size, unk_replace=0.):\n    data_buckets, bucket_sizes = data\n    total_size = float(sum(bucket_sizes))\n    # A bucket scale is a list of increasing numbers from 0 to 1 that we'll use\n    # to select a bucket. Length of [scale[i], scale[i+1]] is proportional to\n    # the size if i-th training bucket, as used later.\n    buckets_scale = [sum(bucket_sizes[:i + 1]) / total_size for i in range(len(bucket_sizes))]\n\n    # Choose a bucket according to data distribution. We pick a random number\n    # in [0, 1] and use the corresponding interval in train_buckets_scale.\n    random_number = np.random.random_sample()\n    bucket_id = min([i for i in range(len(buckets_scale)) if buckets_scale[i] > random_number])\n\n    data = data_buckets[bucket_id]\n    bucket_size = bucket_sizes[bucket_id]\n    batch_size = min(bucket_size, batch_size)\n    index = torch.randperm(bucket_size).long()[:batch_size]\n\n    lengths = data['LENGTH'][index]\n    max_length = lengths.max().item()\n    words = data['WORD']\n    single = data['SINGLE']\n    words = words[index, :max_length]\n    single = single[index, :max_length]\n    if unk_replace:\n        ones = single.new_ones(batch_size, max_length)\n        noise = single.new_empty(batch_size, max_length).bernoulli_(unk_replace).long()\n        words = words * (ones - single * noise)\n\n    stack_keys = ['STACK_HEAD', 'CHILD', 'SIBLING', 'STACK_TYPE', 'SKIP_CONNECT', 'MASK_DEC']\n    exclude_keys = set(['SINGLE', 'WORD', 'LENGTH'] + stack_keys)\n    stack_keys = set(stack_keys)\n    batch = {'WORD': words, 'LENGTH': lengths}\n    batch.update({key: field[index, :max_length] for key, field in data.items() if key not in exclude_keys})\n    batch.update({key: field[index, :2 * max_length - 1] for key, field in data.items() if key in stack_keys})\n    return batch\n\n\ndef iterate_batch(data, batch_size, unk_replace=0., shuffle=False):\n    data, data_size = data\n\n    words = data['WORD']\n    single = data['SINGLE']\n    max_length = words.size(1)\n    if unk_replace:\n        ones = single.new_ones(data_size, max_length)\n        noise = single.new_empty(data_size, max_length).bernoulli_(unk_replace).long()\n        words = words * (ones - single * noise)\n\n    indices = None\n    if shuffle:\n        indices = torch.randperm(data_size).long()\n        indices = indices.to(words.device)\n\n    stack_keys = ['STACK_HEAD', 'CHILD', 'SIBLING', 'STACK_TYPE', 'SKIP_CONNECT', 'MASK_DEC']\n    exclude_keys = set(['SINGLE', 'WORD', 'LENGTH'] + stack_keys)\n    stack_keys = set(stack_keys)\n    for start_idx in range(0, data_size, batch_size):\n        if shuffle:\n            excerpt = indices[start_idx:start_idx + batch_size]\n        else:\n            excerpt = slice(start_idx, start_idx + batch_size)\n\n        lengths = data['LENGTH'][excerpt]\n        batch_length = lengths.max().item()\n        batch = {'WORD': words[excerpt, :batch_length], 'LENGTH': lengths}\n        batch.update({key: field[excerpt, :batch_length] for key, field in data.items() if key not in exclude_keys})\n        batch.update({key: field[excerpt, :2 * batch_length - 1] for key, field in data.items() if key in stack_keys})\n        yield batch\n\n\ndef iterate_bucketed_batch(data, batch_size, unk_replace=0., shuffle=False):\n    data_tensor, bucket_sizes = data\n\n    bucket_indices = np.arange(len(bucket_sizes))\n    if shuffle:\n        np.random.shuffle((bucket_indices))\n\n    stack_keys = ['STACK_HEAD', 'CHILD', 'SIBLING', 'STACK_TYPE', 'SKIP_CONNECT', 'MASK_DEC']\n    exclude_keys = set(['SINGLE', 'WORD', 'LENGTH'] + stack_keys)\n    stack_keys = set(stack_keys)\n    for bucket_id in bucket_indices:\n        data = data_tensor[bucket_id]\n        bucket_size = bucket_sizes[bucket_id]\n        if bucket_size == 0:\n            continue\n\n        words = data['WORD']\n        single = data['SINGLE']\n        bucket_length = words.size(1)\n        if unk_replace:\n            ones = single.new_ones(bucket_size, bucket_length)\n            noise = single.new_empty(bucket_size, bucket_length).bernoulli_(unk_replace).long()\n            words = words * (ones - single * noise)\n\n        indices = None\n        if shuffle:\n            indices = torch.randperm(bucket_size).long()\n            indices = indices.to(words.device)\n        for start_idx in range(0, bucket_size, batch_size):\n            if shuffle:\n                excerpt = indices[start_idx:start_idx + batch_size]\n            else:\n                excerpt = slice(start_idx, start_idx + batch_size)\n\n            lengths = data['LENGTH'][excerpt]\n            batch_length = lengths.max().item()\n            batch = {'WORD': words[excerpt, :batch_length], 'LENGTH': lengths}\n            batch.update({key: field[excerpt, :batch_length] for key, field in data.items() if key not in exclude_keys})\n            batch.update({key: field[excerpt, :2 * batch_length - 1] for key, field in data.items() if key in stack_keys})\n            yield batch\n\n\ndef iterate_data(data, batch_size, bucketed=False, unk_replace=0., shuffle=False):\n    if bucketed:\n        return iterate_bucketed_batch(data, batch_size, unk_replace==unk_replace, shuffle=shuffle)\n    else:\n        return iterate_batch(data, batch_size, unk_replace==unk_replace, shuffle=shuffle)\n"""
neuronlp2/io/writer.py,0,"b""__author__ = 'max'\n\n\nclass CoNLL03Writer(object):\n    def __init__(self, word_alphabet, char_alphabet, pos_alphabet, chunk_alphabet, ner_alphabet):\n        self.__source_file = None\n        self.__word_alphabet = word_alphabet\n        self.__char_alphabet = char_alphabet\n        self.__pos_alphabet = pos_alphabet\n        self.__chunk_alphabet = chunk_alphabet\n        self.__ner_alphabet = ner_alphabet\n\n    def start(self, file_path):\n        self.__source_file = open(file_path, 'w')\n\n    def close(self):\n        self.__source_file.close()\n\n    def write(self, word, pos, chunk, predictions, targets, lengths):\n        batch_size, _ = word.shape\n        for i in range(batch_size):\n            for j in range(lengths[i]):\n                w = self.__word_alphabet.get_instance(word[i, j])\n                p = self.__pos_alphabet.get_instance(pos[i, j])\n                ch = self.__chunk_alphabet.get_instance(chunk[i, j])\n                tgt = self.__ner_alphabet.get_instance(targets[i, j])\n                pred = self.__ner_alphabet.get_instance(predictions[i, j])\n                self.__source_file.write('%d %s %s %s %s %s\\n' % (j + 1, w, p, ch, tgt, pred))\n            self.__source_file.write('\\n')\n\n\nclass POSWriter(object):\n    def __init__(self, word_alphabet, char_alphabet, pos_alphabet):\n        self.__source_file = None\n        self.__word_alphabet = word_alphabet\n        self.__char_alphabet = char_alphabet\n        self.__pos_alphabet = pos_alphabet\n\n    def start(self, file_path):\n        self.__source_file = open(file_path, 'w')\n\n    def close(self):\n        self.__source_file.close()\n\n    def write(self, word, predictions, targets, lengths, symbolic_root=False, symbolic_end=False):\n        batch_size, _ = word.shape\n        start = 1 if symbolic_root else 0\n        end = 1 if symbolic_end else 0\n        for i in range(batch_size):\n            for j in range(start, lengths[i] - end):\n                w = self.__word_alphabet.get_instance(word[i, j])\n                pred = self.__pos_alphabet.get_instance(predictions[i, j])\n                tgt = self.__pos_alphabet.get_instance(targets[i, j])\n                self.__source_file.write('%d\\t%s\\t_\\t%s\\t%s\\n' % (j, w, tgt, pred))\n            self.__source_file.write('\\n')\n\n\nclass CoNLLXWriter(object):\n    def __init__(self, word_alphabet, char_alphabet, pos_alphabet, type_alphabet):\n        self.__source_file = None\n        self.__word_alphabet = word_alphabet\n        self.__char_alphabet = char_alphabet\n        self.__pos_alphabet = pos_alphabet\n        self.__type_alphabet = type_alphabet\n\n    def start(self, file_path):\n        self.__source_file = open(file_path, 'w')\n\n    def close(self):\n        self.__source_file.close()\n\n    def write(self, word, pos, head, type, lengths, symbolic_root=False, symbolic_end=False):\n        batch_size, _ = word.shape\n        start = 1 if symbolic_root else 0\n        end = 1 if symbolic_end else 0\n        for i in range(batch_size):\n            for j in range(start, lengths[i] - end):\n                w = self.__word_alphabet.get_instance(word[i, j])\n                p = self.__pos_alphabet.get_instance(pos[i, j])\n                t = self.__type_alphabet.get_instance(type[i, j])\n                h = head[i, j]\n                self.__source_file.write('%d\\t%s\\t_\\t_\\t%s\\t_\\t%d\\t%s\\n' % (j, w, p, h, t))\n            self.__source_file.write('\\n')\n"""
neuronlp2/models/__init__.py,0,"b""__author__ = 'max'\n\nfrom neuronlp2.models.sequence_labeling import *\nfrom .parsing import DeepBiAffine, NeuroMST, StackPtrNet\n"""
neuronlp2/models/parsing.py,35,"b'__author__ = \'max\'\n\nfrom overrides import overrides\nimport numpy as np\nfrom enum import Enum\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom neuronlp2.nn import TreeCRF, VarGRU, VarRNN, VarLSTM, VarFastLSTM\nfrom neuronlp2.nn import BiAffine, BiLinear, CharCNN\nfrom neuronlp2.tasks import parser\n\n\nclass PriorOrder(Enum):\n    DEPTH = 0\n    INSIDE_OUT = 1\n    LEFT2RIGTH = 2\n\n\nclass DeepBiAffine(nn.Module):\n    def __init__(self, word_dim, num_words, char_dim, num_chars, pos_dim, num_pos, rnn_mode, hidden_size, num_layers, num_labels, arc_space, type_space,\n                 embedd_word=None, embedd_char=None, embedd_pos=None, p_in=0.33, p_out=0.33, p_rnn=(0.33, 0.33), pos=True, activation=\'elu\'):\n        super(DeepBiAffine, self).__init__()\n\n        self.word_embed = nn.Embedding(num_words, word_dim, _weight=embedd_word, padding_idx=1)\n        self.pos_embed = nn.Embedding(num_pos, pos_dim, _weight=embedd_pos, padding_idx=1) if pos else None\n        self.char_embed = nn.Embedding(num_chars, char_dim, _weight=embedd_char, padding_idx=1)\n        self.char_cnn = CharCNN(2, char_dim, char_dim, hidden_channels=char_dim * 4, activation=activation)\n\n        self.dropout_in = nn.Dropout2d(p=p_in)\n        self.dropout_out = nn.Dropout2d(p=p_out)\n        self.num_labels = num_labels\n\n        if rnn_mode == \'RNN\':\n            RNN = VarRNN\n        elif rnn_mode == \'LSTM\':\n            RNN = VarLSTM\n        elif rnn_mode == \'FastLSTM\':\n            RNN = VarFastLSTM\n        elif rnn_mode == \'GRU\':\n            RNN = VarGRU\n        else:\n            raise ValueError(\'Unknown RNN mode: %s\' % rnn_mode)\n\n        dim_enc = word_dim + char_dim\n        if pos:\n            dim_enc += pos_dim\n\n        self.rnn = RNN(dim_enc, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True, dropout=p_rnn)\n\n        out_dim = hidden_size * 2\n        self.arc_h = nn.Linear(out_dim, arc_space)\n        self.arc_c = nn.Linear(out_dim, arc_space)\n        self.biaffine = BiAffine(arc_space, arc_space)\n\n        self.type_h = nn.Linear(out_dim, type_space)\n        self.type_c = nn.Linear(out_dim, type_space)\n        self.bilinear = BiLinear(type_space, type_space, self.num_labels)\n\n        assert activation in [\'elu\', \'tanh\']\n        if activation == \'elu\':\n            self.activation = nn.ELU(inplace=True)\n        else:\n            self.activation = nn.Tanh()\n        self.criterion = nn.CrossEntropyLoss(reduction=\'none\')\n        self.reset_parameters(embedd_word, embedd_char, embedd_pos)\n\n    def reset_parameters(self, embedd_word, embedd_char, embedd_pos):\n        if embedd_word is None:\n            nn.init.uniform_(self.word_embed.weight, -0.1, 0.1)\n        if embedd_char is None:\n            nn.init.uniform_(self.char_embed.weight, -0.1, 0.1)\n        if embedd_pos is None and self.pos_embed is not None:\n            nn.init.uniform_(self.pos_embed.weight, -0.1, 0.1)\n\n        with torch.no_grad():\n            self.word_embed.weight[self.word_embed.padding_idx].fill_(0)\n            self.char_embed.weight[self.char_embed.padding_idx].fill_(0)\n            if self.pos_embed is not None:\n                self.pos_embed.weight[self.pos_embed.padding_idx].fill_(0)\n\n        nn.init.xavier_uniform_(self.arc_h.weight)\n        nn.init.constant_(self.arc_h.bias, 0.)\n        nn.init.xavier_uniform_(self.arc_c.weight)\n        nn.init.constant_(self.arc_c.bias, 0.)\n\n        nn.init.xavier_uniform_(self.type_h.weight)\n        nn.init.constant_(self.type_h.bias, 0.)\n        nn.init.xavier_uniform_(self.type_c.weight)\n        nn.init.constant_(self.type_c.bias, 0.)\n\n    def _get_rnn_output(self, input_word, input_char, input_pos, mask=None):\n        # [batch, length, word_dim]\n        word = self.word_embed(input_word)\n\n        # [batch, length, char_length, char_dim]\n        char = self.char_cnn(self.char_embed(input_char))\n\n        # apply dropout word on input\n        word = self.dropout_in(word)\n        char = self.dropout_in(char)\n\n        # concatenate word and char [batch, length, word_dim+char_filter]\n        enc = torch.cat([word, char], dim=2)\n\n        if self.pos_embed is not None:\n            # [batch, length, pos_dim]\n            pos = self.pos_embed(input_pos)\n            # apply dropout on input\n            pos = self.dropout_in(pos)\n            enc = torch.cat([enc, pos], dim=2)\n\n        # output from rnn [batch, length, hidden_size]\n        output, _ = self.rnn(enc, mask)\n\n        # apply dropout for output\n        # [batch, length, hidden_size] --> [batch, hidden_size, length] --> [batch, length, hidden_size]\n        output = self.dropout_out(output.transpose(1, 2)).transpose(1, 2)\n\n        # output size [batch, length, arc_space]\n        arc_h = self.activation(self.arc_h(output))\n        arc_c = self.activation(self.arc_c(output))\n\n        # output size [batch, length, type_space]\n        type_h = self.activation(self.type_h(output))\n        type_c = self.activation(self.type_c(output))\n\n        # apply dropout on arc\n        # [batch, length, dim] --> [batch, 2 * length, dim]\n        arc = torch.cat([arc_h, arc_c], dim=1)\n        type = torch.cat([type_h, type_c], dim=1)\n        arc = self.dropout_out(arc.transpose(1, 2)).transpose(1, 2)\n        arc_h, arc_c = arc.chunk(2, 1)\n\n        # apply dropout on type\n        # [batch, length, dim] --> [batch, 2 * length, dim]\n        type = self.dropout_out(type.transpose(1, 2)).transpose(1, 2)\n        type_h, type_c = type.chunk(2, 1)\n        type_h = type_h.contiguous()\n        type_c = type_c.contiguous()\n\n        return (arc_h, arc_c), (type_h, type_c)\n\n    def forward(self, input_word, input_char, input_pos, mask=None):\n        # output from rnn [batch, length, dim]\n        arc, type = self._get_rnn_output(input_word, input_char, input_pos, mask=mask)\n        # [batch, length_head, length_child]\n        out_arc = self.biaffine(arc[0], arc[1], mask_query=mask, mask_key=mask)\n        return out_arc, type\n\n    def loss(self, input_word, input_char, input_pos, heads, types, mask=None):\n        # out_arc shape [batch, length_head, length_child]\n        out_arc, out_type  = self(input_word, input_char, input_pos, mask=mask)\n        # out_type shape [batch, length, type_space]\n        type_h, type_c = out_type\n\n        # get vector for heads [batch, length, type_space],\n        type_h = type_h.gather(dim=1, index=heads.unsqueeze(2).expand(type_h.size()))\n        # compute output for type [batch, length, num_labels]\n        out_type = self.bilinear(type_h, type_c)\n\n        # mask invalid position to -inf for log_softmax\n        if mask is not None:\n            minus_mask = mask.eq(0).unsqueeze(2)\n            out_arc = out_arc.masked_fill(minus_mask, float(\'-inf\'))\n\n        # loss_arc shape [batch, length_c]\n        loss_arc = self.criterion(out_arc, heads)\n        loss_type = self.criterion(out_type.transpose(1, 2), types)\n\n        # mask invalid position to 0 for sum loss\n        if mask is not None:\n            loss_arc = loss_arc * mask\n            loss_type = loss_type * mask\n\n        # [batch, length - 1] -> [batch] remove the symbolic root.\n        return loss_arc[:, 1:].sum(dim=1), loss_type[:, 1:].sum(dim=1)\n\n    def _decode_types(self, out_type, heads, leading_symbolic):\n        # out_type shape [batch, length, type_space]\n        type_h, type_c = out_type\n        # get vector for heads [batch, length, type_space],\n        type_h = type_h.gather(dim=1, index=heads.unsqueeze(2).expand(type_h.size()))\n        # compute output for type [batch, length, num_labels]\n        out_type = self.bilinear(type_h, type_c)\n        # remove the first #leading_symbolic types.\n        out_type = out_type[:, :, leading_symbolic:]\n        # compute the prediction of types [batch, length]\n        _, types = out_type.max(dim=2)\n        return types + leading_symbolic\n\n    def decode_local(self, input_word, input_char, input_pos, mask=None, leading_symbolic=0):\n        # out_arc shape [batch, length_h, length_c]\n        out_arc, out_type = self(input_word, input_char, input_pos, mask=mask)\n        batch, max_len, _ = out_arc.size()\n        # set diagonal elements to -inf\n        diag_mask = torch.eye(max_len, device=out_arc.device, dtype=torch.uint8).unsqueeze(0)\n        out_arc.masked_fill_(diag_mask, float(\'-inf\'))\n        # set invalid positions to -inf\n        if mask is not None:\n            minus_mask = mask.eq(0).unsqueeze(2)\n            out_arc.masked_fill_(minus_mask, float(\'-inf\'))\n\n        # compute naive predictions.\n        # predition shape = [batch, length_c]\n        _, heads = out_arc.max(dim=1)\n\n        types = self._decode_types(out_type, heads, leading_symbolic)\n\n        return heads.cpu().numpy(), types.cpu().numpy()\n\n    def decode(self, input_word, input_char, input_pos, mask=None, leading_symbolic=0):\n        """"""\n        Args:\n            input_word: Tensor\n                the word input tensor with shape = [batch, length]\n            input_char: Tensor\n                the character input tensor with shape = [batch, length, char_length]\n            input_pos: Tensor\n                the pos input tensor with shape = [batch, length]\n            mask: Tensor or None\n                the mask tensor with shape = [batch, length]\n            length: Tensor or None\n                the length tensor with shape = [batch]\n            hx: Tensor or None\n                the initial states of RNN\n            leading_symbolic: int\n                number of symbolic labels leading in type alphabets (set it to 0 if you are not sure)\n\n        Returns: (Tensor, Tensor)\n                predicted heads and types.\n\n        """"""\n        # out_arc shape [batch, length_h, length_c]\n        out_arc, out_type = self(input_word, input_char, input_pos, mask=mask)\n\n        # out_type shape [batch, length, type_space]\n        type_h, type_c = out_type\n        batch, max_len, type_space = type_h.size()\n\n        type_h = type_h.unsqueeze(2).expand(batch, max_len, max_len, type_space).contiguous()\n        type_c = type_c.unsqueeze(1).expand(batch, max_len, max_len, type_space).contiguous()\n        # compute output for type [batch, length_h, length_c, num_labels]\n        out_type = self.bilinear(type_h, type_c)\n\n        if mask is not None:\n            minus_mask = mask.eq(0).unsqueeze(2)\n            out_arc.masked_fill_(minus_mask, float(\'-inf\'))\n        # loss_arc shape [batch, length_h, length_c]\n        loss_arc = F.log_softmax(out_arc, dim=1)\n        # loss_type shape [batch, length_h, length_c, num_labels]\n        loss_type = F.log_softmax(out_type, dim=3).permute(0, 3, 1, 2)\n        # [batch, num_labels, length_h, length_c]\n        energy = loss_arc.unsqueeze(1) + loss_type\n\n        # compute lengths\n        length = mask.sum(dim=1).long().cpu().numpy()\n        return parser.decode_MST(energy.cpu().numpy(), length, leading_symbolic=leading_symbolic, labeled=True)\n\n\nclass NeuroMST(DeepBiAffine):\n    def __init__(self, word_dim, num_words, char_dim, num_chars, pos_dim, num_pos, rnn_mode, hidden_size, num_layers, num_labels, arc_space, type_space,\n                 embedd_word=None, embedd_char=None, embedd_pos=None, p_in=0.33, p_out=0.33, p_rnn=(0.33, 0.33), pos=True, activation=\'elu\'):\n        super(NeuroMST, self).__init__(word_dim, num_words, char_dim, num_chars, pos_dim, num_pos, rnn_mode, hidden_size, num_layers, num_labels, arc_space, type_space,\n                                       embedd_word=embedd_word, embedd_char=embedd_char, embedd_pos=embedd_pos, p_in=p_in, p_out=p_out, p_rnn=p_rnn, pos=pos, activation=activation)\n        self.biaffine = None\n        self.treecrf = TreeCRF(arc_space)\n\n    def forward(self, input_word, input_char, input_pos, mask=None):\n        # output from rnn [batch, length, dim]\n        arc, type = self._get_rnn_output(input_word, input_char, input_pos, mask=mask)\n        # [batch, length_head, length_child]\n        out_arc = self.treecrf(arc[0], arc[1], mask=mask)\n        return out_arc, type\n\n    @overrides\n    def loss(self, input_word, input_char, input_pos, heads, types, mask=None):\n        # output from rnn [batch, length, dim]\n        arc, out_type = self._get_rnn_output(input_word, input_char, input_pos, mask=mask)\n        # [batch]\n        loss_arc = self.treecrf.loss(arc[0], arc[1], heads, mask=mask)\n        # out_type shape [batch, length, type_space]\n        type_h, type_c = out_type\n\n        # get vector for heads [batch, length, type_space],\n        type_h = type_h.gather(dim=1, index=heads.unsqueeze(2).expand(type_h.size()))\n        # compute output for type [batch, length, num_labels]\n        out_type = self.bilinear(type_h, type_c)\n        loss_type = self.criterion(out_type.transpose(1, 2), types)\n\n        # mask invalid position to 0 for sum loss\n        if mask is not None:\n            loss_type = loss_type * mask\n\n        return loss_arc, loss_type[:, 1:].sum(dim=1)\n\n    @overrides\n    def decode(self, input_word, input_char, input_pos, mask=None, leading_symbolic=0):\n        """"""\n        Args:\n            input_word: Tensor\n                the word input tensor with shape = [batch, length]\n            input_char: Tensor\n                the character input tensor with shape = [batch, length, char_length]\n            input_pos: Tensor\n                the pos input tensor with shape = [batch, length]\n            mask: Tensor or None\n                the mask tensor with shape = [batch, length]\n            length: Tensor or None\n                the length tensor with shape = [batch]\n            hx: Tensor or None\n                the initial states of RNN\n            leading_symbolic: int\n                number of symbolic labels leading in type alphabets (set it to 0 if you are not sure)\n\n        Returns: (Tensor, Tensor)\n                predicted heads and types.\n\n        """"""\n        # out_arc shape [batch, length_h, length_c]\n        energy, out_type = self(input_word, input_char, input_pos, mask=mask)\n        # compute lengths\n        length = mask.sum(dim=1).long()\n        heads, _ = parser.decode_MST(energy.cpu().numpy(), length.cpu().numpy(), leading_symbolic=leading_symbolic, labeled=False)\n        types = self._decode_types(out_type, torch.from_numpy(heads).type_as(length), leading_symbolic)\n        return heads, types.cpu().numpy()\n\n\nclass StackPtrNet(nn.Module):\n    def __init__(self, word_dim, num_words, char_dim, num_chars, pos_dim, num_pos, rnn_mode, hidden_size,\n                 encoder_layers, decoder_layers, num_labels, arc_space, type_space,\n                 embedd_word=None, embedd_char=None, embedd_pos=None, p_in=0.33, p_out=0.33, p_rnn=(0.33, 0.33),\n                 pos=True, prior_order=\'inside_out\', grandPar=False, sibling=False, activation=\'elu\'):\n\n        super(StackPtrNet, self).__init__()\n        self.word_embed = nn.Embedding(num_words, word_dim, _weight=embedd_word, padding_idx=1)\n        self.pos_embed = nn.Embedding(num_pos, pos_dim, _weight=embedd_pos, padding_idx=1) if pos else None\n        self.char_embed = nn.Embedding(num_chars, char_dim, _weight=embedd_char, padding_idx=1)\n        self.char_cnn = CharCNN(2, char_dim, char_dim, hidden_channels=char_dim * 4, activation=activation)\n\n        self.dropout_in = nn.Dropout2d(p=p_in)\n        self.dropout_out = nn.Dropout2d(p=p_out)\n        self.num_labels = num_labels\n\n        if prior_order in [\'deep_first\', \'shallow_first\']:\n            self.prior_order = PriorOrder.DEPTH\n        elif prior_order == \'inside_out\':\n            self.prior_order = PriorOrder.INSIDE_OUT\n        elif prior_order == \'left2right\':\n            self.prior_order = PriorOrder.LEFT2RIGTH\n        else:\n            raise ValueError(\'Unknown prior order: %s\' % prior_order)\n\n        self.grandPar = grandPar\n        self.sibling = sibling\n\n        if rnn_mode == \'RNN\':\n            RNN_ENCODER = VarRNN\n            RNN_DECODER = VarRNN\n        elif rnn_mode == \'LSTM\':\n            RNN_ENCODER = VarLSTM\n            RNN_DECODER = VarLSTM\n        elif rnn_mode == \'FastLSTM\':\n            RNN_ENCODER = VarFastLSTM\n            RNN_DECODER = VarFastLSTM\n        elif rnn_mode == \'GRU\':\n            RNN_ENCODER = VarGRU\n            RNN_DECODER = VarGRU\n        else:\n            raise ValueError(\'Unknown RNN mode: %s\' % rnn_mode)\n\n        dim_enc = word_dim + char_dim\n        if pos:\n            dim_enc += pos_dim\n\n        self.encoder_layers = encoder_layers\n        self.encoder = RNN_ENCODER(dim_enc, hidden_size, num_layers=encoder_layers, batch_first=True, bidirectional=True, dropout=p_rnn)\n\n        dim_dec = hidden_size // 2\n        self.src_dense = nn.Linear(2 * hidden_size, dim_dec)\n        self.decoder_layers = decoder_layers\n        self.decoder = RNN_DECODER(dim_dec, hidden_size, num_layers=decoder_layers, batch_first=True, bidirectional=False, dropout=p_rnn)\n\n        self.hx_dense = nn.Linear(2 * hidden_size, hidden_size)\n\n        self.arc_h = nn.Linear(hidden_size, arc_space) # arc dense for decoder\n        self.arc_c = nn.Linear(hidden_size * 2, arc_space)  # arc dense for encoder\n        self.biaffine = BiAffine(arc_space, arc_space)\n\n        self.type_h = nn.Linear(hidden_size, type_space) # type dense for decoder\n        self.type_c = nn.Linear(hidden_size * 2, type_space)  # type dense for encoder\n        self.bilinear = BiLinear(type_space, type_space, self.num_labels)\n\n        assert activation in [\'elu\', \'tanh\']\n        if activation == \'elu\':\n            self.activation = nn.ELU(inplace=True)\n        else:\n            self.activation = nn.Tanh()\n\n        self.criterion = nn.CrossEntropyLoss(reduction=\'none\')\n        self.reset_parameters(embedd_word, embedd_char, embedd_pos)\n\n    def reset_parameters(self, embedd_word, embedd_char, embedd_pos):\n        if embedd_word is None:\n            nn.init.uniform_(self.word_embed.weight, -0.1, 0.1)\n        if embedd_char is None:\n            nn.init.uniform_(self.char_embed.weight, -0.1, 0.1)\n        if embedd_pos is None and self.pos_embed is not None:\n            nn.init.uniform_(self.pos_embed.weight, -0.1, 0.1)\n\n        with torch.no_grad():\n            self.word_embed.weight[self.word_embed.padding_idx].fill_(0)\n            self.char_embed.weight[self.char_embed.padding_idx].fill_(0)\n            if self.pos_embed is not None:\n                self.pos_embed.weight[self.pos_embed.padding_idx].fill_(0)\n\n        nn.init.xavier_uniform_(self.arc_h.weight)\n        nn.init.constant_(self.arc_h.bias, 0.)\n        nn.init.xavier_uniform_(self.arc_c.weight)\n        nn.init.constant_(self.arc_c.bias, 0.)\n\n        nn.init.xavier_uniform_(self.type_h.weight)\n        nn.init.constant_(self.type_h.bias, 0.)\n        nn.init.xavier_uniform_(self.type_c.weight)\n        nn.init.constant_(self.type_c.bias, 0.)\n\n    def _get_encoder_output(self, input_word, input_char, input_pos, mask=None):\n        # [batch, length, word_dim]\n        word = self.word_embed(input_word)\n\n        # [batch, length, char_length, char_dim]\n        char = self.char_cnn(self.char_embed(input_char))\n\n        # apply dropout word on input\n        word = self.dropout_in(word)\n        char = self.dropout_in(char)\n\n        # concatenate word and char [batch, length, word_dim+char_filter]\n        enc = torch.cat([word, char], dim=2)\n\n        if self.pos_embed is not None:\n            # [batch, length, pos_dim]\n            pos = self.pos_embed(input_pos)\n            # apply dropout on input\n            pos = self.dropout_in(pos)\n            enc = torch.cat([enc, pos], dim=2)\n\n        # output from rnn [batch, length, hidden_size]\n        output, hn = self.encoder(enc, mask)\n        # apply dropout\n        # [batch, length, hidden_size] --> [batch, hidden_size, length] --> [batch, length, hidden_size]\n        output = self.dropout_out(output.transpose(1, 2)).transpose(1, 2)\n\n        return output, hn\n\n    def _get_decoder_output(self, output_enc, heads, heads_stack, siblings, hx, mask=None):\n        # get vector for heads [batch, length_decoder, input_dim],\n        enc_dim = output_enc.size(2)\n        batch, length_dec = heads_stack.size()\n        src_encoding = output_enc.gather(dim=1, index=heads_stack.unsqueeze(2).expand(batch, length_dec, enc_dim))\n\n        if self.sibling:\n            # [batch, length_decoder, hidden_size * 2]\n            mask_sib = siblings.gt(0).float().unsqueeze(2)\n            output_enc_sibling = output_enc.gather(dim=1, index=siblings.unsqueeze(2).expand(batch, length_dec, enc_dim)) * mask_sib\n            src_encoding = src_encoding + output_enc_sibling\n\n        if self.grandPar:\n            # [batch, length_decoder, 1]\n            gpars = heads.gather(dim=1, index=heads_stack).unsqueeze(2)\n            # mask_gpar = gpars.ge(0).float()\n            # [batch, length_decoder, hidden_size * 2]\n            output_enc_gpar = output_enc.gather(dim=1, index=gpars.expand(batch, length_dec, enc_dim)) #* mask_gpar\n            src_encoding = src_encoding + output_enc_gpar\n\n        # transform to decoder input\n        # [batch, length_decoder, dec_dim]\n        src_encoding = self.activation(self.src_dense(src_encoding))\n        # output from rnn [batch, length, hidden_size]\n        output, hn = self.decoder(src_encoding, mask, hx=hx)\n        # apply dropout\n        # [batch, length, hidden_size] --> [batch, hidden_size, length] --> [batch, length, hidden_size]\n        output = self.dropout_out(output.transpose(1, 2)).transpose(1, 2)\n\n        return output, hn\n\n    def forward(self, input_word, input_char, input_pos, mask=None, length=None, hx=None):\n        raise RuntimeError(\'Stack Pointer Network does not implement forward\')\n\n    def _transform_decoder_init_state(self, hn):\n        if isinstance(hn, tuple):\n            hn, cn = hn\n            _, batch, hidden_size = cn.size()\n            # take the last layers\n            # [batch, 2 * hidden_size] --> [1, batch, 2 * hidden_size]\n            cn = torch.cat([cn[-2], cn[-1]], dim=1).unsqueeze(0)\n            # take hx_dense to [1, batch, hidden_size]\n            cn = self.hx_dense(cn)\n            # [decoder_layers, batch, hidden_size]\n            if self.decoder_layers > 1:\n                cn = torch.cat([cn, cn.new_zeros(self.decoder_layers - 1, batch, hidden_size)], dim=0)\n            # hn is tanh(cn)\n            hn = torch.tanh(cn)\n            hn = (hn, cn)\n        else:\n            # take the last layers\n            # [2, batch, hidden_size]\n            hn = hn[-2:]\n            # hn [2, batch, hidden_size]\n            _, batch, hidden_size = hn.size()\n            # first convert hn t0 [batch, 2, hidden_size]\n            hn = hn.transpose(0, 1).contiguous()\n            # then view to [batch, 1, 2 * hidden_size] --> [1, batch, 2 * hidden_size]\n            hn = hn.view(batch, 1, 2 * hidden_size).transpose(0, 1)\n            # take hx_dense to [1, batch, hidden_size]\n            hn = torch.tanh(self.hx_dense(hn))\n            # [decoder_layers, batch, hidden_size]\n            if self.decoder_layers > 1:\n                hn = torch.cat([hn, hn.new_zeros(self.decoder_layers - 1, batch, hidden_size)], dim=0)\n        return hn\n\n    def loss(self, input_word, input_char, input_pos, heads, stacked_heads, children, siblings, stacked_types, mask_e=None, mask_d=None):\n        # output from encoder [batch, length_encoder, hidden_size]\n        output_enc, hn = self._get_encoder_output(input_word, input_char, input_pos, mask=mask_e)\n\n        # output size [batch, length_encoder, arc_space]\n        arc_c = self.activation(self.arc_c(output_enc))\n        # output size [batch, length_encoder, type_space]\n        type_c = self.activation(self.type_c(output_enc))\n\n        # transform hn to [decoder_layers, batch, hidden_size]\n        hn = self._transform_decoder_init_state(hn)\n\n        # output from decoder [batch, length_decoder, tag_space]\n        output_dec, _ = self._get_decoder_output(output_enc, heads, stacked_heads, siblings, hn, mask=mask_d)\n\n        # output size [batch, length_decoder, arc_space]\n        arc_h = self.activation(self.arc_h(output_dec))\n        type_h = self.activation(self.type_h(output_dec))\n\n        batch, max_len_d, type_space = type_h.size()\n        # apply dropout\n        # [batch, length_decoder, dim] + [batch, length_encoder, dim] --> [batch, length_decoder + length_encoder, dim]\n        arc = self.dropout_out(torch.cat([arc_h, arc_c], dim=1).transpose(1, 2)).transpose(1, 2)\n        arc_h = arc[:, :max_len_d]\n        arc_c = arc[:, max_len_d:]\n\n        type = self.dropout_out(torch.cat([type_h, type_c], dim=1).transpose(1, 2)).transpose(1, 2)\n        type_h = type[:, :max_len_d].contiguous()\n        type_c = type[:, max_len_d:]\n\n        # [batch, length_decoder, length_encoder]\n        out_arc = self.biaffine(arc_h, arc_c, mask_query=mask_d, mask_key=mask_e)\n\n        # get vector for heads [batch, length_decoder, type_space],\n        type_c = type_c.gather(dim=1, index=children.unsqueeze(2).expand(batch, max_len_d, type_space))\n        # compute output for type [batch, length_decoder, num_labels]\n        out_type = self.bilinear(type_h, type_c)\n\n        # mask invalid position to -inf for log_softmax\n        if mask_e is not None:\n            minus_mask_e = mask_e.eq(0).unsqueeze(1)\n            minus_mask_d = mask_d.eq(0).unsqueeze(2)\n            out_arc = out_arc.masked_fill(minus_mask_d * minus_mask_e, float(\'-inf\'))\n\n        # loss_arc shape [batch, length_decoder]\n        loss_arc = self.criterion(out_arc.transpose(1, 2), children)\n        loss_type = self.criterion(out_type.transpose(1, 2), stacked_types)\n\n        if mask_d is not None:\n            loss_arc = loss_arc * mask_d\n            loss_type = loss_type * mask_d\n\n        return loss_arc.sum(dim=1), loss_type.sum(dim=1)\n\n    def decode(self, input_word, input_char, input_pos, mask=None, beam=1, leading_symbolic=0):\n        # reset noise for decoder\n        self.decoder.reset_noise(0)\n\n        # output_enc [batch, length, model_dim]\n        # arc_c [batch, length, arc_space]\n        # type_c [batch, length, type_space]\n        # hn [num_direction, batch, hidden_size]\n        output_enc, hn = self._get_encoder_output(input_word, input_char, input_pos, mask=mask)\n        enc_dim = output_enc.size(2)\n        device = output_enc.device\n        # output size [batch, length_encoder, arc_space]\n        arc_c = self.activation(self.arc_c(output_enc))\n        # output size [batch, length_encoder, type_space]\n        type_c = self.activation(self.type_c(output_enc))\n        type_space = type_c.size(2)\n        # [decoder_layers, batch, hidden_size]\n        hn = self._transform_decoder_init_state(hn)\n        batch, max_len, _ = output_enc.size()\n\n        heads = torch.zeros(batch, 1, max_len, device=device, dtype=torch.int64)\n        types = torch.zeros(batch, 1, max_len, device=device, dtype=torch.int64)\n\n        num_steps = 2 * max_len - 1\n        stacked_heads = torch.zeros(batch, 1, num_steps + 1, device=device, dtype=torch.int64)\n        siblings = torch.zeros(batch, 1, num_steps + 1, device=device, dtype=torch.int64) if self.sibling else None\n        hypothesis_scores = output_enc.new_zeros((batch, 1))\n\n        # [batch, beam, length]\n        children = torch.arange(max_len, device=device, dtype=torch.int64).view(1, 1, max_len).expand(batch, beam, max_len)\n        constraints = torch.zeros(batch, 1, max_len, device=device, dtype=torch.bool)\n        constraints[:, :, 0] = True\n        # [batch, 1]\n        batch_index = torch.arange(batch, device=device, dtype=torch.int64).view(batch, 1)\n\n        # compute lengths\n        if mask is None:\n            steps = torch.new_tensor([num_steps] * batch, dtype=torch.int64, device=device)\n            mask_sent = torch.ones(batch, 1, max_len, dtype=torch.bool, device=device)\n        else:\n            steps = (mask.sum(dim=1) * 2 - 1).long()\n            mask_sent = mask.unsqueeze(1).bool()\n\n        num_hyp = 1\n        mask_hyp = torch.ones(batch, 1, device=device)\n        hx = hn\n        for t in range(num_steps):\n            # [batch, num_hyp]\n            curr_heads = stacked_heads[:, :, t]\n            curr_gpars = heads.gather(dim=2, index=curr_heads.unsqueeze(2)).squeeze(2)\n            curr_sibs = siblings[:, :, t] if self.sibling else None\n            # [batch, num_hyp, enc_dim]\n            src_encoding = output_enc.gather(dim=1, index=curr_heads.unsqueeze(2).expand(batch, num_hyp, enc_dim))\n\n            if self.sibling:\n                mask_sib = curr_sibs.gt(0).float().unsqueeze(2)\n                output_enc_sibling = output_enc.gather(dim=1, index=curr_sibs.unsqueeze(2).expand(batch, num_hyp, enc_dim)) * mask_sib\n                src_encoding = src_encoding + output_enc_sibling\n\n            if self.grandPar:\n                output_enc_gpar = output_enc.gather(dim=1, index=curr_gpars.unsqueeze(2).expand(batch, num_hyp, enc_dim))\n                src_encoding = src_encoding + output_enc_gpar\n\n            # transform to decoder input\n            # [batch, num_hyp, dec_dim]\n            src_encoding = self.activation(self.src_dense(src_encoding))\n\n            # output [batch * num_hyp, dec_dim]\n            # hx [decoder_layer, batch * num_hyp, dec_dim]\n            output_dec, hx = self.decoder.step(src_encoding.view(batch * num_hyp, -1), hx=hx)\n            dec_dim = output_dec.size(1)\n            # [batch, num_hyp, dec_dim]\n            output_dec = output_dec.view(batch, num_hyp, dec_dim)\n\n            # [batch, num_hyp, arc_space]\n            arc_h = self.activation(self.arc_h(output_dec))\n            # [batch, num_hyp, type_space]\n            type_h = self.activation(self.type_h(output_dec))\n            # [batch, num_hyp, length]\n            out_arc = self.biaffine(arc_h, arc_c, mask_query=mask_hyp, mask_key=mask)\n            # mask invalid position to -inf for log_softmax\n            if mask is not None:\n                minus_mask_enc = mask.eq(0).unsqueeze(1)\n                out_arc.masked_fill_(minus_mask_enc, float(\'-inf\'))\n\n            # [batch]\n            mask_last = steps.le(t + 1)\n            mask_stop = steps.le(t)\n            minus_mask_hyp = mask_hyp.eq(0).unsqueeze(2)\n            # [batch, num_hyp, length]\n            hyp_scores = F.log_softmax(out_arc, dim=2).masked_fill_(mask_stop.view(batch, 1, 1) + minus_mask_hyp, 0)\n            # [batch, num_hyp, length]\n            hypothesis_scores = hypothesis_scores.unsqueeze(2) + hyp_scores\n\n            # [batch, num_hyp, length]\n            mask_leaf = curr_heads.unsqueeze(2).eq(children[:, :num_hyp]) * mask_sent\n            mask_non_leaf = (~mask_leaf) * mask_sent\n\n            # apply constrains to select valid hyps\n            # [batch, num_hyp, length]\n            mask_leaf = mask_leaf * (mask_last.unsqueeze(1) + curr_heads.ne(0)).unsqueeze(2)\n            mask_non_leaf = mask_non_leaf * (~constraints)\n\n            hypothesis_scores.masked_fill_(~(mask_non_leaf + mask_leaf), float(\'-inf\'))\n            # [batch, num_hyp * length]\n            hypothesis_scores, hyp_index = torch.sort(hypothesis_scores.view(batch, -1), dim=1, descending=True)\n\n            # [batch]\n            prev_num_hyp = num_hyp\n            num_hyps = (mask_leaf + mask_non_leaf).long().view(batch, -1).sum(dim=1)\n            num_hyp = num_hyps.max().clamp(max=beam).item()\n            # [batch, hum_hyp]\n            hyps = torch.arange(num_hyp, device=device, dtype=torch.int64).view(1, num_hyp)\n            mask_hyp = hyps.lt(num_hyps.unsqueeze(1)).float()\n\n            # [batch, num_hyp]\n            hypothesis_scores = hypothesis_scores[:, :num_hyp]\n            hyp_index = hyp_index[:, :num_hyp]\n            base_index = hyp_index / max_len\n            child_index = hyp_index % max_len\n\n            # [batch, num_hyp]\n            hyp_heads = curr_heads.gather(dim=1, index=base_index)\n            hyp_gpars = curr_gpars.gather(dim=1, index=base_index)\n\n            # [batch, num_hyp, length]\n            base_index_expand = base_index.unsqueeze(2).expand(batch, num_hyp, max_len)\n            constraints = constraints.gather(dim=1, index=base_index_expand)\n            constraints.scatter_(2, child_index.unsqueeze(2), True)\n\n            # [batch, num_hyp]\n            mask_leaf = hyp_heads.eq(child_index)\n            # [batch, num_hyp, length]\n            heads = heads.gather(dim=1, index=base_index_expand)\n            heads.scatter_(2, child_index.unsqueeze(2), torch.where(mask_leaf, hyp_gpars, hyp_heads).unsqueeze(2))\n            types = types.gather(dim=1, index=base_index_expand)\n            # [batch, num_hyp]\n            org_types = types.gather(dim=2, index=child_index.unsqueeze(2)).squeeze(2)\n\n            # [batch, num_hyp, num_steps]\n            base_index_expand = base_index.unsqueeze(2).expand(batch, num_hyp, num_steps + 1)\n            stacked_heads = stacked_heads.gather(dim=1, index=base_index_expand)\n            stacked_heads[:, :, t + 1] = torch.where(mask_leaf, hyp_gpars, child_index)\n            if self.sibling:\n                siblings = siblings.gather(dim=1, index=base_index_expand)\n                siblings[:, :, t + 1] = torch.where(mask_leaf, child_index, torch.zeros_like(child_index))\n\n            # [batch, num_hyp, type_space]\n            base_index_expand = base_index.unsqueeze(2).expand(batch, num_hyp, type_space)\n            child_index_expand = child_index.unsqueeze(2).expand(batch, num_hyp, type_space)\n            # [batch, num_hyp, num_labels]\n            out_type = self.bilinear(type_h.gather(dim=1, index=base_index_expand), type_c.gather(dim=1, index=child_index_expand))\n            hyp_type_scores = F.log_softmax(out_type, dim=2)\n            # compute the prediction of types [batch, num_hyp]\n            hyp_type_scores, hyp_types = hyp_type_scores.max(dim=2)\n            hypothesis_scores = hypothesis_scores + hyp_type_scores.masked_fill_(mask_stop.view(batch, 1), 0)\n            types.scatter_(2, child_index.unsqueeze(2), torch.where(mask_leaf, org_types, hyp_types).unsqueeze(2))\n\n            # hx [decoder_layer, batch * num_hyp, dec_dim]\n            # hack to handle LSTM\n            hx_index = (base_index + batch_index * prev_num_hyp).view(batch * num_hyp)\n            if isinstance(hx, tuple):\n                hx, cx = hx\n                hx = hx[:, hx_index]\n                cx = cx[:, hx_index]\n                hx = (hx, cx)\n            else:\n                hx = hx[:, hx_index]\n\n        heads = heads[:, 0].cpu().numpy()\n        types = types[:, 0].cpu().numpy()\n        return heads, types\n\n\n\n\n'"
neuronlp2/models/sequence_labeling.py,6,"b""__author__ = 'max'\n\nfrom overrides import overrides\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\nfrom neuronlp2.nn import ChainCRF, VarGRU, VarRNN, VarLSTM, VarFastLSTM, CharCNN\n\n\nclass BiRecurrentConv(nn.Module):\n    def __init__(self, word_dim, num_words, char_dim, num_chars, rnn_mode, hidden_size, out_features, num_layers,\n                 num_labels, embedd_word=None, embedd_char=None, p_in=0.33, p_out=0.5, p_rnn=(0.5, 0.5), activation='elu'):\n        super(BiRecurrentConv, self).__init__()\n\n        self.word_embed = nn.Embedding(num_words, word_dim, _weight=embedd_word, padding_idx=1)\n        self.char_embed = nn.Embedding(num_chars, char_dim, _weight=embedd_char, padding_idx=1)\n        self.char_cnn = CharCNN(2, char_dim, char_dim, hidden_channels=4 * char_dim, activation=activation)\n        # dropout word\n        self.dropout_in = nn.Dropout2d(p=p_in)\n        # standard dropout\n        self.dropout_rnn_in = nn.Dropout(p=p_rnn[0])\n        self.dropout_out = nn.Dropout(p_out)\n\n        if rnn_mode == 'RNN':\n            RNN = nn.RNN\n        elif rnn_mode == 'LSTM' or rnn_mode == 'FastLSTM':\n            RNN = nn.LSTM\n        elif rnn_mode == 'GRU':\n            RNN = nn.GRU\n        else:\n            raise ValueError('Unknown RNN mode: %s' % rnn_mode)\n\n        self.rnn = RNN(word_dim + char_dim, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True, dropout=p_rnn[1])\n\n        self.fc = nn.Linear(hidden_size * 2, out_features)\n        assert activation in ['elu', 'tanh']\n        if activation == 'elu':\n            self.activation = nn.ELU()\n        else:\n            self.activation = nn.Tanh()\n        self.readout = nn.Linear(out_features, num_labels)\n        self.criterion = nn.CrossEntropyLoss(reduction='none')\n\n        self.reset_parameters(embedd_word, embedd_char)\n\n    def reset_parameters(self, embedd_word, embedd_char):\n        if embedd_word is None:\n            nn.init.uniform_(self.word_embed.weight, -0.1, 0.1)\n        if embedd_char is None:\n            nn.init.uniform_(self.char_embed.weight, -0.1, 0.1)\n        with torch.no_grad():\n            self.word_embed.weight[self.word_embed.padding_idx].fill_(0)\n            self.char_embed.weight[self.char_embed.padding_idx].fill_(0)\n\n        for param in self.rnn.parameters():\n            if param.dim() == 1:\n                nn.init.constant_(param, 0)\n            else:\n                nn.init.xavier_uniform_(param)\n\n        nn.init.xavier_uniform_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0.)\n\n        nn.init.uniform_(self.readout.weight, -0.1, 0.1)\n        nn.init.constant_(self.readout.bias, 0.)\n\n    def _get_rnn_output(self, input_word, input_char, mask=None):\n        # [batch, length, word_dim]\n        word = self.word_embed(input_word)\n\n        # [batch, length, char_length, char_dim]\n        char = self.char_cnn(self.char_embed(input_char))\n\n        # apply dropout word on input\n        word = self.dropout_in(word)\n        char = self.dropout_in(char)\n\n        # concatenate word and char [batch, length, word_dim+char_filter]\n        enc = torch.cat([word, char], dim=2)\n        # apply dropout rnn input\n        enc = self.dropout_rnn_in(enc)\n\n        # output from rnn [batch, length, hidden_size * 2]\n        if mask is not None:\n            # prepare packed_sequence\n            length = mask.sum(dim=1).long()\n            packed_enc = pack_padded_sequence(enc, length, batch_first=True, enforce_sorted=False)\n            packed_out, _ = self.rnn(packed_enc)\n            output, _ = pad_packed_sequence(packed_out, batch_first=True)\n        else:\n            output, _ = self.rnn(enc)\n\n        output = self.dropout_out(output)\n        # [batch, length, out_features]\n        output = self.dropout_out(self.activation(self.fc(output)))\n        return output\n\n    def forward(self, input_word, input_char, mask=None):\n        # output from rnn [batch, length, out_features]\n        output = self._get_rnn_output(input_word, input_char, mask=mask)\n        return output\n\n    def loss(self, input_word, input_char, target, mask=None):\n        # [batch, length, out_features]\n        output = self(input_word, input_char, mask=mask)\n        # [batch, length, num_labels] -> [batch, num_labels, length]\n        logits = self.readout(output).transpose(1, 2)\n\n        # [batch, length]\n        loss = self.criterion(logits, target)\n        if mask is not None:\n            loss = loss * mask\n        # [batch]\n        loss = loss.sum(dim=1)\n        return loss\n\n    def decode(self, input_word, input_char, mask=None, leading_symbolic=0):\n        output = self(input_word, input_char, mask=mask)\n        # [batch, length, num_labels] -> [batch, num_labels, length]\n        logits = self.readout(output).transpose(1, 2)\n        # [batch, length]\n        _, preds = torch.max(logits[:, leading_symbolic:], dim=1)\n        preds += leading_symbolic\n        if mask is not None:\n            preds = preds * mask.long()\n        return preds\n\n\nclass BiVarRecurrentConv(BiRecurrentConv):\n    def __init__(self, word_dim, num_words, char_dim, num_chars, rnn_mode, hidden_size, out_features, num_layers,\n                 num_labels, embedd_word=None, embedd_char=None, p_in=0.33, p_out=0.33, p_rnn=(0.33, 0.33), activation='elu'):\n        super(BiVarRecurrentConv, self).__init__(word_dim, num_words, char_dim, num_chars, rnn_mode, hidden_size, out_features, num_layers,\n                                                 num_labels, embedd_word=embedd_word, embedd_char=embedd_char,\n                                                 p_in=p_in, p_out=p_out, p_rnn=p_rnn, activation=activation)\n\n        self.dropout_rnn_in = None\n        self.dropout_out = nn.Dropout2d(p_out)\n\n        if rnn_mode == 'RNN':\n            RNN = VarRNN\n        elif rnn_mode == 'LSTM':\n            RNN = VarLSTM\n        elif rnn_mode == 'FastLSTM':\n            RNN = VarFastLSTM\n        elif rnn_mode == 'GRU':\n            RNN = VarGRU\n        else:\n            raise ValueError('Unknown RNN mode: %s' % rnn_mode)\n\n        self.rnn = RNN(word_dim + char_dim, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True, dropout=p_rnn)\n\n    @overrides\n    def _get_rnn_output(self, input_word, input_char, mask=None):\n        # [batch, length, word_dim]\n        word = self.word_embed(input_word)\n\n        # [batch, length, char_length, char_dim]\n        char = self.char_cnn(self.char_embed(input_char))\n\n        # apply dropout word on input\n        word = self.dropout_in(word)\n        char = self.dropout_in(char)\n\n        # concatenate word and char [batch, length, word_dim+char_filter]\n        enc = torch.cat([word, char], dim=2)\n        # output from rnn [batch, length, 2 * hidden_size]\n        output, _ = self.rnn(enc, mask)\n\n        # apply dropout for the output of rnn\n        # [batch, length, 2 * hidden_size] --> [batch, 2 * hidden_size, length] --> [batch, length, 2 * hidden_size]\n        output = self.dropout_out(output.transpose(1, 2)).transpose(1, 2)\n        # [batch, length, out_features]\n        output = self.activation(self.fc(output))\n        # [batch, length, out_features] --> [batch, out_features, length] --> [batch, length, out_features]\n        output = self.dropout_out(output.transpose(1, 2)).transpose(1, 2)\n        return output\n\n\nclass BiRecurrentConvCRF(BiRecurrentConv):\n    def __init__(self, word_dim, num_words, char_dim, num_chars, rnn_mode, hidden_size, out_features, num_layers,\n                 num_labels, embedd_word=None, embedd_char=None, p_in=0.33, p_out=0.5, p_rnn=(0.5, 0.5), bigram=False, activation='elu'):\n        super(BiRecurrentConvCRF, self).__init__(word_dim, num_words, char_dim, num_chars, rnn_mode, hidden_size, out_features, num_layers,\n                                                 num_labels, embedd_word=embedd_word, embedd_char=embedd_char,\n                                                 p_in=p_in, p_out=p_out, p_rnn=p_rnn, activation=activation)\n\n        self.crf = ChainCRF(out_features, num_labels, bigram=bigram)\n        self.readout = None\n        self.criterion = None\n\n    def forward(self, input_word, input_char, mask=None):\n        # output from rnn [batch, length, hidden_size]\n        output = self._get_rnn_output(input_word, input_char, mask=mask)\n        # [batch, length, num_label, num_label]\n        return self.crf(output, mask=mask)\n\n    @overrides\n    def loss(self, input_word, input_char, target, mask=None):\n        # output from rnn [batch, length, hidden_size]\n        output = self._get_rnn_output(input_word, input_char, mask=mask)\n        # [batch]\n        return self.crf.loss(output, target, mask=mask)\n\n    @overrides\n    def decode(self, input_word, input_char, mask=None, leading_symbolic=0):\n        # output from rnn [batch, length, hidden_size]\n        output = self._get_rnn_output(input_word, input_char, mask=mask)\n        # [batch, length]\n        preds = self.crf.decode(output, mask=mask, leading_symbolic=leading_symbolic)\n        if mask is not None:\n            preds = preds * mask.long()\n        return preds\n\n\nclass BiVarRecurrentConvCRF(BiVarRecurrentConv):\n    def __init__(self, word_dim, num_words, char_dim, num_chars, rnn_mode, hidden_size, out_features, num_layers,\n                 num_labels, embedd_word=None, embedd_char=None, p_in=0.33, p_out=0.33, p_rnn=(0.33, 0.33), bigram=False, activation='elu'):\n        super(BiVarRecurrentConvCRF, self).__init__(word_dim, num_words, char_dim, num_chars, rnn_mode, hidden_size, out_features, num_layers,\n                                                    num_labels, embedd_word=embedd_word, embedd_char=embedd_char,\n                                                    p_in=p_in, p_out=p_out, p_rnn=p_rnn, activation=activation)\n\n        self.crf = ChainCRF(out_features, num_labels, bigram=bigram)\n        self.readout = None\n        self.criterion = None\n\n    def forward(self, input_word, input_char, mask=None):\n        # output from rnn [batch, length, hidden_size]\n        output = self._get_rnn_output(input_word, input_char, mask=mask)\n        # [batch, length, num_label,  num_label]\n        return self.crf(output, mask=mask)\n\n    @overrides\n    def loss(self, input_word, input_char, target, mask=None):\n        # output from rnn [batch, length, hidden_size]\n        output = self._get_rnn_output(input_word, input_char, mask=mask)\n        # [batch]\n        return self.crf.loss(output, target, mask=mask)\n\n    @overrides\n    def decode(self, input_word, input_char, mask=None, leading_symbolic=0):\n        # output from rnn [batch, length, hidden_size]\n        output = self._get_rnn_output(input_word, input_char, mask=mask)\n        preds = self.crf.decode(output, mask=mask, leading_symbolic=leading_symbolic)\n        if mask is not None:\n            preds = preds * mask.long()\n        return preds\n"""
neuronlp2/nn/__init__.py,0,"b""__author__ = 'max'\n\nfrom neuronlp2.nn import init\nfrom neuronlp2.nn.crf import ChainCRF, TreeCRF\nfrom neuronlp2.nn.modules import BiLinear, BiAffine, CharCNN\nfrom neuronlp2.nn.variational_rnn import *\nfrom neuronlp2.nn.skip_rnn import *\n"""
neuronlp2/nn/crf.py,16,"b'__author__ = \'max\'\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.nn.parameter import Parameter\nfrom neuronlp2.nn.modules import BiAffine\n\n\nclass ChainCRF(nn.Module):\n    def __init__(self, input_size, num_labels, bigram=True):\n        \'\'\'\n\n        Args:\n            input_size: int\n                the dimension of the input.\n            num_labels: int\n                the number of labels of the crf layer\n            bigram: bool\n                if apply bi-gram parameter.\n        \'\'\'\n        super(ChainCRF, self).__init__()\n        self.input_size = input_size\n        self.num_labels = num_labels + 1\n        self.pad_label_id = num_labels\n        self.bigram = bigram\n\n\n        # state weight tensor\n        self.state_net = nn.Linear(input_size, self.num_labels)\n        if bigram:\n            # transition weight tensor\n            self.transition_net = nn.Linear(input_size, self.num_labels * self.num_labels)\n            self.register_parameter(\'transition_matrix\', None)\n        else:\n            self.transition_net = None\n            self.transition_matrix = Parameter(torch.Tensor(self.num_labels, self.num_labels))\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.constant_(self.state_net.bias, 0.)\n        if self.bigram:\n            nn.init.xavier_uniform_(self.transition_net.weight)\n            nn.init.constant_(self.transition_net.bias, 0.)\n        else:\n            nn.init.normal_(self.transition_matrix)\n\n    def forward(self, input, mask=None):\n        \'\'\'\n\n        Args:\n            input: Tensor\n                the input tensor with shape = [batch, length, model_dim]\n            mask: Tensor or None\n                the mask tensor with shape = [batch, length]\n\n        Returns: Tensor\n            the energy tensor with shape = [batch, length, num_label, num_label]\n\n        \'\'\'\n        batch, length, _ = input.size()\n\n        # compute out_s by tensor dot [batch, length, model_dim] * [model_dim, num_label]\n        # thus out_s should be [batch, length, num_label] --> [batch, length, num_label, 1]\n        out_s = self.state_net(input).unsqueeze(2)\n\n        if self.bigram:\n            # compute out_s by tensor dot: [batch, length, model_dim] * [model_dim, num_label * num_label]\n            # the output should be [batch, length, num_label,  num_label]\n            out_t = self.transition_net(input).view(batch, length, self.num_labels, self.num_labels)\n            output = out_t + out_s\n        else:\n            # [batch, length, num_label, num_label]\n            output = self.transition_matrix + out_s\n\n        if mask is not None:\n            output = output * mask.unsqueeze(2).unsqueeze(3)\n\n        return output\n\n    def loss(self, input, target, mask=None):\n        \'\'\'\n\n        Args:\n            input: Tensor\n                the input tensor with shape = [batch, length, model_dim]\n            target: Tensor\n                the tensor of target labels with shape [batch, length]\n            mask:Tensor or None\n                the mask tensor with shape = [batch, length]\n\n        Returns: Tensor\n                A 1D tensor for minus log likelihood loss [batch]\n        \'\'\'\n        batch, length, _ = input.size()\n        energy = self(input, mask=mask)\n        # shape = [length, batch, num_label, num_label]\n        energy_transpose = energy.transpose(0, 1)\n        # shape = [length, batch]\n        target_transpose = target.transpose(0, 1)\n        # shape = [length, batch, 1]\n        mask_transpose = None\n        if mask is not None:\n            mask_transpose = mask.unsqueeze(2).transpose(0, 1)\n\n        # shape = [batch, num_label]\n        partition = None\n\n        # shape = [batch]\n        batch_index = torch.arange(0, batch).type_as(input).long()\n        prev_label = input.new_full((batch, ), self.num_labels - 1).long()\n        tgt_energy = input.new_zeros(batch)\n\n        for t in range(length):\n            # shape = [batch, num_label, num_label]\n            curr_energy = energy_transpose[t]\n            if t == 0:\n                partition = curr_energy[:, -1, :]\n            else:\n                # shape = [batch, num_label]\n                partition_new = torch.logsumexp(curr_energy + partition.unsqueeze(2), dim=1)\n                if mask_transpose is None:\n                    partition = partition_new\n                else:\n                    mask_t = mask_transpose[t]\n                    partition = partition + (partition_new - partition) * mask_t\n            tgt_energy += curr_energy[batch_index, prev_label, target_transpose[t]]\n            prev_label = target_transpose[t]\n\n        return torch.logsumexp(partition, dim=1) - tgt_energy\n\n    def decode(self, input, mask=None, leading_symbolic=0):\n        """"""\n\n        Args:\n            input: Tensor\n                the input tensor with shape = [batch, length, model_dim]\n            mask: Tensor or None\n                the mask tensor with shape = [batch, length]\n            leading_symbolic: nt\n                number of symbolic labels leading in type alphabets (set it to 0 if you are not sure)\n\n        Returns: Tensor\n            decoding results in shape [batch, length]\n\n        """"""\n\n        energy = self(input, mask=mask)\n\n        # Input should be provided as (n_batch, n_time_steps, num_labels, num_labels)\n        # For convenience, we need to dimshuffle to (n_time_steps, n_batch, num_labels, num_labels)\n        energy_transpose = energy.transpose(0, 1)\n\n        # the last row and column is the tag for pad symbol. reduce these two dimensions by 1 to remove that.\n        # also remove the first #symbolic rows and columns.\n        # now the shape of energies_shuffled is [n_time_steps, b_batch, t, t] where t = num_labels - #symbolic - 1.\n        energy_transpose = energy_transpose[:, :, leading_symbolic:-1, leading_symbolic:-1]\n\n        length, batch_size, num_label, _ = energy_transpose.size()\n\n        batch_index = torch.arange(0, batch_size).type_as(input).long()\n        pi = input.new_zeros([length, batch_size, num_label])\n        pointer = batch_index.new_zeros(length, batch_size, num_label)\n        back_pointer = batch_index.new_zeros(length, batch_size)\n\n        pi[0] = energy[:, 0, -1, leading_symbolic:-1]\n        pointer[0] = -1\n        for t in range(1, length):\n            pi_prev = pi[t - 1]\n            pi[t], pointer[t] = torch.max(energy_transpose[t] + pi_prev.unsqueeze(2), dim=1)\n\n        _, back_pointer[-1] = torch.max(pi[-1], dim=1)\n        for t in reversed(range(length - 1)):\n            pointer_last = pointer[t + 1]\n            back_pointer[t] = pointer_last[batch_index, back_pointer[t + 1]]\n\n        return back_pointer.transpose(0, 1) + leading_symbolic\n\n\nclass TreeCRF(nn.Module):\n    \'\'\'\n    Tree CRF layer.\n    \'\'\'\n    def __init__(self, model_dim):\n        """"""\n\n        Args:\n            model_dim: int\n                the dimension of the input.\n\n        """"""\n        super(TreeCRF, self).__init__()\n        self.model_dim = model_dim\n        self.energy = BiAffine(model_dim, model_dim)\n\n    def forward(self, heads, children, mask=None):\n        \'\'\'\n\n        Args:\n            heads: Tensor\n                the head input tensor with shape = [batch, length, model_dim]\n            children: Tensor\n                the child input tensor with shape = [batch, length, model_dim]\n            mask: Tensor or None\n                the mask tensor with shape = [batch, length]\n            lengths: Tensor or None\n                the length tensor with shape = [batch]\n\n        Returns: Tensor\n            the energy tensor with shape = [batch, length, length]\n\n        \'\'\'\n        batch, length, _ = heads.size()\n        # [batch, length, length]\n        output = self.energy(heads, children, mask_query=mask, mask_key=mask)\n        return output\n\n    def loss(self, heads, children, target_heads, mask=None):\n        \'\'\'\n\n        Args:\n            heads: Tensor\n                the head input tensor with shape = [batch, length, model_dim]\n            children: Tensor\n                the child input tensor with shape = [batch, length, model_dim]\n            target_heads: Tensor\n                the tensor of target labels with shape [batch, length]\n            mask:Tensor or None\n                the mask tensor with shape = [batch, length]\n\n        Returns: Tensor\n                A 1D tensor for minus log likelihood loss\n        \'\'\'\n        batch, length, _ = heads.size()\n        # [batch, length, length]\n        energy = self(heads, children, mask=mask).double()\n        A = torch.exp(energy)\n        # mask out invalid positions\n        if mask is not None:\n            mask = mask.double()\n            A = A * mask.unsqueeze(2) * mask.unsqueeze(1)\n\n        # set diagonal elements to 0\n        diag_mask = 1.0 - torch.eye(length).unsqueeze(0).type_as(energy)\n        A = A * diag_mask\n        energy = energy * diag_mask\n\n        # get D [batch, length]\n        D = A.sum(dim=1)\n        rtol = 1e-4\n        atol = 1e-6\n        D += atol\n        if mask is not None:\n            D = D * mask\n\n        # [batch, length, length]\n        D = torch.diag_embed(D)\n\n        # compute laplacian matrix\n        # [batch, length, length]\n        L = D - A\n\n        if mask is not None:\n            L = L + torch.diag_embed(1. - mask)\n\n        # compute partition Z(x) [batch]\n        L = L[:, 1:, 1:]\n        z = torch.logdet(L)\n\n        # first create index matrix [length, batch]\n        index = torch.arange(0, length).view(length, 1).expand(length, batch)\n        index = index.type_as(energy).long()\n        batch_index = torch.arange(0, batch).type_as(index)\n        # compute target energy [length-1, batch]\n        tgt_energy = energy[batch_index, target_heads.t(), index][1:]\n        # sum over dim=0 shape = [batch]\n        tgt_energy = tgt_energy.sum(dim=0)\n\n        return (z - tgt_energy).float()\n'"
neuronlp2/nn/init.py,3,"b'__author__ = \'max\'\n\nimport torch\n\n\ndef assign_tensor(tensor, val):\n    """"""\n    copy val to tensor\n    Args:\n        tensor: an n-dimensional torch.Tensor or autograd.Variable\n        val: an n-dimensional torch.Tensor to fill the tensor with\n\n    Returns:\n\n    """"""\n    with torch.no_grad():\n        return tensor.copy_(val)\n'"
neuronlp2/nn/modules.py,15,"b'__author__ = \'max\'\n\nfrom overrides import overrides\nfrom collections import OrderedDict\nimport math\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.parameter import Parameter\n\n\nclass BiLinear(nn.Module):\n    """"""\n    Bi-linear layer\n    """"""\n    def __init__(self, left_features, right_features, out_features, bias=True):\n        """"""\n\n        Args:\n            left_features: size of left input\n            right_features: size of right input\n            out_features: size of output\n            bias: If set to False, the layer will not learn an additive bias.\n                Default: True\n        """"""\n        super(BiLinear, self).__init__()\n        self.left_features = left_features\n        self.right_features = right_features\n        self.out_features = out_features\n\n        self.U = Parameter(torch.Tensor(self.out_features, self.left_features, self.right_features))\n        self.weight_left = Parameter(torch.Tensor(self.out_features, self.left_features))\n        self.weight_right = Parameter(torch.Tensor(self.out_features, self.right_features))\n\n        if bias:\n            self.bias = Parameter(torch.Tensor(out_features))\n        else:\n            self.register_parameter(\'bias\', None)\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.weight_left)\n        nn.init.xavier_uniform_(self.weight_right)\n        nn.init.constant_(self.bias, 0.)\n        nn.init.xavier_uniform_(self.U)\n\n    def forward(self, input_left, input_right):\n        """"""\n\n        Args:\n            input_left: Tensor\n                the left input tensor with shape = [batch1, batch2, ..., left_features]\n            input_right: Tensor\n                the right input tensor with shape = [batch1, batch2, ..., right_features]\n\n        Returns:\n\n        """"""\n\n        batch_size = input_left.size()[:-1]\n        batch = int(np.prod(batch_size))\n\n        # convert left and right input to matrices [batch, left_features], [batch, right_features]\n        input_left = input_left.view(batch, self.left_features)\n        input_right = input_right.view(batch, self.right_features)\n\n        # output [batch, out_features]\n        output = F.bilinear(input_left, input_right, self.U, self.bias)\n        output = output + F.linear(input_left, self.weight_left, None) + F.linear(input_right, self.weight_right, None)\n        # convert back to [batch1, batch2, ..., out_features]\n        return output.view(batch_size + (self.out_features, ))\n\n    def __repr__(self):\n        return self.__class__.__name__ + \' (\' \\\n               + \'left_features=\' + str(self.left_features) \\\n               + \', right_features=\' + str(self.right_features) \\\n               + \', out_features=\' + str(self.out_features) + \')\'\n\n\nclass BiAffine(nn.Module):\n    \'\'\'\n    Bi-Affine energy layer.\n    \'\'\'\n\n    def __init__(self, key_dim, query_dim):\n        \'\'\'\n\n        Args:\n            key_dim: int\n                the dimension of the key.\n            query_dim: int\n                the dimension of the query.\n\n        \'\'\'\n        super(BiAffine, self).__init__()\n        self.key_dim = key_dim\n        self.query_dim = query_dim\n\n        self.q_weight = Parameter(torch.Tensor(self.query_dim))\n        self.key_weight = Parameter(torch.Tensor(self.key_dim))\n        self.b = Parameter(torch.Tensor(1))\n        self.U = Parameter(torch.Tensor(self.query_dim, self.key_dim))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        bound = 1 / math.sqrt(self.query_dim)\n        nn.init.uniform_(self.q_weight, -bound, bound)\n        bound = 1 / math.sqrt(self.key_dim)\n        nn.init.uniform_(self.key_weight, -bound, bound)\n        nn.init.constant_(self.b, 0.)\n        nn.init.xavier_uniform_(self.U)\n\n    def forward(self, query, key, mask_query=None, mask_key=None):\n        """"""\n\n        Args:\n            query: Tensor\n                the decoder input tensor with shape = [batch, length_query, query_dim]\n            key: Tensor\n                the child input tensor with shape = [batch, length_key, key_dim]\n            mask_query: Tensor or None\n                the mask tensor for decoder with shape = [batch, length_query]\n            mask_key: Tensor or None\n                the mask tensor for encoder with shape = [batch, length_key]\n\n        Returns: Tensor\n            the energy tensor with shape = [batch, length_query, length_key]\n\n        """"""\n        # output shape [batch, length_query, length_key]\n        # compute bi-affine part\n        # [batch, length_query, query_dim] * [query_dim, key_dim]\n        # output shape [batch, length_query, key_dim]\n        output = torch.matmul(query, self.U)\n        # [batch, length_query, key_dim] * [batch, key_dim, length_key]\n        # output shape [batch, length_query, length_key]\n        output = torch.matmul(output, key.transpose(1, 2))\n\n        # compute query part: [query_dim] * [batch, query_dim, length_query]\n        # the output shape is [batch, length_query, 1]\n        out_q = torch.matmul(self.q_weight, query.transpose(1, 2)).unsqueeze(2)\n        # compute decoder part: [key_dim] * [batch, key_dim, length_key]\n        # the output shape is [batch, 1, length_key]\n        out_k = torch.matmul(self.key_weight, key.transpose(1, 2)).unsqueeze(1)\n\n        output = output + out_q + out_k + self.b\n\n        if mask_query is not None:\n            output = output * mask_query.unsqueeze(2)\n        if mask_key is not None:\n            output = output * mask_key.unsqueeze(1)\n        return output\n\n    @overrides\n    def extra_repr(self):\n        s = \'{key_dim}, {query_dim}\'\n        return s.format(**self.__dict__)\n\n\nclass CharCNN(nn.Module):\n    """"""\n    CNN layers for characters\n    """"""\n    def __init__(self, num_layers, in_channels, out_channels, hidden_channels=None, activation=\'elu\'):\n        super(CharCNN, self).__init__()\n        assert activation in [\'elu\', \'tanh\']\n        if activation == \'elu\':\n            ACT = nn.ELU\n        else:\n            ACT = nn.Tanh\n        layers = list()\n        for i in range(num_layers - 1):\n            layers.append((\'conv{}\'.format(i), nn.Conv1d(in_channels, hidden_channels, kernel_size=3, padding=1)))\n            layers.append((\'act{}\'.format(i), ACT()))\n            in_channels = hidden_channels\n        layers.append((\'conv_top\', nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1)))\n        layers.append((\'act_top\', ACT()))\n        self.act = ACT\n        self.net = nn.Sequential(OrderedDict(layers))\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        for layer in self.net:\n            if isinstance(layer, nn.Conv1d):\n                nn.init.xavier_uniform_(layer.weight)\n                nn.init.constant_(layer.bias, 0.)\n            else:\n                assert isinstance(layer, self.act)\n\n    def forward(self, char):\n        """"""\n\n        Args:\n            char: Tensor\n                the input tensor of character [batch, sent_length, char_length, in_channels]\n\n        Returns: Tensor\n            output character encoding with shape [batch, sent_length, in_channels]\n\n        """"""\n        # [batch, sent_length, char_length, in_channels]\n        char_size = char.size()\n        # first transform to [batch * sent_length, char_length, in_channels]\n        # then transpose to [batch * sent_length, in_channels, char_length]\n        char = char.view(-1, char_size[2], char_size[3]).transpose(1, 2)\n        # [batch * sent_length, out_channels, char_length]\n        char = self.net(char).max(dim=2)[0]\n        # [batch, sent_length, out_channels]\n        return char.view(char_size[0], char_size[1], -1)\n'"
neuronlp2/nn/skip_rnn.py,27,"b'__author__ = \'max\'\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.parameter import Parameter\nfrom neuronlp2.nn._functions import skipconnect_rnn as rnn_F\nfrom neuronlp2.nn.variational_rnn import VarRNNCellBase\n\n\nclass VarSkipRNNBase(nn.Module):\n    def __init__(self, Cell, input_size, hidden_size,\n                 num_layers=1, bias=True, batch_first=False,\n                 dropout=(0, 0), bidirectional=False, **kwargs):\n\n        super(VarSkipRNNBase, self).__init__()\n        self.Cell = Cell\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.bias = bias\n        self.batch_first = batch_first\n        self.bidirectional = bidirectional\n        self.lstm = False\n        num_directions = 2 if bidirectional else 1\n\n        self.all_cells = []\n        for layer in range(num_layers):\n            for direction in range(num_directions):\n                layer_input_size = input_size if layer == 0 else hidden_size * num_directions\n\n                cell = self.Cell(layer_input_size, hidden_size, self.bias, p=dropout, **kwargs)\n                self.all_cells.append(cell)\n                self.add_module(\'cell%d\' % (layer * num_directions + direction), cell)\n\n    def reset_parameters(self):\n        for cell in self.all_cells:\n            cell.reset_parameters()\n\n    def reset_noise(self, batch_size):\n        for cell in self.all_cells:\n            cell.reset_noise(batch_size)\n\n    def forward(self, input, skip_connect, mask=None, hx=None):\n        batch_size = input.size(0) if self.batch_first else input.size(1)\n        if hx is None:\n            num_directions = 2 if self.bidirectional else 1\n            hx = input.new_zeros(self.num_layers * num_directions, batch_size, self.hidden_size)\n            if self.lstm:\n                hx = (hx, hx)\n\n        func = rnn_F.AutogradSkipConnectRNN(num_layers=self.num_layers,\n                                            batch_first=self.batch_first,\n                                            bidirectional=self.bidirectional,\n                                            lstm=self.lstm)\n        self.reset_noise(batch_size)\n\n        output, hidden = func(input, skip_connect, self.all_cells, hx, None if mask is None else mask.view(mask.size() + (1,)))\n        return output, hidden\n\n    def step(self, input, hx=None, hs=None, mask=None):\n        \'\'\'\n        execute one step forward (only for one-directional RNN).\n        Args:\n            input (batch, model_dim): input tensor of this step.\n            hx (num_layers, batch, hidden_size): the hidden state of last step.\n            hs (batch. hidden_size): tensor containing the skip connection state for each element in the batch.\n            mask (batch): the mask tensor of this step.\n\n        Returns:\n            output (batch, hidden_size): tensor containing the output of this step from the last layer of RNN.\n            hn (num_layers, batch, hidden_size): tensor containing the hidden state of this step\n        \'\'\'\n        assert not self.bidirectional, ""step only cannot be applied to bidirectional RNN.""\n        batch_size = input.size(0)\n        if hx is None:\n            hx = input.new_zeros(self.num_layers, batch_size, self.hidden_size)\n            if self.lstm:\n                hx = (hx, hx)\n        if hs is None:\n            hs = input.new_zeros(self.num_layers, batch_size, self.hidden_size)\n\n        func = rnn_F.AutogradSkipConnectStep(num_layers=self.num_layers, lstm=self.lstm)\n\n        output, hidden = func(input, self.all_cells, hx, hs, mask)\n        return output, hidden\n\n\nclass VarSkipRNN(VarSkipRNNBase):\n    r""""""Applies a multi-layer Elman RNN with costomized non-linearity to an\n    input sequence.\n\n\n    For each element in the input sequence, each layer computes the following\n    function:\n\n    .. math::\n\n        h_t = \\tanh(w_{ih} * x_t + b_{ih}  +  w_{hh} * h_{(t-1)} + b_{hh})\n\n    where :math:`h_t` is the hidden state at time `t`, and :math:`x_t` is\n    the hidden state of the previous layer at time `t` or :math:`input_t`\n    for the first layer. If nonlinearity=\'relu\', then `ReLU` is used instead\n    of `tanh`.\n\n    Args:\n        input_size: The number of expected features in the input x\n        hidden_size: The number of features in the hidden state h\n        num_layers: Number of recurrent layers.\n        nonlinearity: The non-linearity to use [\'tanh\'|\'relu\']. Default: \'tanh\'\n        bias: If False, then the layer does not use bias weights b_ih and b_hh.\n            Default: True\n        batch_first: If True, then the input and output tensors are provided\n            as (batch, seq, feature)\n        dropout: (dropout_in, dropout_hidden) tuple.\n            If non-zero, introduces a dropout layer on the input and hidden of the each\n            RNN layer with dropout rate dropout_in and dropout_hidden, resp.\n        bidirectional: If True, becomes a bidirectional RNN. Default: False\n\n    Inputs: input, skip_connect, mask, h_0\n        - **input** (seq_len, batch, model_dim): tensor containing the features\n          of the input sequence.\n        - **skip_connect** (seq_len, batch): long tensor containing the index of skip connections for each step.\n          **mask** (seq_len, batch): 0-1 tensor containing the mask of the input sequence.\n        - **h_0** (num_layers * num_directions, batch, hidden_size): tensor\n          containing the initial hidden state for each element in the batch.\n\n    Outputs: output, h_n\n        - **output** (seq_len, batch, hidden_size * num_directions): tensor\n          containing the output features (h_k) from the last layer of the RNN,\n          for each k.  If a :class:`torch.nn.utils.rnn.PackedSequence` has\n          been given as the input, the output will also be a packed sequence.\n        - **h_n** (num_layers * num_directions, batch, hidden_size): tensor\n          containing the hidden state for k=seq_len.\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super(VarSkipRNN, self).__init__(SkipConnectRNNCell, *args, **kwargs)\n\n\nclass VarSkipFastLSTM(VarSkipRNNBase):\n    r""""""Applies a multi-layer long short-term memory (LSTM) RNN to an input\n    sequence.\n\n\n    For each element in the input sequence, each layer computes the following\n    function:\n\n    .. math::\n\n            \\begin{array}{ll}\n            i_t = \\mathrm{sigmoid}(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n            f_t = \\mathrm{sigmoid}(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\\n            g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hc} h_{(t-1)} + b_{hg}) \\\\\n            o_t = \\mathrm{sigmoid}(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\\n            c_t = f_t * c_{(t-1)} + i_t * g_t \\\\\n            h_t = o_t * \\tanh(c_t)\n            \\end{array}\n\n    where :math:`h_t` is the hidden state at time `t`, :math:`c_t` is the cell\n    state at time `t`, :math:`x_t` is the hidden state of the previous layer at\n    time `t` or :math:`input_t` for the first layer, and :math:`i_t`,\n    :math:`f_t`, :math:`g_t`, :math:`o_t` are the input, forget, cell,\n    and out gates, respectively.\n\n    Args:\n        input_size: The number of expected features in the input x\n        hidden_size: The number of features in the hidden state h\n        num_layers: Number of recurrent layers.\n        bias: If False, then the layer does not use bias weights b_ih and b_hh.\n            Default: True\n        batch_first: If True, then the input and output tensors are provided\n            as (batch, seq, feature)\n        dropout: (dropout_in, dropout_hidden) tuple.\n            If non-zero, introduces a dropout layer on the input and hidden of the each\n            RNN layer with dropout rate dropout_in and dropout_hidden, resp.\n        bidirectional: If True, becomes a bidirectional RNN. Default: False\n\n    Inputs: input, skip_connect, mask, (h_0, c_0)\n        - **input** (seq_len, batch, model_dim): tensor containing the features\n          of the input sequence.\n        - **skip_connect** (seq_len, batch): long tensor containing the index of skip connections for each step.\n        - **mask** (seq_len, batch): 0-1 tensor containing the mask of the input sequence.\n        - **h_0** (num_layers \\* num_directions, batch, hidden_size): tensor\n          containing the initial hidden state for each element in the batch.\n        - **c_0** (num_layers \\* num_directions, batch, hidden_size): tensor\n          containing the initial cell state for each element in the batch.\n\n    Outputs: output, (h_n, c_n)\n        - **output** (seq_len, batch, hidden_size * num_directions): tensor\n          containing the output features `(h_t)` from the last layer of the RNN,\n          for each t. If a :class:`torch.nn.utils.rnn.PackedSequence` has been\n          given as the input, the output will also be a packed sequence.\n        - **h_n** (num_layers * num_directions, batch, hidden_size): tensor\n          containing the hidden state for t=seq_len\n        - **c_n** (num_layers * num_directions, batch, hidden_size): tensor\n          containing the cell state for t=seq_len\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super(VarSkipFastLSTM, self).__init__(SkipConnectFastLSTMCell, *args, **kwargs)\n        self.lstm = True\n\n\nclass VarSkipLSTM(VarSkipRNNBase):\n    r""""""Applies a multi-layer long short-term memory (LSTM) RNN to an input\n    sequence.\n\n\n    For each element in the input sequence, each layer computes the following\n    function:\n\n    .. math::\n\n            \\begin{array}{ll}\n            i_t = \\mathrm{sigmoid}(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n            f_t = \\mathrm{sigmoid}(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\\n            g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hc} h_{(t-1)} + b_{hg}) \\\\\n            o_t = \\mathrm{sigmoid}(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\\n            c_t = f_t * c_{(t-1)} + i_t * g_t \\\\\n            h_t = o_t * \\tanh(c_t)\n            \\end{array}\n\n    where :math:`h_t` is the hidden state at time `t`, :math:`c_t` is the cell\n    state at time `t`, :math:`x_t` is the hidden state of the previous layer at\n    time `t` or :math:`input_t` for the first layer, and :math:`i_t`,\n    :math:`f_t`, :math:`g_t`, :math:`o_t` are the input, forget, cell,\n    and out gates, respectively.\n\n    Args:\n        input_size: The number of expected features in the input x\n        hidden_size: The number of features in the hidden state h\n        num_layers: Number of recurrent layers.\n        bias: If False, then the layer does not use bias weights b_ih and b_hh.\n            Default: True\n        batch_first: If True, then the input and output tensors are provided\n            as (batch, seq, feature)\n        dropout: (dropout_in, dropout_hidden) tuple.\n            If non-zero, introduces a dropout layer on the input and hidden of the each\n            RNN layer with dropout rate dropout_in and dropout_hidden, resp.\n        bidirectional: If True, becomes a bidirectional RNN. Default: False\n\n    Inputs: input, skip_connect, mask, (h_0, c_0)\n        - **input** (seq_len, batch, model_dim): tensor containing the features\n          of the input sequence.\n        - **skip_connect** (seq_len, batch): long tensor containing the index of skip connections for each step.\n        - **mask** (seq_len, batch): 0-1 tensor containing the mask of the input sequence.\n        - **h_0** (num_layers \\* num_directions, batch, hidden_size): tensor\n          containing the initial hidden state for each element in the batch.\n        - **c_0** (num_layers \\* num_directions, batch, hidden_size): tensor\n          containing the initial cell state for each element in the batch.\n\n    Outputs: output, (h_n, c_n)\n        - **output** (seq_len, batch, hidden_size * num_directions): tensor\n          containing the output features `(h_t)` from the last layer of the RNN,\n          for each t. If a :class:`torch.nn.utils.rnn.PackedSequence` has been\n          given as the input, the output will also be a packed sequence.\n        - **h_n** (num_layers * num_directions, batch, hidden_size): tensor\n          containing the hidden state for t=seq_len\n        - **c_n** (num_layers * num_directions, batch, hidden_size): tensor\n          containing the cell state for t=seq_len\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super(VarSkipLSTM, self).__init__(SkipConnectLSTMCell, *args, **kwargs)\n        self.lstm = True\n\n\nclass VarSkipFastGRU(VarSkipRNNBase):\n    r""""""Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.\n\n\n    For each element in the input sequence, each layer computes the following\n    function:\n\n    .. math::\n\n            \\begin{array}{ll}\n            r_t = \\mathrm{sigmoid}(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\\\\n            z_t = \\mathrm{sigmoid}(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\\\\n            n_t = \\tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\\\\n            h_t = (1 - z_t) * n_t + z_t * h_{(t-1)} \\\\\n            \\end{array}\n\n    where :math:`h_t` is the hidden state at time `t`, :math:`x_t` is the hidden\n    state of the previous layer at time `t` or :math:`input_t` for the first\n    layer, and :math:`r_t`, :math:`z_t`, :math:`n_t` are the reset, input,\n    and new gates, respectively.\n\n    Args:\n        input_size: The number of expected features in the input x\n        hidden_size: The number of features in the hidden state h\n        num_layers: Number of recurrent layers.\n        nonlinearity: The non-linearity to use [\'tanh\'|\'relu\']. Default: \'tanh\'\n        bias: If False, then the layer does not use bias weights b_ih and b_hh.\n            Default: True\n        batch_first: If True, then the input and output tensors are provided\n            as (batch, seq, feature)\n        dropout: (dropout_in, dropout_hidden) tuple.\n            If non-zero, introduces a dropout layer on the input and hidden of the each\n            RNN layer with dropout rate dropout_in and dropout_hidden, resp.\n        bidirectional: If True, becomes a bidirectional RNN. Default: False\n\n    Inputs: input, skip_connect, mask, h_0\n        - **input** (seq_len, batch, model_dim): tensor containing the features\n          of the input sequence.\n        - **skip_connect** (seq_len, batch): long tensor containing the index of skip connections for each step.\n          **mask** (seq_len, batch): 0-1 tensor containing the mask of the input sequence.\n        - **h_0** (num_layers * num_directions, batch, hidden_size): tensor\n          containing the initial hidden state for each element in the batch.\n\n    Outputs: output, h_n\n        - **output** (seq_len, batch, hidden_size * num_directions): tensor\n          containing the output features (h_k) from the last layer of the RNN,\n          for each k.  If a :class:`torch.nn.utils.rnn.PackedSequence` has\n          been given as the input, the output will also be a packed sequence.\n        - **h_n** (num_layers * num_directions, batch, hidden_size): tensor\n          containing the hidden state for k=seq_len.\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super(VarSkipFastGRU, self).__init__(SkipConnectFastGRUCell, *args, **kwargs)\n\n\nclass VarSkipGRU(VarSkipRNNBase):\n    r""""""Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.\n\n\n    For each element in the input sequence, each layer computes the following\n    function:\n\n    .. math::\n\n            \\begin{array}{ll}\n            r_t = \\mathrm{sigmoid}(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\\\\n            z_t = \\mathrm{sigmoid}(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\\\\n            n_t = \\tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\\\\n            h_t = (1 - z_t) * n_t + z_t * h_{(t-1)} \\\\\n            \\end{array}\n\n    where :math:`h_t` is the hidden state at time `t`, :math:`x_t` is the hidden\n    state of the previous layer at time `t` or :math:`input_t` for the first\n    layer, and :math:`r_t`, :math:`z_t`, :math:`n_t` are the reset, input,\n    and new gates, respectively.\n\n    Args:\n        input_size: The number of expected features in the input x\n        hidden_size: The number of features in the hidden state h\n        num_layers: Number of recurrent layers.\n        nonlinearity: The non-linearity to use [\'tanh\'|\'relu\']. Default: \'tanh\'\n        bias: If False, then the layer does not use bias weights b_ih and b_hh.\n            Default: True\n        batch_first: If True, then the input and output tensors are provided\n            as (batch, seq, feature)\n        dropout: (dropout_in, dropout_hidden) tuple.\n            If non-zero, introduces a dropout layer on the input and hidden of the each\n            RNN layer with dropout rate dropout_in and dropout_hidden, resp.\n        bidirectional: If True, becomes a bidirectional RNN. Default: False\n\n    Inputs: input, skip_connect, mask, h_0\n        - **input** (seq_len, batch, model_dim): tensor containing the features\n          of the input sequence.\n        - **skip_connect** (seq_len, batch): long tensor containing the index of skip connections for each step.\n          **mask** (seq_len, batch): 0-1 tensor containing the mask of the input sequence.\n        - **h_0** (num_layers * num_directions, batch, hidden_size): tensor\n          containing the initial hidden state for each element in the batch.\n\n    Outputs: output, h_n\n        - **output** (seq_len, batch, hidden_size * num_directions): tensor\n          containing the output features (h_k) from the last layer of the RNN,\n          for each k.  If a :class:`torch.nn.utils.rnn.PackedSequence` has\n          been given as the input, the output will also be a packed sequence.\n        - **h_n** (num_layers * num_directions, batch, hidden_size): tensor\n          containing the hidden state for k=seq_len.\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super(VarSkipGRU, self).__init__(SkipConnectGRUCell, *args, **kwargs)\n\n\nclass SkipConnectRNNCell(VarRNNCellBase):\n    r""""""An Elman RNN cell with tanh non-linearity and variational dropout.\n\n    .. math::\n\n        h\' = \\tanh(w_{ih} * x + b_{ih}  +  w_{hh} * (h * \\gamma) + b_{hh})\n\n    Args:\n        input_size: The number of expected features in the input x\n        hidden_size: The number of features in the hidden state h\n        bias: If False, then the layer does not use bias weights b_ih and b_hh.\n            Default: True\n        nonlinearity: The non-linearity to use [\'tanh\'|\'relu\']. Default: \'tanh\'\n        p: (p_in, p_hidden) (tuple, optional): the drop probability for input and hidden. Default: (0.5, 0.5)\n\n    Inputs: input, hidden, h_s\n        - **input** (batch, model_dim): tensor containing input features\n        - **hidden** (batch, hidden_size): tensor containing the initial hidden\n          state for each element in the batch.\n        - **h_s** (batch. hidden_size): tensor containing the skip connection state\n          for each element in the batch.\n\n    Outputs: h\'\n        - **h\'** (batch, hidden_size): tensor containing the next hidden state\n          for each element in the batch\n\n    Attributes:\n        weight_ih: the learnable input-hidden weights, of shape\n            `(model_dim x hidden_size)`\n        weight_hh: the learnable hidden-hidden weights, of shape\n            `(hidden_size x 2*hidden_size)`\n        bias_ih: the learnable input-hidden bias, of shape `(hidden_size)`\n        bias_hh: the learnable hidden-hidden bias, of shape `(hidden_size)`\n\n    """"""\n\n    def __init__(self, input_size, hidden_size, bias=True, nonlinearity=""tanh"", p=(0.5, 0.5)):\n        super(SkipConnectRNNCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.bias = bias\n        self.nonlinearity = nonlinearity\n        self.weight_ih = Parameter(torch.Tensor(hidden_size, input_size))\n        self.weight_hh = Parameter(torch.Tensor(hidden_size, hidden_size * 2))\n        if bias:\n            self.bias_ih = Parameter(torch.Tensor(hidden_size))\n            self.bias_hh = Parameter(torch.Tensor(hidden_size))\n        else:\n            self.register_parameter(\'bias_ih\', None)\n            self.register_parameter(\'bias_hh\', None)\n\n        self.reset_parameters()\n        p_in, p_hidden = p\n        if p_in < 0 or p_in > 1:\n            raise ValueError(""input dropout probability has to be between 0 and 1, ""\n                             ""but got {}"".format(p_in))\n        if p_hidden < 0 or p_hidden > 1:\n            raise ValueError(""hidden state dropout probability has to be between 0 and 1, ""\n                             ""but got {}"".format(p_hidden))\n        self.p_in = p_in\n        self.p_hidden = p_hidden\n        self.noise_in = None\n        self.noise_hidden = None\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.weight_hh)\n        nn.init.xavier_uniform_(self.weight_ih)\n        if self.bias:\n            nn.init.constant_(self.bias_hh, 0.)\n            nn.init.constant_(self.bias_ih, 0.)\n\n    def reset_noise(self, batch_size):\n        if self.training:\n            if self.p_in:\n                noise = self.weight_ih.new_empty(batch_size, self.input_size)\n                self.noise_in = noise.bernoulli_(1.0 - self.p_in) / (1.0 - self.p_in)\n            else:\n                self.noise_in = None\n\n            if self.p_hidden:\n                noise = self.weight_hh.new_empty(batch_size, self.hidden_size * 2)\n                self.noise_hidden = noise.bernoulli_(1.0 - self.p_hidden) / (1.0 - self.p_hidden)\n            else:\n                self.noise_hidden = None\n        else:\n            self.noise_in = None\n            self.noise_hidden = None\n\n    def forward(self, input, hx, hs):\n        if self.nonlinearity == ""tanh"":\n            func = rnn_F.SkipConnectRNNTanhCell\n        elif self.nonlinearity == ""relu"":\n            func = rnn_F.SkipConnectRNNReLUCell\n        else:\n            raise RuntimeError(\n                ""Unknown nonlinearity: {}"".format(self.nonlinearity))\n\n        return func(\n            input, hx, hs,\n            self.weight_ih, self.weight_hh,\n            self.bias_ih, self.bias_hh,\n            self.noise_in, self.noise_hidden,\n        )\n\n\nclass SkipConnectFastLSTMCell(VarRNNCellBase):\n    """"""\n    A long short-term memory (LSTM) cell with skip connections and variational dropout.\n\n    .. math::\n\n        \\begin{array}{ll}\n        i = \\mathrm{sigmoid}(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\\\\n        f = \\mathrm{sigmoid}(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\\\\n        g = \\tanh(W_{ig} x + b_{ig} + W_{hc} h + b_{hg}) \\\\\n        o = \\mathrm{sigmoid}(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\\\\n        c\' = f * c + i * g \\\\\n        h\' = o * \\tanh(c\') \\\\\n        \\end{array}\n\n    Args:\n        input_size: The number of expected features in the input x\n        hidden_size: The number of features in the hidden state h\n        bias: If `False`, then the layer does not use bias weights `b_ih` and\n            `b_hh`. Default: True\n        p: (p_in, p_hidden) (tuple, optional): the drop probability for input and hidden. Default: (0.5, 0.5)\n\n    Inputs: input, (h_0, c_0), h_s\n        - **input** (batch, model_dim): tensor containing input features\n        - **h_0** (batch, hidden_size): tensor containing the initial hidden\n          state for each element in the batch.\n        - **c_0** (batch. hidden_size): tensor containing the initial cell state\n          for each element in the batch.\n        - **h_s** (batch. hidden_size): tensor containing the skip connection state\n          for each element in the batch.\n\n    Outputs: h_1, c_1\n        - **h_1** (batch, hidden_size): tensor containing the next hidden state\n          for each element in the batch\n        - **c_1** (batch, hidden_size): tensor containing the next cell state\n          for each element in the batch\n\n    Attributes:\n        weight_ih: the learnable input-hidden weights, of shape\n            `(4*hidden_size x model_dim)`\n        weight_hh: the learnable hidden-hidden weights, of shape\n            `(4*hidden_size x 2*hidden_size)`\n        bias_ih: the learnable input-hidden bias, of shape `(4*hidden_size)`\n        bias_hh: the learnable hidden-hidden bias, of shape `(4*hidden_size)`\n    """"""\n\n    def __init__(self, input_size, hidden_size, bias=True, p=(0.5, 0.5)):\n        super(SkipConnectFastLSTMCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.bias = bias\n        self.weight_ih = Parameter(torch.Tensor(4 * hidden_size, input_size))\n        self.weight_hh = Parameter(torch.Tensor(4 * hidden_size, 2 * hidden_size))\n        if bias:\n            self.bias_ih = Parameter(torch.Tensor(4 * hidden_size))\n            self.bias_hh = Parameter(torch.Tensor(4 * hidden_size))\n        else:\n            self.register_parameter(\'bias_ih\', None)\n            self.register_parameter(\'bias_hh\', None)\n\n        self.reset_parameters()\n        p_in, p_hidden = p\n        if p_in < 0 or p_in > 1:\n            raise ValueError(""input dropout probability has to be between 0 and 1, ""\n                             ""but got {}"".format(p_in))\n        if p_hidden < 0 or p_hidden > 1:\n            raise ValueError(""hidden state dropout probability has to be between 0 and 1, ""\n                             ""but got {}"".format(p_hidden))\n        self.p_in = p_in\n        self.p_hidden = p_hidden\n        self.noise_in = None\n        self.noise_hidden = None\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.weight_hh)\n        nn.init.xavier_uniform_(self.weight_ih)\n        if self.bias:\n            nn.init.constant_(self.bias_hh, 0.)\n            nn.init.constant_(self.bias_ih, 0.)\n\n    def reset_noise(self, batch_size):\n        if self.training:\n            if self.p_in:\n                noise = self.weight_ih.new_empty(batch_size, self.input_size)\n                self.noise_in = noise.bernoulli_(1.0 - self.p_in) / (1.0 - self.p_in)\n            else:\n                self.noise_in = None\n\n            if self.p_hidden:\n                noise = self.weight_hh.new_empty(batch_size, self.hidden_size * 2)\n                self.noise_hidden = noise.bernoulli_(1.0 - self.p_hidden) / (1.0 - self.p_hidden)\n            else:\n                self.noise_hidden = None\n        else:\n            self.noise_in = None\n            self.noise_hidden = None\n\n    def forward(self, input, hx, hs):\n        return rnn_F.SkipConnectFastLSTMCell(\n            input, hx, hs,\n            self.weight_ih, self.weight_hh,\n            self.bias_ih, self.bias_hh,\n            self.noise_in, self.noise_hidden,\n        )\n\n\nclass SkipConnectLSTMCell(VarRNNCellBase):\n    """"""\n    A long short-term memory (LSTM) cell with skip connections and variational dropout.\n\n    .. math::\n\n        \\begin{array}{ll}\n        i = \\mathrm{sigmoid}(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\\\\n        f = \\mathrm{sigmoid}(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\\\\n        g = \\tanh(W_{ig} x + b_{ig} + W_{hc} h + b_{hg}) \\\\\n        o = \\mathrm{sigmoid}(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\\\\n        c\' = f * c + i * g \\\\\n        h\' = o * \\tanh(c\') \\\\\n        \\end{array}\n\n    Args:\n        input_size: The number of expected features in the input x\n        hidden_size: The number of features in the hidden state h\n        bias: If `False`, then the layer does not use bias weights `b_ih` and\n            `b_hh`. Default: True\n        p: (p_in, p_hidden) (tuple, optional): the drop probability for input and hidden. Default: (0.5, 0.5)\n\n    Inputs: input, (h_0, c_0), h_s\n        - **input** (batch, model_dim): tensor containing input features\n        - **h_0** (batch, hidden_size): tensor containing the initial hidden\n          state for each element in the batch.\n        - **c_0** (batch. hidden_size): tensor containing the initial cell state\n          for each element in the batch.\n           **h_s** (batch. hidden_size): tensor containing the skip connection state\n          for each element in the batch.\n\n    Outputs: h_1, c_1\n        - **h_1** (batch, hidden_size): tensor containing the next hidden state\n          for each element in the batch\n        - **c_1** (batch, hidden_size): tensor containing the next cell state\n          for each element in the batch\n\n    Attributes:\n        weight_ih: the learnable input-hidden weights, of shape\n            `(4 x model_dim x hidden_size)`\n        weight_hh: the learnable hidden-hidden weights, of shape\n            `(4 x 2*hidden_size x hidden_size)`\n        bias_ih: the learnable input-hidden bias, of shape `(4 x hidden_size)`\n        bias_hh: the learnable hidden-hidden bias, of shape `(4 x hidden_size)`\n    """"""\n\n    def __init__(self, input_size, hidden_size, bias=True, p=(0.5, 0.5)):\n        super(SkipConnectLSTMCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.bias = bias\n        self.weight_ih = Parameter(torch.Tensor(4, input_size, hidden_size))\n        self.weight_hh = Parameter(torch.Tensor(4, 2 * hidden_size, hidden_size))\n        if bias:\n            self.bias_ih = Parameter(torch.Tensor(4, hidden_size))\n            self.bias_hh = Parameter(torch.Tensor(4, hidden_size))\n        else:\n            self.register_parameter(\'bias_ih\', None)\n            self.register_parameter(\'bias_hh\', None)\n\n        self.reset_parameters()\n        p_in, p_hidden = p\n        if p_in < 0 or p_in > 1:\n            raise ValueError(""input dropout probability has to be between 0 and 1, ""\n                             ""but got {}"".format(p_in))\n        if p_hidden < 0 or p_hidden > 1:\n            raise ValueError(""hidden state dropout probability has to be between 0 and 1, ""\n                             ""but got {}"".format(p_hidden))\n        self.p_in = p_in\n        self.p_hidden = p_hidden\n        self.noise_in = None\n        self.noise_hidden = None\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.weight_hh)\n        nn.init.xavier_uniform_(self.weight_ih)\n        if self.bias:\n            nn.init.constant_(self.bias_hh, 0.)\n            nn.init.constant_(self.bias_ih, 0.)\n\n    def reset_noise(self, batch_size):\n        if self.training:\n            if self.p_in:\n                noise = self.weight_ih.new_empty(4, batch_size, self.input_size)\n                self.noise_in = noise.bernoulli_(1.0 - self.p_in) / (1.0 - self.p_in)\n            else:\n                self.noise_in = None\n\n            if self.p_hidden:\n                noise = self.weight_hh.new_empty(4, batch_size, self.hidden_size * 2)\n                self.noise_hidden = noise.bernoulli_(1.0 - self.p_hidden) / (1.0 - self.p_hidden)\n            else:\n                self.noise_hidden = None\n        else:\n            self.noise_in = None\n            self.noise_hidden = None\n\n    def forward(self, input, hx, hs):\n        return rnn_F.SkipConnectLSTMCell(\n            input, hx, hs,\n            self.weight_ih, self.weight_hh,\n            self.bias_ih, self.bias_hh,\n            self.noise_in, self.noise_hidden,\n        )\n\n\nclass SkipConnectFastGRUCell(VarRNNCellBase):\n    """"""A gated recurrent unit (GRU) cell with skip connections and variational dropout.\n\n    .. math::\n\n        \\begin{array}{ll}\n        r = \\mathrm{sigmoid}(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \\\\\n        z = \\mathrm{sigmoid}(W_{iz} x + b_{iz} + W_{hz} h + b_{hz}) \\\\\n        n = \\tanh(W_{in} x + b_{in} + r * (W_{hn} h + b_{hn})) \\\\\n        h\' = (1 - z) * n + z * h\n        \\end{array}\n\n    Args:\n        input_size: The number of expected features in the input x\n        hidden_size: The number of features in the hidden state h\n        bias: If `False`, then the layer does not use bias weights `b_ih` and\n            `b_hh`. Default: True\n        p: (p_in, p_hidden) (tuple, optional): the drop probability for input and hidden. Default: (0.5, 0.5)\n\n    Inputs: input, hidden, h_s\n        - **input** (batch, model_dim): tensor containing input features\n        - **hidden** (batch, hidden_size): tensor containing the initial hidden\n          state for each element in the batch.\n        - **h_s** (batch. hidden_size): tensor containing the skip connection state\n          for each element in the batch.\n\n    Outputs: h\'\n        - **h\'**: (batch, hidden_size): tensor containing the next hidden state\n          for each element in the batch\n\n    Attributes:\n        weight_ih: the learnable input-hidden weights, of shape\n            `(3*hidden_size x model_dim)`\n        weight_hh: the learnable hidden-hidden weights, of shape\n            `(3*hidden_size x 2*hidden_size)`\n        bias_ih: the learnable input-hidden bias, of shape `(3*hidden_size)`\n        bias_hh: the learnable hidden-hidden bias, of shape `(3*hidden_size)`\n    """"""\n\n    def __init__(self, input_size, hidden_size, bias=True, p=(0.5, 0.5)):\n        super(SkipConnectFastGRUCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.bias = bias\n        self.weight_ih = Parameter(torch.Tensor(3 * hidden_size, input_size))\n        self.weight_hh = Parameter(torch.Tensor(3 * hidden_size, hidden_size * 2))\n        if bias:\n            self.bias_ih = Parameter(torch.Tensor(3 * hidden_size))\n            self.bias_hh = Parameter(torch.Tensor(3 * hidden_size))\n        else:\n            self.register_parameter(\'bias_ih\', None)\n            self.register_parameter(\'bias_hh\', None)\n\n        self.reset_parameters()\n        p_in, p_hidden = p\n        if p_in < 0 or p_in > 1:\n            raise ValueError(""input dropout probability has to be between 0 and 1, ""\n                             ""but got {}"".format(p_in))\n        if p_hidden < 0 or p_hidden > 1:\n            raise ValueError(""hidden state dropout probability has to be between 0 and 1, ""\n                             ""but got {}"".format(p_hidden))\n        self.p_in = p_in\n        self.p_hidden = p_hidden\n        self.noise_in = None\n        self.noise_hidden = None\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.weight_hh)\n        nn.init.xavier_uniform_(self.weight_ih)\n        if self.bias:\n            nn.init.constant_(self.bias_hh, 0.)\n            nn.init.constant_(self.bias_ih, 0.)\n\n    def reset_noise(self, batch_size):\n        if self.training:\n            if self.p_in:\n                noise = self.weight_ih.new_empty(batch_size, self.input_size)\n                self.noise_in = noise.bernoulli_(1.0 - self.p_in) / (1.0 - self.p_in)\n            else:\n                self.noise_in = None\n\n            if self.p_hidden:\n                noise = self.weight_hh.new_empty(batch_size, self.hidden_size * 2)\n                self.noise_hidden = noise.bernoulli_(1.0 - self.p_hidden) / (1.0 - self.p_hidden)\n            else:\n                self.noise_hidden = None\n        else:\n            self.noise_in = None\n            self.noise_hidden = None\n\n    def forward(self, input, hx, hs):\n        return rnn_F.SkipConnectFastGRUCell(\n            input, hx, hs,\n            self.weight_ih, self.weight_hh,\n            self.bias_ih, self.bias_hh,\n            self.noise_in, self.noise_hidden,\n        )\n\n\nclass SkipConnectGRUCell(VarRNNCellBase):\n    """"""A gated recurrent unit (GRU) cell with skip connections and variational dropout.\n\n    .. math::\n\n        \\begin{array}{ll}\n        r = \\mathrm{sigmoid}(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \\\\\n        z = \\mathrm{sigmoid}(W_{iz} x + b_{iz} + W_{hz} h + b_{hz}) \\\\\n        n = \\tanh(W_{in} x + b_{in} + r * (W_{hn} h + b_{hn})) \\\\\n        h\' = (1 - z) * n + z * h\n        \\end{array}\n\n    Args:\n        input_size: The number of expected features in the input x\n        hidden_size: The number of features in the hidden state h\n        bias: If `False`, then the layer does not use bias weights `b_ih` and\n            `b_hh`. Default: `True`\n        p: (p_in, p_hidden) (tuple, optional): the drop probability for input and hidden. Default: (0.5, 0.5)\n\n    Inputs: input, hidden, h_s\n        - **input** (batch, model_dim): tensor containing input features\n        - **hidden** (batch, hidden_size): tensor containing the initial hidden\n          state for each element in the batch.\n        - **h_s** (batch. hidden_size): tensor containing the skip connection state\n          for each element in the batch.\n\n    Outputs: h\'\n        - **h\'**: (batch, hidden_size): tensor containing the next hidden state\n          for each element in the batch\n\n    Attributes:\n        weight_ih: the learnable input-hidden weights, of shape\n            `(3 x model_dim x hidden_size)`\n        weight_hh: the learnable hidden-hidden weights, of shape\n            `(3x 2*hidden_size x hidden_size)`\n        bias_ih: the learnable input-hidden bias, of shape `(3 x hidden_size)`\n        bias_hh: the learnable hidden-hidden bias, of shape `(3 x hidden_size)`\n    """"""\n\n    def __init__(self, input_size, hidden_size, bias=True, p=(0.5, 0.5)):\n        super(SkipConnectGRUCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.bias = bias\n        self.weight_ih = Parameter(torch.Tensor(3, input_size, hidden_size))\n        self.weight_hh = Parameter(torch.Tensor(3, hidden_size * 2, hidden_size))\n        if bias:\n            self.bias_ih = Parameter(torch.Tensor(3, hidden_size))\n            self.bias_hh = Parameter(torch.Tensor(3, hidden_size))\n        else:\n            self.register_parameter(\'bias_ih\', None)\n            self.register_parameter(\'bias_hh\', None)\n\n        self.reset_parameters()\n        p_in, p_hidden = p\n        if p_in < 0 or p_in > 1:\n            raise ValueError(""input dropout probability has to be between 0 and 1, ""\n                             ""but got {}"".format(p_in))\n        if p_hidden < 0 or p_hidden > 1:\n            raise ValueError(""hidden state dropout probability has to be between 0 and 1, ""\n                             ""but got {}"".format(p_hidden))\n        self.p_in = p_in\n        self.p_hidden = p_hidden\n        self.noise_in = None\n        self.noise_hidden = None\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.weight_hh)\n        nn.init.xavier_uniform_(self.weight_ih)\n        if self.bias:\n            nn.init.constant_(self.bias_hh, 0.)\n            nn.init.constant_(self.bias_ih, 0.)\n\n    def reset_noise(self, batch_size):\n        if self.training:\n            if self.p_in:\n                noise = self.weight_ih.new_empty(3, batch_size, self.input_size)\n                self.noise_in = noise.bernoulli_(1.0 - self.p_in) / (1.0 - self.p_in)\n            else:\n                self.noise_in = None\n\n            if self.p_hidden:\n                noise = self.weight_hh.new_empty(3, batch_size, self.hidden_size * 2)\n                self.noise_hidden = noise.bernoulli_(1.0 - self.p_hidden) / (1.0 - self.p_hidden)\n            else:\n                self.noise_hidden = None\n        else:\n            self.noise_in = None\n            self.noise_hidden = None\n\n    def forward(self, input, hx, hs):\n        return rnn_F.SkipConnectGRUCell(\n            input, hx, hs,\n            self.weight_ih, self.weight_hh,\n            self.bias_ih, self.bias_hh,\n            self.noise_in, self.noise_hidden,\n        )\n'"
neuronlp2/nn/utils.py,3,"b'import collections\nfrom itertools import repeat\nimport torch\nimport torch.nn as nn\nfrom torch._six import inf\n\n\ndef _ntuple(n):\n    def parse(x):\n        if isinstance(x, collections.Iterable):\n            return x\n        return tuple(repeat(x, n))\n    return parse\n\n_single = _ntuple(1)\n_pair = _ntuple(2)\n_triple = _ntuple(3)\n_quadruple = _ntuple(4)\n\n\ndef freeze_embedding(embedding):\n    assert isinstance(embedding, nn.Embedding), ""input should be an Embedding module.""\n    embedding.weight.detach_()\n\ndef total_grad_norm(parameters, norm_type=2):\n    if isinstance(parameters, torch.Tensor):\n        parameters = [parameters]\n    parameters = list(filter(lambda p: p.grad is not None, parameters))\n    norm_type = float(norm_type)\n    if norm_type == inf:\n        total_norm = max(p.grad.data.abs().max() for p in parameters)\n    else:\n        total_norm = 0\n        for p in parameters:\n            param_norm = p.grad.data.norm(norm_type)\n            total_norm += param_norm.item() ** norm_type\n        total_norm = total_norm ** (1. / norm_type)\n    return total_norm\n'"
neuronlp2/nn/variational_rnn.py,27,"b'__author__ = \'max\'\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.parameter import Parameter\nfrom neuronlp2.nn._functions import variational_rnn as rnn_F\n\n\nclass VarRNNBase(nn.Module):\n    def __init__(self, Cell, input_size, hidden_size,\n                 num_layers=1, bias=True, batch_first=False,\n                 dropout=(0, 0), bidirectional=False, **kwargs):\n\n        super(VarRNNBase, self).__init__()\n        self.Cell = Cell\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.bias = bias\n        self.batch_first = batch_first\n        self.bidirectional = bidirectional\n        self.lstm = False\n        num_directions = 2 if bidirectional else 1\n\n        self.all_cells = []\n        for layer in range(num_layers):\n            for direction in range(num_directions):\n                layer_input_size = input_size if layer == 0 else hidden_size * num_directions\n\n                cell = self.Cell(layer_input_size, hidden_size, self.bias, p=dropout, **kwargs)\n                self.all_cells.append(cell)\n                self.add_module(\'cell%d\' % (layer * num_directions + direction), cell)\n\n    def reset_parameters(self):\n        for cell in self.all_cells:\n            cell.reset_parameters()\n\n    def reset_noise(self, batch_size):\n        for cell in self.all_cells:\n            cell.reset_noise(batch_size)\n\n    def forward(self, input, mask=None, hx=None):\n        batch_size = input.size(0) if self.batch_first else input.size(1)\n        if hx is None:\n            num_directions = 2 if self.bidirectional else 1\n            hx = input.new_zeros(self.num_layers * num_directions, batch_size, self.hidden_size)\n            if self.lstm:\n                hx = (hx, hx)\n\n        func = rnn_F.AutogradVarRNN(num_layers=self.num_layers,\n                                    batch_first=self.batch_first,\n                                    bidirectional=self.bidirectional,\n                                    lstm=self.lstm)\n\n        self.reset_noise(batch_size)\n\n        output, hidden = func(input, self.all_cells, hx, None if mask is None else mask.view(mask.size() + (1,)))\n        return output, hidden\n\n    def step(self, input, hx=None, mask=None):\n        \'\'\'\n        execute one step forward (only for one-directional RNN).\n        Args:\n            input (batch, model_dim): input tensor of this step.\n            hx (num_layers, batch, hidden_size): the hidden state of last step.\n            mask (batch): the mask tensor of this step.\n\n        Returns:\n            output (batch, hidden_size): tensor containing the output of this step from the last layer of RNN.\n            hn (num_layers, batch, hidden_size): tensor containing the hidden state of this step\n        \'\'\'\n        assert not self.bidirectional, ""step only cannot be applied to bidirectional RNN.""\n        batch_size = input.size(0)\n        if hx is None:\n            hx = input.new_zeros(self.num_layers, batch_size, self.hidden_size)\n            if self.lstm:\n                hx = (hx, hx)\n\n        func = rnn_F.AutogradVarRNNStep(num_layers=self.num_layers, lstm=self.lstm)\n\n        output, hidden = func(input, self.all_cells, hx, mask)\n        return output, hidden\n\n\nclass VarRNN(VarRNNBase):\n    r""""""Applies a multi-layer Elman RNN with costomized non-linearity to an\n    input sequence.\n\n\n    For each element in the input sequence, each layer computes the following\n    function:\n\n    .. math::\n\n        h_t = \\tanh(w_{ih} * x_t + b_{ih}  +  w_{hh} * h_{(t-1)} + b_{hh})\n\n    where :math:`h_t` is the hidden state at time `t`, and :math:`x_t` is\n    the hidden state of the previous layer at time `t` or :math:`input_t`\n    for the first layer. If nonlinearity=\'relu\', then `ReLU` is used instead\n    of `tanh`.\n\n    Args:\n        input_size: The number of expected features in the input x\n        hidden_size: The number of features in the hidden state h\n        num_layers: Number of recurrent layers.\n        nonlinearity: The non-linearity to use [\'tanh\'|\'relu\']. Default: \'tanh\'\n        bias: If False, then the layer does not use bias weights b_ih and b_hh.\n            Default: True\n        batch_first: If True, then the input and output tensors are provided\n            as (batch, seq, feature)\n        dropout: (dropout_in, dropout_hidden) tuple.\n            If non-zero, introduces a dropout layer on the input and hidden of the each\n            RNN layer with dropout rate dropout_in and dropout_hidden, resp.\n        bidirectional: If True, becomes a bidirectional RNN. Default: False\n\n    Inputs: input, mask, h_0\n        - **input** (seq_len, batch, model_dim): tensor containing the features\n          of the input sequence.\n          **mask** (seq_len, batch): 0-1 tensor containing the mask of the input sequence.\n        - **h_0** (num_layers * num_directions, batch, hidden_size): tensor\n          containing the initial hidden state for each element in the batch.\n\n    Outputs: output, h_n\n        - **output** (seq_len, batch, hidden_size * num_directions): tensor\n          containing the output features (h_k) from the last layer of the RNN,\n          for each k.  If a :class:`torch.nn.utils.rnn.PackedSequence` has\n          been given as the input, the output will also be a packed sequence.\n        - **h_n** (num_layers * num_directions, batch, hidden_size): tensor\n          containing the hidden state for k=seq_len.\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super(VarRNN, self).__init__(VarRNNCell, *args, **kwargs)\n\n\nclass VarLSTM(VarRNNBase):\n    r""""""Applies a multi-layer long short-term memory (LSTM) RNN to an input\n    sequence.\n\n\n    For each element in the input sequence, each layer computes the following\n    function:\n\n    .. math::\n\n            \\begin{array}{ll}\n            i_t = \\mathrm{sigmoid}(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n            f_t = \\mathrm{sigmoid}(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\\n            g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hc} h_{(t-1)} + b_{hg}) \\\\\n            o_t = \\mathrm{sigmoid}(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\\n            c_t = f_t * c_{(t-1)} + i_t * g_t \\\\\n            h_t = o_t * \\tanh(c_t)\n            \\end{array}\n\n    where :math:`h_t` is the hidden state at time `t`, :math:`c_t` is the cell\n    state at time `t`, :math:`x_t` is the hidden state of the previous layer at\n    time `t` or :math:`input_t` for the first layer, and :math:`i_t`,\n    :math:`f_t`, :math:`g_t`, :math:`o_t` are the input, forget, cell,\n    and out gates, respectively.\n\n    Args:\n        input_size: The number of expected features in the input x\n        hidden_size: The number of features in the hidden state h\n        num_layers: Number of recurrent layers.\n        bias: If False, then the layer does not use bias weights b_ih and b_hh.\n            Default: True\n        batch_first: If True, then the input and output tensors are provided\n            as (batch, seq, feature)\n        dropout: (dropout_in, dropout_hidden) tuple.\n            If non-zero, introduces a dropout layer on the input and hidden of the each\n            RNN layer with dropout rate dropout_in and dropout_hidden, resp.\n        bidirectional: If True, becomes a bidirectional RNN. Default: False\n\n    Inputs: input, mask, (h_0, c_0)\n        - **input** (seq_len, batch, model_dim): tensor containing the features\n          of the input sequence.\n          **mask** (seq_len, batch): 0-1 tensor containing the mask of the input sequence.\n        - **h_0** (num_layers \\* num_directions, batch, hidden_size): tensor\n          containing the initial hidden state for each element in the batch.\n        - **c_0** (num_layers \\* num_directions, batch, hidden_size): tensor\n          containing the initial cell state for each element in the batch.\n\n    Outputs: output, (h_n, c_n)\n        - **output** (seq_len, batch, hidden_size * num_directions): tensor\n          containing the output features `(h_t)` from the last layer of the RNN,\n          for each t. If a :class:`torch.nn.utils.rnn.PackedSequence` has been\n          given as the input, the output will also be a packed sequence.\n        - **h_n** (num_layers * num_directions, batch, hidden_size): tensor\n          containing the hidden state for t=seq_len\n        - **c_n** (num_layers * num_directions, batch, hidden_size): tensor\n          containing the cell state for t=seq_len\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super(VarLSTM, self).__init__(VarLSTMCell, *args, **kwargs)\n        self.lstm = True\n\n\nclass VarFastLSTM(VarRNNBase):\n    r""""""Applies a multi-layer long short-term memory (LSTM) RNN to an input\n    sequence.\n\n\n    For each element in the input sequence, each layer computes the following\n    function:\n\n    .. math::\n\n            \\begin{array}{ll}\n            i_t = \\mathrm{sigmoid}(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n            f_t = \\mathrm{sigmoid}(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\\n            g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hc} h_{(t-1)} + b_{hg}) \\\\\n            o_t = \\mathrm{sigmoid}(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\\n            c_t = f_t * c_{(t-1)} + i_t * g_t \\\\\n            h_t = o_t * \\tanh(c_t)\n            \\end{array}\n\n    where :math:`h_t` is the hidden state at time `t`, :math:`c_t` is the cell\n    state at time `t`, :math:`x_t` is the hidden state of the previous layer at\n    time `t` or :math:`input_t` for the first layer, and :math:`i_t`,\n    :math:`f_t`, :math:`g_t`, :math:`o_t` are the input, forget, cell,\n    and out gates, respectively.\n\n    Args:\n        input_size: The number of expected features in the input x\n        hidden_size: The number of features in the hidden state h\n        num_layers: Number of recurrent layers.\n        bias: If False, then the layer does not use bias weights b_ih and b_hh.\n            Default: True\n        batch_first: If True, then the input and output tensors are provided\n            as (batch, seq, feature)\n        dropout: (dropout_in, dropout_hidden) tuple.\n            If non-zero, introduces a dropout layer on the input and hidden of the each\n            RNN layer with dropout rate dropout_in and dropout_hidden, resp.\n        bidirectional: If True, becomes a bidirectional RNN. Default: False\n\n    Inputs: input, mask, (h_0, c_0)\n        - **input** (seq_len, batch, model_dim): tensor containing the features\n          of the input sequence.\n          **mask** (seq_len, batch): 0-1 tensor containing the mask of the input sequence.\n        - **h_0** (num_layers \\* num_directions, batch, hidden_size): tensor\n          containing the initial hidden state for each element in the batch.\n        - **c_0** (num_layers \\* num_directions, batch, hidden_size): tensor\n          containing the initial cell state for each element in the batch.\n\n\n    Outputs: output, (h_n, c_n)\n        - **output** (seq_len, batch, hidden_size * num_directions): tensor\n          containing the output features `(h_t)` from the last layer of the RNN,\n          for each t. If a :class:`torch.nn.utils.rnn.PackedSequence` has been\n          given as the input, the output will also be a packed sequence.\n        - **h_n** (num_layers * num_directions, batch, hidden_size): tensor\n          containing the hidden state for t=seq_len\n        - **c_n** (num_layers * num_directions, batch, hidden_size): tensor\n          containing the cell state for t=seq_len\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super(VarFastLSTM, self).__init__(VarFastLSTMCell, *args, **kwargs)\n        self.lstm = True\n\n\nclass VarGRU(VarRNNBase):\n    r""""""Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.\n\n\n    For each element in the input sequence, each layer computes the following\n    function:\n\n    .. math::\n\n            \\begin{array}{ll}\n            r_t = \\mathrm{sigmoid}(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\\\\n            z_t = \\mathrm{sigmoid}(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\\\\n            n_t = \\tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\\\\n            h_t = (1 - z_t) * n_t + z_t * h_{(t-1)} \\\\\n            \\end{array}\n\n    where :math:`h_t` is the hidden state at time `t`, :math:`x_t` is the hidden\n    state of the previous layer at time `t` or :math:`input_t` for the first\n    layer, and :math:`r_t`, :math:`z_t`, :math:`n_t` are the reset, input,\n    and new gates, respectively.\n\n    Args:\n        input_size: The number of expected features in the input x\n        hidden_size: The number of features in the hidden state h\n        num_layers: Number of recurrent layers.\n        nonlinearity: The non-linearity to use [\'tanh\'|\'relu\']. Default: \'tanh\'\n        bias: If False, then the layer does not use bias weights b_ih and b_hh.\n            Default: True\n        batch_first: If True, then the input and output tensors are provided\n            as (batch, seq, feature)\n        dropout: (dropout_in, dropout_hidden) tuple.\n            If non-zero, introduces a dropout layer on the input and hidden of the each\n            RNN layer with dropout rate dropout_in and dropout_hidden, resp.\n        bidirectional: If True, becomes a bidirectional RNN. Default: False\n\n    Inputs: input, mask, h_0\n        - **input** (seq_len, batch, model_dim): tensor containing the features\n          of the input sequence.\n          **mask** (seq_len, batch): 0-1 tensor containing the mask of the input sequence.\n        - **h_0** (num_layers * num_directions, batch, hidden_size): tensor\n          containing the initial hidden state for each element in the batch.\n\n    Outputs: output, h_n\n        - **output** (seq_len, batch, hidden_size * num_directions): tensor\n          containing the output features (h_k) from the last layer of the RNN,\n          for each k.  If a :class:`torch.nn.utils.rnn.PackedSequence` has\n          been given as the input, the output will also be a packed sequence.\n        - **h_n** (num_layers * num_directions, batch, hidden_size): tensor\n          containing the hidden state for k=seq_len.\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super(VarGRU, self).__init__(VarGRUCell, *args, **kwargs)\n\n\nclass VarFastGRU(VarRNNBase):\n    r""""""Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.\n\n\n    For each element in the input sequence, each layer computes the following\n    function:\n\n    .. math::\n\n            \\begin{array}{ll}\n            r_t = \\mathrm{sigmoid}(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\\\\n            z_t = \\mathrm{sigmoid}(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\\\\n            n_t = \\tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\\\\n            h_t = (1 - z_t) * n_t + z_t * h_{(t-1)} \\\\\n            \\end{array}\n\n    where :math:`h_t` is the hidden state at time `t`, :math:`x_t` is the hidden\n    state of the previous layer at time `t` or :math:`input_t` for the first\n    layer, and :math:`r_t`, :math:`z_t`, :math:`n_t` are the reset, input,\n    and new gates, respectively.\n\n    Args:\n        input_size: The number of expected features in the input x\n        hidden_size: The number of features in the hidden state h\n        num_layers: Number of recurrent layers.\n        nonlinearity: The non-linearity to use [\'tanh\'|\'relu\']. Default: \'tanh\'\n        bias: If False, then the layer does not use bias weights b_ih and b_hh.\n            Default: True\n        batch_first: If True, then the input and output tensors are provided\n            as (batch, seq, feature)\n        dropout: (dropout_in, dropout_hidden) tuple.\n            If non-zero, introduces a dropout layer on the input and hidden of the each\n            RNN layer with dropout rate dropout_in and dropout_hidden, resp.\n        bidirectional: If True, becomes a bidirectional RNN. Default: False\n\n    Inputs: input, mask, h_0\n        - **input** (seq_len, batch, model_dim): tensor containing the features\n          of the input sequence.\n          **mask** (seq_len, batch): 0-1 tensor containing the mask of the input sequence.\n        - **h_0** (num_layers * num_directions, batch, hidden_size): tensor\n          containing the initial hidden state for each element in the batch.\n\n    Outputs: output, h_n\n        - **output** (seq_len, batch, hidden_size * num_directions): tensor\n          containing the output features (h_k) from the last layer of the RNN,\n          for each k.  If a :class:`torch.nn.utils.rnn.PackedSequence` has\n          been given as the input, the output will also be a packed sequence.\n        - **h_n** (num_layers * num_directions, batch, hidden_size): tensor\n          containing the hidden state for k=seq_len.\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super(VarFastGRU, self).__init__(VarFastGRUCell, *args, **kwargs)\n\n\nclass VarRNNCellBase(nn.Module):\n    def __repr__(self):\n        s = \'{name}({model_dim}, {hidden_size}\'\n        if \'bias\' in self.__dict__ and self.bias is not True:\n            s += \', bias={bias}\'\n        if \'nonlinearity\' in self.__dict__ and self.nonlinearity != ""tanh"":\n            s += \', nonlinearity={nonlinearity}\'\n        s += \')\'\n        return s.format(name=self.__class__.__name__, **self.__dict__)\n\n    def reset_noise(self, batch_size):\n        """"""\n        Should be overriden by all subclasses.\n        Args:\n            batch_size: (int) batch size of input.\n        """"""\n        raise NotImplementedError\n\n\nclass VarRNNCell(VarRNNCellBase):\n    r""""""An Elman RNN cell with tanh non-linearity and variational dropout.\n\n    .. math::\n\n        h\' = \\tanh(w_{ih} * x + b_{ih}  +  w_{hh} * (h * \\gamma) + b_{hh})\n\n    Args:\n        input_size: The number of expected features in the input x\n        hidden_size: The number of features in the hidden state h\n        bias: If False, then the layer does not use bias weights b_ih and b_hh.\n            Default: True\n        nonlinearity: The non-linearity to use [\'tanh\'|\'relu\']. Default: \'tanh\'\n        p: (p_in, p_hidden) (tuple, optional): the drop probability for input and hidden. Default: (0.5, 0.5)\n\n    Inputs: input, hidden\n        - **input** (batch, model_dim): tensor containing input features\n        - **hidden** (batch, hidden_size): tensor containing the initial hidden\n          state for each element in the batch.\n\n    Outputs: h\'\n        - **h\'** (batch, hidden_size): tensor containing the next hidden state\n          for each element in the batch\n\n    Attributes:\n        weight_ih: the learnable input-hidden weights, of shape\n            `(model_dim x hidden_size)`\n        weight_hh: the learnable hidden-hidden weights, of shape\n            `(hidden_size x hidden_size)`\n        bias_ih: the learnable input-hidden bias, of shape `(hidden_size)`\n        bias_hh: the learnable hidden-hidden bias, of shape `(hidden_size)`\n\n    """"""\n\n    def __init__(self, input_size, hidden_size, bias=True, nonlinearity=""tanh"", p=(0.5, 0.5)):\n        super(VarRNNCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.bias = bias\n        self.nonlinearity = nonlinearity\n        self.weight_ih = Parameter(torch.Tensor(hidden_size, input_size))\n        self.weight_hh = Parameter(torch.Tensor(hidden_size, hidden_size))\n        if bias:\n            self.bias_ih = Parameter(torch.Tensor(hidden_size))\n            self.bias_hh = Parameter(torch.Tensor(hidden_size))\n        else:\n            self.register_parameter(\'bias_ih\', None)\n            self.register_parameter(\'bias_hh\', None)\n\n        self.reset_parameters()\n        p_in, p_hidden = p\n        if p_in < 0 or p_in > 1:\n            raise ValueError(""input dropout probability has to be between 0 and 1, ""\n                             ""but got {}"".format(p_in))\n        if p_hidden < 0 or p_hidden > 1:\n            raise ValueError(""hidden state dropout probability has to be between 0 and 1, ""\n                             ""but got {}"".format(p_hidden))\n        self.p_in = p_in\n        self.p_hidden = p_hidden\n        self.noise_in = None\n        self.noise_hidden = None\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.weight_hh)\n        nn.init.xavier_uniform_(self.weight_ih)\n        if self.bias:\n            nn.init.constant_(self.bias_hh, 0.)\n            nn.init.constant_(self.bias_ih, 0.)\n\n    def reset_noise(self, batch_size):\n        if self.training:\n            if self.p_in:\n                noise = self.weight_ih.new_empty(batch_size, self.input_size)\n                self.noise_in = noise.bernoulli_(1.0 - self.p_in) / (1.0 - self.p_in)\n            else:\n                self.noise_in = None\n\n            if self.p_hidden:\n                noise = self.weight_hh.new_empty(batch_size, self.hidden_size)\n                self.noise_hidden = noise.bernoulli_(1.0 - self.p_hidden) / (1.0 - self.p_hidden)\n            else:\n                self.noise_hidden = None\n        else:\n            self.noise_in = None\n            self.noise_hidden = None\n\n    def forward(self, input, hx):\n        if self.nonlinearity == ""tanh"":\n            func = rnn_F.VarRNNTanhCell\n        elif self.nonlinearity == ""relu"":\n            func = rnn_F.VarRNNReLUCell\n        else:\n            raise RuntimeError(\n                ""Unknown nonlinearity: {}"".format(self.nonlinearity))\n\n        return func(\n            input, hx,\n            self.weight_ih, self.weight_hh,\n            self.bias_ih, self.bias_hh,\n            self.noise_in, self.noise_hidden,\n        )\n\n\nclass VarLSTMCell(VarRNNCellBase):\n    """"""\n    A long short-term memory (LSTM) cell with variational dropout.\n\n    .. math::\n\n        \\begin{array}{ll}\n        i = \\mathrm{sigmoid}(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\\\\n        f = \\mathrm{sigmoid}(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\\\\n        g = \\tanh(W_{ig} x + b_{ig} + W_{hc} h + b_{hg}) \\\\\n        o = \\mathrm{sigmoid}(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\\\\n        c\' = f * c + i * g \\\\\n        h\' = o * \\tanh(c\') \\\\\n        \\end{array}\n\n    Args:\n        input_size: The number of expected features in the input x\n        hidden_size: The number of features in the hidden state h\n        bias: If `False`, then the layer does not use bias weights `b_ih` and\n            `b_hh`. Default: True\n        p: (p_in, p_hidden) (tuple, optional): the drop probability for input and hidden. Default: (0.5, 0.5)\n\n    Inputs: input, (h_0, c_0)\n        - **input** (batch, model_dim): tensor containing input features\n        - **h_0** (batch, hidden_size): tensor containing the initial hidden\n          state for each element in the batch.\n        - **c_0** (batch. hidden_size): tensor containing the initial cell state\n          for each element in the batch.\n\n    Outputs: h_1, c_1\n        - **h_1** (batch, hidden_size): tensor containing the next hidden state\n          for each element in the batch\n        - **c_1** (batch, hidden_size): tensor containing the next cell state\n          for each element in the batch\n\n    Attributes:\n        weight_ih: the learnable input-hidden weights, of shape\n            `(4 x model_dim x hidden_size)`\n        weight_hh: the learnable hidden-hidden weights, of shape\n            `(4 x hidden_size x hidden_size)`\n        bias_ih: the learnable input-hidden bias, of shape `(4 x hidden_size)`\n        bias_hh: the learnable hidden-hidden bias, of shape `(4 x hidden_size)`\n    """"""\n\n    def __init__(self, input_size, hidden_size, bias=True, p=(0.5, 0.5)):\n        super(VarLSTMCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.bias = bias\n        self.weight_ih = Parameter(torch.Tensor(4, input_size, hidden_size))\n        self.weight_hh = Parameter(torch.Tensor(4, hidden_size, hidden_size))\n        if bias:\n            self.bias_ih = Parameter(torch.Tensor(4, hidden_size))\n            self.bias_hh = Parameter(torch.Tensor(4, hidden_size))\n        else:\n            self.register_parameter(\'bias_ih\', None)\n            self.register_parameter(\'bias_hh\', None)\n\n        self.reset_parameters()\n        p_in, p_hidden = p\n        if p_in < 0 or p_in > 1:\n            raise ValueError(""input dropout probability has to be between 0 and 1, ""\n                             ""but got {}"".format(p_in))\n        if p_hidden < 0 or p_hidden > 1:\n            raise ValueError(""hidden state dropout probability has to be between 0 and 1, ""\n                             ""but got {}"".format(p_hidden))\n        self.p_in = p_in\n        self.p_hidden = p_hidden\n        self.noise_in = None\n        self.noise_hidden = None\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.weight_hh)\n        nn.init.xavier_uniform_(self.weight_ih)\n        if self.bias:\n            nn.init.constant_(self.bias_hh, 0.)\n            nn.init.constant_(self.bias_ih, 0.)\n\n    def reset_noise(self, batch_size):\n        if self.training:\n            if self.p_in:\n                noise = self.weight_ih.new_empty(4, batch_size, self.input_size)\n                self.noise_in = noise.bernoulli_(1.0 - self.p_in) / (1.0 - self.p_in)\n            else:\n                self.noise_in = None\n\n            if self.p_hidden:\n                noise = self.weight_hh.new_empty(4, batch_size, self.hidden_size)\n                self.noise_hidden = noise.bernoulli_(1.0 - self.p_hidden) / (1.0 - self.p_hidden)\n            else:\n                self.noise_hidden = None\n        else:\n            self.noise_in = None\n            self.noise_hidden = None\n\n    def forward(self, input, hx):\n        return rnn_F.VarLSTMCell(\n            input, hx,\n            self.weight_ih, self.weight_hh,\n            self.bias_ih, self.bias_hh,\n            self.noise_in, self.noise_hidden,\n        )\n\n\nclass VarGRUCell(VarRNNCellBase):\n    """"""A gated recurrent unit (GRU) cell with variational dropout.\n\n    .. math::\n\n        \\begin{array}{ll}\n        r = \\mathrm{sigmoid}(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \\\\\n        z = \\mathrm{sigmoid}(W_{iz} x + b_{iz} + W_{hz} h + b_{hz}) \\\\\n        n = \\tanh(W_{in} x + b_{in} + r * (W_{hn} h + b_{hn})) \\\\\n        h\' = (1 - z) * n + z * h\n        \\end{array}\n\n    Args:\n        input_size: The number of expected features in the input x\n        hidden_size: The number of features in the hidden state h\n        bias: If `False`, then the layer does not use bias weights `b_ih` and\n            `b_hh`. Default: `True`\n        p: (p_in, p_hidden) (tuple, optional): the drop probability for input and hidden. Default: (0.5, 0.5)\n\n    Inputs: input, hidden\n        - **input** (batch, model_dim): tensor containing input features\n        - **hidden** (batch, hidden_size): tensor containing the initial hidden\n          state for each element in the batch.\n\n    Outputs: h\'\n        - **h\'**: (batch, hidden_size): tensor containing the next hidden state\n          for each element in the batch\n\n    Attributes:\n        weight_ih: the learnable input-hidden weights, of shape\n            `(3 x model_dim x hidden_size)`\n        weight_hh: the learnable hidden-hidden weights, of shape\n            `(3x hidden_size x hidden_size)`\n        bias_ih: the learnable input-hidden bias, of shape `(3 x hidden_size)`\n        bias_hh: the learnable hidden-hidden bias, of shape `(3 x hidden_size)`\n    """"""\n\n    def __init__(self, input_size, hidden_size, bias=True, p=(0.5, 0.5)):\n        super(VarGRUCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.bias = bias\n        self.weight_ih = Parameter(torch.Tensor(3, input_size, hidden_size))\n        self.weight_hh = Parameter(torch.Tensor(3, hidden_size, hidden_size))\n        if bias:\n            self.bias_ih = Parameter(torch.Tensor(3, hidden_size))\n            self.bias_hh = Parameter(torch.Tensor(3, hidden_size))\n        else:\n            self.register_parameter(\'bias_ih\', None)\n            self.register_parameter(\'bias_hh\', None)\n\n        self.reset_parameters()\n        p_in, p_hidden = p\n        if p_in < 0 or p_in > 1:\n            raise ValueError(""input dropout probability has to be between 0 and 1, ""\n                             ""but got {}"".format(p_in))\n        if p_hidden < 0 or p_hidden > 1:\n            raise ValueError(""hidden state dropout probability has to be between 0 and 1, ""\n                             ""but got {}"".format(p_hidden))\n        self.p_in = p_in\n        self.p_hidden = p_hidden\n        self.noise_in = None\n        self.noise_hidden = None\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.weight_hh)\n        nn.init.xavier_uniform_(self.weight_ih)\n        if self.bias:\n            nn.init.constant_(self.bias_hh, 0.)\n            nn.init.constant_(self.bias_ih, 0.)\n\n    def reset_noise(self, batch_size):\n        if self.training:\n            if self.p_in:\n                noise = self.weight_ih.new_empty(3, batch_size, self.input_size)\n                self.noise_in = noise.bernoulli_(1.0 - self.p_in) / (1.0 - self.p_in)\n            else:\n                self.noise_in = None\n\n            if self.p_hidden:\n                noise = self.weight_hh.new_empty(3, batch_size, self.hidden_size)\n                self.noise_hidden = noise.bernoulli_(1.0 - self.p_hidden) / (1.0 - self.p_hidden)\n            else:\n                self.noise_hidden = None\n        else:\n            self.noise_in = None\n            self.noise_hidden = None\n\n    def forward(self, input, hx):\n        return rnn_F.VarGRUCell(\n            input, hx,\n            self.weight_ih, self.weight_hh,\n            self.bias_ih, self.bias_hh,\n            self.noise_in, self.noise_hidden,\n        )\n\n\nclass VarFastLSTMCell(VarRNNCellBase):\n    """"""\n    A long short-term memory (LSTM) cell with variational dropout.\n\n    .. math::\n\n        \\begin{array}{ll}\n        i = \\mathrm{sigmoid}(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\\\\n        f = \\mathrm{sigmoid}(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\\\\n        g = \\tanh(W_{ig} x + b_{ig} + W_{hc} h + b_{hg}) \\\\\n        o = \\mathrm{sigmoid}(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\\\\n        c\' = f * c + i * g \\\\\n        h\' = o * \\tanh(c\') \\\\\n        \\end{array}\n\n    Args:\n        input_size: The number of expected features in the input x\n        hidden_size: The number of features in the hidden state h\n        bias: If `False`, then the layer does not use bias weights `b_ih` and\n            `b_hh`. Default: True\n        p: (p_in, p_hidden) (tuple, optional): the drop probability for input and hidden. Default: (0.5, 0.5)\n\n    Inputs: input, (h_0, c_0)\n        - **input** (batch, model_dim): tensor containing input features\n        - **h_0** (batch, hidden_size): tensor containing the initial hidden\n          state for each element in the batch.\n        - **c_0** (batch. hidden_size): tensor containing the initial cell state\n          for each element in the batch.\n\n    Outputs: h_1, c_1\n        - **h_1** (batch, hidden_size): tensor containing the next hidden state\n          for each element in the batch\n        - **c_1** (batch, hidden_size): tensor containing the next cell state\n          for each element in the batch\n\n    Attributes:\n        weight_ih: the learnable input-hidden weights, of shape\n            `(4*hidden_size x model_dim)`\n        weight_hh: the learnable hidden-hidden weights, of shape\n            `(4*hidden_size x hidden_size)`\n        bias_ih: the learnable input-hidden bias, of shape `(4*hidden_size)`\n        bias_hh: the learnable hidden-hidden bias, of shape `(4*hidden_size)`\n    """"""\n\n    def __init__(self, input_size, hidden_size, bias=True, p=(0.5, 0.5)):\n        super(VarFastLSTMCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.bias = bias\n        self.weight_ih = Parameter(torch.Tensor(4 * hidden_size, input_size))\n        self.weight_hh = Parameter(torch.Tensor(4 * hidden_size, hidden_size))\n        if bias:\n            self.bias_ih = Parameter(torch.Tensor(4 * hidden_size))\n            self.bias_hh = Parameter(torch.Tensor(4 * hidden_size))\n        else:\n            self.register_parameter(\'bias_ih\', None)\n            self.register_parameter(\'bias_hh\', None)\n\n        self.reset_parameters()\n        p_in, p_hidden = p\n        if p_in < 0 or p_in > 1:\n            raise ValueError(""input dropout probability has to be between 0 and 1, ""\n                             ""but got {}"".format(p_in))\n        if p_hidden < 0 or p_hidden > 1:\n            raise ValueError(""hidden state dropout probability has to be between 0 and 1, ""\n                             ""but got {}"".format(p_hidden))\n        self.p_in = p_in\n        self.p_hidden = p_hidden\n        self.noise_in = None\n        self.noise_hidden = None\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.weight_hh)\n        nn.init.xavier_uniform_(self.weight_ih)\n        if self.bias:\n            nn.init.constant_(self.bias_hh, 0.)\n            nn.init.constant_(self.bias_ih, 0.)\n\n    def reset_noise(self, batch_size):\n        if self.training:\n            if self.p_in:\n                noise = self.weight_ih.new_empty(batch_size, self.input_size)\n                self.noise_in = noise.bernoulli_(1.0 - self.p_in) / (1.0 - self.p_in)\n            else:\n                self.noise_in = None\n\n            if self.p_hidden:\n                noise = self.weight_hh.new_empty(batch_size, self.hidden_size)\n                self.noise_hidden = noise.bernoulli_(1.0 - self.p_hidden) / (1.0 - self.p_hidden)\n            else:\n                self.noise_hidden = None\n        else:\n            self.noise_in = None\n            self.noise_hidden = None\n\n    def forward(self, input, hx):\n        return rnn_F.VarFastLSTMCell(\n            input, hx,\n            self.weight_ih, self.weight_hh,\n            self.bias_ih, self.bias_hh,\n            self.noise_in, self.noise_hidden,\n        )\n\n\nclass VarFastGRUCell(VarRNNCellBase):\n    """"""A gated recurrent unit (GRU) cell with variational dropout.\n\n    .. math::\n\n        \\begin{array}{ll}\n        r = \\mathrm{sigmoid}(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \\\\\n        z = \\mathrm{sigmoid}(W_{iz} x + b_{iz} + W_{hz} h + b_{hz}) \\\\\n        n = \\tanh(W_{in} x + b_{in} + r * (W_{hn} h + b_{hn})) \\\\\n        h\' = (1 - z) * n + z * h\n        \\end{array}\n\n    Args:\n        input_size: The number of expected features in the input x\n        hidden_size: The number of features in the hidden state h\n        bias: If `False`, then the layer does not use bias weights `b_ih` and\n            `b_hh`. Default: True\n        p: (p_in, p_hidden) (tuple, optional): the drop probability for input and hidden. Default: (0.5, 0.5)\n\n    Inputs: input, hidden\n        - **input** (batch, model_dim): tensor containing input features\n        - **hidden** (batch, hidden_size): tensor containing the initial hidden\n          state for each element in the batch.\n\n    Outputs: h\'\n        - **h\'**: (batch, hidden_size): tensor containing the next hidden state\n          for each element in the batch\n\n    Attributes:\n        weight_ih: the learnable input-hidden weights, of shape\n            `(3*hidden_size x model_dim)`\n        weight_hh: the learnable hidden-hidden weights, of shape\n            `(3*hidden_size x hidden_size)`\n        bias_ih: the learnable input-hidden bias, of shape `(3*hidden_size)`\n        bias_hh: the learnable hidden-hidden bias, of shape `(3*hidden_size)`\n    """"""\n\n    def __init__(self, input_size, hidden_size, bias=True, p=(0.5, 0.5)):\n        super(VarFastGRUCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.bias = bias\n        self.weight_ih = Parameter(torch.Tensor(3 * hidden_size, input_size))\n        self.weight_hh = Parameter(torch.Tensor(3 * hidden_size, hidden_size))\n        if bias:\n            self.bias_ih = Parameter(torch.Tensor(3 * hidden_size))\n            self.bias_hh = Parameter(torch.Tensor(3 * hidden_size))\n        else:\n            self.register_parameter(\'bias_ih\', None)\n            self.register_parameter(\'bias_hh\', None)\n\n        self.reset_parameters()\n        p_in, p_hidden = p\n        if p_in < 0 or p_in > 1:\n            raise ValueError(""input dropout probability has to be between 0 and 1, ""\n                             ""but got {}"".format(p_in))\n        if p_hidden < 0 or p_hidden > 1:\n            raise ValueError(""hidden state dropout probability has to be between 0 and 1, ""\n                             ""but got {}"".format(p_hidden))\n        self.p_in = p_in\n        self.p_hidden = p_hidden\n        self.noise_in = None\n        self.noise_hidden = None\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.weight_hh)\n        nn.init.xavier_uniform_(self.weight_ih)\n        if self.bias:\n            nn.init.constant_(self.bias_hh, 0.)\n            nn.init.constant_(self.bias_ih, 0.)\n\n    def reset_noise(self, batch_size):\n        if self.training:\n            if self.p_in:\n                noise = self.weight_ih.new_empty(batch_size, self.input_size)\n                self.noise_in = noise.bernoulli_(1.0 - self.p_in) / (1.0 - self.p_in)\n            else:\n                self.noise_in = None\n\n            if self.p_hidden:\n                noise = self.weight_hh.new_empty(batch_size, self.hidden_size)\n                self.noise_hidden = noise.bernoulli_(1.0 - self.p_hidden) / (1.0 - self.p_hidden)\n            else:\n                self.noise_hidden = None\n        else:\n            self.noise_in = None\n            self.noise_hidden = None\n\n    def forward(self, input, hx):\n        return rnn_F.VarFastGRUCell(\n            input, hx,\n            self.weight_ih, self.weight_hh,\n            self.bias_ih, self.bias_hh,\n            self.noise_in, self.noise_hidden,\n        )\n'"
neuronlp2/optim/__init__.py,0,"b""__author__ = 'max'\n\nfrom neuronlp2.optim.lr_scheduler import InverseSquareRootScheduler, ExponentialScheduler\n"""
neuronlp2/optim/lr_scheduler.py,2,"b'__author__ = \'max\'\n\nfrom collections import defaultdict\nfrom torch.optim.optimizer import Optimizer\n\n\nclass _LRScheduler(object):\n    def __init__(self, optimizer, last_epoch=-1):\n        if not isinstance(optimizer, Optimizer):\n            raise TypeError(\'{} is not an Optimizer\'.format(\n                type(optimizer).__name__))\n        self.optimizer = optimizer\n        if last_epoch == -1:\n            for group in optimizer.param_groups:\n                group.setdefault(\'initial_lr\', group[\'lr\'])\n            last_epoch = 0\n        else:\n            for i, group in enumerate(optimizer.param_groups):\n                if \'initial_lr\' not in group:\n                    raise KeyError(""param \'initial_lr\' is not specified ""\n                                   ""in param_groups[{}] when resuming an optimizer"".format(i))\n        self.base_lrs = list(map(lambda group: group[\'initial_lr\'], optimizer.param_groups))\n\n    def state_dict(self):\n        """"""Returns the state of the scheduler as a :class:`dict`.\n\n        It contains an entry for every variable in self.__dict__ which\n        is not the optimizer.\n        """"""\n        return {key: value for key, value in self.__dict__.items() if key != \'optimizer\'}\n\n    def load_state_dict(self, state_dict):\n        """"""Loads the schedulers state.\n\n        Arguments:\n            state_dict (dict): scheduler state. Should be an object returned\n                from a call to :meth:`state_dict`.\n        """"""\n        self.__dict__.update(state_dict)\n\n    def get_lr(self):\n        raise NotImplementedError\n\n    def step(self, epoch=None):\n        if epoch is None:\n            epoch = self.last_epoch + 1\n        self.last_epoch = epoch\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n            param_group[\'lr\'] = lr\n\n    def reset_state(self):\n        self.optimizer.state.clear()\n\n\nclass InverseSquareRootScheduler(_LRScheduler):\n    """"""\n    Decay the LR based on the inverse square root of the update number.\n    We also support a warmup phase where we linearly increase the learning rate\n    from zero until the configured learning rate (``--lr``).\n    Thereafter we decay proportional to the number of\n    updates, with a decay factor set to align with the configured learning rate.\n    During warmup::\n      lrs = torch.linspace(0, args.lr, args.warmup_updates)\n      lr = lrs[update_num]\n    After warmup::\n      decay_factor = args.lr * sqrt(args.warmup_updates)\n      lr = decay_factor / sqrt(update_num)\n    """"""\n    def __init__(self, optimizer, warmup_steps, init_lr, last_epoch=-1):\n        assert warmup_steps > 0, \'warmup steps should be larger than 0.\'\n        super(InverseSquareRootScheduler, self).__init__(optimizer, last_epoch)\n        self.warmup_steps = float(warmup_steps)\n        self.init_lr = init_lr\n        self.lr_steps = [(base_lr - init_lr) / warmup_steps for base_lr in self.base_lrs]\n        self.decay_factor = self.warmup_steps ** 0.5\n        if last_epoch == -1:\n            last_epoch = 0\n        self.step(last_epoch)\n\n    def get_lr(self):\n        if self.last_epoch < self.warmup_steps:\n            return [self.init_lr + lr_step * self.last_epoch for lr_step in self.lr_steps]\n        else:\n            lr_factor = self.decay_factor * self.last_epoch**-0.5\n            return [base_lr * lr_factor for base_lr in self.base_lrs]\n\n\nclass ExponentialScheduler(_LRScheduler):\n    """"""Set the learning rate of each parameter group to the initial lr decayed\n    by gamma every epoch. When last_epoch=-1, sets initial lr as lr.\n    We also support a warmup phase where we linearly increase the learning rate\n    from zero until the configured learning rate (``--lr``).\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        gamma (float): Multiplicative factor of learning rate decay.\n        warmup_steps (int): Warmup steps..\n        last_epoch (int): The index of last epoch. Default: -1.\n    """"""\n\n    def __init__(self, optimizer, gamma, warmup_steps, init_lr, last_epoch=-1):\n        super(ExponentialScheduler, self).__init__(optimizer, last_epoch)\n        self.gamma = gamma\n        # handle warmup <= 0\n        self.warmup_steps = max(1, warmup_steps)\n        self.init_lr = init_lr\n        self.lr_steps = [(base_lr - init_lr) / self.warmup_steps for base_lr in self.base_lrs]\n        if last_epoch == -1:\n            last_epoch = 0\n        self.step(last_epoch)\n\n    def get_lr(self):\n        if self.last_epoch < self.warmup_steps:\n            return [self.init_lr + lr_step * self.last_epoch for lr_step in self.lr_steps]\n        else:\n            lr_factor = self.gamma ** (self.last_epoch - self.warmup_steps)\n            return [base_lr * lr_factor for base_lr in self.base_lrs]\n'"
neuronlp2/tasks/__init__.py,0,"b""__author__ = 'max'\n\nfrom neuronlp2.tasks.parser import *\n"""
neuronlp2/tasks/parser.py,0,"b'__author__ = \'max\'\n\nimport re\nimport numpy as np\n\ndef is_uni_punctuation(word):\n    match = re.match(""^[^\\w\\s]+$]"", word, flags=re.UNICODE)\n    return match is not None\n\n\ndef is_punctuation(word, pos, punct_set=None):\n    if punct_set is None:\n        return is_uni_punctuation(word)\n    else:\n        return pos in punct_set\n\n\ndef eval(words, postags, heads_pred, types_pred, heads, types, word_alphabet, pos_alphabet, lengths,\n         punct_set=None, symbolic_root=False, symbolic_end=False):\n    batch_size, _ = words.shape\n    ucorr = 0.\n    lcorr = 0.\n    total = 0.\n    ucomplete_match = 0.\n    lcomplete_match = 0.\n\n    ucorr_nopunc = 0.\n    lcorr_nopunc = 0.\n    total_nopunc = 0.\n    ucomplete_match_nopunc = 0.\n    lcomplete_match_nopunc = 0.\n\n    corr_root = 0.\n    total_root = 0.\n    start = 1 if symbolic_root else 0\n    end = 1 if symbolic_end else 0\n    for i in range(batch_size):\n        ucm = 1.\n        lcm = 1.\n        ucm_nopunc = 1.\n        lcm_nopunc = 1.\n        for j in range(start, lengths[i] - end):\n            word = word_alphabet.get_instance(words[i, j])\n            pos = pos_alphabet.get_instance(postags[i, j])\n\n            total += 1\n            if heads[i, j] == heads_pred[i, j]:\n                ucorr += 1\n                if types[i, j] == types_pred[i, j]:\n                    lcorr += 1\n                else:\n                    lcm = 0\n            else:\n                ucm = 0\n                lcm = 0\n\n            if not is_punctuation(word, pos, punct_set):\n                total_nopunc += 1\n                if heads[i, j] == heads_pred[i, j]:\n                    ucorr_nopunc += 1\n                    if types[i, j] == types_pred[i, j]:\n                        lcorr_nopunc += 1\n                    else:\n                        lcm_nopunc = 0\n                else:\n                    ucm_nopunc = 0\n                    lcm_nopunc = 0\n\n            if heads[i, j] == 0:\n                total_root += 1\n                corr_root += 1 if heads_pred[i, j] == 0 else 0\n\n        ucomplete_match += ucm\n        lcomplete_match += lcm\n        ucomplete_match_nopunc += ucm_nopunc\n        lcomplete_match_nopunc += lcm_nopunc\n\n    return (ucorr, lcorr, total, ucomplete_match, lcomplete_match), \\\n           (ucorr_nopunc, lcorr_nopunc, total_nopunc, ucomplete_match_nopunc, lcomplete_match_nopunc), \\\n           (corr_root, total_root), batch_size\n\n\ndef decode_MST(energies, lengths, leading_symbolic=0, labeled=True):\n    """"""\n    decode best parsing tree with MST algorithm.\n    :param energies: energies: numpy 4D tensor\n        energies of each edge. the shape is [batch_size, num_labels, n_steps, n_steps],\n        where the summy root is at index 0.\n    :param masks: numpy 2D tensor\n        masks in the shape [batch_size, n_steps].\n    :param leading_symbolic: int\n        number of symbolic dependency types leading in type alphabets)\n    :return:\n    """"""\n\n    def find_cycle(par):\n        added = np.zeros([length], np.bool)\n        added[0] = True\n        cycle = set()\n        findcycle = False\n        for i in range(1, length):\n            if findcycle:\n                break\n\n            if added[i] or not curr_nodes[i]:\n                continue\n\n            # init cycle\n            tmp_cycle = set()\n            tmp_cycle.add(i)\n            added[i] = True\n            findcycle = True\n            l = i\n\n            while par[l] not in tmp_cycle:\n                l = par[l]\n                if added[l]:\n                    findcycle = False\n                    break\n                added[l] = True\n                tmp_cycle.add(l)\n\n            if findcycle:\n                lorg = l\n                cycle.add(lorg)\n                l = par[lorg]\n                while l != lorg:\n                    cycle.add(l)\n                    l = par[l]\n                break\n\n        return findcycle, cycle\n\n    def chuLiuEdmonds():\n        par = np.zeros([length], dtype=np.int32)\n        # create best graph\n        par[0] = -1\n        for i in range(1, length):\n            # only interested at current nodes\n            if curr_nodes[i]:\n                max_score = score_matrix[0, i]\n                par[i] = 0\n                for j in range(1, length):\n                    if j == i or not curr_nodes[j]:\n                        continue\n\n                    new_score = score_matrix[j, i]\n                    if new_score > max_score:\n                        max_score = new_score\n                        par[i] = j\n\n        # find a cycle\n        findcycle, cycle = find_cycle(par)\n        # no cycles, get all edges and return them.\n        if not findcycle:\n            final_edges[0] = -1\n            for i in range(1, length):\n                if not curr_nodes[i]:\n                    continue\n\n                pr = oldI[par[i], i]\n                ch = oldO[par[i], i]\n                final_edges[ch] = pr\n            return\n\n        cyc_len = len(cycle)\n        cyc_weight = 0.0\n        cyc_nodes = np.zeros([cyc_len], dtype=np.int32)\n        for id, cyc_node in enumerate(cycle):\n            cyc_nodes[id] = cyc_node\n            cyc_weight += score_matrix[par[cyc_node], cyc_node]\n\n        rep = cyc_nodes[0]\n        for i in range(length):\n            if not curr_nodes[i] or i in cycle:\n                continue\n\n            max1 = float(""-inf"")\n            wh1 = -1\n            max2 = float(""-inf"")\n            wh2 = -1\n\n            for j in cyc_nodes:\n                if score_matrix[j, i] > max1:\n                    max1 = score_matrix[j, i]\n                    wh1 = j\n\n                scr = cyc_weight + score_matrix[i, j] - score_matrix[par[j], j]\n\n                if scr > max2:\n                    max2 = scr\n                    wh2 = j\n\n            score_matrix[rep, i] = max1\n            oldI[rep, i] = oldI[wh1, i]\n            oldO[rep, i] = oldO[wh1, i]\n            score_matrix[i, rep] = max2\n            oldO[i, rep] = oldO[i, wh2]\n            oldI[i, rep] = oldI[i, wh2]\n\n        rep_cons = []\n        for i in range(cyc_len):\n            rep_cons.append(set())\n            cyc_node = cyc_nodes[i]\n            for cc in reps[cyc_node]:\n                rep_cons[i].add(cc)\n\n        for cyc_node in cyc_nodes[1:]:\n            curr_nodes[cyc_node] = False\n            for cc in reps[cyc_node]:\n                reps[rep].add(cc)\n\n        chuLiuEdmonds()\n\n        # check each node in cycle, if one of its representatives is a key in the final_edges, it is the one.\n        found = False\n        wh = -1\n        for i in range(cyc_len):\n            for repc in rep_cons[i]:\n                if repc in final_edges:\n                    wh = cyc_nodes[i]\n                    found = True\n                    break\n            if found:\n                break\n\n        l = par[wh]\n        while l != wh:\n            ch = oldO[par[l], l]\n            pr = oldI[par[l], l]\n            final_edges[ch] = pr\n            l = par[l]\n\n    if labeled:\n        assert energies.ndim == 4, \'dimension of energies is not equal to 4\'\n    else:\n        assert energies.ndim == 3, \'dimension of energies is not equal to 3\'\n    input_shape = energies.shape\n    batch_size = input_shape[0]\n    max_length = input_shape[2]\n\n    pars = np.zeros([batch_size, max_length], dtype=np.int32)\n    types = np.zeros([batch_size, max_length], dtype=np.int32) if labeled else None\n    for i in range(batch_size):\n        energy = energies[i]\n\n        # calc the realy length of this instance\n        length = lengths[i]\n\n        # calc real energy matrix shape = [length, length, num_labels - #symbolic] (remove the label for symbolic types).\n        if labeled:\n            energy = energy[leading_symbolic:, :length, :length]\n            energy = energy - energy.min() + 1e-6\n            # get best label for each edge.\n            label_id_matrix = energy.argmax(axis=0) + leading_symbolic\n            energy = energy.max(axis=0)\n        else:\n            energy = energy[:length, :length]\n            energy = energy - energy.min() + 1e-6\n            label_id_matrix = None\n        # get original score matrix\n        orig_score_matrix = energy\n        # initialize score matrix to original score matrix\n        score_matrix = np.array(orig_score_matrix, copy=True)\n\n        oldI = np.zeros([length, length], dtype=np.int32)\n        oldO = np.zeros([length, length], dtype=np.int32)\n        curr_nodes = np.zeros([length], dtype=np.bool)\n        reps = []\n\n        for s in range(length):\n            orig_score_matrix[s, s] = 0.0\n            score_matrix[s, s] = 0.0\n            curr_nodes[s] = True\n            reps.append(set())\n            reps[s].add(s)\n            for t in range(s + 1, length):\n                oldI[s, t] = s\n                oldO[s, t] = t\n\n                oldI[t, s] = t\n                oldO[t, s] = s\n\n        final_edges = dict()\n        chuLiuEdmonds()\n        par = np.zeros([max_length], np.int32)\n        if labeled:\n            type = np.ones([max_length], np.int32)\n            type[0] = 0\n        else:\n            type = None\n\n        for ch, pr in final_edges.items():\n            par[ch] = pr\n            if labeled and ch != 0:\n                type[ch] = label_id_matrix[pr, ch]\n\n        par[0] = 0\n        pars[i] = par\n        if labeled:\n            types[i] = type\n\n    return pars, types\n'"
neuronlp2/nn/_functions/__init__.py,0,"b""__author__ = 'max'\n\nfrom neuronlp2.nn._functions import variational_rnn\nfrom neuronlp2.nn._functions import skipconnect_rnn\n"""
neuronlp2/nn/_functions/rnnFusedBackend.py,2,"b'from torch.autograd.function import Function, once_differentiable\nfrom torch._thnn import type2backend\n\n\nclass GRUFused(Function):\n    @staticmethod\n    def forward(ctx, input_gate, hidden_gate, hx, ibias=None, hbias=None):\n        ctx.backend = type2backend[input_gate.type()]\n\n        hy = input_gate.new()\n        workspace = input_gate.new(hx.numel() * 5)\n\n        ctx.has_bias = False\n        if ibias is not None:\n            ctx.has_bias = True\n            if ibias.dim() == 1:\n                ibias = ibias.unsqueeze(0)\n            if hbias.dim() == 1:\n                hbias = hbias.unsqueeze(0)\n\n        ctx.backend.GRUFused_updateOutput(\n            ctx.backend.library_state,\n            input_gate, hidden_gate, ibias, hbias, hx, hy, workspace)\n\n        ctx.workspace = workspace\n        ctx.igate_size = input_gate.size()\n        ctx.hgate_size = hidden_gate.size()\n\n        return hy\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, gradOutput):\n        ctx.backend = type2backend[gradOutput.type()]\n\n        gradInputHx = gradOutput.new()\n        gradInInput = gradOutput.new(*ctx.igate_size)\n        gradInHidden = gradOutput.new(*ctx.hgate_size)\n\n        ctx.backend.GRUFused_updateGradInput(\n            ctx.backend.library_state,\n            gradInInput, gradInHidden, gradOutput, gradInputHx, ctx.workspace)\n\n        gb1 = gb2 = None\n        if ctx.has_bias:\n            gb1 = gradInInput.sum(0, keepdim=False)\n            gb2 = gradInHidden.sum(0, keepdim=False)\n        return gradInInput, gradInHidden, gradInputHx, gb1, gb2\n\n\nclass LSTMFused(Function):\n    @staticmethod\n    def forward(ctx, input_gate, hidden_gate, cx, ibias=None, hbias=None):\n        ctx.backend = type2backend[input_gate.type()]\n        hy = input_gate.new()\n        cy = input_gate.new()\n\n        ctx.has_bias = False\n        if ibias is not None:\n            ctx.has_bias = True\n            if ibias.dim() == 1:\n                ibias = ibias.unsqueeze(0)\n            if hbias.dim() == 1:\n                hbias = hbias.unsqueeze(0)\n\n        # input_gate gets overwritten with some intermediate values to use in backwards\n        ctx.backend.LSTMFused_updateOutput(\n            ctx.backend.library_state,\n            input_gate, hidden_gate,\n            ibias, hbias,\n            cx, hy, cy)\n\n        ctx.hgate_size = hidden_gate.size()\n        ctx.save_for_backward(input_gate, cx, cy)\n\n        return hy, cy\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, *gradOutput):\n        ctx.backend = type2backend[gradOutput[0].type()]\n        gradInputCx = gradOutput[0].new()\n        gradInGates = gradOutput[0].new(*ctx.hgate_size)\n\n        saved_tens, cx, cy = ctx.saved_tensors\n        ctx.backend.LSTMFused_updateGradInput(\n            ctx.backend.library_state,\n            saved_tens, gradInGates, cx, cy,\n            gradOutput[0], gradOutput[1], gradInputCx)\n\n        gb1 = gb2 = None\n        if ctx.has_bias:\n            gb1 = gradInGates.sum(0, keepdim=False)\n            gb2 = gradInGates.sum(0, keepdim=False)\n\n        return gradInGates, gradInGates, gradInputCx, gb1, gb2\n'"
neuronlp2/nn/_functions/skipconnect_rnn.py,35,"b""__author__ = 'max'\n\nimport torch\nfrom torch.nn import functional as F\n\n\ndef SkipConnectRNNReLUCell(input, hidden, hidden_skip, w_ih, w_hh, b_ih=None, b_hh=None, noise_in=None, noise_hidden=None, noise_skip=None):\n    if noise_in is not None:\n        input = input * noise_in\n\n    hidden = torch.cat([hidden, hidden_skip], dim=1)\n    if noise_hidden is not None:\n        hidden = hidden * noise_hidden\n\n    hy = F.relu(F.linear(input, w_ih, b_ih) + F.linear(hidden, w_hh, b_hh))\n    return hy\n\n\ndef SkipConnectRNNTanhCell(input, hidden, hidden_skip, w_ih, w_hh, b_ih=None, b_hh=None, noise_in=None, noise_hidden=None):\n    if noise_in is not None:\n        input = input * noise_in\n\n    hidden = torch.cat([hidden, hidden_skip], dim=1)\n    if noise_hidden is not None:\n        hidden = hidden * noise_hidden\n\n    hy = torch.tanh(F.linear(input, w_ih, b_ih) + F.linear(hidden, w_hh, b_hh))\n    return hy\n\n\ndef SkipConnectLSTMCell(input, hidden, hidden_skip, w_ih, w_hh, b_ih=None, b_hh=None, noise_in=None, noise_hidden=None):\n    input = input.expand(4, *input.size()) if noise_in is None else input.unsqueeze(0) * noise_in\n\n    hx, cx = hidden\n    hx = torch.cat([hx, hidden_skip], dim=1)\n    hx = hx.expand(4, *hx.size()) if noise_hidden is None else hx.unsqueeze(0) * noise_hidden\n\n    gates = torch.baddbmm(b_ih.unsqueeze(1), input, w_ih) + torch.baddbmm(b_hh.unsqueeze(1), hx, w_hh)\n\n    ingate, forgetgate, cellgate, outgate = gates\n\n    ingate = torch.sigmoid(ingate)\n    forgetgate = torch.sigmoid(forgetgate)\n    cellgate = torch.tanh(cellgate)\n    outgate = torch.sigmoid(outgate)\n\n    cy = (forgetgate * cx) + (ingate * cellgate)\n    hy = outgate * torch.tanh(cy)\n\n    return hy, cy\n\n\ndef SkipConnectFastLSTMCell(input, hidden, hidden_skip, w_ih, w_hh, b_ih=None, b_hh=None, noise_in=None, noise_hidden=None):\n    if noise_in is not None:\n        input = input * noise_in\n\n    hx, cx = hidden\n    hx = torch.cat([hx, hidden_skip], dim=1)\n    if noise_hidden is not None:\n        hx = hx * noise_hidden\n\n    gates = F.linear(input, w_ih, b_ih) + F.linear(hx, w_hh, b_hh)\n\n    ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n\n    ingate = torch.sigmoid(ingate)\n    forgetgate = torch.sigmoid(forgetgate)\n    cellgate = torch.tanh(cellgate)\n    outgate = torch.sigmoid(outgate)\n\n    cy = (forgetgate * cx) + (ingate * cellgate)\n    hy = outgate * torch.tanh(cy)\n\n    return hy, cy\n\n\ndef SkipConnectGRUCell(input, hidden, hidden_skip, w_ih, w_hh, b_ih=None, b_hh=None, noise_in=None, noise_hidden=None):\n    input = input.expand(3, *input.size()) if noise_in is None else input.unsqueeze(0) * noise_in\n    hx = torch.cat([hidden, hidden_skip], dim=1)\n    hx = hx.expand(3, *hx.size()) if noise_hidden is None else hx.unsqueeze(0) * noise_hidden\n\n    gi = torch.baddbmm(b_ih.unsqueeze(1), input, w_ih)\n    gh = torch.baddbmm(b_hh.unsqueeze(1), hx, w_hh)\n    i_r, i_i, i_n = gi\n    h_r, h_i, h_n = gh\n\n    resetgate = torch.sigmoid(i_r + h_r)\n    inputgate = torch.sigmoid(i_i + h_i)\n    newgate = torch.tanh(i_n + resetgate * h_n)\n    hy = newgate + inputgate * (hidden - newgate)\n\n    return hy\n\n\ndef SkipConnectFastGRUCell(input, hidden, hidden_skip, w_ih, w_hh, b_ih=None, b_hh=None, noise_in=None, noise_hidden=None):\n    if noise_in is not None:\n        input = input * noise_in\n\n    hx = torch.cat([hidden, hidden_skip], dim=1)\n    if noise_hidden is not None:\n        hx = hx * noise_hidden\n\n    gi = F.linear(input, w_ih, b_ih)\n    gh = F.linear(hx, w_hh, b_hh)\n    i_r, i_i, i_n = gi.chunk(3, 1)\n    h_r, h_i, h_n = gh.chunk(3, 1)\n\n    resetgate = torch.sigmoid(i_r + h_r)\n    inputgate = torch.sigmoid(i_i + h_i)\n    newgate = torch.tanh(i_n + resetgate * h_n)\n    hy = newgate + inputgate * (hidden - newgate)\n\n    return hy\n\n\ndef SkipConnectRecurrent(reverse=False):\n    def forward(input, skip_connect, hidden, cell, mask):\n        # hack to handle LSTM\n        h0 = hidden[0] if isinstance(hidden, tuple) else hidden\n        # [length + 1, batch, hidden_size]\n        output = input.new_zeros(input.size(0) + 1, *h0.size()) + h0\n        steps = range(input.size(0) - 1, -1, -1) if reverse else range(input.size(0))\n        # create batch index\n        batch_index = torch.arange(0, h0.size(0)).type_as(skip_connect)\n        for i in steps:\n            if mask is None or mask[i].data.min() > 0.5:\n                hidden_skip = output[skip_connect[i], batch_index]\n                hidden = cell(input[i], hidden, hidden_skip)\n            elif mask[i].data.max() > 0.5:\n                hidden_skip = output[skip_connect[i], batch_index]\n                hidden_next = cell(input[i], hidden, hidden_skip)\n                # hack to handle LSTM\n                if isinstance(hidden, tuple):\n                    hx, cx = hidden\n                    hp1, cp1 = hidden_next\n                    hidden = (hx + (hp1 - hx) * mask[i], cx + (cp1 - cx) * mask[i])\n                else:\n                    hidden = hidden + (hidden_next - hidden) * mask[i]\n            # hack to handle LSTM\n            if reverse:\n                output[i] = hidden[0] if isinstance(hidden, tuple) else hidden\n            else:\n                output[i + 1] = hidden[0] if isinstance(hidden, tuple) else hidden\n\n        if reverse:\n            # remove last position\n            output = output[:-1]\n        else:\n            # remove position 0\n            output = output[1:]\n\n        return hidden, output\n\n    return forward\n\n\ndef StackedRNN(inners, num_layers, lstm=False):\n    num_directions = len(inners)\n    total_layers = num_layers * num_directions\n\n    def reverse_skip_connection(skip_connect):\n        # TODO reverse skip connection for bidirectional rnn.\n        return skip_connect\n\n    def forward(input, skip_connect, hidden, cells, mask):\n        assert (len(cells) == total_layers)\n        next_hidden = []\n\n        skip_connect_forward = skip_connect\n        skip_connec_backward = reverse_skip_connection(skip_connect) if num_directions == 2 else None\n\n        if lstm:\n            hidden = list(zip(*hidden))\n\n        for i in range(num_layers):\n            all_output = []\n            for j, inner in enumerate(inners):\n                l = i * num_directions + j\n                skip_connect = skip_connect_forward if j == 0 else skip_connec_backward\n                hy, output = inner(input, skip_connect, hidden[l], cells[l], mask)\n                next_hidden.append(hy)\n                all_output.append(output)\n\n            input = torch.cat(all_output, input.dim() - 1)\n\n        if lstm:\n            next_h, next_c = zip(*next_hidden)\n            next_hidden = (\n                torch.cat(next_h, 0).view(total_layers, *next_h[0].size()),\n                torch.cat(next_c, 0).view(total_layers, *next_c[0].size())\n            )\n        else:\n            next_hidden = torch.cat(next_hidden, 0).view(total_layers, *next_hidden[0].size())\n\n        return next_hidden, input\n\n    return forward\n\n\ndef AutogradSkipConnectRNN(num_layers=1, batch_first=False, bidirectional=False, lstm=False):\n    rec_factory = SkipConnectRecurrent\n\n    if bidirectional:\n        layer = (rec_factory(), rec_factory(reverse=True))\n    else:\n        layer = (rec_factory(),)\n\n    func = StackedRNN(layer,\n                      num_layers,\n                      lstm=lstm)\n\n    def forward(input, skip_connect, cells, hidden, mask):\n        if batch_first:\n            input = input.transpose(0, 1)\n            skip_connect = skip_connect.transpose(0, 1)\n            if mask is not None:\n                mask = mask.transpose(0, 1)\n\n        nexth, output = func(input, skip_connect, hidden, cells, mask)\n\n        if batch_first:\n            output = output.transpose(0, 1)\n\n        return output, nexth\n\n    return forward\n\n\ndef SkipConnectStep():\n    def forward(input, hidden, hidden_skip, cell, mask):\n        if mask is None or mask.data.min() > 0.5:\n            hidden = cell(input, hidden, hidden_skip)\n        elif mask.data.max() > 0.5:\n            hidden_next = cell(input, hidden, hidden_skip)\n            # hack to handle LSTM\n            if isinstance(hidden, tuple):\n                hx, cx = hidden\n                hp1, cp1 = hidden_next\n                hidden = (hx + (hp1 - hx) * mask, cx + (cp1 - cx) * mask)\n            else:\n                hidden = hidden + (hidden_next - hidden) * mask\n        # hack to handle LSTM\n        output = hidden[0] if isinstance(hidden, tuple) else hidden\n\n        return hidden, output\n\n    return forward\n\n\ndef StackedStep(layer, num_layers, lstm=False):\n    def forward(input, hidden, hidden_skip, cells, mask):\n        assert (len(cells) == num_layers)\n        next_hidden = []\n\n        if lstm:\n            hidden = list(zip(*hidden))\n\n        for l in range(num_layers):\n            hy, output = layer(input, hidden[l], hidden_skip[l], cells[l], mask)\n            next_hidden.append(hy)\n            input = output\n\n        if lstm:\n            next_h, next_c = zip(*next_hidden)\n            next_hidden = (\n                torch.cat(next_h, 0).view(num_layers, *next_h[0].size()),\n                torch.cat(next_c, 0).view(num_layers, *next_c[0].size())\n            )\n        else:\n            next_hidden = torch.cat(next_hidden, 0).view(num_layers, *next_hidden[0].size())\n\n        return next_hidden, input\n\n    return forward\n\n\ndef AutogradSkipConnectStep(num_layers=1, lstm=False):\n    layer = SkipConnectStep()\n\n    func = StackedStep(layer,\n                       num_layers,\n                       lstm=lstm)\n\n    def forward(input, cells, hidden, hidden_skip, mask):\n        nexth, output = func(input, hidden, hidden_skip, cells, mask)\n        return output, nexth\n\n    return forward\n"""
neuronlp2/nn/_functions/variational_rnn.py,29,"b""__author__ = 'max'\n\nimport torch\nfrom torch.nn import functional as F\n\n\ndef VarRNNReLUCell(input, hidden, w_ih, w_hh, b_ih=None, b_hh=None, noise_in=None, noise_hidden=None):\n    if noise_in is not None:\n        input = input * noise_in\n    if noise_hidden is not None:\n        hidden = hidden * noise_hidden\n    hy = F.relu(F.linear(input, w_ih, b_ih) + F.linear(hidden, w_hh, b_hh))\n    return hy\n\n\ndef VarRNNTanhCell(input, hidden, w_ih, w_hh, b_ih=None, b_hh=None, noise_in=None, noise_hidden=None):\n    if noise_in is not None:\n        input = input * noise_in\n    if noise_hidden is not None:\n        hidden = hidden * noise_hidden\n    hy = torch.tanh(F.linear(input, w_ih, b_ih) + F.linear(hidden, w_hh, b_hh))\n    return hy\n\n\ndef VarLSTMCell(input, hidden, w_ih, w_hh, b_ih=None, b_hh=None, noise_in=None, noise_hidden=None):\n    input = input.expand(4, *input.size()) if noise_in is None else input.unsqueeze(0) * noise_in\n\n    hx, cx = hidden\n    hx = hx.expand(4, *hx.size()) if noise_hidden is None else hx.unsqueeze(0) * noise_hidden\n\n    gates = torch.baddbmm(b_ih.unsqueeze(1), input, w_ih) + torch.baddbmm(b_hh.unsqueeze(1), hx, w_hh)\n\n    ingate, forgetgate, cellgate, outgate = gates\n\n    ingate = torch.sigmoid(ingate)\n    forgetgate = torch.sigmoid(forgetgate)\n    cellgate = torch.tanh(cellgate)\n    outgate = torch.sigmoid(outgate)\n\n    cy = (forgetgate * cx) + (ingate * cellgate)\n    hy = outgate * torch.tanh(cy)\n\n    return hy, cy\n\n\ndef VarFastLSTMCell(input, hidden, w_ih, w_hh, b_ih=None, b_hh=None, noise_in=None, noise_hidden=None):\n    if noise_in is not None:\n        input = input * noise_in\n\n    hx, cx = hidden\n    if noise_hidden is not None:\n        hx = hx * noise_hidden\n    gates = F.linear(input, w_ih, b_ih) + F.linear(hx, w_hh, b_hh)\n\n    ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n\n    ingate = torch.sigmoid(ingate)\n    forgetgate = torch.sigmoid(forgetgate)\n    cellgate = torch.tanh(cellgate)\n    outgate = torch.sigmoid(outgate)\n\n    cy = (forgetgate * cx) + (ingate * cellgate)\n    hy = outgate * torch.tanh(cy)\n\n    return hy, cy\n\n\ndef VarGRUCell(input, hidden, w_ih, w_hh, b_ih=None, b_hh=None, noise_in=None, noise_hidden=None):\n    input = input.expand(3, *input.size()) if noise_in is None else input.unsqueeze(0) * noise_in\n    hx = hidden.expand(3, *hidden.size()) if noise_hidden is None else hidden.unsqueeze(0) * noise_hidden\n\n    gi = torch.baddbmm(b_ih.unsqueeze(1), input, w_ih)\n    gh = torch.baddbmm(b_hh.unsqueeze(1), hx, w_hh)\n    i_r, i_i, i_n = gi\n    h_r, h_i, h_n = gh\n\n    resetgate = torch.sigmoid(i_r + h_r)\n    inputgate = torch.sigmoid(i_i + h_i)\n    newgate = torch.tanh(i_n + resetgate * h_n)\n    hy = newgate + inputgate * (hidden - newgate)\n\n    return hy\n\n\ndef VarFastGRUCell(input, hidden, w_ih, w_hh, b_ih=None, b_hh=None, noise_in=None, noise_hidden=None):\n    if noise_in is not None:\n        input = input * noise_in\n\n    hx = hidden if noise_hidden is None else hidden * noise_hidden\n\n    gi = F.linear(input, w_ih, b_ih)\n    gh = F.linear(hx, w_hh, b_hh)\n    i_r, i_i, i_n = gi.chunk(3, 1)\n    h_r, h_i, h_n = gh.chunk(3, 1)\n\n    resetgate = torch.sigmoid(i_r + h_r)\n    inputgate = torch.sigmoid(i_i + h_i)\n    newgate = torch.tanh(i_n + resetgate * h_n)\n    hy = newgate + inputgate * (hidden - newgate)\n\n    return hy\n\n\ndef VarRecurrent(reverse=False):\n    def forward(input, hidden, cell, mask):\n        output = []\n        steps = range(input.size(0) - 1, -1, -1) if reverse else range(input.size(0))\n        for i in steps:\n            if mask is None or mask[i].data.min() > 0.5:\n                hidden = cell(input[i], hidden)\n            elif mask[i].data.max() > 0.5:\n                hidden_next = cell(input[i], hidden)\n                # hack to handle LSTM\n                if isinstance(hidden, tuple):\n                    hx, cx = hidden\n                    hp1, cp1 = hidden_next\n                    hidden = (hx + (hp1 - hx) * mask[i], cx + (cp1 - cx) * mask[i])\n                else:\n                    hidden = hidden + (hidden_next - hidden) * mask[i]\n            # hack to handle LSTM\n            output.append(hidden[0] if isinstance(hidden, tuple) else hidden)\n\n        if reverse:\n            output.reverse()\n        output = torch.cat(output, 0).view(input.size(0), *output[0].size())\n\n        return hidden, output\n\n    return forward\n\n\ndef StackedRNN(inners, num_layers, lstm=False):\n    num_directions = len(inners)\n    total_layers = num_layers * num_directions\n\n    def forward(input, hidden, cells, mask):\n        assert (len(cells) == total_layers)\n        next_hidden = []\n\n        if lstm:\n            hidden = list(zip(*hidden))\n\n        for i in range(num_layers):\n            all_output = []\n            for j, inner in enumerate(inners):\n                l = i * num_directions + j\n                hy, output = inner(input, hidden[l], cells[l], mask)\n                next_hidden.append(hy)\n                all_output.append(output)\n\n            input = torch.cat(all_output, input.dim() - 1)\n\n        if lstm:\n            next_h, next_c = zip(*next_hidden)\n            next_hidden = (\n                torch.cat(next_h, 0).view(total_layers, *next_h[0].size()),\n                torch.cat(next_c, 0).view(total_layers, *next_c[0].size())\n            )\n        else:\n            next_hidden = torch.cat(next_hidden, 0).view(total_layers, *next_hidden[0].size())\n\n        return next_hidden, input\n\n    return forward\n\n\ndef AutogradVarRNN(num_layers=1, batch_first=False, bidirectional=False, lstm=False):\n    rec_factory = VarRecurrent\n\n    if bidirectional:\n        layer = (rec_factory(), rec_factory(reverse=True))\n    else:\n        layer = (rec_factory(),)\n\n    func = StackedRNN(layer,\n                      num_layers,\n                      lstm=lstm)\n\n    def forward(input, cells, hidden, mask):\n        if batch_first:\n            input = input.transpose(0, 1)\n            if mask is not None:\n                mask = mask.transpose(0, 1)\n\n        nexth, output = func(input, hidden, cells, mask)\n\n        if batch_first:\n            output = output.transpose(0, 1)\n\n        return output, nexth\n\n    return forward\n\n\ndef VarRNNStep():\n    def forward(input, hidden, cell, mask):\n        if mask is None or mask.data.min() > 0.5:\n            hidden = cell(input, hidden)\n        elif mask.data.max() > 0.5:\n            hidden_next = cell(input, hidden)\n            # hack to handle LSTM\n            if isinstance(hidden, tuple):\n                hx, cx = hidden\n                hp1, cp1 = hidden_next\n                hidden = (hx + (hp1 - hx) * mask, cx + (cp1 - cx) * mask)\n            else:\n                hidden = hidden + (hidden_next - hidden) * mask\n        # hack to handle LSTM\n        output = hidden[0] if isinstance(hidden, tuple) else hidden\n\n        return hidden, output\n\n    return forward\n\n\ndef StackedStep(layer, num_layers, lstm=False):\n    def forward(input, hidden, cells, mask):\n        assert (len(cells) == num_layers)\n        next_hidden = []\n\n        if lstm:\n            hidden = list(zip(*hidden))\n\n        for l in range(num_layers):\n            hy, output = layer(input, hidden[l], cells[l], mask)\n            next_hidden.append(hy)\n            input = output\n\n        if lstm:\n            next_h, next_c = zip(*next_hidden)\n            next_hidden = (\n                torch.cat(next_h, 0).view(num_layers, *next_h[0].size()),\n                torch.cat(next_c, 0).view(num_layers, *next_c[0].size())\n            )\n        else:\n            next_hidden = torch.cat(next_hidden, 0).view(num_layers, *next_hidden[0].size())\n\n        return next_hidden, input\n\n    return forward\n\n\ndef AutogradVarRNNStep(num_layers=1, lstm=False):\n    layer = VarRNNStep()\n\n    func = StackedStep(layer,\n                       num_layers,\n                       lstm=lstm)\n\n    def forward(input, cells, hidden, mask):\n        nexth, output = func(input, hidden, cells, mask)\n        return output, nexth\n\n    return forward\n"""
