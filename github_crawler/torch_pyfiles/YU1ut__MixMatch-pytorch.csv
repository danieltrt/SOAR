file_path,api_count,code
train.py,22,"b'from __future__ import print_function\n\nimport argparse\nimport os\nimport shutil\nimport time\nimport random\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\n\nimport models.wideresnet as models\nimport dataset.cifar10 as dataset\nfrom utils import Bar, Logger, AverageMeter, accuracy, mkdir_p, savefig\nfrom tensorboardX import SummaryWriter\n\nparser = argparse.ArgumentParser(description=\'PyTorch MixMatch Training\')\n# Optimization options\nparser.add_argument(\'--epochs\', default=1024, type=int, metavar=\'N\',\n                    help=\'number of total epochs to run\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'--batch-size\', default=64, type=int, metavar=\'N\',\n                    help=\'train batchsize\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=0.002, type=float,\n                    metavar=\'LR\', help=\'initial learning rate\')\n# Checkpoints\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\n# Miscs\nparser.add_argument(\'--manualSeed\', type=int, default=0, help=\'manual seed\')\n#Device options\nparser.add_argument(\'--gpu\', default=\'0\', type=str,\n                    help=\'id(s) for CUDA_VISIBLE_DEVICES\')\n#Method options\nparser.add_argument(\'--n-labeled\', type=int, default=250,\n                        help=\'Number of labeled data\')\nparser.add_argument(\'--val-iteration\', type=int, default=1024,\n                        help=\'Number of labeled data\')\nparser.add_argument(\'--out\', default=\'result\',\n                        help=\'Directory to output the result\')\nparser.add_argument(\'--alpha\', default=0.75, type=float)\nparser.add_argument(\'--lambda-u\', default=75, type=float)\nparser.add_argument(\'--T\', default=0.5, type=float)\nparser.add_argument(\'--ema-decay\', default=0.999, type=float)\n\n\nargs = parser.parse_args()\nstate = {k: v for k, v in args._get_kwargs()}\n\n# Use CUDA\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\nuse_cuda = torch.cuda.is_available()\n\n# Random seed\nif args.manualSeed is None:\n    args.manualSeed = random.randint(1, 10000)\nnp.random.seed(args.manualSeed)\n\nbest_acc = 0  # best test accuracy\n\ndef main():\n    global best_acc\n\n    if not os.path.isdir(args.out):\n        mkdir_p(args.out)\n\n    # Data\n    print(f\'==> Preparing cifar10\')\n    transform_train = transforms.Compose([\n        dataset.RandomPadandCrop(32),\n        dataset.RandomFlip(),\n        dataset.ToTensor(),\n    ])\n\n    transform_val = transforms.Compose([\n        dataset.ToTensor(),\n    ])\n\n    train_labeled_set, train_unlabeled_set, val_set, test_set = dataset.get_cifar10(\'./data\', args.n_labeled, transform_train=transform_train, transform_val=transform_val)\n    labeled_trainloader = data.DataLoader(train_labeled_set, batch_size=args.batch_size, shuffle=True, num_workers=0, drop_last=True)\n    unlabeled_trainloader = data.DataLoader(train_unlabeled_set, batch_size=args.batch_size, shuffle=True, num_workers=0, drop_last=True)\n    val_loader = data.DataLoader(val_set, batch_size=args.batch_size, shuffle=False, num_workers=0)\n    test_loader = data.DataLoader(test_set, batch_size=args.batch_size, shuffle=False, num_workers=0)\n\n    # Model\n    print(""==> creating WRN-28-2"")\n\n    def create_model(ema=False):\n        model = models.WideResNet(num_classes=10)\n        model = model.cuda()\n\n        if ema:\n            for param in model.parameters():\n                param.detach_()\n\n        return model\n\n    model = create_model()\n    ema_model = create_model(ema=True)\n\n    cudnn.benchmark = True\n    print(\'    Total params: %.2fM\' % (sum(p.numel() for p in model.parameters())/1000000.0))\n\n    train_criterion = SemiLoss()\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n\n    ema_optimizer= WeightEMA(model, ema_model, alpha=args.ema_decay)\n    start_epoch = 0\n\n    # Resume\n    title = \'noisy-cifar-10\'\n    if args.resume:\n        # Load checkpoint.\n        print(\'==> Resuming from checkpoint..\')\n        assert os.path.isfile(args.resume), \'Error: no checkpoint directory found!\'\n        args.out = os.path.dirname(args.resume)\n        checkpoint = torch.load(args.resume)\n        best_acc = checkpoint[\'best_acc\']\n        start_epoch = checkpoint[\'epoch\']\n        model.load_state_dict(checkpoint[\'state_dict\'])\n        ema_model.load_state_dict(checkpoint[\'ema_state_dict\'])\n        optimizer.load_state_dict(checkpoint[\'optimizer\'])\n        logger = Logger(os.path.join(args.out, \'log.txt\'), title=title, resume=True)\n    else:\n        logger = Logger(os.path.join(args.out, \'log.txt\'), title=title)\n        logger.set_names([\'Train Loss\', \'Train Loss X\', \'Train Loss U\',  \'Valid Loss\', \'Valid Acc.\', \'Test Loss\', \'Test Acc.\'])\n\n    writer = SummaryWriter(args.out)\n    step = 0\n    test_accs = []\n    # Train and val\n    for epoch in range(start_epoch, args.epochs):\n\n        print(\'\\nEpoch: [%d | %d] LR: %f\' % (epoch + 1, args.epochs, state[\'lr\']))\n\n        train_loss, train_loss_x, train_loss_u = train(labeled_trainloader, unlabeled_trainloader, model, optimizer, ema_optimizer, train_criterion, epoch, use_cuda)\n        _, train_acc = validate(labeled_trainloader, ema_model, criterion, epoch, use_cuda, mode=\'Train Stats\')\n        val_loss, val_acc = validate(val_loader, ema_model, criterion, epoch, use_cuda, mode=\'Valid Stats\')\n        test_loss, test_acc = validate(test_loader, ema_model, criterion, epoch, use_cuda, mode=\'Test Stats \')\n\n        step = args.val_iteration * (epoch + 1)\n\n        writer.add_scalar(\'losses/train_loss\', train_loss, step)\n        writer.add_scalar(\'losses/valid_loss\', val_loss, step)\n        writer.add_scalar(\'losses/test_loss\', test_loss, step)\n\n        writer.add_scalar(\'accuracy/train_acc\', train_acc, step)\n        writer.add_scalar(\'accuracy/val_acc\', val_acc, step)\n        writer.add_scalar(\'accuracy/test_acc\', test_acc, step)\n\n        # append logger file\n        logger.append([train_loss, train_loss_x, train_loss_u, val_loss, val_acc, test_loss, test_acc])\n\n        # save model\n        is_best = val_acc > best_acc\n        best_acc = max(val_acc, best_acc)\n        save_checkpoint({\n                \'epoch\': epoch + 1,\n                \'state_dict\': model.state_dict(),\n                \'ema_state_dict\': ema_model.state_dict(),\n                \'acc\': val_acc,\n                \'best_acc\': best_acc,\n                \'optimizer\' : optimizer.state_dict(),\n            }, is_best)\n        test_accs.append(test_acc)\n    logger.close()\n    writer.close()\n\n    print(\'Best acc:\')\n    print(best_acc)\n\n    print(\'Mean acc:\')\n    print(np.mean(test_accs[-20:]))\n\n\ndef train(labeled_trainloader, unlabeled_trainloader, model, optimizer, ema_optimizer, criterion, epoch, use_cuda):\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    losses_x = AverageMeter()\n    losses_u = AverageMeter()\n    ws = AverageMeter()\n    end = time.time()\n\n    bar = Bar(\'Training\', max=args.val_iteration)\n    labeled_train_iter = iter(labeled_trainloader)\n    unlabeled_train_iter = iter(unlabeled_trainloader)\n\n    model.train()\n    for batch_idx in range(args.val_iteration):\n        try:\n            inputs_x, targets_x = labeled_train_iter.next()\n        except:\n            labeled_train_iter = iter(labeled_trainloader)\n            inputs_x, targets_x = labeled_train_iter.next()\n\n        try:\n            (inputs_u, inputs_u2), _ = unlabeled_train_iter.next()\n        except:\n            unlabeled_train_iter = iter(unlabeled_trainloader)\n            (inputs_u, inputs_u2), _ = unlabeled_train_iter.next()\n\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        batch_size = inputs_x.size(0)\n\n        # Transform label to one-hot\n        targets_x = torch.zeros(batch_size, 10).scatter_(1, targets_x.view(-1,1), 1)\n\n        if use_cuda:\n            inputs_x, targets_x = inputs_x.cuda(), targets_x.cuda(non_blocking=True)\n            inputs_u = inputs_u.cuda()\n            inputs_u2 = inputs_u2.cuda()\n\n\n        with torch.no_grad():\n            # compute guessed labels of unlabel samples\n            outputs_u = model(inputs_u)\n            outputs_u2 = model(inputs_u2)\n            p = (torch.softmax(outputs_u, dim=1) + torch.softmax(outputs_u2, dim=1)) / 2\n            pt = p**(1/args.T)\n            targets_u = pt / pt.sum(dim=1, keepdim=True)\n            targets_u = targets_u.detach()\n\n        # mixup\n        all_inputs = torch.cat([inputs_x, inputs_u, inputs_u2], dim=0)\n        all_targets = torch.cat([targets_x, targets_u, targets_u], dim=0)\n\n        l = np.random.beta(args.alpha, args.alpha)\n\n        l = max(l, 1-l)\n\n        idx = torch.randperm(all_inputs.size(0))\n\n        input_a, input_b = all_inputs, all_inputs[idx]\n        target_a, target_b = all_targets, all_targets[idx]\n\n        mixed_input = l * input_a + (1 - l) * input_b\n        mixed_target = l * target_a + (1 - l) * target_b\n\n        # interleave labeled and unlabed samples between batches to get correct batchnorm calculation \n        mixed_input = list(torch.split(mixed_input, batch_size))\n        mixed_input = interleave(mixed_input, batch_size)\n\n        logits = [model(mixed_input[0])]\n        for input in mixed_input[1:]:\n            logits.append(model(input))\n\n        # put interleaved samples back\n        logits = interleave(logits, batch_size)\n        logits_x = logits[0]\n        logits_u = torch.cat(logits[1:], dim=0)\n\n        Lx, Lu, w = criterion(logits_x, mixed_target[:batch_size], logits_u, mixed_target[batch_size:], epoch+batch_idx/args.val_iteration)\n\n        loss = Lx + w * Lu\n\n        # record loss\n        losses.update(loss.item(), inputs_x.size(0))\n        losses_x.update(Lx.item(), inputs_x.size(0))\n        losses_u.update(Lu.item(), inputs_x.size(0))\n        ws.update(w, inputs_x.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        ema_optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # plot progress\n        bar.suffix  = \'({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | Loss_x: {loss_x:.4f} | Loss_u: {loss_u:.4f} | W: {w:.4f}\'.format(\n                    batch=batch_idx + 1,\n                    size=args.val_iteration,\n                    data=data_time.avg,\n                    bt=batch_time.avg,\n                    total=bar.elapsed_td,\n                    eta=bar.eta_td,\n                    loss=losses.avg,\n                    loss_x=losses_x.avg,\n                    loss_u=losses_u.avg,\n                    w=ws.avg,\n                    )\n        bar.next()\n    bar.finish()\n\n    return (losses.avg, losses_x.avg, losses_u.avg,)\n\ndef validate(valloader, model, criterion, epoch, use_cuda, mode):\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to evaluate mode\n    model.eval()\n\n    end = time.time()\n    bar = Bar(f\'{mode}\', max=len(valloader))\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(valloader):\n            # measure data loading time\n            data_time.update(time.time() - end)\n\n            if use_cuda:\n                inputs, targets = inputs.cuda(), targets.cuda(non_blocking=True)\n            # compute output\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n\n            # measure accuracy and record loss\n            prec1, prec5 = accuracy(outputs, targets, topk=(1, 5))\n            losses.update(loss.item(), inputs.size(0))\n            top1.update(prec1.item(), inputs.size(0))\n            top5.update(prec5.item(), inputs.size(0))\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            # plot progress\n            bar.suffix  = \'({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}\'.format(\n                        batch=batch_idx + 1,\n                        size=len(valloader),\n                        data=data_time.avg,\n                        bt=batch_time.avg,\n                        total=bar.elapsed_td,\n                        eta=bar.eta_td,\n                        loss=losses.avg,\n                        top1=top1.avg,\n                        top5=top5.avg,\n                        )\n            bar.next()\n        bar.finish()\n    return (losses.avg, top1.avg)\n\ndef save_checkpoint(state, is_best, checkpoint=args.out, filename=\'checkpoint.pth.tar\'):\n    filepath = os.path.join(checkpoint, filename)\n    torch.save(state, filepath)\n    if is_best:\n        shutil.copyfile(filepath, os.path.join(checkpoint, \'model_best.pth.tar\'))\n\ndef linear_rampup(current, rampup_length=args.epochs):\n    if rampup_length == 0:\n        return 1.0\n    else:\n        current = np.clip(current / rampup_length, 0.0, 1.0)\n        return float(current)\n\nclass SemiLoss(object):\n    def __call__(self, outputs_x, targets_x, outputs_u, targets_u, epoch):\n        probs_u = torch.softmax(outputs_u, dim=1)\n\n        Lx = -torch.mean(torch.sum(F.log_softmax(outputs_x, dim=1) * targets_x, dim=1))\n        Lu = torch.mean((probs_u - targets_u)**2)\n\n        return Lx, Lu, args.lambda_u * linear_rampup(epoch)\n\nclass WeightEMA(object):\n    def __init__(self, model, ema_model, alpha=0.999):\n        self.model = model\n        self.ema_model = ema_model\n        self.alpha = alpha\n        self.params = list(model.state_dict().values())\n        self.ema_params = list(ema_model.state_dict().values())\n        self.wd = 0.02 * args.lr\n\n        for param, ema_param in zip(self.params, self.ema_params):\n            param.data.copy_(ema_param.data)\n\n    def step(self):\n        one_minus_alpha = 1.0 - self.alpha\n        for param, ema_param in zip(self.params, self.ema_params):\n            ema_param.mul_(self.alpha)\n            ema_param.add_(param * one_minus_alpha)\n            # customized weight decay\n            param.mul_(1 - self.wd)\n\ndef interleave_offsets(batch, nu):\n    groups = [batch // (nu + 1)] * (nu + 1)\n    for x in range(batch - sum(groups)):\n        groups[-x - 1] += 1\n    offsets = [0]\n    for g in groups:\n        offsets.append(offsets[-1] + g)\n    assert offsets[-1] == batch\n    return offsets\n\n\ndef interleave(xy, batch):\n    nu = len(xy) - 1\n    offsets = interleave_offsets(batch, nu)\n    xy = [[v[offsets[p]:offsets[p + 1]] for p in range(nu + 1)] for v in xy]\n    for i in range(1, nu + 1):\n        xy[0][i], xy[i][i] = xy[i][i], xy[0][i]\n    return [torch.cat(v, dim=0) for v in xy]\n\nif __name__ == \'__main__\':\n    main()\n'"
dataset/cifar10.py,1,"b'import numpy as np\nfrom PIL import Image\n\nimport torchvision\nimport torch\n\nclass TransformTwice:\n    def __init__(self, transform):\n        self.transform = transform\n\n    def __call__(self, inp):\n        out1 = self.transform(inp)\n        out2 = self.transform(inp)\n        return out1, out2\n\ndef get_cifar10(root, n_labeled,\n                 transform_train=None, transform_val=None,\n                 download=True):\n\n    base_dataset = torchvision.datasets.CIFAR10(root, train=True, download=download)\n    train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(base_dataset.targets, int(n_labeled/10))\n\n    train_labeled_dataset = CIFAR10_labeled(root, train_labeled_idxs, train=True, transform=transform_train)\n    train_unlabeled_dataset = CIFAR10_unlabeled(root, train_unlabeled_idxs, train=True, transform=TransformTwice(transform_train))\n    val_dataset = CIFAR10_labeled(root, val_idxs, train=True, transform=transform_val, download=True)\n    test_dataset = CIFAR10_labeled(root, train=False, transform=transform_val, download=True)\n\n    print (f""#Labeled: {len(train_labeled_idxs)} #Unlabeled: {len(train_unlabeled_idxs)} #Val: {len(val_idxs)}"")\n    return train_labeled_dataset, train_unlabeled_dataset, val_dataset, test_dataset\n    \n\ndef train_val_split(labels, n_labeled_per_class):\n    labels = np.array(labels)\n    train_labeled_idxs = []\n    train_unlabeled_idxs = []\n    val_idxs = []\n\n    for i in range(10):\n        idxs = np.where(labels == i)[0]\n        np.random.shuffle(idxs)\n        train_labeled_idxs.extend(idxs[:n_labeled_per_class])\n        train_unlabeled_idxs.extend(idxs[n_labeled_per_class:-500])\n        val_idxs.extend(idxs[-500:])\n    np.random.shuffle(train_labeled_idxs)\n    np.random.shuffle(train_unlabeled_idxs)\n    np.random.shuffle(val_idxs)\n\n    return train_labeled_idxs, train_unlabeled_idxs, val_idxs\n\ncifar10_mean = (0.4914, 0.4822, 0.4465) # equals np.mean(train_set.train_data, axis=(0,1,2))/255\ncifar10_std = (0.2471, 0.2435, 0.2616) # equals np.std(train_set.train_data, axis=(0,1,2))/255\n\ndef normalise(x, mean=cifar10_mean, std=cifar10_std):\n    x, mean, std = [np.array(a, np.float32) for a in (x, mean, std)]\n    x -= mean*255\n    x *= 1.0/(255*std)\n    return x\n\ndef transpose(x, source=\'NHWC\', target=\'NCHW\'):\n    return x.transpose([source.index(d) for d in target]) \n\ndef pad(x, border=4):\n    return np.pad(x, [(0, 0), (border, border), (border, border)], mode=\'reflect\')\n\nclass RandomPadandCrop(object):\n    """"""Crop randomly the image.\n\n    Args:\n        output_size (tuple or int): Desired output size. If int, square crop\n            is made.\n    """"""\n\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        if isinstance(output_size, int):\n            self.output_size = (output_size, output_size)\n        else:\n            assert len(output_size) == 2\n            self.output_size = output_size\n\n    def __call__(self, x):\n        x = pad(x, 4)\n\n        h, w = x.shape[1:]\n        new_h, new_w = self.output_size\n\n        top = np.random.randint(0, h - new_h)\n        left = np.random.randint(0, w - new_w)\n\n        x = x[:, top: top + new_h, left: left + new_w]\n\n        return x\n\nclass RandomFlip(object):\n    """"""Flip randomly the image.\n    """"""\n    def __call__(self, x):\n        if np.random.rand() < 0.5:\n            x = x[:, :, ::-1]\n\n        return x.copy()\n\nclass GaussianNoise(object):\n    """"""Add gaussian noise to the image.\n    """"""\n    def __call__(self, x):\n        c, h, w = x.shape\n        x += np.random.randn(c, h, w) * 0.15\n        return x\n\nclass ToTensor(object):\n    """"""Transform the image to tensor.\n    """"""\n    def __call__(self, x):\n        x = torch.from_numpy(x)\n        return x\n\nclass CIFAR10_labeled(torchvision.datasets.CIFAR10):\n\n    def __init__(self, root, indexs=None, train=True,\n                 transform=None, target_transform=None,\n                 download=False):\n        super(CIFAR10_labeled, self).__init__(root, train=train,\n                 transform=transform, target_transform=target_transform,\n                 download=download)\n        if indexs is not None:\n            self.data = self.data[indexs]\n            self.targets = np.array(self.targets)[indexs]\n        self.data = transpose(normalise(self.data))\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n\n        Returns:\n            tuple: (image, target) where target is index of the target class.\n        """"""\n        img, target = self.data[index], self.targets[index]\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return img, target\n    \n\nclass CIFAR10_unlabeled(CIFAR10_labeled):\n\n    def __init__(self, root, indexs, train=True,\n                 transform=None, target_transform=None,\n                 download=False):\n        super(CIFAR10_unlabeled, self).__init__(root, indexs, train=train,\n                 transform=transform, target_transform=target_transform,\n                 download=download)\n        self.targets = np.array([-1 for i in range(len(self.targets))])\n        '"
models/wideresnet.py,3,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_planes, out_planes, stride, dropRate=0.0, activate_before_residual=False):\n        super(BasicBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes, momentum=0.001)\n        self.relu1 = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_planes, momentum=0.001)\n        self.relu2 = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        self.droprate = dropRate\n        self.equalInOut = (in_planes == out_planes)\n        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n                               padding=0, bias=False) or None\n        self.activate_before_residual = activate_before_residual\n    def forward(self, x):\n        if not self.equalInOut and self.activate_before_residual == True:\n            x = self.relu1(self.bn1(x))\n        else:\n            out = self.relu1(self.bn1(x))\n        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n        if self.droprate > 0:\n            out = F.dropout(out, p=self.droprate, training=self.training)\n        out = self.conv2(out)\n        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n\nclass NetworkBlock(nn.Module):\n    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0, activate_before_residual=False):\n        super(NetworkBlock, self).__init__()\n        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate, activate_before_residual)\n    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate, activate_before_residual):\n        layers = []\n        for i in range(int(nb_layers)):\n            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate, activate_before_residual))\n        return nn.Sequential(*layers)\n    def forward(self, x):\n        return self.layer(x)\n\nclass WideResNet(nn.Module):\n    def __init__(self, num_classes, depth=28, widen_factor=2, dropRate=0.0):\n        super(WideResNet, self).__init__()\n        nChannels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n        assert((depth - 4) % 6 == 0)\n        n = (depth - 4) / 6\n        block = BasicBlock\n        # 1st conv before any network block\n        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        # 1st block\n        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate, activate_before_residual=True)\n        # 2nd block\n        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n        # 3rd block\n        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n        # global average pooling and classifier\n        self.bn1 = nn.BatchNorm2d(nChannels[3], momentum=0.001)\n        self.relu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n        self.fc = nn.Linear(nChannels[3], num_classes)\n        self.nChannels = nChannels[3]\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight.data)\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.block1(out)\n        out = self.block2(out)\n        out = self.block3(out)\n        out = self.relu(self.bn1(out))\n        out = F.avg_pool2d(out, 8)\n        out = out.view(-1, self.nChannels)\n        return self.fc(out)'"
utils/__init__.py,0,"b'""""""Useful utils\n""""""\nfrom .misc import *\nfrom .logger import *\nfrom .eval import *\n\n# progress bar\nimport os, sys\nsys.path.append(os.path.join(os.path.dirname(__file__), ""progress""))\nfrom progress.bar import Bar as Bar'"
utils/eval.py,0,"b'from __future__ import print_function, absolute_import\n\n__all__ = [\'accuracy\']\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the precision@k for the specified values of k""""""\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res'"
utils/logger.py,0,"b'# A simple torch style logger\n# (C) Wei YANG 2017\nfrom __future__ import absolute_import\nimport matplotlib.pyplot as plt\nimport os\nimport sys\nimport numpy as np\n\n__all__ = [\'Logger\', \'LoggerMonitor\', \'savefig\']\n\ndef savefig(fname, dpi=None):\n    dpi = 150 if dpi == None else dpi\n    plt.savefig(fname, dpi=dpi)\n    \ndef plot_overlap(logger, names=None):\n    names = logger.names if names == None else names\n    numbers = logger.numbers\n    for _, name in enumerate(names):\n        x = np.arange(len(numbers[name]))\n        plt.plot(x, np.asarray(numbers[name]))\n    return [logger.title + \'(\' + name + \')\' for name in names]\n\nclass Logger(object):\n    \'\'\'Save training process to log file with simple plot function.\'\'\'\n    def __init__(self, fpath, title=None, resume=False): \n        self.file = None\n        self.resume = resume\n        self.title = \'\' if title == None else title\n        if fpath is not None:\n            if resume: \n                self.file = open(fpath, \'r\') \n                name = self.file.readline()\n                self.names = name.rstrip().split(\'\\t\')\n                self.numbers = {}\n                for _, name in enumerate(self.names):\n                    self.numbers[name] = []\n\n                for numbers in self.file:\n                    numbers = numbers.rstrip().split(\'\\t\')\n                    for i in range(0, len(numbers)):\n                        self.numbers[self.names[i]].append(numbers[i])\n                self.file.close()\n                self.file = open(fpath, \'a\')  \n            else:\n                self.file = open(fpath, \'w\')\n\n    def set_names(self, names):\n        if self.resume: \n            pass\n        # initialize numbers as empty list\n        self.numbers = {}\n        self.names = names\n        for _, name in enumerate(self.names):\n            self.file.write(name)\n            self.file.write(\'\\t\')\n            self.numbers[name] = []\n        self.file.write(\'\\n\')\n        self.file.flush()\n\n\n    def append(self, numbers):\n        assert len(self.names) == len(numbers), \'Numbers do not match names\'\n        for index, num in enumerate(numbers):\n            self.file.write(""{0:.6f}"".format(num))\n            self.file.write(\'\\t\')\n            self.numbers[self.names[index]].append(num)\n        self.file.write(\'\\n\')\n        self.file.flush()\n\n    def plot(self, names=None):   \n        names = self.names if names == None else names\n        numbers = self.numbers\n        for _, name in enumerate(names):\n            x = np.arange(len(numbers[name]))\n            plt.plot(x, np.asarray(numbers[name]))\n        plt.legend([self.title + \'(\' + name + \')\' for name in names])\n        plt.grid(True)\n\n    def close(self):\n        if self.file is not None:\n            self.file.close()\n\nclass LoggerMonitor(object):\n    \'\'\'Load and visualize multiple logs.\'\'\'\n    def __init__ (self, paths):\n        \'\'\'paths is a distionary with {name:filepath} pair\'\'\'\n        self.loggers = []\n        for title, path in paths.items():\n            logger = Logger(path, title=title, resume=True)\n            self.loggers.append(logger)\n\n    def plot(self, names=None):\n        plt.figure()\n        plt.subplot(121)\n        legend_text = []\n        for logger in self.loggers:\n            legend_text += plot_overlap(logger, names)\n        plt.legend(legend_text, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n        plt.grid(True)\n                    \nif __name__ == \'__main__\':\n    # # Example\n    # logger = Logger(\'test.txt\')\n    # logger.set_names([\'Train loss\', \'Valid loss\',\'Test loss\'])\n\n    # length = 100\n    # t = np.arange(length)\n    # train_loss = np.exp(-t / 10.0) + np.random.rand(length) * 0.1\n    # valid_loss = np.exp(-t / 10.0) + np.random.rand(length) * 0.1\n    # test_loss = np.exp(-t / 10.0) + np.random.rand(length) * 0.1\n\n    # for i in range(0, length):\n    #     logger.append([train_loss[i], valid_loss[i], test_loss[i]])\n    # logger.plot()\n\n    # Example: logger monitor\n    paths = {\n    \'resadvnet20\':\'/home/wyang/code/pytorch-classification/checkpoint/cifar10/resadvnet20/log.txt\', \n    \'resadvnet32\':\'/home/wyang/code/pytorch-classification/checkpoint/cifar10/resadvnet32/log.txt\',\n    \'resadvnet44\':\'/home/wyang/code/pytorch-classification/checkpoint/cifar10/resadvnet44/log.txt\',\n    }\n\n    field = [\'Valid Acc.\']\n\n    monitor = LoggerMonitor(paths)\n    monitor.plot(names=field)\n    savefig(\'test.eps\')'"
utils/misc.py,6,"b'\'\'\'Some helper functions for PyTorch, including:\n    - get_mean_and_std: calculate the mean and std value of dataset.\n    - msr_init: net parameter initialization.\n    - progress_bar: progress bar mimic xlua.progress.\n\'\'\'\nimport errno\nimport os\nimport sys\nimport time\nimport math\n\nimport torch.nn as nn\nimport torch.nn.init as init\nfrom torch.autograd import Variable\n\n__all__ = [\'get_mean_and_std\', \'init_params\', \'mkdir_p\', \'AverageMeter\']\n\n\ndef get_mean_and_std(dataset):\n    \'\'\'Compute the mean and std value of dataset.\'\'\'\n    dataloader = trainloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n\n    mean = torch.zeros(3)\n    std = torch.zeros(3)\n    print(\'==> Computing mean and std..\')\n    for inputs, targets in dataloader:\n        for i in range(3):\n            mean[i] += inputs[:,i,:,:].mean()\n            std[i] += inputs[:,i,:,:].std()\n    mean.div_(len(dataset))\n    std.div_(len(dataset))\n    return mean, std\n\ndef init_params(net):\n    \'\'\'Init layer parameters.\'\'\'\n    for m in net.modules():\n        if isinstance(m, nn.Conv2d):\n            init.kaiming_normal(m.weight, mode=\'fan_out\')\n            if m.bias:\n                init.constant(m.bias, 0)\n        elif isinstance(m, nn.BatchNorm2d):\n            init.constant(m.weight, 1)\n            init.constant(m.bias, 0)\n        elif isinstance(m, nn.Linear):\n            init.normal(m.weight, std=1e-3)\n            if m.bias:\n                init.constant(m.bias, 0)\n\ndef mkdir_p(path):\n    \'\'\'make dir if not exist\'\'\'\n    try:\n        os.makedirs(path)\n    except OSError as exc:  # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else:\n            raise\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value\n       Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n    """"""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count'"
