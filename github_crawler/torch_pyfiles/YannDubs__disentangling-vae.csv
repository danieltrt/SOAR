file_path,api_count,code
main.py,0,"b'import argparse\nimport logging\nimport sys\nimport os\nfrom configparser import ConfigParser\n\nfrom torch import optim\n\nfrom disvae import init_specific_model, Trainer, Evaluator\nfrom disvae.utils.modelIO import save_model, load_model, load_metadata\nfrom disvae.models.losses import LOSSES, RECON_DIST, get_loss_f\nfrom disvae.models.vae import MODELS\nfrom utils.datasets import get_dataloaders, get_img_size, DATASETS\nfrom utils.helpers import (create_safe_directory, get_device, set_seed, get_n_param,\n                           get_config_section, update_namespace_, FormatterNoDuplicate)\nfrom utils.visualize import GifTraversalsTraining\n\n\nCONFIG_FILE = ""hyperparam.ini""\nRES_DIR = ""results""\nLOG_LEVELS = list(logging._levelToName.values())\nADDITIONAL_EXP = [\'custom\', ""debug"", ""best_celeba"", ""best_dsprites""]\nEXPERIMENTS = ADDITIONAL_EXP + [""{}_{}"".format(loss, data)\n                                for loss in LOSSES\n                                for data in DATASETS]\n\n\ndef parse_arguments(args_to_parse):\n    """"""Parse the command line arguments.\n\n    Parameters\n    ----------\n    args_to_parse: list of str\n        Arguments to parse (splitted on whitespaces).\n    """"""\n    default_config = get_config_section([CONFIG_FILE], ""Custom"")\n\n    description = ""PyTorch implementation and evaluation of disentangled Variational AutoEncoders and metrics.""\n    parser = argparse.ArgumentParser(description=description,\n                                     formatter_class=FormatterNoDuplicate)\n\n    # General options\n    general = parser.add_argument_group(\'General options\')\n    general.add_argument(\'name\', type=str,\n                         help=""Name of the model for storing and loading purposes."")\n    general.add_argument(\'-L\', \'--log-level\', help=""Logging levels."",\n                         default=default_config[\'log_level\'], choices=LOG_LEVELS)\n    general.add_argument(\'--no-progress-bar\', action=\'store_true\',\n                         default=default_config[\'no_progress_bar\'],\n                         help=\'Disables progress bar.\')\n    general.add_argument(\'--no-cuda\', action=\'store_true\',\n                         default=default_config[\'no_cuda\'],\n                         help=\'Disables CUDA training, even when have one.\')\n    general.add_argument(\'-s\', \'--seed\', type=int, default=default_config[\'seed\'],\n                         help=\'Random seed. Can be `None` for stochastic behavior.\')\n\n    # Learning options\n    training = parser.add_argument_group(\'Training specific options\')\n    training.add_argument(\'--checkpoint-every\',\n                          type=int, default=default_config[\'checkpoint_every\'],\n                          help=\'Save a checkpoint of the trained model every n epoch.\')\n    training.add_argument(\'-d\', \'--dataset\', help=""Path to training data."",\n                          default=default_config[\'dataset\'], choices=DATASETS)\n    training.add_argument(\'-x\', \'--experiment\',\n                          default=default_config[\'experiment\'], choices=EXPERIMENTS,\n                          help=\'Predefined experiments to run. If not `custom` this will overwrite some other arguments.\')\n    training.add_argument(\'-e\', \'--epochs\', type=int,\n                          default=default_config[\'epochs\'],\n                          help=\'Maximum number of epochs to run for.\')\n    training.add_argument(\'-b\', \'--batch-size\', type=int,\n                          default=default_config[\'batch_size\'],\n                          help=\'Batch size for training.\')\n    training.add_argument(\'--lr\', type=float, default=default_config[\'lr\'],\n                          help=\'Learning rate.\')\n\n    # Model Options\n    model = parser.add_argument_group(\'Model specfic options\')\n    model.add_argument(\'-m\', \'--model-type\',\n                       default=default_config[\'model\'], choices=MODELS,\n                       help=\'Type of encoder and decoder to use.\')\n    model.add_argument(\'-z\', \'--latent-dim\', type=int,\n                       default=default_config[\'latent_dim\'],\n                       help=\'Dimension of the latent variable.\')\n    model.add_argument(\'-l\', \'--loss\',\n                       default=default_config[\'loss\'], choices=LOSSES,\n                       help=""Type of VAE loss function to use."")\n    model.add_argument(\'-r\', \'--rec-dist\', default=default_config[\'rec_dist\'],\n                       choices=RECON_DIST,\n                       help=""Form of the likelihood ot use for each pixel."")\n    model.add_argument(\'-a\', \'--reg-anneal\', type=float,\n                       default=default_config[\'reg_anneal\'],\n                       help=""Number of annealing steps where gradually adding the regularisation. What is annealed is specific to each loss."")\n\n    # Loss Specific Options\n    betaH = parser.add_argument_group(\'BetaH specific parameters\')\n    betaH.add_argument(\'--betaH-B\', type=float,\n                       default=default_config[\'betaH_B\'],\n                       help=""Weight of the KL (beta in the paper)."")\n\n    betaB = parser.add_argument_group(\'BetaB specific parameters\')\n    betaB.add_argument(\'--betaB-initC\', type=float,\n                       default=default_config[\'betaB_initC\'],\n                       help=""Starting annealed capacity."")\n    betaB.add_argument(\'--betaB-finC\', type=float,\n                       default=default_config[\'betaB_finC\'],\n                       help=""Final annealed capacity."")\n    betaB.add_argument(\'--betaB-G\', type=float,\n                       default=default_config[\'betaB_G\'],\n                       help=""Weight of the KL divergence term (gamma in the paper)."")\n\n    factor = parser.add_argument_group(\'factor VAE specific parameters\')\n    factor.add_argument(\'--factor-G\', type=float,\n                        default=default_config[\'factor_G\'],\n                        help=""Weight of the TC term (gamma in the paper)."")\n    factor.add_argument(\'--lr-disc\', type=float,\n                        default=default_config[\'lr_disc\'],\n                        help=\'Learning rate of the discriminator.\')\n\n    btcvae = parser.add_argument_group(\'beta-tcvae specific parameters\')\n    btcvae.add_argument(\'--btcvae-A\', type=float,\n                        default=default_config[\'btcvae_A\'],\n                        help=""Weight of the MI term (alpha in the paper)."")\n    btcvae.add_argument(\'--btcvae-G\', type=float,\n                        default=default_config[\'btcvae_G\'],\n                        help=""Weight of the dim-wise KL term (gamma in the paper)."")\n    btcvae.add_argument(\'--btcvae-B\', type=float,\n                        default=default_config[\'btcvae_B\'],\n                        help=""Weight of the TC term (beta in the paper)."")\n\n    # Learning options\n    evaluation = parser.add_argument_group(\'Evaluation specific options\')\n    evaluation.add_argument(\'--is-eval-only\', action=\'store_true\',\n                            default=default_config[\'is_eval_only\'],\n                            help=\'Whether to only evaluate using precomputed model `name`.\')\n    evaluation.add_argument(\'--is-metrics\', action=\'store_true\',\n                            default=default_config[\'is_metrics\'],\n                            help=""Whether to compute the disentangled metrcics. Currently only possible with `dsprites` as it is the only dataset with known true factors of variations."")\n    evaluation.add_argument(\'--no-test\', action=\'store_true\',\n                            default=default_config[\'no_test\'],\n                            help=""Whether not to compute the test losses.`"")\n    evaluation.add_argument(\'--eval-batchsize\', type=int,\n                            default=default_config[\'eval_batchsize\'],\n                            help=\'Batch size for evaluation.\')\n\n    args = parser.parse_args(args_to_parse)\n    if args.experiment != \'custom\':\n        if args.experiment not in ADDITIONAL_EXP:\n            # update all common sections first\n            model, dataset = args.experiment.split(""_"")\n            common_data = get_config_section([CONFIG_FILE], ""Common_{}"".format(dataset))\n            update_namespace_(args, common_data)\n            common_model = get_config_section([CONFIG_FILE], ""Common_{}"".format(model))\n            update_namespace_(args, common_model)\n\n        try:\n            experiments_config = get_config_section([CONFIG_FILE], args.experiment)\n            update_namespace_(args, experiments_config)\n        except KeyError as e:\n            if args.experiment in ADDITIONAL_EXP:\n                raise e  # only reraise if didn\'t use common section\n\n    return args\n\n\ndef main(args):\n    """"""Main train and evaluation function.\n\n    Parameters\n    ----------\n    args: argparse.Namespace\n        Arguments\n    """"""\n    formatter = logging.Formatter(\'%(asctime)s %(levelname)s - %(funcName)s: %(message)s\',\n                                  ""%H:%M:%S"")\n    logger = logging.getLogger(__name__)\n    logger.setLevel(args.log_level.upper())\n    stream = logging.StreamHandler()\n    stream.setLevel(args.log_level.upper())\n    stream.setFormatter(formatter)\n    logger.addHandler(stream)\n\n    set_seed(args.seed)\n    device = get_device(is_gpu=not args.no_cuda)\n    exp_dir = os.path.join(RES_DIR, args.name)\n    logger.info(""Root directory for saving and loading experiments: {}"".format(exp_dir))\n\n    if not args.is_eval_only:\n\n        create_safe_directory(exp_dir, logger=logger)\n\n        if args.loss == ""factor"":\n            logger.info(""FactorVae needs 2 batches per iteration. To replicate this behavior while being consistent, we double the batch size and the the number of epochs."")\n            args.batch_size *= 2\n            args.epochs *= 2\n\n        # PREPARES DATA\n        train_loader = get_dataloaders(args.dataset,\n                                       batch_size=args.batch_size,\n                                       logger=logger)\n        logger.info(""Train {} with {} samples"".format(args.dataset, len(train_loader.dataset)))\n\n        # PREPARES MODEL\n        args.img_size = get_img_size(args.dataset)  # stores for metadata\n        model = init_specific_model(args.model_type, args.img_size, args.latent_dim)\n        logger.info(\'Num parameters in model: {}\'.format(get_n_param(model)))\n\n        # TRAINS\n        optimizer = optim.Adam(model.parameters(), lr=args.lr)\n\n        model = model.to(device)  # make sure trainer and viz on same device\n        gif_visualizer = GifTraversalsTraining(model, args.dataset, exp_dir)\n        loss_f = get_loss_f(args.loss,\n                            n_data=len(train_loader.dataset),\n                            device=device,\n                            **vars(args))\n        trainer = Trainer(model, optimizer, loss_f,\n                          device=device,\n                          logger=logger,\n                          save_dir=exp_dir,\n                          is_progress_bar=not args.no_progress_bar,\n                          gif_visualizer=gif_visualizer)\n        trainer(train_loader,\n                epochs=args.epochs,\n                checkpoint_every=args.checkpoint_every,)\n\n        # SAVE MODEL AND EXPERIMENT INFORMATION\n        save_model(trainer.model, exp_dir, metadata=vars(args))\n\n    if args.is_metrics or not args.no_test:\n        model = load_model(exp_dir, is_gpu=not args.no_cuda)\n        metadata = load_metadata(exp_dir)\n        # TO-DO: currently uses train datatset\n        test_loader = get_dataloaders(metadata[""dataset""],\n                                      batch_size=args.eval_batchsize,\n                                      shuffle=False,\n                                      logger=logger)\n        loss_f = get_loss_f(args.loss,\n                            n_data=len(test_loader.dataset),\n                            device=device,\n                            **vars(args))\n        evaluator = Evaluator(model, loss_f,\n                              device=device,\n                              logger=logger,\n                              save_dir=exp_dir,\n                              is_progress_bar=not args.no_progress_bar)\n\n        evaluator(test_loader, is_metrics=args.is_metrics, is_losses=not args.no_test)\n\n\nif __name__ == \'__main__\':\n    args = parse_arguments(sys.argv[1:])\n    main(args)\n'"
main_viz.py,0,"b'import argparse\nimport os\nimport sys\n\nfrom utils.helpers import FormatterNoDuplicate, check_bounds, set_seed\nfrom utils.visualize import Visualizer\nfrom utils.viz_helpers import get_samples\nfrom main import RES_DIR\nfrom disvae.utils.modelIO import load_model, load_metadata\n\n\nPLOT_TYPES = [\'generate-samples\', \'data-samples\', \'reconstruct\', ""traversals"",\n              \'reconstruct-traverse\', ""gif-traversals"", ""all""]\n\n\ndef parse_arguments(args_to_parse):\n    """"""Parse the command line arguments.\n\n    Parameters\n    ----------\n    args_to_parse: list of str\n        Arguments to parse (splitted on whitespaces).\n    """"""\n    description = ""CLI for plotting using pretrained models of `disvae`""\n    parser = argparse.ArgumentParser(description=description,\n                                     formatter_class=FormatterNoDuplicate)\n\n    parser.add_argument(\'name\', type=str,\n                        help=""Name of the model for storing and loading purposes."")\n    parser.add_argument(""plots"", type=str, nargs=\'+\', choices=PLOT_TYPES,\n                        help=""List of all plots to generate. `generate-samples`: random decoded samples. `data-samples` samples from the dataset. `reconstruct` first rnows//2 will be the original and rest will be the corresponding reconstructions. `traversals` traverses the most important rnows dimensions with ncols different samples from the prior or posterior. `reconstruct-traverse` first row for original, second are reconstructions, rest are traversals. `gif-traversals` grid of gifs where rows are latent dimensions, columns are examples, each gif shows posterior traversals. `all` runs every plot."")\n    parser.add_argument(\'-s\', \'--seed\', type=int, default=None,\n                        help=\'Random seed. Can be `None` for stochastic behavior.\')\n    parser.add_argument(\'-r\', \'--n-rows\', type=int, default=6,\n                        help=\'The number of rows to visualize (if applicable).\')\n    parser.add_argument(\'-c\', \'--n-cols\', type=int, default=7,\n                        help=\'The number of columns to visualize (if applicable).\')\n    parser.add_argument(\'-t\', \'--max-traversal\', default=2,\n                        type=lambda v: check_bounds(v, lb=0, is_inclusive=False,\n                                                    type=float, name=""max-traversal""),\n                        help=\'The maximum displacement induced by a latent traversal. Symmetrical traversals are assumed. If `m>=0.5` then uses absolute value traversal, if `m<0.5` uses a percentage of the distribution (quantile). E.g. for the prior the distribution is a standard normal so `m=0.45` corresponds to an absolute value of `1.645` because `2m=90%%` of a standard normal is between `-1.645` and `1.645`. Note in the case of the posterior, the distribution is not standard normal anymore.\')\n    parser.add_argument(\'-i\', \'--idcs\', type=int, nargs=\'+\', default=[],\n                        help=\'List of indices to of images to put at the begining of the samples.\')\n    parser.add_argument(\'-u\', \'--upsample-factor\', default=1,\n                        type=lambda v: check_bounds(v, lb=1, is_inclusive=True,\n                                                    type=int, name=""upsample-factor""),\n                        help=\'The scale factor with which to upsample the image (if applicable).\')\n    parser.add_argument(\'--is-show-loss\', action=\'store_true\',\n                        help=\'Displays the loss on the figures (if applicable).\')\n    parser.add_argument(\'--is-posterior\', action=\'store_true\',\n                        help=\'Traverses the posterior instead of the prior.\')\n    args = parser.parse_args()\n\n    return args\n\n\ndef main(args):\n    """"""Main function for plotting fro pretrained models.\n\n    Parameters\n    ----------\n    args: argparse.Namespace\n        Arguments\n    """"""\n    set_seed(args.seed)\n    experiment_name = args.name\n    model_dir = os.path.join(RES_DIR, experiment_name)\n    meta_data = load_metadata(model_dir)\n    model = load_model(model_dir)\n    model.eval()  # don\'t sample from latent: use mean\n    dataset = meta_data[\'dataset\']\n    viz = Visualizer(model=model,\n                     model_dir=model_dir,\n                     dataset=dataset,\n                     max_traversal=args.max_traversal,\n                     loss_of_interest=\'kl_loss_\',\n                     upsample_factor=args.upsample_factor)\n    size = (args.n_rows, args.n_cols)\n    # same samples for all plots: sample max then take first `x`data  for all plots\n    num_samples = args.n_cols * args.n_rows\n    samples = get_samples(dataset, num_samples, idcs=args.idcs)\n\n    if ""all"" in args.plots:\n        args.plots = [p for p in PLOT_TYPES if p != ""all""]\n\n    for plot_type in args.plots:\n        if plot_type == \'generate-samples\':\n            viz.generate_samples(size=size)\n        elif plot_type == \'data-samples\':\n            viz.data_samples(samples, size=size)\n        elif plot_type == ""reconstruct"":\n            viz.reconstruct(samples, size=size)\n        elif plot_type == \'traversals\':\n            viz.traversals(data=samples[0:1, ...] if args.is_posterior else None,\n                           n_per_latent=args.n_cols,\n                           n_latents=args.n_rows,\n                           is_reorder_latents=True)\n        elif plot_type == ""reconstruct-traverse"":\n            viz.reconstruct_traverse(samples,\n                                     is_posterior=args.is_posterior,\n                                     n_latents=args.n_rows,\n                                     n_per_latent=args.n_cols,\n                                     is_show_text=args.is_show_loss)\n        elif plot_type == ""gif-traversals"":\n            viz.gif_traversals(samples[:args.n_cols, ...], n_latents=args.n_rows)\n        else:\n            raise ValueError(""Unkown plot_type={}"".format(plot_type))\n\n\nif __name__ == \'__main__\':\n    args = parse_arguments(sys.argv[1:])\n    main(args)\n'"
disvae/__init__.py,0,b'from disvae.models.vae import init_specific_model\nfrom disvae.training import Trainer\nfrom disvae.evaluate import Evaluator\n'
disvae/evaluate.py,21,"b'import os\nimport logging\nimport math\nfrom functools import reduce\nfrom collections import defaultdict\nimport json\nfrom timeit import default_timer\n\nfrom tqdm import trange, tqdm\nimport numpy as np\nimport torch\n\nfrom disvae.models.losses import get_loss_f\nfrom disvae.utils.math import log_density_gaussian\nfrom disvae.utils.modelIO import save_metadata\n\nTEST_LOSSES_FILE = ""test_losses.log""\nMETRICS_FILENAME = ""metrics.log""\nMETRIC_HELPERS_FILE = ""metric_helpers.pth""\n\n\nclass Evaluator:\n    """"""\n    Class to handle training of model.\n\n    Parameters\n    ----------\n    model: disvae.vae.VAE\n\n    loss_f: disvae.models.BaseLoss\n        Loss function.\n\n    device: torch.device, optional\n        Device on which to run the code.\n\n    logger: logging.Logger, optional\n        Logger.\n\n    save_dir : str, optional\n        Directory for saving logs.\n\n    is_progress_bar: bool, optional\n        Whether to use a progress bar for training.\n    """"""\n\n    def __init__(self, model, loss_f,\n                 device=torch.device(""cpu""),\n                 logger=logging.getLogger(__name__),\n                 save_dir=""results"",\n                 is_progress_bar=True):\n\n        self.device = device\n        self.loss_f = loss_f\n        self.model = model.to(self.device)\n        self.logger = logger\n        self.save_dir = save_dir\n        self.is_progress_bar = is_progress_bar\n        self.logger.info(""Testing Device: {}"".format(self.device))\n\n    def __call__(self, data_loader, is_metrics=False, is_losses=True):\n        """"""Compute all test losses.\n\n        Parameters\n        ----------\n        data_loader: torch.utils.data.DataLoader\n\n        is_metrics: bool, optional\n            Whether to compute and store the disentangling metrics.\n\n        is_losses: bool, optional\n            Whether to compute and store the test losses.\n        """"""\n        start = default_timer()\n        is_still_training = self.model.training\n        self.model.eval()\n\n        metric, losses = None, None\n        if is_metrics:\n            self.logger.info(\'Computing metrics...\')\n            metrics = self.compute_metrics(data_loader)\n            self.logger.info(\'Losses: {}\'.format(metrics))\n            save_metadata(metrics, self.save_dir, filename=METRICS_FILENAME)\n\n        if is_losses:\n            self.logger.info(\'Computing losses...\')\n            losses = self.compute_losses(data_loader)\n            self.logger.info(\'Losses: {}\'.format(losses))\n            save_metadata(losses, self.save_dir, filename=TEST_LOSSES_FILE)\n\n        if is_still_training:\n            self.model.train()\n\n        self.logger.info(\'Finished evaluating after {:.1f} min.\'.format((default_timer() - start) / 60))\n\n        return metric, losses\n\n    def compute_losses(self, dataloader):\n        """"""Compute all test losses.\n\n        Parameters\n        ----------\n        data_loader: torch.utils.data.DataLoader\n        """"""\n        storer = defaultdict(list)\n        for data, _ in tqdm(dataloader, leave=False, disable=not self.is_progress_bar):\n            data = data.to(self.device)\n\n            try:\n                recon_batch, latent_dist, latent_sample = self.model(data)\n                _ = self.loss_f(data, recon_batch, latent_dist, self.model.training,\n                                storer, latent_sample=latent_sample)\n            except ValueError:\n                # for losses that use multiple optimizers (e.g. Factor)\n                _ = self.loss_f.call_optimize(data, self.model, None, storer)\n\n            losses = {k: sum(v) / len(dataloader) for k, v in storer.items()}\n            return losses\n\n    def compute_metrics(self, dataloader):\n        """"""Compute all the metrics.\n\n        Parameters\n        ----------\n        data_loader: torch.utils.data.DataLoader\n        """"""\n        try:\n            lat_sizes = dataloader.dataset.lat_sizes\n            lat_names = dataloader.dataset.lat_names\n        except AttributeError:\n            raise ValueError(""Dataset needs to have known true factors of variations to compute the metric. This does not seem to be the case for {}"".format(type(dataloader.__dict__[""dataset""]).__name__))\n\n        self.logger.info(""Computing the empirical distribution q(z|x)."")\n        samples_zCx, params_zCx = self._compute_q_zCx(dataloader)\n        len_dataset, latent_dim = samples_zCx.shape\n\n        self.logger.info(""Estimating the marginal entropy."")\n        # marginal entropy H(z_j)\n        H_z = self._estimate_latent_entropies(samples_zCx, params_zCx)\n\n        # conditional entropy H(z|v)\n        samples_zCx = samples_zCx.view(*lat_sizes, latent_dim)\n        params_zCx = tuple(p.view(*lat_sizes, latent_dim) for p in params_zCx)\n        H_zCv = self._estimate_H_zCv(samples_zCx, params_zCx, lat_sizes, lat_names)\n\n        H_z = H_z.cpu()\n        H_zCv = H_zCv.cpu()\n\n        # I[z_j;v_k] = E[log \\sum_x q(z_j|x)p(x|v_k)] + H[z_j] = - H[z_j|v_k] + H[z_j]\n        mut_info = - H_zCv + H_z\n        sorted_mut_info = torch.sort(mut_info, dim=1, descending=True)[0].clamp(min=0)\n\n        metric_helpers = {\'marginal_entropies\': H_z, \'cond_entropies\': H_zCv}\n        mig = self._mutual_information_gap(sorted_mut_info, lat_sizes, storer=metric_helpers)\n        aam = self._axis_aligned_metric(sorted_mut_info, storer=metric_helpers)\n\n        metrics = {\'MIG\': mig.item(), \'AAM\': aam.item()}\n        torch.save(metric_helpers, os.path.join(self.save_dir, METRIC_HELPERS_FILE))\n\n        return metrics\n\n    def _mutual_information_gap(self, sorted_mut_info, lat_sizes, storer=None):\n        """"""Compute the mutual information gap as in [1].\n\n        References\n        ----------\n           [1] Chen, Tian Qi, et al. ""Isolating sources of disentanglement in variational\n           autoencoders."" Advances in Neural Information Processing Systems. 2018.\n        """"""\n        # difference between the largest and second largest mutual info\n        delta_mut_info = sorted_mut_info[:, 0] - sorted_mut_info[:, 1]\n        # NOTE: currently only works if balanced dataset for every factor of variation\n        # then H(v_k) = - |V_k|/|V_k| log(1/|V_k|) = log(|V_k|)\n        H_v = torch.from_numpy(lat_sizes).float().log()\n        mig_k = delta_mut_info / H_v\n        mig = mig_k.mean()  # mean over factor of variations\n\n        if storer is not None:\n            storer[""mig_k""] = mig_k\n            storer[""mig""] = mig\n\n        return mig\n\n    def _axis_aligned_metric(self, sorted_mut_info, storer=None):\n        """"""Compute the proposed axis aligned metrics.""""""\n        numerator = (sorted_mut_info[:, 0] - sorted_mut_info[:, 1:].sum(dim=1)).clamp(min=0)\n        aam_k = numerator / sorted_mut_info[:, 0]\n        aam_k[torch.isnan(aam_k)] = 0\n        aam = aam_k.mean()  # mean over factor of variations\n\n        if storer is not None:\n            storer[""aam_k""] = aam_k\n            storer[""aam""] = aam\n\n        return aam\n\n    def _compute_q_zCx(self, dataloader):\n        """"""Compute the empiricall disitribution of q(z|x).\n\n        Parameter\n        ---------\n        dataloader: torch.utils.data.DataLoader\n            Batch data iterator.\n\n        Return\n        ------\n        samples_zCx: torch.tensor\n            Tensor of shape (len_dataset, latent_dim) containing a sample of\n            q(z|x) for every x in the dataset.\n\n        params_zCX: tuple of torch.Tensor\n            Sufficient statistics q(z|x) for each training example. E.g. for\n            gaussian (mean, log_var) each of shape : (len_dataset, latent_dim).\n        """"""\n        len_dataset = len(dataloader.dataset)\n        latent_dim = self.model.latent_dim\n        n_suff_stat = 2\n\n        q_zCx = torch.zeros(len_dataset, latent_dim, n_suff_stat, device=self.device)\n\n        n = 0\n        with torch.no_grad():\n            for x, label in dataloader:\n                batch_size = x.size(0)\n                idcs = slice(n, n + batch_size)\n                q_zCx[idcs, :, 0], q_zCx[idcs, :, 1] = self.model.encoder(x.to(self.device))\n                n += batch_size\n\n        params_zCX = q_zCx.unbind(-1)\n        samples_zCx = self.model.reparameterize(*params_zCX)\n\n        return samples_zCx, params_zCX\n\n    def _estimate_latent_entropies(self, samples_zCx, params_zCX,\n                                   n_samples=10000):\n        r""""""Estimate :math:`H(z_j) = E_{q(z_j)} [-log q(z_j)] = E_{p(x)} E_{q(z_j|x)} [-log q(z_j)]`\n        using the emperical distribution of :math:`p(x)`.\n\n        Note\n        ----\n        - the expectation over the emperical distributio is: :math:`q(z) = 1/N sum_{n=1}^N q(z|x_n)`.\n        - we assume that q(z|x) is factorial i.e. :math:`q(z|x) = \\prod_j q(z_j|x)`.\n        - computes numerically stable NLL: :math:`- log q(z) = log N - logsumexp_n=1^N log q(z|x_n)`.\n\n        Parameters\n        ----------\n        samples_zCx: torch.tensor\n            Tensor of shape (len_dataset, latent_dim) containing a sample of\n            q(z|x) for every x in the dataset.\n\n        params_zCX: tuple of torch.Tensor\n            Sufficient statistics q(z|x) for each training example. E.g. for\n            gaussian (mean, log_var) each of shape : (len_dataset, latent_dim).\n\n        n_samples: int, optional\n            Number of samples to use to estimate the entropies.\n\n        Return\n        ------\n        H_z: torch.Tensor\n            Tensor of shape (latent_dim) containing the marginal entropies H(z_j)\n        """"""\n        len_dataset, latent_dim = samples_zCx.shape\n        device = samples_zCx.device\n        H_z = torch.zeros(latent_dim, device=device)\n\n        # sample from p(x)\n        samples_x = torch.randperm(len_dataset, device=device)[:n_samples]\n        # sample from p(z|x)\n        samples_zCx = samples_zCx.index_select(0, samples_x).view(latent_dim, n_samples)\n\n        mini_batch_size = 10\n        samples_zCx = samples_zCx.expand(len_dataset, latent_dim, n_samples)\n        mean = params_zCX[0].unsqueeze(-1).expand(len_dataset, latent_dim, n_samples)\n        log_var = params_zCX[1].unsqueeze(-1).expand(len_dataset, latent_dim, n_samples)\n        log_N = math.log(len_dataset)\n        with trange(n_samples, leave=False, disable=self.is_progress_bar) as t:\n            for k in range(0, n_samples, mini_batch_size):\n                # log q(z_j|x) for n_samples\n                idcs = slice(k, k + mini_batch_size)\n                log_q_zCx = log_density_gaussian(samples_zCx[..., idcs],\n                                                 mean[..., idcs],\n                                                 log_var[..., idcs])\n                # numerically stable log q(z_j) for n_samples:\n                # log q(z_j) = -log N + logsumexp_{n=1}^N log q(z_j|x_n)\n                # As we don\'t know q(z) we appoximate it with the monte carlo\n                # expectation of q(z_j|x_n) over x. => fix a single z and look at\n                # proba for every x to generate it. n_samples is not used here !\n                log_q_z = -log_N + torch.logsumexp(log_q_zCx, dim=0, keepdim=False)\n                # H(z_j) = E_{z_j}[- log q(z_j)]\n                # mean over n_samples (i.e. dimesnion 1 because already summed over 0).\n                H_z += (-log_q_z).sum(1)\n\n                t.update(mini_batch_size)\n\n        H_z /= n_samples\n\n        return H_z\n\n    def _estimate_H_zCv(self, samples_zCx, params_zCx, lat_sizes, lat_names):\n        """"""Estimate conditional entropies :math:`H[z|v]`.""""""\n        latent_dim = samples_zCx.size(-1)\n        len_dataset = reduce((lambda x, y: x * y), lat_sizes)\n        H_zCv = torch.zeros(len(lat_sizes), latent_dim, device=self.device)\n        for i_fac_var, (lat_size, lat_name) in enumerate(zip(lat_sizes, lat_names)):\n            idcs = [slice(None)] * len(lat_sizes)\n            for i in range(lat_size):\n                self.logger.info(""Estimating conditional entropies for the {}th value of {}."".format(i, lat_name))\n                idcs[i_fac_var] = i\n                # samples from q(z,x|v)\n                samples_zxCv = samples_zCx[idcs].contiguous().view(len_dataset // lat_size,\n                                                                   latent_dim)\n                params_zxCv = tuple(p[idcs].contiguous().view(len_dataset // lat_size, latent_dim)\n                                    for p in params_zCx)\n\n                H_zCv[i_fac_var] += self._estimate_latent_entropies(samples_zxCv, params_zxCv\n                                                                    ) / lat_size\n        return H_zCv\n'"
disvae/training.py,7,"b'import imageio\nimport logging\nimport os\nfrom timeit import default_timer\nfrom collections import defaultdict\n\nfrom tqdm import trange\nimport torch\nfrom torch.nn import functional as F\n\nfrom disvae.utils.modelIO import save_model\n\n\nTRAIN_LOSSES_LOGFILE = ""train_losses.log""\n\n\nclass Trainer():\n    """"""\n    Class to handle training of model.\n\n    Parameters\n    ----------\n    model: disvae.vae.VAE\n\n    optimizer: torch.optim.Optimizer\n\n    loss_f: disvae.models.BaseLoss\n        Loss function.\n\n    device: torch.device, optional\n        Device on which to run the code.\n\n    logger: logging.Logger, optional\n        Logger.\n\n    save_dir : str, optional\n        Directory for saving logs.\n\n    gif_visualizer : viz.Visualizer, optional\n        Gif Visualizer that should return samples at every epochs.\n\n    is_progress_bar: bool, optional\n        Whether to use a progress bar for training.\n    """"""\n\n    def __init__(self, model, optimizer, loss_f,\n                 device=torch.device(""cpu""),\n                 logger=logging.getLogger(__name__),\n                 save_dir=""results"",\n                 gif_visualizer=None,\n                 is_progress_bar=True):\n\n        self.device = device\n        self.model = model.to(self.device)\n        self.loss_f = loss_f\n        self.optimizer = optimizer\n        self.save_dir = save_dir\n        self.is_progress_bar = is_progress_bar\n        self.logger = logger\n        self.losses_logger = LossesLogger(os.path.join(self.save_dir, TRAIN_LOSSES_LOGFILE))\n        self.gif_visualizer = gif_visualizer\n        self.logger.info(""Training Device: {}"".format(self.device))\n\n    def __call__(self, data_loader,\n                 epochs=10,\n                 checkpoint_every=10):\n        """"""\n        Trains the model.\n\n        Parameters\n        ----------\n        data_loader: torch.utils.data.DataLoader\n\n        epochs: int, optional\n            Number of epochs to train the model for.\n\n        checkpoint_every: int, optional\n            Save a checkpoint of the trained model every n epoch.\n        """"""\n        start = default_timer()\n        self.model.train()\n        for epoch in range(epochs):\n            storer = defaultdict(list)\n            mean_epoch_loss = self._train_epoch(data_loader, storer, epoch)\n            self.logger.info(\'Epoch: {} Average loss per image: {:.2f}\'.format(epoch + 1,\n                                                                               mean_epoch_loss))\n            self.losses_logger.log(epoch, storer)\n\n            if self.gif_visualizer is not None:\n                self.gif_visualizer()\n\n            if epoch % checkpoint_every == 0:\n                save_model(self.model, self.save_dir,\n                           filename=""model-{}.pt"".format(epoch))\n\n        if self.gif_visualizer is not None:\n            self.gif_visualizer.save_reset()\n\n        self.model.eval()\n\n        delta_time = (default_timer() - start) / 60\n        self.logger.info(\'Finished training after {:.1f} min.\'.format(delta_time))\n\n    def _train_epoch(self, data_loader, storer, epoch):\n        """"""\n        Trains the model for one epoch.\n\n        Parameters\n        ----------\n        data_loader: torch.utils.data.DataLoader\n\n        storer: dict\n            Dictionary in which to store important variables for vizualisation.\n\n        epoch: int\n            Epoch number\n\n        Return\n        ------\n        mean_epoch_loss: float\n            Mean loss per image\n        """"""\n        epoch_loss = 0.\n        kwargs = dict(desc=""Epoch {}"".format(epoch + 1), leave=False,\n                      disable=not self.is_progress_bar)\n        with trange(len(data_loader), **kwargs) as t:\n            for _, (data, _) in enumerate(data_loader):\n                iter_loss = self._train_iteration(data, storer)\n                epoch_loss += iter_loss\n\n                t.set_postfix(loss=iter_loss)\n                t.update()\n\n        mean_epoch_loss = epoch_loss / len(data_loader)\n        return mean_epoch_loss\n\n    def _train_iteration(self, data, storer):\n        """"""\n        Trains the model for one iteration on a batch of data.\n\n        Parameters\n        ----------\n        data: torch.Tensor\n            A batch of data. Shape : (batch_size, channel, height, width).\n\n        storer: dict\n            Dictionary in which to store important variables for vizualisation.\n        """"""\n        batch_size, channel, height, width = data.size()\n        data = data.to(self.device)\n\n        try:\n            recon_batch, latent_dist, latent_sample = self.model(data)\n            loss = self.loss_f(data, recon_batch, latent_dist, self.model.training,\n                               storer, latent_sample=latent_sample)\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n\n        except ValueError:\n            # for losses that use multiple optimizers (e.g. Factor)\n            loss = self.loss_f.call_optimize(data, self.model, self.optimizer, storer)\n\n        return loss.item()\n\n\nclass LossesLogger(object):\n    """"""Class definition for objects to write data to log files in a\n    form which is then easy to be plotted.\n    """"""\n\n    def __init__(self, file_path_name):\n        """""" Create a logger to store information for plotting. """"""\n        if os.path.isfile(file_path_name):\n            os.remove(file_path_name)\n\n        self.logger = logging.getLogger(""losses_logger"")\n        self.logger.setLevel(1)  # always store\n        file_handler = logging.FileHandler(file_path_name)\n        file_handler.setLevel(1)\n        self.logger.addHandler(file_handler)\n\n        header = "","".join([""Epoch"", ""Loss"", ""Value""])\n        self.logger.debug(header)\n\n    def log(self, epoch, losses_storer):\n        """"""Write to the log file """"""\n        for k, v in losses_storer.items():\n            log_string = "","".join(str(item) for item in [epoch, k, mean(v)])\n            self.logger.debug(log_string)\n\n\n# HELPERS\ndef mean(l):\n    """"""Compute the mean of a list""""""\n    return sum(l) / len(l)\n'"
utils/__init__.py,0,b''
utils/datasets.py,7,"b'import subprocess\nimport os\nimport abc\nimport hashlib\nimport zipfile\nimport glob\nimport logging\nimport tarfile\nfrom skimage.io import imread\nfrom PIL import Image\nfrom tqdm import tqdm\nimport numpy as np\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, datasets\n\nDIR = os.path.abspath(os.path.dirname(__file__))\nCOLOUR_BLACK = 0\nCOLOUR_WHITE = 1\nDATASETS_DICT = {""mnist"": ""MNIST"",\n                 ""fashion"": ""FashionMNIST"",\n                 ""dsprites"": ""DSprites"",\n                 ""celeba"": ""CelebA"",\n                 ""chairs"": ""Chairs""}\nDATASETS = list(DATASETS_DICT.keys())\n\n\ndef get_dataset(dataset):\n    """"""Return the correct dataset.""""""\n    dataset = dataset.lower()\n    try:\n        # eval because stores name as string in order to put it at top of file\n        return eval(DATASETS_DICT[dataset])\n    except KeyError:\n        raise ValueError(""Unkown dataset: {}"".format(dataset))\n\n\ndef get_img_size(dataset):\n    """"""Return the correct image size.""""""\n    return get_dataset(dataset).img_size\n\n\ndef get_background(dataset):\n    """"""Return the image background color.""""""\n    return get_dataset(dataset).background_color\n\n\ndef get_dataloaders(dataset, root=None, shuffle=True, pin_memory=True,\n                    batch_size=128, logger=logging.getLogger(__name__), **kwargs):\n    """"""A generic data loader\n\n    Parameters\n    ----------\n    dataset : {""mnist"", ""fashion"", ""dsprites"", ""celeba"", ""chairs""}\n        Name of the dataset to load\n\n    root : str\n        Path to the dataset root. If `None` uses the default one.\n\n    kwargs :\n        Additional arguments to `DataLoader`. Default values are modified.\n    """"""\n    pin_memory = pin_memory and torch.cuda.is_available  # only pin if GPU available\n    Dataset = get_dataset(dataset)\n    dataset = Dataset(logger=logger) if root is None else Dataset(root=root, logger=logger)\n    return DataLoader(dataset,\n                      batch_size=batch_size,\n                      shuffle=shuffle,\n                      pin_memory=pin_memory,\n                      **kwargs)\n\n\nclass DisentangledDataset(Dataset, abc.ABC):\n    """"""Base Class for disentangled VAE datasets.\n\n    Parameters\n    ----------\n    root : string\n        Root directory of dataset.\n\n    transforms_list : list\n        List of `torch.vision.transforms` to apply to the data when loading it.\n    """"""\n\n    def __init__(self, root, transforms_list=[], logger=logging.getLogger(__name__)):\n        self.root = root\n        self.train_data = os.path.join(root, type(self).files[""train""])\n        self.transforms = transforms.Compose(transforms_list)\n        self.logger = logger\n\n        if not os.path.isdir(root):\n            self.logger.info(""Downloading {} ..."".format(str(type(self))))\n            self.download()\n            self.logger.info(""Finished Downloading."")\n\n    def __len__(self):\n        return len(self.imgs)\n\n    @abc.abstractmethod\n    def __getitem__(self, idx):\n        """"""Get the image of `idx`.\n\n        Return\n        ------\n        sample : torch.Tensor\n            Tensor in [0.,1.] of shape `img_size`.\n        """"""\n        pass\n\n    @abc.abstractmethod\n    def download(self):\n        """"""Download the dataset. """"""\n        pass\n\n\nclass DSprites(DisentangledDataset):\n    """"""DSprites Dataset from [1].\n\n    Disentanglement test Sprites dataset.Procedurally generated 2D shapes, from 6\n    disentangled latent factors. This dataset uses 6 latents, controlling the color,\n    shape, scale, rotation and position of a sprite. All possible variations of\n    the latents are present. Ordering along dimension 1 is fixed and can be mapped\n    back to the exact latent values that generated that image. Pixel outputs are\n    different. No noise added.\n\n    Notes\n    -----\n    - Link : https://github.com/deepmind/dsprites-dataset/\n    - hard coded metadata because issue with python 3 loading of python 2\n\n    Parameters\n    ----------\n    root : string\n        Root directory of dataset.\n\n    References\n    ----------\n    [1] Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick,\n        M., ... & Lerchner, A. (2017). beta-vae: Learning basic visual concepts\n        with a constrained variational framework. In International Conference\n        on Learning Representations.\n\n    """"""\n    urls = {""train"": ""https://github.com/deepmind/dsprites-dataset/blob/master/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz?raw=true""}\n    files = {""train"": ""dsprite_train.npz""}\n    lat_names = (\'shape\', \'scale\', \'orientation\', \'posX\', \'posY\')\n    lat_sizes = np.array([3, 6, 40, 32, 32])\n    img_size = (1, 64, 64)\n    background_color = COLOUR_BLACK\n    lat_values = {\'posX\': np.array([0., 0.03225806, 0.06451613, 0.09677419, 0.12903226,\n                                    0.16129032, 0.19354839, 0.22580645, 0.25806452,\n                                    0.29032258, 0.32258065, 0.35483871, 0.38709677,\n                                    0.41935484, 0.4516129, 0.48387097, 0.51612903,\n                                    0.5483871, 0.58064516, 0.61290323, 0.64516129,\n                                    0.67741935, 0.70967742, 0.74193548, 0.77419355,\n                                    0.80645161, 0.83870968, 0.87096774, 0.90322581,\n                                    0.93548387, 0.96774194, 1.]),\n                  \'posY\': np.array([0., 0.03225806, 0.06451613, 0.09677419, 0.12903226,\n                                    0.16129032, 0.19354839, 0.22580645, 0.25806452,\n                                    0.29032258, 0.32258065, 0.35483871, 0.38709677,\n                                    0.41935484, 0.4516129, 0.48387097, 0.51612903,\n                                    0.5483871, 0.58064516, 0.61290323, 0.64516129,\n                                    0.67741935, 0.70967742, 0.74193548, 0.77419355,\n                                    0.80645161, 0.83870968, 0.87096774, 0.90322581,\n                                    0.93548387, 0.96774194, 1.]),\n                  \'scale\': np.array([0.5, 0.6, 0.7, 0.8, 0.9, 1.]),\n                  \'orientation\': np.array([0., 0.16110732, 0.32221463, 0.48332195,\n                                           0.64442926, 0.80553658, 0.96664389, 1.12775121,\n                                           1.28885852, 1.44996584, 1.61107316, 1.77218047,\n                                           1.93328779, 2.0943951, 2.25550242, 2.41660973,\n                                           2.57771705, 2.73882436, 2.89993168, 3.061039,\n                                           3.22214631, 3.38325363, 3.54436094, 3.70546826,\n                                           3.86657557, 4.02768289, 4.1887902, 4.34989752,\n                                           4.51100484, 4.67211215, 4.83321947, 4.99432678,\n                                           5.1554341, 5.31654141, 5.47764873, 5.63875604,\n                                           5.79986336, 5.96097068, 6.12207799, 6.28318531]),\n                  \'shape\': np.array([1., 2., 3.]),\n                  \'color\': np.array([1.])}\n\n    def __init__(self, root=os.path.join(DIR, \'../data/dsprites/\'), **kwargs):\n        super().__init__(root, [transforms.ToTensor()], **kwargs)\n\n        dataset_zip = np.load(self.train_data)\n        self.imgs = dataset_zip[\'imgs\']\n        self.lat_values = dataset_zip[\'latents_values\']\n\n    def download(self):\n        """"""Download the dataset.""""""\n        os.makedirs(self.root)\n        subprocess.check_call([""curl"", ""-L"", type(self).urls[""train""],\n                               ""--output"", self.train_data])\n\n    def __getitem__(self, idx):\n        """"""Get the image of `idx`\n        Return\n        ------\n        sample : torch.Tensor\n            Tensor in [0.,1.] of shape `img_size`.\n\n        lat_value : np.array\n            Array of length 6, that gives the value of each factor of variation.\n        """"""\n        # stored image have binary and shape (H x W) so multiply by 255 to get pixel\n        # values + add dimension\n        sample = np.expand_dims(self.imgs[idx] * 255, axis=-1)\n\n        # ToTensor transforms numpy.ndarray (H x W x C) in the range\n        # [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n        sample = self.transforms(sample)\n\n        lat_value = self.lat_values[idx]\n        return sample, lat_value\n\n\nclass CelebA(DisentangledDataset):\n    """"""CelebA Dataset from [1].\n\n    CelebFaces Attributes Dataset (CelebA) is a large-scale face attributes dataset\n    with more than 200K celebrity images, each with 40 attribute annotations.\n    The images in this dataset cover large pose variations and background clutter.\n    CelebA has large diversities, large quantities, and rich annotations, including\n    10,177 number of identities, and 202,599 number of face images.\n\n    Notes\n    -----\n    - Link : http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n\n    Parameters\n    ----------\n    root : string\n        Root directory of dataset.\n\n    References\n    ----------\n    [1] Liu, Z., Luo, P., Wang, X., & Tang, X. (2015). Deep learning face\n        attributes in the wild. In Proceedings of the IEEE international conference\n        on computer vision (pp. 3730-3738).\n\n    """"""\n    urls = {""train"": ""https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/celeba.zip""}\n    files = {""train"": ""img_align_celeba""}\n    img_size = (3, 64, 64)\n    background_color = COLOUR_WHITE\n\n    def __init__(self, root=os.path.join(DIR, \'../data/celeba\'), **kwargs):\n        super().__init__(root, [transforms.ToTensor()], **kwargs)\n\n        self.imgs = glob.glob(self.train_data + \'/*\')\n\n    def download(self):\n        """"""Download the dataset.""""""\n        save_path = os.path.join(self.root, \'celeba.zip\')\n        os.makedirs(self.root)\n        subprocess.check_call([""curl"", ""-L"", type(self).urls[""train""],\n                               ""--output"", save_path])\n\n        hash_code = \'00d2c5bc6d35e252742224ab0c1e8fcb\'\n        assert hashlib.md5(open(save_path, \'rb\').read()).hexdigest() == hash_code, \\\n            \'{} file is corrupted.  Remove the file and try again.\'.format(save_path)\n\n        with zipfile.ZipFile(save_path) as zf:\n            self.logger.info(""Extracting CelebA ..."")\n            zf.extractall(self.root)\n\n        os.remove(save_path)\n\n        self.logger.info(""Resizing CelebA ..."")\n        preprocess(self.train_data, size=type(self).img_size[1:])\n\n    def __getitem__(self, idx):\n        """"""Get the image of `idx`\n\n        Return\n        ------\n        sample : torch.Tensor\n            Tensor in [0.,1.] of shape `img_size`.\n\n        placeholder :\n            Placeholder value as their are no targets.\n        """"""\n        img_path = self.imgs[idx]\n        # img values already between 0 and 255\n        img = imread(img_path)\n\n        # put each pixel in [0.,1.] and reshape to (C x H x W)\n        img = self.transforms(img)\n\n        # no label so return 0 (note that can\'t return None because)\n        # dataloaders requires so\n        return img, 0\n\n\nclass Chairs(datasets.ImageFolder):\n    """"""Chairs Dataset from [1].\n\n    Notes\n    -----\n    - Link : https://www.di.ens.fr/willow/research/seeing3Dchairs\n\n    Parameters\n    ----------\n    root : string\n        Root directory of dataset.\n\n    References\n    ----------\n    [1] Aubry, M., Maturana, D., Efros, A. A., Russell, B. C., & Sivic, J. (2014).\n        Seeing 3d chairs: exemplar part-based 2d-3d alignment using a large dataset\n        of cad models. In Proceedings of the IEEE conference on computer vision\n        and pattern recognition (pp. 3762-3769).\n\n    """"""\n    urls = {""train"": ""https://www.di.ens.fr/willow/research/seeing3Dchairs/data/rendered_chairs.tar""}\n    files = {""train"": ""chairs_64""}\n    img_size = (1, 64, 64)\n    background_color = COLOUR_WHITE\n\n    def __init__(self, root=os.path.join(DIR, \'../data/chairs\'),\n                 logger=logging.getLogger(__name__)):\n        self.root = root\n        self.train_data = os.path.join(root, type(self).files[""train""])\n        self.transforms = transforms.Compose([transforms.Grayscale(),\n                                              transforms.ToTensor()])\n        self.logger = logger\n\n        if not os.path.isdir(root):\n            self.logger.info(""Downloading {} ..."".format(str(type(self))))\n            self.download()\n            self.logger.info(""Finished Downloading."")\n\n        super().__init__(self.train_data, transform=self.transforms)\n\n    def download(self):\n        """"""Download the dataset.""""""\n        save_path = os.path.join(self.root, \'chairs.tar\')\n        os.makedirs(self.root)\n        subprocess.check_call([""curl"", type(self).urls[""train""],\n                               ""--output"", save_path])\n\n        self.logger.info(""Extracting Chairs ..."")\n        tar = tarfile.open(save_path)\n        tar.extractall(self.root)\n        tar.close()\n        os.rename(os.path.join(self.root, \'rendered_chairs\'), self.train_data)\n\n        os.remove(save_path)\n\n        self.logger.info(""Preprocessing Chairs ..."")\n        preprocess(os.path.join(self.train_data, \'*/*\'),  # root/*/*/*.png structure\n                   size=type(self).img_size[1:],\n                   center_crop=(400, 400))\n\n\nclass MNIST(datasets.MNIST):\n    """"""Mnist wrapper. Docs: `datasets.MNIST.`""""""\n    img_size = (1, 32, 32)\n    background_color = COLOUR_BLACK\n\n    def __init__(self, root=os.path.join(DIR, \'../data/mnist\'), **kwargs):\n        super().__init__(root,\n                         train=True,\n                         download=True,\n                         transform=transforms.Compose([\n                             transforms.Resize(32),\n                             transforms.ToTensor()\n                         ]))\n\n\nclass FashionMNIST(datasets.FashionMNIST):\n    """"""Fashion Mnist wrapper. Docs: `datasets.FashionMNIST.`""""""\n    img_size = (1, 32, 32)\n\n    def __init__(self, root=os.path.join(DIR, \'../data/fashionMnist\'), **kwargs):\n        super().__init__(root,\n                         train=True,\n                         download=True,\n                         transform=transforms.Compose([\n                             transforms.Resize(32),\n                             transforms.ToTensor()\n                         ]))\n\n\n# HELPERS\ndef preprocess(root, size=(64, 64), img_format=\'JPEG\', center_crop=None):\n    """"""Preprocess a folder of images.\n\n    Parameters\n    ----------\n    root : string\n        Root directory of all images.\n\n    size : tuple of int\n        Size (width, height) to rescale the images. If `None` don\'t rescale.\n\n    img_format : string\n        Format to save the image in. Possible formats:\n        https://pillow.readthedocs.io/en/3.1.x/handbook/image-file-formats.html.\n\n    center_crop : tuple of int\n        Size (width, height) to center-crop the images. If `None` don\'t center-crop.\n    """"""\n    imgs = []\n    for ext in ["".png"", "".jpg"", "".jpeg""]:\n        imgs += glob.glob(os.path.join(root, \'*\' + ext))\n\n    for img_path in tqdm(imgs):\n        img = Image.open(img_path)\n        width, height = img.size\n\n        if size is not None and width != size[1] or height != size[0]:\n            img = img.resize(size, Image.ANTIALIAS)\n\n        if center_crop is not None:\n            new_width, new_height = center_crop\n            left = (width - new_width) // 2\n            top = (height - new_height) // 2\n            right = (width + new_width) // 2\n            bottom = (height + new_height) // 2\n\n            img.crop((left, top, right, bottom))\n\n        img.save(img_path, img_format)\n'"
utils/helpers.py,3,"b'import os\nimport shutil\nimport numpy as np\nimport ast\nimport configparser\nimport argparse\nimport random\n\nimport torch\n\n\ndef create_safe_directory(directory, logger=None):\n    """"""Create a directory and archive the previous one if already existed.""""""\n    if os.path.exists(directory):\n        if logger is not None:\n            warn = ""Directory {} already exists. Archiving it to {}.zip""\n            logger.warning(warn.format(directory, directory))\n        shutil.make_archive(directory, \'zip\', directory)\n        shutil.rmtree(directory)\n    os.makedirs(directory)\n\n\ndef set_seed(seed):\n    """"""Set all random seeds.""""""\n    if seed is not None:\n        np.random.seed(seed)\n        random.seed(seed)\n        torch.manual_seed(seed)\n        # if want pure determinism could uncomment below: but slower\n        # torch.backends.cudnn.deterministic = True\n\n\ndef get_device(is_gpu=True):\n    """"""Return the correct device""""""\n    return torch.device(""cuda"" if torch.cuda.is_available() and is_gpu\n                        else ""cpu"")\n\n\ndef get_model_device(model):\n    """"""Return the device on which a model is.""""""\n    return next(model.parameters()).device\n\n\ndef get_n_param(model):\n    """"""Return the number of parameters.""""""\n    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n    nParams = sum([np.prod(p.size()) for p in model_parameters])\n    return nParams\n\n\ndef update_namespace_(namespace, dictionnary):\n    """"""Update an argparse namespace in_place.""""""\n    vars(namespace).update(dictionnary)\n\n\ndef get_config_section(filenames, section):\n    """"""Return a dictionnary of the section of `.ini` config files. Every value\n    int the `.ini` will be litterally evaluated, such that `l=[1,""as""]` actually\n    returns a list.\n    """"""\n    parser = configparser.ConfigParser(interpolation=configparser.ExtendedInterpolation())\n    parser.optionxform = str\n    files = parser.read(filenames)\n    if len(files) == 0:\n        raise ValueError(""Config files not found: {}"".format(filenames))\n    dict_session = dict(parser[section])\n    dict_session = {k: ast.literal_eval(v) for k, v in dict_session.items()}\n    return dict_session\n\n\ndef check_bounds(value, type=float, lb=-float(""inf""), ub=float(""inf""),\n                 is_inclusive=True, name=""value""):\n    """"""Argparse bound checker""""""\n    value = type(value)\n    is_in_bound = lb <= value <= ub if is_inclusive else lb < value < ub\n    if not is_in_bound:\n        raise argparse.ArgumentTypeError(""{}={} outside of bounds ({},{})"".format(name, value, lb, ub))\n    return value\n\n\nclass FormatterNoDuplicate(argparse.ArgumentDefaultsHelpFormatter):\n    """"""Formatter overriding `argparse.ArgumentDefaultsHelpFormatter` to show\n    `-e, --epoch EPOCH` instead of `-e EPOCH, --epoch EPOCH`\n\n    Note\n    ----\n    - code modified from cPython: https://github.com/python/cpython/blob/master/Lib/argparse.py\n    """"""\n\n    def _format_action_invocation(self, action):\n        # no args given\n        if not action.option_strings:\n            default = self._get_default_metavar_for_positional(action)\n            metavar, = self._metavar_formatter(action, default)(1)\n            return metavar\n        else:\n            parts = []\n            # if the Optional doesn\'t take a value, format is:\n            #    -s, --long\n            if action.nargs == 0:\n                parts.extend(action.option_strings)\n            # if the Optional takes a value, format is:\n            #    -s ARGS, --long ARGS\n            else:\n                default = self._get_default_metavar_for_optional(action)\n                args_string = self._format_args(action, default)\n                for option_string in action.option_strings:\n                    # don\'t store the DEFAULT\n                    parts.append(\'%s\' % (option_string))\n                # store DEFAULT for the last one\n                parts[-1] += \' %s\' % args_string\n            return \', \'.join(parts)\n'"
utils/visualize.py,17,"b'import os\nfrom math import ceil, floor\n\nimport imageio\nfrom PIL import Image\nimport numpy as np\nfrom scipy import stats\nimport torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision.utils import make_grid, save_image\n\nfrom utils.datasets import get_background\nfrom utils.viz_helpers import (read_loss_from_file, add_labels, make_grid_img,\n                               sort_list_by_other, FPS_GIF, concatenate_pad)\n\nTRAIN_FILE = ""train_losses.log""\nDECIMAL_POINTS = 3\nGIF_FILE = ""training.gif""\nPLOT_NAMES = dict(generate_samples=""samples.png"",\n                  data_samples=""data_samples.png"",\n                  reconstruct=""reconstruct.png"",\n                  traversals=""traversals.png"",\n                  reconstruct_traverse=""reconstruct_traverse.png"",\n                  gif_traversals=""posterior_traversals.gif"",)\n\n\nclass Visualizer():\n    def __init__(self, model, dataset, model_dir,\n                 save_images=True,\n                 loss_of_interest=None,\n                 display_loss_per_dim=False,\n                 max_traversal=0.475,  # corresponds to ~2 for standard normal\n                 upsample_factor=1):\n        """"""\n        Visualizer is used to generate images of samples, reconstructions,\n        latent traversals and so on of the trained model.\n\n        Parameters\n        ----------\n        model : disvae.vae.VAE\n\n        dataset : str\n            Name of the dataset.\n\n        model_dir : str\n            The directory that the model is saved to and where the images will\n            be stored.\n\n        save_images : bool, optional\n            Whether to save images or return a tensor.\n\n        loss_of_interest : str, optional\n            The loss type (as saved in the log file) to order the latent dimensions by and display.\n\n        display_loss_per_dim : bool, optional\n            if the loss should be included as text next to the corresponding latent dimension images.\n\n        max_traversal: float, optional\n            The maximum displacement induced by a latent traversal. Symmetrical\n            traversals are assumed. If `m>=0.5` then uses absolute value traversal,\n            if `m<0.5` uses a percentage of the distribution (quantile).\n            E.g. for the prior the distribution is a standard normal so `m=0.45` c\n            orresponds to an absolute value of `1.645` because `2m=90%%` of a\n            standard normal is between `-1.645` and `1.645`. Note in the case\n            of the posterior, the distribution is not standard normal anymore.\n\n        upsample_factor : floar, optional\n            Scale factor to upsample the size of the tensor\n        """"""\n        self.model = model\n        self.device = next(self.model.parameters()).device\n        self.latent_dim = self.model.latent_dim\n        self.max_traversal = max_traversal\n        self.save_images = save_images\n        self.model_dir = model_dir\n        self.dataset = dataset\n        self.upsample_factor = upsample_factor\n        if loss_of_interest is not None:\n            self.losses = read_loss_from_file(os.path.join(self.model_dir, TRAIN_FILE),\n                                              loss_of_interest)\n\n    def _get_traversal_range(self, mean=0, std=1):\n        """"""Return the corresponding traversal range in absolute terms.""""""\n        max_traversal = self.max_traversal\n\n        if max_traversal < 0.5:\n            max_traversal = (1 - 2 * max_traversal) / 2  # from 0.45 to 0.05\n            max_traversal = stats.norm.ppf(max_traversal, loc=mean, scale=std)  # from 0.05 to -1.645\n\n        # symmetrical traversals\n        return (-1 * max_traversal, max_traversal)\n\n    def _traverse_line(self, idx, n_samples, data=None):\n        """"""Return a (size, latent_size) latent sample, corresponding to a traversal\n        of a latent variable indicated by idx.\n\n        Parameters\n        ----------\n        idx : int\n            Index of continuous dimension to traverse. If the continuous latent\n            vector is 10 dimensional and idx = 7, then the 7th dimension\n            will be traversed while all others are fixed.\n\n        n_samples : int\n            Number of samples to generate.\n\n        data : torch.Tensor or None, optional\n            Data to use for computing the posterior. Shape (N, C, H, W). If\n            `None` then use the mean of the prior (all zeros) for all other dimensions.\n        """"""\n        if data is None:\n            # mean of prior for other dimensions\n            samples = torch.zeros(n_samples, self.latent_dim)\n            traversals = torch.linspace(*self._get_traversal_range(), steps=n_samples)\n\n        else:\n            if data.size(0) > 1:\n                raise ValueError(""Every value should be sampled from the same posterior, but {} datapoints given."".format(data.size(0)))\n\n            with torch.no_grad():\n                post_mean, post_logvar = self.model.encoder(data.to(self.device))\n                samples = self.model.reparameterize(post_mean, post_logvar)\n                samples = samples.cpu().repeat(n_samples, 1)\n                post_mean_idx = post_mean.cpu()[0, idx]\n                post_std_idx = torch.exp(post_logvar / 2).cpu()[0, idx]\n\n            # travers from the gaussian of the posterior in case quantile\n            traversals = torch.linspace(*self._get_traversal_range(mean=post_mean_idx,\n                                                                   std=post_std_idx),\n                                        steps=n_samples)\n\n        for i in range(n_samples):\n            samples[i, idx] = traversals[i]\n\n        return samples\n\n    def _save_or_return(self, to_plot, size, filename, is_force_return=False):\n        """"""Create plot and save or return it.""""""\n        to_plot = F.interpolate(to_plot, scale_factor=self.upsample_factor)\n\n        if size[0] * size[1] != to_plot.shape[0]:\n            raise ValueError(""Wrong size {} for datashape {}"".format(size, to_plot.shape))\n\n        # `nrow` is number of images PER row => number of col\n        kwargs = dict(nrow=size[1], pad_value=(1 - get_background(self.dataset)))\n        if self.save_images and not is_force_return:\n            filename = os.path.join(self.model_dir, filename)\n            save_image(to_plot, filename, **kwargs)\n        else:\n            return make_grid_img(to_plot, **kwargs)\n\n    def _decode_latents(self, latent_samples):\n        """"""Decodes latent samples into images.\n\n        Parameters\n        ----------\n        latent_samples : torch.autograd.Variable\n            Samples from latent distribution. Shape (N, L) where L is dimension\n            of latent distribution.\n        """"""\n        latent_samples = latent_samples.to(self.device)\n        return self.model.decoder(latent_samples).cpu()\n\n    def generate_samples(self, size=(8, 8)):\n        """"""Plot generated samples from the prior and decoding.\n\n        Parameters\n        ----------\n        size : tuple of ints, optional\n            Size of the final grid.\n        """"""\n        prior_samples = torch.randn(size[0] * size[1], self.latent_dim)\n        generated = self._decode_latents(prior_samples)\n        return self._save_or_return(generated.data, size, PLOT_NAMES[""generate_samples""])\n\n    def data_samples(self, data, size=(8, 8)):\n        """"""Plot samples from the dataset\n\n        Parameters\n        ----------\n        data : torch.Tensor\n            Data to be reconstructed. Shape (N, C, H, W)\n\n        size : tuple of ints, optional\n            Size of the final grid.\n        """"""\n        data = data[:size[0] * size[1], ...]\n        return self._save_or_return(data, size, PLOT_NAMES[""data_samples""])\n\n    def reconstruct(self, data, size=(8, 8), is_original=True, is_force_return=False):\n        """"""Generate reconstructions of data through the model.\n\n        Parameters\n        ----------\n        data : torch.Tensor\n            Data to be reconstructed. Shape (N, C, H, W)\n\n        size : tuple of ints, optional\n            Size of grid on which reconstructions will be plotted. The number\n            of rows should be even when `is_original`, so that upper\n            half contains true data and bottom half contains reconstructions.contains\n\n        is_original : bool, optional\n            Whether to exclude the original plots.\n\n        is_force_return : bool, optional\n            Force returning instead of saving the image.\n        """"""\n        if is_original:\n            if size[0] % 2 != 0:\n                raise ValueError(""Should be even number of rows when showing originals not {}"".format(size[0]))\n            n_samples = size[0] // 2 * size[1]\n        else:\n            n_samples = size[0] * size[1]\n\n        with torch.no_grad():\n            originals = data.to(self.device)[:n_samples, ...]\n            recs, _, _ = self.model(originals)\n\n        originals = originals.cpu()\n        recs = recs.view(-1, *self.model.img_size).cpu()\n\n        to_plot = torch.cat([originals, recs]) if is_original else recs\n        return self._save_or_return(to_plot, size, PLOT_NAMES[""reconstruct""],\n                                    is_force_return=is_force_return)\n\n    def traversals(self,\n                   data=None,\n                   is_reorder_latents=False,\n                   n_per_latent=8,\n                   n_latents=None,\n                   is_force_return=False):\n        """"""Plot traverse through all latent dimensions (prior or posterior) one\n        by one and plots a grid of images where each row corresponds to a latent\n        traversal of one latent dimension.\n\n        Parameters\n        ----------\n        data : bool, optional\n            Data to use for computing the latent posterior. If `None` traverses\n            the prior.\n\n        n_per_latent : int, optional\n            The number of points to include in the traversal of a latent dimension.\n            I.e. number of columns.\n\n        n_latents : int, optional\n            The number of latent dimensions to display. I.e. number of rows. If `None`\n            uses all latents.\n\n        is_reorder_latents : bool, optional\n            If the latent dimensions should be reordered or not\n\n        is_force_return : bool, optional\n            Force returning instead of saving the image.\n        """"""\n        n_latents = n_latents if n_latents is not None else self.model.latent_dim\n        latent_samples = [self._traverse_line(dim, n_per_latent, data=data)\n                          for dim in range(self.latent_dim)]\n        decoded_traversal = self._decode_latents(torch.cat(latent_samples, dim=0))\n\n        if is_reorder_latents:\n            n_images, *other_shape = decoded_traversal.size()\n            n_rows = n_images // n_per_latent\n            decoded_traversal = decoded_traversal.reshape(n_rows, n_per_latent, *other_shape)\n            decoded_traversal = sort_list_by_other(decoded_traversal, self.losses)\n            decoded_traversal = torch.stack(decoded_traversal, dim=0)\n            decoded_traversal = decoded_traversal.reshape(n_images, *other_shape)\n\n        decoded_traversal = decoded_traversal[range(n_per_latent * n_latents), ...]\n\n        size = (n_latents, n_per_latent)\n        sampling_type = ""prior"" if data is None else ""posterior""\n        filename = ""{}_{}"".format(sampling_type, PLOT_NAMES[""traversals""])\n\n        return self._save_or_return(decoded_traversal.data, size, filename,\n                                    is_force_return=is_force_return)\n\n    def reconstruct_traverse(self, data,\n                             is_posterior=True,\n                             n_per_latent=8,\n                             n_latents=None,\n                             is_show_text=False):\n        """"""\n        Creates a figure whith first row for original images, second are\n        reconstructions, rest are traversals (prior or posterior) of the latent\n        dimensions.\n\n        Parameters\n        ----------\n        data : torch.Tensor\n            Data to be reconstructed. Shape (N, C, H, W)\n\n        n_per_latent : int, optional\n            The number of points to include in the traversal of a latent dimension.\n            I.e. number of columns.\n\n        n_latents : int, optional\n            The number of latent dimensions to display. I.e. number of rows. If `None`\n            uses all latents.\n\n        is_posterior : bool, optional\n            Whether to sample from the posterior.\n\n        is_show_text : bool, optional\n            Whether the KL values next to the traversal rows.\n        """"""\n        n_latents = n_latents if n_latents is not None else self.model.latent_dim\n\n        reconstructions = self.reconstruct(data[:2 * n_per_latent, ...],\n                                           size=(2, n_per_latent),\n                                           is_force_return=True)\n        traversals = self.traversals(data=data[0:1, ...] if is_posterior else None,\n                                     is_reorder_latents=True,\n                                     n_per_latent=n_per_latent,\n                                     n_latents=n_latents,\n                                     is_force_return=True)\n\n        concatenated = np.concatenate((reconstructions, traversals), axis=0)\n        concatenated = Image.fromarray(concatenated)\n\n        if is_show_text:\n            losses = sorted(self.losses, reverse=True)[:n_latents]\n            labels = [\'orig\', \'recon\'] + [""KL={:.4f}"".format(l) for l in losses]\n            concatenated = add_labels(concatenated, labels)\n\n        filename = os.path.join(self.model_dir, PLOT_NAMES[""reconstruct_traverse""])\n        concatenated.save(filename)\n\n    def gif_traversals(self, data, n_latents=None, n_per_gif=15):\n        """"""Generates a grid of gifs of latent posterior traversals where the rows\n        are the latent dimensions and the columns are random images.\n\n        Parameters\n        ----------\n        data : bool\n            Data to use for computing the latent posteriors. The number of datapoint\n            (batchsize) will determine the number of columns of the grid.\n\n        n_latents : int, optional\n            The number of latent dimensions to display. I.e. number of rows. If `None`\n            uses all latents.\n\n        n_per_gif : int, optional\n            Number of images per gif (number of traversals)\n        """"""\n        n_images, _, _, width_col = data.shape\n        width_col = int(width_col * self.upsample_factor)\n        all_cols = [[] for c in range(n_per_gif)]\n        for i in range(n_images):\n            grid = self.traversals(data=data[i:i + 1, ...], is_reorder_latents=True,\n                                   n_per_latent=n_per_gif, n_latents=n_latents,\n                                   is_force_return=True)\n\n            height, width, c = grid.shape\n            padding_width = (width - width_col * n_per_gif) // (n_per_gif + 1)\n\n            # split the grids into a list of column images (and removes padding)\n            for j in range(n_per_gif):\n                all_cols[j].append(grid[:, [(j + 1) * padding_width + j * width_col + i\n                                            for i in range(width_col)], :])\n\n        pad_values = (1 - get_background(self.dataset)) * 255\n        all_cols = [concatenate_pad(cols, pad_size=2, pad_values=pad_values, axis=1)\n                    for cols in all_cols]\n\n        filename = os.path.join(self.model_dir, PLOT_NAMES[""gif_traversals""])\n        imageio.mimsave(filename, all_cols, fps=FPS_GIF)\n\n\nclass GifTraversalsTraining:\n    """"""Creates a Gif of traversals by generating an image at every training epoch.\n\n    Parameters\n    ----------\n    model : disvae.vae.VAE\n\n    dataset : str\n        Name of the dataset.\n\n    model_dir : str\n        The directory that the model is saved to and where the images will\n        be stored.\n\n    is_reorder_latents : bool, optional\n        If the latent dimensions should be reordered or not\n\n    n_per_latent : int, optional\n        The number of points to include in the traversal of a latent dimension.\n        I.e. number of columns.\n\n    n_latents : int, optional\n        The number of latent dimensions to display. I.e. number of rows. If `None`\n        uses all latents.\n\n    kwargs:\n        Additional arguments to `Visualizer`\n    """"""\n\n    def __init__(self, model, dataset, model_dir,\n                 is_reorder_latents=False,\n                 n_per_latent=10,\n                 n_latents=None,\n                 **kwargs):\n        self.save_filename = os.path.join(model_dir, GIF_FILE)\n        self.visualizer = Visualizer(model, dataset, model_dir,\n                                     save_images=False, **kwargs)\n\n        self.images = []\n        self.is_reorder_latents = is_reorder_latents\n        self.n_per_latent = n_per_latent\n        self.n_latents = n_latents if n_latents is not None else model.latent_dim\n\n    def __call__(self):\n        """"""Generate the next gif image. Should be called after each epoch.""""""\n        cached_training = self.visualizer.model.training\n        self.visualizer.model.eval()\n        img_grid = self.visualizer.traversals(data=None,  # GIF from prior\n                                              is_reorder_latents=self.is_reorder_latents,\n                                              n_per_latent=self.n_per_latent,\n                                              n_latents=self.n_latents)\n        self.images.append(img_grid)\n        if cached_training:\n            self.visualizer.model.train()\n\n    def save_reset(self):\n        """"""Saves the GIF and resets the list of images. Call at the end of training.""""""\n        imageio.mimsave(self.save_filename, self.images, fps=FPS_GIF)\n        self.images = []\n'"
utils/viz_helpers.py,3,"b'import random\n\nimport numpy as np\nfrom PIL import Image, ImageDraw\nimport pandas as pd\nimport torch\nimport imageio\n\nfrom torchvision.utils import make_grid\nfrom utils.datasets import get_dataloaders\nfrom utils.helpers import set_seed\n\nFPS_GIF = 12\n\n\ndef get_samples(dataset, num_samples, idcs=[]):\n    """""" Generate a number of samples from the dataset.\n\n    Parameters\n    ----------\n    dataset : str\n        The name of the dataset.\n\n    num_samples : int, optional\n        The number of samples to load from the dataset\n\n    idcs : list of ints, optional\n        List of indices to of images to put at the begning of the samples.\n    """"""\n    data_loader = get_dataloaders(dataset,\n                                  batch_size=1,\n                                  shuffle=idcs is None)\n\n    idcs += random.sample(range(len(data_loader.dataset)), num_samples - len(idcs))\n    samples = torch.stack([data_loader.dataset[i][0] for i in idcs], dim=0)\n    print(""Selected idcs: {}"".format(idcs))\n\n    return samples\n\n\ndef sort_list_by_other(to_sort, other, reverse=True):\n    """"""Sort a list by an other.""""""\n    return [el for _, el in sorted(zip(other, to_sort), reverse=reverse)]\n\n\n# TO-DO: clean\ndef read_loss_from_file(log_file_path, loss_to_fetch):\n    """""" Read the average KL per latent dimension at the final stage of training from the log file.\n        Parameters\n        ----------\n        log_file_path : str\n            Full path and file name for the log file. For example \'experiments/custom/losses.log\'.\n\n        loss_to_fetch : str\n            The loss type to search for in the log file and return. This must be in the exact form as stored.\n    """"""\n    EPOCH = ""Epoch""\n    LOSS = ""Loss""\n\n    logs = pd.read_csv(log_file_path)\n    df_last_epoch_loss = logs[logs.loc[:, EPOCH] == logs.loc[:, EPOCH].max()]\n    df_last_epoch_loss = df_last_epoch_loss.loc[df_last_epoch_loss.loc[:, LOSS].str.startswith(loss_to_fetch), :]\n    df_last_epoch_loss.loc[:, LOSS] = df_last_epoch_loss.loc[:, LOSS].str.replace(loss_to_fetch, """").astype(int)\n    df_last_epoch_loss = df_last_epoch_loss.sort_values(LOSS).loc[:, ""Value""]\n    return list(df_last_epoch_loss)\n\n\ndef add_labels(input_image, labels):\n    """"""Adds labels next to rows of an image.\n\n    Parameters\n    ----------\n    input_image : image\n        The image to which to add the labels\n    labels : list\n        The list of labels to plot\n    """"""\n    new_width = input_image.width + 100\n    new_size = (new_width, input_image.height)\n    new_img = Image.new(""RGB"", new_size, color=\'white\')\n    new_img.paste(input_image, (0, 0))\n    draw = ImageDraw.Draw(new_img)\n\n    for i, s in enumerate(labels):\n        draw.text(xy=(new_width - 100 + 0.005,\n                      int((i / len(labels) + 1 / (2 * len(labels))) * input_image.height)),\n                  text=s,\n                  fill=(0, 0, 0))\n\n    return new_img\n\n\ndef make_grid_img(tensor, **kwargs):\n    """"""Converts a tensor to a grid of images that can be read by imageio.\n\n    Notes\n    -----\n    * from in https://github.com/pytorch/vision/blob/master/torchvision/utils.py\n\n    Parameters\n    ----------\n    tensor (torch.Tensor or list): 4D mini-batch Tensor of shape (B x C x H x W)\n        or a list of images all of the same size.\n\n    kwargs:\n        Additional arguments to `make_grid_img`.\n    """"""\n    grid = make_grid(tensor, **kwargs)\n    img_grid = grid.mul_(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0)\n    img_grid = img_grid.to(\'cpu\', torch.uint8).numpy()\n    return img_grid\n\n\ndef get_image_list(image_file_name_list):\n    image_list = []\n    for file_name in image_file_name_list:\n        image_list.append(Image.open(file_name))\n    return image_list\n\n\ndef arr_im_convert(arr, convert=""RGBA""):\n    """"""Convert an image array.""""""\n    return np.asarray(Image.fromarray(arr).convert(convert))\n\n\ndef plot_grid_gifs(filename, grid_files, pad_size=7, pad_values=255):\n    """"""Take a grid of gif files and merge them in order with padding.""""""\n    grid_gifs = [[imageio.mimread(f) for f in row] for row in grid_files]\n    n_per_gif = len(grid_gifs[0][0])\n\n    # convert all to RGBA which is the most general => can merge any image\n    imgs = [concatenate_pad([concatenate_pad([arr_im_convert(gif[i], convert=""RGBA"")\n                                              for gif in row], pad_size, pad_values, axis=1)\n                             for row in grid_gifs], pad_size, pad_values, axis=0)\n            for i in range(n_per_gif)]\n\n    imageio.mimsave(filename, imgs, fps=FPS_GIF)\n\n\ndef concatenate_pad(arrays, pad_size, pad_values, axis=0):\n    """"""Concatenate lsit of array with padding inbetween.""""""\n    pad = np.ones_like(arrays[0]).take(indices=range(pad_size), axis=axis) * pad_values\n\n    new_arrays = [pad]\n    for arr in arrays:\n        new_arrays += [arr, pad]\n    new_arrays += [pad]\n    return np.concatenate(new_arrays, axis=axis)\n'"
disvae/models/__init__.py,0,b''
disvae/models/decoders.py,7,"b'""""""\nModule containing the decoders.\n""""""\nimport numpy as np\n\nimport torch\nfrom torch import nn\n\n\n# ALL decoders should be called Decoder<Model>\ndef get_decoder(model_type):\n    model_type = model_type.lower().capitalize()\n    return eval(""Decoder{}"".format(model_type))\n\n\nclass DecoderBurgess(nn.Module):\n    def __init__(self, img_size,\n                 latent_dim=10):\n        r""""""Decoder of the model proposed in [1].\n\n        Parameters\n        ----------\n        img_size : tuple of ints\n            Size of images. E.g. (1, 32, 32) or (3, 64, 64).\n\n        latent_dim : int\n            Dimensionality of latent output.\n\n        Model Architecture (transposed for decoder)\n        ------------\n        - 4 convolutional layers (each with 32 channels), (4 x 4 kernel), (stride of 2)\n        - 2 fully connected layers (each of 256 units)\n        - Latent distribution:\n            - 1 fully connected layer of 20 units (log variance and mean for 10 Gaussians)\n\n        References:\n            [1] Burgess, Christopher P., et al. ""Understanding disentangling in\n            $\\beta$-VAE."" arXiv preprint arXiv:1804.03599 (2018).\n        """"""\n        super(DecoderBurgess, self).__init__()\n\n        # Layer parameters\n        hid_channels = 32\n        kernel_size = 4\n        hidden_dim = 256\n        self.img_size = img_size\n        # Shape required to start transpose convs\n        self.reshape = (hid_channels, kernel_size, kernel_size)\n        n_chan = self.img_size[0]\n        self.img_size = img_size\n\n        # Fully connected layers\n        self.lin1 = nn.Linear(latent_dim, hidden_dim)\n        self.lin2 = nn.Linear(hidden_dim, hidden_dim)\n        self.lin3 = nn.Linear(hidden_dim, np.product(self.reshape))\n\n        # Convolutional layers\n        cnn_kwargs = dict(stride=2, padding=1)\n        # If input image is 64x64 do fourth convolution\n        if self.img_size[1] == self.img_size[2] == 64:\n            self.convT_64 = nn.ConvTranspose2d(hid_channels, hid_channels, kernel_size, **cnn_kwargs)\n\n        self.convT1 = nn.ConvTranspose2d(hid_channels, hid_channels, kernel_size, **cnn_kwargs)\n        self.convT2 = nn.ConvTranspose2d(hid_channels, hid_channels, kernel_size, **cnn_kwargs)\n        self.convT3 = nn.ConvTranspose2d(hid_channels, n_chan, kernel_size, **cnn_kwargs)\n\n    def forward(self, z):\n        batch_size = z.size(0)\n\n        # Fully connected layers with ReLu activations\n        x = torch.relu(self.lin1(z))\n        x = torch.relu(self.lin2(x))\n        x = torch.relu(self.lin3(x))\n        x = x.view(batch_size, *self.reshape)\n\n        # Convolutional layers with ReLu activations\n        if self.img_size[1] == self.img_size[2] == 64:\n            x = torch.relu(self.convT_64(x))\n        x = torch.relu(self.convT1(x))\n        x = torch.relu(self.convT2(x))\n        # Sigmoid activation for final conv layer\n        x = torch.sigmoid(self.convT3(x))\n\n        return x\n'"
disvae/models/discriminator.py,0,"b'""""""\nModule containing discriminator for FactorVAE.\n""""""\nfrom torch import nn\n\nfrom disvae.utils.initialization import weights_init\n\n\nclass Discriminator(nn.Module):\n    def __init__(self,\n                 neg_slope=0.2,\n                 latent_dim=10,\n                 hidden_units=1000):\n        """"""Discriminator proposed in [1].\n\n        Parameters\n        ----------\n        neg_slope: float\n            Hyperparameter for the Leaky ReLu\n\n        latent_dim : int\n            Dimensionality of latent variables.\n\n        hidden_units: int\n            Number of hidden units in the MLP\n\n        Model Architecture\n        ------------\n        - 6 layer multi-layer perceptron, each with 1000 hidden units\n        - Leaky ReLu activations\n        - Output 2 logits\n\n        References:\n            [1] Kim, Hyunjik, and Andriy Mnih. ""Disentangling by factorising.""\n            arXiv preprint arXiv:1802.05983 (2018).\n\n        """"""\n        super(Discriminator, self).__init__()\n\n        # Activation parameters\n        self.neg_slope = neg_slope\n        self.leaky_relu = nn.LeakyReLU(self.neg_slope, True)\n\n        # Layer parameters\n        self.z_dim = latent_dim\n        self.hidden_units = hidden_units\n        # theoretically 1 with sigmoid but gives bad results => use 2 and softmax\n        out_units = 2\n\n        # Fully connected layers\n        self.lin1 = nn.Linear(self.z_dim, hidden_units)\n        self.lin2 = nn.Linear(hidden_units, hidden_units)\n        self.lin3 = nn.Linear(hidden_units, hidden_units)\n        self.lin4 = nn.Linear(hidden_units, hidden_units)\n        self.lin5 = nn.Linear(hidden_units, hidden_units)\n        self.lin6 = nn.Linear(hidden_units, out_units)\n\n        self.reset_parameters()\n\n    def forward(self, z):\n\n        # Fully connected layers with leaky ReLu activations\n        z = self.leaky_relu(self.lin1(z))\n        z = self.leaky_relu(self.lin2(z))\n        z = self.leaky_relu(self.lin3(z))\n        z = self.leaky_relu(self.lin4(z))\n        z = self.leaky_relu(self.lin5(z))\n        z = self.lin6(z)\n\n        return z\n\n    def reset_parameters(self):\n        self.apply(weights_init)\n'"
disvae/models/encoders.py,6,"b'""""""\nModule containing the encoders.\n""""""\nimport numpy as np\n\nimport torch\nfrom torch import nn\n\n\n# ALL encoders should be called Enccoder<Model>\ndef get_encoder(model_type):\n    model_type = model_type.lower().capitalize()\n    return eval(""Encoder{}"".format(model_type))\n\n\nclass EncoderBurgess(nn.Module):\n    def __init__(self, img_size,\n                 latent_dim=10):\n        r""""""Encoder of the model proposed in [1].\n\n        Parameters\n        ----------\n        img_size : tuple of ints\n            Size of images. E.g. (1, 32, 32) or (3, 64, 64).\n\n        latent_dim : int\n            Dimensionality of latent output.\n\n        Model Architecture (transposed for decoder)\n        ------------\n        - 4 convolutional layers (each with 32 channels), (4 x 4 kernel), (stride of 2)\n        - 2 fully connected layers (each of 256 units)\n        - Latent distribution:\n            - 1 fully connected layer of 20 units (log variance and mean for 10 Gaussians)\n\n        References:\n            [1] Burgess, Christopher P., et al. ""Understanding disentangling in\n            $\\beta$-VAE."" arXiv preprint arXiv:1804.03599 (2018).\n        """"""\n        super(EncoderBurgess, self).__init__()\n\n        # Layer parameters\n        hid_channels = 32\n        kernel_size = 4\n        hidden_dim = 256\n        self.latent_dim = latent_dim\n        self.img_size = img_size\n        # Shape required to start transpose convs\n        self.reshape = (hid_channels, kernel_size, kernel_size)\n        n_chan = self.img_size[0]\n\n        # Convolutional layers\n        cnn_kwargs = dict(stride=2, padding=1)\n        self.conv1 = nn.Conv2d(n_chan, hid_channels, kernel_size, **cnn_kwargs)\n        self.conv2 = nn.Conv2d(hid_channels, hid_channels, kernel_size, **cnn_kwargs)\n        self.conv3 = nn.Conv2d(hid_channels, hid_channels, kernel_size, **cnn_kwargs)\n\n        # If input image is 64x64 do fourth convolution\n        if self.img_size[1] == self.img_size[2] == 64:\n            self.conv_64 = nn.Conv2d(hid_channels, hid_channels, kernel_size, **cnn_kwargs)\n\n        # Fully connected layers\n        self.lin1 = nn.Linear(np.product(self.reshape), hidden_dim)\n        self.lin2 = nn.Linear(hidden_dim, hidden_dim)\n\n        # Fully connected layers for mean and variance\n        self.mu_logvar_gen = nn.Linear(hidden_dim, self.latent_dim * 2)\n\n    def forward(self, x):\n        batch_size = x.size(0)\n\n        # Convolutional layers with ReLu activations\n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x))\n        x = torch.relu(self.conv3(x))\n        if self.img_size[1] == self.img_size[2] == 64:\n            x = torch.relu(self.conv_64(x))\n\n        # Fully connected layers with ReLu activations\n        x = x.view((batch_size, -1))\n        x = torch.relu(self.lin1(x))\n        x = torch.relu(self.lin2(x))\n\n        # Fully connected layer for log variance and mean\n        # Log std-dev in paper (bear in mind)\n        mu_logvar = self.mu_logvar_gen(x)\n        mu, logvar = mu_logvar.view(-1, self.latent_dim, 2).unbind(-1)\n\n        return mu, logvar\n'"
disvae/models/losses.py,20,"b'""""""\nModule containing all vae losses.\n""""""\nimport abc\nimport math\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch import optim\n\nfrom .discriminator import Discriminator\nfrom disvae.utils.math import (log_density_gaussian, log_importance_weight_matrix,\n                               matrix_log_density_gaussian)\n\n\nLOSSES = [""VAE"", ""betaH"", ""betaB"", ""factor"", ""btcvae""]\nRECON_DIST = [""bernoulli"", ""laplace"", ""gaussian""]\n\n\n# TO-DO: clean n_data and device\ndef get_loss_f(loss_name, **kwargs_parse):\n    """"""Return the correct loss function given the argparse arguments.""""""\n    kwargs_all = dict(rec_dist=kwargs_parse[""rec_dist""],\n                      steps_anneal=kwargs_parse[""reg_anneal""])\n    if loss_name == ""betaH"":\n        return BetaHLoss(beta=kwargs_parse[""betaH_B""], **kwargs_all)\n    elif loss_name == ""VAE"":\n        return BetaHLoss(beta=1, **kwargs_all)\n    elif loss_name == ""betaB"":\n        return BetaBLoss(C_init=kwargs_parse[""betaB_initC""],\n                         C_fin=kwargs_parse[""betaB_finC""],\n                         gamma=kwargs_parse[""betaB_G""],\n                         **kwargs_all)\n    elif loss_name == ""factor"":\n        return FactorKLoss(kwargs_parse[""device""],\n                           gamma=kwargs_parse[""factor_G""],\n                           disc_kwargs=dict(latent_dim=kwargs_parse[""latent_dim""]),\n                           optim_kwargs=dict(lr=kwargs_parse[""lr_disc""], betas=(0.5, 0.9)),\n                           **kwargs_all)\n    elif loss_name == ""btcvae"":\n        return BtcvaeLoss(kwargs_parse[""n_data""],\n                          alpha=kwargs_parse[""btcvae_A""],\n                          beta=kwargs_parse[""btcvae_B""],\n                          gamma=kwargs_parse[""btcvae_G""],\n                          **kwargs_all)\n    else:\n        assert loss_name not in LOSSES\n        raise ValueError(""Uknown loss : {}"".format(loss_name))\n\n\nclass BaseLoss(abc.ABC):\n    """"""\n    Base class for losses.\n\n    Parameters\n    ----------\n    record_loss_every: int, optional\n        Every how many steps to recorsd the loss.\n\n    rec_dist: {""bernoulli"", ""gaussian"", ""laplace""}, optional\n        Reconstruction distribution istribution of the likelihood on the each pixel.\n        Implicitely defines the reconstruction loss. Bernoulli corresponds to a\n        binary cross entropy (bse), Gaussian corresponds to MSE, Laplace\n        corresponds to L1.\n\n    steps_anneal: nool, optional\n        Number of annealing steps where gradually adding the regularisation.\n    """"""\n\n    def __init__(self, record_loss_every=50, rec_dist=""bernoulli"", steps_anneal=0):\n        self.n_train_steps = 0\n        self.record_loss_every = record_loss_every\n        self.rec_dist = rec_dist\n        self.steps_anneal = steps_anneal\n\n    @abc.abstractmethod\n    def __call__(self, data, recon_data, latent_dist, is_train, storer, **kwargs):\n        """"""\n        Calculates loss for a batch of data.\n\n        Parameters\n        ----------\n        data : torch.Tensor\n            Input data (e.g. batch of images). Shape : (batch_size, n_chan,\n            height, width).\n\n        recon_data : torch.Tensor\n            Reconstructed data. Shape : (batch_size, n_chan, height, width).\n\n        latent_dist : tuple of torch.tensor\n            sufficient statistics of the latent dimension. E.g. for gaussian\n            (mean, log_var) each of shape : (batch_size, latent_dim).\n\n        is_train : bool\n            Whether currently in train mode.\n\n        storer : dict\n            Dictionary in which to store important variables for vizualisation.\n\n        kwargs:\n            Loss specific arguments\n        """"""\n\n    def _pre_call(self, is_train, storer):\n        if is_train:\n            self.n_train_steps += 1\n\n        if not is_train or self.n_train_steps % self.record_loss_every == 1:\n            storer = storer\n        else:\n            storer = None\n\n        return storer\n\n\nclass BetaHLoss(BaseLoss):\n    """"""\n    Compute the Beta-VAE loss as in [1]\n\n    Parameters\n    ----------\n    beta : float, optional\n        Weight of the kl divergence.\n\n    kwargs:\n        Additional arguments for `BaseLoss`, e.g. rec_dist`.\n\n    References\n    ----------\n        [1] Higgins, Irina, et al. ""beta-vae: Learning basic visual concepts with\n        a constrained variational framework."" (2016).\n    """"""\n\n    def __init__(self, beta=4, **kwargs):\n        super().__init__(**kwargs)\n        self.beta = beta\n\n    def __call__(self, data, recon_data, latent_dist, is_train, storer, **kwargs):\n        storer = self._pre_call(is_train, storer)\n\n        rec_loss = _reconstruction_loss(data, recon_data,\n                                        storer=storer,\n                                        distribution=self.rec_dist)\n        kl_loss = _kl_normal_loss(*latent_dist, storer)\n        anneal_reg = (linear_annealing(0, 1, self.n_train_steps, self.steps_anneal)\n                      if is_train else 1)\n        loss = rec_loss + anneal_reg * (self.beta * kl_loss)\n\n        if storer is not None:\n            storer[\'loss\'].append(loss.item())\n\n        return loss\n\n\nclass BetaBLoss(BaseLoss):\n    """"""\n    Compute the Beta-VAE loss as in [1]\n\n    Parameters\n    ----------\n    C_init : float, optional\n        Starting annealed capacity C.\n\n    C_fin : float, optional\n        Final annealed capacity C.\n\n    gamma : float, optional\n        Weight of the KL divergence term.\n\n    kwargs:\n        Additional arguments for `BaseLoss`, e.g. rec_dist`.\n\n    References\n    ----------\n        [1] Burgess, Christopher P., et al. ""Understanding disentangling in\n        $\\beta$-VAE."" arXiv preprint arXiv:1804.03599 (2018).\n    """"""\n\n    def __init__(self, C_init=0., C_fin=20., gamma=100., **kwargs):\n        super().__init__(**kwargs)\n        self.gamma = gamma\n        self.C_init = C_init\n        self.C_fin = C_fin\n\n    def __call__(self, data, recon_data, latent_dist, is_train, storer, **kwargs):\n        storer = self._pre_call(is_train, storer)\n\n        rec_loss = _reconstruction_loss(data, recon_data,\n                                        storer=storer,\n                                        distribution=self.rec_dist)\n        kl_loss = _kl_normal_loss(*latent_dist, storer)\n\n        C = (linear_annealing(self.C_init, self.C_fin, self.n_train_steps, self.steps_anneal)\n             if is_train else self.C_fin)\n\n        loss = rec_loss + self.gamma * (kl_loss - C).abs()\n\n        if storer is not None:\n            storer[\'loss\'].append(loss.item())\n\n        return loss\n\n\nclass FactorKLoss(BaseLoss):\n    """"""\n    Compute the Factor-VAE loss as per Algorithm 2 of [1]\n\n    Parameters\n    ----------\n    device : torch.device\n\n    gamma : float, optional\n        Weight of the TC loss term. `gamma` in the paper.\n\n    discriminator : disvae.discriminator.Discriminator\n\n    optimizer_d : torch.optim\n\n    kwargs:\n        Additional arguments for `BaseLoss`, e.g. rec_dist`.\n\n    References\n    ----------\n        [1] Kim, Hyunjik, and Andriy Mnih. ""Disentangling by factorising.""\n        arXiv preprint arXiv:1802.05983 (2018).\n    """"""\n\n    def __init__(self, device,\n                 gamma=10.,\n                 disc_kwargs={},\n                 optim_kwargs=dict(lr=5e-5, betas=(0.5, 0.9)),\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.gamma = gamma\n        self.device = device\n        self.discriminator = Discriminator(**disc_kwargs).to(self.device)\n        self.optimizer_d = optim.Adam(self.discriminator.parameters(), **optim_kwargs)\n\n    def __call__(self, *args, **kwargs):\n        raise ValueError(""Use `call_optimize` to also train the discriminator"")\n\n    def call_optimize(self, data, model, optimizer, storer):\n        storer = self._pre_call(model.training, storer)\n\n        # factor-vae split data into two batches. In the paper they sample 2 batches\n        batch_size = data.size(dim=0)\n        half_batch_size = batch_size // 2\n        data = data.split(half_batch_size)\n        data1 = data[0]\n        data2 = data[1]\n\n        # Factor VAE Loss\n        recon_batch, latent_dist, latent_sample1 = model(data1)\n        rec_loss = _reconstruction_loss(data1, recon_batch,\n                                        storer=storer,\n                                        distribution=self.rec_dist)\n\n        kl_loss = _kl_normal_loss(*latent_dist, storer)\n\n        d_z = self.discriminator(latent_sample1)\n        # We want log(p_true/p_false). If not using logisitc regression but softmax\n        # then p_true = exp(logit_true) / Z; p_false = exp(logit_false) / Z\n        # so log(p_true/p_false) = logit_true - logit_false\n        tc_loss = (d_z[:, 0] - d_z[:, 1]).mean()\n        # with sigmoid (not good results) should be `tc_loss = (2 * d_z.flatten()).mean()`\n\n        anneal_reg = (linear_annealing(0, 1, self.n_train_steps, self.steps_anneal)\n                      if model.training else 1)\n        vae_loss = rec_loss + kl_loss + anneal_reg * self.gamma * tc_loss\n\n        if storer is not None:\n            storer[\'loss\'].append(vae_loss.item())\n            storer[\'tc_loss\'].append(tc_loss.item())\n\n        if not model.training:\n            # don\'t backprop if evaluating\n            return vae_loss\n\n        # Run VAE optimizer\n        optimizer.zero_grad()\n        vae_loss.backward(retain_graph=True)\n        optimizer.step()\n\n        # Discriminator Loss\n        # Get second sample of latent distribution\n        latent_sample2 = model.sample_latent(data2)\n        z_perm = _permute_dims(latent_sample2).detach()\n        d_z_perm = self.discriminator(z_perm)\n\n        # Calculate total correlation loss\n        # for cross entropy the target is the index => need to be long and says\n        # that it\'s first output for d_z and second for perm\n        ones = torch.ones(half_batch_size, dtype=torch.long, device=self.device)\n        zeros = torch.zeros_like(ones)\n        d_tc_loss = 0.5 * (F.cross_entropy(d_z, zeros) + F.cross_entropy(d_z_perm, ones))\n        # with sigmoid would be :\n        # d_tc_loss = 0.5 * (self.bce(d_z.flatten(), ones) + self.bce(d_z_perm.flatten(), 1 - ones))\n\n        # TO-DO: check ifshould also anneals discriminator if not becomes too good ???\n        #d_tc_loss = anneal_reg * d_tc_loss\n\n        # Run discriminator optimizer\n        self.optimizer_d.zero_grad()\n        d_tc_loss.backward()\n        self.optimizer_d.step()\n\n        if storer is not None:\n            storer[\'discrim_loss\'].append(d_tc_loss.item())\n\n        return vae_loss\n\n\nclass BtcvaeLoss(BaseLoss):\n    """"""\n    Compute the decomposed KL loss with either minibatch weighted sampling or\n    minibatch stratified sampling according to [1]\n\n    Parameters\n    ----------\n    n_data: int\n        Number of data in the training set\n\n    alpha : float\n        Weight of the mutual information term.\n\n    beta : float\n        Weight of the total correlation term.\n\n    gamma : float\n        Weight of the dimension-wise KL term.\n\n    is_mss : bool\n        Whether to use minibatch stratified sampling instead of minibatch\n        weighted sampling.\n\n    kwargs:\n        Additional arguments for `BaseLoss`, e.g. rec_dist`.\n\n    References\n    ----------\n       [1] Chen, Tian Qi, et al. ""Isolating sources of disentanglement in variational\n       autoencoders."" Advances in Neural Information Processing Systems. 2018.\n    """"""\n\n    def __init__(self, n_data, alpha=1., beta=6., gamma=1., is_mss=True, **kwargs):\n        super().__init__(**kwargs)\n        self.n_data = n_data\n        self.beta = beta\n        self.alpha = alpha\n        self.gamma = gamma\n        self.is_mss = is_mss  # minibatch stratified sampling\n\n    def __call__(self, data, recon_batch, latent_dist, is_train, storer,\n                 latent_sample=None):\n        storer = self._pre_call(is_train, storer)\n        batch_size, latent_dim = latent_sample.shape\n\n        rec_loss = _reconstruction_loss(data, recon_batch,\n                                        storer=storer,\n                                        distribution=self.rec_dist)\n        log_pz, log_qz, log_prod_qzi, log_q_zCx = _get_log_pz_qz_prodzi_qzCx(latent_sample,\n                                                                             latent_dist,\n                                                                             self.n_data,\n                                                                             is_mss=self.is_mss)\n        # I[z;x] = KL[q(z,x)||q(x)q(z)] = E_x[KL[q(z|x)||q(z)]]\n        mi_loss = (log_q_zCx - log_qz).mean()\n        # TC[z] = KL[q(z)||\\prod_i z_i]\n        tc_loss = (log_qz - log_prod_qzi).mean()\n        # dw_kl_loss is KL[q(z)||p(z)] instead of usual KL[q(z|x)||p(z))]\n        dw_kl_loss = (log_prod_qzi - log_pz).mean()\n\n        anneal_reg = (linear_annealing(0, 1, self.n_train_steps, self.steps_anneal)\n                      if is_train else 1)\n\n        # total loss\n        loss = rec_loss + (self.alpha * mi_loss +\n                           self.beta * tc_loss +\n                           anneal_reg * self.gamma * dw_kl_loss)\n\n        if storer is not None:\n            storer[\'loss\'].append(loss.item())\n            storer[\'mi_loss\'].append(mi_loss.item())\n            storer[\'tc_loss\'].append(tc_loss.item())\n            storer[\'dw_kl_loss\'].append(dw_kl_loss.item())\n            # computing this for storing and comparaison purposes\n            _ = _kl_normal_loss(*latent_dist, storer)\n\n        return loss\n\n\ndef _reconstruction_loss(data, recon_data, distribution=""bernoulli"", storer=None):\n    """"""\n    Calculates the per image reconstruction loss for a batch of data. I.e. negative\n    log likelihood.\n\n    Parameters\n    ----------\n    data : torch.Tensor\n        Input data (e.g. batch of images). Shape : (batch_size, n_chan,\n        height, width).\n\n    recon_data : torch.Tensor\n        Reconstructed data. Shape : (batch_size, n_chan, height, width).\n\n    distribution : {""bernoulli"", ""gaussian"", ""laplace""}\n        Distribution of the likelihood on the each pixel. Implicitely defines the\n        loss Bernoulli corresponds to a binary cross entropy (bse) loss and is the\n        most commonly used. It has the issue that it doesn\'t penalize the same\n        way (0.1,0.2) and (0.4,0.5), which might not be optimal. Gaussian\n        distribution corresponds to MSE, and is sometimes used, but hard to train\n        ecause it ends up focusing only a few pixels that are very wrong. Laplace\n        distribution corresponds to L1 solves partially the issue of MSE.\n\n    storer : dict\n        Dictionary in which to store important variables for vizualisation.\n\n    Returns\n    -------\n    loss : torch.Tensor\n        Per image cross entropy (i.e. normalized per batch but not pixel and\n        channel)\n    """"""\n    batch_size, n_chan, height, width = recon_data.size()\n    is_colored = n_chan == 3\n\n    if distribution == ""bernoulli"":\n        loss = F.binary_cross_entropy(recon_data, data, reduction=""sum"")\n    elif distribution == ""gaussian"":\n        # loss in [0,255] space but normalized by 255 to not be too big\n        loss = F.mse_loss(recon_data * 255, data * 255, reduction=""sum"") / 255\n    elif distribution == ""laplace"":\n        # loss in [0,255] space but normalized by 255 to not be too big but\n        # multiply by 255 and divide 255, is the same as not doing anything for L1\n        loss = F.l1_loss(recon_data, data, reduction=""sum"")\n        loss = loss * 3  # emperical value to give similar values than bernoulli => use same hyperparam\n        loss = loss * (loss != 0)  # masking to avoid nan\n    else:\n        assert distribution not in RECON_DIST\n        raise ValueError(""Unkown distribution: {}"".format(distribution))\n\n    loss = loss / batch_size\n\n    if storer is not None:\n        storer[\'recon_loss\'].append(loss.item())\n\n    return loss\n\n\ndef _kl_normal_loss(mean, logvar, storer=None):\n    """"""\n    Calculates the KL divergence between a normal distribution\n    with diagonal covariance and a unit normal distribution.\n\n    Parameters\n    ----------\n    mean : torch.Tensor\n        Mean of the normal distribution. Shape (batch_size, latent_dim) where\n        D is dimension of distribution.\n\n    logvar : torch.Tensor\n        Diagonal log variance of the normal distribution. Shape (batch_size,\n        latent_dim)\n\n    storer : dict\n        Dictionary in which to store important variables for vizualisation.\n    """"""\n    latent_dim = mean.size(1)\n    # batch mean of kl for each latent dimension\n    latent_kl = 0.5 * (-1 - logvar + mean.pow(2) + logvar.exp()).mean(dim=0)\n    total_kl = latent_kl.sum()\n\n    if storer is not None:\n        storer[\'kl_loss\'].append(total_kl.item())\n        for i in range(latent_dim):\n            storer[\'kl_loss_\' + str(i)].append(latent_kl[i].item())\n\n    return total_kl\n\n\ndef _permute_dims(latent_sample):\n    """"""\n    Implementation of Algorithm 1 in ref [1]. Randomly permutes the sample from\n    q(z) (latent_dist) across the batch for each of the latent dimensions (mean\n    and log_var).\n\n    Parameters\n    ----------\n    latent_sample: torch.Tensor\n        sample from the latent dimension using the reparameterisation trick\n        shape : (batch_size, latent_dim).\n\n    References\n    ----------\n        [1] Kim, Hyunjik, and Andriy Mnih. ""Disentangling by factorising.""\n        arXiv preprint arXiv:1802.05983 (2018).\n\n    """"""\n    perm = torch.zeros_like(latent_sample)\n    batch_size, dim_z = perm.size()\n\n    for z in range(dim_z):\n        pi = torch.randperm(batch_size).to(latent_sample.device)\n        perm[:, z] = latent_sample[pi, z]\n\n    return perm\n\n\ndef linear_annealing(init, fin, step, annealing_steps):\n    """"""Linear annealing of a parameter.""""""\n    if annealing_steps == 0:\n        return fin\n    assert fin > init\n    delta = fin - init\n    annealed = min(init + delta * step / annealing_steps, fin)\n    return annealed\n\n\n# Batch TC specific\n# TO-DO: test if mss is better!\ndef _get_log_pz_qz_prodzi_qzCx(latent_sample, latent_dist, n_data, is_mss=True):\n    batch_size, hidden_dim = latent_sample.shape\n\n    # calculate log q(z|x)\n    log_q_zCx = log_density_gaussian(latent_sample, *latent_dist).sum(dim=1)\n\n    # calculate log p(z)\n    # mean and log var is 0\n    zeros = torch.zeros_like(latent_sample)\n    log_pz = log_density_gaussian(latent_sample, zeros, zeros).sum(1)\n\n    mat_log_qz = matrix_log_density_gaussian(latent_sample, *latent_dist)\n\n    if is_mss:\n        # use stratification\n        log_iw_mat = log_importance_weight_matrix(batch_size, n_data).to(latent_sample.device)\n        mat_log_qz = mat_log_qz + log_iw_mat.view(batch_size, batch_size, 1)\n\n    log_qz = torch.logsumexp(mat_log_qz.sum(2), dim=1, keepdim=False)\n    log_prod_qzi = torch.logsumexp(mat_log_qz, dim=1, keepdim=False).sum(1)\n\n    return log_pz, log_qz, log_prod_qzi, log_q_zCx\n'"
disvae/models/vae.py,7,"b'""""""\nModule containing the main VAE class.\n""""""\nimport torch\nfrom torch import nn, optim\nfrom torch.nn import functional as F\n\nfrom disvae.utils.initialization import weights_init\nfrom .encoders import get_encoder\nfrom .decoders import get_decoder\n\nMODELS = [""Burgess""]\n\n\ndef init_specific_model(model_type, img_size, latent_dim):\n    """"""Return an instance of a VAE with encoder and decoder from `model_type`.""""""\n    model_type = model_type.lower().capitalize()\n    if model_type not in MODELS:\n        err = ""Unkown model_type={}. Possible values: {}""\n        raise ValueError(err.format(model_type, MODELS))\n\n    encoder = get_encoder(model_type)\n    decoder = get_decoder(model_type)\n    model = VAE(img_size, encoder, decoder, latent_dim)\n    model.model_type = model_type  # store to help reloading\n    return model\n\n\nclass VAE(nn.Module):\n    def __init__(self, img_size, encoder, decoder, latent_dim):\n        """"""\n        Class which defines model and forward pass.\n\n        Parameters\n        ----------\n        img_size : tuple of ints\n            Size of images. E.g. (1, 32, 32) or (3, 64, 64).\n        """"""\n        super(VAE, self).__init__()\n\n        if list(img_size[1:]) not in [[32, 32], [64, 64]]:\n            raise RuntimeError(""{} sized images not supported. Only (None, 32, 32) and (None, 64, 64) supported. Build your own architecture or reshape images!"".format(img_size))\n\n        self.latent_dim = latent_dim\n        self.img_size = img_size\n        self.num_pixels = self.img_size[1] * self.img_size[2]\n        self.encoder = encoder(img_size, self.latent_dim)\n        self.decoder = decoder(img_size, self.latent_dim)\n\n        self.reset_parameters()\n\n    def reparameterize(self, mean, logvar):\n        """"""\n        Samples from a normal distribution using the reparameterization trick.\n\n        Parameters\n        ----------\n        mean : torch.Tensor\n            Mean of the normal distribution. Shape (batch_size, latent_dim)\n\n        logvar : torch.Tensor\n            Diagonal log variance of the normal distribution. Shape (batch_size,\n            latent_dim)\n        """"""\n        if self.training:\n            std = torch.exp(0.5 * logvar)\n            eps = torch.randn_like(std)\n            return mean + std * eps\n        else:\n            # Reconstruction mode\n            return mean\n\n    def forward(self, x):\n        """"""\n        Forward pass of model.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Batch of data. Shape (batch_size, n_chan, height, width)\n        """"""\n        latent_dist = self.encoder(x)\n        latent_sample = self.reparameterize(*latent_dist)\n        reconstruct = self.decoder(latent_sample)\n        return reconstruct, latent_dist, latent_sample\n\n    def reset_parameters(self):\n        self.apply(weights_init)\n\n    def sample_latent(self, x):\n        """"""\n        Returns a sample from the latent distribution.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Batch of data. Shape (batch_size, n_chan, height, width)\n        """"""\n        latent_dist = self.encoder(x)\n        latent_sample = self.reparameterize(*latent_dist)\n        return latent_sample\n'"
disvae/utils/__init__.py,0,b''
disvae/utils/initialization.py,4,"b'import torch\nfrom torch import nn\n\n\ndef get_activation_name(activation):\n    """"""Given a string or a `torch.nn.modules.activation` return the name of the activation.""""""\n    if isinstance(activation, str):\n        return activation\n\n    mapper = {nn.LeakyReLU: ""leaky_relu"", nn.ReLU: ""relu"", nn.Tanh: ""tanh"",\n              nn.Sigmoid: ""sigmoid"", nn.Softmax: ""sigmoid""}\n    for k, v in mapper.items():\n        if isinstance(activation, k):\n            return k\n\n    raise ValueError(""Unkown given activation type : {}"".format(activation))\n\n\ndef get_gain(activation):\n    """"""Given an object of `torch.nn.modules.activation` or an activation name\n    return the correct gain.""""""\n    if activation is None:\n        return 1\n\n    activation_name = get_activation_name(activation)\n\n    param = None if activation_name != ""leaky_relu"" else activation.negative_slope\n    gain = nn.init.calculate_gain(activation_name, param)\n\n    return gain\n\n\ndef linear_init(layer, activation=""relu""):\n    """"""Initialize a linear layer.\n    Args:\n        layer (nn.Linear): parameters to initialize.\n        activation (`torch.nn.modules.activation` or str, optional) activation that\n            will be used on the `layer`.\n    """"""\n    x = layer.weight\n\n    if activation is None:\n        return nn.init.xavier_uniform_(x)\n\n    activation_name = get_activation_name(activation)\n\n    if activation_name == ""leaky_relu"":\n        a = 0 if isinstance(activation, str) else activation.negative_slope\n        return nn.init.kaiming_uniform_(x, a=a, nonlinearity=\'leaky_relu\')\n    elif activation_name == ""relu"":\n        return nn.init.kaiming_uniform_(x, nonlinearity=\'relu\')\n    elif activation_name in [""sigmoid"", ""tanh""]:\n        return nn.init.xavier_uniform_(x, gain=get_gain(activation))\n\n\ndef weights_init(module):\n    if isinstance(module, torch.nn.modules.conv._ConvNd):\n        # TO-DO: check litterature\n        linear_init(module)\n    elif isinstance(module, nn.Linear):\n        linear_init(module)\n'"
disvae/utils/math.py,8,"b'\nimport math\n\nfrom tqdm import trange, tqdm\nimport torch\n\n\ndef matrix_log_density_gaussian(x, mu, logvar):\n    """"""Calculates log density of a Gaussian for all combination of bacth pairs of\n    `x` and `mu`. I.e. return tensor of shape `(batch_size, batch_size, dim)`\n    instead of (batch_size, dim) in the usual log density.\n\n    Parameters\n    ----------\n    x: torch.Tensor\n        Value at which to compute the density. Shape: (batch_size, dim).\n\n    mu: torch.Tensor\n        Mean. Shape: (batch_size, dim).\n\n    logvar: torch.Tensor\n        Log variance. Shape: (batch_size, dim).\n\n    batch_size: int\n        number of training images in the batch\n    """"""\n    batch_size, dim = x.shape\n    x = x.view(batch_size, 1, dim)\n    mu = mu.view(1, batch_size, dim)\n    logvar = logvar.view(1, batch_size, dim)\n    return log_density_gaussian(x, mu, logvar)\n\n\ndef log_density_gaussian(x, mu, logvar):\n    """"""Calculates log density of a Gaussian.\n\n    Parameters\n    ----------\n    x: torch.Tensor or np.ndarray or float\n        Value at which to compute the density.\n\n    mu: torch.Tensor or np.ndarray or float\n        Mean.\n\n    logvar: torch.Tensor or np.ndarray or float\n        Log variance.\n    """"""\n    normalization = - 0.5 * (math.log(2 * math.pi) + logvar)\n    inv_var = torch.exp(-logvar)\n    log_density = normalization - 0.5 * ((x - mu)**2 * inv_var)\n    return log_density\n\n\ndef log_importance_weight_matrix(batch_size, dataset_size):\n    """"""\n    Calculates a log importance weight matrix\n\n    Parameters\n    ----------\n    batch_size: int\n        number of training images in the batch\n\n    dataset_size: int\n    number of training images in the dataset\n    """"""\n    N = dataset_size\n    M = batch_size - 1\n    strat_weight = (N - M) / (N * M)\n    W = torch.Tensor(batch_size, batch_size).fill_(1 / M)\n    W.view(-1)[::M + 1] = 1 / N\n    W.view(-1)[1::M + 1] = strat_weight\n    W[M - 1, 0] = strat_weight\n    return W.log()\n'"
disvae/utils/modelIO.py,3,"b'import json\nimport os\nimport re\n\nimport numpy as np\nimport torch\n\nfrom disvae import init_specific_model\n\nMODEL_FILENAME = ""model.pt""\nMETA_FILENAME = ""specs.json""\n\n\ndef save_model(model, directory, metadata=None, filename=MODEL_FILENAME):\n    """"""\n    Save a model and corresponding metadata.\n\n    Parameters\n    ----------\n    model : nn.Module\n        Model.\n\n    directory : str\n        Path to the directory where to save the data.\n\n    metadata : dict\n        Metadata to save.\n    """"""\n    device = next(model.parameters()).device\n    model.cpu()\n\n    if metadata is None:\n        # save the minimum required for loading\n        metadata = dict(img_size=model.img_size, latent_dim=model.latent_dim,\n                        model_type=model.model_type)\n\n    save_metadata(metadata, directory)\n\n    path_to_model = os.path.join(directory, filename)\n    torch.save(model.state_dict(), path_to_model)\n\n    model.to(device)  # restore device\n\n\ndef load_metadata(directory, filename=META_FILENAME):\n    """"""Load the metadata of a training directory.\n\n    Parameters\n    ----------\n    directory : string\n        Path to folder where model is saved. For example \'./experiments/mnist\'.\n    """"""\n    path_to_metadata = os.path.join(directory, filename)\n\n    with open(path_to_metadata) as metadata_file:\n        metadata = json.load(metadata_file)\n\n    return metadata\n\n\ndef save_metadata(metadata, directory, filename=META_FILENAME, **kwargs):\n    """"""Load the metadata of a training directory.\n\n    Parameters\n    ----------\n    metadata:\n        Object to save\n\n    directory: string\n        Path to folder where to save model. For example \'./experiments/mnist\'.\n\n    kwargs:\n        Additional arguments to `json.dump`\n    """"""\n    path_to_metadata = os.path.join(directory, filename)\n\n    with open(path_to_metadata, \'w\') as f:\n        json.dump(metadata, f, indent=4, sort_keys=True, **kwargs)\n\n\ndef load_model(directory, is_gpu=True, filename=MODEL_FILENAME):\n    """"""Load a trained model.\n\n    Parameters\n    ----------\n    directory : string\n        Path to folder where model is saved. For example \'./experiments/mnist\'.\n\n    is_gpu : bool\n        Whether to load on GPU is available.\n    """"""\n    device = torch.device(""cuda"" if torch.cuda.is_available() and is_gpu\n                          else ""cpu"")\n\n    path_to_model = os.path.join(directory, MODEL_FILENAME)\n\n    metadata = load_metadata(directory)\n    img_size = metadata[""img_size""]\n    latent_dim = metadata[""latent_dim""]\n    model_type = metadata[""model_type""]\n\n    path_to_model = os.path.join(directory, filename)\n    model = _get_model(model_type, img_size, latent_dim, device, path_to_model)\n    return model\n\n\ndef load_checkpoints(directory, is_gpu=True):\n    """"""Load all chechpointed models.\n\n    Parameters\n    ----------\n    directory : string\n        Path to folder where model is saved. For example \'./experiments/mnist\'.\n\n    is_gpu : bool\n        Whether to load on GPU .\n    """"""\n    checkpoints = []\n    for root, _, filenames in os.walk(directory):\n        for filename in filenames:\n            results = re.search(r\'.*?-([0-9].*?).pt\', filename)\n            if results is not None:\n                epoch_idx = int(results.group(1))\n                model = load_model(root, is_gpu=is_gpu, filename=filename)\n                checkpoints.append((epoch_idx, model))\n\n    return checkpoints\n\n\ndef _get_model(model_type, img_size, latent_dim, device, path_to_model):\n    """""" Load a single model.\n\n    Parameters\n    ----------\n    model_type : str\n        The name of the model to load. For example Burgess.\n    img_size : tuple\n        Tuple of the number of pixels in the image width and height.\n        For example (32, 32) or (64, 64).\n    latent_dim : int\n        The number of latent dimensions in the bottleneck.\n\n    device : str\n        Either \'cuda\' or \'cpu\'\n    path_to_device : str\n        Full path to the saved model on the device.\n    """"""\n    model = init_specific_model(model_type, img_size, latent_dim).to(device)\n    # works with state_dict to make it independent of the file structure\n    model.load_state_dict(torch.load(path_to_model), strict=False)\n    model.eval()\n\n    return model\n\n\ndef numpy_serialize(obj):\n    if type(obj).__module__ == np.__name__:\n        if isinstance(obj, np.ndarray):\n            return obj.tolist()\n        else:\n            return obj.item()\n    raise TypeError(\'Unknown type:\', type(obj))\n\n\ndef save_np_arrays(arrays, directory, filename):\n    """"""Save dictionary of arrays in json file.""""""\n    save_metadata(arrays, directory, filename=filename, default=numpy_serialize)\n\n\ndef load_np_arrays(directory, filename):\n    """"""Load dictionary of arrays from json file.""""""\n    arrays = load_metadata(directory, filename=filename)\n    return {k: np.array(v) for k, v in arrays.items()}\n'"
