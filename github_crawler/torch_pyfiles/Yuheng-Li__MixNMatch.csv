file_path,api_count,code
code/config.py,0,"b'from __future__ import division\nfrom __future__ import print_function\n\nimport os.path as osp\nimport numpy as np\nfrom easydict import EasyDict as edict\n\n\n__C = edict()\ncfg = __C\n\n# Dataset name: flowers, birds\n#__C.DATASET_NAME = \'birds\'\n#__C.CONFIG_NAME = \'\'\n__C.DATA_DIR = \'../data/birds\'\n#__C.SAVE_DIR = \'\'\n__C.GPU_ID = \'0\'\n#__C.CUDA = True\n\n\n__C.TREE = edict()\n__C.TREE.BRANCH_NUM = 3\n__C.TREE.BASE_SIZE = 64\n__C.SUPER_CATEGORIES = 20\n__C.FINE_GRAINED_CATEGORIES = 200\n__C.TEST_CHILD_CLASS = 0\n__C.TEST_PARENT_CLASS = 0\n__C.TEST_BACKGROUND_CLASS = 0\n__C.TIED_CODES = False\n\n# Test options\n__C.TEST = edict()\n\n# Training options\n__C.TRAIN = edict()\n__C.TRAIN.BATCH_SIZE = 12\n__C.TRAIN.BG_LOSS_WT = 10\n__C.TRAIN.VIS_COUNT = 80\n__C.TRAIN.FIRST_MAX_EPOCH = 600\n__C.TRAIN.SECOND_MAX_EPOCH = 400\n__C.TRAIN.HARDNEG_MAX_ITER = 1500\n__C.TRAIN.SNAPSHOT_INTERVAL = 2000\n__C.TRAIN.SNAPSHOT_INTERVAL_HARDNEG = 500\n__C.TRAIN.DISCRIMINATOR_LR = 2e-4\n__C.TRAIN.GENERATOR_LR = 2e-4\n__C.TRAIN.ENCODER_LR = 2e-4\n__C.TRAIN.FLAG = True\n# __C.TRAIN.NET_G = \n# __C.TRAIN.NET_D = [ ]\n# __C.TRAIN.ENCODER = None\n# #../output/output_premask_500p100c_2/Model/encoder_13.pth\n\n\n\n# Modal options\n__C.GAN = edict()\n__C.GAN.DF_DIM = 64\n__C.GAN.GF_DIM = 64\n__C.GAN.Z_DIM = 100\n__C.GAN.NETWORK_TYPE = \'default\'\n__C.GAN.R_NUM = 2\n\n\n\n\ndef _merge_a_into_b(a, b):\n    """"""Merge config dictionary a into config dictionary b, clobbering the\n    options in b whenever they are also specified in a.\n    """"""\n    if type(a) is not edict:\n        return\n\n    for k, v in a.items():   ######################old python: iteritems#############\n        # a must specify keys that are in b\n        if not k in b:   ######################old python:  if not b.has_key(k):  #############\n            raise KeyError(\'{} is not a valid config key\'.format(k))\n\n        # the types must match, too\n        old_type = type(b[k])\n        if old_type is not type(v):\n            if isinstance(b[k], np.ndarray):\n                v = np.array(v, dtype=b[k].dtype)\n            else:\n                raise ValueError((\'Type mismatch ({} vs. {}) \'\n                                  \'for config key: {}\').format(type(b[k]),\n                                                               type(v), k))\n\n        # recursively merge dicts\n        if type(v) is edict:\n            try:\n                _merge_a_into_b(a[k], b[k])\n            except:\n                print(\'Error under config key: {}\'.format(k))\n                raise\n        else:\n            b[k] = v\n\n\ndef cfg_from_file(filename):\n    """"""Load a config file and merge it into the default options.""""""\n    import yaml\n    with open(filename, \'r\') as f:\n        yaml_cfg = edict(yaml.load(f))\n\n    _merge_a_into_b(yaml_cfg, __C)\n'"
code/datasets.py,5,"b""import torch.utils.data as data\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport os\nimport os.path\nimport random\nimport numpy as np\nfrom config import cfg\nimport torch.utils.data as data\nimport os\nimport os.path\nimport pandas as pd\nimport torch\nfrom copy import deepcopy\n\n\nIMG_EXTENSIONS = ['.jpg', '.JPG', '.jpeg', '.JPEG',\n                  '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP']\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\n\ndef get_imgs(img_path, imsize, bbox=None, transform=None, normalize=None):\n\n\n    img = Image.open(img_path).convert('RGB')\n    width, height = img.size\n    if bbox is not None:\n        r = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n        y1 = np.maximum(0, center_y - r)\n        y2 = np.minimum(height, center_y + r)\n        x1 = np.maximum(0, center_x - r)\n        x2 = np.minimum(width, center_x + r)\n        fimg = deepcopy(img)\n        fimg_arr = np.array(fimg)\n        fimg = Image.fromarray(fimg_arr)\n        cimg = img.crop([x1, y1, x2, y2])\n\n    if transform is not None:\n        cimg = transform(cimg)\n\n    retf = []\n    retc = []\n    re_cimg = transforms.Resize(imsize[1])(cimg)\n    retc.append(normalize(re_cimg))\n\n    # We use full image to get background patches\n\n    # We resize the full image to be 126 X 126 (instead of 128 X 128)  for the full coverage of the input (full) image by\n    # the receptive fields of the final convolution layer of background discriminator\n\n    my_crop_width = 126\n    re_fimg = transforms.Resize(int(my_crop_width * 76 / 64))(fimg)\n    re_width, re_height = re_fimg.size\n\n    # random cropping\n    x_crop_range = re_width-my_crop_width\n    y_crop_range = re_height-my_crop_width\n\n    crop_start_x = np.random.randint(x_crop_range)\n    crop_start_y = np.random.randint(y_crop_range)\n\n    crop_re_fimg = re_fimg.crop([crop_start_x, crop_start_y, crop_start_x + my_crop_width, crop_start_y + my_crop_width])\n    warped_x1 = bbox[0] * re_width / width\n    warped_y1 = bbox[1] * re_height / height\n    warped_x2 = warped_x1 + (bbox[2] * re_width / width)\n    warped_y2 = warped_y1 + (bbox[3] * re_height / height)\n\n    warped_x1 = min(max(0, warped_x1 - crop_start_x), my_crop_width)\n    warped_y1 = min(max(0, warped_y1 - crop_start_y), my_crop_width)\n    warped_x2 = max(min(my_crop_width, warped_x2 - crop_start_x), 0)\n    warped_y2 = max(min(my_crop_width, warped_y2 - crop_start_y), 0)\n\n    # random flipping\n    random_flag = np.random.randint(2)\n    if(random_flag == 0):\n        crop_re_fimg = crop_re_fimg.transpose(Image.FLIP_LEFT_RIGHT)\n        flipped_x1 = my_crop_width - warped_x2\n        flipped_x2 = my_crop_width - warped_x1\n        warped_x1 = flipped_x1\n        warped_x2 = flipped_x2\n\n    retf.append(normalize(crop_re_fimg))\n\n    warped_bbox = []\n    warped_bbox.append(warped_y1)\n    warped_bbox.append(warped_x1)\n    warped_bbox.append(warped_y2)\n    warped_bbox.append(warped_x2)\n\n    return retf[0], retc[0], warped_bbox\n\n\nclass Dataset(data.Dataset):\n    def __init__(self, data_dir, base_size=64, transform=None):\n\n        self.transform = transform\n        self.norm = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n        self.imsize = []\n        for i in range(cfg.TREE.BRANCH_NUM):\n            self.imsize.append(base_size)\n            base_size = base_size * 2\n\n        self.data = []\n        self.data_dir = data_dir\n        self.bbox = self.load_bbox()\n        self.filenames = self.load_filenames(data_dir)\n        if cfg.TRAIN.FLAG:\n            self.iterator = self.prepair_training_pairs\n        else:\n            self.iterator = self.prepair_test_pairs\n\n    # only used in background stage\n\n    def load_bbox(self):\n        # Returns a dictionary with image filename as 'key' and its bounding box coordinates as 'value'\n\n        data_dir = self.data_dir\n        bbox_path = os.path.join(data_dir, 'bounding_boxes.txt')\n        df_bounding_boxes = pd.read_csv(bbox_path,\n                                        delim_whitespace=True,\n                                        header=None).astype(int)\n        filepath = os.path.join(data_dir, 'images.txt')\n        df_filenames = \\\n            pd.read_csv(filepath, delim_whitespace=True, header=None)\n        filenames = df_filenames[1].tolist()\n        #print('Total filenames: ', len(filenames), filenames[0])\n        filename_bbox = {img_file[:-4]: [] for img_file in filenames}\n        numImgs = len(filenames)\n        for i in range(0, numImgs):\n            bbox = df_bounding_boxes.iloc[i][1:].tolist()\n            key = filenames[i][:-4]\n            filename_bbox[key] = bbox\n        return filename_bbox\n\n    def load_filenames(self, data_dir):\n        filepath = os.path.join(data_dir, 'images.txt')\n        df_filenames = \\\n            pd.read_csv(filepath, delim_whitespace=True, header=None)\n        filenames = df_filenames[1].tolist()\n        filenames = [fname[:-4] for fname in filenames]\n        print('Load filenames from: %s (%d)' % (filepath, len(filenames)))\n        return filenames\n\n    def prepair_training_pairs(self, index):\n        key = self.filenames[index]\n        if self.bbox is not None:\n            bbox = self.bbox[key]\n        else:\n            bbox = None\n        data_dir = self.data_dir\n        img_name = '%s/images/%s.jpg' % (data_dir, key)\n\n\n        fimgs, cimgs, warped_bbox = get_imgs(img_name, self.imsize,\n                                             bbox, self.transform, normalize=self.norm)\n\n\n        # Randomly generating child code during training\n        rand_class = random.sample(range(cfg.FINE_GRAINED_CATEGORIES), 1)\n        c_code = torch.zeros([cfg.FINE_GRAINED_CATEGORIES, ])\n        c_code[rand_class] = 1\n\n        return fimgs, cimgs, c_code, key, warped_bbox\n\n    def prepair_test_pairs(self, index):\n        key = self.filenames[index]\n        if self.bbox is not None:\n            bbox = self.bbox[key]\n        else:\n            bbox = None\n        data_dir = self.data_dir\n        c_code = self.c_code[index, :, :]\n        img_name = '%s/images/%s.jpg' % (data_dir, key)\n        _, imgs, _ = get_imgs(img_name, self.imsize,\n                              bbox, self.transform, normalize=self.norm)\n\n        return imgs, c_code, key\n\n    def __getitem__(self, index):\n        return self.iterator(index)\n\n    def __len__(self):\n        return len(self.filenames)\n\n\n\n\ndef get_dataloader(bs=None):\n\n    imsize = cfg.TREE.BASE_SIZE * (2 ** (cfg.TREE.BRANCH_NUM-1))\n    image_transform = transforms.Compose([ transforms.Resize(int(imsize * 76 / 64)),\n                                           transforms.RandomCrop(imsize),\n                                           transforms.RandomHorizontalFlip()])\n\n    dataset = Dataset(cfg.DATA_DIR, base_size=cfg.TREE.BASE_SIZE, transform=image_transform)\n   \n    if bs == None:\n        dataloader = torch.utils.data.DataLoader(dataset, batch_size=cfg.TRAIN.BATCH_SIZE, drop_last=True, shuffle=True)\n    else:\n        dataloader = torch.utils.data.DataLoader(dataset, batch_size=bs, drop_last=True, shuffle=True)\n\n    return dataloader\n\n   \n\n \n\n\n\n\n\n\n"""
code/eval.py,13,"b'from config import cfg\nimport os\nimport time\nfrom PIL import Image\nimport torch.backends.cudnn as cudnn\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.utils as vutils\nfrom tensorboardX import SummaryWriter\nfrom model_eval import G_NET, Encoder, FeatureExtractor\nimport torchvision.transforms as transforms\nimport random\nimport torch.nn.functional as F\nfrom utils import *\nimport pdb\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nfrom random import sample\nimport argparse\n\n\ndevice = torch.device(""cuda:"" + cfg.GPU_ID)\ngpus = [int(ix) for ix in cfg.GPU_ID.split(\',\')]\n\n\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""--z"",  help=""path to z(pose) source image"" )  \nparser.add_argument(""--b"",  help=""path to b(background) source image"" ) \nparser.add_argument(""--p"",  help=""path to p(shape) source image"" )\nparser.add_argument(""--c"",  help=""path to c(texture) source image"" )\nparser.add_argument(""--out"",  help=""path to output image"" )\nparser.add_argument(""--mode"",  help=""either code or feature"" )\nparser.add_argument(""--models"",  help=""dir to pretrained models"" )\nargs = parser.parse_args()\n\n\n\nZ = args.z\nB = args.b\nP = args.p\nC = args.c\nOUT = args.out\nMODE = args.mode\nMODELS = args.models\n\n\n\ndef load_network(names):\n    ""load pretrained generator and encoder""\n\n\n    # prepare G net     \n    netG = G_NET().to(device)  \n    netG = torch.nn.DataParallel(netG, device_ids=gpus)\n    state_dict = torch.load( names[0]  )\n    netG.load_state_dict(state_dict)\n\n    # prepare encoder\n    encoder = Encoder().to(device)\n    encoder = torch.nn.DataParallel(encoder, device_ids=gpus)\n    state_dict = torch.load( names[1] )\n    encoder.load_state_dict(state_dict)\n\n    extractor = FeatureExtractor(3,16)    \n    extractor = torch.nn.DataParallel(extractor , device_ids=gpus)\n    extractor.load_state_dict(torch.load(  names[2] ))\n    extractor.to(device)\n    \n    return netG.eval(), encoder.eval(), extractor.eval()\n\n\n\n\ndef get_images(fire, size=[128,128]):\n    transform = transforms.Compose([ transforms.Resize( (size[0],size[1]) ) ])\n    normalize = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])    \n    img = Image.open(   fire   ).convert(\'RGB\') \n    img = transform(img)\n    img = normalize(img)   \n    return img.unsqueeze(0)\n\n\n\n\ndef save_img(img, file ):  \n    img = img.cpu()    \n    vutils.save_image( img, file, scale_each=True, normalize=True )\n    real_img_set = vutils.make_grid(img).numpy()\n    real_img_set = np.transpose(real_img_set, (1, 2, 0))\n    real_img_set = real_img_set * 255\n    real_img_set = real_img_set.astype(np.uint8)\n\n\n\ndef eval_code():\n\n    names = [ os.path.join(MODELS,\'G.pth\'), os.path.join(MODELS,\'E.pth\'), os.path.join(MODELS,\'EX.pth\')  ] \n    netG, encoder, _ = load_network(names)\n    \n\n    real_img_z  = get_images(Z)          \n    real_img_b  = get_images(B) \n    real_img_p  = get_images(P)            \n    real_img_c  = get_images(C)        \n   \n\n\n    with torch.no_grad():\n        fake_z2, _, _, _ = encoder( real_img_z.to(device), \'softmax\' )\n        fake_z1, fake_b, _, _ = encoder( real_img_b.to(device), \'softmax\' )\n        _, _, fake_p, _ = encoder( real_img_p.to(device), \'softmax\' )\n        _, _, _, fake_c = encoder( real_img_c.to(device), \'softmax\' )    \n        \n        fake_imgs, _, _, _ = netG(fake_z1, fake_z2, fake_c, fake_p,  fake_b, \'code\' )\n        img = fake_imgs[2]\n         \n    save_img(img, OUT)\n\n\n\n\ndef eval_feature():\n\n    names = [ os.path.join(MODELS,\'G.pth\'), os.path.join(MODELS,\'E.pth\'), os.path.join(MODELS,\'EX.pth\')  ]\n    netG, encoder, extractor = load_network(names)   \n    \n       \n    real_img_b  = get_images(B) \n    real_img_p  = get_images(P)            \n    real_img_c  = get_images(C)            \n\n\n    with torch.no_grad():\n        shape_feature = extractor( real_img_p.to(device) )\n        fake_z1, fake_b, _, _ = encoder( real_img_b.to(device), \'softmax\' )\n        _, _, _, fake_c = encoder( real_img_c.to(device), \'softmax\' )     \n        \n        fake_imgs, _, _, _ = netG(fake_z1, None, fake_c, shape_feature, fake_b, \'feature\' )\n        img = fake_imgs[2]       \n\n\n    save_img(img, OUT)\n\n\n\nif __name__ == ""__main__"":\n\n    if MODE==\'code\':\n        eval_code()\n    elif MODE==\'feature\':\n        eval_feature()\n    else:\n        raise ValueError()\n\n\n\n\n\n'"
code/model_eval.py,18,"b""import sys\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nfrom config import cfg\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom torch.nn import Upsample\nimport time\nfrom collections import deque\n\nclass GLU(nn.Module):\n    def __init__(self):\n        super(GLU, self).__init__()\n    def forward(self, x):\n        nc = x.size(1)\n        assert nc % 2 == 0, 'channels dont divide 2!'\n        nc = int(nc/2)\n        return x[:, :nc] * F.sigmoid(x[:, nc:])\n\n\ndef conv3x3(in_planes, out_planes):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=1, padding=1, bias=False)\n\n\n\ndef child_to_parent(child_c_code, classes_child, classes_parent):\n    ratio = classes_child / classes_parent\n    arg_parent = torch.argmax(child_c_code,  dim=1) / ratio\n    parent_c_code = torch.zeros([child_c_code.size(0), classes_parent]).cuda()\n    for i in range(child_c_code.size(0)):\n        parent_c_code[i][arg_parent[i]] = 1\n    return parent_c_code\n\n\n# ############## G networks ################################################\n# Upsale the spatial size by a factor of 2\ndef upBlock(in_planes, out_planes):\n    block = nn.Sequential(  nn.Upsample(scale_factor=2, mode='nearest'),\n                            conv3x3(in_planes, out_planes * 2),\n                            nn.BatchNorm2d(out_planes * 2),\n                            GLU()  )\n    return block\n\n\ndef sameBlock(in_planes, out_planes):\n    block = nn.Sequential(  conv3x3(in_planes, out_planes * 2),\n                            nn.BatchNorm2d(out_planes * 2),\n                            GLU()   )\n    return block\n\n\n\nclass ResBlock(nn.Module):\n    def __init__(self, channel_num):\n        super(ResBlock, self).__init__()\n        self.block = nn.Sequential(  conv3x3(channel_num, channel_num * 2),\n                                     nn.BatchNorm2d(channel_num * 2),\n                                     GLU(),\n                                     conv3x3(channel_num, channel_num),\n                                     nn.BatchNorm2d(channel_num)  )\n\n    def forward(self, x):\n        residual = x\n        out = self.block(x)\n        out += residual\n        return out\n\n\n\nclass GET_IMAGE(nn.Module):\n    def __init__(self, ngf):\n        super().__init__()\n        self.img = nn.Sequential( conv3x3(ngf, 3),  nn.Tanh() )\n    def forward(self, h_code):\n        return self.img(h_code)\n\n\nclass GET_MASK(nn.Module):\n    def __init__(self, ngf):\n        super().__init__()\n        self.img = nn.Sequential( conv3x3(ngf, 1), nn.Sigmoid() )\n    def forward(self, h_code):\n        return self.img(h_code)\n\n\n\nclass BACKGROUND_STAGE(nn.Module):\n    def __init__(self, ngf):\n        super().__init__()\n        \n        self.ngf = ngf        \n        in_dim = cfg.GAN.Z_DIM + cfg.FINE_GRAINED_CATEGORIES  \n\n        self.fc = nn.Sequential( nn.Linear(in_dim, ngf*4*4 * 2, bias=False), nn.BatchNorm1d(ngf*4*4 * 2), GLU())\n        # 1024*4*4\n        self.upsample1 = upBlock(ngf, ngf // 2 )   \n        # 512*8*8\n        self.upsample2 = upBlock(ngf // 2, ngf // 4) \n        # 256*16*16\n        self.upsample3 = upBlock(ngf // 4, ngf // 8) \n        # 128*32*32\n        self.upsample4 = upBlock(ngf // 8, ngf // 8) \n        # 128*64*64\n        self.upsample5 = upBlock(ngf // 8, ngf // 16)\n        # 64*128*128\n\n    def forward(self, z_input, input):\n\n        in_code = torch.cat([z_input, input], dim=1)\n        out_code = self.fc(in_code).view(-1, self.ngf, 4, 4) \n        out_code = self.upsample1(out_code) \n        out_code = self.upsample2(out_code) \n        out_code = self.upsample3(out_code)\n        out_code = self.upsample4(out_code)\n        out_code = self.upsample5(out_code)\n\n        return out_code\n\n\n\n\n\nclass PARENT_STAGE(nn.Module):\n    def __init__(self, ngf):\n        super().__init__()\n        \n        self.ngf = ngf\n        in_dim = cfg.GAN.Z_DIM + cfg.SUPER_CATEGORIES\n        self.code_len = cfg.SUPER_CATEGORIES\n\n        self.fc = nn.Sequential( nn.Linear(in_dim, ngf*4*4 * 2, bias=False), nn.BatchNorm1d(ngf*4*4 * 2), GLU())\n        # 512*4*4\n        self.upsample1 = upBlock(ngf, ngf//2 )\n        # 256*8*8\n        self.upsample2 = upBlock(ngf // 2, ngf // 4)\n        # 128*16*16\n        self.upsample3 = upBlock(ngf // 4, ngf // 8)\n        # 64*32*32\n        self.upsample4 = upBlock(ngf // 8, ngf // 32)\n        # 16*64*64\n        self.upsample5 = upBlock(ngf // 32, ngf // 32)\n        # 16*128*128\n        self.jointConv = sameBlock( cfg.SUPER_CATEGORIES+ngf//32, ngf//32 )\n        # (16+20)*128*128 --> 16*128*128\n        self.residual = self._make_layer(3, ngf//32)\n        # 16*128*128\n\n    def _make_layer(self,num_residual,ngf):\n        layers = []\n        for _ in range(num_residual):\n            layers.append( ResBlock(ngf) )\n        return nn.Sequential(*layers)\n\n    def forward(self, z_input, input, which):\n        \n        if which == 'code':\n            in_code = torch.cat([z_input, input], dim=1)\n            out_code = self.fc(in_code).view(-1, self.ngf, 4, 4) \n            out_code = self.upsample1(out_code) \n            out_code = self.upsample2(out_code)\n            out_code = self.upsample3(out_code)\n            out_code = self.upsample4(out_code)\n            out_code = self.upsample5(out_code)\n\n            h, w  = out_code.size(2),out_code.size(3)\n            input = input.view(-1, self.code_len, 1, 1).repeat(1, 1, h, w) \n            out_code = torch.cat( (out_code, input), dim=1)       \n            out_code = self.jointConv(out_code)\n            out_code = self.residual(out_code)\n            return out_code\n\n        elif which =='feature':\n            return input\n            \n        else:\n            raise ValueError('either code or feature')\n\n\n\n\n\n\n\nclass CHILD_STAGE(nn.Module):\n    def __init__(self, ngf, num_residual=2):\n        super().__init__()\n        \n        self.ngf = ngf \n        self.code_len = cfg.FINE_GRAINED_CATEGORIES\n        self.num_residual = num_residual\n\n        self.jointConv = sameBlock( self.code_len+self.ngf, ngf*2 )\n        self.residual = self._make_layer()\n        self.samesample = sameBlock(ngf*2, ngf)\n\n    def _make_layer(self):\n        layers = []\n        for _ in range(self.num_residual):\n            layers.append( ResBlock(self.ngf*2) )\n        return nn.Sequential(*layers)\n\n    def forward(self, h_code, code):\n        \n        h, w  = h_code.size(2),h_code.size(3)\n        code = code.view(-1, self.code_len, 1, 1).repeat(1, 1, h, w) \n        h_c_code = torch.cat((code, h_code), 1)\n        \n        out_code = self.jointConv(h_c_code)\n        out_code = self.residual(out_code)\n        out_code = self.samesample(out_code)\n        return out_code\n\n\nclass G_NET(nn.Module):\n    def __init__(self):\n        super(G_NET, self).__init__()\n        \n        ngf = cfg.GAN.GF_DIM \n        self.scale_fimg = nn.UpsamplingBilinear2d(size=[126, 126])\n\n        # Background stage\n        self.background_stage = BACKGROUND_STAGE( ngf*8 )\n        self.background_image = GET_IMAGE(ngf//2)\n\n        # Parent stage networks\n        self.parent_stage = PARENT_STAGE( ngf*8 )\n        self.parent_image = GET_IMAGE( ngf//4 )\n        self.parent_mask = GET_MASK( ngf//4 )\n\n        # Child stage networks\n        self.child_stage = CHILD_STAGE( ngf//4 )\n        self.child_image = GET_IMAGE( ngf//4 ) \n        self.child_mask = GET_MASK( ngf//4 )\n\n    def forward(self, z_code1, z_code2, c_code, p_code, b_code, which):\n\n        fake_imgs = []  # Will contain [background image, parent image, child image]\n        fg_imgs = []  # Will contain [parent foreground, child foreground]\n        mk_imgs = []  # Will contain [parent mask, child mask]\n        fg_mk = []  # Will contain [masked parent foreground, masked child foreground]\n\n\n\n        # Background stage\n        temp = self.background_stage( z_code1, b_code )\n        fake_img1 = self.background_image( temp )  # Background image     \n        fake_img1_126 = self.scale_fimg(fake_img1)\n        fake_imgs.append(fake_img1_126)\n\n        # Parent stage\n        p_temp = self.parent_stage( z_code2, p_code, which )\n        fake_img2_foreground = self.parent_image(p_temp)  # Parent foreground\n        fake_img2_mask = self.parent_mask(p_temp)  # Parent mask\n        fg_masked2 = fake_img2_foreground*fake_img2_mask # masked_parent        \n        fake_img2 = fg_masked2 + fake_img1*(1-fake_img2_mask)  # Parent image\n        fg_mk.append(fg_masked2) \n        fake_imgs.append(fake_img2)\n        fg_imgs.append(fake_img2_foreground)\n        mk_imgs.append(fake_img2_mask)\n\n        # Child stage\n        temp = self.child_stage(p_temp, c_code)\n        fake_img3_foreground = self.child_image(temp)  # Child foreground\n        fake_img3_mask = self.child_mask(temp)  # Child mask\n        fg_masked3 = torch.mul(fake_img3_foreground, fake_img3_mask) # masked child        \n        fake_img3 = fg_masked3 + fake_img2*(1-fake_img3_mask)  # Child image\n        fg_mk.append(fg_masked3)\n        fake_imgs.append(fake_img3)\n        fg_imgs.append(fake_img3_foreground)\n        mk_imgs.append(fake_img3_mask)\n        \n\n        return fake_imgs, fg_imgs, mk_imgs, fg_mk\n\n\n\n\n\n################################# Encoder ######################################\n\ndef encode_parent_and_child_img(ndf, in_c=3):\n    encode_img = nn.Sequential(\n        nn.Conv2d(in_c, ndf, 4, 2, 1, bias=False),\n        nn.LeakyReLU(0.2, inplace=True),\n        nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(ndf * 2),\n        nn.LeakyReLU(0.2, inplace=True),\n        nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(ndf * 4),\n        nn.LeakyReLU(0.2, inplace=True),\n        nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(ndf * 8),\n        nn.LeakyReLU(0.2, inplace=True)\n    )\n    return encode_img\n\ndef downBlock(in_planes, out_planes):\n    block = nn.Sequential(\n        nn.Conv2d(in_planes, out_planes, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(out_planes),\n        nn.LeakyReLU(0.2, inplace=True)\n    )\n    return block\n\ndef Block3x3_leakRelu(in_planes, out_planes):\n    block = nn.Sequential( conv3x3(in_planes, out_planes),\n                           nn.BatchNorm2d(out_planes),\n                           nn.LeakyReLU(0.2, inplace=True) )\n    return block\n\n\n\nclass Encoder(nn.Module):\n    def __init__(self, ):\n        super(Encoder, self).__init__()\n        self.ndf = 64\n\n        self.softmax = torch.nn.Softmax(dim=1)\n\n\n        self.model_z = nn.Sequential(encode_parent_and_child_img(self.ndf),\n                                   downBlock(self.ndf*8, self.ndf*16),\n                                   Block3x3_leakRelu(self.ndf*16, self.ndf*8),\n                                   Block3x3_leakRelu(self.ndf*8, self.ndf*8),\n                                   nn.Conv2d(self.ndf*8, cfg.GAN.Z_DIM,  kernel_size=4, stride=4),\n                                   nn.Tanh())\n\n\n        self.model_b = nn.Sequential(encode_parent_and_child_img(self.ndf),\n                                   downBlock(self.ndf*8, self.ndf*16),\n                                   Block3x3_leakRelu(self.ndf*16, self.ndf*8),\n                                   Block3x3_leakRelu(self.ndf*8, self.ndf*8),\n                                   nn.Conv2d(self.ndf*8, cfg.FINE_GRAINED_CATEGORIES,  kernel_size=4, stride=4))\n\n        self.model_p = nn.Sequential(encode_parent_and_child_img(self.ndf),\n                                   downBlock(self.ndf*8, self.ndf*16),\n                                   Block3x3_leakRelu(self.ndf*16, self.ndf*8),\n                                   Block3x3_leakRelu(self.ndf*8, self.ndf*8),\n                                   nn.Conv2d(self.ndf*8, cfg.SUPER_CATEGORIES,  kernel_size=4, stride=4))\n\n        self.model_c = nn.Sequential(encode_parent_and_child_img(self.ndf),\n                                   downBlock(self.ndf*8, self.ndf*16),\n                                   Block3x3_leakRelu(self.ndf*16, self.ndf*8),\n                                   Block3x3_leakRelu(self.ndf*8, self.ndf*8),\n                                   nn.Conv2d(self.ndf*8, cfg.FINE_GRAINED_CATEGORIES,  kernel_size=4, stride=4))\n\n\n    def forward(self, x_var, type_):  \n\n        code_z =   self.model_z(x_var).view(-1, cfg.GAN.Z_DIM)*4\n        code_b =   self.model_b(x_var).view(-1, cfg.FINE_GRAINED_CATEGORIES)\n        code_p =   self.model_p(x_var).view(-1, cfg.SUPER_CATEGORIES)\n        code_c =   self.model_c(x_var).view(-1, cfg.FINE_GRAINED_CATEGORIES)\n\n        if type_ == 'logits':\n            return code_z, code_b, code_p, code_c\n        if type_ == 'softmax':\n            return code_z, self.softmax(code_b),self.softmax(code_p),self.softmax(code_c)\n\n\n####################################### Feature ####################################\n\n\ndef Up_unet(in_c, out_c):\n    return nn.Sequential(  nn.ConvTranspose2d(in_c, out_c*2, 4,2,1), nn.BatchNorm2d(out_c*2), GLU()   )\n\ndef BottleNeck(in_c, out_c):\n    return nn.Sequential(  nn.Conv2d(in_c, out_c*2, 4,4), nn.BatchNorm2d(out_c*2), GLU()   )\n\ndef Down_unet(in_c, out_c):\n    return nn.Sequential(  nn.Conv2d(in_c, out_c*2, 4,2,1), nn.BatchNorm2d(out_c*2), GLU()   )\nclass FeatureExtractor(nn.Module):\n    def __init__(self,in_c, out_c):\n        super().__init__()\n\n        self.first = nn.Sequential( sameBlock(in_c, 32), sameBlock(32, 32) )\n        \n        self.down1 = Down_unet(32, 32)\n        # 32*64*64\n        self.down2 = Down_unet(32,64)\n        # 64*32*32\n        self.down3 = Down_unet(64,128)\n        # 128*16*16\n        self.down4 = Down_unet(128,256)\n        # 256*8*8\n        self.down5 = Down_unet(256,512)\n        # 512*4*4\n        self.down6 =  Down_unet(512,512)\n        # 512*2*2\n        \n        self.up1 = Up_unet(512,256)\n        # 256*4*4\n        self.up2 = Up_unet(256+512,512)\n        # 256*8*8\n        self.up3 = Up_unet(512+256,256)\n        # 256*16*16\n        self.up4 = Up_unet(256+128,128)\n        # 128*32*32\n        self.up5 = Up_unet(128+64,64)\n        # 64*64*64\n        self.up6 = Up_unet(64+32,out_c)\n        # out_c*128*128\n        \n        self.last = nn.Sequential( ResBlock(out_c),ResBlock(out_c) )\n        \n    def forward(self ,x):\n\n        x = self.first(x)\n        \n        x1 = self.down1(x)\n        x2 = self.down2(x1)\n        x3 = self.down3(x2)\n        x4 = self.down4(x3)\n        x5 = self.down5(x4)\n        \n        x =  self.up1( self.down6(x5) )\n\n        \n        x = self.up2(  torch.cat([x,x5], dim=1) )\n        x = self.up3(  torch.cat([x,x4], dim=1) )\n        x = self.up4(  torch.cat([x,x3], dim=1) )\n        x = self.up5(  torch.cat([x,x2], dim=1) )\n        x = self.up6(  torch.cat([x,x1], dim=1) )\n        \n        return self.last(x)\n        \n        \n        \n        \n        \n        \n"""
code/model_train.py,20,"b""import sys\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nfrom config import cfg\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom torch.nn import Upsample\nimport time\nfrom collections import deque\n\nclass GLU(nn.Module):\n    def __init__(self):\n        super(GLU, self).__init__()\n    def forward(self, x):\n        nc = x.size(1)\n        assert nc % 2 == 0, 'channels dont divide 2!'\n        nc = int(nc/2)\n        return x[:, :nc] * F.sigmoid(x[:, nc:])\n\n\ndef conv3x3(in_planes, out_planes):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=1, padding=1, bias=False)\n\n\n\ndef child_to_parent(child_c_code, classes_child, classes_parent):\n    ratio = classes_child / classes_parent\n    arg_parent = torch.argmax(child_c_code,  dim=1) / ratio\n    parent_c_code = torch.zeros([child_c_code.size(0), classes_parent]).cuda()\n    for i in range(child_c_code.size(0)):\n        parent_c_code[i][arg_parent[i]] = 1\n    return parent_c_code\n\n\n# ############## G networks ################################################\n# Upsale the spatial size by a factor of 2\ndef upBlock(in_planes, out_planes):\n    block = nn.Sequential(  nn.Upsample(scale_factor=2, mode='nearest'),\n                            conv3x3(in_planes, out_planes * 2),\n                            nn.BatchNorm2d(out_planes * 2),\n                            GLU()  )\n    return block\n\n\ndef sameBlock(in_planes, out_planes):\n    block = nn.Sequential(  conv3x3(in_planes, out_planes * 2),\n                            nn.BatchNorm2d(out_planes * 2),\n                            GLU()   )\n    return block\n\n\n\nclass ResBlock(nn.Module):\n    def __init__(self, channel_num):\n        super(ResBlock, self).__init__()\n        self.block = nn.Sequential(  conv3x3(channel_num, channel_num * 2),\n                                     nn.BatchNorm2d(channel_num * 2),\n                                     GLU(),\n                                     conv3x3(channel_num, channel_num),\n                                     nn.BatchNorm2d(channel_num)  )\n\n    def forward(self, x):\n        residual = x\n        out = self.block(x)\n        out += residual\n        return out\n\n\n\nclass GET_IMAGE(nn.Module):\n    def __init__(self, ngf):\n        super().__init__()\n        self.img = nn.Sequential( conv3x3(ngf, 3),  nn.Tanh() )\n    def forward(self, h_code):\n        return self.img(h_code)\n\n\nclass GET_MASK(nn.Module):\n    def __init__(self, ngf):\n        super().__init__()\n        self.img = nn.Sequential( conv3x3(ngf, 1), nn.Sigmoid() )\n    def forward(self, h_code):\n        return self.img(h_code)\n\n\n\nclass BACKGROUND_STAGE(nn.Module):\n    def __init__(self, ngf):\n        super().__init__()\n        \n        self.ngf = ngf        \n        in_dim = cfg.GAN.Z_DIM + cfg.FINE_GRAINED_CATEGORIES  \n\n        self.fc = nn.Sequential( nn.Linear(in_dim, ngf*4*4 * 2, bias=False), nn.BatchNorm1d(ngf*4*4 * 2), GLU())\n        # 1024*4*4\n        self.upsample1 = upBlock(ngf, ngf // 2 )   \n        # 512*8*8\n        self.upsample2 = upBlock(ngf // 2, ngf // 4) \n        # 256*16*16\n        self.upsample3 = upBlock(ngf // 4, ngf // 8) \n        # 128*32*32\n        self.upsample4 = upBlock(ngf // 8, ngf // 8) \n        # 128*64*64\n        self.upsample5 = upBlock(ngf // 8, ngf // 16)\n        # 64*128*128\n\n    def forward(self, z_input, input):\n        in_code = torch.cat([z_input, input], dim=1)\n        out_code = self.fc(in_code).view(-1, self.ngf, 4, 4) \n        out_code = self.upsample1(out_code) \n        out_code = self.upsample2(out_code) \n        out_code = self.upsample3(out_code)\n        out_code = self.upsample4(out_code)\n        out_code = self.upsample5(out_code)\n        return out_code\n\n\nclass PARENT_STAGE(nn.Module):\n    def __init__(self, ngf):\n        super().__init__()\n        \n        self.ngf = ngf\n        in_dim = cfg.GAN.Z_DIM + cfg.SUPER_CATEGORIES\n        self.code_len = cfg.SUPER_CATEGORIES\n\n        self.fc = nn.Sequential( nn.Linear(in_dim, ngf*4*4 * 2, bias=False), nn.BatchNorm1d(ngf*4*4 * 2), GLU())\n        # 512*4*4\n        self.upsample1 = upBlock(ngf, ngf//2 )\n        # 256*8*8\n        self.upsample2 = upBlock(ngf // 2, ngf // 4)\n        # 128*16*16\n        self.upsample3 = upBlock(ngf // 4, ngf // 8)\n        # 64*32*32\n        self.upsample4 = upBlock(ngf // 8, ngf // 32)\n        # 16*64*64\n        self.upsample5 = upBlock(ngf // 32, ngf // 32)\n        # 16*128*128\n        self.jointConv = sameBlock( cfg.SUPER_CATEGORIES+ngf//32, ngf//32 )\n        # (16+20)*128*128 --> 16*128*128\n        self.residual = self._make_layer(3, ngf//32)\n        # 16*128*128\n\n    def _make_layer(self,num_residual,ngf):\n        layers = []\n        for _ in range(num_residual):\n            layers.append( ResBlock(ngf) )\n        return nn.Sequential(*layers)\n\n    def forward(self, z_input, input, which):        \n        if which == 'code':\n            in_code = torch.cat([z_input, input], dim=1)\n            out_code = self.fc(in_code).view(-1, self.ngf, 4, 4) \n            out_code = self.upsample1(out_code) \n            out_code = self.upsample2(out_code)\n            out_code = self.upsample3(out_code)\n            out_code = self.upsample4(out_code)\n            out_code = self.upsample5(out_code)\n\n            h, w  = out_code.size(2),out_code.size(3)\n            input = input.view(-1, self.code_len, 1, 1).repeat(1, 1, h, w) \n            out_code = torch.cat( (out_code, input), dim=1)       \n            out_code = self.jointConv(out_code)\n            out_code = self.residual(out_code)\n            return out_code\n\n        elif which =='feature':\n            return input \n            \n        else:\n            raise ValueError('either code or feature')\n\n\nclass CHILD_STAGE(nn.Module):\n    def __init__(self, ngf, num_residual=2):\n        super().__init__()\n        \n        self.ngf = ngf \n        self.code_len = cfg.FINE_GRAINED_CATEGORIES\n        self.num_residual = num_residual\n\n        self.jointConv = sameBlock( self.code_len+self.ngf, ngf*2 )\n        self.residual = self._make_layer()\n        self.samesample = sameBlock(ngf*2, ngf)\n\n    def _make_layer(self):\n        layers = []\n        for _ in range(self.num_residual):\n            layers.append( ResBlock(self.ngf*2) )\n        return nn.Sequential(*layers)\n\n    def forward(self, h_code, code):\n        \n        h, w  = h_code.size(2),h_code.size(3)\n        code = code.view(-1, self.code_len, 1, 1).repeat(1, 1, h, w) \n        h_c_code = torch.cat((code, h_code), 1)\n        \n        out_code = self.jointConv(h_c_code)\n        out_code = self.residual(out_code)\n        out_code = self.samesample(out_code)\n        return out_code\n\n\nclass G_NET(nn.Module):\n    def __init__(self):\n        super(G_NET, self).__init__()\n        \n        ngf = cfg.GAN.GF_DIM \n        self.scale_fimg = nn.UpsamplingBilinear2d(size=[126, 126])\n\n        # Background stage\n        self.background_stage = BACKGROUND_STAGE( ngf*8 )\n        self.background_image = GET_IMAGE(ngf//2)\n\n        # Parent stage networks\n        self.parent_stage = PARENT_STAGE( ngf*8 )\n        self.parent_image = GET_IMAGE( ngf//4 )\n        self.parent_mask = GET_MASK( ngf//4 )\n\n        # Child stage networks\n        self.child_stage = CHILD_STAGE( ngf//4 )\n        self.child_image = GET_IMAGE( ngf//4 ) \n        self.child_mask = GET_MASK( ngf//4 )\n\n    def forward(self, z_code, c_code, p_code, b_code, which, only=False):\n\n        fake_imgs = []  # Will contain [background image, parent image, child image]\n        fg_imgs = []  # Will contain [parent foreground, child foreground]\n        mk_imgs = []  # Will contain [parent mask, child mask]\n        fg_mk = []  # Will contain [masked parent foreground, masked child foreground]\n\n        # Background stage\n        temp = self.background_stage( z_code, b_code )\n        fake_img1 = self.background_image( temp )  # Background image     \n        fake_img1_126 = self.scale_fimg(fake_img1)\n        fake_imgs.append(fake_img1_126)\n\n        # Parent stage\n        p_temp = self.parent_stage( z_code, p_code, which ) \n        fake_img2_foreground = self.parent_image(p_temp)  # Parent foreground\n        fake_img2_mask = self.parent_mask(p_temp)  # Parent mask\n        fg_masked2 = fake_img2_foreground*fake_img2_mask # masked_parent        \n        fake_img2 = fg_masked2 + fake_img1*(1-fake_img2_mask)  # Parent image\n        fg_mk.append(fg_masked2) \n        fake_imgs.append(fake_img2)\n        fg_imgs.append(fake_img2_foreground)\n        mk_imgs.append(fake_img2_mask)\n\n        # Child stage\n        temp = self.child_stage(p_temp, c_code)\n        fake_img3_foreground = self.child_image(temp)  # Child foreground\n        fake_img3_mask = self.child_mask(temp)  # Child mask\n        fg_masked3 = torch.mul(fake_img3_foreground, fake_img3_mask) # masked child        \n        fake_img3 = fg_masked3 + fake_img2*(1-fake_img3_mask)  # Child image\n        fg_mk.append(fg_masked3)\n        fake_imgs.append(fake_img3)\n        fg_imgs.append(fake_img3_foreground)\n        mk_imgs.append(fake_img3_mask)\n        \n        if only:\n            return p_temp, fake_imgs[2]\n\n        return fake_imgs, fg_imgs, mk_imgs, fg_mk\n\n\n# ############## D networks ################################################\n\nclass BACKGROUND_D(nn.Module):\n    def __init__(self, ndf=64):\n        super().__init__()\n        self.encode_img = nn.Sequential( nn.Conv2d(3, ndf, 4, 2, 0, bias=False),\n                                    nn.LeakyReLU(0.2, inplace=True),\n                                    nn.Conv2d(ndf, ndf*2, 4, 2, 0, bias=False),\n                                    nn.LeakyReLU(0.2, inplace=True),\n                                    nn.Conv2d(ndf*2, ndf*4, 4, 1, 0, bias=False),\n                                    nn.LeakyReLU(0.2, inplace=True) )\n        self.class_logits = nn.Sequential( nn.Conv2d(ndf*4, 1, kernel_size=4, stride=1), nn.Sigmoid() )\n        self.rf_logits = nn.Sequential( nn.Conv2d(ndf*4, 1, kernel_size=4, stride=1), nn.Sigmoid() )\n    def forward(self, x):\n        x = self.encode_img(x)\n        return self.class_logits(x), self.rf_logits(x)\n\n\nclass PARENT_D(nn.Module):\n    def __init__(self, ndf=32):\n        super().__init__()\n        self.code_len = cfg.SUPER_CATEGORIES\n        self.encode_mask =  nn.Sequential(  nn.Conv2d(3, ndf, 4, 2, 1, bias=False),\n                                            nn.LeakyReLU(0.2, inplace=True),\n                                            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n                                            nn.BatchNorm2d(ndf * 2),\n                                            nn.LeakyReLU(0.2, inplace=True),\n                                            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n                                            nn.BatchNorm2d(ndf * 4),\n                                            nn.LeakyReLU(0.2, inplace=True),\n                                            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n                                            nn.BatchNorm2d(ndf * 8),\n                                            nn.LeakyReLU(0.2, inplace=True),\n                                            nn.Conv2d(ndf * 8, ndf * 8, 4, 2, 1, bias=False),\n                                            nn.BatchNorm2d(ndf * 8),\n                                            nn.LeakyReLU(0.2, inplace=True),\n                                            nn.Conv2d(ndf * 8, ndf * 8, 3, 1, 1, bias=False),\n                                            nn.BatchNorm2d(ndf * 8),\n                                            nn.LeakyReLU(0.2, inplace=True) )\n        self.code_logits = nn.Sequential(   nn.Conv2d(ndf * 8, ndf * 8, 3, 1, 1, bias=False),\n                                            nn.BatchNorm2d(ndf * 8),\n                                            nn.LeakyReLU(0.2, inplace=True),\n                                            nn.Conv2d(ndf * 8, self.code_len, kernel_size=4, stride=4) ) \n        self.rf_logits = nn.Sequential( nn.Conv2d(ndf*8, 1, kernel_size=4, stride=4), nn.Sigmoid() )\n    def forward(self, x):\n        x = self.encode_mask(x)\n        return self.code_logits(x).view(-1, self.code_len), self.rf_logits(x)\n\n\nclass CHILD_D(nn.Module):\n    def __init__(self, ndf=64):\n        super().__init__()\n        self.code_len = cfg.FINE_GRAINED_CATEGORIES\n        self.encode_img = nn.Sequential(nn.Conv2d(3, ndf, 4, 2, 1, bias=False),\n                                        nn.LeakyReLU(0.2, inplace=True),\n                                        nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n                                        nn.BatchNorm2d(ndf * 2),\n                                        nn.LeakyReLU(0.2, inplace=True),\n                                        nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n                                        nn.BatchNorm2d(ndf * 4),\n                                        nn.LeakyReLU(0.2, inplace=True),\n                                        nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n                                        nn.BatchNorm2d(ndf * 8),\n                                        nn.LeakyReLU(0.2, inplace=True),\n                                        nn.Conv2d(ndf * 8, ndf * 8, 4, 2, 1, bias=False),\n                                        nn.BatchNorm2d(ndf * 8),\n                                        nn.LeakyReLU(0.2, inplace=True),\n                                        nn.Conv2d(ndf * 8, ndf * 8, 3, 1, 1, bias=False),\n                                        nn.BatchNorm2d(ndf * 8),\n                                        nn.LeakyReLU(0.2, inplace=True))\n        self.code_logits = nn.Sequential(   nn.Conv2d(ndf * 8, ndf * 8, 3, 1, 1, bias=False),\n                                            nn.BatchNorm2d(ndf * 8),\n                                            nn.LeakyReLU(0.2, inplace=True),\n                                            nn.Conv2d(ndf * 8, self.code_len, kernel_size=4, stride=4) ) \n        self.rf_logits = nn.Sequential( nn.Conv2d(ndf*8, 1, kernel_size=4, stride=4), nn.Sigmoid() )\n    def forward(self, x):\n        x = self.encode_img(x)\n        return self.code_logits(x).view(-1, self.code_len), self.rf_logits(x)\n\n\n################################# Encoder ######################################\n\ndef encode_parent_and_child_img(ndf, in_c=3):\n    encode_img = nn.Sequential(\n        nn.Conv2d(in_c, ndf, 4, 2, 1, bias=False),\n        nn.LeakyReLU(0.2, inplace=True),\n        nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(ndf * 2),\n        nn.LeakyReLU(0.2, inplace=True),\n        nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(ndf * 4),\n        nn.LeakyReLU(0.2, inplace=True),\n        nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(ndf * 8),\n        nn.LeakyReLU(0.2, inplace=True)\n    )\n    return encode_img\n\ndef downBlock(in_planes, out_planes):\n    block = nn.Sequential(\n        nn.Conv2d(in_planes, out_planes, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(out_planes),\n        nn.LeakyReLU(0.2, inplace=True)\n    )\n    return block\n\ndef Block3x3_leakRelu(in_planes, out_planes):\n    block = nn.Sequential( conv3x3(in_planes, out_planes),\n                           nn.BatchNorm2d(out_planes),\n                           nn.LeakyReLU(0.2, inplace=True) )\n    return block\n\n\n\nclass Encoder(nn.Module):\n    def __init__(self, ):\n        super(Encoder, self).__init__()\n        self.ndf = 64\n\n        self.softmax = torch.nn.Softmax(dim=1)\n\n\n        self.model_z = nn.Sequential(encode_parent_and_child_img(self.ndf),\n                                   downBlock(self.ndf*8, self.ndf*16),\n                                   Block3x3_leakRelu(self.ndf*16, self.ndf*8),\n                                   Block3x3_leakRelu(self.ndf*8, self.ndf*8),\n                                   nn.Conv2d(self.ndf*8, cfg.GAN.Z_DIM,  kernel_size=4, stride=4),\n                                   nn.Tanh())\n\n\n        self.model_b = nn.Sequential(encode_parent_and_child_img(self.ndf),\n                                   downBlock(self.ndf*8, self.ndf*16),\n                                   Block3x3_leakRelu(self.ndf*16, self.ndf*8),\n                                   Block3x3_leakRelu(self.ndf*8, self.ndf*8),\n                                   nn.Conv2d(self.ndf*8, cfg.FINE_GRAINED_CATEGORIES,  kernel_size=4, stride=4))\n\n        self.model_p = nn.Sequential(encode_parent_and_child_img(self.ndf),\n                                   downBlock(self.ndf*8, self.ndf*16),\n                                   Block3x3_leakRelu(self.ndf*16, self.ndf*8),\n                                   Block3x3_leakRelu(self.ndf*8, self.ndf*8),\n                                   nn.Conv2d(self.ndf*8, cfg.SUPER_CATEGORIES,  kernel_size=4, stride=4))\n\n        self.model_c = nn.Sequential(encode_parent_and_child_img(self.ndf),\n                                   downBlock(self.ndf*8, self.ndf*16),\n                                   Block3x3_leakRelu(self.ndf*16, self.ndf*8),\n                                   Block3x3_leakRelu(self.ndf*8, self.ndf*8),\n                                   nn.Conv2d(self.ndf*8, cfg.FINE_GRAINED_CATEGORIES,  kernel_size=4, stride=4))\n\n\n    def forward(self, x_var, type_):  \n\n        code_z =   self.model_z(x_var).view(-1, cfg.GAN.Z_DIM)*4\n        code_b =   self.model_b(x_var).view(-1, cfg.FINE_GRAINED_CATEGORIES)\n        code_p =   self.model_p(x_var).view(-1, cfg.SUPER_CATEGORIES)\n        code_c =   self.model_c(x_var).view(-1, cfg.FINE_GRAINED_CATEGORIES)\n\n        if type_ == 'logits':\n            return code_z, code_b, code_p, code_c\n        if type_ == 'softmax':\n            return code_z, self.softmax(code_b),self.softmax(code_p),self.softmax(code_c),\n\n\n################################## BI_DIS #######################################\n\nclass Gaussian(nn.Module):\n    def __init__(self, std):\n        super(Gaussian, self).__init__()\n        self.std = std\n    def forward(self, x):\n        n =  torch.randn_like(x)*self.std\n        return x+n\n\nclass ConvT_Block(nn.Module):\n    def __init__(self, in_c, out_c, k, s, p, bn=True, activation=None, noise=False, std=None ):\n        super(ConvT_Block, self).__init__()\n        model = [ nn.ConvTranspose2d( in_c, out_c, k ,s, p) ]\n\n        if bn:\n            model.append( nn.BatchNorm2d(out_c) )\n\n        if activation == 'relu':\n            model.append( nn.ReLU() )\n        elif activation == 'elu':\n            model.append( nn.ELU() )\n        elif activation == 'leaky':\n            model.append( nn.LeakyReLU(0.2) )\n        elif activation == 'tanh':\n            model.append( nn.Tanh() )\n        elif activation == 'sigmoid':\n            model.append( nn.Sigmoid() )  \n        elif activation == 'softmax':\n            model.append( nn.Softmax(dim=1) )  \n\n        if noise:\n            model.append(  Gaussian(std) )\n\n        self.model = nn.Sequential( *model )\n    def forward(self,x):\n        return self.model(x)\n\nclass Conv_Block(nn.Module):\n    def __init__(self, in_c, out_c, k, s, p=0, bn=True, activation=None, noise=False, std=None ):\n        super(Conv_Block, self).__init__()\n        model = [ nn.Conv2d( in_c, out_c, k ,s, p) ]\n\n        if bn:\n            model.append( nn.BatchNorm2d(out_c) )\n\n        if activation == 'relu':\n            model.append( nn.ReLU() )\n        if activation == 'elu':\n            model.append( nn.ELU() )\n        if activation == 'leaky':\n            model.append( nn.LeakyReLU(0.2) )\n        if activation == 'tanh':\n            model.append( nn.Tanh() )\n        if activation == 'sigmoid':\n            model.append( nn.Sigmoid() )  \n        if activation == 'softmax':\n            model.append( nn.Softmax(dim=1) )  \n\n        if noise:\n            model.append(  Gaussian(std) )\n\n        self.model = nn.Sequential( *model )\n    def forward(self,x):\n        return self.model(x)\n\nclass Linear_Block(nn.Module):\n    def __init__(self, in_c, out_c, bn=True, activation=None, noise=False, std=None  ):\n        super(Linear_Block, self).__init__()\n        model = [ nn.Linear(in_c, out_c) ]\n\n        if bn:\n            model.append( nn.BatchNorm1d(out_c) )\n\n        if activation == 'relu':\n            model.append( nn.ReLU() )\n        if activation == 'elu':\n            model.append( nn.ELU() )\n        if activation == 'leaky':\n            model.append( nn.LeakyReLU(0.2) )\n        if activation == 'tanh':\n            model.append( nn.Tanh() )\n        if activation == 'sigmoid':\n            model.append( nn.Sigmoid() )  \n        if activation == 'softmax':\n            model.append( nn.Softmax(dim=1) )  \n\n        if noise:\n            model.append(  Gaussian(std) )\n\n        self.model = nn.Sequential( *model )\n    def forward(self,x):\n        return self.model(x)\n\nclass Viewer(nn.Module):\n    def __init__(self, shape):\n        super(Viewer, self).__init__()\n        self.shape = shape\n    def forward(self,x):\n        return x.view(*self.shape)\n\n\n\n\n\nclass Bi_Dis_base(nn.Module):\n    def __init__(self, code_len, ngf=16):\n        super(Bi_Dis_base, self).__init__()\n   \n        self.image_layer = nn.Sequential(  Conv_Block( 3,     ngf, 4,2,1, bn=False, activation='leaky', noise=False, std=0.3),\n                                             Conv_Block( ngf,   ngf*2, 4,2,1, bn=False, activation='leaky', noise=False, std=0.5),\n                                             Conv_Block( ngf*2, ngf*4, 4,2,1, bn=False, activation='leaky', noise=False, std=0.5),\n                                             Conv_Block( ngf*4, ngf*8, 4,2,1, bn=False, activation='leaky', noise=False, std=0.5),\n                                             Conv_Block( ngf*8, ngf*16, 4,2,1, bn=False, activation='leaky', noise=False, std=0.5),\n                                             Conv_Block( ngf*16, 512, 4,1,0, bn=False, activation='leaky', noise=False, std=0.5),\n                                             Viewer( [-1,512] ) )\n        \n        self.code_layer = nn.Sequential( Linear_Block( code_len, 512, bn=False, activation='leaky', noise=True, std=0.5  ),\n                                           Linear_Block( 512, 512, bn=False, activation='leaky', noise=True, std=0.3  ),\n                                           Linear_Block( 512, 512, bn=False, activation='leaky', noise=True, std=0.3  ) )\n\n        self.joint = nn.Sequential(  Linear_Block(1024,1024, bn=False, activation='leaky', noise=False, std=0.5),\n                                     Linear_Block(1024, 1,  bn=False,  activation='None' ),\n                                     Viewer([-1]) )\n\n    def forward(self, img, code ):\n        t1 = self.image_layer(img)\n        t2 = self.code_layer( code )\n        return  self.joint(  torch.cat( [t1,t2],dim=1) )\n\n\n\n\n\nclass Bi_Dis(nn.Module):\n    def __init__(self):\n        super(Bi_Dis, self).__init__()\n\n        self.BD_z = Bi_Dis_base( cfg.GAN.Z_DIM )\n        self.BD_b = Bi_Dis_base( cfg.FINE_GRAINED_CATEGORIES )\n        self.BD_p = Bi_Dis_base( cfg.SUPER_CATEGORIES )\n        self.BD_c = Bi_Dis_base( cfg.FINE_GRAINED_CATEGORIES ) \n\n\n    def forward(self, img, z_code, b_code, p_code, c_code):\n\n        which_pair_z = self.BD_z( img, z_code )\n        which_pair_b = self.BD_b( img, b_code )\n        which_pair_p = self.BD_p( img, p_code )\n        which_pair_c = self.BD_c( img, c_code )\n\n        return which_pair_z, which_pair_b, which_pair_p, which_pair_c\n\n\n        \n        \n        \n\n\n####################################### Feature ####################################\n\n\ndef Up_unet(in_c, out_c):\n    return nn.Sequential(  nn.ConvTranspose2d(in_c, out_c*2, 4,2,1), nn.BatchNorm2d(out_c*2), GLU()   )\n\ndef BottleNeck(in_c, out_c):\n    return nn.Sequential(  nn.Conv2d(in_c, out_c*2, 4,4), nn.BatchNorm2d(out_c*2), GLU()   )\n\ndef Down_unet(in_c, out_c):\n    return nn.Sequential(  nn.Conv2d(in_c, out_c*2, 4,2,1), nn.BatchNorm2d(out_c*2), GLU()   )\nclass FeatureExtractor(nn.Module):\n    def __init__(self,in_c, out_c):\n        super().__init__()\n\n        self.first = nn.Sequential( sameBlock(in_c, 32), sameBlock(32, 32) )\n        \n        self.down1 = Down_unet(32, 32)\n        # 32*64*64\n        self.down2 = Down_unet(32,64)\n        # 64*32*32\n        self.down3 = Down_unet(64,128)\n        # 128*16*16\n        self.down4 = Down_unet(128,256)\n        # 256*8*8\n        self.down5 = Down_unet(256,512)\n        # 512*4*4\n        self.down6 =  Down_unet(512,512)\n        # 512*2*2\n        \n        self.up1 = Up_unet(512,256)\n        # 256*4*4\n        self.up2 = Up_unet(256+512,512)\n        # 256*8*8\n        self.up3 = Up_unet(512+256,256)\n        # 256*16*16\n        self.up4 = Up_unet(256+128,128)\n        # 128*32*32\n        self.up5 = Up_unet(128+64,64)\n        # 64*64*64\n        self.up6 = Up_unet(64+32,out_c)\n        # out_c*128*128\n        \n        self.last = nn.Sequential( ResBlock(out_c),ResBlock(out_c) )\n        \n    def forward(self ,x):\n\n        x = self.first(x)\n        \n        x1 = self.down1(x)\n        x2 = self.down2(x1)\n        x3 = self.down3(x2)\n        x4 = self.down4(x3)\n        x5 = self.down5(x4)\n        \n        x =  self.up1( self.down6(x5) )\n\n        \n        x = self.up2(  torch.cat([x,x5], dim=1) )\n        x = self.up3(  torch.cat([x,x4], dim=1) )\n        x = self.up4(  torch.cat([x,x3], dim=1) )\n        x = self.up5(  torch.cat([x,x2], dim=1) )\n        x = self.up6(  torch.cat([x,x1], dim=1) )\n        \n        return self.last(x)\n        \n        \n        \n                       \n                \n\n\nclass Dis_Dis(nn.Module):\n    def __init__(self, in_c ):\n        super().__init__()\n        self.encode_img = nn.Sequential( nn.Conv2d(in_c, 32, 4, 2, 0, bias=False),\n                                    nn.LeakyReLU(0.2, inplace=True),\n                                    nn.Conv2d(32, 64, 4, 2, 0, bias=False),\n                                    nn.LeakyReLU(0.2, inplace=True),\n                                    nn.Conv2d(64, 128, 4, 1, 0, bias=False),\n                                    nn.LeakyReLU(0.2, inplace=True) ) \n        self.rf_logits = nn.Sequential( nn.Conv2d(128, 1, kernel_size=4, stride=1), nn.Sigmoid() )\n    def forward(self, x):\n        x = F.interpolate( x, [126,126], mode='bilinear', align_corners=True )\n        x = self.encode_img(x)\n        return  self.rf_logits(x)  \n        \n"""
code/train_first_stage.py,46,"b'from config import cfg\nimport os\nimport time\nfrom PIL import Image\nimport torch.backends.cudnn as cudnn\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.utils as vutils\nfrom model_train import G_NET, BACKGROUND_D, PARENT_D, CHILD_D, Encoder, Bi_Dis\nfrom datasets import get_dataloader\nimport random\nfrom utils import *\nfrom itertools import chain\nfrom copy import deepcopy\ncudnn.benchmark = True\ndevice = torch.device(""cuda:"" + cfg.GPU_ID)\n\n\n\n\n################### Useful functions ###################\n\n\n\ndef define_optimizers( netG, netsD, BD, encoder ):\n\n    # define optimizer for D\n    optimizersD = []  \n    for i in range(3):\n        if i == 0 or i==2:\n            optimizersD.append( optim.Adam(netsD[i].parameters(), lr=2e-4, betas=(0.5, 0.999)) )   \n        else:\n            optimizersD.append(None)\n  \n    optimizerBD = optim.Adam( BD.parameters(), lr=2e-4, betas=(0.5, 0.999))\n  \n    params = chain( netG.parameters(), encoder.parameters(), netsD[1].parameters(), netsD[2].module.code_logits.parameters() )       \n    optimizerGE = optim.Adam(  params , lr=2e-4, betas=(0.5, 0.999) ) \n \n   \n    return optimizersD, optimizerBD, optimizerGE\n\n\n\ndef load_params(model, new_param):\n    for p, new_p in zip(model.parameters(), new_param):\n        p.data.copy_(new_p)\n\n\ndef copy_G_params(model):\n    flatten = deepcopy(list(p.data for p in model.parameters()))\n    return flatten\n\n\nclass CrossEntropy():\n    def __init__(self):\n        self.code_loss = nn.CrossEntropyLoss() \n    def __call__(self, prediction, label):\n        # check label if hard (onehot)\n        if label.max(dim=1)[0].min() == 1:\n            return self.code_loss(prediction, torch.nonzero( label.long() )[:, 1] )\n        else:        \n            log_prediction = torch.log_softmax(prediction, dim=1)    \n            return (- log_prediction*label).sum(dim=1).mean(dim=0)\n\n\n\ndef load_network():\n\n    gpus = [int(ix) for ix in cfg.GPU_ID.split(\',\')]\n \n    netG = G_NET()\n    netG.apply(weights_init)\n    netG = torch.nn.DataParallel(netG, device_ids=gpus)\n  \n    netsD = [ BACKGROUND_D(), PARENT_D(), CHILD_D() ]\n    for i in range(len(netsD)):\n        netsD[i].apply(weights_init)\n        netsD[i] = torch.nn.DataParallel(netsD[i], device_ids=gpus)       \n\n    BD = Bi_Dis()\n    BD = torch.nn.DataParallel(BD, device_ids=gpus)     \n\n    encoder = Encoder()\n    encoder.apply(weights_init)\n    encoder = torch.nn.DataParallel(encoder, device_ids=gpus)\n   \n    netG.to(device)  \n    encoder.to(device)  \n    BD.to(device)\n    for i in range(3):\n        netsD[i].to(device)\n\n    return netG, netsD, BD, encoder\n  \n\n\ndef save_model( encoder, myG, D0, D1, D2, BD, epoch, model_dir):\n    torch.save(encoder.state_dict(), \'%s/E_%d.pth\' % (model_dir, epoch))\n    torch.save(myG.state_dict(), \'%s/G_%d.pth\' % (model_dir, epoch))\n    torch.save(D0.state_dict(), \'%s/D0_%d.pth\' % (model_dir, epoch))\n    torch.save(D1.state_dict(), \'%s/D1_%d.pth\' % (model_dir, epoch))\n    torch.save(D2.state_dict(), \'%s/D2_%d.pth\' % (model_dir, epoch))\n    torch.save(BD.state_dict(), \'%s/BD_%d.pth\' % (model_dir, epoch))\n\n\n\ndef save_opt( optimizerGE,  optimizerD0,  optimizerD2, optimizerBD, epoch, opt_dir):\n    torch.save(optimizerGE.state_dict(), \'%s/GE_%d.pth\' % (opt_dir, epoch))\n    torch.save(optimizerD0.state_dict(), \'%s/D0_%d.pth\' % (opt_dir, epoch))\n    torch.save(optimizerD2.state_dict(), \'%s/D2_%d.pth\' % (opt_dir, epoch))\n    torch.save(optimizerBD.state_dict(), \'%s/BD_%d.pth\' % (opt_dir, epoch))\n\n    \n\n############################### Trainer ############################\n\n\nclass Trainer(object):\n    def __init__(self, output_dir):\n\n        # make dir for all kinds of output \n        self.model_dir = os.path.join(output_dir , \'Model\')\n        os.makedirs(self.model_dir)\n        self.image_dir = os.path.join(output_dir , \'Image\')\n        os.makedirs(self.image_dir)\n        self.opt_dir = os.path.join(output_dir , \'Opt\')\n        os.makedirs(self.opt_dir)\n\n        # make dataloader and code buffer \n        self.dataloader = get_dataloader()\n      \n        # other variables\n        self.batch_size = cfg.TRAIN.BATCH_SIZE \n        self.patch_stride = 4.0 \n        self.n_out = 24\n        self.recp_field = 34       \n\n        # get fixed images used for comparison for each epoch \n        self.fixed_image =  self.prepare_data(  next(iter(self.dataloader)) )[1]\n        save_img_results( self.fixed_image.cpu(), None, -1, self.image_dir )\n\n \n    def prepare_code(self):\n\n        free_z = torch.FloatTensor( self.batch_size, cfg.GAN.Z_DIM ).normal_(0, 1).to(device)\n\n        free_c = torch.zeros( self.batch_size, cfg.FINE_GRAINED_CATEGORIES ).to(device)\n        idxs = torch.LongTensor( self.batch_size ).random_(0, cfg.FINE_GRAINED_CATEGORIES)\n        for i, idx in enumerate(idxs):\n            free_c[i,idx] = 1\n        free_p = torch.zeros( self.batch_size, cfg.SUPER_CATEGORIES ).to(device)\n        idxs = torch.LongTensor( self.batch_size ).random_(0, cfg.SUPER_CATEGORIES)\n        for i, idx in enumerate(idxs):\n            free_p[i,idx] = 1\n        free_b = torch.zeros( self.batch_size, cfg.FINE_GRAINED_CATEGORIES ).to(device)\n        idxs = torch.LongTensor( self.batch_size ).random_( 0, cfg.FINE_GRAINED_CATEGORIES )\n        for i, idx in enumerate(idxs):\n            free_b[i,idx] = 1\n\n        return free_z, free_b, free_p, free_c\n\n    \n\n    def prepare_data(self, data):\n\n        real_img126, real_img, real_c, _, warped_bbox = data \n        real_img126 = real_img126.to(device)\n        real_img = real_img.to(device)\n        for i in range(len(warped_bbox)):\n            warped_bbox[i] = warped_bbox[i].float().to(device)\n\n        real_p = child_to_parent(real_c, cfg.FINE_GRAINED_CATEGORIES, cfg.SUPER_CATEGORIES ).to(device)\n        real_z = torch.FloatTensor( self.batch_size, cfg.GAN.Z_DIM ).normal_(0, 1).to(device) \n        real_c = real_c.to(device)\n        real_b = real_c             \n\n        return  real_img126, real_img, real_z, real_b, real_p, real_c, warped_bbox\n\n\n    def train_Dnet(self, idx):\n\n        assert(idx == 0 or idx ==2)\n  \n        # choose net and opt  \n        netD, optD = self.netsD[idx], self.optimizersD[idx]\n        netD.zero_grad()\n\n        # choose real and fake images\n        if idx == 0:\n            real_img = self.real_img126\n            fake_img = self.fake_imgs[0]\n        elif idx == 2:\n            real_img = self.real_img\n            fake_img = self.fake_imgs[2]   \n\n        # # # # # # # #for background stage now  # # # # # # #\n        if idx == 0:\n\n            # go throung D net to get prediction\n            class_prediction, real_prediction = netD(real_img) \n            _, fake_prediction = netD( fake_img.detach() )   \n\n            real_label = torch.ones_like(real_prediction)\n            fake_label = torch.zeros_like(fake_prediction)     \n            weights_real = torch.ones_like(real_prediction)\n            \n            for i in range( self.batch_size ):\n\n                x1 = self.warped_bbox[0][i]\n                x2 = self.warped_bbox[2][i]\n                y1 = self.warped_bbox[1][i]\n                y2 = self.warped_bbox[3][i]\n\n                a1 = max(torch.tensor(0).float().to(device), torch.ceil((x1 - self.recp_field)/self.patch_stride))\n                a2 = min(torch.tensor(self.n_out - 1).float().to(device), torch.floor((self.n_out - 1) - ((126 - self.recp_field) - x2)/self.patch_stride)) + 1\n                b1 = max(torch.tensor(0).float().to(device), torch.ceil( (y1 - self.recp_field)/self.patch_stride))\n                b2 = min(torch.tensor(self.n_out - 1).float().to(device), torch.floor((self.n_out - 1) - ((126 - self.recp_field) - y2)/self.patch_stride)) + 1\n\n                if (x1 != x2 and y1 != y2):\n                    weights_real[i, :, a1.type(torch.int): a2.type(torch.int), b1.type(torch.int): b2.type(torch.int)] = 0.0\n\n            norm_fact_real = weights_real.sum()\n            norm_fact_fake = weights_real.shape[0]*weights_real.shape[1]*weights_real.shape[2]*weights_real.shape[3]\n\n            # Real/Fake loss for \'real background\' (on patch level)\n            real_prediction_loss = self.RF_loss_un( real_prediction, real_label )\n            # Masking output units which correspond to receptive fields which lie within the bounding box\n            real_prediction_loss = torch.mul(real_prediction_loss, weights_real).mean()\n            # Normalizing the real/fake loss for background after accounting the number of masked members in the output.\n            if (norm_fact_real > 0):\n                real_prediction_loss = real_prediction_loss * ((norm_fact_fake * 1.0) / (norm_fact_real * 1.0))\n\n            # Real/Fake loss for \'fake background\' (on patch level)\n            fake_prediction_loss = self.RF_loss_un(fake_prediction, fake_label).mean()        \n          \n            # Background/foreground classification loss\n            class_prediction_loss = self.RF_loss_un( class_prediction, weights_real ).mean()  \n\n            # add three losses together \n            D_loss = cfg.TRAIN.BG_LOSS_WT*(real_prediction_loss + fake_prediction_loss) + class_prediction_loss\n      \n\n        # # # # # # # #for child stage now (only real/fake discriminator)  # # # # # # # \n        if idx == 2:\n\n            # go through D net to get data\n            _, real_prediction = netD(real_img) \n            _, fake_prediction = netD( fake_img.detach() )\n\n            # get real/fake lables\n            real_label = torch.ones_like(real_prediction)\n            fake_label = torch.zeros_like(fake_prediction) \n \n            # get loss \n            real_prediction_loss = self.RF_loss(real_prediction, real_label)         \n            fake_prediction_loss = self.RF_loss(fake_prediction, fake_label)\n            D_loss = real_prediction_loss+fake_prediction_loss\n\n        D_loss.backward()\n        optD.step()\n\n\n\n    def train_BD(self):\n\n        self.optimizerBD.zero_grad()\n\n        # make prediction on pairs \n        pred_enc_z, pred_enc_b, pred_enc_p, pred_enc_c = self.BD(  self.real_img, self.fake_z.detach(), self.fake_b.detach(), self.fake_p.detach(), self.fake_c.detach() )\n        pred_gen_z, pred_gen_b, pred_gen_p, pred_gen_c = self.BD(  self.fake_imgs[2].detach(), self.real_z, self.real_b, self.real_p, self.real_c )\n       \n        real_data = [ self.real_img, self.fake_z.detach(), self.fake_b.detach(), self.fake_p.detach(), self.fake_c.detach() ]\n        fake_data = [ self.fake_imgs[2].detach(), self.real_z, self.real_b, self.real_p, self.real_c ]\n        penalty = cal_gradient_penalty( self.BD, real_data, fake_data, device, type=\'mixed\', constant=1.0)\n\n        D_loss =  -( pred_enc_z.mean()+pred_enc_b.mean()+pred_enc_p.mean()+pred_enc_c.mean()  ) + ( pred_gen_z.mean()+pred_gen_b.mean()+pred_gen_p.mean()+pred_gen_c.mean() ) + penalty*10\n        D_loss.backward()\n        self.optimizerBD.step()\n      \n\n\n    def train_EG(self):\n\n        self.optimizerGE.zero_grad()\n\n        # reconstruct code and calculate loss \n        self.rec_p, _ = self.netsD[1]( self.fg_mk[0])\n        self.rec_c, _ = self.netsD[2]( self.fg_mk[1])\n        p_code_loss = self.CE( self.rec_p , self.real_p )\n        c_code_loss = self.CE( self.rec_c,  self.real_c )\n\n        # pred code and calculate loss (here no code constrain)\n        free_z, free_b, free_p, free_c = self.prepare_code()\n        with torch.no_grad():\n            free_fake_imgs, _, _, _ = self.netG( free_z, free_c, free_p, free_b, \'code\'  )                   \n        pred_z, pred_b, pred_p, pred_c = self.encoder( free_fake_imgs[2].detach(),   \'logits\' )\n        z_pred_loss = self.L1( pred_z , free_z )\n        b_pred_loss = self.CE( pred_b , free_b )\n        p_pred_loss = self.CE( pred_p , free_p )\n        c_pred_loss = self.CE( pred_c,  free_c )        \n    \n    \n        # aux and backgroud real/fake loss\n        self.bg_class_pred, self.bg_rf_pred = self.netsD[0]( self.fake_imgs[0] ) \n        bg_rf_loss = self.RF_loss( self.bg_rf_pred, torch.ones_like( self.bg_rf_pred ) )*cfg.TRAIN.BG_LOSS_WT\n        bg_class_loss = self.RF_loss( self.bg_class_pred, torch.ones_like( self.bg_class_pred ) )\n\n        # child image real/fake loss  \n        _, self.child_rf_pred = self.netsD[2]( self.fake_imgs[-1] )\n        child_rf_loss = self.RF_loss( self.child_rf_pred, torch.ones_like(self.child_rf_pred) )\n  \n        # fool BD loss\n        pred_enc_z, pred_enc_b, pred_enc_p, pred_enc_c = self.BD(  self.real_img, self.fake_z, self.fake_b, self.fake_p, self.fake_c )\n        pred_gen_z, pred_gen_b, pred_gen_p, pred_gen_c = self.BD(  self.fake_imgs[2], self.real_z, self.real_b, self.real_p, self.real_c )\n        fool_BD_loss = ( pred_enc_z.mean()+pred_enc_b.mean()+pred_enc_p.mean()+pred_enc_c.mean()  ) - ( pred_gen_z.mean()+pred_gen_b.mean()+pred_gen_p.mean()+pred_gen_c.mean() ) \n             \n        EG_loss =  (p_code_loss+c_code_loss) + (bg_rf_loss+bg_class_loss) + child_rf_loss + fool_BD_loss + (5*z_pred_loss+5*b_pred_loss+10*p_pred_loss+10*c_pred_loss)\n        EG_loss.backward()\n\n        self.optimizerGE.step()\n\n\n\n\n    def train(self):\n\n        # prepare net, optimizer and loss\n        self.netG, self.netsD, self.BD, self.encoder = load_network()   \n        self.optimizersD, self.optimizerBD, self.optimizerGE = define_optimizers( self.netG, self.netsD, self.BD, self.encoder )\n        self.RF_loss_un = nn.BCELoss(reduction=\'none\')\n        self.RF_loss = nn.BCELoss()   \n        self.CE = CrossEntropy()        \n        self.L1 = nn.L1Loss()\n    \n\n\n        # get init avg_G (the param in avg_G is what we want)\n        avg_param_G = copy_G_params(self.netG) \n\n        for epoch in range(cfg.TRAIN.FIRST_MAX_EPOCH):              \n\n            for data in self.dataloader:  \n                     \n                # prepare data              \n                self.real_img126, self.real_img, self.real_z, self.real_b, self.real_p, self.real_c, self.warped_bbox = self.prepare_data(data)\n\n                # forward for both E and G\n                self.fake_z, self.fake_b, self.fake_p, self.fake_c = self.encoder( self.real_img, \'softmax\' )              \n                self.fake_imgs, self.fg_imgs, self.mk_imgs, self.fg_mk = self.netG( self.real_z, self.real_c, self.real_p, self.real_b, \'code\'  )\n                                \n                # Update Discriminator networks in FineGAN      \n                self.train_Dnet(0)\n                self.train_Dnet(2)\n\n                # Update Bi Discriminator\n                self.train_BD()\n\n                # Update Encoder and G network\n                self.train_EG()\n                for avg_p, p in zip( avg_param_G, self.netG.parameters() ):\n                    avg_p.mul_(0.999).add_(0.001, p.data)\n\n        \n            # Save model&image for each epoch  \n            backup_para = copy_G_params(self.netG)   \n            load_params(self.netG, avg_param_G)\n\n            self.encoder.eval()   \n            self.netG.eval()    \n            with torch.no_grad():   \n                self.code_z, self.code_b, self.code_p, self.code_c = self.encoder( self.fixed_image,\'softmax\')   \n                self.fake_imgs, self.fg_imgs, self.mk_imgs, self.fg_mk = self.netG(self.code_z, self.code_c, self.code_p, self.code_b, \'code\')  \n                save_img_results(None, (self.fake_imgs+self.fg_imgs+self.mk_imgs+self.fg_mk), epoch, self.image_dir)            \n            self.encoder.train() \n            self.netG.train()            \n        \n        \n\n            backup_para = copy_G_params(self.netG)   \n            load_params(self.netG, avg_param_G)\n            save_model( self.encoder, self.netG, self.netsD[0], self.netsD[1], self.netsD[2], self.BD, 0, self.model_dir )   \n            save_opt(  self.optimizerGE,  self.optimizersD[0], self.optimizersD[2], self.optimizerBD,  0, self.opt_dir )   \n            load_params(self.netG, backup_para)        \n\n            print( str(epoch)+\'th epoch finished\' )\n\n\n\n\n\n\n\n\n\n\nif __name__ == ""__main__"":\n\n    \n    \n    manualSeed = random.randint(1, 10000)\n    random.seed(manualSeed)\n    torch.manual_seed(manualSeed)\n    torch.cuda.manual_seed_all(manualSeed)\n\n\n\n    # prepare output folder for this running and save all files \n    output_dir = make_output_dir()\n    shutil.copy2( sys.argv[0], output_dir)\n    shutil.copy2( \'model_train.py\', output_dir)\n    shutil.copy2( \'config.py\', output_dir)\n    shutil.copy2( \'utils.py\', output_dir)\n    shutil.copy2( \'datasets.py\', output_dir)\n\n    \n    trainer = Trainer(output_dir)   \n    print(\'start training now\')\n    trainer.train()\n      \n        \n'"
code/train_second_stage.py,26,"b'from config import cfg\nimport os\nimport torch.backends.cudnn as cudnn\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom model_train import G_NET, Encoder, Dis_Dis, FeatureExtractor \nfrom datasets import get_dataloader\nimport random\nimport torch.nn.functional as F\nfrom utils import *\ncudnn.benchmark = True\ndevice = torch.device(""cuda:"" + cfg.GPU_ID)\n\n\n\n\n\n\n# ################## Shared functions ###################\n\n\n\n\ndef define_optimizers(  extractor, dis_dis ):\n    optimizerEX = optim.Adam( extractor.parameters(), lr=2e-4, betas=(0.5, 0.999) )\n    optimizerDD = optim.Adam( dis_dis.parameters(), lr=2e-5, betas=(0.5, 0.999) )   \n    return  optimizerEX, optimizerDD\n\n\n\ndef load_network():\n\n    gpus = [int(ix) for ix in cfg.GPU_ID.split(\',\')]\n\n \n    netG = G_NET()\n    netG = torch.nn.DataParallel(netG, device_ids=gpus)\n    netG.load_state_dict( torch.load( G_DIR )  )  \n\n    encoder = Encoder()\n    encoder = torch.nn.DataParallel(encoder, device_ids=gpus)\n    encoder.load_state_dict( torch.load( E_DIR ) )\n\n    extractor = FeatureExtractor(3,16) \n    extractor.apply(weights_init)\n    extractor = torch.nn.DataParallel(extractor , device_ids=gpus)\n\n    dis_dis = Dis_Dis(16)\n    dis_dis.apply(weights_init)\n    dis_dis = torch.nn.DataParallel(dis_dis , device_ids=gpus)\n\n    netG.to(device)  \n    encoder.to(device)  \n    extractor.to(device)\n    dis_dis.to(device)\n\n    return netG,  encoder, extractor, dis_dis\n  \n\n\ndef save_model(  dis_dis, extractor, epoch, model_dir):\n    torch.save( extractor.state_dict(), \'%s/EX_%d.pth\' % (model_dir, epoch))\n\n\n\nclass Trainer(object):\n    def __init__(self, output_dir):\n\n        # make dir for all kinds of output \n        self.model_dir = os.path.join(output_dir , \'Model\')\n        os.makedirs(self.model_dir)\n        self.image_dir = os.path.join(output_dir , \'Image\')\n        os.makedirs(self.image_dir)\n\n        # make dataloader \n        self.dataloader = get_dataloader()\n \n        # other variables\n        self.batch_size = cfg.TRAIN.BATCH_SIZE \n\n        # get fixed images used for comparison for each epoch \n        self.fixed_image = self.prepare_data(  next(iter(self.dataloader)) )[0]\n        save_img_results( self.fixed_image.cpu(), None, -1, self.image_dir )\n    \n\n\n    \n\n    def prepare_data(self, data):\n\n        real_img = data[1]       \n        real_img = real_img.to(device)\n\n        real_z = torch.FloatTensor( self.batch_size, cfg.GAN.Z_DIM ).normal_(0, 1).to(device)\n\n        if random.uniform(0, 1)<0.2: \n            real_p = torch.softmax( torch.FloatTensor( self.batch_size, cfg.SUPER_CATEGORIES ).normal_(0, 1), dim =1).to(device)\n        else:\n            real_p = torch.zeros( self.batch_size, cfg.SUPER_CATEGORIES ).to(device)\n            idxs = torch.LongTensor( self.batch_size ).random_(0, cfg.SUPER_CATEGORIES)\n            for i, idx in enumerate(idxs):\n                real_p[i,idx] = 1\n        real_c = torch.zeros( self.batch_size, cfg.FINE_GRAINED_CATEGORIES ).to(device)\n        idxs = torch.LongTensor( self.batch_size ).random_(0, cfg.FINE_GRAINED_CATEGORIES)\n        for i, idx in enumerate(idxs):\n            real_c[i,idx] = 1\n        real_b = torch.zeros( self.batch_size, cfg.FINE_GRAINED_CATEGORIES ).to(device)\n        idxs = torch.LongTensor( self.batch_size ).random_(0, cfg.FINE_GRAINED_CATEGORIES)\n        for i, idx in enumerate(idxs):\n            real_b[i,idx] = 1\n\n        return  real_img, real_z, real_b, real_p, real_c \n\n\n    def train(self):\n\n        # prepare net, optimizer and loss\n        self.netG, self.encoder, self.extractor, self.dis_dis = load_network()   \n        self.netG.eval()\n        self.encoder.eval()\n\n        self.optimizerEX, self.optimizerDD = define_optimizers(  self.extractor, self.dis_dis )    \n        self.RF_loss = nn.BCELoss() \n        self.L1 = nn.L1Loss()\n       \n\n        for epoch in range(cfg.TRAIN.SECOND_MAX_EPOCH):\n          \n\n            for data in self.dataloader:          \n                \n                # prepare data              \n                real_img, real_z, real_b, real_p, real_c   = self.prepare_data(data)\n\n\n                # forward to get real distribution \n                with torch.no_grad():\n                    real_distribution, real_fake_image = self.netG( real_z, real_c, real_p, real_b, \'code\',  only=True )\n\n\n                # forward feature extractor\n                fake_distribution = self.extractor(real_img)                \n\n                # update DD\n                self.optimizerDD.zero_grad()\n                fake_pred = self.dis_dis( fake_distribution.detach() )\n                real_pred = self.dis_dis( real_distribution )\n                DD_loss = self.RF_loss( fake_pred,torch.zeros_like(fake_pred) ) + self.RF_loss( real_pred,torch.ones_like(real_pred) )\n                DD_loss.backward()\n                self.optimizerDD.step()\n\n                # update extractor\n                self.optimizerEX.zero_grad()             \n                fake_pred = self.dis_dis( fake_distribution )\n                l1loss = self.L1( self.extractor(real_fake_image), real_distribution)\n                EX_loss = self.RF_loss( fake_pred,torch.ones_like(fake_pred) )\n                (EX_loss+l1loss).backward()\n                self.optimizerEX.step()\n\n           \n            # Save model&image for each epoch \n            self.extractor.eval()\n            with torch.no_grad():   \n                code_z, code_b, _, code_c = self.encoder( self.fixed_image,\'softmax\')   \n                feat_p = self.extractor(self.fixed_image)\n                fake_imgs, fg_imgs, mk_imgs, fg_mk = self.netG( code_z, code_c, feat_p, code_b, \'feature\')  \n                save_img_results(None, ( fake_imgs+fg_imgs+mk_imgs+fg_mk), epoch, self.image_dir )\n            self.extractor.train()      \n            save_model(  self.dis_dis  ,self.extractor, 0, self.model_dir )   \n            print( str(epoch)+\'th epoch finished\')\n\n\n\n\n\n\n\n\n\nif __name__ == ""__main__"":\n\n    \n    manualSeed = random.randint(1, 10000)\n    random.seed(manualSeed)\n    torch.manual_seed(manualSeed)\n    torch.cuda.manual_seed_all(manualSeed)\n\n\n\n    # prepare output folder for this running and save all files \n    output_dir = make_output_dir()\n    shutil.copy2( sys.argv[0], output_dir)\n    shutil.copy2( \'model_train.py\', output_dir)\n    shutil.copy2( \'config.py\', output_dir)\n    shutil.copy2( \'utils.py\', output_dir)\n\n    G_DIR = sys.argv[2]\n    E_DIR = sys.argv[3]\n\n    \n    trainer = Trainer(output_dir)   \n    print(\'start training now\')\n    trainer.train()\n      \n        \n'"
code/utils.py,8,"b'from config import cfg\nimport numpy as np\nimport os\nimport time\nimport torch.backends.cudnn as cudnn\nimport torch\nimport torchvision.utils as vutils\nimport sys\nimport shutil\nimport torch.nn as nn\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    \n    if classname.find(\'Conv\') != -1:\n        nn.init.orthogonal_(m.weight.data, 1.0)\n    elif classname.find(\'BatchNorm\') != -1:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)\n    elif classname.find(\'Linear\') != -1:\n        nn.init.orthogonal_(m.weight.data, 1.0)\n        if m.bias is not None:\n            m.bias.data.fill_(0.0)\n\n\ndef make_output_dir():\n    root_path = \'../output\'\n    args = sys.argv[1:]\n    if len(args)==0:\n        raise RuntimeError(\'output folder must be specified\')\n    new_output = args[0]\n    path = os.path.join(root_path, new_output)\n    if os.path.exists(path):\n        if len(args)==2 and args[1]==\'-f\': \n            print(\'WARNING: experiment directory exists, it has been erased and recreated\') \n            shutil.rmtree(path)\n            os.makedirs(path)\n        else:\n            print(\'WARNING: experiment directory exists, it will be erased and recreated in 3s\')\n            time.sleep(3)\n            shutil.rmtree(path)\n            os.makedirs(path)\n    else:\n        os.makedirs(path)\n    return path\n\n\ndef child_to_parent(child_c_code, classes_child, classes_parent):\n\n    ratio = classes_child / classes_parent\n    arg_parent = torch.argmax(child_c_code,  dim=1) / ratio\n    parent_c_code = torch.zeros([child_c_code.size(0), classes_parent]).cuda()\n    for i in range(child_c_code.size(0)):\n        parent_c_code[i][ int(arg_parent[i]) ] = 1\n    return parent_c_code\n\n\n\ndef save_img_results(imgs_tcpu, fake_imgs, count, image_dir, nrow=8):\n\n    num = cfg.TRAIN.VIS_COUNT*8888\n\n    if imgs_tcpu is not None:\n        real_img = imgs_tcpu[:][0:num]\n        vutils.save_image(\n            real_img, \'%s/real_samples%09d.png\' % (image_dir, count),\n            scale_each=True, normalize=True, nrow=nrow)\n        real_img_set = vutils.make_grid(real_img).numpy()\n        real_img_set = np.transpose(real_img_set, (1, 2, 0))\n        real_img_set = real_img_set * 255\n        real_img_set = real_img_set.astype(np.uint8)\n\n    if fake_imgs is not None:\n        \n        for i in range(len(fake_imgs)):\n            fake_img = fake_imgs[i][0:num]\n\n            vutils.save_image(\n                fake_img.data, \'%s/count_%09d_fake_samples%d.png\' %\n                (image_dir, count, i), scale_each=True, normalize=True, nrow=nrow)\n\n            fake_img_set = vutils.make_grid(fake_img.data).cpu().numpy()\n\n            fake_img_set = np.transpose(fake_img_set, (1, 2, 0))\n            fake_img_set = (fake_img_set + 1) * 255 / 2\n            fake_img_set = fake_img_set.astype(np.uint8)\n    \ndef zero_last_layer(encoder):\n    encoder.model_z[4].weight.data.fill_(0.0)\n    encoder.model_z[4].bias.data.fill_(0.0)\n    return encoder\n\n\n\ndef cal_gradient_penalty(netD, real_data, fake_data, device, type=\'mixed\', constant=1.0):\n    # adapted from cyclegan \n    """"""Calculate the gradient penalty loss, used in WGAN-GP paper https://arxiv.org/abs/1704.00028\n\n    Arguments:\n        netD (network)              -- discriminator network\n        real_data (tensor array)    -- real images\n        fake_data (tensor array)    -- generated images from the generator\n        device (str)                -- GPU / CPU: from torch.device(\'cuda:{}\'.format(self.gpu_ids[0])) if self.gpu_ids else torch.device(\'cpu\')\n        type (str)                  -- if we mix real and fake data or not [real | fake | mixed].\n        constant (float)            -- the constant used in formula ( | |gradient||_2 - constant)^2\n     \n\n    Returns the gradient penalty loss\n    """"""\n\n    if type == \'real\':   # either use real images, fake images, or a linear interpolation of two.\n        interpolatesv = real_data\n    elif type == \'fake\':\n        interpolatesv = fake_data\n    elif type == \'mixed\':\n        interpolatesv = []\n        for i in range( len(real_data) ):\n            alpha = torch.rand(real_data[i].shape[0], 1)\n            alpha = alpha.expand(real_data[i].shape[0], real_data[i].nelement() // real_data[i].shape[0]).contiguous().view(*real_data[i].shape)\n            alpha = alpha.to(device)\n            interpolatesv.append(  alpha*real_data[i] + ((1-alpha)*fake_data[i])  )\n    else:\n        raise NotImplementedError(\'{} not implemented\'.format(type))\n    \n    # require grad\n    for i in range( len(interpolatesv) ):\n        interpolatesv[i].requires_grad_(True)\n    \n    # feed into D\n    disc_interpolates = netD(*interpolatesv)\n\n    # cal penalty\n\n    gradient_penalty = 0\n    for i in range( len(disc_interpolates) ):\n        for j in range( len(interpolatesv) ):\n            gradients = torch.autograd.grad(outputs=disc_interpolates[i], inputs=interpolatesv[j],\n                                            grad_outputs=torch.ones(disc_interpolates[i].size()).to(device),\n                                            create_graph=True, retain_graph=True, only_inputs=True, allow_unused=True)\n            if gradients[0] is not None:  # it will return None if input is not used in this output (allow unused)\n                gradients = gradients[0].view(real_data[j].size(0), -1)  # flat the data\n                gradient_penalty += (((gradients + 1e-16).norm(2, dim=1) - constant) ** 2).mean()        # added eps\n        \n    return gradient_penalty\n\n\n\n\n\n\n\n\n'"
