file_path,api_count,code
coco_utils.py,0,"b'\'\'\'\nFile: coco_utils.py\nProject: MobilePose-PyTorch\nFile Created: Thursday, 20th December 2018 6:11:07 pm\nAuthor: Yuliang Xiu (yuliangxiu@sjtu.edu.cn)\n-----\nLast Modified: Monday, 11th March 2019 12:51:27 am\nModified By: Yuliang Xiu (yuliangxiu@sjtu.edu.cn>)\n-----\nCopyright 2018 - 2019 Shanghai Jiao Tong University, Machine Vision and Intelligence Group\n\'\'\'\n\n\n\n# define coco class\nimport json\nimport numpy as np\nfrom collections import namedtuple, Mapping\n\n# Create namedtuple without defaults\ndef namedtuple_with_defaults(typename, field_names, default_values=()):\n    T = namedtuple(typename, field_names)\n    T.__new__.__defaults__ = (None,) * len(T._fields)\n    if isinstance(default_values, Mapping):\n        prototype = T(**default_values)\n    else:\n        prototype = T(*default_values)\n    T.__new__.__defaults__ = tuple(prototype)\n    return T\n\n# Used for solving TypeError: Object of type \'float32\' is not JSON serializable\nclass MyEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.integer):\n            return int(obj)\n        elif isinstance(obj, np.floating):\n            return float(obj)\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        else:\n            return super(MyEncoder, self).default(obj)\n\n# Classes for coco groud truth, CocoImage and CocoAnnotation\nCocoImage = namedtuple_with_defaults(\'image\', [\'file_name\', \'height\', \'width\', \'id\'])\nCocoAnnotation = namedtuple_with_defaults(\'annotation\', [\'num_keypoints\', \'area\', \n                                             \'iscrowd\', \'keypoints\', \n                                             \'image_id\', \'bbox\', \'category_id\',\n                                            \'id\'])\nclass CocoData:\n    def __init__(self, coco_images_arr, coco_annotations_arr):\n        self.Coco = {}\n        coco_images_arr = [item._asdict() for item in coco_images_arr]\n        coco_annotations_arr = [item._asdict() for item in coco_annotations_arr]\n        self.Coco[\'images\'] = coco_images_arr\n        self.Coco[\'annotations\'] = coco_annotations_arr\n        self.Coco[\'categories\'] = [{""id"": 1, ""name"": ""test""}]\n        \n    def dumps(self):\n        return json.dumps(self.Coco, cls=MyEncoder) \n\n# Change keypoints [x, y, prob] prob = int(prob)\ndef float2int(str_data):\n    json_data = json.loads(str_data)\n    annotations = []\n    if \'annotations\' in json_data:\n        annotations = json_data[\'annotations\']\n    else:\n        annotations = json_data\n    json_size = len(annotations)\n    for i in range(json_size):\n        annotation = annotations[i]\n        keypoints = annotation[\'keypoints\']\n        keypoints_num = int(len(keypoints) / 3)\n        for j in range(keypoints_num):\n            keypoints[j * 3 + 2] = int(round(keypoints[j * 3 + 2]))\n    return json.dumps(json_data)\n\n# Append coco ground truth to coco_images_arr and coco_annotations_arr\ndef transform_to_coco_gt(datas, coco_images_arr, coco_annotations_arr):\n    """"""\n    data: num_samples * 32, type Tensor\n    16 keypoints\n    \n    output:\n    inside coco_images_arr, coco_annotations_arr\n    """"""\n    for idx, sample in enumerate(datas):\n        coco_image = CocoImage()\n        coco_annotation = CocoAnnotation()\n        sample = np.array(sample.numpy()).reshape(-1, 2)\n        num_keypoints = len(sample)    \n        keypoints = np.append(sample, np.array(np.ones(num_keypoints).reshape(-1, 1) * 2), \n                      axis=1)\n        xmin = np.min(sample[:,0])\n        ymin = np.min(sample[:,1])\n        xmax = np.max(sample[:,0])\n        ymax = np.max(sample[:,1])\n        width = ymax - ymin\n        height = xmax - xmin\n        coco_image = coco_image._replace(id = idx, width=width, height=height, file_name="""")\n        coco_annotation = coco_annotation._replace(num_keypoints=num_keypoints)\n        coco_annotation = coco_annotation._replace(area=width*height)\n        coco_annotation = coco_annotation._replace(keypoints=keypoints.reshape(-1))\n        coco_annotation = coco_annotation._replace(image_id=idx)\n        coco_annotation = coco_annotation._replace(bbox=[xmin, ymin, width, height])\n        coco_annotation = coco_annotation._replace(category_id=1) # default ""1"" for keypoint\n        coco_annotation = coco_annotation._replace(id=idx)\n        coco_annotation = coco_annotation._replace(iscrowd=0)\n        coco_images_arr.append(coco_image)\n        coco_annotations_arr.append(coco_annotation)\n    return ()\n\n# Coco predict result class\nCocoPredictAnnotation = namedtuple_with_defaults(\'predict_anno\', [\'image_id\', \'category_id\', \'keypoints\', \'score\'])\n\n# Append coco predict result to coco_images_arr and coco_pred_annotations_arr\ndef transform_to_coco_pred(datas, coco_pred_annotations_arr, beg_idx):\n    """"""\n    data: num_samples * 32, type Variable\n    16 keypoints\n    \n    output:\n    inside coco_pred_annotations_arr\n    """"""\n    for idx, sample in enumerate(datas):\n        coco_pred_annotation = CocoPredictAnnotation()\n        \n        sample = np.array(sample.data.cpu().numpy()).reshape(-1, 2)\n        num_keypoints = len(sample)        \n        keypoints = np.append(sample, np.array(np.ones(num_keypoints).reshape(-1, 1) * 2), \n                      axis=1)\n        xmin = np.min(sample[:,0])\n        ymin = np.min(sample[:,1])\n        xmax = np.max(sample[:,0])\n        ymax = np.max(sample[:,1])\n        width = ymax - ymin\n        height = xmax - xmin\n        # set value\n        cur_idx = beg_idx + idx\n        coco_pred_annotation = coco_pred_annotation._replace(image_id=cur_idx)\n        coco_pred_annotation = coco_pred_annotation._replace(category_id=1)\n        coco_pred_annotation = coco_pred_annotation._replace(keypoints=keypoints.reshape(-1))\n        coco_pred_annotation = coco_pred_annotation._replace(score=2)\n        # add to arr\n        coco_pred_annotations_arr.append(coco_pred_annotation)\n    return ()'"
dataloader.py,9,"b'\'\'\'\nFile: dataloader.py\nProject: MobilePose-PyTorch\nFile Created: Tuesday, 15th January 2019 6:26:25 pm\nAuthor: Yuliang Xiu (yuliangxiu@sjtu.edu.cn)\n-----\nLast Modified: Monday, 11th March 2019 12:51:19 am\nModified By: Yuliang Xiu (yuliangxiu@sjtu.edu.cn>)\n-----\nCopyright 2018 - 2019 Shanghai Jiao Tong University, Machine Vision and Intelligence Group\n\'\'\'\n\n\nimport csv\nimport numpy as np\nimport os\nfrom skimage import io, transform\nimport cv2\n\nimport torch\nimport alog\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets, transforms, utils, models\n\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\n\ndef crop_camera(image, ratio=0.15):\n    height = image.shape[0]\n    width = image.shape[1]\n    mid_width = width / 2.0\n    width_20 = width * ratio\n    crop_img = image[0:int(height), int(mid_width - width_20):int(mid_width + width_20)]\n    return crop_img\n\ndef display_pose( img, pose, ids):\n    \n    mean=np.array([0.485, 0.456, 0.406])\n    std=np.array([0.229, 0.224, 0.225])\n    pose  = pose.data.cpu().numpy()\n    img = img.cpu().numpy().transpose(1,2,0)\n    colors = [\'g\', \'g\', \'g\', \'g\', \'g\', \'g\', \'m\', \'m\', \'r\', \'r\', \'y\', \'y\', \'y\', \'y\',\'y\',\'y\']\n    pairs = [[8,9],[11,12],[11,10],[2,1],[1,0],[13,14],[14,15],[3,4],[4,5],[8,7],[7,6],[6,2],[6,3],[8,12],[8,13]]\n    colors_skeleton = [\'r\', \'y\', \'y\', \'g\', \'g\', \'y\', \'y\', \'g\', \'g\', \'m\', \'m\', \'g\', \'g\', \'y\',\'y\']\n    img = np.clip(img*std+mean, 0.0, 1.0)\n    img_width, img_height,_ = img.shape\n    pose = ((pose + 1)* np.array([img_width, img_height])-1)/2 # pose ~ [-1,1]\n\n    plt.subplot(25,4,ids+1)\n    ax = plt.gca()\n    plt.imshow(img)\n    for idx in range(len(colors)):\n        plt.plot(pose[idx,0], pose[idx,1], marker=\'o\', color=colors[idx])\n    for idx in range(len(colors_skeleton)):\n        plt.plot(pose[pairs[idx],0], pose[pairs[idx],1],color=colors_skeleton[idx])\n\n    xmin = np.min(pose[:,0])\n    ymin = np.min(pose[:,1])\n    xmax = np.max(pose[:,0])\n    ymax = np.max(pose[:,1])\n\n    bndbox = np.array(expand_bbox(xmin, xmax, ymin, ymax, img_width, img_height))\n    coords = (bndbox[0], bndbox[1]), bndbox[2]-bndbox[0]+1, bndbox[3]-bndbox[1]+1\n    ax.add_patch(plt.Rectangle(*coords, fill=False, edgecolor=\'yellow\', linewidth=1))\n\ndef expand_bbox(left, right, top, bottom, img_width, img_height):\n    width = right-left\n    height = bottom-top\n    ratio = 0.15\n    new_left = np.clip(left-ratio*width,0,img_width)\n    new_right = np.clip(right+ratio*width,0,img_width)\n    new_top = np.clip(top-ratio*height,0,img_height)\n    new_bottom = np.clip(bottom+ratio*height,0,img_height)\n\n    return [int(new_left), int(new_top), int(new_right), int(new_bottom)]\n\n# Rescale implementation of mobilenetV2\n\nclass Wrap(object):\n    \n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        self.output_size = output_size\n\n    def __call__(self, sample):\n\n        image_, pose_ = sample[\'image\']/256.0, sample[\'pose\']\n        h, w = image_.shape[:2]\n        if isinstance(self.output_size, int):\n            if h > w:\n                new_h, new_w = self.output_size * h / w, self.output_size\n            else:\n                new_h, new_w = self.output_size, self.output_size * w / h\n        else:\n            new_h, new_w = self.output_size\n\n        new_h, new_w = int(new_h), int(new_w)\n\n        image = transform.resize(image_, (new_w, new_h))\n        pose = pose_.reshape([-1,2])/np.array([w,h])\n        pose *= -1.0\n\n        return {\'image\': image, \'pose\': pose}\n\n\n# Rescale implementation of Resnet18\n\nclass Rescale(object):\n\n\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        self.output_size = output_size\n\n    def __call__(self, sample):\n        image_, pose_ = sample[\'image\']/256.0, sample[\'pose\']\n        h, w = image_.shape[:2]\n        im_scale = min(float(self.output_size[0]) / float(h), float(self.output_size[1]) / float(w))\n        new_h = int(image_.shape[0] * im_scale)\n        new_w = int(image_.shape[1] * im_scale)\n        image = cv2.resize(image_, (new_w, new_h),\n                    interpolation=cv2.INTER_LINEAR)\n        left_pad = (self.output_size[1] - new_w) // 2\n        right_pad = (self.output_size[1] - new_w) - left_pad\n        top_pad = (self.output_size[0] - new_h) // 2\n        bottom_pad = (self.output_size[0] - new_h) - top_pad\n        mean=np.array([0.485, 0.456, 0.406])\n        pad = ((top_pad, bottom_pad), (left_pad, right_pad))\n        image = np.stack([np.pad(image[:,:,c], pad, mode=\'constant\', constant_values=mean[c]) \n                        for c in range(3)], axis=2)\n        pose = (pose_.reshape([-1,2])/np.array([w,h])*np.array([new_w,new_h]))\n        pose += [left_pad, top_pad]\n        pose = (pose * 2 + 1) / self.output_size - 1 # pose ~ [-1,1]\n\n        return {\'image\': image, \'pose\': pose}\n\n\nclass Expansion(object):\n    \n    def __call__(self, sample):\n        image, pose = sample[\'image\'], sample[\'pose\']\n        h, w = image.shape[:2]\n        x = np.arange(0, h)\n        y = np.arange(0, w) \n        x, y = np.meshgrid(x, y)\n        x = x[:,:, np.newaxis]/h\n        y = y[:,:, np.newaxis]/w\n        image = np.concatenate((image, x, y), axis=2)\n        \n        return {\'image\': image,\n                \'pose\': pose}\n    \nclass ToTensor(object):\n\n    def __call__(self, sample):\n        image, pose = sample[\'image\'], sample[\'pose\']\n\t\t# todo: support heatmap\n        # guass_heatmap = sample[\'guass_heatmap\']\n        h, w = image.shape[:2]\n\n        mean=np.array([0.485, 0.456, 0.406])\n        std=np.array([0.229, 0.224, 0.225])\n\n        image[:,:,:3] = (image[:,:,:3]-mean)/(std)\n        image = torch.from_numpy(image.transpose((2, 0, 1))).float()\n        pose = torch.from_numpy(pose).float()\n\n\t\t# todo: support heatmap\n\t    # guass_heatmap = torch.from_numpy(guass_heatmap).float()\n        return {\'image\': image,\n                \'pose\': pose}\n        #return {\'image\': image,\n        #        \'pose\': pose,\n        #        \'guass_heatmap\': guass_heatmap}\n\nclass PoseDataset(Dataset):\n\n    def __init__(self, csv_file, transform):\n        self.root = os.path.dirname(csv_file)\n        with open(csv_file) as f:\n            self.f_csv = list(csv.reader(f, delimiter=\'\\t\'))\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.f_csv)\n        \n    def __getitem__(self, idx):\n        line = self.f_csv[idx][0].split("","")\n        img_path = os.path.join(self.root,\'images\',line[0])\n        image = io.imread(img_path)\n        height, width = image.shape[0], image.shape[1]\n        pose = np.array([float(item) for item in line[1:]]).reshape([-1,2])\n        \n        xmin = np.min(pose[:,0])\n        ymin = np.min(pose[:,1])\n        xmax = np.max(pose[:,0])\n        ymax = np.max(pose[:,1])\n\n        box = expand_bbox(xmin, xmax, ymin, ymax, width, height)\n        image = image[box[1]:box[3],box[0]:box[2],:]\n        pose = (pose-np.array([box[0],box[1]])).flatten()\n        sample = {\'image\': image, \'pose\':pose}\n        if self.transform:\n            sample = self.transform(sample)\n        return sample\n\n\nimport imgaug as ia\nfrom imgaug import augmenters as iaa\nfrom scipy import misc\nimport copy\nimport random\nfrom imgaug import parameters as iap\n\nclass Augmentation(object):\n    \n    def pose2keypoints(self, image, pose):\n        keypoints = []\n        for row in range(int(pose.shape[0])):\n            x = pose[row,0]\n            y = pose[row,1]\n            keypoints.append(ia.Keypoint(x=x, y=y))\n        return ia.KeypointsOnImage(keypoints, shape=image.shape)\n\n    def keypoints2pose(self, keypoints_aug):\n        one_person = []\n        for kp_idx, keypoint in enumerate(keypoints_aug.keypoints):\n            x_new, y_new = keypoint.x, keypoint.y\n            one_person.append(np.array(x_new).astype(np.float32))\n            one_person.append(np.array(y_new).astype(np.float32))\n        return np.array(one_person).reshape([-1,2])\n\n    def __call__(self, sample):\n        image, pose= sample[\'image\'], sample[\'pose\'].reshape([-1,2])\n\n        sometimes = lambda aug: iaa.Sometimes(0.3, aug)\n\n        seq = iaa.Sequential(\n            [\n                # Apply the following augmenters to most images.\n\n                sometimes(iaa.CropAndPad(percent=(-0.25, 0.25), pad_mode=[""edge""], keep_size=False)),\n\n                sometimes(iaa.Affine(\n                    scale={""x"": (0.75, 1.25), ""y"": (0.75, 1.25)},\n                    translate_percent={""x"": (-0.25, 0.25), ""y"": (-0.25, 0.25)},\n                    rotate=(-45, 45),\n                    shear=(-5, 5),\n                    order=[0, 1],\n                    cval=(0, 255),\n                    mode=ia.ALL\n                )),\n\n                iaa.SomeOf((0, 3),\n                    [\n        \n                        iaa.OneOf([\n                            iaa.GaussianBlur((0, 3.0)),\n                            # iaa.AverageBlur(k=(2, 7)),\n                            iaa.MedianBlur(k=(3, 11)),\n                            iaa.MotionBlur(k=5,angle=[-45, 45])\n                        ]),\n\n                        iaa.OneOf([\n                            iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n                            iaa.AdditivePoissonNoise(lam=(0,8), per_channel=True),\n                        ]),\n\n                        iaa.OneOf([\n                            iaa.Add((-10, 10), per_channel=0.5),\n                            iaa.Multiply((0.2, 1.2), per_channel=0.5),\n                            iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5),\n                        ]),\n                    ],\n                    # do all of the above augmentations in random order\n                    random_order=True\n                )\n            ],\n            # do all of the above augmentations in random order\n            random_order=True\n        )\n\n        # augmentation choices\n        seq_det = seq.to_deterministic()\n\n        image_aug = seq_det.augment_images([image])[0]\n        keypoints_aug = seq_det.augment_keypoints([self.pose2keypoints(image,pose)])[0]\n\n        return {\'image\': image_aug, \'pose\': self.keypoints2pose(keypoints_aug)}\n\n# TBD\nclass OneHot(object):\n    def __call__(self, sample):\n        image, pose = sample[\'image\'], sample[\'pose\']\n        one_hot = torch.zeros_like(torch.from_numpy(image))\n        return {\'image\': image, \'pose\': pose}\n\n\nclass Guass(object):\n    def __call__(self, sample):\n        sigma = 5\n        image, pose = sample[\'image\'], sample[\'pose\']\n        # create meshgrid\n        h, w = image.shape[:2]\n        x = np.arange(0, h)\n        y = np.arange(0, w)\n        x, y = np.meshgrid(x, y)\n        # declare guass\n        guass_heatmap = np.zeros([len(pose) // 2, image.shape[0], image.shape[1]])\n        xy_pose = np.reshape(pose,(-1, 2))\n        guass_hrescale = h // 30\n        guass_wrescale = w // 30\n\n        for idx,(x0,y0) in enumerate(xy_pose):\n            # alog.info(idx)\n            guass_heatmap[idx] = np.exp(- (((x - x0) * 1.0 /  guass_hrescale) ** 2 + ((y - y0) * 1.0 / guass_wrescale) ** 2) / (2 * sigma ** 2))\n        # alog.info(guass_heatmap.shape)\n        return {\'image\': image, \'pose\': pose, \'guass_heatmap\': guass_heatmap}\n'"
dataset_factory.py,0,"b'\'\'\'\nFile: dataset_factory.py\nProject: MobilePose-PyTorch\nFile Created: Sunday, 10th March 2019 8:02:12 pm\nAuthor: Yuliang Xiu (yuliangxiu@sjtu.edu.cn)\n-----\nLast Modified: Monday, 11th March 2019 12:51:11 am\nModified By: Yuliang Xiu (yuliangxiu@sjtu.edu.cn>)\n-----\nCopyright 2018 - 2019 Shanghai Jiao Tong University, Machine Vision and Intelligence Group\n\'\'\'\n\n\nfrom dataloader import Rescale, Wrap, PoseDataset, ToTensor, Augmentation, Expansion\nfrom torchvision import datasets, transforms, utils, models\nimport os\n\nROOT_DIR = ""./pose_dataset/mpii""  # root dir to the dataset\n\nDEBUG_MODE = False\n\ndef get_transform(modeltype, input_size):\n    """"""\n    :param modeltype: ""resnet"" / ""mobilenet""\n    :param input_size:\n    :return:\n    """"""\n    return Rescale((input_size, input_size))\n\n\nclass DatasetFactory:\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def get_train_dataset(modeltype, input_size, debug=DEBUG_MODE):\n        """"""\n        :param modeltype: ""resnet"" / ""mobilenet""\n        :return: type: PoseDataset\n        Example:\n        DataFactory.get_train_dataset(""resnet"", 224)\n        In debug mode, it will return a small dataset\n        """"""\n        csv_name = ""train_joints.csv""\n        if debug:\n            csv_name = ""train_joints-500.csv""\n\n        return PoseDataset(csv_file=os.path.join(ROOT_DIR, csv_name),\n                           transform=transforms.Compose([\n                               Augmentation(),\n                               get_transform(modeltype, input_size),\n                            #    Expansion(),\n                               ToTensor()\n                           ]))\n\n    @staticmethod\n    def get_test_dataset(modeltype, input_size, debug=DEBUG_MODE):\n        """"""\n        :param modeltype: resnet / mobilenet\n        :return: type: PoseDataset\n        Example:\n        DataFactory.get_test_dataset(""resnet"", 224)\n        In debug mode, it will return a small dataset\n        """"""\n        csv_name = ""test_joints.csv""\n        if debug:\n            csv_name = ""test_joints-500.csv""\n        return PoseDataset(\n            csv_file=os.path.join(ROOT_DIR, csv_name),\n            transform=transforms.Compose([\n                get_transform(modeltype, input_size),\n                # Expansion(),\n                ToTensor()\n            ]))\n'"
estimator.py,2,"b""'''\nFile: estimator.py\nProject: MobilePose-PyTorch\nFile Created: Monday, 11th March 2019 12:50:16 am\nAuthor: Yuliang Xiu (yuliangxiu@sjtu.edu.cn)\n-----\nLast Modified: Monday, 11th March 2019 12:50:58 am\nModified By: Yuliang Xiu (yuliangxiu@sjtu.edu.cn>)\n-----\nCopyright 2018 - 2019 Shanghai Jiao Tong University, Machine Vision and Intelligence Group\n'''\n\n\nimport itertools\nimport logging\nimport math\nfrom collections import namedtuple\n\nimport cv2\nimport numpy as np\nimport torch\n\nfrom scipy.ndimage import maximum_filter, gaussian_filter\nfrom skimage import io, transform\n\nclass ResEstimator:\n    def __init__(self, model_path, net, inp_dim=224):\n        self.inp_dim = inp_dim\n        self.net = net\n        self.net.load_state_dict(torch.load(model_path, map_location=lambda storage, loc: storage))\n        self.net.eval()\n\n    def addlayer(self, image):\n        h, w = image.shape[:2]\n        # todo retraining model\n        x = np.arange(0, h)\n        y = np.arange(0, w) \n        x, y = np.meshgrid(x, y)\n        x = x[:,:, np.newaxis]\n        y = y[:,:, np.newaxis]\n        image = np.concatenate((image, x, y), axis=2)\n        \n        return image\n\n    def wrap(self, image, output_size):\n        image_ = image/256.0\n        h, w = image_.shape[:2]\n        if isinstance(output_size, int):\n            if h > w:\n                new_h, new_w = output_size * h / w, output_size\n            else:\n                new_h, new_w = output_size, output_size * w / h\n        else:\n            new_h, new_w = output_size\n\n        new_h, new_w = int(new_h), int(new_w)\n\n        image = transform.resize(image_, (new_w, new_h))\n        pose_fun = lambda x: (x.reshape([-1,2]) * 1.0 /np.array([new_w, new_h])*np.array([w,h]))\n        return {'image': image, 'pose_fun': pose_fun}\n        \n    def rescale(self, image, output_size):\n\n        image_ = image/256.0\n        h, w = image_.shape[:2]\n        im_scale = min(float(output_size[0]) / float(h), float(output_size[1]) / float(w))\n        new_h = int(image_.shape[0] * im_scale)\n        new_w = int(image_.shape[1] * im_scale)\n        image = cv2.resize(image_, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n        left_pad =int( (output_size[1] - new_w) / 2.0)\n        top_pad = int((output_size[0] - new_h) / 2.0)\n        mean=np.array([0.485, 0.456, 0.406])\n        pad = ((top_pad, top_pad), (left_pad, left_pad))\n        image = np.stack([np.pad(image[:,:,c], pad, mode='constant', constant_values=mean[c])for c in range(3)], axis=2)\n        pose_fun = lambda x: ((((x.reshape([-1,2])+np.array([1.0,1.0]))/2.0*np.array(output_size)-[left_pad, top_pad]) * 1.0 /np.array([new_w, new_h])*np.array([w,h])))\n        return {'image': image, 'pose_fun': pose_fun}\n\n    def to_tensor(self, image):\n     \n        mean=np.array([0.485, 0.456, 0.406])\n        std=np.array([0.229, 0.224, 0.225])    \n        image = torch.from_numpy(((image-mean)/std).transpose((2, 0, 1))).float()\n        return image\n\n    def inference(self, in_npimg):\n        canvas = np.zeros_like(in_npimg)\n        height = canvas.shape[0]\n        width = canvas.shape[1]\n\n        rescale_out = self.rescale(in_npimg, (self.inp_dim, self.inp_dim))\n       \n        image = rescale_out['image']\n        image = self.to_tensor(image)\n        image = image.unsqueeze(0)\n        pose_fun = rescale_out['pose_fun']\n\n        keypoints = self.net(image)\n        keypoints = keypoints[0].detach().numpy()\n        keypoints = pose_fun(keypoints).astype(int)\n\n        return keypoints\n\n    @staticmethod\n    def draw_humans(npimg, pose, imgcopy=False):\n        if imgcopy:\n            npimg = np.copy(npimg)\n        image_h, image_w = npimg.shape[:2]\n        centers = {}\n\n        colors = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0],\n              [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255],\n              [170, 0, 255], [255, 0, 255]]\n\n        pairs = [[8,9],[11,12],[11,10],[2,1],[1,0],[13,14],[14,15],[3,4],[4,5],[8,7],[7,6],[6,2],[6,3],[8,12],[8,13]]\n        colors_skeleton = ['r', 'y', 'y', 'g', 'g', 'y', 'y', 'g', 'g', 'm', 'm', 'g', 'g', 'y','y']\n        colors_skeleton = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0],\n              [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255],\n              [170, 0, 255]]\n\n        for idx in range(len(colors)):\n            cv2.circle(npimg, (pose[idx,0], pose[idx,1]), 3, colors[idx], thickness=3, lineType=8, shift=0)\n        for idx in range(len(colors_skeleton)):\n            npimg = cv2.line(npimg, (pose[pairs[idx][0],0], pose[pairs[idx][0],1]), (pose[pairs[idx][1],0], pose[pairs[idx][1],1]), colors_skeleton[idx], 3)\n\n        return npimg\n\n"""
eval.py,6,"b'# coding: utf-8\n\'\'\'\nFile: eval.py\nProject: MobilePose-PyTorch\nFile Created: Thursday, 7th March 2019 1:50:18 pm\nAuthor: Yuliang Xiu (yuliangxiu@sjtu.edu.cn)\n-----\nLast Modified: Monday, 11th March 2019 12:50:50 am\nModified By: Yuliang Xiu (yuliangxiu@sjtu.edu.cn>)\n-----\nCopyright 2018 - 2019 Shanghai Jiao Tong University, Machine Vision and Intelligence Group\n\'\'\'\n\n\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\nfrom tqdm import tqdm\nfrom math import ceil\n\nimport argparse\n\nimport os\nimport multiprocessing\nfrom dataloader import *\nfrom coco_utils import *\nfrom networks import *\nfrom network import CoordRegressionNetwork\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\nfrom dataset_factory import DatasetFactory\n\ngpus = [0,1]\nos.environ[""CUDA_VISIBLE_DEVICES""]=""0""\ntorch.backends.cudnn.enabled = True\nprint(""GPU NUM: "", torch.cuda.device_count())\n\ndef eval_coco(all_test_data, modelname, net_path, result_gt_json_path, result_pred_json_path):\n        """"""\n        Example:\n        eval_coco(\'/home/yuliang/code/PoseFlow/checkpoint140.t7\', \n        \'result-gt-json.txt\', \'result-pred-json.txt\')\n        """"""\n        # gpu mode\n        net = CoordRegressionNetwork(n_locations=16, backbone=modelname).to(device)\n        net.load_state_dict(torch.load(net_path))\n        net = net.eval()\n\n        # cpu mode\n        # net = Net()\n        # net = torch.load(net_path, map_location=lambda storage, loc: storage)\n\n        ## generate groundtruth json\n        total_size = len(all_test_data[\'image\'])\n        all_coco_images_arr = [] \n        all_coco_annotations_arr = []\n        transform_to_coco_gt(all_test_data[\'pose\'], all_coco_images_arr, all_coco_annotations_arr)\n        coco = CocoData(all_coco_images_arr, all_coco_annotations_arr)\n        coco_str =  coco.dumps()\n        result_gt_json = float2int(coco_str)\n\n        # save ground truth json to file\n        dirname = os.path.dirname(result_gt_json_path)\n        if not os.path.exists(dirname):\n            os.makedirs(dirname)\n        f = open(result_gt_json_path, ""w"")\n        print(""==> write"" + result_gt_json_path)\n        f.write(result_gt_json)\n        f.close()\n\n        # generate predictioin json\n        total_size = len(all_test_data[\'image\'])\n        all_coco_pred_annotations_arr = [] \n        \n        bs = 100 # batchsize\n\n        for i in tqdm(range(1, int(ceil(total_size / float(bs) + 1)))):\n            sample_data = {}\n\n            # gpu mode\n            sample_data[\'image\'] = all_test_data[\'image\'][bs * (i - 1) : min(bs * i, total_size)].to(device)\n            # cpu mode\n            # sample_data[\'image\'] = all_test_data[\'image\'][100 * (i - 1) : min(100 * i, total_size)]\n\n            # t0 = time.time()\n            with torch.no_grad():\n                coords, heatmaps = net(sample_data[\'image\'])\n                \n            transform_to_coco_pred(coords.view(-1,16*2), all_coco_pred_annotations_arr, bs * (i - 1))\n\n        all_coco_pred_annotations_arr = [item._asdict() for item in all_coco_pred_annotations_arr]\n        result_pred_json = json.dumps(all_coco_pred_annotations_arr, cls=MyEncoder)\n        result_pred_json = float2int(result_pred_json)\n\n        # save result predict json to file\n        dirname = os.path.dirname(result_pred_json_path)\n        if not os.path.exists(dirname):\n            os.makedirs(dirname)\n        f = open(result_pred_json_path, ""w"")\n        print(""==> save "" + result_pred_json_path)\n        f.write(result_pred_json)\n        f.close()\n\nif __name__ == \'__main__\':\n    \n    parser = argparse.ArgumentParser(description=\'MobilePose Demo\')\n    parser.add_argument(\'--model\', type=str, required=True, default="""")\n    parser.add_argument(\'--t7\', type=str, required=True, default="""")\n    parser.add_argument(\'--gpu\', type=str, required=True, default="""")\n    args = parser.parse_args()\n\n    modelpath = args.t7\n\n    device = torch.device(""cuda"" if len(args.gpu)>0 else ""cpu"")\n\n    # user defined parameters\n    num_threads = multiprocessing.cpu_count()\n    PATH_PREFIX = ""./results/{}"".format(modelpath.split(""."")[0])\n\n    input_size = 224\n    modelname = args.model\n\n    test_dataset = DatasetFactory.get_test_dataset(""resnet"", input_size)\n\n    print(""Loading testing dataset, wait..."")\n    bs_test = len(test_dataset)\n    test_dataloader = DataLoader(test_dataset, batch_size= bs_test,\n                            shuffle=False, num_workers = num_threads)\n\n    # get all test data\n    all_test_data = {}\n    for i_batch, sample_batched in enumerate(tqdm(test_dataloader)):\n        all_test_data = sample_batched\n        eval_coco(all_test_data, modelname, modelpath, os.path.join(PATH_PREFIX, \'result-gt-json.txt\'), os.path.join(PATH_PREFIX, \'result-pred-json.txt\'))\n\n    # evaluation\n    annType = [\'segm\',\'bbox\',\'keypoints\']\n    annType = annType[2]\n    prefix = \'person_keypoints\' if annType==\'keypoints\' else \'instances\'\n\n    print(\'Running demo for *%s* results.\'%(annType))\n\n    annFile = os.path.join(PATH_PREFIX, ""result-gt-json.txt"")\n    cocoGt=COCO(annFile)\n    resFile = os.path.join(PATH_PREFIX,""result-pred-json.txt"")\n    cocoDt=cocoGt.loadRes(resFile)\n    imgIds=sorted(cocoGt.getImgIds())\n\n    cocoEval = COCOeval(cocoGt,cocoDt,annType)\n    cocoEval.params.imgIds = imgIds\n    cocoEval.evaluate()\n    cocoEval.accumulate()\n    cocoEval.summarize()\n'"
network.py,1,"b'\'\'\'\nFile: network.py\nProject: MobilePose-PyTorch\nFile Created: Thursday, 7th March 2019 6:33:57 pm\nAuthor: Yuliang Xiu (yuliangxiu@sjtu.edu.cn)\n-----\nLast Modified: Monday, 11th March 2019 12:50:40 am\nModified By: Yuliang Xiu (yuliangxiu@sjtu.edu.cn>)\n-----\nCopyright 2018 - 2019 Shanghai Jiao Tong University, Machine Vision and Intelligence Group\n\'\'\'\n\n\n\nfrom networks import *\nfrom networks.senet import se_resnet\nimport torch.nn as nn\nimport dsntnn\n\nclass CoordRegressionNetwork(nn.Module):\n    def __init__(self, n_locations, backbone):\n        super(CoordRegressionNetwork, self).__init__()\n\n        if backbone == ""unet"":\n            self.resnet = UNet()\n            self.outsize = 64\n        elif backbone == ""resnet18"":\n            self.resnet = resnet.resnet18_ed(pretrained=False)\n            self.outsize = 32\n        elif backbone == ""resnet34"":\n            self.resnet = resnet.resnet34_ed(pretrained=False)\n            self.outsize = 512\n        elif backbone == ""resnet50"":\n            self.resnet = resnet.resnet50_ed(pretrained=False)\n            self.outsize = 2048\n        elif backbone == ""senet18"":\n            self.resnet = se_resnet.senet18_ed(pretrained=False)\n            self.outsize = 512\n        elif backbone == ""shufflenetv2"":\n            self.resnet = ShuffleNetV2.shufflenetv2_ed(width_mult=1.0)\n            self.outsize = 32\n        elif backbone == ""mobilenetv2"":\n            self.resnet = MobileNetV2.mobilenetv2_ed(width_mult=1.0)\n            self.outsize = 32\n        elif backbone == ""squeezenet"":\n            self.resnet = squeezenet1_1()\n            self.outsize = 64\n\n        self.hm_conv = nn.Conv2d(self.outsize, n_locations, kernel_size=1, bias=False)\n\n    def forward(self, images):\n        # 1. Run the images through our Resnet\n        resnet_out = self.resnet(images)\n        # 2. Use a 1x1 conv to get one unnormalized heatmap per location\n        unnormalized_heatmaps = self.hm_conv(resnet_out)\n        # 3. Normalize the heatmaps\n        heatmaps = dsntnn.flat_softmax(unnormalized_heatmaps)\n        # 4. Calculate the coordinates\n        coords = dsntnn.dsnt(heatmaps)\n\n        return coords, heatmaps'"
run_webcam.py,1,"b'\'\'\'\nFile: run_webcam.py\nProject: MobilePose-PyTorch\nFile Created: Monday, 11th March 2019 12:47:30 am\nAuthor: Yuliang Xiu (yuliangxiu@sjtu.edu.cn)\n-----\nLast Modified: Monday, 11th March 2019 12:48:49 am\nModified By: Yuliang Xiu (yuliangxiu@sjtu.edu.cn>)\n-----\nCopyright 2018 - 2019 Shanghai Jiao Tong University, Machine Vision and Intelligence Group\n\'\'\'\n\nimport argparse\nimport logging\nimport time\n\nimport cv2\nimport os\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import models\n\nfrom estimator import ResEstimator\nfrom networks import *\nfrom network import CoordRegressionNetwork\nfrom dataloader import crop_camera\n\n# import matplotlib\n# matplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\n\nif __name__ == \'__main__\':\n    \n    parser = argparse.ArgumentParser(description=\'MobilePose Realtime Webcam.\')\n    parser.add_argument(\'--model\', type=str, default=\'resnet18\', choices=[\'mobilenetv2\', \'resnet18\', \'shufflenetv2\', \'squeezenet\'])\n    parser.add_argument(\'--inp_dim\', type=int, default=224, help=\'input size\')\n    parser.add_argument(\'--camera\', type=int, default=0)\n\n    args = parser.parse_args()\n\n    # load the model \n    model_path = os.path.join(""./models"", args.model+""_%d_adam_best.t7""%args.inp_dim)\n    net = CoordRegressionNetwork(n_locations=16, backbone=args.model).to(""cpu"")\n    e = ResEstimator(model_path, net, args.inp_dim)\n\n    # initial the camera\n    cam = cv2.VideoCapture(args.camera)\n\n    ret_val, image = cam.read()\n    image = crop_camera(image)\n\n    while True:\n        # read image from the camera and preprocess\n        ret_val , image = cam.read()\n        image = crop_camera(image)\n        # forward the image\n        humans = e.inference(image)\n        image = ResEstimator.draw_humans(image, humans, imgcopy=False)\n        cv2.imshow(\'MobilePose Demo\', image)\n        if cv2.waitKey(1) == 27: # ESC\n            break\n\n    cv2.destroyAllWindows()\n\n    # # single person rgb image test\n    # image = cv2.imread(""./results/test.png"")\n    # humans = e.inference(image)\n    # image = ResEstimator.draw_humans(image, humans, imgcopy=False)\n    # cv2.imwrite(""./results/out.png"", image)\n'"
training.py,16,"b'# coding: utf-8\n\'\'\'\nFile: training.py\nProject: MobilePose-PyTorch\nFile Created: Friday, 8th March 2019 6:53:13 pm\nAuthor: Yuliang Xiu (yuliangxiu@sjtu.edu.cn)\n-----\nLast Modified: Monday, 11th March 2019 12:50:27 am\nModified By: Yuliang Xiu (yuliangxiu@sjtu.edu.cn>)\n-----\nCopyright 2018 - 2019 Shanghai Jiao Tong University, Machine Vision and Intelligence Group\n\'\'\'\n\n\n# remove warning\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\n\nfrom network import *\nfrom dataloader import *\nfrom networks import *\nimport argparse\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torch.optim.lr_scheduler import *\nfrom dataset_factory import DatasetFactory, ROOT_DIR\nimport os\nimport multiprocessing\nfrom tqdm import tqdm\n\n\nif __name__ == \'__main__\':\n\n    parser = argparse.ArgumentParser(description=\'MobilePose Demo\')\n    parser.add_argument(\'--model\', type=str, default=""resnet"")\n    parser.add_argument(\'--gpu\', type=str, default="""")\n    parser.add_argument(\'--inputsize\', type=int, default=224)\n    parser.add_argument(\'--batchsize\', type=int, default=32)\n    parser.add_argument(\'--lr\', type=float, default=1e-3)\n    parser.add_argument(\'--t7\', type=str, default="""")\n\n    args = parser.parse_args()\n    modeltype = args.model\n\n    device = torch.device(""cuda:0"" if len(args.gpu)>1 else ""cuda"")\n\n    # user defined parameters\n    num_threads = int(multiprocessing.cpu_count()/2)\n    minloss = np.float(""inf"")\n    # minloss = 0.43162785\n\n    # gpu setting\n    os.environ[""CUDA_VISIBLE_DEVICES""]=args.gpu\n    torch.backends.cudnn.enabled = True\n    cudnn.benchmark = True\n\n    print(""GPU NUM: %d""%(torch.cuda.device_count()))\n    net = CoordRegressionNetwork(n_locations=16, backbone=modeltype).to(device)\n    net = torch.nn.DataParallel(net).to(device)\n\n    learning_rate = args.lr\n    batchsize = args.batchsize\n    inputsize = args.inputsize\n    modelname = ""%s_%d""%(modeltype,inputsize)\n\n\n    logname = modeltype+\'-log.txt\'\n\n    if args.t7 != """":\n        # load pretrain model\n        pre_net = torch.load(args.t7)\n        net.module.load_state_dict(pre_net)\n        \n        for param in list(net.parameters()):\n            param.requires_grad = True\n\n    net = net.train()\n\n    PATH_PREFIX = \'./models\' # path to save the model\n\n    train_dataset = DatasetFactory.get_train_dataset(modeltype, inputsize)\n    train_dataloader = DataLoader(train_dataset, batch_size=batchsize,\n                            shuffle=True, num_workers = num_threads)\n\n\n    test_dataset = DatasetFactory.get_test_dataset(modeltype, inputsize)\n    test_dataloader = DataLoader(test_dataset, batch_size=batchsize,\n                            shuffle=False, num_workers = num_threads)\n\n\n    criterion = nn.MSELoss().to(device)\n    optimizer = optim.Adam(net.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08)\n    # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n    # optimizer = optim.RMSprop(net.parameters(), lr=learning_rate)\n\n    scheduler = StepLR(optimizer, step_size=80, gamma=0.5)\n\n\n    train_loss_all = []\n    valid_loss_all = []\n\n    for epoch in range(1000):  # loop over the dataset multiple times\n        \n        train_loss_epoch = []\n        train_loss_epoch_coords = []\n        train_loss_epoch_hm = []\n\n        scheduler.step()\n\n        for i, data in enumerate(tqdm(train_dataloader)):\n            # training\n            images, poses = data[\'image\'], data[\'pose\']\n            images, poses = images.to(device), poses.to(device)\n            coords, heatmaps = net(images)\n\n            # Per-location euclidean losses\n            euc_losses = dsntnn.euclidean_losses(coords, poses)\n            # Per-location regularization losses\n            reg_losses = dsntnn.js_reg_losses(heatmaps, poses, sigma_t=1.0)\n            # Combine losses into an overall loss\n            loss = dsntnn.average_loss(euc_losses + reg_losses)\n\n            del data, images, poses, coords, heatmaps\n        \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            train_loss_epoch.append(loss.item())\n            train_loss_epoch_coords.append(torch.mean(euc_losses).item())\n            train_loss_epoch_hm.append(torch.mean(reg_losses).item())\n\n        if epoch%2==0:\n\n            valid_loss_epoch = []\n            valid_loss_epoch_coords = []\n            valid_loss_epoch_hm = []\n\n            with torch.no_grad():  \n                for i_batch, sample_batched in enumerate(tqdm(test_dataloader)):\n                    # calculate the valid loss\n                    images = sample_batched[\'image\'].to(device)\n                    poses = sample_batched[\'pose\'].to(device)\n                    coords, heatmaps = net(images)\n\n                    # Per-location euclidean losses\n                    euc_losses = dsntnn.euclidean_losses(coords, poses)\n                    # Per-location regularization losses\n                    reg_losses = dsntnn.js_reg_losses(heatmaps, poses, sigma_t=1.0)\n                    # Combine losses into an overall loss\n                    loss = dsntnn.average_loss(euc_losses + reg_losses)\n\n                    del sample_batched, images, poses, coords, heatmaps\n\n                    valid_loss_epoch.append(loss.item())\n                    valid_loss_epoch_coords.append(torch.mean(euc_losses).item())\n                    valid_loss_epoch_hm.append(torch.mean(reg_losses).item())\n\n            if np.mean(np.array(valid_loss_epoch)) < minloss:\n                # save the model\n                minloss = np.mean(np.array(valid_loss_epoch))\n                checkpoint_file = ""%s/%s_%.4f.t7""%(PATH_PREFIX, modelname, minloss)\n                checkpoint_best_file = ""%s/%s_adam_best.t7""%(PATH_PREFIX, modelname)\n                # torch.save(net, checkpoint_file)\n                torch.save(net.module.state_dict(), checkpoint_best_file)\n                print(\'==> checkpoint model saving to %s and %s\'%(checkpoint_file, checkpoint_best_file))\n\n            print(\'[epoch %d] train loss(coords): %.8f, train loss(hm): %.8f, train loss: %.8f,\\n          valid loss(coords): %.8f, valid loss(hm): %.8f, valid loss: %.8f\\n\' %\n                (epoch + 1, np.mean(np.array(train_loss_epoch_coords)), np.mean(np.array(train_loss_epoch_hm)), np.mean(np.array(train_loss_epoch)), \n                 np.mean(np.array(valid_loss_epoch_coords)), np.mean(np.array(valid_loss_epoch_hm)), np.mean(np.array(valid_loss_epoch))))\n\n            # write the log of the training process\n            if not os.path.exists(PATH_PREFIX):\n                os.makedirs(PATH_PREFIX)\n\n            with open(os.path.join(PATH_PREFIX,logname), \'a+\') as file_output:\n                file_output.write(\'[epoch %d] train loss(coords): %.8f, train loss(hm): %.8f, train loss: %.8f,\\n          valid loss(coords): %.8f, valid loss(hm): %.8f, valid loss: %.8f\\n\' %\n                (epoch + 1, np.mean(np.array(train_loss_epoch_coords)), np.mean(np.array(train_loss_epoch_hm)), np.mean(np.array(train_loss_epoch)), \n                 np.mean(np.array(valid_loss_epoch_coords)), np.mean(np.array(valid_loss_epoch_hm)), np.mean(np.array(valid_loss_epoch))))\n                file_output.flush() \n                \n    print(\'Finished Training\')\n'"
networks/DUC.py,1,"b""# -----------------------------------------------------\n# Copyright (c) Shanghai Jiao Tong University. All rights reserved.\n# Written by Jiefeng Li (jeff.lee.sjtu@gmail.com)\n# -----------------------------------------------------\n\nimport torch.nn as nn\n\n\nclass DUC(nn.Module):\n    '''\n    Initialize: inplanes, planes, upscale_factor\n    OUTPUT: (planes // upscale_factor^2) * ht * wd\n    '''\n\n    def __init__(self, inplanes, planes, upscale_factor=2):\n        super(DUC, self).__init__()\n        self.conv = nn.Conv2d(\n            inplanes, planes, kernel_size=3, padding=1, bias=False)\n        self.bn = nn.BatchNorm2d(planes, momentum=0.1)\n        self.relu = nn.ReLU(inplace=True)\n        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.pixel_shuffle(x)\n        return x"""
networks/MobileNetV2.py,1,"b'import torch.nn as nn\nimport math\nfrom .DUC import DUC\n\n\ndef conv_bn(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\ndef conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        hidden_dim = round(inp * expand_ratio)\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        if expand_ratio == 1:\n            self.conv = nn.Sequential(\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n        else:\n            self.conv = nn.Sequential(\n                # pw\n                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass MobileNet(nn.Module):\n    def __init__(self, n_class=1000, input_size=224, width_mult=1.):\n        super(MobileNet, self).__init__()\n        block = InvertedResidual\n        input_channel = 32\n        last_channel = 1280\n        interverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # building first layer\n        assert input_size % 32 == 0\n        input_channel = int(input_channel * width_mult)\n        self.last_channel = int(last_channel * width_mult) if width_mult > 1.0 else last_channel\n        self.features = [conv_bn(3, input_channel, 2)]\n        # building inverted residual blocks\n        for t, c, n, s in interverted_residual_setting:\n            output_channel = int(c * width_mult)\n            for i in range(n):\n                if i == 0:\n                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))\n                else:\n                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t))\n                input_channel = output_channel\n        # building last several layers\n        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n        # make it nn.Sequential\n        self.features = nn.Sequential(*self.features)\n        self.conv_compress = nn.Conv2d(1280, 256, 1, 1, 0, bias=False)\n        self.duc1 = DUC(256, 512, upscale_factor=2)\n        self.duc2 = DUC(128, 256, upscale_factor=2)\n        self.duc3 = DUC(64, 128, upscale_factor=2)\n\n        # building classifier\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(self.last_channel, n_class),\n        )\n\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.mean(3).mean(2)\n        x = self.conv_compress(x)\n        x = self.duc1(x)\n        x = self.duc2(x)\n        x = self.duc3(x)\n        x = self.classifier(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                n = m.weight.size(1)\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n\ndef mobilenetv2_ed(width_mult=1.0):\n    model = MobileNet(width_mult=width_mult)\n    model = nn.Sequential(*(list(model.children())[:-1]))\n    for param in model.parameters():\n        param.requires_grad = True\n    return model\n                \n'"
networks/ShuffleNetV2.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom collections import OrderedDict\nfrom torch.nn import init\nimport math\nfrom .DUC import DUC\n\ndef conv_bn(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU(inplace=True)\n    )\n\n\ndef conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU(inplace=True)\n    )\n\ndef channel_shuffle(x, groups):\n    batchsize, num_channels, height, width = x.data.size()\n\n    channels_per_group = num_channels // groups\n    \n    # reshape\n    x = x.view(batchsize, groups, \n        channels_per_group, height, width)\n\n    x = torch.transpose(x, 1, 2).contiguous()\n\n    # flatten\n    x = x.view(batchsize, -1, height, width)\n\n    return x\n    \nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, benchmodel):\n        super(InvertedResidual, self).__init__()\n        self.benchmodel = benchmodel\n        self.stride = stride\n        assert stride in [1, 2]\n\n        oup_inc = oup//2\n        \n        if self.benchmodel == 1:\n            #assert inp == oup_inc\n        \tself.banch2 = nn.Sequential(\n                # pw\n                nn.Conv2d(oup_inc, oup_inc, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                nn.ReLU(inplace=True),\n                # dw\n                nn.Conv2d(oup_inc, oup_inc, 3, stride, 1, groups=oup_inc, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                # pw-linear\n                nn.Conv2d(oup_inc, oup_inc, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                nn.ReLU(inplace=True),\n            )                \n        else:                  \n            self.banch1 = nn.Sequential(\n                # dw\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                # pw-linear\n                nn.Conv2d(inp, oup_inc, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                nn.ReLU(inplace=True),\n            )        \n    \n            self.banch2 = nn.Sequential(\n                # pw\n                nn.Conv2d(inp, oup_inc, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                nn.ReLU(inplace=True),\n                # dw\n                nn.Conv2d(oup_inc, oup_inc, 3, stride, 1, groups=oup_inc, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                # pw-linear\n                nn.Conv2d(oup_inc, oup_inc, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                nn.ReLU(inplace=True),\n            )\n          \n    @staticmethod\n    def _concat(x, out):\n        # concatenate along channel axis\n        return torch.cat((x, out), 1)        \n\n    def forward(self, x):\n        if 1==self.benchmodel:\n            x1 = x[:, :(x.shape[1]//2), :, :]\n            x2 = x[:, (x.shape[1]//2):, :, :]\n            out = self._concat(x1, self.banch2(x2))\n        elif 2==self.benchmodel:\n            out = self._concat(self.banch1(x), self.banch2(x))\n\n        return channel_shuffle(out, 2)\n\n\nclass ShuffleNet(nn.Module):\n    def __init__(self, n_class=1000, input_size=224, width_mult=1.):\n        super(ShuffleNet, self).__init__()\n        \n        assert input_size % 32 == 0\n        \n        self.stage_repeats = [4, 8, 4]\n        # index 0 is invalid and should never be called.\n        # only used for indexing convenience.\n        if width_mult == 0.5:\n            self.stage_out_channels = [-1, 24,  48,  96, 192, 1024]\n        elif width_mult == 1.0:\n            self.stage_out_channels = [-1, 24, 116, 232, 464, 1024]\n        elif width_mult == 1.5:\n            self.stage_out_channels = [-1, 24, 176, 352, 704, 1024]\n        elif width_mult == 2.0:\n            self.stage_out_channels = [-1, 24, 224, 488, 976, 2048]\n        else:\n            raise ValueError(\n                """"""{} groups is not supported for\n                       1x1 Grouped Convolutions"""""".format(num_groups))\n\n        # building first layer\n        input_channel = self.stage_out_channels[1]\n        self.conv1 = conv_bn(3, input_channel, 2)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self.features = []\n        # building inverted residual blocks\n        for idxstage in range(len(self.stage_repeats)):\n            numrepeat = self.stage_repeats[idxstage]\n            output_channel = self.stage_out_channels[idxstage+2]\n            for i in range(numrepeat):\n                if i == 0:\n\t            #inp, oup, stride, benchmodel):\n                    self.features.append(InvertedResidual(input_channel, output_channel, 2, 2))\n                else:\n                    self.features.append(InvertedResidual(input_channel, output_channel, 1, 1))\n                input_channel = output_channel\n                \n                \n        # make it nn.Sequential\n        self.features = nn.Sequential(*self.features)\n\n        # building last several layers\n        self.conv_last      = conv_1x1_bn(input_channel, self.stage_out_channels[-1])\n        self.conv_compress = nn.Conv2d(1024, 256, 1, 1, 0, bias=False)\n        self.duc1 = DUC(256, 512, upscale_factor=2)\n        self.duc2 = DUC(128, 256, upscale_factor=2)\n        self.duc3 = DUC(64, 128, upscale_factor=2)\n        self.globalpool = nn.Sequential(nn.AvgPool2d(int(input_size/32)))              \n    \n        # building classifier\n        self.classifier = nn.Sequential(nn.Linear(self.stage_out_channels[-1], n_class))\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.maxpool(x)\n        x = self.features(x)\n        x = self.conv_last(x)\n        x = self.conv_compress(x)\n        x = self.duc1(x)\n        x = self.duc2(x)\n        x = self.duc3(x)\n\n        x = self.globalpool(x)\n        x = x.view(-1, self.stage_out_channels[-1])\n        x = self.classifier(x)\n        return x\n\n\ndef shufflenetv2_ed(width_mult=1.):\n    model = ShuffleNet(width_mult=width_mult)\n    model = nn.Sequential(*(list(model.children())[:-2]))\n    for param in model.parameters():\n        param.requires_grad = True\n    return model'"
networks/__init__.py,0,b'from .duc_hdc import *\nfrom .fcn16s import *\nfrom .fcn32s import *\nfrom .fcn8s import *\nfrom .gcn import *\nfrom .psp_net import *\nfrom .seg_net import *\nfrom .u_net import *\nfrom .resnet import *\nfrom .MobileNetV2 import *\nfrom .senet import *\nfrom .ShuffleNetV2 import *\nfrom .squeezenet import *\n'
networks/config.py,0,"b""import os\n\n# here (https://github.com/pytorch/vision/tree/master/torchvision/models) to find the download link of pretrained models\n\nroot = '/home/yuliang/code/MobilePose-pytorch/models/pretrained'\nres101_path = os.path.join(root, 'resnet101-5d3b4d8f.pth')\nres152_path = os.path.join(root, 'resnet152-b121ed2d.pth')\ninception_v3_path = os.path.join(root, 'inception_v3_google-1a9a5a14.pth')\nvgg19_bn_path = os.path.join(root, 'vgg19_bn-c79401a0.pth')\nvgg16_path = os.path.join(root, 'vgg16-397923af.pth')\ndense201_path = os.path.join(root, 'densenet201-c1103571.pth')\n\n'''\nvgg16 trained using caffe\nvisit this (https://github.com/jcjohnson/pytorch-vgg) to download the converted vgg16\n'''\nvgg16_caffe_path = os.path.join(root, 'vgg16-00b39a1b.pth')\n"""
networks/duc_hdc.py,2,"b""import torch\nfrom torch import nn\nfrom torchvision import models\n\nfrom .config import res152_path\n\nclass _DenseUpsamplingConvModule(nn.Module):\n    def __init__(self, down_factor, in_dim, num_classes):\n        super(_DenseUpsamplingConvModule, self).__init__()\n        upsample_dim = (down_factor ** 2) * num_classes\n        self.conv = nn.Conv2d(in_dim, upsample_dim, kernel_size=3, padding=1)\n        self.bn = nn.BatchNorm2d(upsample_dim)\n        self.relu = nn.ReLU(inplace=True)\n        self.pixel_shuffle = nn.PixelShuffle(down_factor)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.pixel_shuffle(x)\n        return x\n\n\nclass ResNetDUC(nn.Module):\n    # the size of image should be multiple of 8\n    def __init__(self, num_classes, pretrained=True):\n        super(ResNetDUC, self).__init__()\n        resnet = models.resnet152()\n        if pretrained:\n            resnet.load_state_dict(torch.load(res152_path))\n        self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n        self.layer1 = resnet.layer1\n        self.layer2 = resnet.layer2\n        self.layer3 = resnet.layer3\n        self.layer4 = resnet.layer4\n\n        for n, m in self.layer3.named_modules():\n            if 'conv2' in n:\n                m.dilation = (2, 2)\n                m.padding = (2, 2)\n                m.stride = (1, 1)\n            elif 'downsample.0' in n:\n                m.stride = (1, 1)\n        for n, m in self.layer4.named_modules():\n            if 'conv2' in n:\n                m.dilation = (4, 4)\n                m.padding = (4, 4)\n                m.stride = (1, 1)\n            elif 'downsample.0' in n:\n                m.stride = (1, 1)\n\n        self.duc = _DenseUpsamplingConvModule(8, 2048, num_classes)\n\n    def forward(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.duc(x)\n        return x\n\n\nclass ResNetDUCHDC(nn.Module):\n    # the size of image should be multiple of 8\n    def __init__(self, num_classes, pretrained=True):\n        super(ResNetDUCHDC, self).__init__()\n        resnet = models.resnet152()\n        if pretrained:\n            resnet.load_state_dict(torch.load(res152_path))\n        self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n        self.layer1 = resnet.layer1\n        self.layer2 = resnet.layer2\n        self.layer3 = resnet.layer3\n        self.layer4 = resnet.layer4\n\n        for n, m in self.layer3.named_modules():\n            if 'conv2' in n or 'downsample.0' in n:\n                m.stride = (1, 1)\n        for n, m in self.layer4.named_modules():\n            if 'conv2' in n or 'downsample.0' in n:\n                m.stride = (1, 1)\n        layer3_group_config = [1, 2, 5, 9]\n        for idx in range(len(self.layer3)):\n            self.layer3[idx].conv2.dilation = (layer3_group_config[idx % 4], layer3_group_config[idx % 4])\n            self.layer3[idx].conv2.padding = (layer3_group_config[idx % 4], layer3_group_config[idx % 4])\n        layer4_group_config = [5, 9, 17]\n        for idx in range(len(self.layer4)):\n            self.layer4[idx].conv2.dilation = (layer4_group_config[idx], layer4_group_config[idx])\n            self.layer4[idx].conv2.padding = (layer4_group_config[idx], layer4_group_config[idx])\n\n        self.duc = _DenseUpsamplingConvModule(8, 2048, num_classes)\n\n    def forward(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.duc(x)\n        return x\n"""
networks/fcn16s.py,1,"b""import torch\nfrom torch import nn\nfrom torchvision import models\n\nfrom utils import get_upsampling_weight\nfrom .config import vgg16_caffe_path\n\n\nclass FCN16VGG(nn.Module):\n    def __init__(self, num_classes, pretrained=True):\n        super(FCN16VGG, self).__init__()\n        vgg = models.vgg16()\n        if pretrained:\n            vgg.load_state_dict(torch.load(vgg16_caffe_path))\n        features, classifier = list(vgg.features.children()), list(vgg.classifier.children())\n\n        features[0].padding = (100, 100)\n\n        for f in features:\n            if 'MaxPool' in f.__class__.__name__:\n                f.ceil_mode = True\n            elif 'ReLU' in f.__class__.__name__:\n                f.inplace = True\n\n        self.features4 = nn.Sequential(*features[: 24])\n        self.features5 = nn.Sequential(*features[24:])\n\n        self.score_pool4 = nn.Conv2d(512, num_classes, kernel_size=1)\n        self.score_pool4.weight.data.zero_()\n        self.score_pool4.bias.data.zero_()\n\n        fc6 = nn.Conv2d(512, 4096, kernel_size=7)\n        fc6.weight.data.copy_(classifier[0].weight.data.view(4096, 512, 7, 7))\n        fc6.bias.data.copy_(classifier[0].bias.data)\n        fc7 = nn.Conv2d(4096, 4096, kernel_size=1)\n        fc7.weight.data.copy_(classifier[3].weight.data.view(4096, 4096, 1, 1))\n        fc7.bias.data.copy_(classifier[3].bias.data)\n        score_fr = nn.Conv2d(4096, num_classes, kernel_size=1)\n        score_fr.weight.data.zero_()\n        score_fr.bias.data.zero_()\n        self.score_fr = nn.Sequential(\n            fc6, nn.ReLU(inplace=True), nn.Dropout(), fc7, nn.ReLU(inplace=True), nn.Dropout(), score_fr\n        )\n\n        self.upscore2 = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=4, stride=2, bias=False)\n        self.upscore16 = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=32, stride=16, bias=False)\n        self.upscore2.weight.data.copy_(get_upsampling_weight(num_classes, num_classes, 4))\n        self.upscore16.weight.data.copy_(get_upsampling_weight(num_classes, num_classes, 32))\n\n    def forward(self, x):\n        x_size = x.size()\n        pool4 = self.features4(x)\n        pool5 = self.features5(pool4)\n\n        score_fr = self.score_fr(pool5)\n        upscore2 = self.upscore2(score_fr)\n\n        score_pool4 = self.score_pool4(0.01 * pool4)\n        upscore16 = self.upscore16(score_pool4[:, :, 5: (5 + upscore2.size()[2]), 5: (5 + upscore2.size()[3])]\n                                   + upscore2)\n        return upscore16[:, :, 27: (27 + x_size[2]), 27: (27 + x_size[3])].contiguous()\n"""
networks/fcn32s.py,1,"b""import torch\nfrom torch import nn\nfrom torchvision import models\n\nfrom utils import get_upsampling_weight\nfrom .config import vgg16_caffe_path\n\n\nclass FCN32VGG(nn.Module):\n    def __init__(self, num_classes, pretrained=True):\n        super(FCN32VGG, self).__init__()\n        vgg = models.vgg16()\n        if pretrained:\n            vgg.load_state_dict(torch.load(vgg16_caffe_path))\n        features, classifier = list(vgg.features.children()), list(vgg.classifier.children())\n\n        features[0].padding = (100, 100)\n\n        for f in features:\n            if 'MaxPool' in f.__class__.__name__:\n                f.ceil_mode = True\n            elif 'ReLU' in f.__class__.__name__:\n                f.inplace = True\n\n        self.features5 = nn.Sequential(*features)\n\n        fc6 = nn.Conv2d(512, 4096, kernel_size=7)\n        fc6.weight.data.copy_(classifier[0].weight.data.view(4096, 512, 7, 7))\n        fc6.bias.data.copy_(classifier[0].bias.data)\n        fc7 = nn.Conv2d(4096, 4096, kernel_size=1)\n        fc7.weight.data.copy_(classifier[3].weight.data.view(4096, 4096, 1, 1))\n        fc7.bias.data.copy_(classifier[3].bias.data)\n        score_fr = nn.Conv2d(4096, num_classes, kernel_size=1)\n        score_fr.weight.data.zero_()\n        score_fr.bias.data.zero_()\n        self.score_fr = nn.Sequential(\n            fc6, nn.ReLU(inplace=True), nn.Dropout(), fc7, nn.ReLU(inplace=True), nn.Dropout(), score_fr\n        )\n\n        self.upscore = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=64, stride=32, bias=False)\n        self.upscore.weight.data.copy_(get_upsampling_weight(num_classes, num_classes, 64))\n\n    def forward(self, x):\n        x_size = x.size()\n        pool5 = self.features5(x)\n        score_fr = self.score_fr(pool5)\n        upscore = self.upscore(score_fr)\n        return upscore[:, :, 19: (19 + x_size[2]), 19: (19 + x_size[3])].contiguous()\n"""
networks/fcn8s.py,2,"b""import torch\nfrom torch import nn\nfrom torchvision import models\n\nfrom utils import get_upsampling_weight\nfrom .config import vgg16_path, vgg16_caffe_path\n\n\n# This is implemented in full accordance with the original one (https://github.com/shelhamer/fcn.berkeleyvision.org)\nclass FCN8s(nn.Module):\n    def __init__(self, num_classes, pretrained=True, caffe=False):\n        super(FCN8s, self).__init__()\n        vgg = models.vgg16()\n        if pretrained:\n            if caffe:\n                # load the pretrained vgg16 used by the paper's author\n                vgg.load_state_dict(torch.load(vgg16_caffe_path))\n            else:\n                vgg.load_state_dict(torch.load(vgg16_path))\n        features, classifier = list(vgg.features.children()), list(vgg.classifier.children())\n\n        '''\n        100 padding for 2 reasons:\n            1) support very small input size\n            2) allow cropping in order to match size of different layers' feature maps\n        Note that the cropped part corresponds to a part of the 100 padding\n        Spatial information of different layers' feature maps cannot be align exactly because of cropping, which is bad\n        '''\n        features[0].padding = (100, 100)\n\n        for f in features:\n            if 'MaxPool' in f.__class__.__name__:\n                f.ceil_mode = True\n            elif 'ReLU' in f.__class__.__name__:\n                f.inplace = True\n\n        self.features3 = nn.Sequential(*features[: 17])\n        self.features4 = nn.Sequential(*features[17: 24])\n        self.features5 = nn.Sequential(*features[24:])\n\n        self.score_pool3 = nn.Conv2d(256, num_classes, kernel_size=1)\n        self.score_pool4 = nn.Conv2d(512, num_classes, kernel_size=1)\n        self.score_pool3.weight.data.zero_()\n        self.score_pool3.bias.data.zero_()\n        self.score_pool4.weight.data.zero_()\n        self.score_pool4.bias.data.zero_()\n\n        fc6 = nn.Conv2d(512, 4096, kernel_size=7)\n        fc6.weight.data.copy_(classifier[0].weight.data.view(4096, 512, 7, 7))\n        fc6.bias.data.copy_(classifier[0].bias.data)\n        fc7 = nn.Conv2d(4096, 4096, kernel_size=1)\n        fc7.weight.data.copy_(classifier[3].weight.data.view(4096, 4096, 1, 1))\n        fc7.bias.data.copy_(classifier[3].bias.data)\n        score_fr = nn.Conv2d(4096, num_classes, kernel_size=1)\n        score_fr.weight.data.zero_()\n        score_fr.bias.data.zero_()\n        self.score_fr = nn.Sequential(\n            fc6, nn.ReLU(inplace=True), nn.Dropout(), fc7, nn.ReLU(inplace=True), nn.Dropout(), score_fr\n        )\n\n        self.upscore2 = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=4, stride=2, bias=False)\n        self.upscore_pool4 = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=4, stride=2, bias=False)\n        self.upscore8 = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=16, stride=8, bias=False)\n        self.upscore2.weight.data.copy_(get_upsampling_weight(num_classes, num_classes, 4))\n        self.upscore_pool4.weight.data.copy_(get_upsampling_weight(num_classes, num_classes, 4))\n        self.upscore8.weight.data.copy_(get_upsampling_weight(num_classes, num_classes, 16))\n\n    def forward(self, x):\n        x_size = x.size()\n        pool3 = self.features3(x)\n        pool4 = self.features4(pool3)\n        pool5 = self.features5(pool4)\n\n        score_fr = self.score_fr(pool5)\n        upscore2 = self.upscore2(score_fr)\n\n        score_pool4 = self.score_pool4(0.01 * pool4)\n        upscore_pool4 = self.upscore_pool4(score_pool4[:, :, 5: (5 + upscore2.size()[2]), 5: (5 + upscore2.size()[3])]\n                                           + upscore2)\n\n        score_pool3 = self.score_pool3(0.0001 * pool3)\n        upscore8 = self.upscore8(score_pool3[:, :, 9: (9 + upscore_pool4.size()[2]), 9: (9 + upscore_pool4.size()[3])]\n                                 + upscore_pool4)\n        return upscore8[:, :, 31: (31 + x_size[2]), 31: (31 + x_size[3])].contiguous()\n"""
networks/gcn.py,2,"b'import torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torchvision import models\n\nfrom utils import initialize_weights\nfrom .config import res152_path\n\n\n# many are borrowed from https://github.com/ycszen/pytorch-ss/blob/master/gcn.py\nclass _GlobalConvModule(nn.Module):\n    def __init__(self, in_dim, out_dim, kernel_size):\n        super(_GlobalConvModule, self).__init__()\n        pad0 = (kernel_size[0] - 1) / 2\n        pad1 = (kernel_size[1] - 1) / 2\n        # kernel size had better be odd number so as to avoid alignment error\n        super(_GlobalConvModule, self).__init__()\n        self.conv_l1 = nn.Conv2d(in_dim, out_dim, kernel_size=(kernel_size[0], 1),\n                                 padding=(pad0, 0))\n        self.conv_l2 = nn.Conv2d(out_dim, out_dim, kernel_size=(1, kernel_size[1]),\n                                 padding=(0, pad1))\n        self.conv_r1 = nn.Conv2d(in_dim, out_dim, kernel_size=(1, kernel_size[1]),\n                                 padding=(0, pad1))\n        self.conv_r2 = nn.Conv2d(out_dim, out_dim, kernel_size=(kernel_size[0], 1),\n                                 padding=(pad0, 0))\n\n    def forward(self, x):\n        x_l = self.conv_l1(x)\n        x_l = self.conv_l2(x_l)\n        x_r = self.conv_r1(x)\n        x_r = self.conv_r2(x_r)\n        x = x_l + x_r\n        return x\n\n\nclass _BoundaryRefineModule(nn.Module):\n    def __init__(self, dim):\n        super(_BoundaryRefineModule, self).__init__()\n        self.relu = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        residual = self.conv1(x)\n        residual = self.relu(residual)\n        residual = self.conv2(residual)\n        out = x + residual\n        return out\n\n\nclass GCN(nn.Module):\n    def __init__(self, num_classes, input_size, pretrained=True):\n        super(GCN, self).__init__()\n        self.input_size = input_size\n        resnet = models.resnet152()\n        if pretrained:\n            resnet.load_state_dict(torch.load(res152_path))\n        self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu)\n        self.layer1 = nn.Sequential(resnet.maxpool, resnet.layer1)\n        self.layer2 = resnet.layer2\n        self.layer3 = resnet.layer3\n        self.layer4 = resnet.layer4\n\n        self.gcm1 = _GlobalConvModule(2048, num_classes, (7, 7))\n        self.gcm2 = _GlobalConvModule(1024, num_classes, (7, 7))\n        self.gcm3 = _GlobalConvModule(512, num_classes, (7, 7))\n        self.gcm4 = _GlobalConvModule(256, num_classes, (7, 7))\n\n        self.brm1 = _BoundaryRefineModule(num_classes)\n        self.brm2 = _BoundaryRefineModule(num_classes)\n        self.brm3 = _BoundaryRefineModule(num_classes)\n        self.brm4 = _BoundaryRefineModule(num_classes)\n        self.brm5 = _BoundaryRefineModule(num_classes)\n        self.brm6 = _BoundaryRefineModule(num_classes)\n        self.brm7 = _BoundaryRefineModule(num_classes)\n        self.brm8 = _BoundaryRefineModule(num_classes)\n        self.brm9 = _BoundaryRefineModule(num_classes)\n\n        initialize_weights(self.gcm1, self.gcm2, self.gcm3, self.gcm4, self.brm1, self.brm2, self.brm3,\n                           self.brm4, self.brm5, self.brm6, self.brm7, self.brm8, self.brm9)\n\n    def forward(self, x):\n        # if x: 512\n        fm0 = self.layer0(x)  # 256\n        fm1 = self.layer1(fm0)  # 128\n        fm2 = self.layer2(fm1)  # 64\n        fm3 = self.layer3(fm2)  # 32\n        fm4 = self.layer4(fm3)  # 16\n\n        gcfm1 = self.brm1(self.gcm1(fm4))  # 16\n        gcfm2 = self.brm2(self.gcm2(fm3))  # 32\n        gcfm3 = self.brm3(self.gcm3(fm2))  # 64\n        gcfm4 = self.brm4(self.gcm4(fm1))  # 128\n\n        fs1 = self.brm5(F.upsample_bilinear(gcfm1, fm3.size()[2:]) + gcfm2)  # 32\n        fs2 = self.brm6(F.upsample_bilinear(fs1, fm2.size()[2:]) + gcfm3)  # 64\n        fs3 = self.brm7(F.upsample_bilinear(fs2, fm1.size()[2:]) + gcfm4)  # 128\n        fs4 = self.brm8(F.upsample_bilinear(fs3, fm0.size()[2:]))  # 256\n        out = self.brm9(F.upsample_bilinear(fs4, self.input_size))  # 512\n\n        return out\n'"
networks/psp_net.py,4,"b""import torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torchvision import models\n\nfrom utils import initialize_weights\nfrom utils.misc import Conv2dDeformable\nfrom .config import res101_path\n\n\nclass _PyramidPoolingModule(nn.Module):\n    def __init__(self, in_dim, reduction_dim, setting):\n        super(_PyramidPoolingModule, self).__init__()\n        self.features = []\n        for s in setting:\n            self.features.append(nn.Sequential(\n                nn.AdaptiveAvgPool2d(s),\n                nn.Conv2d(in_dim, reduction_dim, kernel_size=1, bias=False),\n                nn.BatchNorm2d(reduction_dim, momentum=.95),\n                nn.ReLU(inplace=True)\n            ))\n        self.features = nn.ModuleList(self.features)\n\n    def forward(self, x):\n        x_size = x.size()\n        out = [x]\n        for f in self.features:\n            out.append(F.upsample(f(x), x_size[2:], mode='bilinear'))\n        out = torch.cat(out, 1)\n        return out\n\n\nclass PSPNet(nn.Module):\n    def __init__(self, num_classes, pretrained=True, use_aux=True):\n        super(PSPNet, self).__init__()\n        self.use_aux = use_aux\n        resnet = models.resnet101()\n        if pretrained:\n            resnet.load_state_dict(torch.load(res101_path))\n        self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n        self.layer1, self.layer2, self.layer3, self.layer4 = resnet.layer1, resnet.layer2, resnet.layer3, resnet.layer4\n\n        for n, m in self.layer3.named_modules():\n            if 'conv2' in n:\n                m.dilation, m.padding, m.stride = (2, 2), (2, 2), (1, 1)\n            elif 'downsample.0' in n:\n                m.stride = (1, 1)\n        for n, m in self.layer4.named_modules():\n            if 'conv2' in n:\n                m.dilation, m.padding, m.stride = (4, 4), (4, 4), (1, 1)\n            elif 'downsample.0' in n:\n                m.stride = (1, 1)\n\n        self.ppm = _PyramidPoolingModule(2048, 512, (1, 2, 3, 6))\n        self.final = nn.Sequential(\n            nn.Conv2d(4096, 512, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(512, momentum=.95),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.1),\n            nn.Conv2d(512, num_classes, kernel_size=1)\n        )\n\n        if use_aux:\n            self.aux_logits = nn.Conv2d(1024, num_classes, kernel_size=1)\n            initialize_weights(self.aux_logits)\n\n        initialize_weights(self.ppm, self.final)\n\n    def forward(self, x):\n        x_size = x.size()\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        if self.training and self.use_aux:\n            aux = self.aux_logits(x)\n        x = self.layer4(x)\n        x = self.ppm(x)\n        x = self.final(x)\n        if self.training and self.use_aux:\n            return F.upsample(x, x_size[2:], mode='bilinear'), F.upsample(aux, x_size[2:], mode='bilinear')\n        return F.upsample(x, x_size[2:], mode='bilinear')\n\n\n# just a try, not recommend to use\nclass PSPNetDeform(nn.Module):\n    def __init__(self, num_classes, input_size, pretrained=True, use_aux=True):\n        super(PSPNetDeform, self).__init__()\n        self.input_size = input_size\n        self.use_aux = use_aux\n        resnet = models.resnet101()\n        if pretrained:\n            resnet.load_state_dict(torch.load(res101_path))\n        self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n        self.layer1 = resnet.layer1\n        self.layer2 = resnet.layer2\n        self.layer3 = resnet.layer3\n        self.layer4 = resnet.layer4\n\n        for n, m in self.layer3.named_modules():\n            if 'conv2' in n:\n                m.padding = (1, 1)\n                m.stride = (1, 1)\n            elif 'downsample.0' in n:\n                m.stride = (1, 1)\n        for n, m in self.layer4.named_modules():\n            if 'conv2' in n:\n                m.padding = (1, 1)\n                m.stride = (1, 1)\n            elif 'downsample.0' in n:\n                m.stride = (1, 1)\n        for idx in range(len(self.layer3)):\n            self.layer3[idx].conv2 = Conv2dDeformable(self.layer3[idx].conv2)\n        for idx in range(len(self.layer4)):\n            self.layer4[idx].conv2 = Conv2dDeformable(self.layer4[idx].conv2)\n\n        self.ppm = _PyramidPoolingModule(2048, 512, (1, 2, 3, 6))\n        self.final = nn.Sequential(\n            nn.Conv2d(4096, 512, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(512, momentum=.95),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.1),\n            nn.Conv2d(512, num_classes, kernel_size=1)\n        )\n\n        if use_aux:\n            self.aux_logits = nn.Conv2d(1024, num_classes, kernel_size=1)\n            initialize_weights(self.aux_logits)\n\n        initialize_weights(self.ppm, self.final)\n\n    def forward(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        if self.training and self.use_aux:\n            aux = self.aux_logits(x)\n        x = self.layer4(x)\n        x = self.ppm(x)\n        x = self.final(x)\n        if self.training and self.use_aux:\n            return F.upsample(x, self.input_size, mode='bilinear'), F.upsample(aux, self.input_size, mode='bilinear')\n        return F.upsample(x, self.input_size, mode='bilinear')\n"""
networks/resnet.py,8,"b'from torchvision import models\nimport torch.nn as nn\nfrom .DUC import DUC\n\n# class ResNet(nn.Module):\n    \n#     def __init__(self, layers):\n#         super(Net, self).__init__()\n#         if layers == 18:\n#             model = models.resnet18(pretrained=True)\n#         elif layers == 34:\n#             model = models.resnet34(pretrained=True)\n#         # change the first layer to recieve five channel image\n#         model.conv1 = nn.Conv2d(5, 64, kernel_size=7, stride=2, padding=3,bias=True)\n#         # change the last layer to output 32 coordinates\n#         # model.fc=nn.Linear(512,32)\n#         # remove final two layers(fc, avepool)\n#         model = nn.Sequential(*(list(model.children())[:-4]))\n#         for param in model.parameters():\n#             param.requires_grad = True\n#         self.resnet = model\n        \n#     def forward(self, x):\n       \n#         pose_out = self.resnet(x)\n#         return pose_out\n\n\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\n\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\n           \'resnet152\',\'resnet18_ed\']\n\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    """"""1x1 convolution""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = conv1x1(inplanes, planes)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = conv3x3(planes, planes, stride)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = conv1x1(planes, planes * self.expansion)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False):\n        super(ResNet, self).__init__()\n        self.inplanes = 64\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n\n        self.conv_compress = nn.Conv2d(512, 256, 1, 1, 0, bias=False)\n        self.duc1 = DUC(256, 512, upscale_factor=2)\n        self.duc2 = DUC(128, 256, upscale_factor=2)\n        self.duc3 = DUC(64, 128, upscale_factor=2)\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.conv_compress(x)\n        x = self.duc1(x)\n        x = self.duc2(x)\n        x = self.duc3(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet18\']))\n    return model\n\ndef resnet18_ed(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    model = nn.Sequential(*(list(model.children())[:-2]))\n    for param in model.parameters():\n        param.requires_grad = True\n\n    return model\n\ndef resnet34_ed(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    model = nn.Sequential(*(list(model.children())[:-2]))\n    for param in model.parameters():\n        param.requires_grad = True\n\n    return model\n\ndef resnet50_ed(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    model = nn.Sequential(*(list(model.children())[:-2]))\n    for param in model.parameters():\n        param.requires_grad = True\n\n    return model\n\n\ndef resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet34\']))\n    return model\n\n\ndef resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\']))\n    return model\n\n\ndef resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet101\']))\n    return model\n\n\ndef resnet152(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet152\']))\n    return model'"
networks/seg_net.py,5,"b'import torch\nfrom torch import nn\nfrom torchvision import models\n\nfrom utils import initialize_weights\nfrom .config import vgg19_bn_path\n\n\nclass _DecoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, num_conv_layers):\n        super(_DecoderBlock, self).__init__()\n        middle_channels = in_channels / 2\n        layers = [\n            nn.ConvTranspose2d(in_channels, in_channels, kernel_size=2, stride=2),\n            nn.Conv2d(in_channels, middle_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(middle_channels),\n            nn.ReLU(inplace=True)\n        ]\n        layers += [\n                      nn.Conv2d(middle_channels, middle_channels, kernel_size=3, padding=1),\n                      nn.BatchNorm2d(middle_channels),\n                      nn.ReLU(inplace=True),\n                  ] * (num_conv_layers - 2)\n        layers += [\n            nn.Conv2d(middle_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        ]\n        self.decode = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.decode(x)\n\n\nclass SegNet(nn.Module):\n    def __init__(self, num_classes, pretrained=True):\n        super(SegNet, self).__init__()\n        vgg = models.vgg19_bn()\n        if pretrained:\n            vgg.load_state_dict(torch.load(vgg19_bn_path))\n        features = list(vgg.features.children())\n        self.enc1 = nn.Sequential(*features[0:7])\n        self.enc2 = nn.Sequential(*features[7:14])\n        self.enc3 = nn.Sequential(*features[14:27])\n        self.enc4 = nn.Sequential(*features[27:40])\n        self.enc5 = nn.Sequential(*features[40:])\n\n        self.dec5 = nn.Sequential(\n            *([nn.ConvTranspose2d(512, 512, kernel_size=2, stride=2)] +\n              [nn.Conv2d(512, 512, kernel_size=3, padding=1),\n               nn.BatchNorm2d(512),\n               nn.ReLU(inplace=True)] * 4)\n        )\n        self.dec4 = _DecoderBlock(1024, 256, 4)\n        self.dec3 = _DecoderBlock(512, 128, 4)\n        self.dec2 = _DecoderBlock(256, 64, 2)\n        self.dec1 = _DecoderBlock(128, num_classes, 2)\n        initialize_weights(self.dec5, self.dec4, self.dec3, self.dec2, self.dec1)\n\n    def forward(self, x):\n        enc1 = self.enc1(x)\n        enc2 = self.enc2(enc1)\n        enc3 = self.enc3(enc2)\n        enc4 = self.enc4(enc3)\n        enc5 = self.enc5(enc4)\n\n        dec5 = self.dec5(enc5)\n        dec4 = self.dec4(torch.cat([enc4, dec5], 1))\n        dec3 = self.dec3(torch.cat([enc3, dec4], 1))\n        dec2 = self.dec2(torch.cat([enc2, dec3], 1))\n        dec1 = self.dec1(torch.cat([enc1, dec2], 1))\n        return dec1\n'"
networks/squeezenet.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.utils.model_zoo as model_zoo\nfrom .DUC import DUC\n\n\n__all__ = [\'SqueezeNet\', \'squeezenet1_0\', \'squeezenet1_1\']\n\n\nmodel_urls = {\n    \'squeezenet1_0\': \'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth\',\n    \'squeezenet1_1\': \'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth\',\n}\n\n\nclass Fire(nn.Module):\n\n    def __init__(self, inplanes, squeeze_planes,\n                 expand1x1_planes, expand3x3_planes):\n        super(Fire, self).__init__()\n        self.inplanes = inplanes\n        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n                                   kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n                                   kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\n\nclass SqueezeNet(nn.Module):\n\n    def __init__(self, version=1.0, num_classes=1000):\n        super(SqueezeNet, self).__init__()\n        if version not in [1.0, 1.1]:\n            raise ValueError(""Unsupported SqueezeNet version {version}:""\n                             ""1.0 or 1.1 expected"".format(version=version))\n        self.num_classes = num_classes\n        if version == 1.0:\n            self.features = nn.Sequential(\n                nn.Conv2d(3, 96, kernel_size=7, stride=2),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n                Fire(96, 16, 64, 64),\n                Fire(128, 16, 64, 64),\n                Fire(128, 32, 128, 128),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n                Fire(256, 32, 128, 128),\n                Fire(256, 48, 192, 192),\n                Fire(384, 48, 192, 192),\n                Fire(384, 64, 256, 256),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n                Fire(512, 64, 256, 256),\n            )\n        else:\n            self.features = nn.Sequential(\n                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n                Fire(64, 16, 64, 64),\n                Fire(128, 16, 64, 64),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n                Fire(128, 32, 128, 128),\n                Fire(256, 32, 128, 128),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n                Fire(256, 48, 192, 192),\n                Fire(384, 48, 192, 192),\n                Fire(384, 64, 256, 256),\n                Fire(512, 64, 256, 256),\n            )\n        # Final convolution is initialized differently form the rest\n        # final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n        conv_compress = nn.Conv2d(512, 256, 1, 1, 0, bias=False)\n        duc1 = DUC(256, 512, upscale_factor=2)\n        duc2 = DUC(128, 256, upscale_factor=2)\n        self.classifier = nn.Sequential(\n            # nn.Dropout(p=0.5),\n            # final_conv,\n            # nn.ReLU(inplace=True),\n            # nn.AdaptiveAvgPool2d((1, 1))\n            conv_compress,\n            duc1,\n            duc2,\n            \n        )\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                if m is conv_compress:\n                    init.normal_(m.weight, mean=0.0, std=0.01)\n                else:\n                    init.kaiming_uniform_(m.weight)\n                if m.bias is not None:\n                    init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        # return x.view(x.size(0), self.num_classes)\n        return x\n\n\ndef squeezenet1_0(pretrained=False, **kwargs):\n    r""""""SqueezeNet model architecture from the `""SqueezeNet: AlexNet-level\n    accuracy with 50x fewer parameters and <0.5MB model size""\n    <https://arxiv.org/abs/1602.07360>`_ paper.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = SqueezeNet(version=1.0, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'squeezenet1_0\']))\n    return model\n\n\ndef squeezenet1_1(pretrained=False, **kwargs):\n    r""""""SqueezeNet 1.1 model from the `official SqueezeNet repo\n    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n    than SqueezeNet 1.0, without sacrificing accuracy.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = SqueezeNet(version=1.1, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'squeezenet1_1\']))\n    return model'"
networks/u_net.py,5,"b""import torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom utils import initialize_weights\n\n\nclass _EncoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, dropout=False):\n        super(_EncoderBlock, self).__init__()\n        layers = [\n            nn.Conv2d(in_channels, out_channels, kernel_size=3),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        ]\n        if dropout:\n            layers.append(nn.Dropout())\n        layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n        self.encode = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.encode(x)\n\n\nclass _DecoderBlock(nn.Module):\n    def __init__(self, in_channels, middle_channels, out_channels):\n        super(_DecoderBlock, self).__init__()\n        self.decode = nn.Sequential(\n            nn.Conv2d(in_channels, middle_channels, kernel_size=3),\n            nn.BatchNorm2d(middle_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(middle_channels, middle_channels, kernel_size=3),\n            nn.BatchNorm2d(middle_channels),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=2, stride=2),\n        )\n\n    def forward(self, x):\n        return self.decode(x)\n\n\nclass UNet(nn.Module):\n    def __init__(self, num_classes=16):\n        super(UNet, self).__init__()\n        self.enc1 = _EncoderBlock(3, 64)\n        self.enc2 = _EncoderBlock(64, 128)\n        self.enc3 = _EncoderBlock(128, 256)\n        self.enc4 = _EncoderBlock(256, 512, dropout=True)\n        self.center = _DecoderBlock(512, 1024, 512)\n        self.dec4 = _DecoderBlock(1024, 512, 256)\n        self.dec3 = _DecoderBlock(512, 256, 128)\n        self.dec2 = _DecoderBlock(256, 128, 64)\n        self.dec1 = nn.Sequential(\n            nn.Conv2d(128, 64, kernel_size=3),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n        )\n        # self.final = nn.Conv2d(64, num_classes, kernel_size=1)\n        initialize_weights(self)\n\n    def forward(self, x):\n        enc1 = self.enc1(x)\n        enc2 = self.enc2(enc1)\n        enc3 = self.enc3(enc2)\n        enc4 = self.enc4(enc3)\n        center = self.center(enc4)\n        dec4 = self.dec4(torch.cat([center, F.upsample(enc4, center.size()[2:], mode='bilinear')], 1))\n        dec3 = self.dec3(torch.cat([dec4, F.upsample(enc3, dec4.size()[2:], mode='bilinear')], 1))\n        dec2 = self.dec2(torch.cat([dec3, F.upsample(enc2, dec3.size()[2:], mode='bilinear')], 1))\n        dec1 = self.dec1(torch.cat([dec2, F.upsample(enc1, dec2.size()[2:], mode='bilinear')], 1))\n        # final = self.final(dec1)\n        return F.upsample(dec1, x.size()[2:], mode='bilinear')\n"""
pose_dataset/config.py,0,"b""import os\n\n# Full path to the project root\nROOT_DIR = os.path.expanduser('/home/yuliang/code/MobilePose-pytorch')\nOUTPUT_DIR = os.path.join(ROOT_DIR, 'output')\nLSP_DATASET_ROOT = os.path.join(ROOT_DIR, 'pose_dataset/lsp')\nLSP_EXT_DATASET_ROOT = os.path.join(ROOT_DIR, 'pose_dataset/lsp_ext')\nMPII_DATASET_ROOT = os.path.join(ROOT_DIR, 'pose_dataset/mpii')"""
pose_dataset/lsp_dataset.py,0,"b'# Copyright (c) 2016 Artsiom Sanakoyeu\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import basename\nfrom scipy.io import loadmat\nimport argparse\nimport glob\nimport re\nimport os.path\n\nfrom config import *\n\n\ndef create_data(images_dir, joints_mat_path, transpose_order=(2, 0, 1)):\n    """"""\n    Create a list of lines in format:\n      image_path, x1, y1, x2,y2, ...\n      where xi, yi - coordinates of the i-th joint\n    """"""\n    joints = loadmat(joints_mat_path)\n    print(joints[\'joints\'].shape)\n    joints = joints[\'joints\'].transpose(*transpose_order)\n    print(joints.shape)\n    joints = joints[:, :, :2]\n    print(joints.shape)\n    if joints.shape[1:] != (14, 2):\n        raise ValueError(\'Incorrect shape of the joints matrix of joints.mat. \'\n                         \'Expected: (?, 14, 2); received: (?, {}, {})\'.format(\n                            joints.shape[1], joints.shape[2]))\n\n    lines = list()\n    for img_path in sorted(glob.glob(os.path.join(images_dir, \'*.jpg\'))):\n        index = int(re.search(r\'im([0-9]+)\', basename(img_path)).groups()[0]) - 1\n        joints_str_list = [str(j) if j > 0 else \'-1\' for j in joints[index].flatten().tolist()]\n\n        out_list = [os.path.basename(img_path)]\n        out_list.extend(joints_str_list)\n        out_str = \',\'.join(out_list)\n\n        lines.append(out_str)\n    return lines\n\n\nif __name__ == \'__main__\':\n    """"""\n    Write train.csv and test.csv.\n    Each line in csv file will be in the following format:\n      image_name, x1, y1, x2,y2, ...\n      where xi, yi - coordinates of the i-th joint\n    Train file consists of 11000 lines (all images from extended LSP + first 1000 images from small LSP).\n    Test file consists of 1000 lines (last 1000 images from small LSP).\n    """"""\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--extended_lsp_images_dir\', type=str, default=os.path.join(LSP_EXT_DATASET_ROOT, \'images\'))\n    parser.add_argument(\'--extended_lsp_joints_path\', type=str, default=os.path.join(LSP_EXT_DATASET_ROOT, \'joints.mat\'))\n    parser.add_argument(\'--small_lsp_images_dir\', type=str, default=os.path.join(LSP_DATASET_ROOT, \'images\'))\n    parser.add_argument(\'--small_lsp_joints_path\', type=str, default=os.path.join(LSP_DATASET_ROOT, \'joints.mat\'))\n    parser.add_argument(\'--output_dir\', type=str, default=LSP_EXT_DATASET_ROOT)\n    args = parser.parse_args()\n    print(args)\n    if not os.path.exists(args.output_dir):\n        os.makedirs(args.output_dir)\n\n    file_train = open(\'%s/train_joints.csv\' % args.output_dir, \'w\')\n    file_test = open(\'%s/test_joints.csv\' % args.output_dir, \'w\')\n    file_train_lsp_small = open(\'%s/train_lsp_small_joints.csv\' % args.output_dir, \'w\')\n\n    print(\'Read LSP_EXT\')\n    lsp_ext_lines = create_data(args.extended_lsp_images_dir, args.extended_lsp_joints_path,\n                                transpose_order=(2, 0, 1))\n    print(\'Read LSP\')\n    lsp_small_lines = create_data(args.small_lsp_images_dir, args.small_lsp_joints_path,\n                                  transpose_order=(2, 1, 0))  # different dim order\n    print(\'Extended LSP images:\', len(lsp_ext_lines))\n    print(\'Small LSP images:\', len(lsp_small_lines))\n    if len(lsp_ext_lines) != 10000:\n        raise Exception(\'Extended LSP dataset must contain 10000 images!\')\n    if len(lsp_small_lines) != 2000:\n        raise Exception(\'Small LSP dataset must contain 2000 images!\')\n    num_small_lsp_train = 1000\n\n    for line in lsp_ext_lines:\n        print(line, file=file_train)\n    for line in lsp_small_lines[:num_small_lsp_train]:\n        print(line, file=file_train)\n        print(line, file=file_train_lsp_small)\n    for line in lsp_small_lines[num_small_lsp_train:]:\n        print(line, file=file_test)\n\n    file_train.close()\n    file_test.close()\n    file_train_lsp_small.close()\n'"
pose_dataset/mpii_dataset.py,0,"b'#!/usr/bin/env python\n# Copyright (c) 2016 Shunta Saito (original code)\n# Copyright (c) 2016 Artsiom Sanakoyeu\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\nfrom scipy.io import loadmat\nfrom itertools import izip\nimport json\nimport numpy as np\n\nfrom scripts.config import *\n\nMPII_DATA_DIR = MPII_DATASET_ROOT\nMPII_OUT_DIR = MPII_DATASET_ROOT\n\n\ndef fix_wrong_joints(joint):\n    if \'12\' in joint and \'13\' in joint and \'2\' in joint and \'3\' in joint:\n        if ((joint[\'12\'][0] < joint[\'13\'][0]) and\n                (joint[\'3\'][0] < joint[\'2\'][0])):\n            joint[\'2\'], joint[\'3\'] = joint[\'3\'], joint[\'2\']\n        if ((joint[\'12\'][0] > joint[\'13\'][0]) and\n                (joint[\'3\'][0] > joint[\'2\'][0])):\n            joint[\'2\'], joint[\'3\'] = joint[\'3\'], joint[\'2\']\n\n    return joint\n\n\ndef save_joints():\n    """"""\n    Convert annotations mat file to json and save on disk.\n    Only persons with annotations of all 16 joints will be written in the json.\n    """"""\n    joint_data_fn = os.path.join(MPII_OUT_DIR, \'data.json\')\n    mat = loadmat(os.path.join(MPII_DATA_DIR, \'mpii_human_pose_v1_u12_1.mat\'))\n\n    fp = open(joint_data_fn, \'w\')\n\n    for i, (anno, train_flag) in enumerate(\n        izip(mat[\'RELEASE\'][\'annolist\'][0, 0][0],\n            mat[\'RELEASE\'][\'img_train\'][0, 0][0])):\n\n        img_fn = anno[\'image\'][\'name\'][0, 0][0]\n        train_flag = int(train_flag)\n\n        if \'annopoints\' in str(anno[\'annorect\'].dtype):\n            annopoints = anno[\'annorect\'][\'annopoints\'][0]\n            head_x1s = anno[\'annorect\'][\'x1\'][0]\n            head_y1s = anno[\'annorect\'][\'y1\'][0]\n            head_x2s = anno[\'annorect\'][\'x2\'][0]\n            head_y2s = anno[\'annorect\'][\'y2\'][0]\n            for annopoint, head_x1, head_y1, head_x2, head_y2 in \\\n                    izip(annopoints, head_x1s, head_y1s, head_x2s, head_y2s):\n                if len(annopoint) > 0:\n                    head_rect = [float(head_x1[0, 0]),\n                                 float(head_y1[0, 0]),\n                                 float(head_x2[0, 0]),\n                                 float(head_y2[0, 0])]\n\n                    # joint coordinates\n                    annopoint = annopoint[\'point\'][0, 0]\n                    j_id = [str(j_i[0, 0]) for j_i in annopoint[\'id\'][0]]\n                    x = [x[0, 0] for x in annopoint[\'x\'][0]]\n                    y = [y[0, 0] for y in annopoint[\'y\'][0]]\n                    joint_pos = {}\n                    for _j_id, (_x, _y) in zip(j_id, zip(x, y)):\n                        joint_pos[str(_j_id)] = [float(_x), float(_y)]\n                    # joint_pos = fix_wrong_joints(joint_pos)\n\n                    # visiblity list\n                    if \'is_visible\' in str(annopoint.dtype):\n                        vis = [v[0] if v else [0]\n                               for v in annopoint[\'is_visible\'][0]]\n                        vis = dict([(k, int(v[0])) if len(v) > 0 else v\n                                    for k, v in zip(j_id, vis)])\n                    else:\n                        vis = None\n\n                    if len(joint_pos) == 16:\n                        data = {\n                            \'filename\': img_fn,\n                            \'train\': train_flag,\n                            \'head_rect\': head_rect,\n                            \'is_visible\': vis,\n                            \'joint_pos\': joint_pos\n                        }\n\n                        print(json.dumps(data), file=fp)\n\n\ndef write_line(datum, fp):\n    """"""\n    Write a line in format:\n      image_name, x1, y1, x2,y2, ...\n      where xi, yi - coordinates of the i-th joint\n    """"""\n    joints = sorted([[int(k), v] for k, v in datum[\'joint_pos\'].items()])\n    joints = np.array([j for i, j in joints]).flatten()\n\n    out = [datum[\'filename\']]\n    out.extend(joints)\n    out = [str(o) for o in out]\n    out = \',\'.join(out)\n\n    print(out, file=fp)\n\n\ndef split_train_test():\n    fp_test = open(os.path.join(MPII_OUT_DIR, \'test_joints.csv\'), \'w\')\n    fp_train = open(os.path.join(MPII_OUT_DIR, \'train_joints.csv\'), \'w\')\n    all_data = open(os.path.join(MPII_OUT_DIR, \'data.json\')).readlines()\n    N = len(all_data)\n    N_test = int(N * 0.1)\n    N_train = N - N_test\n\n    print(\'N:{}\'.format(N))\n    print(\'N_train:{}\'.format(N_train))\n    print(\'N_test:{}\'.format(N_test))\n\n    np.random.seed(1701)\n    perm = np.random.permutation(N)\n    test_indices = perm[:N_test]\n    train_indices = perm[N_test:]\n\n    print(\'train_indices:{}\'.format(len(train_indices)))\n    print(\'test_indices:{}\'.format(len(test_indices)))\n\n    for i in train_indices:\n        datum = json.loads(all_data[i].strip())\n        write_line(datum, fp_train)\n\n    for i in test_indices:\n        datum = json.loads(all_data[i].strip())\n        write_line(datum, fp_test)\n\n\nif __name__ == \'__main__\':\n    save_joints()\n    split_train_test()\n'"
utils/__init__.py,0,b'from .misc import *\nfrom .joint_transforms import *\nfrom .transforms import *\n'
utils/joint_transforms.py,0,"b""'''\nFile: joint_transforms.py\nProject: MobilePose-PyTorch\nFile Created: Monday, 7th January 2019 1:25:58 pm\nAuthor: Yuliang Xiu (yuliangxiu@sjtu.edu.cn)\n-----\nLast Modified: Monday, 11th March 2019 12:51:40 am\nModified By: Yuliang Xiu (yuliangxiu@sjtu.edu.cn>)\n-----\nCopyright 2018 - 2019 Shanghai Jiao Tong University, Machine Vision and Intelligence Group\n'''\n\n\nimport math\nimport numbers\nimport random\n\nfrom PIL import Image, ImageOps\nimport numpy as np\n\n\nclass Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n        for t in self.transforms:\n            img, mask = t(img, mask)\n        return img, mask\n\n\nclass RandomCrop(object):\n    def __init__(self, size, padding=0):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n        self.padding = padding\n\n    def __call__(self, img, mask):\n        if self.padding > 0:\n            img = ImageOps.expand(img, border=self.padding, fill=0)\n            mask = ImageOps.expand(mask, border=self.padding, fill=0)\n\n        assert img.size == mask.size\n        w, h = img.size\n        th, tw = self.size\n        if w == tw and h == th:\n            return img, mask\n        if w < tw or h < th:\n            return img.resize((tw, th), Image.BILINEAR), mask.resize((tw, th), Image.NEAREST)\n\n        x1 = random.randint(0, w - tw)\n        y1 = random.randint(0, h - th)\n        return img.crop((x1, y1, x1 + tw, y1 + th)), mask.crop((x1, y1, x1 + tw, y1 + th))\n\n\nclass CenterCrop(object):\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n        w, h = img.size\n        th, tw = self.size\n        x1 = int(round((w - tw) / 2.))\n        y1 = int(round((h - th) / 2.))\n        return img.crop((x1, y1, x1 + tw, y1 + th)), mask.crop((x1, y1, x1 + tw, y1 + th))\n\n\nclass RandomHorizontallyFlip(object):\n    def __call__(self, img, mask):\n        if random.random() < 0.5:\n            return img.transpose(Image.FLIP_LEFT_RIGHT), mask.transpose(Image.FLIP_LEFT_RIGHT)\n        return img, mask\n\n\nclass FreeScale(object):\n    def __init__(self, size):\n        self.size = tuple(reversed(size))  # size: (h, w)\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n        return img.resize(self.size, Image.BILINEAR), mask.resize(self.size, Image.NEAREST)\n\n\nclass Scale(object):\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n        w, h = img.size\n        if (w >= h and w == self.size) or (h >= w and h == self.size):\n            return img, mask\n        if w > h:\n            ow = self.size\n            oh = int(self.size * h / w)\n            return img.resize((ow, oh), Image.BILINEAR), mask.resize((ow, oh), Image.NEAREST)\n        else:\n            oh = self.size\n            ow = int(self.size * w / h)\n            return img.resize((ow, oh), Image.BILINEAR), mask.resize((ow, oh), Image.NEAREST)\n\n\nclass RandomSizedCrop(object):\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n        for attempt in range(10):\n            area = img.size[0] * img.size[1]\n            target_area = random.uniform(0.45, 1.0) * area\n            aspect_ratio = random.uniform(0.5, 2)\n\n            w = int(round(math.sqrt(target_area * aspect_ratio)))\n            h = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if random.random() < 0.5:\n                w, h = h, w\n\n            if w <= img.size[0] and h <= img.size[1]:\n                x1 = random.randint(0, img.size[0] - w)\n                y1 = random.randint(0, img.size[1] - h)\n\n                img = img.crop((x1, y1, x1 + w, y1 + h))\n                mask = mask.crop((x1, y1, x1 + w, y1 + h))\n                assert (img.size == (w, h))\n\n                return img.resize((self.size, self.size), Image.BILINEAR), mask.resize((self.size, self.size),\n                                                                                       Image.NEAREST)\n\n        # Fallback\n        scale = Scale(self.size)\n        crop = CenterCrop(self.size)\n        return crop(*scale(img, mask))\n\n\nclass RandomRotate(object):\n    def __init__(self, degree):\n        self.degree = degree\n\n    def __call__(self, img, mask):\n        rotate_degree = random.random() * 2 * self.degree - self.degree\n        return img.rotate(rotate_degree, Image.BILINEAR), mask.rotate(rotate_degree, Image.NEAREST)\n\n\nclass RandomSized(object):\n    def __init__(self, size):\n        self.size = size\n        self.scale = Scale(self.size)\n        self.crop = RandomCrop(self.size)\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n\n        w = int(random.uniform(0.5, 2) * img.size[0])\n        h = int(random.uniform(0.5, 2) * img.size[1])\n\n        img, mask = img.resize((w, h), Image.BILINEAR), mask.resize((w, h), Image.NEAREST)\n\n        return self.crop(*self.scale(img, mask))\n\n\nclass SlidingCropOld(object):\n    def __init__(self, crop_size, stride_rate, ignore_label):\n        self.crop_size = crop_size\n        self.stride_rate = stride_rate\n        self.ignore_label = ignore_label\n\n    def _pad(self, img, mask):\n        h, w = img.shape[: 2]\n        pad_h = max(self.crop_size - h, 0)\n        pad_w = max(self.crop_size - w, 0)\n        img = np.pad(img, ((0, pad_h), (0, pad_w), (0, 0)), 'constant')\n        mask = np.pad(mask, ((0, pad_h), (0, pad_w)), 'constant', constant_values=self.ignore_label)\n        return img, mask\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n\n        w, h = img.size\n        long_size = max(h, w)\n\n        img = np.array(img)\n        mask = np.array(mask)\n\n        if long_size > self.crop_size:\n            stride = int(math.ceil(self.crop_size * self.stride_rate))\n            h_step_num = int(math.ceil((h - self.crop_size) / float(stride))) + 1\n            w_step_num = int(math.ceil((w - self.crop_size) / float(stride))) + 1\n            img_sublist, mask_sublist = [], []\n            for yy in xrange(h_step_num):\n                for xx in xrange(w_step_num):\n                    sy, sx = yy * stride, xx * stride\n                    ey, ex = sy + self.crop_size, sx + self.crop_size\n                    img_sub = img[sy: ey, sx: ex, :]\n                    mask_sub = mask[sy: ey, sx: ex]\n                    img_sub, mask_sub = self._pad(img_sub, mask_sub)\n                    img_sublist.append(Image.fromarray(img_sub.astype(np.uint8)).convert('RGB'))\n                    mask_sublist.append(Image.fromarray(mask_sub.astype(np.uint8)).convert('P'))\n            return img_sublist, mask_sublist\n        else:\n            img, mask = self._pad(img, mask)\n            img = Image.fromarray(img.astype(np.uint8)).convert('RGB')\n            mask = Image.fromarray(mask.astype(np.uint8)).convert('P')\n            return img, mask\n\n\nclass SlidingCrop(object):\n    def __init__(self, crop_size, stride_rate, ignore_label):\n        self.crop_size = crop_size\n        self.stride_rate = stride_rate\n        self.ignore_label = ignore_label\n\n    def _pad(self, img, mask):\n        h, w = img.shape[: 2]\n        pad_h = max(self.crop_size - h, 0)\n        pad_w = max(self.crop_size - w, 0)\n        img = np.pad(img, ((0, pad_h), (0, pad_w), (0, 0)), 'constant')\n        mask = np.pad(mask, ((0, pad_h), (0, pad_w)), 'constant', constant_values=self.ignore_label)\n        return img, mask, h, w\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n\n        w, h = img.size\n        long_size = max(h, w)\n\n        img = np.array(img)\n        mask = np.array(mask)\n\n        if long_size > self.crop_size:\n            stride = int(math.ceil(self.crop_size * self.stride_rate))\n            h_step_num = int(math.ceil((h - self.crop_size) / float(stride))) + 1\n            w_step_num = int(math.ceil((w - self.crop_size) / float(stride))) + 1\n            img_slices, mask_slices, slices_info = [], [], []\n            for yy in xrange(h_step_num):\n                for xx in xrange(w_step_num):\n                    sy, sx = yy * stride, xx * stride\n                    ey, ex = sy + self.crop_size, sx + self.crop_size\n                    img_sub = img[sy: ey, sx: ex, :]\n                    mask_sub = mask[sy: ey, sx: ex]\n                    img_sub, mask_sub, sub_h, sub_w = self._pad(img_sub, mask_sub)\n                    img_slices.append(Image.fromarray(img_sub.astype(np.uint8)).convert('RGB'))\n                    mask_slices.append(Image.fromarray(mask_sub.astype(np.uint8)).convert('P'))\n                    slices_info.append([sy, ey, sx, ex, sub_h, sub_w])\n            return img_slices, mask_slices, slices_info\n        else:\n            img, mask, sub_h, sub_w = self._pad(img, mask)\n            img = Image.fromarray(img.astype(np.uint8)).convert('RGB')\n            mask = Image.fromarray(mask.astype(np.uint8)).convert('P')\n            return [img], [mask], [[0, sub_h, 0, sub_w, sub_h, sub_w]]\n"""
utils/misc.py,15,"b""import os\nfrom math import ceil\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.autograd import Variable\n\n\ndef check_mkdir(dir_name):\n    if not os.path.exists(dir_name):\n        os.mkdir(dir_name)\n\n\ndef initialize_weights(*models):\n    for model in models:\n        for module in model.modules():\n            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n                nn.init.kaiming_normal(module.weight)\n                if module.bias is not None:\n                    module.bias.data.zero_()\n            elif isinstance(module, nn.BatchNorm2d):\n                module.weight.data.fill_(1)\n                module.bias.data.zero_()\n\n\ndef get_upsampling_weight(in_channels, out_channels, kernel_size):\n    factor = (kernel_size + 1) // 2\n    if kernel_size % 2 == 1:\n        center = factor - 1\n    else:\n        center = factor - 0.5\n    og = np.ogrid[:kernel_size, :kernel_size]\n    filt = (1 - abs(og[0] - center) / factor) * (1 - abs(og[1] - center) / factor)\n    weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size), dtype=np.float64)\n    weight[list(range(in_channels)), list(range(out_channels)), :, :] = filt\n    return torch.from_numpy(weight).float()\n\n\nclass CrossEntropyLoss2d(nn.Module):\n    def __init__(self, weight=None, size_average=True, ignore_index=255):\n        super(CrossEntropyLoss2d, self).__init__()\n        self.nll_loss = nn.NLLLoss2d(weight, size_average, ignore_index)\n\n    def forward(self, inputs, targets):\n        return self.nll_loss(F.log_softmax(inputs), targets)\n\n\nclass FocalLoss2d(nn.Module):\n    def __init__(self, gamma=2, weight=None, size_average=True, ignore_index=255):\n        super(FocalLoss2d, self).__init__()\n        self.gamma = gamma\n        self.nll_loss = nn.NLLLoss2d(weight, size_average, ignore_index)\n\n    def forward(self, inputs, targets):\n        return self.nll_loss((1 - F.softmax(inputs)) ** self.gamma * F.log_softmax(inputs), targets)\n\n\ndef _fast_hist(label_pred, label_true, num_classes):\n    mask = (label_true >= 0) & (label_true < num_classes)\n    hist = np.bincount(\n        num_classes * label_true[mask].astype(int) +\n        label_pred[mask], minlength=num_classes ** 2).reshape(num_classes, num_classes)\n    return hist\n\n\ndef evaluate(predictions, gts, num_classes):\n    hist = np.zeros((num_classes, num_classes))\n    for lp, lt in zip(predictions, gts):\n        hist += _fast_hist(lp.flatten(), lt.flatten(), num_classes)\n    # axis 0: gt, axis 1: prediction\n    acc = np.diag(hist).sum() / hist.sum()\n    acc_cls = np.diag(hist) / hist.sum(axis=1)\n    acc_cls = np.nanmean(acc_cls)\n    iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n    mean_iu = np.nanmean(iu)\n    freq = hist.sum(axis=1) / hist.sum()\n    fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n    return acc, acc_cls, mean_iu, fwavacc\n\n\nclass AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\nclass PolyLR(object):\n    def __init__(self, optimizer, curr_iter, max_iter, lr_decay):\n        self.max_iter = float(max_iter)\n        self.init_lr_groups = []\n        for p in optimizer.param_groups:\n            self.init_lr_groups.append(p['lr'])\n        self.param_groups = optimizer.param_groups\n        self.curr_iter = curr_iter\n        self.lr_decay = lr_decay\n\n    def step(self):\n        for idx, p in enumerate(self.param_groups):\n            p['lr'] = self.init_lr_groups[idx] * (1 - self.curr_iter / self.max_iter) ** self.lr_decay\n\n\n# just a try, not recommend to use\nclass Conv2dDeformable(nn.Module):\n    def __init__(self, regular_filter, cuda=True):\n        super(Conv2dDeformable, self).__init__()\n        assert isinstance(regular_filter, nn.Conv2d)\n        self.regular_filter = regular_filter\n        self.offset_filter = nn.Conv2d(regular_filter.in_channels, 2 * regular_filter.in_channels, kernel_size=3,\n                                       padding=1, bias=False)\n        self.offset_filter.weight.data.normal_(0, 0.0005)\n        self.input_shape = None\n        self.grid_w = None\n        self.grid_h = None\n        self.cuda = cuda\n\n    def forward(self, x):\n        x_shape = x.size()  # (b, c, h, w)\n        offset = self.offset_filter(x)  # (b, 2*c, h, w)\n        offset_w, offset_h = torch.split(offset, self.regular_filter.in_channels, 1)  # (b, c, h, w)\n        offset_w = offset_w.contiguous().view(-1, int(x_shape[2]), int(x_shape[3]))  # (b*c, h, w)\n        offset_h = offset_h.contiguous().view(-1, int(x_shape[2]), int(x_shape[3]))  # (b*c, h, w)\n        if not self.input_shape or self.input_shape != x_shape:\n            self.input_shape = x_shape\n            grid_w, grid_h = np.meshgrid(np.linspace(-1, 1, x_shape[3]), np.linspace(-1, 1, x_shape[2]))  # (h, w)\n            grid_w = torch.Tensor(grid_w)\n            grid_h = torch.Tensor(grid_h)\n            if self.cuda:\n                grid_w = grid_w.cuda()\n                grid_h = grid_h.cuda()\n            self.grid_w = nn.Parameter(grid_w)\n            self.grid_h = nn.Parameter(grid_h)\n        offset_w = offset_w + self.grid_w  # (b*c, h, w)\n        offset_h = offset_h + self.grid_h  # (b*c, h, w)\n        x = x.contiguous().view(-1, int(x_shape[2]), int(x_shape[3])).unsqueeze(1)  # (b*c, 1, h, w)\n        x = F.grid_sample(x, torch.stack((offset_h, offset_w), 3))  # (b*c, h, w)\n        x = x.contiguous().view(-1, int(x_shape[1]), int(x_shape[2]), int(x_shape[3]))  # (b, c, h, w)\n        x = self.regular_filter(x)\n        return x\n\n\ndef sliced_forward(single_forward):\n    def _pad(x, crop_size):\n        h, w = x.size()[2:]\n        pad_h = max(crop_size - h, 0)\n        pad_w = max(crop_size - w, 0)\n        x = F.pad(x, (0, pad_w, 0, pad_h))\n        return x, pad_h, pad_w\n\n    def wrapper(self, x):\n        batch_size, _, ori_h, ori_w = x.size()\n        if self.training and self.use_aux:\n            outputs_all_scales = Variable(torch.zeros((batch_size, self.num_classes, ori_h, ori_w))).cuda()\n            aux_all_scales = Variable(torch.zeros((batch_size, self.num_classes, ori_h, ori_w))).cuda()\n            for s in self.scales:\n                new_size = (int(ori_h * s), int(ori_w * s))\n                scaled_x = F.upsample(x, size=new_size, mode='bilinear')\n                scaled_x = Variable(scaled_x).cuda()\n                scaled_h, scaled_w = scaled_x.size()[2:]\n                long_size = max(scaled_h, scaled_w)\n                print(scaled_x.size())\n\n                if long_size > self.crop_size:\n                    count = torch.zeros((scaled_h, scaled_w))\n                    outputs = Variable(torch.zeros((batch_size, self.num_classes, scaled_h, scaled_w))).cuda()\n                    aux_outputs = Variable(torch.zeros((batch_size, self.num_classes, scaled_h, scaled_w))).cuda()\n                    stride = int(ceil(self.crop_size * self.stride_rate))\n                    h_step_num = int(ceil((scaled_h - self.crop_size) / stride)) + 1\n                    w_step_num = int(ceil((scaled_w - self.crop_size) / stride)) + 1\n                    for yy in range(h_step_num):\n                        for xx in range(w_step_num):\n                            sy, sx = yy * stride, xx * stride\n                            ey, ex = sy + self.crop_size, sx + self.crop_size\n                            x_sub = scaled_x[:, :, sy: ey, sx: ex]\n                            x_sub, pad_h, pad_w = _pad(x_sub, self.crop_size)\n                            print(x_sub.size())\n                            outputs_sub, aux_sub = single_forward(self, x_sub)\n\n                            if sy + self.crop_size > scaled_h:\n                                outputs_sub = outputs_sub[:, :, : -pad_h, :]\n                                aux_sub = aux_sub[:, :, : -pad_h, :]\n\n                            if sx + self.crop_size > scaled_w:\n                                outputs_sub = outputs_sub[:, :, :, : -pad_w]\n                                aux_sub = aux_sub[:, :, :, : -pad_w]\n\n                            outputs[:, :, sy: ey, sx: ex] = outputs_sub\n                            aux_outputs[:, :, sy: ey, sx: ex] = aux_sub\n\n                            count[sy: ey, sx: ex] += 1\n                    count = Variable(count).cuda()\n                    outputs = (outputs / count)\n                    aux_outputs = (outputs / count)\n                else:\n                    scaled_x, pad_h, pad_w = _pad(scaled_x, self.crop_size)\n                    outputs, aux_outputs = single_forward(self, scaled_x)\n                    outputs = outputs[:, :, : -pad_h, : -pad_w]\n                    aux_outputs = aux_outputs[:, :, : -pad_h, : -pad_w]\n                outputs_all_scales += outputs\n                aux_all_scales += aux_outputs\n            return outputs_all_scales / len(self.scales), aux_all_scales\n        else:\n            outputs_all_scales = Variable(torch.zeros((batch_size, self.num_classes, ori_h, ori_w))).cuda()\n            for s in self.scales:\n                new_size = (int(ori_h * s), int(ori_w * s))\n                scaled_x = F.upsample(x, size=new_size, mode='bilinear')\n                scaled_h, scaled_w = scaled_x.size()[2:]\n                long_size = max(scaled_h, scaled_w)\n\n                if long_size > self.crop_size:\n                    count = torch.zeros((scaled_h, scaled_w))\n                    outputs = Variable(torch.zeros((batch_size, self.num_classes, scaled_h, scaled_w))).cuda()\n                    stride = int(ceil(self.crop_size * self.stride_rate))\n                    h_step_num = int(ceil((scaled_h - self.crop_size) / stride)) + 1\n                    w_step_num = int(ceil((scaled_w - self.crop_size) / stride)) + 1\n                    for yy in range(h_step_num):\n                        for xx in range(w_step_num):\n                            sy, sx = yy * stride, xx * stride\n                            ey, ex = sy + self.crop_size, sx + self.crop_size\n                            x_sub = scaled_x[:, :, sy: ey, sx: ex]\n                            x_sub, pad_h, pad_w = _pad(x_sub, self.crop_size)\n\n                            outputs_sub = single_forward(self, x_sub)\n\n                            if sy + self.crop_size > scaled_h:\n                                outputs_sub = outputs_sub[:, :, : -pad_h, :]\n\n                            if sx + self.crop_size > scaled_w:\n                                outputs_sub = outputs_sub[:, :, :, : -pad_w]\n\n                            outputs[:, :, sy: ey, sx: ex] = outputs_sub\n\n                            count[sy: ey, sx: ex] += 1\n                    count = Variable(count).cuda()\n                    outputs = (outputs / count)\n                else:\n                    scaled_x, pad_h, pad_w = _pad(scaled_x, self.crop_size)\n                    outputs = single_forward(self, scaled_x)\n                    outputs = outputs[:, :, : -pad_h, : -pad_w]\n                outputs_all_scales += outputs\n            return outputs_all_scales\n\n    return wrapper\n"""
utils/transforms.py,1,"b'import random\n\nimport numpy as np\nfrom skimage.filters import gaussian\nimport torch\nfrom PIL import Image, ImageFilter\n\n\nclass RandomVerticalFlip(object):\n    def __call__(self, img):\n        if random.random() < 0.5:\n            return img.transpose(Image.FLIP_TOP_BOTTOM)\n        return img\n\n\nclass DeNormalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        for t, m, s in zip(tensor, self.mean, self.std):\n            t.mul_(s).add_(m)\n        return tensor\n\n\nclass MaskToTensor(object):\n    def __call__(self, img):\n        return torch.from_numpy(np.array(img, dtype=np.int32)).long()\n\n\nclass FreeScale(object):\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        self.size = tuple(reversed(size))  # size: (h, w)\n        self.interpolation = interpolation\n\n    def __call__(self, img):\n        return img.resize(self.size, self.interpolation)\n\n\nclass FlipChannels(object):\n    def __call__(self, img):\n        img = np.array(img)[:, :, ::-1]\n        return Image.fromarray(img.astype(np.uint8))\n\n\nclass RandomGaussianBlur(object):\n    def __call__(self, img):\n        sigma = 0.15 + random.random() * 1.15\n        blurred_img = gaussian(np.array(img), sigma=sigma, multichannel=True)\n        blurred_img *= 255\n        return Image.fromarray(blurred_img.astype(np.uint8))\n'"
cocoapi/PythonAPI/main.py,0,"b'from pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\nimport numpy as np\nimport skimage.io as io\nimport json\nimport os\n\nannType = [\'segm\',\'bbox\',\'keypoints\']\nannType = annType[2]      #specify type here\nprefix = \'person_keypoints\' if annType==\'keypoints\' else \'instances\'\nprint(\'Running demo for *%s* results.\'%(annType))\n\nPATH_PREFIX = ""./txts/scale2""\n\nannFile = os.path.join(PATH_PREFIX, ""result-gt-scale2-100-json.txt"")\ncocoGt=COCO(annFile)\n\nresFile = os.path.join(PATH_PREFIX, ""result-pred-scale2-100-json.txt"")\n\ncocoDt=cocoGt.loadRes(resFile)\nimgIds=sorted(cocoGt.getImgIds())\n\ncocoEval = COCOeval(cocoGt,cocoDt,annType)\ncocoEval.params.imgIds = imgIds\ncocoEval.evaluate()\ncocoEval.accumulate()\ncocoEval.summarize()\n'"
cocoapi/PythonAPI/mul-main.py,0,"b'from pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\nimport numpy as np\nimport skimage.io as io\nimport json\nimport os\n\nannType = [\'segm\',\'bbox\',\'keypoints\']\nannType = annType[2]      #specify type here\nprefix = \'person_keypoints\' if annType==\'keypoints\' else \'instances\'\nprint(\'Running demo for *%s* results.\'%(annType))\n\nPATH_PREFIX = ""./txts/scale2""\n\nfor i in range(0,280, 10):\n\tprint(""i=%d"" % i)\n\tannFile = os.path.join(PATH_PREFIX, \'result-gt-scale2-{}-json.txt\'.format(i))\n\tresFile = os.path.join(PATH_PREFIX, \'result-pred-scale2-{}-json.txt\'.format(i))\n\n\t# annFile = os.path.join(PATH_PREFIX, ""result-gt-json.txt"")\n\tcocoGt=COCO(annFile)\n\n\t# resFile = os.path.join(PATH_PREFIX, ""result-pred-json.txt"")\n\n\tcocoDt=cocoGt.loadRes(resFile)\n\timgIds=sorted(cocoGt.getImgIds())\n\n\tcocoEval = COCOeval(cocoGt,cocoDt,annType)\n\tcocoEval.params.imgIds = imgIds\n\tcocoEval.evaluate()\n\tcocoEval.accumulate()\n\tcocoEval.summarize()\n\n\tprint(""\\n\\n\\n"")\n'"
cocoapi/PythonAPI/setup.py,0,"b'from distutils.core import setup\nfrom Cython.Build import cythonize\nfrom distutils.extension import Extension\nimport numpy as np\n\n# To compile and install locally run ""python setup.py build_ext --inplace""\n# To install library to Python site-packages run ""python setup.py build_ext install""\n\next_modules = [\n    Extension(\n        \'pycocotools._mask\',\n        sources=[\'../common/maskApi.c\', \'pycocotools/_mask.pyx\'],\n        include_dirs = [np.get_include(), \'../common\'],\n        extra_compile_args=[\'-Wno-cpp\', \'-Wno-unused-function\', \'-std=c99\'],\n    )\n]\n\nsetup(name=\'pycocotools\',\n      packages=[\'pycocotools\'],\n      package_dir = {\'pycocotools\': \'pycocotools\'},\n      version=\'2.0\',\n      ext_modules=\n          cythonize(ext_modules)\n      )'"
networks/senet/__init__.py,0,b'from .se_inception import *\nfrom .se_module import *\nfrom .se_resnet import *\nfrom .baseline import * '
networks/senet/baseline.py,1,"b'""""""\nResNet for CIFAR dataset proposed in He+15, p 7. and\nhttps://github.com/facebook/fb.resnet.torch/blob/master/models/resnet.lua\n""""""\n\nimport torch.nn as nn\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, inplanes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        if inplanes != planes:\n            self.downsample = nn.Sequential(nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False),\n                                            nn.BatchNorm2d(planes))\n        else:\n            self.downsample = lambda x: x\n        self.stride = stride\n\n    def forward(self, x):\n        residual = self.downsample(x)\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass PreActBasicBlock(BasicBlock):\n    def __init__(self, inplanes, planes, stride):\n        super(PreActBasicBlock, self).__init__(inplanes, planes, stride)\n        if inplanes != planes:\n            self.downsample = nn.Sequential(nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False))\n        else:\n            self.downsample = lambda x: x\n        self.bn1 = nn.BatchNorm2d(inplanes)\n\n    def forward(self, x):\n        residual = self.downsample(x)\n        out = self.bn1(x)\n        out = self.relu(out)\n        out = self.conv1(out)\n\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n\n        out += residual\n\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, n_size, num_classes=10):\n        super(ResNet, self).__init__()\n        self.inplane = 16\n        self.conv1 = nn.Conv2d(3, self.inplane, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(self.inplane)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_layer(block, 16, blocks=n_size, stride=1)\n        self.layer2 = self._make_layer(block, 32, blocks=n_size, stride=2)\n        self.layer3 = self._make_layer(block, 64, blocks=n_size, stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(64, num_classes)\n\n        self.initialize()\n\n    def initialize(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride):\n\n        strides = [stride] + [1] * (blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.inplane, planes, stride))\n            self.inplane = planes\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\nclass PreActResNet(ResNet):\n    def __init__(self, block, n_size, num_classes=10):\n        super(PreActResNet, self).__init__(block, n_size, num_classes)\n\n        self.bn1 = nn.BatchNorm2d(self.inplane)\n        self.initialize()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef resnet20(**kwargs):\n    model = ResNet(BasicBlock, 3, **kwargs)\n    return model\n\n\ndef resnet32(**kwargs):\n    model = ResNet(BasicBlock, 5, **kwargs)\n    return model\n\n\ndef resnet56(**kwargs):\n    model = ResNet(BasicBlock, 9, **kwargs)\n    return model\n\n\ndef resnet110(**kwargs):\n    model = ResNet(BasicBlock, 18, **kwargs)\n    return model\n\n\ndef preact_resnet20(**kwargs):\n    model = PreActResNet(PreActBasicBlock, 3, **kwargs)\n    return model\n\n\ndef preact_resnet32(**kwargs):\n    model = PreActResNet(PreActBasicBlock, 5, **kwargs)\n    return model\n\n\ndef preact_resnet56(**kwargs):\n    model = PreActResNet(PreActBasicBlock, 9, **kwargs)\n    return model\n\n\ndef preact_resnet110(**kwargs):\n    model = PreActResNet(PreActBasicBlock, 18, **kwargs)\n    return model\n'"
networks/senet/se_inception.py,0,"b'from .se_module import SELayer\nfrom torch import nn\nfrom torchvision.models.inception import Inception3\n\n\nclass SEInception3(nn.Module):\n    def __init__(self, num_classes, aux_logits=True, transform_input=False):\n        super(SEInception3, self).__init__()\n        model = Inception3(num_classes=num_classes, aux_logits=aux_logits,\n                           transform_input=transform_input)\n        model.Mixed_5b.add_module(""SELayer"", SELayer(192))\n        model.Mixed_5c.add_module(""SELayer"", SELayer(256))\n        model.Mixed_5d.add_module(""SELayer"", SELayer(288))\n        model.Mixed_6a.add_module(""SELayer"", SELayer(288))\n        model.Mixed_6b.add_module(""SELayer"", SELayer(768))\n        model.Mixed_6c.add_module(""SELayer"", SELayer(768))\n        model.Mixed_6d.add_module(""SELayer"", SELayer(768))\n        model.Mixed_6e.add_module(""SELayer"", SELayer(768))\n        if aux_logits:\n            model.AuxLogits.add_module(""SELayer"", SELayer(768))\n        model.Mixed_7a.add_module(""SELayer"", SELayer(768))\n        model.Mixed_7b.add_module(""SELayer"", SELayer(1280))\n        model.Mixed_7c.add_module(""SELayer"", SELayer(2048))\n\n        self.model = model\n\n    def forward(self, x):\n        _, _, h, w = x.size()\n        if (h, w) != (299, 299):\n            raise ValueError(""input size must be (299, 299)"")\n\n        return self.model(x)\n\n\ndef se_inception_v3(**kwargs):\n    return SEInception3(**kwargs)\n'"
networks/senet/se_module.py,0,"b'from torch import nn\n\n\nclass SELayer(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y.expand_as(x)\n'"
networks/senet/se_resnet.py,1,"b'import torch.nn as nn\nfrom ..resnet import *\nfrom .se_module import SELayer\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n\n\nclass SEBasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, reduction=16):\n        super(SEBasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes, 1)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.se = SELayer(planes, reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.se(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, reduction=16):\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se = SELayer(planes * 4, reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n        out = self.se(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\ndef se_resnet18(num_classes):\n    """"""Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(SEBasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n    model.avgpool = nn.AdaptiveAvgPool2d(1)\n    return model\n\ndef senet18_ed(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(SEBasicBlock, [2, 2, 2, 2], **kwargs)\n    model = nn.Sequential(*(list(model.children())[:-2]))\n    for param in model.parameters():\n        param.requires_grad = True\n\n    return model\n\n\ndef se_resnet34(num_classes):\n    """"""Constructs a ResNet-34 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(SEBasicBlock, [3, 4, 6, 3], num_classes=num_classes)\n    model.avgpool = nn.AdaptiveAvgPool2d(1)\n    return model\n\n\ndef se_resnet50(num_classes):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(SEBottleneck, [3, 4, 6, 3], num_classes=num_classes)\n    model.avgpool = nn.AdaptiveAvgPool2d(1)\n    return model\n\n\ndef se_resnet101(num_classes):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(SEBottleneck, [3, 4, 23, 3], num_classes=num_classes)\n    model.avgpool = nn.AdaptiveAvgPool2d(1)\n    return model\n\n\ndef se_resnet152(num_classes):\n    """"""Constructs a ResNet-152 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(SEBottleneck, [3, 8, 36, 3], num_classes=num_classes)\n    model.avgpool = nn.AdaptiveAvgPool2d(1)\n    return model\n\n\nclass CifarSEBasicBlock(nn.Module):\n    def __init__(self, inplanes, planes, stride=1, reduction=16):\n        super(CifarSEBasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.se = SELayer(planes, reduction)\n        if inplanes != planes:\n            self.downsample = nn.Sequential(nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False),\n                                            nn.BatchNorm2d(planes))\n        else:\n            self.downsample = lambda x: x\n        self.stride = stride\n\n    def forward(self, x):\n        residual = self.downsample(x)\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.se(out)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass CifarSEResNet(nn.Module):\n    def __init__(self, block, n_size, num_classes=10, reduction=16):\n        super(CifarSEResNet, self).__init__()\n        self.inplane = 16\n        self.conv1 = nn.Conv2d(3, self.inplane, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(self.inplane)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_layer(block, 16, blocks=n_size, stride=1, reduction=reduction)\n        self.layer2 = self._make_layer(block, 32, blocks=n_size, stride=2, reduction=reduction)\n        self.layer3 = self._make_layer(block, 64, blocks=n_size, stride=2, reduction=reduction)\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(64, num_classes)\n        self.initialize()\n\n    def initialize(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride, reduction):\n        strides = [stride] + [1] * (blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.inplane, planes, stride, reduction))\n            self.inplane = planes\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\nclass CifarSEPreActResNet(CifarSEResNet):\n    def __init__(self, block, n_size, num_classes=10, reduction=16):\n        super(CifarSEPreActResNet, self).__init__(block, n_size, num_classes, reduction)\n        self.bn1 = nn.BatchNorm2d(self.inplane)\n        self.initialize()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n\ndef se_resnet20(**kwargs):\n    """"""Constructs a ResNet-18 model.\n\n    """"""\n    model = CifarSEResNet(CifarSEBasicBlock, 3, **kwargs)\n    return model\n\n\ndef se_resnet32(**kwargs):\n    """"""Constructs a ResNet-34 model.\n\n    """"""\n    model = CifarSEResNet(CifarSEBasicBlock, 5, **kwargs)\n    return model\n\n\ndef se_resnet56(**kwargs):\n    """"""Constructs a ResNet-34 model.\n\n    """"""\n    model = CifarSEResNet(CifarSEBasicBlock, 9, **kwargs)\n    return model\n\n\ndef se_preactresnet20(**kwargs):\n    """"""Constructs a ResNet-18 model.\n\n    """"""\n    model = CifarSEPreActResNet(CifarSEBasicBlock, 3, **kwargs)\n    return model\n\n\ndef se_preactresnet32(**kwargs):\n    """"""Constructs a ResNet-34 model.\n\n    """"""\n    model = CifarSEPreActResNet(CifarSEBasicBlock, 5, **kwargs)\n    return model\n\n\ndef se_preactresnet56(**kwargs):\n    """"""Constructs a ResNet-34 model.\n\n    """"""\n    model = CifarSEPreActResNet(CifarSEBasicBlock, 9, **kwargs)\n    return model\n'"
cocoapi/PythonAPI/pycocotools/__init__.py,0,"b""__author__ = 'tylin'\n"""
cocoapi/PythonAPI/pycocotools/coco.py,0,"b'__author__ = \'tylin\'\n__version__ = \'2.0\'\n# Interface for accessing the Microsoft COCO dataset.\n\n# Microsoft COCO is a large image dataset designed for object detection,\n# segmentation, and caption generation. pycocotools is a Python API that\n# assists in loading, parsing and visualizing the annotations in COCO.\n# Please visit http://mscoco.org/ for more information on COCO, including\n# for the data, paper, and tutorials. The exact format of the annotations\n# is also described on the COCO website. For example usage of the pycocotools\n# please see pycocotools_demo.ipynb. In addition to this API, please download both\n# the COCO images and annotations in order to run the demo.\n\n# An alternative to using the API is to load the annotations directly\n# into Python dictionary\n# Using the API provides additional utility functions. Note that this API\n# supports both *instance* and *caption* annotations. In the case of\n# captions not all functions are defined (e.g. categories are undefined).\n\n# The following API functions are defined:\n#  COCO       - COCO api class that loads COCO annotation file and prepare data structures.\n#  decodeMask - Decode binary mask M encoded via run-length encoding.\n#  encodeMask - Encode binary mask M using run-length encoding.\n#  getAnnIds  - Get ann ids that satisfy given filter conditions.\n#  getCatIds  - Get cat ids that satisfy given filter conditions.\n#  getImgIds  - Get img ids that satisfy given filter conditions.\n#  loadAnns   - Load anns with the specified ids.\n#  loadCats   - Load cats with the specified ids.\n#  loadImgs   - Load imgs with the specified ids.\n#  annToMask  - Convert segmentation in an annotation to binary mask.\n#  showAnns   - Display the specified annotations.\n#  loadRes    - Load algorithm results and create API for accessing them.\n#  download   - Download COCO images from mscoco.org server.\n# Throughout the API ""ann""=annotation, ""cat""=category, and ""img""=image.\n# Help on each functions can be accessed by: ""help COCO>function"".\n\n# See also COCO>decodeMask,\n# COCO>encodeMask, COCO>getAnnIds, COCO>getCatIds,\n# COCO>getImgIds, COCO>loadAnns, COCO>loadCats,\n# COCO>loadImgs, COCO>annToMask, COCO>showAnns\n\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2014.\n# Licensed under the Simplified BSD License [see bsd.txt]\n\nimport json\nimport time\nimport matplotlib.pyplot as plt\nfrom shapely.geometry import Polygon\n\n#from matplotlib.collections import PatchCollection\n#from matplotlib.patches import Polygon\nimport numpy as np\nimport copy\nimport itertools\nfrom . import mask as maskUtils\nimport os\nfrom collections import defaultdict\nimport sys\nPYTHON_VERSION = sys.version_info[0]\nif PYTHON_VERSION == 2:\n    from urllib import urlretrieve\nelif PYTHON_VERSION == 3:\n    from urllib.request import urlretrieve\n\n\ndef _isArrayLike(obj):\n    return hasattr(obj, \'__iter__\') and hasattr(obj, \'__len__\')\n\n\nclass COCO:\n    def __init__(self, annotation_file=None):\n        """"""\n        Constructor of Microsoft COCO helper class for reading and visualizing annotations.\n        :param annotation_file (str): location of annotation file\n        :param image_folder (str): location to the folder that hosts images.\n        :return:\n        """"""\n        # load dataset\n        self.dataset,self.anns,self.cats,self.imgs = dict(),dict(),dict(),dict()\n        self.imgToAnns, self.catToImgs = defaultdict(list), defaultdict(list)\n        if not annotation_file == None:\n            print(\'loading annotations into memory...\')\n            tic = time.time()\n            dataset = json.load(open(annotation_file, \'r\'))\n            assert type(dataset)==dict, \'annotation file format {} not supported\'.format(type(dataset))\n            print(\'Done (t={:0.2f}s)\'.format(time.time()- tic))\n            self.dataset = dataset\n            self.createIndex()\n\n    def createIndex(self):\n        # create index\n        print(\'creating index...\')\n        anns, cats, imgs = {}, {}, {}\n        imgToAnns,catToImgs = defaultdict(list),defaultdict(list)\n        if \'annotations\' in self.dataset:\n            for ann in self.dataset[\'annotations\']:\n                imgToAnns[ann[\'image_id\']].append(ann)\n                anns[ann[\'id\']] = ann\n\n        if \'images\' in self.dataset:\n            for img in self.dataset[\'images\']:\n                imgs[img[\'id\']] = img\n\n        if \'categories\' in self.dataset:\n            for cat in self.dataset[\'categories\']:\n                cats[cat[\'id\']] = cat\n\n        if \'annotations\' in self.dataset and \'categories\' in self.dataset:\n            for ann in self.dataset[\'annotations\']:\n                catToImgs[ann[\'category_id\']].append(ann[\'image_id\'])\n\n        print(\'index created!\')\n\n        # create class members\n        self.anns = anns\n        self.imgToAnns = imgToAnns\n        self.catToImgs = catToImgs\n        self.imgs = imgs\n        self.cats = cats\n\n    def info(self):\n        """"""\n        Print information about the annotation file.\n        :return:\n        """"""\n        for key, value in self.dataset[\'info\'].items():\n            print(\'{}: {}\'.format(key, value))\n\n    def getAnnIds(self, imgIds=[], catIds=[], areaRng=[], iscrowd=None):\n        """"""\n        Get ann ids that satisfy given filter conditions. default skips that filter\n        :param imgIds  (int array)     : get anns for given imgs\n               catIds  (int array)     : get anns for given cats\n               areaRng (float array)   : get anns for given area range (e.g. [0 inf])\n               iscrowd (boolean)       : get anns for given crowd label (False or True)\n        :return: ids (int array)       : integer array of ann ids\n        """"""\n        imgIds = imgIds if _isArrayLike(imgIds) else [imgIds]\n        catIds = catIds if _isArrayLike(catIds) else [catIds]\n\n        if len(imgIds) == len(catIds) == len(areaRng) == 0:\n            anns = self.dataset[\'annotations\']\n        else:\n            if not len(imgIds) == 0:\n                lists = [self.imgToAnns[imgId] for imgId in imgIds if imgId in self.imgToAnns]\n                anns = list(itertools.chain.from_iterable(lists))\n            else:\n                anns = self.dataset[\'annotations\']\n            anns = anns if len(catIds)  == 0 else [ann for ann in anns if ann[\'category_id\'] in catIds]\n            anns = anns if len(areaRng) == 0 else [ann for ann in anns if ann[\'area\'] > areaRng[0] and ann[\'area\'] < areaRng[1]]\n        if not iscrowd == None:\n            ids = [ann[\'id\'] for ann in anns if ann[\'iscrowd\'] == iscrowd]\n        else:\n            ids = [ann[\'id\'] for ann in anns]\n        return ids\n\n    def getCatIds(self, catNms=[], supNms=[], catIds=[]):\n        """"""\n        filtering parameters. default skips that filter.\n        :param catNms (str array)  : get cats for given cat names\n        :param supNms (str array)  : get cats for given supercategory names\n        :param catIds (int array)  : get cats for given cat ids\n        :return: ids (int array)   : integer array of cat ids\n        """"""\n        catNms = catNms if _isArrayLike(catNms) else [catNms]\n        supNms = supNms if _isArrayLike(supNms) else [supNms]\n        catIds = catIds if _isArrayLike(catIds) else [catIds]\n\n        if len(catNms) == len(supNms) == len(catIds) == 0:\n            cats = self.dataset[\'categories\']\n        else:\n            cats = self.dataset[\'categories\']\n            cats = cats if len(catNms) == 0 else [cat for cat in cats if cat[\'name\']          in catNms]\n            cats = cats if len(supNms) == 0 else [cat for cat in cats if cat[\'supercategory\'] in supNms]\n            cats = cats if len(catIds) == 0 else [cat for cat in cats if cat[\'id\']            in catIds]\n        ids = [cat[\'id\'] for cat in cats]\n        return ids\n\n    def getImgIds(self, imgIds=[], catIds=[]):\n        \'\'\'\n        Get img ids that satisfy given filter conditions.\n        :param imgIds (int array) : get imgs for given ids\n        :param catIds (int array) : get imgs with all given cats\n        :return: ids (int array)  : integer array of img ids\n        \'\'\'\n        imgIds = imgIds if _isArrayLike(imgIds) else [imgIds]\n        catIds = catIds if _isArrayLike(catIds) else [catIds]\n\n        if len(imgIds) == len(catIds) == 0:\n            ids = self.imgs.keys()\n        else:\n            ids = set(imgIds)\n            for i, catId in enumerate(catIds):\n                if i == 0 and len(ids) == 0:\n                    ids = set(self.catToImgs[catId])\n                else:\n                    ids &= set(self.catToImgs[catId])\n        return list(ids)\n\n    def loadAnns(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying anns\n        :return: anns (object array) : loaded ann objects\n        """"""\n        if _isArrayLike(ids):\n            return [self.anns[id] for id in ids]\n        elif type(ids) == int:\n            return [self.anns[ids]]\n\n    def loadCats(self, ids=[]):\n        """"""\n        Load cats with the specified ids.\n        :param ids (int array)       : integer ids specifying cats\n        :return: cats (object array) : loaded cat objects\n        """"""\n        if _isArrayLike(ids):\n            return [self.cats[id] for id in ids]\n        elif type(ids) == int:\n            return [self.cats[ids]]\n\n    def loadImgs(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying img\n        :return: imgs (object array) : loaded img objects\n        """"""\n        if _isArrayLike(ids):\n            return [self.imgs[id] for id in ids]\n        elif type(ids) == int:\n            return [self.imgs[ids]]\n\n    def showAnns(self, anns):\n        """"""\n        Display the specified annotations.\n        :param anns (array of object): annotations to display\n        :return: None\n        """"""\n        if len(anns) == 0:\n            return 0\n        if \'segmentation\' in anns[0] or \'keypoints\' in anns[0]:\n            datasetType = \'instances\'\n        elif \'caption\' in anns[0]:\n            datasetType = \'captions\'\n        else:\n            raise Exception(\'datasetType not supported\')\n        if datasetType == \'instances\':\n            ax = plt.gca()\n            ax.set_autoscale_on(False)\n            polygons = []\n            color = []\n            for ann in anns:\n                c = (np.random.random((1, 3))*0.6+0.4).tolist()[0]\n                if \'segmentation\' in ann:\n                    if type(ann[\'segmentation\']) == list:\n                        # polygon\n                        for seg in ann[\'segmentation\']:\n                            poly = np.array(seg).reshape((int(len(seg)/2), 2))\n                            polygons.append(Polygon(poly))\n                            color.append(c)\n                    else:\n                        # mask\n                        t = self.imgs[ann[\'image_id\']]\n                        if type(ann[\'segmentation\'][\'counts\']) == list:\n                            rle = maskUtils.frPyObjects([ann[\'segmentation\']], t[\'height\'], t[\'width\'])\n                        else:\n                            rle = [ann[\'segmentation\']]\n                        m = maskUtils.decode(rle)\n                        img = np.ones( (m.shape[0], m.shape[1], 3) )\n                        if ann[\'iscrowd\'] == 1:\n                            color_mask = np.array([2.0,166.0,101.0])/255\n                        if ann[\'iscrowd\'] == 0:\n                            color_mask = np.random.random((1, 3)).tolist()[0]\n                        for i in range(3):\n                            img[:,:,i] = color_mask[i]\n                        ax.imshow(np.dstack( (img, m*0.5) ))\n                if \'keypoints\' in ann and type(ann[\'keypoints\']) == list:\n                    # turn skeleton into zero-based index\n                    sks = np.array(self.loadCats(ann[\'category_id\'])[0][\'skeleton\'])-1\n                    kp = np.array(ann[\'keypoints\'])\n                    x = kp[0::3]\n                    y = kp[1::3]\n                    v = kp[2::3]\n                    for sk in sks:\n                        if np.all(v[sk]>0):\n                            plt.plot(x[sk],y[sk], linewidth=3, color=c)\n                    plt.plot(x[v>0], y[v>0],\'o\',markersize=8, markerfacecolor=c, markeredgecolor=\'k\',markeredgewidth=2)\n                    plt.plot(x[v>1], y[v>1],\'o\',markersize=8, markerfacecolor=c, markeredgecolor=c, markeredgewidth=2)\n            p = PatchCollection(polygons, facecolor=color, linewidths=0, alpha=0.4)\n            ax.add_collection(p)\n            p = PatchCollection(polygons, facecolor=\'none\', edgecolors=color, linewidths=2)\n            ax.add_collection(p)\n        elif datasetType == \'captions\':\n            for ann in anns:\n                print(ann[\'caption\'])\n\n    def loadRes(self, resFile):\n        """"""\n        Load result file and return a result api object.\n        :param   resFile (str)     : file name of result file\n        :return: res (obj)         : result api object\n        """"""\n        res = COCO()\n        res.dataset[\'images\'] = [img for img in self.dataset[\'images\']]\n\n        print(\'Loading and preparing results...\')\n        tic = time.time()\n        if type(resFile) == str or type(resFile) == unicode:\n            anns = json.load(open(resFile))\n        elif type(resFile) == np.ndarray:\n            anns = self.loadNumpyAnnotations(resFile)\n        else:\n            anns = resFile\n        assert type(anns) == list, \'results in not an array of objects\'\n        annsImgIds = [ann[\'image_id\'] for ann in anns]\n        assert set(annsImgIds) == (set(annsImgIds) & set(self.getImgIds())), \\\n               \'Results do not correspond to current coco set\'\n        if \'caption\' in anns[0]:\n            imgIds = set([img[\'id\'] for img in res.dataset[\'images\']]) & set([ann[\'image_id\'] for ann in anns])\n            res.dataset[\'images\'] = [img for img in res.dataset[\'images\'] if img[\'id\'] in imgIds]\n            for id, ann in enumerate(anns):\n                ann[\'id\'] = id+1\n        elif \'bbox\' in anns[0] and not anns[0][\'bbox\'] == []:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                bb = ann[\'bbox\']\n                x1, x2, y1, y2 = [bb[0], bb[0]+bb[2], bb[1], bb[1]+bb[3]]\n                if not \'segmentation\' in ann:\n                    ann[\'segmentation\'] = [[x1, y1, x1, y2, x2, y2, x2, y1]]\n                ann[\'area\'] = bb[2]*bb[3]\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        elif \'segmentation\' in anns[0]:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                # now only support compressed RLE format as segmentation results\n                ann[\'area\'] = maskUtils.area(ann[\'segmentation\'])\n                if not \'bbox\' in ann:\n                    ann[\'bbox\'] = maskUtils.toBbox(ann[\'segmentation\'])\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        elif \'keypoints\' in anns[0]:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                s = ann[\'keypoints\']\n                x = s[0::3]\n                y = s[1::3]\n                x0,x1,y0,y1 = np.min(x), np.max(x), np.min(y), np.max(y)\n                ann[\'area\'] = (x1-x0)*(y1-y0)\n                ann[\'id\'] = id + 1\n                ann[\'bbox\'] = [x0,y0,x1-x0,y1-y0]\n        print(\'DONE (t={:0.2f}s)\'.format(time.time()- tic))\n\n        res.dataset[\'annotations\'] = anns\n        res.createIndex()\n        return res\n\n    def download(self, tarDir = None, imgIds = [] ):\n        \'\'\'\n        Download COCO images from mscoco.org server.\n        :param tarDir (str): COCO results directory name\n               imgIds (list): images to be downloaded\n        :return:\n        \'\'\'\n        if tarDir is None:\n            print(\'Please specify target directory\')\n            return -1\n        if len(imgIds) == 0:\n            imgs = self.imgs.values()\n        else:\n            imgs = self.loadImgs(imgIds)\n        N = len(imgs)\n        if not os.path.exists(tarDir):\n            os.makedirs(tarDir)\n        for i, img in enumerate(imgs):\n            tic = time.time()\n            fname = os.path.join(tarDir, img[\'file_name\'])\n            if not os.path.exists(fname):\n                urlretrieve(img[\'coco_url\'], fname)\n            print(\'downloaded {}/{} images (t={:0.1f}s)\'.format(i, N, time.time()- tic))\n\n    def loadNumpyAnnotations(self, data):\n        """"""\n        Convert result data from a numpy array [Nx7] where each row contains {imageID,x1,y1,w,h,score,class}\n        :param  data (numpy.ndarray)\n        :return: annotations (python nested list)\n        """"""\n        print(\'Converting ndarray to lists...\')\n        assert(type(data) == np.ndarray)\n        print(data.shape)\n        assert(data.shape[1] == 7)\n        N = data.shape[0]\n        ann = []\n        for i in range(N):\n            if i % 1000000 == 0:\n                print(\'{}/{}\'.format(i,N))\n            ann += [{\n                \'image_id\'  : int(data[i, 0]),\n                \'bbox\'  : [ data[i, 1], data[i, 2], data[i, 3], data[i, 4] ],\n                \'score\' : data[i, 5],\n                \'category_id\': int(data[i, 6]),\n                }]\n        return ann\n\n    def annToRLE(self, ann):\n        """"""\n        Convert annotation which can be polygons, uncompressed RLE to RLE.\n        :return: binary mask (numpy 2D array)\n        """"""\n        t = self.imgs[ann[\'image_id\']]\n        h, w = t[\'height\'], t[\'width\']\n        segm = ann[\'segmentation\']\n        if type(segm) == list:\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            rles = maskUtils.frPyObjects(segm, h, w)\n            rle = maskUtils.merge(rles)\n        elif type(segm[\'counts\']) == list:\n            # uncompressed RLE\n            rle = maskUtils.frPyObjects(segm, h, w)\n        else:\n            # rle\n            rle = ann[\'segmentation\']\n        return rle\n\n    def annToMask(self, ann):\n        """"""\n        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n        :return: binary mask (numpy 2D array)\n        """"""\n        rle = self.annToRLE(ann)\n        m = maskUtils.decode(rle)\n        return m\n'"
cocoapi/PythonAPI/pycocotools/cocoeval.py,0,"b'__author__ = \'tsungyi\'\n\nimport numpy as np\nimport datetime\nimport time\nfrom collections import defaultdict\nfrom . import mask as maskUtils\nimport copy\n\nclass COCOeval:\n    # Interface for evaluating detection on the Microsoft COCO dataset.\n    #\n    # The usage for CocoEval is as follows:\n    #  cocoGt=..., cocoDt=...       # load dataset and results\n    #  E = CocoEval(cocoGt,cocoDt); # initialize CocoEval object\n    #  E.params.recThrs = ...;      # set parameters as desired\n    #  E.evaluate();                # run per image evaluation\n    #  E.accumulate();              # accumulate per image results\n    #  E.summarize();               # display summary metrics of results\n    # For example usage see evalDemo.m and http://mscoco.org/.\n    #\n    # The evaluation parameters are as follows (defaults in brackets):\n    #  imgIds     - [all] N img ids to use for evaluation\n    #  catIds     - [all] K cat ids to use for evaluation\n    #  iouThrs    - [.5:.05:.95] T=10 IoU thresholds for evaluation\n    #  recThrs    - [0:.01:1] R=101 recall thresholds for evaluation\n    #  areaRng    - [...] A=4 object area ranges for evaluation\n    #  maxDets    - [1 10 100] M=3 thresholds on max detections per image\n    #  iouType    - [\'segm\'] set iouType to \'segm\', \'bbox\' or \'keypoints\'\n    #  iouType replaced the now DEPRECATED useSegm parameter.\n    #  useCats    - [1] if true use category labels for evaluation\n    # Note: if useCats=0 category labels are ignored as in proposal scoring.\n    # Note: multiple areaRngs [Ax2] and maxDets [Mx1] can be specified.\n    #\n    # evaluate(): evaluates detections on every image and every category and\n    # concats the results into the ""evalImgs"" with fields:\n    #  dtIds      - [1xD] id for each of the D detections (dt)\n    #  gtIds      - [1xG] id for each of the G ground truths (gt)\n    #  dtMatches  - [TxD] matching gt id at each IoU or 0\n    #  gtMatches  - [TxG] matching dt id at each IoU or 0\n    #  dtScores   - [1xD] confidence of each dt\n    #  gtIgnore   - [1xG] ignore flag for each gt\n    #  dtIgnore   - [TxD] ignore flag for each dt at each IoU\n    #\n    # accumulate(): accumulates the per-image, per-category evaluation\n    # results in ""evalImgs"" into the dictionary ""eval"" with fields:\n    #  params     - parameters used for evaluation\n    #  date       - date evaluation was performed\n    #  counts     - [T,R,K,A,M] parameter dimensions (see above)\n    #  precision  - [TxRxKxAxM] precision for every evaluation setting\n    #  recall     - [TxKxAxM] max recall for every evaluation setting\n    # Note: precision and recall==-1 for settings with no gt objects.\n    #\n    # See also coco, mask, pycocoDemo, pycocoEvalDemo\n    #\n    # Microsoft COCO Toolbox.      version 2.0\n    # Data, paper, and tutorials available at:  http://mscoco.org/\n    # Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n    # Licensed under the Simplified BSD License [see coco/license.txt]\n    def __init__(self, cocoGt=None, cocoDt=None, iouType=\'segm\'):\n        \'\'\'\n        Initialize CocoEval using coco APIs for gt and dt\n        :param cocoGt: coco object with ground truth annotations\n        :param cocoDt: coco object with detection results\n        :return: None\n        \'\'\'\n        if not iouType:\n            print(\'iouType not specified. use default iouType segm\')\n        self.cocoGt   = cocoGt              # ground truth COCO API\n        self.cocoDt   = cocoDt              # detections COCO API\n        self.params   = {}                  # evaluation parameters\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results [KxAxI] elements\n        self.eval     = {}                  # accumulated evaluation results\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        self.params = Params(iouType=iouType) # parameters\n        self._paramsEval = {}               # parameters for evaluation\n        self.stats = []                     # result summarization\n        self.ious = {}                      # ious between all gts and dts\n        if not cocoGt is None:\n            self.params.imgIds = sorted(cocoGt.getImgIds())\n            self.params.catIds = sorted(cocoGt.getCatIds())\n\n\n    def _prepare(self):\n        \'\'\'\n        Prepare ._gts and ._dts for evaluation based on params\n        :return: None\n        \'\'\'\n        def _toMask(anns, coco):\n            # modify ann[\'segmentation\'] by reference\n            for ann in anns:\n                rle = coco.annToRLE(ann)\n                ann[\'segmentation\'] = rle\n        p = self.params\n        if p.useCats:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n        else:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds))\n\n        # convert ground truth to mask if iouType == \'segm\'\n        if p.iouType == \'segm\':\n            _toMask(gts, self.cocoGt)\n            _toMask(dts, self.cocoDt)\n        # set ignore flag\n        for gt in gts:\n            gt[\'ignore\'] = gt[\'ignore\'] if \'ignore\' in gt else 0\n            gt[\'ignore\'] = \'iscrowd\' in gt and gt[\'iscrowd\']\n            if p.iouType == \'keypoints\':\n                gt[\'ignore\'] = (gt[\'num_keypoints\'] == 0) or gt[\'ignore\']\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        for gt in gts:\n            self._gts[gt[\'image_id\'], gt[\'category_id\']].append(gt)\n        for dt in dts:\n            self._dts[dt[\'image_id\'], dt[\'category_id\']].append(dt)\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results\n        self.eval     = {}                  # accumulated evaluation results\n\n    def evaluate(self):\n        \'\'\'\n        Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n        :return: None\n        \'\'\'\n        tic = time.time()\n        print(\'Running per image evaluation...\')\n        p = self.params\n        # add backward compatibility if useSegm is specified in params\n        if not p.useSegm is None:\n            p.iouType = \'segm\' if p.useSegm == 1 else \'bbox\'\n            print(\'useSegm (deprecated) is not None. Running {} evaluation\'.format(p.iouType))\n        print(\'Evaluate annotation type *{}*\'.format(p.iouType))\n        p.imgIds = list(np.unique(p.imgIds))\n        if p.useCats:\n            p.catIds = list(np.unique(p.catIds))\n        p.maxDets = sorted(p.maxDets)\n        self.params=p\n\n        self._prepare()\n        # loop through images, area range, max detection number\n        catIds = p.catIds if p.useCats else [-1]\n\n        if p.iouType == \'segm\' or p.iouType == \'bbox\':\n            computeIoU = self.computeIoU\n        elif p.iouType == \'keypoints\':\n            computeIoU = self.computeOks\n        self.ious = {(imgId, catId): computeIoU(imgId, catId) \\\n                        for imgId in p.imgIds\n                        for catId in catIds}\n\n        evaluateImg = self.evaluateImg\n        maxDet = p.maxDets[-1]\n        self.evalImgs = [evaluateImg(imgId, catId, areaRng, maxDet)\n                 for catId in catIds\n                 for areaRng in p.areaRng\n                 for imgId in p.imgIds\n             ]\n        self._paramsEval = copy.deepcopy(self.params)\n        toc = time.time()\n        print(\'DONE (t={:0.2f}s).\'.format(toc-tic))\n\n    def computeIoU(self, imgId, catId):\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return []\n        inds = np.argsort([-d[\'score\'] for d in dt], kind=\'mergesort\')\n        dt = [dt[i] for i in inds]\n        if len(dt) > p.maxDets[-1]:\n            dt=dt[0:p.maxDets[-1]]\n\n        if p.iouType == \'segm\':\n            g = [g[\'segmentation\'] for g in gt]\n            d = [d[\'segmentation\'] for d in dt]\n        elif p.iouType == \'bbox\':\n            g = [g[\'bbox\'] for g in gt]\n            d = [d[\'bbox\'] for d in dt]\n        else:\n            raise Exception(\'unknown iouType for iou computation\')\n\n        # compute iou between each dt and gt region\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        ious = maskUtils.iou(d,g,iscrowd)\n        return ious\n\n    def computeOks(self, imgId, catId):\n        p = self.params\n        # dimention here should be Nxm\n        gts = self._gts[imgId, catId]\n        dts = self._dts[imgId, catId]\n        inds = np.argsort([-d[\'score\'] for d in dts], kind=\'mergesort\')\n        dts = [dts[i] for i in inds]\n        if len(dts) > p.maxDets[-1]:\n            dts = dts[0:p.maxDets[-1]]\n        # if len(gts) == 0 and len(dts) == 0:\n        if len(gts) == 0 or len(dts) == 0:\n            return []\n        ious = np.zeros((len(dts), len(gts)))\n\n        mpii_part_names = [\'RAnkle\', \'RKnee\', \'RHip\', \'LHip\', \'LKnee\', \'LAnkle\',\n                           \'Pelv\', \'Thrx\', \'Neck\', \'Head\', \'RWrist\', \'RElbow\',\n                           \'RShoulder\', \'LShoulder\', \'LElbow\', \'LWrist\']\n\n        mpii_mapping_part_names = [\'RAnkle\', \'RKnee\', \'RHip\', \'LHip\', \'LKnee\', \'LAnkle\',\n                           \'LHip\', \'LHip\', \'LEar\', \'LEar\', \'RWrist\', \'RElbow\',\n                           \'RShoulder\', \'LShoulder\', \'LElbow\', \'LWrist\']\n\n        coco_part_names = [\'Nose\', \'LEye\', \'REye\', \'LEar\', \'REar\', \'LShoulder\',\n                           \'RShoulder\', \'LElbow\', \'RElbow\', \'LWrist\', \'RWrist\',\n                           \'LHip\', \'RHip\', \'LKnee\', \'RKnee\', \'LAnkle\', \'RAnkle\']\n\n\n        coco_sigmas = np.array(\n            [.26, .25, .25, .35, .35, .79,\n             .79, .72, .72, .62, .62, 1.07, 1.07,\n             .87, .87, .89, .89]) / 10.0\n\n        sigmas = np.array([])\n        for name in mpii_mapping_part_names:\n            pos = coco_part_names.index(name)\n            sigmas = np.append(sigmas, coco_sigmas[pos])\n        # for x in sigmas:\n        #     print(x,"","")\n        # print(sigmas)\n        # sigmas =  np.array([0.089,  0.087,  0.107,  0.107,  0.087,  0.089,\n        #            0.107,  0.107,  0.035,  0.035,  0.062,  0.072,\n        #            0.079,  0.079,  0.072,  0.062])\n        sigmas = np.array([0.089, 0.087, 0.107, 0.107, 0.087, 0.089,\n                           0.107, 0.107, 0.107, 0.107, 0.062, 0.072,\n                           0.079, 0.079, 0.072, 0.062])\n        # sigmas = np.array([10.089, 10.087, 10.107, 10.107, 10.087, 10.089,\n        #                    10.089, 11.026, 11.035, 10.089, 10.062, 10.072,\n        #                    10.079, 10.079, 10.072, 10.062])\n\n        vars = (sigmas * 2)**2\n        k = len(sigmas)\n        # compute oks between each detection and ground truth object\n        for j, gt in enumerate(gts):\n            # create bounds for ignore regions(double the gt bbox)\n            g = np.array(gt[\'keypoints\'])\n            xg = g[0::3]; yg = g[1::3]; vg = g[2::3]\n            k1 = np.count_nonzero(vg > 0)\n            bb = gt[\'bbox\']\n            x0 = bb[0] - bb[2]; x1 = bb[0] + bb[2] * 2\n            y0 = bb[1] - bb[3]; y1 = bb[1] + bb[3] * 2\n            for i, dt in enumerate(dts):\n                d = np.array(dt[\'keypoints\'])\n                xd = d[0::3]; yd = d[1::3]\n                if k1>0:\n                    # measure the per-keypoint distance if keypoints visible\n                    dx = xd - xg\n                    dy = yd - yg\n                else:\n                    # measure minimum distance to keypoints in (x0,y0) & (x1,y1)\n                    z = np.zeros((k))\n                    dx = np.max((z, x0-xd),axis=0)+np.max((z, xd-x1),axis=0)\n                    dy = np.max((z, y0-yd),axis=0)+np.max((z, yd-y1),axis=0)\n                e = (dx**2 + dy**2) / vars / (gt[\'area\']+np.spacing(1)) / 2\n                if k1 > 0:\n                    e=e[vg > 0]\n                ious[i, j] = np.sum(np.exp(-e)) / e.shape[0]\n        return ious\n\n    def evaluateImg(self, imgId, catId, aRng, maxDet):\n        \'\'\'\n        perform evaluation for single category and image\n        :return: dict (single image results)\n        \'\'\'\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return None\n\n        for g in gt:\n            if g[\'ignore\'] or (g[\'area\']<aRng[0] or g[\'area\']>aRng[1]):\n                g[\'_ignore\'] = 1\n            else:\n                g[\'_ignore\'] = 0\n\n        # sort dt highest score first, sort gt ignore last\n        gtind = np.argsort([g[\'_ignore\'] for g in gt], kind=\'mergesort\')\n        gt = [gt[i] for i in gtind]\n        dtind = np.argsort([-d[\'score\'] for d in dt], kind=\'mergesort\')\n        dt = [dt[i] for i in dtind[0:maxDet]]\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        # load computed ious\n        ious = self.ious[imgId, catId][:, gtind] if len(self.ious[imgId, catId]) > 0 else self.ious[imgId, catId]\n\n        T = len(p.iouThrs)\n        G = len(gt)\n        D = len(dt)\n        gtm  = np.zeros((T,G))\n        dtm  = np.zeros((T,D))\n        gtIg = np.array([g[\'_ignore\'] for g in gt])\n        dtIg = np.zeros((T,D))\n        if not len(ious)==0:\n            for tind, t in enumerate(p.iouThrs):\n                for dind, d in enumerate(dt):\n                    # information about best match so far (m=-1 -> unmatched)\n                    iou = min([t,1-1e-10])\n                    m   = -1\n                    for gind, g in enumerate(gt):\n                        # if this gt already matched, and not a crowd, continue\n                        if gtm[tind,gind]>0 and not iscrowd[gind]:\n                            continue\n                        # if dt matched to reg gt, and on ignore gt, stop\n                        if m>-1 and gtIg[m]==0 and gtIg[gind]==1:\n                            break\n                        # continue to next gt unless better match made\n                        if ious[dind,gind] < iou:\n                            continue\n                        # if match successful and best so far, store appropriately\n                        iou=ious[dind,gind]\n                        m=gind\n                    # if match made store id of match for both dt and gt\n                    if m ==-1:\n                        continue\n                    dtIg[tind,dind] = gtIg[m]\n                    dtm[tind,dind]  = gt[m][\'id\']\n                    gtm[tind,m]     = d[\'id\']\n        # set unmatched detections outside of area range to ignore\n        a = np.array([d[\'area\']<aRng[0] or d[\'area\']>aRng[1] for d in dt]).reshape((1, len(dt)))\n        dtIg = np.logical_or(dtIg, np.logical_and(dtm==0, np.repeat(a,T,0)))\n        # store results for given image and category\n        return {\n                \'image_id\':     imgId,\n                \'category_id\':  catId,\n                \'aRng\':         aRng,\n                \'maxDet\':       maxDet,\n                \'dtIds\':        [d[\'id\'] for d in dt],\n                \'gtIds\':        [g[\'id\'] for g in gt],\n                \'dtMatches\':    dtm,\n                \'gtMatches\':    gtm,\n                \'dtScores\':     [d[\'score\'] for d in dt],\n                \'gtIgnore\':     gtIg,\n                \'dtIgnore\':     dtIg,\n            }\n\n    def accumulate(self, p = None):\n        \'\'\'\n        Accumulate per image evaluation results and store the result in self.eval\n        :param p: input params for evaluation\n        :return: None\n        \'\'\'\n        print(\'Accumulating evaluation results...\')\n        tic = time.time()\n        if not self.evalImgs:\n            print(\'Please run evaluate() first\')\n        # allows input customized parameters\n        if p is None:\n            p = self.params\n        p.catIds = p.catIds if p.useCats == 1 else [-1]\n        T           = len(p.iouThrs)\n        R           = len(p.recThrs)\n        K           = len(p.catIds) if p.useCats else 1\n        A           = len(p.areaRng)\n        M           = len(p.maxDets)\n        precision   = -np.ones((T,R,K,A,M)) # -1 for the precision of absent categories\n        recall      = -np.ones((T,K,A,M))\n        scores      = -np.ones((T,R,K,A,M))\n\n        # create dictionary for future indexing\n        _pe = self._paramsEval\n        catIds = _pe.catIds if _pe.useCats else [-1]\n        setK = set(catIds)\n        setA = set(map(tuple, _pe.areaRng))\n        setM = set(_pe.maxDets)\n        setI = set(_pe.imgIds)\n        # get inds to evaluate\n        k_list = [n for n, k in enumerate(p.catIds)  if k in setK]\n        m_list = [m for n, m in enumerate(p.maxDets) if m in setM]\n        a_list = [n for n, a in enumerate(map(lambda x: tuple(x), p.areaRng)) if a in setA]\n        i_list = [n for n, i in enumerate(p.imgIds)  if i in setI]\n        I0 = len(_pe.imgIds)\n        A0 = len(_pe.areaRng)\n        # retrieve E at each category, area range, and max number of detections\n        for k, k0 in enumerate(k_list):\n            Nk = k0*A0*I0\n            for a, a0 in enumerate(a_list):\n                Na = a0*I0\n                for m, maxDet in enumerate(m_list):\n                    E = [self.evalImgs[Nk + Na + i] for i in i_list]\n                    E = [e for e in E if not e is None]\n                    if len(E) == 0:\n                        continue\n                    dtScores = np.concatenate([e[\'dtScores\'][0:maxDet] for e in E])\n\n                    # different sorting method generates slightly different results.\n                    # mergesort is used to be consistent as Matlab implementation.\n                    inds = np.argsort(-dtScores, kind=\'mergesort\')\n                    dtScoresSorted = dtScores[inds]\n\n                    dtm  = np.concatenate([e[\'dtMatches\'][:,0:maxDet] for e in E], axis=1)[:,inds]\n                    dtIg = np.concatenate([e[\'dtIgnore\'][:,0:maxDet]  for e in E], axis=1)[:,inds]\n                    gtIg = np.concatenate([e[\'gtIgnore\'] for e in E])\n                    npig = np.count_nonzero(gtIg==0 )\n                    if npig == 0:\n                        continue\n                    tps = np.logical_and(               dtm,  np.logical_not(dtIg) )\n                    fps = np.logical_and(np.logical_not(dtm), np.logical_not(dtIg) )\n\n                    tp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)\n                    fp_sum = np.cumsum(fps, axis=1).astype(dtype=np.float)\n                    for t, (tp, fp) in enumerate(zip(tp_sum, fp_sum)):\n                        tp = np.array(tp)\n                        fp = np.array(fp)\n                        nd = len(tp)\n                        rc = tp / npig\n                        pr = tp / (fp+tp+np.spacing(1))\n                        q  = np.zeros((R,))\n                        ss = np.zeros((R,))\n\n                        if nd:\n                            recall[t,k,a,m] = rc[-1]\n                        else:\n                            recall[t,k,a,m] = 0\n\n                        # numpy is slow without cython optimization for accessing elements\n                        # use python array gets significant speed improvement\n                        pr = pr.tolist(); q = q.tolist()\n\n                        for i in range(nd-1, 0, -1):\n                            if pr[i] > pr[i-1]:\n                                pr[i-1] = pr[i]\n\n                        inds = np.searchsorted(rc, p.recThrs, side=\'left\')\n                        try:\n                            for ri, pi in enumerate(inds):\n                                q[ri] = pr[pi]\n                                ss[ri] = dtScoresSorted[pi]\n                        except:\n                            pass\n                        precision[t,:,k,a,m] = np.array(q)\n                        scores[t,:,k,a,m] = np.array(ss)\n        self.eval = {\n            \'params\': p,\n            \'counts\': [T, R, K, A, M],\n            \'date\': datetime.datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\'),\n            \'precision\': precision,\n            \'recall\':   recall,\n            \'scores\': scores,\n        }\n        toc = time.time()\n        print(\'DONE (t={:0.2f}s).\'.format( toc-tic))\n\n    def summarize(self):\n        \'\'\'\n        Compute and display summary metrics for evaluation results.\n        Note this functin can *only* be applied on the default parameter setting\n        \'\'\'\n        def _summarize( ap=1, iouThr=None, areaRng=\'all\', maxDets=100 ):\n            p = self.params\n            iStr = \' {:<18} {} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}\'\n            titleStr = \'Average Precision\' if ap == 1 else \'Average Recall\'\n            typeStr = \'(AP)\' if ap==1 else \'(AR)\'\n            iouStr = \'{:0.2f}:{:0.2f}\'.format(p.iouThrs[0], p.iouThrs[-1]) \\\n                if iouThr is None else \'{:0.2f}\'.format(iouThr)\n\n            aind = [i for i, aRng in enumerate(p.areaRngLbl) if aRng == areaRng]\n            mind = [i for i, mDet in enumerate(p.maxDets) if mDet == maxDets]\n            if ap == 1:\n                # dimension of precision: [TxRxKxAxM]\n                s = self.eval[\'precision\']\n                # IoU\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                s = s[:,:,:,aind,mind]\n            else:\n                # dimension of recall: [TxKxAxM]\n                s = self.eval[\'recall\']\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                s = s[:,:,aind,mind]\n            if len(s[s>-1])==0:\n                mean_s = -1\n            else:\n                mean_s = np.mean(s[s>-1])\n            print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))\n            return mean_s\n        def _summarizeDets():\n            stats = np.zeros((12,))\n            stats[0] = _summarize(1)\n            stats[1] = _summarize(1, iouThr=.5, maxDets=self.params.maxDets[2])\n            stats[2] = _summarize(1, iouThr=.75, maxDets=self.params.maxDets[2])\n            stats[3] = _summarize(1, areaRng=\'small\', maxDets=self.params.maxDets[2])\n            stats[4] = _summarize(1, areaRng=\'medium\', maxDets=self.params.maxDets[2])\n            stats[5] = _summarize(1, areaRng=\'large\', maxDets=self.params.maxDets[2])\n            stats[6] = _summarize(0, maxDets=self.params.maxDets[0])\n            stats[7] = _summarize(0, maxDets=self.params.maxDets[1])\n            stats[8] = _summarize(0, maxDets=self.params.maxDets[2])\n            stats[9] = _summarize(0, areaRng=\'small\', maxDets=self.params.maxDets[2])\n            stats[10] = _summarize(0, areaRng=\'medium\', maxDets=self.params.maxDets[2])\n            stats[11] = _summarize(0, areaRng=\'large\', maxDets=self.params.maxDets[2])\n            return stats\n        def _summarizeKps():\n            stats = np.zeros((10,))\n            stats[0] = _summarize(1, maxDets=20)\n            stats[1] = _summarize(1, maxDets=20, iouThr=.5)\n            stats[2] = _summarize(1, maxDets=20, iouThr=.75)\n            stats[3] = _summarize(1, maxDets=20, areaRng=\'medium\')\n            stats[4] = _summarize(1, maxDets=20, areaRng=\'large\')\n            stats[5] = _summarize(0, maxDets=20)\n            stats[6] = _summarize(0, maxDets=20, iouThr=.5)\n            stats[7] = _summarize(0, maxDets=20, iouThr=.75)\n            stats[8] = _summarize(0, maxDets=20, areaRng=\'medium\')\n            stats[9] = _summarize(0, maxDets=20, areaRng=\'large\')\n            return stats\n        if not self.eval:\n            raise Exception(\'Please run accumulate() first\')\n        iouType = self.params.iouType\n        if iouType == \'segm\' or iouType == \'bbox\':\n            summarize = _summarizeDets\n        elif iouType == \'keypoints\':\n            summarize = _summarizeKps\n        self.stats = summarize()\n\n    def __str__(self):\n        self.summarize()\n\nclass Params:\n    \'\'\'\n    Params for coco evaluation api\n    \'\'\'\n    def setDetParams(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, np.round((1.00 - .0) / .01) + 1, endpoint=True)\n        self.maxDets = [1, 10, 100]\n        self.areaRng = [[0 ** 2, 1e5 ** 2], [0 ** 2, 32 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n        self.areaRngLbl = [\'all\', \'small\', \'medium\', \'large\']\n        self.useCats = 1\n\n    def setKpParams(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, np.round((1.00 - .0) / .01) + 1, endpoint=True)\n        self.maxDets = [20]\n        self.areaRng = [[0 ** 2, 1e5 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n        self.areaRngLbl = [\'all\', \'medium\', \'large\']\n        self.useCats = 1\n\n    def __init__(self, iouType=\'segm\'):\n        if iouType == \'segm\' or iouType == \'bbox\':\n            self.setDetParams()\n        elif iouType == \'keypoints\':\n            self.setKpParams()\n        else:\n            raise Exception(\'iouType not supported\')\n        self.iouType = iouType\n        # useSegm is deprecated\n        self.useSegm = None'"
cocoapi/PythonAPI/pycocotools/mask.py,0,"b'__author__ = \'tsungyi\'\n\nimport pycocotools._mask as _mask\n\n# Interface for manipulating masks stored in RLE format.\n#\n# RLE is a simple yet efficient format for storing binary masks. RLE\n# first divides a vector (or vectorized image) into a series of piecewise\n# constant regions and then for each piece simply stores the length of\n# that piece. For example, given M=[0 0 1 1 1 0 1] the RLE counts would\n# be [2 3 1 1], or for M=[1 1 1 1 1 1 0] the counts would be [0 6 1]\n# (note that the odd counts are always the numbers of zeros). Instead of\n# storing the counts directly, additional compression is achieved with a\n# variable bitrate representation based on a common scheme called LEB128.\n#\n# Compression is greatest given large piecewise constant regions.\n# Specifically, the size of the RLE is proportional to the number of\n# *boundaries* in M (or for an image the number of boundaries in the y\n# direction). Assuming fairly simple shapes, the RLE representation is\n# O(sqrt(n)) where n is number of pixels in the object. Hence space usage\n# is substantially lower, especially for large simple objects (large n).\n#\n# Many common operations on masks can be computed directly using the RLE\n# (without need for decoding). This includes computations such as area,\n# union, intersection, etc. All of these operations are linear in the\n# size of the RLE, in other words they are O(sqrt(n)) where n is the area\n# of the object. Computing these operations on the original mask is O(n).\n# Thus, using the RLE can result in substantial computational savings.\n#\n# The following API functions are defined:\n#  encode         - Encode binary masks using RLE.\n#  decode         - Decode binary masks encoded via RLE.\n#  merge          - Compute union or intersection of encoded masks.\n#  iou            - Compute intersection over union between masks.\n#  area           - Compute area of encoded masks.\n#  toBbox         - Get bounding boxes surrounding encoded masks.\n#  frPyObjects    - Convert polygon, bbox, and uncompressed RLE to encoded RLE mask.\n#\n# Usage:\n#  Rs     = encode( masks )\n#  masks  = decode( Rs )\n#  R      = merge( Rs, intersect=false )\n#  o      = iou( dt, gt, iscrowd )\n#  a      = area( Rs )\n#  bbs    = toBbox( Rs )\n#  Rs     = frPyObjects( [pyObjects], h, w )\n#\n# In the API the following formats are used:\n#  Rs      - [dict] Run-length encoding of binary masks\n#  R       - dict Run-length encoding of binary mask\n#  masks   - [hxwxn] Binary mask(s) (must have type np.ndarray(dtype=uint8) in column-major order)\n#  iscrowd - [nx1] list of np.ndarray. 1 indicates corresponding gt image has crowd region to ignore\n#  bbs     - [nx4] Bounding box(es) stored as [x y w h]\n#  poly    - Polygon stored as [[x1 y1 x2 y2...],[x1 y1 ...],...] (2D list)\n#  dt,gt   - May be either bounding boxes or encoded masks\n# Both poly and bbs are 0-indexed (bbox=[0 0 1 1] encloses first pixel).\n#\n# Finally, a note about the intersection over union (iou) computation.\n# The standard iou of a ground truth (gt) and detected (dt) object is\n#  iou(gt,dt) = area(intersect(gt,dt)) / area(union(gt,dt))\n# For ""crowd"" regions, we use a modified criteria. If a gt object is\n# marked as ""iscrowd"", we allow a dt to match any subregion of the gt.\n# Choosing gt\' in the crowd gt that best matches the dt can be done using\n# gt\'=intersect(dt,gt). Since by definition union(gt\',dt)=dt, computing\n#  iou(gt,dt,iscrowd) = iou(gt\',dt) = area(intersect(gt,dt)) / area(dt)\n# For crowd gt regions we use this modified criteria above for the iou.\n#\n# To compile run ""python setup.py build_ext --inplace""\n# Please do not contact us for help with compiling.\n#\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n# Licensed under the Simplified BSD License [see coco/license.txt]\n\niou         = _mask.iou\nmerge       = _mask.merge\nfrPyObjects = _mask.frPyObjects\n\ndef encode(bimask):\n    if len(bimask.shape) == 3:\n        return _mask.encode(bimask)\n    elif len(bimask.shape) == 2:\n        h, w = bimask.shape\n        return _mask.encode(bimask.reshape((h, w, 1), order=\'F\'))[0]\n\ndef decode(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.decode(rleObjs)\n    else:\n        return _mask.decode([rleObjs])[:,:,0]\n\ndef area(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.area(rleObjs)\n    else:\n        return _mask.area([rleObjs])[0]\n\ndef toBbox(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.toBbox(rleObjs)\n    else:\n        return _mask.toBbox([rleObjs])[0]'"
cocoapi/PythonAPI/build/lib.linux-x86_64-3.6/pycocotools/__init__.py,0,"b""__author__ = 'tylin'\n"""
cocoapi/PythonAPI/build/lib.linux-x86_64-3.6/pycocotools/coco.py,0,"b'__author__ = \'tylin\'\n__version__ = \'2.0\'\n# Interface for accessing the Microsoft COCO dataset.\n\n# Microsoft COCO is a large image dataset designed for object detection,\n# segmentation, and caption generation. pycocotools is a Python API that\n# assists in loading, parsing and visualizing the annotations in COCO.\n# Please visit http://mscoco.org/ for more information on COCO, including\n# for the data, paper, and tutorials. The exact format of the annotations\n# is also described on the COCO website. For example usage of the pycocotools\n# please see pycocotools_demo.ipynb. In addition to this API, please download both\n# the COCO images and annotations in order to run the demo.\n\n# An alternative to using the API is to load the annotations directly\n# into Python dictionary\n# Using the API provides additional utility functions. Note that this API\n# supports both *instance* and *caption* annotations. In the case of\n# captions not all functions are defined (e.g. categories are undefined).\n\n# The following API functions are defined:\n#  COCO       - COCO api class that loads COCO annotation file and prepare data structures.\n#  decodeMask - Decode binary mask M encoded via run-length encoding.\n#  encodeMask - Encode binary mask M using run-length encoding.\n#  getAnnIds  - Get ann ids that satisfy given filter conditions.\n#  getCatIds  - Get cat ids that satisfy given filter conditions.\n#  getImgIds  - Get img ids that satisfy given filter conditions.\n#  loadAnns   - Load anns with the specified ids.\n#  loadCats   - Load cats with the specified ids.\n#  loadImgs   - Load imgs with the specified ids.\n#  annToMask  - Convert segmentation in an annotation to binary mask.\n#  showAnns   - Display the specified annotations.\n#  loadRes    - Load algorithm results and create API for accessing them.\n#  download   - Download COCO images from mscoco.org server.\n# Throughout the API ""ann""=annotation, ""cat""=category, and ""img""=image.\n# Help on each functions can be accessed by: ""help COCO>function"".\n\n# See also COCO>decodeMask,\n# COCO>encodeMask, COCO>getAnnIds, COCO>getCatIds,\n# COCO>getImgIds, COCO>loadAnns, COCO>loadCats,\n# COCO>loadImgs, COCO>annToMask, COCO>showAnns\n\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2014.\n# Licensed under the Simplified BSD License [see bsd.txt]\n\nimport json\nimport time\nimport matplotlib.pyplot as plt\n\n#from matplotlib.collections import PatchCollection\n#from matplotlib.patches import Polygon\nimport numpy as np\nimport copy\nimport itertools\nfrom . import mask as maskUtils\nimport os\nfrom collections import defaultdict\nimport sys\nPYTHON_VERSION = sys.version_info[0]\nif PYTHON_VERSION == 2:\n    from urllib import urlretrieve\nelif PYTHON_VERSION == 3:\n    from urllib.request import urlretrieve\n\n\ndef _isArrayLike(obj):\n    return hasattr(obj, \'__iter__\') and hasattr(obj, \'__len__\')\n\n\nclass COCO:\n    def __init__(self, annotation_file=None):\n        """"""\n        Constructor of Microsoft COCO helper class for reading and visualizing annotations.\n        :param annotation_file (str): location of annotation file\n        :param image_folder (str): location to the folder that hosts images.\n        :return:\n        """"""\n        # load dataset\n        self.dataset,self.anns,self.cats,self.imgs = dict(),dict(),dict(),dict()\n        self.imgToAnns, self.catToImgs = defaultdict(list), defaultdict(list)\n        if not annotation_file == None:\n            print(\'loading annotations into memory...\')\n            tic = time.time()\n            dataset = json.load(open(annotation_file, \'r\'))\n            assert type(dataset)==dict, \'annotation file format {} not supported\'.format(type(dataset))\n            print(\'Done (t={:0.2f}s)\'.format(time.time()- tic))\n            self.dataset = dataset\n            self.createIndex()\n\n    def createIndex(self):\n        # create index\n        print(\'creating index...\')\n        anns, cats, imgs = {}, {}, {}\n        imgToAnns,catToImgs = defaultdict(list),defaultdict(list)\n        if \'annotations\' in self.dataset:\n            for ann in self.dataset[\'annotations\']:\n                imgToAnns[ann[\'image_id\']].append(ann)\n                anns[ann[\'id\']] = ann\n\n        if \'images\' in self.dataset:\n            for img in self.dataset[\'images\']:\n                imgs[img[\'id\']] = img\n\n        if \'categories\' in self.dataset:\n            for cat in self.dataset[\'categories\']:\n                cats[cat[\'id\']] = cat\n\n        if \'annotations\' in self.dataset and \'categories\' in self.dataset:\n            for ann in self.dataset[\'annotations\']:\n                catToImgs[ann[\'category_id\']].append(ann[\'image_id\'])\n\n        print(\'index created!\')\n\n        # create class members\n        self.anns = anns\n        self.imgToAnns = imgToAnns\n        self.catToImgs = catToImgs\n        self.imgs = imgs\n        self.cats = cats\n\n    def info(self):\n        """"""\n        Print information about the annotation file.\n        :return:\n        """"""\n        for key, value in self.dataset[\'info\'].items():\n            print(\'{}: {}\'.format(key, value))\n\n    def getAnnIds(self, imgIds=[], catIds=[], areaRng=[], iscrowd=None):\n        """"""\n        Get ann ids that satisfy given filter conditions. default skips that filter\n        :param imgIds  (int array)     : get anns for given imgs\n               catIds  (int array)     : get anns for given cats\n               areaRng (float array)   : get anns for given area range (e.g. [0 inf])\n               iscrowd (boolean)       : get anns for given crowd label (False or True)\n        :return: ids (int array)       : integer array of ann ids\n        """"""\n        imgIds = imgIds if _isArrayLike(imgIds) else [imgIds]\n        catIds = catIds if _isArrayLike(catIds) else [catIds]\n\n        if len(imgIds) == len(catIds) == len(areaRng) == 0:\n            anns = self.dataset[\'annotations\']\n        else:\n            if not len(imgIds) == 0:\n                lists = [self.imgToAnns[imgId] for imgId in imgIds if imgId in self.imgToAnns]\n                anns = list(itertools.chain.from_iterable(lists))\n            else:\n                anns = self.dataset[\'annotations\']\n            anns = anns if len(catIds)  == 0 else [ann for ann in anns if ann[\'category_id\'] in catIds]\n            anns = anns if len(areaRng) == 0 else [ann for ann in anns if ann[\'area\'] > areaRng[0] and ann[\'area\'] < areaRng[1]]\n        if not iscrowd == None:\n            ids = [ann[\'id\'] for ann in anns if ann[\'iscrowd\'] == iscrowd]\n        else:\n            ids = [ann[\'id\'] for ann in anns]\n        return ids\n\n    def getCatIds(self, catNms=[], supNms=[], catIds=[]):\n        """"""\n        filtering parameters. default skips that filter.\n        :param catNms (str array)  : get cats for given cat names\n        :param supNms (str array)  : get cats for given supercategory names\n        :param catIds (int array)  : get cats for given cat ids\n        :return: ids (int array)   : integer array of cat ids\n        """"""\n        catNms = catNms if _isArrayLike(catNms) else [catNms]\n        supNms = supNms if _isArrayLike(supNms) else [supNms]\n        catIds = catIds if _isArrayLike(catIds) else [catIds]\n\n        if len(catNms) == len(supNms) == len(catIds) == 0:\n            cats = self.dataset[\'categories\']\n        else:\n            cats = self.dataset[\'categories\']\n            cats = cats if len(catNms) == 0 else [cat for cat in cats if cat[\'name\']          in catNms]\n            cats = cats if len(supNms) == 0 else [cat for cat in cats if cat[\'supercategory\'] in supNms]\n            cats = cats if len(catIds) == 0 else [cat for cat in cats if cat[\'id\']            in catIds]\n        ids = [cat[\'id\'] for cat in cats]\n        return ids\n\n    def getImgIds(self, imgIds=[], catIds=[]):\n        \'\'\'\n        Get img ids that satisfy given filter conditions.\n        :param imgIds (int array) : get imgs for given ids\n        :param catIds (int array) : get imgs with all given cats\n        :return: ids (int array)  : integer array of img ids\n        \'\'\'\n        imgIds = imgIds if _isArrayLike(imgIds) else [imgIds]\n        catIds = catIds if _isArrayLike(catIds) else [catIds]\n\n        if len(imgIds) == len(catIds) == 0:\n            ids = self.imgs.keys()\n        else:\n            ids = set(imgIds)\n            for i, catId in enumerate(catIds):\n                if i == 0 and len(ids) == 0:\n                    ids = set(self.catToImgs[catId])\n                else:\n                    ids &= set(self.catToImgs[catId])\n        return list(ids)\n\n    def loadAnns(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying anns\n        :return: anns (object array) : loaded ann objects\n        """"""\n        if _isArrayLike(ids):\n            return [self.anns[id] for id in ids]\n        elif type(ids) == int:\n            return [self.anns[ids]]\n\n    def loadCats(self, ids=[]):\n        """"""\n        Load cats with the specified ids.\n        :param ids (int array)       : integer ids specifying cats\n        :return: cats (object array) : loaded cat objects\n        """"""\n        if _isArrayLike(ids):\n            return [self.cats[id] for id in ids]\n        elif type(ids) == int:\n            return [self.cats[ids]]\n\n    def loadImgs(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying img\n        :return: imgs (object array) : loaded img objects\n        """"""\n        if _isArrayLike(ids):\n            return [self.imgs[id] for id in ids]\n        elif type(ids) == int:\n            return [self.imgs[ids]]\n\n    def showAnns(self, anns):\n        """"""\n        Display the specified annotations.\n        :param anns (array of object): annotations to display\n        :return: None\n        """"""\n        if len(anns) == 0:\n            return 0\n        if \'segmentation\' in anns[0] or \'keypoints\' in anns[0]:\n            datasetType = \'instances\'\n        elif \'caption\' in anns[0]:\n            datasetType = \'captions\'\n        else:\n            raise Exception(\'datasetType not supported\')\n        if datasetType == \'instances\':\n            ax = plt.gca()\n            ax.set_autoscale_on(False)\n            polygons = []\n            color = []\n            for ann in anns:\n                c = (np.random.random((1, 3))*0.6+0.4).tolist()[0]\n                if \'segmentation\' in ann:\n                    if type(ann[\'segmentation\']) == list:\n                        # polygon\n                        for seg in ann[\'segmentation\']:\n                            poly = np.array(seg).reshape((int(len(seg)/2), 2))\n                            polygons.append(Polygon(poly))\n                            color.append(c)\n                    else:\n                        # mask\n                        t = self.imgs[ann[\'image_id\']]\n                        if type(ann[\'segmentation\'][\'counts\']) == list:\n                            rle = maskUtils.frPyObjects([ann[\'segmentation\']], t[\'height\'], t[\'width\'])\n                        else:\n                            rle = [ann[\'segmentation\']]\n                        m = maskUtils.decode(rle)\n                        img = np.ones( (m.shape[0], m.shape[1], 3) )\n                        if ann[\'iscrowd\'] == 1:\n                            color_mask = np.array([2.0,166.0,101.0])/255\n                        if ann[\'iscrowd\'] == 0:\n                            color_mask = np.random.random((1, 3)).tolist()[0]\n                        for i in range(3):\n                            img[:,:,i] = color_mask[i]\n                        ax.imshow(np.dstack( (img, m*0.5) ))\n                if \'keypoints\' in ann and type(ann[\'keypoints\']) == list:\n                    # turn skeleton into zero-based index\n                    sks = np.array(self.loadCats(ann[\'category_id\'])[0][\'skeleton\'])-1\n                    kp = np.array(ann[\'keypoints\'])\n                    x = kp[0::3]\n                    y = kp[1::3]\n                    v = kp[2::3]\n                    for sk in sks:\n                        if np.all(v[sk]>0):\n                            plt.plot(x[sk],y[sk], linewidth=3, color=c)\n                    plt.plot(x[v>0], y[v>0],\'o\',markersize=8, markerfacecolor=c, markeredgecolor=\'k\',markeredgewidth=2)\n                    plt.plot(x[v>1], y[v>1],\'o\',markersize=8, markerfacecolor=c, markeredgecolor=c, markeredgewidth=2)\n            p = PatchCollection(polygons, facecolor=color, linewidths=0, alpha=0.4)\n            ax.add_collection(p)\n            p = PatchCollection(polygons, facecolor=\'none\', edgecolors=color, linewidths=2)\n            ax.add_collection(p)\n        elif datasetType == \'captions\':\n            for ann in anns:\n                print(ann[\'caption\'])\n\n    def loadRes(self, resFile):\n        """"""\n        Load result file and return a result api object.\n        :param   resFile (str)     : file name of result file\n        :return: res (obj)         : result api object\n        """"""\n        res = COCO()\n        res.dataset[\'images\'] = [img for img in self.dataset[\'images\']]\n\n        print(\'Loading and preparing results...\')\n        tic = time.time()\n        if type(resFile) == str or type(resFile) == unicode:\n            anns = json.load(open(resFile))\n        elif type(resFile) == np.ndarray:\n            anns = self.loadNumpyAnnotations(resFile)\n        else:\n            anns = resFile\n        assert type(anns) == list, \'results in not an array of objects\'\n        annsImgIds = [ann[\'image_id\'] for ann in anns]\n        assert set(annsImgIds) == (set(annsImgIds) & set(self.getImgIds())), \\\n               \'Results do not correspond to current coco set\'\n        if \'caption\' in anns[0]:\n            imgIds = set([img[\'id\'] for img in res.dataset[\'images\']]) & set([ann[\'image_id\'] for ann in anns])\n            res.dataset[\'images\'] = [img for img in res.dataset[\'images\'] if img[\'id\'] in imgIds]\n            for id, ann in enumerate(anns):\n                ann[\'id\'] = id+1\n        elif \'bbox\' in anns[0] and not anns[0][\'bbox\'] == []:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                bb = ann[\'bbox\']\n                x1, x2, y1, y2 = [bb[0], bb[0]+bb[2], bb[1], bb[1]+bb[3]]\n                if not \'segmentation\' in ann:\n                    ann[\'segmentation\'] = [[x1, y1, x1, y2, x2, y2, x2, y1]]\n                ann[\'area\'] = bb[2]*bb[3]\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        elif \'segmentation\' in anns[0]:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                # now only support compressed RLE format as segmentation results\n                ann[\'area\'] = maskUtils.area(ann[\'segmentation\'])\n                if not \'bbox\' in ann:\n                    ann[\'bbox\'] = maskUtils.toBbox(ann[\'segmentation\'])\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        elif \'keypoints\' in anns[0]:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                s = ann[\'keypoints\']\n                x = s[0::3]\n                y = s[1::3]\n                x0,x1,y0,y1 = np.min(x), np.max(x), np.min(y), np.max(y)\n                ann[\'area\'] = (x1-x0)*(y1-y0)\n                ann[\'id\'] = id + 1\n                ann[\'bbox\'] = [x0,y0,x1-x0,y1-y0]\n        print(\'DONE (t={:0.2f}s)\'.format(time.time()- tic))\n\n        res.dataset[\'annotations\'] = anns\n        res.createIndex()\n        return res\n\n    def download(self, tarDir = None, imgIds = [] ):\n        \'\'\'\n        Download COCO images from mscoco.org server.\n        :param tarDir (str): COCO results directory name\n               imgIds (list): images to be downloaded\n        :return:\n        \'\'\'\n        if tarDir is None:\n            print(\'Please specify target directory\')\n            return -1\n        if len(imgIds) == 0:\n            imgs = self.imgs.values()\n        else:\n            imgs = self.loadImgs(imgIds)\n        N = len(imgs)\n        if not os.path.exists(tarDir):\n            os.makedirs(tarDir)\n        for i, img in enumerate(imgs):\n            tic = time.time()\n            fname = os.path.join(tarDir, img[\'file_name\'])\n            if not os.path.exists(fname):\n                urlretrieve(img[\'coco_url\'], fname)\n            print(\'downloaded {}/{} images (t={:0.1f}s)\'.format(i, N, time.time()- tic))\n\n    def loadNumpyAnnotations(self, data):\n        """"""\n        Convert result data from a numpy array [Nx7] where each row contains {imageID,x1,y1,w,h,score,class}\n        :param  data (numpy.ndarray)\n        :return: annotations (python nested list)\n        """"""\n        print(\'Converting ndarray to lists...\')\n        assert(type(data) == np.ndarray)\n        print(data.shape)\n        assert(data.shape[1] == 7)\n        N = data.shape[0]\n        ann = []\n        for i in range(N):\n            if i % 1000000 == 0:\n                print(\'{}/{}\'.format(i,N))\n            ann += [{\n                \'image_id\'  : int(data[i, 0]),\n                \'bbox\'  : [ data[i, 1], data[i, 2], data[i, 3], data[i, 4] ],\n                \'score\' : data[i, 5],\n                \'category_id\': int(data[i, 6]),\n                }]\n        return ann\n\n    def annToRLE(self, ann):\n        """"""\n        Convert annotation which can be polygons, uncompressed RLE to RLE.\n        :return: binary mask (numpy 2D array)\n        """"""\n        t = self.imgs[ann[\'image_id\']]\n        h, w = t[\'height\'], t[\'width\']\n        segm = ann[\'segmentation\']\n        if type(segm) == list:\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            rles = maskUtils.frPyObjects(segm, h, w)\n            rle = maskUtils.merge(rles)\n        elif type(segm[\'counts\']) == list:\n            # uncompressed RLE\n            rle = maskUtils.frPyObjects(segm, h, w)\n        else:\n            # rle\n            rle = ann[\'segmentation\']\n        return rle\n\n    def annToMask(self, ann):\n        """"""\n        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n        :return: binary mask (numpy 2D array)\n        """"""\n        rle = self.annToRLE(ann)\n        m = maskUtils.decode(rle)\n        return m\n'"
cocoapi/PythonAPI/build/lib.linux-x86_64-3.6/pycocotools/cocoeval.py,0,"b'__author__ = \'tsungyi\'\n\nimport numpy as np\nimport datetime\nimport time\nfrom collections import defaultdict\nfrom . import mask as maskUtils\nimport copy\n\nclass COCOeval:\n    # Interface for evaluating detection on the Microsoft COCO dataset.\n    #\n    # The usage for CocoEval is as follows:\n    #  cocoGt=..., cocoDt=...       # load dataset and results\n    #  E = CocoEval(cocoGt,cocoDt); # initialize CocoEval object\n    #  E.params.recThrs = ...;      # set parameters as desired\n    #  E.evaluate();                # run per image evaluation\n    #  E.accumulate();              # accumulate per image results\n    #  E.summarize();               # display summary metrics of results\n    # For example usage see evalDemo.m and http://mscoco.org/.\n    #\n    # The evaluation parameters are as follows (defaults in brackets):\n    #  imgIds     - [all] N img ids to use for evaluation\n    #  catIds     - [all] K cat ids to use for evaluation\n    #  iouThrs    - [.5:.05:.95] T=10 IoU thresholds for evaluation\n    #  recThrs    - [0:.01:1] R=101 recall thresholds for evaluation\n    #  areaRng    - [...] A=4 object area ranges for evaluation\n    #  maxDets    - [1 10 100] M=3 thresholds on max detections per image\n    #  iouType    - [\'segm\'] set iouType to \'segm\', \'bbox\' or \'keypoints\'\n    #  iouType replaced the now DEPRECATED useSegm parameter.\n    #  useCats    - [1] if true use category labels for evaluation\n    # Note: if useCats=0 category labels are ignored as in proposal scoring.\n    # Note: multiple areaRngs [Ax2] and maxDets [Mx1] can be specified.\n    #\n    # evaluate(): evaluates detections on every image and every category and\n    # concats the results into the ""evalImgs"" with fields:\n    #  dtIds      - [1xD] id for each of the D detections (dt)\n    #  gtIds      - [1xG] id for each of the G ground truths (gt)\n    #  dtMatches  - [TxD] matching gt id at each IoU or 0\n    #  gtMatches  - [TxG] matching dt id at each IoU or 0\n    #  dtScores   - [1xD] confidence of each dt\n    #  gtIgnore   - [1xG] ignore flag for each gt\n    #  dtIgnore   - [TxD] ignore flag for each dt at each IoU\n    #\n    # accumulate(): accumulates the per-image, per-category evaluation\n    # results in ""evalImgs"" into the dictionary ""eval"" with fields:\n    #  params     - parameters used for evaluation\n    #  date       - date evaluation was performed\n    #  counts     - [T,R,K,A,M] parameter dimensions (see above)\n    #  precision  - [TxRxKxAxM] precision for every evaluation setting\n    #  recall     - [TxKxAxM] max recall for every evaluation setting\n    # Note: precision and recall==-1 for settings with no gt objects.\n    #\n    # See also coco, mask, pycocoDemo, pycocoEvalDemo\n    #\n    # Microsoft COCO Toolbox.      version 2.0\n    # Data, paper, and tutorials available at:  http://mscoco.org/\n    # Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n    # Licensed under the Simplified BSD License [see coco/license.txt]\n    def __init__(self, cocoGt=None, cocoDt=None, iouType=\'segm\'):\n        \'\'\'\n        Initialize CocoEval using coco APIs for gt and dt\n        :param cocoGt: coco object with ground truth annotations\n        :param cocoDt: coco object with detection results\n        :return: None\n        \'\'\'\n        if not iouType:\n            print(\'iouType not specified. use default iouType segm\')\n        self.cocoGt   = cocoGt              # ground truth COCO API\n        self.cocoDt   = cocoDt              # detections COCO API\n        self.params   = {}                  # evaluation parameters\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results [KxAxI] elements\n        self.eval     = {}                  # accumulated evaluation results\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        self.params = Params(iouType=iouType) # parameters\n        self._paramsEval = {}               # parameters for evaluation\n        self.stats = []                     # result summarization\n        self.ious = {}                      # ious between all gts and dts\n        if not cocoGt is None:\n            self.params.imgIds = sorted(cocoGt.getImgIds())\n            self.params.catIds = sorted(cocoGt.getCatIds())\n\n\n    def _prepare(self):\n        \'\'\'\n        Prepare ._gts and ._dts for evaluation based on params\n        :return: None\n        \'\'\'\n        def _toMask(anns, coco):\n            # modify ann[\'segmentation\'] by reference\n            for ann in anns:\n                rle = coco.annToRLE(ann)\n                ann[\'segmentation\'] = rle\n        p = self.params\n        if p.useCats:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n        else:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds))\n\n        # convert ground truth to mask if iouType == \'segm\'\n        if p.iouType == \'segm\':\n            _toMask(gts, self.cocoGt)\n            _toMask(dts, self.cocoDt)\n        # set ignore flag\n        for gt in gts:\n            gt[\'ignore\'] = gt[\'ignore\'] if \'ignore\' in gt else 0\n            gt[\'ignore\'] = \'iscrowd\' in gt and gt[\'iscrowd\']\n            if p.iouType == \'keypoints\':\n                gt[\'ignore\'] = (gt[\'num_keypoints\'] == 0) or gt[\'ignore\']\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        for gt in gts:\n            self._gts[gt[\'image_id\'], gt[\'category_id\']].append(gt)\n        for dt in dts:\n            self._dts[dt[\'image_id\'], dt[\'category_id\']].append(dt)\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results\n        self.eval     = {}                  # accumulated evaluation results\n\n    def evaluate(self):\n        \'\'\'\n        Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n        :return: None\n        \'\'\'\n        tic = time.time()\n        print(\'Running per image evaluation...\')\n        p = self.params\n        # add backward compatibility if useSegm is specified in params\n        if not p.useSegm is None:\n            p.iouType = \'segm\' if p.useSegm == 1 else \'bbox\'\n            print(\'useSegm (deprecated) is not None. Running {} evaluation\'.format(p.iouType))\n        print(\'Evaluate annotation type *{}*\'.format(p.iouType))\n        p.imgIds = list(np.unique(p.imgIds))\n        if p.useCats:\n            p.catIds = list(np.unique(p.catIds))\n        p.maxDets = sorted(p.maxDets)\n        self.params=p\n\n        self._prepare()\n        # loop through images, area range, max detection number\n        catIds = p.catIds if p.useCats else [-1]\n\n        if p.iouType == \'segm\' or p.iouType == \'bbox\':\n            computeIoU = self.computeIoU\n        elif p.iouType == \'keypoints\':\n            computeIoU = self.computeOks\n        self.ious = {(imgId, catId): computeIoU(imgId, catId) \\\n                        for imgId in p.imgIds\n                        for catId in catIds}\n\n        evaluateImg = self.evaluateImg\n        maxDet = p.maxDets[-1]\n        self.evalImgs = [evaluateImg(imgId, catId, areaRng, maxDet)\n                 for catId in catIds\n                 for areaRng in p.areaRng\n                 for imgId in p.imgIds\n             ]\n        self._paramsEval = copy.deepcopy(self.params)\n        toc = time.time()\n        print(\'DONE (t={:0.2f}s).\'.format(toc-tic))\n\n    def computeIoU(self, imgId, catId):\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return []\n        inds = np.argsort([-d[\'score\'] for d in dt], kind=\'mergesort\')\n        dt = [dt[i] for i in inds]\n        if len(dt) > p.maxDets[-1]:\n            dt=dt[0:p.maxDets[-1]]\n\n        if p.iouType == \'segm\':\n            g = [g[\'segmentation\'] for g in gt]\n            d = [d[\'segmentation\'] for d in dt]\n        elif p.iouType == \'bbox\':\n            g = [g[\'bbox\'] for g in gt]\n            d = [d[\'bbox\'] for d in dt]\n        else:\n            raise Exception(\'unknown iouType for iou computation\')\n\n        # compute iou between each dt and gt region\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        ious = maskUtils.iou(d,g,iscrowd)\n        return ious\n\n    def computeOks(self, imgId, catId):\n        p = self.params\n        # dimention here should be Nxm\n        gts = self._gts[imgId, catId]\n        dts = self._dts[imgId, catId]\n        inds = np.argsort([-d[\'score\'] for d in dts], kind=\'mergesort\')\n        dts = [dts[i] for i in inds]\n        if len(dts) > p.maxDets[-1]:\n            dts = dts[0:p.maxDets[-1]]\n        # if len(gts) == 0 and len(dts) == 0:\n        if len(gts) == 0 or len(dts) == 0:\n            return []\n        ious = np.zeros((len(dts), len(gts)))\n\n        mpii_part_names = [\'RAnkle\', \'RKnee\', \'RHip\', \'LHip\', \'LKnee\', \'LAnkle\',\n                           \'Pelv\', \'Thrx\', \'Neck\', \'Head\', \'RWrist\', \'RElbow\',\n                           \'RShoulder\', \'LShoulder\', \'LElbow\', \'LWrist\']\n\n        mpii_mapping_part_names = [\'RAnkle\', \'RKnee\', \'RHip\', \'LHip\', \'LKnee\', \'LAnkle\',\n                           \'LHip\', \'LHip\', \'LEar\', \'LEar\', \'RWrist\', \'RElbow\',\n                           \'RShoulder\', \'LShoulder\', \'LElbow\', \'LWrist\']\n\n        coco_part_names = [\'Nose\', \'LEye\', \'REye\', \'LEar\', \'REar\', \'LShoulder\',\n                           \'RShoulder\', \'LElbow\', \'RElbow\', \'LWrist\', \'RWrist\',\n                           \'LHip\', \'RHip\', \'LKnee\', \'RKnee\', \'LAnkle\', \'RAnkle\']\n\n\n        coco_sigmas = np.array(\n            [.26, .25, .25, .35, .35, .79,\n             .79, .72, .72, .62, .62, 1.07, 1.07,\n             .87, .87, .89, .89]) / 10.0\n\n        sigmas = np.array([])\n        for name in mpii_mapping_part_names:\n            pos = coco_part_names.index(name)\n            sigmas = np.append(sigmas, coco_sigmas[pos])\n        # for x in sigmas:\n        #     print(x,"","")\n        # print(sigmas)\n        # sigmas =  np.array([0.089,  0.087,  0.107,  0.107,  0.087,  0.089,\n        #            0.107,  0.107,  0.035,  0.035,  0.062,  0.072,\n        #            0.079,  0.079,  0.072,  0.062])\n        sigmas = np.array([0.089, 0.087, 0.107, 0.107, 0.087, 0.089,\n                           0.107, 0.107, 0.107, 0.107, 0.062, 0.072,\n                           0.079, 0.079, 0.072, 0.062])\n        # sigmas = np.array([10.089, 10.087, 10.107, 10.107, 10.087, 10.089,\n        #                    10.089, 11.026, 11.035, 10.089, 10.062, 10.072,\n        #                    10.079, 10.079, 10.072, 10.062])\n\n        vars = (sigmas * 2)**2\n        k = len(sigmas)\n        # compute oks between each detection and ground truth object\n        for j, gt in enumerate(gts):\n            # create bounds for ignore regions(double the gt bbox)\n            g = np.array(gt[\'keypoints\'])\n            xg = g[0::3]; yg = g[1::3]; vg = g[2::3]\n            k1 = np.count_nonzero(vg > 0)\n            bb = gt[\'bbox\']\n            x0 = bb[0] - bb[2]; x1 = bb[0] + bb[2] * 2\n            y0 = bb[1] - bb[3]; y1 = bb[1] + bb[3] * 2\n            for i, dt in enumerate(dts):\n                d = np.array(dt[\'keypoints\'])\n                xd = d[0::3]; yd = d[1::3]\n                if k1>0:\n                    # measure the per-keypoint distance if keypoints visible\n                    dx = xd - xg\n                    dy = yd - yg\n                else:\n                    # measure minimum distance to keypoints in (x0,y0) & (x1,y1)\n                    z = np.zeros((k))\n                    dx = np.max((z, x0-xd),axis=0)+np.max((z, xd-x1),axis=0)\n                    dy = np.max((z, y0-yd),axis=0)+np.max((z, yd-y1),axis=0)\n                e = (dx**2 + dy**2) / vars / (gt[\'area\']+np.spacing(1)) / 2\n                if k1 > 0:\n                    e=e[vg > 0]\n                ious[i, j] = np.sum(np.exp(-e)) / e.shape[0]\n        return ious\n\n    def evaluateImg(self, imgId, catId, aRng, maxDet):\n        \'\'\'\n        perform evaluation for single category and image\n        :return: dict (single image results)\n        \'\'\'\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return None\n\n        for g in gt:\n            if g[\'ignore\'] or (g[\'area\']<aRng[0] or g[\'area\']>aRng[1]):\n                g[\'_ignore\'] = 1\n            else:\n                g[\'_ignore\'] = 0\n\n        # sort dt highest score first, sort gt ignore last\n        gtind = np.argsort([g[\'_ignore\'] for g in gt], kind=\'mergesort\')\n        gt = [gt[i] for i in gtind]\n        dtind = np.argsort([-d[\'score\'] for d in dt], kind=\'mergesort\')\n        dt = [dt[i] for i in dtind[0:maxDet]]\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        # load computed ious\n        ious = self.ious[imgId, catId][:, gtind] if len(self.ious[imgId, catId]) > 0 else self.ious[imgId, catId]\n\n        T = len(p.iouThrs)\n        G = len(gt)\n        D = len(dt)\n        gtm  = np.zeros((T,G))\n        dtm  = np.zeros((T,D))\n        gtIg = np.array([g[\'_ignore\'] for g in gt])\n        dtIg = np.zeros((T,D))\n        if not len(ious)==0:\n            for tind, t in enumerate(p.iouThrs):\n                for dind, d in enumerate(dt):\n                    # information about best match so far (m=-1 -> unmatched)\n                    iou = min([t,1-1e-10])\n                    m   = -1\n                    for gind, g in enumerate(gt):\n                        # if this gt already matched, and not a crowd, continue\n                        if gtm[tind,gind]>0 and not iscrowd[gind]:\n                            continue\n                        # if dt matched to reg gt, and on ignore gt, stop\n                        if m>-1 and gtIg[m]==0 and gtIg[gind]==1:\n                            break\n                        # continue to next gt unless better match made\n                        if ious[dind,gind] < iou:\n                            continue\n                        # if match successful and best so far, store appropriately\n                        iou=ious[dind,gind]\n                        m=gind\n                    # if match made store id of match for both dt and gt\n                    if m ==-1:\n                        continue\n                    dtIg[tind,dind] = gtIg[m]\n                    dtm[tind,dind]  = gt[m][\'id\']\n                    gtm[tind,m]     = d[\'id\']\n        # set unmatched detections outside of area range to ignore\n        a = np.array([d[\'area\']<aRng[0] or d[\'area\']>aRng[1] for d in dt]).reshape((1, len(dt)))\n        dtIg = np.logical_or(dtIg, np.logical_and(dtm==0, np.repeat(a,T,0)))\n        # store results for given image and category\n        return {\n                \'image_id\':     imgId,\n                \'category_id\':  catId,\n                \'aRng\':         aRng,\n                \'maxDet\':       maxDet,\n                \'dtIds\':        [d[\'id\'] for d in dt],\n                \'gtIds\':        [g[\'id\'] for g in gt],\n                \'dtMatches\':    dtm,\n                \'gtMatches\':    gtm,\n                \'dtScores\':     [d[\'score\'] for d in dt],\n                \'gtIgnore\':     gtIg,\n                \'dtIgnore\':     dtIg,\n            }\n\n    def accumulate(self, p = None):\n        \'\'\'\n        Accumulate per image evaluation results and store the result in self.eval\n        :param p: input params for evaluation\n        :return: None\n        \'\'\'\n        print(\'Accumulating evaluation results...\')\n        tic = time.time()\n        if not self.evalImgs:\n            print(\'Please run evaluate() first\')\n        # allows input customized parameters\n        if p is None:\n            p = self.params\n        p.catIds = p.catIds if p.useCats == 1 else [-1]\n        T           = len(p.iouThrs)\n        R           = len(p.recThrs)\n        K           = len(p.catIds) if p.useCats else 1\n        A           = len(p.areaRng)\n        M           = len(p.maxDets)\n        precision   = -np.ones((T,R,K,A,M)) # -1 for the precision of absent categories\n        recall      = -np.ones((T,K,A,M))\n        scores      = -np.ones((T,R,K,A,M))\n\n        # create dictionary for future indexing\n        _pe = self._paramsEval\n        catIds = _pe.catIds if _pe.useCats else [-1]\n        setK = set(catIds)\n        setA = set(map(tuple, _pe.areaRng))\n        setM = set(_pe.maxDets)\n        setI = set(_pe.imgIds)\n        # get inds to evaluate\n        k_list = [n for n, k in enumerate(p.catIds)  if k in setK]\n        m_list = [m for n, m in enumerate(p.maxDets) if m in setM]\n        a_list = [n for n, a in enumerate(map(lambda x: tuple(x), p.areaRng)) if a in setA]\n        i_list = [n for n, i in enumerate(p.imgIds)  if i in setI]\n        I0 = len(_pe.imgIds)\n        A0 = len(_pe.areaRng)\n        # retrieve E at each category, area range, and max number of detections\n        for k, k0 in enumerate(k_list):\n            Nk = k0*A0*I0\n            for a, a0 in enumerate(a_list):\n                Na = a0*I0\n                for m, maxDet in enumerate(m_list):\n                    E = [self.evalImgs[Nk + Na + i] for i in i_list]\n                    E = [e for e in E if not e is None]\n                    if len(E) == 0:\n                        continue\n                    dtScores = np.concatenate([e[\'dtScores\'][0:maxDet] for e in E])\n\n                    # different sorting method generates slightly different results.\n                    # mergesort is used to be consistent as Matlab implementation.\n                    inds = np.argsort(-dtScores, kind=\'mergesort\')\n                    dtScoresSorted = dtScores[inds]\n\n                    dtm  = np.concatenate([e[\'dtMatches\'][:,0:maxDet] for e in E], axis=1)[:,inds]\n                    dtIg = np.concatenate([e[\'dtIgnore\'][:,0:maxDet]  for e in E], axis=1)[:,inds]\n                    gtIg = np.concatenate([e[\'gtIgnore\'] for e in E])\n                    npig = np.count_nonzero(gtIg==0 )\n                    if npig == 0:\n                        continue\n                    tps = np.logical_and(               dtm,  np.logical_not(dtIg) )\n                    fps = np.logical_and(np.logical_not(dtm), np.logical_not(dtIg) )\n\n                    tp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)\n                    fp_sum = np.cumsum(fps, axis=1).astype(dtype=np.float)\n                    for t, (tp, fp) in enumerate(zip(tp_sum, fp_sum)):\n                        tp = np.array(tp)\n                        fp = np.array(fp)\n                        nd = len(tp)\n                        rc = tp / npig\n                        pr = tp / (fp+tp+np.spacing(1))\n                        q  = np.zeros((R,))\n                        ss = np.zeros((R,))\n\n                        if nd:\n                            recall[t,k,a,m] = rc[-1]\n                        else:\n                            recall[t,k,a,m] = 0\n\n                        # numpy is slow without cython optimization for accessing elements\n                        # use python array gets significant speed improvement\n                        pr = pr.tolist(); q = q.tolist()\n\n                        for i in range(nd-1, 0, -1):\n                            if pr[i] > pr[i-1]:\n                                pr[i-1] = pr[i]\n\n                        inds = np.searchsorted(rc, p.recThrs, side=\'left\')\n                        try:\n                            for ri, pi in enumerate(inds):\n                                q[ri] = pr[pi]\n                                ss[ri] = dtScoresSorted[pi]\n                        except:\n                            pass\n                        precision[t,:,k,a,m] = np.array(q)\n                        scores[t,:,k,a,m] = np.array(ss)\n        self.eval = {\n            \'params\': p,\n            \'counts\': [T, R, K, A, M],\n            \'date\': datetime.datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\'),\n            \'precision\': precision,\n            \'recall\':   recall,\n            \'scores\': scores,\n        }\n        toc = time.time()\n        print(\'DONE (t={:0.2f}s).\'.format( toc-tic))\n\n    def summarize(self):\n        \'\'\'\n        Compute and display summary metrics for evaluation results.\n        Note this functin can *only* be applied on the default parameter setting\n        \'\'\'\n        def _summarize( ap=1, iouThr=None, areaRng=\'all\', maxDets=100 ):\n            p = self.params\n            iStr = \' {:<18} {} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}\'\n            titleStr = \'Average Precision\' if ap == 1 else \'Average Recall\'\n            typeStr = \'(AP)\' if ap==1 else \'(AR)\'\n            iouStr = \'{:0.2f}:{:0.2f}\'.format(p.iouThrs[0], p.iouThrs[-1]) \\\n                if iouThr is None else \'{:0.2f}\'.format(iouThr)\n\n            aind = [i for i, aRng in enumerate(p.areaRngLbl) if aRng == areaRng]\n            mind = [i for i, mDet in enumerate(p.maxDets) if mDet == maxDets]\n            if ap == 1:\n                # dimension of precision: [TxRxKxAxM]\n                s = self.eval[\'precision\']\n                # IoU\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                s = s[:,:,:,aind,mind]\n            else:\n                # dimension of recall: [TxKxAxM]\n                s = self.eval[\'recall\']\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                s = s[:,:,aind,mind]\n            if len(s[s>-1])==0:\n                mean_s = -1\n            else:\n                mean_s = np.mean(s[s>-1])\n            print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))\n            return mean_s\n        def _summarizeDets():\n            stats = np.zeros((12,))\n            stats[0] = _summarize(1)\n            stats[1] = _summarize(1, iouThr=.5, maxDets=self.params.maxDets[2])\n            stats[2] = _summarize(1, iouThr=.75, maxDets=self.params.maxDets[2])\n            stats[3] = _summarize(1, areaRng=\'small\', maxDets=self.params.maxDets[2])\n            stats[4] = _summarize(1, areaRng=\'medium\', maxDets=self.params.maxDets[2])\n            stats[5] = _summarize(1, areaRng=\'large\', maxDets=self.params.maxDets[2])\n            stats[6] = _summarize(0, maxDets=self.params.maxDets[0])\n            stats[7] = _summarize(0, maxDets=self.params.maxDets[1])\n            stats[8] = _summarize(0, maxDets=self.params.maxDets[2])\n            stats[9] = _summarize(0, areaRng=\'small\', maxDets=self.params.maxDets[2])\n            stats[10] = _summarize(0, areaRng=\'medium\', maxDets=self.params.maxDets[2])\n            stats[11] = _summarize(0, areaRng=\'large\', maxDets=self.params.maxDets[2])\n            return stats\n        def _summarizeKps():\n            stats = np.zeros((10,))\n            stats[0] = _summarize(1, maxDets=20)\n            stats[1] = _summarize(1, maxDets=20, iouThr=.5)\n            stats[2] = _summarize(1, maxDets=20, iouThr=.75)\n            stats[3] = _summarize(1, maxDets=20, areaRng=\'medium\')\n            stats[4] = _summarize(1, maxDets=20, areaRng=\'large\')\n            stats[5] = _summarize(0, maxDets=20)\n            stats[6] = _summarize(0, maxDets=20, iouThr=.5)\n            stats[7] = _summarize(0, maxDets=20, iouThr=.75)\n            stats[8] = _summarize(0, maxDets=20, areaRng=\'medium\')\n            stats[9] = _summarize(0, maxDets=20, areaRng=\'large\')\n            return stats\n        if not self.eval:\n            raise Exception(\'Please run accumulate() first\')\n        iouType = self.params.iouType\n        if iouType == \'segm\' or iouType == \'bbox\':\n            summarize = _summarizeDets\n        elif iouType == \'keypoints\':\n            summarize = _summarizeKps\n        self.stats = summarize()\n\n    def __str__(self):\n        self.summarize()\n\nclass Params:\n    \'\'\'\n    Params for coco evaluation api\n    \'\'\'\n    def setDetParams(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, np.round((1.00 - .0) / .01) + 1, endpoint=True)\n        self.maxDets = [1, 10, 100]\n        self.areaRng = [[0 ** 2, 1e5 ** 2], [0 ** 2, 32 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n        self.areaRngLbl = [\'all\', \'small\', \'medium\', \'large\']\n        self.useCats = 1\n\n    def setKpParams(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, np.round((1.00 - .0) / .01) + 1, endpoint=True)\n        self.maxDets = [20]\n        self.areaRng = [[0 ** 2, 1e5 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n        self.areaRngLbl = [\'all\', \'medium\', \'large\']\n        self.useCats = 1\n\n    def __init__(self, iouType=\'segm\'):\n        if iouType == \'segm\' or iouType == \'bbox\':\n            self.setDetParams()\n        elif iouType == \'keypoints\':\n            self.setKpParams()\n        else:\n            raise Exception(\'iouType not supported\')\n        self.iouType = iouType\n        # useSegm is deprecated\n        self.useSegm = None'"
cocoapi/PythonAPI/build/lib.linux-x86_64-3.6/pycocotools/mask.py,0,"b'__author__ = \'tsungyi\'\n\nimport pycocotools._mask as _mask\n\n# Interface for manipulating masks stored in RLE format.\n#\n# RLE is a simple yet efficient format for storing binary masks. RLE\n# first divides a vector (or vectorized image) into a series of piecewise\n# constant regions and then for each piece simply stores the length of\n# that piece. For example, given M=[0 0 1 1 1 0 1] the RLE counts would\n# be [2 3 1 1], or for M=[1 1 1 1 1 1 0] the counts would be [0 6 1]\n# (note that the odd counts are always the numbers of zeros). Instead of\n# storing the counts directly, additional compression is achieved with a\n# variable bitrate representation based on a common scheme called LEB128.\n#\n# Compression is greatest given large piecewise constant regions.\n# Specifically, the size of the RLE is proportional to the number of\n# *boundaries* in M (or for an image the number of boundaries in the y\n# direction). Assuming fairly simple shapes, the RLE representation is\n# O(sqrt(n)) where n is number of pixels in the object. Hence space usage\n# is substantially lower, especially for large simple objects (large n).\n#\n# Many common operations on masks can be computed directly using the RLE\n# (without need for decoding). This includes computations such as area,\n# union, intersection, etc. All of these operations are linear in the\n# size of the RLE, in other words they are O(sqrt(n)) where n is the area\n# of the object. Computing these operations on the original mask is O(n).\n# Thus, using the RLE can result in substantial computational savings.\n#\n# The following API functions are defined:\n#  encode         - Encode binary masks using RLE.\n#  decode         - Decode binary masks encoded via RLE.\n#  merge          - Compute union or intersection of encoded masks.\n#  iou            - Compute intersection over union between masks.\n#  area           - Compute area of encoded masks.\n#  toBbox         - Get bounding boxes surrounding encoded masks.\n#  frPyObjects    - Convert polygon, bbox, and uncompressed RLE to encoded RLE mask.\n#\n# Usage:\n#  Rs     = encode( masks )\n#  masks  = decode( Rs )\n#  R      = merge( Rs, intersect=false )\n#  o      = iou( dt, gt, iscrowd )\n#  a      = area( Rs )\n#  bbs    = toBbox( Rs )\n#  Rs     = frPyObjects( [pyObjects], h, w )\n#\n# In the API the following formats are used:\n#  Rs      - [dict] Run-length encoding of binary masks\n#  R       - dict Run-length encoding of binary mask\n#  masks   - [hxwxn] Binary mask(s) (must have type np.ndarray(dtype=uint8) in column-major order)\n#  iscrowd - [nx1] list of np.ndarray. 1 indicates corresponding gt image has crowd region to ignore\n#  bbs     - [nx4] Bounding box(es) stored as [x y w h]\n#  poly    - Polygon stored as [[x1 y1 x2 y2...],[x1 y1 ...],...] (2D list)\n#  dt,gt   - May be either bounding boxes or encoded masks\n# Both poly and bbs are 0-indexed (bbox=[0 0 1 1] encloses first pixel).\n#\n# Finally, a note about the intersection over union (iou) computation.\n# The standard iou of a ground truth (gt) and detected (dt) object is\n#  iou(gt,dt) = area(intersect(gt,dt)) / area(union(gt,dt))\n# For ""crowd"" regions, we use a modified criteria. If a gt object is\n# marked as ""iscrowd"", we allow a dt to match any subregion of the gt.\n# Choosing gt\' in the crowd gt that best matches the dt can be done using\n# gt\'=intersect(dt,gt). Since by definition union(gt\',dt)=dt, computing\n#  iou(gt,dt,iscrowd) = iou(gt\',dt) = area(intersect(gt,dt)) / area(dt)\n# For crowd gt regions we use this modified criteria above for the iou.\n#\n# To compile run ""python setup.py build_ext --inplace""\n# Please do not contact us for help with compiling.\n#\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n# Licensed under the Simplified BSD License [see coco/license.txt]\n\niou         = _mask.iou\nmerge       = _mask.merge\nfrPyObjects = _mask.frPyObjects\n\ndef encode(bimask):\n    if len(bimask.shape) == 3:\n        return _mask.encode(bimask)\n    elif len(bimask.shape) == 2:\n        h, w = bimask.shape\n        return _mask.encode(bimask.reshape((h, w, 1), order=\'F\'))[0]\n\ndef decode(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.decode(rleObjs)\n    else:\n        return _mask.decode([rleObjs])[:,:,0]\n\ndef area(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.area(rleObjs)\n    else:\n        return _mask.area([rleObjs])[0]\n\ndef toBbox(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.toBbox(rleObjs)\n    else:\n        return _mask.toBbox([rleObjs])[0]'"
