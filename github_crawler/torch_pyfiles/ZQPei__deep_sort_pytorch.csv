file_path,api_count,code
ped_det_server.py,1,"b'""""""\nThis module gets video in input and outputs the\njson file with coordination of bboxes in the video.\n\n""""""\nfrom os.path import basename, splitext, join, isfile, isdir, dirname\nfrom os import makedirs\n\nfrom tqdm import tqdm\nimport cv2\nimport argparse\nimport torch\n\nfrom detector import build_detector\nfrom deep_sort import build_tracker\nfrom utils.tools import tik_tok, is_video\nfrom utils.draw import compute_color_for_labels\nfrom utils.parser import get_config\nfrom utils.json_logger import BboxToJsonLogger\nimport warnings\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--VIDEO_PATH"", type=str, default=""./demo/ped.avi"")\n    parser.add_argument(""--config_detection"", type=str, default=""./configs/yolov3.yaml"")\n    parser.add_argument(""--config_deepsort"", type=str, default=""./configs/deep_sort.yaml"")\n    parser.add_argument(""--write-fps"", type=int, default=20)\n    parser.add_argument(""--frame_interval"", type=int, default=1)\n    parser.add_argument(""--save_path"", type=str, default=""./output"")\n    parser.add_argument(""--cpu"", dest=""use_cuda"", action=""store_false"", default=True)\n    args = parser.parse_args()\n\n    assert isfile(args.VIDEO_PATH), ""Error: Video not found""\n    assert is_video(args.VIDEO_PATH), ""Error: Not Supported format""\n    if args.frame_interval < 1: args.frame_interval = 1\n\n    return args\n\n\nclass VideoTracker(object):\n    def __init__(self, cfg, args):\n        self.cfg = cfg\n        self.args = args\n        use_cuda = args.use_cuda and torch.cuda.is_available()\n        if not use_cuda:\n            warnings.warn(""Running in cpu mode!"")\n\n        self.vdo = cv2.VideoCapture()\n        self.detector = build_detector(cfg, use_cuda=use_cuda)\n        self.deepsort = build_tracker(cfg, use_cuda=use_cuda)\n        self.class_names = self.detector.class_names\n\n        # Configure output video and json\n        self.logger = BboxToJsonLogger()\n        filename, extension = splitext(basename(self.args.VIDEO_PATH))\n        self.output_file = join(self.args.save_path, f\'{filename}.avi\')\n        self.json_output = join(self.args.save_path, f\'{filename}.json\')\n        if not isdir(dirname(self.json_output)):\n            makedirs(dirname(self.json_output))\n\n    def __enter__(self):\n        self.vdo.open(self.args.VIDEO_PATH)\n        self.total_frames = int(cv2.VideoCapture.get(self.vdo, cv2.CAP_PROP_FRAME_COUNT))\n        self.im_width = int(self.vdo.get(cv2.CAP_PROP_FRAME_WIDTH))\n        self.im_height = int(self.vdo.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n        video_details = {\'frame_width\': self.im_width,\n                         \'frame_height\': self.im_height,\n                         \'frame_rate\': self.args.write_fps,\n                         \'video_name\': self.args.VIDEO_PATH}\n        codec = cv2.VideoWriter_fourcc(*\'XVID\')\n        self.writer = cv2.VideoWriter(self.output_file, codec, self.args.write_fps,\n                                      (self.im_width, self.im_height))\n        self.logger.add_video_details(**video_details)\n\n        assert self.vdo.isOpened()\n        return self\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        if exc_type:\n            print(exc_type, exc_value, exc_traceback)\n\n    def run(self):\n        idx_frame = 0\n        pbar = tqdm(total=self.total_frames + 1)\n        while self.vdo.grab():\n            if idx_frame % args.frame_interval == 0:\n                _, ori_im = self.vdo.retrieve()\n                timestamp = self.vdo.get(cv2.CAP_PROP_POS_MSEC)\n                frame_id = int(self.vdo.get(cv2.CAP_PROP_POS_FRAMES))\n                self.logger.add_frame(frame_id=frame_id, timestamp=timestamp)\n                self.detection(frame=ori_im, frame_id=frame_id)\n                self.save_frame(ori_im)\n                idx_frame += 1\n            pbar.update()\n        self.logger.json_output(self.json_output)\n\n    @tik_tok\n    def detection(self, frame, frame_id):\n        im = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        # do detection\n        bbox_xywh, cls_conf, cls_ids = self.detector(im)\n        if bbox_xywh is not None:\n            # select person class\n            mask = cls_ids == 0\n\n            bbox_xywh = bbox_xywh[mask]\n            bbox_xywh[:, 3:] *= 1.2  # bbox dilation just in case bbox too small\n            cls_conf = cls_conf[mask]\n\n            # do tracking\n            outputs = self.deepsort.update(bbox_xywh, cls_conf, im)\n\n            # draw boxes for visualization\n            if len(outputs) > 0:\n                frame = self.draw_boxes(img=frame, frame_id=frame_id, output=outputs)\n\n    def draw_boxes(self, img, frame_id, output, offset=(0, 0)):\n        for i, box in enumerate(output):\n            x1, y1, x2, y2, identity = [int(ii) for ii in box]\n            self.logger.add_bbox_to_frame(frame_id=frame_id,\n                                          bbox_id=identity,\n                                          top=y1,\n                                          left=x1,\n                                          width=x2 - x1,\n                                          height=y2 - y1)\n            x1 += offset[0]\n            x2 += offset[0]\n            y1 += offset[1]\n            y2 += offset[1]\n\n            # box text and bar\n            self.logger.add_label_to_bbox(frame_id=frame_id, bbox_id=identity, category=\'pedestrian\', confidence=0.9)\n            color = compute_color_for_labels(identity)\n            label = \'{}{:d}\'.format("""", identity)\n            t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 2, 2)[0]\n            cv2.rectangle(img, (x1, y1), (x2, y2), color, 3)\n            cv2.rectangle(img, (x1, y1), (x1 + t_size[0] + 3, y1 + t_size[1] + 4), color, -1)\n            cv2.putText(img, label, (x1, y1 + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 2, [255, 255, 255], 2)\n        return img\n\n    def save_frame(self, frame) -> None:\n        if frame is not None: self.writer.write(frame)\n\n\nif __name__ == ""__main__"":\n    args = parse_args()\n    cfg = get_config()\n    cfg.merge_from_file(args.config_detection)\n    cfg.merge_from_file(args.config_deepsort)\n\n    with VideoTracker(cfg, args) as vdo_trk:\n        vdo_trk.run()\n\n'"
yolov3_deepsort.py,1,"b'import os\nimport cv2\nimport time\nimport argparse\nimport torch\nimport warnings\nimport numpy as np\n\nfrom detector import build_detector\nfrom deep_sort import build_tracker\nfrom utils.draw import draw_boxes\nfrom utils.parser import get_config\nfrom utils.log import get_logger\nfrom utils.io import write_results\n\n\nclass VideoTracker(object):\n    def __init__(self, cfg, args, video_path):\n        self.cfg = cfg\n        self.args = args\n        self.video_path = video_path\n        self.logger = get_logger(""root"")\n\n        use_cuda = args.use_cuda and torch.cuda.is_available()\n        if not use_cuda:\n            warnings.warn(""Running in cpu mode which maybe very slow!"", UserWarning)\n\n        if args.display:\n            cv2.namedWindow(""test"", cv2.WINDOW_NORMAL)\n            cv2.resizeWindow(""test"", args.display_width, args.display_height)\n\n        if args.cam != -1:\n            print(""Using webcam "" + str(args.cam))\n            self.vdo = cv2.VideoCapture(args.cam)\n        else:\n            self.vdo = cv2.VideoCapture()\n        self.detector = build_detector(cfg, use_cuda=use_cuda)\n        self.deepsort = build_tracker(cfg, use_cuda=use_cuda)\n        self.class_names = self.detector.class_names\n\n    def __enter__(self):\n        if self.args.cam != -1:\n            ret, frame = self.vdo.read()\n            assert ret, ""Error: Camera error""\n            self.im_width = frame.shape[0]\n            self.im_height = frame.shape[1]\n\n        else:\n            assert os.path.isfile(self.video_path), ""Path error""\n            self.vdo.open(self.video_path)\n            self.im_width = int(self.vdo.get(cv2.CAP_PROP_FRAME_WIDTH))\n            self.im_height = int(self.vdo.get(cv2.CAP_PROP_FRAME_HEIGHT))\n            assert self.vdo.isOpened()\n\n        if self.args.save_path:\n            os.makedirs(self.args.save_path, exist_ok=True)\n\n            # path of saved video and results\n            self.save_video_path = os.path.join(self.args.save_path, ""results.avi"")\n            self.save_results_path = os.path.join(self.args.save_path, ""results.txt"")\n\n            # create video writer\n            fourcc = cv2.VideoWriter_fourcc(*\'MJPG\')\n            self.writer = cv2.VideoWriter(self.save_video_path, fourcc, 20, (self.im_width, self.im_height))\n\n            # logging\n            self.logger.info(""Save results to {}"".format(self.args.save_path))\n\n        return self\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        if exc_type:\n            print(exc_type, exc_value, exc_traceback)\n\n    def run(self):\n        results = []\n        idx_frame = 0\n        while self.vdo.grab():\n            idx_frame += 1\n            if idx_frame % self.args.frame_interval:\n                continue\n\n            start = time.time()\n            _, ori_im = self.vdo.retrieve()\n            im = cv2.cvtColor(ori_im, cv2.COLOR_BGR2RGB)\n\n            # do detection\n            bbox_xywh, cls_conf, cls_ids = self.detector(im)\n\n            # select person class\n            mask = cls_ids == 0\n\n            bbox_xywh = bbox_xywh[mask]\n            # bbox dilation just in case bbox too small, delete this line if using a better pedestrian detector\n            bbox_xywh[:, 3:] *= 1.2\n            cls_conf = cls_conf[mask]\n\n            # do tracking\n            outputs = self.deepsort.update(bbox_xywh, cls_conf, im)\n\n            # draw boxes for visualization\n            if len(outputs) > 0:\n                bbox_tlwh = []\n                bbox_xyxy = outputs[:, :4]\n                identities = outputs[:, -1]\n                ori_im = draw_boxes(ori_im, bbox_xyxy, identities)\n\n                for bb_xyxy in bbox_xyxy:\n                    bbox_tlwh.append(self.deepsort._xyxy_to_tlwh(bb_xyxy))\n\n                results.append((idx_frame - 1, bbox_tlwh, identities))\n\n            end = time.time()\n\n            if self.args.display:\n                cv2.imshow(""test"", ori_im)\n                cv2.waitKey(1)\n\n            if self.args.save_path:\n                self.writer.write(ori_im)\n\n            # save results\n            write_results(self.save_results_path, results, \'mot\')\n\n            # logging\n            self.logger.info(""time: {:.03f}s, fps: {:.03f}, detection numbers: {}, tracking numbers: {}"" \\\n                             .format(end - start, 1 / (end - start), bbox_xywh.shape[0], len(outputs)))\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""VIDEO_PATH"", type=str)\n    parser.add_argument(""--config_detection"", type=str, default=""./configs/yolov3.yaml"")\n    parser.add_argument(""--config_deepsort"", type=str, default=""./configs/deep_sort.yaml"")\n    # parser.add_argument(""--ignore_display"", dest=""display"", action=""store_false"", default=True)\n    parser.add_argument(""--display"", action=""store_true"")\n    parser.add_argument(""--frame_interval"", type=int, default=1)\n    parser.add_argument(""--display_width"", type=int, default=800)\n    parser.add_argument(""--display_height"", type=int, default=600)\n    parser.add_argument(""--save_path"", type=str, default=""./output/"")\n    parser.add_argument(""--cpu"", dest=""use_cuda"", action=""store_false"", default=True)\n    parser.add_argument(""--camera"", action=""store"", dest=""cam"", type=int, default=""-1"")\n    return parser.parse_args()\n\n\nif __name__ == ""__main__"":\n    args = parse_args()\n    cfg = get_config()\n    cfg.merge_from_file(args.config_detection)\n    cfg.merge_from_file(args.config_deepsort)\n\n    with VideoTracker(cfg, args, video_path=args.VIDEO_PATH) as vdo_trk:\n        vdo_trk.run()\n'"
yolov3_deepsort_eval.py,0,"b'import os\r\nimport os.path as osp\r\nimport logging\r\nimport argparse\r\nfrom pathlib import Path\r\n\r\nfrom utils.log import get_logger\r\nfrom yolov3_deepsort import VideoTracker\r\nfrom utils.parser import get_config\r\n\r\nimport motmetrics as mm\r\nmm.lap.default_solver = \'lap\'\r\nfrom utils.evaluation import Evaluator\r\n\r\ndef mkdir_if_missing(dir):\r\n    os.makedirs(dir, exist_ok=True)\r\n\r\ndef main(data_root=\'\', seqs=(\'\',), args=""""):\r\n    logger = get_logger()\r\n    logger.setLevel(logging.INFO)\r\n    data_type = \'mot\'\r\n    result_root = os.path.join(Path(data_root), ""mot_results"")\r\n    mkdir_if_missing(result_root)\r\n\r\n    cfg = get_config()\r\n    cfg.merge_from_file(args.config_detection)\r\n    cfg.merge_from_file(args.config_deepsort)\r\n\r\n    # run tracking\r\n    accs = []\r\n    for seq in seqs:\r\n        logger.info(\'start seq: {}\'.format(seq))\r\n        result_filename = os.path.join(result_root, \'{}.txt\'.format(seq))\r\n        video_path = data_root+""/""+seq+""/video/video.mp4""\r\n\r\n        with VideoTracker(cfg, args, video_path, result_filename) as vdo_trk:\r\n            vdo_trk.run()\r\n\r\n        # eval\r\n        logger.info(\'Evaluate seq: {}\'.format(seq))\r\n        evaluator = Evaluator(data_root, seq, data_type)\r\n        accs.append(evaluator.eval_file(result_filename))\r\n\r\n    # get summary\r\n    metrics = mm.metrics.motchallenge_metrics\r\n    mh = mm.metrics.create()\r\n    summary = Evaluator.get_summary(accs, seqs, metrics)\r\n    strsummary = mm.io.render_summary(\r\n        summary,\r\n        formatters=mh.formatters,\r\n        namemap=mm.io.motchallenge_metric_names\r\n    )\r\n    print(strsummary)\r\n    Evaluator.save_summary(summary, os.path.join(result_root, \'summary_global.xlsx\'))\r\n\r\n\r\ndef parse_args():\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(""--config_detection"", type=str, default=""./configs/yolov3.yaml"")\r\n    parser.add_argument(""--config_deepsort"", type=str, default=""./configs/deep_sort.yaml"")\r\n    parser.add_argument(""--ignore_display"", dest=""display"", action=""store_false"", default=False)\r\n    parser.add_argument(""--frame_interval"", type=int, default=1)\r\n    parser.add_argument(""--display_width"", type=int, default=800)\r\n    parser.add_argument(""--display_height"", type=int, default=600)\r\n    parser.add_argument(""--save_path"", type=str, default=""./demo/demo.avi"")\r\n    parser.add_argument(""--cpu"", dest=""use_cuda"", action=""store_false"", default=True)\r\n    parser.add_argument(""--camera"", action=""store"", dest=""cam"", type=int, default=""-1"")\r\n    return parser.parse_args()\r\n\r\nif __name__ == \'__main__\':\r\n    args = parse_args()\r\n\r\n    seqs_str = \'\'\'MOT16-02       \r\n                  MOT16-04\r\n                  MOT16-05\r\n                  MOT16-09\r\n                  MOT16-10\r\n                  MOT16-11\r\n                  MOT16-13\r\n                  \'\'\'        \r\n    data_root = \'data/dataset/MOT16/train/\'\r\n\r\n    seqs = [seq.strip() for seq in seqs_str.split()]\r\n\r\n    main(data_root=data_root,\r\n         seqs=seqs,\r\n         args=args)'"
deep_sort/__init__.py,0,"b""from .deep_sort import DeepSort\n\n\n__all__ = ['DeepSort', 'build_tracker']\n\n\ndef build_tracker(cfg, use_cuda):\n    return DeepSort(cfg.DEEPSORT.REID_CKPT, \n                max_dist=cfg.DEEPSORT.MAX_DIST, min_confidence=cfg.DEEPSORT.MIN_CONFIDENCE, \n                nms_max_overlap=cfg.DEEPSORT.NMS_MAX_OVERLAP, max_iou_distance=cfg.DEEPSORT.MAX_IOU_DISTANCE, \n                max_age=cfg.DEEPSORT.MAX_AGE, n_init=cfg.DEEPSORT.N_INIT, nn_budget=cfg.DEEPSORT.NN_BUDGET, use_cuda=use_cuda)\n    \n\n\n\n\n\n\n\n\n\n"""
deep_sort/deep_sort.py,1,"b'import numpy as np\nimport torch\n\nfrom .deep.feature_extractor import Extractor\nfrom .sort.nn_matching import NearestNeighborDistanceMetric\nfrom .sort.preprocessing import non_max_suppression\nfrom .sort.detection import Detection\nfrom .sort.tracker import Tracker\n\n\n__all__ = [\'DeepSort\']\n\n\nclass DeepSort(object):\n    def __init__(self, model_path, max_dist=0.2, min_confidence=0.3, nms_max_overlap=1.0, max_iou_distance=0.7, max_age=70, n_init=3, nn_budget=100, use_cuda=True):\n        self.min_confidence = min_confidence\n        self.nms_max_overlap = nms_max_overlap\n\n        self.extractor = Extractor(model_path, use_cuda=use_cuda)\n\n        max_cosine_distance = max_dist\n        nn_budget = 100\n        metric = NearestNeighborDistanceMetric(""cosine"", max_cosine_distance, nn_budget)\n        self.tracker = Tracker(metric, max_iou_distance=max_iou_distance, max_age=max_age, n_init=n_init)\n\n    def update(self, bbox_xywh, confidences, ori_img):\n        self.height, self.width = ori_img.shape[:2]\n        # generate detections\n        features = self._get_features(bbox_xywh, ori_img)\n        bbox_tlwh = self._xywh_to_tlwh(bbox_xywh)\n        detections = [Detection(bbox_tlwh[i], conf, features[i]) for i,conf in enumerate(confidences) if conf>self.min_confidence]\n\n        # run on non-maximum supression\n        boxes = np.array([d.tlwh for d in detections])\n        scores = np.array([d.confidence for d in detections])\n        indices = non_max_suppression(boxes, self.nms_max_overlap, scores)\n        detections = [detections[i] for i in indices]\n\n        # update tracker\n        self.tracker.predict()\n        self.tracker.update(detections)\n\n        # output bbox identities\n        outputs = []\n        for track in self.tracker.tracks:\n            if not track.is_confirmed() or track.time_since_update > 1:\n                continue\n            box = track.to_tlwh()\n            x1,y1,x2,y2 = self._tlwh_to_xyxy(box)\n            track_id = track.track_id\n            outputs.append(np.array([x1,y1,x2,y2,track_id], dtype=np.int))\n        if len(outputs) > 0:\n            outputs = np.stack(outputs,axis=0)\n        return outputs\n\n\n    """"""\n    TODO:\n        Convert bbox from xc_yc_w_h to xtl_ytl_w_h\n    Thanks JieChen91@github.com for reporting this bug!\n    """"""\n    @staticmethod\n    def _xywh_to_tlwh(bbox_xywh):\n        if isinstance(bbox_xywh, np.ndarray):\n            bbox_tlwh = bbox_xywh.copy()\n        elif isinstance(bbox_xywh, torch.Tensor):\n            bbox_tlwh = bbox_xywh.clone()\n        bbox_tlwh[:,0] = bbox_xywh[:,0] - bbox_xywh[:,2]/2.\n        bbox_tlwh[:,1] = bbox_xywh[:,1] - bbox_xywh[:,3]/2.\n        return bbox_tlwh\n\n\n    def _xywh_to_xyxy(self, bbox_xywh):\n        x,y,w,h = bbox_xywh\n        x1 = max(int(x-w/2),0)\n        x2 = min(int(x+w/2),self.width-1)\n        y1 = max(int(y-h/2),0)\n        y2 = min(int(y+h/2),self.height-1)\n        return x1,y1,x2,y2\n\n    def _tlwh_to_xyxy(self, bbox_tlwh):\n        """"""\n        TODO:\n            Convert bbox from xtl_ytl_w_h to xc_yc_w_h\n        Thanks JieChen91@github.com for reporting this bug!\n        """"""\n        x,y,w,h = bbox_tlwh\n        x1 = max(int(x),0)\n        x2 = min(int(x+w),self.width-1)\n        y1 = max(int(y),0)\n        y2 = min(int(y+h),self.height-1)\n        return x1,y1,x2,y2\n\n    def _xyxy_to_tlwh(self, bbox_xyxy):\n        x1,y1,x2,y2 = bbox_xyxy\n\n        t = x1\n        l = y1\n        w = int(x2-x1)\n        h = int(y2-y1)\n        return t,l,w,h\n    \n    def _get_features(self, bbox_xywh, ori_img):\n        im_crops = []\n        for box in bbox_xywh:\n            x1,y1,x2,y2 = self._xywh_to_xyxy(box)\n            im = ori_img[y1:y2,x1:x2]\n            im_crops.append(im)\n        if im_crops:\n            features = self.extractor(im_crops)\n        else:\n            features = np.array([])\n        return features\n\n\n'"
detector/__init__.py,0,"b""from .YOLOv3 import YOLOv3\n\n\n__all__ = ['build_detector']\n\ndef build_detector(cfg, use_cuda):\n    return YOLOv3(cfg.YOLOV3.CFG, cfg.YOLOV3.WEIGHT, cfg.YOLOV3.CLASS_NAMES, \n                    score_thresh=cfg.YOLOV3.SCORE_THRESH, nms_thresh=cfg.YOLOV3.NMS_THRESH, \n                    is_xywh=True, use_cuda=use_cuda)\n"""
utils/__init__.py,0,b''
utils/asserts.py,0,"b'from os import environ\n\n\ndef assert_in(file, files_to_check):\n    if file not in files_to_check:\n        raise AssertionError(""{} does not exist in the list"".format(str(file)))\n    return True\n\n\ndef assert_in_env(check_list: list):\n    for item in check_list:\n        assert_in(item, environ.keys())\n    return True\n'"
utils/draw.py,0,"b'import numpy as np\r\nimport cv2\r\n\r\npalette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)\r\n\r\n\r\ndef compute_color_for_labels(label):\r\n    """"""\r\n    Simple function that adds fixed color depending on the class\r\n    """"""\r\n    color = [int((p * (label ** 2 - label + 1)) % 255) for p in palette]\r\n    return tuple(color)\r\n\r\n\r\ndef draw_boxes(img, bbox, identities=None, offset=(0,0)):\r\n    for i,box in enumerate(bbox):\r\n        x1,y1,x2,y2 = [int(i) for i in box]\r\n        x1 += offset[0]\r\n        x2 += offset[0]\r\n        y1 += offset[1]\r\n        y2 += offset[1]\r\n        # box text and bar\r\n        id = int(identities[i]) if identities is not None else 0    \r\n        color = compute_color_for_labels(id)\r\n        label = \'{}{:d}\'.format("""", id)\r\n        t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 2 , 2)[0]\r\n        cv2.rectangle(img,(x1, y1),(x2,y2),color,3)\r\n        cv2.rectangle(img,(x1, y1),(x1+t_size[0]+3,y1+t_size[1]+4), color,-1)\r\n        cv2.putText(img,label,(x1,y1+t_size[1]+4), cv2.FONT_HERSHEY_PLAIN, 2, [255,255,255], 2)\r\n    return img\r\n\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    for i in range(82):\r\n        print(compute_color_for_labels(i))\r\n'"
utils/evaluation.py,0,"b""import os\nimport numpy as np\nimport copy\nimport motmetrics as mm\nmm.lap.default_solver = 'lap'\nfrom utils.io import read_results, unzip_objs\n\n\nclass Evaluator(object):\n\n    def __init__(self, data_root, seq_name, data_type):\n        self.data_root = data_root\n        self.seq_name = seq_name\n        self.data_type = data_type\n\n        self.load_annotations()\n        self.reset_accumulator()\n\n    def load_annotations(self):\n        assert self.data_type == 'mot'\n\n        gt_filename = os.path.join(self.data_root, self.seq_name, 'gt', 'gt.txt')\n        self.gt_frame_dict = read_results(gt_filename, self.data_type, is_gt=True)\n        self.gt_ignore_frame_dict = read_results(gt_filename, self.data_type, is_ignore=True)\n\n    def reset_accumulator(self):\n        self.acc = mm.MOTAccumulator(auto_id=True)\n\n    def eval_frame(self, frame_id, trk_tlwhs, trk_ids, rtn_events=False):\n        # results\n        trk_tlwhs = np.copy(trk_tlwhs)\n        trk_ids = np.copy(trk_ids)\n\n        # gts\n        gt_objs = self.gt_frame_dict.get(frame_id, [])\n        gt_tlwhs, gt_ids = unzip_objs(gt_objs)[:2]\n\n        # ignore boxes\n        ignore_objs = self.gt_ignore_frame_dict.get(frame_id, [])\n        ignore_tlwhs = unzip_objs(ignore_objs)[0]\n\n\n        # remove ignored results\n        keep = np.ones(len(trk_tlwhs), dtype=bool)\n        iou_distance = mm.distances.iou_matrix(ignore_tlwhs, trk_tlwhs, max_iou=0.5)\n        if len(iou_distance) > 0:\n            match_is, match_js = mm.lap.linear_sum_assignment(iou_distance)\n            match_is, match_js = map(lambda a: np.asarray(a, dtype=int), [match_is, match_js])\n            match_ious = iou_distance[match_is, match_js]\n\n            match_js = np.asarray(match_js, dtype=int)\n            match_js = match_js[np.logical_not(np.isnan(match_ious))]\n            keep[match_js] = False\n            trk_tlwhs = trk_tlwhs[keep]\n            trk_ids = trk_ids[keep]\n\n        # get distance matrix\n        iou_distance = mm.distances.iou_matrix(gt_tlwhs, trk_tlwhs, max_iou=0.5)\n\n        # acc\n        self.acc.update(gt_ids, trk_ids, iou_distance)\n\n        if rtn_events and iou_distance.size > 0 and hasattr(self.acc, 'last_mot_events'):\n            events = self.acc.last_mot_events  # only supported by https://github.com/longcw/py-motmetrics\n        else:\n            events = None\n        return events\n\n    def eval_file(self, filename):\n        self.reset_accumulator()\n\n        result_frame_dict = read_results(filename, self.data_type, is_gt=False)\n        frames = sorted(list(set(self.gt_frame_dict.keys()) | set(result_frame_dict.keys())))\n        for frame_id in frames:\n            trk_objs = result_frame_dict.get(frame_id, [])\n            trk_tlwhs, trk_ids = unzip_objs(trk_objs)[:2]\n            self.eval_frame(frame_id, trk_tlwhs, trk_ids, rtn_events=False)\n\n        return self.acc\n\n    @staticmethod\n    def get_summary(accs, names, metrics=('mota', 'num_switches', 'idp', 'idr', 'idf1', 'precision', 'recall')):\n        names = copy.deepcopy(names)\n        if metrics is None:\n            metrics = mm.metrics.motchallenge_metrics\n        metrics = copy.deepcopy(metrics)\n\n        mh = mm.metrics.create()\n        summary = mh.compute_many(\n            accs,\n            metrics=metrics,\n            names=names,\n            generate_overall=True\n        )\n\n        return summary\n\n    @staticmethod\n    def save_summary(summary, filename):\n        import pandas as pd\n        writer = pd.ExcelWriter(filename)\n        summary.to_excel(writer)\n        writer.save()\n"""
utils/io.py,0,"b'import os\nfrom typing import Dict\nimport numpy as np\n\n# from utils.log import get_logger\n\n\ndef write_results(filename, results, data_type):\n    if data_type == \'mot\':\n        save_format = \'{frame},{id},{x1},{y1},{w},{h},-1,-1,-1,-1\\n\'\n    elif data_type == \'kitti\':\n        save_format = \'{frame} {id} pedestrian 0 0 -10 {x1} {y1} {x2} {y2} -10 -10 -10 -1000 -1000 -1000 -10\\n\'\n    else:\n        raise ValueError(data_type)\n\n    with open(filename, \'w\') as f:\n        for frame_id, tlwhs, track_ids in results:\n            if data_type == \'kitti\':\n                frame_id -= 1\n            for tlwh, track_id in zip(tlwhs, track_ids):\n                if track_id < 0:\n                    continue\n                x1, y1, w, h = tlwh\n                x2, y2 = x1 + w, y1 + h\n                line = save_format.format(frame=frame_id, id=track_id, x1=x1, y1=y1, x2=x2, y2=y2, w=w, h=h)\n                f.write(line)\n\n\n# def write_results(filename, results_dict: Dict, data_type: str):\n#     if not filename:\n#         return\n#     path = os.path.dirname(filename)\n#     if not os.path.exists(path):\n#         os.makedirs(path)\n\n#     if data_type in (\'mot\', \'mcmot\', \'lab\'):\n#         save_format = \'{frame},{id},{x1},{y1},{w},{h},1,-1,-1,-1\\n\'\n#     elif data_type == \'kitti\':\n#         save_format = \'{frame} {id} pedestrian -1 -1 -10 {x1} {y1} {x2} {y2} -1 -1 -1 -1000 -1000 -1000 -10 {score}\\n\'\n#     else:\n#         raise ValueError(data_type)\n\n#     with open(filename, \'w\') as f:\n#         for frame_id, frame_data in results_dict.items():\n#             if data_type == \'kitti\':\n#                 frame_id -= 1\n#             for tlwh, track_id in frame_data:\n#                 if track_id < 0:\n#                     continue\n#                 x1, y1, w, h = tlwh\n#                 x2, y2 = x1 + w, y1 + h\n#                 line = save_format.format(frame=frame_id, id=track_id, x1=x1, y1=y1, x2=x2, y2=y2, w=w, h=h, score=1.0)\n#                 f.write(line)\n#     logger.info(\'Save results to {}\'.format(filename))\n\n\ndef read_results(filename, data_type: str, is_gt=False, is_ignore=False):\n    if data_type in (\'mot\', \'lab\'):\n        read_fun = read_mot_results\n    else:\n        raise ValueError(\'Unknown data type: {}\'.format(data_type))\n\n    return read_fun(filename, is_gt, is_ignore)\n\n\n""""""\nlabels={\'ped\', ...\t\t\t% 1\n\'person_on_vhcl\', ...\t% 2\n\'car\', ...\t\t\t\t% 3\n\'bicycle\', ...\t\t\t% 4\n\'mbike\', ...\t\t\t% 5\n\'non_mot_vhcl\', ...\t\t% 6\n\'static_person\', ...\t% 7\n\'distractor\', ...\t\t% 8\n\'occluder\', ...\t\t\t% 9\n\'occluder_on_grnd\', ...\t\t%10\n\'occluder_full\', ...\t\t% 11\n\'reflection\', ...\t\t% 12\n\'crowd\' ...\t\t\t% 13\n};\n""""""\n\n\ndef read_mot_results(filename, is_gt, is_ignore):\n    valid_labels = {1}\n    ignore_labels = {2, 7, 8, 12}\n    results_dict = dict()\n    if os.path.isfile(filename):\n        with open(filename, \'r\') as f:\n            for line in f.readlines():\n                linelist = line.split(\',\')\n                if len(linelist) < 7:\n                    continue\n                fid = int(linelist[0])\n                if fid < 1:\n                    continue\n                results_dict.setdefault(fid, list())\n\n                if is_gt:\n                    if \'MOT16-\' in filename or \'MOT17-\' in filename:\n                        label = int(float(linelist[7]))\n                        mark = int(float(linelist[6]))\n                        if mark == 0 or label not in valid_labels:\n                            continue\n                    score = 1\n                elif is_ignore:\n                    if \'MOT16-\' in filename or \'MOT17-\' in filename:\n                        label = int(float(linelist[7]))\n                        vis_ratio = float(linelist[8])\n                        if label not in ignore_labels and vis_ratio >= 0:\n                            continue\n                    else:\n                        continue\n                    score = 1\n                else:\n                    score = float(linelist[6])\n\n                tlwh = tuple(map(float, linelist[2:6]))\n                target_id = int(linelist[1])\n\n                results_dict[fid].append((tlwh, target_id, score))\n\n    return results_dict\n\n\ndef unzip_objs(objs):\n    if len(objs) > 0:\n        tlwhs, ids, scores = zip(*objs)\n    else:\n        tlwhs, ids, scores = [], [], []\n    tlwhs = np.asarray(tlwhs, dtype=float).reshape(-1, 4)\n\n    return tlwhs, ids, scores'"
utils/json_logger.py,0,"b'""""""\nReferences:\n    https://medium.com/analytics-vidhya/creating-a-custom-logging-mechanism-for-real-time-object-detection-using-tdd-4ca2cfcd0a2f\n""""""\nimport json\nfrom os import makedirs\nfrom os.path import exists, join\nfrom datetime import datetime\n\n\nclass JsonMeta(object):\n    HOURS = 3\n    MINUTES = 59\n    SECONDS = 59\n    PATH_TO_SAVE = \'LOGS\'\n    DEFAULT_FILE_NAME = \'remaining\'\n\n\nclass BaseJsonLogger(object):\n    """"""\n    This is the base class that returns __dict__ of its own\n    it also returns the dicts of objects in the attributes that are list instances\n\n    """"""\n\n    def dic(self):\n        # returns dicts of objects\n        out = {}\n        for k, v in self.__dict__.items():\n            if hasattr(v, \'dic\'):\n                out[k] = v.dic()\n            elif isinstance(v, list):\n                out[k] = self.list(v)\n            else:\n                out[k] = v\n        return out\n\n    @staticmethod\n    def list(values):\n        # applies the dic method on items in the list\n        return [v.dic() if hasattr(v, \'dic\') else v for v in values]\n\n\nclass Label(BaseJsonLogger):\n    """"""\n    For each bounding box there are various categories with confidences. Label class keeps track of that information.\n    """"""\n\n    def __init__(self, category: str, confidence: float):\n        self.category = category\n        self.confidence = confidence\n\n\nclass Bbox(BaseJsonLogger):\n    """"""\n    This module stores the information for each frame and use them in JsonParser\n    Attributes:\n        labels (list): List of label module.\n        top (int):\n        left (int):\n        width (int):\n        height (int):\n\n    Args:\n        bbox_id (float):\n        top (int):\n        left (int):\n        width (int):\n        height (int):\n\n    References:\n        Check Label module for better understanding.\n\n\n    """"""\n\n    def __init__(self, bbox_id, top, left, width, height):\n        self.labels = []\n        self.bbox_id = bbox_id\n        self.top = top\n        self.left = left\n        self.width = width\n        self.height = height\n\n    def add_label(self, category, confidence):\n        # adds category and confidence only if top_k is not exceeded.\n        self.labels.append(Label(category, confidence))\n\n    def labels_full(self, value):\n        return len(self.labels) == value\n\n\nclass Frame(BaseJsonLogger):\n    """"""\n    This module stores the information for each frame and use them in JsonParser\n    Attributes:\n        timestamp (float): The elapsed time of captured frame\n        frame_id (int): The frame number of the captured video\n        bboxes (list of Bbox objects): Stores the list of bbox objects.\n\n    References:\n        Check Bbox class for better information\n\n    Args:\n        timestamp (float):\n        frame_id (int):\n\n    """"""\n\n    def __init__(self, frame_id: int, timestamp: float = None):\n        self.frame_id = frame_id\n        self.timestamp = timestamp\n        self.bboxes = []\n\n    def add_bbox(self, bbox_id: int, top: int, left: int, width: int, height: int):\n        bboxes_ids = [bbox.bbox_id for bbox in self.bboxes]\n        if bbox_id not in bboxes_ids:\n            self.bboxes.append(Bbox(bbox_id, top, left, width, height))\n        else:\n            raise ValueError(""Frame with id: {} already has a Bbox with id: {}"".format(self.frame_id, bbox_id))\n\n    def add_label_to_bbox(self, bbox_id: int, category: str, confidence: float):\n        bboxes = {bbox.id: bbox for bbox in self.bboxes}\n        if bbox_id in bboxes.keys():\n            res = bboxes.get(bbox_id)\n            res.add_label(category, confidence)\n        else:\n            raise ValueError(\'the bbox with id: {} does not exists!\'.format(bbox_id))\n\n\nclass BboxToJsonLogger(BaseJsonLogger):\n    """"""\n    \xd9\x8f This module is designed to automate the task of logging jsons. An example json is used\n    to show the contents of json file shortly\n    Example:\n          {\n          ""video_details"": {\n            ""frame_width"": 1920,\n            ""frame_height"": 1080,\n            ""frame_rate"": 20,\n            ""video_name"": ""/home/gpu/codes/MSD/pedestrian_2/project/public/camera1.avi""\n          },\n          ""frames"": [\n            {\n              ""frame_id"": 329,\n              ""timestamp"": 3365.1254\n              ""bboxes"": [\n                {\n                  ""labels"": [\n                    {\n                      ""category"": ""pedestrian"",\n                      ""confidence"": 0.9\n                    }\n                  ],\n                  ""bbox_id"": 0,\n                  ""top"": 1257,\n                  ""left"": 138,\n                  ""width"": 68,\n                  ""height"": 109\n                }\n              ]\n            }],\n\n    Attributes:\n        frames (dict): It\'s a dictionary that maps each frame_id to json attributes.\n        video_details (dict): information about video file.\n        top_k_labels (int): shows the allowed number of labels\n        start_time (datetime object): we use it to automate the json output by time.\n\n    Args:\n        top_k_labels (int): shows the allowed number of labels\n\n    """"""\n\n    def __init__(self, top_k_labels: int = 1):\n        self.frames = {}\n        self.video_details = self.video_details = dict(frame_width=None, frame_height=None, frame_rate=None,\n                                                       video_name=None)\n        self.top_k_labels = top_k_labels\n        self.start_time = datetime.now()\n\n    def set_top_k(self, value):\n        self.top_k_labels = value\n\n    def frame_exists(self, frame_id: int) -> bool:\n        """"""\n        Args:\n            frame_id (int):\n\n        Returns:\n            bool: true if frame_id is recognized\n        """"""\n        return frame_id in self.frames.keys()\n\n    def add_frame(self, frame_id: int, timestamp: float = None) -> None:\n        """"""\n        Args:\n            frame_id (int):\n            timestamp (float): opencv captured frame time property\n\n        Raises:\n             ValueError: if frame_id would not exist in class frames attribute\n\n        Returns:\n            None\n\n        """"""\n        if not self.frame_exists(frame_id):\n            self.frames[frame_id] = Frame(frame_id, timestamp)\n        else:\n            raise ValueError(""Frame id: {} already exists"".format(frame_id))\n\n    def bbox_exists(self, frame_id: int, bbox_id: int) -> bool:\n        """"""\n        Args:\n            frame_id:\n            bbox_id:\n\n        Returns:\n            bool: if bbox exists in frame bboxes list\n        """"""\n        bboxes = []\n        if self.frame_exists(frame_id=frame_id):\n            bboxes = [bbox.bbox_id for bbox in self.frames[frame_id].bboxes]\n        return bbox_id in bboxes\n\n    def find_bbox(self, frame_id: int, bbox_id: int):\n        """"""\n\n        Args:\n            frame_id:\n            bbox_id:\n\n        Returns:\n            bbox_id (int):\n\n        Raises:\n            ValueError: if bbox_id does not exist in the bbox list of specific frame.\n        """"""\n        if not self.bbox_exists(frame_id, bbox_id):\n            raise ValueError(""frame with id: {} does not contain bbox with id: {}"".format(frame_id, bbox_id))\n        bboxes = {bbox.bbox_id: bbox for bbox in self.frames[frame_id].bboxes}\n        return bboxes.get(bbox_id)\n\n    def add_bbox_to_frame(self, frame_id: int, bbox_id: int, top: int, left: int, width: int, height: int) -> None:\n        """"""\n\n        Args:\n            frame_id (int):\n            bbox_id (int):\n            top (int):\n            left (int):\n            width (int):\n            height (int):\n\n        Returns:\n            None\n\n        Raises:\n            ValueError: if bbox_id already exist in frame information with frame_id\n            ValueError: if frame_id does not exist in frames attribute\n        """"""\n        if self.frame_exists(frame_id):\n            frame = self.frames[frame_id]\n            if not self.bbox_exists(frame_id, bbox_id):\n                frame.add_bbox(bbox_id, top, left, width, height)\n            else:\n                raise ValueError(\n                    ""frame with frame_id: {} already contains the bbox with id: {} "".format(frame_id, bbox_id))\n        else:\n            raise ValueError(""frame with frame_id: {} does not exist"".format(frame_id))\n\n    def add_label_to_bbox(self, frame_id: int, bbox_id: int, category: str, confidence: float):\n        """"""\n        Args:\n            frame_id:\n            bbox_id:\n            category:\n            confidence: the confidence value returned from yolo detection\n\n        Returns:\n            None\n\n        Raises:\n            ValueError: if labels quota (top_k_labels) exceeds.\n        """"""\n        bbox = self.find_bbox(frame_id, bbox_id)\n        if not bbox.labels_full(self.top_k_labels):\n            bbox.add_label(category, confidence)\n        else:\n            raise ValueError(""labels in frame_id: {}, bbox_id: {} is fulled"".format(frame_id, bbox_id))\n\n    def add_video_details(self, frame_width: int = None, frame_height: int = None, frame_rate: int = None,\n                          video_name: str = None):\n        self.video_details[\'frame_width\'] = frame_width\n        self.video_details[\'frame_height\'] = frame_height\n        self.video_details[\'frame_rate\'] = frame_rate\n        self.video_details[\'video_name\'] = video_name\n\n    def output(self):\n        output = {\'video_details\': self.video_details}\n        result = list(self.frames.values())\n        output[\'frames\'] = [item.dic() for item in result]\n        return output\n\n    def json_output(self, output_name):\n        """"""\n        Args:\n            output_name:\n\n        Returns:\n            None\n\n        Notes:\n            It creates the json output with `output_name` name.\n        """"""\n        if not output_name.endswith(\'.json\'):\n            output_name += \'.json\'\n        with open(output_name, \'w\') as file:\n            json.dump(self.output(), file)\n        file.close()\n\n    def set_start(self):\n        self.start_time = datetime.now()\n\n    def schedule_output_by_time(self, output_dir=JsonMeta.PATH_TO_SAVE, hours: int = 0, minutes: int = 0,\n                                seconds: int = 60) -> None:\n        """"""\n        Notes:\n            Creates folder and then periodically stores the jsons on that address.\n\n        Args:\n            output_dir (str): the directory where output files will be stored\n            hours (int):\n            minutes (int):\n            seconds (int):\n\n        Returns:\n            None\n\n        """"""\n        end = datetime.now()\n        interval = 0\n        interval += abs(min([hours, JsonMeta.HOURS]) * 3600)\n        interval += abs(min([minutes, JsonMeta.MINUTES]) * 60)\n        interval += abs(min([seconds, JsonMeta.SECONDS]))\n        diff = (end - self.start_time).seconds\n\n        if diff > interval:\n            output_name = self.start_time.strftime(\'%Y-%m-%d %H-%M-%S\') + \'.json\'\n            if not exists(output_dir):\n                makedirs(output_dir)\n            output = join(output_dir, output_name)\n            self.json_output(output_name=output)\n            self.frames = {}\n            self.start_time = datetime.now()\n\n    def schedule_output_by_frames(self, frames_quota, frame_counter, output_dir=JsonMeta.PATH_TO_SAVE):\n        """"""\n        saves as the number of frames quota increases higher.\n        :param frames_quota:\n        :param frame_counter:\n        :param output_dir:\n        :return:\n        """"""\n        pass\n\n    def flush(self, output_dir):\n        """"""\n        Notes:\n            We use this function to output jsons whenever possible.\n            like the time that we exit the while loop of opencv.\n\n        Args:\n            output_dir:\n\n        Returns:\n            None\n\n        """"""\n        filename = self.start_time.strftime(\'%Y-%m-%d %H-%M-%S\') + \'-remaining.json\'\n        output = join(output_dir, filename)\n        self.json_output(output_name=output)\n'"
utils/log.py,0,"b""import logging\r\n\r\n\r\ndef get_logger(name='root'):\r\n    formatter = logging.Formatter(\r\n        # fmt='%(asctime)s [%(levelname)s]: %(filename)s(%(funcName)s:%(lineno)s) >> %(message)s')\r\n        fmt='%(asctime)s [%(levelname)s]: %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\r\n\r\n    handler = logging.StreamHandler()\r\n    handler.setFormatter(formatter)\r\n\r\n    logger = logging.getLogger(name)\r\n    logger.setLevel(logging.INFO)\r\n    logger.addHandler(handler)\r\n    return logger\r\n\r\n\r\n"""
utils/parser.py,0,"b'import os\nimport yaml\nfrom easydict import EasyDict as edict\n\nclass YamlParser(edict):\n    """"""\n    This is yaml parser based on EasyDict.\n    """"""\n    def __init__(self, cfg_dict=None, config_file=None):\n        if cfg_dict is None:\n            cfg_dict = {}\n\n        if config_file is not None:\n            assert(os.path.isfile(config_file))\n            with open(config_file, \'r\') as fo:\n                cfg_dict.update(yaml.load(fo.read()))\n\n        super(YamlParser, self).__init__(cfg_dict)\n\n    \n    def merge_from_file(self, config_file):\n        with open(config_file, \'r\') as fo:\n            self.update(yaml.load(fo.read()))\n\n    \n    def merge_from_dict(self, config_dict):\n        self.update(config_dict)\n\n\ndef get_config(config_file=None):\n    return YamlParser(config_file=config_file)\n\n\nif __name__ == ""__main__"":\n    cfg = YamlParser(config_file=""../configs/yolov3.yaml"")\n    cfg.merge_from_file(""../configs/deep_sort.yaml"")\n\n    import ipdb; ipdb.set_trace()'"
utils/tools.py,0,"b'from functools import wraps\nfrom time import time\n\n\ndef is_video(ext: str):\n    """"""\n    Returns true if ext exists in\n    allowed_exts for video files.\n\n    Args:\n        ext:\n\n    Returns:\n\n    """"""\n\n    allowed_exts = (\'.mp4\', \'.webm\', \'.ogg\', \'.avi\', \'.wmv\', \'.mkv\', \'.3gp\')\n    return any((ext.endswith(x) for x in allowed_exts))\n\n\ndef tik_tok(func):\n    """"""\n    keep track of time for each process.\n    Args:\n        func:\n\n    Returns:\n\n    """"""\n    @wraps(func)\n    def _time_it(*args, **kwargs):\n        start = time()\n        try:\n            return func(*args, **kwargs)\n        finally:\n            end_ = time()\n            print(""time: {:.03f}s, fps: {:.03f}"".format(end_ - start, 1 / (end_ - start)))\n\n    return _time_it\n'"
webserver/__init__.py,0,b''
webserver/rtsp_threaded_tracker.py,1,"b'import warnings\nfrom os import getenv\nimport sys\nfrom os.path import dirname, abspath\n\nsys.path.append(dirname(dirname(abspath(__file__))))\n\nimport torch\nfrom deep_sort import build_tracker\nfrom detector import build_detector\nimport cv2\nfrom utils.draw import compute_color_for_labels\nfrom concurrent.futures import ThreadPoolExecutor\nfrom redis import Redis\n\nredis_cache = Redis(\'127.0.0.1\')\n\n\nclass RealTimeTracking(object):\n    """"""\n    This class is built to get frame from rtsp link and continuously\n    assign each frame to an attribute namely as frame in order to\n    compensate the network packet loss. then we use flask to give it\n    as service to client.\n    Args:\n        args: parse_args inputs\n        cfg: deepsort dict and yolo-model cfg from server_cfg file\n\n    """"""\n\n    def __init__(self, cfg, args):\n        # Create a VideoCapture object\n        self.cfg = cfg\n        self.args = args\n        use_cuda = self.args.use_cuda and torch.cuda.is_available()\n\n        if not use_cuda:\n            warnings.warn(UserWarning(""Running in cpu mode!""))\n\n        self.detector = build_detector(cfg, use_cuda=use_cuda)\n        self.deepsort = build_tracker(cfg, use_cuda=use_cuda)\n        self.class_names = self.detector.class_names\n\n        self.vdo = cv2.VideoCapture(self.args.input)\n        self.status, self.frame = None, None\n        self.total_frames = int(cv2.VideoCapture.get(self.vdo, cv2.CAP_PROP_FRAME_COUNT))\n        self.im_width = int(self.vdo.get(cv2.CAP_PROP_FRAME_WIDTH))\n        self.im_height = int(self.vdo.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n        self.output_frame = None\n\n        self.thread = ThreadPoolExecutor(max_workers=1)\n        self.thread.submit(self.update)\n\n    def update(self):\n        while True:\n            if self.vdo.isOpened():\n                (self.status, self.frame) = self.vdo.read()\n\n    def run(self):\n        print(\'streaming started ...\')\n        while getenv(\'in_progress\') != \'off\':\n            try:\n                frame = self.frame.copy()\n                self.detection(frame=frame)\n                frame_to_bytes = cv2.imencode(\'.jpg\', frame)[1].tobytes()\n                redis_cache.set(\'frame\', frame_to_bytes)\n            except AttributeError:\n                pass\n        print(\'streaming stopped ...\')\n\n\n    def detection(self, frame):\n        im = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        # do detection\n        bbox_xywh, cls_conf, cls_ids = self.detector(im)\n        if bbox_xywh is not None:\n            # select person class\n            mask = cls_ids == 0\n\n            bbox_xywh = bbox_xywh[mask]\n            bbox_xywh[:, 3:] *= 1.2  # bbox dilation just in case bbox too small\n            cls_conf = cls_conf[mask]\n\n            # do tracking\n            outputs = self.deepsort.update(bbox_xywh, cls_conf, im)\n\n            # draw boxes for visualization\n            if len(outputs) > 0:\n                self.draw_boxes(img=frame, output=outputs)\n\n    @staticmethod\n    def draw_boxes(img, output, offset=(0, 0)):\n        for i, box in enumerate(output):\n            x1, y1, x2, y2, identity = [int(ii) for ii in box]\n            x1 += offset[0]\n            x2 += offset[0]\n            y1 += offset[1]\n            y2 += offset[1]\n\n            # box text and bar\n            color = compute_color_for_labels(identity)\n            label = \'{}{:d}\'.format("""", identity)\n            t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 2, 2)[0]\n            cv2.rectangle(img, (x1, y1), (x2, y2), color, 3)\n            cv2.rectangle(img, (x1, y1), (x1 + t_size[0] + 3, y1 + t_size[1] + 4), color, -1)\n            cv2.putText(img, label, (x1, y1 + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 2, [255, 255, 255], 2)\n        return img\n'"
webserver/rtsp_webserver.py,0,"b'""""""\n\n# TODO: Load ML model with redis and keep it for sometime.\n    1- detector/yolov3/detector.py |=> yolov3 weightfile -> redis cache\n    2- deepsort/deep/feature_extractor |=> model_path -> redis cache\n    3- Use tmpfs (Insert RAM as a virtual disk and store model state): https://pypi.org/project/memory-tempfile/\n\n""""""\nfrom os.path import join\nfrom os import getenv, environ\nfrom dotenv import load_dotenv\nimport argparse\nfrom threading import Thread\n\nfrom redis import Redis\nfrom flask import Response, Flask, jsonify, request, abort\n\nfrom rtsp_threaded_tracker import RealTimeTracking\nfrom server_cfg import model, deep_sort_dict\nfrom config.config import DevelopmentConfig\nfrom utils.parser import get_config\n\nredis_cache = Redis(\'127.0.0.1\')\napp = Flask(__name__)\nenviron[\'in_progress\'] = \'off\'\n\n\ndef parse_args():\n    """"""\n    Parses the arguments\n    Returns:\n        argparse Namespace\n    """"""\n    assert \'project_root\' in environ.keys()\n    project_root = getenv(\'project_root\')\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(""--input"",\n                        type=str,\n                        default=getenv(\'camera_stream\'))\n\n    parser.add_argument(""--model"",\n                        type=str,\n                        default=join(project_root,\n                                     getenv(\'model_type\')))\n\n    parser.add_argument(""--cpu"",\n                        dest=""use_cuda"",\n                        action=""store_false"", default=True)\n    args = parser.parse_args()\n\n    return args\n\n\ndef gen():\n    """"""\n\n    Returns: video frames from redis cache\n\n    """"""\n    while True:\n        frame = redis_cache.get(\'frame\')\n        if frame is not None:\n            yield b\'--frame\\r\\n\'b\'Content-Type: image/jpeg\\r\\n\\r\\n\' + frame + b\'\\r\\n\'\n\n\ndef pedestrian_tracking(cfg, args):\n    """"""\n    starts the pedestrian detection on rtsp link\n    Args:\n        cfg:\n        args:\n\n    Returns:\n\n    """"""\n    tracker = RealTimeTracking(cfg, args)\n    tracker.run()\n\n\ndef trigger_process(cfg, args):\n    """"""\n    triggers pedestrian_tracking process on rtsp link using a thread\n    Args:\n        cfg:\n        args:\n\n    Returns:\n    """"""\n    try:\n        t = Thread(target=pedestrian_tracking, args=(cfg, args))\n        t.start()\n        return jsonify({""message"": ""Pedestrian detection started successfully""})\n    except Exception:\n        return jsonify({\'message\': ""Unexpected exception occured in process""})\n\n\n@app.errorhandler(400)\ndef bad_argument(error):\n    return jsonify({\'message\': error.description[\'message\']})\n\n\n# Routes\n@app.route(\'/stream\', methods=[\'GET\'])\ndef stream():\n    """"""\n    Provides video frames on http link\n    Returns:\n\n    """"""\n    return Response(gen(),\n                    mimetype=\'multipart/x-mixed-replace; boundary=frame\')\n\n\n@app.route(""/run"", methods=[\'GET\'])\ndef process_manager():\n    """"""\n    request parameters:\n    run (bool): 1  -> start the pedestrian tracking\n                0  -> stop it\n    camera_stream: str -> rtsp link to security camera\n\n    :return:\n    """"""\n    # data = request.args\n    data = request.args\n    status = data[\'run\']\n    status = int(status) if status.isnumeric() else abort(400, {\'message\': f""bad argument for run {data[\'run\']}""})\n    if status == 1:\n        # if pedestrian tracking is not running, start it off!\n        try:\n            if environ.get(\'in_progress\', \'off\') == \'off\':\n                global cfg, args\n                vdo = data.get(\'camera_stream\')\n                if vdo is not None:\n                    args.input = int(vdo)\n                environ[\'in_progress\'] = \'on\'\n                return trigger_process(cfg, args)\n            elif environ.get(\'in_progress\') == \'on\':\n                # if pedestrian tracking is running, don\'t start another one (we are short of gpu resources)\n                return jsonify({""message"": "" Pedestrian detection is already in progress.""})\n        except Exception:\n            environ[\'in_progress\'] = \'off\'\n            return abort(503)\n    elif status == 0:\n        if environ.get(\'in_progress\', \'off\') == \'off\':\n            return jsonify({""message"": ""pedestrian detection is already terminated!""})\n        else:\n            environ[\'in_progress\'] = \'off\'\n            return jsonify({""message"": ""Pedestrian detection terminated!""})\n\n\nif __name__ == \'__main__\':\n    load_dotenv()\n    app.config.from_object(DevelopmentConfig)\n\n    # BackProcess Initialization\n    args = parse_args()\n    cfg = get_config()\n    cfg.merge_from_dict(model)\n    cfg.merge_from_dict(deep_sort_dict)\n    # Start the flask app\n    app.run()\n'"
webserver/server_cfg.py,0,"b'""""""""""""\nimport sys\nfrom os.path import dirname, abspath, isfile\n\nsys.path.append(dirname(dirname(abspath(__file__))))\n\nfrom dotenv import load_dotenv\nfrom utils.asserts import assert_in_env\nfrom os import getenv\nfrom os.path import join\n\nload_dotenv(\'.env\')\n# Configure deep sort info\ndeep_sort_info = dict(REID_CKPT=join(getenv(\'project_root\'), getenv(\'reid_ckpt\')),\n                      MAX_DIST=0.2,\n                      MIN_CONFIDENCE=.3,\n                      NMS_MAX_OVERLAP=0.5,\n                      MAX_IOU_DISTANCE=0.7,\n                      N_INIT=3,\n                      MAX_AGE=70,\n                      NN_BUDGET=100)\ndeep_sort_dict = {\'DEEPSORT\': deep_sort_info}\n\n# Configure yolov3 info\n\nyolov3_info = dict(CFG=join(getenv(\'project_root\'), getenv(\'yolov3_cfg\')),\n                   WEIGHT=join(getenv(\'project_root\'), getenv(\'yolov3_weight\')),\n                   CLASS_NAMES=join(getenv(\'project_root\'), getenv(\'yolov3_class_names\')),\n                   SCORE_THRESH=0.5,\n                   NMS_THRESH=0.4\n                   )\nyolov3_dict = {\'YOLOV3\': yolov3_info}\n\n# Configure yolov3-tiny info\n\nyolov3_tiny_info = dict(CFG=join(getenv(\'project_root\'), getenv(\'yolov3_tiny_cfg\')),\n                        WEIGHT=join(getenv(\'project_root\'), getenv(\'yolov3_tiny_weight\')),\n                        CLASS_NAMES=join(getenv(\'project_root\'), getenv(\'yolov3_class_names\')),\n                        SCORE_THRESH=0.5,\n                        NMS_THRESH=0.4\n                        )\nyolov3_tiny_dict = {\'YOLOV3\': yolov3_tiny_info}\n\n\ncheck_list = [\'project_root\', \'reid_ckpt\', \'yolov3_class_names\', \'model_type\', \'yolov3_cfg\', \'yolov3_weight\',\n              \'yolov3_tiny_cfg\', \'yolov3_tiny_weight\', \'yolov3_class_names\']\n\nif assert_in_env(check_list):\n    assert isfile(deep_sort_info[\'REID_CKPT\'])\n    if getenv(\'model_type\') == \'yolov3\':\n        assert isfile(yolov3_info[\'WEIGHT\'])\n        assert isfile(yolov3_info[\'CFG\'])\n        assert isfile(yolov3_info[\'CLASS_NAMES\'])\n        model = yolov3_dict.copy()\n\n    elif getenv(\'model_type\') == \'yolov3_tiny\':\n        assert isfile(yolov3_tiny_info[\'WEIGHT\'])\n        assert isfile(yolov3_tiny_info[\'CFG\'])\n        assert isfile(yolov3_tiny_info[\'CLASS_NAMES\'])\n        model = yolov3_tiny_dict.copy()\n    else:\n        raise ValueError(""Value \'{}\' for model_type is not valid"".format(getenv(\'model_type\')))\n'"
deep_sort/deep/__init__.py,0,b''
deep_sort/deep/evaluate.py,1,"b'import torch\r\n\r\nfeatures = torch.load(""features.pth"")\r\nqf = features[""qf""]\r\nql = features[""ql""]\r\ngf = features[""gf""]\r\ngl = features[""gl""]\r\n\r\nscores = qf.mm(gf.t())\r\nres = scores.topk(5, dim=1)[1][:,0]\r\ntop1correct = gl[res].eq(ql).sum().item()\r\n\r\nprint(""Acc top1:{:.3f}"".format(top1correct/ql.size(0)))\r\n\r\n\r\n'"
deep_sort/deep/feature_extractor.py,4,"b'import torch\nimport torchvision.transforms as transforms\nimport numpy as np\nimport cv2\nimport logging\n\nfrom .model import Net\n\nclass Extractor(object):\n    def __init__(self, model_path, use_cuda=True):\n        self.net = Net(reid=True)\n        self.device = ""cuda"" if torch.cuda.is_available() and use_cuda else ""cpu""\n        state_dict = torch.load(model_path, map_location=lambda storage, loc: storage)[\'net_dict\']\n        self.net.load_state_dict(state_dict)\n        logger = logging.getLogger(""root.tracker"")\n        logger.info(""Loading weights from {}... Done!"".format(model_path))\n        self.net.to(self.device)\n        self.size = (64, 128)\n        self.norm = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ])\n        \n\n\n    def _preprocess(self, im_crops):\n        """"""\n        TODO:\n            1. to float with scale from 0 to 1\n            2. resize to (64, 128) as Market1501 dataset did\n            3. concatenate to a numpy array\n            3. to torch Tensor\n            4. normalize\n        """"""\n        def _resize(im, size):\n            return cv2.resize(im.astype(np.float32)/255., size)\n\n        im_batch = torch.cat([self.norm(_resize(im, self.size)).unsqueeze(0) for im in im_crops], dim=0).float()\n        return im_batch\n\n\n    def __call__(self, im_crops):\n        im_batch = self._preprocess(im_crops)\n        with torch.no_grad():\n            im_batch = im_batch.to(self.device)\n            features = self.net(im_batch)\n        return features.cpu().numpy()\n\n\nif __name__ == \'__main__\':\n    img = cv2.imread(""demo.jpg"")[:,:,(2,1,0)]\n    extr = Extractor(""checkpoint/ckpt.t7"")\n    feature = extr(img)\n    print(feature.shape)\n\n'"
deep_sort/deep/model.py,3,"b""import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nclass BasicBlock(nn.Module):\r\n    def __init__(self, c_in, c_out,is_downsample=False):\r\n        super(BasicBlock,self).__init__()\r\n        self.is_downsample = is_downsample\r\n        if is_downsample:\r\n            self.conv1 = nn.Conv2d(c_in, c_out, 3, stride=2, padding=1, bias=False)\r\n        else:\r\n            self.conv1 = nn.Conv2d(c_in, c_out, 3, stride=1, padding=1, bias=False)\r\n        self.bn1 = nn.BatchNorm2d(c_out)\r\n        self.relu = nn.ReLU(True)\r\n        self.conv2 = nn.Conv2d(c_out,c_out,3,stride=1,padding=1, bias=False)\r\n        self.bn2 = nn.BatchNorm2d(c_out)\r\n        if is_downsample:\r\n            self.downsample = nn.Sequential(\r\n                nn.Conv2d(c_in, c_out, 1, stride=2, bias=False),\r\n                nn.BatchNorm2d(c_out)\r\n            )\r\n        elif c_in != c_out:\r\n            self.downsample = nn.Sequential(\r\n                nn.Conv2d(c_in, c_out, 1, stride=1, bias=False),\r\n                nn.BatchNorm2d(c_out)\r\n            )\r\n            self.is_downsample = True\r\n\r\n    def forward(self,x):\r\n        y = self.conv1(x)\r\n        y = self.bn1(y)\r\n        y = self.relu(y)\r\n        y = self.conv2(y)\r\n        y = self.bn2(y)\r\n        if self.is_downsample:\r\n            x = self.downsample(x)\r\n        return F.relu(x.add(y),True)\r\n\r\ndef make_layers(c_in,c_out,repeat_times, is_downsample=False):\r\n    blocks = []\r\n    for i in range(repeat_times):\r\n        if i ==0:\r\n            blocks += [BasicBlock(c_in,c_out, is_downsample=is_downsample),]\r\n        else:\r\n            blocks += [BasicBlock(c_out,c_out),]\r\n    return nn.Sequential(*blocks)\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self, num_classes=751 ,reid=False):\r\n        super(Net,self).__init__()\r\n        # 3 128 64\r\n        self.conv = nn.Sequential(\r\n            nn.Conv2d(3,64,3,stride=1,padding=1),\r\n            nn.BatchNorm2d(64),\r\n            nn.ReLU(inplace=True),\r\n            # nn.Conv2d(32,32,3,stride=1,padding=1),\r\n            # nn.BatchNorm2d(32),\r\n            # nn.ReLU(inplace=True),\r\n            nn.MaxPool2d(3,2,padding=1),\r\n        )\r\n        # 32 64 32\r\n        self.layer1 = make_layers(64,64,2,False)\r\n        # 32 64 32\r\n        self.layer2 = make_layers(64,128,2,True)\r\n        # 64 32 16\r\n        self.layer3 = make_layers(128,256,2,True)\r\n        # 128 16 8\r\n        self.layer4 = make_layers(256,512,2,True)\r\n        # 256 8 4\r\n        self.avgpool = nn.AvgPool2d((8,4),1)\r\n        # 256 1 1 \r\n        self.reid = reid\r\n        self.classifier = nn.Sequential(\r\n            nn.Linear(512, 256),\r\n            nn.BatchNorm1d(256),\r\n            nn.ReLU(inplace=True),\r\n            nn.Dropout(),\r\n            nn.Linear(256, num_classes),\r\n        )\r\n    \r\n    def forward(self, x):\r\n        x = self.conv(x)\r\n        x = self.layer1(x)\r\n        x = self.layer2(x)\r\n        x = self.layer3(x)\r\n        x = self.layer4(x)\r\n        x = self.avgpool(x)\r\n        x = x.view(x.size(0),-1)\r\n        # B x 128\r\n        if self.reid:\r\n            x = x.div(x.norm(p=2,dim=1,keepdim=True))\r\n            return x\r\n        # classifier\r\n        x = self.classifier(x)\r\n        return x\r\n\r\n\r\nif __name__ == '__main__':\r\n    net = Net()\r\n    x = torch.randn(4,3,128,64)\r\n    y = net(x)\r\n    import ipdb; ipdb.set_trace()\r\n\r\n\r\n"""
deep_sort/deep/original_model.py,3,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BasicBlock(nn.Module):\n    def __init__(self, c_in, c_out,is_downsample=False):\n        super(BasicBlock,self).__init__()\n        self.is_downsample = is_downsample\n        if is_downsample:\n            self.conv1 = nn.Conv2d(c_in, c_out, 3, stride=2, padding=1, bias=False)\n        else:\n            self.conv1 = nn.Conv2d(c_in, c_out, 3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(c_out)\n        self.relu = nn.ReLU(True)\n        self.conv2 = nn.Conv2d(c_out,c_out,3,stride=1,padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(c_out)\n        if is_downsample:\n            self.downsample = nn.Sequential(\n                nn.Conv2d(c_in, c_out, 1, stride=2, bias=False),\n                nn.BatchNorm2d(c_out)\n            )\n        elif c_in != c_out:\n            self.downsample = nn.Sequential(\n                nn.Conv2d(c_in, c_out, 1, stride=1, bias=False),\n                nn.BatchNorm2d(c_out)\n            )\n            self.is_downsample = True\n\n    def forward(self,x):\n        y = self.conv1(x)\n        y = self.bn1(y)\n        y = self.relu(y)\n        y = self.conv2(y)\n        y = self.bn2(y)\n        if self.is_downsample:\n            x = self.downsample(x)\n        return F.relu(x.add(y),True)\n\ndef make_layers(c_in,c_out,repeat_times, is_downsample=False):\n    blocks = []\n    for i in range(repeat_times):\n        if i ==0:\n            blocks += [BasicBlock(c_in,c_out, is_downsample=is_downsample),]\n        else:\n            blocks += [BasicBlock(c_out,c_out),]\n    return nn.Sequential(*blocks)\n\nclass Net(nn.Module):\n    def __init__(self, num_classes=625 ,reid=False):\n        super(Net,self).__init__()\n        # 3 128 64\n        self.conv = nn.Sequential(\n            nn.Conv2d(3,32,3,stride=1,padding=1),\n            nn.BatchNorm2d(32),\n            nn.ELU(inplace=True),\n            nn.Conv2d(32,32,3,stride=1,padding=1),\n            nn.BatchNorm2d(32),\n            nn.ELU(inplace=True),\n            nn.MaxPool2d(3,2,padding=1),\n        )\n        # 32 64 32\n        self.layer1 = make_layers(32,32,2,False)\n        # 32 64 32\n        self.layer2 = make_layers(32,64,2,True)\n        # 64 32 16\n        self.layer3 = make_layers(64,128,2,True)\n        # 128 16 8\n        self.dense = nn.Sequential(\n            nn.Dropout(p=0.6),\n            nn.Linear(128*16*8, 128),\n            nn.BatchNorm1d(128),\n            nn.ELU(inplace=True)\n        )\n        # 256 1 1 \n        self.reid = reid\n        self.batch_norm = nn.BatchNorm1d(128)\n        self.classifier = nn.Sequential(\n            nn.Linear(128, num_classes),\n        )\n    \n    def forward(self, x):\n        x = self.conv(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n\n        x = x.view(x.size(0),-1)\n        if self.reid:\n            x = self.dense[0](x)\n            x = self.dense[1](x)\n            x = x.div(x.norm(p=2,dim=1,keepdim=True))\n            return x\n        x = self.dense(x)\n        # B x 128\n        # classifier\n        x = self.classifier(x)\n        return x\n\n\nif __name__ == '__main__':\n    net = Net(reid=True)\n    x = torch.randn(4,3,128,64)\n    y = net(x)\n    import ipdb; ipdb.set_trace()\n\n\n"""
deep_sort/deep/test.py,16,"b'import torch\r\nimport torch.backends.cudnn as cudnn\r\nimport torchvision\r\n\r\nimport argparse\r\nimport os\r\n\r\nfrom model import Net\r\n\r\nparser = argparse.ArgumentParser(description=""Train on market1501"")\r\nparser.add_argument(""--data-dir"",default=\'data\',type=str)\r\nparser.add_argument(""--no-cuda"",action=""store_true"")\r\nparser.add_argument(""--gpu-id"",default=0,type=int)\r\nargs = parser.parse_args()\r\n\r\n# device\r\ndevice = ""cuda:{}"".format(args.gpu_id) if torch.cuda.is_available() and not args.no_cuda else ""cpu""\r\nif torch.cuda.is_available() and not args.no_cuda:\r\n    cudnn.benchmark = True\r\n\r\n# data loader\r\nroot = args.data_dir\r\nquery_dir = os.path.join(root,""query"")\r\ngallery_dir = os.path.join(root,""gallery"")\r\ntransform = torchvision.transforms.Compose([\r\n    torchvision.transforms.Resize((128,64)),\r\n    torchvision.transforms.ToTensor(),\r\n    torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\r\n])\r\nqueryloader = torch.utils.data.DataLoader(\r\n    torchvision.datasets.ImageFolder(query_dir, transform=transform),\r\n    batch_size=64, shuffle=False\r\n)\r\ngalleryloader = torch.utils.data.DataLoader(\r\n    torchvision.datasets.ImageFolder(gallery_dir, transform=transform),\r\n    batch_size=64, shuffle=False\r\n)\r\n\r\n# net definition\r\nnet = Net(reid=True)\r\nassert os.path.isfile(""./checkpoint/ckpt.t7""), ""Error: no checkpoint file found!""\r\nprint(\'Loading from checkpoint/ckpt.t7\')\r\ncheckpoint = torch.load(""./checkpoint/ckpt.t7"")\r\nnet_dict = checkpoint[\'net_dict\']\r\nnet.load_state_dict(net_dict, strict=False)\r\nnet.eval()\r\nnet.to(device)\r\n\r\n# compute features\r\nquery_features = torch.tensor([]).float()\r\nquery_labels = torch.tensor([]).long()\r\ngallery_features = torch.tensor([]).float()\r\ngallery_labels = torch.tensor([]).long()\r\n\r\nwith torch.no_grad():\r\n    for idx,(inputs,labels) in enumerate(queryloader):\r\n        inputs = inputs.to(device)\r\n        features = net(inputs).cpu()\r\n        query_features = torch.cat((query_features, features), dim=0)\r\n        query_labels = torch.cat((query_labels, labels))\r\n\r\n    for idx,(inputs,labels) in enumerate(galleryloader):\r\n        inputs = inputs.to(device)\r\n        features = net(inputs).cpu()\r\n        gallery_features = torch.cat((gallery_features, features), dim=0)\r\n        gallery_labels = torch.cat((gallery_labels, labels))\r\n\r\ngallery_labels -= 2\r\n\r\n# save features\r\nfeatures = {\r\n    ""qf"": query_features,\r\n    ""ql"": query_labels,\r\n    ""gf"": gallery_features,\r\n    ""gl"": gallery_labels\r\n}\r\ntorch.save(features,""features.pth"")'"
deep_sort/deep/train.py,10,"b'import argparse\nimport os\nimport time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torchvision\n\nfrom model import Net\n\nparser = argparse.ArgumentParser(description=""Train on market1501"")\nparser.add_argument(""--data-dir"",default=\'data\',type=str)\nparser.add_argument(""--no-cuda"",action=""store_true"")\nparser.add_argument(""--gpu-id"",default=0,type=int)\nparser.add_argument(""--lr"",default=0.1, type=float)\nparser.add_argument(""--interval"",\'-i\',default=20,type=int)\nparser.add_argument(\'--resume\', \'-r\',action=\'store_true\')\nargs = parser.parse_args()\n\n# device\ndevice = ""cuda:{}"".format(args.gpu_id) if torch.cuda.is_available() and not args.no_cuda else ""cpu""\nif torch.cuda.is_available() and not args.no_cuda:\n    cudnn.benchmark = True\n\n# data loading\nroot = args.data_dir\ntrain_dir = os.path.join(root,""train"")\ntest_dir = os.path.join(root,""test"")\ntransform_train = torchvision.transforms.Compose([\n    torchvision.transforms.RandomCrop((128,64),padding=4),\n    torchvision.transforms.RandomHorizontalFlip(),\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\ntransform_test = torchvision.transforms.Compose([\n    torchvision.transforms.Resize((128,64)),\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\ntrainloader = torch.utils.data.DataLoader(\n    torchvision.datasets.ImageFolder(train_dir, transform=transform_train),\n    batch_size=64,shuffle=True\n)\ntestloader = torch.utils.data.DataLoader(\n    torchvision.datasets.ImageFolder(test_dir, transform=transform_test),\n    batch_size=64,shuffle=True\n)\nnum_classes = max(len(trainloader.dataset.classes), len(testloader.dataset.classes))\n\n# net definition\nstart_epoch = 0\nnet = Net(num_classes=num_classes)\nif args.resume:\n    assert os.path.isfile(""./checkpoint/ckpt.t7""), ""Error: no checkpoint file found!""\n    print(\'Loading from checkpoint/ckpt.t7\')\n    checkpoint = torch.load(""./checkpoint/ckpt.t7"")\n    # import ipdb; ipdb.set_trace()\n    net_dict = checkpoint[\'net_dict\']\n    net.load_state_dict(net_dict)\n    best_acc = checkpoint[\'acc\']\n    start_epoch = checkpoint[\'epoch\']\nnet.to(device)\n\n# loss and optimizer\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(net.parameters(), args.lr, momentum=0.9, weight_decay=5e-4)\nbest_acc = 0.\n\n# train function for each epoch\ndef train(epoch):\n    print(""\\nEpoch : %d""%(epoch+1))\n    net.train()\n    training_loss = 0.\n    train_loss = 0.\n    correct = 0\n    total = 0\n    interval = args.interval\n    start = time.time()\n    for idx, (inputs, labels) in enumerate(trainloader):\n        # forward\n        inputs,labels = inputs.to(device),labels.to(device)\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n\n        # backward\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # accumurating\n        training_loss += loss.item()\n        train_loss += loss.item()\n        correct += outputs.max(dim=1)[1].eq(labels).sum().item()\n        total += labels.size(0)\n\n        # print \n        if (idx+1)%interval == 0:\n            end = time.time()\n            print(""[progress:{:.1f}%]time:{:.2f}s Loss:{:.5f} Correct:{}/{} Acc:{:.3f}%"".format(\n                100.*(idx+1)/len(trainloader), end-start, training_loss/interval, correct, total, 100.*correct/total\n            ))\n            training_loss = 0.\n            start = time.time()\n    \n    return train_loss/len(trainloader), 1.- correct/total\n\ndef test(epoch):\n    global best_acc\n    net.eval()\n    test_loss = 0.\n    correct = 0\n    total = 0\n    start = time.time()\n    with torch.no_grad():\n        for idx, (inputs, labels) in enumerate(testloader):\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n\n            test_loss += loss.item()\n            correct += outputs.max(dim=1)[1].eq(labels).sum().item()\n            total += labels.size(0)\n        \n        print(""Testing ..."")\n        end = time.time()\n        print(""[progress:{:.1f}%]time:{:.2f}s Loss:{:.5f} Correct:{}/{} Acc:{:.3f}%"".format(\n                100.*(idx+1)/len(testloader), end-start, test_loss/len(testloader), correct, total, 100.*correct/total\n            ))\n\n    # saving checkpoint\n    acc = 100.*correct/total\n    if acc > best_acc:\n        best_acc = acc\n        print(""Saving parameters to checkpoint/ckpt.t7"")\n        checkpoint = {\n            \'net_dict\':net.state_dict(),\n            \'acc\':acc,\n            \'epoch\':epoch,\n        }\n        if not os.path.isdir(\'checkpoint\'):\n            os.mkdir(\'checkpoint\')\n        torch.save(checkpoint, \'./checkpoint/ckpt.t7\')\n\n    return test_loss/len(testloader), 1.- correct/total\n\n# plot figure\nx_epoch = []\nrecord = {\'train_loss\':[], \'train_err\':[], \'test_loss\':[], \'test_err\':[]}\nfig = plt.figure()\nax0 = fig.add_subplot(121, title=""loss"")\nax1 = fig.add_subplot(122, title=""top1err"")\ndef draw_curve(epoch, train_loss, train_err, test_loss, test_err):\n    global record\n    record[\'train_loss\'].append(train_loss)\n    record[\'train_err\'].append(train_err)\n    record[\'test_loss\'].append(test_loss)\n    record[\'test_err\'].append(test_err)\n\n    x_epoch.append(epoch)\n    ax0.plot(x_epoch, record[\'train_loss\'], \'bo-\', label=\'train\')\n    ax0.plot(x_epoch, record[\'test_loss\'], \'ro-\', label=\'val\')\n    ax1.plot(x_epoch, record[\'train_err\'], \'bo-\', label=\'train\')\n    ax1.plot(x_epoch, record[\'test_err\'], \'ro-\', label=\'val\')\n    if epoch == 0:\n        ax0.legend()\n        ax1.legend()\n    fig.savefig(""train.jpg"")\n\n# lr decay\ndef lr_decay():\n    global optimizer\n    for params in optimizer.param_groups:\n        params[\'lr\'] *= 0.1\n        lr = params[\'lr\']\n        print(""Learning rate adjusted to {}"".format(lr))\n\ndef main():\n    for epoch in range(start_epoch, start_epoch+40):\n        train_loss, train_err = train(epoch)\n        test_loss, test_err = test(epoch)\n        draw_curve(epoch, train_loss, train_err, test_loss, test_err)\n        if (epoch+1)%20==0:\n            lr_decay()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
deep_sort/sort/__init__.py,0,b''
deep_sort/sort/detection.py,0,"b'# vim: expandtab:ts=4:sw=4\r\nimport numpy as np\r\n\r\n\r\nclass Detection(object):\r\n    """"""\r\n    This class represents a bounding box detection in a single image.\r\n\r\n    Parameters\r\n    ----------\r\n    tlwh : array_like\r\n        Bounding box in format `(x, y, w, h)`.\r\n    confidence : float\r\n        Detector confidence score.\r\n    feature : array_like\r\n        A feature vector that describes the object contained in this image.\r\n\r\n    Attributes\r\n    ----------\r\n    tlwh : ndarray\r\n        Bounding box in format `(top left x, top left y, width, height)`.\r\n    confidence : ndarray\r\n        Detector confidence score.\r\n    feature : ndarray | NoneType\r\n        A feature vector that describes the object contained in this image.\r\n\r\n    """"""\r\n\r\n    def __init__(self, tlwh, confidence, feature):\r\n        self.tlwh = np.asarray(tlwh, dtype=np.float)\r\n        self.confidence = float(confidence)\r\n        self.feature = np.asarray(feature, dtype=np.float32)\r\n\r\n    def to_tlbr(self):\r\n        """"""Convert bounding box to format `(min x, min y, max x, max y)`, i.e.,\r\n        `(top left, bottom right)`.\r\n        """"""\r\n        ret = self.tlwh.copy()\r\n        ret[2:] += ret[:2]\r\n        return ret\r\n\r\n    def to_xyah(self):\r\n        """"""Convert bounding box to format `(center x, center y, aspect ratio,\r\n        height)`, where the aspect ratio is `width / height`.\r\n        """"""\r\n        ret = self.tlwh.copy()\r\n        ret[:2] += ret[2:] / 2\r\n        ret[2] /= ret[3]\r\n        return ret\r\n'"
deep_sort/sort/iou_matching.py,0,"b'# vim: expandtab:ts=4:sw=4\r\nfrom __future__ import absolute_import\r\nimport numpy as np\r\nfrom . import linear_assignment\r\n\r\n\r\ndef iou(bbox, candidates):\r\n    """"""Computer intersection over union.\r\n\r\n    Parameters\r\n    ----------\r\n    bbox : ndarray\r\n        A bounding box in format `(top left x, top left y, width, height)`.\r\n    candidates : ndarray\r\n        A matrix of candidate bounding boxes (one per row) in the same format\r\n        as `bbox`.\r\n\r\n    Returns\r\n    -------\r\n    ndarray\r\n        The intersection over union in [0, 1] between the `bbox` and each\r\n        candidate. A higher score means a larger fraction of the `bbox` is\r\n        occluded by the candidate.\r\n\r\n    """"""\r\n    bbox_tl, bbox_br = bbox[:2], bbox[:2] + bbox[2:]\r\n    candidates_tl = candidates[:, :2]\r\n    candidates_br = candidates[:, :2] + candidates[:, 2:]\r\n\r\n    tl = np.c_[np.maximum(bbox_tl[0], candidates_tl[:, 0])[:, np.newaxis],\r\n               np.maximum(bbox_tl[1], candidates_tl[:, 1])[:, np.newaxis]]\r\n    br = np.c_[np.minimum(bbox_br[0], candidates_br[:, 0])[:, np.newaxis],\r\n               np.minimum(bbox_br[1], candidates_br[:, 1])[:, np.newaxis]]\r\n    wh = np.maximum(0., br - tl)\r\n\r\n    area_intersection = wh.prod(axis=1)\r\n    area_bbox = bbox[2:].prod()\r\n    area_candidates = candidates[:, 2:].prod(axis=1)\r\n    return area_intersection / (area_bbox + area_candidates - area_intersection)\r\n\r\n\r\ndef iou_cost(tracks, detections, track_indices=None,\r\n             detection_indices=None):\r\n    """"""An intersection over union distance metric.\r\n\r\n    Parameters\r\n    ----------\r\n    tracks : List[deep_sort.track.Track]\r\n        A list of tracks.\r\n    detections : List[deep_sort.detection.Detection]\r\n        A list of detections.\r\n    track_indices : Optional[List[int]]\r\n        A list of indices to tracks that should be matched. Defaults to\r\n        all `tracks`.\r\n    detection_indices : Optional[List[int]]\r\n        A list of indices to detections that should be matched. Defaults\r\n        to all `detections`.\r\n\r\n    Returns\r\n    -------\r\n    ndarray\r\n        Returns a cost matrix of shape\r\n        len(track_indices), len(detection_indices) where entry (i, j) is\r\n        `1 - iou(tracks[track_indices[i]], detections[detection_indices[j]])`.\r\n\r\n    """"""\r\n    if track_indices is None:\r\n        track_indices = np.arange(len(tracks))\r\n    if detection_indices is None:\r\n        detection_indices = np.arange(len(detections))\r\n\r\n    cost_matrix = np.zeros((len(track_indices), len(detection_indices)))\r\n    for row, track_idx in enumerate(track_indices):\r\n        if tracks[track_idx].time_since_update > 1:\r\n            cost_matrix[row, :] = linear_assignment.INFTY_COST\r\n            continue\r\n\r\n        bbox = tracks[track_idx].to_tlwh()\r\n        candidates = np.asarray([detections[i].tlwh for i in detection_indices])\r\n        cost_matrix[row, :] = 1. - iou(bbox, candidates)\r\n    return cost_matrix\r\n'"
deep_sort/sort/kalman_filter.py,0,"b'# vim: expandtab:ts=4:sw=4\r\nimport numpy as np\r\nimport scipy.linalg\r\n\r\n\r\n""""""\r\nTable for the 0.95 quantile of the chi-square distribution with N degrees of\r\nfreedom (contains values for N=1, ..., 9). Taken from MATLAB/Octave\'s chi2inv\r\nfunction and used as Mahalanobis gating threshold.\r\n""""""\r\nchi2inv95 = {\r\n    1: 3.8415,\r\n    2: 5.9915,\r\n    3: 7.8147,\r\n    4: 9.4877,\r\n    5: 11.070,\r\n    6: 12.592,\r\n    7: 14.067,\r\n    8: 15.507,\r\n    9: 16.919}\r\n\r\n\r\nclass KalmanFilter(object):\r\n    """"""\r\n    A simple Kalman filter for tracking bounding boxes in image space.\r\n\r\n    The 8-dimensional state space\r\n\r\n        x, y, a, h, vx, vy, va, vh\r\n\r\n    contains the bounding box center position (x, y), aspect ratio a, height h,\r\n    and their respective velocities.\r\n\r\n    Object motion follows a constant velocity model. The bounding box location\r\n    (x, y, a, h) is taken as direct observation of the state space (linear\r\n    observation model).\r\n\r\n    """"""\r\n\r\n    def __init__(self):\r\n        ndim, dt = 4, 1.\r\n\r\n        # Create Kalman filter model matrices.\r\n        self._motion_mat = np.eye(2 * ndim, 2 * ndim)\r\n        for i in range(ndim):\r\n            self._motion_mat[i, ndim + i] = dt\r\n        self._update_mat = np.eye(ndim, 2 * ndim)\r\n\r\n        # Motion and observation uncertainty are chosen relative to the current\r\n        # state estimate. These weights control the amount of uncertainty in\r\n        # the model. This is a bit hacky.\r\n        self._std_weight_position = 1. / 20\r\n        self._std_weight_velocity = 1. / 160\r\n\r\n    def initiate(self, measurement):\r\n        """"""Create track from unassociated measurement.\r\n\r\n        Parameters\r\n        ----------\r\n        measurement : ndarray\r\n            Bounding box coordinates (x, y, a, h) with center position (x, y),\r\n            aspect ratio a, and height h.\r\n\r\n        Returns\r\n        -------\r\n        (ndarray, ndarray)\r\n            Returns the mean vector (8 dimensional) and covariance matrix (8x8\r\n            dimensional) of the new track. Unobserved velocities are initialized\r\n            to 0 mean.\r\n\r\n        """"""\r\n        mean_pos = measurement\r\n        mean_vel = np.zeros_like(mean_pos)\r\n        mean = np.r_[mean_pos, mean_vel]\r\n\r\n        std = [\r\n            2 * self._std_weight_position * measurement[3],\r\n            2 * self._std_weight_position * measurement[3],\r\n            1e-2,\r\n            2 * self._std_weight_position * measurement[3],\r\n            10 * self._std_weight_velocity * measurement[3],\r\n            10 * self._std_weight_velocity * measurement[3],\r\n            1e-5,\r\n            10 * self._std_weight_velocity * measurement[3]]\r\n        covariance = np.diag(np.square(std))\r\n        return mean, covariance\r\n\r\n    def predict(self, mean, covariance):\r\n        """"""Run Kalman filter prediction step.\r\n\r\n        Parameters\r\n        ----------\r\n        mean : ndarray\r\n            The 8 dimensional mean vector of the object state at the previous\r\n            time step.\r\n        covariance : ndarray\r\n            The 8x8 dimensional covariance matrix of the object state at the\r\n            previous time step.\r\n\r\n        Returns\r\n        -------\r\n        (ndarray, ndarray)\r\n            Returns the mean vector and covariance matrix of the predicted\r\n            state. Unobserved velocities are initialized to 0 mean.\r\n\r\n        """"""\r\n        std_pos = [\r\n            self._std_weight_position * mean[3],\r\n            self._std_weight_position * mean[3],\r\n            1e-2,\r\n            self._std_weight_position * mean[3]]\r\n        std_vel = [\r\n            self._std_weight_velocity * mean[3],\r\n            self._std_weight_velocity * mean[3],\r\n            1e-5,\r\n            self._std_weight_velocity * mean[3]]\r\n        motion_cov = np.diag(np.square(np.r_[std_pos, std_vel]))\r\n\r\n        mean = np.dot(self._motion_mat, mean)\r\n        covariance = np.linalg.multi_dot((\r\n            self._motion_mat, covariance, self._motion_mat.T)) + motion_cov\r\n\r\n        return mean, covariance\r\n\r\n    def project(self, mean, covariance):\r\n        """"""Project state distribution to measurement space.\r\n\r\n        Parameters\r\n        ----------\r\n        mean : ndarray\r\n            The state\'s mean vector (8 dimensional array).\r\n        covariance : ndarray\r\n            The state\'s covariance matrix (8x8 dimensional).\r\n\r\n        Returns\r\n        -------\r\n        (ndarray, ndarray)\r\n            Returns the projected mean and covariance matrix of the given state\r\n            estimate.\r\n\r\n        """"""\r\n        std = [\r\n            self._std_weight_position * mean[3],\r\n            self._std_weight_position * mean[3],\r\n            1e-1,\r\n            self._std_weight_position * mean[3]]\r\n        innovation_cov = np.diag(np.square(std))\r\n\r\n        mean = np.dot(self._update_mat, mean)\r\n        covariance = np.linalg.multi_dot((\r\n            self._update_mat, covariance, self._update_mat.T))\r\n        return mean, covariance + innovation_cov\r\n\r\n    def update(self, mean, covariance, measurement):\r\n        """"""Run Kalman filter correction step.\r\n\r\n        Parameters\r\n        ----------\r\n        mean : ndarray\r\n            The predicted state\'s mean vector (8 dimensional).\r\n        covariance : ndarray\r\n            The state\'s covariance matrix (8x8 dimensional).\r\n        measurement : ndarray\r\n            The 4 dimensional measurement vector (x, y, a, h), where (x, y)\r\n            is the center position, a the aspect ratio, and h the height of the\r\n            bounding box.\r\n\r\n        Returns\r\n        -------\r\n        (ndarray, ndarray)\r\n            Returns the measurement-corrected state distribution.\r\n\r\n        """"""\r\n        projected_mean, projected_cov = self.project(mean, covariance)\r\n\r\n        chol_factor, lower = scipy.linalg.cho_factor(\r\n            projected_cov, lower=True, check_finite=False)\r\n        kalman_gain = scipy.linalg.cho_solve(\r\n            (chol_factor, lower), np.dot(covariance, self._update_mat.T).T,\r\n            check_finite=False).T\r\n        innovation = measurement - projected_mean\r\n\r\n        new_mean = mean + np.dot(innovation, kalman_gain.T)\r\n        new_covariance = covariance - np.linalg.multi_dot((\r\n            kalman_gain, projected_cov, kalman_gain.T))\r\n        return new_mean, new_covariance\r\n\r\n    def gating_distance(self, mean, covariance, measurements,\r\n                        only_position=False):\r\n        """"""Compute gating distance between state distribution and measurements.\r\n\r\n        A suitable distance threshold can be obtained from `chi2inv95`. If\r\n        `only_position` is False, the chi-square distribution has 4 degrees of\r\n        freedom, otherwise 2.\r\n\r\n        Parameters\r\n        ----------\r\n        mean : ndarray\r\n            Mean vector over the state distribution (8 dimensional).\r\n        covariance : ndarray\r\n            Covariance of the state distribution (8x8 dimensional).\r\n        measurements : ndarray\r\n            An Nx4 dimensional matrix of N measurements, each in\r\n            format (x, y, a, h) where (x, y) is the bounding box center\r\n            position, a the aspect ratio, and h the height.\r\n        only_position : Optional[bool]\r\n            If True, distance computation is done with respect to the bounding\r\n            box center position only.\r\n\r\n        Returns\r\n        -------\r\n        ndarray\r\n            Returns an array of length N, where the i-th element contains the\r\n            squared Mahalanobis distance between (mean, covariance) and\r\n            `measurements[i]`.\r\n\r\n        """"""\r\n        mean, covariance = self.project(mean, covariance)\r\n        if only_position:\r\n            mean, covariance = mean[:2], covariance[:2, :2]\r\n            measurements = measurements[:, :2]\r\n\r\n        cholesky_factor = np.linalg.cholesky(covariance)\r\n        d = measurements - mean\r\n        z = scipy.linalg.solve_triangular(\r\n            cholesky_factor, d.T, lower=True, check_finite=False,\r\n            overwrite_b=True)\r\n        squared_maha = np.sum(z * z, axis=0)\r\n        return squared_maha\r\n'"
deep_sort/sort/linear_assignment.py,0,"b'# vim: expandtab:ts=4:sw=4\r\nfrom __future__ import absolute_import\r\nimport numpy as np\r\n# from sklearn.utils.linear_assignment_ import linear_assignment\r\nfrom scipy.optimize import linear_sum_assignment as linear_assignment\r\nfrom . import kalman_filter\r\n\r\n\r\nINFTY_COST = 1e+5\r\n\r\n\r\ndef min_cost_matching(\r\n        distance_metric, max_distance, tracks, detections, track_indices=None,\r\n        detection_indices=None):\r\n    """"""Solve linear assignment problem.\r\n\r\n    Parameters\r\n    ----------\r\n    distance_metric : Callable[List[Track], List[Detection], List[int], List[int]) -> ndarray\r\n        The distance metric is given a list of tracks and detections as well as\r\n        a list of N track indices and M detection indices. The metric should\r\n        return the NxM dimensional cost matrix, where element (i, j) is the\r\n        association cost between the i-th track in the given track indices and\r\n        the j-th detection in the given detection_indices.\r\n    max_distance : float\r\n        Gating threshold. Associations with cost larger than this value are\r\n        disregarded.\r\n    tracks : List[track.Track]\r\n        A list of predicted tracks at the current time step.\r\n    detections : List[detection.Detection]\r\n        A list of detections at the current time step.\r\n    track_indices : List[int]\r\n        List of track indices that maps rows in `cost_matrix` to tracks in\r\n        `tracks` (see description above).\r\n    detection_indices : List[int]\r\n        List of detection indices that maps columns in `cost_matrix` to\r\n        detections in `detections` (see description above).\r\n\r\n    Returns\r\n    -------\r\n    (List[(int, int)], List[int], List[int])\r\n        Returns a tuple with the following three entries:\r\n        * A list of matched track and detection indices.\r\n        * A list of unmatched track indices.\r\n        * A list of unmatched detection indices.\r\n\r\n    """"""\r\n    if track_indices is None:\r\n        track_indices = np.arange(len(tracks))\r\n    if detection_indices is None:\r\n        detection_indices = np.arange(len(detections))\r\n\r\n    if len(detection_indices) == 0 or len(track_indices) == 0:\r\n        return [], track_indices, detection_indices  # Nothing to match.\r\n\r\n    cost_matrix = distance_metric(\r\n        tracks, detections, track_indices, detection_indices)\r\n    cost_matrix[cost_matrix > max_distance] = max_distance + 1e-5\r\n\r\n    row_indices, col_indices = linear_assignment(cost_matrix)\r\n\r\n    matches, unmatched_tracks, unmatched_detections = [], [], []\r\n    for col, detection_idx in enumerate(detection_indices):\r\n        if col not in col_indices:\r\n            unmatched_detections.append(detection_idx)\r\n    for row, track_idx in enumerate(track_indices):\r\n        if row not in row_indices:\r\n            unmatched_tracks.append(track_idx)\r\n    for row, col in zip(row_indices, col_indices):\r\n        track_idx = track_indices[row]\r\n        detection_idx = detection_indices[col]\r\n        if cost_matrix[row, col] > max_distance:\r\n            unmatched_tracks.append(track_idx)\r\n            unmatched_detections.append(detection_idx)\r\n        else:\r\n            matches.append((track_idx, detection_idx))\r\n    return matches, unmatched_tracks, unmatched_detections\r\n\r\n\r\ndef matching_cascade(\r\n        distance_metric, max_distance, cascade_depth, tracks, detections,\r\n        track_indices=None, detection_indices=None):\r\n    """"""Run matching cascade.\r\n\r\n    Parameters\r\n    ----------\r\n    distance_metric : Callable[List[Track], List[Detection], List[int], List[int]) -> ndarray\r\n        The distance metric is given a list of tracks and detections as well as\r\n        a list of N track indices and M detection indices. The metric should\r\n        return the NxM dimensional cost matrix, where element (i, j) is the\r\n        association cost between the i-th track in the given track indices and\r\n        the j-th detection in the given detection indices.\r\n    max_distance : float\r\n        Gating threshold. Associations with cost larger than this value are\r\n        disregarded.\r\n    cascade_depth: int\r\n        The cascade depth, should be se to the maximum track age.\r\n    tracks : List[track.Track]\r\n        A list of predicted tracks at the current time step.\r\n    detections : List[detection.Detection]\r\n        A list of detections at the current time step.\r\n    track_indices : Optional[List[int]]\r\n        List of track indices that maps rows in `cost_matrix` to tracks in\r\n        `tracks` (see description above). Defaults to all tracks.\r\n    detection_indices : Optional[List[int]]\r\n        List of detection indices that maps columns in `cost_matrix` to\r\n        detections in `detections` (see description above). Defaults to all\r\n        detections.\r\n\r\n    Returns\r\n    -------\r\n    (List[(int, int)], List[int], List[int])\r\n        Returns a tuple with the following three entries:\r\n        * A list of matched track and detection indices.\r\n        * A list of unmatched track indices.\r\n        * A list of unmatched detection indices.\r\n\r\n    """"""\r\n    if track_indices is None:\r\n        track_indices = list(range(len(tracks)))\r\n    if detection_indices is None:\r\n        detection_indices = list(range(len(detections)))\r\n\r\n    unmatched_detections = detection_indices\r\n    matches = []\r\n    for level in range(cascade_depth):\r\n        if len(unmatched_detections) == 0:  # No detections left\r\n            break\r\n\r\n        track_indices_l = [\r\n            k for k in track_indices\r\n            if tracks[k].time_since_update == 1 + level\r\n        ]\r\n        if len(track_indices_l) == 0:  # Nothing to match at this level\r\n            continue\r\n\r\n        matches_l, _, unmatched_detections = \\\r\n            min_cost_matching(\r\n                distance_metric, max_distance, tracks, detections,\r\n                track_indices_l, unmatched_detections)\r\n        matches += matches_l\r\n    unmatched_tracks = list(set(track_indices) - set(k for k, _ in matches))\r\n    return matches, unmatched_tracks, unmatched_detections\r\n\r\n\r\ndef gate_cost_matrix(\r\n        kf, cost_matrix, tracks, detections, track_indices, detection_indices,\r\n        gated_cost=INFTY_COST, only_position=False):\r\n    """"""Invalidate infeasible entries in cost matrix based on the state\r\n    distributions obtained by Kalman filtering.\r\n\r\n    Parameters\r\n    ----------\r\n    kf : The Kalman filter.\r\n    cost_matrix : ndarray\r\n        The NxM dimensional cost matrix, where N is the number of track indices\r\n        and M is the number of detection indices, such that entry (i, j) is the\r\n        association cost between `tracks[track_indices[i]]` and\r\n        `detections[detection_indices[j]]`.\r\n    tracks : List[track.Track]\r\n        A list of predicted tracks at the current time step.\r\n    detections : List[detection.Detection]\r\n        A list of detections at the current time step.\r\n    track_indices : List[int]\r\n        List of track indices that maps rows in `cost_matrix` to tracks in\r\n        `tracks` (see description above).\r\n    detection_indices : List[int]\r\n        List of detection indices that maps columns in `cost_matrix` to\r\n        detections in `detections` (see description above).\r\n    gated_cost : Optional[float]\r\n        Entries in the cost matrix corresponding to infeasible associations are\r\n        set this value. Defaults to a very large value.\r\n    only_position : Optional[bool]\r\n        If True, only the x, y position of the state distribution is considered\r\n        during gating. Defaults to False.\r\n\r\n    Returns\r\n    -------\r\n    ndarray\r\n        Returns the modified cost matrix.\r\n\r\n    """"""\r\n    gating_dim = 2 if only_position else 4\r\n    gating_threshold = kalman_filter.chi2inv95[gating_dim]\r\n    measurements = np.asarray(\r\n        [detections[i].to_xyah() for i in detection_indices])\r\n    for row, track_idx in enumerate(track_indices):\r\n        track = tracks[track_idx]\r\n        gating_distance = kf.gating_distance(\r\n            track.mean, track.covariance, measurements, only_position)\r\n        cost_matrix[row, gating_distance > gating_threshold] = gated_cost\r\n    return cost_matrix\r\n'"
deep_sort/sort/nn_matching.py,0,"b'# vim: expandtab:ts=4:sw=4\r\nimport numpy as np\r\n\r\n\r\ndef _pdist(a, b):\r\n    """"""Compute pair-wise squared distance between points in `a` and `b`.\r\n\r\n    Parameters\r\n    ----------\r\n    a : array_like\r\n        An NxM matrix of N samples of dimensionality M.\r\n    b : array_like\r\n        An LxM matrix of L samples of dimensionality M.\r\n\r\n    Returns\r\n    -------\r\n    ndarray\r\n        Returns a matrix of size len(a), len(b) such that eleement (i, j)\r\n        contains the squared distance between `a[i]` and `b[j]`.\r\n\r\n    """"""\r\n    a, b = np.asarray(a), np.asarray(b)\r\n    if len(a) == 0 or len(b) == 0:\r\n        return np.zeros((len(a), len(b)))\r\n    a2, b2 = np.square(a).sum(axis=1), np.square(b).sum(axis=1)\r\n    r2 = -2. * np.dot(a, b.T) + a2[:, None] + b2[None, :]\r\n    r2 = np.clip(r2, 0., float(np.inf))\r\n    return r2\r\n\r\n\r\ndef _cosine_distance(a, b, data_is_normalized=False):\r\n    """"""Compute pair-wise cosine distance between points in `a` and `b`.\r\n\r\n    Parameters\r\n    ----------\r\n    a : array_like\r\n        An NxM matrix of N samples of dimensionality M.\r\n    b : array_like\r\n        An LxM matrix of L samples of dimensionality M.\r\n    data_is_normalized : Optional[bool]\r\n        If True, assumes rows in a and b are unit length vectors.\r\n        Otherwise, a and b are explicitly normalized to lenght 1.\r\n\r\n    Returns\r\n    -------\r\n    ndarray\r\n        Returns a matrix of size len(a), len(b) such that eleement (i, j)\r\n        contains the squared distance between `a[i]` and `b[j]`.\r\n\r\n    """"""\r\n    if not data_is_normalized:\r\n        a = np.asarray(a) / np.linalg.norm(a, axis=1, keepdims=True)\r\n        b = np.asarray(b) / np.linalg.norm(b, axis=1, keepdims=True)\r\n    return 1. - np.dot(a, b.T)\r\n\r\n\r\ndef _nn_euclidean_distance(x, y):\r\n    """""" Helper function for nearest neighbor distance metric (Euclidean).\r\n\r\n    Parameters\r\n    ----------\r\n    x : ndarray\r\n        A matrix of N row-vectors (sample points).\r\n    y : ndarray\r\n        A matrix of M row-vectors (query points).\r\n\r\n    Returns\r\n    -------\r\n    ndarray\r\n        A vector of length M that contains for each entry in `y` the\r\n        smallest Euclidean distance to a sample in `x`.\r\n\r\n    """"""\r\n    distances = _pdist(x, y)\r\n    return np.maximum(0.0, distances.min(axis=0))\r\n\r\n\r\ndef _nn_cosine_distance(x, y):\r\n    """""" Helper function for nearest neighbor distance metric (cosine).\r\n\r\n    Parameters\r\n    ----------\r\n    x : ndarray\r\n        A matrix of N row-vectors (sample points).\r\n    y : ndarray\r\n        A matrix of M row-vectors (query points).\r\n\r\n    Returns\r\n    -------\r\n    ndarray\r\n        A vector of length M that contains for each entry in `y` the\r\n        smallest cosine distance to a sample in `x`.\r\n\r\n    """"""\r\n    distances = _cosine_distance(x, y)\r\n    return distances.min(axis=0)\r\n\r\n\r\nclass NearestNeighborDistanceMetric(object):\r\n    """"""\r\n    A nearest neighbor distance metric that, for each target, returns\r\n    the closest distance to any sample that has been observed so far.\r\n\r\n    Parameters\r\n    ----------\r\n    metric : str\r\n        Either ""euclidean"" or ""cosine"".\r\n    matching_threshold: float\r\n        The matching threshold. Samples with larger distance are considered an\r\n        invalid match.\r\n    budget : Optional[int]\r\n        If not None, fix samples per class to at most this number. Removes\r\n        the oldest samples when the budget is reached.\r\n\r\n    Attributes\r\n    ----------\r\n    samples : Dict[int -> List[ndarray]]\r\n        A dictionary that maps from target identities to the list of samples\r\n        that have been observed so far.\r\n\r\n    """"""\r\n\r\n    def __init__(self, metric, matching_threshold, budget=None):\r\n\r\n\r\n        if metric == ""euclidean"":\r\n            self._metric = _nn_euclidean_distance\r\n        elif metric == ""cosine"":\r\n            self._metric = _nn_cosine_distance\r\n        else:\r\n            raise ValueError(\r\n                ""Invalid metric; must be either \'euclidean\' or \'cosine\'"")\r\n        self.matching_threshold = matching_threshold\r\n        self.budget = budget\r\n        self.samples = {}\r\n\r\n    def partial_fit(self, features, targets, active_targets):\r\n        """"""Update the distance metric with new data.\r\n\r\n        Parameters\r\n        ----------\r\n        features : ndarray\r\n            An NxM matrix of N features of dimensionality M.\r\n        targets : ndarray\r\n            An integer array of associated target identities.\r\n        active_targets : List[int]\r\n            A list of targets that are currently present in the scene.\r\n\r\n        """"""\r\n        for feature, target in zip(features, targets):\r\n            self.samples.setdefault(target, []).append(feature)\r\n            if self.budget is not None:\r\n                self.samples[target] = self.samples[target][-self.budget:]\r\n        self.samples = {k: self.samples[k] for k in active_targets}\r\n\r\n    def distance(self, features, targets):\r\n        """"""Compute distance between features and targets.\r\n\r\n        Parameters\r\n        ----------\r\n        features : ndarray\r\n            An NxM matrix of N features of dimensionality M.\r\n        targets : List[int]\r\n            A list of targets to match the given `features` against.\r\n\r\n        Returns\r\n        -------\r\n        ndarray\r\n            Returns a cost matrix of shape len(targets), len(features), where\r\n            element (i, j) contains the closest squared distance between\r\n            `targets[i]` and `features[j]`.\r\n\r\n        """"""\r\n        cost_matrix = np.zeros((len(targets), len(features)))\r\n        for i, target in enumerate(targets):\r\n            cost_matrix[i, :] = self._metric(self.samples[target], features)\r\n        return cost_matrix\r\n'"
deep_sort/sort/preprocessing.py,0,"b'# vim: expandtab:ts=4:sw=4\r\nimport numpy as np\r\nimport cv2\r\n\r\n\r\ndef non_max_suppression(boxes, max_bbox_overlap, scores=None):\r\n    """"""Suppress overlapping detections.\r\n\r\n    Original code from [1]_ has been adapted to include confidence score.\r\n\r\n    .. [1] http://www.pyimagesearch.com/2015/02/16/\r\n           faster-non-maximum-suppression-python/\r\n\r\n    Examples\r\n    --------\r\n\r\n        >>> boxes = [d.roi for d in detections]\r\n        >>> scores = [d.confidence for d in detections]\r\n        >>> indices = non_max_suppression(boxes, max_bbox_overlap, scores)\r\n        >>> detections = [detections[i] for i in indices]\r\n\r\n    Parameters\r\n    ----------\r\n    boxes : ndarray\r\n        Array of ROIs (x, y, width, height).\r\n    max_bbox_overlap : float\r\n        ROIs that overlap more than this values are suppressed.\r\n    scores : Optional[array_like]\r\n        Detector confidence score.\r\n\r\n    Returns\r\n    -------\r\n    List[int]\r\n        Returns indices of detections that have survived non-maxima suppression.\r\n\r\n    """"""\r\n    if len(boxes) == 0:\r\n        return []\r\n\r\n    boxes = boxes.astype(np.float)\r\n    pick = []\r\n\r\n    x1 = boxes[:, 0]\r\n    y1 = boxes[:, 1]\r\n    x2 = boxes[:, 2] + boxes[:, 0]\r\n    y2 = boxes[:, 3] + boxes[:, 1]\r\n\r\n    area = (x2 - x1 + 1) * (y2 - y1 + 1)\r\n    if scores is not None:\r\n        idxs = np.argsort(scores)\r\n    else:\r\n        idxs = np.argsort(y2)\r\n\r\n    while len(idxs) > 0:\r\n        last = len(idxs) - 1\r\n        i = idxs[last]\r\n        pick.append(i)\r\n\r\n        xx1 = np.maximum(x1[i], x1[idxs[:last]])\r\n        yy1 = np.maximum(y1[i], y1[idxs[:last]])\r\n        xx2 = np.minimum(x2[i], x2[idxs[:last]])\r\n        yy2 = np.minimum(y2[i], y2[idxs[:last]])\r\n\r\n        w = np.maximum(0, xx2 - xx1 + 1)\r\n        h = np.maximum(0, yy2 - yy1 + 1)\r\n\r\n        overlap = (w * h) / area[idxs[:last]]\r\n\r\n        idxs = np.delete(\r\n            idxs, np.concatenate(\r\n                ([last], np.where(overlap > max_bbox_overlap)[0])))\r\n\r\n    return pick\r\n'"
deep_sort/sort/track.py,0,"b'# vim: expandtab:ts=4:sw=4\r\n\r\n\r\nclass TrackState:\r\n    """"""\r\n    Enumeration type for the single target track state. Newly created tracks are\r\n    classified as `tentative` until enough evidence has been collected. Then,\r\n    the track state is changed to `confirmed`. Tracks that are no longer alive\r\n    are classified as `deleted` to mark them for removal from the set of active\r\n    tracks.\r\n\r\n    """"""\r\n\r\n    Tentative = 1\r\n    Confirmed = 2\r\n    Deleted = 3\r\n\r\n\r\nclass Track:\r\n    """"""\r\n    A single target track with state space `(x, y, a, h)` and associated\r\n    velocities, where `(x, y)` is the center of the bounding box, `a` is the\r\n    aspect ratio and `h` is the height.\r\n\r\n    Parameters\r\n    ----------\r\n    mean : ndarray\r\n        Mean vector of the initial state distribution.\r\n    covariance : ndarray\r\n        Covariance matrix of the initial state distribution.\r\n    track_id : int\r\n        A unique track identifier.\r\n    n_init : int\r\n        Number of consecutive detections before the track is confirmed. The\r\n        track state is set to `Deleted` if a miss occurs within the first\r\n        `n_init` frames.\r\n    max_age : int\r\n        The maximum number of consecutive misses before the track state is\r\n        set to `Deleted`.\r\n    feature : Optional[ndarray]\r\n        Feature vector of the detection this track originates from. If not None,\r\n        this feature is added to the `features` cache.\r\n\r\n    Attributes\r\n    ----------\r\n    mean : ndarray\r\n        Mean vector of the initial state distribution.\r\n    covariance : ndarray\r\n        Covariance matrix of the initial state distribution.\r\n    track_id : int\r\n        A unique track identifier.\r\n    hits : int\r\n        Total number of measurement updates.\r\n    age : int\r\n        Total number of frames since first occurance.\r\n    time_since_update : int\r\n        Total number of frames since last measurement update.\r\n    state : TrackState\r\n        The current track state.\r\n    features : List[ndarray]\r\n        A cache of features. On each measurement update, the associated feature\r\n        vector is added to this list.\r\n\r\n    """"""\r\n\r\n    def __init__(self, mean, covariance, track_id, n_init, max_age,\r\n                 feature=None):\r\n        self.mean = mean\r\n        self.covariance = covariance\r\n        self.track_id = track_id\r\n        self.hits = 1\r\n        self.age = 1\r\n        self.time_since_update = 0\r\n\r\n        self.state = TrackState.Tentative\r\n        self.features = []\r\n        if feature is not None:\r\n            self.features.append(feature)\r\n\r\n        self._n_init = n_init\r\n        self._max_age = max_age\r\n\r\n    def to_tlwh(self):\r\n        """"""Get current position in bounding box format `(top left x, top left y,\r\n        width, height)`.\r\n\r\n        Returns\r\n        -------\r\n        ndarray\r\n            The bounding box.\r\n\r\n        """"""\r\n        ret = self.mean[:4].copy()\r\n        ret[2] *= ret[3]\r\n        ret[:2] -= ret[2:] / 2\r\n        return ret\r\n\r\n    def to_tlbr(self):\r\n        """"""Get current position in bounding box format `(min x, miny, max x,\r\n        max y)`.\r\n\r\n        Returns\r\n        -------\r\n        ndarray\r\n            The bounding box.\r\n\r\n        """"""\r\n        ret = self.to_tlwh()\r\n        ret[2:] = ret[:2] + ret[2:]\r\n        return ret\r\n\r\n    def predict(self, kf):\r\n        """"""Propagate the state distribution to the current time step using a\r\n        Kalman filter prediction step.\r\n\r\n        Parameters\r\n        ----------\r\n        kf : kalman_filter.KalmanFilter\r\n            The Kalman filter.\r\n\r\n        """"""\r\n        self.mean, self.covariance = kf.predict(self.mean, self.covariance)\r\n        self.age += 1\r\n        self.time_since_update += 1\r\n\r\n    def update(self, kf, detection):\r\n        """"""Perform Kalman filter measurement update step and update the feature\r\n        cache.\r\n\r\n        Parameters\r\n        ----------\r\n        kf : kalman_filter.KalmanFilter\r\n            The Kalman filter.\r\n        detection : Detection\r\n            The associated detection.\r\n\r\n        """"""\r\n        self.mean, self.covariance = kf.update(\r\n            self.mean, self.covariance, detection.to_xyah())\r\n        self.features.append(detection.feature)\r\n\r\n        self.hits += 1\r\n        self.time_since_update = 0\r\n        if self.state == TrackState.Tentative and self.hits >= self._n_init:\r\n            self.state = TrackState.Confirmed\r\n\r\n    def mark_missed(self):\r\n        """"""Mark this track as missed (no association at the current time step).\r\n        """"""\r\n        if self.state == TrackState.Tentative:\r\n            self.state = TrackState.Deleted\r\n        elif self.time_since_update > self._max_age:\r\n            self.state = TrackState.Deleted\r\n\r\n    def is_tentative(self):\r\n        """"""Returns True if this track is tentative (unconfirmed).\r\n        """"""\r\n        return self.state == TrackState.Tentative\r\n\r\n    def is_confirmed(self):\r\n        """"""Returns True if this track is confirmed.""""""\r\n        return self.state == TrackState.Confirmed\r\n\r\n    def is_deleted(self):\r\n        """"""Returns True if this track is dead and should be deleted.""""""\r\n        return self.state == TrackState.Deleted\r\n'"
deep_sort/sort/tracker.py,0,"b'# vim: expandtab:ts=4:sw=4\r\nfrom __future__ import absolute_import\r\nimport numpy as np\r\nfrom . import kalman_filter\r\nfrom . import linear_assignment\r\nfrom . import iou_matching\r\nfrom .track import Track\r\n\r\n\r\nclass Tracker:\r\n    """"""\r\n    This is the multi-target tracker.\r\n\r\n    Parameters\r\n    ----------\r\n    metric : nn_matching.NearestNeighborDistanceMetric\r\n        A distance metric for measurement-to-track association.\r\n    max_age : int\r\n        Maximum number of missed misses before a track is deleted.\r\n    n_init : int\r\n        Number of consecutive detections before the track is confirmed. The\r\n        track state is set to `Deleted` if a miss occurs within the first\r\n        `n_init` frames.\r\n\r\n    Attributes\r\n    ----------\r\n    metric : nn_matching.NearestNeighborDistanceMetric\r\n        The distance metric used for measurement to track association.\r\n    max_age : int\r\n        Maximum number of missed misses before a track is deleted.\r\n    n_init : int\r\n        Number of frames that a track remains in initialization phase.\r\n    kf : kalman_filter.KalmanFilter\r\n        A Kalman filter to filter target trajectories in image space.\r\n    tracks : List[Track]\r\n        The list of active tracks at the current time step.\r\n\r\n    """"""\r\n\r\n    def __init__(self, metric, max_iou_distance=0.7, max_age=70, n_init=3):\r\n        self.metric = metric\r\n        self.max_iou_distance = max_iou_distance\r\n        self.max_age = max_age\r\n        self.n_init = n_init\r\n\r\n        self.kf = kalman_filter.KalmanFilter()\r\n        self.tracks = []\r\n        self._next_id = 1\r\n\r\n    def predict(self):\r\n        """"""Propagate track state distributions one time step forward.\r\n\r\n        This function should be called once every time step, before `update`.\r\n        """"""\r\n        for track in self.tracks:\r\n            track.predict(self.kf)\r\n\r\n    def update(self, detections):\r\n        """"""Perform measurement update and track management.\r\n\r\n        Parameters\r\n        ----------\r\n        detections : List[deep_sort.detection.Detection]\r\n            A list of detections at the current time step.\r\n\r\n        """"""\r\n        # Run matching cascade.\r\n        matches, unmatched_tracks, unmatched_detections = \\\r\n            self._match(detections)\r\n\r\n        # Update track set.\r\n        for track_idx, detection_idx in matches:\r\n            self.tracks[track_idx].update(\r\n                self.kf, detections[detection_idx])\r\n        for track_idx in unmatched_tracks:\r\n            self.tracks[track_idx].mark_missed()\r\n        for detection_idx in unmatched_detections:\r\n            self._initiate_track(detections[detection_idx])\r\n        self.tracks = [t for t in self.tracks if not t.is_deleted()]\r\n\r\n        # Update distance metric.\r\n        active_targets = [t.track_id for t in self.tracks if t.is_confirmed()]\r\n        features, targets = [], []\r\n        for track in self.tracks:\r\n            if not track.is_confirmed():\r\n                continue\r\n            features += track.features\r\n            targets += [track.track_id for _ in track.features]\r\n            track.features = []\r\n        self.metric.partial_fit(\r\n            np.asarray(features), np.asarray(targets), active_targets)\r\n\r\n    def _match(self, detections):\r\n\r\n        def gated_metric(tracks, dets, track_indices, detection_indices):\r\n            features = np.array([dets[i].feature for i in detection_indices])\r\n            targets = np.array([tracks[i].track_id for i in track_indices])\r\n            cost_matrix = self.metric.distance(features, targets)\r\n            cost_matrix = linear_assignment.gate_cost_matrix(\r\n                self.kf, cost_matrix, tracks, dets, track_indices,\r\n                detection_indices)\r\n\r\n            return cost_matrix\r\n\r\n        # Split track set into confirmed and unconfirmed tracks.\r\n        confirmed_tracks = [\r\n            i for i, t in enumerate(self.tracks) if t.is_confirmed()]\r\n        unconfirmed_tracks = [\r\n            i for i, t in enumerate(self.tracks) if not t.is_confirmed()]\r\n\r\n        # Associate confirmed tracks using appearance features.\r\n        matches_a, unmatched_tracks_a, unmatched_detections = \\\r\n            linear_assignment.matching_cascade(\r\n                gated_metric, self.metric.matching_threshold, self.max_age,\r\n                self.tracks, detections, confirmed_tracks)\r\n\r\n        # Associate remaining tracks together with unconfirmed tracks using IOU.\r\n        iou_track_candidates = unconfirmed_tracks + [\r\n            k for k in unmatched_tracks_a if\r\n            self.tracks[k].time_since_update == 1]\r\n        unmatched_tracks_a = [\r\n            k for k in unmatched_tracks_a if\r\n            self.tracks[k].time_since_update != 1]\r\n        matches_b, unmatched_tracks_b, unmatched_detections = \\\r\n            linear_assignment.min_cost_matching(\r\n                iou_matching.iou_cost, self.max_iou_distance, self.tracks,\r\n                detections, iou_track_candidates, unmatched_detections)\r\n\r\n        matches = matches_a + matches_b\r\n        unmatched_tracks = list(set(unmatched_tracks_a + unmatched_tracks_b))\r\n        return matches, unmatched_tracks, unmatched_detections\r\n\r\n    def _initiate_track(self, detection):\r\n        mean, covariance = self.kf.initiate(detection.to_xyah())\r\n        self.tracks.append(Track(\r\n            mean, covariance, self._next_id, self.n_init, self.max_age,\r\n            detection.feature))\r\n        self._next_id += 1\r\n'"
detector/YOLOv3/__init__.py,0,"b'import sys\nsys.path.append(""detector/YOLOv3"")\n\n\nfrom .detector import YOLOv3\n__all__ = [\'YOLOv3\']\n\n\n\n'"
detector/YOLOv3/cfg.py,10,"b'import torch\nfrom .yolo_utils import convert2cpu\n\n\ndef parse_cfg(cfgfile):\n    blocks = []\n    fp = open(cfgfile)\n    block = None\n    line = fp.readline()\n    while line != \'\':\n        line = line.rstrip()\n        if line == \'\' or line[0] == \'#\':\n            line = fp.readline()\n            continue\n        elif line[0] == \'[\':\n            if block:\n                blocks.append(block)\n            block = dict()\n            block[\'type\'] = line.lstrip(\'[\').rstrip(\']\')\n            # set default value\n            if block[\'type\'] == \'convolutional\':\n                block[\'batch_normalize\'] = 0\n        else:\n            key, value = line.split(\'=\')\n            key = key.strip()\n            if key == \'type\':\n                key = \'_type\'\n            value = value.strip()\n            block[key] = value\n        line = fp.readline()\n\n    if block:\n        blocks.append(block)\n    fp.close()\n    return blocks\n\n\ndef print_cfg(blocks):\n    print(\'layer     filters    size              input                output\');\n    prev_width = 416\n    prev_height = 416\n    prev_filters = 3\n    out_filters = []\n    out_widths = []\n    out_heights = []\n    ind = -2\n    for block in blocks:\n        ind += 1\n        if block[\'type\'] == \'net\':\n            prev_width = int(block[\'width\'])\n            prev_height = int(block[\'height\'])\n            continue\n        elif block[\'type\'] == \'convolutional\':\n            filters = int(block[\'filters\'])\n            kernel_size = int(block[\'size\'])\n            stride = int(block[\'stride\'])\n            is_pad = int(block[\'pad\'])\n            pad = (kernel_size - 1) // 2 if is_pad else 0\n            width = (prev_width + 2 * pad - kernel_size) // stride + 1\n            height = (prev_height + 2 * pad - kernel_size) // stride + 1\n            print(\'%5d %-6s %4d  %d x %d / %d   %3d x %3d x%4d   ->   %3d x %3d x%4d\' % (\n            ind, \'conv\', filters, kernel_size, kernel_size, stride, prev_width, prev_height, prev_filters, width,\n            height, filters))\n            prev_width = width\n            prev_height = height\n            prev_filters = filters\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block[\'type\'] == \'maxpool\':\n            pool_size = int(block[\'size\'])\n            stride = int(block[\'stride\'])\n            width = prev_width // stride\n            height = prev_height // stride\n            print(\'%5d %-6s       %d x %d / %d   %3d x %3d x%4d   ->   %3d x %3d x%4d\' % (\n            ind, \'max\', pool_size, pool_size, stride, prev_width, prev_height, prev_filters, width, height, filters))\n            prev_width = width\n            prev_height = height\n            prev_filters = filters\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block[\'type\'] == \'avgpool\':\n            width = 1\n            height = 1\n            print(\'%5d %-6s                   %3d x %3d x%4d   ->  %3d\' % (\n            ind, \'avg\', prev_width, prev_height, prev_filters, prev_filters))\n            prev_width = width\n            prev_height = height\n            prev_filters = filters\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block[\'type\'] == \'softmax\':\n            print(\'%5d %-6s                                    ->  %3d\' % (ind, \'softmax\', prev_filters))\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block[\'type\'] == \'cost\':\n            print(\'%5d %-6s                                     ->  %3d\' % (ind, \'cost\', prev_filters))\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block[\'type\'] == \'reorg\':\n            stride = int(block[\'stride\'])\n            filters = stride * stride * prev_filters\n            width = prev_width // stride\n            height = prev_height // stride\n            print(\'%5d %-6s             / %d   %3d x %3d x%4d   ->   %3d x %3d x%4d\' % (\n            ind, \'reorg\', stride, prev_width, prev_height, prev_filters, width, height, filters))\n            prev_width = width\n            prev_height = height\n            prev_filters = filters\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block[\'type\'] == \'upsample\':\n            stride = int(block[\'stride\'])\n            filters = prev_filters\n            width = prev_width * stride\n            height = prev_height * stride\n            print(\'%5d %-6s           * %d   %3d x %3d x%4d   ->   %3d x %3d x%4d\' % (\n            ind, \'upsample\', stride, prev_width, prev_height, prev_filters, width, height, filters))\n            prev_width = width\n            prev_height = height\n            prev_filters = filters\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block[\'type\'] == \'route\':\n            layers = block[\'layers\'].split(\',\')\n            layers = [int(i) if int(i) > 0 else int(i) + ind for i in layers]\n            if len(layers) == 1:\n                print(\'%5d %-6s %d\' % (ind, \'route\', layers[0]))\n                prev_width = out_widths[layers[0]]\n                prev_height = out_heights[layers[0]]\n                prev_filters = out_filters[layers[0]]\n            elif len(layers) == 2:\n                print(\'%5d %-6s %d %d\' % (ind, \'route\', layers[0], layers[1]))\n                prev_width = out_widths[layers[0]]\n                prev_height = out_heights[layers[0]]\n                assert (prev_width == out_widths[layers[1]])\n                assert (prev_height == out_heights[layers[1]])\n                prev_filters = out_filters[layers[0]] + out_filters[layers[1]]\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block[\'type\'] in [\'region\', \'yolo\']:\n            print(\'%5d %-6s\' % (ind, \'detection\'))\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block[\'type\'] == \'shortcut\':\n            from_id = int(block[\'from\'])\n            from_id = from_id if from_id > 0 else from_id + ind\n            print(\'%5d %-6s %d\' % (ind, \'shortcut\', from_id))\n            prev_width = out_widths[from_id]\n            prev_height = out_heights[from_id]\n            prev_filters = out_filters[from_id]\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block[\'type\'] == \'connected\':\n            filters = int(block[\'output\'])\n            print(\'%5d %-6s                            %d  ->  %3d\' % (ind, \'connected\', prev_filters, filters))\n            prev_filters = filters\n            out_widths.append(1)\n            out_heights.append(1)\n            out_filters.append(prev_filters)\n        else:\n            print(\'unknown type %s\' % (block[\'type\']))\n\n\ndef load_conv(buf, start, conv_model):\n    num_w = conv_model.weight.numel()\n    num_b = conv_model.bias.numel()\n    # print(""start: {}, num_w: {}, num_b: {}"".format(start, num_w, num_b))\n    # by ysyun, use .view_as()\n    conv_model.bias.data.copy_(torch.from_numpy(buf[start:start + num_b]).view_as(conv_model.bias.data));\n    start = start + num_b\n    conv_model.weight.data.copy_(torch.from_numpy(buf[start:start + num_w]).view_as(conv_model.weight.data));\n    start = start + num_w\n    return start\n\n\ndef save_conv(fp, conv_model):\n    if conv_model.bias.is_cuda:\n        convert2cpu(conv_model.bias.data).numpy().tofile(fp)\n        convert2cpu(conv_model.weight.data).numpy().tofile(fp)\n    else:\n        conv_model.bias.data.numpy().tofile(fp)\n        conv_model.weight.data.numpy().tofile(fp)\n\n\ndef load_conv_bn(buf, start, conv_model, bn_model):\n    num_w = conv_model.weight.numel()\n    num_b = bn_model.bias.numel()\n    bn_model.bias.data.copy_(torch.from_numpy(buf[start:start + num_b]));\n    start = start + num_b\n    bn_model.weight.data.copy_(torch.from_numpy(buf[start:start + num_b]));\n    start = start + num_b\n    bn_model.running_mean.copy_(torch.from_numpy(buf[start:start + num_b]));\n    start = start + num_b\n    bn_model.running_var.copy_(torch.from_numpy(buf[start:start + num_b]));\n    start = start + num_b\n    # conv_model.weight.data.copy_(torch.from_numpy(buf[start:start+num_w])); start = start + num_w\n    conv_model.weight.data.copy_(torch.from_numpy(buf[start:start + num_w]).view_as(conv_model.weight.data));\n    start = start + num_w\n    return start\n\n\ndef save_conv_bn(fp, conv_model, bn_model):\n    if bn_model.bias.is_cuda:\n        convert2cpu(bn_model.bias.data).numpy().tofile(fp)\n        convert2cpu(bn_model.weight.data).numpy().tofile(fp)\n        convert2cpu(bn_model.running_mean).numpy().tofile(fp)\n        convert2cpu(bn_model.running_var).numpy().tofile(fp)\n        convert2cpu(conv_model.weight.data).numpy().tofile(fp)\n    else:\n        bn_model.bias.data.numpy().tofile(fp)\n        bn_model.weight.data.numpy().tofile(fp)\n        bn_model.running_mean.numpy().tofile(fp)\n        bn_model.running_var.numpy().tofile(fp)\n        conv_model.weight.data.numpy().tofile(fp)\n\n\ndef load_fc(buf, start, fc_model):\n    num_w = fc_model.weight.numel()\n    num_b = fc_model.bias.numel()\n    fc_model.bias.data.copy_(torch.from_numpy(buf[start:start + num_b]));\n    start = start + num_b\n    fc_model.weight.data.copy_(torch.from_numpy(buf[start:start + num_w]));\n    start = start + num_w\n    return start\n\n\ndef save_fc(fp, fc_model):\n    fc_model.bias.data.numpy().tofile(fp)\n    fc_model.weight.data.numpy().tofile(fp)\n\n\nif __name__ == \'__main__\':\n    import sys\n\n    blocks = parse_cfg(\'cfg/yolo.cfg\')\n    if len(sys.argv) == 2:\n        blocks = parse_cfg(sys.argv[1])\n    print_cfg(blocks)\n'"
detector/YOLOv3/darknet.py,5,"b""import torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom .cfg import *\nfrom .region_layer import RegionLayer\nfrom .yolo_layer import YoloLayer\n\nclass MaxPoolStride1(nn.Module):\n    def __init__(self):\n        super(MaxPoolStride1, self).__init__()\n\n    def forward(self, x):\n        x = F.max_pool2d(F.pad(x, (0,1,0,1), mode='replicate'), 2, stride=1)\n        return x\n\nclass Upsample(nn.Module):\n    def __init__(self, stride=2):\n        super(Upsample, self).__init__()\n        self.stride = stride\n    def forward(self, x):\n        stride = self.stride\n        assert(x.data.dim() == 4)\n        B = x.data.size(0)\n        C = x.data.size(1)\n        H = x.data.size(2)\n        W = x.data.size(3)\n        ws = stride\n        hs = stride\n        x = x.view(B, C, H, 1, W, 1).expand(B, C, H, hs, W, ws).contiguous().view(B, C, H*hs, W*ws)\n        return x\n\nclass Reorg(nn.Module):\n    def __init__(self, stride=2):\n        super(Reorg, self).__init__()\n        self.stride = stride\n    def forward(self, x):\n        stride = self.stride\n        assert(x.data.dim() == 4)\n        B = x.data.size(0)\n        C = x.data.size(1)\n        H = x.data.size(2)\n        W = x.data.size(3)\n        assert(H % stride == 0)\n        assert(W % stride == 0)\n        ws = stride\n        hs = stride\n        x = x.view(B, C, H//hs, hs, W//ws, ws).transpose(3,4).contiguous()\n        x = x.view(B, C, (H//hs)*(W//ws), hs*ws).transpose(2,3).contiguous()\n        x = x.view(B, C, hs*ws, H//hs, W//ws).transpose(1,2).contiguous()\n        x = x.view(B, hs*ws*C, H//hs, W//ws)\n        return x\n\nclass GlobalAvgPool2d(nn.Module):\n    def __init__(self):\n        super(GlobalAvgPool2d, self).__init__()\n\n    def forward(self, x):\n        N = x.data.size(0)\n        C = x.data.size(1)\n        H = x.data.size(2)\n        W = x.data.size(3)\n        x = F.avg_pool2d(x, (H, W))\n        x = x.view(N, C)\n        return x\n\n# for route and shortcut\nclass EmptyModule(nn.Module):\n    def __init__(self):\n        super(EmptyModule, self).__init__()\n\n    def forward(self, x):\n        return x\n\n# support route shortcut and reorg\n\nclass Darknet(nn.Module):\n    def getLossLayers(self):\n        loss_layers = []\n        for m in self.models:\n            if isinstance(m, RegionLayer) or isinstance(m, YoloLayer):\n                loss_layers.append(m)\n        return loss_layers\n\n    def __init__(self, cfgfile, use_cuda=True):\n        super(Darknet, self).__init__()\n        self.use_cuda = use_cuda\n        self.blocks = parse_cfg(cfgfile)\n        self.models = self.create_network(self.blocks) # merge conv, bn,leaky\n        self.loss_layers = self.getLossLayers()\n\n        #self.width = int(self.blocks[0]['width'])\n        #self.height = int(self.blocks[0]['height'])\n\n        if len(self.loss_layers) > 0:\n            last = len(self.loss_layers)-1\n            self.anchors = self.loss_layers[last].anchors\n            self.num_anchors = self.loss_layers[last].num_anchors\n            self.anchor_step = self.loss_layers[last].anchor_step\n            self.num_classes = self.loss_layers[last].num_classes\n\n        # default format : major=0, minor=1\n        self.header = torch.IntTensor([0,1,0,0])\n        self.seen = 0\n\n    def forward(self, x):\n        ind = -2\n        self.loss_layers = None\n        outputs = dict()\n        out_boxes = dict()\n        outno = 0\n        for block in self.blocks:\n            ind = ind + 1\n\n            if block['type'] == 'net':\n                continue\n            elif block['type'] in ['convolutional', 'maxpool', 'reorg', 'upsample', 'avgpool', 'softmax', 'connected']:\n                x = self.models[ind](x)\n                outputs[ind] = x\n            elif block['type'] == 'route':\n                layers = block['layers'].split(',')\n                layers = [int(i) if int(i) > 0 else int(i)+ind for i in layers]\n                if len(layers) == 1:\n                    x = outputs[layers[0]]\n                elif len(layers) == 2:\n                    x1 = outputs[layers[0]]\n                    x2 = outputs[layers[1]]\n                    x = torch.cat((x1,x2),1)\n                outputs[ind] = x\n            elif block['type'] == 'shortcut':\n                from_layer = int(block['from'])\n                activation = block['activation']\n                from_layer = from_layer if from_layer > 0 else from_layer + ind\n                x1 = outputs[from_layer]\n                x2 = outputs[ind-1]\n                x  = x1 + x2\n                if activation == 'leaky':\n                    x = F.leaky_relu(x, 0.1, inplace=True)\n                elif activation == 'relu':\n                    x = F.relu(x, inplace=True)\n                outputs[ind] = x\n            elif block['type'] in [ 'region', 'yolo']:\n                boxes = self.models[ind].get_mask_boxes(x)\n                out_boxes[outno]= boxes\n                outno += 1\n                outputs[ind] = None\n            elif block['type'] == 'cost':\n                continue\n            else:\n                print('unknown type %s' % (block['type']))\n        return x if outno == 0 else out_boxes\n\n    def print_network(self):\n        print_cfg(self.blocks)\n\n    def create_network(self, blocks):\n        models = nn.ModuleList()\n    \n        prev_filters = 3\n        out_filters =[]\n        prev_stride = 1\n        out_strides = []\n        conv_id = 0\n        ind = -2\n        for block in blocks:\n            ind += 1\n            if block['type'] == 'net':\n                prev_filters = int(block['channels'])\n                self.width = int(block['width'])\n                self.height = int(block['height'])\n                continue\n            elif block['type'] == 'convolutional':\n                conv_id = conv_id + 1\n                batch_normalize = int(block['batch_normalize'])\n                filters = int(block['filters'])\n                kernel_size = int(block['size'])\n                stride = int(block['stride'])\n                is_pad = int(block['pad'])\n                pad = (kernel_size-1)//2 if is_pad else 0\n                activation = block['activation']\n                model = nn.Sequential()\n                if batch_normalize:\n                    model.add_module('conv{0}'.format(conv_id), nn.Conv2d(prev_filters, filters, kernel_size, stride, pad, bias=False))\n                    model.add_module('bn{0}'.format(conv_id), nn.BatchNorm2d(filters))\n                    #model.add_module('bn{0}'.format(conv_id), BN2d(filters))\n                else:\n                    model.add_module('conv{0}'.format(conv_id), nn.Conv2d(prev_filters, filters, kernel_size, stride, pad))\n                if activation == 'leaky':\n                    model.add_module('leaky{0}'.format(conv_id), nn.LeakyReLU(0.1, inplace=True))\n                elif activation == 'relu':\n                    model.add_module('relu{0}'.format(conv_id), nn.ReLU(inplace=True))\n                prev_filters = filters\n                out_filters.append(prev_filters)\n                prev_stride = stride * prev_stride\n                out_strides.append(prev_stride)                \n                models.append(model)\n            elif block['type'] == 'maxpool':\n                pool_size = int(block['size'])\n                stride = int(block['stride'])\n                if stride > 1:\n                    model = nn.MaxPool2d(pool_size, stride)\n                else:\n                    model = MaxPoolStride1()\n                out_filters.append(prev_filters)\n                prev_stride = stride * prev_stride\n                out_strides.append(prev_stride)                \n                models.append(model)\n            elif block['type'] == 'avgpool':\n                model = GlobalAvgPool2d()\n                out_filters.append(prev_filters)\n                models.append(model)\n            elif block['type'] == 'softmax':\n                model = nn.Softmax()\n                out_strides.append(prev_stride)\n                out_filters.append(prev_filters)\n                models.append(model)\n            elif block['type'] == 'cost':\n                if block['_type'] == 'sse':\n                    model = nn.MSELoss(size_average=True)\n                elif block['_type'] == 'L1':\n                    model = nn.L1Loss(size_average=True)\n                elif block['_type'] == 'smooth':\n                    model = nn.SmoothL1Loss(size_average=True)\n                out_filters.append(1)\n                out_strides.append(prev_stride)\n                models.append(model)\n            elif block['type'] == 'reorg':\n                stride = int(block['stride'])\n                prev_filters = stride * stride * prev_filters\n                out_filters.append(prev_filters)\n                prev_stride = prev_stride * stride\n                out_strides.append(prev_stride)                \n                models.append(Reorg(stride))\n            elif block['type'] == 'upsample':\n                stride = int(block['stride'])\n                out_filters.append(prev_filters)\n                prev_stride = prev_stride / stride\n                out_strides.append(prev_stride)                \n                #models.append(nn.Upsample(scale_factor=stride, mode='nearest'))\n                models.append(Upsample(stride))\n            elif block['type'] == 'route':\n                layers = block['layers'].split(',')\n                ind = len(models)\n                layers = [int(i) if int(i) > 0 else int(i)+ind for i in layers]\n                if len(layers) == 1:\n                    prev_filters = out_filters[layers[0]]\n                    prev_stride = out_strides[layers[0]]\n                elif len(layers) == 2:\n                    assert(layers[0] == ind - 1)\n                    prev_filters = out_filters[layers[0]] + out_filters[layers[1]]\n                    prev_stride = out_strides[layers[0]]\n                out_filters.append(prev_filters)\n                out_strides.append(prev_stride)\n                models.append(EmptyModule())\n            elif block['type'] == 'shortcut':\n                ind = len(models)\n                prev_filters = out_filters[ind-1]\n                out_filters.append(prev_filters)\n                prev_stride = out_strides[ind-1]\n                out_strides.append(prev_stride)\n                models.append(EmptyModule())\n            elif block['type'] == 'connected':\n                filters = int(block['output'])\n                if block['activation'] == 'linear':\n                    model = nn.Linear(prev_filters, filters)\n                elif block['activation'] == 'leaky':\n                    model = nn.Sequential(\n                               nn.Linear(prev_filters, filters),\n                               nn.LeakyReLU(0.1, inplace=True))\n                elif block['activation'] == 'relu':\n                    model = nn.Sequential(\n                               nn.Linear(prev_filters, filters),\n                               nn.ReLU(inplace=True))\n                prev_filters = filters\n                out_filters.append(prev_filters)\n                out_strides.append(prev_stride)\n                models.append(model)\n            elif block['type'] == 'region':\n                region_layer = RegionLayer(use_cuda=self.use_cuda)\n                anchors = block['anchors'].split(',')\n                region_layer.anchors = [float(i) for i in anchors]\n                region_layer.num_classes = int(block['classes'])\n                region_layer.num_anchors = int(block['num'])\n                region_layer.anchor_step = len(region_layer.anchors)//region_layer.num_anchors\n                region_layer.rescore = int(block['rescore'])\n                region_layer.object_scale = float(block['object_scale'])\n                region_layer.noobject_scale = float(block['noobject_scale'])\n                region_layer.class_scale = float(block['class_scale'])\n                region_layer.coord_scale = float(block['coord_scale'])\n                region_layer.thresh = float(block['thresh'])\n                out_filters.append(prev_filters)\n                out_strides.append(prev_stride)\n                models.append(region_layer)\n            elif block['type'] == 'yolo':\n                yolo_layer = YoloLayer(use_cuda=self.use_cuda)\n                anchors = block['anchors'].split(',')\n                anchor_mask = block['mask'].split(',')\n                yolo_layer.anchor_mask = [int(i) for i in anchor_mask]\n                yolo_layer.anchors = [float(i) for i in anchors]\n                yolo_layer.num_classes = int(block['classes'])\n                yolo_layer.num_anchors = int(block['num'])\n                yolo_layer.anchor_step = len(yolo_layer.anchors)//yolo_layer.num_anchors\n                try:\n                    yolo_layer.rescore = int(block['rescore'])\n                except:\n                    pass\n                yolo_layer.ignore_thresh = float(block['ignore_thresh'])\n                yolo_layer.truth_thresh = float(block['truth_thresh'])\n                yolo_layer.stride = prev_stride\n                yolo_layer.nth_layer = ind\n                yolo_layer.net_width = self.width\n                yolo_layer.net_height = self.height\n                out_filters.append(prev_filters)\n                out_strides.append(prev_stride)\n                models.append(yolo_layer)                \n            else:\n                print('unknown type %s' % (block['type']))\n    \n        return models\n\n    def load_binfile(self, weightfile):\n        fp = open(weightfile, 'rb')\n       \n        version = np.fromfile(fp, count=3, dtype=np.int32)\n        version = [int(i) for i in version]\n        if version[0]*10+version[1] >=2 and version[0] < 1000 and version[1] < 1000:\n            seen = np.fromfile(fp, count=1, dtype=np.int64)\n        else:\n            seen = np.fromfile(fp, count=1, dtype=np.int32)\n        self.header = torch.from_numpy(np.concatenate((version, seen), axis=0))\n        self.seen = int(seen)\n        body = np.fromfile(fp, dtype=np.float32)\n        fp.close()\n        return body\n\n    def load_weights(self, weightfile):\n        buf = self.load_binfile(weightfile)\n\n        start = 0\n        ind = -2\n        for block in self.blocks:\n            if start >= buf.size:\n                break\n            ind = ind + 1\n            if block['type'] == 'net':\n                continue\n            elif block['type'] == 'convolutional':\n                model = self.models[ind]\n                batch_normalize = int(block['batch_normalize'])\n                if batch_normalize:\n                    start = load_conv_bn(buf, start, model[0], model[1])\n                else:\n                    start = load_conv(buf, start, model[0])\n            elif block['type'] == 'connected':\n                model = self.models[ind]\n                if block['activation'] != 'linear':\n                    start = load_fc(buf, start, model[0])\n                else:\n                    start = load_fc(buf, start, model)\n            elif block['type'] == 'maxpool':\n                pass\n            elif block['type'] == 'reorg':\n                pass\n            elif block['type'] == 'upsample':\n                pass\n            elif block['type'] == 'route':\n                pass\n            elif block['type'] == 'shortcut':\n                pass\n            elif block['type'] == 'region':\n                pass\n            elif block['type'] == 'yolo':\n                pass                \n            elif block['type'] == 'avgpool':\n                pass\n            elif block['type'] == 'softmax':\n                pass\n            elif block['type'] == 'cost':\n                pass\n            else:\n                print('unknown type %s' % (block['type']))\n\n    def save_weights(self, outfile, cutoff=0):\n        if cutoff <= 0:\n            cutoff = len(self.blocks)-1\n\n        fp = open(outfile, 'wb')\n        self.header[3] = self.seen\n        header = np.array(self.header[0:3].numpy(), np.int32)\n        header.tofile(fp)\n        if (self.header[0]*10+self.header[1]) >= 2:\n            seen = np.array(self.seen, np.int64)\n        else:\n            seen = np.array(self.seen, np.int32)\n        seen.tofile(fp)\n\n        ind = -1\n        for blockId in range(1, cutoff+1):\n            ind = ind + 1\n            block = self.blocks[blockId]\n            if block['type'] == 'convolutional':\n                model = self.models[ind]\n                batch_normalize = int(block['batch_normalize'])\n                if batch_normalize:\n                    save_conv_bn(fp, model[0], model[1])\n                else:\n                    save_conv(fp, model[0])\n            elif block['type'] == 'connected':\n                model = self.models[ind]\n                if block['activation'] != 'linear':\n                    save_fc(fc, model)\n                else:\n                    save_fc(fc, model[0])\n            elif block['type'] == 'maxpool':\n                pass\n            elif block['type'] == 'reorg':\n                pass\n            elif block['type'] == 'upsample':\n                pass                \n            elif block['type'] == 'route':\n                pass\n            elif block['type'] == 'shortcut':\n                pass\n            elif block['type'] == 'region':\n                pass\n            elif block['type'] == 'yolo':\n                pass\n            elif block['type'] == 'avgpool':\n                pass\n            elif block['type'] == 'softmax':\n                pass\n            elif block['type'] == 'cost':\n                pass\n            else:\n                print('unknown type %s' % (block['type']))\n        fp.close()\n"""
detector/YOLOv3/detect.py,0,"b'import sys\nimport time\nfrom PIL import Image, ImageDraw\n#from models.tiny_yolo import TinyYoloNet\nfrom yolo_utils import *\nfrom darknet import Darknet\n\nimport cv2\n\nnamesfile=None\ndef detect(cfgfile, weightfile, imgfolder):\n    m = Darknet(cfgfile)\n\n    #m.print_network()\n    m.load_weights(weightfile)\n    print(\'Loading weights from %s... Done!\' % (weightfile))\n\n    # if m.num_classes == 20:\n    #     namesfile = \'data/voc.names\'\n    # elif m.num_classes == 80:\n    #     namesfile = \'data/coco.names\'\n    # else:\n    #     namesfile = \'data/names\'\n    \n    use_cuda = True\n    if use_cuda:\n        m.cuda()\n\n    imgfiles = [x for x in os.listdir(imgfolder) if x[-4:] == \'.jpg\']\n    imgfiles.sort()\n    for imgname in imgfiles:\n        imgfile = os.path.join(imgfolder,imgname)\n        \n        img = Image.open(imgfile).convert(\'RGB\')\n        sized = img.resize((m.width, m.height))\n\n        #for i in range(2):\n        start = time.time()\n        boxes = do_detect(m, sized, 0.5, 0.4, use_cuda)\n        finish = time.time()\n            #if i == 1:\n        print(\'%s: Predicted in %f seconds.\' % (imgfile, (finish-start)))\n\n        class_names = load_class_names(namesfile)\n        img = plot_boxes(img, boxes, \'result/{}\'.format(os.path.basename(imgfile)), class_names)\n        img = np.array(img)\n        cv2.imshow(\'{}\'.format(os.path.basename(imgfolder)), img)\n        cv2.resizeWindow(\'{}\'.format(os.path.basename(imgfolder)), 1000,800)\n        cv2.waitKey(1000)\n\ndef detect_cv2(cfgfile, weightfile, imgfile):\n    import cv2\n    m = Darknet(cfgfile)\n\n    m.print_network()\n    m.load_weights(weightfile)\n    print(\'Loading weights from %s... Done!\' % (weightfile))\n\n    if m.num_classes == 20:\n        namesfile = \'data/voc.names\'\n    elif m.num_classes == 80:\n        namesfile = \'data/coco.names\'\n    else:\n        namesfile = \'data/names\'\n    \n    use_cuda = True\n    if use_cuda:\n        m.cuda()\n\n    img = cv2.imread(imgfile)\n    sized = cv2.resize(img, (m.width, m.height))\n    sized = cv2.cvtColor(sized, cv2.COLOR_BGR2RGB)\n    \n    for i in range(2):\n        start = time.time()\n        boxes = do_detect(m, sized, 0.5, 0.4, use_cuda)\n        finish = time.time()\n        if i == 1:\n            print(\'%s: Predicted in %f seconds.\' % (imgfile, (finish-start)))\n\n    class_names = load_class_names(namesfile)\n    plot_boxes_cv2(img, boxes, savename=\'predictions.jpg\', class_names=class_names)\n\ndef detect_skimage(cfgfile, weightfile, imgfile):\n    from skimage import io\n    from skimage.transform import resize\n    m = Darknet(cfgfile)\n\n    m.print_network()\n    m.load_weights(weightfile)\n    print(\'Loading weights from %s... Done!\' % (weightfile))\n\n    if m.num_classes == 20:\n        namesfile = \'data/voc.names\'\n    elif m.num_classes == 80:\n        namesfile = \'data/coco.names\'\n    else:\n        namesfile = \'data/names\'\n    \n    use_cuda = True\n    if use_cuda:\n        m.cuda()\n\n    img = io.imread(imgfile)\n    sized = resize(img, (m.width, m.height)) * 255\n    \n    for i in range(2):\n        start = time.time()\n        boxes = do_detect(m, sized, 0.5, 0.4, use_cuda)\n        finish = time.time()\n        if i == 1:\n            print(\'%s: Predicted in %f seconds.\' % (imgfile, (finish-start)))\n\n    class_names = load_class_names(namesfile)\n    plot_boxes_cv2(img, boxes, savename=\'predictions.jpg\', class_names=class_names)\n\nif __name__ == \'__main__\':\n    if len(sys.argv) == 5:\n        cfgfile = sys.argv[1]\n        weightfile = sys.argv[2]\n        imgfolder = sys.argv[3]\n        cv2.namedWindow(\'{}\'.format(os.path.basename(imgfolder)), cv2.WINDOW_NORMAL )\n        cv2.resizeWindow(\'{}\'.format(os.path.basename(imgfolder)), 1000,800)\n        globals()[""namesfile""] = sys.argv[4]\n        detect(cfgfile, weightfile, imgfolder)\n        #detect_cv2(cfgfile, weightfile, imgfile)\n        #detect_skimage(cfgfile, weightfile, imgfile)\n    else:\n        print(\'Usage: \')\n        print(\'  python detect.py cfgfile weightfile imgfolder names\')\n        #detect(\'cfg/tiny-yolo-voc.cfg\', \'tiny-yolo-voc.weights\', \'data/person.jpg\', version=1)\n'"
detector/YOLOv3/detector.py,6,"b'import torch\nimport logging\nimport numpy as np\nimport cv2\n\nfrom .darknet import Darknet\nfrom .yolo_utils import get_all_boxes, nms, post_process, xywh_to_xyxy, xyxy_to_xywh\nfrom .nms import boxes_nms\n\n\nclass YOLOv3(object):\n    def __init__(self, cfgfile, weightfile, namesfile, score_thresh=0.7, conf_thresh=0.01, nms_thresh=0.45,\n                 is_xywh=False, use_cuda=True):\n        # net definition\n        self.net = Darknet(cfgfile)\n        self.net.load_weights(weightfile)\n        logger = logging.getLogger(""root.detector"")\n        logger.info(\'Loading weights from %s... Done!\' % (weightfile))\n        self.device = ""cuda"" if use_cuda else ""cpu""\n        self.net.eval()\n        self.net.to(self.device)\n\n        # constants\n        self.size = self.net.width, self.net.height\n        self.score_thresh = score_thresh\n        self.conf_thresh = conf_thresh\n        self.nms_thresh = nms_thresh\n        self.use_cuda = use_cuda\n        self.is_xywh = is_xywh\n        self.num_classes = self.net.num_classes\n        self.class_names = self.load_class_names(namesfile)\n\n    def __call__(self, ori_img):\n        # img to tensor\n        assert isinstance(ori_img, np.ndarray), ""input must be a numpy array!""\n        img = ori_img.astype(np.float) / 255.\n\n        img = cv2.resize(img, self.size)\n        img = torch.from_numpy(img).float().permute(2, 0, 1).unsqueeze(0)\n\n        # forward\n        with torch.no_grad():\n            img = img.to(self.device)\n            out_boxes = self.net(img)\n            boxes = get_all_boxes(out_boxes, self.conf_thresh, self.num_classes,\n                                  use_cuda=self.use_cuda)  # batch size is 1\n            # boxes = nms(boxes, self.nms_thresh)\n\n            boxes = post_process(boxes, self.net.num_classes, self.conf_thresh, self.nms_thresh)[0].cpu()\n            boxes = boxes[boxes[:, -2] > self.score_thresh, :]  # bbox xmin ymin xmax ymax\n\n        if len(boxes) == 0:\n            bbox = torch.FloatTensor([]).reshape([0, 4])\n            cls_conf = torch.FloatTensor([])\n            cls_ids = torch.LongTensor([])\n        else:\n            height, width = ori_img.shape[:2]\n            bbox = boxes[:, :4]\n            if self.is_xywh:\n                # bbox x y w h\n                bbox = xyxy_to_xywh(bbox)\n\n            bbox *= torch.FloatTensor([[width, height, width, height]])\n            cls_conf = boxes[:, 5]\n            cls_ids = boxes[:, 6].long()\n        return bbox.numpy(), cls_conf.numpy(), cls_ids.numpy()\n\n    def load_class_names(self, namesfile):\n        with open(namesfile, \'r\', encoding=\'utf8\') as fp:\n            class_names = [line.strip() for line in fp.readlines()]\n        return class_names\n\n\ndef demo():\n    import os\n    from vizer.draw import draw_boxes\n\n    yolo = YOLOv3(""cfg/yolo_v3.cfg"", ""weight/yolov3.weights"", ""cfg/coco.names"")\n    print(""yolo.size ="", yolo.size)\n    root = ""./demo""\n    resdir = os.path.join(root, ""results"")\n    os.makedirs(resdir, exist_ok=True)\n    files = [os.path.join(root, file) for file in os.listdir(root) if file.endswith(\'.jpg\')]\n    files.sort()\n    for filename in files:\n        img = cv2.imread(filename)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        bbox, cls_conf, cls_ids = yolo(img)\n\n        if bbox is not None:\n            img = draw_boxes(img, bbox, cls_ids, cls_conf, class_name_map=yolo.class_names)\n        # save results\n        cv2.imwrite(os.path.join(resdir, os.path.basename(filename)), img[:, :, (2, 1, 0)])\n        # imshow\n        # cv2.namedWindow(""yolo"", cv2.WINDOW_NORMAL)\n        # cv2.resizeWindow(""yolo"", 600,600)\n        # cv2.imshow(""yolo"",res[:,:,(2,1,0)])\n        # cv2.waitKey(0)\n\n\nif __name__ == ""__main__"":\n    demo()\n'"
detector/YOLOv3/region_layer.py,30,"b'import math\nimport sys\nimport time\nimport torch\nimport torch.nn as nn\nfrom .yolo_utils import bbox_iou, multi_bbox_ious, convert2cpu\n\n\nclass RegionLayer(nn.Module):\n    def __init__(self, num_classes=0, anchors=[], num_anchors=1, use_cuda=None):\n        super(RegionLayer, self).__init__()\n        use_cuda = torch.cuda.is_available() and (True if use_cuda is None else use_cuda)\n        self.device = torch.device(""cuda"" if use_cuda else ""cpu"")\n        self.num_classes = num_classes\n        self.num_anchors = num_anchors\n        self.anchor_step = len(anchors) // num_anchors\n        # self.anchors = torch.stack(torch.FloatTensor(anchors).split(self.anchor_step)).to(self.device)\n        self.anchors = torch.FloatTensor(anchors).view(self.num_anchors, self.anchor_step).to(self.device)\n        self.rescore = 1\n        self.coord_scale = 1\n        self.noobject_scale = 1\n        self.object_scale = 5\n        self.class_scale = 1\n        self.thresh = 0.6\n        self.seen = 0\n\n    def build_targets(self, pred_boxes, target, nH, nW):\n        nB = target.size(0)\n        nA = self.num_anchors\n        conf_mask = torch.ones(nB, nA, nH, nW) * self.noobject_scale\n        coord_mask = torch.zeros(nB, nA, nH, nW)\n        cls_mask = torch.zeros(nB, nA, nH, nW)\n        tcoord = torch.zeros(4, nB, nA, nH, nW)\n        tconf = torch.zeros(nB, nA, nH, nW)\n        tcls = torch.zeros(nB, nA, nH, nW)\n\n        nAnchors = nA * nH * nW\n        nPixels = nH * nW\n        nGT = 0  # number of ground truth\n        nRecall = 0\n        # it works faster on CPU than on GPU.\n        anchors = self.anchors.to(""cpu"")\n\n        if self.seen < 12800:\n            tcoord[0].fill_(0.5)\n            tcoord[1].fill_(0.5)\n            coord_mask.fill_(1)\n\n        for b in range(nB):\n            cur_pred_boxes = pred_boxes[b * nAnchors:(b + 1) * nAnchors].t()\n            cur_ious = torch.zeros(nAnchors)\n            tbox = target[b].view(-1, 5).to(""cpu"")\n            for t in range(50):\n                if tbox[t][1] == 0:\n                    break\n                gx, gw = [i * nW for i in (tbox[t][1], tbox[t][3])]\n                gy, gh = [i * nH for i in (tbox[t][2], tbox[t][4])]\n                cur_gt_boxes = torch.FloatTensor([gx, gy, gw, gh]).repeat(nAnchors, 1).t()\n                cur_ious = torch.max(cur_ious, multi_bbox_ious(cur_pred_boxes, cur_gt_boxes, x1y1x2y2=False))\n            ignore_ix = cur_ious > self.thresh\n            conf_mask[b][ignore_ix.view(nA, nH, nW)] = 0\n\n            for t in range(50):\n                if tbox[t][1] == 0:\n                    break\n                nGT += 1\n                gx, gw = [i * nW for i in (tbox[t][1], tbox[t][3])]\n                gy, gh = [i * nH for i in (tbox[t][2], tbox[t][4])]\n                gw, gh = gw.float(), gh.float()\n                gi, gj = int(gx), int(gy)\n\n                tmp_gt_boxes = torch.FloatTensor([0, 0, gw, gh]).repeat(nA, 1).t()\n                anchor_boxes = torch.cat((torch.zeros(nA, 2), anchors), 1).t()\n                tmp_ious = multi_bbox_ious(tmp_gt_boxes, anchor_boxes, x1y1x2y2=False)\n                best_iou, best_n = torch.max(tmp_ious, 0)\n\n                if self.anchor_step == 4:  # this part is not tested.\n                    tmp_ious_mask = (tmp_ious == best_iou)\n                    if tmp_ious_mask.sum() > 0:\n                        gt_pos = torch.FloatTensor([gi, gj, gx, gy]).repeat(nA, 1).t()\n                        an_pos = anchor_boxes[4:6]  # anchor_boxes are consisted of [0 0 aw ah ax ay]\n                        dist = pow(((gt_pos[0] + an_pos[0]) - gt_pos[2]), 2) + pow(\n                            ((gt_pos[1] + an_pos[1]) - gt_pos[3]), 2)\n                        dist[1 - tmp_ious_mask] = 10000  # set the large number for the small ious\n                        _, best_n = torch.min(dist, 0)\n\n                gt_box = torch.FloatTensor([gx, gy, gw, gh])\n                pred_box = pred_boxes[b * nAnchors + best_n * nPixels + gj * nW + gi]\n                iou = bbox_iou(gt_box, pred_box, x1y1x2y2=False)\n\n                coord_mask[b][best_n][gj][gi] = 1\n                cls_mask[b][best_n][gj][gi] = 1\n                conf_mask[b][best_n][gj][gi] = self.object_scale\n                tcoord[0][b][best_n][gj][gi] = gx - gi\n                tcoord[1][b][best_n][gj][gi] = gy - gj\n                tcoord[2][b][best_n][gj][gi] = math.log(gw / anchors[best_n][0])\n                tcoord[3][b][best_n][gj][gi] = math.log(gh / anchors[best_n][1])\n                tcls[b][best_n][gj][gi] = tbox[t][0]\n                tconf[b][best_n][gj][gi] = iou if self.rescore else 1.\n                if iou > 0.5:\n                    nRecall += 1\n\n        return nGT, nRecall, coord_mask, conf_mask, cls_mask, tcoord, tconf, tcls\n\n    def get_mask_boxes(self, output):\n        if not isinstance(self.anchors, torch.Tensor):\n            self.anchors = torch.FloatTensor(self.anchors).view(self.num_anchors, self.anchor_step).to(self.device)\n        masked_anchors = self.anchors.view(-1)\n        num_anchors = torch.IntTensor([self.num_anchors]).to(self.device)\n        return {\'x\': output, \'a\': masked_anchors, \'n\': num_anchors}\n\n    def forward(self, output, target):\n        # output : BxAs*(4+1+num_classes)*H*W\n        t0 = time.time()\n        nB = output.data.size(0)  # batch size\n        nA = self.num_anchors\n        nC = self.num_classes\n        nH = output.data.size(2)\n        nW = output.data.size(3)\n        cls_anchor_dim = nB * nA * nH * nW\n\n        if not isinstance(self.anchors, torch.Tensor):\n            self.anchors = torch.FloatTensor(self.anchors).view(self.num_anchors, self.anchor_step).to(self.device)\n\n        output = output.view(nB, nA, (5 + nC), nH, nW)\n        cls_grid = torch.linspace(5, 5 + nC - 1, nC).long().to(self.device)\n        ix = torch.LongTensor(range(0, 5)).to(self.device)\n        pred_boxes = torch.FloatTensor(4, cls_anchor_dim).to(self.device)\n\n        coord = output.index_select(2, ix[0:4]).view(nB * nA, -1, nH * nW).transpose(0, 1).contiguous().view(-1,\n                                                                                                             cls_anchor_dim)  # x, y, w, h\n        coord[0:2] = coord[0:2].sigmoid()  # x, y\n        conf = output.index_select(2, ix[4]).view(nB, nA, nH, nW).sigmoid()\n        cls = output.index_select(2, cls_grid)\n        cls = cls.view(nB * nA, nC, nH * nW).transpose(1, 2).contiguous().view(cls_anchor_dim, nC)\n\n        t1 = time.time()\n        grid_x = torch.linspace(0, nW - 1, nW).repeat(nB * nA, nH, 1).view(cls_anchor_dim).to(self.device)\n        grid_y = torch.linspace(0, nH - 1, nH).repeat(nW, 1).t().repeat(nB * nA, 1, 1).view(cls_anchor_dim).to(\n            self.device)\n        anchor_w = self.anchors.index_select(1, ix[0]).repeat(1, nB * nH * nW).view(cls_anchor_dim)\n        anchor_h = self.anchors.index_select(1, ix[1]).repeat(1, nB * nH * nW).view(cls_anchor_dim)\n\n        pred_boxes[0] = coord[0] + grid_x\n        pred_boxes[1] = coord[1] + grid_y\n        pred_boxes[2] = coord[2].exp() * anchor_w\n        pred_boxes[3] = coord[3].exp() * anchor_h\n        # for build_targets. it works faster on CPU than on GPU\n        pred_boxes = convert2cpu(pred_boxes.transpose(0, 1).contiguous().view(-1, 4)).detach()\n\n        t2 = time.time()\n        nGT, nRecall, coord_mask, conf_mask, cls_mask, tcoord, tconf, tcls = \\\n            self.build_targets(pred_boxes, target.detach(), nH, nW)\n\n        cls_mask = (cls_mask == 1)\n        tcls = tcls[cls_mask].long().view(-1)\n        cls_mask = cls_mask.view(-1, 1).repeat(1, nC).to(self.device)\n        cls = cls[cls_mask].view(-1, nC)\n\n        nProposals = int((conf > 0.25).sum())\n\n        tcoord = tcoord.view(4, cls_anchor_dim).to(self.device)\n        tconf, tcls = tconf.to(self.device), tcls.to(self.device)\n        coord_mask, conf_mask = coord_mask.view(cls_anchor_dim).to(self.device), conf_mask.sqrt().to(self.device)\n\n        t3 = time.time()\n        loss_coord = self.coord_scale * nn.MSELoss(size_average=False)(coord * coord_mask, tcoord * coord_mask) / 2\n        # sqrt(object_scale)/2 is almost equal to 1.\n        loss_conf = nn.MSELoss(size_average=False)(conf * conf_mask, tconf * conf_mask) / 2\n        loss_cls = self.class_scale * nn.CrossEntropyLoss(size_average=False)(cls, tcls) if cls.size(0) > 0 else 0\n        loss = loss_coord + loss_conf + loss_cls\n        t4 = time.time()\n        if False:\n            print(\'-\' * 30)\n            print(\'        activation : %f\' % (t1 - t0))\n            print(\' create pred_boxes : %f\' % (t2 - t1))\n            print(\'     build targets : %f\' % (t3 - t2))\n            print(\'       create loss : %f\' % (t4 - t3))\n            print(\'             total : %f\' % (t4 - t0))\n        print(\'%d: nGT %3d, nRC %3d, nPP %3d, loss: box %6.3f, conf %6.3f, class %6.3f, total %7.3f\'\n              % (self.seen, nGT, nRecall, nProposals, loss_coord, loss_conf, loss_cls, loss))\n        if math.isnan(loss.item()):\n            print(conf, tconf)\n            sys.exit(0)\n        return loss\n'"
detector/YOLOv3/yolo_layer.py,23,"b'import math\nimport sys\nimport time\nimport torch\nimport torch.nn as nn\nfrom .yolo_utils import bbox_iou, multi_bbox_ious, convert2cpu\n\n\nclass YoloLayer(nn.Module):\n    def __init__(self, anchor_mask=[], num_classes=0, anchors=[], num_anchors=1, use_cuda=None):\n        super(YoloLayer, self).__init__()\n        use_cuda = torch.cuda.is_available() and (True if use_cuda is None else use_cuda)\n        self.device = torch.device(""cuda"" if use_cuda else ""cpu"")\n\n        self.anchor_mask = anchor_mask\n        self.num_classes = num_classes\n        self.anchors = anchors\n        self.num_anchors = num_anchors\n        self.anchor_step = len(anchors) // num_anchors\n        self.rescore = 0\n        self.ignore_thresh = 0.5\n        self.truth_thresh = 1.\n        self.stride = 32\n        self.nth_layer = 0\n        self.seen = 0\n        self.net_width = 0\n        self.net_height = 0\n\n    def get_mask_boxes(self, output):\n        masked_anchors = []\n        for m in self.anchor_mask:\n            masked_anchors += self.anchors[m * self.anchor_step:(m + 1) * self.anchor_step]\n        masked_anchors = [anchor / self.stride for anchor in masked_anchors]\n\n        masked_anchors = torch.FloatTensor(masked_anchors).to(self.device)\n        num_anchors = torch.IntTensor([len(self.anchor_mask)]).to(self.device)\n        return {\'x\': output, \'a\': masked_anchors, \'n\': num_anchors}\n\n    def build_targets(self, pred_boxes, target, anchors, nA, nH, nW):\n        nB = target.size(0)\n        anchor_step = anchors.size(1)  # anchors[nA][anchor_step]\n        conf_mask = torch.ones(nB, nA, nH, nW)\n        coord_mask = torch.zeros(nB, nA, nH, nW)\n        cls_mask = torch.zeros(nB, nA, nH, nW)\n        tcoord = torch.zeros(4, nB, nA, nH, nW)\n        tconf = torch.zeros(nB, nA, nH, nW)\n        tcls = torch.zeros(nB, nA, nH, nW)\n        twidth, theight = self.net_width / self.stride, self.net_height / self.stride\n\n        nAnchors = nA * nH * nW\n        nPixels = nH * nW\n        nGT = 0\n        nRecall = 0\n        nRecall75 = 0\n\n        # it works faster on CPU than on GPU.\n        anchors = anchors.to(""cpu"")\n\n        for b in range(nB):\n            cur_pred_boxes = pred_boxes[b * nAnchors:(b + 1) * nAnchors].t()\n            cur_ious = torch.zeros(nAnchors)\n            tbox = target[b].view(-1, 5).to(""cpu"")\n            for t in range(50):\n                if tbox[t][1] == 0:\n                    break\n                gx, gy = tbox[t][1] * nW, tbox[t][2] * nH\n                gw, gh = tbox[t][3] * twidth, tbox[t][4] * theight\n                cur_gt_boxes = torch.FloatTensor([gx, gy, gw, gh]).repeat(nAnchors, 1).t()\n                cur_ious = torch.max(cur_ious, multi_bbox_ious(cur_pred_boxes, cur_gt_boxes, x1y1x2y2=False))\n            ignore_ix = cur_ious > self.ignore_thresh\n            conf_mask[b][ignore_ix.view(nA, nH, nW)] = 0\n\n            for t in range(50):\n                if tbox[t][1] == 0:\n                    break\n                nGT += 1\n                gx, gy = tbox[t][1] * nW, tbox[t][2] * nH\n                gw, gh = tbox[t][3] * twidth, tbox[t][4] * theight\n                gw, gh = gw.float(), gh.float()\n                gi, gj = int(gx), int(gy)\n\n                tmp_gt_boxes = torch.FloatTensor([0, 0, gw, gh]).repeat(nA, 1).t()\n                anchor_boxes = torch.cat((torch.zeros(nA, anchor_step), anchors), 1).t()\n                _, best_n = torch.max(multi_bbox_ious(tmp_gt_boxes, anchor_boxes, x1y1x2y2=False), 0)\n\n                gt_box = torch.FloatTensor([gx, gy, gw, gh])\n                pred_box = pred_boxes[b * nAnchors + best_n * nPixels + gj * nW + gi]\n                iou = bbox_iou(gt_box, pred_box, x1y1x2y2=False)\n\n                coord_mask[b][best_n][gj][gi] = 1\n                cls_mask[b][best_n][gj][gi] = 1\n                conf_mask[b][best_n][gj][gi] = 1\n                tcoord[0][b][best_n][gj][gi] = gx - gi\n                tcoord[1][b][best_n][gj][gi] = gy - gj\n                tcoord[2][b][best_n][gj][gi] = math.log(gw / anchors[best_n][0])\n                tcoord[3][b][best_n][gj][gi] = math.log(gh / anchors[best_n][1])\n                tcls[b][best_n][gj][gi] = tbox[t][0]\n                tconf[b][best_n][gj][gi] = iou if self.rescore else 1.\n\n                if iou > 0.5:\n                    nRecall += 1\n                    if iou > 0.75:\n                        nRecall75 += 1\n\n        return nGT, nRecall, nRecall75, coord_mask, conf_mask, cls_mask, tcoord, tconf, tcls\n\n    def forward(self, output, target):\n        # output : BxAs*(4+1+num_classes)*H*W\n        mask_tuple = self.get_mask_boxes(output)\n        t0 = time.time()\n        nB = output.data.size(0)  # batch size\n        nA = mask_tuple[\'n\'].item()  # num_anchors\n        nC = self.num_classes\n        nH = output.data.size(2)\n        nW = output.data.size(3)\n        anchor_step = mask_tuple[\'a\'].size(0) // nA\n        anchors = mask_tuple[\'a\'].view(nA, anchor_step).to(self.device)\n        cls_anchor_dim = nB * nA * nH * nW\n\n        output = output.view(nB, nA, (5 + nC), nH, nW)\n        cls_grid = torch.linspace(5, 5 + nC - 1, nC).long().to(self.device)\n        ix = torch.LongTensor(range(0, 5)).to(self.device)\n        pred_boxes = torch.FloatTensor(4, cls_anchor_dim).to(self.device)\n\n        coord = output.index_select(2, ix[0:4]).view(nB * nA, -1, nH * nW).transpose(0, 1).contiguous().view(-1,\n                                                                                                             cls_anchor_dim)  # x, y, w, h\n        coord[0:2] = coord[0:2].sigmoid()  # x, y\n        conf = output.index_select(2, ix[4]).view(nB, nA, nH, nW).sigmoid()\n        cls = output.index_select(2, cls_grid)\n        cls = cls.view(nB * nA, nC, nH * nW).transpose(1, 2).contiguous().view(cls_anchor_dim, nC)\n\n        t1 = time.time()\n        grid_x = torch.linspace(0, nW - 1, nW).repeat(nB * nA, nH, 1).view(cls_anchor_dim).to(self.device)\n        grid_y = torch.linspace(0, nH - 1, nH).repeat(nW, 1).t().repeat(nB * nA, 1, 1).view(cls_anchor_dim).to(\n            self.device)\n        anchor_w = anchors.index_select(1, ix[0]).repeat(1, nB * nH * nW).view(cls_anchor_dim)\n        anchor_h = anchors.index_select(1, ix[1]).repeat(1, nB * nH * nW).view(cls_anchor_dim)\n\n        pred_boxes[0] = coord[0] + grid_x\n        pred_boxes[1] = coord[1] + grid_y\n        pred_boxes[2] = coord[2].exp() * anchor_w\n        pred_boxes[3] = coord[3].exp() * anchor_h\n        # for build_targets. it works faster on CPU than on GPU\n        pred_boxes = convert2cpu(pred_boxes.transpose(0, 1).contiguous().view(-1, 4)).detach()\n\n        t2 = time.time()\n        nGT, nRecall, nRecall75, coord_mask, conf_mask, cls_mask, tcoord, tconf, tcls = \\\n            self.build_targets(pred_boxes, target.detach(), anchors.detach(), nA, nH, nW)\n\n        cls_mask = (cls_mask == 1)\n        tcls = tcls[cls_mask].long().view(-1)\n        cls_mask = cls_mask.view(-1, 1).repeat(1, nC).to(self.device)\n        cls = cls[cls_mask].view(-1, nC)\n\n        nProposals = int((conf > 0.25).sum())\n\n        tcoord = tcoord.view(4, cls_anchor_dim).to(self.device)\n        tconf, tcls = tconf.to(self.device), tcls.to(self.device)\n        coord_mask, conf_mask = coord_mask.view(cls_anchor_dim).to(self.device), conf_mask.to(self.device)\n\n        t3 = time.time()\n        loss_coord = nn.MSELoss(size_average=False)(coord * coord_mask, tcoord * coord_mask) / 2\n        loss_conf = nn.MSELoss(size_average=False)(conf * conf_mask, tconf * conf_mask)\n        loss_cls = nn.CrossEntropyLoss(size_average=False)(cls, tcls) if cls.size(0) > 0 else 0\n        loss = loss_coord + loss_conf + loss_cls\n\n        t4 = time.time()\n        if False:\n            print(\'-\' * 30)\n            print(\'        activation : %f\' % (t1 - t0))\n            print(\' create pred_boxes : %f\' % (t2 - t1))\n            print(\'     build targets : %f\' % (t3 - t2))\n            print(\'       create loss : %f\' % (t4 - t3))\n            print(\'             total : %f\' % (t4 - t0))\n        print(\n            \'%d: Layer(%03d) nGT %3d, nRC %3d, nRC75 %3d, nPP %3d, loss: box %6.3f, conf %6.3f, class %6.3f, total %7.3f\'\n            % (self.seen, self.nth_layer, nGT, nRecall, nRecall75, nProposals, loss_coord, loss_conf, loss_cls, loss))\n        if math.isnan(loss.item()):\n            print(conf, tconf)\n            sys.exit(0)\n        return loss\n'"
detector/YOLOv3/yolo_utils.py,42,"b'import os\nimport time\nimport math\nimport torch\nimport numpy as np\nfrom PIL import Image, ImageDraw\nimport struct  # get_image_size\nimport imghdr  # get_image_size\n\n\ndef sigmoid(x):\n    return 1.0 / (math.exp(-x) + 1.)\n\n\ndef softmax(x):\n    x = torch.exp(x - torch.max(x))\n    x /= x.sum()\n    return x\n\n\ndef bbox_iou(box1, box2, x1y1x2y2=True):\n    if x1y1x2y2:\n        x1_min = min(box1[0], box2[0])\n        x2_max = max(box1[2], box2[2])\n        y1_min = min(box1[1], box2[1])\n        y2_max = max(box1[3], box2[3])\n        w1, h1 = box1[2] - box1[0], box1[3] - box1[1]\n        w2, h2 = box2[2] - box2[0], box2[3] - box2[1]\n    else:\n        w1, h1 = box1[2], box1[3]\n        w2, h2 = box2[2], box2[3]\n        x1_min = min(box1[0] - w1 / 2.0, box2[0] - w2 / 2.0)\n        x2_max = max(box1[0] + w1 / 2.0, box2[0] + w2 / 2.0)\n        y1_min = min(box1[1] - h1 / 2.0, box2[1] - h2 / 2.0)\n        y2_max = max(box1[1] + h1 / 2.0, box2[1] + h2 / 2.0)\n\n    w_union = x2_max - x1_min\n    h_union = y2_max - y1_min\n    w_cross = w1 + w2 - w_union\n    h_cross = h1 + h2 - h_union\n    carea = 0\n    if w_cross <= 0 or h_cross <= 0:\n        return 0.0\n\n    area1 = w1 * h1\n    area2 = w2 * h2\n    carea = w_cross * h_cross\n    uarea = area1 + area2 - carea\n    return float(carea / uarea)\n\n\ndef multi_bbox_ious(boxes1, boxes2, x1y1x2y2=True):\n    if x1y1x2y2:\n        x1_min = torch.min(boxes1[0], boxes2[0])\n        x2_max = torch.max(boxes1[2], boxes2[2])\n        y1_min = torch.min(boxes1[1], boxes2[1])\n        y2_max = torch.max(boxes1[3], boxes2[3])\n        w1, h1 = boxes1[2] - boxes1[0], boxes1[3] - boxes1[1]\n        w2, h2 = boxes2[2] - boxes2[0], boxes2[3] - boxes2[1]\n    else:\n        w1, h1 = boxes1[2], boxes1[3]\n        w2, h2 = boxes2[2], boxes2[3]\n        x1_min = torch.min(boxes1[0] - w1 / 2.0, boxes2[0] - w2 / 2.0)\n        x2_max = torch.max(boxes1[0] + w1 / 2.0, boxes2[0] + w2 / 2.0)\n        y1_min = torch.min(boxes1[1] - h1 / 2.0, boxes2[1] - h2 / 2.0)\n        y2_max = torch.max(boxes1[1] + h1 / 2.0, boxes2[1] + h2 / 2.0)\n\n    w_union = x2_max - x1_min\n    h_union = y2_max - y1_min\n    w_cross = w1 + w2 - w_union\n    h_cross = h1 + h2 - h_union\n    mask = (((w_cross <= 0) + (h_cross <= 0)) > 0)\n    area1 = w1 * h1\n    area2 = w2 * h2\n    carea = w_cross * h_cross\n    carea[mask] = 0\n    uarea = area1 + area2 - carea\n    return carea / uarea\n\n\nfrom .nms import boxes_nms\n\n\ndef post_process(boxes, num_classes, conf_thresh=0.01, nms_thresh=0.45, obj_thresh=0.3):\n    batch_size = boxes.size(0)\n\n    # nms\n    results_boxes = []\n    for batch_id in range(batch_size):\n        processed_boxes = []\n        for cls_id in range(num_classes):\n            mask = (boxes[batch_id, :, -1] == cls_id) * (boxes[batch_id, :, 4] > obj_thresh)\n            masked_boxes = boxes[batch_id, mask]\n\n            keep = boxes_nms(masked_boxes[:, :4], masked_boxes[:, 5], nms_thresh)\n\n            nmsed_boxes = masked_boxes[keep, :]\n\n            processed_boxes.append(nmsed_boxes)\n        processed_boxes = torch.cat(processed_boxes, dim=0)\n\n    results_boxes.append(processed_boxes)\n\n    return results_boxes\n\n\ndef xywh_to_xyxy(boxes_xywh):\n    boxes_xyxy = boxes_xywh.copy()\n    boxes_xyxy[:, 0] = boxes_xywh[:, 0] - boxes_xywh[:, 2] / 2.\n    boxes_xyxy[:, 0] = boxes_xywh[:, 0] - boxes_xywh[:, 2] / 2.\n    boxes_xyxy[:, 0] = boxes_xywh[:, 0] - boxes_xywh[:, 2] / 2.\n    boxes_xyxy[:, 0] = boxes_xywh[:, 0] - boxes_xywh[:, 2] / 2.\n\n    return boxes_xyxy\n\n\ndef xyxy_to_xywh(boxes_xyxy):\n    if isinstance(boxes_xyxy, torch.Tensor):\n        boxes_xywh = boxes_xyxy.clone()\n    elif isinstance(boxes_xyxy, np.ndarray):\n        boxes_xywh = boxes_xyxy.copy()\n\n    boxes_xywh[:, 0] = (boxes_xyxy[:, 0] + boxes_xyxy[:, 2]) / 2.\n    boxes_xywh[:, 1] = (boxes_xyxy[:, 1] + boxes_xyxy[:, 3]) / 2.\n    boxes_xywh[:, 2] = boxes_xyxy[:, 2] - boxes_xyxy[:, 0]\n    boxes_xywh[:, 3] = boxes_xyxy[:, 3] - boxes_xyxy[:, 1]\n\n    return boxes_xywh\n\n\ndef nms(boxes, nms_thresh):\n    if len(boxes) == 0:\n        return boxes\n\n    det_confs = torch.zeros(len(boxes))\n    for i in range(len(boxes)):\n        det_confs[i] = boxes[i][4]\n\n    _, sortIds = torch.sort(det_confs, descending=True)\n    out_boxes = []\n    for i in range(len(boxes)):\n        box_i = boxes[sortIds[i]]\n        if box_i[4] > 0:\n            out_boxes.append(box_i)\n            for j in range(i + 1, len(boxes)):\n                box_j = boxes[sortIds[j]]\n                if bbox_iou(box_i, box_j, x1y1x2y2=False) > nms_thresh:\n                    # print(box_i, box_j, bbox_iou(box_i, box_j, x1y1x2y2=False))\n                    box_j[4] = 0\n    return out_boxes\n\n\ndef convert2cpu(gpu_matrix):\n    return torch.FloatTensor(gpu_matrix.size()).copy_(gpu_matrix)\n\n\ndef convert2cpu_long(gpu_matrix):\n    return torch.LongTensor(gpu_matrix.size()).copy_(gpu_matrix)\n\n\ndef get_all_boxes(output, conf_thresh, num_classes, only_objectness=1, validation=False, use_cuda=True):\n    # total number of inputs (batch size)\n    # first element (x) for first tuple (x, anchor_mask, num_anchor)\n    batchsize = output[0][\'x\'].data.size(0)\n\n    all_boxes = []\n    for i in range(len(output)):\n        pred, anchors, num_anchors = output[i][\'x\'].data, output[i][\'a\'], output[i][\'n\'].item()\n        boxes = get_region_boxes(pred, conf_thresh, num_classes, anchors, num_anchors, \\\n                                 only_objectness=only_objectness, validation=validation, use_cuda=use_cuda)\n\n        all_boxes.append(boxes)\n    return torch.cat(all_boxes, dim=1)\n\n\ndef get_region_boxes(output, obj_thresh, num_classes, anchors, num_anchors, only_objectness=1, validation=False,\n                     use_cuda=True):\n    device = torch.device(""cuda"" if use_cuda else ""cpu"")\n    anchors = anchors.to(device)\n    anchor_step = anchors.size(0) // num_anchors\n    if output.dim() == 3:\n        output = output.unsqueeze(0)\n    batch = output.size(0)\n    assert (output.size(1) == (5 + num_classes) * num_anchors)\n    h = output.size(2)\n    w = output.size(3)\n    cls_anchor_dim = batch * num_anchors * h * w\n\n    # all_boxes = []\n    output = output.view(batch * num_anchors, 5 + num_classes, h * w).transpose(0, 1).contiguous().view(5 + num_classes,\n                                                                                                        cls_anchor_dim)\n\n    grid_x = torch.linspace(0, w - 1, w).repeat(batch * num_anchors, h, 1).view(cls_anchor_dim).to(device)\n    grid_y = torch.linspace(0, h - 1, h).repeat(w, 1).t().repeat(batch * num_anchors, 1, 1).view(cls_anchor_dim).to(\n        device)\n    ix = torch.LongTensor(range(0, 2)).to(device)\n    anchor_w = anchors.view(num_anchors, anchor_step).index_select(1, ix[0]).repeat(1, batch, h * w).view(\n        cls_anchor_dim)\n    anchor_h = anchors.view(num_anchors, anchor_step).index_select(1, ix[1]).repeat(1, batch, h * w).view(\n        cls_anchor_dim)\n\n    xs, ys = torch.sigmoid(output[0]) + grid_x, torch.sigmoid(output[1]) + grid_y\n    ws, hs = torch.exp(output[2]) * anchor_w.detach(), torch.exp(output[3]) * anchor_h.detach()\n    det_confs = torch.sigmoid(output[4])\n\n    # by ysyun, dim=1 means input is 2D or even dimension else dim=0\n    cls_confs = torch.nn.Softmax(dim=1)(output[5:5 + num_classes].transpose(0, 1)).detach()\n    cls_max_confs, cls_max_ids = torch.max(cls_confs, 1)\n    cls_max_confs = cls_max_confs.view(-1)\n    cls_max_ids = cls_max_ids.view(-1).float()\n\n    # sz_hw = h*w\n    # sz_hwa = sz_hw*num_anchors\n    # det_confs = convert2cpu(det_confs)\n    # cls_max_confs = convert2cpu(cls_max_confs)\n    # cls_max_ids = convert2cpu_long(cls_max_ids)\n    # xs, ys = convert2cpu(xs), convert2cpu(ys)\n    # ws, hs = convert2cpu(ws), convert2cpu(hs)\n\n    cls_confs = det_confs * cls_max_confs\n\n    # boxes = [xs/w, ys/h, ws/w, hs/h, det_confs, cls_confs, cls_max_ids]\n    xs, ys, ws, hs = xs / w, ys / h, ws / w, hs / h\n    x1, y1, x2, y2 = torch.clamp_min(xs - ws / 2., 0.), torch.clamp_min(ys - hs / 2., 0.), torch.clamp_max(xs + ws / 2.,\n                                                                                                           1.), torch.clamp_max(\n        ys + hs / 2., 1.)\n    boxes = [x1, y1, x2, y2, det_confs, cls_confs, cls_max_ids]\n    boxes = list(map(lambda x: x.view(batch, -1), boxes))\n    boxes = torch.stack(boxes, dim=2)\n\n    # for b in range(batch):\n    #     boxes = []\n    #     for cy in range(h):\n    #         for cx in range(w):\n    #             for i in range(num_anchors):\n    #                 ind = b*sz_hwa + i*sz_hw + cy*w + cx\n    #                 det_conf =  det_confs[ind]\n    #                 if only_objectness:\n    #                     conf = det_confs[ind]\n    #                 else:\n    #                     conf = det_confs[ind] * cls_max_confs[ind]\n\n    #                 if conf > conf_thresh:\n    #                     bcx = xs[ind]\n    #                     bcy = ys[ind]\n    #                     bw = ws[ind]\n    #                     bh = hs[ind]\n    #                     cls_max_conf = cls_max_confs[ind]\n    #                     cls_max_id = cls_max_ids[ind]\n    #                     box = [bcx/w, bcy/h, bw/w, bh/h, det_conf, cls_max_conf, cls_max_id]\n\n    #                     boxes.append(box)\n    #     all_boxes.append(boxes)\n    return boxes\n\n\n# def get_all_boxes(output, conf_thresh, num_classes, only_objectness=1, validation=False, use_cuda=True):\n#     # total number of inputs (batch size)\n#     # first element (x) for first tuple (x, anchor_mask, num_anchor)\n#     tot = output[0][\'x\'].data.size(0)\n#     all_boxes = [[] for i in range(tot)]\n#     for i in range(len(output)):\n#         pred, anchors, num_anchors = output[i][\'x\'].data, output[i][\'a\'], output[i][\'n\'].item()\n#         b = get_region_boxes(pred, conf_thresh, num_classes, anchors, num_anchors, \\\n#                 only_objectness=only_objectness, validation=validation, use_cuda=use_cuda)\n#         for t in range(tot):\n#             all_boxes[t] += b[t]\n#     return all_boxes\n\n# def get_region_boxes(output, conf_thresh, num_classes, anchors, num_anchors, only_objectness=1, validation=False, use_cuda=True):\n#     device = torch.device(""cuda"" if use_cuda else ""cpu"")\n#     anchors = anchors.to(device)\n#     anchor_step = anchors.size(0)//num_anchors\n#     if output.dim() == 3:\n#         output = output.unsqueeze(0)\n#     batch = output.size(0)\n#     assert(output.size(1) == (5+num_classes)*num_anchors)\n#     h = output.size(2)\n#     w = output.size(3)\n#     cls_anchor_dim = batch*num_anchors*h*w\n\n#     t0 = time.time()\n#     all_boxes = []\n#     output = output.view(batch*num_anchors, 5+num_classes, h*w).transpose(0,1).contiguous().view(5+num_classes, cls_anchor_dim)\n\n#     grid_x = torch.linspace(0, w-1, w).repeat(batch*num_anchors, h, 1).view(cls_anchor_dim).to(device)\n#     grid_y = torch.linspace(0, h-1, h).repeat(w,1).t().repeat(batch*num_anchors, 1, 1).view(cls_anchor_dim).to(device)\n#     ix = torch.LongTensor(range(0,2)).to(device)\n#     anchor_w = anchors.view(num_anchors, anchor_step).index_select(1, ix[0]).repeat(1, batch, h*w).view(cls_anchor_dim)\n#     anchor_h = anchors.view(num_anchors, anchor_step).index_select(1, ix[1]).repeat(1, batch, h*w).view(cls_anchor_dim)\n\n#     xs, ys = torch.sigmoid(output[0]) + grid_x, torch.sigmoid(output[1]) + grid_y\n#     ws, hs = torch.exp(output[2]) * anchor_w.detach(), torch.exp(output[3]) * anchor_h.detach()\n#     det_confs = torch.sigmoid(output[4])\n\n#     # by ysyun, dim=1 means input is 2D or even dimension else dim=0\n#     cls_confs = torch.nn.Softmax(dim=1)(output[5:5+num_classes].transpose(0,1)).detach()\n#     cls_max_confs, cls_max_ids = torch.max(cls_confs, 1)\n#     cls_max_confs = cls_max_confs.view(-1)\n#     cls_max_ids = cls_max_ids.view(-1)\n#     t1 = time.time()\n\n#     sz_hw = h*w\n#     sz_hwa = sz_hw*num_anchors\n#     det_confs = convert2cpu(det_confs)\n#     cls_max_confs = convert2cpu(cls_max_confs)\n#     cls_max_ids = convert2cpu_long(cls_max_ids)\n#     xs, ys = convert2cpu(xs), convert2cpu(ys)\n#     ws, hs = convert2cpu(ws), convert2cpu(hs)\n#     if validation:\n#         cls_confs = convert2cpu(cls_confs.view(-1, num_classes))\n\n#     t2 = time.time()\n#     for b in range(batch):\n#         boxes = []\n#         for cy in range(h):\n#             for cx in range(w):\n#                 for i in range(num_anchors):\n#                     ind = b*sz_hwa + i*sz_hw + cy*w + cx\n#                     det_conf =  det_confs[ind]\n#                     if only_objectness:\n#                         conf = det_confs[ind]\n#                     else:\n#                         conf = det_confs[ind] * cls_max_confs[ind]\n\n#                     if conf > conf_thresh:\n#                         bcx = xs[ind]\n#                         bcy = ys[ind]\n#                         bw = ws[ind]\n#                         bh = hs[ind]\n#                         cls_max_conf = cls_max_confs[ind]\n#                         cls_max_id = cls_max_ids[ind]\n#                         box = [bcx/w, bcy/h, bw/w, bh/h, det_conf, cls_max_conf, cls_max_id]\n#                         if (not only_objectness) and validation:\n#                             for c in range(num_classes):\n#                                 tmp_conf = cls_confs[ind][c]\n#                                 if c != cls_max_id and det_confs[ind]*tmp_conf > conf_thresh:\n#                                     box.append(tmp_conf)\n#                                     box.append(c)\n#                         boxes.append(box)\n#         all_boxes.append(boxes)\n#     t3 = time.time()\n#     if False:\n#         print(\'---------------------------------\')\n#         print(\'matrix computation : %f\' % (t1-t0))\n#         print(\'        gpu to cpu : %f\' % (t2-t1))\n#         print(\'      boxes filter : %f\' % (t3-t2))\n#         print(\'---------------------------------\')\n#     return all_boxes\n\ndef plot_boxes_cv2(img, boxes, savename=None, class_names=None, color=None):\n    import cv2\n    colors = torch.FloatTensor([[1, 0, 1], [0, 0, 1], [0, 1, 1], [0, 1, 0], [1, 1, 0], [1, 0, 0]])\n\n    def get_color(c, x, max_val):\n        ratio = float(x) / max_val * 5\n        i = int(math.floor(ratio))\n        j = int(math.ceil(ratio))\n        ratio -= i\n        r = (1 - ratio) * colors[i][c] + ratio * colors[j][c]\n        return int(r * 255)\n\n    width = img.shape[1]\n    height = img.shape[0]\n    for i in range(len(boxes)):\n        box = boxes[i]\n        x1 = int(round((box[0] - box[2] / 2.0) * width))\n        y1 = int(round((box[1] - box[3] / 2.0) * height))\n        x2 = int(round((box[0] + box[2] / 2.0) * width))\n        y2 = int(round((box[1] + box[3] / 2.0) * height))\n\n        if color:\n            rgb = color\n        else:\n            rgb = (255, 0, 0)\n        if len(box) >= 7 and class_names:\n            cls_conf = box[5]\n            cls_id = box[6]\n            # print(\'%s: %f\' % (class_names[cls_id], cls_conf))\n            classes = len(class_names)\n            offset = cls_id * 123457 % classes\n            red = get_color(2, offset, classes)\n            green = get_color(1, offset, classes)\n            blue = get_color(0, offset, classes)\n            if color is None:\n                rgb = (red, green, blue)\n            img = cv2.putText(img, class_names[cls_id], (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 1.2, rgb, 1)\n        img = cv2.rectangle(img, (x1, y1), (x2, y2), rgb, 1)\n    if savename:\n        print(""save plot results to %s"" % savename)\n        cv2.imwrite(savename, img)\n    return img\n\n\ndef plot_boxes(img, boxes, savename=None, class_names=None):\n    colors = torch.FloatTensor([[1, 0, 1], [0, 0, 1], [0, 1, 1], [0, 1, 0], [1, 1, 0], [1, 0, 0]])\n\n    def get_color(c, x, max_val):\n        ratio = float(x) / max_val * 5\n        i = int(math.floor(ratio))\n        j = int(math.ceil(ratio))\n        ratio -= i\n        r = (1 - ratio) * colors[i][c] + ratio * colors[j][c]\n        return int(r * 255)\n\n    width = img.width\n    height = img.height\n    draw = ImageDraw.Draw(img)\n    print(""%d box(es) is(are) found"" % len(boxes))\n    for i in range(len(boxes)):\n        box = boxes[i]\n        x1 = (box[0] - box[2] / 2.0) * width\n        y1 = (box[1] - box[3] / 2.0) * height\n        x2 = (box[0] + box[2] / 2.0) * width\n        y2 = (box[1] + box[3] / 2.0) * height\n\n        rgb = (255, 0, 0)\n        if len(box) >= 7 and class_names:\n            cls_conf = box[5]\n            cls_id = box[6]\n            print(\'%s: %f\' % (class_names[cls_id], cls_conf))\n            classes = len(class_names)\n            offset = cls_id * 123457 % classes\n            red = get_color(2, offset, classes)\n            green = get_color(1, offset, classes)\n            blue = get_color(0, offset, classes)\n            rgb = (red, green, blue)\n            draw.text((x1, y1), class_names[cls_id], fill=rgb)\n        draw.rectangle([x1, y1, x2, y2], outline=rgb)\n    if savename:\n        print(""save plot results to %s"" % savename)\n        img.save(savename)\n    return img\n\n\ndef read_truths(lab_path):\n    if not os.path.exists(lab_path):\n        return np.array([])\n    if os.path.getsize(lab_path):\n        truths = np.loadtxt(lab_path)\n        truths = truths.reshape(truths.size // 5, 5)  # to avoid single truth problem\n        return truths\n    else:\n        return np.array([])\n\n\ndef read_truths_args(lab_path, min_box_scale):\n    truths = read_truths(lab_path)\n    new_truths = []\n    for i in range(truths.shape[0]):\n        if truths[i][3] < min_box_scale:\n            continue\n        new_truths.append([truths[i][0], truths[i][1], truths[i][2], truths[i][3], truths[i][4]])\n    return np.array(new_truths)\n\n\ndef load_class_names(namesfile):\n    class_names = []\n    with open(namesfile, \'r\', encoding=\'utf8\') as fp:\n        lines = fp.readlines()\n    for line in lines:\n        class_names.append(line.strip())\n    return class_names\n\n\ndef image2torch(img):\n    if isinstance(img, Image.Image):\n        width = img.width\n        height = img.height\n        img = torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes()))\n        img = img.view(height, width, 3).transpose(0, 1).transpose(0, 2).contiguous()\n        img = img.view(1, 3, height, width)\n        img = img.float().div(255.0)\n    elif type(img) == np.ndarray:  # cv2 image\n        img = torch.from_numpy(img.transpose(2, 0, 1)).float().div(255.0).unsqueeze(0)\n    else:\n        print(""unknown image type"")\n        exit(-1)\n    return img\n\n\ndef do_detect(model, img, conf_thresh, nms_thresh, use_cuda=True):\n    model.eval()\n    t0 = time.time()\n    img = image2torch(img)\n    t1 = time.time()\n\n    img = img.to(torch.device(""cuda"" if use_cuda else ""cpu""))\n    t2 = time.time()\n\n    out_boxes = model(img)\n    boxes = get_all_boxes(out_boxes, conf_thresh, model.num_classes, use_cuda=use_cuda)[0]\n\n    t3 = time.time()\n    boxes = nms(boxes, nms_thresh)\n    t4 = time.time()\n\n    if False:\n        print(\'-----------------------------------\')\n        print(\' image to tensor : %f\' % (t1 - t0))\n        print(\'  tensor to cuda : %f\' % (t2 - t1))\n        print(\'         predict : %f\' % (t3 - t2))\n        print(\'             nms : %f\' % (t4 - t3))\n        print(\'           total : %f\' % (t4 - t0))\n        print(\'-----------------------------------\')\n    return boxes\n\n\ndef read_data_cfg(datacfg):\n    options = dict()\n    options[\'gpus\'] = \'0,1,2,3\'\n    options[\'num_workers\'] = \'10\'\n    with open(datacfg) as fp:\n        lines = fp.readlines()\n\n    for line in lines:\n        line = line.strip()\n        if line == \'\':\n            continue\n        key, value = line.split(\'=\')\n        key = key.strip()\n        value = value.strip()\n        options[key] = value\n    return options\n\n\ndef scale_bboxes(bboxes, width, height):\n    import copy\n    dets = copy.deepcopy(bboxes)\n    for i in range(len(dets)):\n        dets[i][0] = dets[i][0] * width\n        dets[i][1] = dets[i][1] * height\n        dets[i][2] = dets[i][2] * width\n        dets[i][3] = dets[i][3] * height\n    return dets\n\n\ndef file_lines(thefilepath):\n    count = 0\n    thefile = open(thefilepath, \'rb\')\n    while True:\n        buffer = thefile.read(8192 * 1024)\n        if not buffer:\n            break\n        count += buffer.count(b\'\\n\')\n    thefile.close()\n    return count\n\n\ndef get_image_size(fname):\n    """"""\n    Determine the image type of fhandle and return its size.\n    from draco\n    """"""\n    with open(fname, \'rb\') as fhandle:\n        head = fhandle.read(24)\n        if len(head) != 24:\n            return\n        if imghdr.what(fname) == \'png\':\n            check = struct.unpack(\'>i\', head[4:8])[0]\n            if check != 0x0d0a1a0a:\n                return\n            width, height = struct.unpack(\'>ii\', head[16:24])\n        elif imghdr.what(fname) == \'gif\':\n            width, height = struct.unpack(\'<HH\', head[6:10])\n        elif imghdr.what(fname) == \'jpeg\' or imghdr.what(fname) == \'jpg\':\n            try:\n                fhandle.seek(0)  # Read 0xff next\n                size = 2\n                ftype = 0\n                while not 0xc0 <= ftype <= 0xcf:\n                    fhandle.seek(size, 1)\n                    byte = fhandle.read(1)\n                    while ord(byte) == 0xff:\n                        byte = fhandle.read(1)\n                    ftype = ord(byte)\n                    size = struct.unpack(\'>H\', fhandle.read(2))[0] - 2\n                    # We are at a SOFn block\n                fhandle.seek(1, 1)  # Skip `precision\' byte.\n                height, width = struct.unpack(\'>HH\', fhandle.read(4))\n            except Exception:  # IGNORE:W0703\n                return\n        else:\n            return\n        return width, height\n\n\ndef logging(message):\n    print(\'%s %s\' % (time.strftime(""%Y-%m-%d %H:%M:%S"", time.localtime()), message))\n'"
webserver/config/config.py,0,"b""import os\n\napp_dir = os.path.abspath(os.path.dirname(__file__))\n\n\nclass BaseConfig:\n    SECRET_KEY = os.environ.get('SECRET_KEY') or 'Sm9obiBTY2hyb20ga2lja3MgYXNz'\n    SERVER_NAME = '127.0.0.1:8888'\n\n\nclass DevelopmentConfig(BaseConfig):\n    ENV = 'development'\n    DEBUG = True\n\n\nclass TestingConfig(BaseConfig):\n    DEBUG = True\n\n\nclass ProductionConfig(BaseConfig):\n    DEBUG = False\n"""
detector/YOLOv3/nms/__init__.py,0,b'from .nms import boxes_nms'
detector/YOLOv3/nms/nms.py,0,"b'import warnings\nimport torchvision\n\ntry:\n    import torch\n    import torch_extension\n\n    _nms = torch_extension.nms\nexcept ImportError:\n    if torchvision.__version__ >= \'0.3.0\':\n        _nms = torchvision.ops.nms\n    else:\n        from .python_nms import python_nms\n\n        _nms = python_nms\n        warnings.warn(\'You are using python version NMS, which is very very slow. Try compile c++ NMS \'\n                      \'using `cd ext & python build.py build_ext develop`\')\n\n\ndef boxes_nms(boxes, scores, nms_thresh, max_count=-1):\n    """""" Performs non-maximum suppression, run on GPU or CPU according to\n    boxes\'s device.\n    Args:\n        boxes(Tensor): `xyxy` mode boxes, use absolute coordinates(or relative coordinates), shape is (n, 4)\n        scores(Tensor): scores, shape is (n, )\n        nms_thresh(float): thresh\n        max_count (int): if > 0, then only the top max_proposals are kept  after non-maximum suppression\n    Returns:\n        indices kept.\n    """"""\n    keep = _nms(boxes, scores, nms_thresh)\n    if max_count > 0:\n        keep = keep[:max_count]\n    return keep\n'"
detector/YOLOv3/nms/python_nms.py,3,"b'import torch\nimport numpy as np\n\n\ndef python_nms(boxes, scores, nms_thresh):\n    """""" Performs non-maximum suppression using numpy\n        Args:\n            boxes(Tensor): `xyxy` mode boxes, use absolute coordinates(not support relative coordinates),\n                shape is (n, 4)\n            scores(Tensor): scores, shape is (n, )\n            nms_thresh(float): thresh\n        Returns:\n            indices kept.\n    """"""\n    if boxes.numel() == 0:\n        return torch.empty((0,), dtype=torch.long)\n    # Use numpy to run nms. Running nms in PyTorch code on CPU is really slow.\n    origin_device = boxes.device\n    cpu_device = torch.device(\'cpu\')\n    boxes = boxes.to(cpu_device).numpy()\n    scores = scores.to(cpu_device).numpy()\n\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    areas = (x2 - x1) * (y2 - y1)\n    order = np.argsort(scores)[::-1]\n    num_detections = boxes.shape[0]\n    suppressed = np.zeros((num_detections,), dtype=np.bool)\n    for _i in range(num_detections):\n        i = order[_i]\n        if suppressed[i]:\n            continue\n        ix1 = x1[i]\n        iy1 = y1[i]\n        ix2 = x2[i]\n        iy2 = y2[i]\n        iarea = areas[i]\n\n        for _j in range(_i + 1, num_detections):\n            j = order[_j]\n            if suppressed[j]:\n                continue\n\n            xx1 = max(ix1, x1[j])\n            yy1 = max(iy1, y1[j])\n            xx2 = min(ix2, x2[j])\n            yy2 = min(iy2, y2[j])\n            w = max(0, xx2 - xx1)\n            h = max(0, yy2 - yy1)\n\n            inter = w * h\n            ovr = inter / (iarea + areas[j] - inter)\n            if ovr >= nms_thresh:\n                suppressed[j] = True\n    keep = np.nonzero(suppressed == 0)[0]\n    keep = torch.from_numpy(keep).to(origin_device)\n    return keep\n'"
detector/YOLOv3/nms/ext/__init__.py,0,b''
detector/YOLOv3/nms/ext/build.py,5,"b'import glob\nimport os\n\nimport torch\nfrom setuptools import setup\nfrom torch.utils.cpp_extension import CUDA_HOME\nfrom torch.utils.cpp_extension import CppExtension\nfrom torch.utils.cpp_extension import CUDAExtension\n\nrequirements = [""torch""]\n\n\ndef get_extensions():\n    extensions_dir = os.path.dirname(os.path.abspath(__file__))\n\n    main_file = glob.glob(os.path.join(extensions_dir, ""*.cpp""))\n    source_cpu = glob.glob(os.path.join(extensions_dir, ""cpu"", ""*.cpp""))\n    source_cuda = glob.glob(os.path.join(extensions_dir, ""cuda"", ""*.cu""))\n\n    sources = main_file + source_cpu\n    extension = CppExtension\n\n    extra_compile_args = {""cxx"": []}\n    define_macros = []\n\n    if torch.cuda.is_available() and CUDA_HOME is not None:\n        extension = CUDAExtension\n        sources += source_cuda\n        define_macros += [(""WITH_CUDA"", None)]\n        extra_compile_args[""nvcc""] = [\n            ""-DCUDA_HAS_FP16=1"",\n            ""-D__CUDA_NO_HALF_OPERATORS__"",\n            ""-D__CUDA_NO_HALF_CONVERSIONS__"",\n            ""-D__CUDA_NO_HALF2_OPERATORS__"",\n        ]\n\n    sources = [os.path.join(extensions_dir, s) for s in sources]\n\n    include_dirs = [extensions_dir]\n\n    ext_modules = [\n        extension(\n            ""torch_extension"",\n            sources,\n            include_dirs=include_dirs,\n            define_macros=define_macros,\n            extra_compile_args=extra_compile_args,\n        )\n    ]\n\n    return ext_modules\n\n\nsetup(\n    name=""torch_extension"",\n    version=""0.1"",\n    ext_modules=get_extensions(),\n    cmdclass={""build_ext"": torch.utils.cpp_extension.BuildExtension})\n'"
