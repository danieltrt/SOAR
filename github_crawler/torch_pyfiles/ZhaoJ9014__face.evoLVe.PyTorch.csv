file_path,api_count,code
config.py,1,"b'import torch\n\n\nconfigurations = {\n    1: dict(\n        SEED = 1337, # random seed for reproduce results\n\n        DATA_ROOT = \'/media/pc/6T/jasonjzhao/data/faces_emore\', # the parent root where your train/val/test data are stored\n        MODEL_ROOT = \'/media/pc/6T/jasonjzhao/buffer/model\', # the root to buffer your checkpoints\n        LOG_ROOT = \'/media/pc/6T/jasonjzhao/buffer/log\', # the root to log your train/val status\n        BACKBONE_RESUME_ROOT = \'./\', # the root to resume training from a saved checkpoint\n        HEAD_RESUME_ROOT = \'./\', # the root to resume training from a saved checkpoint\n\n        BACKBONE_NAME = \'IR_SE_50\', # support: [\'ResNet_50\', \'ResNet_101\', \'ResNet_152\', \'IR_50\', \'IR_101\', \'IR_152\', \'IR_SE_50\', \'IR_SE_101\', \'IR_SE_152\']\n        HEAD_NAME = \'ArcFace\', # support:  [\'Softmax\', \'ArcFace\', \'CosFace\', \'SphereFace\', \'Am_softmax\']\n        LOSS_NAME = \'Focal\', # support: [\'Focal\', \'Softmax\']\n\n        INPUT_SIZE = [112, 112], # support: [112, 112] and [224, 224]\n        RGB_MEAN = [0.5, 0.5, 0.5], # for normalize inputs to [-1, 1]\n        RGB_STD = [0.5, 0.5, 0.5],\n        EMBEDDING_SIZE = 512, # feature dimension\n        BATCH_SIZE = 512,\n        DROP_LAST = True, # whether drop the last batch to ensure consistent batch_norm statistics\n        LR = 0.1, # initial LR\n        NUM_EPOCH = 125, # total epoch number (use the firt 1/25 epochs to warm up)\n        WEIGHT_DECAY = 5e-4, # do not apply to batch_norm parameters\n        MOMENTUM = 0.9,\n        STAGES = [35, 65, 95], # epoch stages to decay learning rate\n\n        DEVICE = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu""),\n        MULTI_GPU = True, # flag to use multiple GPUs; if you choose to train with single GPU, you should first run ""export CUDA_VISILE_DEVICES=device_id"" to specify the GPU card you want to use\n        GPU_ID = [0, 1, 2, 3], # specify your GPU ids\n        PIN_MEMORY = True,\n        NUM_WORKERS = 0,\n),\n}\n'"
train.py,13,"b'import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n\nfrom config import configurations\nfrom backbone.model_resnet import ResNet_50, ResNet_101, ResNet_152\nfrom backbone.model_irse import IR_50, IR_101, IR_152, IR_SE_50, IR_SE_101, IR_SE_152\nfrom head.metrics import ArcFace, CosFace, SphereFace, Am_softmax\nfrom loss.focal import FocalLoss\nfrom util.utils import make_weights_for_balanced_classes, get_val_data, separate_irse_bn_paras, separate_resnet_bn_paras, warm_up_lr, schedule_lr, perform_val, get_time, buffer_val, AverageMeter, accuracy\n\nfrom tensorboardX import SummaryWriter\nfrom tqdm import tqdm\nimport os\n\n\nif __name__ == \'__main__\':\n\n    #======= hyperparameters & data loaders =======#\n    cfg = configurations[1]\n\n    SEED = cfg[\'SEED\'] # random seed for reproduce results\n    torch.manual_seed(SEED)\n\n    DATA_ROOT = cfg[\'DATA_ROOT\'] # the parent root where your train/val/test data are stored\n    MODEL_ROOT = cfg[\'MODEL_ROOT\'] # the root to buffer your checkpoints\n    LOG_ROOT = cfg[\'LOG_ROOT\'] # the root to log your train/val status\n    BACKBONE_RESUME_ROOT = cfg[\'BACKBONE_RESUME_ROOT\'] # the root to resume training from a saved checkpoint\n    HEAD_RESUME_ROOT = cfg[\'HEAD_RESUME_ROOT\']  # the root to resume training from a saved checkpoint\n\n    BACKBONE_NAME = cfg[\'BACKBONE_NAME\'] # support: [\'ResNet_50\', \'ResNet_101\', \'ResNet_152\', \'IR_50\', \'IR_101\', \'IR_152\', \'IR_SE_50\', \'IR_SE_101\', \'IR_SE_152\']\n    HEAD_NAME = cfg[\'HEAD_NAME\'] # support:  [\'Softmax\', \'ArcFace\', \'CosFace\', \'SphereFace\', \'Am_softmax\']\n    LOSS_NAME = cfg[\'LOSS_NAME\'] # support: [\'Focal\', \'Softmax\']\n\n    INPUT_SIZE = cfg[\'INPUT_SIZE\']\n    RGB_MEAN = cfg[\'RGB_MEAN\'] # for normalize inputs\n    RGB_STD = cfg[\'RGB_STD\']\n    EMBEDDING_SIZE = cfg[\'EMBEDDING_SIZE\'] # feature dimension\n    BATCH_SIZE = cfg[\'BATCH_SIZE\']\n    DROP_LAST = cfg[\'DROP_LAST\'] # whether drop the last batch to ensure consistent batch_norm statistics\n    LR = cfg[\'LR\'] # initial LR\n    NUM_EPOCH = cfg[\'NUM_EPOCH\']\n    WEIGHT_DECAY = cfg[\'WEIGHT_DECAY\']\n    MOMENTUM = cfg[\'MOMENTUM\']\n    STAGES = cfg[\'STAGES\'] # epoch stages to decay learning rate\n\n    DEVICE = cfg[\'DEVICE\']\n    MULTI_GPU = cfg[\'MULTI_GPU\'] # flag to use multiple GPUs\n    GPU_ID = cfg[\'GPU_ID\'] # specify your GPU ids\n    PIN_MEMORY = cfg[\'PIN_MEMORY\']\n    NUM_WORKERS = cfg[\'NUM_WORKERS\']\n    print(""="" * 60)\n    print(""Overall Configurations:"")\n    print(cfg)\n    print(""="" * 60)\n\n    writer = SummaryWriter(LOG_ROOT) # writer for buffering intermedium results\n\n    train_transform = transforms.Compose([ # refer to https://pytorch.org/docs/stable/torchvision/transforms.html for more build-in online data augmentation\n        transforms.Resize([int(128 * INPUT_SIZE[0] / 112), int(128 * INPUT_SIZE[0] / 112)]), # smaller side resized\n        transforms.RandomCrop([INPUT_SIZE[0], INPUT_SIZE[1]]),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean = RGB_MEAN,\n                             std = RGB_STD),\n    ])\n\n    dataset_train = datasets.ImageFolder(os.path.join(DATA_ROOT, \'imgs\'), train_transform)\n\n    # create a weighted random sampler to process imbalanced data\n    weights = make_weights_for_balanced_classes(dataset_train.imgs, len(dataset_train.classes))\n    weights = torch.DoubleTensor(weights)\n    sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights))\n\n    train_loader = torch.utils.data.DataLoader(\n        dataset_train, batch_size = BATCH_SIZE, sampler = sampler, pin_memory = PIN_MEMORY,\n        num_workers = NUM_WORKERS, drop_last = DROP_LAST\n    )\n\n    NUM_CLASS = len(train_loader.dataset.classes)\n    print(""Number of Training Classes: {}"".format(NUM_CLASS))\n\n    lfw, cfp_ff, cfp_fp, agedb, calfw, cplfw, vgg2_fp, lfw_issame, cfp_ff_issame, cfp_fp_issame, agedb_issame, calfw_issame, cplfw_issame, vgg2_fp_issame = get_val_data(DATA_ROOT)\n\n\n    #======= model & loss & optimizer =======#\n    BACKBONE_DICT = {\'ResNet_50\': ResNet_50(INPUT_SIZE), \n                     \'ResNet_101\': ResNet_101(INPUT_SIZE), \n                     \'ResNet_152\': ResNet_152(INPUT_SIZE),\n                     \'IR_50\': IR_50(INPUT_SIZE), \n                     \'IR_101\': IR_101(INPUT_SIZE), \n                     \'IR_152\': IR_152(INPUT_SIZE),\n                     \'IR_SE_50\': IR_SE_50(INPUT_SIZE), \n                     \'IR_SE_101\': IR_SE_101(INPUT_SIZE), \n                     \'IR_SE_152\': IR_SE_152(INPUT_SIZE)}\n    BACKBONE = BACKBONE_DICT[BACKBONE_NAME]\n    print(""="" * 60)\n    print(BACKBONE)\n    print(""{} Backbone Generated"".format(BACKBONE_NAME))\n    print(""="" * 60)\n\n    HEAD_DICT = {\'ArcFace\': ArcFace(in_features = EMBEDDING_SIZE, out_features = NUM_CLASS, device_id = GPU_ID),\n                 \'CosFace\': CosFace(in_features = EMBEDDING_SIZE, out_features = NUM_CLASS, device_id = GPU_ID),\n                 \'SphereFace\': SphereFace(in_features = EMBEDDING_SIZE, out_features = NUM_CLASS, device_id = GPU_ID),\n                 \'Am_softmax\': Am_softmax(in_features = EMBEDDING_SIZE, out_features = NUM_CLASS, device_id = GPU_ID)}\n    HEAD = HEAD_DICT[HEAD_NAME]\n    print(""="" * 60)\n    print(HEAD)\n    print(""{} Head Generated"".format(HEAD_NAME))\n    print(""="" * 60)\n\n    LOSS_DICT = {\'Focal\': FocalLoss(), \n                 \'Softmax\': nn.CrossEntropyLoss()}\n    LOSS = LOSS_DICT[LOSS_NAME]\n    print(""="" * 60)\n    print(LOSS)\n    print(""{} Loss Generated"".format(LOSS_NAME))\n    print(""="" * 60)\n\n    if BACKBONE_NAME.find(""IR"") >= 0:\n        backbone_paras_only_bn, backbone_paras_wo_bn = separate_irse_bn_paras(BACKBONE) # separate batch_norm parameters from others; do not do weight decay for batch_norm parameters to improve the generalizability\n        _, head_paras_wo_bn = separate_irse_bn_paras(HEAD)\n    else:\n        backbone_paras_only_bn, backbone_paras_wo_bn = separate_resnet_bn_paras(BACKBONE) # separate batch_norm parameters from others; do not do weight decay for batch_norm parameters to improve the generalizability\n        _, head_paras_wo_bn = separate_resnet_bn_paras(HEAD)\n    OPTIMIZER = optim.SGD([{\'params\': backbone_paras_wo_bn + head_paras_wo_bn, \'weight_decay\': WEIGHT_DECAY}, {\'params\': backbone_paras_only_bn}], lr = LR, momentum = MOMENTUM)\n    print(""="" * 60)\n    print(OPTIMIZER)\n    print(""Optimizer Generated"")\n    print(""="" * 60)\n\n    # optionally resume from a checkpoint\n    if BACKBONE_RESUME_ROOT and HEAD_RESUME_ROOT:\n        print(""="" * 60)\n        if os.path.isfile(BACKBONE_RESUME_ROOT) and os.path.isfile(HEAD_RESUME_ROOT):\n            print(""Loading Backbone Checkpoint \'{}\'"".format(BACKBONE_RESUME_ROOT))\n            BACKBONE.load_state_dict(torch.load(BACKBONE_RESUME_ROOT))\n            print(""Loading Head Checkpoint \'{}\'"".format(HEAD_RESUME_ROOT))\n            HEAD.load_state_dict(torch.load(HEAD_RESUME_ROOT))\n        else:\n            print(""No Checkpoint Found at \'{}\' and \'{}\'. Please Have a Check or Continue to Train from Scratch"".format(BACKBONE_RESUME_ROOT, HEAD_RESUME_ROOT))\n        print(""="" * 60)\n\n    if MULTI_GPU:\n        # multi-GPU setting\n        BACKBONE = nn.DataParallel(BACKBONE, device_ids = GPU_ID)\n        BACKBONE = BACKBONE.to(DEVICE)\n    else:\n        # single-GPU setting\n        BACKBONE = BACKBONE.to(DEVICE)\n\n\n    #======= train & validation & save checkpoint =======#\n    DISP_FREQ = len(train_loader) // 100 # frequency to display training loss & acc\n\n    NUM_EPOCH_WARM_UP = NUM_EPOCH // 25  # use the first 1/25 epochs to warm up\n    NUM_BATCH_WARM_UP = len(train_loader) * NUM_EPOCH_WARM_UP  # use the first 1/25 epochs to warm up\n    batch = 0  # batch index\n\n    for epoch in range(NUM_EPOCH): # start training process\n        \n        if epoch == STAGES[0]: # adjust LR for each training stage after warm up, you can also choose to adjust LR manually (with slight modification) once plaueau observed\n            schedule_lr(OPTIMIZER)\n        if epoch == STAGES[1]:\n            schedule_lr(OPTIMIZER)\n        if epoch == STAGES[2]:\n            schedule_lr(OPTIMIZER)\n\n        BACKBONE.train()  # set to training mode\n        HEAD.train()\n\n        losses = AverageMeter()\n        top1 = AverageMeter()\n        top5 = AverageMeter()\n\n        for inputs, labels in tqdm(iter(train_loader)):\n\n            if (epoch + 1 <= NUM_EPOCH_WARM_UP) and (batch + 1 <= NUM_BATCH_WARM_UP): # adjust LR for each training batch during warm up\n                warm_up_lr(batch + 1, NUM_BATCH_WARM_UP, LR, OPTIMIZER)\n\n            # compute output\n            inputs = inputs.to(DEVICE)\n            labels = labels.to(DEVICE).long()\n            features = BACKBONE(inputs)\n            outputs = HEAD(features, labels)\n            loss = LOSS(outputs, labels)\n\n            # measure accuracy and record loss\n            prec1, prec5 = accuracy(outputs.data, labels, topk = (1, 5))\n            losses.update(loss.data.item(), inputs.size(0))\n            top1.update(prec1.data.item(), inputs.size(0))\n            top5.update(prec5.data.item(), inputs.size(0))\n\n            # compute gradient and do SGD step\n            OPTIMIZER.zero_grad()\n            loss.backward()\n            OPTIMIZER.step()\n            \n            # dispaly training loss & acc every DISP_FREQ\n            if ((batch + 1) % DISP_FREQ == 0) and batch != 0:\n                print(""="" * 60)\n                print(\'Epoch {}/{} Batch {}/{}\\t\'\n                      \'Training Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                      \'Training Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t\'\n                      \'Training Prec@5 {top5.val:.3f} ({top5.avg:.3f})\'.format(\n                    epoch + 1, NUM_EPOCH, batch + 1, len(train_loader) * NUM_EPOCH, loss = losses, top1 = top1, top5 = top5))\n                print(""="" * 60)\n\n            batch += 1 # batch index\n\n        # training statistics per epoch (buffer for visualization)\n        epoch_loss = losses.avg\n        epoch_acc = top1.avg\n        writer.add_scalar(""Training_Loss"", epoch_loss, epoch + 1)\n        writer.add_scalar(""Training_Accuracy"", epoch_acc, epoch + 1)\n        print(""="" * 60)\n        print(\'Epoch: {}/{}\\t\'\n              \'Training Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n              \'Training Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t\'\n              \'Training Prec@5 {top5.val:.3f} ({top5.avg:.3f})\'.format(\n            epoch + 1, NUM_EPOCH, loss = losses, top1 = top1, top5 = top5))\n        print(""="" * 60)\n\n        # perform validation & save checkpoints per epoch\n        # validation statistics per epoch (buffer for visualization)\n        print(""="" * 60)\n        print(""Perform Evaluation on LFW, CFP_FF, CFP_FP, AgeDB, CALFW, CPLFW and VGG2_FP, and Save Checkpoints..."")\n        accuracy_lfw, best_threshold_lfw, roc_curve_lfw = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, lfw, lfw_issame)\n        buffer_val(writer, ""LFW"", accuracy_lfw, best_threshold_lfw, roc_curve_lfw, epoch + 1)\n        accuracy_cfp_ff, best_threshold_cfp_ff, roc_curve_cfp_ff = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, cfp_ff, cfp_ff_issame)\n        buffer_val(writer, ""CFP_FF"", accuracy_cfp_ff, best_threshold_cfp_ff, roc_curve_cfp_ff, epoch + 1)\n        accuracy_cfp_fp, best_threshold_cfp_fp, roc_curve_cfp_fp = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, cfp_fp, cfp_fp_issame)\n        buffer_val(writer, ""CFP_FP"", accuracy_cfp_fp, best_threshold_cfp_fp, roc_curve_cfp_fp, epoch + 1)\n        accuracy_agedb, best_threshold_agedb, roc_curve_agedb = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, agedb, agedb_issame)\n        buffer_val(writer, ""AgeDB"", accuracy_agedb, best_threshold_agedb, roc_curve_agedb, epoch + 1)\n        accuracy_calfw, best_threshold_calfw, roc_curve_calfw = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, calfw, calfw_issame)\n        buffer_val(writer, ""CALFW"", accuracy_calfw, best_threshold_calfw, roc_curve_calfw, epoch + 1)\n        accuracy_cplfw, best_threshold_cplfw, roc_curve_cplfw = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, cplfw, cplfw_issame)\n        buffer_val(writer, ""CPLFW"", accuracy_cplfw, best_threshold_cplfw, roc_curve_cplfw, epoch + 1)\n        accuracy_vgg2_fp, best_threshold_vgg2_fp, roc_curve_vgg2_fp = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, vgg2_fp, vgg2_fp_issame)\n        buffer_val(writer, ""VGGFace2_FP"", accuracy_vgg2_fp, best_threshold_vgg2_fp, roc_curve_vgg2_fp, epoch + 1)\n        print(""Epoch {}/{}, Evaluation: LFW Acc: {}, CFP_FF Acc: {}, CFP_FP Acc: {}, AgeDB Acc: {}, CALFW Acc: {}, CPLFW Acc: {}, VGG2_FP Acc: {}"".format(epoch + 1, NUM_EPOCH, accuracy_lfw, accuracy_cfp_ff, accuracy_cfp_fp, accuracy_agedb, accuracy_calfw, accuracy_cplfw, accuracy_vgg2_fp))\n        print(""="" * 60)\n\n        # save checkpoints per epoch\n        if MULTI_GPU:\n            torch.save(BACKBONE.module.state_dict(), os.path.join(MODEL_ROOT, ""Backbone_{}_Epoch_{}_Batch_{}_Time_{}_checkpoint.pth"".format(BACKBONE_NAME, epoch + 1, batch, get_time())))\n            torch.save(HEAD.state_dict(), os.path.join(MODEL_ROOT, ""Head_{}_Epoch_{}_Batch_{}_Time_{}_checkpoint.pth"".format(HEAD_NAME, epoch + 1, batch, get_time())))\n        else:\n            torch.save(BACKBONE.state_dict(), os.path.join(MODEL_ROOT, ""Backbone_{}_Epoch_{}_Batch_{}_Time_{}_checkpoint.pth"".format(BACKBONE_NAME, epoch + 1, batch, get_time())))\n            torch.save(HEAD.state_dict(), os.path.join(MODEL_ROOT, ""Head_{}_Epoch_{}_Batch_{}_Time_{}_checkpoint.pth"".format(HEAD_NAME, epoch + 1, batch, get_time())))\n'"
align/__init__.py,0,b'\n'
align/align_trans.py,0,"b'import numpy as np\nimport cv2\nfrom matlab_cp2tform import get_similarity_transform_for_cv2\n\n\n# reference facial points, a list of coordinates (x,y)\nREFERENCE_FACIAL_POINTS = [        # default reference facial points for crop_size = (112, 112); should adjust REFERENCE_FACIAL_POINTS accordingly for other crop_size\n    [30.29459953,  51.69630051], \n    [65.53179932,  51.50139999],\n    [48.02519989,  71.73660278],\n    [33.54930115,  92.3655014],\n    [62.72990036,  92.20410156]\n]\n\nDEFAULT_CROP_SIZE = (96, 112)\n\n\nclass FaceWarpException(Exception):\n    def __str__(self):\n        return \'In File {}:{}\'.format(\n            __file__, super.__str__(self))\n\n\ndef get_reference_facial_points(output_size = None,\n                                inner_padding_factor = 0.0,\n                                outer_padding=(0, 0),\n                                default_square = False):\n    """"""\n    Function:\n    ----------\n        get reference 5 key points according to crop settings:\n        0. Set default crop_size:\n            if default_square: \n                crop_size = (112, 112)\n            else: \n                crop_size = (96, 112)\n        1. Pad the crop_size by inner_padding_factor in each side;\n        2. Resize crop_size into (output_size - outer_padding*2),\n            pad into output_size with outer_padding;\n        3. Output reference_5point;\n    Parameters:\n    ----------\n        @output_size: (w, h) or None\n            size of aligned face image\n        @inner_padding_factor: (w_factor, h_factor)\n            padding factor for inner (w, h)\n        @outer_padding: (w_pad, h_pad)\n            each row is a pair of coordinates (x, y)\n        @default_square: True or False\n            if True:\n                default crop_size = (112, 112)\n            else:\n                default crop_size = (96, 112);\n        !!! make sure, if output_size is not None:\n                (output_size - outer_padding) \n                = some_scale * (default crop_size * (1.0 + inner_padding_factor))\n    Returns:\n    ----------\n        @reference_5point: 5x2 np.array\n            each row is a pair of transformed coordinates (x, y)\n    """"""\n    #print(\'\\n===> get_reference_facial_points():\')\n\n    #print(\'---> Params:\')\n    #print(\'            output_size: \', output_size)\n    #print(\'            inner_padding_factor: \', inner_padding_factor)\n    #print(\'            outer_padding:\', outer_padding)\n    #print(\'            default_square: \', default_square)\n\n    tmp_5pts = np.array(REFERENCE_FACIAL_POINTS)\n    tmp_crop_size = np.array(DEFAULT_CROP_SIZE)\n\n    # 0) make the inner region a square\n    if default_square:\n        size_diff = max(tmp_crop_size) - tmp_crop_size\n        tmp_5pts += size_diff / 2\n        tmp_crop_size += size_diff\n\n    #print(\'---> default:\')\n    #print(\'              crop_size = \', tmp_crop_size)\n    #print(\'              reference_5pts = \', tmp_5pts)\n\n    if (output_size and\n            output_size[0] == tmp_crop_size[0] and\n            output_size[1] == tmp_crop_size[1]):\n        #print(\'output_size == DEFAULT_CROP_SIZE {}: return default reference points\'.format(tmp_crop_size))\n        return tmp_5pts\n\n    if (inner_padding_factor == 0 and\n            outer_padding == (0, 0)):\n        if output_size is None:\n            #print(\'No paddings to do: return default reference points\')\n            return tmp_5pts\n        else:\n            raise FaceWarpException(\n                \'No paddings to do, output_size must be None or {}\'.format(tmp_crop_size))\n\n    # check output size\n    if not (0 <= inner_padding_factor <= 1.0):\n        raise FaceWarpException(\'Not (0 <= inner_padding_factor <= 1.0)\')\n\n    if ((inner_padding_factor > 0 or outer_padding[0] > 0 or outer_padding[1] > 0)\n            and output_size is None):\n        output_size = tmp_crop_size * \\\n            (1 + inner_padding_factor * 2).astype(np.int32)\n        output_size += np.array(outer_padding)\n        #print(\'              deduced from paddings, output_size = \', output_size)\n\n    if not (outer_padding[0] < output_size[0]\n            and outer_padding[1] < output_size[1]):\n        raise FaceWarpException(\'Not (outer_padding[0] < output_size[0]\'\n                                \'and outer_padding[1] < output_size[1])\')\n\n    # 1) pad the inner region according inner_padding_factor\n    #print(\'---> STEP1: pad the inner region according inner_padding_factor\')\n    if inner_padding_factor > 0:\n        size_diff = tmp_crop_size * inner_padding_factor * 2\n        tmp_5pts += size_diff / 2\n        tmp_crop_size += np.round(size_diff).astype(np.int32)\n\n    #print(\'              crop_size = \', tmp_crop_size)\n    #print(\'              reference_5pts = \', tmp_5pts)\n\n    # 2) resize the padded inner region\n    #print(\'---> STEP2: resize the padded inner region\')\n    size_bf_outer_pad = np.array(output_size) - np.array(outer_padding) * 2\n    #print(\'              crop_size = \', tmp_crop_size)\n    #print(\'              size_bf_outer_pad = \', size_bf_outer_pad)\n\n    if size_bf_outer_pad[0] * tmp_crop_size[1] != size_bf_outer_pad[1] * tmp_crop_size[0]:\n        raise FaceWarpException(\'Must have (output_size - outer_padding)\'\n                                \'= some_scale * (crop_size * (1.0 + inner_padding_factor)\')\n\n    scale_factor = size_bf_outer_pad[0].astype(np.float32) / tmp_crop_size[0]\n    #print(\'              resize scale_factor = \', scale_factor)\n    tmp_5pts = tmp_5pts * scale_factor\n#    size_diff = tmp_crop_size * (scale_factor - min(scale_factor))\n#    tmp_5pts = tmp_5pts + size_diff / 2\n    tmp_crop_size = size_bf_outer_pad\n    #print(\'              crop_size = \', tmp_crop_size)\n    #print(\'              reference_5pts = \', tmp_5pts)\n\n    # 3) add outer_padding to make output_size\n    reference_5point = tmp_5pts + np.array(outer_padding)\n    tmp_crop_size = output_size\n    #print(\'---> STEP3: add outer_padding to make output_size\')\n    #print(\'              crop_size = \', tmp_crop_size)\n    #print(\'              reference_5pts = \', tmp_5pts)\n\n    #print(\'===> end get_reference_facial_points\\n\')\n\n    return reference_5point\n\n\ndef get_affine_transform_matrix(src_pts, dst_pts):\n    """"""\n    Function:\n    ----------\n        get affine transform matrix \'tfm\' from src_pts to dst_pts\n    Parameters:\n    ----------\n        @src_pts: Kx2 np.array\n            source points matrix, each row is a pair of coordinates (x, y)\n        @dst_pts: Kx2 np.array\n            destination points matrix, each row is a pair of coordinates (x, y)\n    Returns:\n    ----------\n        @tfm: 2x3 np.array\n            transform matrix from src_pts to dst_pts\n    """"""\n\n    tfm = np.float32([[1, 0, 0], [0, 1, 0]])\n    n_pts = src_pts.shape[0]\n    ones = np.ones((n_pts, 1), src_pts.dtype)\n    src_pts_ = np.hstack([src_pts, ones])\n    dst_pts_ = np.hstack([dst_pts, ones])\n\n#    #print((\'src_pts_:\\n\' + str(src_pts_))\n#    #print((\'dst_pts_:\\n\' + str(dst_pts_))\n\n    A, res, rank, s = np.linalg.lstsq(src_pts_, dst_pts_)\n\n#    #print((\'np.linalg.lstsq return A: \\n\' + str(A))\n#    #print((\'np.linalg.lstsq return res: \\n\' + str(res))\n#    #print((\'np.linalg.lstsq return rank: \\n\' + str(rank))\n#    #print((\'np.linalg.lstsq return s: \\n\' + str(s))\n\n    if rank == 3:\n        tfm = np.float32([\n            [A[0, 0], A[1, 0], A[2, 0]],\n            [A[0, 1], A[1, 1], A[2, 1]]\n        ])\n    elif rank == 2:\n        tfm = np.float32([\n            [A[0, 0], A[1, 0], 0],\n            [A[0, 1], A[1, 1], 0]\n        ])\n\n    return tfm\n\n\ndef warp_and_crop_face(src_img,\n                       facial_pts,\n                       reference_pts = None,\n                       crop_size=(96, 112),\n                       align_type = \'smilarity\'):\n    """"""\n    Function:\n    ----------\n        apply affine transform \'trans\' to uv\n    Parameters:\n    ----------\n        @src_img: 3x3 np.array\n            input image\n        @facial_pts: could be\n            1)a list of K coordinates (x,y)\n        or\n            2) Kx2 or 2xK np.array\n            each row or col is a pair of coordinates (x, y)\n        @reference_pts: could be\n            1) a list of K coordinates (x,y)\n        or\n            2) Kx2 or 2xK np.array\n            each row or col is a pair of coordinates (x, y)\n        or\n            3) None\n            if None, use default reference facial points\n        @crop_size: (w, h)\n            output face image size\n        @align_type: transform type, could be one of\n            1) \'similarity\': use similarity transform\n            2) \'cv2_affine\': use the first 3 points to do affine transform,\n                    by calling cv2.getAffineTransform()\n            3) \'affine\': use all points to do affine transform\n    Returns:\n    ----------\n        @face_img: output face image with size (w, h) = @crop_size\n    """"""\n\n    if reference_pts is None:\n        if crop_size[0] == 96 and crop_size[1] == 112:\n            reference_pts = REFERENCE_FACIAL_POINTS\n        else:\n            default_square = False\n            inner_padding_factor = 0\n            outer_padding = (0, 0)\n            output_size = crop_size\n\n            reference_pts = get_reference_facial_points(output_size,\n                                                        inner_padding_factor,\n                                                        outer_padding,\n                                                        default_square)\n\n    ref_pts = np.float32(reference_pts)\n    ref_pts_shp = ref_pts.shape\n    if max(ref_pts_shp) < 3 or min(ref_pts_shp) != 2:\n        raise FaceWarpException(\n            \'reference_pts.shape must be (K,2) or (2,K) and K>2\')\n\n    if ref_pts_shp[0] == 2:\n        ref_pts = ref_pts.T\n\n    src_pts = np.float32(facial_pts)\n    src_pts_shp = src_pts.shape\n    if max(src_pts_shp) < 3 or min(src_pts_shp) != 2:\n        raise FaceWarpException(\n            \'facial_pts.shape must be (K,2) or (2,K) and K>2\')\n\n    if src_pts_shp[0] == 2:\n        src_pts = src_pts.T\n\n#    #print(\'--->src_pts:\\n\', src_pts\n#    #print(\'--->ref_pts\\n\', ref_pts\n\n    if src_pts.shape != ref_pts.shape:\n        raise FaceWarpException(\n            \'facial_pts and reference_pts must have the same shape\')\n\n    if align_type is \'cv2_affine\':\n        tfm = cv2.getAffineTransform(src_pts[0:3], ref_pts[0:3])\n#        #print((\'cv2.getAffineTransform() returns tfm=\\n\' + str(tfm))\n    elif align_type is \'affine\':\n        tfm = get_affine_transform_matrix(src_pts, ref_pts)\n#        #print((\'get_affine_transform_matrix() returns tfm=\\n\' + str(tfm))\n    else:\n        tfm = get_similarity_transform_for_cv2(src_pts, ref_pts)\n#        #print((\'get_similarity_transform_for_cv2() returns tfm=\\n\' + str(tfm))\n\n#    #print(\'--->Transform matrix: \'\n#    #print((\'type(tfm):\' + str(type(tfm)))\n#    #print((\'tfm.dtype:\' + str(tfm.dtype))\n#    #print( tfm\n\n    face_img = cv2.warpAffine(src_img, tfm, (crop_size[0], crop_size[1]))\n\n    return face_img'"
align/box_utils.py,0,"b'import numpy as np\nfrom PIL import Image\n\n\ndef nms(boxes, overlap_threshold = 0.5, mode = \'union\'):\n    """"""Non-maximum suppression.\n\n    Arguments:\n        boxes: a float numpy array of shape [n, 5],\n            where each row is (xmin, ymin, xmax, ymax, score).\n        overlap_threshold: a float number.\n        mode: \'union\' or \'min\'.\n\n    Returns:\n        list with indices of the selected boxes\n    """"""\n\n    # if there are no boxes, return the empty list\n    if len(boxes) == 0:\n        return []\n\n    # list of picked indices\n    pick = []\n\n    # grab the coordinates of the bounding boxes\n    x1, y1, x2, y2, score = [boxes[:, i] for i in range(5)]\n\n    area = (x2 - x1 + 1.0)*(y2 - y1 + 1.0)\n    ids = np.argsort(score)  # in increasing order\n\n    while len(ids) > 0:\n\n        # grab index of the largest value\n        last = len(ids) - 1\n        i = ids[last]\n        pick.append(i)\n\n        # compute intersections\n        # of the box with the largest score\n        # with the rest of boxes\n\n        # left top corner of intersection boxes\n        ix1 = np.maximum(x1[i], x1[ids[:last]])\n        iy1 = np.maximum(y1[i], y1[ids[:last]])\n\n        # right bottom corner of intersection boxes\n        ix2 = np.minimum(x2[i], x2[ids[:last]])\n        iy2 = np.minimum(y2[i], y2[ids[:last]])\n\n        # width and height of intersection boxes\n        w = np.maximum(0.0, ix2 - ix1 + 1.0)\n        h = np.maximum(0.0, iy2 - iy1 + 1.0)\n\n        # intersections\' areas\n        inter = w * h\n        if mode == \'min\':\n            overlap = inter/np.minimum(area[i], area[ids[:last]])\n        elif mode == \'union\':\n            # intersection over union (IoU)\n            overlap = inter/(area[i] + area[ids[:last]] - inter)\n\n        # delete all boxes where overlap is too big\n        ids = np.delete(\n            ids,\n            np.concatenate([[last], np.where(overlap > overlap_threshold)[0]])\n        )\n\n    return pick\n\n\ndef convert_to_square(bboxes):\n    """"""Convert bounding boxes to a square form.\n\n    Arguments:\n        bboxes: a float numpy array of shape [n, 5].\n\n    Returns:\n        a float numpy array of shape [n, 5],\n            squared bounding boxes.\n    """"""\n\n    square_bboxes = np.zeros_like(bboxes)\n    x1, y1, x2, y2 = [bboxes[:, i] for i in range(4)]\n    h = y2 - y1 + 1.0\n    w = x2 - x1 + 1.0\n    max_side = np.maximum(h, w)\n    square_bboxes[:, 0] = x1 + w*0.5 - max_side*0.5\n    square_bboxes[:, 1] = y1 + h*0.5 - max_side*0.5\n    square_bboxes[:, 2] = square_bboxes[:, 0] + max_side - 1.0\n    square_bboxes[:, 3] = square_bboxes[:, 1] + max_side - 1.0\n    return square_bboxes\n\n\ndef calibrate_box(bboxes, offsets):\n    """"""Transform bounding boxes to be more like true bounding boxes.\n    \'offsets\' is one of the outputs of the nets.\n\n    Arguments:\n        bboxes: a float numpy array of shape [n, 5].\n        offsets: a float numpy array of shape [n, 4].\n\n    Returns:\n        a float numpy array of shape [n, 5].\n    """"""\n    x1, y1, x2, y2 = [bboxes[:, i] for i in range(4)]\n    w = x2 - x1 + 1.0\n    h = y2 - y1 + 1.0\n    w = np.expand_dims(w, 1)\n    h = np.expand_dims(h, 1)\n\n    # this is what happening here:\n    # tx1, ty1, tx2, ty2 = [offsets[:, i] for i in range(4)]\n    # x1_true = x1 + tx1*w\n    # y1_true = y1 + ty1*h\n    # x2_true = x2 + tx2*w\n    # y2_true = y2 + ty2*h\n    # below is just more compact form of this\n\n    # are offsets always such that\n    # x1 < x2 and y1 < y2 ?\n\n    translation = np.hstack([w, h, w, h])*offsets\n    bboxes[:, 0:4] = bboxes[:, 0:4] + translation\n    return bboxes\n\n\ndef get_image_boxes(bounding_boxes, img, size = 24):\n    """"""Cut out boxes from the image.\n\n    Arguments:\n        bounding_boxes: a float numpy array of shape [n, 5].\n        img: an instance of PIL.Image.\n        size: an integer, size of cutouts.\n\n    Returns:\n        a float numpy array of shape [n, 3, size, size].\n    """"""\n\n    num_boxes = len(bounding_boxes)\n    width, height = img.size\n\n    [dy, edy, dx, edx, y, ey, x, ex, w, h] = correct_bboxes(bounding_boxes, width, height)\n    img_boxes = np.zeros((num_boxes, 3, size, size), \'float32\')\n\n    for i in range(num_boxes):\n        img_box = np.zeros((h[i], w[i], 3), \'uint8\')\n\n        img_array = np.asarray(img, \'uint8\')\n        img_box[dy[i]:(edy[i] + 1), dx[i]:(edx[i] + 1), :] =\\\n            img_array[y[i]:(ey[i] + 1), x[i]:(ex[i] + 1), :]\n\n        # resize\n        img_box = Image.fromarray(img_box)\n        img_box = img_box.resize((size, size), Image.BILINEAR)\n        img_box = np.asarray(img_box, \'float32\')\n\n        img_boxes[i, :, :, :] = _preprocess(img_box)\n\n    return img_boxes\n\n\ndef correct_bboxes(bboxes, width, height):\n    """"""Crop boxes that are too big and get coordinates\n    with respect to cutouts.\n\n    Arguments:\n        bboxes: a float numpy array of shape [n, 5],\n            where each row is (xmin, ymin, xmax, ymax, score).\n        width: a float number.\n        height: a float number.\n\n    Returns:\n        dy, dx, edy, edx: a int numpy arrays of shape [n],\n            coordinates of the boxes with respect to the cutouts.\n        y, x, ey, ex: a int numpy arrays of shape [n],\n            corrected ymin, xmin, ymax, xmax.\n        h, w: a int numpy arrays of shape [n],\n            just heights and widths of boxes.\n\n        in the following order:\n            [dy, edy, dx, edx, y, ey, x, ex, w, h].\n    """"""\n\n    x1, y1, x2, y2 = [bboxes[:, i] for i in range(4)]\n    w, h = x2 - x1 + 1.0,  y2 - y1 + 1.0\n    num_boxes = bboxes.shape[0]\n\n    # \'e\' stands for end\n    # (x, y) -> (ex, ey)\n    x, y, ex, ey = x1, y1, x2, y2\n\n    # we need to cut out a box from the image.\n    # (x, y, ex, ey) are corrected coordinates of the box\n    # in the image.\n    # (dx, dy, edx, edy) are coordinates of the box in the cutout\n    # from the image.\n    dx, dy = np.zeros((num_boxes,)), np.zeros((num_boxes,))\n    edx, edy = w.copy() - 1.0, h.copy() - 1.0\n\n    # if box\'s bottom right corner is too far right\n    ind = np.where(ex > width - 1.0)[0]\n    edx[ind] = w[ind] + width - 2.0 - ex[ind]\n    ex[ind] = width - 1.0\n\n    # if box\'s bottom right corner is too low\n    ind = np.where(ey > height - 1.0)[0]\n    edy[ind] = h[ind] + height - 2.0 - ey[ind]\n    ey[ind] = height - 1.0\n\n    # if box\'s top left corner is too far left\n    ind = np.where(x < 0.0)[0]\n    dx[ind] = 0.0 - x[ind]\n    x[ind] = 0.0\n\n    # if box\'s top left corner is too high\n    ind = np.where(y < 0.0)[0]\n    dy[ind] = 0.0 - y[ind]\n    y[ind] = 0.0\n\n    return_list = [dy, edy, dx, edx, y, ey, x, ex, w, h]\n    return_list = [i.astype(\'int32\') for i in return_list]\n\n    return return_list\n\n\ndef _preprocess(img):\n    """"""Preprocessing step before feeding the network.\n\n    Arguments:\n        img: a float numpy array of shape [h, w, c].\n\n    Returns:\n        a float numpy array of shape [1, c, h, w].\n    """"""\n    img = img.transpose((2, 0, 1))\n    img = np.expand_dims(img, 0)\n    img = (img - 127.5) * 0.0078125\n    return img\n'"
align/detector.py,3,"b'import numpy as np\nimport torch\nfrom torch.autograd import Variable\nfrom get_nets import PNet, RNet, ONet\nfrom box_utils import nms, calibrate_box, get_image_boxes, convert_to_square\nfrom first_stage import run_first_stage\n\n\ndef detect_faces(image, min_face_size = 20.0,\n                 thresholds=[0.6, 0.7, 0.8],\n                 nms_thresholds=[0.7, 0.7, 0.7]):\n    """"""\n    Arguments:\n        image: an instance of PIL.Image.\n        min_face_size: a float number.\n        thresholds: a list of length 3.\n        nms_thresholds: a list of length 3.\n\n    Returns:\n        two float numpy arrays of shapes [n_boxes, 4] and [n_boxes, 10],\n        bounding boxes and facial landmarks.\n    """"""\n\n    # LOAD MODELS\n    pnet = PNet()\n    rnet = RNet()\n    onet = ONet()\n    onet.eval()\n\n    # BUILD AN IMAGE PYRAMID\n    width, height = image.size\n    min_length = min(height, width)\n\n    min_detection_size = 12\n    factor = 0.707  # sqrt(0.5)\n\n    # scales for scaling the image\n    scales = []\n\n    # scales the image so that\n    # minimum size that we can detect equals to\n    # minimum face size that we want to detect\n    m = min_detection_size/min_face_size\n    min_length *= m\n\n    factor_count = 0\n    while min_length > min_detection_size:\n        scales.append(m*factor**factor_count)\n        min_length *= factor\n        factor_count += 1\n\n    # STAGE 1\n\n    # it will be returned\n    bounding_boxes = []\n\n    # run P-Net on different scales\n    for s in scales:\n        boxes = run_first_stage(image, pnet, scale = s, threshold = thresholds[0])\n        bounding_boxes.append(boxes)\n\n    # collect boxes (and offsets, and scores) from different scales\n    bounding_boxes = [i for i in bounding_boxes if i is not None]\n    bounding_boxes = np.vstack(bounding_boxes)\n\n    keep = nms(bounding_boxes[:, 0:5], nms_thresholds[0])\n    bounding_boxes = bounding_boxes[keep]\n\n    # use offsets predicted by pnet to transform bounding boxes\n    bounding_boxes = calibrate_box(bounding_boxes[:, 0:5], bounding_boxes[:, 5:])\n    # shape [n_boxes, 5]\n\n    bounding_boxes = convert_to_square(bounding_boxes)\n    bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n\n    # STAGE 2\n\n    img_boxes = get_image_boxes(bounding_boxes, image, size = 24)\n    img_boxes = Variable(torch.FloatTensor(img_boxes), volatile = True)\n    output = rnet(img_boxes)\n    offsets = output[0].data.numpy()  # shape [n_boxes, 4]\n    probs = output[1].data.numpy()  # shape [n_boxes, 2]\n\n    keep = np.where(probs[:, 1] > thresholds[1])[0]\n    bounding_boxes = bounding_boxes[keep]\n    bounding_boxes[:, 4] = probs[keep, 1].reshape((-1, ))\n    offsets = offsets[keep]\n\n    keep = nms(bounding_boxes, nms_thresholds[1])\n    bounding_boxes = bounding_boxes[keep]\n    bounding_boxes = calibrate_box(bounding_boxes, offsets[keep])\n    bounding_boxes = convert_to_square(bounding_boxes)\n    bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n\n    # STAGE 3\n\n    img_boxes = get_image_boxes(bounding_boxes, image, size = 48)\n    if len(img_boxes) == 0: \n        return [], []\n    img_boxes = Variable(torch.FloatTensor(img_boxes), volatile = True)\n    output = onet(img_boxes)\n    landmarks = output[0].data.numpy()  # shape [n_boxes, 10]\n    offsets = output[1].data.numpy()  # shape [n_boxes, 4]\n    probs = output[2].data.numpy()  # shape [n_boxes, 2]\n\n    keep = np.where(probs[:, 1] > thresholds[2])[0]\n    bounding_boxes = bounding_boxes[keep]\n    bounding_boxes[:, 4] = probs[keep, 1].reshape((-1, ))\n    offsets = offsets[keep]\n    landmarks = landmarks[keep]\n\n    # compute landmark points\n    width = bounding_boxes[:, 2] - bounding_boxes[:, 0] + 1.0\n    height = bounding_boxes[:, 3] - bounding_boxes[:, 1] + 1.0\n    xmin, ymin = bounding_boxes[:, 0], bounding_boxes[:, 1]\n    landmarks[:, 0:5] = np.expand_dims(xmin, 1) + np.expand_dims(width, 1)*landmarks[:, 0:5]\n    landmarks[:, 5:10] = np.expand_dims(ymin, 1) + np.expand_dims(height, 1)*landmarks[:, 5:10]\n\n    bounding_boxes = calibrate_box(bounding_boxes, offsets)\n    keep = nms(bounding_boxes, nms_thresholds[2], mode = \'min\')\n    bounding_boxes = bounding_boxes[keep]\n    landmarks = landmarks[keep]\n\n    return bounding_boxes, landmarks\n'"
align/face_align.py,0,"b'from PIL import Image\nfrom detector import detect_faces\nfrom align_trans import get_reference_facial_points, warp_and_crop_face\nimport numpy as np\nimport os\nfrom tqdm import tqdm\nimport argparse\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description = ""face alignment"")\n    parser.add_argument(""-source_root"", ""--source_root"", help = ""specify your source dir"", default = ""./data/test"", type = str)\n    parser.add_argument(""-dest_root"", ""--dest_root"", help = ""specify your destination dir"", default = ""./data/test_Aligned"", type = str)\n    parser.add_argument(""-crop_size"", ""--crop_size"", help = ""specify size of aligned faces, align and crop with padding"", default = 112, type = int)\n    args = parser.parse_args()\n\n    source_root = args.source_root # specify your source dir\n    dest_root = args.dest_root # specify your destination dir\n    crop_size = args.crop_size # specify size of aligned faces, align and crop with padding\n    scale = crop_size / 112.\n    reference = get_reference_facial_points(default_square = True) * scale\n\n    cwd = os.getcwd() # delete \'.DS_Store\' existed in the source_root\n    os.chdir(source_root)\n    os.system(""find . -name \'*.DS_Store\' -type f -delete"")\n    os.chdir(cwd)\n\n    if not os.path.isdir(dest_root):\n        os.mkdir(dest_root)\n\n    for subfolder in tqdm(os.listdir(source_root)):\n        if not os.path.isdir(os.path.join(dest_root, subfolder)):\n            os.mkdir(os.path.join(dest_root, subfolder))\n        for image_name in os.listdir(os.path.join(source_root, subfolder)):\n            print(""Processing\\t{}"".format(os.path.join(source_root, subfolder, image_name)))\n            img = Image.open(os.path.join(source_root, subfolder, image_name))\n            try: # Handle exception\n                _, landmarks = detect_faces(img)\n            except Exception:\n                print(""{} is discarded due to exception!"".format(os.path.join(source_root, subfolder, image_name)))\n                continue\n            if len(landmarks) == 0: # If the landmarks cannot be detected, the img will be discarded\n                print(""{} is discarded due to non-detected landmarks!"".format(os.path.join(source_root, subfolder, image_name)))\n                continue\n            facial5points = [[landmarks[0][j], landmarks[0][j + 5]] for j in range(5)]\n            warped_face = warp_and_crop_face(np.array(img), facial5points, reference, crop_size=(crop_size, crop_size))\n            img_warped = Image.fromarray(warped_face)\n            if image_name.split(\'.\')[-1].lower() not in [\'jpg\', \'jpeg\']: #not from jpg\n                image_name = \'.\'.join(image_name.split(\'.\')[:-1]) + \'.jpg\'\n            img_warped.save(os.path.join(dest_root, subfolder, image_name))\n'"
align/face_resize.py,0,"b'import os\nimport cv2\nfrom tqdm import tqdm\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.mkdir(path)\n\n\ndef process_image(img):\n\n    size = img.shape\n    h, w = size[0], size[1]\n    scale = max(w, h) / float(min_side)\n    new_w, new_h = int(w / scale), int(h / scale)\n    resize_img = cv2.resize(img, (new_w, new_h))\n    if new_w % 2 != 0 and new_h % 2 == 0:\n        top, bottom, left, right = (min_side - new_h) / 2, (min_side - new_h) / 2, (min_side - new_w) / 2 + 1, (\n                    min_side - new_w) / 2\n    elif new_h % 2 != 0 and new_w % 2 == 0:\n        top, bottom, left, right = (min_side - new_h) / 2 + 1, (min_side - new_h) / 2, (min_side - new_w) / 2, (\n                    min_side - new_w) / 2\n    elif new_h % 2 == 0 and new_w % 2 == 0:\n        top, bottom, left, right = (min_side - new_h) / 2, (min_side - new_h) / 2, (min_side - new_w) / 2, (\n                    min_side - new_w) / 2\n    else:\n        top, bottom, left, right = (min_side - new_h) / 2 + 1, (min_side - new_h) / 2, (min_side - new_w) / 2 + 1, (\n                    min_side - new_w) / 2\n    pad_img = cv2.copyMakeBorder(resize_img, top, bottom, left, right, cv2.BORDER_CONSTANT,\n                                 value=[0, 0, 0])\n\n    return pad_img\n\n\ndef main(source_root):\n\n    dest_root = ""/media/pc/6T/jasonjzhao/data/MS-Celeb-1M_Resized""\n    mkdir(dest_root)\n    cwd = os.getcwd()  # delete \'.DS_Store\' existed in the source_root\n    os.chdir(source_root)\n    os.system(""find . -name \'*.DS_Store\' -type f -delete"")\n    os.chdir(cwd)\n\n    if not os.path.isdir(dest_root):\n        os.mkdir(dest_root)\n\n    for subfolder in tqdm(os.listdir(source_root)):\n        if not os.path.isdir(os.path.join(dest_root, subfolder)):\n            os.mkdir(os.path.join(dest_root, subfolder))\n        for image_name in os.listdir(os.path.join(source_root, subfolder)):\n            print(""Processing\\t{}"".format(os.path.join(source_root, subfolder, image_name)))\n            img = cv2.imread(os.path.join(source_root, subfolder, image_name))\n            if type(img) == type(None):\n                print(""damaged image %s, del it"" % (img))\n                os.remove(img)\n                continue\n            size = img.shape\n            h, w = size[0], size[1]\n            if max(w, h) > 512:\n                img_pad = process_image(img)\n            else:\n                img_pad = img\n            cv2.imwrite(os.path.join(dest_root, subfolder, image_name.split(\'.\')[0] + \'.jpg\'), img_pad)\n\n\nif __name__ == ""__main__"":\n    min_side = 512\n    main(source_root = ""/media/pc/6T/jasonjzhao/data/MS-Celeb-1M/database/base"")'"
align/first_stage.py,2,"b'import torch\nfrom torch.autograd import Variable\nimport math\nfrom PIL import Image\nimport numpy as np\nfrom box_utils import nms, _preprocess\n\n\ndef run_first_stage(image, net, scale, threshold):\n    """"""Run P-Net, generate bounding boxes, and do NMS.\n\n    Arguments:\n        image: an instance of PIL.Image.\n        net: an instance of pytorch\'s nn.Module, P-Net.\n        scale: a float number,\n            scale width and height of the image by this number.\n        threshold: a float number,\n            threshold on the probability of a face when generating\n            bounding boxes from predictions of the net.\n\n    Returns:\n        a float numpy array of shape [n_boxes, 9],\n            bounding boxes with scores and offsets (4 + 1 + 4).\n    """"""\n\n    # scale the image and convert it to a float array\n    width, height = image.size\n    sw, sh = math.ceil(width*scale), math.ceil(height*scale)\n    img = image.resize((sw, sh), Image.BILINEAR)\n    img = np.asarray(img, \'float32\')\n\n    img = Variable(torch.FloatTensor(_preprocess(img)), volatile = True)\n    output = net(img)\n    probs = output[1].data.numpy()[0, 1, :, :]\n    offsets = output[0].data.numpy()\n    # probs: probability of a face at each sliding window\n    # offsets: transformations to true bounding boxes\n\n    boxes = _generate_bboxes(probs, offsets, scale, threshold)\n    if len(boxes) == 0:\n        return None\n\n    keep = nms(boxes[:, 0:5], overlap_threshold = 0.5)\n    return boxes[keep]\n\n\ndef _generate_bboxes(probs, offsets, scale, threshold):\n    """"""Generate bounding boxes at places\n    where there is probably a face.\n\n    Arguments:\n        probs: a float numpy array of shape [n, m].\n        offsets: a float numpy array of shape [1, 4, n, m].\n        scale: a float number,\n            width and height of the image were scaled by this number.\n        threshold: a float number.\n\n    Returns:\n        a float numpy array of shape [n_boxes, 9]\n    """"""\n\n    # applying P-Net is equivalent, in some sense, to\n    # moving 12x12 window with stride 2\n    stride = 2\n    cell_size = 12\n\n    # indices of boxes where there is probably a face\n    inds = np.where(probs > threshold)\n\n    if inds[0].size == 0:\n        return np.array([])\n\n    # transformations of bounding boxes\n    tx1, ty1, tx2, ty2 = [offsets[0, i, inds[0], inds[1]] for i in range(4)]\n    # they are defined as:\n    # w = x2 - x1 + 1\n    # h = y2 - y1 + 1\n    # x1_true = x1 + tx1*w\n    # x2_true = x2 + tx2*w\n    # y1_true = y1 + ty1*h\n    # y2_true = y2 + ty2*h\n\n    offsets = np.array([tx1, ty1, tx2, ty2])\n    score = probs[inds[0], inds[1]]\n\n    # P-Net is applied to scaled images\n    # so we need to rescale bounding boxes back\n    bounding_boxes = np.vstack([\n        np.round((stride*inds[1] + 1.0)/scale),\n        np.round((stride*inds[0] + 1.0)/scale),\n        np.round((stride*inds[1] + 1.0 + cell_size)/scale),\n        np.round((stride*inds[0] + 1.0 + cell_size)/scale),\n        score, offsets\n    ])\n    # why one is added?\n\n    return bounding_boxes.T'"
align/get_nets.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict\nimport numpy as np\n\n\nclass Flatten(nn.Module):\n\n    def __init__(self):\n        super(Flatten, self).__init__()\n\n    def forward(self, x):\n        """"""\n        Arguments:\n            x: a float tensor with shape [batch_size, c, h, w].\n        Returns:\n            a float tensor with shape [batch_size, c*h*w].\n        """"""\n\n        # without this pretrained model isn\'t working\n        x = x.transpose(3, 2).contiguous()\n\n        return x.view(x.size(0), -1)\n\n\nclass PNet(nn.Module):\n\n    def __init__(self):\n\n        super(PNet, self).__init__()\n\n        # suppose we have input with size HxW, then\n        # after first layer: H - 2,\n        # after pool: ceil((H - 2)/2),\n        # after second conv: ceil((H - 2)/2) - 2,\n        # after last conv: ceil((H - 2)/2) - 4,\n        # and the same for W\n\n        self.features = nn.Sequential(OrderedDict([\n            (\'conv1\', nn.Conv2d(3, 10, 3, 1)),\n            (\'prelu1\', nn.PReLU(10)),\n            (\'pool1\', nn.MaxPool2d(2, 2, ceil_mode = True)),\n\n            (\'conv2\', nn.Conv2d(10, 16, 3, 1)),\n            (\'prelu2\', nn.PReLU(16)),\n\n            (\'conv3\', nn.Conv2d(16, 32, 3, 1)),\n            (\'prelu3\', nn.PReLU(32))\n        ]))\n\n        self.conv4_1 = nn.Conv2d(32, 2, 1, 1)\n        self.conv4_2 = nn.Conv2d(32, 4, 1, 1)\n\n        weights = np.load(""./pnet.npy"", allow_pickle=True)[()]\n        for n, p in self.named_parameters():\n            p.data = torch.FloatTensor(weights[n])\n\n    def forward(self, x):\n        """"""\n        Arguments:\n            x: a float tensor with shape [batch_size, 3, h, w].\n        Returns:\n            b: a float tensor with shape [batch_size, 4, h\', w\'].\n            a: a float tensor with shape [batch_size, 2, h\', w\'].\n        """"""\n        x = self.features(x)\n        a = self.conv4_1(x)\n        b = self.conv4_2(x)\n        a = F.softmax(a)\n        return b, a\n\n\nclass RNet(nn.Module):\n\n    def __init__(self):\n\n        super(RNet, self).__init__()\n\n        self.features = nn.Sequential(OrderedDict([\n            (\'conv1\', nn.Conv2d(3, 28, 3, 1)),\n            (\'prelu1\', nn.PReLU(28)),\n            (\'pool1\', nn.MaxPool2d(3, 2, ceil_mode = True)),\n\n            (\'conv2\', nn.Conv2d(28, 48, 3, 1)),\n            (\'prelu2\', nn.PReLU(48)),\n            (\'pool2\', nn.MaxPool2d(3, 2, ceil_mode = True)),\n\n            (\'conv3\', nn.Conv2d(48, 64, 2, 1)),\n            (\'prelu3\', nn.PReLU(64)),\n\n            (\'flatten\', Flatten()),\n            (\'conv4\', nn.Linear(576, 128)),\n            (\'prelu4\', nn.PReLU(128))\n        ]))\n\n        self.conv5_1 = nn.Linear(128, 2)\n        self.conv5_2 = nn.Linear(128, 4)\n\n        weights = np.load(""./rnet.npy"", allow_pickle=True)[()]\n        for n, p in self.named_parameters():\n            p.data = torch.FloatTensor(weights[n])\n\n    def forward(self, x):\n        """"""\n        Arguments:\n            x: a float tensor with shape [batch_size, 3, h, w].\n        Returns:\n            b: a float tensor with shape [batch_size, 4].\n            a: a float tensor with shape [batch_size, 2].\n        """"""\n        x = self.features(x)\n        a = self.conv5_1(x)\n        b = self.conv5_2(x)\n        a = F.softmax(a)\n        return b, a\n\n\nclass ONet(nn.Module):\n\n    def __init__(self):\n\n        super(ONet, self).__init__()\n\n        self.features = nn.Sequential(OrderedDict([\n            (\'conv1\', nn.Conv2d(3, 32, 3, 1)),\n            (\'prelu1\', nn.PReLU(32)),\n            (\'pool1\', nn.MaxPool2d(3, 2, ceil_mode = True)),\n\n            (\'conv2\', nn.Conv2d(32, 64, 3, 1)),\n            (\'prelu2\', nn.PReLU(64)),\n            (\'pool2\', nn.MaxPool2d(3, 2, ceil_mode = True)),\n\n            (\'conv3\', nn.Conv2d(64, 64, 3, 1)),\n            (\'prelu3\', nn.PReLU(64)),\n            (\'pool3\', nn.MaxPool2d(2, 2, ceil_mode = True)),\n\n            (\'conv4\', nn.Conv2d(64, 128, 2, 1)),\n            (\'prelu4\', nn.PReLU(128)),\n\n            (\'flatten\', Flatten()),\n            (\'conv5\', nn.Linear(1152, 256)),\n            (\'drop5\', nn.Dropout(0.25)),\n            (\'prelu5\', nn.PReLU(256)),\n        ]))\n\n        self.conv6_1 = nn.Linear(256, 2)\n        self.conv6_2 = nn.Linear(256, 4)\n        self.conv6_3 = nn.Linear(256, 10)\n\n        weights = np.load(""./onet.npy"", allow_pickle=True)[()]\n        for n, p in self.named_parameters():\n            p.data = torch.FloatTensor(weights[n])\n\n    def forward(self, x):\n        """"""\n        Arguments:\n            x: a float tensor with shape [batch_size, 3, h, w].\n        Returns:\n            c: a float tensor with shape [batch_size, 10].\n            b: a float tensor with shape [batch_size, 4].\n            a: a float tensor with shape [batch_size, 2].\n        """"""\n        x = self.features(x)\n        a = self.conv6_1(x)\n        b = self.conv6_2(x)\n        c = self.conv6_3(x)\n        a = F.softmax(a)\n        return c, b, a'"
align/matlab_cp2tform.py,0,"b'import numpy as np\nfrom numpy.linalg import inv, norm, lstsq\nfrom numpy.linalg import matrix_rank as rank\n\n\nclass MatlabCp2tormException(Exception):\n    def __str__(self):\n        return ""In File {}:{}"".format(\n                __file__, super.__str__(self))\n\ndef tformfwd(trans, uv):\n    """"""\n    Function:\n    ----------\n        apply affine transform \'trans\' to uv\n\n    Parameters:\n    ----------\n        @trans: 3x3 np.array\n            transform matrix\n        @uv: Kx2 np.array\n            each row is a pair of coordinates (x, y)\n\n    Returns:\n    ----------\n        @xy: Kx2 np.array\n            each row is a pair of transformed coordinates (x, y)\n    """"""\n    uv = np.hstack((\n        uv, np.ones((uv.shape[0], 1))\n    ))\n    xy = np.dot(uv, trans)\n    xy = xy[:, 0:-1]\n    return xy\n\n\ndef tforminv(trans, uv):\n    """"""\n    Function:\n    ----------\n        apply the inverse of affine transform \'trans\' to uv\n\n    Parameters:\n    ----------\n        @trans: 3x3 np.array\n            transform matrix\n        @uv: Kx2 np.array\n            each row is a pair of coordinates (x, y)\n\n    Returns:\n    ----------\n        @xy: Kx2 np.array\n            each row is a pair of inverse-transformed coordinates (x, y)\n    """"""\n    Tinv = inv(trans)\n    xy = tformfwd(Tinv, uv)\n    return xy\n\n\ndef findNonreflectiveSimilarity(uv, xy, options=None):\n\n    options = {\'K\': 2}\n\n    K = options[\'K\']\n    M = xy.shape[0]\n    x = xy[:, 0].reshape((-1, 1))  # use reshape to keep a column vector\n    y = xy[:, 1].reshape((-1, 1))  # use reshape to keep a column vector\n    # print(\'--->x, y:\\n\', x, y\n\n    tmp1 = np.hstack((x, y, np.ones((M, 1)), np.zeros((M, 1))))\n    tmp2 = np.hstack((y, -x, np.zeros((M, 1)), np.ones((M, 1))))\n    X = np.vstack((tmp1, tmp2))\n    # print(\'--->X.shape: \', X.shape\n    # print(\'X:\\n\', X\n\n    u = uv[:, 0].reshape((-1, 1))  # use reshape to keep a column vector\n    v = uv[:, 1].reshape((-1, 1))  # use reshape to keep a column vector\n    U = np.vstack((u, v))\n    # print(\'--->U.shape: \', U.shape\n    # print(\'U:\\n\', U\n\n    # We know that X * r = U\n    if rank(X) >= 2 * K:\n        r, _, _, _ = lstsq(X, U)\n        r = np.squeeze(r)\n    else:\n        raise Exception(""cp2tform: two Unique Points Req"")\n\n    # print(\'--->r:\\n\', r\n\n    sc = r[0]\n    ss = r[1]\n    tx = r[2]\n    ty = r[3]\n\n    Tinv = np.array([\n        [sc, -ss, 0],\n        [ss,  sc, 0],\n        [tx,  ty, 1]\n    ])\n\n    # print(\'--->Tinv:\\n\', Tinv\n\n    T = inv(Tinv)\n    # print(\'--->T:\\n\', T\n\n    T[:, 2] = np.array([0, 0, 1])\n\n    return T, Tinv\n\n\ndef findSimilarity(uv, xy, options=None):\n\n    options = {\'K\': 2}\n\n#    uv = np.array(uv)\n#    xy = np.array(xy)\n\n    # Solve for trans1\n    trans1, trans1_inv = findNonreflectiveSimilarity(uv, xy, options)\n\n    # Solve for trans2\n\n    # manually reflect the xy data across the Y-axis\n    xyR = xy\n    xyR[:, 0] = -1 * xyR[:, 0]\n\n    trans2r, trans2r_inv = findNonreflectiveSimilarity(uv, xyR, options)\n\n    # manually reflect the tform to undo the reflection done on xyR\n    TreflectY = np.array([\n        [-1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1]\n    ])\n\n    trans2 = np.dot(trans2r, TreflectY)\n\n    # Figure out if trans1 or trans2 is better\n    xy1 = tformfwd(trans1, uv)\n    norm1 = norm(xy1 - xy)\n\n    xy2 = tformfwd(trans2, uv)\n    norm2 = norm(xy2 - xy)\n\n    if norm1 <= norm2:\n        return trans1, trans1_inv\n    else:\n        trans2_inv = inv(trans2)\n        return trans2, trans2_inv\n\n\ndef get_similarity_transform(src_pts, dst_pts, reflective = True):\n    """"""\n    Function:\n    ----------\n        Find Similarity Transform Matrix \'trans\':\n            u = src_pts[:, 0]\n            v = src_pts[:, 1]\n            x = dst_pts[:, 0]\n            y = dst_pts[:, 1]\n            [x, y, 1] = [u, v, 1] * trans\n\n    Parameters:\n    ----------\n        @src_pts: Kx2 np.array\n            source points, each row is a pair of coordinates (x, y)\n        @dst_pts: Kx2 np.array\n            destination points, each row is a pair of transformed\n            coordinates (x, y)\n        @reflective: True or False\n            if True:\n                use reflective similarity transform\n            else:\n                use non-reflective similarity transform\n\n    Returns:\n    ----------\n       @trans: 3x3 np.array\n            transform matrix from uv to xy\n        trans_inv: 3x3 np.array\n            inverse of trans, transform matrix from xy to uv\n    """"""\n\n    if reflective:\n        trans, trans_inv = findSimilarity(src_pts, dst_pts)\n    else:\n        trans, trans_inv = findNonreflectiveSimilarity(src_pts, dst_pts)\n\n    return trans, trans_inv\n\n\ndef cvt_tform_mat_for_cv2(trans):\n    """"""\n    Function:\n    ----------\n        Convert Transform Matrix \'trans\' into \'cv2_trans\' which could be\n        directly used by cv2.warpAffine():\n            u = src_pts[:, 0]\n            v = src_pts[:, 1]\n            x = dst_pts[:, 0]\n            y = dst_pts[:, 1]\n            [x, y].T = cv_trans * [u, v, 1].T\n\n    Parameters:\n    ----------\n        @trans: 3x3 np.array\n            transform matrix from uv to xy\n\n    Returns:\n    ----------\n        @cv2_trans: 2x3 np.array\n            transform matrix from src_pts to dst_pts, could be directly used\n            for cv2.warpAffine()\n    """"""\n    cv2_trans = trans[:, 0:2].T\n\n    return cv2_trans\n\n\ndef get_similarity_transform_for_cv2(src_pts, dst_pts, reflective = True):\n    """"""\n    Function:\n    ----------\n        Find Similarity Transform Matrix \'cv2_trans\' which could be\n        directly used by cv2.warpAffine():\n            u = src_pts[:, 0]\n            v = src_pts[:, 1]\n            x = dst_pts[:, 0]\n            y = dst_pts[:, 1]\n            [x, y].T = cv_trans * [u, v, 1].T\n\n    Parameters:\n    ----------\n        @src_pts: Kx2 np.array\n            source points, each row is a pair of coordinates (x, y)\n        @dst_pts: Kx2 np.array\n            destination points, each row is a pair of transformed\n            coordinates (x, y)\n        reflective: True or False\n            if True:\n                use reflective similarity transform\n            else:\n                use non-reflective similarity transform\n\n    Returns:\n    ----------\n        @cv2_trans: 2x3 np.array\n            transform matrix from src_pts to dst_pts, could be directly used\n            for cv2.warpAffine()\n    """"""\n    trans, trans_inv = get_similarity_transform(src_pts, dst_pts, reflective)\n    cv2_trans = cvt_tform_mat_for_cv2(trans)\n\n    return cv2_trans\n\n\nif __name__ == \'__main__\':\n    """"""\n    u = [0, 6, -2]\n    v = [0, 3, 5]\n    x = [-1, 0, 4]\n    y = [-1, -10, 4]\n\n    # In Matlab, run:\n    #\n    #   uv = [u\'; v\'];\n    #   xy = [x\'; y\'];\n    #   tform_sim=cp2tform(uv,xy,\'similarity\');\n    #\n    #   trans = tform_sim.tdata.T\n    #   ans =\n    #       -0.0764   -1.6190         0\n    #        1.6190   -0.0764         0\n    #       -3.2156    0.0290    1.0000\n    #   trans_inv = tform_sim.tdata.Tinv\n    #    ans =\n    #\n    #       -0.0291    0.6163         0\n    #       -0.6163   -0.0291         0\n    #       -0.0756    1.9826    1.0000\n    #    xy_m=tformfwd(tform_sim, u,v)\n    #\n    #    xy_m =\n    #\n    #       -3.2156    0.0290\n    #        1.1833   -9.9143\n    #        5.0323    2.8853\n    #    uv_m=tforminv(tform_sim, x,y)\n    #\n    #    uv_m =\n    #\n    #        0.5698    1.3953\n    #        6.0872    2.2733\n    #       -2.6570    4.3314\n    """"""\n    u = [0, 6, -2]\n    v = [0, 3, 5]\n    x = [-1, 0, 4]\n    y = [-1, -10, 4]\n\n    uv = np.array((u, v)).T\n    xy = np.array((x, y)).T\n\n    print(""\\n--->uv:"")\n    print(uv)\n    print(""\\n--->xy:"")\n    print(xy)\n\n    trans, trans_inv = get_similarity_transform(uv, xy)\n\n    print(""\\n--->trans matrix:"")\n    print(trans)\n\n    print(""\\n--->trans_inv matrix:"")\n    print(trans_inv)\n\n    print(""\\n---> apply transform to uv"")\n    print(""\\nxy_m = uv_augmented * trans"")\n    uv_aug = np.hstack((\n        uv, np.ones((uv.shape[0], 1))\n    ))\n    xy_m = np.dot(uv_aug, trans)\n    print(xy_m)\n\n    print(""\\nxy_m = tformfwd(trans, uv)"")\n    xy_m = tformfwd(trans, uv)\n    print(xy_m)\n\n    print(""\\n---> apply inverse transform to xy"")\n    print(""\\nuv_m = xy_augmented * trans_inv"")\n    xy_aug = np.hstack((\n        xy, np.ones((xy.shape[0], 1))\n    ))\n    uv_m = np.dot(xy_aug, trans_inv)\n    print(uv_m)\n\n    print(""\\nuv_m = tformfwd(trans_inv, xy)"")\n    uv_m = tformfwd(trans_inv, xy)\n    print(uv_m)\n\n    uv_m = tforminv(trans, xy)\n    print(""\\nuv_m = tforminv(trans, xy)"")\n    print(uv_m)'"
align/visualization_utils.py,0,"b'from PIL import ImageDraw\n\n\ndef show_results(img, bounding_boxes, facial_landmarks = []):\n    """"""Draw bounding boxes and facial landmarks.\n    Arguments:\n        img: an instance of PIL.Image.\n        bounding_boxes: a float numpy array of shape [n, 5].\n        facial_landmarks: a float numpy array of shape [n, 10].\n    Returns:\n        an instance of PIL.Image.\n    """"""\n    img_copy = img.copy()\n    draw = ImageDraw.Draw(img_copy)\n\n    for b in bounding_boxes:\n        draw.rectangle([\n            (b[0], b[1]), (b[2], b[3])\n        ], outline = \'white\')\n\n    inx = 0\n    for p in facial_landmarks:\n        for i in range(5):\n            draw.ellipse([\n                (p[i] - 1.0, p[i + 5] - 1.0),\n                (p[i] + 1.0, p[i + 5] + 1.0)\n            ], outline = \'blue\')\n\n    return img_copy'"
backbone/__init__.py,0,b'\n'
backbone/model_irse.py,4,"b'import torch\nimport torch.nn as nn\nfrom torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, PReLU, ReLU, Sigmoid, Dropout, MaxPool2d, \\\n    AdaptiveAvgPool2d, Sequential, Module\nfrom collections import namedtuple\n\n\n# Support: [\'IR_50\', \'IR_101\', \'IR_152\', \'IR_SE_50\', \'IR_SE_101\', \'IR_SE_152\']\n\n\nclass Flatten(Module):\n    def forward(self, input):\n        return input.view(input.size(0), -1)\n\n\ndef l2_norm(input, axis=1):\n    norm = torch.norm(input, 2, axis, True)\n    output = torch.div(input, norm)\n\n    return output\n\n\nclass SEModule(Module):\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        self.avg_pool = AdaptiveAvgPool2d(1)\n        self.fc1 = Conv2d(\n            channels, channels // reduction, kernel_size=1, padding=0, bias=False)\n\n        nn.init.xavier_uniform_(self.fc1.weight.data)\n\n        self.relu = ReLU(inplace=True)\n        self.fc2 = Conv2d(\n            channels // reduction, channels, kernel_size=1, padding=0, bias=False)\n\n        self.sigmoid = Sigmoid()\n\n    def forward(self, x):\n        module_input = x\n        x = self.avg_pool(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n\n        return module_input * x\n\n\nclass bottleneck_IR(Module):\n    def __init__(self, in_channel, depth, stride):\n        super(bottleneck_IR, self).__init__()\n        if in_channel == depth:\n            self.shortcut_layer = MaxPool2d(1, stride)\n        else:\n            self.shortcut_layer = Sequential(\n                Conv2d(in_channel, depth, (1, 1), stride, bias=False), BatchNorm2d(depth))\n        self.res_layer = Sequential(\n            BatchNorm2d(in_channel),\n            Conv2d(in_channel, depth, (3, 3), (1, 1), 1, bias=False), PReLU(depth),\n            Conv2d(depth, depth, (3, 3), stride, 1, bias=False), BatchNorm2d(depth))\n\n    def forward(self, x):\n        shortcut = self.shortcut_layer(x)\n        res = self.res_layer(x)\n\n        return res + shortcut\n\n\nclass bottleneck_IR_SE(Module):\n    def __init__(self, in_channel, depth, stride):\n        super(bottleneck_IR_SE, self).__init__()\n        if in_channel == depth:\n            self.shortcut_layer = MaxPool2d(1, stride)\n        else:\n            self.shortcut_layer = Sequential(\n                Conv2d(in_channel, depth, (1, 1), stride, bias=False),\n                BatchNorm2d(depth))\n        self.res_layer = Sequential(\n            BatchNorm2d(in_channel),\n            Conv2d(in_channel, depth, (3, 3), (1, 1), 1, bias=False),\n            PReLU(depth),\n            Conv2d(depth, depth, (3, 3), stride, 1, bias=False),\n            BatchNorm2d(depth),\n            SEModule(depth, 16)\n        )\n\n    def forward(self, x):\n        shortcut = self.shortcut_layer(x)\n        res = self.res_layer(x)\n\n        return res + shortcut\n\n\nclass Bottleneck(namedtuple(\'Block\', [\'in_channel\', \'depth\', \'stride\'])):\n    \'\'\'A named tuple describing a ResNet block.\'\'\'\n\n\ndef get_block(in_channel, depth, num_units, stride=2):\n\n    return [Bottleneck(in_channel, depth, stride)] + [Bottleneck(depth, depth, 1) for i in range(num_units - 1)]\n\n\ndef get_blocks(num_layers):\n    if num_layers == 50:\n        blocks = [\n            get_block(in_channel=64, depth=64, num_units=3),\n            get_block(in_channel=64, depth=128, num_units=4),\n            get_block(in_channel=128, depth=256, num_units=14),\n            get_block(in_channel=256, depth=512, num_units=3)\n        ]\n    elif num_layers == 100:\n        blocks = [\n            get_block(in_channel=64, depth=64, num_units=3),\n            get_block(in_channel=64, depth=128, num_units=13),\n            get_block(in_channel=128, depth=256, num_units=30),\n            get_block(in_channel=256, depth=512, num_units=3)\n        ]\n    elif num_layers == 152:\n        blocks = [\n            get_block(in_channel=64, depth=64, num_units=3),\n            get_block(in_channel=64, depth=128, num_units=8),\n            get_block(in_channel=128, depth=256, num_units=36),\n            get_block(in_channel=256, depth=512, num_units=3)\n        ]\n\n    return blocks\n\n\nclass Backbone(Module):\n    def __init__(self, input_size, num_layers, mode=\'ir\'):\n        super(Backbone, self).__init__()\n        assert input_size[0] in [112, 224], ""input_size should be [112, 112] or [224, 224]""\n        assert num_layers in [50, 100, 152], ""num_layers should be 50, 100 or 152""\n        assert mode in [\'ir\', \'ir_se\'], ""mode should be ir or ir_se""\n        blocks = get_blocks(num_layers)\n        if mode == \'ir\':\n            unit_module = bottleneck_IR\n        elif mode == \'ir_se\':\n            unit_module = bottleneck_IR_SE\n        self.input_layer = Sequential(Conv2d(3, 64, (3, 3), 1, 1, bias=False),\n                                      BatchNorm2d(64),\n                                      PReLU(64))\n        if input_size[0] == 112:\n            self.output_layer = Sequential(BatchNorm2d(512),\n                                           Dropout(),\n                                           Flatten(),\n                                           Linear(512 * 7 * 7, 512),\n                                           BatchNorm1d(512))\n        else:\n            self.output_layer = Sequential(BatchNorm2d(512),\n                                           Dropout(),\n                                           Flatten(),\n                                           Linear(512 * 14 * 14, 512),\n                                           BatchNorm1d(512))\n\n        modules = []\n        for block in blocks:\n            for bottleneck in block:\n                modules.append(\n                    unit_module(bottleneck.in_channel,\n                                bottleneck.depth,\n                                bottleneck.stride))\n        self.body = Sequential(*modules)\n\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.input_layer(x)\n        x = self.body(x)\n        x = self.output_layer(x)\n\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_uniform_(m.weight.data)\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm1d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight.data)\n                if m.bias is not None:\n                    m.bias.data.zero_()\n\n\ndef IR_50(input_size):\n    """"""Constructs a ir-50 model.\n    """"""\n    model = Backbone(input_size, 50, \'ir\')\n\n    return model\n\n\ndef IR_101(input_size):\n    """"""Constructs a ir-101 model.\n    """"""\n    model = Backbone(input_size, 100, \'ir\')\n\n    return model\n\n\ndef IR_152(input_size):\n    """"""Constructs a ir-152 model.\n    """"""\n    model = Backbone(input_size, 152, \'ir\')\n\n    return model\n\n\ndef IR_SE_50(input_size):\n    """"""Constructs a ir_se-50 model.\n    """"""\n    model = Backbone(input_size, 50, \'ir_se\')\n\n    return model\n\n\ndef IR_SE_101(input_size):\n    """"""Constructs a ir_se-101 model.\n    """"""\n    model = Backbone(input_size, 100, \'ir_se\')\n\n    return model\n\n\ndef IR_SE_152(input_size):\n    """"""Constructs a ir_se-152 model.\n    """"""\n    model = Backbone(input_size, 152, \'ir_se\')\n\n    return model\n'"
backbone/model_resnet.py,2,"b'import torch.nn as nn\nfrom torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, ReLU, Dropout, MaxPool2d, Sequential, Module\n\n\n# Support: [\'ResNet_50\', \'ResNet_101\', \'ResNet_152\']\n\n\ndef conv3x3(in_planes, out_planes, stride = 1):\n    """"""3x3 convolution with padding""""""\n\n    return Conv2d(in_planes, out_planes, kernel_size = 3, stride = stride,\n                     padding = 1, bias = False)\n\n\ndef conv1x1(in_planes, out_planes, stride = 1):\n    """"""1x1 convolution""""""\n\n    return Conv2d(in_planes, out_planes, kernel_size = 1, stride = stride, bias = False)\n\n\nclass BasicBlock(Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride = 1, downsample = None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = BatchNorm2d(planes)\n        self.relu = ReLU(inplace = True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride = 1, downsample = None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = conv1x1(inplanes, planes)\n        self.bn1 = BatchNorm2d(planes)\n        self.conv2 = conv3x3(planes, planes, stride)\n        self.bn2 = BatchNorm2d(planes)\n        self.conv3 = conv1x1(planes, planes * self.expansion)\n        self.bn3 = BatchNorm2d(planes * self.expansion)\n        self.relu = ReLU(inplace = True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(Module):\n\n    def __init__(self, input_size, block, layers, zero_init_residual = True):\n        super(ResNet, self).__init__()\n        assert input_size[0] in [112, 224], ""input_size should be [112, 112] or [224, 224]""\n        self.inplanes = 64\n        self.conv1 = Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3, bias = False)\n        self.bn1 = BatchNorm2d(64)\n        self.relu = ReLU(inplace = True)\n        self.maxpool = MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride = 2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride = 2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride = 2)\n\n        self.bn_o1 = BatchNorm2d(2048)\n        self.dropout = Dropout()\n        if input_size[0] == 112:\n            self.fc = Linear(2048 * 4 * 4, 512)\n        else:\n            self.fc = Linear(2048 * 8 * 8, 512)\n        self.bn_o2 = BatchNorm1d(512)\n\n        for m in self.modules():\n            if isinstance(m, Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode = \'fan_out\', nonlinearity = \'relu\')\n            elif isinstance(m, BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride = 1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.bn_o1(x)\n        x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        x = self.bn_o2(x)\n\n        return x\n\n\ndef ResNet_50(input_size, **kwargs):\n    """"""Constructs a ResNet-50 model.\n    """"""\n    model = ResNet(input_size, Bottleneck, [3, 4, 6, 3], **kwargs)\n\n    return model\n\n\ndef ResNet_101(input_size, **kwargs):\n    """"""Constructs a ResNet-101 model.\n    """"""\n    model = ResNet(input_size, Bottleneck, [3, 4, 23, 3], **kwargs)\n\n    return model\n\n\ndef ResNet_152(input_size, **kwargs):\n    """"""Constructs a ResNet-152 model.\n    """"""\n    model = ResNet(input_size, Bottleneck, [3, 8, 36, 3], **kwargs)\n\n    return model\n'"
backup/__init__.py,0,b'\n'
backup/config.py,1,"b'import torch\n\n\nconfigurations = {\n    1: dict(\n        SEED = 1337, # random seed for reproduce results\n\n        DATA_ROOT = \'/media/pc/6T/jasonjzhao/data/faces_emore\', # the parent root where your train/val/test data are stored\n        MODEL_ROOT = \'/media/pc/6T/jasonjzhao/buffer/model_ir152\', # the root to buffer your checkpoints\n        LOG_ROOT = \'/media/pc/6T/jasonjzhao/buffer/log_ir152\', # the root to log your train/val status\n        BACKBONE_RESUME_ROOT = \'./\', # the root to resume training from a saved checkpoint\n        HEAD_RESUME_ROOT = \'./\', # the root to resume training from a saved checkpoint\n\n        BACKBONE_NAME = \'IR_152\', # support: [\'ResNet_50\', \'ResNet_101\', \'ResNet_152\', \'IR_50\', \'IR_101\', \'IR_152\', \'IR_SE_50\', \'IR_SE_101\', \'IR_SE_152\']\n        HEAD_NAME = \'CosFace\', # support:  [\'Softmax\', \'ArcFace\', \'CosFace\', \'SphereFace\', \'Am_softmax\']\n        LOSS_NAME = \'Softmax\', # support: [\'Focal\', \'Softmax\']\n\n        INPUT_SIZE = [112, 112], # support: [112, 112] and [224, 224]\n        RGB_MEAN = [0.5, 0.5, 0.5], # for normalize inputs to [-1, 1]\n        RGB_STD = [0.5, 0.5, 0.5],\n        EMBEDDING_SIZE = 512, # feature dimension\n        BATCH_SIZE = 384,\n        DROP_LAST = True, # whether drop the last batch to ensure consistent batch_norm statistics\n        LR = 0.1, # initial LR\n        NUM_EPOCH = 200, # total epoch number (use the firt 1/25 epochs to warm up)\n        WEIGHT_DECAY = 5e-4, # do not apply to batch_norm parameters\n        MOMENTUM = 0.9,\n        STAGES = [100000, 140000, 160000], # batch stages to decay learning rate\n\n        DEVICE = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu""),\n        MULTI_GPU = True, # flag to use multiple GPUs; if you choose to train with single GPU, you should first run ""export CUDA_VISILE_DEVICES=device_id"" to specify the GPU card you want to use\n        GPU_ID = [0, 1, 2], # specify your GPU ids\n        PIN_MEMORY = True,\n        NUM_WORKERS = 0,\n),\n}\n'"
backup/data_pipe.py,0,"b""from PIL import Image, ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\nimport numpy as np\nimport cv2\nimport bcolz\nimport pickle\nimport mxnet as mx\nfrom tqdm import tqdm\n\n\ndef load_bin(path, rootdir, transform, image_size = [112, 112]):\n    if not rootdir.exists():\n        rootdir.mkdir()\n    bins, issame_list = pickle.load(open(path, 'rb'), encoding='bytes')\n    data = bcolz.fill([len(bins), 3, image_size[0], image_size[1]], dtype = np.float32, rootdir = rootdir, mode = 'w')\n    for i in range(len(bins)):\n        _bin = bins[i]\n        img = mx.image.imdecode(_bin).asnumpy()\n        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n        img = Image.fromarray(img.astype(np.uint8))\n        data[i, ...] = transform(img)\n        i += 1\n        if i % 1000 == 0:\n            print('loading bin', i)\n    print(data.shape)\n    np.save(str(rootdir) + '_list', np.array(issame_list))\n    return data, issame_list\n\n\ndef load_mx_rec(rec_path):\n    save_path = rec_path/'imgs'\n    if not save_path.exists():\n        save_path.mkdir()\n    imgrec = mx.recordio.MXIndexedRecordIO(str(rec_path/'train.idx'), str(rec_path/'train.rec'), 'r')\n    img_info = imgrec.read_idx(0)\n    header,_ = mx.recordio.unpack(img_info)\n    max_idx = int(header.label[0])\n    for idx in tqdm(range(1, max_idx)):\n        img_info = imgrec.read_idx(idx)\n        header, img = mx.recordio.unpack_img(img_info)\n        label = int(header.label)\n        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n        img = Image.fromarray(img)\n        label_path = save_path/str(label)\n        if not label_path.exists():\n            label_path.mkdir()\n        img.save(label_path/'{}.jpg'.format(idx), quality = 95)"""
backup/metrics.py,22,"b'from __future__ import print_function\r\nfrom __future__ import division\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.nn import Parameter\r\nimport math\r\n\r\n\r\n# Support: [\'Softmax\', \'ArcFace\', \'CosFace\', \'SphereFace\', \'Am_softmax\']\r\n\r\n\r\nclass Softmax(nn.Module):\r\n    r""""""Implement of Softmax (normal classification head):\r\n        Args:\r\n            in_features: size of each input sample\r\n            out_features: size of each output sample\r\n        """"""\r\n    def __init__(self, in_features, out_features):\r\n        super(Softmax, self).__init__()\r\n        self.in_features = in_features\r\n        self.out_features = out_features\r\n        self.fc = nn.Linear(self.in_features, self.out_features)\r\n\r\n        self._initialize_weights()\r\n\r\n    def forward(self, x):\r\n        out = self.fc(x)\r\n\r\n        return out\r\n\r\n    def _initialize_weights(self):\r\n        for m in self.modules():\r\n            if isinstance(m, nn.Conv2d):\r\n                nn.init.xavier_uniform_(m.weight.data)\r\n                if m.bias is not None:\r\n                    m.bias.data.zero_()\r\n            elif isinstance(m, nn.BatchNorm2d):\r\n                m.weight.data.fill_(1)\r\n                m.bias.data.zero_()\r\n            elif isinstance(m, nn.BatchNorm1d):\r\n                m.weight.data.fill_(1)\r\n                m.bias.data.zero_()\r\n            elif isinstance(m, nn.Linear):\r\n                nn.init.xavier_uniform_(m.weight.data)\r\n                if m.bias is not None:\r\n                    m.bias.data.zero_()\r\n\r\n\r\nclass ArcFace(nn.Module):\r\n    r""""""Implement of ArcFace (https://arxiv.org/pdf/1801.07698v1.pdf):\r\n        Args:\r\n            in_features: size of each input sample\r\n            out_features: size of each output sample\r\n            s: norm of input feature\r\n            m: margin\r\n            cos(theta+m)\r\n        """"""\r\n    def __init__(self, in_features, out_features, s = 64.0, m = 0.50, easy_margin = False): # s = 30.0\r\n        super(ArcFace, self).__init__()\r\n        self.in_features = in_features\r\n        self.out_features = out_features\r\n        self.s = s\r\n        self.m = m\r\n        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\r\n        nn.init.xavier_uniform_(self.weight)\r\n\r\n        self.easy_margin = easy_margin\r\n        self.cos_m = math.cos(m)\r\n        self.sin_m = math.sin(m)\r\n        self.th = math.cos(math.pi - m)\r\n        self.mm = math.sin(math.pi - m) * m\r\n\r\n    def forward(self, input, label):\r\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\r\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\r\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\r\n        phi = cosine * self.cos_m - sine * self.sin_m\r\n        if self.easy_margin:\r\n            phi = torch.where(cosine > 0, phi, cosine)\r\n        else:\r\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\r\n        # --------------------------- convert label to one-hot ---------------------------\r\n        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device=\'cuda\')\r\n        one_hot = torch.zeros(cosine.size(), device = \'cuda\')\r\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\r\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\r\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\r\n        output *= self.s\r\n\r\n        return output\r\n\r\n\r\nclass CosFace(nn.Module):\r\n    r""""""Implement of CosFace (https://arxiv.org/pdf/1801.09414.pdf):\r\n    Args:\r\n        in_features: size of each input sample\r\n        out_features: size of each output sample\r\n        s: norm of input feature\r\n        m: margin\r\n        cos(theta)-m\r\n    """"""\r\n    def __init__(self, in_features, out_features, s = 64.0, m = 0.35): # s = 30.0, m = 0.40\r\n        super(CosFace, self).__init__()\r\n        self.in_features = in_features\r\n        self.out_features = out_features\r\n        self.s = s\r\n        self.m = m\r\n        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\r\n        nn.init.xavier_uniform_(self.weight)\r\n\r\n    def forward(self, input, label):\r\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\r\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\r\n        phi = cosine - self.m\r\n        # --------------------------- convert label to one-hot ---------------------------\r\n        one_hot = torch.zeros(cosine.size(), device = \'cuda\')\r\n        # one_hot = one_hot.cuda() if cosine.is_cuda else one_hot\r\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\r\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\r\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\r\n        output *= self.s\r\n\r\n        return output\r\n\r\n    def __repr__(self):\r\n        return self.__class__.__name__ + \'(\' \\\r\n               + \'in_features = \' + str(self.in_features) \\\r\n               + \', out_features = \' + str(self.out_features) \\\r\n               + \', s = \' + str(self.s) \\\r\n               + \', m = \' + str(self.m) + \')\'\r\n\r\n\r\nclass SphereFace(nn.Module):\r\n    r""""""Implement of SphereFace (https://arxiv.org/pdf/1704.08063.pdf):\r\n    Args:\r\n        in_features: size of each input sample\r\n        out_features: size of each output sample\r\n        m: margin\r\n        cos(m*theta)\r\n    """"""\r\n    def __init__(self, in_features, out_features, m = 4.0):\r\n        super(SphereFace, self).__init__()\r\n        self.in_features = in_features\r\n        self.out_features = out_features\r\n        self.m = m\r\n        self.base = 1000.0\r\n        self.gamma = 0.12\r\n        self.power = 1\r\n        self.LambdaMin = 5.0\r\n        self.iter = 0\r\n        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\r\n        nn.init.xavier_uniform_(self.weight)\r\n\r\n        # duplication formula\r\n        self.mlambda = [\r\n            lambda x: x ** 0,\r\n            lambda x: x ** 1,\r\n            lambda x: 2 * x ** 2 - 1,\r\n            lambda x: 4 * x ** 3 - 3 * x,\r\n            lambda x: 8 * x ** 4 - 8 * x ** 2 + 1,\r\n            lambda x: 16 * x ** 5 - 20 * x ** 3 + 5 * x\r\n        ]\r\n\r\n    def forward(self, input, label):\r\n        # lambda = max(lambda_min,base*(1+gamma*iteration)^(-power))\r\n        self.iter += 1\r\n        self.lamb = max(self.LambdaMin, self.base * (1 + self.gamma * self.iter) ** (-1 * self.power))\r\n\r\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\r\n        cos_theta = F.linear(F.normalize(input), F.normalize(self.weight))\r\n        cos_theta = cos_theta.clamp(-1, 1)\r\n        cos_m_theta = self.mlambda[self.m](cos_theta)\r\n        theta = cos_theta.data.acos()\r\n        k = (self.m * theta / 3.14159265).floor()\r\n        phi_theta = ((-1.0) ** k) * cos_m_theta - 2 * k\r\n        NormOfFeature = torch.norm(input, 2, 1)\r\n\r\n        # --------------------------- convert label to one-hot ---------------------------\r\n        one_hot = torch.zeros(cos_theta.size())\r\n        one_hot = one_hot.cuda() if cos_theta.is_cuda else one_hot\r\n        one_hot.scatter_(1, label.view(-1, 1), 1)\r\n\r\n        # --------------------------- Calculate output ---------------------------\r\n        output = (one_hot * (phi_theta - cos_theta) / (1 + self.lamb)) + cos_theta\r\n        output *= NormOfFeature.view(-1, 1)\r\n\r\n        return output\r\n\r\n    def __repr__(self):\r\n        return self.__class__.__name__ + \'(\' \\\r\n               + \'in_features = \' + str(self.in_features) \\\r\n               + \', out_features = \' + str(self.out_features) \\\r\n               + \', m = \' + str(self.m) + \')\'\r\n\r\n\r\ndef l2_norm(input, axis = 1):\r\n    norm = torch.norm(input, 2, axis, True)\r\n    output = torch.div(input, norm)\r\n\r\n    return output\r\n\r\n\r\nclass Am_softmax(nn.Module):\r\n    r""""""Implement of Am_softmax (https://arxiv.org/pdf/1704.06369.pdf):\r\n    Args:\r\n        in_features: size of each input sample\r\n        out_features: size of each output sample\r\n        m: margin\r\n        s: scale of outputs\r\n    """"""\r\n    def __init__(self, in_features, out_features, m = 0.35, s = 30.0):\r\n        super(Am_softmax, self).__init__()\r\n        self.in_features = in_features\r\n        self.out_features = out_features\r\n        self.kernel = Parameter(torch.Tensor(self.in_features, self.out_features))\r\n        self.kernel.data.uniform_(-1, 1).renorm_(2, 1, 1e-5).mul_(1e5)  # initialize kernel\r\n        self.m = m\r\n        self.s = s\r\n\r\n    def forward(self, embbedings, label):\r\n        kernel_norm = l2_norm(self.kernel, axis = 0)\r\n        cos_theta = torch.mm(embbedings, kernel_norm)\r\n        cos_theta = cos_theta.clamp(-1, 1)  # for numerical stability\r\n        phi = cos_theta - self.m\r\n        label = label.view(-1, 1)  # size=(B,1)\r\n        index = cos_theta.data * 0.0  # size=(B,Classnum)\r\n        index.scatter_(1, label.data.view(-1, 1), 1)\r\n        index = index.byte()\r\n        output = cos_theta * 1.0\r\n        output[index] = phi[index]  # only change the correct predicted output\r\n        output *= self.s  # scale up in order to make softmax work, first introduced in normface\r\n\r\n        return output\r\n'"
backup/prepare_data.py,0,"b'from config import get_config\nfrom data.data_pipe import load_bin, load_mx_rec\nimport argparse\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description = \'for extracting faces_emore data\')\n    parser.add_argument(""-r"", ""--rec_path"", help=""mxnet record file path"",default = \'faces_emore\', type = str)\n    args = parser.parse_args()\n    conf = get_config()\n    rec_path = conf.data_path/args.rec_path\n    load_mx_rec(rec_path)\n    \n    bin_files = [\'agedb_30\', \'cfp_fp\', \'lfw\', \'calfw\', \'cfp_ff\', \'cplfw\', \'vgg2_fp\']\n    \n    for i in range(len(bin_files)):\n        load_bin(rec_path/(bin_files[i] + \'.bin\'), rec_path/bin_files[i], conf.test_transform)'"
backup/train.py,13,"b'import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n\nfrom config import configurations\nfrom backbone.model_resnet import ResNet_50, ResNet_101, ResNet_152\nfrom backbone.model_irse import IR_50, IR_101, IR_152, IR_SE_50, IR_SE_101, IR_SE_152\nfrom head.metrics import ArcFace, CosFace, SphereFace, Am_softmax\nfrom loss.focal import FocalLoss\nfrom util.utils import make_weights_for_balanced_classes, get_val_data, separate_irse_bn_paras, \\\n    separate_resnet_bn_paras, warm_up_lr, schedule_lr, perform_val, get_time, buffer_val, AverageMeter, accuracy, \\\n    save_checkpoint\n\nfrom tensorboardX import SummaryWriter\nfrom tqdm import tqdm\nimport os\n\nif __name__ == \'__main__\':\n\n    # ======= hyperparameters & data loaders =======#\n    cfg = configurations[1]\n\n    SEED = cfg[\'SEED\']  # random seed for reproduce results\n    torch.manual_seed(SEED)\n\n    DATA_ROOT = cfg[\'DATA_ROOT\']  # the parent root where your train/val/test data are stored\n    MODEL_ROOT = cfg[\'MODEL_ROOT\']  # the root to buffer your checkpoints\n    LOG_ROOT = cfg[\'LOG_ROOT\']  # the root to log your train/val status\n    BACKBONE_RESUME_ROOT = cfg[\'BACKBONE_RESUME_ROOT\']  # the root to resume training from a saved checkpoint\n    HEAD_RESUME_ROOT = cfg[\'HEAD_RESUME_ROOT\']  # the root to resume training from a saved checkpoint\n\n    BACKBONE_NAME = cfg[\n        \'BACKBONE_NAME\']  # support: [\'ResNet_50\', \'ResNet_101\', \'ResNet_152\', \'IR_50\', \'IR_101\', \'IR_152\', \'IR_SE_50\', \'IR_SE_101\', \'IR_SE_152\']\n    HEAD_NAME = cfg[\'HEAD_NAME\']  # support:  [\'Softmax\', \'ArcFace\', \'CosFace\', \'SphereFace\', \'Am_softmax\']\n    LOSS_NAME = cfg[\'LOSS_NAME\']  # support: [\'Focal\', \'Softmax\']\n\n    INPUT_SIZE = cfg[\'INPUT_SIZE\']\n    RGB_MEAN = cfg[\'RGB_MEAN\']  # for normalize inputs\n    RGB_STD = cfg[\'RGB_STD\']\n    EMBEDDING_SIZE = cfg[\'EMBEDDING_SIZE\']  # feature dimension\n    BATCH_SIZE = cfg[\'BATCH_SIZE\']\n    DROP_LAST = cfg[\'DROP_LAST\']  # whether drop the last batch to ensure consistent batch_norm statistics\n    LR = cfg[\'LR\']  # initial LR\n    NUM_EPOCH = cfg[\'NUM_EPOCH\']\n    WEIGHT_DECAY = cfg[\'WEIGHT_DECAY\']\n    MOMENTUM = cfg[\'MOMENTUM\']\n    STAGES = cfg[\'STAGES\']  # epoch stages to decay learning rate\n\n    DEVICE = cfg[\'DEVICE\']\n    MULTI_GPU = cfg[\'MULTI_GPU\']  # flag to use multiple GPUs\n    GPU_ID = cfg[\'GPU_ID\']  # specify your GPU ids\n    PIN_MEMORY = cfg[\'PIN_MEMORY\']\n    NUM_WORKERS = cfg[\'NUM_WORKERS\']\n    print(""="" * 60)\n    print(""Overall Configurations:"")\n    print(cfg)\n    print(""="" * 60)\n\n    writer = SummaryWriter(LOG_ROOT)  # writer for buffering intermedium results\n\n    train_transform = transforms.Compose([\n        # refer to https://pytorch.org/docs/stable/torchvision/transforms.html for more build-in online data augmentation\n        transforms.Resize([112, 112]),  # smaller side resized\n        # transforms.RandomCrop([INPUT_SIZE[0], INPUT_SIZE[1]]),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=RGB_MEAN,\n                             std=RGB_STD),\n    ])\n\n    dataset_train = datasets.ImageFolder(os.path.join(DATA_ROOT, \'imgs\'), train_transform)\n\n    # create a weighted random sampler to process imbalanced data\n    weights = make_weights_for_balanced_classes(dataset_train.imgs, len(dataset_train.classes))\n    weights = torch.DoubleTensor(weights)\n    sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights))\n\n    train_loader = torch.utils.data.DataLoader(\n        dataset_train, batch_size=BATCH_SIZE, sampler=sampler, pin_memory=PIN_MEMORY,\n        num_workers=NUM_WORKERS, drop_last=DROP_LAST\n    )\n\n    NUM_CLASS = len(train_loader.dataset.classes)\n    print(""Number of Training Classes: {}"".format(NUM_CLASS))\n\n    lfw, cfp_ff, cfp_fp, agedb, calfw, cplfw, vgg2_fp, lfw_issame, cfp_ff_issame, cfp_fp_issame, agedb_issame, calfw_issame, cplfw_issame, vgg2_fp_issame = get_val_data(DATA_ROOT)\n\n    # ======= model & loss & optimizer =======#\n    BACKBONE_DICT = {\'ResNet_50\': ResNet_50(INPUT_SIZE),\n                     \'ResNet_101\': ResNet_101(INPUT_SIZE),\n                     \'ResNet_152\': ResNet_152(INPUT_SIZE),\n                     \'IR_50\': IR_50(INPUT_SIZE),\n                     \'IR_101\': IR_101(INPUT_SIZE),\n                     \'IR_152\': IR_152(INPUT_SIZE),\n                     \'IR_SE_50\': IR_SE_50(INPUT_SIZE),\n                     \'IR_SE_101\': IR_SE_101(INPUT_SIZE),\n                     \'IR_SE_152\': IR_SE_152(INPUT_SIZE)}\n    BACKBONE = BACKBONE_DICT[BACKBONE_NAME]\n    print(""="" * 60)\n    print(BACKBONE)\n    print(""{} Backbone Generated"".format(BACKBONE_NAME))\n    print(""="" * 60)\n\n    HEAD_DICT = {\'ArcFace\': ArcFace(in_features=EMBEDDING_SIZE, out_features=NUM_CLASS),\n                 \'CosFace\': CosFace(in_features=EMBEDDING_SIZE, out_features=NUM_CLASS),\n                 \'SphereFace\': SphereFace(in_features=EMBEDDING_SIZE, out_features=NUM_CLASS),\n                 \'Am_softmax\': Am_softmax(in_features=EMBEDDING_SIZE, out_features=NUM_CLASS)}\n    HEAD = HEAD_DICT[HEAD_NAME]\n    print(""="" * 60)\n    print(HEAD)\n    print(""{} Head Generated"".format(HEAD_NAME))\n    print(""="" * 60)\n\n    LOSS_DICT = {\'Focal\': FocalLoss(),\n                 \'Softmax\': nn.CrossEntropyLoss()}\n    LOSS = LOSS_DICT[LOSS_NAME]\n    print(""="" * 60)\n    print(LOSS)\n    print(""{} Loss Generated"".format(LOSS_NAME))\n    print(""="" * 60)\n\n    if BACKBONE_NAME.find(""IR"") >= 0:\n        backbone_paras_only_bn, backbone_paras_wo_bn = separate_irse_bn_paras(\n            BACKBONE)  # separate batch_norm parameters from others; do not do weight decay for batch_norm parameters to improve the generalizability\n        _, head_paras_wo_bn = separate_irse_bn_paras(HEAD)\n    else:\n        backbone_paras_only_bn, backbone_paras_wo_bn = separate_resnet_bn_paras(\n            BACKBONE)  # separate batch_norm parameters from others; do not do weight decay for batch_norm parameters to improve the generalizability\n        _, head_paras_wo_bn = separate_resnet_bn_paras(HEAD)\n    OPTIMIZER = optim.SGD(\n        [{\'params\': backbone_paras_wo_bn}, {\'params\': backbone_paras_only_bn}, {\'params\': head_paras_wo_bn}], lr=LR,\n        momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n    print(""="" * 60)\n    print(OPTIMIZER)\n    print(""Optimizer Generated"")\n    print(""="" * 60)\n\n    # optionally resume from a checkpoint\n    if BACKBONE_RESUME_ROOT and HEAD_RESUME_ROOT:\n        print(""="" * 60)\n        if os.path.isfile(BACKBONE_RESUME_ROOT):\n            print(""Loading Backbone Checkpoint \'{}\'"".format(BACKBONE_RESUME_ROOT))\n            BACKBONE.load_state_dict(torch.load(BACKBONE_RESUME_ROOT))\n        else:\n            print(""No BACKBONE Checkpoint Found!"")\n\n        if os.path.isfile(HEAD_RESUME_ROOT):\n            print(""Loading Head Checkpoint \'{}\'"".format(HEAD_RESUME_ROOT))\n            HEAD.load_state_dict(torch.load(HEAD_RESUME_ROOT))\n        else:\n            print(""No Head Checkpoint Found!"")\n        print(""="" * 60)\n\n    if MULTI_GPU:\n        # multi-GPU setting\n        BACKBONE = nn.DataParallel(BACKBONE, device_ids=GPU_ID)\n        BACKBONE = BACKBONE.to(DEVICE)\n        HEAD = nn.DataParallel(HEAD, device_ids=GPU_ID)\n        HEAD = HEAD.to(DEVICE)\n    else:\n        # single-GPU setting\n        BACKBONE = BACKBONE.to(DEVICE)\n        HEAD = HEAD.to(DEVICE)\n\n    # ======= train & validation & save checkpoint =======#\n    DISP_FREQ = len(train_loader) // 100  # frequency to display training loss & acc\n\n    # NUM_EPOCH_WARM_UP = NUM_EPOCH // 25  # use the first 1/25 epochs to warm up\n    # NUM_BATCH_WARM_UP = len(train_loader) * NUM_EPOCH_WARM_UP  # use the first 1/25 epochs to warm up\n\n    batch = 0  # batch index\n    for epoch in range(NUM_EPOCH):  # start training process\n\n        BACKBONE.train()  # set to training mode\n        HEAD.train()\n\n        losses = AverageMeter()\n        top1 = AverageMeter()\n        top5 = AverageMeter()\n\n        for inputs, labels in tqdm(iter(train_loader)):\n\n            if batch == STAGES[\n                0]:  # adjust LR for each training stage after warm up, you can also choose to adjust LR manually (with slight modification) once plaueau observed\n                schedule_lr(OPTIMIZER)\n            if batch == STAGES[1]:\n                schedule_lr(OPTIMIZER)\n            if batch == STAGES[2]:\n                schedule_lr(OPTIMIZER)\n\n            # compute output\n            inputs = inputs.to(DEVICE)\n            labels = labels.to(DEVICE).long()\n            features = BACKBONE(inputs)\n            outputs = HEAD(features, labels)\n            loss = LOSS(outputs, labels)\n\n            # measure accuracy and record loss\n            prec1, prec5 = accuracy(outputs.data, labels, topk=(1, 5))\n            losses.update(loss.data.item(), inputs.size(0))\n            top1.update(prec1.data.item(), inputs.size(0))\n            top5.update(prec5.data.item(), inputs.size(0))\n\n            # compute gradient and do SGD step\n            OPTIMIZER.zero_grad()\n            loss.backward()\n            OPTIMIZER.step()\n\n            # dispaly training loss & acc every DISP_FREQ\n            if batch % 2000 == 0 and batch != 0:\n                print(""="" * 60)\n                print(\'Epoch {}/{} Batch {}/{}\\t\'\n                      \'Training Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                      \'Training Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t\'\n                      \'Training Prec@5 {top5.val:.3f} ({top5.avg:.3f})\'.format(\n                    epoch + 1, NUM_EPOCH, batch + 1, len(train_loader) * NUM_EPOCH, loss=losses, top1=top1, top5=top5))\n                print(""="" * 60)\n\n                # perform validation & save checkpoints per epoch\n                # validation statistics per epoch (buffer for visualization)\n                print(""="" * 60)\n                print(""Perform Evaluation on LFW, CFP_FF, CFP_FP, AgeDB, CALFW, CPLFW and VGG2_FP, and Save Checkpoints..."")\n                accuracy_lfw, best_threshold_lfw, roc_curve_lfw = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, lfw, lfw_issame)\n                buffer_val(writer, ""LFW"", accuracy_lfw, best_threshold_lfw, roc_curve_lfw, batch + 1)\n                accuracy_cfp_ff, best_threshold_cfp_ff, roc_curve_cfp_ff = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, cfp_ff, cfp_ff_issame)\n                buffer_val(writer, ""CFP_FF"", accuracy_cfp_ff, best_threshold_cfp_ff, roc_curve_cfp_ff, batch + 1)\n                accuracy_cfp_fp, best_threshold_cfp_fp, roc_curve_cfp_fp = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, cfp_fp, cfp_fp_issame)\n                buffer_val(writer, ""CFP_FP"", accuracy_cfp_fp, best_threshold_cfp_fp, roc_curve_cfp_fp, batch + 1)\n                accuracy_agedb, best_threshold_agedb, roc_curve_agedb = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, agedb, agedb_issame)\n                buffer_val(writer, ""AgeDB"", accuracy_agedb, best_threshold_agedb, roc_curve_agedb, batch + 1)\n                accuracy_calfw, best_threshold_calfw, roc_curve_calfw = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, calfw, calfw_issame)\n                buffer_val(writer, ""CALFW"", accuracy_calfw, best_threshold_calfw, roc_curve_calfw, batch + 1)\n                accuracy_cplfw, best_threshold_cplfw, roc_curve_cplfw = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, cplfw, cplfw_issame)\n                buffer_val(writer, ""CPLFW"", accuracy_cplfw, best_threshold_cplfw, roc_curve_cplfw, batch + 1)\n                accuracy_vgg2_fp, best_threshold_vgg2_fp, roc_curve_vgg2_fp = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, vgg2_fp, vgg2_fp_issame)\n                buffer_val(writer, ""VGGFace2_FP"", accuracy_vgg2_fp, best_threshold_vgg2_fp, roc_curve_vgg2_fp, batch + 1)\n                print(""Batch {}/{}, Evaluation: LFW Acc: {}, CFP_FF Acc: {}, CFP_FP Acc: {}, AgeDB Acc: {}, CALFW Acc: {}, CPLFW Acc: {}, VGG2_FP Acc: {}"".format(batch + 1, len(train_loader) * NUM_EPOCH, accuracy_lfw, accuracy_cfp_ff, accuracy_cfp_fp, accuracy_agedb, accuracy_calfw, accuracy_cplfw, accuracy_vgg2_fp))\n                print(""="" * 60)\n\n                if MULTI_GPU:\n                    torch.save(BACKBONE.module.state_dict(), os.path.join(MODEL_ROOT,\n                                                                          ""Backbone_{}_Epoch_{}_Batch_{}_Time_{}_checkpoint.pth"".format(\n                                                                              BACKBONE_NAME, epoch + 1, batch,\n                                                                              get_time())))\n                    torch.save(HEAD.module.state_dict(), os.path.join(MODEL_ROOT,\n                                                                      ""Head_{}_Epoch_{}_Batch_{}_Time_{}_checkpoint.pth"".format(\n                                                                          HEAD_NAME, epoch + 1, batch, get_time())))\n                else:\n                    torch.save(BACKBONE.state_dict(), os.path.join(MODEL_ROOT,\n                                                                   ""Backbone_{}_Epoch_{}_Batch_{}_Time_{}_checkpoint.pth"".format(\n                                                                       BACKBONE_NAME, epoch + 1, batch, get_time())))\n                    torch.save(HEAD.state_dict(), os.path.join(MODEL_ROOT,\n                                                               ""Head_{}_Epoch_{}_Batch_{}_Time_{}_checkpoint.pth"".format(\n                                                                   HEAD_NAME, epoch + 1, batch, get_time())))\n\n            batch += 1  # batch index\n\n        # training statistics per epoch (buffer for visualization)\n        # epoch_loss = losses.avg\n        # epoch_acc = top1.avg\n        # writer.add_scalar(""Training_Loss"", epoch_loss, epoch + 1)\n        # writer.add_scalar(""Training_Accuracy"", epoch_acc, epoch + 1)\n        # print(""="" * 60)\n        # print(\'Epoch: {}/{}\\t\'\n        #      \'Training Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n        #      \'Training Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t\'\n        #      \'Training Prec@5 {top5.val:.3f} ({top5.avg:.3f})\'.format(\n        #    epoch + 1, NUM_EPOCH, loss = losses, top1 = top1, top5 = top5))\n        # print(""="" * 60)\n\n        # perform validation & save checkpoints per epoch\n        # validation statistics per epoch (buffer for visualization)\n        # print(""="" * 60)\n        # print(""Perform Evaluation on LFW, CFP_FF, CFP_FP, AgeDB, CALFW, CPLFW and VGG2_FP, and Save Checkpoints..."")\n        # accuracy_lfw, best_threshold_lfw, roc_curve_lfw = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, lfw, lfw_issame)\n        # buffer_val(writer, ""LFW"", accuracy_lfw, best_threshold_lfw, roc_curve_lfw, epoch + 1)\n        # accuracy_cfp_ff, best_threshold_cfp_ff, roc_curve_cfp_ff = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, cfp_ff, cfp_ff_issame)\n        # buffer_val(writer, ""CFP_FF"", accuracy_cfp_ff, best_threshold_cfp_ff, roc_curve_cfp_ff, epoch + 1)\n        # accuracy_cfp_fp, best_threshold_cfp_fp, roc_curve_cfp_fp = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, cfp_fp, cfp_fp_issame)\n        # buffer_val(writer, ""CFP_FP"", accuracy_cfp_fp, best_threshold_cfp_fp, roc_curve_cfp_fp, epoch + 1)\n        # accuracy_agedb, best_threshold_agedb, roc_curve_agedb = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, agedb, agedb_issame)\n        # buffer_val(writer, ""AgeDB"", accuracy_agedb, best_threshold_agedb, roc_curve_agedb, epoch + 1)\n        # accuracy_calfw, best_threshold_calfw, roc_curve_calfw = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, calfw, calfw_issame)\n        # buffer_val(writer, ""CALFW"", accuracy_calfw, best_threshold_calfw, roc_curve_calfw, epoch + 1)\n        # accuracy_cplfw, best_threshold_cplfw, roc_curve_cplfw = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, cplfw, cplfw_issame)\n        # buffer_val(writer, ""CPLFW"", accuracy_cplfw, best_threshold_cplfw, roc_curve_cplfw, epoch + 1)\n        # accuracy_vgg2_fp, best_threshold_vgg2_fp, roc_curve_vgg2_fp = perform_val(MULTI_GPU, DEVICE, EMBEDDING_SIZE, BATCH_SIZE, BACKBONE, vgg2_fp, vgg2_fp_issame)\n        # buffer_val(writer, ""VGGFace2_FP"", accuracy_vgg2_fp, best_threshold_vgg2_fp, roc_curve_vgg2_fp, epoch + 1)\n        # print(""Epoch {}/{}, Evaluation: LFW Acc: {}, CFP_FF Acc: {}, CFP_FP Acc: {}, AgeDB Acc: {}, CALFW Acc: {}, CPLFW Acc: {}, VGG2_FP Acc: {}"".format(epoch + 1, NUM_EPOCH, accuracy_lfw, accuracy_cfp_ff, accuracy_cfp_fp, accuracy_agedb, accuracy_calfw, accuracy_cplfw, accuracy_vgg2_fp))\n        # print(""="" * 60)\n\n        # save checkpoints per epoch\n'"
backup/utils.py,9,"b'import torch\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\n\nfrom .verification import evaluate\n\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nplt.switch_backend(\'agg\')\nimport numpy as np\nfrom PIL import Image\nimport bcolz\nimport io\nimport os\n\n\n# Support: [\'get_time\', \'l2_norm\', \'make_weights_for_balanced_classes\', \'get_val_pair\', \'get_val_data\', \'separate_irse_bn_paras\', \'separate_resnet_bn_paras\', \'warm_up_lr\', \'schedule_lr\', \'de_preprocess\', \'hflip_batch\', \'gen_plot\', \'perform_val\', \'buffer_val\', \'AverageMeter\', \'accuracy\', \'save_checkpoint\']\n\n\ndef get_time():\n    return (str(datetime.now())[:-10]).replace(\' \', \'-\').replace(\':\', \'-\')\n\n\ndef l2_norm(input, axis = 1):\n    norm = torch.norm(input, 2, axis, True)\n    output = torch.div(input, norm)\n\n    return output\n\n\ndef make_weights_for_balanced_classes(images, nclasses):\n    \'\'\'\n        Make a vector of weights for each image in the dataset, based\n        on class frequency. The returned vector of weights can be used\n        to create a WeightedRandomSampler for a DataLoader to have\n        class balancing when sampling for a training batch.\n            images - torchvisionDataset.imgs\n            nclasses - len(torchvisionDataset.classes)\n        https://discuss.pytorch.org/t/balanced-sampling-between-classes-with-torchvision-dataloader/2703/3\n    \'\'\'\n    count = [0] * nclasses\n    for item in images:\n        count[item[1]] += 1  # item is (img-data, label-id)\n    weight_per_class = [0.] * nclasses\n    N = float(sum(count))  # total number of images\n    for i in range(nclasses):\n        weight_per_class[i] = N / float(count[i])\n    weight = [0] * len(images)\n    for idx, val in enumerate(images):\n        weight[idx] = weight_per_class[val[1]]\n\n    return weight\n\n\ndef get_val_pair(path, name):\n    carray = bcolz.carray(rootdir = os.path.join(path, name), mode = \'r\')\n    issame = np.load(\'{}/{}_list.npy\'.format(path, name))\n\n    return carray, issame\n\n\ndef get_val_data(data_path):\n    lfw, lfw_issame = get_val_pair(data_path, \'lfw\')\n    cfp_ff, cfp_ff_issame = get_val_pair(data_path, \'cfp_ff\')\n    cfp_fp, cfp_fp_issame = get_val_pair(data_path, \'cfp_fp\')\n    agedb_30, agedb_30_issame = get_val_pair(data_path, \'agedb_30\')\n    calfw, calfw_issame = get_val_pair(data_path, \'calfw\')\n    cplfw, cplfw_issame = get_val_pair(data_path, \'cplfw\')\n    vgg2_fp, vgg2_fp_issame = get_val_pair(data_path, \'vgg2_fp\')\n\n    return lfw, cfp_ff, cfp_fp, agedb_30, calfw, cplfw, vgg2_fp, lfw_issame, cfp_ff_issame, cfp_fp_issame, agedb_30_issame, calfw_issame, cplfw_issame, vgg2_fp_issame\n\n\ndef separate_irse_bn_paras(modules):\n    if not isinstance(modules, list):\n        modules = [*modules.modules()]\n    paras_only_bn = []\n    paras_wo_bn = []\n    for layer in modules:\n        if \'model\' in str(layer.__class__):\n            continue\n        if \'container\' in str(layer.__class__):\n            continue\n        else:\n            if \'batchnorm\' in str(layer.__class__):\n                paras_only_bn.extend([*layer.parameters()])\n            else:\n                paras_wo_bn.extend([*layer.parameters()])\n\n    return paras_only_bn, paras_wo_bn\n\n\ndef separate_resnet_bn_paras(modules):\n    all_parameters = modules.parameters()\n    paras_only_bn = []\n\n    for pname, p in modules.named_parameters():\n        if pname.find(\'bn\') >= 0:\n            paras_only_bn.append(p)\n            \n    paras_only_bn_id = list(map(id, paras_only_bn))\n    paras_wo_bn = list(filter(lambda p: id(p) not in paras_only_bn_id, all_parameters))\n    \n    return paras_only_bn, paras_wo_bn\n\n\ndef warm_up_lr(batch, num_batch_warm_up, init_lr, optimizer):\n    for params in optimizer.param_groups:\n        params[\'lr\'] = batch * init_lr / num_batch_warm_up\n\n    # print(optimizer)\n\n\ndef schedule_lr(optimizer):\n    for params in optimizer.param_groups:\n        params[\'lr\'] /= 10.\n\n    print(optimizer)\n\n\ndef de_preprocess(tensor):\n\n    return tensor * 0.5 + 0.5\n\n\nhflip = transforms.Compose([\n            de_preprocess,\n            transforms.ToPILImage(),\n            transforms.functional.hflip,\n            transforms.ToTensor(),\n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n        ])\n\n\ndef hflip_batch(imgs_tensor):\n    hfliped_imgs = torch.empty_like(imgs_tensor)\n    for i, img_ten in enumerate(imgs_tensor):\n        hfliped_imgs[i] = hflip(img_ten)\n\n    return hfliped_imgs\n\n\ndef gen_plot(fpr, tpr):\n    """"""Create a pyplot plot and save to buffer.""""""\n    plt.figure()\n    plt.xlabel(""FPR"", fontsize = 14)\n    plt.ylabel(""TPR"", fontsize = 14)\n    plt.title(""ROC Curve"", fontsize = 14)\n    plot = plt.plot(fpr, tpr, linewidth = 2)\n    buf = io.BytesIO()\n    plt.savefig(buf, format = \'jpeg\')\n    buf.seek(0)\n    plt.close()\n\n    return buf\n\n\ndef perform_val(multi_gpu, device, embedding_size, batch_size, backbone, carray, issame, nrof_folds = 10, tta = True):\n    if multi_gpu:\n        backbone = backbone.module # unpackage model from DataParallel\n        backbone = backbone.to(device)\n    else:\n        backbone = backbone.to(device)\n    backbone.eval() # switch to evaluation mode\n\n    idx = 0\n    embeddings = np.zeros([len(carray), embedding_size])\n    with torch.no_grad():\n        while idx + batch_size <= len(carray):\n            batch = torch.tensor(carray[idx:idx + batch_size][:, [2, 1, 0], :, :])\n            if tta:\n                fliped = hflip_batch(batch)\n                emb_batch = backbone(batch.to(device)).cpu() + backbone(fliped.to(device)).cpu()\n                embeddings[idx:idx + batch_size] = l2_norm(emb_batch)\n            else:\n                embeddings[idx:idx + batch_size] = backbone(batch.to(device)).cpu()\n            idx += batch_size\n        if idx < len(carray):\n            batch = torch.tensor(carray[idx:])\n            if tta:\n                fliped = hflip_batch(batch)\n                emb_batch = backbone(batch.to(device)).cpu() + backbone(fliped.to(device)).cpu()\n                embeddings[idx:] = l2_norm(emb_batch)\n            else:\n                embeddings[idx:] = backbone(batch.to(device)).cpu()\n\n    tpr, fpr, accuracy, best_thresholds = evaluate(embeddings, issame, nrof_folds)\n    buf = gen_plot(fpr, tpr)\n    roc_curve = Image.open(buf)\n    roc_curve_tensor = transforms.ToTensor()(roc_curve)\n\n    return accuracy.mean(), best_thresholds.mean(), roc_curve_tensor\n\n\ndef buffer_val(writer, db_name, acc, best_threshold, roc_curve_tensor, epoch):\n    writer.add_scalar(\'{}_Accuracy\'.format(db_name), acc, epoch)\n    writer.add_scalar(\'{}_Best_Threshold\'.format(db_name), best_threshold, epoch)\n    writer.add_image(\'{}_ROC_Curve\'.format(db_name), roc_curve_tensor, epoch)\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val   = 0\n        self.avg   = 0\n        self.sum   = 0\n        self.count = 0\n\n    def update(self, val, n = 1):\n        self.val   = val\n        self.sum   += val * n\n        self.count += n\n        self.avg   = self.sum / self.count\n\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the precision@k for the specified values of k""""""\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred    = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n\n    return res\n\n\ndef save_checkpoint(state, filename):\n    torch.save(state, filename)\n'"
balance/__init__.py,0,b'\n'
balance/remove_lowshot.py,0,"b'import os, shutil\nimport argparse\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description = \'remove low-shot classes\')\n    parser.add_argument(""-root"", ""--root"", help = ""specify your dir"",default = \'./data/train\', type = str)\n    parser.add_argument(""-min_num"", ""--min_num"", help = ""remove the classes with less than min_num samples"", default = 10, type = int)\n    args = parser.parse_args()\n\n    root = args.root # specify your dir\n    min_num = args.min_num # remove the classes with less than min_num samples\n\n    cwd = os.getcwd()  # delete \'.DS_Store\' existed in the source_root\n    os.chdir(root)\n    os.system(""find . -name \'*.DS_Store\' -type f -delete"")\n    os.chdir(cwd)\n\n    for subfolder in os.listdir(root):\n        file_num = len(os.listdir(os.path.join(root, subfolder)))\n        if file_num <= min_num:\n            print(""Class {} has less than {} samples, removed!"".format(subfolder, min_num))\n            shutil.rmtree(os.path.join(root, subfolder))'"
head/__init__.py,0,b'\n'
head/metrics.py,35,"b'from __future__ import print_function\r\nfrom __future__ import division\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.nn import Parameter\r\nimport math\r\n\r\n\r\n# Support: [\'Softmax\', \'ArcFace\', \'CosFace\', \'SphereFace\', \'Am_softmax\']\r\n\r\nclass Softmax(nn.Module):\r\n    r""""""Implement of Softmax (normal classification head):\r\n        Args:\r\n            in_features: size of each input sample\r\n            out_features: size of each output sample\r\n            device_id: the ID of GPU where the model will be trained by model parallel. \r\n                       if device_id=None, it will be trained on CPU without model parallel.\r\n        """"""\r\n    def __init__(self, in_features, out_features, device_id):\r\n        super(Softmax, self).__init__()\r\n        self.in_features = in_features\r\n        self.out_features = out_features\r\n        self.device_id = device_id\r\n\r\n        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\r\n        self.bias = Parameter(torch.FloatTensor(out_features))\r\n        nn.init.xavier_uniform_(self.weight)\r\n        nn.init.zero_(self.bias)\r\n\r\n    def forward(self, x):\r\n        if self.device_id == None:\r\n            out = F.linear(x, self.weight, self.bias)\r\n        else:\r\n            sub_weights = torch.chunk(self.weight, len(self.device_id), dim=0)\r\n            sub_biases = torch.chunk(self.bias, len(self.device_id), dim=0)\r\n            temp_x = x.cuda(self.device_id[0])\r\n            weight = sub_weights[0].cuda(self.device_id[0])\r\n            bias = sub_biases[0].cuda(self.device_id[0])\r\n            out = F.linear(temp_x, weight, bias)\r\n            for i in range(1, len(self.device_id)):\r\n                temp_x = x.cuda(self.device_id[i])\r\n                weight = sub_weights[i].cuda(self.device_id[i])\r\n                bias = sub_biases[i].cuda(self.device_id[i])\r\n                out = torch.cat((out, F.linear(temp_x, weight, bias).cuda(self.device_id[0])), dim=1)\r\n        return out\r\n\r\n    def _initialize_weights(self):\r\n        for m in self.modules():\r\n            if isinstance(m, nn.Conv2d):\r\n                nn.init.xavier_uniform_(m.weight.data)\r\n                if m.bias is not None:\r\n                    m.bias.data.zero_()\r\n            elif isinstance(m, nn.BatchNorm2d):\r\n                m.weight.data.fill_(1)\r\n                m.bias.data.zero_()\r\n            elif isinstance(m, nn.BatchNorm1d):\r\n                m.weight.data.fill_(1)\r\n                m.bias.data.zero_()\r\n            elif isinstance(m, nn.Linear):\r\n                nn.init.xavier_uniform_(m.weight.data)\r\n                if m.bias is not None:\r\n                    m.bias.data.zero_()\r\n\r\n\r\nclass ArcFace(nn.Module):\r\n    r""""""Implement of ArcFace (https://arxiv.org/pdf/1801.07698v1.pdf):\r\n        Args:\r\n            in_features: size of each input sample\r\n            out_features: size of each output sample\r\n            device_id: the ID of GPU where the model will be trained by model parallel. \r\n                       if device_id=None, it will be trained on CPU without model parallel.\r\n            s: norm of input feature\r\n            m: margin\r\n            cos(theta+m)\r\n        """"""\r\n    def __init__(self, in_features, out_features, device_id, s = 64.0, m = 0.50, easy_margin = False):\r\n        super(ArcFace, self).__init__()\r\n        self.in_features = in_features\r\n        self.out_features = out_features\r\n        self.device_id = device_id\r\n\r\n        self.s = s\r\n        self.m = m\r\n        \r\n        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\r\n        nn.init.xavier_uniform_(self.weight)\r\n      \r\n        self.easy_margin = easy_margin\r\n        self.cos_m = math.cos(m)\r\n        self.sin_m = math.sin(m)\r\n        self.th = math.cos(math.pi - m)\r\n        self.mm = math.sin(math.pi - m) * m\r\n\r\n    def forward(self, input, label):\r\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\r\n        if self.device_id == None:\r\n            cosine = F.linear(F.normalize(input), F.normalize(self.weight))\r\n        else:\r\n            x = input\r\n            sub_weights = torch.chunk(self.weight, len(self.device_id), dim=0)\r\n            temp_x = x.cuda(self.device_id[0])\r\n            weight = sub_weights[0].cuda(self.device_id[0])\r\n            cosine = F.linear(F.normalize(temp_x), F.normalize(weight))\r\n            for i in range(1, len(self.device_id)):\r\n                temp_x = x.cuda(self.device_id[i])\r\n                weight = sub_weights[i].cuda(self.device_id[i])\r\n                cosine = torch.cat((cosine, F.linear(F.normalize(temp_x), F.normalize(weight)).cuda(self.device_id[0])), dim=1) \r\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\r\n        phi = cosine * self.cos_m - sine * self.sin_m\r\n        if self.easy_margin:\r\n            phi = torch.where(cosine > 0, phi, cosine)\r\n        else:\r\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\r\n        # --------------------------- convert label to one-hot ---------------------------\r\n        one_hot = torch.zeros(cosine.size())\r\n        if self.device_id != None:\r\n            one_hot = one_hot.cuda(self.device_id[0])\r\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\r\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\r\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\r\n        output *= self.s\r\n\r\n        return output\r\n\r\n\r\nclass CosFace(nn.Module):\r\n    r""""""Implement of CosFace (https://arxiv.org/pdf/1801.09414.pdf):\r\n    Args:\r\n        in_features: size of each input sample\r\n        out_features: size of each output sample\r\n        device_id: the ID of GPU where the model will be trained by model parallel. \r\n                       if device_id=None, it will be trained on CPU without model parallel.\r\n        s: norm of input feature\r\n        m: margin\r\n        cos(theta)-m\r\n    """"""\r\n    def __init__(self, in_features, out_features, device_id, s = 64.0, m = 0.35):\r\n        super(CosFace, self).__init__()\r\n        self.in_features = in_features\r\n        self.out_features = out_features\r\n        self.device_id = device_id\r\n        self.s = s\r\n        self.m = m\r\n\r\n        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\r\n        nn.init.xavier_uniform_(self.weight)\r\n\r\n    def forward(self, input, label):\r\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\r\n        if self.device_id == None:\r\n            cosine = F.linear(F.normalize(input), F.normalize(self.weight))\r\n        else:\r\n            x = input\r\n            sub_weights = torch.chunk(self.weight, len(self.device_id), dim=0)\r\n            temp_x = x.cuda(self.device_id[0])\r\n            weight = sub_weights[0].cuda(self.device_id[0])\r\n            cosine = F.linear(F.normalize(temp_x), F.normalize(weight))\r\n            for i in range(1, len(self.device_id)):\r\n                temp_x = x.cuda(self.device_id[i])\r\n                weight = sub_weights[i].cuda(self.device_id[i])\r\n                cosine = torch.cat((cosine, F.linear(F.normalize(temp_x), F.normalize(weight)).cuda(self.device_id[0])), dim=1)\r\n        phi = cosine - self.m\r\n        # --------------------------- convert label to one-hot ---------------------------\r\n        one_hot = torch.zeros(cosine.size())\r\n        if self.device_id != None:\r\n            one_hot = one_hot.cuda(self.device_id[0])\r\n        # one_hot = one_hot.cuda() if cosine.is_cuda else one_hot\r\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\r\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\r\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\r\n        output *= self.s\r\n\r\n        return output\r\n\r\n    def __repr__(self):\r\n        return self.__class__.__name__ + \'(\' \\\r\n               + \'in_features = \' + str(self.in_features) \\\r\n               + \', out_features = \' + str(self.out_features) \\\r\n               + \', s = \' + str(self.s) \\\r\n               + \', m = \' + str(self.m) + \')\'\r\n\r\nclass SphereFace(nn.Module):\r\n    r""""""Implement of SphereFace (https://arxiv.org/pdf/1704.08063.pdf):\r\n    Args:\r\n        in_features: size of each input sample\r\n        out_features: size of each output sample\r\n        device_id: the ID of GPU where the model will be trained by model parallel. \r\n                       if device_id=None, it will be trained on CPU without model parallel.\r\n        m: margin\r\n        cos(m*theta)\r\n    """"""\r\n    def __init__(self, in_features, out_features, device_id, m = 4):\r\n        super(SphereFace, self).__init__()\r\n        self.in_features = in_features\r\n        self.out_features = out_features\r\n        self.m = m\r\n        self.base = 1000.0\r\n        self.gamma = 0.12\r\n        self.power = 1\r\n        self.LambdaMin = 5.0\r\n        self.iter = 0\r\n        self.device_id = device_id\r\n\r\n        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\r\n        nn.init.xavier_uniform_(self.weight)\r\n\r\n        # duplication formula\r\n        self.mlambda = [\r\n            lambda x: x ** 0,\r\n            lambda x: x ** 1,\r\n            lambda x: 2 * x ** 2 - 1,\r\n            lambda x: 4 * x ** 3 - 3 * x,\r\n            lambda x: 8 * x ** 4 - 8 * x ** 2 + 1,\r\n            lambda x: 16 * x ** 5 - 20 * x ** 3 + 5 * x\r\n        ]\r\n\r\n    def forward(self, input, label):\r\n        # lambda = max(lambda_min,base*(1+gamma*iteration)^(-power))\r\n        self.iter += 1\r\n        self.lamb = max(self.LambdaMin, self.base * (1 + self.gamma * self.iter) ** (-1 * self.power))\r\n\r\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\r\n        if self.device_id == None:\r\n            cos_theta = F.linear(F.normalize(input), F.normalize(self.weight))\r\n        else:\r\n            x = input\r\n            sub_weights = torch.chunk(self.weight, len(self.device_id), dim=0)\r\n            temp_x = x.cuda(self.device_id[0])\r\n            weight = sub_weights[0].cuda(self.device_id[0])\r\n            cos_theta = F.linear(F.normalize(temp_x), F.normalize(weight))\r\n            for i in range(1, len(self.device_id)):\r\n                temp_x = x.cuda(self.device_id[i])\r\n                weight = sub_weights[i].cuda(self.device_id[i])\r\n                cos_theta = torch.cat((cos_theta, F.linear(F.normalize(temp_x), F.normalize(weight)).cuda(self.device_id[0])), dim=1)\r\n\r\n        cos_theta = cos_theta.clamp(-1, 1)\r\n        cos_m_theta = self.mlambda[self.m](cos_theta)\r\n        theta = cos_theta.data.acos()\r\n        k = (self.m * theta / 3.14159265).floor()\r\n        phi_theta = ((-1.0) ** k) * cos_m_theta - 2 * k\r\n        NormOfFeature = torch.norm(input, 2, 1)\r\n\r\n        # --------------------------- convert label to one-hot ---------------------------\r\n        one_hot = torch.zeros(cos_theta.size())\r\n        if self.device_id != None:\r\n            one_hot = one_hot.cuda(self.device_id[0])\r\n        one_hot.scatter_(1, label.view(-1, 1), 1)\r\n\r\n        # --------------------------- Calculate output ---------------------------\r\n        output = (one_hot * (phi_theta - cos_theta) / (1 + self.lamb)) + cos_theta\r\n        output *= NormOfFeature.view(-1, 1)\r\n\r\n        return output\r\n\r\n    def __repr__(self):\r\n        return self.__class__.__name__ + \'(\' \\\r\n               + \'in_features = \' + str(self.in_features) \\\r\n               + \', out_features = \' + str(self.out_features) \\\r\n               + \', m = \' + str(self.m) + \')\'\r\n\r\n\r\ndef l2_norm(input, axis = 1):\r\n    norm = torch.norm(input, 2, axis, True)\r\n    output = torch.div(input, norm)\r\n\r\n    return output\r\n\r\n\r\nclass Am_softmax(nn.Module):\r\n    r""""""Implement of Am_softmax (https://arxiv.org/pdf/1801.05599.pdf):\r\n    Args:\r\n        in_features: size of each input sample\r\n        out_features: size of each output sample\r\n        device_id: the ID of GPU where the model will be trained by model parallel. \r\n                       if device_id=None, it will be trained on CPU without model parallel.\r\n        m: margin\r\n        s: scale of outputs\r\n    """"""\r\n    def __init__(self, in_features, out_features, device_id, m = 0.35, s = 30.0):\r\n        super(Am_softmax, self).__init__()\r\n        self.in_features = in_features\r\n        self.out_features = out_features\r\n        self.m = m\r\n        self.s = s\r\n        self.device_id = device_id\r\n\r\n        self.kernel = Parameter(torch.Tensor(in_features, out_features))\r\n        self.kernel.data.uniform_(-1, 1).renorm_(2, 1, 1e-5).mul_(1e5)  # initialize kernel\r\n \r\n    def forward(self, embbedings, label):\r\n        if self.device_id == None:\r\n            kernel_norm = l2_norm(self.kernel, axis = 0)\r\n            cos_theta = torch.mm(embbedings, kernel_norm)\r\n        else:\r\n            x = embbedings\r\n            sub_kernels = torch.chunk(self.kernel, len(self.device_id), dim=1)\r\n            temp_x = x.cuda(self.device_id[0])\r\n            kernel_norm = l2_norm(sub_kernels[0], axis = 0).cuda(self.device_id[0])\r\n            cos_theta = torch.mm(temp_x, kernel_norm)\r\n            for i in range(1, len(self.device_id)):\r\n                temp_x = x.cuda(self.device_id[i])\r\n                kernel_norm = l2_norm(sub_kernels[i], axis = 0).cuda(self.device_id[i])\r\n                cos_theta = torch.cat((cos_theta, torch.mm(temp_x, kernel_norm).cuda(self.device_id[0])), dim=1)\r\n\r\n        cos_theta = cos_theta.clamp(-1, 1)  # for numerical stability\r\n        phi = cos_theta - self.m\r\n        label = label.view(-1, 1)  # size=(B,1)\r\n        index = cos_theta.data * 0.0  # size=(B,Classnum)\r\n        index.scatter_(1, label.data.view(-1, 1), 1)\r\n        index = index.byte()\r\n        output = cos_theta * 1.0\r\n        output[index] = phi[index]  # only change the correct predicted output\r\n        output *= self.s  # scale up in order to make softmax work, first introduced in normface\r\n\r\n        return output\r\n'"
loss/__init__.py,0,b'\n'
loss/focal.py,2,"b""import torch\nimport torch.nn as nn\n\n\n# Support: ['FocalLoss']\n\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma = 2, eps = 1e-7):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.eps = eps\n        self.ce = nn.CrossEntropyLoss()\n\n    def forward(self, input, target):\n        logp = self.ce(input, target)\n        p = torch.exp(-logp)\n        loss = (1 - p) ** self.gamma * logp\n        return loss.mean()\n"""
util/__init__.py,0,b'\n'
util/extract_feature_v1.py,7,"b'# Helper function for extracting features from pre-trained models\nimport torch\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n\nimport numpy as np\nimport os\n\n\ndef l2_norm(input, axis = 1):\n    norm = torch.norm(input, 2, axis, True)\n    output = torch.div(input, norm)\n\n    return output\n\n\ndef de_preprocess(tensor):\n\n    return tensor * 0.5 + 0.5\n\n\nhflip = transforms.Compose([\n            de_preprocess,\n            transforms.ToPILImage(),\n            transforms.functional.hflip,\n            transforms.ToTensor(),\n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n        ])\n\n\ndef hflip_batch(imgs_tensor):\n    hfliped_imgs = torch.empty_like(imgs_tensor)\n    for i, img_ten in enumerate(imgs_tensor):\n        hfliped_imgs[i] = hflip(img_ten)\n\n    return hfliped_imgs\n\n\ndef extract_feature(data_root, backbone, model_root, input_size = [112, 112], rgb_mean = [0.5, 0.5, 0.5], rgb_std = [0.5, 0.5, 0.5], embedding_size = 512, batch_size = 512, device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu""), tta = True):\n\n    # pre-requisites\n    assert(os.path.exists(data_root))\n    print(\'Testing Data Root:\', data_root)\n    assert (os.path.exists(model_root))\n    print(\'Backbone Model Root:\', model_root)\n\n    # define data loader\n    transform = transforms.Compose([\n        transforms.Resize([int(128 * input_size[0] / 112), int(128 * input_size[0] / 112)]),  # smaller side resized\n        transforms.CenterCrop([input_size[0], input_size[1]]),\n        transforms.ToTensor(),\n        transforms.Normalize(mean = rgb_mean, std = rgb_std)])\n    dataset = datasets.ImageFolder(data_root, transform)\n    loader = torch.utils.data.DataLoader(dataset, batch_size = batch_size, shuffle = False, pin_memory = True, num_workers = 0)\n    NUM_CLASS = len(loader.dataset.classes)\n    print(""Number of Classes: {}"".format(NUM_CLASS))\n\n    # load backbone from a checkpoint\n    print(""Loading Backbone Checkpoint \'{}\'"".format(model_root))\n    backbone.load_state_dict(torch.load(model_root))\n    backbone.to(device)\n\n    # extract features\n    backbone.eval() # set to evaluation mode\n    idx = 0\n    features = np.zeros([len(loader.dataset), embedding_size])\n    with torch.no_grad():\n        iter_loader = iter(loader)\n        while idx + batch_size <= len(loader.dataset):\n            batch, _ = iter_loader.next()\n            if tta:\n                fliped = hflip_batch(batch)\n                emb_batch = backbone(batch.to(device)).cpu() + backbone(fliped.to(device)).cpu()\n                features[idx:idx + batch_size] = l2_norm(emb_batch)\n            else:\n                features[idx:idx + batch_size] = l2_norm(backbone(batch.to(device))).cpu()\n            idx += batch_size\n\n        if idx < len(loader.dataset):\n            batch, _ = iter_loader.next()\n            if tta:\n                fliped = hflip_batch(batch)\n                emb_batch = backbone(batch.to(device)).cpu() + backbone(fliped.to(device)).cpu()\n                features[idx:] = l2_norm(emb_batch)\n            else:\n                features[idx:] = l2_norm(backbone(batch.to(device)).cpu())\n                \n#     np.save(""features.npy"", features) \n#     features = np.load(""features.npy"")\n\n    return features\n'"
util/extract_feature_v2.py,7,"b'# Helper function for extracting features from pre-trained models\nimport torch\nimport cv2\nimport numpy as np\nimport os\n\nimport matplotlib.pyplot as plt\n\ndef l2_norm(input, axis = 1):\n    norm = torch.norm(input, 2, axis, True)\n    output = torch.div(input, norm)\n\n    return output\n\n\ndef extract_feature(img_root, backbone, model_root, device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu""), tta = True):\n    # pre-requisites\n    assert(os.path.exists(img_root))\n    print(\'Testing Data Root:\', img_root)\n    assert (os.path.exists(model_root))\n    print(\'Backbone Model Root:\', model_root)\n\n    # load image\n    img = cv2.imread(img_root)\n\n    # resize image to [128, 128]\n    resized = cv2.resize(img, (128, 128))\n\n    # center crop image\n    a=int((128-112)/2) # x start\n    b=int((128-112)/2+112) # x end\n    c=int((128-112)/2) # y start\n    d=int((128-112)/2+112) # y end\n    ccropped = resized[a:b, c:d] # center crop the image\n    ccropped = ccropped[...,::-1] # BGR to RGB\n\n    # flip image horizontally\n    flipped = cv2.flip(ccropped, 1)\n\n    # load numpy to tensor\n    ccropped = ccropped.swapaxes(1, 2).swapaxes(0, 1)\n    ccropped = np.reshape(ccropped, [1, 3, 112, 112])\n    ccropped = np.array(ccropped, dtype = np.float32)\n    ccropped = (ccropped - 127.5) / 128.0\n    ccropped = torch.from_numpy(ccropped)\n\n    flipped = flipped.swapaxes(1, 2).swapaxes(0, 1)\n    flipped = np.reshape(flipped, [1, 3, 112, 112])\n    flipped = np.array(flipped, dtype = np.float32)\n    flipped = (flipped - 127.5) / 128.0\n    flipped = torch.from_numpy(flipped)\n\n\n    # load backbone from a checkpoint\n    print(""Loading Backbone Checkpoint \'{}\'"".format(model_root))\n    backbone.load_state_dict(torch.load(model_root))\n    backbone.to(device)\n\n    # extract features\n    backbone.eval() # set to evaluation mode\n    with torch.no_grad():\n        if tta:\n            emb_batch = backbone(ccropped.to(device)).cpu() + backbone(flipped.to(device)).cpu()\n            features = l2_norm(emb_batch)\n        else:\n            features = l2_norm(backbone(ccropped.to(device)).cpu())\n            \n#     np.save(""features.npy"", features) \n#     features = np.load(""features.npy"")\n\n    return features\n'"
util/utils.py,9,"b'import torch\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\n\nfrom .verification import evaluate\n\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nplt.switch_backend(\'agg\')\nimport numpy as np\nfrom PIL import Image\nimport bcolz\nimport io\nimport os\n\n\n# Support: [\'get_time\', \'l2_norm\', \'make_weights_for_balanced_classes\', \'get_val_pair\', \'get_val_data\', \'separate_irse_bn_paras\', \'separate_resnet_bn_paras\', \'warm_up_lr\', \'schedule_lr\', \'de_preprocess\', \'hflip_batch\', \'ccrop_batch\', \'gen_plot\', \'perform_val\', \'buffer_val\', \'AverageMeter\', \'accuracy\']\n\n\ndef get_time():\n    return (str(datetime.now())[:-10]).replace(\' \', \'-\').replace(\':\', \'-\')\n\n\ndef l2_norm(input, axis = 1):\n    norm = torch.norm(input, 2, axis, True)\n    output = torch.div(input, norm)\n\n    return output\n\n\ndef make_weights_for_balanced_classes(images, nclasses):\n    \'\'\'\n        Make a vector of weights for each image in the dataset, based\n        on class frequency. The returned vector of weights can be used\n        to create a WeightedRandomSampler for a DataLoader to have\n        class balancing when sampling for a training batch.\n            images - torchvisionDataset.imgs\n            nclasses - len(torchvisionDataset.classes)\n        https://discuss.pytorch.org/t/balanced-sampling-between-classes-with-torchvision-dataloader/2703/3\n    \'\'\'\n    count = [0] * nclasses\n    for item in images:\n        count[item[1]] += 1  # item is (img-data, label-id)\n    weight_per_class = [0.] * nclasses\n    N = float(sum(count))  # total number of images\n    for i in range(nclasses):\n        weight_per_class[i] = N / float(count[i])\n    weight = [0] * len(images)\n    for idx, val in enumerate(images):\n        weight[idx] = weight_per_class[val[1]]\n\n    return weight\n\n\ndef get_val_pair(path, name):\n    carray = bcolz.carray(rootdir = os.path.join(path, name), mode = \'r\')\n    issame = np.load(\'{}/{}_list.npy\'.format(path, name))\n\n    return carray, issame\n\n\ndef get_val_data(data_path):\n    lfw, lfw_issame = get_val_pair(data_path, \'lfw\')\n    cfp_ff, cfp_ff_issame = get_val_pair(data_path, \'cfp_ff\')\n    cfp_fp, cfp_fp_issame = get_val_pair(data_path, \'cfp_fp\')\n    agedb_30, agedb_30_issame = get_val_pair(data_path, \'agedb_30\')\n    calfw, calfw_issame = get_val_pair(data_path, \'calfw\')\n    cplfw, cplfw_issame = get_val_pair(data_path, \'cplfw\')\n    vgg2_fp, vgg2_fp_issame = get_val_pair(data_path, \'vgg2_fp\')\n\n    return lfw, cfp_ff, cfp_fp, agedb_30, calfw, cplfw, vgg2_fp, lfw_issame, cfp_ff_issame, cfp_fp_issame, agedb_30_issame, calfw_issame, cplfw_issame, vgg2_fp_issame\n\n\ndef separate_irse_bn_paras(modules):\n    if not isinstance(modules, list):\n        modules = [*modules.modules()]\n    paras_only_bn = []\n    paras_wo_bn = []\n    for layer in modules:\n        if \'model\' in str(layer.__class__):\n            continue\n        if \'container\' in str(layer.__class__):\n            continue\n        else:\n            if \'batchnorm\' in str(layer.__class__):\n                paras_only_bn.extend([*layer.parameters()])\n            else:\n                paras_wo_bn.extend([*layer.parameters()])\n\n    return paras_only_bn, paras_wo_bn\n\n\ndef separate_resnet_bn_paras(modules):\n    all_parameters = modules.parameters()\n    paras_only_bn = []\n\n    for pname, p in modules.named_parameters():\n        if pname.find(\'bn\') >= 0:\n            paras_only_bn.append(p)\n            \n    paras_only_bn_id = list(map(id, paras_only_bn))\n    paras_wo_bn = list(filter(lambda p: id(p) not in paras_only_bn_id, all_parameters))\n    \n    return paras_only_bn, paras_wo_bn\n\n\ndef warm_up_lr(batch, num_batch_warm_up, init_lr, optimizer):\n    for params in optimizer.param_groups:\n        params[\'lr\'] = batch * init_lr / num_batch_warm_up\n\n    # print(optimizer)\n\n\ndef schedule_lr(optimizer):\n    for params in optimizer.param_groups:\n        params[\'lr\'] /= 10.\n\n    print(optimizer)\n\n\ndef de_preprocess(tensor):\n\n    return tensor * 0.5 + 0.5\n\n\nhflip = transforms.Compose([\n            de_preprocess,\n            transforms.ToPILImage(),\n            transforms.functional.hflip,\n            transforms.ToTensor(),\n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n        ])\n\n\ndef hflip_batch(imgs_tensor):\n    hfliped_imgs = torch.empty_like(imgs_tensor)\n    for i, img_ten in enumerate(imgs_tensor):\n        hfliped_imgs[i] = hflip(img_ten)\n\n    return hfliped_imgs\n\n\nccrop = transforms.Compose([\n            de_preprocess,\n            transforms.ToPILImage(),\n            transforms.Resize([128, 128]),  # smaller side resized\n            transforms.CenterCrop([112, 112]),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n        ])\n\n\ndef ccrop_batch(imgs_tensor):\n    ccropped_imgs = torch.empty_like(imgs_tensor)\n    for i, img_ten in enumerate(imgs_tensor):\n        ccropped_imgs[i] = ccrop(img_ten)\n\n    return ccropped_imgs\n\n\ndef gen_plot(fpr, tpr):\n    """"""Create a pyplot plot and save to buffer.""""""\n    plt.figure()\n    plt.xlabel(""FPR"", fontsize = 14)\n    plt.ylabel(""TPR"", fontsize = 14)\n    plt.title(""ROC Curve"", fontsize = 14)\n    plot = plt.plot(fpr, tpr, linewidth = 2)\n    buf = io.BytesIO()\n    plt.savefig(buf, format = \'jpeg\')\n    buf.seek(0)\n    plt.close()\n\n    return buf\n\n\ndef perform_val(multi_gpu, device, embedding_size, batch_size, backbone, carray, issame, nrof_folds = 10, tta = True):\n    if multi_gpu:\n        backbone = backbone.module # unpackage model from DataParallel\n        backbone = backbone.to(device)\n    else:\n        backbone = backbone.to(device)\n    backbone.eval() # switch to evaluation mode\n\n    idx = 0\n    embeddings = np.zeros([len(carray), embedding_size])\n    with torch.no_grad():\n        while idx + batch_size <= len(carray):\n            batch = torch.tensor(carray[idx:idx + batch_size][:, [2, 1, 0], :, :])\n            if tta:\n                ccropped = ccrop_batch(batch)\n                fliped = hflip_batch(ccropped)\n                emb_batch = backbone(ccropped.to(device)).cpu() + backbone(fliped.to(device)).cpu()\n                embeddings[idx:idx + batch_size] = l2_norm(emb_batch)\n            else:\n                ccropped = ccrop_batch(batch)\n                embeddings[idx:idx + batch_size] = l2_norm(backbone(ccropped.to(device))).cpu()\n            idx += batch_size\n        if idx < len(carray):\n            batch = torch.tensor(carray[idx:])\n            if tta:\n                ccropped = ccrop_batch(batch)\n                fliped = hflip_batch(ccropped)\n                emb_batch = backbone(ccropped.to(device)).cpu() + backbone(fliped.to(device)).cpu()\n                embeddings[idx:] = l2_norm(emb_batch)\n            else:\n                ccropped = ccrop_batch(batch)\n                embeddings[idx:] = l2_norm(backbone(ccropped.to(device))).cpu()\n\n    tpr, fpr, accuracy, best_thresholds = evaluate(embeddings, issame, nrof_folds)\n    buf = gen_plot(fpr, tpr)\n    roc_curve = Image.open(buf)\n    roc_curve_tensor = transforms.ToTensor()(roc_curve)\n\n    return accuracy.mean(), best_thresholds.mean(), roc_curve_tensor\n\n\ndef buffer_val(writer, db_name, acc, best_threshold, roc_curve_tensor, epoch):\n    writer.add_scalar(\'{}_Accuracy\'.format(db_name), acc, epoch)\n    writer.add_scalar(\'{}_Best_Threshold\'.format(db_name), best_threshold, epoch)\n    writer.add_image(\'{}_ROC_Curve\'.format(db_name), roc_curve_tensor, epoch)\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val   = 0\n        self.avg   = 0\n        self.sum   = 0\n        self.count = 0\n\n    def update(self, val, n = 1):\n        self.val   = val\n        self.sum   += val * n\n        self.count += n\n        self.avg   = self.sum / self.count\n\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the precision@k for the specified values of k""""""\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred    = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n\n    return res\n'"
util/verification.py,0,"b'""""""Helper for evaluation on the Labeled Faces in the Wild dataset\n""""""\n\n# MIT License\n#\n# Copyright (c) 2016 David Sandberg\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.decomposition import PCA\nimport sklearn\nfrom scipy import interpolate\nfrom scipy.spatial.distance import pdist\n\n\n# Support: [\'calculate_roc\', \'calculate_accuracy\', \'calculate_val\', \'calculate_val_far\', \'evaluate\']\n\n\ndef calculate_roc(thresholds, embeddings1, embeddings2, actual_issame, nrof_folds = 10, pca = 0):\n    assert (embeddings1.shape[0] == embeddings2.shape[0])\n    assert (embeddings1.shape[1] == embeddings2.shape[1])\n    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n    nrof_thresholds = len(thresholds)\n    k_fold = KFold(n_splits = nrof_folds, shuffle = False)\n\n    tprs = np.zeros((nrof_folds, nrof_thresholds))\n    fprs = np.zeros((nrof_folds, nrof_thresholds))\n    accuracy = np.zeros((nrof_folds))\n    best_thresholds = np.zeros((nrof_folds))\n    indices = np.arange(nrof_pairs)\n    # print(\'pca\', pca)\n\n    if pca == 0:\n        diff = np.subtract(embeddings1, embeddings2)\n        dist = np.sum(np.square(diff), 1)\n        # dist = pdist(np.vstack([embeddings1, embeddings2]), \'cosine\')\n\n    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n        # print(\'train_set\', train_set)\n        # print(\'test_set\', test_set)\n        if pca > 0:\n            print(""doing pca on"", fold_idx)\n            embed1_train = embeddings1[train_set]\n            embed2_train = embeddings2[train_set]\n            _embed_train = np.concatenate((embed1_train, embed2_train), axis = 0)\n            # print(_embed_train.shape)\n            pca_model = PCA(n_components = pca)\n            pca_model.fit(_embed_train)\n            embed1 = pca_model.transform(embeddings1)\n            embed2 = pca_model.transform(embeddings2)\n            embed1 = sklearn.preprocessing.normalize(embed1)\n            embed2 = sklearn.preprocessing.normalize(embed2)\n            # print(embed1.shape, embed2.shape)\n            diff = np.subtract(embed1, embed2)\n            dist = np.sum(np.square(diff), 1)\n\n        # Find the best threshold for the fold\n        acc_train = np.zeros((nrof_thresholds))\n        for threshold_idx, threshold in enumerate(thresholds):\n            _, _, acc_train[threshold_idx] = calculate_accuracy(threshold, dist[train_set], actual_issame[train_set])\n        best_threshold_index = np.argmax(acc_train)\n#         print(\'best_threshold_index\', best_threshold_index, acc_train[best_threshold_index])\n        best_thresholds[fold_idx] = thresholds[best_threshold_index]\n        for threshold_idx, threshold in enumerate(thresholds):\n            tprs[fold_idx, threshold_idx], fprs[fold_idx, threshold_idx], _ = calculate_accuracy(threshold,\n                                                                                                 dist[test_set],\n                                                                                                 actual_issame[\n                                                                                                     test_set])\n        _, _, accuracy[fold_idx] = calculate_accuracy(thresholds[best_threshold_index], dist[test_set], actual_issame[test_set])\n\n    tpr = np.mean(tprs, 0)\n    fpr = np.mean(fprs, 0)\n    return tpr, fpr, accuracy, best_thresholds\n\n\ndef calculate_accuracy(threshold, dist, actual_issame):\n    predict_issame = np.less(dist, threshold)\n    tp = np.sum(np.logical_and(predict_issame, actual_issame))\n    fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n    tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\n    fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\n\n    tpr = 0 if (tp + fn == 0) else float(tp) / float(tp + fn)\n    fpr = 0 if (fp + tn == 0) else float(fp) / float(fp + tn)\n    acc = float(tp + tn) / dist.size\n    return tpr, fpr, acc\n\n\ndef calculate_val(thresholds, embeddings1, embeddings2, actual_issame, far_target, nrof_folds = 10):\n    \'\'\'\n    Copy from [insightface](https://github.com/deepinsight/insightface)\n    :param thresholds:\n    :param embeddings1:\n    :param embeddings2:\n    :param actual_issame:\n    :param far_target:\n    :param nrof_folds:\n    :return:\n    \'\'\'\n    assert (embeddings1.shape[0] == embeddings2.shape[0])\n    assert (embeddings1.shape[1] == embeddings2.shape[1])\n    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n    nrof_thresholds = len(thresholds)\n    k_fold = KFold(n_splits = nrof_folds, shuffle = False)\n\n    val = np.zeros(nrof_folds)\n    far = np.zeros(nrof_folds)\n\n    diff = np.subtract(embeddings1, embeddings2)\n    dist = np.sum(np.square(diff), 1)\n    indices = np.arange(nrof_pairs)\n\n    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n\n        # Find the threshold that gives FAR = far_target\n        far_train = np.zeros(nrof_thresholds)\n        for threshold_idx, threshold in enumerate(thresholds):\n            _, far_train[threshold_idx] = calculate_val_far(threshold, dist[train_set], actual_issame[train_set])\n        if np.max(far_train) >= far_target:\n            f = interpolate.interp1d(far_train, thresholds, kind = \'slinear\')\n            threshold = f(far_target)\n        else:\n            threshold = 0.0\n\n        val[fold_idx], far[fold_idx] = calculate_val_far(threshold, dist[test_set], actual_issame[test_set])\n\n    val_mean = np.mean(val)\n    far_mean = np.mean(far)\n    val_std = np.std(val)\n    return val_mean, val_std, far_mean\n\n\ndef calculate_val_far(threshold, dist, actual_issame):\n    predict_issame = np.less(dist, threshold)\n    true_accept = np.sum(np.logical_and(predict_issame, actual_issame))\n    false_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n    n_same = np.sum(actual_issame)\n    n_diff = np.sum(np.logical_not(actual_issame))\n    val = float(true_accept) / float(n_same)\n    far = float(false_accept) / float(n_diff)\n    return val, far\n\n\ndef evaluate(embeddings, actual_issame, nrof_folds = 10, pca = 0):\n    # Calculate evaluation metrics\n    thresholds = np.arange(0, 4, 0.01)\n    embeddings1 = embeddings[0::2]\n    embeddings2 = embeddings[1::2]\n    tpr, fpr, accuracy, best_thresholds = calculate_roc(thresholds, embeddings1, embeddings2, np.asarray(actual_issame), nrof_folds = nrof_folds, pca = pca)\n#     thresholds = np.arange(0, 4, 0.001)\n#     val, val_std, far = calculate_val(thresholds, embeddings1, embeddings2,\n#                                       np.asarray(actual_issame), 1e-3, nrof_folds=nrof_folds)\n#     return tpr, fpr, accuracy, best_thresholds, val, val_std, far\n    return tpr, fpr, accuracy, best_thresholds\n'"
