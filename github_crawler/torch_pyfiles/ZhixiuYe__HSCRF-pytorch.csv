file_path,api_count,code
eval.py,3,"b'from __future__ import print_function\r\nimport torch\r\nimport codecs\r\nimport argparse\r\nimport json\r\n\r\nimport model.utils as utils\r\nfrom model.evaluator import evaluator\r\nfrom model.model import ner_model\r\nfrom model.data_packer import Repack\r\n\r\nif __name__ == ""__main__"":\r\n    parser = argparse.ArgumentParser(description=\'Evaluating LM-BLSTM-CRF\')\r\n    parser.add_argument(\'--load_arg\', default=\'./checkpoint/6365035.json\', help=\'path to arg json\')\r\n    parser.add_argument(\'--load_check_point\', default=\'./checkpoint/6365035.model\',\r\n                        help=\'path to model checkpoint file\')\r\n    parser.add_argument(\'--dev_file\', default=\'data/eng.testa\',\r\n                        help=\'path to development file, if set to none, would use dev_file path in the checkpoint file\')\r\n    parser.add_argument(\'--test_file\', default=\'data/eng.testb\',\r\n                        help=\'path to test file, if set to none, would use test_file path in the checkpoint file\')\r\n    args = parser.parse_args()\r\n\r\n\r\n    with open(args.load_arg, \'r\') as f:\r\n        jd = json.load(f)\r\n    jd = jd[\'args\']\r\n\r\n    checkpoint_file = torch.load(args.load_check_point)\r\n    f_map = checkpoint_file[\'f_map\']\r\n    CRF_l_map = checkpoint_file[\'CRF_l_map\']\r\n    c_map = checkpoint_file[\'c_map\']\r\n    in_doc_words = checkpoint_file[\'in_doc_words\']\r\n    SCRF_l_map = checkpoint_file[\'SCRF_l_map\']\r\n    ALLOW_SPANLEN = checkpoint_file[\'ALLOW_SPANLEN\']\r\n\r\n    with codecs.open(args.dev_file, \'r\', \'utf-8\') as f:\r\n        dev_lines = f.readlines()\r\n\r\n    with codecs.open(args.test_file, \'r\', \'utf-8\') as f:\r\n        test_lines = f.readlines()\r\n\r\n\r\n    dev_features, dev_labels = utils.read_corpus(dev_lines)\r\n    test_features, test_labels = utils.read_corpus(test_lines)\r\n\r\n    dev_dataset = utils.construct_bucket_mean_vb_wc(dev_features, dev_labels, CRF_l_map, SCRF_l_map, c_map, f_map, SCRF_stop_tag=SCRF_l_map[\'<STOP>\'], train_set=False)\r\n    test_dataset = utils.construct_bucket_mean_vb_wc(test_features, test_labels, CRF_l_map, SCRF_l_map, c_map, f_map, SCRF_stop_tag=SCRF_l_map[\'<STOP>\'], train_set=False)\r\n\r\n    dev_dataset_loader = [torch.utils.data.DataLoader(tup, 50, shuffle=False, drop_last=False) for tup in dev_dataset]\r\n    test_dataset_loader = [torch.utils.data.DataLoader(tup, 50, shuffle=False, drop_last=False) for tup in test_dataset]\r\n\r\n    print(\'build model\')\r\n    model = ner_model(jd[\'word_embedding_dim\'], jd[\'word_hidden_dim\'], jd[\'word_lstm_layers\'],\r\n                      len(f_map), len(c_map), jd[\'char_embedding_dim\'], jd[\'char_lstm_hidden_dim\'],\r\n                      jd[\'cnn_filter_num\'], jd[\'char_lstm_layers\'], jd[\'char_lstm\'],jd[\'dropout_ratio\'],\r\n                      jd[\'high_way\'], jd[\'highway_layers\'], CRF_l_map[\'<start>\'], CRF_l_map[\'<pad>\'],\r\n                      len(CRF_l_map), SCRF_l_map, jd[\'scrf_dense_dim\'], in_doc_words,\r\n                      jd[\'index_embeds_dim\'], jd[\'allowspan\'], SCRF_l_map[\'<START>\'], SCRF_l_map[\'<STOP>\'],\r\n                      jd[\'grconv\'])\r\n\r\n    print(\'load model\')\r\n    model.load_state_dict(checkpoint_file[\'state_dict\'])\r\n\r\n    model.cuda()\r\n    packer = Repack()\r\n\r\n    evaluator = evaluator(packer, CRF_l_map, SCRF_l_map)\r\n\r\n\r\n    print(\'dev...\')\r\n    dev_f1_crf, dev_pre_crf, dev_rec_crf, dev_acc_crf, dev_f1_scrf, dev_pre_scrf, dev_rec_scrf, dev_acc_scrf, dev_f1_jnt, dev_pre_jnt, dev_rec_jnt, dev_acc_jnt = \\\r\n            evaluator.calc_score(model, dev_dataset_loader)\r\n    print(\'test...\')\r\n    test_f1_crf, test_pre_crf, test_rec_crf, test_acc_crf, test_f1_scrf, test_pre_scrf, test_rec_scrf, test_acc_scrf, test_f1_jnt, test_pre_jnt, test_rec_jnt, test_acc_jnt = \\\r\n            evaluator.calc_score(model, test_dataset_loader)\r\n\r\n    print(\' dev_f1: %.4f\\n\' % (dev_f1_crf))\r\n    print(\' dev_f1_scrf: %.4f\\n\' % (dev_f1_scrf))\r\n    print(\' dev_f1_jnt: %.4f\\n\' % (dev_f1_jnt))\r\n\r\n    print(\' test_f1: %.4f\\n\' % (test_f1_crf))\r\n    print(\' test_f1_scrf: %.4f\\n\' % (test_f1_scrf))\r\n    print(\' test_f1_jnt: %.4f\\n\' % (test_f1_jnt))\r\n\r\n\r\n'"
train.py,9,"b'from __future__ import print_function\r\nimport time\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport codecs\r\nimport argparse\r\nimport os\r\nimport sys\r\nfrom tqdm import tqdm\r\nimport itertools\r\nimport functools\r\nimport numpy as np\r\n\r\nimport model.utils as utils\r\nfrom model.evaluator import evaluator\r\nfrom model.model import ner_model\r\nfrom model.data_packer import Repack\r\n\r\n\r\n# seed = int(np.random.uniform(0,1)*10000000)\r\nseed = 5703958\r\ntorch.manual_seed(seed)\r\ntorch.cuda.manual_seed(seed)\r\nnp.random.seed(seed)\r\nprint(\'seed: \', seed)\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    parser = argparse.ArgumentParser(description=\'Learning with LM-LSTM-CRF together with Language Model\')\r\n    parser.add_argument(\'--emb_file\', default=\'./data/glove.6B.100d.txt\', help=\'path to pre-trained embedding\')\r\n    parser.add_argument(\'--train_file\', default=\'./data/eng.train\', help=\'path to training file\')\r\n    parser.add_argument(\'--dev_file\', default=\'./data/eng.testa\', help=\'path to development file\')\r\n    parser.add_argument(\'--test_file\', default=\'./data/eng.testb\', help=\'path to test file\')\r\n    parser.add_argument(\'--batch_size\', type=int, default=10, help=\'batch_size\')\r\n    parser.add_argument(\'--unk\', default=\'unk\', help=\'unknow-token in pre-trained embedding\')\r\n    parser.add_argument(\'--char_lstm_hidden_dim\', type=int, default=300, help=\'dimension of char-level lstm layer for language model\')\r\n    parser.add_argument(\'--word_hidden_dim\', type=int, default=300, help=\'dimension of word-level lstm layer\')\r\n    parser.add_argument(\'--dropout_ratio\', type=float, default=0.55, help=\'dropout ratio\')\r\n    parser.add_argument(\'--epoch\', type=int, default=150, help=\'maximum epoch number\')\r\n    parser.add_argument(\'--least_epoch\', type=int, default=75, help=\'minimum epoch number\')\r\n    parser.add_argument(\'--early_stop\', type=int, default=10, help=\'early stop epoch number\')\r\n    parser.add_argument(\'--start_epoch\', type=int, default=0, help=\'start point of epoch\')\r\n    parser.add_argument(\'--checkpoint\', default=\'./checkpoint/\', help=\'checkpoint path\')\r\n    parser.add_argument(\'--word_embedding_dim\', type=int, default=100, help=\'dimension of word embedding\')\r\n    parser.add_argument(\'--char_embedding_dim\', type=int, default=30, help=\'dimension of character embedding\')\r\n    parser.add_argument(\'--scrf_dense_dim\', type=int, default=100, help=\'dimension of scrf features\')\r\n    parser.add_argument(\'--index_embeds_dim\', type=int, default=10, help=\'dimension of index embedding\')\r\n    parser.add_argument(\'--cnn_filter_num\', type=int, default=30, help=\'the number of cnn filters\')\r\n    parser.add_argument(\'--char_lstm_layers\', type=int, default=1, help=\'number of char level layers for language model\')\r\n    parser.add_argument(\'--word_lstm_layers\', type=int, default=1, help=\'number of word level layers\')\r\n    parser.add_argument(\'--lr\', type=float, default=0.015, help=\'initial learning rate\')\r\n    parser.add_argument(\'--lr_decay\', type=float, default=0.05, help=\'decay ratio of learning rate\')\r\n    parser.add_argument(\'--load_check_point\', default=\'\', help=\'path previous checkpoint that want to be loaded\')\r\n    parser.add_argument(\'--load_opt\', action=\'store_true\', help=\'also load optimizer from the checkpoint\')\r\n    parser.add_argument(\'--momentum\', type=float, default=0.9, help=\'momentum for sgd\')\r\n    parser.add_argument(\'--clip_grad\', type=float, default=5.0, help=\'clip grad at\')\r\n    parser.add_argument(\'--mini_count\', type=float, default=5, help=\'thresholds to replace rare words with <unk>\')\r\n    parser.add_argument(\'--high_way\', action=\'store_true\', help=\'use highway layers\')\r\n    parser.add_argument(\'--highway_layers\', type=int, default=1, help=\'number of highway layers\')\r\n    parser.add_argument(\'--shrink_embedding\', action=\'store_true\', help=\'shrink the embedding dictionary to corpus (open this if pre-trained embedding dictionary is too large, but disable this may yield better results on external corpus)\')\r\n    parser.add_argument(\'--model_name\', default=\'HSCRF\', help=\'model name\')\r\n    parser.add_argument(\'--char_lstm\', action=\'store_true\', help=\'use lstm for characters embedding or not\')\r\n    parser.add_argument(\'--allowspan\', type=int, default=6, help=\'allowed max segment length\')\r\n    parser.add_argument(\'--grconv\', action=\'store_true\', help=\'use grconv\')\r\n\r\n    args = parser.parse_args()\r\n\r\n    CRF_l_map, SCRF_l_map = utils.get_crf_scrf_label()\r\n\r\n    print(\'setting:\')\r\n    print(args)\r\n\r\n    print(\'loading corpus\')\r\n    with codecs.open(args.train_file, \'r\', \'utf-8\') as f:\r\n        lines = f.readlines()\r\n    with codecs.open(args.dev_file, \'r\', \'utf-8\') as f:\r\n        dev_lines = f.readlines()\r\n    with codecs.open(args.test_file, \'r\', \'utf-8\') as f:\r\n        test_lines = f.readlines()\r\n\r\n    dev_features, dev_labels = utils.read_corpus(dev_lines)\r\n    test_features, test_labels = utils.read_corpus(test_lines)\r\n\r\n    if args.load_check_point:\r\n        if os.path.isfile(args.load_check_point):\r\n            print(""loading checkpoint: \'{}\'"".format(args.load_check_point))\r\n            checkpoint_file = torch.load(args.load_check_point)\r\n            args.start_epoch = checkpoint_file[\'epoch\']\r\n            f_map = checkpoint_file[\'f_map\']\r\n            c_map = checkpoint_file[\'c_map\']\r\n            in_doc_words = checkpoint_file[\'in_doc_words\']\r\n            train_features, train_labels = utils.read_corpus(lines)\r\n        else:\r\n            print(""no checkpoint found at: \'{}\'"".format(args.load_check_point))\r\n            sys.exit()\r\n    else:\r\n        print(\'constructing coding table\')\r\n\r\n        train_features, train_labels, f_map, _, c_map = \\\r\n            utils.generate_corpus_char(lines, if_shrink_c_feature=True,\r\n                                       c_thresholds=args.mini_count,\r\n                                       if_shrink_w_feature=False)\r\n\r\n        f_set = {v for v in f_map}\r\n\r\n        f_map = utils.shrink_features(f_map, train_features, args.mini_count)\r\n        dt_f_set = functools.reduce(lambda x, y: x | y, map(lambda t: set(t), dev_features), f_set)\r\n        dt_f_set = functools.reduce(lambda x, y: x | y, map(lambda t: set(t), test_features), dt_f_set)\r\n\r\n        f_map, embedding_tensor, in_doc_words = utils.load_embedding(args.emb_file, \' \', f_map, dt_f_set, args.unk, args.word_embedding_dim, shrink_to_corpus=args.shrink_embedding)\r\n\r\n        l_set = functools.reduce(lambda x, y: x | y, map(lambda t: set(t), dev_labels))\r\n        l_set = functools.reduce(lambda x, y: x | y, map(lambda t: set(t), test_labels), l_set)\r\n\r\n    print(\'constructing dataset\')\r\n    dataset, dataset_onlycrf = utils.construct_bucket_mean_vb_wc(train_features, train_labels, CRF_l_map, SCRF_l_map, c_map, f_map, SCRF_stop_tag=SCRF_l_map[\'<STOP>\'], ALLOW_SPANLEN=args.allowspan, train_set=True)\r\n    dev_dataset = utils.construct_bucket_mean_vb_wc(dev_features, dev_labels, CRF_l_map, SCRF_l_map, c_map, f_map, SCRF_stop_tag=SCRF_l_map[\'<STOP>\'], train_set=False)\r\n    test_dataset = utils.construct_bucket_mean_vb_wc(test_features, test_labels, CRF_l_map, SCRF_l_map, c_map, f_map, SCRF_stop_tag=SCRF_l_map[\'<STOP>\'], train_set=False)\r\n\r\n    dataset_loader = [torch.utils.data.DataLoader(tup, args.batch_size, shuffle=True, drop_last=False) for tup in dataset]\r\n    dataset_loader_crf = [torch.utils.data.DataLoader(tup, 3, shuffle=True, drop_last=False) for tup in dataset_onlycrf] if dataset_onlycrf else None\r\n    dev_dataset_loader = [torch.utils.data.DataLoader(tup, 50, shuffle=False, drop_last=False) for tup in dev_dataset]\r\n    test_dataset_loader = [torch.utils.data.DataLoader(tup, 50, shuffle=False, drop_last=False) for tup in test_dataset]\r\n\r\n    print(\'building model\')\r\n    model = ner_model(args.word_embedding_dim, args.word_hidden_dim, args.word_lstm_layers, len(f_map),\r\n                      len(c_map), args.char_embedding_dim, args.char_lstm_hidden_dim, args.cnn_filter_num,\r\n                      args.char_lstm_layers, args.char_lstm, args.dropout_ratio, args.high_way, args.highway_layers,\r\n                      CRF_l_map[\'<start>\'], CRF_l_map[\'<pad>\'], len(CRF_l_map), SCRF_l_map, args.scrf_dense_dim,\r\n                      in_doc_words,args.index_embeds_dim, args.allowspan, SCRF_l_map[\'<START>\'], SCRF_l_map[\'<STOP>\'], args.grconv)\r\n\r\n    if args.load_check_point:\r\n        model.load_state_dict(checkpoint_file[\'state_dict\'])\r\n    else:\r\n        model.word_rep.load_pretrained_word_embedding(embedding_tensor)\r\n        model.word_rep.rand_init()\r\n\r\n    optimizer = optim.SGD(model.parameters(),\r\n                           lr=args.lr, momentum=args.momentum)\r\n    # optimizer = optim.Adam(model.parameters())\r\n\r\n    if args.load_check_point and args.load_opt:\r\n        optimizer.load_state_dict(checkpoint_file[\'optimizer\'])\r\n\r\n    model.cuda()\r\n    packer = Repack()\r\n\r\n    tot_length = sum(map(lambda t: len(t), dataset_loader))\r\n\r\n    best_dev_f1_jnt = float(\'-inf\')\r\n    best_test_f1_crf = float(\'-inf\')\r\n    best_test_f1_scrf = float(\'-inf\')\r\n    best_test_f1_jnt = float(\'-inf\')\r\n    start_time = time.time()\r\n    early_stop_epochs = 0\r\n    epoch_list = range(args.start_epoch, args.start_epoch + args.epoch)\r\n\r\n    evaluator = evaluator(packer, CRF_l_map, SCRF_l_map)\r\n\r\n    for epoch_idx, args.start_epoch in enumerate(epoch_list):\r\n\r\n        epoch_loss = 0\r\n        model.train()\r\n        if dataset_loader_crf:\r\n            for f_f, f_p, b_f, b_p, w_f, tg_v, mask_v, len_v, SCRF_labels, mask_SCRF_labels, cnn_features in tqdm(\r\n                itertools.chain.from_iterable(dataset_loader_crf), mininterval=2,\r\n                desc=\' - Tot it %d (epoch %d)\' % (tot_length, args.start_epoch), leave=False, file=sys.stderr):\r\n\r\n                f_f, f_p, b_f, b_p, w_f, tg_v, mask_v, SCRF_labels, mask_SCRF_labels, cnn_features = packer.repack(f_f, f_p, b_f, b_p, w_f, tg_v, mask_v, len_v, SCRF_labels, mask_SCRF_labels, cnn_features, test=False)\r\n\r\n                optimizer.zero_grad()\r\n\r\n                loss = model(f_f, f_p, b_f, b_p, w_f, cnn_features, tg_v, mask_v,\r\n                      mask_v.long().sum(0), SCRF_labels, mask_SCRF_labels, onlycrf=True)\r\n\r\n                epoch_loss += utils.to_scalar(loss)\r\n\r\n                loss.backward()\r\n                nn.utils.clip_grad_norm(model.parameters(), args.clip_grad)\r\n                optimizer.step()\r\n\r\n        for f_f, f_p, b_f, b_p, w_f, tg_v, mask_v, len_v, SCRF_labels, mask_SCRF_labels, cnn_features in tqdm(\r\n                itertools.chain.from_iterable(dataset_loader), mininterval=2,\r\n                desc=\' - Tot it %d (epoch %d)\' % (tot_length, args.start_epoch), leave=False, file=sys.stderr):\r\n\r\n            f_f, f_p, b_f, b_p, w_f, tg_v, mask_v, SCRF_labels, mask_SCRF_labels, cnn_features = packer.repack(f_f, f_p, b_f, b_p, w_f, tg_v, mask_v, len_v, SCRF_labels, mask_SCRF_labels, cnn_features, test=False)\r\n            optimizer.zero_grad()\r\n\r\n            loss = model(f_f, f_p, b_f, b_p, w_f, cnn_features, tg_v, mask_v,\r\n                         mask_v.long().sum(0), SCRF_labels, mask_SCRF_labels, onlycrf=False)\r\n\r\n            epoch_loss += utils.to_scalar(loss)\r\n            loss.backward()\r\n            nn.utils.clip_grad_norm(model.parameters(), args.clip_grad)\r\n            optimizer.step()\r\n\r\n        epoch_loss /= tot_length\r\n        print(\'epoch_loss: \', epoch_loss)\r\n\r\n        utils.adjust_learning_rate(optimizer, args.lr / (1 + (args.start_epoch + 1) * args.lr_decay))\r\n\r\n\r\n        dev_f1_crf, dev_pre_crf, dev_rec_crf, dev_acc_crf, dev_f1_scrf, dev_pre_scrf, dev_rec_scrf, dev_acc_scrf, dev_f1_jnt, dev_pre_jnt, dev_rec_jnt, dev_acc_jnt = \\\r\n                evaluator.calc_score(model, dev_dataset_loader)\r\n\r\n        if dev_f1_jnt > best_dev_f1_jnt:\r\n            early_stop_epochs = 0\r\n            test_f1_crf, test_pre_crf, test_rec_crf, test_acc_crf, test_f1_scrf, test_pre_scrf, test_rec_scrf, test_acc_scrf, test_f1_jnt, test_pre_jnt, test_rec_jnt, test_acc_jnt = \\\r\n                        evaluator.calc_score(model, test_dataset_loader)\r\n\r\n            best_test_f1_crf = test_f1_crf\r\n            best_test_f1_scrf = test_f1_scrf\r\n\r\n            best_dev_f1_jnt = dev_f1_jnt\r\n            best_test_f1_jnt = test_f1_jnt\r\n\r\n            try:\r\n                utils.save_checkpoint({\r\n                        \'epoch\': args.start_epoch,\r\n                        \'state_dict\': model.state_dict(),\r\n                        \'optimizer\': optimizer.state_dict(),\r\n                        \'f_map\': f_map,\r\n                        \'c_map\': c_map,\r\n                        \'SCRF_l_map\': SCRF_l_map,\r\n                        \'CRF_l_map\': CRF_l_map,\r\n                        \'in_doc_words\': in_doc_words,\r\n                        \'ALLOW_SPANLEN\': args.allowspan\r\n                    }, {\'args\': vars(args)\r\n                        }, args.checkpoint + str(seed))\r\n            except Exception as inst:\r\n                    print(inst)\r\n\r\n        else:\r\n            early_stop_epochs += 1\r\n\r\n        print(\'best_test_f1_crf is: %.4f\' % (best_test_f1_crf))\r\n        print(\'best_test_f1_scrf is: %.4f\' % (best_test_f1_scrf))\r\n        print(\'best_test_f1_jnt is: %.4f\' % (best_test_f1_jnt))\r\n\r\n        print(\'epoch: \' + str(args.start_epoch) + \'\\t in \' + str(args.epoch) + \' take: \' + str(\r\n            time.time() - start_time) + \' s\')\r\n\r\n        sys.stdout.flush()\r\n\r\n        if early_stop_epochs >= args.early_stop and epoch_idx > args.least_epoch:\r\n            break\r\n\r\n\r\n    print(\'setting:\')\r\n    print(args)\r\n    print(\'seed: \', seed)'"
model/__init__.py,0,b''
model/crf_layer.py,6,"b'import torch\r\nimport torch.nn as nn\r\nimport utils\r\nfrom torch.autograd import Variable\r\n\r\n\r\nclass CRF(nn.Module):\r\n    """"""\r\n    Conditional Random Field (CRF) layer.\r\n\r\n    """"""\r\n    \r\n    def __init__(self, start_tag, end_tag, hidden_dim, tagset_size):\r\n        """"""\r\n\r\n        args:\r\n            start_tag  (scalar) : special start tag for CRF\r\n            end_tag    (scalar) : special end tag for CRF\r\n            hidden_dim (scalar) : input dim size\r\n            tagset_size(scalar) : target_set_size\r\n\r\n        """"""\r\n        super(CRF, self).__init__()\r\n        self.tagset_size = tagset_size\r\n        self.start_tag = start_tag\r\n        self.end_tag = end_tag\r\n        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size * self.tagset_size)\r\n        self.rand_init()\r\n\r\n    def rand_init(self):\r\n        """"""\r\n        random initialization\r\n\r\n        """"""\r\n\r\n        utils.init_linear(self.hidden2tag)\r\n\r\n    def cal_score(self, feats):\r\n        """"""\r\n        calculate CRF score\r\n\r\n        :param feats (sentlen, batch_size, feature_num) : input features\r\n        """"""\r\n\r\n        sentlen = feats.size(0)\r\n        batch_size = feats.size(1)\r\n        crf_scores = self.hidden2tag(feats).view(-1, self.tagset_size, self.tagset_size)\r\n        self.crf_scores = crf_scores.view(sentlen, batch_size, self.tagset_size, self.tagset_size)\r\n        return self.crf_scores\r\n\r\n    def forward(self, feats, target, mask):\r\n        """"""\r\n        calculate viterbi loss\r\n\r\n        args:\r\n            feats  (batch_size, seq_len, hidden_dim) : input features from word_rep layers\r\n            target (batch_size, seq_len, 1) : crf label\r\n            mask   (batch_size, seq_len) : mask for crf label\r\n\r\n        """"""\r\n\r\n        crf_scores = self.cal_score(feats)\r\n        loss = self.get_loss(crf_scores, target, mask)\r\n        return loss\r\n\r\n    def get_loss(self, scores, target, mask):\r\n        """"""\r\n        calculate viterbi loss\r\n\r\n        args:\r\n            scores (seq_len, bat_size, target_size_from, target_size_to) : class score for CRF\r\n            target (seq_len, bat_size, 1) : crf label\r\n            mask   (seq_len, bat_size) : mask for crf label\r\n\r\n        """"""\r\n\r\n        seq_len = scores.size(0)\r\n        bat_size = scores.size(1)\r\n\r\n        tg_energy = torch.gather(scores.view(seq_len, bat_size, -1), 2, target).view(seq_len, bat_size)  # seq_len * bat_size\r\n        tg_energy = tg_energy.masked_select(mask).sum()\r\n\r\n        seq_iter = enumerate(scores)\r\n        _, inivalues = seq_iter.next()\r\n        partition = inivalues[:, self.start_tag, :].clone()\r\n        for idx, cur_values in seq_iter:\r\n            cur_values = cur_values + partition.contiguous().view(bat_size, self.tagset_size, 1).\\\r\n                expand(bat_size, self.tagset_size, self.tagset_size)\r\n            cur_partition = utils.log_sum_exp(cur_values, self.tagset_size)\r\n            mask_idx = mask[idx, :].view(bat_size, 1).expand(bat_size, self.tagset_size)\r\n            partition.masked_scatter_(mask_idx,\r\n                                      cur_partition.masked_select(mask_idx))\r\n\r\n        partition = partition[:, self.end_tag].sum()\r\n        loss = (partition - tg_energy) / bat_size\r\n\r\n        return loss\r\n\r\n    def decode(self, feats, mask):\r\n        """"""\r\n        decode with dynamic programming\r\n\r\n        args:\r\n            feats (sentlen, batch_size, feature_num) : input features\r\n            mask (seq_len, bat_size) : mask for padding\r\n\r\n        """"""\r\n\r\n        scores = self.cal_score(feats)\r\n        seq_len = scores.size(0)\r\n        bat_size = scores.size(1)\r\n\r\n        mask = Variable(1 - mask.data, volatile=True)\r\n        decode_idx = Variable(torch.cuda.LongTensor(seq_len-1, bat_size), volatile=True)\r\n\r\n        seq_iter = enumerate(scores)\r\n        _, inivalues = seq_iter.next()\r\n        forscores = inivalues[:, self.start_tag, :]\r\n        back_points = list()\r\n        for idx, cur_values in seq_iter:\r\n            cur_values = cur_values + forscores.contiguous().view(bat_size, self.tagset_size, 1).\\\r\n                                                  expand(bat_size, self.tagset_size, self.tagset_size)\r\n            forscores, cur_bp = torch.max(cur_values, 1)\r\n            cur_bp.masked_fill_(mask[idx].view(bat_size, 1).expand(bat_size, self.tagset_size), self.end_tag)\r\n            back_points.append(cur_bp)\r\n\r\n        pointer = back_points[-1][:, self.end_tag]\r\n        decode_idx[-1] = pointer\r\n        for idx in range(len(back_points)-2, -1, -1):\r\n            pointer = torch.gather(back_points[idx], 1, pointer.contiguous().view(bat_size, 1))\r\n            decode_idx[idx] = pointer\r\n        return decode_idx\r\n\r\n\r\n'"
model/data_packer.py,2,"b'from torch.autograd import Variable\nfrom torch.utils.data import Dataset\n\n\nclass Repack:\n    """"""\n    Packer for data\n\n    """"""\n\n    def __init__(self):\n        pass\n\n    def repack(self, f_f, f_p, b_f, b_p, w_f, target, mask, len_b, SCRF_labels, mask_SCRF_laebls, cnn_features, test=False):\n        """"""\n        packing data\n\n        args:\n            f_f              (Batch_size, Char_Seq_len) : forward_char input feature\n            f_p              (Batch_size, Word_Seq_len) : forward_char input position\n            b_f              (Batch_size, Char_Seq_len) : backward_char input feature\n            b_p              (Batch_size, Word_Seq_len) : backward_char input position\n            w_f              (Batch_size, Word_Seq_len) : input word feature\n            target           (Batch_size, Seq_len) : output target\n            mask             (Batch_size, Word_Seq_len) : padding mask\n            len_b            (Batch_size, 3) : length of instances in one batch\n            SCRF_labels      (Batch_size, Word_Seq_len, 4)  : Semi-CRF labels\n            mask_SCRF_laebls (Batch_size, Word_Seq_len) : mask of Semi-CRF labels\n            cnn_features     (Batch_size, Word_Seq_len, Word_len): characters features for cnn\n\n        return:\n            f_f (Char_Reduced_Seq_len, Batch_size),\n            f_p (Word_Reduced_Seq_len, Batch_size),\n            b_f (Char_Reduced_Seq_len, Batch_size),\n            b_p (Word_Reduced_Seq_len, Batch_size),\n            w_f (size Word_Seq_Len, Batch_size),\n            target (Reduced_Seq_len, Batch_size),\n            mask  (Word_Reduced_Seq_len, Batch_size)\n            SCRF_labels (Batch_size, Word_Reduced_Seq_len, 4)\n            mask_SCRF_laebls (Batch_size, Word_Reduced_Seq_len)\n            cnn_features     (Batch_size, Word_Reduced_Seq_len, word_len)\n\n        """"""\n        mlen, _ = len_b.max(0)\n        mlen = mlen.squeeze()\n        ocl = b_f.size(1)\n\n        if test:\n            f_f = Variable(f_f[:, 0:mlen[0]].transpose(0, 1), volatile=True).cuda()\n            f_p = Variable(f_p[:, 0:mlen[1]].transpose(0, 1), volatile=True).cuda()\n            b_f = Variable(b_f[:, -mlen[0]:].transpose(0, 1), volatile=True).cuda()\n            b_p = Variable((b_p[:, 0:mlen[1]] - ocl + mlen[0]).transpose(0, 1), volatile=True).cuda()\n            w_f = Variable(w_f[:, 0:mlen[1]].transpose(0, 1), volatile=True).cuda()\n            tg_v = Variable(target[:, 0:mlen[1]].transpose(0, 1), volatile=True).unsqueeze(2).cuda()\n            mask_v = Variable(mask[:, 0:mlen[1]].transpose(0, 1), volatile=True).cuda()\n            SCRF_labels = Variable(SCRF_labels[:, 0:mlen[2]], volatile=True).cuda()\n            mask_SCRF_laebls = Variable(mask_SCRF_laebls[:, 0:mlen[2]], volatile=True).cuda()\n            cnn_features = Variable(cnn_features[:, 0:mlen[1], 0:mlen[3]].transpose(0, 1), volatile=True).cuda().contiguous()\n        else:\n            f_f = Variable(f_f[:, 0:mlen[0]].transpose(0, 1)).cuda()\n            f_p = Variable(f_p[:, 0:mlen[1]].transpose(0, 1)).cuda()\n            b_f = Variable(b_f[:, -mlen[0]:].transpose(0, 1)).cuda()\n            b_p = Variable((b_p[:, 0:mlen[1]] - ocl + mlen[0]).transpose(0, 1)).cuda()\n            w_f = Variable(w_f[:, 0:mlen[1]].transpose(0, 1)).cuda()\n            tg_v = Variable(target[:, 0:mlen[1]].transpose(0, 1)).unsqueeze(2).cuda()\n            mask_v = Variable(mask[:, 0:mlen[1]].transpose(0, 1)).cuda()\n            SCRF_labels = Variable(SCRF_labels[:, 0:mlen[2]]).cuda()\n            mask_SCRF_laebls = Variable(mask_SCRF_laebls[:, 0:mlen[2]]).cuda()\n            cnn_features = Variable(cnn_features[:, 0:mlen[1], 0:mlen[3]].transpose(0, 1)).cuda().contiguous()\n\n        return f_f, f_p, b_f, b_p, w_f, tg_v, mask_v, SCRF_labels, mask_SCRF_laebls, cnn_features\n\n\n\nclass CRFDataset_WC(Dataset):\n    """"""\n    Dataset Class for ner model\n\n    """"""\n    def __init__(self, forw_tensor, forw_index, back_tensor, back_index, word_tensor, label_tensor, mask_tensor, len_tensor, SCRFlabels, mask_SCRFlabels, cnn_features):\n        assert forw_tensor.size(0) == label_tensor.size(0)\n        assert forw_tensor.size(0) == mask_tensor.size(0)\n        assert forw_tensor.size(0) == forw_index.size(0)\n        assert forw_tensor.size(0) == back_tensor.size(0)\n        assert forw_tensor.size(0) == back_index.size(0)\n        assert forw_tensor.size(0) == word_tensor.size(0)\n        assert forw_tensor.size(0) == len_tensor.size(0)\n        assert forw_tensor.size(0) == SCRFlabels.size(0)\n        assert forw_tensor.size(0) == mask_SCRFlabels.size(0)\n        assert forw_tensor.size(0) == cnn_features.size(0)\n\n        self.forw_tensor = forw_tensor\n        self.forw_index = forw_index\n        self.back_tensor = back_tensor\n        self.back_index = back_index\n        self.word_tensor = word_tensor\n        self.label_tensor = label_tensor\n        self.mask_tensor = mask_tensor\n        self.len_tensor = len_tensor\n        self.SCRFlabels = SCRFlabels\n        self.mask_SCRFlabels = mask_SCRFlabels\n        self.cnn_features = cnn_features\n\n    def __getitem__(self, index):\n        return self.forw_tensor[index], self.forw_index[index], self.back_tensor[index], self.back_index[index], self.word_tensor[index], self.label_tensor[index], \\\n               self.mask_tensor[index], self.len_tensor[index], self.SCRFlabels[index], self.mask_SCRFlabels[index], self.cnn_features[index]\n\n    def __len__(self):\n        return self.forw_tensor.size(0)\n\n\n\n'"
model/evaluator.py,0,"b'from __future__ import division\r\nimport numpy as np\r\nimport itertools\r\n\r\nimport utils\r\n\r\n\r\nclass evaluator():\r\n    """"""\r\n    evaluation class for ner task\r\n\r\n    """"""\r\n\r\n    def __init__(self, packer, l_map, SCRF_l_map):\r\n\r\n        self.packer = packer\r\n        self.l_map = l_map\r\n        self.SCRF_l_map = SCRF_l_map\r\n        self.r_l_map = utils.revlut(l_map)\r\n        self.SCRF_r_l_map = utils.revlut(SCRF_l_map)\r\n\r\n    def reset(self):\r\n        """"""\r\n        re-set all states\r\n\r\n        """"""\r\n        self.correct_labels_crf = 0\r\n        self.total_labels_crf = 0\r\n        self.gold_count_crf = 0\r\n        self.guess_count_crf = 0\r\n        self.overlap_count_crf = 0\r\n\r\n        self.correct_labels_scrf = 0\r\n        self.total_labels_scrf = 0\r\n        self.gold_count_scrf = 0\r\n        self.guess_count_scrf = 0\r\n        self.overlap_count_scrf = 0\r\n\r\n        self.correct_labels_jnt = 0\r\n        self.total_labels_jnt = 0\r\n        self.gold_count_jnt = 0\r\n        self.guess_count_jnt = 0\r\n        self.overlap_count_jnt = 0\r\n\r\n\r\n    def calc_f1_batch(self, target_data, decoded_data_crfs, decode_data_scrfs, decode_data_jnts):\r\n        """"""\r\n        update statics for f1 score\r\n\r\n        args:\r\n            decoded_data (batch_size, seq_len): prediction sequence\r\n            target_data (batch_size, seq_len): ground-truth\r\n\r\n        """"""\r\n        for target, decoded_data_crf, decode_data_scrf, decode_data_jnt in zip(target_data, decoded_data_crfs, decode_data_scrfs, decode_data_jnts):\r\n\r\n            length = utils.find_length_from_labels(target, self.l_map)\r\n            gold = target[:length]\r\n            decoded_data_crf = decoded_data_crf[:length]\r\n            decode_data_scrf = decode_data_scrf[:length]\r\n            decode_data_jnt = decode_data_jnt[:length]\r\n\r\n            correct_labels_i, total_labels_i, gold_count_i, guess_count_i, overlap_count_i = self.eval_instance(\r\n                decoded_data_crf, gold)\r\n            self.correct_labels_crf += correct_labels_i\r\n            self.total_labels_crf += total_labels_i\r\n            self.gold_count_crf += gold_count_i\r\n            self.guess_count_crf += guess_count_i\r\n            self.overlap_count_crf += overlap_count_i\r\n\r\n            correct_labels_i, total_labels_i, gold_count_i, guess_count_i, overlap_count_i = self.eval_instance(\r\n                decode_data_scrf, gold)\r\n            self.correct_labels_scrf += correct_labels_i\r\n            self.total_labels_scrf += total_labels_i\r\n            self.gold_count_scrf += gold_count_i\r\n            self.guess_count_scrf += guess_count_i\r\n            self.overlap_count_scrf += overlap_count_i\r\n\r\n            correct_labels_i, total_labels_i, gold_count_i, guess_count_i, overlap_count_i = self.eval_instance(\r\n                decode_data_jnt, gold)\r\n            self.correct_labels_jnt += correct_labels_i\r\n            self.total_labels_jnt += total_labels_i\r\n            self.gold_count_jnt += gold_count_i\r\n            self.guess_count_jnt += guess_count_i\r\n            self.overlap_count_jnt += overlap_count_i\r\n\r\n\r\n    def f1_score(self):\r\n        """"""\r\n        calculate f1 score batgsed on statics\r\n\r\n        """"""\r\n        if self.guess_count_crf == 0:\r\n            f_crf, precision_crf, recall_crf, accuracy_crf = 0.0, 0.0, 0.0, 0.0\r\n        else:\r\n            precision_crf = self.overlap_count_crf / float(self.guess_count_crf)\r\n            recall_crf = self.overlap_count_crf / float(self.gold_count_crf)\r\n            if precision_crf == 0.0 or recall_crf == 0.0:\r\n                f_crf, precision_crf, recall_crf, accuracy_crf = 0.0, 0.0, 0.0, 0.0\r\n            else:\r\n                f_crf = 2 * (precision_crf * recall_crf) / (precision_crf + recall_crf)\r\n                accuracy_crf = float(self.correct_labels_crf) / self.total_labels_crf\r\n\r\n        if self.guess_count_scrf == 0:\r\n            f_scrf, precision_scrf, recall_scrf, accuracy_scrf = 0.0, 0.0, 0.0, 0.0\r\n        else:\r\n            precision_scrf = self.overlap_count_scrf / float(self.guess_count_scrf)\r\n            recall_scrf = self.overlap_count_scrf / float(self.gold_count_scrf)\r\n            if precision_scrf == 0.0 or recall_scrf == 0.0:\r\n                f_scrf, precision_scrf, recall_scrf, accuracy_scrf = 0.0, 0.0, 0.0, 0.0\r\n            else:\r\n                f_scrf = 2 * (precision_scrf * recall_scrf) / (precision_scrf + recall_scrf)\r\n                accuracy_scrf = float(self.correct_labels_scrf) / self.total_labels_scrf\r\n\r\n        if self.guess_count_jnt == 0:\r\n            f_jnt, precision_jnt, recall_jnt, accuracy_jnt = 0.0, 0.0, 0.0, 0.0\r\n        else:\r\n            precision_jnt = self.overlap_count_jnt / float(self.guess_count_jnt)\r\n            recall_jnt = self.overlap_count_jnt / float(self.gold_count_jnt)\r\n            if precision_jnt == 0.0 or recall_jnt == 0.0:\r\n                f_jnt, precision_jnt, recall_jnt, accuracy_jnt = 0.0, 0.0, 0.0, 0.0\r\n            else:\r\n                f_jnt = 2 * (precision_jnt * recall_jnt) / (precision_jnt + recall_jnt)\r\n                accuracy_jnt = float(self.correct_labels_jnt) / self.total_labels_jnt\r\n\r\n        return f_crf, precision_crf, recall_crf, accuracy_crf, f_scrf, precision_scrf, recall_scrf, accuracy_scrf, f_jnt, precision_jnt, recall_jnt, accuracy_jnt\r\n\r\n    def eval_instance(self, best_path, gold):\r\n        """"""\r\n        update statics for one instance\r\n\r\n        args:\r\n            best_path (seq_len): predicted\r\n            gold (seq_len): ground-truth\r\n\r\n        """"""\r\n        total_labels = len(best_path)\r\n        correct_labels = np.sum(np.equal(best_path, gold))\r\n\r\n        gold_chunks = utils.iobes_to_spans(gold, self.r_l_map)\r\n        gold_count = len(gold_chunks)\r\n\r\n        guess_chunks = utils.iobes_to_spans(best_path, self.r_l_map)\r\n        guess_count = len(guess_chunks)\r\n\r\n        overlap_chunks = gold_chunks & guess_chunks\r\n        overlap_count = len(overlap_chunks)\r\n\r\n        return correct_labels, total_labels, gold_count, guess_count, overlap_count\r\n\r\n\r\n    def calc_score(self, ner_model, dataset_loader):\r\n        ### TODO: need to improve\r\n        """"""\r\n        calculate F1 score for dev and test sets\r\n\r\n        args:\r\n            ner_model: ner model\r\n            dataset_loader: loader class for dev/test set\r\n        """"""\r\n\r\n        ner_model.eval()\r\n        self.reset()\r\n\r\n        for f_f, f_p, b_f, b_p, w_f, tg, mask_v, len_v, SCRF_labels, mask_SCRF_labels, cnn_features in itertools.chain.from_iterable(dataset_loader):\r\n\r\n            f_f, f_p, b_f, b_p, w_f, _, mask_v, SCRF_labels, mask_SCRF_labels, cnn_features = self.packer.repack(f_f,f_p,b_f,b_p,w_f,tg,mask_v,len_v,SCRF_labels,mask_SCRF_labels,cnn_features, test=True)\r\n\r\n            word_representations = ner_model.word_rep(f_f, f_p, b_f, b_p, w_f, cnn_features)\r\n            bat_size = word_representations.size(1)\r\n            tg = tg.numpy() % len(self.l_map)\r\n            decoded_crf, crf_result_scored_by_crf = utils.decode_with_crf(ner_model.crf, word_representations, mask_v,self.l_map)\r\n            decoded_scrf_seq, scrf_result_scored_by_scrf = ner_model.hscrf.get_scrf_decode(word_representations.transpose(0, 1),mask_v.long().sum(0).data)\r\n            decoded_scrf = []\r\n            for i in decoded_scrf_seq:\r\n                decoded_scrf.append(\r\n                    [self.l_map[j] if j in self.l_map else self.l_map[\'<pad>\'] for j in i] + [\r\n                        self.l_map[\'<pad>\']] * (\r\n                            mask_v.size(0) - 1 - len(i)))\r\n            decoded_scrf = np.array(decoded_scrf)\r\n            if (decoded_crf == decoded_scrf).all():\r\n                decoded_jnt = decoded_scrf\r\n            else:\r\n                decoded_jnt = []\r\n\r\n                crf_result_scored_by_scrf = utils.rescored_with_scrf(decoded_crf, self.r_l_map, self.SCRF_l_map, ner_model.hscrf)\r\n                scrf_result_scored_by_crf = utils.rescored_with_crf(decoded_scrf, self.l_map, ner_model.crf.crf_scores)\r\n\r\n                crfscores = crf_result_scored_by_crf + crf_result_scored_by_scrf\r\n                scrfscores = scrf_result_scored_by_crf + scrf_result_scored_by_scrf\r\n\r\n                for i in range(bat_size):\r\n                    if crfscores[i] > scrfscores[i]:\r\n                        decoded_jnt.append(decoded_crf[i])\r\n                    else:\r\n                        decoded_jnt.append(decoded_scrf[i])\r\n                decoded_jnt = np.array(decoded_jnt)\r\n\r\n            self.calc_f1_batch(tg, decoded_crf, decoded_scrf, decoded_jnt)\r\n\r\n        return self.f1_score()\r\n'"
model/highway_layer.py,1,"b'import torch.nn as nn\r\nimport utils\r\n\r\n\r\nclass hw(nn.Module):\r\n    """"""\r\n    Highway layers\r\n\r\n    """"""\r\n   \r\n    def __init__(self, size, num_layers = 1, dropout_ratio = 0.5):\r\n        super(hw, self).__init__()\r\n        self.size = size\r\n        self.num_layers = num_layers\r\n        self.trans = nn.ModuleList()\r\n        self.gate = nn.ModuleList()\r\n        self.dropout = nn.Dropout(p=dropout_ratio)\r\n\r\n        for i in range(num_layers):\r\n            tmptrans = nn.Linear(size, size)\r\n            tmpgate = nn.Linear(size, size)\r\n            self.trans.append(tmptrans)\r\n            self.gate.append(tmpgate)\r\n\r\n    def rand_init(self):\r\n        """"""\r\n        random initialization\r\n\r\n        """"""\r\n        for i in range(self.num_layers):\r\n            utils.init_linear(self.trans[i])\r\n            utils.init_linear(self.gate[i])\r\n\r\n    def forward(self, x):\r\n        """"""\r\n        update statics for f1 score\r\n\r\n        args: \r\n            x (ins_num, hidden_dim): input tensor\r\n\r\n        """"""\r\n        \r\n        t = self.gate[0](x)\r\n        g = nn.functional.sigmoid(t)\r\n        h = nn.functional.relu(self.trans[0](x))\r\n        x = g * h + (1 - g) * x\r\n\r\n        for i in range(1, self.num_layers):\r\n            x = self.dropout(x)\r\n            g = nn.functional.sigmoid(self.gate[i](x))\r\n            h = nn.functional.relu(self.trans[i](x))\r\n            x = g * h + (1 - g) * x\r\n\r\n        return x'"
model/hscrf_layer.py,26,"b'from __future__ import print_function, division\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\n\nimport utils\n\n\nclass HSCRF(nn.Module):\n\n    def __init__(self, tag_to_ix, word_rep_dim=300, SCRF_feature_dim=100, index_embeds_dim=10, ALLOWED_SPANLEN=6, start_id=4, stop_id=5, noBIES=False, no_index=False, no_sub=False, grconv=False):\n        super(HSCRF, self).__init__()\n\n        self.tag_to_ix = tag_to_ix\n        self.ix_to_tag = {v:k for k,v in self.tag_to_ix.items()}\n        self.tagset_size = len(tag_to_ix)\n        self.index_embeds_dim = index_embeds_dim\n        self.SCRF_feature_dim = SCRF_feature_dim\n        self.ALLOWED_SPANLEN = ALLOWED_SPANLEN\n        self.start_id = start_id\n        self.stop_id = stop_id\n        self.grconv = grconv\n\n        self.index_embeds = nn.Embedding(self.ALLOWED_SPANLEN, self.index_embeds_dim)\n        self.init_embedding(self.index_embeds.weight)\n\n        self.dense = nn.Linear(word_rep_dim, self.SCRF_feature_dim)\n        self.init_linear(self.dense)\n\n        # 4 for SBIE, 3 for START, STOP, O and 2 for START and O\n        self.CRF_tagset_size = 4*(self.tagset_size-3)+2\n\n        self.transition = nn.Parameter(\n            torch.zeros(self.tagset_size, self.tagset_size))\n\n        span_word_embedding_dim = 2*self.SCRF_feature_dim + self.index_embeds_dim\n        self.new_hidden2CRFtag = nn.Linear(span_word_embedding_dim, self.CRF_tagset_size)\n        self.init_linear(self.new_hidden2CRFtag)\n\n        if self.grconv:\n            self.Wl = nn.Linear(self.SCRF_feature_dim, self.SCRF_feature_dim)\n            self.Wr = nn.Linear(self.SCRF_feature_dim, self.SCRF_feature_dim)\n            self.Gl = nn.Linear(self.SCRF_feature_dim, 3*self.SCRF_feature_dim)\n            self.Gr = nn.Linear(self.SCRF_feature_dim, 3*self.SCRF_feature_dim)\n            self.toSCRF = nn.Linear(self.SCRF_feature_dim, self.tagset_size)\n            self.init_linear(self.Wl)\n            self.init_linear(self.Wr)\n            self.init_linear(self.Gl)\n            self.init_linear(self.Gr)\n            self.init_linear(self.toSCRF)\n\n\n    def init_embedding(self, input_embedding):\n        """"""\n        Initialize embedding\n\n        """"""\n        bias = np.sqrt(3.0 / input_embedding.size(1))\n        nn.init.uniform(input_embedding, -bias, bias)\n\n    def init_linear(self, input_linear):\n        """"""\n        Initialize linear transformation\n\n        """"""\n        bias = np.sqrt(6.0 / (input_linear.weight.size(0) + input_linear.weight.size(1)))\n        nn.init.uniform(input_linear.weight, -bias, bias)\n        if input_linear.bias is not None:\n            input_linear.bias.data.zero_()\n\n    def get_logloss_denominator(self, scores, mask):\n        """"""\n        calculate all path scores of SCRF with dynamic programming\n\n        args:\n            scores (batch_size, sent_len, sent_len, self.tagset_size, self.tagset_size) : features for SCRF\n            mask   (batch_size) : mask for words\n\n        """"""\n\n        logalpha = Variable(torch.FloatTensor(self.batch_size, self.sent_len+1, self.tagset_size).fill_(-10000.)).cuda()\n        logalpha[:, 0, self.start_id] = 0.\n        istarts = [0] * self.ALLOWED_SPANLEN + range(self.sent_len - self.ALLOWED_SPANLEN+1)\n        for i in range(1, self.sent_len+1):\n                tmp = scores[:, istarts[i]:i, i-1] + \\\n                        logalpha[:, istarts[i]:i].unsqueeze(3).expand(self.batch_size, i - istarts[i], self.tagset_size, self.tagset_size)\n                tmp = tmp.transpose(1, 3).contiguous().view(self.batch_size, self.tagset_size, (i-istarts[i])*self.tagset_size)\n                max_tmp, _ = torch.max(tmp, dim=2)\n                tmp = tmp - max_tmp.view(self.batch_size, self.tagset_size, 1)\n                logalpha[:, i] = max_tmp + torch.log(torch.sum(torch.exp(tmp), dim=2))\n\n        mask = mask.unsqueeze(1).unsqueeze(1).expand(self.batch_size, 1, self.tagset_size)\n        alpha = torch.gather(logalpha, 1, mask).squeeze(1)\n        return alpha[:,self.stop_id].sum()\n\n    def decode(self, factexprscalars, mask):\n        """"""\n        decode SCRF labels with dynamic programming\n\n        args:\n            factexprscalars (batch_size, sent_len, sent_len, self.tagset_size, self.tagset_size) : features for SCRF\n            mask            (batch_size) : mask for words\n\n        """"""\n\n        batch_size = factexprscalars.size(0)\n        sentlen = factexprscalars.size(1)\n        factexprscalars = factexprscalars.data\n        logalpha = torch.FloatTensor(batch_size, sentlen+1, self.tagset_size).fill_(-10000.).cuda()\n        logalpha[:, 0, self.start_id] = 0.\n        starts = torch.zeros((batch_size, sentlen, self.tagset_size)).cuda()\n        ys = torch.zeros((batch_size, sentlen, self.tagset_size)).cuda()\n\n        for j in range(1, sentlen + 1):\n            istart = 0\n            if j > self.ALLOWED_SPANLEN:\n                istart = max(0, j - self.ALLOWED_SPANLEN)\n            f = factexprscalars[:, istart:j, j - 1].permute(0, 3, 1, 2).contiguous().view(batch_size, self.tagset_size, -1) + \\\n                logalpha[:, istart:j].contiguous().view(batch_size, 1, -1).expand(batch_size, self.tagset_size, (j - istart) * self.tagset_size)\n            logalpha[:, j, :], argm = torch.max(f, dim=2)\n            starts[:, j-1, :] = (argm / self.tagset_size + istart)\n            ys[:, j-1, :] = (argm % self.tagset_size)\n\n        batch_scores = []\n        batch_spans = []\n        for i in range(batch_size):\n            spans = []\n            batch_scores.append(max(logalpha[i, mask[i]-1]))\n            end = mask[i]-1\n            y = self.stop_id\n            while end >= 0:\n                start = int(starts[i, end, y])\n                y_1 = int(ys[i, end, y])\n                spans.append((start, end, y_1, y))\n                y = y_1\n                end = start - 1\n            batch_spans.append(spans)\n        return batch_spans, batch_scores\n\n    def get_logloss_numerator(self, goldfactors, scores, mask):\n        """"""\n        get scores of best path\n\n        args:\n            goldfactors (batch_size, tag_len, 4) : path labels\n            scores      (batch_size, sent_len, sent_len, self.tagset_size, self.tagset_size) : all tag scores\n            mask        (batch_size, tag_len) : mask for goldfactors\n\n        """"""\n        batch_size = scores.size(0)\n        sent_len = scores.size(1)\n        tagset_size = scores.size(3)\n        goldfactors = goldfactors[:, :, 0]*sent_len*tagset_size*tagset_size + goldfactors[:,:,1]*tagset_size*tagset_size+goldfactors[:,:,2]*tagset_size+goldfactors[:,:,3]\n        factorexprs = scores.view(batch_size, -1)\n        val = torch.gather(factorexprs, 1, goldfactors)\n        numerator = val.masked_select(mask)\n        return numerator\n\n    def grConv_scores(self, feats):\n        """"""\n        calculate SCRF scores with grConv\n\n        args:\n            feats (batch_size, sentence_len, featsdim) : word representations\n\n        """"""\n\n        scores = Variable(torch.zeros(self.batch_size, self.sent_len, self.sent_len, self.SCRF_feature_dim)).cuda()\n        diag0 = torch.LongTensor(range(self.sent_len)).cuda()\n        ht = feats\n        scores[:, diag0, diag0] = ht\n        if self.sent_len == 1:\n            return self.toSCRF(scores).unsqueeze(3) + self.transition.unsqueeze(0).unsqueeze(0).unsqueeze(0)\n        for span_len in range(1, min(self.ALLOWED_SPANLEN, self.sent_len)):\n            ht_1_l = ht[:, :-1]\n            ht_1_r = ht[:, 1:]\n            h_t_hat = 4 * nn.functional.sigmoid(self.Wl(ht_1_l) + self.Wr(ht_1_r)) - 2\n            w = torch.exp(self.Gl(ht_1_l) + self.Gr(ht_1_r)).view(self.batch_size, self.sent_len-span_len, 3, self.SCRF_feature_dim).permute(2,0,1,3)\n            w = w / w.sum(0).unsqueeze(0).expand(3, self.batch_size, self.sent_len-span_len, self.SCRF_feature_dim)\n            ht = w[0]*h_t_hat + w[1]*ht_1_l + w[2]*ht_1_r\n            scores[:, diag0[:-span_len], diag0[span_len:]] = ht\n\n        return self.toSCRF(scores).unsqueeze(3) + self.transition.unsqueeze(0).unsqueeze(0).unsqueeze(0)\n\n    def HSCRF_scores(self, feats):\n        ### TODO: need to improve\n        """"""\n        calculate SCRF scores with HSCRF\n\n        args:\n            feats (batch_size, sentence_len, featsdim) : word representations\n\n        """"""\n\n        # 3 for O, STOP, START\n        validtag_size = self.tagset_size-3\n        scores = Variable(torch.zeros(self.batch_size, self.sent_len, self.sent_len, self.tagset_size, self.tagset_size)).cuda()\n        diag0 = torch.LongTensor(range(self.sent_len)).cuda()\n        # m10000 for STOP\n        m10000 = Variable(torch.FloatTensor([-10000.]).expand(self.batch_size, self.sent_len, self.tagset_size, 1)).cuda()\n        # m30000 for STOP, START, O\n        m30000 = Variable(torch.FloatTensor([-10000.]).expand(self.batch_size, self.sent_len, self.tagset_size, 3)).cuda()\n        for span_len in range(min(self.ALLOWED_SPANLEN, self.sent_len)):\n            emb_x = self.concat_features(feats, span_len)\n            emb_x = self.new_hidden2CRFtag(emb_x)\n            if span_len == 0:\n                tmp = torch.cat((self.transition[:, :validtag_size].unsqueeze(0).unsqueeze(0) + emb_x[:, 0, :, :validtag_size].unsqueeze(2),\n                                 m10000,\n                                 self.transition[:, -2:].unsqueeze(0).unsqueeze(0) + emb_x[:, 0, :, -2:].unsqueeze(2)), 3)\n                scores[:, diag0, diag0] = tmp\n            elif span_len == 1:\n                tmp = torch.cat((self.transition[:, :validtag_size].unsqueeze(0).unsqueeze(0).expand(self.batch_size, self.sent_len-1, self.tagset_size, validtag_size) + \\\n                                                           (emb_x[:, 0, :, validtag_size:2*validtag_size] +\n                                                            emb_x[:, 1, :, 3*validtag_size:4*validtag_size]).unsqueeze(2), m30000[:, 1:]), 3)\n                scores[:, diag0[:-1], diag0[1:]] = tmp\n\n            elif span_len == 2:\n                tmp = torch.cat((self.transition[:, :validtag_size].unsqueeze(0).unsqueeze(0).expand(self.batch_size, self.sent_len-2, self.tagset_size, validtag_size) + \\\n                                                           (emb_x[:, 0, :, validtag_size:2*validtag_size] +\n                                                            emb_x[:, 1, :, 2*validtag_size:3*validtag_size] +\n                                                            emb_x[:, 2, :, 3*validtag_size:4*validtag_size]).unsqueeze(2), m30000[:, 2:]), 3)\n                scores[:, diag0[:-2], diag0[2:]] = tmp\n\n            elif span_len >= 3:\n                tmp0 = self.transition[:, :validtag_size].unsqueeze(0).unsqueeze(0).expand(self.batch_size, self.sent_len-span_len, self.tagset_size, validtag_size) + \\\n                                                           (emb_x[:, 0, :, validtag_size:2*validtag_size] +\n                                                            emb_x[:, 1:span_len, :, 2*validtag_size:3*validtag_size].sum(1) +\n                                                            emb_x[:, span_len,:, 3*validtag_size:4*validtag_size]).unsqueeze(2)\n                tmp = torch.cat((tmp0, m30000[:, span_len:]), 3)\n                scores[:, diag0[:-span_len], diag0[span_len:]] = tmp\n\n        return scores\n\n    def concat_features(self, emb_z, span_len):\n        """"""\n        concatenate two features\n\n        args:\n\n            emb_z (batch_size, sentence_len, featsdim) : word representations\n            span_len: a number (from 0)\n\n        """"""\n\n        batch_size = emb_z.size(0)\n        sent_len = emb_z.size(1)\n        hidden_dim = emb_z.size(2)\n        emb_z = emb_z.unsqueeze(1).expand(batch_size, sent_len, sent_len, hidden_dim)\n        new_emb_z1 = [emb_z[:, i:i + 1, i:i + span_len + 1] for i in range(sent_len - span_len)]\n        new_emb_z1 = torch.cat(new_emb_z1, 1)\n        new_emb_z2 = (new_emb_z1[:, :, 0]-new_emb_z1[:, :, span_len]).unsqueeze(2).expand(batch_size, sent_len-span_len, span_len+1, hidden_dim)\n        index = Variable(torch.LongTensor(range(span_len+1))).cuda()\n        index = self.index_embeds(index).unsqueeze(0).unsqueeze(0).expand(batch_size, sent_len-span_len, span_len+1, self.index_embeds_dim)\n        new_emb = torch.cat((new_emb_z1, new_emb_z2, index), 3).transpose(1,2).contiguous()\n\n        return new_emb\n\n    def forward(self, feats, mask_word, tags, mask_tag):\n        """"""\n        calculate loss\n\n        args:\n            feats (batch_size, sent_len, featsdim) : word representations\n            mask_word (batch_size) : sentence lengths\n            tags (batch_size, tag_len, 4) : target\n            mask_tag (batch_size, tag_len) : tag_len <= sentence_len\n\n        """"""\n\n        self.batch_size = feats.size(0)\n        self.sent_len = feats.size(1)\n        feats = self.dense(feats)\n        if self.grconv:\n            self.SCRF_scores = self.grConv_scores(feats)\n        else:\n            self.SCRF_scores = self.HSCRF_scores(feats)\n\n        forward_score = self.get_logloss_denominator(self.SCRF_scores, mask_word)\n        numerator = self.get_logloss_numerator(tags, self.SCRF_scores, mask_tag)\n\n        return (forward_score - numerator.sum()) / self.batch_size\n\n    def get_scrf_decode(self, feats, mask):\n        """"""\n        decode with SCRF\n\n        args:\n            feats (batch_size, sent_len, featsdim) : word representations\n            mask  (batch_size) : mask for words\n\n        """"""\n        self.batch_size = feats.size(0)\n        self.sent_len = feats.size(1)\n        feats = self.dense(feats)\n        if self.grconv:\n            self.SCRF_scores = self.grConv_scores(feats)\n        else:\n            self.SCRF_scores = self.HSCRF_scores(feats)\n        batch_spans, batch_scores = self.decode(self.SCRF_scores, mask)\n        batch_answer = self.tuple_to_seq(batch_spans)\n        return batch_answer, np.array(batch_scores)\n\n    def tuple_to_seq(self, batch_spans):\n        batch_answer = []\n        for spans in batch_spans:\n            answer = utils.tuple_to_seq_BIOES(spans, self.ix_to_tag)\n            batch_answer.append(answer[:-1])\n        return batch_answer\n'"
model/model.py,1,"b'from __future__ import print_function, division\nimport torch.nn as nn\nfrom word_rep_layer import WORD_REP\nfrom crf_layer import CRF\nfrom hscrf_layer import HSCRF\n\n\nclass ner_model(nn.Module):\n\n    def __init__(self, word_embedding_dim, word_hidden_dim, word_lstm_layers, vocab_size, char_size,\n                 char_embedding_dim, char_lstm_hidden_dim, cnn_filter_num, char_lstm_layers, char_lstm,\n                 dropout_ratio, if_highway, highway_layers, crf_start_tag, crf_end_tag, crf_target_size,\n                 scrf_tag_map, scrf_dense_dim, in_doc_words, index_embeds_dim, ALLOWED_SPANLEN,\n                 scrf_start_tag, scrf_end_tag, grconv):\n\n        super(ner_model, self).__init__()\n\n        self.char_lstm = char_lstm\n        self.word_rep = WORD_REP(char_size, char_embedding_dim, char_lstm_hidden_dim, cnn_filter_num,\n                 char_lstm_layers, word_embedding_dim,\n                 word_hidden_dim, word_lstm_layers, vocab_size, dropout_ratio, if_highway=if_highway,\n                 in_doc_words=in_doc_words, highway_layers=highway_layers, char_lstm=char_lstm)\n\n        self.crf = CRF(crf_start_tag, crf_end_tag, word_hidden_dim, crf_target_size)\n\n        self.hscrf = HSCRF(scrf_tag_map, word_rep_dim=word_hidden_dim, SCRF_feature_dim=scrf_dense_dim,\n                           index_embeds_dim=index_embeds_dim, ALLOWED_SPANLEN=ALLOWED_SPANLEN,\n                           start_id=scrf_start_tag, stop_id=scrf_end_tag,grconv=grconv)\n\n\n    def forward(self, forw_sentence, forw_position, back_sentence, back_position, word_seq,\n                cnn_features, crf_target, crf_mask, scrf_mask_words, scrf_target, scrf_mask_target, onlycrf=True):\n        """"""\n        calculate loss\n\n        :param forw_sentence   (char_seq_len, batch_size) : char-level representation of sentence\n        :param forw_position   (word_seq_len, batch_size) : position of blank space in char-level representation of sentence\n        :param back_sentence   (char_seq_len, batch_size) : char-level representation of sentence (inverse order)\n        :param back_position   (word_seq_len, batch_size) : position of blank space in inversed char-level representation of sentence\n        :param word_seq        (word_seq_len, batch_size) : word-level representation of sentence\n        :param cnn_features    (word_seq_len, batch_size, word_len) : char-level representation of words\n        :param crf_target      (word_seq_len, batch_size, 1): labels for CRF\n        :param crf_mask        (word_seq_len, batch_size) : mask for crf_target and word_seq\n        :param scrf_mask_words (batch_size) : lengths of sentences\n        :param scrf_target     (batch_size, tag_len, 4) : labels for SCRF\n        :param scrf_mask_target(batch_size, tag_len) : mask for scrf_target\n        :param onlycrf         (True or False) : whether training data is suitable for SCRF\n        :return:\n        """"""\n\n        word_representations = self.word_rep(forw_sentence, forw_position, back_sentence, back_position, word_seq, cnn_features)\n        loss_crf = self.crf(word_representations, crf_target, crf_mask)\n        loss = loss_crf\n        if not onlycrf:\n            loss_scrf = self.hscrf(word_representations.transpose(0,1), scrf_mask_words, scrf_target, scrf_mask_target)\n            loss = loss + loss_scrf\n        if self.char_lstm:\n            loss_lm = self.word_rep.lm_loss(forw_sentence, forw_position, back_sentence, back_position, word_seq)\n            loss = loss + loss_lm\n        return loss\n'"
model/utils.py,28,"b'import itertools\r\nfrom functools import reduce\r\n\r\nimport numpy as np\r\nimport torch\r\nimport json\r\n\r\nimport torch.nn as nn\r\nimport torch.nn.init\r\nfrom data_packer import CRFDataset_WC\r\n\r\n\r\nzip = getattr(itertools, \'izip\', zip)\r\n\r\n\r\ndef to_scalar(var):\r\n    """"""change the first element of a tensor to scalar\r\n    """"""\r\n    return var.view(-1).data.tolist()[0]\r\n\r\n\r\ndef log_sum_exp(vec, m_size):\r\n    """"""\r\n    calculate log of exp sum\r\n\r\n    args:\r\n        vec (batch_size, vanishing_dim, hidden_dim) : input tensor\r\n        m_size : hidden_dim\r\n    return:\r\n        batch_size, hidden_dim\r\n    """"""\r\n    _, idx = torch.max(vec, 1)  # B * 1 * M\r\n    max_score = torch.gather(vec, 1, idx.view(-1, 1, m_size)).view(-1, 1, m_size)  # B * M\r\n\r\n    return max_score.view(-1, m_size) + torch.log(torch.sum(torch.exp(vec - max_score.expand_as(vec)), 1)).view(-1, m_size)  # B * M\r\n\r\n\r\ndef encode2char_safe(input_lines, char_dict):\r\n    """"""\r\n    get char representation of lines\r\n\r\n    args:\r\n        input_lines (list of strings) : input corpus\r\n        char_dict (dictionary) : char-level dictionary\r\n\r\n    """"""\r\n    unk = char_dict[\'<u>\']\r\n    forw_lines = [list(map(lambda m: list(map(lambda t: char_dict.get(t, unk), m)), line)) for line in input_lines]\r\n    return forw_lines\r\n\r\n\r\ndef concatChar(input_lines, char_dict):\r\n    """"""\r\n    concat char into string\r\n\r\n    args:\r\n        input_lines (list of list of char) : input corpus\r\n        char_dict (dictionary) : char-level dictionary\r\n\r\n    """"""\r\n    features = [[char_dict[\' \']] + list(reduce(lambda x, y: x + [char_dict[\' \']] + y, sentence)) + [char_dict[\'\\n\']] for sentence in input_lines]\r\n    return features\r\n\r\n\r\ndef get_crf_scrf_label():\r\n    SCRF_l_map = {}\r\n    SCRF_l_map[\'PER\'] = 0\r\n    SCRF_l_map[\'LOC\'] = 1\r\n    SCRF_l_map[\'ORG\'] = 2\r\n    SCRF_l_map[\'MISC\'] = 3\r\n    CRF_l_map = {}\r\n    for pre in [\'S-\', \'B-\', \'I-\', \'E-\']:\r\n        for suf in SCRF_l_map.keys():\r\n            CRF_l_map[pre + suf] = len(CRF_l_map)\r\n    SCRF_l_map[\'<START>\'] = 4\r\n    SCRF_l_map[\'<STOP>\'] = 5\r\n    SCRF_l_map[\'O\'] = 6\r\n    CRF_l_map[\'<start>\'] = len(CRF_l_map)\r\n    CRF_l_map[\'<pad>\'] = len(CRF_l_map)\r\n    CRF_l_map[\'O\'] = len(CRF_l_map)\r\n\r\n    return CRF_l_map, SCRF_l_map\r\n\r\n\r\ndef encode_safe(input_lines, word_dict, unk):\r\n    """"""\r\n    encode list of strings into word-level representation with unk\r\n\r\n    """"""\r\n\r\n    lines = list(map(lambda t: list(map(lambda m: word_dict.get(m, unk), t)), input_lines))\r\n    return lines\r\n\r\n\r\ndef encode(input_lines, word_dict):\r\n    """"""\r\n    encode list of strings into word-level representation\r\n\r\n    """"""\r\n\r\n    lines = list(map(lambda t: list(map(lambda m: word_dict[m], t)), input_lines))\r\n    return lines\r\n\r\ndef encode_SCRF(input_lines, word_dict):\r\n    """"""\r\n    encode list of strings into word-level representation\r\n\r\n    """"""\r\n\r\n    lines = list(map(lambda t: list(map(lambda m: [m[0], m[1], word_dict[m[2]], word_dict[m[3]]], t)), input_lines))\r\n    return lines\r\n\r\n\r\ndef generate_corpus_char(lines, if_shrink_c_feature=False, c_thresholds=1, if_shrink_w_feature=False, w_thresholds=1):\r\n    """"""\r\n    generate label, feature, word dictionary, char dictionary and label dictionary\r\n\r\n    args:\r\n        lines : corpus\r\n        if_shrink_c_feature: whether shrink char-dictionary\r\n        c_threshold: threshold for shrinking char-dictionary\r\n        if_shrink_w_feature: whether shrink word-dictionary\r\n        w_threshold: threshold for shrinking word-dictionary\r\n        \r\n    """"""\r\n    features, labels, feature_map, label_map = generate_corpus(lines, if_shrink_feature=if_shrink_w_feature, thresholds=w_thresholds)\r\n    char_count = dict()\r\n    for feature in features:\r\n        for word in feature:\r\n            for tup in word:\r\n                if tup not in char_count:\r\n                    char_count[tup] = 0\r\n                else:\r\n                    char_count[tup] += 1\r\n    if if_shrink_c_feature:\r\n        shrink_char_count = [k for (k, v) in iter(char_count.items()) if v >= c_thresholds]\r\n        char_map = {shrink_char_count[ind]: ind for ind in range(0, len(shrink_char_count))}\r\n    else:\r\n        char_map = {k: v for (v, k) in enumerate(char_count.keys())}\r\n\r\n    # add three special chars\r\n    char_map[\'<u>\'] = len(char_map)\r\n    char_map[\' \'] = len(char_map)\r\n    char_map[\'\\n\'] = len(char_map)\r\n    return features, labels, feature_map, label_map, char_map\r\n\r\n\r\ndef shrink_features(feature_map, features, thresholds):\r\n    """"""\r\n    filter un-common features by threshold\r\n\r\n    """"""\r\n\r\n    feature_count = {k: 0 for (k, v) in iter(feature_map.items())}\r\n    for feature_list in features:\r\n        for feature in feature_list:\r\n            feature_count[feature] += 1\r\n    shrinked_feature_count = [k for (k, v) in iter(feature_count.items()) if v >= thresholds]\r\n    feature_map = {shrinked_feature_count[ind]: (ind + 1) for ind in range(0, len(shrinked_feature_count))}\r\n\r\n    feature_map[\'<unk>\'] = 0\r\n    feature_map[\'<eof>\'] = len(feature_map)\r\n    return feature_map\r\n\r\n\r\ndef generate_corpus(lines, if_shrink_feature=False, thresholds=1):\r\n    """"""\r\n    generate label, feature, word dictionary and label dictionary\r\n\r\n    args:\r\n        lines : corpus\r\n        if_shrink_feature: whether shrink word-dictionary\r\n        threshold: threshold for shrinking word-dictionary\r\n        \r\n    """"""\r\n    features = list()\r\n    labels = list()\r\n    tmp_fl = list()\r\n    tmp_ll = list()\r\n    feature_map = dict()\r\n    label_map = dict()\r\n    for line in lines:\r\n        if not (line.isspace() or (len(line) > 10 and line[0:10] == \'-DOCSTART-\')):\r\n            line = line.rstrip(\'\\n\').split()\r\n            tmp_fl.append(line[0])\r\n            if line[0] not in feature_map:\r\n                feature_map[line[0]] = len(feature_map) + 1 #0 for unk\r\n            tmp_ll.append(line[-1])\r\n        elif len(tmp_fl) > 0:\r\n            features.append(tmp_fl)\r\n            labels.append(iob_iobes(tmp_ll))\r\n            tmp_fl = list()\r\n            tmp_ll = list()\r\n    if len(tmp_fl) > 0:\r\n        features.append(tmp_fl)\r\n        labels.append(iob_iobes(tmp_ll))\r\n    for ls in labels:\r\n        for l in ls:\r\n            if l not in label_map:\r\n                label_map[l] = len(label_map)\r\n    label_map[\'<start>\'] = len(label_map)\r\n    label_map[\'<pad>\'] = len(label_map)\r\n    if if_shrink_feature:\r\n        feature_map = shrink_features(feature_map, features, thresholds)\r\n    else:\r\n        feature_map[\'<unk>\'] = 0\r\n        feature_map[\'<eof>\'] = len(feature_map)\r\n\r\n    return features, labels, feature_map, label_map\r\n\r\n\r\ndef read_corpus(lines):\r\n    """"""\r\n    convert corpus into features and labels\r\n\r\n    """"""\r\n\r\n    features = list()\r\n    labels = list()\r\n    tmp_fl = list()\r\n    tmp_ll = list()\r\n    for line in lines:\r\n        if not (line.isspace() or (len(line) > 10 and line[0:10] == \'-DOCSTART-\')):\r\n            line = line.rstrip(\'\\n\').split()\r\n            tmp_fl.append(line[0])\r\n            tmp_ll.append(line[-1])\r\n        elif len(tmp_fl) > 0:\r\n            features.append(tmp_fl)\r\n            labels.append(iob_iobes(tmp_ll))\r\n            tmp_fl = list()\r\n            tmp_ll = list()\r\n    if len(tmp_fl) > 0:\r\n        features.append(tmp_fl)\r\n        labels.append(iob_iobes(tmp_ll))\r\n\r\n    return features, labels\r\n\r\n\r\ndef iob_iobes(tags):\r\n    """"""\r\n    IOB -> IOBES\r\n    """"""\r\n    iob2(tags)\r\n    new_tags = []\r\n    for i, tag in enumerate(tags):\r\n        if tag == \'O\':\r\n            new_tags.append(tag)\r\n        elif tag.split(\'-\')[0] == \'B\':\r\n            if i + 1 != len(tags) and \\\r\n               tags[i + 1].split(\'-\')[0] == \'I\':\r\n                new_tags.append(tag)\r\n            else:\r\n                new_tags.append(tag.replace(\'B-\', \'S-\'))\r\n        elif tag.split(\'-\')[0] == \'I\':\r\n            if i + 1 < len(tags) and \\\r\n                    tags[i + 1].split(\'-\')[0] == \'I\':\r\n                new_tags.append(tag)\r\n            else:\r\n                new_tags.append(tag.replace(\'I-\', \'E-\'))\r\n        else:\r\n            raise Exception(\'Invalid IOB format!\')\r\n    return new_tags\r\n\r\n\r\ndef iob2(tags):\r\n    """"""\r\n    Check that tags have a valid IOB format.\r\n    Tags in IOB1 format are converted to IOB2.\r\n    """"""\r\n    for i, tag in enumerate(tags):\r\n        if tag == \'O\':\r\n            continue\r\n        split = tag.split(\'-\')\r\n        if len(split) != 2 or split[0] not in [\'I\', \'B\']:\r\n            return False\r\n        if split[0] == \'B\':\r\n            continue\r\n        elif i == 0 or tags[i - 1] == \'O\':  # conversion IOB1 to IOB2\r\n            tags[i] = \'B\' + tag[1:]\r\n        elif tags[i - 1][1:] == tag[1:]:\r\n            continue\r\n        else:\r\n            tags[i] = \'B\' + tag[1:]\r\n    return True\r\n\r\n\r\ndef load_embedding(emb_file, delimiter, feature_map, full_feature_set, unk, emb_len, shrink_to_train=False, shrink_to_corpus=False):\r\n    """"""\r\n    load embedding, indoc words would be listed before outdoc words\r\n\r\n    args: \r\n        emb_file: path to embedding file\r\n        delimiter: delimiter of lines\r\n        feature_map: word dictionary\r\n        full_feature_set: all words in the corpus\r\n        caseless: convert into casesless style\r\n        unk: string for unknown token\r\n        emb_len: dimension of embedding vectors\r\n        shrink_to_train: whether to shrink out-of-training set or not\r\n        shrink_to_corpus: whether to shrink out-of-corpus or not\r\n\r\n    """"""\r\n\r\n    feature_set = set([key.lower() for key in feature_map])\r\n    full_feature_set = set([key.lower() for key in full_feature_set])\r\n\r\n    \r\n    word_dict = {v:(k+1) for (k,v) in enumerate(feature_set - set([\'<unk>\']))}\r\n    word_dict[\'<unk>\'] = 0\r\n\r\n    in_doc_freq_num = len(word_dict)\r\n    rand_embedding_tensor = torch.FloatTensor(in_doc_freq_num, emb_len)\r\n    init_embedding(rand_embedding_tensor)\r\n\r\n    indoc_embedding_array = list()\r\n    indoc_word_array = list()\r\n    outdoc_embedding_array = list()\r\n    outdoc_word_array = list()\r\n\r\n    for line in open(emb_file, \'r\'):\r\n        line = line.split(delimiter)\r\n        vector = list(map(lambda t: float(t), filter(lambda n: n and not n.isspace(), line[1:])))\r\n\r\n        if shrink_to_train and line[0] not in feature_set:\r\n            continue\r\n\r\n        if line[0] == unk:\r\n            rand_embedding_tensor[0] = torch.FloatTensor(vector) #unk is 0\r\n        elif line[0] in word_dict:\r\n            rand_embedding_tensor[word_dict[line[0]]] = torch.FloatTensor(vector)\r\n        elif line[0] in full_feature_set:\r\n            indoc_embedding_array.append(vector)\r\n            indoc_word_array.append(line[0])\r\n        elif not shrink_to_corpus:\r\n            outdoc_word_array.append(line[0])\r\n            outdoc_embedding_array.append(vector)\r\n    \r\n    embedding_tensor_0 = torch.FloatTensor(np.asarray(indoc_embedding_array))\r\n\r\n    if not shrink_to_corpus:\r\n        embedding_tensor_1 = torch.FloatTensor(np.asarray(outdoc_embedding_array))\r\n        word_emb_len = embedding_tensor_0.size(1)\r\n        assert(word_emb_len == emb_len)\r\n\r\n    if shrink_to_corpus:\r\n        embedding_tensor = torch.cat([rand_embedding_tensor, embedding_tensor_0], 0)\r\n    else:\r\n        embedding_tensor = torch.cat([rand_embedding_tensor, embedding_tensor_0, embedding_tensor_1], 0)\r\n\r\n    for word in indoc_word_array:\r\n        word_dict[word] = len(word_dict)\r\n    in_doc_num = len(word_dict)\r\n    if  not shrink_to_corpus:\r\n        for word in outdoc_word_array:\r\n            word_dict[word] = len(word_dict)\r\n\r\n    return word_dict, embedding_tensor, in_doc_num\r\n\r\n\r\ndef calc_threshold_mean(features):\r\n    """"""\r\n    calculate the threshold for bucket by mean\r\n    """"""\r\n    lines_len = list(map(lambda t: len(t) + 1, features))\r\n    average = int(sum(lines_len) / len(lines_len))\r\n    lower_line = list(filter(lambda t: t < average, lines_len))\r\n    upper_line = list(filter(lambda t: t >= average, lines_len))\r\n    lower_average = int(sum(lower_line) / len(lower_line))\r\n    upper_average = int(sum(upper_line) / len(upper_line))\r\n    max_len = max(lines_len)\r\n    return [lower_average, average, upper_average, max_len]\r\n\r\n\r\ndef CRFtag_to_SCRFtag(inputs):\r\n    alltags = []\r\n    for input in inputs:\r\n        tags = []\r\n        beg = 0\r\n        oldtag = \'<START>\'\r\n        for i, tag in enumerate(input):\r\n            if tag == u\'O\':\r\n                tags.append((i, i, oldtag, tag))\r\n                oldtag = tag\r\n            if tag[0] == u\'S\':\r\n                tags.append((i, i, oldtag, tag[2:]))\r\n                oldtag = tag[2:]\r\n            if tag[0] == u\'B\':\r\n                beg = i\r\n            if tag[0] == u\'E\':\r\n                tags.append((beg, i, oldtag, tag[2:]))\r\n                oldtag = tag[2:]\r\n        alltags.append(tags)\r\n    return alltags\r\n\r\n\r\ndef construct_bucket_mean_vb_wc(word_features, input_label, label_dict, SCRF_label_dict, char_dict, word_dict, SCRF_stop_tag, ALLOW_SPANLEN=6, train_set=False):\r\n    """"""\r\n    Construct bucket by mean for viterbi decode, word-level and char-level\r\n    """"""\r\n\r\n    SCRFtags = CRFtag_to_SCRFtag(input_label)\r\n    labels = encode(input_label, label_dict)\r\n    SCRFlabels = encode_SCRF(SCRFtags, SCRF_label_dict)\r\n\r\n    new_SCRFlabels = []\r\n    new_labels = []\r\n    new_word_features = []\r\n    nolycrf_labels = []\r\n    nolycrf_word_features = []\r\n    nolycrf_SCRFlabels = []\r\n    if train_set:\r\n        for word, SCRFlabel, label in zip(word_features, SCRFlabels, labels):\r\n            keep = True\r\n            for t in SCRFlabel:\r\n                if t[1]-t[0] >= ALLOW_SPANLEN:\r\n                    keep = False\r\n                    break\r\n            if keep:\r\n                new_word_features.append(word)\r\n                new_labels.append(label)\r\n                new_SCRFlabels.append(SCRFlabel)\r\n            else:\r\n                nolycrf_labels.append(label)\r\n                nolycrf_word_features.append(word)\r\n                nolycrf_SCRFlabels.append(SCRFlabel)\r\n    else:\r\n        new_word_features = word_features\r\n        new_labels = labels\r\n        new_SCRFlabels = SCRFlabels\r\n\r\n    char_features = encode2char_safe(new_word_features, char_dict)\r\n    fea_len = [list(map(lambda t: len(t) + 1, f)) for f in char_features]\r\n    forw_features = concatChar(char_features, char_dict)\r\n    new_labels = list(map(lambda t: [label_dict[\'<start>\']] + list(t), new_labels))\r\n    thresholds = calc_threshold_mean(fea_len)\r\n    new_word_features = list(map(lambda t: list(map(lambda x: x.lower(), t)), new_word_features))\r\n    new_word_features = encode_safe(new_word_features, word_dict, word_dict[\'<unk>\'])\r\n    dataset = construct_bucket_vb_wc(new_word_features, forw_features, fea_len, new_labels, new_SCRFlabels, char_features,\r\n                                  thresholds, word_dict[\'<eof>\'], char_dict[\'\\n\'],\r\n                                  label_dict[\'<pad>\'], len(label_dict), SCRF_stop_tag)\r\n\r\n    if train_set:\r\n        if nolycrf_word_features:\r\n            nolycrf_char_features = encode2char_safe(nolycrf_word_features, char_dict)\r\n            nolycrf_fea_len = [list(map(lambda t: len(t) + 1, f)) for f in nolycrf_char_features]\r\n            nolycrf_forw_features = concatChar(nolycrf_char_features, char_dict)\r\n            nolycrf_labels = list(map(lambda t: [label_dict[\'<start>\']] + list(t), nolycrf_labels))\r\n            nolycrf_thresholds = [max(list(map(lambda t: len(t) + 1, nolycrf_fea_len)))]\r\n            nolycrf_word_features = list(map(lambda t: list(map(lambda x: x.lower(), t)), nolycrf_word_features))\r\n            nolycrf_word_features = encode_safe(nolycrf_word_features, word_dict, word_dict[\'<unk>\'])\r\n            nolycrf_dataset = construct_bucket_vb_wc(nolycrf_word_features, nolycrf_forw_features, nolycrf_fea_len,\r\n                                                     nolycrf_labels, nolycrf_SCRFlabels, nolycrf_char_features,\r\n                                                     nolycrf_thresholds, word_dict[\'<eof>\'], char_dict[\'\\n\'],\r\n                                                     label_dict[\'<pad>\'], len(label_dict), SCRF_stop_tag)\r\n            return dataset, nolycrf_dataset\r\n        else:\r\n            return dataset, None\r\n    else:\r\n        return dataset\r\n\r\n\r\ndef accumulate(iterator):\r\n    total = 0\r\n    for item in iterator:\r\n        total += item\r\n        yield total\r\n\r\n\r\ndef construct_bucket_vb_wc(word_features, forw_features, fea_len, input_labels, SCRFlabels, char_features, thresholds, pad_word_feature, pad_char_feature, pad_label, label_size, SCRF_stop_tag):\r\n    """"""\r\n    Construct bucket by thresholds for viterbi decode, word-level and char-level\r\n    """"""\r\n    word_max_len = max([len(c) for c_fs in char_features for c in c_fs])\r\n    buckets = [[[], [], [], [], [], [], [], [], [], [], []] for ind in range(len(thresholds))]\r\n    buckets_len = [0 for ind in range(len(thresholds))]\r\n    for f_f, f_l in zip(forw_features, fea_len):\r\n        cur_len_1 = len(f_l) + 1\r\n        idx = 0\r\n        while thresholds[idx] < cur_len_1:\r\n            idx += 1\r\n        tmp_concat_len = len(f_f) + thresholds[idx] - len(f_l)\r\n        if buckets_len[idx] < tmp_concat_len:\r\n            buckets_len[idx] = tmp_concat_len\r\n    for f_f, f_l, w_f, i_l, s_l, c_f in zip(forw_features, fea_len, word_features, input_labels, SCRFlabels, char_features):\r\n        cur_len = len(f_l)\r\n        idx = 0\r\n        cur_len_1 = cur_len + 1\r\n        cur_scrf_len = len(s_l)\r\n        cur_scrf_len_1 = cur_scrf_len + 1\r\n        w_l = max(f_l)-1\r\n\r\n        while thresholds[idx] < cur_len_1:\r\n            idx += 1\r\n\r\n        padded_feature = f_f + [pad_char_feature] * (buckets_len[idx] - len(f_f))  # pad feature with <\'\\n\'>, at least one\r\n\r\n        padded_feature_len = f_l + [1] * (thresholds[idx] - len(f_l)) # pad feature length with <\'\\n\'>, at least one\r\n\r\n        padded_feature_len_cum = list(accumulate(padded_feature_len)) # start from 0, but the first is \' \', so the position need not to be -1\r\n        buckets[idx][0].append(padded_feature) # char\r\n        buckets[idx][1].append(padded_feature_len_cum)\r\n        buckets[idx][2].append(padded_feature[::-1])\r\n        buckets[idx][3].append([buckets_len[idx] - 1] + [buckets_len[idx] - 1 - tup for tup in padded_feature_len_cum[:-1]])\r\n        buckets[idx][4].append(w_f + [pad_word_feature] * (thresholds[idx] - cur_len)) #word\r\n        buckets[idx][5].append([i_l[ind] * label_size + i_l[ind + 1] for ind in range(0, cur_len)] + [i_l[cur_len] * label_size + pad_label] + [pad_label * label_size + pad_label] * (thresholds[idx] - cur_len_1))  # has additional start, label\r\n        buckets[idx][6].append([1] * cur_len_1 + [0] * (thresholds[idx] - cur_len_1))  # has additional start, mask\r\n        buckets[idx][7].append([len(f_f) + thresholds[idx] - len(f_l), cur_len_1, cur_scrf_len_1, w_l])\r\n        buckets[idx][8].append(s_l + [[s_l[-1][1]+1, s_l[-1][1]+1, s_l[-1][-1], SCRF_stop_tag]] + [[0, 0, 0, 0] for _ in range(thresholds[idx]-cur_scrf_len_1)])\r\n        buckets[idx][9].append([1] * cur_scrf_len_1 + [0] * (thresholds[idx] - cur_scrf_len_1))\r\n        buckets[idx][10].append([c + [pad_char_feature]*(word_max_len-len(c)) for c in c_f] + [[pad_char_feature]*word_max_len]*(thresholds[idx]-cur_len))\r\n    bucket_dataset = [CRFDataset_WC(torch.LongTensor(bucket[0]), torch.LongTensor(bucket[1]),\r\n                                    torch.LongTensor(bucket[2]), torch.LongTensor(bucket[3]),\r\n                                    torch.LongTensor(bucket[4]), torch.LongTensor(bucket[5]),\r\n                                    torch.ByteTensor(bucket[6]), torch.LongTensor(bucket[7]),\r\n                                    torch.LongTensor(bucket[8]), torch.ByteTensor(bucket[9]),\r\n                                    torch.LongTensor(bucket[10]))\r\n                                    for bucket in buckets]\r\n    return bucket_dataset\r\n\r\n\r\ndef tuple_to_seq_BIOES(tuples, id_to_tag):\r\n\r\n    sentlen = max([tuple[1] for tuple in tuples]) + 1\r\n    seq = [None for _ in range(sentlen)]\r\n    for tuple in tuples:\r\n        if id_to_tag[tuple[-1]] == \'O\':\r\n                for i in range(tuple[0], tuple[1]+1):\r\n                    seq[i] = \'O\'\r\n        else:\r\n            if tuple[1]-tuple[0] == 0:\r\n                seq[tuple[0]] = \'S-\' + id_to_tag[tuple[-1]]\r\n            elif tuple[1]-tuple[0] >= 1:\r\n                seq[tuple[0]] = \'B-\' + id_to_tag[tuple[-1]]\r\n                seq[tuple[1]] = \'E-\' + id_to_tag[tuple[-1]]\r\n                for i in range(tuple[0] + 1, tuple[1]):\r\n                    seq[i] = \'I-\' + id_to_tag[tuple[-1]]\r\n    return seq\r\n\r\ndef find_length_from_labels(labels, label_to_ix):\r\n    """"""\r\n    find length of unpadded features based on labels\r\n    """"""\r\n    end_position = len(labels) - 1\r\n    for position, label in enumerate(labels):\r\n        if label == label_to_ix[\'<pad>\']:\r\n            end_position = position\r\n            break\r\n    return end_position\r\n\r\n\r\ndef revlut(lut):\r\n    return {v: k for k, v in lut.items()}\r\n\r\ndef iobes_to_spans(sequence, lut, strict_iob2=False):\r\n    """"""\r\n    convert to iobes to span\r\n    """"""\r\n    iobtype = 2 if strict_iob2 else 1\r\n    chunks = []\r\n    current = None\r\n\r\n    for i, y in enumerate(sequence):\r\n        label = lut[y]\r\n        if label.startswith(\'B-\'):\r\n            if current is not None:\r\n                chunks.append(\'@\'.join(current))\r\n            current = [label.replace(\'B-\', \'\'), \'%d\' % i]\r\n\r\n        elif label.startswith(\'S-\'):\r\n            if current is not None:\r\n                chunks.append(\'@\'.join(current))\r\n                current = None\r\n            base = label.replace(\'S-\', \'\')\r\n            chunks.append(\'@\'.join([base, \'%d\' % i]))\r\n\r\n        elif label.startswith(\'I-\'):\r\n            if current is not None:\r\n                base = label.replace(\'I-\', \'\')\r\n                if base == current[0]:\r\n                    current.append(\'%d\' % i)\r\n                else:\r\n                    chunks.append(\'@\'.join(current))\r\n                    if iobtype == 2:\r\n                        print(\'Warning\')\r\n                    current = [base, \'%d\' % i]\r\n            else:\r\n                current = [label.replace(\'I-\', \'\'), \'%d\' % i]\r\n                if iobtype == 2:\r\n                    print(\'Warning\')\r\n\r\n        elif label.startswith(\'E-\'):\r\n            if current is not None:\r\n                base = label.replace(\'E-\', \'\')\r\n                if base == current[0]:\r\n                    current.append(\'%d\' % i)\r\n                    chunks.append(\'@\'.join(current))\r\n                    current = None\r\n                else:\r\n                    chunks.append(\'@\'.join(current))\r\n                    if iobtype == 2:\r\n                        print(\'Warning\')\r\n                    current = [base, \'%d\' % i]\r\n                    chunks.append(\'@\'.join(current))\r\n                    current = None\r\n            else:\r\n                current = [label.replace(\'E-\', \'\'), \'%d\' % i]\r\n                if iobtype == 2:\r\n                    print(\'Warning\')\r\n                chunks.append(\'@\'.join(current))\r\n                current = None\r\n        else:\r\n            if current is not None:\r\n                chunks.append(\'@\'.join(current))\r\n            current = None\r\n\r\n    if current is not None:\r\n        chunks.append(\'@\'.join(current))\r\n\r\n    return set(chunks)\r\n\r\n\r\ndef save_checkpoint(state, track_list, filename):\r\n    """"""\r\n    save checkpoint\r\n    """"""\r\n    with open(filename+\'.json\', \'w\') as f:\r\n        json.dump(track_list, f)\r\n    torch.save(state, filename+\'.model\')\r\n\r\ndef adjust_learning_rate(optimizer, lr):\r\n    """"""\r\n    shrink learning rate for pytorch\r\n    """"""\r\n    for param_group in optimizer.param_groups:\r\n        param_group[\'lr\'] = lr\r\n\r\ndef init_embedding(input_embedding):\r\n    """"""\r\n    Initialize embedding\r\n    """"""\r\n    bias = np.sqrt(3.0 / input_embedding.size(1))\r\n    nn.init.uniform(input_embedding, -bias, bias)\r\n\r\ndef init_linear(input_linear):\r\n    """"""\r\n    Initialize linear transformation\r\n    """"""\r\n    bias = np.sqrt(6.0 / (input_linear.weight.size(0) + input_linear.weight.size(1)))\r\n    nn.init.uniform(input_linear.weight, -bias, bias)\r\n    if input_linear.bias is not None:\r\n        input_linear.bias.data.zero_()\r\n\r\ndef init_lstm(input_lstm):\r\n    """"""\r\n    Initialize lstm\r\n    """"""\r\n    for ind in range(0, input_lstm.num_layers):\r\n        weight = eval(\'input_lstm.weight_ih_l\' + str(ind))\r\n        bias = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\r\n        nn.init.uniform(weight, -bias, bias)\r\n        weight = eval(\'input_lstm.weight_hh_l\' + str(ind))\r\n        bias = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\r\n        nn.init.uniform(weight, -bias, bias)\r\n    # if input_lstm.bidirectional:\r\n    #     for ind in range(0, input_lstm.num_layers):\r\n    #         weight = eval(\'input_lstm.weight_ih_l\' + str(ind) + \'_reverse\')\r\n    #         bias = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\r\n    #         nn.init.uniform(weight, -bias, bias)\r\n    #         weight = eval(\'input_lstm.weight_hh_l\' + str(ind) + \'_reverse\')\r\n    #         bias = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\r\n    #         nn.init.uniform(weight, -bias, bias)\r\n\r\n    if input_lstm.bias:\r\n        for ind in range(0, input_lstm.num_layers):\r\n            weight = eval(\'input_lstm.bias_ih_l\' + str(ind))\r\n            weight.data.zero_()\r\n            weight.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\r\n            weight = eval(\'input_lstm.bias_hh_l\' + str(ind))\r\n            weight.data.zero_()\r\n            weight.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\r\n        # if input_lstm.bidirectional:\r\n        #     for ind in range(0, input_lstm.num_layers):\r\n        #         weight = eval(\'input_lstm.bias_ih_l\' + str(ind) + \'_reverse\')\r\n        #         weight.data.zero_()\r\n        #         weight.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\r\n        #         weight = eval(\'input_lstm.bias_hh_l\' + str(ind) + \'_reverse\')\r\n        #         weight.data.zero_()\r\n        #         weight.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\r\n\r\n\r\ndef crf_to_scrf(decoded_crf, r_l_map, scrf_l_map):\r\n    """"""\r\n    crf labels to scrf labels\r\n\r\n    """"""\r\n\r\n    input_label = []\r\n    for seq in decoded_crf:\r\n        sentencecrf = []\r\n        for i, l in enumerate(seq):\r\n            tag = r_l_map[l]\r\n            if tag == \'<pad>\':\r\n                break\r\n            sentencecrf.append(tag)\r\n        input_label.append(sentencecrf)\r\n    SCRFtags = CRFtag_to_SCRFtag(input_label)\r\n    SCRFlabels = encode_SCRF(SCRFtags, scrf_l_map)\r\n    maxl_1 = max([j[1] for i in SCRFlabels for j in i]) + 2\r\n    scrfdata = []\r\n    masks = []\r\n    for s_l in SCRFlabels:\r\n        cur_scrf_len = len(s_l)\r\n        s_l_pad = s_l + \\\r\n                  [[0, 0, 0, 0] for _ in range(maxl_1 - cur_scrf_len)]\r\n        mask = [1] * cur_scrf_len + [0] * (maxl_1 - cur_scrf_len)\r\n        scrfdata.append(s_l_pad)\r\n        masks.append(mask)\r\n    scrfdata = torch.cuda.LongTensor(scrfdata)\r\n    masks = torch.cuda.ByteTensor(masks)\r\n    return scrfdata, masks\r\n\r\ndef scrf_to_crf(decoded_scrf, l_map):\r\n    """"""\r\n    scrf labels to crf labels\r\n\r\n    """"""\r\n    label_size = len(l_map)\r\n    crf_labels = []\r\n    pad_label = l_map[\'<pad>\']\r\n    for i_l in decoded_scrf:\r\n        sent_labels = [l_map[\'<start>\']]\r\n        for label in i_l:\r\n            if label != l_map[\'<pad>\']:\r\n                sent_labels.append(label)\r\n            else:\r\n                break\r\n        crf_labels.append(sent_labels)\r\n\r\n    crfdata = []\r\n    masks = []\r\n    maxl_1 = max([len(i) for i in crf_labels])\r\n    for i_l in crf_labels:\r\n        cur_len_1 = len(i_l)\r\n        cur_len = cur_len_1 - 1\r\n        i_l_pad = [i_l[ind] * label_size + i_l[ind + 1] for ind in range(0, cur_len)] + [i_l[cur_len] * label_size + pad_label] + [\r\n                    pad_label * label_size + pad_label] * (maxl_1 - cur_len_1)\r\n\r\n        mask = [1] * cur_len_1 + [0] * (maxl_1 - cur_len_1)\r\n        crfdata.append(i_l_pad)\r\n        masks.append(mask)\r\n    crfdata = torch.cuda.LongTensor(crfdata).transpose(0,1).unsqueeze(2)\r\n    masks = torch.cuda.ByteTensor(masks).transpose(0,1)\r\n    return crfdata, masks\r\n\r\ndef decode_with_crf(crf, word_reps, mask_v, l_map):\r\n    """"""\r\n    decode with viterbi algorithm and return score\r\n\r\n    """"""\r\n\r\n    seq_len = word_reps.size(0)\r\n    bat_size = word_reps.size(1)\r\n    decoded_crf = crf.decode(word_reps, mask_v)\r\n    scores = crf.cal_score(word_reps).data\r\n    mask_v = mask_v.data\r\n    decoded_crf = decoded_crf.data\r\n    decoded_crf_withpad = torch.cat((torch.cuda.LongTensor(1,bat_size).fill_(l_map[\'<start>\']), decoded_crf), 0)\r\n    decoded_crf_withpad = decoded_crf_withpad.transpose(0,1).cpu().numpy()\r\n    label_size = len(l_map)\r\n\r\n    bi_crf = []\r\n    cur_len = decoded_crf_withpad.shape[1]-1\r\n    for i_l in decoded_crf_withpad:\r\n        bi_crf.append([i_l[ind] * label_size + i_l[ind + 1] for ind in range(0, cur_len)] + [\r\n            i_l[cur_len] * label_size + l_map[\'<pad>\']])\r\n    bi_crf = torch.cuda.LongTensor(bi_crf).transpose(0,1).unsqueeze(2)\r\n\r\n    tg_energy = torch.gather(scores.view(seq_len, bat_size, -1), 2, bi_crf).view(seq_len, bat_size)  # seq_len * bat_size\r\n    tg_energy = tg_energy.transpose(0,1).masked_select(mask_v.transpose(0,1))\r\n    tg_energy = tg_energy.cpu().numpy()\r\n    masks = mask_v.sum(0)\r\n    crf_result_scored_by_crf = []\r\n    start = 0\r\n    for i, mask in enumerate(masks):\r\n        end = start + mask\r\n        crf_result_scored_by_crf.append(tg_energy[start:end].sum())\r\n        start = end\r\n    crf_result_scored_by_crf = np.array(crf_result_scored_by_crf)\r\n    return decoded_crf.cpu().transpose(0,1).numpy(), crf_result_scored_by_crf\r\n\r\ndef rescored_with_scrf(decoded_crf, r_l_map, SCRF_l_map, decoder_scrf):\r\n    """"""\r\n    re-score crf deocded labels with scrf\r\n\r\n    """"""\r\n    scrfdata, masks = crf_to_scrf(decoded_crf, r_l_map, SCRF_l_map)\r\n    scrf_batch_score = decoder_scrf.get_logloss_numerator(scrfdata, decoder_scrf.SCRF_scores.data, masks)\r\n    masks = masks.sum(1)\r\n    scrf_batch_score = scrf_batch_score.cpu().numpy()\r\n    crf_result_scored_by_scrf = []\r\n    start = 0\r\n    for i, mask in enumerate(masks):\r\n        end = start + mask\r\n        crf_result_scored_by_scrf.append(scrf_batch_score[start:end].sum())\r\n        start = end\r\n    crf_result_scored_by_scrf = np.array(crf_result_scored_by_scrf)\r\n    return crf_result_scored_by_scrf\r\n\r\ndef rescored_with_crf(decoded_scrf, l_map, scores):\r\n    """"""\r\n    re-score scrf decoded labels with crf\r\n\r\n    """"""\r\n    scrfdata, masks = scrf_to_crf(decoded_scrf, l_map)\r\n    seq_len = scores.size(0)\r\n    bat_size = scores.size(1)\r\n    tg_energy = torch.gather(scores.view(seq_len, bat_size, -1), 2, scrfdata).view(seq_len, bat_size)  # seq_len * bat_size\r\n    crf_batch_score = tg_energy.transpose(0,1).masked_select(masks.transpose(0,1))\r\n    masks = masks.sum(0)\r\n    scrf_result_scored_by_crf = []\r\n    start = 0\r\n    for i, mask in enumerate(masks):\r\n        end = start + mask\r\n        scrf_result_scored_by_crf.append(crf_batch_score[start:end].sum())\r\n        start = end\r\n\r\n    scrf_result_scored_by_crf = torch.cat(scrf_result_scored_by_crf).cpu().data.numpy()\r\n    return scrf_result_scored_by_crf\r\n\r\n'"
model/word_rep_layer.py,8,"b'import torch\nimport torch.nn as nn\nimport utils\nimport highway_layer\n\n\nclass WORD_REP(nn.Module):\n\n    def __init__(self, char_size, char_embedding_dim, char_hidden_dim, cnn_filter_num, char_lstm_layers, word_embedding_dim,\n                 word_hidden_dim, word_lstm_layers, vocab_size, dropout_ratio, if_highway=False,\n                 in_doc_words=2, highway_layers=1, char_lstm=True):\n\n        super(WORD_REP, self).__init__()\n        self.char_embedding_dim = char_embedding_dim\n        self.char_hidden_dim = char_hidden_dim\n        self.cnn_filter_num = cnn_filter_num\n        self.char_size = char_size\n        self.word_embedding_dim = word_embedding_dim\n        self.word_hidden_dim = word_hidden_dim\n        self.word_size = vocab_size\n        self.char_lstm = char_lstm\n        self.if_highway = if_highway\n\n        self.char_embeds = nn.Embedding(char_size, char_embedding_dim)\n        self.word_embeds = nn.Embedding(vocab_size, word_embedding_dim)\n\n        if char_lstm:\n            self.crit_lm = nn.CrossEntropyLoss()\n            self.forw_char_lstm = nn.LSTM(char_embedding_dim, char_hidden_dim, num_layers=char_lstm_layers, bidirectional=False,\n                                      dropout=dropout_ratio)\n            self.back_char_lstm = nn.LSTM(char_embedding_dim, char_hidden_dim, num_layers=char_lstm_layers, bidirectional=False,\n                                      dropout=dropout_ratio)\n            self.word_lstm_lm = nn.LSTM(word_embedding_dim + char_hidden_dim * 2, word_hidden_dim // 2,\n                                        num_layers=word_lstm_layers,\n                                        bidirectional=True, dropout=dropout_ratio)\n            self.char_pre_train_out = nn.Linear(char_hidden_dim, char_size)\n            self.word_pre_train_out = nn.Linear(char_hidden_dim, in_doc_words)\n            if self.if_highway:\n                self.forw2char = highway_layer.hw(char_hidden_dim, num_layers=highway_layers, dropout_ratio=dropout_ratio)\n                self.back2char = highway_layer.hw(char_hidden_dim, num_layers=highway_layers, dropout_ratio=dropout_ratio)\n                self.forw2word = highway_layer.hw(char_hidden_dim, num_layers=highway_layers, dropout_ratio=dropout_ratio)\n                self.back2word = highway_layer.hw(char_hidden_dim, num_layers=highway_layers, dropout_ratio=dropout_ratio)\n                self.fb2char = highway_layer.hw(2*char_hidden_dim, num_layers=highway_layers, dropout_ratio=dropout_ratio)\n        else:\n            self.cnn = nn.Conv2d(1, cnn_filter_num, (3, char_embedding_dim), padding=(2, 0))\n\n            self.word_lstm_cnn = nn.LSTM(word_embedding_dim + cnn_filter_num, word_hidden_dim // 2, num_layers=word_lstm_layers, bidirectional=True,\n                              dropout=dropout_ratio)\n        self.dropout = nn.Dropout(p=dropout_ratio)\n        self.batch_size = 1\n        self.word_seq_length = 1\n\n\n    def set_batch_seq_size(self, sentence):\n        """"""\n        set batch size and sequence length\n        """"""\n        tmp = sentence.size()\n        self.word_seq_length = tmp[0]\n        self.batch_size = tmp[1]\n\n    def load_pretrained_word_embedding(self, pre_word_embeddings):\n        """"""\n        load pre-trained word embedding\n\n        args:\n            pre_word_embeddings (self.word_size, self.word_dim) : pre-trained embedding\n\n        """"""\n        assert (pre_word_embeddings.size()[1] == self.word_embedding_dim)\n        self.word_embeds.weight = nn.Parameter(pre_word_embeddings)\n\n    def rand_init(self):\n        """"""\n        random initialization\n\n        args:\n            init_char_embedding: random initialize char embedding or not\n\n        """"""\n\n        utils.init_embedding(self.char_embeds.weight)\n        if self.char_lstm:\n            utils.init_lstm(self.forw_char_lstm)\n            utils.init_lstm(self.back_char_lstm)\n            utils.init_lstm(self.word_lstm_lm)\n            utils.init_linear(self.char_pre_train_out)\n            utils.init_linear(self.word_pre_train_out)\n            if self.if_highway:\n                self.forw2char.rand_init()\n                self.back2char.rand_init()\n                self.forw2word.rand_init()\n                self.back2word.rand_init()\n                self.fb2char.rand_init()\n        else:\n            utils.init_lstm(self.word_lstm_cnn)\n\n    def word_pre_train_forward(self, sentence, position):\n        """"""\n        output of forward language model\n\n        args:\n            sentence (char_seq_len, batch_size): char-level representation of sentence\n            position (word_seq_len, batch_size): position of blank space in char-level representation of sentence\n\n        """"""\n\n        embeds = self.char_embeds(sentence)\n        d_embeds = self.dropout(embeds)\n        lstm_out, hidden = self.forw_char_lstm(d_embeds)\n\n        tmpsize = position.size()\n        position = position.unsqueeze(2).expand(tmpsize[0], tmpsize[1], self.char_hidden_dim)\n        select_lstm_out = torch.gather(lstm_out, 0, position)\n        d_lstm_out = self.dropout(select_lstm_out).view(-1, self.char_hidden_dim)\n\n        if self.if_highway:\n            char_out = self.forw2word(d_lstm_out)\n            d_char_out = self.dropout(char_out)\n        else:\n            d_char_out = d_lstm_out\n\n        pre_score = self.word_pre_train_out(d_char_out)\n        return pre_score, hidden\n\n    def word_pre_train_backward(self, sentence, position):\n        """"""\n        output of backward language model\n\n        args:\n            sentence (char_seq_len, batch_size): char-level representation of sentence (inverse order)\n            position (word_seq_len, batch_size): position of blank space in inversed char-level representation of sentence\n\n        """"""\n        embeds = self.char_embeds(sentence)\n        d_embeds = self.dropout(embeds)\n        lstm_out, hidden = self.back_char_lstm(d_embeds)\n\n        tmpsize = position.size()\n        position = position.unsqueeze(2).expand(tmpsize[0], tmpsize[1], self.char_hidden_dim)\n        select_lstm_out = torch.gather(lstm_out, 0, position)\n        d_lstm_out = self.dropout(select_lstm_out).view(-1, self.char_hidden_dim)\n\n        if self.if_highway:\n            char_out = self.back2word(d_lstm_out)\n            d_char_out = self.dropout(char_out)\n        else:\n            d_char_out = d_lstm_out\n\n        pre_score = self.word_pre_train_out(d_char_out)\n        return pre_score, hidden\n\n    def lm_loss(self, f_f, f_p, b_f, b_p, w_f):\n        """"""\n        language model loss\n\n        args:\n            f_f (char_seq_len, batch_size) : char-level representation of sentence\n            f_p (word_seq_len, batch_size) : position of blank space in char-level representation of sentence\n            b_f (char_seq_len, batch_size) : char-level representation of sentence (inverse order)\n            b_p (word_seq_len, batch_size) : position of blank space in inversed char-level representation of sentence\n            w_f (word_seq_len, batch_size) : word-level representation of sentence\n\n        """"""\n\n        cf_p = f_p[0:-1, :].contiguous()\n        cb_p = b_p[1:, :].contiguous()\n        cf_y = w_f[1:, :].contiguous()\n        cb_y = w_f[0:-1, :].contiguous()\n        cfs, _ = self.word_pre_train_forward(f_f, cf_p)\n        loss = self.crit_lm(cfs, cf_y.view(-1))\n        cbs, _ = self.word_pre_train_backward(b_f, cb_p)\n        loss = loss + self.crit_lm(cbs, cb_y.view(-1))\n\n        return loss\n\n    def cnn_lstm(self, word_seq, cnn_features):\n        """"""\n        return word representations with character-cnn\n\n        args:\n            word_seq:     word_seq_len, batch_size\n            cnn_features: word_seq_len, batch_size, word_len\n\n        """"""\n        self.set_batch_seq_size(word_seq)\n        cnn_features = cnn_features.view(cnn_features.size(0) * cnn_features.size(1), -1)\n        cnn_features = self.char_embeds(cnn_features).view(cnn_features.size(0), 1, cnn_features.size(1), -1)\n        cnn_features = self.cnn(cnn_features)\n        d_char_out = nn.functional.max_pool2d(cnn_features,\n                                              kernel_size=(cnn_features.size(2), 1)).view(self.word_seq_length, self.batch_size, self.cnn_filter_num)\n        word_emb = self.word_embeds(word_seq)\n\n        word_input = torch.cat((word_emb, d_char_out), dim=2)\n        word_input = self.dropout(word_input)\n\n        lstm_out, _ = self.word_lstm_cnn(word_input)\n        lstm_out = self.dropout(lstm_out)\n\n        return lstm_out\n\n    def lm_lstm(self, forw_sentence, forw_position, back_sentence, back_position, word_seq):\n        \'\'\'\n        return word representations with character-language-model\n\n        args:\n            forw_sentence (char_seq_len, batch_size) : char-level representation of sentence\n            forw_position (word_seq_len, batch_size) : position of blank space in char-level representation of sentence\n            back_sentence (char_seq_len, batch_size) : char-level representation of sentence (inverse order)\n            back_position (word_seq_len, batch_size) : position of blank space in inversed char-level representation of sentence\n            word_seq (word_seq_len, batch_size) : word-level representation of sentence\n\n        \'\'\'\n\n        self.set_batch_seq_size(forw_position)\n\n        forw_emb = self.char_embeds(forw_sentence)\n        back_emb = self.char_embeds(back_sentence)\n\n        d_f_emb = self.dropout(forw_emb)\n        d_b_emb = self.dropout(back_emb)\n\n        forw_lstm_out, _ = self.forw_char_lstm(d_f_emb)\n\n        back_lstm_out, _ = self.back_char_lstm(d_b_emb)\n\n        forw_position = forw_position.unsqueeze(2).expand(self.word_seq_length, self.batch_size, self.char_hidden_dim)\n        select_forw_lstm_out = torch.gather(forw_lstm_out, 0, forw_position)\n\n        back_position = back_position.unsqueeze(2).expand(self.word_seq_length, self.batch_size, self.char_hidden_dim)\n        select_back_lstm_out = torch.gather(back_lstm_out, 0, back_position)\n\n        fb_lstm_out = self.dropout(torch.cat((select_forw_lstm_out, select_back_lstm_out), dim=2))\n        if self.if_highway:\n            char_out = self.fb2char(fb_lstm_out)\n            d_char_out = self.dropout(char_out)\n        else:\n            d_char_out = fb_lstm_out\n\n        word_emb = self.word_embeds(word_seq)\n        d_word_emb = self.dropout(word_emb)\n\n        word_input = torch.cat((d_word_emb, d_char_out), dim=2)\n\n        lstm_out, _ = self.word_lstm_lm(word_input)\n        d_lstm_out = self.dropout(lstm_out)\n\n        return d_lstm_out\n\n    def forward(self, forw_sentence, forw_position, back_sentence, back_position, word_seq, cnn_features):\n        \'\'\'\n        word representations\n\n        args:\n            forw_sentence (char_seq_len, batch_size) : char-level representation of sentence\n            forw_position (word_seq_len, batch_size) : position of blank space in char-level representation of sentence\n            back_sentence (char_seq_len, batch_size) : char-level representation of sentence (inverse order)\n            back_position (word_seq_len, batch_size) : position of blank space in inversed char-level representation of sentence\n            word_seq (word_seq_len, batch_size) : word-level representation of sentence\n            hidden: initial hidden state\n\n        \'\'\'\n        if self.char_lstm:\n            return self.lm_lstm(forw_sentence, forw_position, back_sentence, back_position, word_seq)\n        else:\n            return self.cnn_lstm(word_seq, cnn_features)\n'"
