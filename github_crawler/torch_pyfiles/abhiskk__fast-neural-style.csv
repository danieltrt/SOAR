file_path,api_count,code
neural_style/__init__.py,0,b''
neural_style/neural_style.py,10,"b'import argparse\nimport os\nimport sys\nimport time\n\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision import transforms\n\nimport utils\nfrom transformer_net import TransformerNet\nfrom vgg16 import Vgg16\n\n\ndef train(args):\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n\n    if args.cuda:\n        torch.cuda.manual_seed(args.seed)\n        kwargs = {\'num_workers\': 0, \'pin_memory\': False}\n    else:\n        kwargs = {}\n\n    transform = transforms.Compose([transforms.Scale(args.image_size),\n                                    transforms.CenterCrop(args.image_size),\n                                    transforms.ToTensor(),\n                                    transforms.Lambda(lambda x: x.mul(255))])\n    train_dataset = datasets.ImageFolder(args.dataset, transform)\n    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, **kwargs)\n\n    transformer = TransformerNet()\n    optimizer = Adam(transformer.parameters(), args.lr)\n    mse_loss = torch.nn.MSELoss()\n\n    vgg = Vgg16()\n    utils.init_vgg16(args.vgg_model_dir)\n    vgg.load_state_dict(torch.load(os.path.join(args.vgg_model_dir, ""vgg16.weight"")))\n\n    if args.cuda:\n        transformer.cuda()\n        vgg.cuda()\n\n    style = utils.tensor_load_rgbimage(args.style_image, size=args.style_size)\n    style = style.repeat(args.batch_size, 1, 1, 1)\n    style = utils.preprocess_batch(style)\n    if args.cuda:\n        style = style.cuda()\n    style_v = Variable(style, volatile=True)\n    style_v = utils.subtract_imagenet_mean_batch(style_v)\n    features_style = vgg(style_v)\n    gram_style = [utils.gram_matrix(y) for y in features_style]\n\n    for e in range(args.epochs):\n        transformer.train()\n        agg_content_loss = 0.\n        agg_style_loss = 0.\n        count = 0\n        for batch_id, (x, _) in enumerate(train_loader):\n            n_batch = len(x)\n            count += n_batch\n            optimizer.zero_grad()\n            x = Variable(utils.preprocess_batch(x))\n            if args.cuda:\n                x = x.cuda()\n\n            y = transformer(x)\n\n            xc = Variable(x.data.clone(), volatile=True)\n\n            y = utils.subtract_imagenet_mean_batch(y)\n            xc = utils.subtract_imagenet_mean_batch(xc)\n\n            features_y = vgg(y)\n            features_xc = vgg(xc)\n\n            f_xc_c = Variable(features_xc[1].data, requires_grad=False)\n\n            content_loss = args.content_weight * mse_loss(features_y[1], f_xc_c)\n\n            style_loss = 0.\n            for m in range(len(features_y)):\n                gram_s = Variable(gram_style[m].data, requires_grad=False)\n                gram_y = utils.gram_matrix(features_y[m])\n                style_loss += args.style_weight * mse_loss(gram_y, gram_s[:n_batch, :, :])\n\n            total_loss = content_loss + style_loss\n            total_loss.backward()\n            optimizer.step()\n\n            agg_content_loss += content_loss.data[0]\n            agg_style_loss += style_loss.data[0]\n\n            if (batch_id + 1) % args.log_interval == 0:\n                mesg = ""{}\\tEpoch {}:\\t[{}/{}]\\tcontent: {:.6f}\\tstyle: {:.6f}\\ttotal: {:.6f}"".format(\n                    time.ctime(), e + 1, count, len(train_dataset),\n                                  agg_content_loss / (batch_id + 1),\n                                  agg_style_loss / (batch_id + 1),\n                                  (agg_content_loss + agg_style_loss) / (batch_id + 1)\n                )\n                print(mesg)\n\n    # save model\n    transformer.eval()\n    transformer.cpu()\n    save_model_filename = ""epoch_"" + str(args.epochs) + ""_"" + str(time.ctime()).replace(\' \', \'_\') + ""_"" + str(\n        args.content_weight) + ""_"" + str(args.style_weight) + "".model""\n    save_model_path = os.path.join(args.save_model_dir, save_model_filename)\n    torch.save(transformer.state_dict(), save_model_path)\n\n    print(""\\nDone, trained model saved at"", save_model_path)\n\n\ndef check_paths(args):\n    try:\n        if not os.path.exists(args.vgg_model_dir):\n            os.makedirs(args.vgg_model_dir)\n        if not os.path.exists(args.save_model_dir):\n            os.makedirs(args.save_model_dir)\n    except OSError as e:\n        print(e)\n        sys.exit(1)\n\n\ndef stylize(args):\n    content_image = utils.tensor_load_rgbimage(args.content_image, scale=args.content_scale)\n    content_image = content_image.unsqueeze(0)\n\n    if args.cuda:\n        content_image = content_image.cuda()\n    content_image = Variable(utils.preprocess_batch(content_image), volatile=True)\n    style_model = TransformerNet()\n    style_model.load_state_dict(torch.load(args.model))\n\n    if args.cuda:\n        style_model.cuda()\n\n    output = style_model(content_image)\n    utils.tensor_save_bgrimage(output.data[0], args.output_image, args.cuda)\n\n\ndef main():\n    main_arg_parser = argparse.ArgumentParser(description=""parser for fast-neural-style"")\n    subparsers = main_arg_parser.add_subparsers(title=""subcommands"", dest=""subcommand"")\n\n    train_arg_parser = subparsers.add_parser(""train"",\n                                             help=""parser for training arguments"")\n    train_arg_parser.add_argument(""--epochs"", type=int, default=2,\n                                  help=""number of training epochs, default is 2"")\n    train_arg_parser.add_argument(""--batch-size"", type=int, default=4,\n                                  help=""batch size for training, default is 4"")\n    train_arg_parser.add_argument(""--dataset"", type=str, required=True,\n                                  help=""path to training dataset, the path should point to a folder ""\n                                       ""containing another folder with all the training images"")\n    train_arg_parser.add_argument(""--style-image"", type=str, default=""images/style-images/mosaic.jpg"",\n                                  help=""path to style-image"")\n    train_arg_parser.add_argument(""--vgg-model-dir"", type=str, required=True,\n                                  help=""directory for vgg, if model is not present in the directory it is downloaded"")\n    train_arg_parser.add_argument(""--save-model-dir"", type=str, required=True,\n                                  help=""path to folder where trained model will be saved."")\n    train_arg_parser.add_argument(""--image-size"", type=int, default=256,\n                                  help=""size of training images, default is 256 X 256"")\n    train_arg_parser.add_argument(""--style-size"", type=int, default=None,\n                                  help=""size of style-image, default is the original size of style image"")\n    train_arg_parser.add_argument(""--cuda"", type=int, required=True, help=""set it to 1 for running on GPU, 0 for CPU"")\n    train_arg_parser.add_argument(""--seed"", type=int, default=42, help=""random seed for training"")\n    train_arg_parser.add_argument(""--content-weight"", type=float, default=1.0,\n                                  help=""weight for content-loss, default is 1.0"")\n    train_arg_parser.add_argument(""--style-weight"", type=float, default=5.0,\n                                  help=""weight for style-loss, default is 5.0"")\n    train_arg_parser.add_argument(""--lr"", type=float, default=1e-3,\n                                  help=""learning rate, default is 0.001"")\n    train_arg_parser.add_argument(""--log-interval"", type=int, default=500,\n                                  help=""number of images after which the training loss is logged, default is 500"")\n\n    eval_arg_parser = subparsers.add_parser(""eval"", help=""parser for evaluation/stylizing arguments"")\n    eval_arg_parser.add_argument(""--content-image"", type=str, required=True,\n                                 help=""path to content image you want to stylize"")\n    eval_arg_parser.add_argument(""--content-scale"", type=float, default=None,\n                                 help=""factor for scaling down the content image"")\n    eval_arg_parser.add_argument(""--output-image"", type=str, required=True,\n                                 help=""path for saving the output image"")\n    eval_arg_parser.add_argument(""--model"", type=str, required=True,\n                                 help=""saved model to be used for stylizing the image"")\n    eval_arg_parser.add_argument(""--cuda"", type=int, required=True,\n                                 help=""set it to 1 for running on GPU, 0 for CPU"")\n\n    args = main_arg_parser.parse_args()\n\n    if args.subcommand is None:\n        print(""ERROR: specify either train or eval"")\n        sys.exit(1)\n\n    if args.cuda and not torch.cuda.is_available():\n        print(""ERROR: cuda is not available, try running on CPU"")\n        sys.exit(1)\n\n    if args.subcommand == ""train"":\n        check_paths(args)\n        train(args)\n    else:\n        stylize(args)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
neural_style/transformer_net.py,14,"b'import torch\nimport torch.nn as nn\nimport numpy as np\n\n\nclass TransformerNet(torch.nn.Module):\n    def __init__(self):\n        super(TransformerNet, self).__init__()\n\n        # Initial convolution layers\n        self.conv1 = ConvLayer(3, 32, kernel_size=9, stride=1)\n        self.in1 = InstanceNormalization(32)\n        self.conv2 = ConvLayer(32, 64, kernel_size=3, stride=2)\n        self.in2 = InstanceNormalization(64)\n        self.conv3 = ConvLayer(64, 128, kernel_size=3, stride=2)\n        self.in3 = InstanceNormalization(128)\n\n        # Residual layers\n        self.res1 = ResidualBlock(128)\n        self.res2 = ResidualBlock(128)\n        self.res3 = ResidualBlock(128)\n        self.res4 = ResidualBlock(128)\n        self.res5 = ResidualBlock(128)\n\n        # Upsampling Layers\n        self.deconv1 = UpsampleConvLayer(128, 64, kernel_size=3, stride=1, upsample=2)\n        self.in4 = InstanceNormalization(64)\n        self.deconv2 = UpsampleConvLayer(64, 32, kernel_size=3, stride=1, upsample=2)\n        self.in5 = InstanceNormalization(32)\n        self.deconv3 = ConvLayer(32, 3, kernel_size=9, stride=1)\n\n        # Non-linearities\n        self.relu = nn.ReLU()\n\n    def forward(self, X):\n        in_X = X\n        y = self.relu(self.in1(self.conv1(in_X)))\n        y = self.relu(self.in2(self.conv2(y)))\n        y = self.relu(self.in3(self.conv3(y)))\n        y = self.res1(y)\n        y = self.res2(y)\n        y = self.res3(y)\n        y = self.res4(y)\n        y = self.res5(y)\n        y = self.relu(self.in4(self.deconv1(y)))\n        y = self.relu(self.in5(self.deconv2(y)))\n        y = self.deconv3(y)\n        return y\n\n\nclass ConvLayer(torch.nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride):\n        super(ConvLayer, self).__init__()\n        reflection_padding = int(np.floor(kernel_size / 2))\n        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n\n    def forward(self, x):\n        out = self.reflection_pad(x)\n        out = self.conv2d(out)\n        return out\n\n\nclass ResidualBlock(torch.nn.Module):\n    """"""ResidualBlock\n    introduced in: https://arxiv.org/abs/1512.03385\n    recommended architecture: http://torch.ch/blog/2016/02/04/resnets.html\n    """"""\n\n    def __init__(self, channels):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n        self.in1 = InstanceNormalization(channels)\n        self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n        self.in2 = InstanceNormalization(channels)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        residual = x\n        out = self.relu(self.in1(self.conv1(x)))\n        out = self.in2(self.conv2(out))\n        out = out + residual\n        return out\n\n\nclass UpsampleConvLayer(torch.nn.Module):\n    """"""UpsampleConvLayer\n    Upsamples the input and then does a convolution. This method gives better results\n    compared to ConvTranspose2d.\n    ref: http://distill.pub/2016/deconv-checkerboard/\n    """"""\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n        super(UpsampleConvLayer, self).__init__()\n        self.upsample = upsample\n        if upsample:\n            self.upsample_layer = torch.nn.Upsample(scale_factor=upsample)\n        reflection_padding = int(np.floor(kernel_size / 2))\n        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n\n    def forward(self, x):\n        x_in = x\n        if self.upsample:\n            x_in = self.upsample_layer(x_in)\n        out = self.reflection_pad(x_in)\n        out = self.conv2d(out)\n        return out\n\n\nclass InstanceNormalization(torch.nn.Module):\n    """"""InstanceNormalization\n    Improves convergence of neural-style.\n    ref: https://arxiv.org/pdf/1607.08022.pdf\n    """"""\n\n    def __init__(self, dim, eps=1e-9):\n        super(InstanceNormalization, self).__init__()\n        self.scale = nn.Parameter(torch.FloatTensor(dim))\n        self.shift = nn.Parameter(torch.FloatTensor(dim))\n        self.eps = eps\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        self.scale.data.uniform_()\n        self.shift.data.zero_()\n\n    def forward(self, x):\n        n = x.size(2) * x.size(3)\n        t = x.view(x.size(0), x.size(1), n)\n        mean = torch.mean(t, 2).unsqueeze(2).unsqueeze(3).expand_as(x)\n        # Calculate the biased var. torch.var returns unbiased var\n        var = torch.var(t, 2).unsqueeze(2).unsqueeze(3).expand_as(x) * ((n - 1) / float(n))\n        scale_broadcast = self.scale.unsqueeze(1).unsqueeze(1).unsqueeze(0)\n        scale_broadcast = scale_broadcast.expand_as(x)\n        shift_broadcast = self.shift.unsqueeze(1).unsqueeze(1).unsqueeze(0)\n        shift_broadcast = shift_broadcast.expand_as(x)\n        out = (x - mean) / torch.sqrt(var + self.eps)\n        out = out * scale_broadcast + shift_broadcast\n        return out\n'"
neural_style/utils.py,8,"b""import os\n\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom torch.autograd import Variable\nfrom torch.utils.serialization import load_lua\n\nfrom vgg16 import Vgg16\n\n\ndef tensor_load_rgbimage(filename, size=None, scale=None):\n    img = Image.open(filename)\n    if size is not None:\n        img = img.resize((size, size), Image.ANTIALIAS)\n    elif scale is not None:\n        img = img.resize((int(img.size[0] / scale), int(img.size[1] / scale)), Image.ANTIALIAS)\n    img = np.array(img).transpose(2, 0, 1)\n    img = torch.from_numpy(img).float()\n    return img\n\n\ndef tensor_save_rgbimage(tensor, filename, cuda=False):\n    if cuda:\n        img = tensor.clone().cpu().clamp(0, 255).numpy()\n    else:\n        img = tensor.clone().clamp(0, 255).numpy()\n    img = img.transpose(1, 2, 0).astype('uint8')\n    img = Image.fromarray(img)\n    img.save(filename)\n\n\ndef tensor_save_bgrimage(tensor, filename, cuda=False):\n    (b, g, r) = torch.chunk(tensor, 3)\n    tensor = torch.cat((r, g, b))\n    tensor_save_rgbimage(tensor, filename, cuda)\n\n\ndef gram_matrix(y):\n    (b, ch, h, w) = y.size()\n    features = y.view(b, ch, w * h)\n    features_t = features.transpose(1, 2)\n    gram = features.bmm(features_t) / (ch * h * w)\n    return gram\n\n\ndef subtract_imagenet_mean_batch(batch):\n    tensortype = type(batch.data)\n    mean = tensortype(batch.data.size())\n    mean[:, 0, :, :] = 103.939\n    mean[:, 1, :, :] = 116.779\n    mean[:, 2, :, :] = 123.680\n    batch = batch.sub(Variable(mean))\n    return batch\n\n\ndef preprocess_batch(batch):\n    batch = batch.transpose(0, 1)\n    (r, g, b) = torch.chunk(batch, 3)\n    batch = torch.cat((b, g, r))\n    batch = batch.transpose(0, 1)\n    return batch\n\n\ndef init_vgg16(model_folder):\n    if not os.path.exists(os.path.join(model_folder, 'vgg16.weight')):\n        if not os.path.exists(os.path.join(model_folder, 'vgg16.t7')):\n            os.system(\n                'wget https://www.dropbox.com/s/76l3rt4kyi3s8x7/vgg16.t7?dl=1 -O ' + os.path.join(model_folder, 'vgg16.t7'))\n        vgglua = load_lua(os.path.join(model_folder, 'vgg16.t7'))\n        vgg = Vgg16()\n        for (src, dst) in zip(vgglua.parameters()[0], vgg.parameters()):\n            dst.data[:] = src\n        torch.save(vgg.state_dict(), os.path.join(model_folder, 'vgg16.weight'))\n"""
neural_style/vgg16.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Vgg16(torch.nn.Module):\n    def __init__(self):\n        super(Vgg16, self).__init__()\n        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n\n        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n\n        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n\n        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n\n        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n\n    def forward(self, X):\n        h = F.relu(self.conv1_1(X))\n        h = F.relu(self.conv1_2(h))\n        relu1_2 = h\n        h = F.max_pool2d(h, kernel_size=2, stride=2)\n\n        h = F.relu(self.conv2_1(h))\n        h = F.relu(self.conv2_2(h))\n        relu2_2 = h\n        h = F.max_pool2d(h, kernel_size=2, stride=2)\n\n        h = F.relu(self.conv3_1(h))\n        h = F.relu(self.conv3_2(h))\n        h = F.relu(self.conv3_3(h))\n        relu3_3 = h\n        h = F.max_pool2d(h, kernel_size=2, stride=2)\n\n        h = F.relu(self.conv4_1(h))\n        h = F.relu(self.conv4_2(h))\n        h = F.relu(self.conv4_3(h))\n        relu4_3 = h\n\n        return [relu1_2, relu2_2, relu3_3, relu4_3]\n'"
