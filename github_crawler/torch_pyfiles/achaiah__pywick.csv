file_path,api_count,code
setup.py,0,"b""#!/usr/bin/env python\n\nfrom setuptools import setup, find_packages\n\n# read the contents of your README file per https://packaging.python.org/guides/making-a-pypi-friendly-readme/\nfrom os import path\nthis_directory = path.abspath(path.dirname(__file__))\nwith open(path.join(this_directory, 'README.md'), encoding='utf-8') as f:\n    long_description = f.read()\n\nsetup(name='pywick',\n      version='0.5.6',\n      description='High-level batteries-included training framework for Pytorch',\n      long_description=long_description,\n      long_description_content_type='text/markdown',\n      author='Achaiah',\n      install_requires=[\n                  'h5py',\n                  'hickle',\n                  'numpy',\n                  'pandas',\n                  'pillow',\n                  'six',\n                  'torch',\n                  'torchvision',\n                  'tqdm',\n            ],\n      packages=find_packages(),\n      url='https://github.com/achaiah/pywick',\n      download_url='https://github.com/achaiah/pywick/archive/v0.5.6.tar.gz',\n      keywords=['pytorch', 'classification', 'deep learning', 'neural networks', 'semantic-segmentation', 'framework'],\n      classifiers=['Development Status :: 4 - Beta',\n                   'Intended Audience :: Developers',\n                   'License :: OSI Approved :: MIT License',\n                   'Programming Language :: Python :: 3.6',\n                   ],\n      )\n"""
examples/mnist_example.py,2,"b""\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom pywick.modules import ModuleTrainer\nfrom pywick.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom pywick.regularizers import L1Regularizer, L2Regularizer\nfrom pywick.constraints import UnitNorm\nfrom pywick.initializers import XavierUniform\nfrom pywick.metrics import CategoricalAccuracy\n\nimport os\nfrom torchvision import datasets\nROOT = '/data/mnist'\ndataset = datasets.MNIST(ROOT, train=True, download=True)\nx_train, y_train = th.load(os.path.join(dataset.root, 'processed/training.pt'))\nx_test, y_test = th.load(os.path.join(dataset.root, 'processed/test.pt'))\n\nx_train = x_train.float()\ny_train = y_train.long()\nx_test = x_test.float()\ny_test = y_test.long()\n\nx_train = x_train / 255.\nx_test = x_test / 255.\nx_train = x_train.unsqueeze(1)\nx_test = x_test.unsqueeze(1)\n\n# only train on a subset\nx_train = x_train[:10000]\ny_train = y_train[:10000]\nx_test = x_test[:1000]\ny_test = y_test[:1000]\n\n\n# Define your model EXACTLY as if you were using nn.Module\nclass Network(nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n        self.fc1 = nn.Linear(1600, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n        x = x.view(-1, 1600)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x)\n\n\nmodel = Network()\ntrainer = ModuleTrainer(model)\n\n\ncallbacks = [EarlyStopping(patience=10),\n             ReduceLROnPlateau(factor=0.5, patience=5)]\nregularizers = [L1Regularizer(scale=1e-3, module_filter='conv*'),\n                L2Regularizer(scale=1e-5, module_filter='fc*')]\nconstraints = [UnitNorm(frequency=3, unit='batch', module_filter='fc*')]\ninitializers = [XavierUniform(bias=False, module_filter='fc*')]\nmetrics = [CategoricalAccuracy(top_k=3)]\n\ntrainer.compile(criterion='nll_loss',\n                optimizer='adadelta',\n                regularizers=regularizers,\n                constraints=constraints,\n                initializers=initializers,\n                metrics=metrics)\n\n#summary = trainer.summary([1,28,28])\n#print(summary)\n\ntrainer.fit(x_train, y_train, \n          val_data=(x_test, y_test),\n          num_epoch=20,\n          batch_size=128,\n          verbose=1)\n\n\n"""
examples/mnist_loader_example.py,3,"b""\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.utils.data import DataLoader\n\nfrom pywick.modules import ModuleTrainer\nfrom pywick.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom pywick.regularizers import L1Regularizer, L2Regularizer\nfrom pywick.constraints import UnitNorm\nfrom pywick.initializers import XavierUniform\nfrom pywick.metrics import CategoricalAccuracy\nfrom pywick import TensorDataset\n\nimport os\nfrom torchvision import datasets\nROOT = '/data/mnist'\ndataset = datasets.MNIST(ROOT, train=True, download=True)\nx_train, y_train = th.load(os.path.join(dataset.root, 'processed/training.pt'))\nx_test, y_test = th.load(os.path.join(dataset.root, 'processed/test.pt'))\n\nx_train = x_train.float()\ny_train = y_train.long()\nx_test = x_test.float()\ny_test = y_test.long()\n\nx_train = x_train / 255.\nx_test = x_test / 255.\nx_train = x_train.unsqueeze(1)\nx_test = x_test.unsqueeze(1)\n\n# only train on a subset\nx_train = x_train[:10000]\ny_train = y_train[:10000]\nx_test = x_test[:1000]\ny_test = y_test[:1000]\n\ntrain_dataset = TensorDataset(x_train, y_train)\ntrain_loader = DataLoader(train_dataset, batch_size=32)\nval_dataset = TensorDataset(x_test, y_test)\nval_loader = DataLoader(val_dataset, batch_size=32)\n\n# Define your model EXACTLY as if you were using nn.Module\nclass Network(nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n        self.fc1 = nn.Linear(1600, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n        x = x.view(-1, 1600)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x)\n\n\nmodel = Network()\ntrainer = ModuleTrainer(model)\n\ncallbacks = [EarlyStopping(patience=10),\n             ReduceLROnPlateau(factor=0.5, patience=5)]\nregularizers = [L1Regularizer(scale=1e-3, module_filter='conv*'),\n                L2Regularizer(scale=1e-5, module_filter='fc*')]\nconstraints = [UnitNorm(frequency=3, unit='batch', module_filter='fc*')]\ninitializers = [XavierUniform(bias=False, module_filter='fc*')]\nmetrics = [CategoricalAccuracy(top_k=3)]\n\ntrainer.compile(criterion='nll_loss',\n                optimizer='adadelta',\n                regularizers=regularizers,\n                constraints=constraints,\n                initializers=initializers,\n                metrics=metrics,\n                callbacks=callbacks)\n\ntrainer.fit_loader(train_loader, val_loader, num_epoch=20, verbose=1)\n\n\n\n"""
pywick/__init__.py,0,"b""__version__ = '0.5.3'\n__author__ = 'Achaiah'\n__description__ = 'High-level batteries-included neural network training library for Pytorch'\n\nfrom pywick import (\n    callbacks,\n    conditions,\n    constraints,\n    datasets,\n    functions,\n    gridsearch,\n    losses,\n    metrics,\n    meters,\n    misc,\n    models,\n    modules,\n    optimizers,\n    regularizers,\n    samplers,\n    transforms\n)\n"""
pywick/conditions.py,0,"b'""""""\nConditions are useful for any custom pre- and post-processing that must be done on batches of data.\nModule trainer maintains two separate condition lists that are executed before/after the network forward pass.\n\nAn example of a condition could be an Assert that needs to be performed before data is processed.\nA more advanced example of a condition could be code that modifies the network based on input or output\n""""""\n\nfrom enum import Enum, auto\nfrom .misc import is_tuple_or_list\n\nclass CondType(Enum):\n    PRE = auto()\n    POST = auto()\n\nclass ConditionsContainer(object):\n    \'\'\'\n    This container maintains metadata about the execution environment in which the conditions are performed\n\n    exec_type of the container indicates whether it is being run during training or evaluation\n    \'\'\'\n    def __init__(self, exec_type, prefix=\'\'):\n        \'\'\'\n        :param exec_type: ExecType of the container (metadata flag about its execution environment)\n        :param prefix: Custom prefix (if any) for output logs\n        \'\'\'\n        self.conditions = {CondType.PRE:[], CondType.POST:[]}\n        self.prefix = prefix\n        self.exec_type = exec_type\n\n    def add_preconditions(self, conditions):\n        \'\'\'\n        :param conditions: pre-condition(s) to add - can be single or a list\n        \'\'\'\n        self._add_conditions(conditions, CondType.PRE)\n\n    def add_postconditions(self, conditions):\n        \'\'\'\n        :param conditions: post-condition(s) to add - can be single or a list\n        \'\'\'\n        self._add_conditions(conditions, CondType.POST)\n\n    def _add_conditions(self, conditions, type):\n        \'\'\'\n        :param conditions: condition(s) to add - can be single or a list\n        :param type: CondType\n        :return:\n        \'\'\'\n        conditionz = [conditions] if not is_tuple_or_list(conditions) else conditions\n        self.conditions[type].extend(conditionz)\n\n\n    def reset(self):\n        \'\'\'\n        Reset conditions in the container\n        :return:\n        \'\'\'\n        for condition in self.conditions[CondType.PRE]:\n            condition.reset()\n        for condition in self.conditions[CondType.POST]:\n            condition.reset()\n\n\n    def __call__(self, cond_type, epoch_num, batch_num, net=None, input_batch=None, output_batch=None, target_batch=None):\n        \'\'\'\n\n        :param cond_type: ContType to execute\n        :param epoch_num: Number of the current epoch\n        :param batch_num: Number of the current batch\n        :param net: Network that is being used\n        :param input_batch: Input that is being used\n        :param output_batch: Output that was generated in the forward pass\n        :param target_batch: Ground truth if available\n        :return:\n        \'\'\'\n        logs = {}\n        for condition in self.conditions[cond_type]:\n            logs_out = condition(self.exec_type, epoch_num, batch_num, net, input_batch, output_batch, target_batch)\n            if logs_out is not None:\n                logs[self.prefix + condition._name] = logs_out\n        return logs\n\nclass Condition(object):\n    """"""\n    Default class from which all other Condition implementations inherit.\n    """"""\n\n    def __call__(self, exec_type, epoch_num, batch_num, net=None, inputs=None, outputs=None, labels=None):\n        \'\'\'\n        :param exec_type: Type of execution from ExecType enum\n        :param epoch_num: The epoch of execution\n        :param batch_num: The batch of execution\n        :param net: network which did the forward pass\n        :param inputs: The inputs that were used\n        :param outputs: Outputs of the forward pass\n        :param labels: Ground Truth\n\n        :return:\n        \'\'\'\n        raise NotImplementedError(\'Custom Conditions must implement this function\')\n\n    def reset(self):\n        raise NotImplementedError(\'Custom Conditions must implement this function\')\n\n\n# class ConditionCallback(Callback):\n#\n#     def __init__(self, container):\n#         self.container = container\n#     def on_epoch_begin(self, epoch_idx, logs):\n#         self.container.reset()\n\n\n\nclass SegmentationInputAsserts(Condition):\n    \'\'\'\n    Executes segmentation-specific asserts before executing forward pass on inputs\n    \'\'\'\n\n    def __call__(self, exec_type, epoch_num, batch_num, net=None, inputs=None, outputs=None, labels=None):\n        assert inputs.size()[2:] == labels.size()[1:]\n\n    def reset(self):\n        pass\n\n\nclass SegmentationOutputAsserts(Condition):\n    \'\'\'\n    Executes segmentation-specific asserts after executing forward pass on inputs\n    \'\'\'\n\n    def __init__(self, num_classes):\n        self.num_classes = num_classes\n\n    def __call__(self, exec_type, epoch_num, batch_num, net=None, inputs=None, outputs=None, labels=None):\n        if isinstance(outputs, tuple):  # if we have an auxiliary output as well\n            if any(item is None for item in outputs) or len(outputs) < 2:      # seriously... why?  I\'m looking at you OCNet\n                outs = outputs[0]\n            else:\n                outs, aux = outputs\n        else:\n            outs = outputs\n        assert outs.size()[2:] == labels.size()[1:]\n        assert outs.size()[1] == self.num_classes\n\n    def reset(self):\n        pass\n'"
pywick/constraints.py,0,"b'""""""\nConstraints can be selectively applied on layers using regular expressions.\nConstraints can be explicit (hard) constraints applied at an arbitrary batch or epoch frequency, or they can be implicit (soft)\nconstraints similar to regularizers where the the constraint deviation is added as a penalty to the total model loss.\n""""""\n\nfrom fnmatch import fnmatch\n\nimport torch as th\nfrom .callbacks import Callback\n\n\nclass ConstraintContainer(object):\n\n    def __init__(self, constraints):\n        self.constraints = constraints\n        self.batch_constraints = [c for c in self.constraints if c.unit.upper() == \'BATCH\']\n        self.epoch_constraints = [c for c in self.constraints if c.unit.upper() == \'EPOCH\']\n\n    def register_constraints(self, model):\n        """"""\n        Grab pointers to the weights which will be modified by constraints so\n        that we don\'t have to search through the entire network using `apply`\n        each time\n        """"""\n        # get batch constraint pointers\n        self._batch_c_ptrs = {}\n        for c_idx, constraint in enumerate(self.batch_constraints):\n            self._batch_c_ptrs[c_idx] = []\n            for name, module in model.named_modules():\n                if fnmatch(name, constraint.module_filter) and hasattr(module, \'weight\'):\n                    self._batch_c_ptrs[c_idx].append(module)\n\n        # get epoch constraint pointers\n        self._epoch_c_ptrs = {}\n        for c_idx, constraint in enumerate(self.epoch_constraints):\n            self._epoch_c_ptrs[c_idx] = []\n            for name, module in model.named_modules():\n                if fnmatch(name, constraint.module_filter) and hasattr(module, \'weight\'):\n                    self._epoch_c_ptrs[c_idx].append(module)\n\n    def apply_batch_constraints(self, batch_idx):\n        for c_idx, modules in self._batch_c_ptrs.items():\n            if (batch_idx+1) % self.constraints[c_idx].frequency == 0:\n                for module in modules:\n                    self.constraints[c_idx](module)\n\n    def apply_epoch_constraints(self, epoch_idx):\n        for c_idx, modules in self._epoch_c_ptrs.items():\n            if (epoch_idx+1) % self.constraints[c_idx].frequency == 0:\n                for module in modules:\n                    self.constraints[c_idx](module)\n\n\nclass ConstraintCallback(Callback):\n\n    def __init__(self, container):\n        self.container = container\n\n    def on_batch_end(self, batch_idx, logs):\n        self.container.apply_batch_constraints(batch_idx)\n\n    def on_epoch_end(self, epoch_idx, logs):\n        self.container.apply_epoch_constraints(epoch_idx)\n\n\nclass Constraint(object):\n    """"""\n    Default class from which all Constraint implementations inherit.\n    """"""\n\n    def __call__(self):\n        raise NotImplementedError(\'Subclasses must implement this method\')\n\n\nclass UnitNorm(Constraint):\n    """"""\n    UnitNorm constraint.\n\n    Constraints the weights to have column-wise unit norm\n    """"""\n    def __init__(self, \n                 frequency=1, \n                 unit=\'batch\',\n                 module_filter=\'*\'):\n\n        self.frequency = frequency\n        self.unit = unit\n        self.module_filter = module_filter\n\n    def __call__(self, module):\n        w = module.weight.data\n        module.weight.data = w.div(th.norm(w,2,0))\n\n\nclass MaxNorm(Constraint):\n    """"""\n    MaxNorm weight constraint.\n\n    Constrains the weights incident to each hidden unit\n    to have a norm less than or equal to a desired value.\n\n    Any hidden unit vector with a norm less than the max norm\n    constaint will not be altered.\n    """"""\n\n    def __init__(self, \n                 value, \n                 axis=0, \n                 frequency=1, \n                 unit=\'batch\',\n                 module_filter=\'*\'):\n        self.value = float(value)\n        self.axis = axis\n\n        self.frequency = frequency\n        self.unit = unit\n        self.module_filter = module_filter\n\n    def __call__(self, module):\n        w = module.weight.data\n        module.weight.data = th.renorm(w, 2, self.axis, self.value)\n\n\nclass NonNeg(Constraint):\n    """"""\n    Constrains the weights to be non-negative.\n    """"""\n    def __init__(self, \n                 frequency=1, \n                 unit=\'batch\',\n                 module_filter=\'*\'):\n        self.frequency = frequency\n        self.unit = unit\n        self.module_filter = module_filter\n\n    def __call__(self, module):\n        w = module.weight.data\n        module.weight.data = w.gt(0).float().mul(w)\n\n\n\n\n\n\n'"
pywick/custom_regularizers.py,3,"b'import torch.nn as nn\nimport numpy as np\nimport torch\n\n# ==== Laplace === #\n# Source: https://raw.githubusercontent.com/atlab/attorch/master/attorch/regularizers.py\n# License: MIT\ndef laplace():\n    return np.array([[0.25, 0.5, 0.25], [0.5, -3.0, 0.5], [0.25, 0.5, 0.25]]).astype(np.float32)[None, None, ...]\n\n\ndef laplace3d():\n    l = np.zeros((3, 3, 3))\n    l[1, 1, 1] = -6.\n    l[1, 1, 2] = 1.\n    l[1, 1, 0] = 1.\n    l[1, 0, 1] = 1.\n    l[1, 2, 1] = 1.\n    l[0, 1, 1] = 1.\n    l[2, 1, 1] = 1.\n    return l.astype(np.float32)[None, None, ...]\n\n\nclass Laplace(nn.Module):\n    """"""\n    Laplace filter for a stack of data.\n    """"""\n\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Conv2d(1, 1, 3, bias=False, padding=1)\n        self.conv.weight.data.copy_(torch.from_numpy(laplace()))\n        self.conv.weight.requires_grad = False\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass Laplace3D(nn.Module):\n    """"""\n    Laplace filter for a stack of data.\n    """"""\n\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Conv3d(1, 1, 3, bias=False, padding=1)\n        self.conv.weight.data.copy_(torch.from_numpy(laplace3d()))\n        self.conv.weight.requires_grad = False\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass LaplaceL2(nn.Module):\n    """"""\n    Laplace regularizer for a 2D convolutional layer.\n    """"""\n\n    def __init__(self):\n        super().__init__()\n        self.laplace = Laplace()\n\n    def forward(self, x):\n        ic, oc, k1, k2 = x.size()\n        return self.laplace(x.view(ic * oc, 1, k1, k2)).pow(2).mean() / 2\n\n\nclass LaplaceL23D(nn.Module):\n    """"""\n    Laplace regularizer for a 2D convolutional layer.\n    """"""\n\n    def __init__(self):\n        super().__init__()\n        self.laplace = Laplace3D()\n\n    def forward(self, x):\n        ic, oc, k1, k2, k3 = x.size()\n        return self.laplace(x.view(ic * oc, 1, k1, k2, k3)).pow(2).mean() / 2\n\n\nclass LaplaceL1(nn.Module):\n    """"""\n    Laplace regularizer for a 2D convolutional layer.\n    """"""\n\n    def __init__(self):\n        super().__init__()\n        self.laplace = Laplace()\n\n    def forward(self, x):\n        ic, oc, k1, k2 = x.size()\n        return self.laplace(x.view(ic * oc, 1, k1, k2)).abs().mean()'"
pywick/data_stats.py,0,"b'import json\nimport os\nimport os.path\nimport argparse\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom pywick.datasets.FolderDataset import FolderDataset, rgb_image_loader\n\nopt = dict()\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--root_path\', required=False, type=str, help=\'Path to root directory of the images\')\nparser.add_argument(\'--output_path\', required=False, type=str, help=\'Path to save computed statistics to. If not provided, will save inside root_path\')\n\nopt = vars(parser.parse_args())\n\n# clean up the dictionary so it doesn\'t contain \'None\' values\nremovals = list()\nfor key, val in opt.items():\n    if val is None:\n        removals.append(key)\nfor rem in removals:\n    # print(\'removing: \', rem)\n    opt.pop(rem)\n\ndataset_mean_std = {\n    \'imagenet\': ([0.485, 0.456, 0.406]),\n    \'general\': ([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n}\n\n\ndef get_dataset_mean_std(dataset=None, img_size=None, output_div=255.0, dataset_name=None):\n    """"""\n        Computes channel-wise mean and std of the dataset. The process is memory-intensive as the entire dataset must fit into memory.\n        Therefore, each image is scaled down to img_size first (default: 256).\n        Assumptions: 1. dataset uses PIL to read images    2. Images are in RGB format.\n        :param dataset: pytorch Dataset\n        :param img_size: scale of images at which to compute mean/std (default: 256)\n        :param output_div: float {1.0, 255.0} - Image values are naturally in 0-255 value range so the returned output is divided by output_div.\n        For example, if output_div = 255.0 then mean/std will be in 0-1 range.\n        :param dataset_name: name of a well-known dataset to return (one of {\'imagenet\', \'general\'})\n\n        :return: (mean, std) as per-channel values ([r,g,b], [r,g,b])\n    """"""\n    if dataset_name in dataset_mean_std.keys():\n        return dataset_mean_std[dataset_name]\n    else:\n        total = np.zeros((3, (len(dataset) * img_size * img_size)), dtype=int)\n        position = 0  # keep track of position in the total array\n\n        for src, _ in tqdm(dataset, ascii=True, desc=""Process"", unit=\'images\'):\n            src = src.resize((img_size, img_size))  # resize to same size\n            src = np.array(src)\n\n            # reshape into correct shape\n            src = src.reshape(img_size * img_size, 3)\n            src = src.swapaxes(1, 0)\n\n            # np.concatenate((a, b, c), axis=1)  # NOPE NOPE NOPE -- makes a memory re-allocation for every concatenate operation\n\n            # -- In-place value substitution -- #\n            place = img_size * img_size * position\n            total[0:src.shape[0], place:place + src.shape[1]] = src  # copies the src data into the total position at specified index\n\n            position = position + 1\n\n        return total.mean(1) / output_div, total.std(1) / output_div  # return channel-wise mean for the entire dataset\n\n\ndef create_dataset_stats(data_path, output_path=None, verbose=False):\n    \'\'\'\n    Generates statistics for the given dataset and writes them to a JSON file. Expects the data to be in the following dir structure:\n    dataroot\n     | - Class Dir\n         | - image 1\n         | - image 2\n         | - image N\n\n    :param data_path: string - path to dataroot\n    :param output_path: - path/filename to write the stats to (default: None - will output stats.json file in the dataroot)\n\n    :return: None\n    \'\'\'\n\n    stats = {}\n    if output_path is None:\n        output_path = os.path.join(data_path, \'stats.json\')\n\n    dataset = FolderDataset(root=data_path, class_mode=\'label\', default_loader=rgb_image_loader)\n\n    stats[\'num_items\'] = len(dataset)\n    mean, std = get_dataset_mean_std(dataset=dataset, img_size=256)\n    stats[\'mean\'], stats[\'std\'] = mean.tolist(), std.tolist()       # convert from numpy array to python\n\n    if verbose:\n        print(\'------- Dataset Stats --------\')\n        print(stats)\n        print(\'Written to: \', output_path)\n        print(\'------ End Dataset Stats ------\')\n\n    with open(output_path, \'a\') as statsfile:\n        json.dump(stats, statsfile)\n\n    return stats\n\nif __name__ == ""__main__"":\n    \'\'\'\n        Sample command: python3 data_stats.py --root_path /Users/Shared/test/images\n    \'\'\'\n    import sys\n    sys.path.append("".."")\n    sys.path.append(""../.."")\n\n    # path = opt.get(\'root_path\',\'/Users/Shared/test/images\')\n    path = opt.get(\'root_path\',\'/Users/Shared/test/deleteme\')\n    stats = create_dataset_stats(data_path=path, output_path=opt.get(\'output_path\', None))\n    # print(\'----- RESULT -----\')\n    # print(stats)\n    # print(\'------------------\')'"
pywick/image_utils.py,0,"b'import numpy as np\n\ndef draw_dice_on_image(label, prob, threshold=125, is_0_255=False):\n    \'\'\'\n    Draws a combined color map depicting how closely the guessed image mask (prob) corresponds to\n    the ground truth (label).  Both label and prob must be black-and-white images of the same size.\n\n    :param label: numpy array - the ground truth (b/w image)\n    :param prob: numpy array - the prediction (b/w image)\n    :param threshold: int - threshold {0-255} below which all values will be set to zero (default 125)\n    :param is_0_255: bool - whether the labels and probs are in 0-255 range or 0-1 range (default: False)\n\n    :return: numpy array depicting the overlap of prob image on top of label (ground truth)\n    \'\'\'\n\n    if not is_0_255:\n        label = label * 255\n        prob = prob * 255\n\n    label = label > threshold\n    prob  = prob > threshold\n\n    H,W  = label.shape\n    results = np.zeros((H*W,3),np.uint8)\n    a = (2*label+prob).reshape(-1)\n    miss = np.where(a==2)[0]\n    hit  = np.where(a==3)[0]\n    fp   = np.where(a==1)[0]\n\n    results[miss] = np.array([255,255,255])\n    results[hit]  = np.array([19,138,249])\n    results[fp]   = np.array([246,249,16])\n    results = results.reshape(H,W,3)\n\n    return results\n\n\ndef draw_mask_on_image(image, mask, bg_color=(19, 138, 249), mask_color=[255, 255, 0], threshold=125, foreground_alpha=[1.0, 1.0, 0.5], is_0_255=False):\n    \'\'\'\n\n    Draws a mask on top of the original image. This is pretty CPU intensive so may want to revise for production environment\n    image and mask must be the same size!\n\n    :param image: numpy array of the image [Width, Height, RGB]\n    :param mask: numpy array representing b/w image mask (black pixels - mask, white pixels - background)\n    :param bg_color: numpy tuple (R,G,B) desired color to fill background with (default: magenta)\n    :param mask_color: numpy array [R,G,B] desired color to colorize the extracted pixels with (default: yellow)\n    :param threshold: threshold value used to determine which pixels in the mask to keep (default: >= 125)\n    :param foreground_alpha: the proportions of blending between the pixels from the image and from mask_color (default: [1.0, 1.0, 0.5] of image mixed with 1-foreground_alpha of mask_color)\n    :param is_0_255: whether the image and mask are in 0-1 floating range or 0-255 integer range (default: False)\n\n    :return: numpy array containing composite image [RGB]\n\n    \'\'\'\n    if not is_0_255:\n        image = image * 255\n        mask = mask * 255\n\n    mask = mask > threshold     # make sure all values below threshold are zero!\n\n    H, W, _ = image.shape\n\n    assert (H,W) == mask.shape, ""image size does not equal mask size!""\n\n    results = np.zeros((H, W, 3), np.uint8)  # create new image and fill with zeros\n    results[...] = bg_color     # fill entire image with bg_color at first\n\n    for x in range(W):          # iterate over every pixel and calculate new values\n        for y in range(H):\n            if mask[x][y] > 0:\n                results[x][y][0] = (image[x][y][0] * foreground_alpha[0]) + (mask_color[0] * (1.0 - foreground_alpha[0]))\n                results[x][y][1] = (image[x][y][1] * foreground_alpha[1]) + (mask_color[1] * (1.0 - foreground_alpha[1]))\n                results[x][y][2] = (image[x][y][2] * foreground_alpha[2]) + (mask_color[2] * (1.0 - foreground_alpha[2]))\n\n    return results'"
pywick/initializers.py,30,"b'""""""\nIt is very important to initialize your neural network with correct weights before training.\nThis is not as trivial as it seems as simple initialization like 0, 1, or even\nthe normal distribution usually yield poor results. Most commonly, weights are initialized\nto be small non-zero values. See `this discussion <https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch/>`_\nfor more info.\n""""""\n\nfrom fnmatch import fnmatch\n\nimport torch.nn.init\n\n\ndef _validate_initializer_string(init):\n    dir_f = dir(torch.nn.init)\n    loss_fns = [d.lower() for d in dir_f]\n    if isinstance(init, str):\n        try:\n            str_idx = loss_fns.index(init.lower())\n        except:\n            raise ValueError(\'Invalid loss string input - must match pytorch function.\')\n        return getattr(torch.nn.init, dir(torch.nn.init)[str_idx])\n    elif callable(init):\n        return init\n    else:\n        raise ValueError(\'Invalid loss input\')\n\n\nclass InitializerContainer(object):\n\n    def __init__(self, initializers):\n        self._initializers = initializers\n\n    def apply(self, model):\n        for initializer in self._initializers:\n            model.apply(initializer)\n\n\nclass Initializer(object):\n    """"""\n    Blank Initializer class from which all other Initializers must inherit\n    """"""\n\n    def __call__(self, module):\n        raise NotImplementedError(\'Initializer must implement this method\')\n\n\nclass GeneralInitializer(Initializer):\n\n    def __init__(self, initializer, bias=False, bias_only=False, **kwargs):\n        self._initializer = _validate_initializer_string(initializer)\n        self.kwargs = kwargs\n\n    def __call__(self, module):\n        classname = module.__class__.__name__\n        if fnmatch(classname, self.module_filter) and hasattr(module, \'weight\'):\n            if self.bias_only:\n                self._initializer(module.bias.data, **self.kwargs)\n            else:\n                self._initializer(module.weight.data, **self.kwargs)\n                if self.bias:\n                    self._initializer(module.bias.data, **self.kwargs)\n\n\nclass Normal(Initializer):\n\n    def __init__(self, mean=0.0, std=0.02, bias=False, \n                 bias_only=False, module_filter=\'*\'):\n        self.mean = mean\n        self.std = std\n\n        self.bias = bias\n        self.bias_only = bias_only\n        self.module_filter = module_filter\n\n        super(Normal, self).__init__()\n\n    def __call__(self, module):\n        classname = module.__class__.__name__\n        if fnmatch(classname, self.module_filter) and hasattr(module, \'weight\'):\n            if self.bias_only:\n                torch.nn.init.normal(module.bias.data, mean=self.mean, std=self.std)\n            else:\n                torch.nn.init.normal(module.weight.data, mean=self.mean, std=self.std)\n                if self.bias:\n                    torch.nn.init.normal(module.bias.data, mean=self.mean, std=self.std)\n\n\nclass Uniform(Initializer):\n\n    def __init__(self, a=0, b=1, bias=False, bias_only=False, module_filter=\'*\'):\n        self.a = a\n        self.b = b\n\n        self.bias = bias\n        self.bias_only = bias_only\n        self.module_filter = module_filter\n\n        super(Uniform, self).__init__()\n\n    def __call__(self, module):\n        classname = module.__class__.__name__\n        if fnmatch(classname, self.module_filter) and hasattr(module, \'weight\'):\n            if self.bias_only:\n                torch.nn.init.uniform(module.bias.data, a=self.a, b=self.b)\n            else:\n                torch.nn.init.uniform(module.weight.data, a=self.a, b=self.b)\n                if self.bias:\n                    torch.nn.init.uniform(module.bias.data, a=self.a, b=self.b)\n\n\nclass ConstantInitializer(Initializer):\n\n    def __init__(self, value, bias=False, bias_only=False, module_filter=\'*\'):\n        self.value = value\n\n        self.bias = bias\n        self.bias_only = bias_only\n        self.module_filter = module_filter\n\n        super(ConstantInitializer, self).__init__()\n\n    def __call__(self, module, bias=False, bias_only=False, module_filter=\'*\'):\n        classname = module.__class__.__name__\n        if fnmatch(classname, self.module_filter) and hasattr(module, \'weight\'):\n            if self.bias_only:\n                torch.nn.init.constant(module.bias.data, val=self.value)\n            else:\n                torch.nn.init.constant(module.weight.data, val=self.value)\n                if self.bias:\n                    torch.nn.init.constant(module.bias.data, val=self.value)\n\n\nclass XavierUniform(Initializer):\n\n    def __init__(self, gain=1, bias=False, bias_only=False, module_filter=\'*\'):\n        self.gain = gain\n\n        self.bias = bias\n        self.bias_only = bias_only\n        self.module_filter = module_filter\n\n        super(XavierUniform, self).__init__()\n\n    def __call__(self, module):\n        classname = module.__class__.__name__\n        if fnmatch(classname, self.module_filter) and hasattr(module, \'weight\'):\n            if self.bias_only:\n                torch.nn.init.xavier_uniform(module.bias.data, gain=self.gain)\n            else:\n                torch.nn.init.xavier_uniform(module.weight.data, gain=self.gain)\n                if self.bias:\n                    torch.nn.init.xavier_uniform(module.bias.data, gain=self.gain)\n\n\nclass XavierNormal(Initializer):\n\n    def __init__(self, gain=1, bias=False, bias_only=False, module_filter=\'*\'):\n        self.gain = gain\n        \n        self.bias = bias\n        self.bias_only = bias_only\n        self.module_filter = module_filter\n\n        super(XavierNormal, self).__init__()\n\n    def __call__(self, module):\n        classname = module.__class__.__name__\n        if fnmatch(classname, self.module_filter) and hasattr(module, \'weight\'):\n            if self.bias_only:\n                torch.nn.init.xavier_normal(module.bias.data, gain=self.gain)\n            else:\n                torch.nn.init.xavier_normal(module.weight.data, gain=self.gain)\n                if self.bias:\n                    torch.nn.init.xavier_normal(module.bias.data, gain=self.gain)\n\n\nclass KaimingUniform(Initializer):\n\n    def __init__(self, a=0, mode=\'fan_in\', bias=False, bias_only=False, module_filter=\'*\'):\n        self.a = a\n        self.mode = mode\n        \n        self.bias = bias\n        self.bias_only = bias_only\n        self.module_filter = module_filter\n\n        super(KaimingUniform, self).__init__()\n\n    def __call__(self, module):\n        classname = module.__class__.__name__\n        if fnmatch(classname, self.module_filter) and hasattr(module, \'weight\'):\n            if self.bias_only:\n                torch.nn.init.kaiming_uniform(module.bias.data, a=self.a, mode=self.mode)\n            else:\n                torch.nn.init.kaiming_uniform(module.weight.data, a=self.a, mode=self.mode)\n                if self.bias:\n                    torch.nn.init.kaiming_uniform(module.bias.data, a=self.a, mode=self.mode)\n\n\nclass KaimingNormal(Initializer):\n\n    def __init__(self, a=0, mode=\'fan_in\', bias=False, bias_only=False, module_filter=\'*\'):\n        self.a = a\n        self.mode = mode\n        \n        self.bias = bias\n        self.bias_only = bias_only\n        self.module_filter = module_filter\n\n        super(KaimingNormal, self).__init__()\n\n    def __call__(self, module):\n        classname = module.__class__.__name__\n        if fnmatch(classname, self.module_filter) and hasattr(module, \'weight\'):\n            if self.bias_only:\n                torch.nn.init.kaiming_normal_(module.bias.data, a=self.a, mode=self.mode)\n            else:\n                torch.nn.init.kaiming_normal_(module.weight.data, a=self.a, mode=self.mode)\n                if self.bias:\n                    torch.nn.init.kaiming_normal_(module.bias.data, a=self.a, mode=self.mode)\n\n\nclass Orthogonal(Initializer):\n\n    def __init__(self, gain=1, bias=False, bias_only=False, module_filter=\'*\'):\n        self.gain = gain\n        \n        self.bias = bias\n        self.bias_only = bias_only\n        self.module_filter = module_filter\n\n        super(Orthogonal, self).__init__()\n\n    def __call__(self, module):\n        classname = module.__class__.__name__\n        if fnmatch(classname, self.module_filter) and hasattr(module, \'weight\'):\n            if self.bias_only:\n                torch.nn.init.orthogonal(module.bias.data, gain=self.gain)\n            else:\n                torch.nn.init.orthogonal(module.weight.data, gain=self.gain)\n                if self.bias:\n                    torch.nn.init.orthogonal(module.bias.data, gain=self.gain)\n\n\nclass Sparse(Initializer):\n\n    def __init__(self, sparsity, std=0.01, bias=False, bias_only=False, module_filter=\'*\'):\n        self.sparsity = sparsity\n        self.std = std\n        \n        self.bias = bias\n        self.bias_only = bias_only\n        self.module_filter = module_filter\n\n        super(Sparse, self).__init__()\n\n    def __call__(self, module):\n        classname = module.__class__.__name__\n        if fnmatch(classname, self.module_filter) and hasattr(module, \'weight\'):\n            if self.bias_only:\n                torch.nn.init.sparse(module.bias.data, sparsity=self.sparsity, std=self.std)\n            else:\n                torch.nn.init.sparse(module.weight.data, sparsity=self.sparsity, std=self.std)\n                if self.bias:\n                    torch.nn.init.sparse(module.bias.data, sparsity=self.sparsity, std=self.std)\n\n\n\n'"
pywick/losses.py,130,"b'""""""\nLosses are critical to training a neural network well. The training can only make progress if you\nprovide a meaningful measure of loss for each training step. What the loss looks like usually depends\non your application. Pytorch has a number of `loss functions <https://pytorch.org/docs/stable/nn.html#loss-functions/>`_ that\nyou can use out of the box. However, some more advanced and cutting edge loss functions exist that are not (yet) part of\nPytorch. We include those below for your experimenting.\\n\n**Caution:** if you decide to use one of these, you will definitely want to peruse the source code first, as it has\nmany additional useful notes and references which will help you.\n\nKeep in mind that losses are specific to the type of task. Classification losses are computed differently from Segmentation losses.\nWithin segmentation domain make sure to use BCE (Binary Cross Entropy) for any work involving binary masks (e.g. num_classes = 1)\nMake sure to read the documentation and notes (in the code) for each loss to understand how it is applied.\n\n`Read this blog post <https://gombru.github.io/2018/05/23/cross_entropy_loss/>`_\n\nNote:\n    Logit is the vector of raw (non-normalized) predictions that a classification model generates, which is ordinarily then passed to a normalization function.\n    If the model is solving a multi-class classification problem, logits typically become an input to the softmax function. The softmax function then generates\n    a vector of (normalized) probabilities with one value for each possible class.\n\nFor example, BCEWithLogitsLoss is a BCE that accepts R((-inf, inf)) and automatically applies torch.sigmoid to convert it to ([0,1]) space.\n""""""\n\n##  Various loss calculation functions  ##\n# Sources:  https://github.com/bermanmaxim/jaccardSegment/blob/master/losses.py (?)\n#           https://github.com/doodledood/carvana-image-masking-challenge/blob/master/losses.py (MIT)\n#           https://github.com/atlab/attorch/blob/master/attorch/losses.py (MIT)\n#           https://github.com/EKami/carvana-challenge (MIT)\n#           https://github.com/DingKe/pytorch_workplace (MIT)\n\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Function\nfrom torch.autograd import Variable\nfrom torch import Tensor\nfrom typing import Iterable, Set\n\nVOID_LABEL = 255\nN_CLASSES = 1\n\n\nclass StableBCELoss(nn.Module):\n    def __init__(self, **kwargs):\n        super(StableBCELoss, self).__init__()\n\n    def forward(self, input, target):\n        neg_abs = - input.abs()\n        loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\n        return loss.mean()\n\n\n# WARN: Only applicable to Binary Segmentation!\ndef binaryXloss(logits, label):\n    mask = (label.view(-1) != VOID_LABEL)\n    nonvoid = mask.long().sum()\n    if nonvoid == 0:\n        # only void pixels, the gradients should be 0\n        return logits.sum() * 0.\n    # if nonvoid == mask.numel():\n    #     # no void pixel, use builtin\n    #     return F.cross_entropy(logits, label)\n    target = label.contiguous().view(-1)[mask]\n    logits = logits.contiguous().view(-1)[mask]\n    # loss = F.binary_cross_entropy(logits, target.float())\n    loss = StableBCELoss()(logits, target.float())\n    return loss\n\n\ndef naive_single(logit, label):\n    # single images\n    mask = (label.view(-1) != 255)\n    num_preds = mask.long().sum()\n    if num_preds == 0:\n        # only void pixels, the gradients should be 0\n        return logit.sum() * 0.\n    target = label.contiguous().view(-1)[mask].float()\n    logit = logit.contiguous().view(-1)[mask]\n    prob = torch.sigmoid(logit)\n    intersect = target * prob\n    union = target + prob - intersect\n    loss = (1. - intersect / union).sum()\n    return loss\n\n\n# WARN: Only applicable to Binary Segmentation!\ndef hingeloss(logits, label):\n    mask = (label.view(-1) != 255)\n    num_preds = mask.long().sum().item()\n    if num_preds == 0:\n        # only void pixels, the gradients should be 0\n        return logits.sum().item() * 0.\n    target = label.contiguous().view(-1)[mask]\n    target = 2. * target.float() - 1.  # [target == 0] = -1\n    logits = logits.contiguous().view(-1)[mask]\n    hinge = 1. / num_preds * F.relu(1. - logits * target).sum().item()\n    return hinge\n\n\ndef gamma_fast(gt, permutation):\n    p = len(permutation)\n    gt = gt.gather(0, permutation)\n    gts = gt.sum()\n\n    intersection = gts - gt.float().cumsum(0)\n    union = gts + (1 - gt).float().cumsum(0)\n    jaccard = 1. - intersection / union\n\n    jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n    return jaccard\n\n# WARN: Only applicable to Binary Segmentation right now (zip function needs to be replaced)!\ndef lovaszloss(logits, labels, prox=False, max_steps=20, debug={}):\n    """"""\n    `The Lovasz-Softmax loss <https://arxiv.org/abs/1705.08790>`_\n\n    :param logits:\n    :param labels:\n    :param prox:\n    :param max_steps:\n    :param debug:\n    :return:\n    """"""\n\n    # image-level Lovasz hinge\n    if logits.size(0) == 1:\n        # single image case\n        loss = lovasz_single(logits.squeeze(0), labels.squeeze(0), prox, max_steps, debug)\n    else:\n        losses = []\n        # assert len(logits[0]) == len(labels[0])\n        for logit, label in zip(logits, labels):\n            loss = lovasz_single(logit, label, prox, max_steps, debug)\n            losses.append(loss)\n        loss = sum(losses) / len(losses)\n    return loss\n\n\ndef naiveloss(logits, labels):\n    # image-level Lovasz hinge\n    if logits.size(0) == 1:\n        # single image case\n        loss = naive_single(logits.squeeze(0), labels.squeeze(0))\n    else:\n        losses = []\n        for logit, label in zip(logits, labels):\n            loss = naive_single(logit, label)\n            losses.append(loss)\n        loss = sum(losses) / len(losses)\n    return loss\n\n\ndef iouloss(pred, gt):\n    # works for one binary pred and associated target\n    # make byte tensors\n    pred = (pred == 1)\n    mask = (gt != 255)\n    gt = (gt == 1)\n    union = (gt | pred)[mask].long().sum()\n    if not union:\n        return 0.\n    else:\n        intersection = (gt & pred)[mask].long().sum()\n        return 1. - intersection / union\n\n\ndef compute_step_length(x, grad, active, eps=1e-6):\n    # compute next intersection with an edge in the direction grad\n    # OR next intersection with a 0 - border\n    # returns: delta in ind such that:\n    # after a step delta in the direction grad, x[ind] and x[ind+1] will be equal\n    delta = np.inf\n    ind = -1\n    if active > 0:\n        numerator = (x[:active] - x[1:active + 1])  # always positive (because x is sorted)\n        denominator = (grad[:active] - grad[1:active + 1])\n        # indices corresponding to negative denominator won\'t intersect\n        # also, we are not interested in indices in x that are *already equal*\n        valid = (denominator > eps) & (numerator > eps)\n        valid_indices = valid.nonzero()\n        intersection_times = numerator[valid] / denominator[valid]\n        if intersection_times.size():\n            delta, ind = intersection_times.min(0)\n            ind = valid_indices[ind]\n            delta, ind = delta[0], ind[0, 0]\n    if grad[active] > 0:\n        intersect_zero = x[active] / grad[active]\n        if intersect_zero > 0. and intersect_zero < delta:\n            return intersect_zero, -1\n    return delta, ind\n\n\ndef project(gam, active, members):\n    tovisit = set(range(active + 1))\n    while tovisit:\n        v = tovisit.pop()\n        if len(members[v]) > 1:\n            avg = 0.\n            for k in members[v]:\n                if k != v: tovisit.remove(k)\n                avg += gam[k] / len(members[v])\n            for k in members[v]:\n                gam[k] = avg\n    if active + 1 < len(gam):\n        gam[active + 1:] = 0.\n\n\ndef find_proximal(x0, gam, lam, eps=1e-6, max_steps=20, debug={}):\n    # x0: sorted margins data\n    # gam: initial gamma_fast(target, perm)\n    # regularisation parameter lam\n    x = x0.clone()\n    act = (x >= eps).nonzero()\n    finished = False\n    if not act.size():\n        finished = True\n    else:\n        active = act[-1, 0]\n        members = {i: {i} for i in range(active + 1)}\n        if active > 0:\n            equal = (x[:active] - x[1:active + 1]) < eps\n            for i, e in enumerate(equal):\n                if e:\n                    members[i].update(members[i + 1])\n                    members[i + 1] = members[i]\n            project(gam, active, members)\n    step = 0\n    while not finished and step < max_steps and active > -1:\n        step += 1\n        res = compute_step_length(x, gam, active, eps)\n        delta, ind = res\n\n        if ind == -1:\n            active = active - len(members[active])\n\n        stop = torch.dot(x - x0, gam) / torch.dot(gam, gam) + 1. / lam\n        if 0 <= stop < delta:\n            delta = stop\n            finished = True\n\n        x = x - delta * gam\n        if not finished:\n            if ind >= 0:\n                repr = min(members[ind])\n                members[repr].update(members[ind + 1])\n                for m in members[ind]:\n                    if m != repr:\n                        members[m] = members[repr]\n            project(gam, active, members)\n        if ""path"" in debug:\n            debug[""path""].append(x.numpy())\n\n    if ""step"" in debug:\n        debug[""step""] = step\n    if ""finished"" in debug:\n        debug[""finished""] = finished\n    return x, gam\n\n\ndef lovasz_binary(margins, label, prox=False, max_steps=20, debug={}):\n    # 1d vector inputs\n    # Workaround: can\'t sort Variable bug\n    # prox: False or lambda regularization value\n    _, perm = torch.sort(margins.detach(), dim=0, descending=True)\n    margins_sorted = margins[perm]\n    grad = gamma_fast(label, perm)\n    loss = torch.dot(F.relu(margins_sorted), grad)\n    if prox is not False:\n        xp, gam = find_proximal(margins_sorted.detach(), grad, prox, max_steps=max_steps, eps=1e-6, debug=debug)\n        hook = margins_sorted.register_hook(lambda grad: (margins_sorted.detach() - xp))\n        return loss, hook, gam\n    else:\n        return loss\n\n\ndef lovasz_single(logit, label, prox=False, max_steps=20, debug={}):\n    # single images\n    mask = (label.view(-1) != 255)\n    num_preds = mask.long().sum()\n    if num_preds == 0:\n        # only void pixels, the gradients should be 0\n        return logit.sum() * 0.\n    target = label.contiguous().view(-1)[mask]\n    signs = 2. * target.float() - 1.\n    logit = logit.contiguous().view(-1)[mask]\n    margins = (1. - logit * signs)\n    loss = lovasz_binary(margins, target, prox, max_steps, debug=debug)\n    return loss\n\n\ndef dice_coefficient(logit, label, isCuda=True):\n    \'\'\'\n    WARNING THIS IS VERY SLOW FOR SOME REASON!!\n\n    :param logit:   calculated guess   (expects torch.Tensor)\n    :param label:   truth label        (expects torch.Tensor)\n    :return:        dice coefficient\n    \'\'\'\n    A = label.view(-1)\n    B = logit.view(-1)\n\n    A = A.clone()\n    B = B.clone()\n\n    assert len(A) == len(B)\n\n    for i in list(range(len(A))):\n        if A[i] > 0.5:\n            A[i] = 1.0\n        else:\n            A[i] = 0.0\n\n        if B[i] > 0.5:\n            B[i] = 1.0\n        else:\n            B[i] = 0.0\n\n    if isCuda:\n        A = A.type(torch.cuda.ByteTensor)\n    else:\n        A = A.type(torch.ByteTensor)\n\n    dice = torch.masked_select(B, A).sum()*2.0 / (B.sum() + A.sum())\n    return dice\n\n\n# ==================================== #\n# Source: https://github.com/EKami/carvana-challenge\nclass WeightedSoftDiceLoss(torch.nn.Module):\n    def __init__(self):\n        super(WeightedSoftDiceLoss, self).__init__()\n\n    def forward(self, logits, labels, weights):\n        probs = torch.sigmoid(logits)\n        num   = labels.size(0)\n        w     = weights.view(num,-1)\n        w2    = w*w\n        m1    = probs.view(num,-1)\n        m2    = labels.view(num,-1)\n        intersection = (m1 * m2)\n        score = 2. * ((w2*intersection).sum(1)+1) / ((w2*m1).sum(1) + (w2*m2).sum(1)+1)\n        score = 1 - score.sum()/num\n        return score\n\n\ndef dice_coeff(pred, target):\n    smooth = 1.\n    num = pred.size(0)\n    m1 = pred.view(num, -1)  # Flatten\n    m2 = target.view(num, -1)  # Flatten\n    intersection = (m1 * m2).sum()\n\n    return (2. * intersection + smooth) / (m1.sum() + m2.sum() + smooth)\n\n\ndef dice_coeff_hard_np(y_true, y_pred):\n    smooth = 1.\n    y_true_f = np.flatten(y_true)\n    y_pred_f = np.round(np.flatten(y_pred))\n    intersection = np.sum(y_true_f * y_pred_f)\n    score = (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)\n\n    return score\n\n\n# ==================================== #\n# Source: https://github.com/doodledood/carvana-image-masking-challenge/blob/master/losses.py\n# TODO Replace this with nn.BCEWithLogitsLoss??\nclass BCELoss2d(nn.Module):\n    def __init__(self, weight=None, size_average=True, **kwargs):\n        super(BCELoss2d, self).__init__()\n        self.bce_loss = nn.BCELoss(weight, size_average)\n\n    def forward(self, logits, targets):\n        probs = torch.sigmoid(logits)\n        probs_flat = probs.view(-1)\n        targets_flat = targets.view(-1)\n        return self.bce_loss(probs_flat, targets_flat)\n\n\nclass SoftDiceLoss(nn.Module):\n    def __init__(self, smooth=1.0, **kwargs):\n        super(SoftDiceLoss, self).__init__()\n        self.smooth = smooth\n\n    def forward(self, logits, targets):\n        #print(\'logits: {}, targets: {}\'.format(logits.size(), targets.size()))\n        num = targets.size(0)\n        probs = torch.sigmoid(logits)\n        m1 = probs.view(num, -1)\n        m2 = targets.view(num, -1)\n        intersection = (m1 * m2)\n\n        # smooth = 1.\n\n        score = 2. * (intersection.sum(1) + self.smooth) / (m1.sum(1) + m2.sum(1) + self.smooth)\n        score = 1 - score.sum() / num\n        return score\n\n\nclass FocalLoss(nn.Module):\n    """"""\n    Weighs the contribution of each sample to the loss based in the classification error.\n    If a sample is already classified correctly by the CNN, its contribution to the loss decreases.\n\n    :eps: Focusing parameter. eps=0 is equivalent to BCE_loss\n    """"""\n    def __init__(self, l=0.5, eps=1e-6):\n        super(FocalLoss, self).__init__()\n        self.l = l\n        self.eps = eps\n\n    def forward(self, logits, targets):\n        targets = targets.view(-1)\n        probs = torch.sigmoid(logits).view(-1)\n\n        losses = -(targets * torch.pow((1. - probs), self.l) * torch.log(probs + self.eps) + \\\n                   (1. - targets) * torch.pow(probs, self.l) * torch.log(1. - probs + self.eps))\n        loss = torch.mean(losses)\n\n        return loss\n\n\nclass ThresholdedL1Loss(nn.Module):\n    def __init__(self, threshold=0.5, **kwargs):\n        super(ThresholdedL1Loss, self).__init__()\n        self.threshold = threshold\n\n    def forward(self, logits, targets):\n        targets = targets.view(-1)\n        probs = torch.sigmoid(logits).view(-1)\n        probs = (probs > 0.5).float()\n\n        losses = torch.abs(targets - probs)\n        loss = torch.mean(losses)\n\n        return loss\n\n\nclass BCEDiceTL1Loss(nn.Module):\n    def __init__(self, threshold=0.5):\n        super(BCEDiceTL1Loss, self).__init__()\n        self.bce = nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, reduction=\'mean\', pos_weight=None)\n        self.dice = SoftDiceLoss()\n        self.tl1 = ThresholdedL1Loss(threshold=threshold)\n\n    def forward(self, logits, targets):\n        return self.bce(logits, targets) + self.dice(logits, targets) + self.tl1(logits, targets)\n\n\nclass BCEDiceFocalLoss(nn.Module):\n    \'\'\'\n        :param num_classes: number of classes\n        :param gamma: (float,double) gamma > 0 reduces the relative loss for well-classified examples (p>0.5) putting more\n                            focus on hard misclassified example\n        :param size_average: (bool, optional) By default, the losses are averaged over each loss element in the batch.\n        :param weights: (list(), default = [1,1,1]) Optional weighing (0.0-1.0) of the losses in order of [bce, dice, focal]\n    \'\'\'\n    def __init__(self, focal_param, weights=[1.0,1.0,1.0], **kwargs):\n        super(BCEDiceFocalLoss, self).__init__()\n        self.bce = nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, reduction=\'mean\', pos_weight=None)\n        self.dice = SoftDiceLoss()\n        self.focal = FocalLoss(l=focal_param)\n        self.weights = weights\n\n    def forward(self, logits, targets):\n        logits = logits.squeeze()\n        return self.weights[0] * self.bce(logits, targets) + self.weights[1] * self.dice(logits, targets) + self.weights[2] * self.focal(logits.unsqueeze(1), targets.unsqueeze(1))\n\n\nclass BCEDiceLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(BCEDiceLoss, self).__init__()\n        self.bce = BCELoss2d()\n        self.dice = SoftDiceLoss()\n\n    def forward(self, logits, targets):\n        return self.bce(logits, targets) + self.dice(logits, targets)\n\n\nclass WeightedBCELoss2d(nn.Module):\n    def __init__(self, **kwargs):\n        super(WeightedBCELoss2d, self).__init__()\n\n    def forward(self, logits, labels, weights):\n        w = weights.view(-1)\n        z = logits.view(-1)\n        t = labels.view(-1)\n        loss = w*z.clamp(min=0) - w*z*t + w*torch.log(1 + torch.exp(-z.abs()))\n        loss = loss.sum()/w.sum()\n        return loss\n\n\nclass WeightedSoftDiceLoss(nn.Module):\n    def __init__(self, **kwargs):\n        super(WeightedSoftDiceLoss, self).__init__()\n\n    def forward(self, logits, labels, weights):\n        probs = torch.sigmoid(logits)\n        num   = labels.size(0)\n        w     = (weights).view(num,-1)\n        w2    = w*w\n        m1    = (probs  ).view(num,-1)\n        m2    = (labels ).view(num,-1)\n        intersection = (m1 * m2)\n        smooth = 1.\n        score = 2. * ((w2*intersection).sum(1)+smooth) / ((w2*m1).sum(1) + (w2*m2).sum(1)+smooth)\n        score = 1 - score.sum()/num\n        return score\n\n\nclass BCEDicePenalizeBorderLoss(nn.Module):\n    def __init__(self, kernel_size=21, **kwargs):\n        super(BCEDicePenalizeBorderLoss, self).__init__()\n        self.bce = WeightedBCELoss2d()\n        self.dice = WeightedSoftDiceLoss()\n        self.kernel_size = kernel_size\n\n    def to(self, device):\n        super().to(device=device)\n        self.bce.to(device=device)\n        self.dice.to(device=device)\n\n    def forward(self, logits, labels):\n        a = F.avg_pool2d(labels, kernel_size=self.kernel_size, padding=self.kernel_size // 2, stride=1)\n        ind = a.ge(0.01) * a.le(0.99)\n        ind = ind.float()\n        weights = torch.ones(a.size()).to(device=logits.device)\n\n        w0 = weights.sum()\n        weights = weights + ind * 2\n        w1 = weights.sum()\n        weights = weights / w1 * w0\n\n        loss = self.bce(logits, labels, weights) + self.dice(logits, labels, weights)\n\n        return loss\n\n\n# ==== Focal Loss with extra parameters ==== #\n# Source: https://github.com/Hsuxu/Loss_ToolBox-PyTorch/blob/master/FocalLoss/FocalLoss.py\n# License: MIT\nclass FocalLoss2(nn.Module):\n    """"""\n    This is a implementation of Focal Loss with smooth label cross entropy supported which is proposed in\n    \'Focal Loss for Dense Object Detection. (https://arxiv.org/abs/1708.02002)\'\n        Focal_Loss= -1*alpha*(1-pt)*log(pt)\n\n    Params:\n        :param num_class:\n        :param alpha: (tensor) 3D or 4D the scalar factor for this criterion\n        :param gamma: (float,double) gamma > 0 reduces the relative loss for well-classified examples (p>0.5) putting more\n                        focus on hard misclassified example\n        :param smooth: (float,double) smooth value when cross entropy\n        :param balance_index: (int) balance class index, should be specific when alpha is float\n        :param size_average: (bool, optional) By default, the losses are averaged over each loss element in the batch.\n    """"""\n\n    def __init__(self, num_class, alpha=None, gamma=2, balance_index=-1, smooth=None, size_average=True):\n        super(FocalLoss2, self).__init__()\n        self.num_class = num_class\n        self.alpha = alpha\n        self.gamma = gamma\n        self.smooth = smooth\n        self.size_average = size_average\n\n        if self.alpha is None:\n            self.alpha = torch.ones(self.num_class, 1)\n        elif isinstance(self.alpha, (list, np.ndarray)):\n            assert len(self.alpha) == self.num_class\n            self.alpha = torch.FloatTensor(alpha).view(self.num_class, 1)\n            self.alpha = self.alpha / self.alpha.sum()\n        elif isinstance(self.alpha, float):\n            alpha = torch.ones(self.num_class, 1)\n            alpha = alpha * (1 - self.alpha)\n            alpha[balance_index] = self.alpha\n            self.alpha = alpha\n        else:\n            raise TypeError(\'Not support alpha type\')\n\n        if self.smooth is not None:\n            if self.smooth < 0 or self.smooth > 1.0:\n                raise ValueError(\'smooth value should be in [0,1]\')\n\n    def forward(self, logit, target):\n\n        # logit = F.softmax(input, dim=1)\n\n        if logit.dim() > 2:\n            # N,C,d1,d2 -> N,C,m (m=d1*d2*...)\n            logit = logit.view(logit.size(0), logit.size(1), -1)\n            logit = logit.permute(0, 2, 1).contiguous()\n            logit = logit.view(-1, logit.size(-1))\n        target = target.view(-1, 1)\n\n        # N = input.size(0)\n        # alpha = torch.ones(N, self.num_class)\n        # alpha = alpha * (1 - self.alpha)\n        # alpha = alpha.scatter_(1, target.long(), self.alpha)\n        epsilon = 1e-10\n        alpha = self.alpha.to(logit.device)\n\n        idx = target.cpu().long()\n\n        one_hot_key = torch.FloatTensor(target.size(0), self.num_class).zero_()\n        one_hot_key = one_hot_key.scatter_(1, idx, 1)\n        one_hot_key = one_hot_key.to(logit.device)\n\n        if self.smooth:\n            one_hot_key = torch.clamp(one_hot_key, self.smooth / (self.num_class - 1), 1.0 - self.smooth)\n        pt = (one_hot_key * logit).sum(1) + epsilon\n        logpt = pt.log()\n\n        gamma = self.gamma\n\n        alpha = alpha[idx]\n        loss = -1 * alpha * torch.pow((1 - pt), gamma) * logpt\n\n        if self.size_average:\n            loss = loss.mean()\n        else:\n            loss = loss.sum()\n        return loss\n\n\n# -------- #\n# Source: https://github.com/huaifeng1993/DFANet/blob/master/loss.py\nclass FocalLoss3(nn.Module):\n    """"""\n        This criterion is a implemenation of Focal Loss, which is proposed in Focal Loss for Dense Object Detection.\n            Loss(x, class) = - \\alpha (1-softmax(x)[class])^gamma \\log(softmax(x)[class])\n        The losses are averaged across observations for each minibatch.\n\n        Params:\n            :param alpha: (1D Tensor, Variable) - the scalar factor for this criterion\n            :param gamma: (float, double) - gamma > 0\n            :param size_average: (bool) - size_average(bool): By default, the losses are averaged over observations for each minibatch.\n                                However, if the field size_average is set to False, the losses are instead summed for each minibatch.\n    """"""\n\n    def __init__(self, class_num, alpha=None, gamma=2, size_average=True):\n        super(FocalLoss3, self).__init__()\n        if alpha is None:\n            self.alpha = Variable(torch.ones(class_num+1))\n        else:\n            if isinstance(alpha, Variable):\n                self.alpha = alpha\n            else:\n                self.alpha = Variable(alpha)\n        self.gamma = gamma\n        self.class_num = class_num\n        self.size_average = size_average\n\n    def forward(self, inputs, targets):  # variables\n        P = F.softmax(inputs)\n\n        if len(inputs.size()) == 3:\n            torch_out = torch.zeros(inputs.size())\n        else:\n            b,c,h,w = inputs.size()\n            torch_out = torch.zeros([b,c+1,h,w])\n\n        if inputs.is_cuda:\n            torch_out = torch_out.cuda()\n\n        class_mask = Variable(torch_out)\n        class_mask.scatter_(1, targets.long(), 1.)\n        class_mask = class_mask[:,:-1,:,:]\n\n        if inputs.is_cuda and not self.alpha.is_cuda:\n            self.alpha = self.alpha.cuda()\n        # print(\'alpha\',self.alpha.size())\n        alpha = self.alpha[targets.data.view(-1)].view_as(targets)\n        # print (alpha.size(),class_mask.size(),P.size())\n        probs = (P * class_mask).sum(1)  # + 1e-6#.view(-1, 1)\n        log_p = probs.log()\n\n        batch_loss = -alpha * (torch.pow((1 - probs), self.gamma)) * log_p\n\n        if self.size_average:\n            loss = batch_loss.mean()\n        else:\n            loss = batch_loss.sum()\n        return loss\n# -------- #\n\n\n# -------- #\n# Source: https://discuss.pytorch.org/t/is-this-a-correct-implementation-for-focal-loss-in-pytorch/43327/4\nclass BinaryFocalLoss(nn.Module):\n    \'\'\'\n        Implementation of binary focal loss. For multi-class focal loss use one of the other implementations.\n\n        gamma = 0 is equivalent to BinaryCrossEntropy Loss\n    \'\'\'\n    def __init__(self, gamma=1.333, eps=1e-6, alpha=1.0, **kwargs):\n        super().__init__()\n        self.gamma = gamma\n        self.eps = eps\n        self.alpha = alpha\n\n    def forward(self, inputs, targets):\n        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\'none\')\n        pt = torch.exp(-BCE_loss)  # prevents nans when probability 0\n        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n        return F_loss.mean()\n# -------- #\n\n\n# ==== Additional Losses === #\n# Source: https://github.com/atlab/attorch/blob/master/attorch/losses.py\n# License: MIT\nclass PoissonLoss(nn.Module):\n    def __init__(self, bias=1e-12, **kwargs):\n        super().__init__()\n        self.bias = bias\n\n    def forward(self, output, target):\n        # _assert_no_grad(target)\n        with torch.no_grad:         # Pytorch 0.4.0 replacement (should be ok to use like this)\n            return (output - target * torch.log(output + self.bias)).mean()\n\n\nclass PoissonLoss3d(nn.Module):\n    def __init__(self, bias=1e-12, **kwargs):\n        super().__init__()\n        self.bias = bias\n\n    def forward(self, output, target):\n        # _assert_no_grad(target)\n        with torch.no_grad:  # Pytorch 0.4.0 replacement (should be ok to use like this)\n            lag = target.size(1) - output.size(1)\n            return (output - target[:, lag:, :] * torch.log(output + self.bias)).mean()\n\n\nclass L1Loss3d(nn.Module):\n    def __init__(self, bias=1e-12, **kwargs):\n        super().__init__()\n        self.bias = bias\n\n    def forward(self, output, target):\n        # _assert_no_grad(target)\n        with torch.no_grad:  # Pytorch 0.4.0 replacement (should be ok to use like this)\n            lag = target.size(1) - output.size(1)\n            return (output - target[:, lag:, :]).abs().mean()\n\n\nclass MSE3D(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, output, target):\n        # _assert_no_grad(target)\n        with torch.no_grad:  # Pytorch 0.4.0 replacement (should be ok to use like this)\n            lag = target.size(1) - output.size(1)\n            return (output - target[:, lag:, :]).pow(2).mean()\n\n\n# ==== Custom ==== #\nclass BCEWithLogitsViewLoss(nn.BCEWithLogitsLoss):\n    \'\'\'\n    Silly wrapper of nn.BCEWithLogitsLoss because BCEWithLogitsLoss only takes a 1-D array\n    \'\'\'\n    def __init__(self, weight=None, size_average=True, **kwargs):\n        super().__init__(weight=weight, size_average=size_average)\n\n    def forward(self, input, target):\n        \'\'\'\n        :param input:\n        :param target:\n        :return:\n\n        Simply passes along input.view(-1), target.view(-1)\n        \'\'\'\n        return super().forward(input.view(-1), target.view(-1))\n\n\n# ===================== #\n# Source: https://discuss.pytorch.org/t/one-hot-encoding-with-autograd-dice-loss/9781/5\n# For calculating dice loss on images where multiple classes are present at the same time\ndef multi_class_dice_loss(output, target, weights=None, ignore_index=None):\n    # output : NxCxHxW float tensor\n    # target :  NxHxW long tensor\n    # weights : C float tensor\n    # ignore_index : int value to ignore from loss\n    smooth = 1.\n    loss = 0.\n\n    output = output.exp()\n    encoded_target = output.detach().clone().zero_()\n    if ignore_index is not None:\n        mask = target == ignore_index\n        target = target.clone()\n        target[mask] = 0\n        encoded_target.scatter_(1, target.unsqueeze(1), 1)\n        mask = mask.unsqueeze(1).expand_as(encoded_target)\n        encoded_target[mask] = 0\n    else:\n        encoded_target.scatter_(1, target.unsqueeze(1), 1)\n\n    if weights is None:\n        weights = torch.ones(output.size(1)).type_as(output.detach())\n\n    intersection = output * encoded_target\n    numerator = 2 * intersection.sum(3).sum(2).sum(0) + smooth\n    denominator = (output + encoded_target).sum(3).sum(2).sum(0) + smooth\n    loss_per_channel = weights * (1 - (numerator / denominator))\n\n    return loss_per_channel.sum() / output.size(1)\n\n# ====================== #\n# Source: https://discuss.pytorch.org/t/how-to-implement-soft-iou-loss/15152\n# Calculation of soft-IOU loss\ndef to_one_hot(tensor, nClasses):\n    n, h, w = tensor.size()\n    one_hot = torch.zeros(n, nClasses, h, w).scatter_(1, tensor.view(n, 1, h, w), 1)\n    return one_hot\n\n\n# ====================== #\n# Source: https://gist.github.com/jeremyjordan/9ea3032a32909f71dd2ab35fe3bacc08\n# Another calculation of dice loss over multiple classes. Input is numpy matrices.\ndef soft_multiclass_dice_loss(y_true, y_pred, epsilon=1e-6):\n    \'\'\'\n    Soft dice loss calculation for arbitrary batch size, number of classes, and number of spatial dimensions.\n    Assumes the `channels_last` format.\n\n    # Arguments\n        y_true: b x X x Y( x Z...) x c One hot encoding of ground truth\n        y_pred: b x X x Y( x Z...) x c Network output, must sum to 1 over c channel (such as after softmax)\n        epsilon: Used for numerical stability to avoid divide by zero errors\n\n    # References\n        V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation\n        https://arxiv.org/abs/1606.04797\n        More details on Dice loss formulation\n        https://mediatum.ub.tum.de/doc/1395260/1395260.pdf (page 72)\n\n        Adapted from https://github.com/Lasagne/Recipes/issues/99#issuecomment-347775022\n    \'\'\'\n\n    # skip the batch and class axis for calculating Dice score\n    axes = tuple(range(1, len(y_pred.shape) - 1))\n    numerator = 2. * np.sum(y_pred * y_true, axes)\n    denominator = np.sum(np.square(y_pred) + np.square(y_true), axes)\n\n    return 1 - np.mean(numerator / (denominator + epsilon))  # average over classes and batch\n\n\nclass mIoULoss(nn.Module):\n    def __init__(self, weight=None, size_average=True, num_classes=2, **kwargs):\n        super(mIoULoss, self).__init__()\n        self.classes = num_classes\n\n    def forward(self, inputs, target_oneHot):\n        # inputs => N x Classes x H x W\n        # target_oneHot => N x Classes x H x W\n\n        N = inputs.size()[0]\n\n        # predicted probabilities for each pixel along channel\n        inputs = F.softmax(inputs, dim=1)\n\n        # Numerator Product\n        inter = inputs * target_oneHot\n        ## Sum over all pixels N x C x H x W => N x C\n        inter = inter.view(N, self.classes, -1).sum(2)\n\n        # Denominator\n        union = inputs + target_oneHot - (inputs * target_oneHot)\n        ## Sum over all pixels N x C x H x W => N x C\n        union = union.view(N, self.classes, -1).sum(2)\n\n        loss = inter / union\n\n        ## Return average loss over classes and batch\n        return -loss.mean()\n\n\n# ====================== #\n# Source: https://github.com/snakers4/mnasnet-pytorch/blob/master/src/models/semseg_loss.py\n# Combination Loss from BCE and Dice\nclass ComboBCEDiceLoss(nn.Module):\n    """"""\n        Combination BinaryCrossEntropy (BCE) and Dice Loss with an optional running mean and loss weighing.\n    """"""\n\n    def __init__(self, use_running_mean=False, bce_weight=1, dice_weight=1, eps=1e-6, gamma=0.9, combined_loss_only=True, **kwargs):\n        """"""\n\n        :param use_running_mean: - bool (default: False) Whether to accumulate a running mean and add it to the loss with (1-gamma)\n        :param bce_weight: - float (default: 1.0) Weight multiplier for the BCE loss (relative to dice)\n        :param dice_weight: - float (default: 1.0) Weight multiplier for the Dice loss (relative to BCE)\n        :param eps: -\n        :param gamma:\n        :param combined_loss_only: - bool (default: True) whether to return a single combined loss or three separate losses\n        """"""\n\n        super().__init__()\n        \'\'\'\n        Note: BCEWithLogitsLoss already performs a torch.sigmoid(pred)\n        before applying BCE!\n        \'\'\'\n        self.bce_logits_loss = nn.BCEWithLogitsLoss()\n\n        self.dice_weight = dice_weight\n        self.bce_weight = bce_weight\n        self.eps = eps\n        self.gamma = gamma\n        self.combined_loss_only = combined_loss_only\n\n        self.use_running_mean = use_running_mean\n        self.bce_weight = bce_weight\n        self.dice_weight = dice_weight\n\n        if self.use_running_mean == True:\n            self.register_buffer(\'running_bce_loss\', torch.zeros(1))\n            self.register_buffer(\'running_dice_loss\', torch.zeros(1))\n            self.reset_parameters()\n\n    def to(self, device):\n        super().to(device=device)\n        self.bce_logits_loss.to(device=device)\n\n    def reset_parameters(self):\n        self.running_bce_loss.zero_()\n        self.running_dice_loss.zero_()\n\n    def forward(self, outputs, targets):\n        # inputs and targets are assumed to be BxCxWxH (batch, color, width, height)\n        outputs = outputs.squeeze()       # necessary in case we\'re dealing with binary segmentation (color dim of 1)\n        assert len(outputs.shape) == len(targets.shape)\n        # assert that B, W and H are the same\n        assert outputs.size(-0) == targets.size(-0)\n        assert outputs.size(-1) == targets.size(-1)\n        assert outputs.size(-2) == targets.size(-2)\n\n        bce_loss = self.bce_logits_loss(outputs, targets)\n\n        dice_target = (targets == 1).float()\n        dice_output = F.sigmoid(outputs)\n        intersection = (dice_output * dice_target).sum()\n        union = dice_output.sum() + dice_target.sum() + self.eps\n        dice_loss = (-torch.log(2 * intersection / union))\n\n        if self.use_running_mean == False:\n            bmw = self.bce_weight\n            dmw = self.dice_weight\n            # loss += torch.clamp(1 - torch.log(2 * intersection / union),0,100)  * self.dice_weight\n        else:\n            self.running_bce_loss = self.running_bce_loss * self.gamma + bce_loss.data * (1 - self.gamma)\n            self.running_dice_loss = self.running_dice_loss * self.gamma + dice_loss.data * (1 - self.gamma)\n\n            bm = float(self.running_bce_loss)\n            dm = float(self.running_dice_loss)\n\n            bmw = 1 - bm / (bm + dm)\n            dmw = 1 - dm / (bm + dm)\n\n        loss = bce_loss * bmw + dice_loss * dmw\n\n        if self.combined_loss_only:\n            return loss\n        else:\n            return loss, bce_loss, dice_loss\n\n\nclass ComboSemsegLossWeighted(nn.Module):\n    def __init__(self,\n                 use_running_mean=False,\n                 bce_weight=1,\n                 dice_weight=1,\n                 eps=1e-6,\n                 gamma=0.9,\n                 use_weight_mask=False,\n                 combined_loss_only=False,\n                 **kwargs\n                 ):\n        super().__init__()\n\n        self.use_weight_mask = use_weight_mask\n\n        self.nll_loss = nn.BCEWithLogitsLoss()\n        self.dice_weight = dice_weight\n        self.bce_weight = bce_weight\n        self.eps = eps\n        self.gamma = gamma\n        self.combined_loss_only = combined_loss_only\n\n        self.use_running_mean = use_running_mean\n        self.bce_weight = bce_weight\n        self.dice_weight = dice_weight\n\n        if self.use_running_mean == True:\n            self.register_buffer(\'running_bce_loss\', torch.zeros(1))\n            self.register_buffer(\'running_dice_loss\', torch.zeros(1))\n            self.reset_parameters()\n\n    def to(self, device):\n        super().to(device=device)\n        self.nll_loss.to(device=device)\n\n    def reset_parameters(self):\n        self.running_bce_loss.zero_()\n        self.running_dice_loss.zero_()\n\n    def forward(self,\n                outputs,\n                targets,\n                weights):\n        # inputs and targets are assumed to be BxCxWxH\n        assert len(outputs.shape) == len(targets.shape)\n        # assert that B, W and H are the same\n        assert outputs.size(0) == targets.size(0)\n        assert outputs.size(2) == targets.size(2)\n        assert outputs.size(3) == targets.size(3)\n\n        # weights are assumed to be BxWxH\n        # assert that B, W and H are the are the same for target and mask\n        assert outputs.size(0) == weights.size(0)\n        assert outputs.size(2) == weights.size(1)\n        assert outputs.size(3) == weights.size(2)\n\n        if self.use_weight_mask:\n            bce_loss = F.binary_cross_entropy_with_logits(input=outputs,\n                                                          target=targets,\n                                                          weight=weights)\n        else:\n            bce_loss = self.nll_loss(input=outputs,\n                                     target=targets)\n\n        dice_target = (targets == 1).float()\n        dice_output = F.sigmoid(outputs)\n        intersection = (dice_output * dice_target).sum()\n        union = dice_output.sum() + dice_target.sum() + self.eps\n        dice_loss = (-torch.log(2 * intersection / union))\n\n        if self.use_running_mean == False:\n            bmw = self.bce_weight\n            dmw = self.dice_weight\n            # loss += torch.clamp(1 - torch.log(2 * intersection / union),0,100)  * self.dice_weight\n        else:\n            self.running_bce_loss = self.running_bce_loss * self.gamma + bce_loss.data * (1 - self.gamma)\n            self.running_dice_loss = self.running_dice_loss * self.gamma + dice_loss.data * (1 - self.gamma)\n\n            bm = float(self.running_bce_loss)\n            dm = float(self.running_dice_loss)\n\n            bmw = 1 - bm / (bm + dm)\n            dmw = 1 - dm / (bm + dm)\n\n        loss = bce_loss * bmw + dice_loss * dmw\n\n        if self.combined_loss_only:\n            return loss\n        else:\n            return loss, bce_loss, dice_loss\n\n\n# ====================== #\n# Source: https://github.com/PkuRainBow/OCNet/blob/master/utils/loss.py\n# Description: http://www.erogol.com/online-hard-example-mining-pytorch/\n# Online Hard Example Loss\nclass OhemCrossEntropy2d(nn.Module):\n    """"""\n    Online Hard Example Loss with Cross Entropy (used for classification)\n\n    OHEM description: http://www.erogol.com/online-hard-example-mining-pytorch/\n    """"""\n    def __init__(self, ignore_label=-1, thresh=0.7, min_kept=100000, use_weight=True, **kwargs):\n        super(OhemCrossEntropy2d, self).__init__()\n        self.ignore_label = ignore_label\n        self.thresh = float(thresh)\n        self.min_kept = int(min_kept)\n        if use_weight:\n            print(""w/ class balance"")\n            weight = torch.FloatTensor([0.8373, 0.918, 0.866, 1.0345, 1.0166, 0.9969, 0.9754,\n                1.0489, 0.8786, 1.0023, 0.9539, 0.9843, 1.1116, 0.9037, 1.0865, 1.0955,\n                1.0865, 1.1529, 1.0507])\n            self.criterion = torch.nn.CrossEntropyLoss(weight=weight, ignore_index=ignore_label)\n        else:\n            print(""w/o class balance"")\n            self.criterion = torch.nn.CrossEntropyLoss(ignore_index=ignore_label)\n\n    def to(self, device):\n        super().to(device=device)\n        self.criterion.to(device=device)\n\n    def forward(self, predict, target, weight=None):\n        """"""\n        Args:\n            predict:(n, c, h, w)\n            target:(n, h, w)\n            weight (Tensor, optional): a manual rescaling weight given to each class.\n                                       If given, has to be a Tensor of size ""nclasses""\n        """"""\n        assert not target.requires_grad\n        assert predict.dim() == 4\n        assert target.dim() == 3\n        assert predict.size(0) == target.size(0), ""{0} vs {1} "".format(predict.size(0), target.size(0))\n        assert predict.size(2) == target.size(1), ""{0} vs {1} "".format(predict.size(2), target.size(1))\n        assert predict.size(3) == target.size(2), ""{0} vs {1} "".format(predict.size(3), target.size(3))\n\n        n, c, h, w = predict.size()\n        input_label = target.data.cpu().numpy().ravel().astype(np.int32)\n        x = np.rollaxis(predict.data.cpu().numpy(), 1).reshape((c, -1))\n        input_prob = np.exp(x - x.max(axis=0).reshape((1, -1)))\n        input_prob /= input_prob.sum(axis=0).reshape((1, -1))\n\n        valid_flag = input_label != self.ignore_label\n        valid_inds = np.where(valid_flag)[0]\n        label = input_label[valid_flag]\n        num_valid = valid_flag.sum()\n        if self.min_kept >= num_valid:\n            print(\'Labels: {}\'.format(num_valid))\n        elif num_valid > 0:\n            prob = input_prob[:,valid_flag]\n            pred = prob[label, np.arange(len(label), dtype=np.int32)]\n            threshold = self.thresh\n            if self.min_kept > 0:\n                index = pred.argsort()\n                threshold_index = index[ min(len(index), self.min_kept) - 1 ]\n                if pred[threshold_index] > self.thresh:\n                    threshold = pred[threshold_index]\n            kept_flag = pred <= threshold\n            valid_inds = valid_inds[kept_flag]\n\n        label = input_label[valid_inds].copy()\n        input_label.fill(self.ignore_label)\n        input_label[valid_inds] = label\n        valid_flag_new = input_label != self.ignore_label\n        # print(np.sum(valid_flag_new))\n        target = Variable(torch.from_numpy(input_label.reshape(target.size())).long().cuda())\n\n        return self.criterion(predict, target)\n\n\n# ====================== #\n# Source: https://github.com/Tramac/awesome-semantic-segmentation-pytorch/blob/master/core/utils/loss.py\n# Loss used for EncNet\nclass EncNetLoss(nn.CrossEntropyLoss):\n    """"""\n    2D Cross Entropy Loss with SE Loss\n\n    Specifically used for EncNet.\n    se_loss is the Semantic Encoding Loss from the paper `Context Encoding for Semantic Segmentation <https://arxiv.org/pdf/1803.08904v1>`_.\n    It computes probabilities of contexts appearing together.\n\n    Without SE_loss and Aux_loss this class simply forwards inputs to Torch\'s Cross Entropy Loss (nn.CrossEntropyLoss)\n    """"""\n\n    def __init__(self, se_loss=True, se_weight=0.2, nclass=19, aux=False, aux_weight=0.4, weight=None, ignore_index=-1, **kwargs):\n        super(EncNetLoss, self).__init__(weight, None, ignore_index)\n        self.se_loss = se_loss\n        self.aux = aux\n        self.nclass = nclass\n        self.se_weight = se_weight\n        self.aux_weight = aux_weight\n        self.bceloss = nn.BCELoss(weight)\n\n    def forward(self, *inputs):\n        preds, target = tuple(inputs)\n        inputs = tuple(list(preds) + [target])\n        if not self.se_loss and not self.aux:\n            return super(EncNetLoss, self).forward(*inputs)\n        elif not self.se_loss:\n            pred1, pred2, target = tuple(inputs)\n            loss1 = super(EncNetLoss, self).forward(pred1, target)\n            loss2 = super(EncNetLoss, self).forward(pred2, target)\n            return dict(loss=loss1 + self.aux_weight * loss2)\n        elif not self.aux:\n            pred, se_pred, target = tuple(inputs)\n            se_target = self._get_batch_label_vector(target, nclass=self.nclass).type_as(pred)\n            loss1 = super(EncNetLoss, self).forward(pred, target)\n            loss2 = self.bceloss(torch.sigmoid(se_pred), se_target)\n            return dict(loss=loss1 + self.se_weight * loss2)\n        else:\n            pred1, se_pred, pred2, target = tuple(inputs)\n            se_target = self._get_batch_label_vector(target, nclass=self.nclass).type_as(pred1)\n            loss1 = super(EncNetLoss, self).forward(pred1, target)\n            loss2 = super(EncNetLoss, self).forward(pred2, target)\n            loss3 = self.bceloss(torch.sigmoid(se_pred), se_target)\n            return dict(loss=loss1 + self.aux_weight * loss2 + self.se_weight * loss3)\n\n    @staticmethod\n    def _get_batch_label_vector(target, nclass):\n        # target is a 3D Variable BxHxW, output is 2D BxnClass\n        batch = target.size(0)\n        tvect = Variable(torch.zeros(batch, nclass))\n        for i in range(batch):\n            hist = torch.histc(target[i].cpu().data.float(),\n                               bins=nclass, min=0,\n                               max=nclass - 1)\n            vect = hist > 0\n            tvect[i] = vect\n        return tvect\n\n\nclass MixSoftmaxCrossEntropyOHEMLoss(OhemCrossEntropy2d):\n    """"""\n    Loss taking into consideration class and segmentation targets together, as well as, using OHEM\n    """"""\n    def __init__(self, aux=False, aux_weight=0.4, weight=None, ignore_index=-1, **kwargs):\n        super(MixSoftmaxCrossEntropyOHEMLoss, self).__init__(ignore_index=ignore_index)\n        self.aux = aux\n        self.aux_weight = aux_weight\n        self.bceloss = nn.BCELoss(weight)\n\n    def to(self, device):\n        super().to(device=device)\n        self.bceloss.to(device=device)\n\n    def _aux_forward(self, *inputs, **kwargs):\n        *preds, target = tuple(inputs)\n\n        loss = super(MixSoftmaxCrossEntropyOHEMLoss, self).forward(preds[0], target)\n        for i in range(1, len(preds)):\n            aux_loss = super(MixSoftmaxCrossEntropyOHEMLoss, self).forward(preds[i], target)\n            loss += self.aux_weight * aux_loss\n        return loss\n\n    def forward(self, *inputs):\n        preds, target = tuple(inputs)\n        inputs = tuple(list(preds) + [target])\n        if self.aux:\n            return dict(loss=self._aux_forward(*inputs))\n        else:\n            return dict(loss=super(MixSoftmaxCrossEntropyOHEMLoss, self).forward(*inputs))\n\n\n# ====================== #\n# Source: https://github.com/zhanghang1989/PyTorch-Encoding/blob/master/encoding/nn/loss.py\n# OHEM Segmentation Loss\nclass OHEMSegmentationLosses(OhemCrossEntropy2d):\n    """"""\n    2D Cross Entropy Loss with Auxiliary Loss\n\n    """"""\n    def __init__(self, se_loss=False, se_weight=0.2, nclass=-1,\n                 aux=False, aux_weight=0.4, weight=None,\n                 ignore_index=-1):\n        super(OHEMSegmentationLosses, self).__init__(ignore_index)\n        self.se_loss = se_loss\n        self.aux = aux\n        self.nclass = nclass\n        self.se_weight = se_weight\n        self.aux_weight = aux_weight\n        self.bceloss = nn.BCELoss(weight)\n\n    def to(self, device):\n        super().to(device=device)\n        self.bceloss.to(device=device)\n\n    def forward(self, *inputs):\n        if not self.se_loss and not self.aux:\n            return super(OHEMSegmentationLosses, self).forward(*inputs)\n        elif not self.se_loss:\n            pred1, pred2, target = tuple(inputs)\n            loss1 = super(OHEMSegmentationLosses, self).forward(pred1, target)\n            loss2 = super(OHEMSegmentationLosses, self).forward(pred2, target)\n            return loss1 + self.aux_weight * loss2\n        elif not self.aux:\n            pred, se_pred, target = tuple(inputs)\n            se_target = self._get_batch_label_vector(target, nclass=self.nclass).type_as(pred)\n            loss1 = super(OHEMSegmentationLosses, self).forward(pred, target)\n            loss2 = self.bceloss(torch.sigmoid(se_pred), se_target)\n            return loss1 + self.se_weight * loss2\n        else:\n            pred1, se_pred, pred2, target = tuple(inputs)\n            se_target = self._get_batch_label_vector(target, nclass=self.nclass).type_as(pred1)\n            loss1 = super(OHEMSegmentationLosses, self).forward(pred1, target)\n            loss2 = super(OHEMSegmentationLosses, self).forward(pred2, target)\n            loss3 = self.bceloss(torch.sigmoid(se_pred), se_target)\n            return loss1 + self.aux_weight * loss2 + self.se_weight * loss3\n\n    @staticmethod\n    def _get_batch_label_vector(target, nclass):\n        # target is a 3D Variable BxHxW, output is 2D BxnClass\n        batch = target.size(0)\n        tvect = Variable(torch.zeros(batch, nclass))\n        for i in range(batch):\n            hist = torch.histc(target[i].cpu().data.float(),\n                               bins=nclass, min=0,\n                               max=nclass-1)\n            vect = hist>0\n            tvect[i] = vect\n        return tvect\n\n\n# Source: https://github.com/Hsuxu/Loss_ToolBox-PyTorch/blob/master/TverskyLoss/binarytverskyloss.py (MIT)\nclass FocalBinaryTverskyFunc(Function):\n    """"""\n        Focal Tversky Loss as defined in `this paper <https://arxiv.org/abs/1810.07842>`_\n\n        `Authors\' implementation <https://github.com/nabsabraham/focal-tversky-unet>`_ in Keras.\n\n        Params:\n            :param alpha: controls the penalty for false positives.\n            :param beta: penalty for false negative.\n            :param gamma : focal coefficient range[1,3]\n            :param reduction: return mode\n\n        Notes:\n            alpha = beta = 0.5 => dice coeff\n            alpha = beta = 1 => tanimoto coeff\n            alpha + beta = 1 => F beta coeff\n            add focal index -> loss=(1-T_index)**(1/gamma)\n    """"""\n\n    def __init__(ctx, alpha=0.5, beta=0.7, gamma=1.0, reduction=\'mean\'):\n        """"""\n        :param alpha: controls the penalty for false positives.\n        :param beta: penalty for false negative.\n        :param gamma : focal coefficient range[1,3]\n        :param reduction: return mode\n        Notes:\n        alpha = beta = 0.5 => dice coeff\n        alpha = beta = 1 => tanimoto coeff\n        alpha + beta = 1 => F beta coeff\n        add focal index -> loss=(1-T_index)**(1/gamma)\n        """"""\n        ctx.alpha = alpha\n        ctx.beta = beta\n        ctx.epsilon = 1e-6\n        ctx.reduction = reduction\n        ctx.gamma = gamma\n        sum = ctx.beta + ctx.alpha\n        if sum != 1:\n            ctx.beta = ctx.beta / sum\n            ctx.alpha = ctx.alpha / sum\n\n    # @staticmethod\n    def forward(ctx, input, target):\n        batch_size = input.size(0)\n        _, input_label = input.max(1)\n\n        input_label = input_label.float()\n        target_label = target.float()\n\n        ctx.save_for_backward(input, target_label)\n\n        input_label = input_label.view(batch_size, -1)\n        target_label = target_label.view(batch_size, -1)\n\n        ctx.P_G = torch.sum(input_label * target_label, 1)  # TP\n        ctx.P_NG = torch.sum(input_label * (1 - target_label), 1)  # FP\n        ctx.NP_G = torch.sum((1 - input_label) * target_label, 1)  # FN\n\n        index = ctx.P_G / (ctx.P_G + ctx.alpha * ctx.P_NG + ctx.beta * ctx.NP_G + ctx.epsilon)\n        loss = torch.pow((1 - index), 1 / ctx.gamma)\n        # target_area = torch.sum(target_label, 1)\n        # loss[target_area == 0] = 0\n        if ctx.reduction == \'none\':\n            loss = loss\n        elif ctx.reduction == \'sum\':\n            loss = torch.sum(loss)\n        else:\n            loss = torch.mean(loss)\n        return loss\n\n    # @staticmethod\n    def backward(ctx, grad_out):\n        """"""\n        :param ctx:\n        :param grad_out:\n        :return:\n        d_loss/dT_loss=(1/gamma)*(T_loss)**(1/gamma-1)\n        (dT_loss/d_P1)  = 2*P_G*[G*(P_G+alpha*P_NG+beta*NP_G)-(G+alpha*NG)]/[(P_G+alpha*P_NG+beta*NP_G)**2]\n                        = 2*P_G\n        (dT_loss/d_p0)=\n        """"""\n        inputs, target = ctx.saved_tensors\n        inputs = inputs.float()\n        target = target.float()\n        batch_size = inputs.size(0)\n        sum = ctx.P_G + ctx.alpha * ctx.P_NG + ctx.beta * ctx.NP_G + ctx.epsilon\n        P_G = ctx.P_G.view(batch_size, 1, 1, 1, 1)\n        if inputs.dim() == 5:\n            sum = sum.view(batch_size, 1, 1, 1, 1)\n        elif inputs.dim() == 4:\n            sum = sum.view(batch_size, 1, 1, 1)\n            P_G = ctx.P_G.view(batch_size, 1, 1, 1)\n        sub = (ctx.alpha * (1 - target) + target) * P_G\n\n        dL_dT = (1 / ctx.gamma) * torch.pow((P_G / sum), (1 / ctx.gamma - 1))\n        dT_dp0 = -2 * (target / sum - sub / sum / sum)\n        dL_dp0 = dL_dT * dT_dp0\n\n        dT_dp1 = ctx.beta * (1 - target) * P_G / sum / sum\n        dL_dp1 = dL_dT * dT_dp1\n        grad_input = torch.cat((dL_dp1, dL_dp0), dim=1)\n        # grad_input = torch.cat((grad_out.item() * dL_dp0, dL_dp0 * grad_out.item()), dim=1)\n        return grad_input, None\n\n\nclass MultiTverskyLoss(nn.Module):\n    """"""\n    Tversky Loss for segmentation adaptive with multi class segmentation\n\n    Args\n        :param alpha (Tensor, float, optional): controls the penalty for false positives.\n        :param beta (Tensor, float, optional): controls the penalty for false negative.\n        :param gamma (Tensor, float, optional): focal coefficient\n        :param weights (Tensor, optional): a manual rescaling weight given to each class. If given, it has to be a Tensor of size `C`\n    """"""\n\n    def __init__(self, alpha=0.5, beta=0.5, gamma=1.0, reduction=\'mean\', weights=None):\n        """"""\n        :param alpha (Tensor, float, optional): controls the penalty for false positives.\n        :param beta (Tensor, float, optional): controls the penalty for false negative.\n        :param gamma (Tensor, float, optional): focal coefficient\n        :param weights (Tensor, optional): a manual rescaling weight given to each\n            class. If given, it has to be a Tensor of size `C`\n        """"""\n        super(MultiTverskyLoss, self).__init__()\n        self.alpha = alpha\n        self.beta = beta\n        self.gamma = gamma\n        self.reduction = reduction\n        self.weights = weights\n\n    def forward(self, inputs, targets):\n        num_class = inputs.size(1)\n        weight_losses = 0.0\n        if self.weights is not None:\n            assert len(self.weights) == num_class, \'number of classes should be equal to length of weights \'\n            weights = self.weights\n        else:\n            weights = [1.0 / num_class] * num_class\n        input_slices = torch.split(inputs, [1] * num_class, dim=1)\n        for idx in range(num_class):\n            input_idx = input_slices[idx]\n            input_idx = torch.cat((1 - input_idx, input_idx), dim=1)\n            target_idx = (targets == idx) * 1\n            loss_func = FocalBinaryTverskyFunc(self.alpha, self.beta, self.gamma, self.reduction)\n            loss_idx = loss_func(input_idx, target_idx)\n            weight_losses+=loss_idx * weights[idx]\n        # loss = torch.Tensor(weight_losses)\n        # loss = loss.to(inputs.device)\n        # loss = torch.sum(loss)\n        return weight_losses\n\n\nclass FocalBinaryTverskyLoss(MultiTverskyLoss):\n    """"""\n            Binary version of Focal Tversky Loss as defined in `this paper <https://arxiv.org/abs/1810.07842>`_\n\n            `Authors\' implementation <https://github.com/nabsabraham/focal-tversky-unet>`_ in Keras.\n\n            Params:\n                :param alpha: controls the penalty for false positives.\n                :param beta: penalty for false negative.\n                :param gamma : focal coefficient range[1,3]\n                :param reduction: return mode\n\n            Notes:\n                alpha = beta = 0.5 => dice coeff\n                alpha = beta = 1 => tanimoto coeff\n                alpha + beta = 1 => F beta coeff\n                add focal index -> loss=(1-T_index)**(1/gamma)\n        """"""\n\n    def __init__(self, alpha=0.5, beta=0.7, gamma=1.0, reduction=\'mean\', **kwargs):\n        """"""\n        :param alpha (Tensor, float, optional): controls the penalty for false positives.\n        :param beta (Tensor, float, optional): controls the penalty for false negative.\n        :param gamma (Tensor, float, optional): focal coefficient\n        """"""\n        super().__init__(alpha, beta, gamma, reduction)\n\n    def forward(self, inputs, targets):\n        return super().forward(inputs, targets.unsqueeze(1))\n\n# ===================== #\n# Source: https://github.com/Hsuxu/Loss_ToolBox-PyTorch/blob/master/LovaszSoftmax/lovasz_loss.py\ndef lovasz_grad(gt_sorted):\n    """"""\n    Computes gradient of the Lovasz extension w.r.t sorted errors\n    See Alg. 1 in paper\n    """"""\n    p = len(gt_sorted)\n    gts = gt_sorted.sum()\n    intersection = gts - gt_sorted.float().cumsum(0)\n    union = gts + (1 - gt_sorted).float().cumsum(0)\n    jaccard = 1. - intersection / union\n    if p > 1:  # cover 1-pixel case\n        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n    return jaccard\n\n\nclass LovaszSoftmax(nn.Module):\n    def __init__(self, reduction=\'mean\', **kwargs):\n        super(LovaszSoftmax, self).__init__()\n        self.reduction = reduction\n\n    def prob_flatten(self, input, target):\n        assert input.dim() in [4, 5]\n        num_class = input.size(1)\n        if input.dim() == 4:\n            input = input.permute(0, 2, 3, 1).contiguous()\n            input_flatten = input.view(-1, num_class)\n        elif input.dim() == 5:\n            input = input.permute(0, 2, 3, 4, 1).contiguous()\n            input_flatten = input.view(-1, num_class)\n        target_flatten = target.view(-1)\n        return input_flatten, target_flatten\n\n    def lovasz_softmax_flat(self, inputs, targets):\n        num_classes = inputs.size(1)\n        losses = []\n        for c in range(num_classes):\n            target_c = (targets == c).float()\n            if num_classes == 1:\n                input_c = inputs[:, 0]\n            else:\n                input_c = inputs[:, c]\n            loss_c = (torch.autograd.Variable(target_c) - input_c).abs()\n            loss_c_sorted, loss_index = torch.sort(loss_c, 0, descending=True)\n            target_c_sorted = target_c[loss_index]\n            losses.append(torch.dot(loss_c_sorted, torch.autograd.Variable(lovasz_grad(target_c_sorted))))\n        losses = torch.stack(losses)\n\n        if self.reduction == \'none\':\n            loss = losses\n        elif self.reduction == \'sum\':\n            loss = losses.sum()\n        else:\n            loss = losses.mean()\n        return loss\n\n    def forward(self, inputs, targets):\n        inputs, targets = self.prob_flatten(inputs, targets)\n        losses = self.lovasz_softmax_flat(inputs, targets)\n        return losses\n\n\n# ===================== #\n# Inspired by: https://github.com/xuuuuuuchen/Active-Contour-Loss/blob/master/Active-Contour-Loss.py (MIT)\n# Unfortunately the implementation above seems wrong, so reimplementing per the gist of the paper\nclass ActiveContourLoss(nn.Module):\n    """"""\n        `Learning Active Contour Models for Medical Image Segmentation <http://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Learning_Active_Contour_Models_for_Medical_Image_Segmentation_CVPR_2019_paper.pdf>`_\n        Note that is only works for B/W masks right now... which is kind of the point of this loss as contours in RGB should be cast to B/W\n        before computing the loss.\n\n        Params:\n            :param len_w: (float, default=1.0) - The multiplier to use when adding boundary loss.\n            :param reg_w: (float, default=1.0) - The multiplier to use when adding region loss.\n            :param apply_log: (bool, default=True) - Whether to transform the log into log space (due to the\n    """"""\n\n    def __init__(self, len_w=1., reg_w=1., apply_log=True, **kwargs):\n        super(ActiveContourLoss, self).__init__()\n        self.len_w = len_w\n        self.reg_w = reg_w\n        self.epsilon = 1e-8  # a parameter to avoid square root = zero issues\n        self.apply_log = apply_log\n\n    def forward(self, logits, target):\n        image_size = logits.size(3)\n        target = target.unsqueeze(1)\n\n        # must convert raw logits to predicted probabilities for each pixel along channel\n        probs = F.softmax(logits, dim=0)\n\n        """"""\n        length term:\n            - Subtract adjacent pixels from each other in X and Y directions\n            - Determine where they differ from the ground truth (targets)\n            - Calculate MSE\n        """"""\n        # horizontal and vertical directions\n        x = probs[:, :, 1:, :] - probs[:, :, :-1, :]      # differences in horizontal direction\n        y = probs[:, :, :, 1:] - probs[:, :, :, :-1]      # differences in vertical direction\n\n        target_x = target[:, :, 1:, :] - target[:, :, :-1, :]\n        target_y = target[:, :, :, 1:] - target[:, :, :, :-1]\n\n        # find difference between values of probs and targets\n        delta_x = (target_x - x).abs()          # do we need to subtract absolute values or relative?\n        delta_y = (target_y - y).abs()\n\n        # get MSE of the differences per pixel\n        # importantly because deltas are mostly < 1, a simple square of the error will actually yield LOWER results\n        # so we select 0.5 as the middle ground where small error will be further minimized while large error will\n        # be highlighted (pushed to be > 1 and up to 2.5 for maximum error).\n        # len_error_sq = ((delta_x + 0.5) ** 2) + ((delta_y + 0.5) ** 2)\n        # length = torch.sqrt(len_error_sq.sum() + self.epsilon)\n\n        # the length loss here is simply the MSE of x and y deltas\n        length_loss = torch.sqrt(delta_x.sum() ** 2 + delta_y.sum() ** 2 + self.epsilon)\n\n\n        """"""\n        region term (should this be done in log space to avoid instabilities?)\n            - compute the error produced by all pixels that are not equal to 0 outside of the ground truth mask\n            - compute error produced by all pixels that are not equal to 1 inside the mask\n        """"""\n        # reference code for selecting masked values from a tensor\n        # t_m_bool = t_mask.type(torch.ByteTensor)\n        # t_result = t_in.masked_select(t_m_bool)\n\n        # C_1 = torch.ones((image_size, image_size), device=target.device)\n        # C_2 = torch.zeros((image_size, image_size), device=target.device)\n\n        # the sum of all pixel values that are not equal 0 outside of the ground truth mask\n        error_in = probs[:, 0, :, :] * ((target[:, 0, :, :] - 1) ** 2)  # invert the ground truth mask and multiply by probs\n\n        # the sum of all pixel values that are not equal 1 inside of the ground truth mask\n        probs_diff = (probs[:, 0, :, :] - target[:, 0, :, :]).abs()     # subtract mask from probs giving us the errors\n        error_out = (probs_diff * target[:, 0, :, :])                   # multiply mask by error, giving us the error terms inside the mask.\n\n        if self.apply_log:\n            loss = torch.log(length_loss) + torch.log(error_in.sum() + error_out.sum())\n        else:\n            # loss = self.len_w * length_loss\n            loss = self.reg_w * (error_in.sum() + error_out.sum())\n\n        return torch.clamp(loss, min=0.0)        # make sure we don\'t return negative values\n\n\n# ===================== #\n# Sources:  https://github.com/JunMa11/SegLoss\n#           https://github.com/MIC-DKFZ/nnUNet/tree/master/nnunet (Apache 2.0)\ndef uniq(a: Tensor) -> Set:\n    return set(torch.unique(a.cpu()).numpy())\n\n\ndef sset(a: Tensor, sub: Iterable) -> bool:\n    return uniq(a).issubset(sub)\n\n\ndef simplex(t: Tensor, axis=1) -> bool:\n    _sum = t.sum(axis).type(torch.float32)\n    _ones = torch.ones_like(_sum, dtype=torch.float32)\n    return torch.allclose(_sum, _ones)\n\n\ndef one_hot(t: Tensor, axis=1) -> bool:\n    return simplex(t, axis) and sset(t, [0, 1])\n\n\ndef numpy_haussdorf(pred: np.ndarray, target: np.ndarray) -> float:\n    from scipy.spatial.distance import directed_hausdorff\n    assert len(pred.shape) == 2\n    assert pred.shape == target.shape\n\n    return max(directed_hausdorff(pred, target)[0], directed_hausdorff(target, pred)[0])\n\n\ndef haussdorf(preds: Tensor, target: Tensor) -> Tensor:\n    assert preds.shape == target.shape\n    assert one_hot(preds)\n    assert one_hot(target)\n\n    B, C, _, _ = preds.shape\n\n    res = torch.zeros((B, C), dtype=torch.float32, device=preds.device)\n    n_pred = preds.cpu().numpy()\n    n_target = target.cpu().numpy()\n\n    for b in range(B):\n        if C == 2:\n            res[b, :] = numpy_haussdorf(n_pred[b, 0], n_target[b, 0])\n            continue\n\n        for c in range(C):\n            res[b, c] = numpy_haussdorf(n_pred[b, c], n_target[b, c])\n\n    return res\n\n\ndef softmax_helper(x):\n    rpt = [1 for _ in range(len(x.size()))]\n    rpt[1] = x.size(1)\n    x_max = x.max(1, keepdim=True)[0].repeat(*rpt)\n    e_x = torch.exp(x - x_max)\n    return e_x / e_x.sum(1, keepdim=True).repeat(*rpt)\n\n\ndef sum_tensor(inp, axes, keepdim=False):\n    axes = np.unique(axes).astype(int)\n    if keepdim:\n        for ax in axes:\n            inp = inp.sum(int(ax), keepdim=True)\n    else:\n        for ax in sorted(axes, reverse=True):\n            inp = inp.sum(int(ax))\n    return inp\n\n\ndef get_tp_fp_fn(net_output, gt, axes=None, mask=None, square=False):\n    """"""\n    net_output must be (b, c, x, y(, z)))\n    gt must be a label map (shape (b, 1, x, y(, z)) OR shape (b, x, y(, z))) or one hot encoding (b, c, x, y(, z))\n    if mask is provided it must have shape (b, 1, x, y(, z)))\n    :param net_output:\n    :param gt:\n    :param axes:\n    :param mask: mask must be 1 for valid pixels and 0 for invalid pixels\n    :param square: if True then fp, tp and fn will be squared before summation\n    :return:\n    """"""\n    if axes is None:\n        axes = tuple(range(2, len(net_output.size())))\n\n    shp_x = net_output.shape\n    shp_y = gt.shape\n\n    with torch.no_grad():\n        if len(shp_x) != len(shp_y):\n            gt = gt.view((shp_y[0], 1, *shp_y[1:]))\n\n        if all([i == j for i, j in zip(net_output.shape, gt.shape)]):\n            # if this is the case then gt is probably already a one hot encoding\n            y_onehot = gt\n        else:\n            gt = gt.long()\n            y_onehot = torch.zeros(shp_x)\n            if net_output.device.type == ""cuda"":\n                y_onehot = y_onehot.cuda(net_output.device.index)\n            y_onehot.scatter_(1, gt, 1)\n\n    tp = net_output * y_onehot\n    fp = net_output * (1 - y_onehot)\n    fn = (1 - net_output) * y_onehot\n\n    if mask is not None:\n        tp = torch.stack(tuple(x_i * mask[:, 0] for x_i in torch.unbind(tp, dim=1)), dim=1)\n        fp = torch.stack(tuple(x_i * mask[:, 0] for x_i in torch.unbind(fp, dim=1)), dim=1)\n        fn = torch.stack(tuple(x_i * mask[:, 0] for x_i in torch.unbind(fn, dim=1)), dim=1)\n\n    if square:\n        tp = tp ** 2\n        fp = fp ** 2\n        fn = fn ** 2\n\n    tp = sum_tensor(tp, axes, keepdim=False)\n    fp = sum_tensor(fp, axes, keepdim=False)\n    fn = sum_tensor(fn, axes, keepdim=False)\n\n    return tp, fp, fn\n\n\nclass BDLoss(nn.Module):\n    def __init__(self, **kwargs):\n        """"""\n        compute boudary loss\n        only compute the loss of foreground\n        ref: https://github.com/LIVIAETS/surface-loss/blob/108bd9892adca476e6cdf424124bc6268707498e/losses.py#L74\n        """"""\n        super(BDLoss, self).__init__()\n        # self.do_bg = do_bg\n\n    def forward(self, logits, target, bound):\n        """"""\n        Takes 2D or 3D logits.\n\n        logits: (batch_size, class, x,y,(z))\n        target: ground truth, shape: (batch_size, 1, x,y,(z))\n        bound: precomputed distance map, shape (batch_size, class, x,y,(z))\n\n        Torch Eigensum description: https://stackoverflow.com/questions/55894693/understanding-pytorch-einsum\n        """"""\n        compute_directive = ""bcxy,bcxy->bcxy""\n        if len(logits) == 5:\n            compute_directive = ""bcxyz,bcxyz->bcxyz""\n\n        net_output = softmax_helper(logits)\n        # print(\'net_output shape: \', net_output.shape)\n        pc = net_output[:, 1:, ...].type(torch.float32)\n        dc = bound[:,1:, ...].type(torch.float32)\n\n        multipled = torch.einsum(compute_directive, pc, dc)\n        bd_loss = multipled.mean()\n\n        return bd_loss\n\n\n# ===================== #\n# Source:  https://github.com/kevinzakka/pytorch-goodies/blob/master/losses.py\nclass TverskyLoss(nn.Module):\n    """"""Computes the Tversky loss [1].\n        Args:\n            :param alpha: controls the penalty for false positives.\n            :param beta: controls the penalty for false negatives.\n            :param eps: added to the denominator for numerical stability.\n        Returns:\n            tversky_loss: the Tversky loss.\n        Notes:\n            alpha = beta = 0.5 => dice coeff\n            alpha = beta = 1 => tanimoto coeff\n            alpha + beta = 1 => F beta coeff\n        References:\n            [1]: https://arxiv.org/abs/1706.05721\n    """"""\n\n    def __init__(self, alpha, beta, eps=1e-7, **kwargs):\n        super(TverskyLoss, self).__init__()\n\n        self.alpha = alpha\n        self.beta = beta\n        self.eps = eps\n\n    def forward(self, logits, targets):\n        """"""\n        Args:\n            :param logits: a tensor of shape [B, C, H, W]. Corresponds to the raw output or logits of the model.\n            :param targets: a tensor of shape [B, H, W] or [B, 1, H, W].\n            :return: loss\n        """"""\n        num_classes = logits.shape[1]\n        if num_classes == 1:\n            true_1_hot = torch.eye(num_classes + 1)[targets.squeeze(1).long()]\n            true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n            true_1_hot_f = true_1_hot[:, 0:1, :, :]\n            true_1_hot_s = true_1_hot[:, 1:2, :, :]\n            true_1_hot = torch.cat([true_1_hot_s, true_1_hot_f], dim=1)\n            pos_prob = torch.sigmoid(logits)\n            neg_prob = 1 - pos_prob\n            probas = torch.cat([pos_prob, neg_prob], dim=1)\n        else:\n            true_1_hot = torch.eye(num_classes)[targets.squeeze(1)]\n            true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n            probas = F.softmax(logits, dim=1)\n        true_1_hot = true_1_hot.type(logits.type())\n        dims = (0,) + tuple(range(2, logits.ndimension()))\n        intersection = torch.sum(probas * true_1_hot, dims)\n        fps = torch.sum(probas * (1 - true_1_hot), dims)\n        fns = torch.sum((1 - probas) * true_1_hot, dims)\n        num = intersection\n        denom = intersection + (self.alpha * fps) + (self.beta * fns)\n        tversky_loss = (num / (denom + self.eps)).mean()\n\n        return 1 - tversky_loss\n\n\n# ===================== #\n# Source:  https://github.com/cvqluu/Angular-Penalty-Softmax-Losses-Pytorch\nclass AngularPenaltySMLoss(nn.Module):\n\n    def __init__(self, in_features, out_features, loss_type=\'arcface\', eps=1e-7, s=None, m=None):\n        \'\'\'\n        Angular Penalty Softmax Loss\n\n        Three \'loss_types\' available: [\'arcface\', \'sphereface\', \'cosface\']\n        These losses are described in the following papers:\n\n        ArcFace: https://arxiv.org/abs/1801.07698\n        SphereFace: https://arxiv.org/abs/1704.08063\n        CosFace/Ad Margin: https://arxiv.org/abs/1801.05599\n\n        - Example -\n        criterion = AngularPenaltySMLoss(in_features, out_features, loss_type=\'arcface\') # loss_type in [\'arcface\', \'sphereface\', \'cosface\']\n        \'\'\'\n        super(AngularPenaltySMLoss, self).__init__()\n        loss_type = loss_type.lower()\n        assert loss_type in [\'arcface\', \'sphereface\', \'cosface\']\n        if loss_type == \'arcface\':\n            self.s = 64.0 if not s else s\n            self.m = 0.5 if not m else m\n        if loss_type == \'sphereface\':\n            self.s = 64.0 if not s else s\n            self.m = 1.35 if not m else m\n        if loss_type == \'cosface\':\n            self.s = 30.0 if not s else s\n            self.m = 0.4 if not m else m\n        self.loss_type = loss_type\n        self.in_features = in_features\n        self.out_features = out_features\n        self.fc = nn.Linear(in_features, out_features, bias=False)\n        self.eps = eps\n\n    def forward(self, x, labels):\n        \'\'\'\n        input shape (N, in_features)\n        \'\'\'\n        assert len(x) == len(labels)\n        assert torch.min(labels) >= 0\n        assert torch.max(labels) < self.out_features\n\n        for W in self.fc.parameters():\n            W = F.normalize(W, p=2, dim=1)\n\n        x = F.normalize(x, p=2, dim=1)\n\n        wf = self.fc(x)\n        if self.loss_type == \'cosface\':\n            numerator = self.s * (torch.diagonal(wf.transpose(0, 1)[labels]) - self.m)\n        if self.loss_type == \'arcface\':\n            numerator = self.s * torch.cos(torch.acos(torch.clamp(torch.diagonal(wf.transpose(0, 1)[labels]), -1. + self.eps, 1 - self.eps)) + self.m)\n        if self.loss_type == \'sphereface\':\n            numerator = self.s * torch.cos(self.m * torch.acos(torch.clamp(torch.diagonal(wf.transpose(0, 1)[labels]), -1. + self.eps, 1 - self.eps)))\n\n        excl = torch.cat([torch.cat((wf[i, :y], wf[i, y + 1:])).unsqueeze(0) for i, y in enumerate(labels)], dim=0)\n        denominator = torch.exp(numerator) + torch.sum(torch.exp(self.s * excl), dim=1)\n        L = numerator - torch.log(denominator)\n        return -torch.mean(L)\n'"
pywick/lovasz_losses.py,8,"b'# Source: https://github.com/bermanmaxim/LovaszSoftmax (License: MIT)\n\n""""""\nLovasz-Softmax and Jaccard hinge loss in PyTorch\nMaxim Berman 2018 ESAT-PSI KU Leuven (MIT License)\n""""""\n\nfrom __future__ import print_function, division\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport numpy as np\n\ntry:\n    from itertools import ifilterfalse\nexcept ImportError:  # py3k\n    from itertools import filterfalse as ifilterfalse\n\n\ndef lovasz_grad(gt_sorted):\n    """"""\n    Computes gradient of the Lovasz extension w.r.t sorted errors\n    See Alg. 1 in paper\n    """"""\n    p = len(gt_sorted)\n    gts = gt_sorted.sum()\n    intersection = gts - gt_sorted.float().cumsum(0)\n    union = gts + (1 - gt_sorted).float().cumsum(0)\n    jaccard = 1. - intersection / union\n    if p > 1:  # cover 1-pixel case\n        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n    return jaccard\n\n\ndef iou_binary(preds, labels, EMPTY=1., ignore=None, per_image=True):\n    """"""\n    IoU for foreground class\n    binary: 1 foreground, 0 background\n    """"""\n    if not per_image:\n        preds, labels = (preds,), (labels,)\n    ious = []\n    for pred, label in zip(preds, labels):\n        intersection = ((label == 1) & (pred == 1)).sum()\n        union = ((label == 1) | ((pred == 1) & (label != ignore))).sum()\n        if not union:\n            iou = EMPTY\n        else:\n            iou = float(intersection) / union\n        ious.append(iou)\n    iou = mean(ious)  # mean accross images if per_image\n    return 100 * iou\n\n\ndef iou(preds, labels, C, EMPTY=1., ignore=None, per_image=False):\n    """"""\n    Array of IoU for each (non ignored) class\n    """"""\n    if not per_image:\n        preds, labels = (preds,), (labels,)\n    ious = []\n    for pred, label in zip(preds, labels):\n        iou = []\n        for i in range(C):\n            if i != ignore:  # The ignored label is sometimes among predicted classes (ENet - CityScapes)\n                intersection = ((label == i) & (pred == i)).sum()\n                union = ((label == i) | ((pred == i) & (label != ignore))).sum()\n                if not union:\n                    iou.append(EMPTY)\n                else:\n                    iou.append(float(intersection) / union)\n        ious.append(iou)\n    ious = map(mean, zip(*ious))  # mean accross images if per_image\n    return 100 * np.array(ious)\n\n\n# --------------------------- BINARY LOSSES ---------------------------\n\n\ndef lovasz_hinge(logits, labels, per_image=True, ignore=None):\n    """"""\n    Binary Lovasz hinge loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      per_image: compute the loss per image instead of per batch\n      ignore: void class id\n    """"""\n    if per_image:\n        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n                    for log, lab in zip(logits, labels))\n    else:\n        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n    return loss\n\n\ndef lovasz_hinge_flat(logits, labels):\n    """"""\n    Binary Lovasz hinge loss\n      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n      labels: [P] Tensor, binary ground truth labels (0 or 1)\n      ignore: label to ignore\n    """"""\n    if len(labels) == 0:\n        # only void pixels, the gradients should be 0\n        return logits.sum() * 0.\n    signs = 2. * labels.float() - 1.\n    errors = (1. - logits * Variable(signs))\n    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n    perm = perm.data\n    gt_sorted = labels[perm]\n    grad = lovasz_grad(gt_sorted)\n    loss = torch.dot(F.relu(errors_sorted), Variable(grad))\n    return loss\n\n\ndef flatten_binary_scores(scores, labels, ignore=None):\n    """"""\n    Flattens predictions in the batch (binary case)\n    Remove labels equal to \'ignore\'\n    """"""\n    scores = scores.view(-1)\n    labels = labels.view(-1)\n    if ignore is None:\n        return scores, labels\n    valid = (labels != ignore)\n    vscores = scores[valid]\n    vlabels = labels[valid]\n    return vscores, vlabels\n\n\nclass LovaszBinaryLoss(torch.nn.modules.Module):\n    def __init__(self):\n        super(LovaszBinaryLoss, self).__init__()\n\n    def forward(self, input, target):\n        return lovasz_hinge(input, target)\n\n\nclass StableBCELoss(torch.nn.modules.Module):\n    def __init__(self):\n        super(StableBCELoss, self).__init__()\n\n    def forward(self, input, target):\n        neg_abs = - input.abs()\n        loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\n        return loss.mean()\n\n\ndef binary_xloss(logits, labels, ignore=None):\n    """"""\n    Binary Cross entropy loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      ignore: void class id\n    """"""\n    logits, labels = flatten_binary_scores(logits, labels, ignore)\n    loss = StableBCELoss()(logits, Variable(labels.float()))\n    return loss\n\n\n# --------------------------- MULTICLASS LOSSES ---------------------------\n\n\ndef lovasz_softmax(probas, labels, only_present=False, per_image=False, ignore=None):\n    """"""\n    Multi-class Lovasz-Softmax loss\n      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1)\n      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n      only_present: average only on classes present in ground truth\n      per_image: compute the loss per image instead of per batch\n      ignore: void class labels\n    """"""\n    if per_image:\n        loss = mean(lovasz_softmax_flat(*flatten_probas(prob.unsqueeze(0), lab.unsqueeze(0), ignore), only_present=only_present)\n                    for prob, lab in zip(probas, labels))\n    else:\n        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore), only_present=only_present)\n    return loss\n\n\ndef lovasz_softmax_flat(probas, labels, only_present=False):\n    """"""\n    Multi-class Lovasz-Softmax loss\n      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n      only_present: average only on classes present in ground truth\n    """"""\n    if probas.numel() == 0:\n        # only void pixels, the gradients should be 0\n        return probas * 0.\n    C = probas.size(1)\n\n    C = probas.size(1)\n    losses = []\n    for c in range(C):\n        fg = (labels == c).float()  # foreground for class c\n        if only_present and fg.sum() == 0:\n            continue\n        errors = (Variable(fg) - probas[:, c]).abs()\n        errors_sorted, perm = torch.sort(errors, 0, descending=True)\n        perm = perm.data\n        fg_sorted = fg[perm]\n        losses.append(torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted))))\n    return mean(losses)\n\n\ndef flatten_probas(probas, labels, ignore=None):\n    """"""\n    Flattens predictions in the batch\n    """"""\n    B, C, H, W = probas.size()\n    probas = probas.permute(0, 2, 3, 1).contiguous().view(-1, C)  # B * H * W, C = P, C\n    labels = labels.view(-1)\n    if ignore is None:\n        return probas, labels\n    valid = (labels != ignore)\n    vprobas = probas[valid.nonzero().squeeze()]\n    vlabels = labels[valid]\n    return vprobas, vlabels\n\n\ndef xloss(logits, labels, ignore=None):\n    """"""\n    Cross entropy loss\n    """"""\n    return F.cross_entropy(logits, Variable(labels), ignore_index=255)\n\n\n# --------------------------- HELPER FUNCTIONS ---------------------------\ndef isnan(x):\n    return x != x\n\n\ndef mean(l, ignore_nan=True, empty=0):\n    """"""\n    nanmean compatible with generators.\n    """"""\n    l = iter(l)\n    if ignore_nan:\n        l = ifilterfalse(isnan, l)\n    try:\n        n = 1\n        acc = next(l)\n    except StopIteration:\n        if empty == \'raise\':\n            raise ValueError(\'Empty mean\')\n        return empty\n    for n, v in enumerate(l, 2):\n        acc += v\n    if n == 1:\n        return acc\n    return acc / n\n'"
pywick/metrics.py,8,"b'import torch\nfrom .utils import th_matrixcorr\nfrom .meters.averagemeter import AverageMeter\nfrom .callbacks import Callback\nfrom .losses import lovaszloss, hingeloss, dice_coeff\n\ndef is_iterable(x):\n    return isinstance(x, (tuple, list))\n\nclass MetricContainer(object):\n    def __init__(self, metrics, prefix=\'\'):\n        self.metrics = metrics\n        self.helper = None\n        self.prefix = prefix\n\n    def set_helper(self, helper):\n        self.helper = helper\n\n    def reset(self):\n        for metric in self.metrics:\n            metric.reset()\n\n    def __call__(self, input_batch, output_batch, target_batch, is_val=False):\n        logs = {}\n        for metric in self.metrics:\n            # logs[self.prefix+metric._name] = self.helper.calculate_loss(output_batch, target_batch, metric)\n            metric_out = metric(input_batch, output_batch, target_batch, is_val)\n            if metric_out is not None:\n                logs[self.prefix + metric._name] = metric_out\n        return logs\n\nclass Metric(object):\n\n    def __call__(self, inputs, y_pred, y_true, is_val):\n        \'\'\'\n\n        :param y_pred: Predictions from doing the forward pass\n        :param y_true: Ground Truth\n        :param is_val: Whether this is a validation pass (otherwise assumed training pass)\n\n        :return:\n        \'\'\'\n        raise NotImplementedError(\'Custom Metrics must implement this function\')\n\n    def reset(self):\n        raise NotImplementedError(\'Custom Metrics must implement this function\')\n\n\nclass MetricCallback(Callback):\n\n    def __init__(self, container):\n        self.container = container\n    def on_epoch_begin(self, epoch_idx, logs):\n        self.container.reset()\n\nclass CategoricalAccuracy(Metric):\n\n    def __init__(self, top_k=1):\n        self.top_k = top_k\n        self.correct_count = 0\n        self.total_count = 0\n\n        self._name = \'top_\'+str(top_k)+\':acc_metric\'\n\n    def reset(self):\n        self.correct_count = 0\n        self.total_count = 0\n\n    def __call__(self, inputs, y_pred, y_true, is_val=False):\n        top_k = y_pred.topk(self.top_k,1)[1]\n        true_k = y_true.view(len(y_true),1).expand_as(top_k)\n        self.correct_count += top_k.eq(true_k).float().sum().item()\n        self.total_count += len(y_pred)\n        accuracy = 100. * float(self.correct_count) / float(self.total_count)\n        return accuracy\n\n\nclass CategoricalAccuracySingleInput(CategoricalAccuracy):\n    \'\'\'\n    This class is a tiny modification of CategoricalAccuracy to handle the issue when we desire a single output but\n    the network outputs multiple y_pred (e.g. inception)\n    \'\'\'\n    def __init__(self, top_k=1):\n        super().__init__(top_k)\n\n    def __call__(self, inputs, y_pred, y_true, is_val=False):\n        if is_iterable(y_pred):\n            return super().__call__(inputs, y_pred[0], y_true, is_val=False)\n        else:\n            return super().__call__(inputs, y_pred, y_true, is_val=False)\n\n\nclass BinaryAccuracy(Metric):\n\n    def __init__(self):\n        self.correct_count = 0\n        self.total_count = 0\n\n        self._name = \'acc_metric\'\n\n    def reset(self):\n        self.correct_count = 0\n        self.total_count = 0\n\n    def __call__(self, inputs, y_pred, y_true, is_val):\n        y_pred_round = y_pred.round().long()\n        self.correct_count += y_pred_round.eq(y_true).float().sum().item()\n        self.total_count += len(y_pred)\n        accuracy = 100. * float(self.correct_count) / float(self.total_count)\n        return accuracy\n\n\nclass ProjectionCorrelation(Metric):\n\n    def __init__(self):\n        self.corr_sum = 0.\n        self.total_count = 0.\n\n        self._name = \'corr_metric\'\n\n    def reset(self):\n        self.corr_sum = 0.\n        self.total_count = 0.\n\n    def __call__(self, inputs, y_pred, y_true=None, is_val=False):\n        """"""\n        y_pred should be two projections\n        """"""\n        # covar_mat = torch.abs(th_matrixcorr(y_pred[0].data, y_pred[1].data))       # changed after pytorch 0.4\n        covar_mat = torch.abs(th_matrixcorr(y_pred[0].detach(), y_pred[1].detach()))\n        self.corr_sum += torch.trace(covar_mat)\n        self.total_count += covar_mat.size(0)\n        return self.corr_sum / self.total_count\n\n\nclass ProjectionAntiCorrelation(Metric):\n\n    def __init__(self):\n        self.anticorr_sum = 0.\n        self.total_count = 0.\n\n        self._name = \'anticorr_metric\'\n\n    def reset(self):\n        self.anticorr_sum = 0.\n        self.total_count = 0.\n\n    def __call__(self, inputs, y_pred, y_true=None, is_val=False):\n        """"""\n        y_pred should be two projections\n        """"""\n        # covar_mat = torch.abs(th_matrixcorr(y_pred[0].data, y_pred[1].data))\n        covar_mat = torch.abs(th_matrixcorr(y_pred[0].detach(), y_pred[1].detach()))       # changed after pytorch 0.4\n        upper_sum = torch.sum(torch.triu(covar_mat,1))\n        lower_sum = torch.sum(torch.tril(covar_mat,-1))\n        self.anticorr_sum += upper_sum\n        self.anticorr_sum += lower_sum\n        self.total_count += covar_mat.size(0)*(covar_mat.size(1) - 1)\n        return self.anticorr_sum / self.total_count\n\n\nclass DiceCoefficientMetric(Metric):\n    \'\'\'\n    Calculates the Dice Coefficient (typically used for image segmentation)\n    \'\'\'\n    def __init__(self, is_binary=True, run_on_val_only=False):\n        \'\'\'\n        :param is_binary: (default: True) Whether this is binary segmentation.\n        :param run_on_val_only: (default: False) Whether we only want this to execute during evaluation loop\n        \'\'\'\n        self._name = \'dice_coeff\'\n        self.run_on_val_only = run_on_val_only\n        self.dices = AverageMeter()\n        self.is_binary = is_binary\n\n    def reset(self):\n        self.dices.reset()\n\n    def __call__(self, inputs, y_pred, y_true, is_val):\n        N = y_pred.size(0) * y_pred.size(2) * y_pred.size(3)\n        if not self.run_on_val_only or (is_val and self.run_on_val_only):\n            if self.is_binary:      # need to transpose into 0-1 range\n                y_pred = torch.sigmoid(y_pred)\n            # self.dices.update(dice_coeff(y_pred, y_true).data[0], N)\n            self.dices.update(dice_coeff(y_pred, y_true).item(), N)     # changed after pytorch 0.4\n            return self.dices.avg\n        else:\n            return -1337.0\n\n\nclass JaccardLossMetric(Metric):\n    \'\'\'\n    Calculates the Jaccard Loss (typically used for image segmentation)\n    \'\'\'\n    def __init__(self, run_on_val_only=False):\n        \'\'\'\n        :param is_binary: (default: True) Whether this is binary segmentation.\n        :param run_on_val_only: (default: False) Whether we only want this to execute during evaluation loop\n        \'\'\'\n        self._name = \'jacc_loss\'\n        self.run_on_val_only = run_on_val_only\n        self.jaccard = AverageMeter()\n\n    def reset(self):\n        self.jaccard.reset()\n\n    def __call__(self, inputs, y_pred, y_true, is_val):\n        N = y_pred.size(0) * y_pred.size(2) * y_pred.size(3)\n        if not self.run_on_val_only or (is_val and self.run_on_val_only):\n            # self.jaccard.update(lovaszloss(y_pred, y_true.data).data[0], N)     # changed after pytorch 0.4\n            self.jaccard.update(lovaszloss(y_pred, y_true).item(), N)\n            return self.jaccard.avg\n        else:\n            return -1337.0\n\nclass HingeLossMetric(Metric):\n    \'\'\'\n    Calculates the Hinge Loss (typically used for image segmentation)\n    \'\'\'\n    def __init__(self, run_on_val_only=False):\n        \'\'\'\n        :param is_binary: (default: True) Whether this is binary segmentation.\n        :param run_on_val_only: (default: False) Whether we only want this to execute during evaluation loop\n        \'\'\'\n        self._name = \'hinge_loss\'\n        self.run_on_val_only = run_on_val_only\n        self.hinge = AverageMeter()\n\n    def reset(self):\n        self.hinge.reset()\n\n    def __call__(self, inputs, y_pred, y_true, is_val):\n        N = y_pred.size(0) * y_pred.size(2) * y_pred.size(3)\n        if not self.run_on_val_only or (is_val and self.run_on_val_only):\n            # self.hinge.update(hingeloss(y_pred, y_true.data).data[0], N)    # changed after pytorch 0.4\n            self.hinge.update(hingeloss(y_pred, y_true), N)\n            return self.hinge.avg\n        else:\n            return -1337.0\n'"
pywick/misc.py,2,"b'from enum import Enum, auto\nimport errno\nimport random\nimport os\nimport numpy as np\nimport torch\n\nclass ExecType(Enum):\n    TRAIN = auto()\n    VAL = auto()\n\n# from https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py\nIMG_EXTENSIONS = [\n    \'.jpg\', \'.JPG\', \'.jpeg\', \'.JPEG\',\n    \'.png\', \'.PNG\', \'.ppm\', \'.PPM\', \'.bmp\', \'.BMP\',\n]\n\ndef trun_n_d(n,d):\n    \'\'\'\n    Truncate float (n) to (d) decimal places\n    :param n: float to truncate\n    :param d: how many decimal places to truncate to\n    :return:\n    \'\'\'\n    return int(n*10**d)/10**d\n\ndef is_iterable(x):\n    return isinstance(x, (tuple, list))\ndef is_tuple_or_list(x):\n    return isinstance(x, (tuple, list))\n\n\ndef time_left_str(seconds):\n    \'\'\'\n    Produces a human-readable string in hh:mm:ss format\n    :param seconds:\n\n    :return:\n    \'\'\'\n    # seconds = 370000.0\n    m, s = divmod(int(seconds), 60)\n    h, m = divmod(m, 60)\n    d, h = divmod(h, 24)\n    if d > 0:\n        thetime = ""Projected time remaining  |  {:d}d:{:d}h:{:02d}m"".format(d, h, m)\n    elif h > 0:\n        thetime = ""Projected time remaining:  |  {:d}h:{:02d}m"".format(h, m)\n    elif m > 0:\n        thetime = ""Projected time remaining:  |  {:02d}m:{:02d}s"".format(m, s)\n    else:\n        thetime = ""Projected time remaining:  |  {:02d}s"".format(seconds)\n    return thetime\n\n# Source: https://github.com/keras-team/keras/blob/master/keras/utils/np_utils.py#L9-L37\ndef initialize_random(seed, init_cuda=True):\n    \'\'\'\n    Initializes random seed for all aspects of training: python, numpy, torch, cuda\n\n    :param seed:\n    :param init_cuda: whether to init cuda seed\n    :return:\n    \'\'\'\n    ## Initialize random seed for repeatability ##\n    random.seed(seed)\n    os.environ[\'PYTHONHASHSEED\'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if init_cuda:\n        torch.cuda.manual_seed_all(seed)\n    ## END ##\n\n\ndef check_mkdir(dir_name):\n    """"""\n    Delegates to mkdir_p and is kept around for legacy support\n    :param dir_name:\n    :return:\n    """"""\n    mkdir_p(dir_name)\n\n\ndef mkdir_p(path):\n    """"""Equivalent of a \'mkdir -p\' linux command which creates directories if they don\'t exist. This also correctly resolves paths with ~ in them.""""""\n    path1 = os.path.expanduser(path)\n    try:\n        os.makedirs(path1)\n    except OSError as exc:  # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path1):\n            pass\n        else:\n            raise\n\n\ndef to_categorical(y, num_classes=None, dtype=\'float32\'):\n    """"""Converts a class vector (integers) to binary class matrix.\n    E.g. for use with categorical_crossentropy.\n    # Arguments\n        y: class vector to be converted into a matrix\n            (integers from 0 to num_classes).\n        num_classes: total number of classes.\n        dtype: The data type expected by the input, as a string\n            (`float32`, `float64`, `int32`...)\n    # Returns\n        A binary matrix representation of the input. The classes axis\n        is placed last.\n    """"""\n    y = np.array(y, dtype=\'int\')\n    input_shape = y.shape\n    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n        input_shape = tuple(input_shape[:-1])\n    y = y.ravel()\n    if not num_classes:\n        num_classes = np.max(y) + 1\n    n = y.shape[0]\n    categorical = np.zeros((n, num_classes), dtype=dtype)\n    categorical[np.arange(n), y] = 1\n    output_shape = input_shape + (num_classes,)\n    categorical = np.reshape(categorical, output_shape)\n    return categorical'"
pywick/regularizers.py,0,"b'\nimport torch as th\nfrom fnmatch import fnmatch\n\nfrom .callbacks import Callback\n\nclass RegularizerContainer(object):\n\n    def __init__(self, regularizers):\n        self.regularizers = regularizers\n        self._forward_hooks = []\n\n    def register_forward_hooks(self, model):\n        for regularizer in self.regularizers:\n            for module_name, module in model.named_modules():\n                if fnmatch(module_name, regularizer.module_filter) and hasattr(module, \'weight\'):\n                    hook = module.register_forward_hook(regularizer)\n                    self._forward_hooks.append(hook)\n\n        if len(self._forward_hooks) == 0:\n            raise Exception(\'Tried to register regularizers but no modules \'\n                \'were found that matched any module_filter argument.\')\n\n    def unregister_forward_hooks(self):\n        for hook in self._forward_hooks:\n            hook.remove()\n\n    def reset(self):\n        for r in self.regularizers:\n            r.reset()\n\n    def get_value(self):\n        value = sum([r.value for r in self.regularizers])\n        self.current_value = value.data[0]\n        return value\n\n    def __len__(self):\n        return len(self.regularizers)\n\n\nclass RegularizerCallback(Callback):\n\n    def __init__(self, container):\n        self.container = container\n\n    def on_batch_end(self, batch, logs=None):\n        self.container.reset()\n\n\nclass Regularizer(object):\n\n    def reset(self):\n        raise NotImplementedError(\'subclass must implement this method\')\n\n    def __call__(self, module, input=None, output=None):\n        raise NotImplementedError(\'subclass must implement this method\')\n\n\nclass L1Regularizer(Regularizer):\n\n    def __init__(self, scale=1e-3, module_filter=\'*\'):\n        self.scale = float(scale)\n        self.module_filter = module_filter\n        self.value = 0.\n\n    def reset(self):\n        self.value = 0.\n\n    def __call__(self, module, input=None, output=None):\n        value = th.sum(th.abs(module.weight)) * self.scale\n        self.value += value\n\n\nclass L2Regularizer(Regularizer):\n\n    def __init__(self, scale=1e-3, module_filter=\'*\'):\n        self.scale = float(scale)\n        self.module_filter = module_filter\n        self.value = 0.\n\n    def reset(self):\n        self.value = 0.\n\n    def __call__(self, module, input=None, output=None):\n        value = th.sum(th.pow(module.weight,2)) * self.scale\n        self.value += value\n\n\nclass L1L2Regularizer(Regularizer):\n\n    def __init__(self, l1_scale=1e-3, l2_scale=1e-3, module_filter=\'*\'):\n        self.l1 = L1Regularizer(l1_scale)\n        self.l2 = L2Regularizer(l2_scale)\n        self.module_filter = module_filter\n        self.value = 0.\n\n    def reset(self):\n        self.value = 0.\n\n    def __call__(self, module, input=None, output=None):\n        self.l1(module, input, output)\n        self.l2(module, input, output)\n        self.value += (self.l1.value + self.l2.value)\n\n\n# ------------------------------------------------------------------\n# ------------------------------------------------------------------\n# ------------------------------------------------------------------\n\nclass UnitNormRegularizer(Regularizer):\n    """"""\n    UnitNorm constraint on Weights\n\n    Constraints the weights to have column-wise unit norm\n    """"""\n    def __init__(self,\n                 scale=1e-3,\n                 module_filter=\'*\'):\n\n        self.scale = scale\n        self.module_filter = module_filter\n        self.value = 0.\n\n    def reset(self):\n        self.value = 0.\n\n    def __call__(self, module, input=None, output=None):\n        w = module.weight\n        norm_diff = th.norm(w, 2, 1).sub(1.)\n        value = self.scale * th.sum(norm_diff.gt(0).float().mul(norm_diff))\n        self.value += value\n\n\nclass MaxNormRegularizer(Regularizer):\n    """"""\n    MaxNorm regularizer on Weights\n\n    Constraints the weights to have column-wise unit norm\n    """"""\n    def __init__(self,\n                 scale=1e-3,\n                 module_filter=\'*\'):\n\n        self.scale = scale\n        self.module_filter = module_filter\n        self.value = 0.\n\n    def reset(self):\n        self.value = 0.\n\n    def __call__(self, module, input=None, output=None):\n        w = module.weight\n        norm_diff = th.norm(w,2,self.axis).sub(self.value)\n        value = self.scale * th.sum(norm_diff.gt(0).float().mul(norm_diff))\n        self.value += value\n\n\nclass NonNegRegularizer(Regularizer):\n    """"""\n    Non-Negativity regularizer on Weights\n\n    Constraints the weights to have column-wise unit norm\n    """"""\n    def __init__(self,\n                 scale=1e-3,\n                 module_filter=\'*\'):\n\n        self.scale = scale\n        self.module_filter = module_filter\n        self.value = 0.\n\n    def reset(self):\n        self.value = 0.\n\n    def __call__(self, module, input=None, output=None):\n        w = module.weight\n        value = -1 * self.scale * th.sum(w.gt(0).float().mul(w))\n        self.value += value\n\n'"
pywick/samplers.py,7,"b'""""""\nSamplers are used during the training phase and are especially useful when your training data is not uniformly distributed among\nall of your classes.\n""""""\n\nimport math\nfrom .utils import th_random_choice\nimport torch.utils.data\nimport torchvision\n\nfrom .datasets import FolderDataset, MultiFolderDataset, PredictFolderDataset, ClonedFolderDataset\nfrom torch.utils.data.sampler import Sampler\n\n\nclass StratifiedSampler(Sampler):\n    """"""Stratified Sampling\n\n    Provides equal representation of target classes in each batch\n\n    :param class_vector: (torch tensor):\n            a vector of class labels\n    :param batch_size: (int):\n            size of the batch\n    """"""\n    def __init__(self, class_vector, batch_size):\n        self.n_splits = int(class_vector.size(0) / batch_size)\n        self.class_vector = class_vector\n\n    def gen_sample_array(self):\n        try:\n            from sklearn.model_selection import StratifiedShuffleSplit\n        except:\n            print(\'Need scikit-learn for this functionality\')\n        import numpy as np\n        \n        s = StratifiedShuffleSplit(n_splits=self.n_splits, test_size=0.5)\n        X = torch.randn(self.class_vector.size(0),2).numpy()\n        y = self.class_vector.numpy()\n        s.get_n_splits(X, y)\n\n        train_index, test_index = next(s.split(X, y))\n        return np.hstack([train_index, test_index])\n\n    def __iter__(self):\n        return iter(self.gen_sample_array())\n\n    def __len__(self):\n        return len(self.class_vector)\n\n\nclass MultiSampler(Sampler):\n    """"""Samples elements more than once in a single pass through the data.\n\n    This allows the number of samples per epoch to be larger than the number\n    of samples itself, which can be useful when training on 2D slices taken\n    from 3D images, for instance.\n    """"""\n    def __init__(self, nb_samples, desired_samples, shuffle=False):\n        """"""Initialize MultiSampler\n\n        :param data_source: (dataset):\n            the dataset to sample from\n        :param desired_samples: (int):\n            number of samples per batch you want.\n            Whatever the difference is between an even division will\n            be randomly selected from the samples.\n            e.g. if len(data_source) = 3 and desired_samples = 4, then\n            all 3 samples will be included and the last sample will be\n            randomly chosen from the 3 original samples.\n        :param shuffle: (bool):\n            whether to shuffle the indices or not\n        \n        Example:\n            >>> m = MultiSampler(2, 6)\n            >>> x = m.gen_sample_array()\n            >>> print(x) # [0,1,0,1,0,1]\n        """"""\n        self.data_samples = nb_samples\n        self.desired_samples = desired_samples\n        self.shuffle = shuffle\n\n    def gen_sample_array(self):\n        n_repeats = self.desired_samples / self.data_samples\n        cat_list = []\n        for i in range(math.floor(n_repeats)):\n            cat_list.append(torch.arange(0,self.data_samples))\n        # add the left over samples\n        left_over = self.desired_samples % self.data_samples\n        if left_over > 0:\n            cat_list.append(th_random_choice(self.data_samples, left_over))\n        self.sample_idx_array = torch.cat(cat_list).long()\n        return self.sample_idx_array\n\n    def __iter__(self):\n        return iter(self.gen_sample_array())\n\n    def __len__(self):\n        return self.desired_samples\n\n\n# Source: https://github.com/ufoym/imbalanced-dataset-sampler (MIT license)\nclass ImbalancedDatasetSampler(Sampler):\n    """"""Samples elements randomly from a given list of indices for imbalanced dataset\n\n    :param indices: (list, optional): a list of indices\n    :param num_samples: (int, optional): number of samples to draw\n    """"""\n\n    def __init__(self, dataset, indices=None, num_samples=None):\n\n        # if indices is not provided, all elements in the dataset will be considered\n        self.indices = list(range(len(dataset))) \\\n            if indices is None else indices\n\n        # if num_samples is not provided, draw `len(indices)` samples in each iteration\n        self.num_samples = len(self.indices) \\\n            if num_samples is None else num_samples\n\n        # distribution of classes in the dataset\n        label_to_count = {}\n        for idx in self.indices:\n            label = self._get_label(dataset, idx)\n            if label in label_to_count:\n                label_to_count[label] += 1\n            else:\n                label_to_count[label] = 1\n\n        # weight for each sample\n        weights = [1.0 / label_to_count[self._get_label(dataset, idx)] for idx in self.indices]\n        self.weights = torch.DoubleTensor(weights)\n\n    def _get_label(self, dataset, idx):\n        dataset_type = type(dataset)\n        if dataset_type is torchvision.datasets.MNIST:\n            return dataset.train_labels[idx].item()\n        elif dataset_type is torchvision.datasets.ImageFolder:\n            return dataset.imgs[idx][1]\n        elif dataset_type is FolderDataset or dataset_type is MultiFolderDataset or dataset_type is PredictFolderDataset or dataset_type is ClonedFolderDataset:\n            return dataset.getdata()[idx][1]\n        else:\n            raise NotImplementedError\n\n    def __iter__(self):\n        return (self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True))\n\n    def __len__(self):\n        return self.num_samples\n'"
pywick/utils.py,0,"b'""""""\nUtility functions for th.Tensors\n""""""\n\nimport pickle\nimport random\nimport numpy as np\n\nimport torch as th\n\n\ndef th_allclose(x, y):\n    """"""\n    Determine whether two torch tensors have same values\n    Mimics np.allclose\n    """"""\n    return th.sum(th.abs(x-y)) < 1e-5\n\n\ndef th_flatten(x):\n    """"""Flatten tensor""""""\n    return x.contiguous().view(-1)\n\ndef th_c_flatten(x):\n    """"""\n    Flatten tensor, leaving channel intact.\n    Assumes CHW format.\n    """"""\n    return x.contiguous().view(x.size(0), -1)\n\ndef th_bc_flatten(x):\n    """"""\n    Flatten tensor, leaving batch and channel dims intact.\n    Assumes BCHW format\n    """"""\n    return x.contiguous().view(x.size(0), x.size(1), -1)\n\n\ndef th_zeros_like(x):\n    return x.new().resize_as_(x).zero_()\n\ndef th_ones_like(x):\n    return x.new().resize_as_(x).fill_(1)\n\ndef th_constant_like(x, val):\n    return x.new().resize_as_(x).fill_(val)\n\n\ndef th_iterproduct(*args):\n    return th.from_numpy(np.indices(args).reshape((len(args),-1)).T)\n\ndef th_iterproduct_like(x):\n    return th_iterproduct(*x.size())\n\n\ndef th_uniform(lower, upper):\n    return random.uniform(lower, upper)\n\n\ndef th_gather_nd(x, coords):\n    x = x.contiguous()\n    inds = coords.mv(th.LongTensor(x.stride()))\n    x_gather = th.index_select(th_flatten(x), 0, inds)\n    return x_gather\n\n\ndef th_affine2d(x, matrix, mode=\'bilinear\', center=True):\n    """"""\n    2D Affine image transform on th.Tensor\n    \n    Arguments\n    ---------\n    x : th.Tensor of size (C, H, W)\n        image tensor to be transformed\n\n    matrix : th.Tensor of size (3, 3) or (2, 3)\n        transformation matrix\n\n    mode : string in {\'nearest\', \'bilinear\'}\n        interpolation scheme to use\n\n    center : boolean\n        whether to alter the bias of the transform \n        so the transform is applied about the center\n        of the image rather than the origin\n\n    Example\n    ------- \n    >>> import torch\n    >>> from pywick.utils import *\n    >>> x = th.zeros(2,1000,1000)\n    >>> x[:,100:1500,100:500] = 10\n    >>> matrix = th.FloatTensor([[1.,0,-50],\n    ...                             [0,1.,-50]])\n    >>> xn = th_affine2d(x, matrix, mode=\'nearest\')\n    >>> xb = th_affine2d(x, matrix, mode=\'bilinear\')\n    """"""\n\n    if matrix.dim() == 2:\n        matrix = matrix[:2,:]\n        matrix = matrix.unsqueeze(0)\n    elif matrix.dim() == 3:\n        if matrix.size()[1:] == (3,3):\n            matrix = matrix[:,:2,:]\n\n    A_batch = matrix[:,:,:2]\n    if A_batch.size(0) != x.size(0):\n        A_batch = A_batch.repeat(x.size(0),1,1)\n    b_batch = matrix[:,:,2].unsqueeze(1)\n\n    # make a meshgrid of normal coordinates\n    _coords = th_iterproduct(x.size(1),x.size(2))\n    coords = _coords.unsqueeze(0).repeat(x.size(0),1,1).float()\n\n    if center:\n        # shift the coordinates so center is the origin\n        coords[:,:,0] = coords[:,:,0] - (x.size(1) / 2. - 0.5)\n        coords[:,:,1] = coords[:,:,1] - (x.size(2) / 2. - 0.5)\n    # apply the coordinate transformation\n    new_coords = coords.bmm(A_batch.transpose(1,2)) + b_batch.expand_as(coords)\n\n    if center:\n        # shift the coordinates back so origin is origin\n        new_coords[:,:,0] = new_coords[:,:,0] + (x.size(1) / 2. - 0.5)\n        new_coords[:,:,1] = new_coords[:,:,1] + (x.size(2) / 2. - 0.5)\n\n    # map new coordinates using bilinear interpolation\n    if mode == \'nearest\':\n        x_transformed = th_nearest_interp2d(x.contiguous(), new_coords)\n    elif mode == \'bilinear\':\n        x_transformed = th_bilinear_interp2d(x.contiguous(), new_coords)\n\n    return x_transformed\n\n\ndef th_nearest_interp2d(input, coords):\n    """"""\n    2d nearest neighbor interpolation th.Tensor\n    """"""\n    # take clamp of coords so they\'re in the image bounds\n    x = th.clamp(coords[:,:,0], 0, input.size(1)-1).round()\n    y = th.clamp(coords[:,:,1], 0, input.size(2)-1).round()\n\n    stride = th.LongTensor(input.stride())\n    x_ix = x.mul(stride[1]).long()\n    y_ix = y.mul(stride[2]).long()\n\n    input_flat = input.view(input.size(0),-1)\n\n    mapped_vals = input_flat.gather(1, x_ix.add(y_ix))\n\n    return mapped_vals.view_as(input)\n\n\ndef th_bilinear_interp2d(input, coords):\n    """"""\n    bilinear interpolation in 2d\n    """"""\n    x = th.clamp(coords[:,:,0], 0, input.size(1)-2)\n    x0 = x.floor()\n    x1 = x0 + 1\n    y = th.clamp(coords[:,:,1], 0, input.size(2)-2)\n    y0 = y.floor()\n    y1 = y0 + 1\n\n    stride = th.LongTensor(input.stride())\n    x0_ix = x0.mul(stride[1]).long()\n    x1_ix = x1.mul(stride[1]).long()\n    y0_ix = y0.mul(stride[2]).long()\n    y1_ix = y1.mul(stride[2]).long()\n\n    input_flat = input.view(input.size(0),-1)\n\n    vals_00 = input_flat.gather(1, x0_ix.add(y0_ix))\n    vals_10 = input_flat.gather(1, x1_ix.add(y0_ix))\n    vals_01 = input_flat.gather(1, x0_ix.add(y1_ix))\n    vals_11 = input_flat.gather(1, x1_ix.add(y1_ix))\n    \n    xd = x - x0\n    yd = y - y0\n    xm = 1 - xd\n    ym = 1 - yd\n\n    x_mapped = (vals_00.mul(xm).mul(ym) +\n                vals_10.mul(xd).mul(ym) +\n                vals_01.mul(xm).mul(yd) +\n                vals_11.mul(xd).mul(yd))\n\n    return x_mapped.view_as(input)\n\n\ndef th_affine3d(x, matrix, mode=\'trilinear\', center=True):\n    """"""\n    3D Affine image transform on th.Tensor\n    """"""\n    A = matrix[:3,:3]\n    b = matrix[:3,3]\n\n    # make a meshgrid of normal coordinates\n    coords = th_iterproduct(x.size(1),x.size(2),x.size(3)).float()\n\n\n    if center:\n        # shift the coordinates so center is the origin\n        coords[:,0] = coords[:,0] - (x.size(1) / 2. - 0.5)\n        coords[:,1] = coords[:,1] - (x.size(2) / 2. - 0.5)\n        coords[:,2] = coords[:,2] - (x.size(3) / 2. - 0.5)\n\n    \n    # apply the coordinate transformation\n    new_coords = coords.mm(A.t().contiguous()) + b.expand_as(coords)\n\n    if center:\n        # shift the coordinates back so origin is origin\n        new_coords[:,0] = new_coords[:,0] + (x.size(1) / 2. - 0.5)\n        new_coords[:,1] = new_coords[:,1] + (x.size(2) / 2. - 0.5)\n        new_coords[:,2] = new_coords[:,2] + (x.size(3) / 2. - 0.5)\n\n    # map new coordinates using bilinear interpolation\n    if mode == \'nearest\':\n        x_transformed = th_nearest_interp3d(x, new_coords)\n    elif mode == \'trilinear\':\n        x_transformed = th_trilinear_interp3d(x, new_coords)\n    else:\n        x_transformed = th_trilinear_interp3d(x, new_coords)\n\n    return x_transformed\n\n\ndef th_nearest_interp3d(input, coords):\n    """"""\n    2d nearest neighbor interpolation th.Tensor\n    """"""\n    # take clamp of coords so they\'re in the image bounds\n    coords[:,0] = th.clamp(coords[:,0], 0, input.size(1)-1).round()\n    coords[:,1] = th.clamp(coords[:,1], 0, input.size(2)-1).round()\n    coords[:,2] = th.clamp(coords[:,2], 0, input.size(3)-1).round()\n\n    stride = th.LongTensor(input.stride())[1:].float()\n    idx = coords.mv(stride).long()\n\n    input_flat = th_flatten(input)\n\n    mapped_vals = input_flat[idx]\n\n    return mapped_vals.view_as(input)\n\n\ndef th_trilinear_interp3d(input, coords):\n    """"""\n    trilinear interpolation of 3D th.Tensor image\n    """"""\n    # take clamp then floor/ceil of x coords\n    x = th.clamp(coords[:,0], 0, input.size(1)-2)\n    x0 = x.floor()\n    x1 = x0 + 1\n    # take clamp then floor/ceil of y coords\n    y = th.clamp(coords[:,1], 0, input.size(2)-2)\n    y0 = y.floor()\n    y1 = y0 + 1\n    # take clamp then floor/ceil of z coords\n    z = th.clamp(coords[:,2], 0, input.size(3)-2)\n    z0 = z.floor()\n    z1 = z0 + 1\n\n    stride = th.LongTensor(input.stride())[1:]\n    x0_ix = x0.mul(stride[0]).long()\n    x1_ix = x1.mul(stride[0]).long()\n    y0_ix = y0.mul(stride[1]).long()\n    y1_ix = y1.mul(stride[1]).long()\n    z0_ix = z0.mul(stride[2]).long()\n    z1_ix = z1.mul(stride[2]).long()\n\n    input_flat = th_flatten(input)\n\n    vals_000 = input_flat[x0_ix+y0_ix+z0_ix]\n    vals_100 = input_flat[x1_ix+y0_ix+z0_ix]\n    vals_010 = input_flat[x0_ix+y1_ix+z0_ix]\n    vals_001 = input_flat[x0_ix+y0_ix+z1_ix]\n    vals_101 = input_flat[x1_ix+y0_ix+z1_ix]\n    vals_011 = input_flat[x0_ix+y1_ix+z1_ix]\n    vals_110 = input_flat[x1_ix+y1_ix+z0_ix]\n    vals_111 = input_flat[x1_ix+y1_ix+z1_ix]\n\n    xd = x - x0\n    yd = y - y0\n    zd = z - z0\n    xm1 = 1 - xd\n    ym1 = 1 - yd\n    zm1 = 1 - zd\n\n    x_mapped = (vals_000.mul(xm1).mul(ym1).mul(zm1) +\n                vals_100.mul(xd).mul(ym1).mul(zm1) +\n                vals_010.mul(xm1).mul(yd).mul(zm1) +\n                vals_001.mul(xm1).mul(ym1).mul(zd) +\n                vals_101.mul(xd).mul(ym1).mul(zd) +\n                vals_011.mul(xm1).mul(yd).mul(zd) +\n                vals_110.mul(xd).mul(yd).mul(zm1) +\n                vals_111.mul(xd).mul(yd).mul(zd))\n\n    return x_mapped.view_as(input)\n\n\ndef th_pearsonr(x, y):\n    """"""\n    mimics scipy.stats.pearsonr\n    """"""\n    mean_x = th.mean(x)\n    mean_y = th.mean(y)\n    xm = x.sub(mean_x)\n    ym = y.sub(mean_y)\n    r_num = xm.dot(ym)\n    r_den = th.norm(xm, 2) * th.norm(ym, 2)\n    r_val = r_num / r_den\n    return r_val\n\n\ndef th_corrcoef(x):\n    """"""\n    mimics np.corrcoef\n    """"""\n    # calculate covariance matrix of rows\n    mean_x = th.mean(x, 1)\n    xm = x.sub(mean_x.expand_as(x))\n    c = xm.mm(xm.t())\n    c = c / (x.size(1) - 1)\n\n    # normalize covariance matrix\n    d = th.diag(c)\n    stddev = th.pow(d, 0.5)\n    c = c.div(stddev.expand_as(c))\n    c = c.div(stddev.expand_as(c).t())\n\n    # clamp between -1 and 1\n    c = th.clamp(c, -1.0, 1.0)\n\n    return c\n\n\ndef th_matrixcorr(x, y):\n    """"""\n    return a correlation matrix between\n    columns of x and columns of y.\n\n    So, if X.size() == (1000,4) and Y.size() == (1000,5),\n    then the result will be of size (4,5) with the\n    (i,j) value equal to the pearsonr correlation coeff\n    between column i in X and column j in Y\n    """"""\n    mean_x = th.mean(x, 0)\n    mean_y = th.mean(y, 0)\n    xm = x.sub(mean_x.expand_as(x))\n    ym = y.sub(mean_y.expand_as(y))\n    r_num = xm.t().mm(ym)\n    r_den1 = th.norm(xm,2,0)\n    r_den2 = th.norm(ym,2,0)\n    r_den = r_den1.t().mm(r_den2)\n    r_mat = r_num.div(r_den)\n    return r_mat\n\n\ndef th_random_choice(a, n_samples=1, replace=True, p=None):\n    """"""\n    Parameters\n    -----------\n    a : 1-D array-like\n        If a th.Tensor, a random sample is generated from its elements.\n        If an int, the random sample is generated as if a was th.range(n)\n    n_samples : int, optional\n        Number of samples to draw. Default is None, in which case a\n        single value is returned.\n    replace : boolean, optional\n        Whether the sample is with or without replacement\n    p : 1-D array-like, optional\n        The probabilities associated with each entry in a.\n        If not given the sample assumes a uniform distribution over all\n        entries in a.\n\n    Returns\n    --------\n    samples : 1-D ndarray, shape (size,)\n        The generated random samples\n    """"""\n    if isinstance(a, int):\n        a = th.arange(0, a)\n\n    if p is None:\n        if replace:\n            idx = th.floor(th.rand(n_samples)*a.size(0)).long()\n        else:\n            idx = th.randperm(len(a))[:n_samples]\n    else:\n        if abs(1.0-sum(p)) > 1e-3:\n            raise ValueError(\'p must sum to 1.0\')\n        if not replace:\n            raise ValueError(\'replace must equal true if probabilities given\')\n        idx_vec = th.cat([th.zeros(round(p[i]*1000))+i for i in range(len(p))])\n        idx = (th.floor(th.rand(n_samples)*999)).long()\n        idx = idx_vec[idx].long()\n    selection = a[idx]\n    if n_samples == 1:\n        selection = selection[0]\n    return selection\n\n\ndef save_transform(file, transform):\n    """"""\n    Save a transform object\n    """"""\n    with open(file, \'wb\') as output_file:\n        pickler = pickle.Pickler(output_file, -1)\n        pickler.dump(transform)\n\n\ndef load_transform(file):\n    """"""\n    Load a transform object\n    """"""\n    with open(file, \'rb\') as input_file:\n        transform = pickle.load(input_file)\n    return transform\n    \n\n\n    \n'"
tests/test_meters.py,49,"b'import unittest\nimport math\nimport torch\nimport pywick.meters as meter\nimport numpy as np\n\n\nclass TestMeters(unittest.TestCase):\n\n    def testAverageValueMeter(self):\n        m = meter.AverageValueMeter()\n        for i in range(1, 10):\n            m.add(i)\n        mean, std = m.value()\n        self.assertEqual(mean, 5.0)\n        m.reset()\n        mean, std = m.value()\n\n        self.assertTrue(np.isnan(mean))\n\n    def testClassErrorMeter(self):\n        mtr = meter.ClassErrorMeter(topk=[1])\n        output = torch.eye(3)\n        target = torch.range(0, 2)\n        mtr.add(output, target)\n        err = mtr.value()\n\n        self.assertEqual(err, [0], ""All should be correct"")\n\n        target[0] = 1\n        target[1] = 0\n        target[2] = 0\n        mtr.add(output, target)\n        err = mtr.value()\n        self.assertEqual(err, [50.0], ""Half should be correct"")\n\n    def testConfusionMeter(self):\n        mtr = meter.ConfusionMeter(k=3)\n\n        output = torch.Tensor([[.8, 0.1, 0.1], [10, 11, 10], [0.2, 0.2, .3]])\n        target = torch.range(0, 2)\n        mtr.add(output, target)\n\n        conf_mtrx = mtr.value()\n        self.assertEqual(conf_mtrx.sum(), 3, ""All should be correct"")\n        self.assertEqual(conf_mtrx.diagonal().sum(),\n                         3, ""All should be correct"")\n\n        target = torch.Tensor([1, 0, 0])\n        mtr.add(output, target)\n\n        self.assertEqual(conf_mtrx.sum(), 6,\n                         ""Six tests should give six values"")\n        self.assertEqual(conf_mtrx.diagonal().sum(), 3,\n                         ""Shouldn\'t have changed since all new values were false"")\n        self.assertEqual(conf_mtrx[0].sum(), 3,\n                         ""All top have gotten one guess"")\n        self.assertEqual(conf_mtrx[1].sum(), 2,\n                         ""Two first at the 2nd row have a guess"")\n        self.assertEqual(conf_mtrx[1][2], 0,\n                         ""The last one should be empty"")\n        self.assertEqual(conf_mtrx[2].sum(), 1,\n                         ""Bottom row has only the first test correct"")\n        self.assertEqual(conf_mtrx[2][2], 1,\n                         ""Bottom row has only the first test correct"")\n\n        mtr = meter.ConfusionMeter(k=4, normalized=True)\n        output = torch.Tensor([\n            [.8, 0.1, 0.1, 0],\n            [10, 11, 10, 0],\n            [0.2, 0.2, .3, 0],\n            [0, 0, 0, 1],\n        ])\n\n        target = torch.Tensor([0, 1, 2, 3])\n        mtr.add(output, target)\n        conf_mtrx = mtr.value()\n\n        self.assertEqual(conf_mtrx.sum(), output.size(1),\n                         ""All should be correct"")\n        self.assertEqual(conf_mtrx.diagonal().sum(), output.size(1),\n                         ""All should be correct"")\n\n        target[0] = 1\n        target[1] = 0\n        target[2] = 0\n\n        mtr.add(output, target)\n        conf_mtrx = mtr.value()\n\n        self.assertEqual(conf_mtrx.sum(), output.size(1),\n                         ""The normalization should sum all values to 1"")\n        for i, row in enumerate(conf_mtrx):\n            self.assertEqual(row.sum(), 1,\n                             ""Row no "" + str(i) + "" fails to sum to one in normalized mode"")\n\n    def testMSEMeter(self):\n        a = torch.ones(7)\n        b = torch.zeros(7)\n\n        mtr = meter.MSEMeter()\n        mtr.add(a, b)\n        self.assertEqual(1.0, mtr.value())\n\n    def testMovingAverageValueMeter(self):\n        mtr = meter.MovingAverageValueMeter(3)\n\n        mtr.add(1)\n        avg, var = mtr.value()\n\n        self.assertEqual(avg, 1.0)\n        self.assertEqual(var, 0.0)\n        mtr.add(3)\n        avg, var = mtr.value()\n        self.assertEqual(avg, 2.0)\n        self.assertEqual(var, math.sqrt(2))\n\n        mtr.add(5)\n        avg, var = mtr.value()\n        self.assertEqual(avg, 3.0)\n        self.assertEqual(var, 2.0)\n\n        mtr.add(4)\n        avg, var = mtr.value()\n        self.assertEqual(avg, 4.0)\n        self.assertEqual(var, 1.0)\n\n        mtr.add(0)\n        avg, var = mtr.value()\n        self.assertEqual(avg, 3.0)\n        self.assertEqual(var, math.sqrt(7))\n\n    def testAUCMeter(self):\n        mtr = meter.AUCMeter()\n\n        test_size = 1000\n        mtr.add(torch.rand(test_size), torch.zeros(test_size))\n        mtr.add(torch.rand(test_size), torch.Tensor(test_size).fill_(1))\n\n        val, tpr, fpr = mtr.value()\n        self.assertTrue(math.fabs(val - 0.5) < 0.1, msg=""AUC Meter fails"")\n\n        mtr.reset()\n        mtr.add(torch.Tensor(test_size).fill_(0), torch.zeros(test_size))\n        mtr.add(torch.Tensor(test_size).fill_(0.1), torch.zeros(test_size))\n        mtr.add(torch.Tensor(test_size).fill_(0.2), torch.zeros(test_size))\n        mtr.add(torch.Tensor(test_size).fill_(0.3), torch.zeros(test_size))\n        mtr.add(torch.Tensor(test_size).fill_(0.4), torch.zeros(test_size))\n        mtr.add(torch.Tensor(test_size).fill_(1), torch.Tensor(test_size).fill_(1))\n        val, tpr, fpr = mtr.value()\n\n        self.assertEqual(val, 1.0, msg=""AUC Meter fails"")\n\n    def testAPMeter(self):\n        mtr = meter.APMeter()\n\n        target = torch.Tensor([0, 1, 0, 1])\n        output = torch.Tensor([0.1,  0.2, 0.3, 4])\n        weight = torch.Tensor([0.5, 1.0, 2.0, 0.1])\n        mtr.add(output, target, weight)\n\n        ap = mtr.value()\n        val = (1*0.1/0.1 + 0*2.0/2.1 + 1.1*1/3.1 + 0*1/4)/2.0\n        self.assertTrue(\n          math.fabs(ap[0]-val) < 0.01,\n          msg=\'ap test1 failed\'\n        )\n\n        mtr.reset()\n        mtr.add(output, target)\n        ap = mtr.value()\n        val = (1*1.0/1.0 + 0*1.0/2.0 + 2*1.0/3.0 + 0*1.0/4.0)/2.0\n        self.assertTrue(\n            math.fabs(ap[0]-val) < 0.01, msg=\'ap test2 failed\')\n\n        target = torch.Tensor([0, 1, 0, 1])\n        output = torch.Tensor([4, 3, 2, 1])\n        weight = torch.Tensor([1, 2, 3, 4])\n\n        mtr.reset()\n        mtr.add(output, target, weight)\n        ap = mtr.value()\n        val = (0*1.0/1.0 + 1.0*2.0/3.0 + 2.0*0/6.0 + 6.0*1.0/10.0)/2.0\n        self.assertTrue(math.fabs(ap[0]-val) < 0.01, msg=\'ap test3 failed\')\n\n        mtr.reset()\n        mtr.add(output, target)\n        ap = mtr.value()\n        val = (0*1.0 + 1*1.0/2.0 + 0*1.0/3.0 + 2*1.0/4.0)/2.0\n        self.assertTrue(math.fabs(ap[0]-val) < 0.01, msg=\'ap test4 failed\')\n\n        target = torch.Tensor([0, 1, 0, 1])\n        output = torch.Tensor([1, 4, 2, 3])\n        weight = torch.Tensor([1, 2, 3, 4])\n        mtr.reset()\n        mtr.add(output, target, weight)\n        ap = mtr.value()\n        val = (4*1.0/4.0 + 6*1.0/6.0 + 0*6.0/9.0 + 0*6.0/10.0)/2.0\n        self.assertTrue(math.fabs(ap[0]-val) < 0.01, msg=\'ap test5 failed\')\n\n        mtr.reset()\n        mtr.add(output, target)\n        ap = mtr.value()\n        val = (1*1.0 + 2*1.0/2.0 + 0*1.0/3.0 + 0*1.0/4.0)/2.0\n        self.assertTrue(math.fabs(ap[0]-val) < 0.01, msg=\'ap test6 failed\')\n\n        target = torch.Tensor([0, 0, 0, 0])\n        output = torch.Tensor([1, 4, 2, 3])\n        weight = torch.Tensor([1.0, 0.1, 0.0, 0.5])\n        mtr.reset()\n        mtr.add(output, target, weight)\n\n        ap = mtr.value()\n        self.assertEqual(ap[0], 0.)\n\n        mtr.reset()\n        mtr.add(output, target)\n        ap = mtr.value()\n        self.assertEqual(ap[0], 0.)\n\n        target = torch.Tensor([1, 1, 0])\n        output = torch.Tensor([3, 1, 2])\n        weight = torch.Tensor([1, 0.1, 3])\n        mtr.reset()\n        mtr.add(output, target, weight)\n        ap = mtr.value()\n        val = (1*1.0/1.0 + 1*0.0/4.0 + 1.1/4.1)/2.0\n        self.assertTrue(math.fabs(ap[0]-val) < 0.01, msg=\'ap test7 failed\')\n\n        mtr.reset()\n        mtr.add(output, target)\n        ap = mtr.value()\n        val = (1*1.0 + 0*1.0/2.0 + 2*1.0/3.0)/2.0\n        self.assertTrue(math.fabs(ap[0]-val) < 0.01, msg=\'ap test8 failed\')\n\n        # Test multiple K\'s\n        target = torch.Tensor([[0, 1, 0, 1], [0, 1, 0, 1]]).transpose(0, 1)\n        output = torch.Tensor([[.1, .2, .3, 4], [4, 3, 2, 1]]).transpose(0, 1)\n        weight = torch.Tensor([[1.0, 0.5, 2.0, 3.0]]).transpose(0, 1)\n        mtr.reset()\n        mtr.add(output, target, weight)\n        ap = mtr.value()\n        self.assertTrue(\n          math.fabs(ap.sum() -\n                    torch.Tensor([\n                        (1*3.0/3.0 + 0*3.0/5.0 + 3.5*1/5.5 + 0*3.5/6.5)/2.0,\n                        (0*1.0/1.0 + 1*0.5/1.5 + 0*0.5/3.5 + 1*3.5/6.5)/2.0\n                        ]).sum()) < 0.01, msg=\'ap test9 failed\')\n\n        mtr.reset()\n        mtr.add(output, target)\n        ap = mtr.value()\n        self.assertTrue(\n          math.fabs(ap.sum() -\n                    torch.Tensor([\n                        (1*1.0 + 0*1.0/2.0 + 2*1.0/3 + 0*1.0/4.0)/2.0,\n                        (0*1.0 + 1*1.0/2.0 + 0*1.0/3.0 + 2.0*1.0/4.0)/2.0\n                        ]).sum()) < 0.01, msg=\'ap test10 failed\')\n\n        mtr.reset()\n        output = torch.Tensor(5, 4).fill_(0.25)\n        target = torch.ones(5, 4)\n        mtr.add(output, target)\n        output = torch.Tensor(1, 4).fill_(0.25)\n        target = torch.ones(1, 4)\n        mtr.add(output, target)\n        self.assertEqual(mtr.value().size(0), 4, msg=\'ap test11 failed\')\n\n    def testmAPMeter(self):\n        mtr = meter.mAPMeter()\n        target = torch.Tensor([0, 1, 0, 1])\n        output = torch.Tensor([0.1,  0.2, 0.3, 4])\n        weight = torch.Tensor([0.5, 1.0, 2.0, 0.1])\n        mtr.add(output, target)\n\n        ap = mtr.value()\n        val = (1*1.0/1.0 + 0*1.0/2.0 + 2.0*1.0/3.0 + 0*1.0/4.0)/2.0\n        self.assertTrue(\n          math.fabs(ap-val) < 0.01,\n          msg=\'mAP test1 failed\'\n        )\n\n        mtr.reset()\n        mtr.add(output, target, weight)\n        ap = mtr.value()\n        val = (1*0.1/0.1 + 0*2.0/2.1 + 1.1*1/3.1 + 0*1.0/4.0)/2.0\n        self.assertTrue(\n            math.fabs(ap-val) < 0.01, msg=\'mAP test2 failed\')\n\n        # Test multiple K\'s\n        target = torch.Tensor([[0, 1, 0, 1], [0, 1, 0, 1]]).transpose(0, 1)\n        output = torch.Tensor([[.1, .2, .3, 4], [4, 3, 2, 1]]).transpose(0, 1)\n        weight = torch.Tensor([[1.0, 0.5, 2.0, 3.0]]).transpose(0, 1)\n        mtr.reset()\n        mtr.add(output, target, weight)\n        ap = mtr.value()\n        self.assertTrue(\n          math.fabs(ap -\n                    torch.Tensor([\n                        (1*3.0/3.0 + 0*3.0/5.0 + 3.5*1/5.5 + 0*3.5/6.5)/2.0,\n                        (0*1.0/1.0 + 1*0.5/1.5 + 0*0.5/3.5 + 1*3.5/6.5)/2.0\n                        ]).mean()) < 0.01, msg=\'mAP test3 failed\')\n\n        mtr.reset()\n        mtr.add(output, target)\n        ap = mtr.value()\n        self.assertTrue(\n          math.fabs(ap -\n                    torch.Tensor([\n                        (1*1.0 + 0*1.0/2.0 + 2*1.0/3.0 + 0*1.0/4.0)/2.0,\n                        (0*1.0 + 1*1.0/2.0 + 0*1.0/3.0 + 2*1.0/4.0)/2.0\n                        ]).mean()) < 0.01, msg=\'mAP test4 failed\')\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
docs/source/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(\'../../\'))\n\n\n# -- Project information -----------------------------------------------------\n\nproject = u\'pywick\'\ncopyright = u\'2019, Achaiah\'\nauthor = u\'Achaiah\'\n\n\'\'\'\n# UNCOMMENT to generate local documentation\nversion_file = \'../../pywick/__init__.py\'\nwith open(version_file, \'r\') as f:\n    exec(compile(f.read(), version_file, \'exec\'))\n__version__ = locals()[\'__version__\']\n\n# The short X.Y version\nversion = __version__\n# The full version, including alpha/beta/rc tags\nrelease = __version__\n\n\'\'\'\n# UNCOMMENT to generate readthedocs.io documentation\n# The short X.Y version\nversion = \'\'\n# The full version, including alpha/beta/rc tags\nrelease = \'0.5.3\'\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.githubpages\',\n    \'sphinx.ext.mathjax\',\n    \'recommonmark\'\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'ntemplates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\nsource_suffix = [\'.rst\', \'.md\']\n# source_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path .\nexclude_patterns = [u\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'nstatic\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'pywickdoc\'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'pywick.tex\', \'pywick Documentation\',\n     \'Achaiah\', \'manual\'),\n]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'pywick\', \'pywick Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'pywick\', \'pywick Documentation\',\n     author, \'pywick\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n# -- Extension configuration -------------------------------------------------\n\n# Exclude imports\nautodoc_mock_imports = [\n    ""h5py"",\n    ""torch"",\n    ""torchvision"",\n    ""numpy"",\n    ""pandas"",\n]'"
pywick/callbacks/CSVLogger.py,1,"b'import csv\nimport os\nfrom collections import Iterable\nfrom collections import OrderedDict\n\nimport torch\n\nfrom . import Callback\n\n\nclass CSVLogger(Callback):\n    """"""\n    Logs epoch-level metrics to a CSV file\n\n    :param file: (string) path to csv file\n    :param separator: (string) delimiter for file\n    :param append: (bool) whether to append result to existing file or make new file\n    """"""\n\n    def __init__(self, file, separator=\',\', append=False):\n\n        self.file = file\n        self.sep = separator\n        self.append = append\n        self.writer = None\n        self.keys = None\n        self.append_header = True\n        super(CSVLogger, self).__init__()\n\n    def on_train_begin(self, logs=None):\n        if self.append:\n            if os.path.exists(self.file):\n                with open(self.file) as f:\n                    self.append_header = not bool(len(f.readline()))\n            self.csv_file = open(self.file, \'a\')\n        else:\n            self.csv_file = open(self.file, \'w\')\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        RK = {\'num_batches\', \'num_epoch\'}\n\n        def handle_value(k):\n            is_zero_dim_tensor = isinstance(k, torch.Tensor) and k.dim() == 0\n            if isinstance(k, Iterable) and not is_zero_dim_tensor:\n                return \'""[%s]""\' % (\', \'.join(map(str, k)))\n            else:\n                return k\n\n        if not self.writer:\n            self.keys = sorted(logs.keys())\n\n            class CustomDialect(csv.excel):\n                delimiter = self.sep\n\n            self.writer = csv.DictWriter(self.csv_file,\n                                         fieldnames=[\'epoch\'] + [k for k in self.keys if k not in RK],\n                                         dialect=CustomDialect)\n            if self.append_header:\n                self.writer.writeheader()\n\n        row_dict = OrderedDict({\'epoch\': epoch})\n        row_dict.update((key, handle_value(logs[key])) for key in self.keys if key not in RK)\n        self.writer.writerow(row_dict)\n        self.csv_file.flush()\n\n    def on_train_end(self, logs=None):\n        self.csv_file.close()\n        self.writer = None'"
pywick/callbacks/Callback.py,0,"b'class Callback(object):\n    """"""\n    Abstract base class used to build new callbacks. Extend this class to build your own callbacks and overwrite functions\n    that you want to monitor. Functions will be called automatically from the trainer once per relevant training event\n    (e.g. at the beginning of epoch, end of epoch, beginning of batch, end of batch etc.)\n    """"""\n\n    def __init__(self):\n        pass\n\n    def set_params(self, params):\n        self.params = params\n\n    def set_trainer(self, trainer):\n        self.trainer = trainer\n\n    def on_epoch_begin(self, epoch, logs=None):\n        pass\n\n    def on_epoch_end(self, epoch, logs=None):\n        pass\n\n    def on_batch_begin(self, batch, logs=None):\n        pass\n\n    def on_batch_end(self, batch, logs=None):\n        pass\n\n    def on_train_begin(self, logs=None):\n        pass\n\n    def on_train_end(self, logs=None):\n        pass'"
pywick/callbacks/CallbackContainer.py,0,"b'import time\nimport datetime\n\ndef _get_current_time():\n    time_s = time.time()\n    return time_s, datetime.datetime.fromtimestamp(time_s).strftime(""%B %d, %Y - %I:%M%p"")\n\n\nclass CallbackContainer(object):\n    """"""\n    Container holding a list of callbacks.\n    """"""\n\n    def __init__(self, callbacks=None, queue_length=10):\n        self.initial_epoch = -1\n        self.final_epoch = -1\n        self.has_val_data = False\n        callbacks = callbacks or []\n        self.callbacks = [c for c in callbacks]\n        self.queue_length = queue_length\n\n    def append(self, callback):\n        self.callbacks.append(callback)\n\n    def set_params(self, params):\n        for callback in self.callbacks:\n            callback.set_params(params)\n\n    def set_trainer(self, trainer):\n        self.trainer = trainer\n        for callback in self.callbacks:\n            callback.set_trainer(trainer)\n\n    def on_epoch_begin(self, epoch, logs=None):\n        if self.initial_epoch == -1:\n            self.initial_epoch = epoch\n        logs = logs or {}\n        for callback in self.callbacks:\n            callback.on_epoch_begin(epoch, logs)\n\n    def on_epoch_end(self, epoch, logs=None):\n        if self.final_epoch < epoch:\n            self.final_epoch = epoch\n\n        logs = logs or {}\n        for callback in self.callbacks:\n            callback.on_epoch_end(epoch, logs)\n\n    def on_batch_begin(self, batch, logs=None):\n        logs = logs or {}\n        for callback in self.callbacks:\n            callback.on_batch_begin(batch, logs)\n\n    def on_batch_end(self, batch, logs=None):\n        logs = logs or {}\n        for callback in self.callbacks:\n            callback.on_batch_end(batch, logs)\n\n    def on_train_begin(self, logs=None):\n        self.has_val_data = logs[\'has_val_data\']\n        logs = logs or {}\n        self.start_time_s, self.start_time_date = _get_current_time()\n        logs[\'start_time\'] = self.start_time_date\n        logs[\'start_time_s\'] = self.start_time_s\n        for callback in self.callbacks:\n            callback.on_train_begin(logs)\n\n    def on_train_end(self, logs=None):\n        logs = logs or {}\n        logs[\'initial_epoch\'] = self.initial_epoch\n        logs[\'final_epoch\'] = self.final_epoch\n\n        logs[\'final_loss\'] = self.trainer.history.epoch_metrics[\'loss\'][-1]\n        logs[\'best_loss\'] = min(self.trainer.history.epoch_metrics[\'loss\'])\n        if self.has_val_data:\n            logs[\'final_val_loss\'] = self.trainer.history.epoch_metrics[\'val_loss\'][-1]\n            logs[\'best_val_loss\'] = min(self.trainer.history.epoch_metrics[\'val_loss\'])\n\n        logs[\'start_time\'] = self.start_time_date\n        logs[\'start_time_s\'] = self.start_time_s\n\n        time_s, time_date = _get_current_time()\n        logs[\'stop_time\'] = time_date\n        logs[\'stop_time_s\'] = time_s\n        for callback in self.callbacks:\n            callback.on_train_end(logs)'"
pywick/callbacks/CyclicLRScheduler.py,5,"b'# Source: https://github.com/anandsaha/pytorch.cyclic.learning.rate (MIT)\n# Good description of how it functions is here: https://github.com/bckenstler/CLR\n\n# This code is from https://github.com/thomasjpfan/pytorch/blob/401ec389db2c9d2978917a6e4d1101b20340d7e7/torch/optim/lr_scheduler.py\n# This code is under review at PyTorch and is to be merged eventually to make CLR available to all.\n# Tested with pytorch 0.2.0\n\nfrom torch.optim.optimizer import Optimizer\nimport numpy as np\nfrom . import Callback\nfrom ..misc import trun_n_d\n\nwidegap_scale_fn = lambda x: 1/(5**(x*0.0001))\n\nclass CyclicLRScheduler(Callback):\n    """"""Sets the learning rate of each parameter group according to\n    cyclical learning rate policy (CLR). The policy cycles the learning\n    rate between two boundaries with a constant frequency, as detailed in\n    the paper `Cyclical Learning Rates for Training Neural Networks`_.\n    The distance between the two boundaries can be scaled on a per-iteration\n    or per-cycle basis.\n\n    Cyclical learning rate policy changes the learning rate after every batch.\n    `batch_step` should be called after a batch has been used for training.\n    To resume training, save `last_batch_iteration` and use it to instantiate `CycleLR`.\n\n    This class has three built-in policies, as put forth in the paper:\n    ""triangular"":\n        A basic triangular cycle w/ no amplitude scaling.\n    ""triangular2"":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    ""exp_range"":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each\n        cycle iteration.\n\n    This implementation was adapted from the github repo: `bckenstler/CLR`_\n\n    :param optimizer: (Optimizer):\n        Wrapped optimizer.\n    :param base_lr: (float or list):\n        Initial learning rate which is the\n        lower boundary in the cycle for eachparam groups.\n        Default: 0.001\n    :param max_lr: (float or list): Upper boundaries in the cycle for\n        each parameter group. Functionally,\n        it defines the cycle amplitude (max_lr - base_lr).\n        The lr at any cycle is the sum of base_lr\n        and some scaling of the amplitude; therefore\n        max_lr may not actually be reached depending on\n        scaling function. Default: 0.006\n    :param step_size: (int): Number of training iterations per\n        half cycle. Authors suggest setting step_size\n        2-8 x training iterations in epoch. Default: 2000\n    :param mode: (str): One of {triangular, triangular2, exp_range}.\n        Values correspond to policies detailed above.\n        If scale_fn is not None, this argument is ignored.\n        Default: \'triangular\'\n    :param gamma: (float): Constant in \'exp_range\' scaling function:\n        gamma**(cycle iterations)\n        Default: 1.0\n    :param scale_fn: (function): Custom scaling policy defined by a single\n        argument lambda function, where\n        0 <= scale_fn(x) <= 1 for all x >= 0.\n        mode paramater is ignored\n        Default: None\n    :param scale_mode: (str): {\'cycle\', \'iterations\'}.\n        Defines whether scale_fn is evaluated on\n        cycle number or cycle iterations (training\n        iterations since start of cycle).\n        Default: \'cycle\'\n    :param verbose: (bool): Whether to produce some output during initialization\n        Default: True\n\n    Example:\n        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n        >>> scheduler = torch.optim.CyclicLR(optimizer)\n        >>> data_loader = torch.utils.data.DataLoader(...)\n        >>> for epoch in range(10):\n        >>>     for batch in data_loader:\n        >>>         scheduler.batch_step()\n        >>>         train_batch(...)\n\n    .. _Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186\n    .. _bckenstler/CLR: https://github.com/bckenstler/CLR\n    """"""\n\n    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n                 step_size=2000, mode=\'triangular\', gamma=1.,\n                 scale_fn=None, scale_mode=\'cycle\', verbose=True):\n\n        if not isinstance(optimizer, Optimizer):\n            raise TypeError(\'{} is not an Optimizer\'.format(type(optimizer).__name__))\n        self.optimizer = optimizer\n\n        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n            if len(base_lr) != len(optimizer.param_groups):\n                raise ValueError(""expected {} base_lr, got {}"".format(len(optimizer.param_groups), len(base_lr)))\n            self.base_lrs = list(base_lr)\n        else:\n            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n\n        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n            if len(max_lr) != len(optimizer.param_groups):\n                raise ValueError(""expected {} max_lr, got {}"".format(len(optimizer.param_groups), len(max_lr)))\n            self.max_lrs = list(max_lr)\n        else:\n            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n\n        self.step_size = step_size\n\n        if verbose:\n            print(\'CyclicLRScheduler params:\')\n            print(\'\\tstep_size: {}\'.format(step_size))\n            print(\'\\tmode: {}\'.format(mode))\n            print(\'\\tbase_lr: {}\'.format(base_lr))\n            print(\'\\tmax_lr: {}\'.format(max_lr))\n\n        if mode not in [\'triangular\', \'triangular2\', \'exp_range\'] and scale_fn is None:\n            raise ValueError(\'mode is invalid and scale_fn is None\')\n\n        self.mode = mode\n        self.gamma = gamma\n\n        if scale_fn is None:\n            if self.mode == \'triangular\':\n                self.scale_fn = self._triangular_scale_fn\n                self.scale_mode = \'cycle\'\n            elif self.mode == \'triangular2\':\n                self.scale_fn = self._triangular2_scale_fn\n                self.scale_mode = \'cycle\'\n            elif self.mode == \'exp_range\':\n                self.scale_fn = self._exp_range_scale_fn\n                self.scale_mode = \'iterations\'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n\n        self.last_batch_iteration = 0\n        self.epoch_count = 0\n\n        self.optimizer_name = optimizer.__class__.__name__.lower()\n\n    def on_batch_end(self, batch, logs=None):\n        if \'yellowfin\' in self.optimizer_name:\n            computed_lr = [self.optimizer._optimizer.param_groups[0][\'lr\']]  # this is because trainer history expects a list\n        else:\n            computed_lr = self.get_lr()                 # returns a list\n            self.last_batch_iteration = self.last_batch_iteration + 1               # global iteration counter\n            for param_group, lr in zip(self.optimizer.param_groups, computed_lr):\n                param_group[\'lr\'] = lr\n\n        if self.trainer.history is not None:\n            for i,lr in enumerate(computed_lr):\n                computed_lr[i] = trun_n_d(lr.item(), 5)         # .item() is a numpy way of obtaining a float\n\n            self.trainer.history.lrs = computed_lr\n\n    def _triangular_scale_fn(self, x):\n        return 1.\n\n    def _triangular2_scale_fn(self, x):\n        return 1 / (2. ** (x - 1))\n\n    def _exp_range_scale_fn(self, x):\n        return self.gamma**(x)\n\n    def get_lr(self):\n        step_size = float(self.step_size)\n        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))       # cycle number is based on global batch counter\n        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n\n        lrs = []\n        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n        for param_group, base_lr, max_lr in param_lrs:\n            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n            if self.scale_mode == \'cycle\':\n                lr = base_lr + base_height * self.scale_fn(cycle)\n            else:\n                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n            lrs.append(lr)\n        return lrs\n'"
pywick/callbacks/EarlyStopping.py,0,"b'from . import Callback\n\nclass EarlyStopping(Callback):\n    """"""\n    Early Stopping to terminate training early under certain conditions\n\n    EarlyStopping callback to exit the training loop if training or\n        validation loss does not improve by a certain amount for a certain\n        number of epochs\n\n    :param monitor: (string in {\'val_loss\', \'loss\'}):\n        whether to monitor train or val loss\n    :param min_delta: (float):\n        minimum change in monitored value to qualify as improvement.\n        This number should be positive.\n    :param patience: (int):\n        number of epochs to wait for improvment before terminating.\n        the counter be reset after each improvment\n    """"""\n\n    def __init__(self, monitor=\'val_loss\', min_delta=0, patience=5):\n        self.monitor = monitor\n        self.min_delta = min_delta\n        self.patience = patience\n        self.wait = 0\n        self.best_loss = 1e-15\n        self.stopped_epoch = 0\n        super(EarlyStopping, self).__init__()\n\n    def on_train_begin(self, logs=None):\n        self.wait = 0\n        self.best_loss = 1e15\n\n    def on_epoch_end(self, epoch, logs=None):\n        current_loss = logs.get(self.monitor)\n        if current_loss is None:\n            pass\n        else:\n            if (current_loss - self.best_loss) < -self.min_delta:\n                self.best_loss = current_loss\n                self.wait = 1\n            else:\n                if self.wait >= self.patience:\n                    self.stopped_epoch = epoch + 1\n                    self.trainer._stop_training = True\n                self.wait += 1\n\n    def on_train_end(self, logs):\n        if self.stopped_epoch > 0:\n            print(\'\\nTerminated Training for Early Stopping at Epoch %04i\' %\n                  (self.stopped_epoch))'"
pywick/callbacks/ExperimentLogger.py,0,"b'import csv\nimport os\nimport shutil\nfrom collections import OrderedDict\nfrom tempfile import NamedTemporaryFile\n\nfrom . import Callback\n\n\nclass ExperimentLogger(Callback):\n    """"""\n    Generic logger callback for dumping experiment data. Can be extended for more utility.\n    """"""\n\n    def __init__(self, directory, filename=\'Experiment_Logger.csv\', save_prefix=\'Model_\', separator=\',\', append=True):\n\n        self.directory = directory\n        self.filename = filename\n        self.file = os.path.join(self.directory, self.filename)\n        self.save_prefix = save_prefix\n        self.sep = separator\n        self.append = append\n        self.writer = None\n        self.keys = None\n        self.append_header = True\n        super(ExperimentLogger, self).__init__()\n\n    def on_train_begin(self, logs=None):\n        if self.append:\n            open_type = \'a\'\n        else:\n            open_type = \'w\'\n\n        # if append is True, find whether the file already has header\n        num_lines = 0\n        if self.append:\n            if os.path.exists(self.file):\n                with open(self.file) as f:\n                    for num_lines, l in enumerate(f):\n                        pass\n                    # if header exists, DONT append header again\n                with open(self.file) as f:\n                    self.append_header = not bool(len(f.readline()))\n\n        model_idx = num_lines\n        REJECT_KEYS = {\'has_validation_data\'}\n        MODEL_NAME = self.save_prefix + str(model_idx)  # figure out how to get model name\n        self.row_dict = OrderedDict({\'model\': MODEL_NAME})\n        self.keys = sorted(logs.keys())\n        for k in self.keys:\n            if k not in REJECT_KEYS:\n                self.row_dict[k] = logs[k]\n\n        class CustomDialect(csv.excel):\n            delimiter = self.sep\n\n        with open(self.file, open_type) as csv_file:\n            writer = csv.DictWriter(csv_file,\n                                    fieldnames=[\'model\'] + [k for k in self.keys if k not in REJECT_KEYS],\n                                    dialect=CustomDialect)\n            if self.append_header:\n                writer.writeheader()\n\n            writer.writerow(self.row_dict)\n            csv_file.flush()\n\n    def on_train_end(self, logs=None):\n        REJECT_KEYS = {\'has_validation_data\'}\n        row_dict = self.row_dict\n\n        class CustomDialect(csv.excel):\n            delimiter = self.sep\n\n        self.keys = self.keys\n        temp_file = NamedTemporaryFile(delete=False, mode=\'w\')\n        with open(self.file, \'r\') as csv_file, temp_file:\n            reader = csv.DictReader(csv_file,\n                                    fieldnames=[\'model\'] + [k for k in self.keys if k not in REJECT_KEYS],\n                                    dialect=CustomDialect)\n            writer = csv.DictWriter(temp_file,\n                                    fieldnames=[\'model\'] + [k for k in self.keys if k not in REJECT_KEYS],\n                                    dialect=CustomDialect)\n            for row_idx, row in enumerate(reader):\n                if row_idx == 0:\n                    # re-write header with on_train_end\'s metrics\n                    pass\n                if row[\'model\'] == self.row_dict[\'model\']:\n                    writer.writerow(row_dict)\n                else:\n                    writer.writerow(row)\n        shutil.move(temp_file.name, self.file)'"
pywick/callbacks/History.py,0,"b'from . import Callback\n\nclass History(Callback):\n    """"""\n    Callback that records events into a `History` object.\n\n    This callback is automatically applied to\n    every SuperModule.\n    """"""\n\n    def __init__(self, trainer):\n        super(History, self).__init__()\n        self.samples_seen = 0.\n        self.trainer = trainer\n\n    def on_train_begin(self, logs=None):\n        self.epoch_metrics = {\n            \'loss\': []\n        }\n        self.batch_size = logs[\'batch_size\']\n        self.has_val_data = logs[\'has_val_data\']\n        self.has_regularizers = logs[\'has_regularizers\']\n        if self.has_val_data:\n            self.epoch_metrics[\'val_loss\'] = []\n        if self.has_regularizers:\n            self.epoch_metrics[\'reg_loss\'] = []\n\n    def on_epoch_begin(self, epoch, logs=None):\n        if hasattr(self.trainer._optimizer, \'_optimizer\'):  # accounts for meta-optimizers like YellowFin\n            self.lrs = [p[\'lr\'] for p in self.trainer._optimizer._optimizer.param_groups]\n        else:\n            self.lrs = [p[\'lr\'] for p in self.trainer._optimizer.param_groups]\n        self.batch_metrics = {\n            \'loss\': 0.\n        }\n        if self.has_regularizers:\n            self.batch_metrics[\'reg_loss\'] = 0.\n        self.samples_seen = 0.\n\n    def on_epoch_end(self, epoch, logs=None):\n        if logs:\n            self.epoch_metrics[\'loss\'].append(logs[\'loss\'])\n        if logs.get(\'val_loss\'):  # if it exists\n            self.epoch_metrics[\'val_loss\'].append(logs[\'val_loss\'])\n\n    def on_batch_end(self, batch, logs=None):\n        for k in self.batch_metrics:\n            self.batch_metrics[k] = (self.samples_seen * self.batch_metrics[k] + logs[k] * self.batch_size) / (self.samples_seen + self.batch_size)\n        self.samples_seen += self.batch_size\n\n    def __getitem__(self, name):\n        return self.epoch_metrics[name]\n\n    def __repr__(self):\n        return str(self.epoch_metrics)\n\n    def __str__(self):\n        return str(self.epoch_metrics)'"
pywick/callbacks/LRScheduler.py,0,"b'import warnings\n\nfrom . import Callback\n\n\nclass LRScheduler(Callback):\n    """"""\n    Schedule the learning rate according to some function of the\n    current epoch index, current learning rate, and current train/val loss.\n\n    :param schedule: (callable):\n        should return a number of learning rates equal to the number\n        of optimizer.param_groups. It should take the epoch index and\n        **kwargs (or logs) as argument. **kwargs (or logs) will return\n        the epoch logs such as mean training and validation loss from\n        the epoch\n    """"""\n\n    def __init__(self, schedule):\n        if isinstance(schedule, dict):\n            schedule = self.schedule_from_dict\n            self.schedule_dict = schedule\n            if any([k < 1.0 for k in schedule.keys()]):\n                self.fractional_bounds = False\n            else:\n                self.fractional_bounds = True\n        self.schedule = schedule\n        super(LRScheduler, self).__init__()\n\n    def schedule_from_dict(self, epoch, logs=None):\n        for epoch_bound, learn_rate in self.schedule_dict.items():\n            # epoch_bound is in units of ""epochs""\n            if not self.fractional_bounds:\n                if epoch_bound < epoch:\n                    return learn_rate\n            # epoch_bound is in units of ""cumulative percent of epochs""\n            else:\n                if epoch <= epoch_bound * logs[\'num_epoch\']:\n                    return learn_rate\n        warnings.warn(\'Check the keys in the schedule dict.. Returning last value\')\n        return learn_rate\n\n    def on_epoch_begin(self, epoch, logs=None):\n        """"""\n            WARNING: Do NOT use this callback with self-adjusting learners like Yellowfin\n        """"""\n        current_lrs = [p[\'lr\'] for p in self.trainer._optimizer.param_groups]\n        lr_list = self.schedule(epoch, current_lrs, **logs)\n        if not isinstance(lr_list, list):\n            lr_list = [lr_list]\n\n        for param_group, lr_change in zip(self.trainer._optimizer.param_groups, lr_list):\n            param_group[\'lr\'] = lr_change'"
pywick/callbacks/LambdaCallback.py,0,"b'from . import Callback\n\nclass LambdaCallback(Callback):\n    """"""\n    Callback for creating simple, custom callbacks on-the-fly.\n    """"""\n\n    def __init__(self,\n                 on_epoch_begin=None,\n                 on_epoch_end=None,\n                 on_batch_begin=None,\n                 on_batch_end=None,\n                 on_train_begin=None,\n                 on_train_end=None,\n                 **kwargs):\n        super(LambdaCallback, self).__init__()\n        self.__dict__.update(kwargs)\n        if on_epoch_begin is not None:\n            self.on_epoch_begin = on_epoch_begin\n        else:\n            self.on_epoch_begin = lambda epoch, logs: None\n        if on_epoch_end is not None:\n            self.on_epoch_end = on_epoch_end\n        else:\n            self.on_epoch_end = lambda epoch, logs: None\n        if on_batch_begin is not None:\n            self.on_batch_begin = on_batch_begin\n        else:\n            self.on_batch_begin = lambda batch, logs: None\n        if on_batch_end is not None:\n            self.on_batch_end = on_batch_end\n        else:\n            self.on_batch_end = lambda batch, logs: None\n        if on_train_begin is not None:\n            self.on_train_begin = on_train_begin\n        else:\n            self.on_train_begin = lambda logs: None\n        if on_train_end is not None:\n            self.on_train_end = on_train_end\n        else:\n            self.on_train_end = lambda logs: None\n'"
pywick/callbacks/ModelCheckpoint.py,3,"b'import json\nimport math\nimport os\nimport shutil\n\nimport torch\n\nfrom . import Callback\n\n\nclass ModelCheckpoint(Callback):\n    """"""\n    Model Checkpoint to save model weights during training. \'Best\' is determined by minimizing the value found under monitored_log_key in the logs\n    Saved checkpoints contain these keys by default:\n        \'run_id\'\n        \'epoch\'\n        \'loss_type\'\n        \'loss_val\'\n        \'best_epoch\'\n        - plus any additional key/value pairs produced by custom_func\n\n    Additionally saves a .json file with statistics about the run such as:\n         \'run_id\'\n         \'num_epochs\'\n         \'best_epoch\'\n         \'best_loss_or_gain\'\n         \'metric_name\'\n         - plus any additional key/value pairs produced by custom_func\n\n    :param run_id: (string):\n        Uniquely identifies the run\n    :param monitored_log_key: (string):\n        Name of the key in the logs that will contain the value we want to minimize (and thus that will dictate whether the model is \'best\')\n    :param save_dir: (string):\n        Path indicating where to save the checkpoint\n    :param addl_k_v: (dict):\n        dictionary of additional key/value pairs to save with the model. Typically these include some initialization parameters, name of the model etc.\n        (e.g. from the initialization dictionary \'opt\'), as well as other useful params (e.g. mean, std, proc_type: gpu/cpu etc)\n    :param epoch_log_keys: (list):\n        list of keys to save from the epoch log dictionary (Note: the logs dictionary is automatically provided by the learning framework)\n    :param save_interval: (int):\n        How often to save the model (if none then will default to every 5 iterations)\n    :param save_best_only: (bool):\n        Whether only to save the best result (and overwrite all previous)\n        Default: False\n    :param max_saves: (integer > 0 or -1):\n        the max number of models to save. Older model checkpoints will be overwritten if necessary.\n        Set equal to -1 to have no limit.\n        Default: 5\n    :param custom_func: func(k_v_dict, logs, out_dict, monitored_log_key, is_end_training):\n        Custom function for performing any additional logic (to add values to the model). The function will be passed the addl_k_v dictionary,\n        the event logs dictionary, an output dictionary to process, the monitored_log_key and a bool indicating whether the training is finished.\n        The function is expected to modify the output dictionary in order to preserve values across epochs. The function will be called at the\n        end of each epoch and at the end of the training (with is_end_traing = True)\n    :param do_minimize: (bool):\n        whether to minimize or maximize the \'monitored_log_key\' value\n    :param verbose: (bool):\n        verbosity of the console output\n        Default: False\n    """"""\n\n    def __init__(self, run_id, monitored_log_key, save_dir, addl_k_v=dict(), epoch_log_keys=[], save_interval=5, save_best_only=False, max_saves=5,\n                 custom_func=None, do_minimize=True, verbose=False):\n\n        self.run_id = run_id\n        self.addl_k_v = addl_k_v\n        self.save_dir = os.path.expanduser(save_dir)\n        self.save_interval = save_interval\n        self.epoch_log_keys = epoch_log_keys\n        self.save_best_only = save_best_only\n        self.max_saves = max_saves\n        self.custom_func = custom_func\n        self.custom_func_dict = dict()  # this is expected to be filled by the custom_func\n        self.verbose = verbose\n        self.monitored_log_key = monitored_log_key  # \'e.g. dice_coeff\'\n        self.do_minimize = do_minimize\n        self.last_saved_ep = 0\n        self.last_epoch_logs = None\n        self.last_epoch = -1\n        self.best_epoch = -1\n\n        # keep track of old files if necessary\n        if self.max_saves > 0:\n            self.old_files = []\n\n        # mode = \'min\' only supported\n        if do_minimize:\n            self.best_loss = math.inf\n        else:\n            self.best_loss = -89293.923\n        super().__init__()\n\n    def on_epoch_end(self, epoch, logs=None):\n        # import pdb\n        # pdb.set_trace()\n        self.last_epoch_logs = logs\n        self.last_epoch = epoch\n\n        if ((epoch + 1) % self.save_interval == 0):  # only save with given frequency\n            current_loss = logs.get(self.monitored_log_key)\n\n            if (current_loss < self.best_loss and self.save_best_only) or not self.save_best_only or (not self.do_minimize and current_loss > self.best_loss):\n                if current_loss is None:\n                    pass\n                else:\n                    # Call custom function (if set) to process things like best-N results etc\n                    if self.custom_func is not None:\n                        self.custom_func(self.addl_k_v, logs, self.custom_func_dict, False)\n\n                    checkpt_name = generate_checkpoint_name(self.run_id, self.addl_k_v, epoch, False)\n\n                    if self.verbose:\n                        print(\'\\nEpoch %i: loss metric changed from %0.4f to %0.4f saving model to %s\' % (\n                            epoch + 1, self.best_loss, current_loss, os.path.join(self.save_dir, checkpt_name)))\n\n                    if (self.do_minimize and current_loss < self.best_loss) or (not self.do_minimize and current_loss > self.best_loss):\n                        self.best_loss = current_loss\n                        self.best_epoch = epoch\n                        # print(\'Best Loss of {} saved at epoch: {}\'.format(self.best_loss, epoch + 1))\n\n                    save_dict = {\n                        \'run_id\': self.run_id,\n                        \'epoch\': epoch + 1,\n                        \'metric_type\': self.monitored_log_key,\n                        \'metric_value\': current_loss,\n                        \'best_epoch\': self.best_epoch + 1\n                    }\n                    # correctly handle saving parallelized models (https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-torch-nn-dataparallel-models)\n                    if isinstance(self.trainer.model, torch.nn.DataParallel):\n                        save_dict[\'state_dict\'] = self.trainer.model.module.state_dict()\n                    else:\n                        save_dict[\'state_dict\'] = self.trainer.model.state_dict()\n\n                    # add values from other dictionaries\n                    save_dict.update(self.addl_k_v)\n                    save_dict.update(self.custom_func_dict)\n                    for key in self.epoch_log_keys:\n                        save_dict[key] = logs.get(key)  # this is not guaranteed to be found so may return \'None\'\n\n                    save_checkpoint(save_dict, is_best=(self.best_epoch == epoch), save_path=self.save_dir, filename=checkpt_name)\n                    self.last_saved_ep = epoch\n\n                if self.max_saves > 0:\n                    if len(self.old_files) >= self.max_saves:\n                        try:\n                            os.remove(self.old_files[0])\n                        except:\n                            pass\n                        self.old_files = self.old_files[1:]\n                    self.old_files.append(os.path.join(self.save_dir, checkpt_name))\n\n    def on_train_end(self, logs=None):\n        final_epoch = self.last_epoch\n        current_loss = self.last_epoch_logs[self.monitored_log_key]\n\n        ## Save model if it hasn\'t been previously saved and it has best loss value\n        if self.last_saved_ep < final_epoch and ((self.do_minimize and current_loss < self.best_loss) or (not self.do_minimize and current_loss > self.best_loss)):\n            # Call custom function (if set) to process things like best-N results etc\n            if self.custom_func is not None:\n                self.custom_func(self.addl_k_v, self.last_epoch_logs, self.custom_func_dict, False)\n\n            self.best_loss = current_loss\n            self.best_epoch = final_epoch\n            save_dict = {\n                \'run_id\': self.run_id,\n                \'epoch\': final_epoch + 1,\n                \'state_dict\': self.trainer.model.state_dict(),\n                \'metric_type\': self.monitored_log_key,\n                \'metric_value\': current_loss,\n                \'best_epoch\': self.best_epoch\n            }\n            # add values from other dictionaries\n            save_dict.update(self.addl_k_v)\n            save_dict.update(self.custom_func_dict)\n            for key in self.epoch_log_keys:\n                save_dict[key] = self.last_epoch_logs[key]\n\n            save_checkpoint(save_dict, is_best=True, save_path=self.save_dir, filename=generate_checkpoint_name(self.run_id, self.addl_k_v, final_epoch, False))\n            self.last_saved_ep = final_epoch\n\n        stats = {\'run_id\': self.run_id,\n                 \'num_epochs\': final_epoch + 1,\n                 \'best_epoch\': self.best_epoch + 1,\n                 \'best_loss_or_gain\': self.best_loss,\n                 \'metric_type\': self.monitored_log_key\n                 }\n        stats.update(self.addl_k_v)\n        stats.update(self.custom_func_dict)\n        statsfile_path = generate_statsfile_name(self.run_id, self.save_dir)\n        with open(statsfile_path, \'a\') as statsfile:\n            json.dump(stats, statsfile)\n\n\ndef generate_statsfile_name(run_id, save_dir):\n    save_dir1 = os.path.expanduser(save_dir)\n    return os.path.join(save_dir1, str(run_id) + ""_stats.json"")\n\n\ndef generate_checkpoint_name(run_id, kv_dict, epoch, is_best):\n    model_name = kv_dict.get(\'model_name\', \'model\')\n    optimizer_name = kv_dict.get(\'optimizer\', \'o\')\n    if is_best:\n        return str(run_id) + ""_"" + model_name + ""_"" + optimizer_name + ""_ep_best.pth.tar""\n    else:\n        return str(run_id) + ""_"" + model_name + ""_"" + optimizer_name + ""_ep_"" + str(epoch + 1) + "".pth.tar""\n\n\ndef save_checkpoint(state, is_best=False, save_path=""."", filename=None):\n    """"""\n    Saves checkpoint to file.\n\n    :param state: (dict): the dictionary to save. Can have other values besides just model weights.\n    :param is_best: (bool): whether this is the best result we\'ve seen thus far\n    :param save_path: (string): local dir to save to\n    :param filename: (string): name of the file to save under `save_path`\n\n    :return:\n    """"""\n    if not filename:\n        print(""ERROR: No filename defined.  Checkpoint is NOT saved."")\n    save_path1 = os.path.expanduser(save_path)\n    if not os.path.exists(save_path1): os.makedirs(save_path1)\n    torch.save(state, os.path.join(save_path1, filename))\n    if is_best:\n        pos = filename.find(""_ep_"")\n        if pos and pos > 0:\n            bestname = filename[:pos] + ""_best.pth.tar""\n        shutil.copyfile(os.path.join(save_path1, filename), os.path.join(save_path1, bestname))\n'"
pywick/callbacks/ReduceLROnPlateau.py,0,"b'from . import Callback\n\nclass ReduceLROnPlateau(Callback):\n    """"""\n    Reduce the learning rate if the train or validation loss plateaus\n\n    :param monitor: (string in {\'loss\', \'val_loss\'}):\n        which metric to monitor\n    :param factor: (float):\n        factor to decrease learning rate by\n    :param patience: (int):\n        number of epochs to wait for loss improvement before reducing lr\n    :param epsilon: (float):\n        how much improvement must be made to reset patience\n    :param cooldown: (int):\n        number of epochs to cooldown after a lr reduction\n    :param min_lr: (float):\n        minimum value to ever let the learning rate decrease to\n    :param verbose: (int):\n        whether to print reduction to console\n    """"""\n\n    def __init__(self, monitor=\'val_loss\', factor=0.1, patience=10, epsilon=0, cooldown=0, min_lr=0, verbose=0):\n\n        self.monitor = monitor\n        if factor >= 1.0:\n            raise ValueError(\'ReduceLROnPlateau does not support a factor >= 1.0.\')\n        self.factor = factor\n        self.min_lr = min_lr\n        self.epsilon = epsilon\n        self.patience = patience\n        self.verbose = verbose\n        self.cooldown = cooldown\n        self.cooldown_counter = 0\n        self.wait = 0\n        self.best_loss = 1e15\n        self._reset()\n        super(ReduceLROnPlateau, self).__init__()\n\n    def _reset(self):\n        """"""\n        Reset the wait and cooldown counters\n        """"""\n        self.monitor_op = lambda a, b: (a - b) < -self.epsilon\n        self.best_loss = 1e15\n        self.cooldown_counter = 0\n        self.wait = 0\n\n    def on_train_begin(self, logs=None):\n        self._reset()\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        logs[\'lr\'] = [p[\'lr\'] for p in self.trainer._optimizer.param_groups]\n        current_loss = logs.get(self.monitor)\n        if current_loss is None:\n            pass\n        else:\n            # if in cooldown phase\n            if self.cooldown_counter > 0:\n                self.cooldown_counter -= 1\n                self.wait = 0\n            # if loss improved, grab new loss and reset wait counter\n            if self.monitor_op(current_loss, self.best_loss):\n                self.best_loss = current_loss\n                self.wait = 0\n            # loss didnt improve, and not in cooldown phase\n            elif not (self.cooldown_counter > 0):\n                if self.wait >= self.patience:\n                    for p in self.trainer._optimizer.param_groups:\n                        old_lr = p[\'lr\']\n                        if old_lr > self.min_lr + 1e-4:\n                            new_lr = old_lr * self.factor\n                            new_lr = max(new_lr, self.min_lr)\n                            if self.verbose > 0:\n                                print(\'\\nEpoch %05d: reducing lr from %0.3f to %0.3f\' %\n                                      (epoch, old_lr, new_lr))\n                            p[\'lr\'] = new_lr\n                            self.cooldown_counter = self.cooldown\n                            self.wait = 0\n                self.wait += 1\n'"
pywick/callbacks/SimpleModelCheckpoint.py,1,"b'import os\nimport shutil\n\nimport torch\n\nfrom . import Callback\n\n\nclass SimpleModelCheckpoint(Callback):\n    """"""\n    Simple Checkpoint to save model weights during training. This class is mostly superceded by ModelCheckpoint which provides flexible saving functionality.\n\n    :param file: (string):\n        file to which model will be saved.\n        It can be written \'filename_{epoch}_{loss}\' and those\n        values will be filled in before saving.\n    :param monitor: (string in {\'val_loss\', \'loss\'}):\n        whether to monitor train or val loss\n    :param save_best_only: (bool):\n        whether to only save if monitored value has improved\n    :param save_weights_only: (bool):\n        whether to save entire model or just weights\n        NOTE: only `True` is supported at the moment\n    :param max_save: (integer > 0 or -1):\n        the max number of models to save. Older model checkpoints\n        will be overwritten if necessary. Set equal to -1 to have\n        no limit\n    :param verbose: (integer in {0, 1}):\n        verbosity level\n\n    """"""\n\n    def __init__(self,\n                 directory,\n                 filename=\'ckpt.pth.tar\',\n                 monitor=\'val_loss\',\n                 save_best_only=False,\n                 save_weights_only=True,\n                 max_save=-1,\n                 verbose=0):\n        if directory.startswith(\'~\'):\n            directory = os.path.expanduser(directory)\n        self.directory = directory\n        self.filename = filename\n        self.file = os.path.join(self.directory, self.filename)\n        self.monitor = monitor\n        self.save_best_only = save_best_only\n        self.save_weights_only = save_weights_only\n        self.max_save = max_save\n        self.verbose = verbose\n\n        if self.max_save > 0:\n            self.old_files = []\n\n        # mode = \'min\' only supported\n        self.best_loss = float(\'inf\')\n        super(SimpleModelCheckpoint, self).__init__()\n\n    def save_checkpoint(self, epoch, file, is_best=False):\n        """"""\n        Saves checkpoint to file\n        :param epoch: (int): epoch number\n        :param file: (string): file location\n        :param is_best: (bool): whether this is the best result seen thus far\n        :return:\n        """"""\n        torch.save({\n            \'epoch\': epoch + 1,\n            \'state_dict\': self.trainer.model.state_dict(),\n            \'optimizer\': self.trainer._optimizer.state_dict(),\n        }, file)\n        if is_best:\n            shutil.copyfile(file, \'model_best.pth.tar\')\n\n    def on_epoch_end(self, epoch, logs=None):\n\n        file = self.file.format(epoch=\'%03i\' % (epoch + 1),\n                                loss=\'%0.4f\' % logs[self.monitor])\n        if self.save_best_only:\n            current_loss = logs.get(self.monitor)\n            if current_loss is None:\n                pass\n            else:\n                if current_loss < self.best_loss:\n                    if self.verbose > 0:\n                        print(\'\\nEpoch %i: improved from %0.4f to %0.4f saving model to %s\' %\n                              (epoch + 1, self.best_loss, current_loss, file))\n                    self.best_loss = current_loss\n                    # if self.save_weights_only:\n                    # else:\n                    self.save_checkpoint(epoch, file)\n                    if self.max_save > 0:\n                        if len(self.old_files) == self.max_save:\n                            try:\n                                os.remove(self.old_files[0])\n                            except:\n                                pass\n                            self.old_files = self.old_files[1:]\n                        self.old_files.append(file)\n        else:\n            if self.verbose > 0:\n                print(\'\\nEpoch %i: saving model to %s\' % (epoch + 1, file))\n            self.save_checkpoint(epoch, file)\n            if self.max_save > 0:\n                if len(self.old_files) == self.max_save:\n                    try:\n                        os.remove(self.old_files[0])\n                    except:\n                        pass\n                    self.old_files = self.old_files[1:]\n                self.old_files.append(file)\n'"
pywick/callbacks/TQDM.py,0,"b'from tqdm import tqdm\nfrom . import Callback\n\nclass TQDM(Callback):\n\n    def __init__(self):\n        """"""\n        TQDM Progress Bar callback\n\n        This callback is automatically applied to\n        every SuperModule if verbose > 0\n        """"""\n        self.progbar = None\n        super(TQDM, self).__init__()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        # make sure the dbconnection gets closed\n        if self.progbar is not None:\n            self.progbar.close()\n\n    def on_train_begin(self, logs):\n        self.train_logs = logs\n\n    def on_epoch_begin(self, epoch, logs=None):\n        try:\n            self.progbar = tqdm(total=self.train_logs[\'num_batches\'],\n                                unit=\' batches\')\n            self.progbar.set_description(\'Epoch %i/%i\' %\n                                         (epoch + 1, self.train_logs[\'num_epoch\']))\n        except:\n            pass\n\n    def on_epoch_end(self, epoch, logs=None):\n        log_data = {key: \'%.04f\' % value for key, value in self.trainer.history.batch_metrics.items()}\n        for k, v in logs.items():\n            if k.endswith(\'metric\'):\n                log_data[k.split(\'_metric\')[0]] = \'%.02f\' % v\n            else:\n                log_data[k] = v\n        log_data[\'learn_rates\'] = self.trainer.history.lrs\n        self.progbar.set_postfix(log_data)\n        self.progbar.update()\n        self.progbar.close()\n\n    def on_batch_begin(self, batch, logs=None):\n        self.progbar.update(1)\n\n    def on_batch_end(self, batch, logs=None):\n        log_data = {key: \'%.04f\' % value for key, value in self.trainer.history.batch_metrics.items()}\n        for k, v in logs.items():\n            if k.endswith(\'metric\'):\n                log_data[k.split(\'_metric\')[0]] = \'%.02f\' % v\n        log_data[\'learn_rates\'] = self.trainer.history.lrs\n        self.progbar.set_postfix(log_data)'"
pywick/callbacks/__init__.py,0,"b'""""""\nCallbacks are the primary mechanism by which one can embed event hooks into the training process. Many useful callbacks are provided\nout of the box but in all likelihood you will want to implement your own to execute actions based on training events. To do so,\nsimply extend the pywick.callbacks.Callback class and overwrite functions that you are interested in acting upon.\n\n""""""\nfrom .Callback import *\nfrom .CyclicLRScheduler import *\nfrom .CallbackContainer import *\nfrom .CSVLogger import *\nfrom .EarlyStopping import *\nfrom .ExperimentLogger import *\nfrom .History import *\nfrom .LambdaCallback import *\nfrom .LRScheduler import *\nfrom .ModelCheckpoint import *\nfrom .ReduceLROnPlateau import *\nfrom .SimpleModelCheckpoint import *\nfrom .TQDM import *'"
pywick/datasets/BaseDataset.py,0,"b'import numpy as np\nimport torch as th\nfrom torchvision import transforms\nfrom .data_utils import is_tuple_or_list\n\n\nclass BaseDataset(object):\n    """"""An abstract class representing a Dataset.\n\n    All other datasets should subclass it. All subclasses should override\n    ``__len__``, that provides the size of the dataset, and ``__getitem__``,\n    supporting integer indexing in range from 0 to len(self) exclusive.\n    """"""\n\n    def __len__(self):\n        return len(self.inputs) if not isinstance(self.inputs, (tuple,list)) else len(self.inputs[0])\n\n    def add_input_transform(self, transform, add_to_front=True, idx=None):\n        if idx is None:\n            idx = np.arange(len(self.num_inputs))\n        elif not is_tuple_or_list(idx):\n            idx = [idx]\n\n        if add_to_front:\n            for i in idx:\n                self.input_transform[i] = transforms.Compose([transform, self.input_transform[i]])\n        else:\n            for i in idx:\n                self.input_transform[i] = transforms.Compose([self.input_transform[i], transform])\n\n    def add_target_transform(self, transform, add_to_front=True, idx=None):\n        if idx is None:\n            idx = np.arange(len(self.num_targets))\n        elif not is_tuple_or_list(idx):\n            idx = [idx]\n\n        if add_to_front:\n            for i in idx:\n                self.target_transform[i] = transforms.Compose([transform, self.target_transform[i]])\n        else:\n            for i in idx:\n                self.target_transform[i] = transforms.Compose([self.target_transform[i], transform])\n\n    def add_co_transform(self, transform, add_to_front=True, idx=None):\n        if idx is None:\n            idx = np.arange(len(self.min_inputs_or_targets))\n        elif not is_tuple_or_list(idx):\n            idx = [idx]\n\n        if add_to_front:\n            for i in idx:\n                self.co_transform[i] = transforms.Compose([transform, self.co_transform[i]])\n        else:\n            for i in idx:\n                self.co_transform[i] = transforms.Compose([self.co_transform[i], transform])\n\n    def load(self, num_samples=None, load_range=None):\n        """"""\n        Load all data or a subset of the data into actual memory.\n        For instance, if the inputs are paths to image files, then this\n        function will actually load those images.\n\n        :param num_samples: (int (optional)):\n            number of samples to load. if None, will load all\n        :param load_range: (numpy array of integers (optional)):\n            the index range of images to load\n            e.g. np.arange(4) loads the first 4 inputs+targets\n        """"""\n        def _parse_shape(x):\n            if isinstance(x, (list,tuple)):\n                return (len(x),)\n            elif isinstance(x, th.Tensor):\n                return x.size()\n            else:\n                return (1,)\n\n        if num_samples is None and load_range is None:\n            num_samples = len(self)\n            load_range = np.arange(num_samples)\n        elif num_samples is None and load_range is not None:\n            num_samples = len(load_range)\n        elif num_samples is not None and load_range is None:\n            load_range = np.arange(num_samples)\n\n\n        if self.has_target:\n            for enum_idx, sample_idx in enumerate(load_range):\n                input_sample, target_sample = self.__getitem__(sample_idx)\n\n                if enum_idx == 0:\n                    if self.num_inputs == 1:\n                        _shape = [len(load_range)] + list(_parse_shape(input_sample))\n                        inputs = np.empty(_shape)\n                    else:\n                        inputs = []\n                        for i in range(self.num_inputs):\n                            _shape = [len(load_range)] + list(_parse_shape(input_sample[i]))\n                            inputs.append(np.empty(_shape))\n                        #inputs = [np.empty((len(load_range), *_parse_shape(input_sample[i]))) for i in range(self.num_inputs)]\n\n                    if self.num_targets == 1:\n                        _shape = [len(load_range)] + list(_parse_shape(target_sample))\n                        targets = np.empty(_shape)\n                        #targets = np.empty((len(load_range), *_parse_shape(target_sample)))\n                    else:\n                        targets = []\n                        for i in range(self.num_targets):\n                            _shape = [len(load_range)] + list(_parse_shape(target_sample[i]))\n                            targets.append(np.empty(_shape))\n                        #targets = [np.empty((len(load_range), *_parse_shape(target_sample[i]))) for i in range(self.num_targets)]\n\n                if self.num_inputs == 1:\n                    inputs[enum_idx] = input_sample\n                else:\n                    for i in range(self.num_inputs):\n                        inputs[i][enum_idx] = input_sample[i]\n\n                if self.num_targets == 1:\n                    targets[enum_idx] = target_sample\n                else:\n                    for i in range(self.num_targets):\n                        targets[i][enum_idx] = target_sample[i]\n\n            return inputs, targets\n        else:\n            for enum_idx, sample_idx in enumerate(load_range):\n                input_sample = self.__getitem__(sample_idx)\n\n                if enum_idx == 0:\n                    if self.num_inputs == 1:\n                        _shape = [len(load_range)] + list(_parse_shape(input_sample))\n                        inputs = np.empty(_shape)\n                        #inputs = np.empty((len(load_range), *_parse_shape(input_sample)))\n                    else:\n                        inputs = []\n                        for i in range(self.num_inputs):\n                            _shape = [len(load_range)] + list(_parse_shape(input_sample[i]))\n                            inputs.append(np.empty(_shape))\n                        #inputs = [np.empty((len(load_range), *_parse_shape(input_sample[i]))) for i in range(self.num_inputs)]\n\n                if self.num_inputs == 1:\n                    inputs[enum_idx] = input_sample\n                else:\n                    for i in range(self.num_inputs):\n                        inputs[i][enum_idx] = input_sample[i]\n\n            return inputs\n\n    def fit_transforms(self):\n        """"""\n        Make a single pass through the entire dataset in order to fit\n        any parameters of the transforms which require the entire dataset.\n        e.g. StandardScaler() requires mean and std for the entire dataset.\n\n        If you dont call this fit function, then transforms which require properties\n        of the entire dataset will just work at the batch level.\n        e.g. StandardScaler() will normalize each batch by the specific batch mean/std\n        """"""\n        it_fit = hasattr(self.input_transform, \'update_fit\')\n        tt_fit = hasattr(self.target_transform, \'update_fit\')\n        ct_fit = hasattr(self.co_transform, \'update_fit\')\n        if it_fit or tt_fit or ct_fit:\n            for sample_idx in range(len(self)):\n                if hasattr(self, \'input_loader\'):\n                    x = self.input_loader(self.inputs[sample_idx])\n                else:\n                    x = self.inputs[sample_idx]\n                if it_fit:\n                    self.input_transform.update_fit(x)\n                if self.has_target:\n                    if hasattr(self, \'target_loader\'):\n                        y = self.target_loader(self.targets[sample_idx])\n                    else:\n                        y = self.targets[sample_idx]\n                if tt_fit:\n                    self.target_transform.update_fit(y)\n                if ct_fit:\n                    self.co_transform.update_fit(x,y)'"
pywick/datasets/CSVDataset.py,0,"b'from .BaseDataset import BaseDataset\nimport numpy as np\nimport pandas as pd\nfrom .data_utils import _return_first_element_of_list, default_file_reader, _pass_through, _process_transform_argument, _process_co_transform_argument\n\n\nclass CSVDataset(BaseDataset):\n    """"""\n    Initialize a Dataset from a CSV file/dataframe. This does NOT\n    actually load the data into memory if the ``csv`` parameter contains filepaths.\n\n    :param csv: (string or pandas.DataFrame):\n        if string, should be a path to a .csv file which\n        can be loaded as a pandas dataframe\n\n    :param input_cols: (list of ints, or list of strings):\n        which column(s) to use as input arrays.\n        If int(s), should be column indicies.\n        If str(s), should be column names\n\n    :param target_cols: (list of ints, or list of strings):\n        which column(s) to use as input arrays.\n        If int(s), should be column indicies.\n        If str(s), should be column names\n\n    :param input_transform: (transform):\n        tranform to apply to inputs during runtime loading\n\n    :param target_tranform: (transform):\n        transform to apply to targets during runtime loading\n\n    :param co_transform: (transform):\n        transform to apply to both inputs and targets simultaneously\n        during runtime loading\n\n    :param apply_transforms_individually: (bool):\n        Whether to apply transforms to individual inputs or to an input row as a whole (default: False)\n    """"""\n    def __init__(self,\n                 csv,\n                 input_cols=None,\n                 target_cols=None,\n                 input_transform=None,\n                 target_transform=None,\n                 co_transform=None,\n                 apply_transforms_individually=False):\n        assert(input_cols is not None)\n\n        self.input_cols = _process_cols_argument(input_cols)\n        self.target_cols = _process_cols_argument(target_cols)\n\n        self.do_individual_transforms = apply_transforms_individually\n\n        self.df = _process_csv_argument(csv)\n\n        self.inputs = _select_dataframe_columns(self.df, self.input_cols)\n        self.num_inputs = self.inputs.shape[1]\n        self.input_return_processor = _return_first_element_of_list if self.num_inputs==1 else _pass_through\n\n        if self.target_cols is None:\n            self.num_targets = 0\n            self.has_target = False\n        else:\n            self.targets = _select_dataframe_columns(self.df, self.target_cols)\n            self.num_targets = self.targets.shape[1]\n            self.target_return_processor = _return_first_element_of_list if self.num_targets==1 else _pass_through\n            self.has_target = True\n            self.min_inputs_or_targets = min(self.num_inputs, self.num_targets)\n\n        self.input_loader = default_file_reader\n        self.target_loader = default_file_reader\n\n        # The more common use-case would be to apply the transform to the row as a whole, but we support\n        # applying transform to individual elements as well (with a flag)\n        if self.do_individual_transforms:\n            self.input_transform = _process_transform_argument(input_transform, self.num_inputs)\n        else:\n            self.input_transform = _process_transform_argument(input_transform, 1)\n\n        if self.has_target:\n            if self.do_individual_transforms:\n                self.target_transform = _process_transform_argument(target_transform, self.num_targets)\n                self.co_transform = _process_co_transform_argument(co_transform, self.num_inputs, self.num_targets)\n            else:\n                self.target_transform = _process_transform_argument(target_transform, 1)\n                self.co_transform = _process_co_transform_argument(co_transform, 1, 1)\n\n    def __getitem__(self, index):\n        """"""\n        Index the dataset and return the input + target\n        """"""\n\n        # input_sample = list()\n        # for i in range(self.num_inputs):\n        #     input_sample.append(self.input_transform[i](self.input_loader(self.inputs[index, i])))\n\n        # input_sample\n        if self.do_individual_transforms:\n            input_sample = [self.input_transform[i](self.input_loader(self.inputs[index, i])) for i in range(self.num_inputs)]\n        else:\n            input_sample = self.input_transform[0](self.inputs[index])\n\n        if self.has_target:\n            if self.do_individual_transforms:\n                target_sample = [self.target_transform[i](self.target_loader(self.targets[index, i])) for i in range(self.num_targets)]\n                for i in range(self.min_inputs_or_targets):\n                    input_sample[i], target_sample[i] = self.co_transform[i](input_sample[i], target_sample[i])\n            else:\n                target_sample = self.target_transform[0](self.targets[index])\n                input_sample, target_sample = self.co_transform[0](input_sample, target_sample)\n\n\n\n            return self.input_return_processor(input_sample), self.target_return_processor(target_sample)\n        else:\n            return self.input_return_processor(input_sample)\n\n    def split_by_column(self, col):\n        """"""\n        Split this dataset object into multiple dataset objects based on\n        the unique factors of the given column. The number of returned\n        datasets will be equal to the number of unique values in the given\n        column. The transforms and original dataframe will all be transferred\n        to the new datasets\n\n        Useful for splitting a dataset into train/val/test datasets.\n\n        :param col: (integer or string)\n            which column to split the data on.\n            if int, should be column index.\n            if str, should be column name\n\n        :return: list of new datasets with transforms copied\n        """"""\n        if isinstance(col, int):\n            split_vals = self.df.iloc[:,col].values.flatten()\n\n            new_df_list = []\n            for unique_split_val in np.unique(split_vals):\n                new_df = self.df[:][self.df.iloc[:,col]==unique_split_val]\n                new_df_list.append(new_df)\n        elif isinstance(col, str):\n            split_vals = self.df.loc[:,col].values.flatten()\n\n            new_df_list = []\n            for unique_split_val in np.unique(split_vals):\n                new_df = self.df[:][self.df.loc[:,col]==unique_split_val]\n                new_df_list.append(new_df)\n        else:\n            raise ValueError(\'col argument not valid - must be column name or index\')\n\n        new_datasets = []\n        for new_df in new_df_list:\n            new_dataset = self.copy(new_df)\n            new_datasets.append(new_dataset)\n\n        return new_datasets\n\n    def train_test_split(self, train_size):\n        """"""\n        Define a split for the current dataset where some part of it is used for\n        training while the remainder is used for testing\n\n        :param train_size: (int): length of the training dataset. The remainder will be\n            returned as the test dataset\n        :return: tuple of datasets (train, test)\n        """"""\n        if train_size < 1:\n            train_size = int(train_size * len(self))\n\n        train_indices = np.random.choice(len(self), train_size, replace=False)\n        test_indices = np.array([i for i in range(len(self)) if i not in train_indices])\n\n        train_df = self.df.iloc[train_indices,:]\n        test_df = self.df.iloc[test_indices,:]\n\n        train_dataset = self.copy(train_df)\n        test_dataset = self.copy(test_df)\n\n        return train_dataset, test_dataset\n\n    def copy(self, df=None):\n        """"""\n        Creates a copy of itself (including transforms and other params).\n\n        :param df: dataframe to include in the copy. If not specified, uses the\n            internal dataframe inside this instance (if any)\n\n        :return:\n        """"""\n        if df is None:\n            df = self.df\n\n        return CSVDataset(df,\n                          input_cols=self.input_cols,\n                          target_cols=self.target_cols,\n                          input_transform=self.input_transform,\n                          target_transform=self.target_transform,\n                          co_transform=self.co_transform)\n\n\ndef _process_cols_argument(cols):\n    if isinstance(cols, tuple):\n        cols = list(cols)\n    return cols\n\ndef _process_csv_argument(csv):\n    if isinstance(csv, str):\n        df = pd.read_csv(csv)\n    elif isinstance(csv, pd.DataFrame):\n        df = csv\n    else:\n        raise ValueError(\'csv argument must be string or dataframe\')\n    return df\n\ndef _select_dataframe_columns(df, cols):\n    if isinstance(cols[0], str):\n        inputs = df.loc[:,cols].values\n    elif isinstance(cols[0], int):\n        inputs = df.iloc[:,cols].values\n    else:\n        raise ValueError(\'Provided columns should be string column names or integer column indices\')\n    return inputs'"
pywick/datasets/ClonedFolderDataset.py,0,"b'import random\nfrom .FolderDataset import FolderDataset\n\n\nclass ClonedFolderDataset(FolderDataset):\n    """"""\n    Dataset that can be initialized with a dictionary of internal parameters (useful when trying to clone a FolderDataset)\n\n    :param data: (list):\n        list of data on which the dataset operates\n\n    :param meta_data: (dict):\n        parameters that correspond to the target dataset\'s attributes\n\n    :param kwargs: (args):\n        variable set of key-value pairs to set as attributes for the dataset\n    """"""\n    def __init__(self, data, meta_data, **kwargs):\n\n        if len(data) == 0:\n            raise (RuntimeError(\'No data provided\'))\n        else:\n            print(\'Initializing with %i data items\' % len(data))\n\n        self.data = data\n\n        # Source: https://stackoverflow.com/questions/2466191/set-attributes-from-dictionary-in-python\n        # generic way of initializing the object\n        for key in meta_data:\n            setattr(self, key, meta_data[key])\n        for key in kwargs:\n            setattr(self, key, kwargs[key])\n\n\ndef random_split_dataset(orig_dataset, splitRatio=0.8, random_seed=None):\n    \'\'\'\n    Randomly split the given dataset into two datasets based on the provided ratio\n\n    :param orig_dataset: (UsefulDataset):\n        dataset to split (of type pywick.datasets.UsefulDataset)\n\n    :param splitRatio: (float):\n        ratio to use when splitting the data\n\n    :param random_seed: (int):\n        random seed for replicability of results\n\n    :return: tuple of split ClonedFolderDatasets\n    \'\'\'\n    random.seed(a=random_seed)\n\n    # not cloning the dictionary at this point... maybe it should be?\n    orig_dict = orig_dataset.getmeta_data()\n    part1 = []\n    part2 = []\n\n    for i, item in enumerate(orig_dataset.getdata()):\n        if random.random() < splitRatio:\n            part1.append(item)\n        else:\n            part2.append(item)\n\n    return ClonedFolderDataset(part1, orig_dict), ClonedFolderDataset(part2, orig_dict)'"
pywick/datasets/FolderDataset.py,0,"b'import numpy as np\nimport os\n\nfrom PIL import Image\nfrom .UsefulDataset import UsefulDataset\nfrom .data_utils import npy_loader, pil_loader, _find_classes, _finds_inputs_and_targets\n\n# convenience loaders one can use (in order not to reinvent the wheel)\nrgb_image_loader = lambda path: Image.open(path).convert(\'RGB\')   # a loader for images that require RGB color space\nrgba_image_loader = lambda path: Image.open(path).convert(\'RGBA\')   # a loader for images that require RGBA color space\nbw_image_loader = lambda path: Image.open(path).convert(\'L\')      # a loader for images that require B/W color space\nidentity_x = lambda x: x\n\n\nclass FolderDataset(UsefulDataset):\n    """"""\n    An incredibly versatile dataset class for loading out-of-memory data.\\n\n    First, the relevant directory structures are traversed to find all necessary files.\\n\n    Then provided loader(s) is/(are) invoked on inputs and targets.\\n\n    Finally provided transforms are applied with optional ability to specify the order of individual and co-transforms.\\n\n\n    The rel_target_root parameter is used for image segmentation cases\n        Typically the structure will look like the following:\\n\n        |- root (aka training images)\\n\n        |  - dir1\\n\n        |  - dir2\\n\n        |- masks (aka label images)\\n\n        |  - dir1\\n\n        |  - dir2\\n\n\n    :param root: (string):\n        path to main directory\n    :param class_mode: (string in `{\'label\', \'image\', \'path\'}`):\n        type of target sample to look for and return\\n\n        `label` = return class folder as target\\n\n        `image` = return another image as target (determined by optional target_prefix/postfix).\n        NOTE: if class_mode == \'image\', in addition to input, you must also provide ``rel_target_root``,\n        ``target_prefix`` or ``target_postfix`` (in any combination).\\n\n        `path` = determines paths for inputs and targets and applies the respective loaders to the path\n    :param class_to_idx: (dict):\n        If specified, the given class_to_idx map will be used. Otherwise one will be derived from the directory structure.\n    :param input_regex: (string `(default is any valid image file)`):\n        regular expression to find input images.\n        e.g. if all your inputs have the word \'input\',\n        you\'d enter something like input_regex=\'*input*\'\n    :param rel_target_root: (string `(default is Nothing)`):\n        root of directory where to look for target images RELATIVE to the root dir (first arg)\n    :param target_prefix: (string `(default is Nothing)`):\n        prefix to use (if any) when trying to locate the matching target\n    :param target_postfix: (string):\n        postfix to use (if any) when trying to locate the matching target\n    :param transform: (torch transform):\n        transform to apply to input sample individually\n    :param target_transform: (torch transform):\n        transform to apply to target sample individually\n    :param co_transform: (torch transform):\n        transform to apply to both the input and the target\n    :param apply_co_transform_first: (bool):\n        whether to apply the co-transform before or after individual transforms (default: True = before)\n    :param default_loader: (string in `{\'npy\', \'pil\'}` or function `(default: pil)`):\n        defines how to load samples from file. Will be applied to both input and target unless a separate target_loader is defined.\n        if a function is provided, it should take in a file path as input and return the loaded sample.\n    :param target_loader: (string in `{\'npy\', \'pil\'}` or function `(default: pil)`):\n        defines how to load target samples from file.\n        If a function is provided, it should take in a file path as input and return the loaded sample.\n    :param exclusion_file: (string):\n        list of files to exclude when enumerating all files.\n        The list must be a full path relative to the root parameter\n    :param target_index_map: (dict `(defaults to binary mask: {255:1})`):\n        a dictionary that maps pixel values in the image to classes to be recognized.\\n\n        Used in conjunction with \'image\' class_mode to produce a label for semantic segmentation\n        For semantic segmentation this is required so the default is a binary mask. However, if you want to turn off\n        this feature then specify target_index_map=None\n    """"""\n\n    def __init__(self,\n                 root,\n                 class_mode=\'label\',\n                 class_to_idx=None,\n                 input_regex=\'*\',\n                 rel_target_root=\'\',\n                 target_prefix=\'\',\n                 target_postfix=\'\',\n                 target_extension=\'png\',\n                 transform=None,\n                 target_transform=None,\n                 co_transform=None,\n                 apply_co_transform_first=True,\n                 default_loader=\'pil\',\n                 target_loader=None,\n                 exclusion_file=None,\n                 target_index_map=None):\n\n\n        # call the super constructor first, then set our own parameters\n        super().__init__()\n\n        if default_loader == \'npy\':\n            default_loader = npy_loader\n        elif default_loader == \'pil\':\n            default_loader = pil_loader\n        self.default_loader = default_loader\n\n        # separate loading for targets (e.g. for black/white masks)\n        self.target_loader = target_loader\n\n        root = os.path.expanduser(root)\n\n        if class_to_idx:\n            self.classes = class_to_idx.keys()\n            self.class_to_idx = class_to_idx\n        else:\n            self.classes, self.class_to_idx = _find_classes([root])\n        data, _ = _finds_inputs_and_targets(root, class_mode=class_mode, class_to_idx=self.class_to_idx, input_regex=input_regex,\n                                            rel_target_root=rel_target_root, target_prefix=target_prefix, target_postfix=target_postfix,\n                                            target_extension=target_extension, exclusion_file=exclusion_file)\n\n        if len(data) == 0:\n            raise (RuntimeError(\'Found 0 data items in subfolders of: %s\' % root))\n        else:\n            print(\'Found %i data items\' % len(data))\n\n        self.root = os.path.expanduser(root)\n        self.data = data\n        self.transform = transform\n        self.target_transform = target_transform\n        self.co_transform = co_transform\n        self.apply_co_transform_first = apply_co_transform_first\n        self.target_index_map = target_index_map\n\n        self.class_mode = class_mode\n\n    def __getitem__(self, index):\n        # get paths\n        input_sample, target_sample = self.data[index]\n\n        in_base = input_sample\n        out_base = target_sample\n\n        try:\n            if self.target_loader is not None:\n                target_sample = self.target_loader(target_sample)\n\n            ## DELETEME\n            # if len(self.classes) == 1 and self.class_mode == \'image\':  # this is a binary segmentation map\n            #     target_sample = self.default_loader(target_sample, color_space=\'L\')\n            # else:\n            #     if self.class_mode == \'image\':\n            #         target_sample = self.default_loader(target_sample)\n            ## END DELETEME\n\n            # load samples into memory\n            input_sample = self.default_loader(input_sample)\n            if self.class_mode == \'image\' and self.target_index_map is not None:   # if we\'re dealing with image masks, we need to change the underlying pixels\n                target_sample = np.array(target_sample)  # convert to np\n                for k, v in self.target_index_map.items():\n                    target_sample[target_sample == k] = v  # replace pixels with class values\n                target_sample = Image.fromarray(target_sample.astype(np.float32))  # convert back to image\n\n            # apply transforms\n            if self.apply_co_transform_first and self.co_transform is not None:\n                input_sample, target_sample = self.co_transform(input_sample, target_sample)\n            if self.transform is not None:\n                # input_sample = self.transform(image=input_sample)     # needed for albumentations to work (but currently albumentations dies with multiple workers)\n                input_sample = self.transform(input_sample)\n            if self.target_transform is not None:\n                target_sample = self.target_transform(target_sample)\n            if not self.apply_co_transform_first and self.co_transform is not None:\n                input_sample, target_sample = self.co_transform(input_sample, target_sample)\n\n            return input_sample, target_sample\n        except Exception as e:\n            print(\'########## ERROR ########\')\n            print(str(e))\n            print(\'=========================\')\n            print(""ERROR: Exception occurred while processing dataset with input {} and output {}"".format(str(in_base), str(out_base)))\n\n    def __len__(self):\n        return len(self.data)\n\n    def getdata(self):\n        return self.data\n\n    def getmeta_data(self):\n        meta = {\'num_inputs\': self.num_inputs,  # these are hardcoded for the fit module to work\n                \'num_targets\': self.num_targets,\n                \'transform\': self.transform,\n                \'target_transform\': self.target_transform,\n                \'co_transform\': self.co_transform,\n                \'class_to_idx\': self.class_to_idx,\n                \'class_mode\': self.class_mode,\n                \'classes\': self.classes,\n                \'default_loader\': self.default_loader,\n                \'target_loader\': self.target_loader,\n                \'apply_co_transform_first\': self.apply_co_transform_first,\n                \'target_index_map\': self.target_index_map\n                }\n        return meta'"
pywick/datasets/MultiFolderDataset.py,0,"b'import itertools\nimport os\n\nfrom PIL import Image\nfrom .FolderDataset import FolderDataset, npy_loader, pil_loader, rgb_image_loader, rgba_image_loader, _find_classes, _finds_inputs_and_targets\n\n\nclass MultiFolderDataset(FolderDataset):\n    """"""\n    This class extends the FolderDataset with abilty to supply multiple root directories. The ``rel_target_root`` must exist\n    relative to each root directory. For complete description of functionality see ``FolderDataset``\n\n    :param roots: (list):\n        list of root directories to traverse\\n\n\n    :param class_mode: (string in `{\'label\', \'image\', \'path\'}):`\n        type of target sample to look for and return\\n\n        `label` = return class folder as target\\n\n        `image` = return another image as target (determined by optional target_prefix/postfix)\\n\n            NOTE: if class_mode == \'image\', in addition to input, you must also provide rel_target_root,\n            target_prefix or target_postfix (in any combination).\n        `path` = determines paths for inputs and targets and applies the respective loaders to the path\n\n    :param class_to_idx: (dict):\n        If specified, the given class_to_idx map will be used. Otherwise one will be derived from the directory structure.\n\n    :param input_regex: (string `(default is any valid image file)`):\n        regular expression to find input images\\n\n        e.g. if all your inputs have the word \'input\',\n        you\'d enter something like input_regex=\'*input*\'\n\n    :param rel_target_root: (string `(default is Nothing)`):\n        root of directory where to look for target images RELATIVE to the root dir (first arg)\n\n    :param target_prefix: (string `(default is Nothing)`):\n        prefix to use (if any) when trying to locate the matching target\n\n    :param target_postfix: (string):\n        postfix to use (if any) when trying to locate the matching target\n\n    :param transform: (torch transform):\n        transform to apply to input sample individually\n\n    :param target_transform: (torch transform):\n        transform to apply to target sample individually\n\n    :param co_transform: (torch transform):\n        transform to apply to both the input and the target\n\n    :param apply_co_transform_first: (bool):\n        whether to apply the co-transform before or after individual transforms (default: True = before)\n\n    :param default_loader: (string in `{\'npy\', \'pil\'}` or function  `(default: pil)`):\n        defines how to load samples from file. Will be applied to both input and target unless a separate target_loader is defined.\\n\n        if a function is provided, it should take in a file path as input and return the loaded sample.\n\n    :param target_loader: (string in `{\'npy\', \'pil\'}` or function  `(default: pil)`):\n        defines how to load target samples from file\\n\n        if a function is provided, it should take in a file path as input and return the loaded sample.\n\n    :param exclusion_file: (string):\n        list of files to exclude when enumerating all files.\n        The list must be a full path relative to the root parameter\n\n    :param target_index_map: (dict `(defaults to binary mask: {255:1})):\n        a dictionary that maps pixel values in the image to classes to be recognized.\\n\n        Used in conjunction with \'image\' class_mode to produce a label for semantic segmentation\n        For semantic segmentation this is required so the default is a binary mask. However, if you want to turn off\n        this feature then specify target_index_map=None\n    """"""\n\n\n    def __init__(self,\n                 roots,\n                 class_mode=\'label\',\n                 class_to_idx=None,\n                 input_regex=\'*\',\n                 rel_target_root=\'\',\n                 target_prefix=\'\',\n                 target_postfix=\'\',\n                 target_extension=\'png\',\n                 transform=None,\n                 target_transform=None,\n                 co_transform=None,\n                 apply_co_transform_first=True,\n                 default_loader=\'pil\',\n                 target_loader=None,\n                 exclusion_file=None,\n                 target_index_map=None):\n\n            # call the super constructor first, then set our own parameters\n            # super().__init__()\n            self.num_inputs = 1  # these are hardcoded for the fit module to work\n            self.num_targets = 1  # these are hardcoded for the fit module to work\n\n            if default_loader == \'npy\':\n                default_loader = npy_loader\n            elif default_loader == \'pil\':\n                default_loader = pil_loader\n            self.default_loader = default_loader\n\n            # separate loading for targets (e.g. for black/white masks)\n            self.target_loader = target_loader\n\n            if class_to_idx:\n                self.classes = class_to_idx.keys()\n                self.class_to_idx = class_to_idx\n            else:\n                self.classes, self.class_to_idx = _find_classes(roots)\n\n            data_list = list()\n            for root in roots:\n                datai, _ = _finds_inputs_and_targets(root, class_mode=class_mode, class_to_idx=self.class_to_idx, input_regex=input_regex,\n                                                     rel_target_root=rel_target_root, target_prefix=target_prefix, target_postfix=target_postfix,\n                                                     target_extension=target_extension, exclusion_file=exclusion_file)\n                data_list.append(datai)\n\n            self.data = list(itertools.chain.from_iterable(data_list))\n\n            if len(self.data) == 0:\n                raise (RuntimeError(\'Found 0 data items in subfolders of: {}\'.format(roots)))\n            else:\n                print(\'Found %i data items\' % len(self.data))\n\n            self.roots = [os.path.expanduser(x) for x in roots]\n            self.transform = transform\n            self.target_transform = target_transform\n            self.co_transform = co_transform\n            self.apply_co_transform_first = apply_co_transform_first\n            self.target_index_map = target_index_map\n\n            self.class_mode = class_mode\n'"
pywick/datasets/PredictFolderDataset.py,0,"b'from .FolderDataset import FolderDataset, identity_x\n\nclass PredictFolderDataset(FolderDataset):\n    """"""\n    Convenience class for loading out-of-memory data that is more geared toward prediction data loading (where ground truth is not available). \\n\n    If not transformed in any way (either via one of the loaders or transforms) the inputs and targets will be identical (paths to the discovered files)\\n\n    Instead, the intended use is that the input path is loaded into some kind of binary representation (usually an image), while the target is either\n    left as a path or is post-processed to accommodate some special need.\n\n    Arguments\n    ---------\n    :param root: (string):\n        path to main directory\n\n    :param input_regex: (string `(default is any valid image file)`):\n        regular expression to find inputs.\n        e.g. if all your inputs have the word \'input\',\n        you\'d enter something like input_regex=\'*input*\'\n\n    :param input_transform: (torch transform):\n        transform to apply to each input before returning\n\n    :param input_loader: (callable `(default: identity)`):\n        defines how to load input samples from file.\n        If a function is provided, it should take in a file path as input and return the loaded sample. Identity simply returns the input.\n\n    :param target_loader: (callable `(default: None)`):\n        defines how to load target samples from file (which, in our case, are the same as inputs)\n        If a function is provided, it should take in a file path as input and return the loaded sample.\n\n    :param exclusion_file: (string):\n        list of files to exclude when enumerating all files.\n        The list must be a full path relative to the root parameter\n    """"""\n    def __init__(self, root, input_regex=\'*\', input_transform=None, input_loader=identity_x, target_loader=None,  exclusion_file=None):\n\n        super().__init__(root=root, class_mode=\'path\', input_regex=input_regex, target_extension=None, transform=input_transform,\n                         default_loader=input_loader, target_loader=target_loader, exclusion_file=exclusion_file, target_index_map=None)\n\n'"
pywick/datasets/TensorDataset.py,0,"b'from .BaseDataset import BaseDataset\nfrom .data_utils import _process_array_argument, _return_first_element_of_list, _process_transform_argument, _process_co_transform_argument, _pass_through\n\nclass TensorDataset(BaseDataset):\n\n    """"""\n    Dataset class for loading in-memory data.\n\n    :param inputs: (numpy array)\n\n    :param targets: (numpy array)\n\n    :param input_transform: (transform):\n        transform to apply to input sample individually\n\n    :param target_transform: (transform):\n        transform to apply to target sample individually\n\n    :param co_transform: (transform):\n        transform to apply to both input and target sample simultaneously\n\n    """"""\n    def __init__(self,\n                 inputs,\n                 targets=None,\n                 input_transform=None,\n                 target_transform=None,\n                 co_transform=None):\n        self.inputs = _process_array_argument(inputs)\n        self.num_inputs = len(self.inputs)\n        self.input_return_processor = _return_first_element_of_list if self.num_inputs==1 else _pass_through\n\n        if targets is None:\n            self.has_target = False\n        else:\n            self.targets = _process_array_argument(targets)\n            self.num_targets = len(self.targets)\n            self.target_return_processor = _return_first_element_of_list if self.num_targets==1 else _pass_through\n            self.min_inputs_or_targets = min(self.num_inputs, self.num_targets)\n            self.has_target = True\n\n        self.input_transform = _process_transform_argument(input_transform, self.num_inputs)\n        if self.has_target:\n            self.target_transform = _process_transform_argument(target_transform, self.num_targets)\n            self.co_transform = _process_co_transform_argument(co_transform, self.num_inputs, self.num_targets)\n\n    def __getitem__(self, index):\n        """"""\n        Index the dataset and return the input + target\n        """"""\n        input_sample = [self.input_transform[i](self.inputs[i][index]) for i in range(self.num_inputs)]\n\n        if self.has_target:\n            target_sample = [self.target_transform[i](self.targets[i][index]) for i in range(self.num_targets)]\n            #for i in range(self.min_inputs_or_targets):\n            #    input_sample[i], target_sample[i] = self.co_transform[i](input_sample[i], target_sample[i])\n\n            return self.input_return_processor(input_sample), self.target_return_processor(target_sample)\n        else:\n            return self.input_return_processor(input_sample)'"
pywick/datasets/UsefulDataset.py,2,"b'import torch.utils.data.dataset as ds\n\nclass UsefulDataset(ds.Dataset):\n    \'\'\'\n    A ``torch.utils.data.Dataset`` class with additional useful functions.\n    \'\'\'\n\n    def __init__(self):\n        self.num_inputs = 1         # these are hardcoded for the fit module to work\n        self.num_targets = 1        # these are hardcoded for the fit module to work\n\n    def getdata(self):\n        """"""\n        Data that the Dataset class operates on. Typically iterable/list of tuple(label,target).\n        Note: This is different than simply calling myDataset.data because some datasets are comprised of multiple other datasets!\n        The dataset returned should be the `combined` dataset!\n\n        :return: iterable - Representation of the entire dataset (combined if necessary from multiple other datasets)\n        """"""\n        raise NotImplementedError\n\n    def getmeta_data(self):\n        """"""\n        Additional data to return that might be useful to consumer. Typically a dict.\n\n        :return: dict(any)\n        """"""\n        raise NotImplementedError'"
pywick/datasets/__init__.py,3,"b'""""""\nDatasets are the primary mechanism by which Pytorch assembles training and testing data\nto be used while training neural networks. While `pytorch` already provides a number of\nhandy `datasets <https://pytorch.org/docs/stable/data.html#module-torch.utils.data>`_ and\n`torchvision` further extends them to common\n`academic sets <https://pytorch.org/docs/stable/torchvision/datasets.html/>`_,\nthe implementations below provide some very powerful options for loading all kinds of data.\nWe had to extend the default Pytorch implementation as by default it does not keep track\nof some useful metadata. That said, you can use our datasets in the normal fashion you\'re used to\nwith Pytorch.\n""""""\n\nfrom . import BaseDataset, ClonedFolderDataset, CSVDataset, FolderDataset, PredictFolderDataset, UsefulDataset, data_utils\nfrom .tnt import *\n'"
pywick/datasets/data_utils.py,0,"b'import fnmatch\nimport os\nimport os.path\nimport random\nimport warnings\n\nimport numpy as np\nimport tqdm\n\ntry:\n    from PIL import Image\nexcept:\n    warnings.warn(\'Cant import PIL.. Cant load PIL images\')\n\nIMG_EXTENSIONS = [\n    \'.jpg\', \'.JPG\', \'.jpeg\', \'.JPEG\',\n    \'.png\', \'.PNG\', \'.ppm\', \'.PPM\', \'.bmp\', \'.BMP\',\n]\n\n\ndef pil_loader(path, color_space=\'\'):\n    """"""\n    Attempts to load a file using PIL with provided ``color_space``.\n\n    :param path: (string): file to load\n    :param color_space: (string, one of `{rgb, rgba, L, 1, binary}`): Specifies the colorspace\n        to use for PIL loading. If not provided a simple ``Image.open(path)`` will be performed.\n\n    :return: PIL image\n    """"""\n    try:\n        if color_space.lower() == \'rgb\':\n            return Image.open(path).convert(\'RGB\')\n        if color_space.lower() == \'rgba\':\n            return Image.open(path).convert(\'RGBA\')\n        elif color_space.lower() == \'l\':\n            return Image.open(path).convert(\'L\')\n        elif color_space.lower() == \'1\' or color_space.lower() == \'binary\':\n            return Image.open(path).convert(\'1\')\n        else:\n            return Image.open(path)\n    except OSError:\n        print(""!!!  Could not read path: "" + path)\n        exit(2)\n\n\ndef pil_loader_rgb(path):\n    """"""Convenience loader for RGB files (e.g. `.jpg`)""""""\n    with open(path, \'rb\', 0) as f:\n        return Image.open(f).convert(\'RGB\')\n\n\ndef pil_loader_bw(path):\n    """"""Convenience loader for B/W files (e.g. `.png with only one color chanel`)""""""\n    with open(path, \'rb\', 0) as f:\n        return Image.open(f).convert(\'L\')\n\n\ndef npy_loader(path, color_space=None):     # color space is unused here\n    """"""Convenience loader for numeric files (e.g. arrays of numbers)""""""\n    return np.load(path)\n\n\ndef _process_array_argument(x):\n    if not is_tuple_or_list(x):\n        x = [x]\n    return x\n\n\ndef default_file_reader(x):\n    if isinstance(x, str):\n        if x.endswith(\'.npy\'):\n            x = npy_loader(x)\n        else:\n            try:\n                x = pil_loader(x, color_space=\'RGB\')\n            except:\n                raise ValueError(\'File Format is not supported\')\n    #else:\n        #raise ValueError(\'x should be string, but got %s\' % type(x))\n    return x\n\ndef is_tuple_or_list(x):\n    return isinstance(x, (tuple,list))\n\ndef _process_transform_argument(tform, num_inputs):\n    tform = tform if tform is not None else _pass_through\n    if is_tuple_or_list(tform):\n        if len(tform) != num_inputs:\n            raise Exception(\'If transform is list, must provide one transform for each input\')\n        tform = [t if t is not None else _pass_through for t in tform]\n    else:\n        tform = [tform] * num_inputs\n    return tform\n\n\ndef _process_co_transform_argument(tform, num_inputs, num_targets):\n    tform = tform if tform is not None else _multi_arg_pass_through\n    if is_tuple_or_list(tform):\n        if len(tform) != num_inputs:\n            raise Exception(\'If transform is list, must provide one transform for each input\')\n        tform = [t if t is not None else _multi_arg_pass_through for t in tform]\n    else:\n        tform = [tform] * min(num_inputs, num_targets)\n    return tform\n\n\ndef _return_first_element_of_list(x):\n    return x[0]\n\n\ndef _pass_through(x):\n    return x\n\n\ndef _multi_arg_pass_through(*x):\n    return x\n\n\ndef _find_classes(dirs):\n    classes = list()\n    for dir in dirs:\n        dir = os.path.expanduser(dir)\n        loc_classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n        for cls in loc_classes:\n            if cls not in classes:\n                classes.append(cls)\n\n    classes.sort()\n    class_to_idx = {classes[i]: i for i in range(len(classes))}\n    return classes, class_to_idx\n\n\ndef _is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\n\ndef _finds_inputs_and_targets(root, class_mode, class_to_idx=None, input_regex=\'*\',\n                              rel_target_root=\'\', target_prefix=\'\', target_postfix=\'\', target_extension=\'png\',\n                              splitRatio=1.0, random_seed=None, exclusion_file=None):\n    """"""\n    Map a dataset from a root folder. Optionally, split the dataset randomly into two partitions (e.g. train and val)\n\n    :param root: string\\n\n        root dir to scan\n    :param class_mode: string in `{\'label\', \'image\', \'path\'}`\\n\n        whether to return a label, an image or a path (of the input) as target\n    :param class_to_idx: list\\n\n        classes to map to indices\n    :param input_regex: string (default: *)\\n\n        regex to apply to scanned input entries\n    :param rel_target_root: string\\n\n        relative target root to scan (if any)\n    :param target_prefix: string\\n\n        prefix to use (if any) when trying to locate the matching target\n    :param target_postfix: string\\n\n        postfix to use (if any) when trying to locate the matching target\n    :param splitRatio: float\\n\n        if set to 0.0 < splitRatio < 1.0 the function will return two datasets\n    :param random_seed: int (default: None)\\n\n        you can control replicability of the split by explicitly setting the random seed\n    :param exclusion_file: string (default: None)\\n\n        list of files (one per line) to exclude when enumerating all files\\n\n        The list must contain paths relative to the root parameter\\n\n        each line may include the filename and additional comma-separated metadata, in which case the first item will be considered the path itself and the rest will be ignored\n\n    :return: partition1 (list of (input, target)), partition2 (list of (input, target))\n    """"""\n    if class_mode not in (\'image\', \'label\', \'path\'):\n        raise ValueError(\'class_mode must be one of: {label, image, path}\')\n\n    if class_mode == \'image\' and rel_target_root == \'\' and target_prefix == \'\' and target_postfix == \'\':\n            raise ValueError(\'must provide either rel_target_root or a value for target prefix/postfix when class_mode is set to: image\')\n\n    ## Handle exclusion list, if any\n    exclusion_list = set()\n    if exclusion_file:\n        with open(exclusion_file, \'r\') as exclfile:\n            for line in exclfile:\n                exclusion_list.add(line.split(\',\')[0])\n\n    trainlist_inputs = []\n    trainlist_targets = []\n    vallist_inputs = []\n    vallist_targets = []\n    icount = 0\n    root = os.path.expanduser(root)\n    for subdir in sorted(os.listdir(root)):\n        d = os.path.join(root, subdir)\n        if not os.path.isdir(d):\n            continue\n\n        for rootz, _, fnames in sorted(os.walk(d)):\n            for fname in fnames:\n                if _is_image_file(fname):\n                    if fnmatch.fnmatch(fname, input_regex):\n                        icount = icount + 1\n\n                        # enforce random split\n                        if random.random() < splitRatio:\n                            inputs = trainlist_inputs\n                            targets = trainlist_targets\n                        else:\n                            inputs = vallist_inputs\n                            targets = vallist_targets\n\n                        if not os.path.join(subdir,fname) in exclusion_list:        # exclude any undesired files\n                            path = os.path.join(rootz, fname)\n                            inputs.append(path)\n                            if class_mode == \'path\':\n                                targets.append(path)\n                            elif class_mode == \'label\':\n                                target_name = class_to_idx.get(subdir)\n                                if target_name is None:\n                                    print(""WARN WARN: !!! Label "" + subdir + "" does NOT have a valid mapping to ID!  Ignoring..."")\n                                    inputs.pop()   # Also remove last entry from inputs\n                                else:\n                                    targets.append(target_name)\n                            elif class_mode == \'image\':\n                                name_vs_ext = fname.rsplit(\'.\', 1)\n                                target_fname = os.path.join(root, rel_target_root, subdir, target_prefix + name_vs_ext[0] + target_postfix + \'.\' + target_extension)\n                                if os.path.exists(target_fname):\n                                    targets.append(target_fname)\n                                else:\n                                    raise ValueError(\'Could not locate file: \' + target_fname + \' corresponding to input: \' + path)\n    if class_mode is None:\n        return trainlist_inputs, vallist_inputs\n    else:\n        assert len(trainlist_inputs) == len(trainlist_targets) and len(vallist_inputs) == len(vallist_targets)\n        print(""Total processed: %i    Train-list: %i items   Val-list: %i items    Exclusion-list: %i items"" % (icount, len(trainlist_inputs), len(vallist_inputs), len(exclusion_list)))\n        return list(zip(trainlist_inputs, trainlist_targets)), list(zip(vallist_inputs, vallist_targets))\n\n\ndef get_dataset_mean_std(data_set, img_size=256, output_div=255.0):\n    """"""\n    Computes channel-wise mean and std of the dataset. The process is memory-intensive as the entire dataset must fit into memory.\n    Therefore, each image is scaled down to img_size first (default: 256).\n\n    Assumptions:\n        1. dataset uses PIL to read images\n        2. Images are in RGB format.\n\n    :param data_set: (pytorch Dataset)\n    :param img_size: (int):\n        scale of images at which to compute mean/std (default: 256)\n    :param output_div: (float `{1.0, 255.0}`):\n        Image values are naturally in 0-255 value range so the returned output is divided by output_div. For example, if output_div = 255.0 then mean/std will be in 0-1 range.\n\n    :return: (mean, std) as per-channel values ([r,g,b], [r,g,b])\n    """"""\n\n    total = np.zeros((3, (len(data_set) * img_size * img_size)), dtype=int)\n    position = 0        # keep track of position in the total array\n\n    for src, _ in tqdm(data_set, ascii=True, desc=""Process"", unit=\'images\'):\n        src = src.resize((img_size, img_size))      # resize to same size\n        src = np.array(src)\n\n        # reshape into correct shape\n        src = src.reshape(img_size * img_size, 3)\n        src = src.swapaxes(1,0)\n\n        # np.concatenate((a, b, c), axis=1)  # NOPE NOPE NOPE -- makes a memory re-allocation for every concatenate operation\n\n        # -- In-place value substitution -- #\n        place = img_size * img_size * position\n        total[0:src.shape[0], place:place+src.shape[1]] = src   # copies the src data into the total position at specified index\n\n        position = position+1\n\n    return total.mean(1) / output_div, total.std(1) / output_div        # return channel-wise mean for the entire dataset\n\n\nif __name__ == ""__main__"":\n    from pywick.datasets.FolderDataset import FolderDataset\n    from pywick.datasets.data_utils import pil_loader_rgb\n\n    dataset = FolderDataset(root=\'/home/users/youruser/images\', class_mode=\'label\', default_loader=pil_loader_rgb)\n    mean, std = get_dataset_mean_std(dataset)\n    print(\'----- RESULT -----\')\n    print(\'mean: {}\'.format(mean))\n    print(\'std:  {}\'.format(std))\n    print (\'----- DONE ------\')\n'"
pywick/functions/__init__.py,1,"b'""""""\nHere you can find a collection of functions that are used in neural networks. One of the most important aspects of a neural\nnetwork is a good activation function. Pytorch already has a solid `collection <https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity/>`_\nof activation functions but here are a few more experimental ones to play around with.\n""""""\n\nfrom . import *\n'"
pywick/functions/activations_autofn.py,7,"b'# Source: https://github.com/rwightman/gen-efficientnet-pytorch/blob/master/geffnet/activations/activations_autofn.py (Apache 2.0)\n\nimport torch\nfrom torch import nn as nn\nfrom torch.nn import functional as F\n\n\n__all__ = [\'swish_auto\', \'SwishAuto\', \'mish_auto\', \'MishAuto\']\n\n\nclass SwishAutoFn(torch.autograd.Function):\n    """"""Swish - Described in: https://arxiv.org/abs/1710.05941\n    Memory efficient variant from:\n     https://medium.com/the-artificial-impostor/more-memory-efficient-swish-activation-function-e07c22c12a76\n    """"""\n    @staticmethod\n    def forward(ctx, x):\n        result = x.mul(torch.sigmoid(x))\n        ctx.save_for_backward(x)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x = ctx.saved_tensors[0]\n        x_sigmoid = torch.sigmoid(x)\n        return grad_output.mul(x_sigmoid * (1 + x * (1 - x_sigmoid)))\n\n\ndef swish_auto(x, inplace=False):\n    # inplace ignored\n    return SwishAutoFn.apply(x)\n\n\nclass SwishAuto(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(SwishAuto, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return SwishAutoFn.apply(x)\n\n\nclass MishAutoFn(torch.autograd.Function):\n    """"""Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\n    Experimental memory-efficient variant\n    """"""\n\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        y = x.mul(torch.tanh(F.softplus(x)))  # x * tanh(ln(1 + exp(x)))\n        return y\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x = ctx.saved_tensors[0]\n        x_sigmoid = torch.sigmoid(x)\n        x_tanh_sp = F.softplus(x).tanh()\n        return grad_output.mul(x_tanh_sp + x * x_sigmoid * (1 - x_tanh_sp * x_tanh_sp))\n\n\ndef mish_auto(x, inplace=False):\n    # inplace ignored\n    return MishAutoFn.apply(x)\n\n\nclass MishAuto(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(MishAuto, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return MishAutoFn.apply(x)\n\n'"
pywick/functions/activations_jit.py,14,"b'# Source: https://github.com/rwightman/gen-efficientnet-pytorch/blob/master/geffnet/activations/activations_jit.py (Apache 2.0)\n\nimport torch\nfrom torch import nn as nn\nfrom torch.nn import functional as F\n\n\n__all__ = [\'swish_jit\', \'SwishJit\', \'mish_jit\', \'MishJit\']\n#\'hard_swish_jit\', \'HardSwishJit\', \'hard_sigmoid_jit\', \'HardSigmoidJit\']\n\n\n@torch.jit.script\ndef swish_jit_fwd(x):\n    return x.mul(torch.sigmoid(x))\n\n\n@torch.jit.script\ndef swish_jit_bwd(x, grad_output):\n    x_sigmoid = torch.sigmoid(x)\n    return grad_output * (x_sigmoid * (1 + x * (1 - x_sigmoid)))\n\n\nclass SwishJitAutoFn(torch.autograd.Function):\n    """""" torch.jit.script optimised Swish\n    Inspired by conversation btw Jeremy Howard & Adam Pazske\n    https://twitter.com/jeremyphoward/status/1188251041835315200\n    """"""\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        return swish_jit_fwd(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x = ctx.saved_tensors[0]\n        return swish_jit_bwd(x, grad_output)\n\n\ndef swish_jit(x, inplace=False):\n    # inplace ignored\n    return SwishJitAutoFn.apply(x)\n\n\nclass SwishJit(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(SwishJit, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return SwishJitAutoFn.apply(x)\n\n\n@torch.jit.script\ndef mish_jit_fwd(x):\n    return x.mul(torch.tanh(F.softplus(x)))\n\n\n@torch.jit.script\ndef mish_jit_bwd(x, grad_output):\n    x_sigmoid = torch.sigmoid(x)\n    x_tanh_sp = F.softplus(x).tanh()\n    return grad_output.mul(x_tanh_sp + x * x_sigmoid * (1 - x_tanh_sp * x_tanh_sp))\n\n\nclass MishJitAutoFn(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        return mish_jit_fwd(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x = ctx.saved_tensors[0]\n        return mish_jit_bwd(x, grad_output)\n\n\ndef mish_jit(x, inplace=False):\n    # inplace ignored\n    return MishJitAutoFn.apply(x)\n\n\nclass MishJit(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(MishJit, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return MishJitAutoFn.apply(x)\n\n\n# @torch.jit.script\n# def hard_swish_jit(x, inplac: bool = False):\n#     return x.mul(F.relu6(x + 3.).mul_(1./6.))\n#\n#\n# class HardSwishJit(nn.Module):\n#     def __init__(self, inplace: bool = False):\n#         super(HardSwishJit, self).__init__()\n#\n#     def forward(self, x):\n#         return hard_swish_jit(x)\n#\n#\n# @torch.jit.script\n# def hard_sigmoid_jit(x, inplace: bool = False):\n#     return F.relu6(x + 3.).mul(1./6.)\n#\n#\n# class HardSigmoidJit(nn.Module):\n#     def __init__(self, inplace: bool = False):\n#         super(HardSigmoidJit, self).__init__()\n#\n#     def forward(self, x):\n#         return hard_sigmoid_jit(x)\n'"
pywick/functions/affine.py,37,"b'import torch\nimport torch.nn.functional as F\n\nfrom ..utils import th_iterproduct, th_flatten\n\n\ndef F_affine2d(x, matrix, center=True):\n    """"""\n    2D Affine image transform on torch Tensor\n    """"""\n    if matrix.dim() == 2:\n        matrix = matrix.view(-1, 2, 3)\n\n    A_batch = matrix[:, :, :2]\n    if A_batch.size(0) != x.size(0):\n        A_batch = A_batch.repeat(x.size(0), 1, 1)\n    b_batch = matrix[:, :, 2].unsqueeze(1)\n\n    # make a meshgrid of normal coordinates\n    _coords = th_iterproduct(x.size(1), x.size(2))\n    with torch.no_grad:\n        coords = _coords.unsqueeze(0).repeat(x.size(0), 1, 1).float()\n    if center:\n        # shift the coordinates so center is the origin\n        coords[:, :, 0] = coords[:, :, 0] - (x.size(1) / 2. + 0.5)\n        coords[:, :, 1] = coords[:, :, 1] - (x.size(2) / 2. + 0.5)\n\n    # apply the coordinate transformation\n    new_coords = coords.bmm(A_batch.transpose(1, 2)) + b_batch.expand_as(coords)\n\n    if center:\n        # shift the coordinates back so origin is origin\n        new_coords[:, :, 0] = new_coords[:, :, 0] + (x.size(1) / 2. + 0.5)\n        new_coords[:, :, 1] = new_coords[:, :, 1] + (x.size(2) / 2. + 0.5)\n\n    # map new coordinates using bilinear interpolation\n    x_transformed = F_bilinear_interp2d(x, new_coords)\n\n    return x_transformed\n\n\ndef F_bilinear_interp2d(input, coords):\n    """"""\n    bilinear interpolation of 2d torch Tensor\n    """"""\n    x = torch.clamp(coords[:, :, 0], 0, input.size(1) - 2)\n    x0 = x.floor()\n    x1 = x0 + 1\n    y = torch.clamp(coords[:, :, 1], 0, input.size(2) - 2)\n    y0 = y.floor()\n    y1 = y0 + 1\n\n    stride = torch.LongTensor(input.stride())\n    x0_ix = x0.mul(stride[1]).long()\n    x1_ix = x1.mul(stride[1]).long()\n    y0_ix = y0.mul(stride[2]).long()\n    y1_ix = y1.mul(stride[2]).long()\n\n    input_flat = input.view(input.size(0), -1).contiguous()\n\n    vals_00 = input_flat.gather(1, x0_ix.add(y0_ix).detach())\n    vals_10 = input_flat.gather(1, x1_ix.add(y0_ix).detach())\n    vals_01 = input_flat.gather(1, x0_ix.add(y1_ix).detach())\n    vals_11 = input_flat.gather(1, x1_ix.add(y1_ix).detach())\n\n    xd = x - x0\n    yd = y - y0\n    xm = 1 - xd\n    ym = 1 - yd\n\n    x_mapped = (vals_00.mul(xm).mul(ym) +\n                vals_10.mul(xd).mul(ym) +\n                vals_01.mul(xm).mul(yd) +\n                vals_11.mul(xd).mul(yd))\n\n    return x_mapped.view_as(input)\n\n\ndef F_batch_affine2d(x, matrix, center=True):\n    """"""\n\n    x : torch.Tensor\n        shape = (Samples, C, H, W)\n        NOTE: Assume C is always equal to 1!\n    matrix : torch.Tensor\n        shape = (Samples, 6) or (Samples, 2, 3)\n\n    Example\n    -------\n    >>> x = torch.zeros(3,1,10,10)\n    >>> x[:,:,3:7,3:7] = 1\n    >>> m1 = torch.FloatTensor([[1.2,0,0],[0,1.2,0]])\n    >>> m2 = torch.FloatTensor([[0.8,0,0],[0,0.8,0]])\n    >>> m3 = torch.FloatTensor([[1.0,0,3],[0,1.0,3]])\n    >>> matrix = torch.stack([m1,m2,m3])\n    >>> xx = F_batch_affine2d(x,matrix)\n    """"""\n    if matrix.dim() == 2:\n        matrix = matrix.view(-1, 2, 3)\n\n    A_batch = matrix[:, :, :2]\n    b_batch = matrix[:, :, 2].unsqueeze(1)\n\n    # make a meshgrid of normal coordinates\n    _coords = th_iterproduct(x.size(2), x.size(3))\n    with torch.no_grad:\n        coords = _coords.unsqueeze(0).repeat(x.size(0), 1, 1).float()\n\n    if center:\n        # shift the coordinates so center is the origin\n        coords[:, :, 0] = coords[:, :, 0] - (x.size(2) / 2. + 0.5)\n        coords[:, :, 1] = coords[:, :, 1] - (x.size(3) / 2. + 0.5)\n\n    # apply the coordinate transformation\n    new_coords = coords.bmm(A_batch.transpose(1, 2)) + b_batch.expand_as(coords)\n\n    if center:\n        # shift the coordinates back so origin is origin\n        new_coords[:, :, 0] = new_coords[:, :, 0] + (x.size(2) / 2. + 0.5)\n        new_coords[:, :, 1] = new_coords[:, :, 1] + (x.size(3) / 2. + 0.5)\n\n    # map new coordinates using bilinear interpolation\n    x_transformed = F_batch_bilinear_interp2d(x, new_coords)\n\n    return x_transformed\n\n\ndef F_batch_bilinear_interp2d(input, coords):\n    """"""\n    input : torch.Tensor\n        size = (N,H,W,C)\n    coords : torch.Tensor\n        size = (N,H*W*C,2)\n    """"""\n    x = torch.clamp(coords[:, :, 0], 0, input.size(2) - 2)\n    x0 = x.floor()\n    x1 = x0 + 1\n    y = torch.clamp(coords[:, :, 1], 0, input.size(3) - 2)\n    y0 = y.floor()\n    y1 = y0 + 1\n\n    stride = torch.LongTensor(input.stride())\n    x0_ix = x0.mul(stride[2]).long()\n    x1_ix = x1.mul(stride[2]).long()\n    y0_ix = y0.mul(stride[3]).long()\n    y1_ix = y1.mul(stride[3]).long()\n\n    input_flat = input.view(input.size(0), -1).contiguous()\n\n    vals_00 = input_flat.gather(1, x0_ix.add(y0_ix).detach())\n    vals_10 = input_flat.gather(1, x1_ix.add(y0_ix).detach())\n    vals_01 = input_flat.gather(1, x0_ix.add(y1_ix).detach())\n    vals_11 = input_flat.gather(1, x1_ix.add(y1_ix).detach())\n\n    xd = x - x0\n    yd = y - y0\n    xm = 1 - xd\n    ym = 1 - yd\n\n    x_mapped = (vals_00.mul(xm).mul(ym) +\n                vals_10.mul(xd).mul(ym) +\n                vals_01.mul(xm).mul(yd) +\n                vals_11.mul(xd).mul(yd))\n\n    return x_mapped.view_as(input)\n\n\ndef F_affine3d(x, matrix, center=True):\n    A = matrix[:3, :3]\n    b = matrix[:3, 3]\n\n    # make a meshgrid of normal coordinates\n    with torch.no_grad:\n        coords = th_iterproduct(x.size(1), x.size(2), x.size(3)).float()\n\n    if center:\n        # shift the coordinates so center is the origin\n        coords[:, 0] = coords[:, 0] - (x.size(1) / 2. + 0.5)\n        coords[:, 1] = coords[:, 1] - (x.size(2) / 2. + 0.5)\n        coords[:, 2] = coords[:, 2] - (x.size(3) / 2. + 0.5)\n\n    # apply the coordinate transformation\n    new_coords = F.linear(coords, A, b)\n\n    if center:\n        # shift the coordinates back so origin is origin\n        new_coords[:, 0] = new_coords[:, 0] + (x.size(1) / 2. + 0.5)\n        new_coords[:, 1] = new_coords[:, 1] + (x.size(2) / 2. + 0.5)\n        new_coords[:, 2] = new_coords[:, 2] + (x.size(3) / 2. + 0.5)\n\n    # map new coordinates using bilinear interpolation\n    x_transformed = F_trilinear_interp3d(x, new_coords)\n\n    return x_transformed\n\n\ndef F_trilinear_interp3d(input, coords):\n    """"""\n    trilinear interpolation of 3D image\n    """"""\n    # take clamp then floor/ceil of x coords\n    x = torch.clamp(coords[:, 0], 0, input.size(1) - 2)\n    x0 = x.floor()\n    x1 = x0 + 1\n    # take clamp then floor/ceil of y coords\n    y = torch.clamp(coords[:, 1], 0, input.size(2) - 2)\n    y0 = y.floor()\n    y1 = y0 + 1\n    # take clamp then floor/ceil of z coords\n    z = torch.clamp(coords[:, 2], 0, input.size(3) - 2)\n    z0 = z.floor()\n    z1 = z0 + 1\n\n    stride = torch.LongTensor(input.stride())[1:]\n    x0_ix = x0.mul(stride[0]).long()\n    x1_ix = x1.mul(stride[0]).long()\n    y0_ix = y0.mul(stride[1]).long()\n    y1_ix = y1.mul(stride[1]).long()\n    z0_ix = z0.mul(stride[2]).long()\n    z1_ix = z1.mul(stride[2]).long()\n\n    input_flat = th_flatten(input)\n\n    vals_000 = input_flat[x0_ix.add(y0_ix).add(z0_ix).detach()]\n    vals_100 = input_flat[x1_ix.add(y0_ix).add(z0_ix).detach()]\n    vals_010 = input_flat[x0_ix.add(y1_ix).add(z0_ix).detach()]\n    vals_001 = input_flat[x0_ix.add(y0_ix).add(z1_ix).detach()]\n    vals_101 = input_flat[x1_ix.add(y0_ix).add(z1_ix).detach()]\n    vals_011 = input_flat[x0_ix.add(y1_ix).add(z1_ix).detach()]\n    vals_110 = input_flat[x1_ix.add(y1_ix).add(z0_ix).detach()]\n    vals_111 = input_flat[x1_ix.add(y1_ix).add(z1_ix).detach()]\n\n    xd = x - x0\n    yd = y - y0\n    zd = z - z0\n    xm = 1 - xd\n    ym = 1 - yd\n    zm = 1 - zd\n\n    x_mapped = (vals_000.mul(xm).mul(ym).mul(zm) +\n                vals_100.mul(xd).mul(ym).mul(zm) +\n                vals_010.mul(xm).mul(yd).mul(zm) +\n                vals_001.mul(xm).mul(ym).mul(zd) +\n                vals_101.mul(xd).mul(ym).mul(zd) +\n                vals_011.mul(xm).mul(yd).mul(zd) +\n                vals_110.mul(xd).mul(yd).mul(zm) +\n                vals_111.mul(xd).mul(yd).mul(zd))\n\n    return x_mapped.view_as(input)\n\n\ndef F_batch_affine3d(x, matrix, center=True):\n    """"""\n\n    x : torch.Tensor\n        shape = (Samples, C, H, W)\n        NOTE: Assume C is always equal to 1!\n    matrix : torch.Tensor\n        shape = (Samples, 6) or (Samples, 2, 3)\n\n    Example\n    -------\n    >>> x = torch.zeros(3,1,10,10,10)\n    >>> x[:,:,3:7,3:7,3:7] = 1\n    >>> m1 = torch.FloatTensor([[1.2,0,0,0],[0,1.2,0,0],[0,0,1.2,0]])\n    >>> m2 = torch.FloatTensor([[0.8,0,0,0],[0,0.8,0,0],[0,0,0.8,0]])\n    >>> m3 = torch.FloatTensor([[1.0,0,0,3],[0,1.0,0,3],[0,0,1.0,3]])\n    >>> matrix = torch.stack([m1,m2,m3])\n    >>> xx = F_batch_affine3d(x,matrix)\n    """"""\n    if matrix.dim() == 2:\n        matrix = matrix.view(-1, 3, 4)\n\n    A_batch = matrix[:, :3, :3]\n    b_batch = matrix[:, :3, 3].unsqueeze(1)\n\n    # make a meshgrid of normal coordinates\n    _coords = th_iterproduct(x.size(2), x.size(3), x.size(4))\n    with torch.no_grad:\n        coords = _coords.unsqueeze(0).repeat(x.size(0), 1, 1).float()\n\n    if center:\n        # shift the coordinates so center is the origin\n        coords[:, :, 0] = coords[:, :, 0] - (x.size(2) / 2. + 0.5)\n        coords[:, :, 1] = coords[:, :, 1] - (x.size(3) / 2. + 0.5)\n        coords[:, :, 2] = coords[:, :, 2] - (x.size(4) / 2. + 0.5)\n\n    # apply the coordinate transformation\n    new_coords = coords.bmm(A_batch.transpose(1, 2)) + b_batch.expand_as(coords)\n\n    if center:\n        # shift the coordinates back so origin is origin\n        new_coords[:, :, 0] = new_coords[:, :, 0] + (x.size(2) / 2. + 0.5)\n        new_coords[:, :, 1] = new_coords[:, :, 1] + (x.size(3) / 2. + 0.5)\n        new_coords[:, :, 2] = new_coords[:, :, 2] + (x.size(4) / 2. + 0.5)\n\n    # map new coordinates using bilinear interpolation\n    x_transformed = F_batch_trilinear_interp3d(x, new_coords)\n\n    return x_transformed\n\n\ndef F_batch_trilinear_interp3d(input, coords):\n    """"""\n    input : torch.Tensor\n        size = (N,H,W,C)\n    coords : torch.Tensor\n        size = (N,H*W*C,2)\n    """"""\n    x = torch.clamp(coords[:, :, 0], 0, input.size(2) - 2)\n    x0 = x.floor()\n    x1 = x0 + 1\n    y = torch.clamp(coords[:, :, 1], 0, input.size(3) - 2)\n    y0 = y.floor()\n    y1 = y0 + 1\n    z = torch.clamp(coords[:, :, 2], 0, input.size(4) - 2)\n    z0 = z.floor()\n    z1 = z0 + 1\n\n    stride = torch.LongTensor(input.stride())\n    x0_ix = x0.mul(stride[2]).long()\n    x1_ix = x1.mul(stride[2]).long()\n    y0_ix = y0.mul(stride[3]).long()\n    y1_ix = y1.mul(stride[3]).long()\n    z0_ix = z0.mul(stride[4]).long()\n    z1_ix = z1.mul(stride[4]).long()\n\n    input_flat = input.contiguous().view(input.size(0), -1)\n\n    vals_000 = input_flat.gather(1, x0_ix.add(y0_ix).add(z0_ix).detach())\n    vals_100 = input_flat.gather(1, x1_ix.add(y0_ix).add(z0_ix).detach())\n    vals_010 = input_flat.gather(1, x0_ix.add(y1_ix).add(z0_ix).detach())\n    vals_001 = input_flat.gather(1, x0_ix.add(y0_ix).add(z1_ix).detach())\n    vals_101 = input_flat.gather(1, x1_ix.add(y0_ix).add(z1_ix).detach())\n    vals_011 = input_flat.gather(1, x0_ix.add(y1_ix).add(z1_ix).detach())\n    vals_110 = input_flat.gather(1, x1_ix.add(y1_ix).add(z0_ix).detach())\n    vals_111 = input_flat.gather(1, x1_ix.add(y1_ix).add(z1_ix).detach())\n\n    xd = x - x0\n    yd = y - y0\n    zd = z - z0\n    xm = 1 - xd\n    ym = 1 - yd\n    zm = 1 - zd\n\n    x_mapped = (vals_000.mul(xm).mul(ym).mul(zm) +\n                vals_100.mul(xd).mul(ym).mul(zm) +\n                vals_010.mul(xm).mul(yd).mul(zm) +\n                vals_001.mul(xm).mul(ym).mul(zd) +\n                vals_101.mul(xd).mul(ym).mul(zd) +\n                vals_011.mul(xm).mul(yd).mul(zd) +\n                vals_110.mul(xd).mul(yd).mul(zm) +\n                vals_111.mul(xd).mul(yd).mul(zd))\n\n    return x_mapped.view_as(input)\n'"
pywick/functions/batchrenorm.py,14,"b""# Source: https://pastebin.com/31J3JD7r\n\nimport torch\nfrom torch.nn.modules import Module\nfrom torch.nn.parameter import Parameter\n\n\nclass BatchReNorm1d(Module):\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, rmax=3.0, dmax=5.0):\n        super(BatchReNorm1d, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n        self.affine = affine\n        self.rmax = rmax\n        self.dmax = dmax\n\n        if self.affine:\n            self.weight = Parameter(torch.Tensor(num_features))\n            self.bias = Parameter(torch.Tensor(num_features))\n        else:\n            self.register_parameter('weight', None)\n            self.register_parameter('bias', None)\n\n        self.register_buffer('running_mean', torch.zeros(num_features))\n        self.register_buffer('running_var', torch.ones(num_features))\n        self.register_buffer('r', torch.ones(num_features))\n        self.register_buffer('d', torch.zeros(num_features))\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        self.running_mean.zero_()\n        self.running_var.fill_(1)\n        self.r.fill_(1)\n        self.d.zero_()\n        if self.affine:\n            self.weight.data.uniform_()\n            self.bias.data.zero_()\n\n    def _check_input_dim(self, input):\n        if input.size(1) != self.running_mean.nelement():\n            raise ValueError('got {}-feature tensor, expected {}'\n                             .format(input.size(1), self.num_features))\n\n    def forward(self, input):\n        self._check_input_dim(input)\n        n = input.size()[0]\n\n        if self.training:\n            mean = torch.mean(input, dim=0)\n\n            sum = torch.sum((input - mean.expand_as(input)) ** 2, dim=0)\n            if sum == 0 and self.eps == 0:\n                invstd = 0.0\n            else:\n                invstd = 1. / torch.sqrt(sum / n + self.eps)\n            unbiased_var = sum / (n - 1)\n\n            self.r = torch.clamp(torch.sqrt(unbiased_var).data / torch.sqrt(self.running_var),\n                                 1. / self.rmax, self.rmax)\n            self.d = torch.clamp((mean.data - self.running_mean) / torch.sqrt(self.running_var),\n                                 -self.dmax, self.dmax)\n\n            r = self.r.expand_as(input)\n            d = self.d.expand_as(input)\n\n            input_normalized = (input - mean.expand_as(input)) * invstd.expand_as(input)\n\n            input_normalized = input_normalized * r + d\n\n            self.running_mean += self.momentum * (mean.data - self.running_mean)\n            self.running_var += self.momentum * (unbiased_var.data - self.running_var)\n\n            if not self.affine:\n                return input_normalized\n\n            output = input_normalized * self.weight.expand_as(input)\n            output += self.bias.unsqueeze(0).expand_as(input)\n\n            return output\n\n        else:\n            mean = self.running_mean.expand_as(input)\n            invstd = 1. / torch.sqrt(self.running_var.expand_as(input) + self.eps)\n\n            input_normalized = (input - mean.expand_as(input)) * invstd.expand_as(input)\n\n            if not self.affine:\n                return input_normalized\n\n            output = input_normalized * self.weight.expand_as(input)\n            output += self.bias.unsqueeze(0).expand_as(input)\n\n            return output\n\n    def __repr__(self):\n        return ('{name}({num_features}, eps={eps}, momentum={momentum},'\n                'affine={affine}, rmax={rmax}, dmax={dmax})'\n                .format(name=self.__class__.__name__, **self.__dict__))"""
pywick/functions/cyclicLR.py,5,"b'# Source: https://github.com/anandsaha/pytorch.cyclic.learning.rate (MIT)\n# Good description of how it functions is here: https://github.com/bckenstler/CLR\n\n# This code is from https://github.com/thomasjpfan/pytorch/blob/401ec389db2c9d2978917a6e4d1101b20340d7e7/torch/optim/lr_scheduler.py\n# This code is under review at PyTorch and is to be merged eventually to make CLR available to all.\n# Tested with pytorch 0.2.0\n\nfrom torch.optim.optimizer import Optimizer\nimport numpy as np\n\n\nclass CyclicLR(object):\n    """"""Sets the learning rate of each parameter group according to\n    cyclical learning rate policy (CLR). The policy cycles the learning\n    rate between two boundaries with a constant frequency, as detailed in\n    the paper `Cyclical Learning Rates for Training Neural Networks`_.\n    The distance between the two boundaries can be scaled on a per-iteration\n    or per-cycle basis.\n\n    Cyclical learning rate policy changes the learning rate after every batch.\n    `batch_step` should be called after a batch has been used for training.\n    To resume training, save `last_batch_iteration` and use it to instantiate `CycleLR`.\n\n    This class has three built-in policies, as put forth in the paper:\\n\n    `triangular`: A basic triangular cycle w/ no amplitude scaling.\\n\n    `triangular2`: A basic triangular cycle that scales initial amplitude by half each cycle.\\n\n    `exp_range`: A cycle that scales initial amplitude by gamma**(cycle iterations) at each cycle iteration.\n\n    This implementation was adapted from the github repo: `bckenstler/CLR`_\n\n    :param optimizer: (Optimizer): Wrapped optimizer.\n    :param base_lr: (float or list): Initial learning rate which is the\n        lower boundary in the cycle for each param groups.\n        Default: 0.001\n    :param max_lr: (float or list): Upper boundaries in the cycle for\n        each parameter group. Functionally,\n        it defines the cycle amplitude (max_lr - base_lr).\n        The lr at any cycle is the sum of base_lr\n        and some scaling of the amplitude; therefore\n        max_lr may not actually be reached depending on\n        scaling function. Default: 0.006\n    :param step_size: (int): Number of training iterations per\n        half cycle. Authors suggest setting step_size\n        2-8 x training iterations in epoch. Default: 2000\n    :param mode: (str): One of {triangular, triangular2, exp_range}.\n        Values correspond to policies detailed above.\n        If scale_fn is not None, this argument is ignored.\n        Default: \'triangular\'\n    :param gamma: (float): Constant in \'exp_range\' scaling function:\n        gamma**(cycle iterations)\n        Default: 1.0\n    :param scale_fn: (function): Custom scaling policy defined by a single\n        argument lambda function, where\n        0 <= scale_fn(x) <= 1 for all x >= 0.\n        mode paramater is ignored\n        Default: None\n    :param scale_mode: (str): {\'cycle\', \'iterations\'}.\n        Defines whether scale_fn is evaluated on\n        cycle number or cycle iterations (training\n        iterations since start of cycle).\n        Default: \'cycle\'\n    :param last_batch_iteration: (int): The index of the last batch. Default: -1\n\n    Example:\n        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n        >>> scheduler = torch.optim.CyclicLR(optimizer)\n        >>> data_loader = torch.utils.data.DataLoader(...)\n        >>> for epoch in range(10):\n        >>>     for batch in data_loader:\n        >>>         scheduler.batch_step()\n        >>>         train_batch(...)\n\n    .. _Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186\n    .. _bckenstler/CLR: https://github.com/bckenstler/CLR\n    """"""\n\n    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n                 step_size=2000, mode=\'triangular\', gamma=1.,\n                 scale_fn=None, scale_mode=\'cycle\', last_batch_iteration=-1):\n\n        if not isinstance(optimizer, Optimizer):\n            raise TypeError(\'{} is not an Optimizer\'.format(type(optimizer).__name__))\n        self.optimizer = optimizer\n\n        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n            if len(base_lr) != len(optimizer.param_groups):\n                raise ValueError(""expected {} base_lr, got {}"".format(len(optimizer.param_groups), len(base_lr)))\n            self.base_lrs = list(base_lr)\n        else:\n            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n\n        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n            if len(max_lr) != len(optimizer.param_groups):\n                raise ValueError(""expected {} max_lr, got {}"".format(len(optimizer.param_groups), len(max_lr)))\n            self.max_lrs = list(max_lr)\n        else:\n            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n\n        self.step_size = step_size\n\n        if mode not in [\'triangular\', \'triangular2\', \'exp_range\'] and scale_fn is None:\n            raise ValueError(\'mode is invalid and scale_fn is None\')\n\n        self.mode = mode\n        self.gamma = gamma\n\n        if scale_fn is None:\n            if self.mode == \'triangular\':\n                self.scale_fn = self._triangular_scale_fn\n                self.scale_mode = \'cycle\'\n            elif self.mode == \'triangular2\':\n                self.scale_fn = self._triangular2_scale_fn\n                self.scale_mode = \'cycle\'\n            elif self.mode == \'exp_range\':\n                self.scale_fn = self._exp_range_scale_fn\n                self.scale_mode = \'iterations\'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n\n        self.batch_step(last_batch_iteration + 1)\n        self.last_batch_iteration = last_batch_iteration\n\n    def batch_step(self, batch_iteration=None):\n        if batch_iteration is None:\n            batch_iteration = self.last_batch_iteration + 1\n        self.last_batch_iteration = batch_iteration\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n            param_group[\'lr\'] = lr\n\n    def _triangular_scale_fn(self, x):\n        return 1.\n\n    def _triangular2_scale_fn(self, x):\n        return 1 / (2. ** (x - 1))\n\n    def _exp_range_scale_fn(self, x):\n        return self.gamma**(x)\n\n    def get_lr(self):\n        step_size = float(self.step_size)\n        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n\n        lrs = []\n        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n        for param_group, base_lr, max_lr in param_lrs:\n            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n            if self.scale_mode == \'cycle\':\n                lr = base_lr + base_height * self.scale_fn(cycle)\n            else:\n                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n            lrs.append(lr)\n        return lrs\n'"
pywick/functions/group_norm.py,4,"b'# Source: https://github.com/chengyangfu/pytorch-groupnormalization\n\'\'\'\nThis file is modified from the Instance Norm implementation in PyTorch\n\'\'\'\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.modules.batchnorm import _BatchNorm\n\ndef group_norm(input, group, running_mean, running_var, weight=None, bias=None,\n                  use_input_stats=True, momentum=0.1, eps=1e-5):\n    r""""""Applies Group Normalization for channels in the same group in each data sample in a\n    batch.\n\n    See :class:`~torch.nn.GroupNorm2d`, for details.\n    """"""\n    if not use_input_stats and (running_mean is None or running_var is None):\n        raise ValueError(\'Expected running_mean and running_var to be not None when use_input_stats=False\')\n\n    b, c = input.size(0), input.size(1)\n    if weight is not None:\n        weight = weight.repeat(b)\n    if bias is not None:\n        bias = bias.repeat(b)\n\n    def _group_norm(input, group, running_mean=None, running_var=None, weight=None,\n                       bias=None, use_input_stats=None, momentum=None, eps=None):\n        # Repeat stored stats and affine transform params if necessary\n        if running_mean is not None:\n            running_mean_orig = running_mean\n            running_mean = running_mean_orig.repeat(b)\n        if running_var is not None:\n            running_var_orig = running_var\n            running_var = running_var_orig.repeat(b)\n\n        #norm_shape = [1, b * c / group, group]\n        #print(norm_shape)\n        # Apply group norm\n        input_reshaped = input.contiguous().view(1, int(b * c/group), group, *input.size()[2:])\n\n        out = F.batch_norm(\n            input_reshaped, running_mean, running_var, weight=weight, bias=bias,\n            training=use_input_stats, momentum=momentum, eps=eps)\n\n        # Reshape back\n        if running_mean is not None:\n            running_mean_orig.copy_(running_mean.view(b, int(c/group)).mean(0, keepdim=False))\n        if running_var is not None:\n            running_var_orig.copy_(running_var.view(b, int(c/group)).mean(0, keepdim=False))\n\n        return out.view(b, c, *input.size()[2:])\n    return _group_norm(input, group, running_mean=running_mean,\n                          running_var=running_var, weight=weight, bias=bias,\n                          use_input_stats=use_input_stats, momentum=momentum,\n                          eps=eps)\n\n\nclass _GroupNorm(_BatchNorm):\n    def __init__(self, num_features, num_groups=1, eps=1e-5, momentum=0.1,\n                 affine=False, track_running_stats=False):\n        self.num_groups = num_groups\n        self.track_running_stats = track_running_stats\n        super(_GroupNorm, self).__init__(int(num_features/num_groups), eps,\n                                         momentum, affine)\n\n    def _check_input_dim(self, input):\n        return NotImplemented\n\n    def forward(self, input):\n        self._check_input_dim(input)\n\n        return group_norm(\n            input, self.num_groups, self.running_mean, self.running_var, self.weight, self.bias,\n            self.training or not self.track_running_stats, self.momentum, self.eps)\n\n\nclass GroupNorm2d(_GroupNorm):\n    r""""""Applies Group Normalization over a 4D input (a mini-batch of 2D inputs\n    with additional channel dimension) as described in the paper\n    https://arxiv.org/abs/1803.08494\n    `Group Normalization`_ .\n\n    Args:\n        num_features: :math:`C` from an expected input of size\n            :math:`(N, C, H, W)`\n        num_groups: number of channels in a group.\n        eps: a value added to the denominator for numerical stability. Default: 1e-5\n        momentum: the value used for the running_mean and running_var computation. Default: 0.1\n        affine: a boolean value that when set to ``True``, this module has\n            learnable affine parameters. Default: ``True``\n        track_running_stats: a boolean value that when set to ``True``, this\n            module tracks the running mean and variance, and when set to ``False``,\n            this module does not track such statistics and always uses batch\n            statistics in both training and eval modes. Default: ``False``\n\n    Shape:\n        - Input: :math:`(N, C, H, W)`\n        - Output: :math:`(N, C, H, W)` (same shape as input)\n\n    Examples:\n        >>> # Without Learnable Parameters\n        >>> m = nn.GroupNorm2d(100, 4)\n        >>> # With Learnable Parameters\n        >>> m = nn.GroupNorm2d(100, 4, affine=True)\n        >>> input = torch.randn(20, 100, 35, 45)\n        >>> output = m(input)\n\n    """"""\n\n    def _check_input_dim(self, input):\n        if input.dim() != 4:\n            raise ValueError(\'expected 4D input (got {}D input)\'\n                             .format(input.dim()))\n\n'"
pywick/functions/mish.py,2,"b'# Source: https://github.com/rwightman/gen-efficientnet-pytorch/blob/master/geffnet/activations/activations.py (Apache 2.0)\n# Note. Cuda-compiled source can be found here: https://github.com/thomasbrandon/mish-cuda (MIT)\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef mish(x, inplace: bool = False):\n    """"""Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\n    """"""\n    return x.mul(F.softplus(x).tanh())\n\nclass Mish(nn.Module):\n    """"""\n        Mish - ""Mish: A Self Regularized Non-Monotonic Neural Activation Function""\n        https://arxiv.org/abs/1908.08681v1\n        implemented for PyTorch / FastAI by lessw2020\n        github: https://github.com/lessw2020/mish\n    """"""\n    def __init__(self, inplace: bool = False):\n        super(Mish, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return mish(x, self.inplace)\n'"
pywick/functions/swish.py,2,"b'# Source: https://forums.fast.ai/t/implementing-new-activation-functions-in-fastai-library/17697\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Swish(nn.Module):\n    """"""\n    Swish activation function, a special case of ARiA,\n    for ARiA = f(x, 1, 0, 1, 1, b, 1)\n    """"""\n\n    def __init__(self, b = 1.):\n        super(Swish, self).__init__()\n        self.b = b\n\n    def forward(self, x):\n        sigmoid = F.sigmoid(x) ** self.b\n        return x * sigmoid\n\n\nclass Aria(nn.Module):\n    """"""\n    Aria activation function described in `this paper <https://arxiv.org/abs/1805.08878/>`_.\n    """"""\n\n    def __init__(self, A=0, K=1., B = 1., v=1., C=1., Q=1.):\n        super(Aria, self).__init__()\n        # ARiA parameters\n        self.A = A # lower asymptote, values tested were A = -1, 0, 1\n        self.k = K # upper asymptote, values tested were K = 1, 2\n        self.B = B # exponential rate, values tested were B = [0.5, 2]\n        self.v = v # v > 0 the direction of growth, values tested were v = [0.5, 2]\n        self.C = C # constant set to 1\n        self.Q = Q # related to initial value, values tested were Q = [0.5, 2]\n\n    def forward(self, x):\n        aria = self.A + (self.k - self.A) / ((self.C + self.Q * F.exp(-x) ** self.B) ** (1/self.v))\n        return x * aria\n\n\nclass Aria2(nn.Module):\n    """"""\n    ARiA2 activation function, a special case of ARiA, for ARiA = f(x, 1, 0, 1, 1, b, 1/a)\n    """"""\n\n    def __init__(self, a=1.5, b = 2.):\n        super(Aria2, self).__init__()\n        self.a = a\n        self.b = b\n\n    def forward(self, x):\n        aria2 = 1 + ((F.exp(-x) ** self.b) ** (-self.a))\n        return x * aria2\n\n\n# Source: https://github.com/rwightman/gen-efficientnet-pytorch/blob/master/geffnet/activations/activations.py (Apache 2.0)\ndef hard_swish(x, inplace: bool = False):\n    inner = F.relu6(x + 3.).div_(6.)\n    return x.mul_(inner) if inplace else x.mul(inner)\n\n\nclass HardSwish(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(HardSwish, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return hard_swish(x, self.inplace)\n'"
pywick/gridsearch/__init__.py,0,"b'""""""\nWhen trying to find the right hyperparameters for your neural network, sometimes you just have to do a lot of trial and error.\nCurrently, our Gridsearch implementation is pretty basic, but it allows you to supply ranges of input values for various\nmetaparameters and then executes training runs in either random or sequential fashion.\\n\nWarning: this class is a bit underdeveloped. Tread with care.\n""""""\n\nfrom .gridsearch import GridSearch\nfrom .pipeline import Pipeline'"
pywick/gridsearch/grid_test.py,0,"b'import json\nfrom .gridsearch import GridSearch\n\nmy_args = {\n    \'shape\': \'+plus+\',\n    \'animal\':[\'cat\', \'mouse\', \'dog\'],\n    \'number\':[4, 5, 6],\n    \'device\':[\'CUP\', \'MUG\', \'TPOT\'],\n    \'flower\' : \'=Rose=\'\n}\n\ndef tryme(args_dict):\n    print(json.dumps(args_dict, indent=4))\n\ndef tryme_vars(animal=\'\', number=0, device=\'\', shape=\'\', flower=\'\'):\n    print(animal + "" : "" + str(number) + "" : "" + device + "" : "" + shape + "" : "" + flower)\n\ndef main():\n    grids = GridSearch(tryme, grid_params=my_args, search_behavior=\'exhaustive\', args_as_dict=True)\n\n    print(\'-------- INITIAL SETTINGS ---------\')\n    tryme(my_args)\n    print(\'-------------- END ----------------\')\n    print(\'-------------- --- ----------------\')\n    print()\n    print(\'+++++++++++ Dict Result ++++++++++\')\n    grids.run()\n    print(\'+++++++++++++ End Dict Result +++++++++++\\n\\n\')\n\n    grids = GridSearch(tryme_vars, grid_params=my_args, search_behavior=\'sampled_0.5\', args_as_dict=False)\n    print(\'========== Vars Result ==========\')\n    grids.run()\n    print(\'========== End Vars Result ==========\')\n    # exit()\n\nif __name__ == \'__main__\':\n    main()\n'"
pywick/gridsearch/gridsearch.py,0,"b'import random\nimport collections\n\nclass GridSearch(object):\n    """"""\n    Simple GridSearch to apply to a generic function\n\n    :param function: (function):\n        function to perform grid search on\n    :param grid_params: (dict):\n        dictionary mapping variable names to lists of possible inputs aka..\\n\n        {\'input_a\':[\'dog\', \'cat\', \'stuff\'],\n        \'input_b\':[3, 10, 22]}\n    :param search_behavior: (string):\n        how to perform the search.\n        Options are: \'exhaustive\', \'sampled_x.x\' (where `x.x` is sample threshold 0.0 < 1.0)\\n\n        `exhaustive` - try every parameter in order they are specified in the dictionary (last key gets all its values searched first)\\n\n        `sampled`    - sample from the dictionary of params with specified threshold. The random tries *below* the threshold will be executed\n    :param args_as_dict: (bool):\n        There are two ways to pass parameters into a function:\\n\n        1. Simply use each key in grid_params as a variable to pass to the function (and change those variable values according\n        to the mapping inside grid_params)\\n\n        2. Pass a single dictionary to the function where the keys of the dictionary themselves are changed according to the\n        grid_params\\n\n        defaults to dict\n    """"""\n    def __init__(self, function, grid_params, search_behavior=\'exhaustive\', args_as_dict=True):\n        self.func = function\n        self.args = grid_params\n        self.sampled_thresh = 1.0\n\n        if \'sampled_\' in search_behavior:\n            behaviors = search_behavior.split(\'_\')\n            self.behavior = behaviors[0]\n            self.sampled_thresh = float(behaviors[1])\n        else:\n            self.behavior = search_behavior\n        self.args_as_dict = args_as_dict\n\n    def _execute(self, input_args, available_args):\n        """"""\n        Recursively reduce parameters and finally execute the function when all params have been selected\n        :param input_args:\n            dictionary into which to collect input arguments (used in the recursive call to keep just the needed params)\n        :param available_args:\n            list of available (arg_name, arg_values) tuples for the rest of the arguments\n        """"""\n\n        if len(available_args) == 0:  # We\'ve reached the bottom of the recursive stack, execute function\n            doExecute = True\n            if self.behavior == \'sampled\':\n                if random.random() > self.sampled_thresh:\n                    doExecute = False\n\n            if doExecute:\n                if self.args_as_dict:  # this passes ONE argument to the function which is the dictionary\n                    self.func(input_args)\n                else:\n                    self.func(**input_args)  # this calls the function with arguments specified in the dictionary\n\n        # get all keys\n        keys = available_args.keys()\n        keys_to_remove = list()\n\n        for i, key in enumerate(keys):\n            values = available_args.get(key)\n\n            # this is a list of possible inputs so iterate over it. Strings are iterable in python so filter out\n            if isinstance(values, collections.Iterable) and not isinstance(values, str):\n                # first, augment available_args so it no longer contains keys that we have already carried over\n                keys_to_remove.append(key)\n                for k in keys_to_remove:\n                    available_args.pop(k)\n\n                for value in values:\n                    input_args[key] = value\n                    self._execute(input_args, available_args)\n\n                available_args[key] = values  # replace values so they can be used in the next iterative call\n                break    # don\'t do any more iterations after we handled the first key with multiple choices\n            else:\n                input_args[key] = values\n                keys_to_remove.append(key)\n                if (i+1) == len(keys):        # we\'ve reached the final item in the available args\n                    self._execute(input_args, dict())\n\n    def run(self):\n        """"""\n        Runs GridSearch by iterating over options as specified\n        :return:\n        """"""\n\n        input_args = dict()\n        self._execute(input_args, self.args)\n'"
pywick/gridsearch/pipeline.py,0,"b'def merge_dicts(*dict_args):\n    """"""\n    Given any number of dicts, shallow copy and merge into a new dict,\n    precedence goes to key value pairs in latter dicts.\n    """"""\n    result = {}\n    for dictionary in dict_args:\n        result.update(dictionary)\n    return result\n\nclass Pipeline(object):\n    """"""\n    Defines a pipeline for operating on data. Output of first function will be passed to the second and so forth.\n\n    :param ordered_func_list: (list):\n        list of functions to call\n    :param func_args: (dict):\n        optional dictionary of params to pass to functions in addition to last output\n        the dictionary should be in the form of:\n        func_name: list(params)\n    """"""\n\n    def __init__(self, ordered_func_list, func_args=None):\n        self.pipes = ordered_func_list\n        self.func_args = func_args\n        self.output = None\n\n    def call(self, input):\n        """"""Apply the functions in current Pipeline to an input.\n\n        :param input: The input to process with the Pipeline.\n        """"""\n        out = input\n        for pipe in self.pipes:\n            if pipe.__name__ in self.func_args:     # if additional arguments present\n                all_args = self.func_args[pipe.__name__]\n                all_args.insert(0, out)\n            else:\n                all_args = list(out)\n            out = pipe(*all_args)       # pass list to the function to be executed\n        return out\n\n    def add_before(self, func, args_dict=None):\n        """"""\n        Add a function to be applied before the rest in the pipeline\n\n        :param func: The function to apply\n        """"""\n        if args_dict:       # update args dictionary if necessary\n            self.func_args = merge_dicts(self.func_args, args_dict)\n\n        self.pipes.insert(0, func)\n        return self\n\n    def add_after(self, func, args_dict=None):\n        """"""\n        Add a function to be applied at the end of the pipeline\n\n        :param func: The function to apply\n        """"""\n        if args_dict:       # update args dictionary if necessary\n            self.func_args = merge_dicts(self.func_args, args_dict)\n\n        self.pipes.append(func)\n        return self\n\n    @staticmethod\n    def identity(x):\n        """"""Return a copy of the input.\n\n        This is here for serialization compatibility with pickle.\n        """"""\n        return x\n'"
pywick/meters/__init__.py,0,"b'""""""\nMeters are used to accumulate values over time or batch and generally provide some statistical measure of your process.\n""""""\n\nfrom pywick.meters.averagemeter import AverageMeter\nfrom pywick.meters.averagevaluemeter import AverageValueMeter\nfrom pywick.meters.classerrormeter import ClassErrorMeter\nfrom pywick.meters.confusionmeter import ConfusionMeter\nfrom pywick.meters.timemeter import TimeMeter\nfrom pywick.meters.msemeter import MSEMeter\nfrom pywick.meters.movingaveragevaluemeter import MovingAverageValueMeter\nfrom pywick.meters.aucmeter import AUCMeter\nfrom pywick.meters.apmeter import APMeter\nfrom pywick.meters.mapmeter import mAPMeter\n'"
pywick/meters/apmeter.py,15,"b'import math\nfrom . import meter\nimport torch\n\n\nclass APMeter(meter.Meter):\n    """"""\n    The APMeter measures the average precision per class.\n\n    The APMeter is designed to operate on `NxK` Tensors `output` and\n    `target`, and optionally a `Nx1` Tensor weight where (1) the `output`\n    contains model output scores for `N` examples and `K` classes that ought to\n    be higher when the model is more convinced that the example should be\n    positively labeled, and smaller when the model believes the example should\n    be negatively labeled (for instance, the output of a sigmoid function); (2)\n    the `target` contains only values 0 (for negative examples) and 1\n    (for positive examples); and (3) the `weight` ( > 0) represents weight for\n    each sample.\n\n    """"""\n\n    def __init__(self):\n        super(APMeter, self).__init__()\n        self.reset()\n\n    def reset(self):\n        """"""Resets the meter with empty member variables""""""\n        self.scores = torch.FloatTensor(torch.FloatStorage())\n        self.targets = torch.LongTensor(torch.LongStorage())\n        self.weights = torch.FloatTensor(torch.FloatStorage())\n\n    def add(self, output, target, weight=None):\n        """"""Add a new observation\n\n        Args:\n            output (Tensor): NxK tensor that for each of the N examples\n                indicates the probability of the example belonging to each of\n                the K classes, according to the model. The probabilities should\n                sum to one over all classes\n            target (Tensor): binary NxK tensort that encodes which of the K\n                classes are associated with the N-th input\n                (eg: a row [0, 1, 0, 1] indicates that the example is\n                associated with classes 2 and 4)\n            weight (optional, Tensor): Nx1 tensor representing the weight for\n                each example (each weight > 0)\n\n        """"""\n        if not torch.is_tensor(output):\n            output = torch.from_numpy(output)\n        if not torch.is_tensor(target):\n            target = torch.from_numpy(target)\n\n        if weight is not None:\n            if not torch.is_tensor(weight):\n                weight = torch.from_numpy(weight)\n            weight = weight.squeeze()\n        if output.dim() == 1:\n            output = output.view(-1, 1)\n        else:\n            assert output.dim() == 2, \\\n                \'wrong output size (should be 1D or 2D with one column \\\n                per class)\'\n        if target.dim() == 1:\n            target = target.view(-1, 1)\n        else:\n            assert target.dim() == 2, \\\n                \'wrong target size (should be 1D or 2D with one column \\\n                per class)\'\n        if weight is not None:\n            assert weight.dim() == 1, \'Weight dimension should be 1\'\n            assert weight.numel() == target.size(0), \\\n                \'Weight dimension 1 should be the same as that of target\'\n            assert torch.min(weight) >= 0, \'Weight should be non-negative only\'\n        assert torch.equal(target**2, target), \\\n            \'targets should be binary (0 or 1)\'\n        if self.scores.numel() > 0:\n            assert target.size(1) == self.targets.size(1), \\\n                \'dimensions for output should match previously added examples.\'\n\n        # make sure storage is of sufficient size\n        if self.scores.storage().size() < self.scores.numel() + output.numel():\n            new_size = math.ceil(self.scores.storage().size() * 1.5)\n            new_weight_size = math.ceil(self.weights.storage().size() * 1.5)\n            self.scores.storage().resize_(int(new_size + output.numel()))\n            self.targets.storage().resize_(int(new_size + output.numel()))\n            if weight is not None:\n                self.weights.storage().resize_(int(new_weight_size + output.size(0)))\n\n        # store scores and targets\n        offset = self.scores.size(0) if self.scores.dim() > 0 else 0\n        self.scores.resize_(offset + output.size(0), output.size(1))\n        self.targets.resize_(offset + target.size(0), target.size(1))\n        self.scores.narrow(0, offset, output.size(0)).copy_(output)\n        self.targets.narrow(0, offset, target.size(0)).copy_(target)\n\n        if weight is not None:\n            self.weights.resize_(offset + weight.size(0))\n            self.weights.narrow(0, offset, weight.size(0)).copy_(weight)\n\n    def value(self):\n        """"""Returns the model\'s average precision for each class\n\n        Return:\n            ap (FloatTensor): 1xK tensor, with avg precision for each class k\n\n        """"""\n\n        if self.scores.numel() == 0:\n            return 0\n        ap = torch.zeros(self.scores.size(1))\n        if hasattr(torch, ""arange""):\n            rg = torch.arange(1, self.scores.size(0) + 1).float()\n        else:\n            rg = torch.range(1, self.scores.size(0)).float()\n        if self.weights.numel() > 0:\n            weight = self.weights.new(self.weights.size())\n            weighted_truth = self.weights.new(self.weights.size())\n\n        # compute average precision for each class\n        for k in range(self.scores.size(1)):\n            # sort scores\n            scores = self.scores[:, k]\n            targets = self.targets[:, k]\n            _, sortind = torch.sort(scores, 0, True)\n            truth = targets[sortind]\n            if self.weights.numel() > 0:\n                weight = self.weights[sortind]\n                weighted_truth = truth.float() * weight\n                rg = weight.cumsum(0)\n\n            # compute true positive sums\n            if self.weights.numel() > 0:\n                tp = weighted_truth.cumsum(0)\n            else:\n                tp = truth.float().cumsum(0)\n\n            # compute precision curve\n            precision = tp.div(rg)\n\n            # compute average precision\n            ap[k] = precision[truth.byte()].sum() / max(float(truth.sum()), 1)\n        return ap\n'"
pywick/meters/aucmeter.py,5,"b'import numbers\nfrom . import meter\nimport numpy as np\nimport torch\n\n\nclass AUCMeter(meter.Meter):\n    """"""\n    The AUCMeter measures the area under the receiver-operating characteristic\n    (ROC) curve for binary classification problems. The area under the curve (AUC)\n    can be interpreted as the probability that, given a randomly selected positive\n    example and a randomly selected negative example, the positive example is\n    assigned a higher score by the classification model than the negative example.\n\n    The AUCMeter is designed to operate on one-dimensional Tensors `output`\n    and `target`, where (1) the `output` contains model output scores that ought to\n    be higher when the model is more convinced that the example should be positively\n    labeled, and smaller when the model believes the example should be negatively\n    labeled (for instance, the output of a signoid function); and (2) the `target`\n    contains only values 0 (for negative examples) and 1 (for positive examples).\n    """"""\n    def __init__(self):\n        super(AUCMeter, self).__init__()\n        self.reset()\n\n    def reset(self):\n        self.scores = torch.DoubleTensor(torch.DoubleStorage()).numpy()\n        self.targets = torch.LongTensor(torch.LongStorage()).numpy()\n\n    def add(self, output, target):\n        if torch.is_tensor(output):\n            output = output.cpu().squeeze().numpy()\n        if torch.is_tensor(target):\n            target = target.cpu().squeeze().numpy()\n        elif isinstance(target, numbers.Number):\n            target = np.asarray([target])\n        assert np.ndim(output) == 1, \\\n            \'wrong output size (1D expected)\'\n        assert np.ndim(target) == 1, \\\n            \'wrong target size (1D expected)\'\n        assert output.shape[0] == target.shape[0], \\\n            \'number of outputs and targets does not match\'\n        assert np.all(np.add(np.equal(target, 1), np.equal(target, 0))), \\\n            \'targets should be binary (0, 1)\'\n\n        self.scores = np.append(self.scores, output)\n        self.targets = np.append(self.targets, target)\n\n    def value(self):\n        # case when number of elements added are 0\n        if self.scores.shape[0] == 0:\n            return 0.5\n\n        # sorting the arrays\n        scores, sortind = torch.sort(torch.from_numpy(self.scores), dim=0, descending=True)\n        scores = scores.numpy()\n        sortind = sortind.numpy()\n\n        # creating the roc curve\n        tpr = np.zeros(shape=(scores.size + 1), dtype=np.float64)\n        fpr = np.zeros(shape=(scores.size + 1), dtype=np.float64)\n\n        for i in range(1, scores.size + 1):\n            if self.targets[sortind[i - 1]] == 1:\n                tpr[i] = tpr[i - 1] + 1\n                fpr[i] = fpr[i - 1]\n            else:\n                tpr[i] = tpr[i - 1]\n                fpr[i] = fpr[i - 1] + 1\n\n        tpr /= (self.targets.sum() * 1.0)\n        fpr /= ((self.targets - 1.0).sum() * -1.0)\n\n        # calculating area under curve using trapezoidal rule\n        n = tpr.shape[0]\n        h = fpr[1:n] - fpr[0:n - 1]\n        sum_h = np.zeros(fpr.shape)\n        sum_h[0:n - 1] = h\n        sum_h[1:n] += h\n        area = (sum_h * tpr).sum() / 2.0\n\n        return (area, tpr, fpr)\n'"
pywick/meters/averagemeter.py,0,"b'class AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count'"
pywick/meters/averagevaluemeter.py,0,"b'import numpy as np\n\nfrom . import meter\n\n\nclass AverageValueMeter(meter.Meter):\n    """"""\n    Keeps track of mean and standard deviation for some value.\n    """"""\n    def __init__(self):\n        super(AverageValueMeter, self).__init__()\n        self.reset()\n        self.val = 0\n\n    def add(self, value, n=1):\n        self.val = value\n        self.sum += value\n        self.var += value * value\n        self.n += n\n\n        if self.n == 0:\n            self.mean, self.std = np.nan, np.nan\n        elif self.n == 1:\n            self.mean = 0.0 + self.sum  # This is to force a copy in torch/numpy\n            self.std = np.inf\n            self.mean_old = self.mean\n            self.m_s = 0.0\n        else:\n            self.mean = self.mean_old + (value - n * self.mean_old) / float(self.n)\n            self.m_s += (value - self.mean_old) * (value - self.mean)\n            self.mean_old = self.mean\n            self.std = np.sqrt(self.m_s / (self.n - 1.0))\n\n    def value(self):\n        return self.mean, self.std\n\n    def reset(self):\n        self.n = 0\n        self.sum = 0.0\n        self.var = 0.0\n        self.val = 0.0\n        self.mean = np.nan\n        self.mean_old = 0.0\n        self.m_s = 0.0\n        self.std = np.nan\n'"
pywick/meters/classerrormeter.py,3,"b""import numpy as np\nimport torch\nimport numbers\nfrom . import meter\n\n\nclass ClassErrorMeter(meter.Meter):\n    def __init__(self, topk=[1], accuracy=False):\n        super(ClassErrorMeter, self).__init__()\n        self.topk = np.sort(topk)\n        self.accuracy = accuracy\n        self.reset()\n\n    def reset(self):\n        self.sum = {v: 0 for v in self.topk}\n        self.n = 0\n\n    def add(self, output, target):\n        if torch.is_tensor(output):\n            output = output.cpu().squeeze().numpy()\n        if torch.is_tensor(target):\n            target = np.atleast_1d(target.cpu().squeeze().numpy())\n        elif isinstance(target, numbers.Number):\n            target = np.asarray([target])\n        if np.ndim(output) == 1:\n            output = output[np.newaxis]\n        else:\n            assert np.ndim(output) == 2, \\\n                'wrong output size (1D or 2D expected)'\n            assert np.ndim(target) == 1, \\\n                'target and output do not match'\n        assert target.shape[0] == output.shape[0], \\\n            'target and output do not match'\n        topk = self.topk\n        maxk = int(topk[-1])  # seems like Python3 wants int and not np.int64\n        no = output.shape[0]\n\n        pred = torch.from_numpy(output).topk(maxk, 1, True, True)[1].numpy()\n        correct = pred == target[:, np.newaxis].repeat(pred.shape[1], 1)\n\n        for k in topk:\n            self.sum[k] += no - correct[:, 0:k].sum()\n        self.n += no\n\n    def value(self, k=-1):\n        if k != -1:\n            assert k in self.sum.keys(), \\\n                'invalid k (this k was not provided at construction time)'\n            if self.accuracy:\n                return (1. - float(self.sum[k]) / self.n) * 100.0\n            else:\n                return float(self.sum[k]) / self.n * 100.0\n        else:\n            return [self.value(k_) for k_ in self.topk]\n"""
pywick/meters/confusionmeter.py,0,"b'from . import meter\nimport numpy as np\n\n\nclass ConfusionMeter(meter.Meter):\n    """"""Maintains a confusion matrix for a given classification problem.\n\n    The ConfusionMeter constructs a confusion matrix for a multi-class\n    classification problems. It does not support multi-label, multi-class problems:\n    for such problems, please use MultiLabelConfusionMeter.\n\n    :param k (int): number of classes in the classification problem\n    :param normalized (boolean): Determines whether or not the confusion matrix is normalized or not\n\n    """"""\n\n    def __init__(self, k, normalized=False):\n        super(ConfusionMeter, self).__init__()\n        self.conf = np.ndarray((k, k), dtype=np.int32)\n        self.normalized = normalized\n        self.k = k\n        self.reset()\n\n    def reset(self):\n        self.conf.fill(0)\n\n    def add(self, predicted, target):\n        """"""Computes the confusion matrix of K x K size where K is no of classes\n\n        :param predicted (tensor): Can be an N x K tensor of predicted scores obtained from\n            the model for N examples and K classes or an N-tensor of\n            integer values between 0 and K-1.\n        :param target (tensor): Can be a N-tensor of integer values assumed to be integer\n            values between 0 and K-1 or N x K tensor, where targets are\n            assumed to be provided as one-hot vectors\n\n        """"""\n        predicted = predicted.cpu().numpy()\n        target = target.cpu().numpy()\n\n        assert predicted.shape[0] == target.shape[0], \\\n            \'number of targets and predicted outputs do not match\'\n\n        if np.ndim(predicted) != 1:\n            assert predicted.shape[1] == self.k, \\\n                \'number of predictions does not match size of confusion matrix\'\n            predicted = np.argmax(predicted, 1)\n        else:\n            assert (predicted.max() < self.k) and (predicted.min() >= 0), \\\n                \'predicted values are not between 1 and k\'\n\n        onehot_target = np.ndim(target) != 1\n        if onehot_target:\n            assert target.shape[1] == self.k, \\\n                \'Onehot target does not match size of confusion matrix\'\n            assert (target >= 0).all() and (target <= 1).all(), \\\n                \'in one-hot encoding, target values should be 0 or 1\'\n            assert (target.sum(1) == 1).all(), \\\n                \'multi-label setting is not supported\'\n            target = np.argmax(target, 1)\n        else:\n            assert (predicted.max() < self.k) and (predicted.min() >= 0), \\\n                \'predicted values are not between 0 and k-1\'\n\n        # hack for bincounting 2 arrays together\n        x = predicted + self.k * target\n        bincount_2d = np.bincount(x.astype(np.int32),\n                                  minlength=self.k ** 2)\n        assert bincount_2d.size == self.k ** 2\n        conf = bincount_2d.reshape((self.k, self.k))\n\n        self.conf += conf\n\n    def value(self):\n        """"""\n        Returns:\n            Confustion matrix of K rows and K columns, where rows corresponds\n            to ground-truth targets and columns corresponds to predicted\n            targets.\n        """"""\n        if self.normalized:\n            conf = self.conf.astype(np.float32)\n            return conf / conf.sum(1).clip(min=1e-12)[:, None]\n        else:\n            return self.conf\n'"
pywick/meters/mapmeter.py,0,"b'from . import meter, APMeter\n\nclass mAPMeter(meter.Meter):\n    """"""\n    The mAPMeter measures the mean average precision over all classes.\n\n    The mAPMeter is designed to operate on `NxK` Tensors `output` and\n    `target`, and optionally a `Nx1` Tensor weight where (1) the `output`\n    contains model output scores for `N` examples and `K` classes that ought to\n    be higher when the model is more convinced that the example should be\n    positively labeled, and smaller when the model believes the example should\n    be negatively labeled (for instance, the output of a sigmoid function); (2)\n    the `target` contains only values 0 (for negative examples) and 1\n    (for positive examples); and (3) the `weight` ( > 0) represents weight for\n    each sample.\n    """"""\n    def __init__(self):\n        super(mAPMeter, self).__init__()\n        self.apmeter = APMeter()\n\n    def reset(self):\n        self.apmeter.reset()\n\n    def add(self, output, target, weight=None):\n        self.apmeter.add(output, target, weight)\n\n    def value(self):\n        return self.apmeter.value().mean()\n'"
pywick/meters/meter.py,0,"b'\nclass Meter(object):\n    """"""\n    Abstract meter class from which all other meters inherit\n    """"""\n    def reset(self):\n        pass\n\n    def add(self):\n        pass\n\n    def value(self):\n        pass\n'"
pywick/meters/movingaveragevaluemeter.py,1,"b'import math\nfrom . import meter\nimport torch\n\n\nclass MovingAverageValueMeter(meter.Meter):\n    """"""\n    Keeps track of mean and standard deviation of some value for a given window.\n    """"""\n    def __init__(self, windowsize):\n        super(MovingAverageValueMeter, self).__init__()\n        self.windowsize = windowsize\n        self.valuequeue = torch.Tensor(windowsize)\n        self.reset()\n\n    def reset(self):\n        self.sum = 0.0\n        self.n = 0\n        self.var = 0.0\n        self.valuequeue.fill_(0)\n\n    def add(self, value):\n        queueid = (self.n % self.windowsize)\n        oldvalue = self.valuequeue[queueid]\n        self.sum += value - oldvalue\n        self.var += value * value - oldvalue * oldvalue\n        self.valuequeue[queueid] = value\n        self.n += 1\n\n    def value(self):\n        n = min(self.n, self.windowsize)\n        mean = self.sum / max(1, n)\n        std = math.sqrt(max((self.var - n * mean * mean) / max(1, n-1), 0))\n        return mean, std\n\n'"
pywick/meters/msemeter.py,4,"b'import math\nfrom . import meter\nimport torch\n\n\nclass MSEMeter(meter.Meter):\n    def __init__(self, root=False):\n        super(MSEMeter, self).__init__()\n        self.reset()\n        self.root = root\n\n    def reset(self):\n        self.n = 0\n        self.sesum = 0.0\n\n    def add(self, output, target):\n        if not torch.is_tensor(output) and not torch.is_tensor(target):\n            output = torch.from_numpy(output)\n            target = torch.from_numpy(target)\n        self.n += output.numel()\n        self.sesum += torch.sum((output - target) ** 2)\n\n    def value(self):\n        mse = self.sesum / max(1, self.n)\n        return math.sqrt(mse) if self.root else mse\n'"
pywick/meters/timemeter.py,0,"b'import time\nfrom . import meter\n\nclass TimeMeter(meter.Meter):\n    """"""\n    This meter is designed to measure the time between events and can be\n    used to measure, for instance, the average processing time per batch of data.\n    It is different from most other meters in terms of the methods it provides:\n\n    Mmethods:\n\n       * `reset()` resets the timer, setting the timer and unit counter to zero.\n       * `value()` returns the time passed since the last `reset()`; divided by the counter value when `unit=true`.\n    """"""\n    def __init__(self, unit):\n        super(TimeMeter, self).__init__()\n        self.unit = unit\n        self.reset()\n\n    def reset(self):\n        self.n = 0\n        self.time = time.time()\n\n    def value(self):\n        return time.time() - self.time\n'"
pywick/models/__init__.py,1,"b'""""""\nNeural network models is what deep learning is all about! While you can download some standard models from\n`torchvision <https://pytorch.org/docs/stable/torchvision/models.html/>`_, we strive to create a library of models\nthat are on the cutting edge of AI. Whenever possible, `we provide pretrained solutions as well!`\\n\nThat said, we didn\'t come up with any of these on our own so we owe a huge debt of gratitude to the many researchers who have shared\ntheir models and weights on github.\\n\n**Caution:** While we strive to ensure that all models can be used out of the box, sometimes things become broken due to Pytorch updates\nor misalignment of the planets. Please don\'t yell at us. Gently point out what\'s broken, or even better, submit a pull request to fix it!\\n\n**Here Be Dragons:** Aaand one more thing - we constantly plumb the depths of github for new models or tweaks to existing ones. While we don\'t\nlist this in the docs, there is a special `testnets` directory with tons of probably broken, semi-working, and at times crazy awesome\nmodels and model-variations. If you\'re interested in the bleeding edge, that\'s where you\'d look (see ``models.__init__.py`` for what\'s available)\n""""""\n\nfrom . import model_locations'"
pywick/models/model_locations.py,1,"b""cadeneroot = 'http://data.lip6.fr/cadene/pretrainedmodels/'\ndpnroot = 'https://s3.amazonaws.com/dpn-pytorch-weights/'\ndrnroot = 'https://tigress-web.princeton.edu/~fy/drn/models/'\ntorchroot = 'https://download.pytorch.org/models/'\n\n\nmodel_urls = {\n    'alexnet': torchroot + 'alexnet-owt-4df8aa71.pth',\n    'bninception': cadeneroot + 'bn_inception-52deb4733.pth',\n    'densenet121': cadeneroot + 'densenet121-fbdb23505.pth',\n    'densenet169': cadeneroot + 'densenet169-f470b90a4.pth',\n    'densenet201': cadeneroot + 'densenet201-5750cbb1e.pth',\n    'densenet161': cadeneroot + 'densenet161-347e6b360.pth',\n    'dpn68': dpnroot + 'dpn68-4af7d88d2.pth',\n    'dpn68b-extra': dpnroot + 'dpn68b_extra-363ab9c19.pth',\n    'dpn92-extra': dpnroot + 'dpn92_extra-fda993c95.pth',\n    'dpn98': dpnroot + 'dpn98-722954780.pth',\n    'dpn107-extra': dpnroot + 'dpn107_extra-b7f9f4cc9.pth',\n    'dpn131': dpnroot + 'dpn131-7af84be88.pth',\n    'drn-c-26': drnroot + 'drn_c_26-ddedf421.pth',\n    'drn-c-42': drnroot + 'drn_c_42-9d336e8c.pth',\n    'drn-c-58': drnroot + 'drn_c_58-0a53a92c.pth',\n    'drn-d-22': drnroot + 'drn_d_22-4bd2f8ea.pth',\n    'drn-d-38': drnroot + 'drn_d_38-eebb45f0.pth',\n    'drn-d-54': drnroot + 'drn_d_54-0e0534ff.pth',\n    'drn-d-105': drnroot + 'drn_d_105-12b40979.pth',\n    'fbresnet152': cadeneroot + 'fbresnet152-2e20f6b4.pth',\n    'inception_v3': torchroot + 'inception_v3_google-1a9a5a14.pth',\n    'inceptionv4': cadeneroot + 'inceptionv4-8e4777a0.pth',\n    'bninception': cadeneroot + 'bn_inception-52deb4733.pth',\n    'inceptionresnetv2': cadeneroot + 'inceptionresnetv2-520b38e4.pth',\n    'nasnetalarge': cadeneroot + 'nasnetalarge-a1897284.pth',\n    'nasnetamobile': cadeneroot + 'nasnetamobile-7e03cead.pth',\n    'pnasnet5large': cadeneroot + 'pnasnet5large-bf079911.pth',\n    'resnet18': torchroot + 'resnet18-5c106cde.pth',\n    'resnet34': torchroot + 'resnet34-333f7ec4.pth',\n    'resnet50': torchroot + 'resnet50-19c8e357.pth',\n    'resnet101': torchroot + 'resnet101-5d3b4d8f.pth',\n    'resnet152': torchroot + 'resnet152-b121ed2d.pth',\n    'resnext101_32x4d': cadeneroot + 'resnext101_32x4d-29e315fa.pth',\n    'resnext101_64x4d': cadeneroot + 'resnext101_64x4d-e77a0586.pth',\n    'senet_res50': 'http://ideaflux.net/files/models/senet_res50.pkl',\n    'se_resnet50': cadeneroot + 'se_resnet50-ce0d4300.pth',\n    'se_resnet101': cadeneroot + 'se_resnet101-7e38fcc6.pth',\n    'se_resnet152': cadeneroot + 'se_resnet152-d17c99b7.pth',\n    'se_resnext50_32x4d': cadeneroot + 'se_resnext50_32x4d-a260b3a4.pth',\n    'se_resnext101_32x4d': cadeneroot + 'se_resnext101_32x4d-3b2fe3d8.pth',\n    'senet154': cadeneroot + 'senet154-c7b49a05.pth',\n    'squeezenet1_0': torchroot + 'squeezenet1_0-a815701f.pth',\n    'squeezenet1_1': torchroot + 'squeezenet1_1-f364aa15.pth',\n    'vgg11': torchroot + 'vgg11-bbd30ac9.pth',\n    'vgg13': torchroot + 'vgg13-c768596a.pth',\n    'vgg16': torchroot + 'vgg16-397923af.pth',\n    'vgg19': torchroot + 'vgg19-dcbb9e9d.pth',\n    'wideresnet50': 'https://s3.amazonaws.com/pytorch/h5models/wide-resnet-50-2-export.hkl',\n    'xception': cadeneroot + 'xception-43020ad28.pth'\n}"""
pywick/models/model_utils.py,11,"b'from . import classification\nfrom .segmentation import *\nfrom . import segmentation\nfrom enum import Enum\nfrom torchvision import models as torch_models\nfrom torchvision.models.inception import InceptionAux\nimport torch\nimport torch.nn as nn\nimport os\nimport errno\n\nrwightman_repo = \'rwightman/pytorch-image-models\'\n\n\nclass ModelType(Enum):\n    """"""\n    Enum to use for looking up task-specific attributes\n    """"""\n    CLASSIFICATION = \'classification\'\n    SEGMENTATION = \'segmentation\'\n\n\ndef get_fc_names(model_name, model_type=ModelType.CLASSIFICATION):\n    """"""\n    Look up the name of the FC (fully connected) layer(s) of a model. Typically these are the layers that are replaced when transfer-learning from another model.\n    Note that only a handful of models have more than one FC layer. Currently only \'classification\' models are supported.\n\n    :param model_name: (string)\n        name of the model\n    :param model_type: (ModelType)\n        only classification is supported at this time\n\n    :return: list\n        names of the FC layers (usually a single one)\n    """"""\n\n    if model_type == ModelType.CLASSIFICATION:\n        fc_names = [\'last_linear\']  # most common name of the last layer (to be replaced)\n\n        if model_name in torch_models.__dict__:\n            if \'densenet\' in model_name or \'squeezenet\' in model_name or \'vgg\' in model_name:    # apparently these are different...\n                fc_names = [\'classifier\']\n            elif \'inception_v3\' in model_name or \'inceptionv3\' in model_name or \'Inception3\' in model_name:\n                fc_names = [\'AuxLogits.fc\', \'fc\']\n            else:\n                fc_names = [\'fc\']  # the name of the last layer to be replaced in torchvision models\n        ## NOTE NOTE NOTE\n        # \'squeezenet\' pretrained model weights are saved as [\'classifier.1\']\n        # \'vgg\' pretrained model weights are saved as [\'classifier.0\', \'classifier.3\', \'classifier.6\']\n\n        return fc_names\n\n    else:\n        return [None]\n\n\ndef get_model(model_type, model_name, num_classes, pretrained=True, **kwargs):\n    """"""\n    :param model_type: (ModelType):\n        type of model we\'re trying to obtain (classification or segmentation)\n    :param model_name: (string):\n        name of the model. By convention (for classification models) lowercase names represent pretrained model variants while Uppercase do not.\n    :param num_classes: (int):\n        number of classes to initialize with (this will replace the last classification layer or set the number of segmented classes)\n    :param pretrained: (bool):\n        whether to load the default pretrained version of the model\n        NOTE! NOTE! For classification, the lowercase model names are the pretrained variants while the Uppercase model names are not.\n        The only exception applies to torch.hub models (all efficientnet, mixnet, mobilenetv3, mnasnet, spnasnet variants) where a single\n        lower-case string can be used for vanilla and pretrained versions. Otherwise, it is IN ERROR to specify an Uppercase model name variant\n        with pretrained=True but one can specify a lowercase model variant with pretrained=False\n        (default: True)\n    :return: model\n    """"""\n\n    if model_name not in get_supported_models(model_type) and not model_name.startswith(\'TEST\'):\n        raise ValueError(\'The supplied model name: {} was not found in the list of acceptable model names.\'\n                         \' Use get_supported_models() to obtain a list of supported models.\'.format(model_name))\n\n    print(""INFO: Loading Model:   --   "" + model_name + ""  with number of classes: "" + str(num_classes))\n    \n    if model_type == ModelType.CLASSIFICATION:\n        torch_hub_names = torch.hub.list(rwightman_repo)\n        if model_name in torch_hub_names:\n            model = torch.hub.load(rwightman_repo, model_name, pretrained=pretrained, num_classes=num_classes)\n        else:\n            # 1. Load model (pretrained or vanilla)\n            import ssl\n            ssl._create_default_https_context = ssl._create_unverified_context\n            fc_name = get_fc_names(model_name=model_name, model_type=model_type)[-1:][0]    # we\'re only interested in the last layer name\n            new_fc = None            # Custom layer to replace with (if none, then it will be handled generically)\n            if model_name in torch_models.__dict__:\n                print(\'INFO: Loading torchvision model: {}\\t Pretrained: {}\'.format(model_name, pretrained))\n                model = torch_models.__dict__[model_name](pretrained=pretrained)  # find a model included in the torchvision package\n            else:\n                net_list = [\'fbresnet\', \'inception\', \'mobilenet\', \'nasnet\', \'polynet\', \'resnext\', \'se_resnet\', \'senet\', \'shufflenet\', \'xception\']\n                if pretrained:\n                    print(\'INFO: Loading a pretrained model: {}\'.format(model_name))\n                    if \'dpn\' in model_name:\n                        model = classification.__dict__[model_name](pretrained=True)  # find a model included in the pywick classification package\n                    elif any(net_name in model_name for net_name in net_list):\n                        model = classification.__dict__[model_name](pretrained=\'imagenet\')\n                else:\n                    print(\'INFO: Loading a vanilla model: {}\'.format(model_name))\n                    model = classification.__dict__[model_name](pretrained=None)  # pretrained must be set to None for the extra models... go figure\n\n            # 2. Create custom FC layers for non-standardized models\n            if \'squeezenet\' in model_name:\n                final_conv = nn.Conv2d(512, num_classes, kernel_size=1)\n                new_fc = nn.Sequential(\n                    nn.Dropout(p=0.5),\n                    final_conv,\n                    nn.ReLU(inplace=True),\n                    nn.AvgPool2d(13, stride=1)\n                )\n                model.num_classes = num_classes\n            elif \'vgg\' in model_name:\n                new_fc = nn.Sequential(\n                    nn.Linear(512 * 7 * 7, 4096),\n                    nn.ReLU(True),\n                    nn.Dropout(),\n                    nn.Linear(4096, 4096),\n                    nn.ReLU(True),\n                    nn.Dropout(),\n                    nn.Linear(4096, num_classes)\n                )\n            elif \'inception3\' in model_name.lower() or \'inception_v3\' in model_name.lower():\n                # Replace the extra aux_logits FC layer if aux_logits are enabled\n                if getattr(model, \'aux_logits\', False):\n                    model.AuxLogits = InceptionAux(768, num_classes)\n            elif \'dpn\' in model_name.lower():\n                old_fc = getattr(model, fc_name)\n                new_fc = nn.Conv2d(old_fc.in_channels, num_classes, kernel_size=1, bias=True)\n\n            # 3. For standard FC layers (nn.Linear) perform a reflection lookup and generate a new FC\n            if new_fc is None:\n                old_fc = getattr(model, fc_name)\n                new_fc = nn.Linear(old_fc.in_features, num_classes)\n\n            # 4. perform replacement of the last FC / Linear layer with a new one\n            setattr(model, fc_name, new_fc)\n\n        return model\n\n    elif model_type == ModelType.SEGMENTATION:\n        """"""\n        Additional Segmentation Option Parameters\n        -----------------------------------------\n        \n        BiSeNet\n            - :param backbone: (str, default: \'resnet18\') The type of backbone to use (one of `{\'resnet18\'}`)\n            - :param aux: (bool, default: False) Whether to output auxiliary loss (typically an FC loss to help with multi-class segmentation)\n            \n        DANet_ResnetXXX, DUNet_ResnetXXX, EncNet, OCNet_XXX_XXX, PSANet_XXX\n            - :param aux: (bool, default: False) Whether to output auxiliary loss (typically an FC loss to help with multi-class segmentation)\n            - :param backbone: (str, default: \'resnet101\') The type of backbone to use (one of `{\'resnet50\', \'resnet101\', \'resnet152\'}`)\n            - :param norm_layer (Pytorch nn.Module, default: nn.BatchNorm2d) The normalization layer to use. Typically it is not necessary to change this parameter unless you know what you\'re doing.\n        \n        DenseASPP_XXX\n            - :param aux: (bool, default: False) Whether to output auxiliary loss (typically an FC loss to help with multi-class segmentation)\n            - :param backbone: (str, default: \'densenet161\') The type of backbone to use (one of `{\'densenet121\', \'densenet161\', \'densenet169\', \'densenet201\'}`)\n            - :param dilate_scale (int, default: 8) The size of the dilation to use (one of `{8, 16}`)\n            - :param norm_layer (Pytorch nn.Module, default: nn.BatchNorm2d) The normalization layer to use. Typically it is not necessary to change this parameter unless you know what you\'re doing.\n            \n        DRNSeg\n            - :param model_name: (str - required) The type of backbone to use. One of `{\'DRN_C_42\', \'DRN_C_58\', \'DRN_D_38\', \'DRN_D_54\', \'DRN_D_105\'}`\n            \n        EncNet_ResnetXXX\n            - :param aux: (bool, default: False) Whether to output auxiliary loss (typically an FC loss to help with multi-class segmentation)\n            - :param backbone: (str, default: \'resnet101\') The type of backbone to use (one of `{\'resnet50\', \'resnet101\', \'resnet152\'}`)\n            - :param norm_layer (Pytorch nn.Module, default: nn.BatchNorm2d) The normalization layer to use. Typically it is not necessary to change this parameter unless you know what you\'re doing.\n            - :param se_loss (bool, default: True) Whether to compute se_loss\n            - :param lateral (bool, default: False)\n        \n        frrn\n            - :param model_type: (str - required) The type of model to use. One of `{\'A\', \'B\'}`\n        \n        GCN, GCN_DENSENET, GCN_NASNET, GCN_PSP, GCN_RESNEXT\n            - :param k: (int - optional) The size of global kernel\n        \n        GCN_PSP, GCN_RESNEXT, Unet_stack\n            - :param input_size: (int - required) The size of output image (will be square)\n        \n        LinkCeption, \'LinkDenseNet121\', \'LinkDenseNet161\', \'LinkInceptionResNet\', \'LinkNet18\', \'LinkNet34\', \'LinkNet50\', \'LinkNet101\', \'LinkNet152\', \'LinkNeXt\', \'CoarseLinkNet50\'\n            - :param num_channels: (int, default: 3) Number of channels in the image (e.g. 3 = RGB)\n            - :param is_deconv: (bool, default: False)\n            - :param decoder_kernel_size: (int, default: 3) Size of the decoder kernel\n        \n        PSPNet\n            - :param backend: (str, default: densenet121) The type of extractor to use. One of `{\'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\', \'resnet152\', \'densenet121\'}`\n        \n        RefineNet4Cascade, RefineNet4CascadePoolingImproved\n            - :param input_shape: (tuple(int, int), default: (1, 512) - required!) Tuple representing input shape (num_channels, dim)\n            - :param freeze_resnet: (bool, default: False) - whether to freeze the underlying resnet\n        """"""\n\n        model_exists = False\n        for m_name in get_supported_models(model_type):\n            if model_name in m_name:\n                model_exists = True\n                break\n        if model_exists:\n            # Print warnings and helpful messages for nets that require additional configuration\n            if model_name in [\'GCN_PSP\', \'GCN_RESNEXT\', \'RefineNet4Cascade\', \'RefineNet4CascadePoolingImproved\', \'Unet_stack\']:\n                print(\'WARN: Did you remember to set the input_size parameter: (int) ?\')\n            elif model_name in [\'RefineNet4Cascade\', \'RefineNet4CascadePoolingImproved\']:\n                print(\'WARN: Did you remember to set the input_shape parameter: tuple(int, int)?\')\n\n            # logic to switch between different constructors\n            if model_name in [\'FusionNet\', \'Enet\', \'frrn\', \'Tiramisu57\', \'Tiramisu67\', \'Tiramisu101\'] or model_name.startswith(\'UNet\') and pretrained:  # FusionNet\n                    print(""WARN: FusionNet, Enet, FRRN, Tiramisu, UNetXXX do not have a pretrained model! Empty model as been created instead."")\n\n            net = segmentation.__dict__[model_name](num_classes=num_classes, pretrained=pretrained, **kwargs)\n\n        else:\n            raise Exception(\'Combination of type: {} and model_name: {} is not valid\'.format(model_type, model_name))\n\n    return net\n\ndef get_supported_models(type):\n    \'\'\'\n\n    :param type: (ModelType):\n        classification or segmentation\n    :return: list (strings) of supported models\n    \'\'\'\n\n    import pkgutil\n    if type == ModelType.SEGMENTATION:\n        excludes = list()  # <-- exclude non-model names\n        for importer, modname, ispkg in pkgutil.walk_packages(path=segmentation.__path__, prefix=segmentation.__name__+""."", onerror=lambda x: None):\n            excludes.append(modname.split(\'.\')[-1])\n        return [x for x in segmentation.__dict__.keys() if (\'__\' not in x and x not in excludes)]  # filter out hidden object attributes and module names\n    elif type == ModelType.CLASSIFICATION:\n        pywick_excludes = list()\n        for importer, modname, ispkg in pkgutil.walk_packages(path=classification.__path__, prefix=classification.__name__+""."", onerror=lambda x: None):\n            pywick_excludes.append(modname.split(\'.\')[-1])\n        pywick_names = [x for x in classification.__dict__.keys() if \'__\' not in x and x not in pywick_excludes]     # includes directory and filenames\n\n        pt_excludes = list()\n        for importer, modname, ispkg in pkgutil.walk_packages(path=torch_models.__path__, prefix=torch_models.__name__+""."", onerror=lambda x: None):\n            pt_excludes.append(modname.split(\'.\')[-1])\n        pt_names = [x for x in torch_models.__dict__.keys() if \'__\' not in x and x not in pt_excludes]  # includes directory and filenames\n\n        torch_hub_names = torch.hub.list(rwightman_repo)\n\n        return pywick_names + pt_names + torch_hub_names\n    else:\n        return None\n\ndef _get_untrained_model(model_name, num_classes):\n    """"""\n    Primarily, this method exists to return an untrained / vanilla version of a specified (pretrained) model.\n    This is on best-attempt basis only and may be out of sync with actual model definitions. The code is manually maintained.\n\n    :param model_name: Lower-case model names are pretrained by convention.\n    :param num_classes: Number of classes to initialize the vanilla model with.\n\n    :return: default model for the model_name with custom number of classes\n    """"""\n\n    if model_name.startswith(\'bninception\'):\n        return classification.BNInception(num_classes=num_classes)\n    elif model_name.startswith(\'densenet\'):\n        return torch_models.DenseNet(num_classes=num_classes)\n    elif model_name.startswith(\'dpn\'):\n        return classification.DPN(num_classes=num_classes)\n    elif model_name.startswith(\'inceptionresnetv2\'):\n        return classification.InceptionResNetV2(num_classes=num_classes)\n    elif model_name.startswith(\'inception_v3\'):\n        return torch_models.Inception3(num_classes=num_classes)\n    elif model_name.startswith(\'inceptionv4\'):\n        return classification.InceptionV4(num_classes=num_classes)\n    elif model_name.startswith(\'nasnetalarge\'):\n        return classification.NASNetALarge(num_classes=num_classes)\n    elif model_name.startswith(\'nasnetamobile\'):\n        return classification.NASNetAMobile(num_classes=num_classes)\n    elif model_name.startswith(\'pnasnet5large\'):\n        return classification.PNASNet5Large(num_classes=num_classes)\n    elif model_name.startswith(\'polynet\'):\n        return classification.PolyNet(num_classes=num_classes)\n    elif model_name.startswith(\'pyresnet\'):\n        return classification.PyResNet(num_classes=num_classes)\n    elif model_name.startswith(\'resnet\'):\n        return torch_models.ResNet(num_classes=num_classes)\n    elif model_name.startswith(\'resnext101_32x4d\'):\n        return classification.ResNeXt101_32x4d(num_classes=num_classes)\n    elif model_name.startswith(\'resnext101_64x4d\'):\n        return classification.ResNeXt101_64x4d(num_classes=num_classes)\n    elif model_name.startswith(\'se_inception\'):\n        return classification.SEInception3(num_classes=num_classes)\n    elif model_name.startswith(\'se_resnext50_32x4d\'):\n        return classification.se_resnext50_32x4d(num_classes=num_classes, pretrained=None)\n    elif model_name.startswith(\'se_resnext101_32x4d\'):\n        return classification.se_resnext101_32x4d(num_classes=num_classes, pretrained=None)\n    elif model_name.startswith(\'senet154\'):\n        return classification.senet154(num_classes=num_classes, pretrained=None)\n    elif model_name.startswith(\'se_resnet50\'):\n        return classification.se_resnet50(num_classes=num_classes, pretrained=None)\n    elif model_name.startswith(\'se_resnet101\'):\n        return classification.se_resnet101(num_classes=num_classes, pretrained=None)\n    elif model_name.startswith(\'se_resnet152\'):\n        return classification.se_resnet152(num_classes=num_classes, pretrained=None)\n    elif model_name.startswith(\'squeezenet1_0\'):\n        return torch_models.squeezenet1_0(num_classes=num_classes, pretrained=False)\n    elif model_name.startswith(\'squeezenet1_1\'):\n        return torch_models.squeezenet1_1(num_classes=num_classes, pretrained=False)\n    elif model_name.startswith(\'xception\'):\n        return classification.Xception(num_classes=num_classes)\n    else:\n        raise ValueError(\'No vanilla model found for model name: {}\'.format(model_name))\n\n# We solve the dimensionality mismatch between final layers in the constructed vs pretrained modules at the data level.\ndef diff_states(dict_canonical, dict_subset):\n    """"""\n    **DEPRECATED - DO NOT USE**\n    """"""\n    names1, names2 = (list(dict_canonical.keys()), list(dict_subset.keys()))\n\n    # Sanity check that param names overlap\n    # Note that params are not necessarily in the same order\n    # for every pretrained model\n    not_in_1 = [n for n in names1 if n not in names2]\n    not_in_2 = [n for n in names2 if n not in names1]\n    assert len(not_in_1) == 0\n    assert len(not_in_2) == 0\n\n    for name, v1 in dict_canonical.items():\n        v2 = dict_subset[name]\n        assert hasattr(v2, \'size\')\n        if v1.size() != v2.size():\n            yield (name, v1)\n\n\ndef load_checkpoint(checkpoint_path, model=None, device=\'cpu\', strict=True, ignore_chkpt_layers=None):\n    """"""\n    Loads weights from a checkpoint into memory. If model is not None then the weights are loaded into the model.\n\n    :param checkpoint_path: (string):\n        path to a pretrained network to load weights from\n    :param model: the model object to load weights onto (default: None)\n    :param device: (string):\n        which device to load model onto (default:\'cpu\')\n    :param strict: (bool):\n        whether to ensure strict key matching (True) or to ignore non-matching keys. (default: True)\n    :param ignore_chkpt_layers: one of {string, list) -- CURRENTLY UNIMPLEMENTED:\n        whether to ignore some subset of layers from checkpoint. This is usually done when loading\n        checkpoint data into a model with a different number of final classes. In that case, you can pass in a\n        special string: \'last_layer\' which will trigger the logic to chop off the last layer of the checkpoint dictionary. Otherwise\n        you can pass in a list of layers to remove from the checkpoint before loading it (e.g. you would do that when\n        loading an inception model that has more than one output layer).\n\n    :return: checkpoint\n    """"""\n\n    # Handle incompatibility between pytorch0.4 and pytorch0.4.x\n    # Source: https://discuss.pytorch.org/t/question-about-rebuild-tensor-v2/14560/2\n\n    import torch._utils\n    try:\n        torch._utils._rebuild_tensor_v2\n    except AttributeError:\n        def _rebuild_tensor_v2(storage, storage_offset, size, stride, requires_grad, backward_hooks):\n            tensor = torch._utils._rebuild_tensor(storage, storage_offset, size, stride)\n            tensor.requires_grad = requires_grad\n            tensor._backward_hooks = backward_hooks\n            return tensor\n\n        torch._utils._rebuild_tensor_v2 = _rebuild_tensor_v2\n\n    checkpoint = None\n    if checkpoint_path:\n        # load data directly from a checkpoint\n        checkpoint_path = os.path.expanduser(checkpoint_path)\n        if os.path.isfile(checkpoint_path):\n            print(\'=> Loading checkpoint: {} onto device: {}\'.format(checkpoint_path, device))\n            checkpoint = torch.load(checkpoint_path, map_location=device)\n\n            pretrained_state = checkpoint[\'state_dict\']\n            print(""INFO: => loaded checkpoint {} (epoch {})"".format(checkpoint_path, checkpoint.get(\'epoch\')))\n            print(\'INFO: => checkpoint model name: \', checkpoint.get(\'modelname\', checkpoint.get(\'model_name\')), \' Make sure the checkpoint model name matches your model!!!\')\n        else:\n            raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), checkpoint_path)\n\n        # If the state_dict was saved from parallelized process the key names will start with \'module.\'\n        # If using ModelCheckpoint the model should already be correctly saved regardless of whether the model was parallelized or not\n        is_parallel = False\n        for key in pretrained_state:\n            if key.startswith(\'module.\'):\n                is_parallel = True\n                break\n\n        if is_parallel:         # do the work of re-assigning each key (must create a copy due to the use of OrderedDict)\n            from collections import OrderedDict\n            new_state_dict = OrderedDict()\n            for k, v in pretrained_state.items():\n                if k.startswith(\'module.\'):\n                    name = k[7:]  # remove `module.`\n                    new_state_dict[name] = v\n                else:\n                    new_state_dict[k] = v\n            checkpoint[\'state_dict\'] = new_state_dict\n\n        # finally load the model weights\n        if model:\n            print(\'INFO: => Attempting to load checkpoint data onto model. Device: {}    Strict: {}\'.format(device, strict))\n            model.load_state_dict(checkpoint[\'state_dict\'], strict=strict)\n    return checkpoint\n'"
pywick/modules/__init__.py,0,b'from .module_trainer import ModuleTrainer\n'
pywick/modules/_utils.py,2,"b'\nimport datetime\nimport warnings\n\ntry:\n    from inspect import signature\nexcept:\n    warnings.warn(\'inspect.signature not available... you should upgrade to Python 3.x\')\n\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom ..metrics import Metric, CategoricalAccuracy, BinaryAccuracy\nfrom ..initializers import GeneralInitializer\n\ndef _add_regularizer_to_loss_fn(loss_fn,\n                                regularizer_container):\n    def new_loss_fn(output_batch, target_batch):\n        return loss_fn(output_batch, target_batch) + regularizer_container.get_value()\n    return new_loss_fn\n\n\ndef _parse_num_inputs_and_targets_from_loader(loader):\n    """""" NOT IMPLEMENTED """"""\n    #batch = next(iter(loader))\n    num_inputs = loader.dataset.num_inputs\n    num_targets = loader.dataset.num_targets\n    return num_inputs, num_targets\n\ndef _parse_num_inputs_and_targets(inputs, targets=None):\n    if isinstance(inputs, (list, tuple)):\n        num_inputs = len(inputs)\n    else:\n        num_inputs = 1\n    if targets is not None:\n        if isinstance(targets, (list, tuple)):\n            num_targets = len(targets)\n        else:\n            num_targets = 1\n    else:\n        num_targets = 0\n    return num_inputs, num_targets\n\ndef _standardize_user_data(inputs, targets=None):\n    if not isinstance(inputs, (list,tuple)):\n        inputs = [inputs]\n    if targets is not None:\n        if not isinstance(targets, (list,tuple)):\n            targets = [targets]\n        return inputs, targets\n    else:\n        return inputs\n\ndef _validate_metric_input(metric):\n    if isinstance(metric, str):\n        if metric.upper() == \'CATEGORICAL_ACCURACY\' or metric.upper() == \'ACCURACY\':\n            return CategoricalAccuracy()\n        elif metric.upper() == \'BINARY_ACCURACY\':\n            return BinaryAccuracy()\n        else:\n            raise ValueError(\'Invalid metric string input - must match pytorch function.\')\n    elif isinstance(metric, Metric):\n        return metric\n    else:\n        raise ValueError(\'Invalid metric input\')\n\ndef _validate_loss_input(loss):\n    dir_f = dir(F)\n    loss_fns = [d.lower() for d in dir_f]\n    if isinstance(loss, str):\n        if loss.lower() == \'unconstrained\':\n            return lambda x: x\n        elif loss.lower() == \'unconstrained_sum\':\n            return lambda x: x.sum()\n        elif loss.lower() == \'unconstrained_mean\':\n            return lambda x: x.mean()\n        else:\n            try:\n                str_idx = loss_fns.index(loss.lower())\n            except:\n                raise ValueError(\'Invalid loss string input - must match pytorch function.\')\n            return getattr(F, dir(F)[str_idx])\n    elif callable(loss):\n        return loss\n    else:\n        raise ValueError(\'Invalid loss input\')\n\ndef _validate_optimizer_input(optimizer):\n    dir_optim = dir(optim)\n    opts = [o.lower() for o in dir_optim]\n    if isinstance(optimizer, str):\n        try:\n            str_idx = opts.index(optimizer.lower())    \n        except:\n            raise ValueError(\'Invalid optimizer string input - must match pytorch function.\')\n        return getattr(optim, dir_optim[str_idx])\n    elif hasattr(optimizer, \'step\') and hasattr(optimizer, \'zero_grad\'):\n        return optimizer\n    else:\n        raise ValueError(\'Invalid optimizer input\')\n\ndef _validate_initializer_input(initializer):\n    if isinstance(initializer, str):\n        try:\n            initializer = GeneralInitializer(initializer)\n        except:\n            raise ValueError(\'Invalid initializer string input - must match pytorch function.\')\n        return initializer\n    elif callable(initializer):\n        return initializer\n    else:\n        raise ValueError(\'Invalid optimizer input\')\n\ndef _get_current_time():\n    return datetime.datetime.now().strftime(""%B %d, %Y - %I:%M%p"")\n\ndef _nb_function_args(fn):\n    return len(signature(fn).parameters)'"
pywick/modules/module_trainer.py,3,"b'""""""\nModuleTrainer for high level training on Pytorch models\n""""""\n\nimport functools\nimport math\nfrom collections import OrderedDict\n\nimport torch as th\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\n\n# local imports\nfrom ._utils import (_validate_loss_input, _validate_metric_input,\n                     _validate_optimizer_input, _validate_initializer_input,\n                     _parse_num_inputs_and_targets, _parse_num_inputs_and_targets_from_loader,\n                     _add_regularizer_to_loss_fn)\n\nfrom ..conditions import ConditionsContainer, CondType\nfrom ..callbacks import CallbackContainer, History, TQDM\nfrom ..regularizers import RegularizerContainer, RegularizerCallback\nfrom ..initializers import InitializerContainer\nfrom ..constraints import ConstraintContainer, ConstraintCallback\nfrom ..metrics import MetricContainer, MetricCallback\nfrom ..misc import ExecType, is_tuple_or_list\n\nfrom tqdm import tqdm\n\n\nclass ModuleTrainer(object):\n\n    def __init__(self, model, cuda_devices=[]):\n        """"""\n        ModelTrainer for high-level training of Pytorch models\n\n        Major Parts\n        -----------\n        - optimizer(s)\n        - criterion(s)\n        - loss_multipliers (to handle multiple losses)\n        - named_helpers\n        - preconditions\n        - postconditions\n        - regularizers\n        - initializers\n        - constraints\n        - metrics\n        - callbacks\n        """"""\n        if not isinstance(model, nn.Module):\n            raise ValueError(\'model argument must inherit from torch.nn.Module\')\n        self.model = model\n        self.device = ""cuda:"" + str(cuda_devices[0]) if cuda_devices else ""cpu""     # Empty lists in python are False\n\n        # custom loss weights\n        self._loss_multipliers = None\n\n        # custom fit helpers\n        self._named_helpers = dict()       # custom trainers that can be initialized during compilation time\n\n        # preconditions\n        self._preconditions = []\n        self._has_preconditions = False\n\n        # postconditions\n        self._postconditions = []\n        self._has_postconditions = False\n\n        # callbacks\n        self._callbacks = []\n\n        # regularizers\n        self._regularizers = []\n        self._has_regularizers = False\n\n        # initializers\n        self._initializers = []\n\n        # constraints\n        self._constraints = []\n        self._has_constraints = False\n\n        # metrics\n        self._metrics = []\n        self._has_metrics = False\n\n        # transforms\n        self._transforms = []\n        self._has_transforms = False\n\n        # losses\n        self._criterion = None\n        self._criterion_fn = None\n\n        # other properties\n        self._stop_training = False\n\n        if cuda_devices and th.cuda.is_available():\n            # turn on the cudnn autotuner that selects efficient algorithms\n            cudnn.benchmark = True\n            # Handle multiple GPUs. Single gpu gets normal treatment while multi-GPU must be wrapped in DataParallel\n            if len(cuda_devices) > 1:\n                self.model = th.nn.DataParallel(self.model, device_ids=cuda_devices)\n        # TODO: This might not be correct. If things break, check here (below line used to be part of the \'if\' block above)\n        self.model = self.model.to(self.device)\n\n    def set_criterion(self, criterion):\n        self._criterion = criterion\n        if is_tuple_or_list(criterion):\n            self._criterion_fn = [_validate_loss_input(l) for l in criterion]\n        else:\n            self._criterion_fn = _validate_loss_input(criterion)\n\n    def set_optimizer(self, optimizer, **kwargs):\n        if type(optimizer) is type or isinstance(optimizer, str):\n            if \'parameters\' in kwargs:\n                parameters = kwargs[\'parameters\']\n            else:\n                parameters = self.model.parameters()\n\n            optimizer = _validate_optimizer_input(optimizer)\n            self._optimizer = optimizer(parameters, **kwargs)\n        else:\n            self._optimizer = optimizer\n\n    def set_callbacks(self, callbacks):\n        if not is_tuple_or_list(callbacks):\n            callbacks = [callbacks]\n        self._callbacks = [self.history] + callbacks\n\n    def set_regularizers(self, regularizers):\n        regularizers = [regularizers] if not is_tuple_or_list(regularizers) else regularizers\n        self._regularizers = regularizers\n        self._has_regularizers = True\n\n    def set_initializers(self, initializers):\n        initializers = [initializers] if not is_tuple_or_list(initializers) else initializers\n        initializers = [_validate_initializer_input(it) for it in initializers]\n        self._initializers = initializers\n\n    def set_constraints(self, constraints):\n        constraints = [constraints] if not is_tuple_or_list(constraints) else constraints\n        self._has_constraints = True\n        self._constraints = constraints\n\n    def set_metrics(self, metrics):\n        metrics = [metrics] if not is_tuple_or_list(metrics) else metrics\n        metrics = [_validate_metric_input(m) for m in metrics]\n        self._has_metrics = True\n        self._metrics = metrics\n\n    def set_preconditions(self, conditions):\n        conditions = [conditions] if not is_tuple_or_list(conditions) else conditions\n        self._preconditions = conditions\n        self._has_preconditions = True\n\n    def set_postconditions(self, conditions):\n        conditions = [conditions] if not is_tuple_or_list(conditions) else conditions\n        self._postconditions = conditions\n        self._has_postconditions = True\n\n    def set_transforms(self, transforms):\n        if not is_tuple_or_list(transforms):\n            transforms = (transforms, lambda x: x, lambda x,y: (x,y))\n        if len(transforms) == 1:\n            transforms = (transforms, lambda x: x, lambda x,y: (x,y))\n        elif len(transforms) == 2:\n            transforms = (transforms, transforms, lambda x,y: (x,y))\n\n        self._has_input_transform = transforms[0] is not None\n        self._has_target_transform = transforms[1] is not None\n        self._has_co_transform = transforms[2] is not None\n\n        self._has_transforms = True\n        self._transforms = transforms\n\n    def compile(self,\n                optimizer,\n                criterion,\n                loss_multipliers=None,\n                named_helpers=None,\n                preconditions=None,\n                postconditions=None,\n                callbacks=None,\n                regularizers=None,\n                initializers=None,\n                constraints=None,\n                metrics=None,\n                transforms=None):\n        \'\'\'\n        :param optimizer: the optimizer to use for learning\n        :param criterion: the criterion to use for calculating loss\n        :param loss_multipliers: (type: list) A way to provide preset loss multipliers for multi-loss criterions\n        :param named_helpers: (type: dict) A way to provide custom handler for loss calculation and forward pass. In most cases not necessary to override.\n        :param preconditions: (type: list) Conditions to check for before executing a forward pass (e.g. asserts)\n        :param postconditions: (type: list) Conditions to check for after the forward pass (e.g. asserts, dynamic network modification)\n        :param callbacks: (type: list) Callbacks to use when calling the fit* functions\n        :param regularizers: (type: list) Regularizers to use when calling the fit* functions\n        :param initializers: (type: list) Initializers to use when calling the fit* functions\n        :param constraints: (type: list) Constraints to use when calling the fit* functions\n        :param metrics: (type: list) Metrics to use when calling the fit* functions\n        :param transforms: (type: list) Unused at the moment\n\n        :return:\n        \'\'\'\n        self.set_optimizer(optimizer)\n        self.set_criterion(criterion)\n        self._loss_multipliers = loss_multipliers\n        self._named_helpers = named_helpers\n\n        if preconditions is not None or postconditions is not None:\n            self._conditions_container = ConditionsContainer(exec_type=ExecType.TRAIN)\n            if preconditions is not None:\n                self.set_preconditions(preconditions)\n                self._conditions_container.add_preconditions(self._preconditions)\n            if postconditions is not None:\n                self.set_postconditions(postconditions)\n                self._conditions_container.add_postconditions(self._postconditions)\n\n        if regularizers is not None:\n            self.set_regularizers(regularizers)\n            self.regularizer_container = RegularizerContainer(self._regularizers)\n            self.regularizer_container.register_forward_hooks(self.model)\n        else:\n            self._has_regularizers = False\n\n        self.history = History(self)\n        self._callbacks = [self.history]\n        if callbacks is not None:\n            self.set_callbacks(callbacks)\n\n\n        if initializers is not None:\n            self.set_initializers(initializers)\n            self.initializer_container = InitializerContainer(self._initializers)\n            # actually initialize the model\n            self.initializer_container.apply(self.model)\n\n        if constraints is not None:\n            self.set_constraints(constraints)\n            self.constraint_container = ConstraintContainer(self._constraints)\n            self.constraint_container.register_constraints(self.model)\n        else:\n            self._has_constraints = False\n\n        if metrics is not None:\n            self.set_metrics(metrics)\n            self.metric_container = MetricContainer(self._metrics)\n        else:\n            self._has_metrics = False\n\n        if transforms is not None:\n            self.set_transforms(transforms)\n        else:\n            self._has_transforms = False\n\n    def fit(self,\n            inputs,\n            targets=None,\n            val_data=None,\n            initial_epoch=0,\n            num_epoch=100,\n            batch_size=32,\n            shuffle=False,\n            fit_helper_name=None,\n            verbose=1):\n        """"""\n        Fit a model on in-memory tensors using ModuleTrainer\n        """"""\n        self.model.train(True)\n        # ----------------------------------------------------------------------\n        num_inputs, num_targets = _parse_num_inputs_and_targets(inputs, targets)\n        len_inputs = len(inputs) if not is_tuple_or_list(inputs) else len(inputs[0])\n\n        if val_data is not None:\n            if num_targets == 0:\n                val_data = (val_data, None)\n            if len(val_data) != 2:\n                raise Exception(\'val_data must be a 2-tuple\')\n            num_val_inputs, num_val_targets = _parse_num_inputs_and_targets(val_data[0], val_data[1])\n            if (num_inputs != num_val_inputs) or (num_targets != num_val_targets):\n                raise Exception(\'The number of input/target tensors must be the same for training and validation data\\n\'\n                                 \'Num Input tensors: (%i train, %i val), Num Target tensors: (%i train, %i val)\' % (num_inputs, num_val_inputs, num_targets, num_val_targets) )\n            val_inputs, val_targets = val_data\n        has_val_data = val_data is not None\n        num_batches = int(math.ceil(len_inputs / batch_size))\n        # ----------------------------------------------------------------------\n\n        fit_helper = _get_helper(self, num_inputs, num_targets, helper_name=fit_helper_name)\n        fit_loss_fn = fit_helper.get_partial_loss_fn(self._criterion_fn)\n        fit_forward_fn = fit_helper.get_partial_forward_fn(self.model)\n\n        with TQDM() as pbar:\n            tmp_callbacks = []\n            if verbose > 0:\n                tmp_callbacks.append(pbar)\n            if self._has_regularizers:\n                tmp_callbacks.append(RegularizerCallback(self.regularizer_container))\n                fit_loss_fn = _add_regularizer_to_loss_fn(fit_loss_fn, self.regularizer_container)\n            if self._has_constraints:\n                tmp_callbacks.append(ConstraintCallback(self.constraint_container))\n            if self._has_metrics:\n                self.metric_container.set_helper(fit_helper)\n                tmp_callbacks.append(MetricCallback(self.metric_container))\n\n            callback_container = CallbackContainer(self._callbacks+tmp_callbacks)\n            callback_container.set_trainer(self)\n            callback_container.on_train_begin({\'batch_size\': batch_size,\n                                               \'num_batches\': num_batches,\n                                               \'num_epoch\': num_epoch,\n                                               \'has_val_data\': has_val_data,\n                                               \'has_regularizers\': self._has_regularizers,\n                                               \'has_metrics\': self._has_metrics})\n\n            try:\n                for epoch_idx in range(initial_epoch,num_epoch):\n                    epoch_logs = {}\n                    callback_container.on_epoch_begin(epoch_idx, epoch_logs)\n\n                    if shuffle:\n                        inputs, targets = fit_helper.shuffle_arrays(inputs, targets)\n\n                    for batch_idx in range(num_batches):\n                        batch_logs = {}\n                        callback_container.on_batch_begin(batch_idx, batch_logs)\n\n                        input_batch, target_batch = fit_helper.grab_batch(batch_idx, batch_size, inputs, targets)\n\n                        if self._has_preconditions:\n                            precond_logs = self._conditions_container(CondType.PRE, epoch_num=epoch_idx, batch_num=batch_idx, net=self.model, input_batch=input_batch, target_batch=target_batch)\n                            batch_logs.update(precond_logs)\n\n                        input_batch, target_batch = fit_helper.move_to_device(self.device, input_batch, target_batch)\n                        if self._has_transforms:\n                            input_batch, target_batch = fit_helper.apply_transforms(self._transforms, input_batch, target_batch)\n\n                        # ---------------------------------------------\n                        self._optimizer.zero_grad()\n                        output_batch = fit_forward_fn(input_batch)\n                        loss = fit_loss_fn(output_batch, target_batch)\n                        assert not math.isnan(loss), \'Assertion failed: Loss is not NaN.\'\n                        loss.backward()\n                        self._optimizer.step()\n                        # ---------------------------------------------\n\n                        if self._has_regularizers:\n                            batch_logs[\'reg_loss\'] = self.regularizer_container.current_value\n                        if self._has_metrics:\n                            metrics_logs = self.metric_container(input_batch, output_batch, target_batch, is_val=False)\n                            batch_logs.update(metrics_logs)\n                        if self._has_postconditions:\n                            postcond_logs = self._conditions_container(CondType.POST, epoch_idx, batch_idx, self.model, input_batch=input_batch, output_batch=output_batch, target_batch=target_batch)\n                            batch_logs.update(postcond_logs)\n\n                        batch_logs[\'loss\'] = loss.item()\n                        callback_container.on_batch_end(batch_idx, batch_logs)\n\n                    epoch_logs.update(self.history.batch_metrics)\n                    if has_val_data:\n                        val_epoch_logs = self.evaluate(val_inputs, val_targets, batch_size=batch_size, verbose=verbose)\n                        epoch_logs.update(val_epoch_logs)\n                        epoch_logs.update(batch_logs)\n                        # TODO how to fix this?\n                        # self.history.batch_metrics.update(val_epoch_logs)\n\n                    callback_container.on_epoch_end(epoch_idx, epoch_logs)\n\n                    if self._stop_training:\n                        break\n            # handles Ctrl-C gracefully\n            except KeyboardInterrupt:\n                print(""||  Caught Ctrl-C -- exiting gracefully  || "")\n        self.model.train(mode=False)\n        callback_container.on_train_end()\n\n    def fit_loader(self,\n                   loader,\n                   val_loader=None,\n                   initial_epoch=0,\n                   num_epoch=100,\n                   fit_helper_name = None,\n                   verbose=1):\n        """"""\n        Fit a model on in-memory tensors using ModuleTrainer\n        """"""\n        self.model.train(mode=True)\n        # ----------------------------------------------------------------------\n\n        # set defaults here (assuming a standard Pytorch dataset)\n        num_inputs = 1\n        num_targets = 1\n\n        # More flexible datasets can provide their own num_inputs / num_targets\n        if hasattr(loader.dataset, \'num_inputs\'):\n            num_inputs = loader.dataset.num_inputs\n\n        if hasattr(loader.dataset, \'num_targets\'):\n            num_targets = loader.dataset.num_targets\n\n        len_inputs = len(loader.sampler) if loader.sampler else len(loader.dataset)\n        batch_size = loader.batch_size\n\n        if val_loader is not None:\n            num_val_inputs = val_loader.dataset.num_inputs\n            num_val_targets = val_loader.dataset.num_targets\n            if (num_inputs != num_val_inputs) or (num_targets != num_val_targets):\n                raise ValueError(\'num_inputs != num_val_inputs or num_targets != num_val_targets\')\n        has_val_data = val_loader is not None\n        num_batches = int(math.ceil(len_inputs / batch_size))\n        # ----------------------------------------------------------------------\n\n        fit_helper = _get_helper(self, num_inputs, num_targets, helper_name=fit_helper_name)\n        fit_loss_fn = fit_helper.get_partial_loss_fn(self._criterion_fn)\n        fit_forward_fn = fit_helper.get_partial_forward_fn(self.model)\n\n        with TQDM() as pbar:\n            tmp_callbacks = []\n            if verbose > 0:\n                tmp_callbacks.append(pbar)\n            if self._has_regularizers:\n                tmp_callbacks.append(RegularizerCallback(self.regularizer_container))\n                fit_loss_fn = _add_regularizer_to_loss_fn(fit_loss_fn, self.regularizer_container)\n            if self._has_constraints:\n                tmp_callbacks.append(ConstraintCallback(self.constraint_container))\n            if self._has_metrics:\n                self.metric_container.set_helper(fit_helper)\n                tmp_callbacks.append(MetricCallback(self.metric_container))\n\n            callback_container = CallbackContainer(self._callbacks+tmp_callbacks)\n            callback_container.set_trainer(self)\n            callback_container.on_train_begin({\'batch_size\': loader.batch_size,\n                                               \'num_batches\': num_batches,\n                                               \'num_epoch\': num_epoch,\n                                               \'has_val_data\': has_val_data,\n                                               \'has_regularizers\': self._has_regularizers,\n                                               \'has_metrics\': self._has_metrics})\n\n            try:\n                for epoch_idx in range(initial_epoch, num_epoch):\n                    epoch_logs = {}\n                    callback_container.on_epoch_begin(epoch_idx, epoch_logs)\n                    loader_iter = iter(loader)\n                    for batch_idx in range(num_batches):\n                        # if batch_idx == 5000 or batch_idx == 10000:\n                        #     pdb.set_trace()\n                        batch_logs = {}\n                        callback_container.on_batch_begin(batch_idx, batch_logs)\n\n                        input_batch, target_batch = fit_helper.grab_batch_from_loader(loader_iter)\n\n                        if self._has_preconditions:\n                            precond_logs = self._conditions_container(CondType.PRE, epoch_num=epoch_idx, batch_num=batch_idx, net=self.model, input_batch=input_batch, target_batch=target_batch)\n                            batch_logs.update(precond_logs)\n                        input_batch, target_batch = fit_helper.move_to_device(self.device, input_batch, target_batch)\n\n                        # ---------------------------------------------\n                        self._optimizer.zero_grad()\n                        output_batch = fit_forward_fn(input_batch)\n\n                        loss = fit_loss_fn(output_batch, target_batch)\n                        assert not math.isnan(loss), \'Assertion failed: Loss is not NaN.\'\n                        loss.backward()\n                        self._optimizer.step()\n                        # ---------------------------------------------\n\n                        if self._has_regularizers:\n                            batch_logs[\'reg_loss\'] = self.regularizer_container.current_value\n                        if self._has_postconditions:\n                            cond_logs = self._conditions_container(CondType.POST, epoch_num=epoch_idx, batch_num=batch_idx, net=self.model, input_batch=input_batch, output_batch=output_batch, target_batch=target_batch)\n                            batch_logs.update(cond_logs)\n                        if self._has_metrics:\n                            metrics_logs = self.metric_container(input_batch, output_batch, target_batch, is_val=False)\n                            batch_logs.update(metrics_logs)\n\n                        batch_logs[\'loss\'] = loss.item()\n                        callback_container.on_batch_end(batch_idx, batch_logs)\n\n                    epoch_logs.update(self.history.batch_metrics)\n                    if has_val_data:\n                        val_epoch_logs = self.evaluate_loader(val_loader, verbose=verbose)\n                        self._in_train_loop = False\n                        #self.history.batch_metrics.update(val_epoch_logs)\n                        #epoch_logs.update(val_epoch_logs)\n                        epoch_logs.update(val_epoch_logs)\n                        epoch_logs.update(batch_logs)\n                        # TODO how to fix this?\n                        # self.history.batch_metrics.update(val_epoch_logs)\n\n                    callback_container.on_epoch_end(epoch_idx, epoch_logs)\n\n                    if self._stop_training:\n                        break\n            # handles Ctrl-C gracefully\n            except KeyboardInterrupt:\n                print(""||  Caught Ctrl-C -- exiting gracefully  || "")\n        self.model.train(mode=False)\n        callback_container.on_train_end()\n\n    def predict(self,\n                inputs,\n                batch_size=32,\n                pred_helper_name=None,\n                verbose=1):\n        self.model.train(mode=False)\n        # --------------------------------------------------------\n        num_inputs, _ = _parse_num_inputs_and_targets(inputs, None)\n        len_inputs = len(inputs) if not is_tuple_or_list(inputs) else len(inputs[0])\n        num_batches = int(math.ceil(len_inputs / batch_size))\n        # --------------------------------------------------------\n\n        predict_helper = _get_helper(self, num_inputs, num_targets=0, helper_name=pred_helper_name)\n        pred_forward_fn = predict_helper.get_partial_forward_fn(self.model)\n\n        with th.no_grad():          # locally disable grad calculations for forward-pass only\n            for batch_idx in range(num_batches):\n                input_batch, _ = predict_helper.grab_batch(batch_idx, batch_size, inputs, None)\n                inputs = predict_helper.move_to_device(self.device, inputs)\n                output_batch = pred_forward_fn(input_batch)\n\n                if batch_idx == 0:\n                    len_outputs = 1 if not is_tuple_or_list(output_batch) else len(output_batch)\n                    prediction_lists = [[] for _ in range(len_outputs)]\n\n                if len_outputs == 1:\n                    prediction_lists[0].append(output_batch)\n                else:\n                    for out_idx in range(len_outputs):\n                        prediction_lists[out_idx].append(output_batch[out_idx])\n\n        final_pred_list = [th.cat(pred_list,0) for pred_list in prediction_lists]\n        self.model.train(mode=True)\n        return final_pred_list if len_outputs > 1 else final_pred_list[0]\n\n    def predict_loader(self,\n                       loader,\n                       pred_helper_name=None,\n                       verbose=1):\n        self.model.train(mode=False)\n        # --------------------------------------------------------\n        num_inputs, num_targets = _parse_num_inputs_and_targets_from_loader(loader)\n        batch_size = loader.batch_size\n        len_inputs = len(loader.sampler) if loader.sampler else len(loader.dataset)\n        num_batches = int(math.ceil(len_inputs / batch_size))\n        # --------------------------------------------------------\n\n        predict_helper = _get_helper(self, num_inputs, num_targets=0, helper_name=pred_helper_name)\n        pred_forward_fn = predict_helper.get_partial_forward_fn(self.model)\n\n        loader_iter = iter(loader)\n\n        _range = tqdm(range(num_batches)) if verbose > 0 else range(num_batches)\n\n        with th.no_grad():  # locally disable grad calculations for forward-pass only\n            for batch_idx in _range:\n                input_batch, _ = predict_helper.grab_batch_from_loader(loader_iter)\n                input_batch, _ = predict_helper.move_to_device(self.device, input_batch)\n\n                output_batch = pred_forward_fn(input_batch)\n\n                if batch_idx == 0:\n                    len_outputs = 1 if not is_tuple_or_list(output_batch) else len(output_batch)\n                    prediction_lists = [[] for _ in range(len_outputs)]\n\n                if len_outputs == 1:\n                    prediction_lists[0].append(output_batch)\n                else:\n                    for out_idx in range(len_outputs):\n                        prediction_lists[out_idx].append(output_batch[out_idx])\n\n        final_pred_list = [th.cat(pred_list,0) for pred_list in prediction_lists]\n        self.model.train(mode=True)\n        return final_pred_list if len_outputs > 1 else final_pred_list[0]\n\n    def evaluate(self,\n                 inputs,\n                 targets=None,\n                 batch_size=32,\n                 eval_helper_name=None,\n                 verbose=1):\n        self.model.train(mode=False)\n        num_inputs, num_targets = _parse_num_inputs_and_targets(inputs, targets)\n        len_inputs = len(inputs) if not is_tuple_or_list(inputs) else len(inputs[0])\n        num_batches = int(math.ceil(len_inputs / batch_size))\n\n        evaluate_helper = _get_helper(self, num_inputs, num_targets, helper_name=eval_helper_name)\n        eval_loss_fn = evaluate_helper.get_partial_loss_fn(self._criterion_fn)\n        eval_forward_fn = evaluate_helper.get_partial_forward_fn(self.model)\n        eval_logs= {\'val_loss\': 0.}\n\n        if self._has_metrics:\n            metric_container = MetricContainer(self._metrics, prefix=\'val_\')\n            metric_container.set_helper(evaluate_helper)\n            metric_container.reset()\n\n        if self._has_preconditions or self._has_postconditions:\n            conditions_container = ConditionsContainer(ExecType.VAL, prefix=\'val_\')\n            if self._has_preconditions:\n                conditions_container.add_preconditions(self._preconditions)\n            if self._has_postconditions:\n                conditions_container.add_postconditions(self._postconditions)\n            conditions_container.reset()\n        else:\n            conditions_container = None\n\n        samples_seen = 0\n        with th.no_grad():  # locally disable grad calculations for forward-pass only\n            for batch_idx in range(num_batches):\n                input_batch, target_batch = evaluate_helper.grab_batch(batch_idx, batch_size, inputs, targets)\n                if conditions_container:\n                    cond_logs = conditions_container(CondType.PRE, epoch_num=None, batch_num=batch_idx, net=self.model, input_batch=input_batch, target_batch=target_batch)\n                    eval_logs.update(cond_logs)\n                input_batch, target_batch = evaluate_helper.move_to_device(self.device, input_batch, target_batch)\n\n                self._optimizer.zero_grad()\n                output_batch = eval_forward_fn(input_batch)\n                loss = eval_loss_fn(output_batch, target_batch)\n                assert not math.isnan(loss), \'Assertion failed: Loss is not NaN.\'\n\n                if conditions_container:\n                    cond_logs = conditions_container(CondType.POST, epoch_num=None, batch_num=batch_idx, net=self.model, input_batch=input_batch, output_batch=output_batch, target_batch=target_batch)\n                    eval_logs.update(cond_logs)\n\n                eval_logs[\'val_loss\'] = (samples_seen*eval_logs[\'val_loss\'] + loss.item()*len(input_batch)) / (samples_seen+len(input_batch))\n                samples_seen += len(input_batch)\n\n                if self._has_metrics:\n                    metrics_logs = metric_container(input_batch, output_batch, target_batch, is_val=True)\n                    eval_logs.update(metrics_logs)\n\n        self.model.train(mode=True)\n        return eval_logs\n\n    def evaluate_loader(self, loader, eval_helper_name=None, verbose=1):\n\n        self.model.train(mode=False)\n        num_inputs, num_targets = _parse_num_inputs_and_targets_from_loader(loader)\n        batch_size = loader.batch_size\n        len_inputs = len(loader.sampler) if loader.sampler else len(loader.dataset)\n        num_batches = int(math.ceil(len_inputs / batch_size))\n\n        evaluate_helper = _get_helper(self, num_inputs, num_targets, helper_name=eval_helper_name)\n        eval_loss_fn = evaluate_helper.get_partial_loss_fn(self._criterion_fn)\n        eval_forward_fn = evaluate_helper.get_partial_forward_fn(self.model)\n        eval_logs= {\'val_loss\': 0.}\n        loader_iter = iter(loader)\n\n        if self._has_metrics:\n            metric_container = MetricContainer(self._metrics, prefix=\'val_\')\n            metric_container.set_helper(evaluate_helper)\n            metric_container.reset()\n\n        if self._has_preconditions or self._has_postconditions:\n            conditions_container = ConditionsContainer(ExecType.VAL, prefix=\'val_\')\n            if self._has_preconditions:\n                conditions_container.add_preconditions(self._preconditions)\n            if self._has_postconditions:\n                conditions_container.add_postconditions(self._postconditions)\n            conditions_container.reset()\n        else:\n            conditions_container = None\n\n        samples_seen = 0\n        with th.no_grad():  # locally disable grad calculations for forward-pass only\n            for batch_idx in range(num_batches):\n                input_batch, target_batch = evaluate_helper.grab_batch_from_loader(loader_iter)\n                if conditions_container:\n                    cond_logs = conditions_container(CondType.PRE, epoch_num=None, batch_num=batch_idx, net=self.model, input_batch=input_batch, target_batch=target_batch)\n                    eval_logs.update(cond_logs)\n                input_batch, target_batch = evaluate_helper.move_to_device(self.device, input_batch, target_batch)\n\n                self._optimizer.zero_grad()\n                output_batch = eval_forward_fn(input_batch)\n                loss = eval_loss_fn(output_batch, target_batch)\n                assert not math.isnan(loss), \'Assertion failed: Loss is not NaN.\'\n\n                if conditions_container:\n                    cond_logs = conditions_container(CondType.POST, epoch_num=None, batch_num=batch_idx, net=self.model, input_batch=input_batch, output_batch=output_batch, target_batch=target_batch)\n                    eval_logs.update(cond_logs)\n\n                samples_seen += len(input_batch)\n                eval_logs[\'val_loss\'] = (samples_seen*eval_logs[\'val_loss\'] + loss.item()*len(input_batch)) / (samples_seen+len(input_batch))\n\n                if self._has_metrics:\n                    metrics_logs = metric_container(input_batch, output_batch, target_batch, is_val=True)\n                    eval_logs.update(metrics_logs)\n\n        self.model.train(mode=True)\n        return eval_logs\n\n    def summary(self, input_size):\n        def register_hook(module):\n            def hook(module, input, output):\n                class_name = str(module.__class__).split(\'.\')[-1].split(""\'"")[0]\n                module_idx = len(summary)\n\n                m_key = \'%s-%i\' % (class_name, module_idx+1)\n                summary[m_key] = OrderedDict()\n                summary[m_key][\'input_shape\'] = list(input[0].size())\n                summary[m_key][\'input_shape\'][0] = -1\n                summary[m_key][\'output_shape\'] = list(output.size())\n                summary[m_key][\'output_shape\'][0] = -1\n\n                params = 0\n                if hasattr(module, \'weight\'):\n                    params += th.prod(th.LongTensor(list(module.weight.size())))\n                    if module.weight.requires_grad:\n                        summary[m_key][\'trainable\'] = True\n                    else:\n                        summary[m_key][\'trainable\'] = False\n                if hasattr(module, \'bias\'):\n                    params +=  th.prod(th.LongTensor(list(module.bias.size())))\n                summary[m_key][\'nb_params\'] = params\n\n            if not isinstance(module, nn.Sequential) and \\\n               not isinstance(module, nn.ModuleList) and \\\n               not (module == self.model):\n                hooks.append(module.register_forward_hook(hook))\n\n        # create properties\n        summary = OrderedDict()\n        hooks = []\n        # register forward hooks\n        self.model.apply(register_hook)\n\n        if isinstance(input_size[0], (list, tuple)):\n            x = [th.rand(1,*in_size) for in_size in input_size]\n            self.model(*x)\n        else:\n            x = th.rand(1,*input_size)\n            self.model(x)\n\n        # remove these hooks\n        for h in hooks:\n            h.remove()\n\n        return summary\n\ndef _get_helper(trainer, num_inputs, num_targets, helper_name=None):\n    \'\'\'\n    :param trainer:\n    :param num_inputs:\n    :param num_targets:\n    :param helper_name: Generally a helper will be determined from number of inputs and targets. However may want to supply your own in some instances.\\n\n    If a helper_name is specified then num_inputs and num_targets are ignored.\n    :return:\n    \'\'\'\n    if not helper_name:\n        if (num_inputs == 1) and (num_targets == 1):\n            helper = SingleInput_SingleTarget_Helper(trainer._loss_multipliers)\n\n        elif (num_inputs == 1) and (num_targets > 1):\n            # use same loss function for all targets if multiple loss fns not explicitly given\n            if not is_tuple_or_list(trainer._criterion_fn):\n                trainer._criterion_fn = [trainer._criterion_fn] * num_targets\n            else:\n                if len(trainer._criterion_fn) != num_targets:\n                    raise ValueError(\'must give one loss function for every input if you give multiple\')\n            helper = SingleInput_MultiTarget_Helper()\n\n        elif (num_inputs == 1) and (num_targets == 0):\n            helper = SingleInput_NoTarget_Helper()\n\n        elif (num_inputs > 1) and (num_targets == 1):\n            helper = MultiInput_SingleTarget_Helper()\n\n        elif (num_inputs > 1) and (num_targets > 1):\n            # use same loss function for all targets if multiple loss fns not explicitly given\n            if not is_tuple_or_list(trainer._criterion_fn):\n                trainer._criterion_fn = [trainer._criterion_fn] * num_targets\n            else:\n                if len(trainer._criterion_fn) != num_targets:\n                    raise ValueError(\'must give one loss function for every input if you give multiple\')\n            helper = MultiInput_MultiTarget_Helper()\n\n        elif (num_inputs > 1) and (num_targets == 0):\n            helper = MultiInput_NoTarget_Helper()\n\n    else:\n        helper = trainer._named_helpers.get(helper_name)\n\n    return helper\n\nclass SingleInput_SingleTarget_Helper(object):\n\n    def __init__(self, loss_multipliers=None):\n        \'\'\'\n\n        :param loss_multipliers: (type: list) Some networks return multiple losses that are then added together. This optional list\\n\n            specifies different weights to apply to corresponding losses before they are summed.\n        \'\'\'\n        self.loss_multipliers = loss_multipliers\n\n    def move_to_device(self, device, inputs, targets):\n        return inputs.to(device), targets.to(device)\n\n    def shuffle_arrays(self, inputs, targets):\n        rand_indices = th.randperm(len(inputs))\n        inputs = inputs[rand_indices]\n        targets = targets[rand_indices]\n        return inputs, targets\n\n    def grab_batch(self, batch_idx, batch_size, inputs, targets):\n        input_batch = inputs[batch_idx*batch_size:(batch_idx+1)*batch_size]\n        target_batch = targets[batch_idx*batch_size:(batch_idx+1)*batch_size]\n        return input_batch, target_batch\n\n    def grab_batch_from_loader(self, loader_iter):\n        return next(loader_iter)        # input_batch, target_batch\n\n    def apply_transforms(self, tforms, input_batch, target_batch):\n        input_batch = tforms[0](input_batch)\n        target_batch = tforms[1](target_batch)\n        input_batch, target_batch = tforms[2](input_batch, target_batch)\n        return input_batch, target_batch\n\n    def forward_pass(self, input_batch, model):\n        return model(input_batch)\n\n    def get_partial_forward_fn(self, model):\n        return functools.partial(self.forward_pass, model=model)\n\n    def calculate_loss(self, output_batch, target_batch, loss_fn):\n        total_loss = 0.\n        if is_tuple_or_list(output_batch):     # some networks output multiple results (to compute separate losses)\n            if self.loss_multipliers:\n                assert len(output_batch) == len(self.loss_multipliers)\n\n            for i, output in enumerate(output_batch):\n                if self.loss_multipliers:\n                    total_loss += loss_fn(output, target_batch) * self.loss_multipliers[i]\n                else:\n                    total_loss += loss_fn(output, target_batch)\n        else:\n            total_loss = loss_fn(output_batch, target_batch)\n\n        return total_loss\n\n    def get_partial_loss_fn(self, loss_fn):\n        return functools.partial(self.calculate_loss, loss_fn=loss_fn)\n\n\nclass SingleInput_MultiTarget_Helper(object):\n\n    def move_to_device(self, device, inputs, targets):\n        return inputs.to(device), [target_.to(device) for target_ in targets]\n\n    def shuffle_arrays(self, inputs, targets):\n        rand_indices = th.randperm(len(inputs))\n        inputs = inputs[rand_indices]\n        targets = [target_[rand_indices] for target_ in targets]\n        return inputs, targets\n\n    def grab_batch(self, batch_idx, batch_size, inputs, targets):\n        input_batch = inputs[batch_idx*batch_size:(batch_idx+1)*batch_size]\n        target_batch = [target_[batch_idx*batch_size:(batch_idx+1)*batch_size] for target_ in targets]\n        return input_batch, target_batch\n\n    def grab_batch_from_loader(self, loader_iter):\n        return next(loader_iter)        # OLD: # input_batch, [target_ for target_ in target_batch]\n\n    def apply_transforms(self, tforms, input_batch, target_batch):\n        input_batch = tforms[0](input_batch)\n        target_batch = [tforms[1](target_) for target_ in target_batch]\n        return input_batch, target_batch\n\n    def forward_pass(self, input_batch, model):\n        return model(input_batch)\n\n    def get_partial_forward_fn(self, model):\n        return functools.partial(self.forward_pass, model=model)\n\n    def calculate_loss(self, output_batch, target_batch, loss_fn):\n        return sum([loss_fn[idx](output_batch[idx], target_batch[idx])\n                    for idx in range(len(output_batch))])\n\n    def get_partial_loss_fn(self, loss_fn):\n        return functools.partial(self.calculate_loss, loss_fn=loss_fn)\n\n\nclass MultiInput_SingleTarget_Helper(object):\n    def move_to_device(self, device, inputs, targets):\n        return [input_.to(device) for input_ in inputs], targets.to(device)\n\n    def shuffle_arrays(self, inputs, targets):\n        rand_indices = th.randperm(len(inputs))\n        inputs = [input_[rand_indices] for input_ in inputs]\n        targets = targets[rand_indices]\n        return inputs, targets\n\n    def grab_batch(self, batch_idx, batch_size, inputs, targets):\n        input_batch = [input_[batch_idx*batch_size:(batch_idx+1)*batch_size] for input_ in inputs]\n        target_batch = targets[batch_idx*batch_size:(batch_idx+1)*batch_size]\n        return input_batch, target_batch\n\n    def grab_batch_from_loader(self, loader_iter):\n        return next(loader_iter)        # OLD: # [input_ for input_ in input_batch], target_batch\n\n    def apply_transforms(self, tforms, input_batch, target_batch):\n        input_batch = [tforms[0](input_) for input_ in input_batch]\n        target_batch = tforms[1](target_batch)\n        return input_batch, target_batch\n\n    def forward_pass(self, input_batch, model):\n        return model(*input_batch)\n\n    def get_partial_forward_fn(self, model):\n        return functools.partial(self.forward_pass, model=model)\n\n    def calculate_loss(self, output_batch, target_batch, loss_fn):\n        return loss_fn(output_batch, target_batch)\n\n    def get_partial_loss_fn(self, loss_fn):\n        return functools.partial(self.calculate_loss, loss_fn=loss_fn)\n\n\nclass MultiInput_MultiTarget_Helper(object):\n\n    def move_to_device(self, device, inputs, targets):\n        return [input_.to(device) for input_ in inputs], [target_.to(device) for target_ in targets]\n\n    def shuffle_arrays(self, inputs, targets):\n        rand_indices = th.randperm(len(inputs))\n        inputs = [input_[rand_indices] for input_ in inputs]\n        targets = [input_[rand_indices] for input_ in inputs]\n        return inputs, targets\n\n    def grab_batch(self, batch_idx, batch_size, inputs, targets):\n        input_batch = [input_[batch_idx*batch_size:(batch_idx+1)*batch_size] for input_ in inputs]\n        target_batch = [target_[batch_idx*batch_size:(batch_idx+1)*batch_size] for target_ in targets]\n        return input_batch, target_batch\n\n    def grab_batch_from_loader(self, loader_iter):\n        return next(loader_iter)        # OLD: # [input_ for input_ in input_batch], [target_ for target_ in target_batch]\n\n    def apply_transforms(self, tforms, input_batch, target_batch):\n        input_batch = [tforms[0](input_) for input_ in input_batch]\n        target_batch = [tforms[1](target_) for target_ in target_batch]\n        return input_batch, target_batch\n\n    def forward_pass(self, input_batch, model):\n        return model(*input_batch)\n\n    def get_partial_forward_fn(self, model):\n        return functools.partial(self.forward_pass, model=model)\n\n    def calculate_loss(self, output_batch, target_batch, loss_fn):\n        return sum([loss_fn[idx](output_batch[idx], target_batch[idx]) for idx in range(len(output_batch))])\n\n    def get_partial_loss_fn(self, loss_fn):\n        return functools.partial(self.calculate_loss, loss_fn=loss_fn)\n\n\nclass SingleInput_NoTarget_Helper(object):\n    def move_to_device(self, device, inputs, targets=None):\n        return inputs.to(device), None\n\n    def shuffle_arrays(self, inputs, targets=None):\n        rand_indices = th.randperm(len(inputs))\n        inputs = inputs[rand_indices]\n        return inputs, None\n\n    def grab_batch(self, batch_idx, batch_size, inputs, targets=None):\n        input_batch = inputs[batch_idx*batch_size:(batch_idx+1)*batch_size]\n        return input_batch, None\n\n    def grab_batch_from_loader(self, loader_iter):\n        input_batch = next(loader_iter)\n        return input_batch, None\n\n    def apply_transforms(self, tforms, input_batch, target_batch=None):\n        input_batch = tforms[0](input_batch)\n        return input_batch, None\n\n    def forward_pass(self, input_batch, model):\n        return model(input_batch)\n\n    def get_partial_forward_fn(self, model):\n        return functools.partial(self.forward_pass, model=model)\n\n    def calculate_loss(self, output_batch, target_batch, loss_fn):\n        return loss_fn(output_batch)\n\n    def get_partial_loss_fn(self, loss_fn):\n        return functools.partial(self.calculate_loss, loss_fn=loss_fn)\n\n\nclass MultiInput_NoTarget_Helper(object):\n\n    def move_to_device(self, device, inputs, targets=None):\n        return [input_.to(device) for input_ in inputs], None\n\n    def shuffle_arrays(self, inputs, targets=None):\n        rand_indices = th.randperm(len(inputs))\n        inputs = [input_[rand_indices] for input_ in inputs]\n        return inputs, None\n\n    def grab_batch(self, batch_idx, batch_size, inputs, targets=None):\n        input_batch = [input_[batch_idx*batch_size:(batch_idx+1)*batch_size] for input_ in inputs]\n        return input_batch, None\n\n    def grab_batch_from_loader(self, loader_iter):\n        input_batch = next(loader_iter)\n        return input_batch, None\n\n    def apply_transforms(self, tforms, input_batch, target_batch=None):\n        input_batch = [tforms[0](input_) for input_ in input_batch]\n        return input_batch, None\n\n    def forward_pass(self, input_batch, model):\n        return model(*input_batch)\n\n    def get_partial_forward_fn(self, model):\n        return functools.partial(self.forward_pass, model=model)\n\n    def calculate_loss(self, output_batch, target_batch, loss_fn):\n        return loss_fn(output_batch)\n\n    def get_partial_loss_fn(self, loss_fn):\n        return functools.partial(self.calculate_loss, loss_fn=loss_fn)\n'"
pywick/modules/stn.py,1,"b'\nimport torch.nn as nn\n\nfrom ..functions import F_affine2d, F_affine3d\n\n\nclass STN2d(nn.Module):\n\n    def __init__(self, local_net):\n        super(STN2d, self).__init__()\n        self.local_net = local_net\n\n    def forward(self, x):\n        params = self.local_net(x)\n        x_transformed = F_affine2d(x[0], params.view(2,3))\n        return x_transformed\n\n\nclass STN3d(nn.Module):\n\n    def __init__(self, local_net):\n        self.local_net = local_net\n\n    def forward(self, x):\n        params = self.local_net(x)\n        x_transformed = F_affine3d(x, params.view(3,4))\n        return x_transformed\n\n'"
pywick/optimizers/__init__.py,0,"b'""""""\nOptimizers govern the path that your neural network takes as it tries to minimize error.\nPicking the right optimizer and initializing it with the right parameters will either make your network learn successfully\nor will cause it not to learn at all! Pytorch already implements the most widely used flavors such as SGD, Adam, RMSProp etc.\nHere we strive to include optimizers that Pytorch has missed (and any cutting edge ones that have not yet been added).\n""""""\n\nfrom .adamw import AdamW\nfrom .addsign import AddSign\nfrom .eve import Eve\nfrom .nadam import Nadam\nfrom .powersign import PowerSign\nfrom .sgdw import SGDW\nfrom .swa import SWA\nfrom .radam import RAdam\nfrom .lookahead import Lookahead\nfrom .ralamb import Ralamb\nfrom .rangerlars import RangerLars\n'"
pywick/optimizers/adamw.py,5,"b'# Source: https://github.com/pytorch\n\nimport math\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\n\nclass AdamW(Optimizer):\n    r""""""Implements AdamW algorithm.\n\n    The original Adam algorithm was proposed in `Adam: A Method for Stochastic Optimization`_.\n    The AdamW variant was proposed in `Decoupled Weight Decay Regularization`_.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay coefficient (default: 1e-2)\n        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n            algorithm from the paper `On the Convergence of Adam and Beyond`_\n            (default: False)\n\n    .. _Adam\\: A Method for Stochastic Optimization:\n        https://arxiv.org/abs/1412.6980\n    .. _Decoupled Weight Decay Regularization:\n        https://arxiv.org/abs/1711.05101\n    .. _On the Convergence of Adam and Beyond:\n        https://openreview.net/forum?id=ryQu7f-RZ\n    """"""\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n                 weight_decay=1e-2, amsgrad=False):\n        if not 0.0 <= lr:\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(""Invalid epsilon value: {}"".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(""Invalid beta parameter at index 0: {}"".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(""Invalid beta parameter at index 1: {}"".format(betas[1]))\n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        weight_decay=weight_decay, amsgrad=amsgrad)\n        super(AdamW, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(AdamW, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault(\'amsgrad\', False)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n\n                # Perform stepweight decay\n                p.data.mul_(1 - group[\'lr\'] * group[\'weight_decay\'])\n\n                # Perform optimization step\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\'Adam does not support sparse gradients, please consider SparseAdam instead\')\n                amsgrad = group[\'amsgrad\']\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\n                    if amsgrad:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state[\'max_exp_avg_sq\'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                if amsgrad:\n                    max_exp_avg_sq = state[\'max_exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                state[\'step\'] += 1\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                if amsgrad:\n                    # Maintains the maximum of all 2nd moment running avg. till now\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                    # Use the max. for normalizing running avg. of gradient\n                    denom = max_exp_avg_sq.sqrt().add_(group[\'eps\'])\n                else:\n                    denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n\n                bias_correction1 = 1 - beta1 ** state[\'step\']\n                bias_correction2 = 1 - beta2 ** state[\'step\']\n                step_size = group[\'lr\'] * math.sqrt(bias_correction2) / bias_correction1\n\n                p.data.addcdiv_(-step_size, exp_avg, denom)\n\n        return loss\n'"
pywick/optimizers/addsign.py,2,"b'# Source: https://github.com/cydonia999/AddSign_PowerSign_in_PyTorch/tree/master/torch/optim\n\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\n\nclass AddSign(Optimizer):\n    """"""Implements AddSign algorithm.\n\n    It has been proposed in `Neural Optimizer Search with Reinforcement Learning`_.\n        \n    :param params: (iterable): iterable of parameters to optimize or dicts defining\n        parameter groups\n    :param lr: (float, optional): learning rate (default: 1e-3)\n    :param beta: (float, optional): coefficients used for computing\n        running averages of gradient (default: 0.9)\n    :param alpha: (float, optional): term added to\n        the internal_decay * sign(g) * sign(m) (default: 1)\n    :param sign_internal_decay: (callable, optional): a function that returns\n        an internal decay calculated based on the current training step and\n        the total number of training steps.\n        If None, the internal decay is assumed to be 1.\n        \n    .. _Neural Optimizer Search with Reinforcement Learning:\n        https://arxiv.org/abs/1709.07417\n    """"""\n\n    def __init__(self, params, lr=1e-3, beta=0.9, alpha=1, sign_internal_decay=None):\n        if sign_internal_decay is not None and not callable(sign_internal_decay):\n            raise TypeError(\'{} is not a callable\'.format(\n                type(sign_internal_decay).__name__))\n        defaults = dict(lr=lr, beta=beta, alpha=alpha,\n                        sign_internal_decay=sign_internal_decay if sign_internal_decay is not None else lambda _: 1)\n        super(AddSign, self).__init__(params, defaults)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        :param closure: (callable, optional): A closure that reevaluates the model\n            and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = torch.zeros_like(p.data)\n\n                exp_avg = state[\'exp_avg\']\n                beta = group[\'beta\']\n                alpha = group[\'alpha\']\n\n                state[\'step\'] += 1\n                internal_decay = group[\'sign_internal_decay\'](state[\'step\'] - 1)\n\n                # Decay the first moment running average coefficient\n                exp_avg.mul_(beta).add_(1 - beta, grad)\n                add_sign = grad.mul(alpha + internal_decay * grad.sign() * exp_avg.sign())\n                p.data.add_(-group[\'lr\'], add_sign)\n\n        return loss\n'"
pywick/optimizers/eve.py,2,"b'# Source: https://github.com/moskomule/eve.pytorch\n\nimport math\nfrom torch.optim.optimizer import Optimizer\n\n\nclass Eve(Optimizer):\n    """"""\n    Implementation of `Eve:  A Gradient Based Optimization Method with Locally and Globally Adaptive Learning Rates <https://arxiv.org/pdf/1611.01505.pdf>`_\n    """"""\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999, 0.999), eps=1e-8, k=0.1, K=10, weight_decay=0):\n\n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        k=k, K=K, weight_decay=weight_decay)\n        super(Eve, self).__init__(params, defaults)\n\n    def step(self, closure):\n        """"""\n        :param closure: (closure). see http://pytorch.org/docs/optim.html#optimizer-step-closure\n        :return: loss\n        """"""\n        loss = closure()\n        _loss = loss.item()  # float\n\n        for group in self.param_groups:\n\n            for p in group[\'params\']:\n                grad = p.grad.data\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'m_t\'] = grad.new().resize_as_(grad).zero_()\n                    # Exponential moving average of squared gradient values\n                    state[\'v_t\'] = grad.new().resize_as_(grad).zero_()\n                    # f hats, smoothly tracked objective functions\n                    # \\hat{f}_0 = f_0\n                    state[\'ft_2\'], state[\'ft_1\'] = _loss, None\n                    state[\'d\'] = 1\n\n                m_t, v_t = state[\'m_t\'], state[\'v_t\']\n                beta1, beta2, beta3 = group[\'betas\']\n                k, K = group[\'k\'], group[\'K\']\n                d = state[\'d\']\n                state[\'step\'] += 1\n                t = state[\'step\']\n                # initialization of \\hat{f}_1\n                if t == 1:\n                    # \\hat{f}_1 = f_1\n                    state[\'ft_1\'] = _loss\n                # \\hat{f_{t-1}}, \\hat{f_{t-2}}\n                ft_1, ft_2 = state[\'ft_1\'], state[\'ft_2\']\n                # f(\\theta_{t-1})\n                f = _loss\n\n                if group[\'weight_decay\'] != 0:\n                    grad = grad.add(group[\'weight_decay\'], p.data)\n\n                # Decay the first and second moment running average coefficient\n                m_t.mul_(beta1).add_(1 - beta1, grad)\n                v_t.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n\n                m_t_hat = m_t / (1 - beta1 ** t)\n                v_t_hat = v_t / (1 - beta2 ** t)\n\n                if t > 1:\n                    if f >= state[\'ft_2\']:\n                        delta = k + 1\n                        Delta = K + 1\n                    else:\n                        delta = 1 / (K + 1)\n                        Delta = 1 / (k + 1)\n\n                    c = min(max(delta, f / ft_2), Delta)\n                    r = abs(c - 1) / min(c, 1)\n                    state[\'ft_1\'], state[\'ft_2\'] = c * ft_2, ft_1\n                    state[\'d\'] = beta3 * d + (1 - beta3) * r\n\n                # update parameters\n                p.data.addcdiv_(-group[\'lr\'] / state[\'d\'],\n                                m_t_hat,\n                                v_t_hat.sqrt().add_(group[\'eps\']))\n\n        return loss\n'"
pywick/optimizers/lookahead.py,3,"b'# Source: https://github.com/alphadl/lookahead.pytorch/blob/master/lookahead.py (MIT)\n\nfrom collections import defaultdict\nfrom torch.optim.optimizer import Optimizer\nimport torch\n\n\nclass Lookahead(Optimizer):\n    r""""""\n        Implementation of `Lookahead Optimizer: k steps forward, 1 step back <https://arxiv.org/abs/1907.08610>`_\n\n        Args:\n            :param optimizer:       - the optimizer to work with (sgd, adam etc)\n            :param k: (int)         - number of steps to look ahead (default=5)\n            :param alpha: (float)   - slow weights step size\n    """"""\n\n    def __init__(self, optimizer, k=5, alpha=0.5):\n        """"""\n        :param optimizer:       - the optimizer to work with (sgd, adam etc)\n        :param k: (int)         - number of steps to look ahead (default=5)\n        :param alpha: (float)   - slow weights step size\n        """"""\n        self.optimizer = optimizer\n        self.k = k\n        self.alpha = alpha\n        self.param_groups = self.optimizer.param_groups\n        self.state = defaultdict(dict)\n        self.fast_state = self.optimizer.state\n        for group in self.param_groups:\n            group[""counter""] = 0\n\n    def update(self, group):\n        for fast in group[""params""]:\n            param_state = self.state[fast]\n            if ""slow_param"" not in param_state:\n                param_state[""slow_param""] = torch.zeros_like(fast.data)\n                param_state[""slow_param""].copy_(fast.data)\n            slow = param_state[""slow_param""]\n            slow += (fast.data - slow) * self.alpha\n            fast.data.copy_(slow)\n\n    def update_lookahead(self):\n        for group in self.param_groups:\n            self.update(group)\n\n    def step(self, closure=None):\n        loss = self.optimizer.step(closure)\n        for group in self.param_groups:\n            if group[""counter""] == 0:\n                self.update(group)\n            group[""counter""] += 1\n            if group[""counter""] >= self.k:\n                group[""counter""] = 0\n        return loss\n\n    def state_dict(self):\n        fast_state_dict = self.optimizer.state_dict()\n        slow_state = {\n            (id(k) if isinstance(k, torch.Tensor) else k): v\n            for k, v in self.state.items()\n        }\n        fast_state = fast_state_dict[""state""]\n        param_groups = fast_state_dict[""param_groups""]\n        return {\n            ""fast_state"": fast_state,\n            ""slow_state"": slow_state,\n            ""param_groups"": param_groups,\n        }\n\n    def load_state_dict(self, state_dict):\n        slow_state_dict = {\n            ""state"": state_dict[""slow_state""],\n            ""param_groups"": state_dict[""param_groups""],\n        }\n        fast_state_dict = {\n            ""state"": state_dict[""fast_state""],\n            ""param_groups"": state_dict[""param_groups""],\n        }\n        super(Lookahead, self).load_state_dict(slow_state_dict)\n        self.optimizer.load_state_dict(fast_state_dict)\n        self.fast_state = self.optimizer.state\n\n    def add_param_group(self, param_group):\n        param_group[""counter""] = 0\n        self.optimizer.add_param_group(param_group)\n'"
pywick/optimizers/nadam.py,1,"b'import torch\nfrom torch.optim.optimizer import Optimizer\n\n\nclass Nadam(Optimizer):\n    """"""Implements Nadam algorithm (a variant of Adam based on Nesterov momentum).\n\n    It has been proposed in `Incorporating Nesterov Momentum into Adam`__.\n\n    :param params: (iterable): iterable of parameters to optimize or dicts defining\n        parameter groups\n    :param lr: (float, optional): learning rate (default: 2e-3)\n    :param betas: (Tuple[float, float], optional): coefficients used for computing\n        running averages of gradient and its square\n    :param eps: (float, optional): term added to the denominator to improve\n        numerical stability (default: 1e-8)\n    :param weight_decay: (float, optional): weight decay (L2 penalty) (default: 0)\n    :param schedule_decay: (float, optional): momentum schedule decay (default: 4e-3)\n\n    __ http://cs229.stanford.edu/proj2015/054_report.pdf\n    __ http://www.cs.toronto.edu/~fritz/absps/momentum.pdf\n\n    """"""\n\n    def __init__(self, params, lr=2e-3, betas=(0.9, 0.999), eps=1e-8,\n                 weight_decay=0, schedule_decay=4e-3):\n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        weight_decay=weight_decay, schedule_decay=schedule_decay)\n        super(Nadam, self).__init__(params, defaults)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        :param closure: (callable, optional): A closure that reevaluates the model\n            and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    state[\'m_schedule\'] = 1.\n                    state[\'exp_avg\'] = grad.new().resize_as_(grad).zero_()\n                    state[\'exp_avg_sq\'] = grad.new().resize_as_(grad).zero_()\n\n                # Warming momentum schedule\n                m_schedule = state[\'m_schedule\']\n                schedule_decay = group[\'schedule_decay\']\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n                eps = group[\'eps\']\n\n                state[\'step\'] += 1\n\n                if group[\'weight_decay\'] != 0:\n                    grad = grad.add(group[\'weight_decay\'], p.data)\n\n                momentum_cache_t = beta1 * \\\n                    (1. - 0.5 * (0.96 ** (state[\'step\'] * schedule_decay)))\n                momentum_cache_t_1 = beta1 * \\\n                    (1. - 0.5 *\n                     (0.96 ** ((state[\'step\'] + 1) * schedule_decay)))\n                m_schedule_new = m_schedule * momentum_cache_t\n                m_schedule_next = m_schedule * momentum_cache_t * momentum_cache_t_1\n                state[\'m_schedule\'] = m_schedule_new\n\n                # Decay the first and second moment running average coefficient\n                bias_correction2 = 1 - beta2 ** state[\'step\']\n\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                exp_avg_sq_prime = exp_avg_sq.div(1. - bias_correction2)\n\n                denom = exp_avg_sq_prime.sqrt_().add_(group[\'eps\'])\n\n                p.data.addcdiv_(-group[\'lr\'] * (1. - momentum_cache_t) / (1. - m_schedule_new), grad, denom)\n                p.data.addcdiv_(-group[\'lr\'] * momentum_cache_t_1 / (1. - m_schedule_next), exp_avg, denom)\n\n        return loss\n'"
pywick/optimizers/powersign.py,3,"b'# Source: https://github.com/cydonia999/AddSign_PowerSign_in_PyTorch/tree/master/torch/optim\n\nimport torch\nfrom torch.optim.optimizer import Optimizer\nimport math\n\n\nclass PowerSign(Optimizer):\n    """"""Implements PowerSign algorithm.\n\n    It has been proposed in `Neural Optimizer Search with Reinforcement Learning`_.\n\n    :param params: (iterable): iterable of parameters to optimize or dicts defining\n        parameter groups\n    :param lr: (float, optional): learning rate (default: 1e-3)\n    :param beta: (float, optional): coefficients used for computing\n        running averages of gradient (default: 0.9)\n    :param alpha: (float, optional): term powered to\n        the internal_decay * sign(g) * sign(m) (default: math.e)\n    :param sign_internal_decay: (callable, optional): a function that returns\n        an internal decay calculated based on the current training step and\n        the total number of training steps.\n        If None, the internal decay is assumed to be 1.\n\n    .. _Neural Optimizer Search with Reinforcement Learning:\n        https://arxiv.org/abs/1709.07417\n\n    """"""\n\n    def __init__(self, params, lr=1e-3, beta=0.9, alpha=math.e, sign_internal_decay=None):\n        if sign_internal_decay is not None and not callable(sign_internal_decay):\n            raise TypeError(\'{} is not a callable\'.format(\n                type(sign_internal_decay).__name__))\n        if alpha <= 0:\n            raise ValueError(""alpha should be > 0."")\n        defaults = dict(lr=lr, beta=beta, alpha=alpha,\n                        sign_internal_decay=sign_internal_decay if sign_internal_decay is not None else lambda _: 1)\n        super(PowerSign, self).__init__(params, defaults)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        :param closure: (callable, optional): A closure that reevaluates the model\n            and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = torch.zeros_like(p.data)\n\n                exp_avg = state[\'exp_avg\']\n                beta = group[\'beta\']\n                alpha = group[\'alpha\']\n\n                state[\'step\'] += 1\n                internal_decay = group[\'sign_internal_decay\'](state[\'step\'] - 1)\n\n                # Decay the first moment running average coefficient\n                exp_avg.mul_(beta).add_(1 - beta, grad)\n\n                power_sign = grad.mul(torch.pow(alpha, internal_decay * grad.sign() * exp_avg.sign()))\n                p.data.add_(-group[\'lr\'], power_sign)\n\n        return loss\n'"
pywick/optimizers/radam.py,3,"b'# Source: https://github.com/LiyuanLucasLiu/RAdam/blob/master/cifar_imagenet/utils/radam.py (Apache 2.0)\n\nimport math\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\n\nclass RAdam(Optimizer):\n    r""""""Implementation of the `RAdam optimizer`_.\n\n        The learning rate warmup for Adam is a must-have trick for stable training in certain situations (or eps tuning).\n        But the underlying mechanism is largely unknown. In our study, we suggest one fundamental cause is the large\n        variance of the adaptive learning rates, and provide both theoretical and empirical support evidence.\n\n        Args:\n            params (iterable): iterable of parameters to optimize or dicts defining\n                parameter groups\n            lr (float, optional): learning rate (default: 1e-3)\n            betas (Tuple[float, float], optional): coefficients used for computing\n                running averages of gradient and its square (default: (0.9, 0.999))\n            eps (float, optional): term added to the denominator to improve\n                numerical stability (default: 1e-8)\n            weight_decay (float, optional): weight decay coefficient (default: 0)\n\n        .. _RAdam\\: On the Variance of the Adaptive Learning Rate and Beyond:\n            https://arxiv.org/abs/1908.03265\n        """"""\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-4):\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        self.buffer = [[None, None, None] for ind in range(10)]\n        super(RAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(RAdam, self).__setstate__(state)\n\n    def step(self, closure=None):\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError(\'RAdam does not support sparse gradients\')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    state[\'exp_avg\'] = torch.zeros_like(p_data_fp32)\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state[\'exp_avg\'] = state[\'exp_avg\'].type_as(p_data_fp32)\n                    state[\'exp_avg_sq\'] = state[\'exp_avg_sq\'].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                state[\'step\'] += 1\n                buffered = self.buffer[int(state[\'step\'] % 10)]\n                if state[\'step\'] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state[\'step\']\n                    beta2_t = beta2 ** state[\'step\']\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state[\'step\'] * beta2_t / (1 - beta2_t)\n                    buffered[1] = N_sma\n\n                    # more conservative since it\'s an approximated value\n                    if N_sma >= 5:\n                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state[\'step\'])\n                    else:\n                        step_size = 1.0 / (1 - beta1 ** state[\'step\'])\n                    buffered[2] = step_size\n\n                if group[\'weight_decay\'] != 0:\n                    p_data_fp32.add_(-group[\'weight_decay\'] * group[\'lr\'], p_data_fp32)\n\n                # more conservative since it\'s an approximated value\n                if N_sma >= 5:\n                    denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n                    p_data_fp32.addcdiv_(-step_size * group[\'lr\'], exp_avg, denom)\n                else:\n                    p_data_fp32.add_(-step_size * group[\'lr\'], exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n        return loss'"
pywick/optimizers/ralamb.py,3,"b""# Source: https://gist.github.com/redknightlois/c4023d393eb8f92bb44b2ab582d7ec20\n\nfrom torch.optim.optimizer import Optimizer\nimport torch\nimport math\n\n\nclass Ralamb(Optimizer):\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-4):\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        self.buffer = [[None, None, None] for ind in range(10)]\n        super(Ralamb, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(Ralamb, self).__setstate__(state)\n\n    def step(self, closure=None):\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError('Ralamb does not support sparse gradients')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                # Decay the first and second moment running average coefficient\n                # m_t\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                # v_t\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n\n                state['step'] += 1\n                buffered = self.buffer[int(state['step'] % 10)]\n\n                if state['step'] == buffered[0]:\n                    N_sma, radam_step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state['step']\n                    beta2_t = beta2 ** state['step']\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n                    buffered[1] = N_sma\n\n                    # more conservative since it's an approximated value\n                    if N_sma >= 5:\n                        radam_step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n                    else:\n                        radam_step_size = group['lr'] / (1 - beta1 ** state['step'])\n                    buffered[2] = radam_step_size\n\n                if group['weight_decay'] != 0:\n                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n\n                # more conservative since it's an approximated value\n                radam_step = p_data_fp32.clone()\n                if N_sma >= 5:\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n                    radam_step.addcdiv_(-radam_step_size, exp_avg, denom)\n                else:\n                    radam_step.add_(-radam_step_size, exp_avg)\n\n                radam_norm = radam_step.pow(2).sum().sqrt()\n                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)\n                if weight_norm == 0 or radam_norm == 0:\n                    trust_ratio = 1\n                else:\n                    trust_ratio = weight_norm / radam_norm\n\n                state['weight_norm'] = weight_norm\n                state['adam_norm'] = radam_norm\n                state['trust_ratio'] = trust_ratio\n\n                if N_sma >= 5:\n                    p_data_fp32.addcdiv_(-radam_step_size * trust_ratio, exp_avg, denom)\n                else:\n                    p_data_fp32.add_(-radam_step_size * trust_ratio, exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n        return loss"""
pywick/optimizers/rangerlars.py,0,"b'from .lookahead import *\nfrom .ralamb import *\n\n# RAdam + LARS + LookAHead\n\ndef RangerLars(params, alpha=0.5, k=6, *args, **kwargs):\n    """"""\n    Combination of RAdam + LARS + LookAhead\n\n    :param params:\n    :param alpha:\n    :param k:\n    :param args:\n    :param kwargs:\n    :return:\n    """"""\n    ralamb = Ralamb(params, *args, **kwargs)\n    return Lookahead(ralamb, alpha, k)\n'"
pywick/optimizers/sgdw.py,4,"b'# Source: https://github.com/pytorch/pytorch/pull/3740\n\nimport torch\nfrom torch.optim.optimizer import Optimizer\nfrom torch.optim import SGD\n\n\nclass SGDW(Optimizer):\n    r""""""Implements stochastic gradient descent warm (optionally with momentum).\n\n    It has been proposed in `Fixing Weight Decay Regularization in Adam <https://arxiv.org/abs/1711.05101>`_.\n\n    Nesterov momentum is based on the formula from\n    `On the importance of initialization and momentum in deep learning <http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf>`_.\n\n    :param params (iterable): iterable of parameters to optimize or dicts defining parameter groups\n    :param lr: (float): learning rate\n    :param momentum: (float, optional): momentum factor (default: 0)\n    :param weight_decay: (float, optional): weight decay (L2 penalty) (default: 0)\n    :param dampening: (float, optional): dampening for momentum (default: 0)\n    :param nesterov: (bool, optional): enables Nesterov momentum (default: False)\n\n    Example:\n        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n        >>> optimizer.zero_grad()\n        >>> loss_fn(model(input), target).backward()\n        >>> optimizer.step()\n\n    .. note::\n        The implementation of SGD with Momentum/Nesterov subtly differs from\n        Sutskever et. al. and implementations in some other frameworks.\n\n        Considering the specific case of Momentum, the update can be written as\n\n        .. math::\n                  v = \\rho * v + g \\\\\n                  p = p - lr * v\n\n        where p, g, v and :math:`\\rho` denote the parameters, gradient,\n        velocity, and momentum respectively.\n\n        This is in contrast to Sutskever et. al. and\n        other frameworks which employ an update of the form\n\n        .. math::\n             v = \\rho * v + lr * g \\\\\n             p = p - v\n\n        The Nesterov version is analogously modified.\n    """"""\n\n    def __init__(self, params, lr=0.003, momentum=0, dampening=0, weight_decay=0, nesterov=False):\n        defaults = dict(lr=lr, momentum=momentum, dampening=dampening, weight_decay=weight_decay, nesterov=nesterov)\n        if nesterov and (momentum <= 0 or dampening != 0):\n            raise ValueError(""Nesterov momentum requires a momentum and zero dampening"")\n        super(SGD, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(SGD, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault(\'nesterov\', False)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        :param closure: (callable, optional): A closure that reevaluates the model\n            and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            weight_decay = group[\'weight_decay\']\n            momentum = group[\'momentum\']\n            dampening = group[\'dampening\']\n            nesterov = group[\'nesterov\']\n\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                d_p = p.grad.data\n                if momentum != 0:\n                    param_state = self.state[p]\n                    if \'momentum_buffer\' not in param_state:\n                        buf = param_state[\'momentum_buffer\'] = torch.zeros_like(p.data)\n                        buf.mul_(momentum).add_(d_p)\n                    else:\n                        buf = param_state[\'momentum_buffer\']\n                        buf.mul_(momentum).add_(1 - dampening, d_p)\n                    if nesterov:\n                        d_p = d_p.add(momentum, buf)\n                    else:\n                        d_p = buf\n                if weight_decay != 0:\n                    p.data.add_(-weight_decay, p.data)\n\n        return loss\n'"
pywick/optimizers/sign_internal_decay.py,0,"b'# Source: https://github.com/cydonia999/AddSign_PowerSign_in_PyTorch/tree/master/torch/optim\n\nimport math\n\nclass _SignInternalDecay(object):\n    """"""Base class for internal decays for PowerSign and AddSign optimizers.\n\n    Arguments:\n        T_max (int): the total number of training steps\n            to be used to compute internal decays.\n    """"""\n    def __init__(self, T_max):\n        if T_max < 1:\n            raise ValueError(\'T_max should be >= 1.\')\n\n        self.T_max = T_max\n\n    \nclass LinearInternalDecay(_SignInternalDecay):\n    """"""Implements a linear decay used internally in PowerSign and AddSign optimizers.\n\n    It has been proposed in `Neural Optimizer Search with Reinforcement Learning`_.\n\n    Arguments:\n        T_max (int): the total number of training steps\n            to be used to compute internal decays.\n\n    .. _Neural Optimizer Search with Reinforcement Learning:\n        https://arxiv.org/abs/1709.07417\n    """"""\n    def __init__(self, T_max):\n        super(LinearInternalDecay, self).__init__(T_max)\n\n    def __call__(self, step):\n        """"""Returns a linear decay at the current training step:\n            1 - step / T_max\n\n        Args:\n          step: the current training step.\n        """"""\n        if step is None:\n            raise ValueError(""step is required for linear_decay."")\n        if step < 0:\n            raise ValueError(""step should be >= 0."")\n        step = min(step, self.T_max)\n        decay = 1 - float(step) / float(self.T_max)\n        return decay\n\n\nclass CosineInternalDecay(_SignInternalDecay):\n    """"""Implements a cyclical decay used internally in PowerSign and AddSign optimizers.\n\n    It has been proposed in `Neural Optimizer Search with Reinforcement Learning`_.\n\n    Arguments:\n        T_max (int): the total number of training steps\n            to be used to compute internal decays\n        num_periods: number of periods of cosine from 0 to T_max (default: 0.5)\n        zero_after: if not None, number after which 0 is returned\n\n    .. _Neural Optimizer Search with Reinforcement Learning:\n        https://arxiv.org/abs/1709.07417\n    """"""\n    def __init__(self, T_max, num_periods=0.5, zero_after=None):\n        super(CosineInternalDecay, self).__init__(T_max)\n        if zero_after is not None and zero_after < 0:\n            raise ValueError(""zero_after should be >= 0."")\n        self.num_periods = num_periods\n        self.zero_after = zero_after\n\n    def __call__(self, step):\n        """"""Returns a cyclical decay at the current training step:\n            0.5 * (1 + cos(2 * pi * num_periods * step / T_max))\n\n        Args:\n          step: the current training step.\n        """"""\n        if step is None:\n            raise ValueError(""step is required for cosine_decay."")\n        if step < 0:\n            raise ValueError(""step should be >= 0."")\n        step = min(step, self.T_max)\n        frac = 2.0 * self.num_periods * step / float(self.T_max)\n        if self.zero_after is not None and frac >= 2 * self.zero_after:\n            return 0.0\n        decay = 0.5 * (1 + math.cos(math.pi * frac))\n        return decay\n\n\nclass RestartCosineInternalDecay(_SignInternalDecay):\n    """"""Implements a restart decay used internally in PowerSign and AddSign optimizers.\n\n    It has been proposed in `Neural Optimizer Search with Reinforcement Learning`_.\n\n    Arguments:\n        T_max (int): the total number of training steps\n            to be used to compute internal decays\n        num_periods: number of half periods of cosine from 0 to T_max (default: 1)\n        zero_after: if not None, number after which 0 is returned\n\n    .. _Neural Optimizer Search with Reinforcement Learning:\n        https://arxiv.org/abs/1709.07417\n    """"""\n    def __init__(self, T_max, num_periods=1, zero_after=None):\n        super(RestartCosineInternalDecay, self).__init__(T_max)\n        if zero_after is not None and zero_after < 0:\n            raise ValueError(""zero_after should be >= 0."")\n        self.num_periods = num_periods\n        self.zero_after = zero_after\n\n    def __call__(self, step):\n        """"""Returns a restart decay at the current training step:\n            0.5 * (1 + cos(pi * (num_periods * step) % T_max / T_max))\n\n        Args:\n          step: the current training step.\n        """"""\n        if step is None:\n            raise ValueError(""step is required for cosine_decay."")\n        if step < 0:\n            raise ValueError(""step should be >= 0."")\n        step = min(step, self.T_max)\n        frac = (self.num_periods * step) % self.T_max / float(self.T_max)\n        if self.zero_after is not None and frac >= 2 * self.zero_after:\n            return 0.0\n        decay = 0.5 * (1 + math.cos(math.pi * frac))\n        return decay\n'"
pywick/optimizers/swa.py,17,"b'# Source: https://github.com/pytorch/contrib\n\nfrom collections import defaultdict\nfrom torch.optim.optimizer import Optimizer\nimport torch\nimport warnings\n\n\nclass SWA(Optimizer):\n    r""""""Implements Stochastic Weight Averaging (SWA).\n\n    Stochastic Weight Averaging was proposed in `Averaging Weights Leads to\n    Wider Optima and Better Generalization`_ by Pavel Izmailov, Dmitrii\n    Podoprikhin, Timur Garipov, Dmitry Vetrov and Andrew Gordon Wilson\n    (UAI 2018).\n\n    SWA is implemented as a wrapper class taking optimizer instance as input\n    and applying SWA on top of that optimizer.\n\n    SWA can be used in two modes: automatic and manual. In the automatic\n    mode SWA running averages are automatically updated every\n    :attr:`swa_freq` steps after :attr:`swa_start` steps of optimization. If\n    :attr:`swa_lr` is provided, the learning rate of the optimizer is reset\n    to :attr:`swa_lr` at every step starting from :attr:`swa_start`. To use\n    SWA in automatic mode provide values for both :attr:`swa_start` and\n    :attr:`swa_freq` arguments.\n\n    Alternatively, in the manual mode, use :meth:`update_swa` or\n    :meth:`update_swa_group` methods to update the SWA running averages.\n\n    In the end of training use `swap_swa_sgd` method to set the optimized\n    variables to the computed averages.\n\n    :param optimizer: (torch.optim.Optimizer): optimizer to use with SWA\n    :param swa_start: (int): number of steps before starting to apply SWA in\n        automatic mode; if None, manual mode is selected (default: None)\n    :param swa_freq: (int): number of steps between subsequent updates of\n        SWA running averages in automatic mode; if None, manual mode is\n        selected (default: None)\n    :param swa_lr: (float): learning rate to use starting from step swa_start\n        in automatic mode; if None, learning rate is not changed\n        (default: None)\n\n    Examples:\n        >>> from pywick.optimizers import SWA\n        >>> # automatic mode\n        >>> base_opt = torch.optim.SGD(model.parameters(), lr=0.1)\n        >>> opt = SWA(base_opt, swa_start=10, swa_freq=5, swa_lr=0.05)\n        >>> for _ in range(100):\n        >>>     opt.zero_grad()\n        >>>     loss_fn(model(input), target).backward()\n        >>>     opt.step()\n        >>> opt.swap_swa_sgd()\n        >>> # manual mode\n        >>> opt = SWA(base_opt)\n        >>> for i in range(100):\n        >>>     opt.zero_grad()\n        >>>     loss_fn(model(input), target).backward()\n        >>>     opt.step()\n        >>>     if i > 10 and i % 5 == 0:\n        >>>         opt.update_swa()\n        >>> opt.swap_swa_sgd()\n\n    .. note::\n        SWA does not support parameter-specific values of :attr:`swa_start`,\n        :attr:`swa_freq` or :attr:`swa_lr`. In automatic mode SWA uses the\n        same :attr:`swa_start`, :attr:`swa_freq` and :attr:`swa_lr` for all\n        parameter groups. If needed, use manual mode with\n        :meth:`update_swa_group` to use different update schedules for\n        different parameter groups.\n\n    .. note::\n        Call :meth:`swap_swa_sgd` in the end of training to use the computed\n        running averages.\n\n    .. note::\n        If you are using SWA to optimize the parameters of a Neural Network\n        containing Batch Normalization layers, you need to update the\n        :attr:`running_mean` and :attr:`running_var` statistics of the\n        Batch Normalization module. You can do so by using\n        `torchcontrib.optim.swa.bn_update` utility. For further description\n        see `this article <https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/>`_.\n\n    .. _Averaging Weights Leads to Wider Optima and Better Generalization:\n        https://arxiv.org/abs/1803.05407\n    .. _Improving Consistency-Based Semi-Supervised Learning with Weight Averaging:\n        https://arxiv.org/abs/1806.05594\n    """"""\n    def __init__(self, optimizer, swa_start=None, swa_freq=None, swa_lr=None):\n        self._auto_mode, (self.swa_start, self.swa_freq) = \\\n            self._check_params(self, swa_start, swa_freq)\n        self.swa_lr = swa_lr\n\n        if self._auto_mode:\n            if swa_start < 0:\n                raise ValueError(""Invalid swa_start: {}"".format(swa_start))\n            if swa_freq < 1:\n                raise ValueError(""Invalid swa_freq: {}"".format(swa_freq))\n        else:\n            if self.swa_lr is not None:\n                warnings.warn(\n                    ""Some of swa_start, swa_freq is None, ignoring swa_lr"")\n            # If not in auto mode make all swa parameters None\n            self.swa_lr = None\n            self.swa_start = None\n            self.swa_freq = None\n\n        if self.swa_lr is not None and self.swa_lr < 0:\n            raise ValueError(""Invalid SWA learning rate: {}"".format(swa_lr))\n\n        self.optimizer = optimizer\n\n        self.param_groups = self.optimizer.param_groups\n        self.state = defaultdict(dict)\n        self.opt_state = self.optimizer.state\n        for group in self.param_groups:\n            group[\'n_avg\'] = 0\n            group[\'step_counter\'] = 0\n\n    @staticmethod\n    def _check_params(self, swa_start, swa_freq):\n        params = [swa_start, swa_freq]\n        params_none = [param is None for param in params]\n        if not all(params_none) and any(params_none):\n            warnings.warn(\n                ""Some of swa_start, swa_freq is None, ignoring other"")\n        for i, param in enumerate(params):\n            if param is not None and not isinstance(param, int):\n                params[i] = int(param)\n                warnings.warn(""Casting swa_start, swa_freq to int"")\n        return not any(params_none), params\n\n    def _reset_lr_to_swa(self):\n        if self.swa_lr is None:\n            return\n        for param_group in self.param_groups:\n            if param_group[\'step_counter\'] >= self.swa_start:\n                param_group[\'lr\'] = self.swa_lr\n\n    def update_swa_group(self, group):\n        r""""""Updates the SWA running averages for the given parameter group.\n\n        :param group (dict): Specifies for what parameter group SWA running\n                averages should be updated\n\n        Examples:\n            >>> # automatic mode\n            >>> base_opt = torch.optim.SGD([{\'params\': [x]},\n            >>>             {\'params\': [y], \'lr\': 1e-3}], lr=1e-2, momentum=0.9)\n            >>> opt = torchcontrib.optim.SWA(base_opt)\n            >>> for i in range(100):\n            >>>     opt.zero_grad()\n            >>>     loss_fn(model(input), target).backward()\n            >>>     opt.step()\n            >>>     if i > 10 and i % 5 == 0:\n            >>>         # Update SWA for the second parameter group\n            >>>         opt.update_swa_group(opt.param_groups[1])\n            >>> opt.swap_swa_sgd()\n        """"""\n        for p in group[\'params\']:\n            param_state = self.state[p]\n            if \'swa_buffer\' not in param_state:\n                param_state[\'swa_buffer\'] = torch.zeros_like(p.data)\n            buf = param_state[\'swa_buffer\']\n            virtual_decay = 1 / float(group[""n_avg""] + 1)\n            diff = (p.data - buf) * virtual_decay\n            buf.add_(diff)\n        group[""n_avg""] += 1\n\n    def update_swa(self):\n        r""""""Updates the SWA running averages of all optimized parameters.\n        """"""\n        for group in self.param_groups:\n            self.update_swa_group(group)\n\n    def swap_swa_sgd(self):\n        r""""""Swaps the values of the optimized variables and swa buffers.\n\n        It\'s meant to be called in the end of training to use the collected\n        swa running averages. It can also be used to evaluate the running\n        averages during training; to continue training `swap_swa_sgd`\n        should be called again.\n        """"""\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                param_state = self.state[p]\n                if \'swa_buffer\' not in param_state:\n                    # If swa wasn\'t applied we don\'t swap params\n                    warnings.warn(\n                        ""SWA wasn\'t applied to param {}; skipping it"".format(p))\n                    continue\n                buf = param_state[\'swa_buffer\']\n                tmp = torch.empty_like(p.data)\n                tmp.copy_(p.data)\n                p.data.copy_(buf)\n                buf.copy_(tmp)\n\n    def step(self, closure=None):\n        r""""""Performs a single optimization step.\n\n        In automatic mode also updates SWA running averages.\n        """"""\n        self._reset_lr_to_swa()\n        loss = self.optimizer.step(closure)\n        for group in self.param_groups:\n            group[""step_counter""] += 1\n            steps = group[""step_counter""]\n            if self._auto_mode:\n                if steps > self.swa_start and steps % self.swa_freq == 0:\n                    self.update_swa_group(group)\n        return loss\n\n    def state_dict(self):\n        r""""""Returns the state of SWA as a :class:`dict`.\n\n        It contains three entries:\n            * opt_state - a dict holding current optimization state of the base\n                optimizer. Its content differs between optimizer classes.\n            * swa_state - a dict containing current state of SWA. For each\n                optimized variable it contains swa_buffer keeping the running\n                average of the variable\n            * param_groups - a dict containing all parameter groups\n        """"""\n        opt_state_dict = self.optimizer.state_dict()\n        swa_state = {(id(k) if isinstance(k, torch.Tensor) else k): v\n                     for k, v in self.state.items()}\n        opt_state = opt_state_dict[""state""]\n        param_groups = opt_state_dict[""param_groups""]\n        return {""opt_state"": opt_state, ""swa_state"": swa_state,\n                ""param_groups"": param_groups}\n\n    def load_state_dict(self, state_dict):\n        r""""""Loads the optimizer state.\n\n        :param state_dict (dict): SWA optimizer state. Should be an object returned\n            from a call to `state_dict`.\n        """"""\n        swa_state_dict = {""state"": state_dict[""swa_state""],\n                          ""param_groups"": state_dict[""param_groups""]}\n        opt_state_dict = {""state"": state_dict[""opt_state""],\n                          ""param_groups"": state_dict[""param_groups""]}\n        super(SWA, self).load_state_dict(swa_state_dict)\n        self.optimizer.load_state_dict(opt_state_dict)\n        self.opt_state = self.optimizer.state\n\n    def add_param_group(self, param_group):\n        r""""""Add a param group to the :class:`Optimizer` s `param_groups`.\n\n        This can be useful when fine tuning a pre-trained network as frozen\n        layers can be made trainable and added to the :class:`Optimizer` as\n        training progresses.\n\n        :param param_group (dict): Specifies what Tensors should be optimized along\n        with group specific optimization options.\n        """"""\n        param_group[\'n_avg\'] = 0\n        param_group[\'step_counter\'] = 0\n        self.optimizer.add_param_group(param_group)\n\n    @staticmethod\n    def bn_update(loader, model, device=None):\n        r""""""Updates BatchNorm running_mean, running_var buffers in the model.\n\n        It performs one pass over data in `loader` to estimate the activation\n        statistics for BatchNorm layers in the model.\n\n        :param loader (torch.utils.data.DataLoader): dataset loader to compute the\n            activation statistics on. Each data batch should be either a\n            tensor, or a list/tuple whose first element is a tensor\n            containing data.\n\n        :param model (torch.nn.Module): model for which we seek to update BatchNorm\n            statistics.\n\n        :param device (torch.device, optional): If set, data will be trasferred to\n            :attr:`device` before being passed into :attr:`model`.\n        """"""\n        if not _check_bn(model):\n            return\n        was_training = model.training\n        model.train()\n        momenta = {}\n        model.apply(_reset_bn)\n        model.apply(lambda module: _get_momenta(module, momenta))\n        n = 0\n        for input in loader:\n            if isinstance(input, (list, tuple)):\n                input = input[0]\n            b = input.size(0)\n\n            momentum = b / float(n + b)\n            for module in momenta.keys():\n                module.momentum = momentum\n\n            if device is not None:\n                input = input.to(device)\n\n            model(input)\n            n += b\n\n        model.apply(lambda module: _set_momenta(module, momenta))\n        model.train(was_training)\n\n\n# BatchNorm utils\ndef _check_bn_apply(module, flag):\n    if issubclass(module.__class__, torch.nn.modules.batchnorm._BatchNorm):\n        flag[0] = True\n\n\ndef _check_bn(model):\n    flag = [False]\n    model.apply(lambda module: _check_bn_apply(module, flag))\n    return flag[0]\n\n\ndef _reset_bn(module):\n    if issubclass(module.__class__, torch.nn.modules.batchnorm._BatchNorm):\n        module.running_mean = torch.zeros_like(module.running_mean)\n        module.running_var = torch.ones_like(module.running_var)\n\n\ndef _get_momenta(module, momenta):\n    if issubclass(module.__class__, torch.nn.modules.batchnorm._BatchNorm):\n        momenta[module] = module.momentum\n\n\ndef _set_momenta(module, momenta):\n    if issubclass(module.__class__, torch.nn.modules.batchnorm._BatchNorm):\n        module.momentum = momenta[module]\n'"
pywick/transforms/__init__.py,0,b'\nfrom .affine_transforms import *\nfrom .image_transforms import *\nfrom .tensor_transforms import *'
pywick/transforms/affine_transforms.py,0,"b'""""""\nAffine transforms implemented on torch tensors, and\nrequiring only one interpolation\n""""""\n\nimport math\nimport random\nimport torch as th\n\nfrom ..utils import th_affine2d, th_random_choice\n\n\nclass RandomAffine(object):\n\n    def __init__(self, \n                 rotation_range=None, \n                 translation_range=None,\n                 shear_range=None, \n                 zoom_range=None,\n                 interp=\'bilinear\',\n                 lazy=False):\n        """"""\n        Perform an affine transforms with various sub-transforms, using\n        only one interpolation and without having to instantiate each\n        sub-transform individually.\n\n        Arguments\n        ---------\n        rotation_range : one integer or float\n            image will be rotated randomly between (-degrees, degrees) \n\n        translation_range : a float or a tuple/list with 2 floats between [0, 1)\n            first value:\n                image will be horizontally shifted between \n                (-height_range * height_dimension, height_range * height_dimension)\n            second value:\n                Image will be vertically shifted between \n                (-width_range * width_dimension, width_range * width_dimension)\n\n        shear_range : float\n            image will be sheared randomly between (-degrees, degrees)\n\n        zoom_range : list/tuple with two floats between [0, infinity).\n            first float should be less than the second\n            lower and upper bounds on percent zoom. \n            Anything less than 1.0 will zoom in on the image, \n            anything greater than 1.0 will zoom out on the image.\n            e.g. (0.7, 1.0) will only zoom in, \n                 (1.0, 1.4) will only zoom out,\n                 (0.7, 1.4) will randomly zoom in or out\n\n        interp : string in {\'bilinear\', \'nearest\'} or list of strings\n            type of interpolation to use. You can provide a different\n            type of interpolation for each input, e.g. if you have two\n            inputs then you can say `interp=[\'bilinear\',\'nearest\']\n\n        """"""\n        self.transforms = []\n        if rotation_range is not None:\n            rotation_tform = RandomRotate(rotation_range, lazy=True)\n            self.transforms.append(rotation_tform)\n\n        if translation_range is not None:\n            translation_tform = RandomTranslate(translation_range, lazy=True)\n            self.transforms.append(translation_tform)\n\n        if shear_range is not None:\n            shear_tform = RandomShear(shear_range, lazy=True)\n            self.transforms.append(shear_tform) \n\n        if zoom_range is not None:\n            zoom_tform = RandomZoom(zoom_range, lazy=True)\n            self.transforms.append(zoom_tform)\n\n        self.interp = interp\n        self.lazy = lazy\n\n        if len(self.transforms) == 0:\n            raise Exception(\'Must give at least one transform parameter\')\n\n    def __call__(self, *inputs):\n        # collect all of the lazily returned tform matrices\n        tform_matrix = self.transforms[0](inputs[0])\n        for tform in self.transforms[1:]:\n            tform_matrix = tform_matrix.mm(tform(inputs[0])) \n        self.tform_matrix = tform_matrix\n\n        if self.lazy:\n            return tform_matrix\n        else:\n            outputs = Affine(tform_matrix,\n                             interp=self.interp)(*inputs)\n            return outputs\n\n\nclass Affine(object):\n\n    def __init__(self, \n                 tform_matrix,\n                 interp=\'bilinear\'):\n        """"""\n        Perform an affine transforms with various sub-transforms, using\n        only one interpolation and without having to instantiate each\n        sub-transform individually.\n\n        Arguments\n        ---------\n        tform_matrix : a 2x3 or 3x3 matrix\n            affine transformation matrix to apply\n\n        interp : string in {\'bilinear\', \'nearest\'} or list of strings\n            type of interpolation to use. You can provide a different\n            type of interpolation for each input, e.g. if you have two\n            inputs then you can say `interp=[\'bilinear\',\'nearest\']\n\n        """"""\n        self.tform_matrix = tform_matrix\n        self.interp = interp\n\n    def __call__(self, *inputs):\n        if not isinstance(self.interp, (tuple,list)):\n            interp = [self.interp]*len(inputs)\n        else:\n            interp = self.interp\n\n        outputs = []\n        for idx, _input in enumerate(inputs):\n            input_tf = th_affine2d(_input,\n                                   self.tform_matrix,\n                                   mode=interp[idx])\n            outputs.append(input_tf)\n        return outputs if idx >= 1 else outputs[0]\n\n\nclass AffineCompose(object):\n\n    def __init__(self, \n                 transforms,\n                 interp=\'bilinear\'):\n        """"""\n        Apply a collection of explicit affine transforms to an input image,\n        and to a target image if necessary\n\n        Arguments\n        ---------\n        transforms : list or tuple\n            each element in the list/tuple should be an affine transform.\n            currently supported transforms:\n                - Rotate()\n                - Translate()\n                - Shear()\n                - Zoom()\n\n        interp : string in {\'bilinear\', \'nearest\'} or list of strings\n            type of interpolation to use. You can provide a different\n            type of interpolation for each input, e.g. if you have two\n            inputs then you can say `interp=[\'bilinear\',\'nearest\']\n\n        """"""\n        self.transforms = transforms\n        self.interp = interp\n        # set transforms to lazy so they only return the tform matrix\n        for t in self.transforms:\n            t.lazy = True\n\n    def __call__(self, *inputs):\n        # collect all of the lazily returned tform matrices\n        tform_matrix = self.transforms[0](inputs[0])\n        for tform in self.transforms[1:]:\n            tform_matrix = tform_matrix.mm(tform(inputs[0])) \n\n        if not isinstance(self.interp, (tuple,list)):\n            interp = [self.interp]*len(inputs)\n        else:\n            interp = self.interp\n\n        outputs = []\n        for idx, _input in enumerate(inputs):\n            input_tf = th_affine2d(_input,\n                                   tform_matrix,\n                                   mode=interp[idx])\n            outputs.append(input_tf)\n        return outputs if idx >= 1 else outputs[0]\n\n\nclass RandomRotate(object):\n\n    def __init__(self, \n                 rotation_range,\n                 interp=\'bilinear\',\n                 lazy=False):\n        """"""\n        Randomly rotate an image between (-degrees, degrees). If the image\n        has multiple channels, the same rotation will be applied to each channel.\n\n        Arguments\n        ---------\n        rotation_range : integer or float\n            image will be rotated between (-degrees, degrees) degrees\n\n        interp : string in {\'bilinear\', \'nearest\'} or list of strings\n            type of interpolation to use. You can provide a different\n            type of interpolation for each input, e.g. if you have two\n            inputs then you can say `interp=[\'bilinear\',\'nearest\']\n\n        lazy    : boolean\n            if true, only create the affine transform matrix and return that\n            if false, perform the transform on the tensor and return the tensor\n        """"""\n        self.rotation_range = rotation_range\n        self.interp = interp\n        self.lazy = lazy\n\n    def __call__(self, *inputs):\n        degree = random.uniform(-self.rotation_range, self.rotation_range)\n\n        if self.lazy:\n            return Rotate(degree, lazy=True)(inputs[0])\n        else:\n            outputs = Rotate(degree,\n                             interp=self.interp)(*inputs)\n            return outputs\n\n\nclass RandomChoiceRotate(object):\n\n    def __init__(self, \n                 values,\n                 p=None,\n                 interp=\'bilinear\',\n                 lazy=False):\n        """"""\n        Randomly rotate an image from a list of values. If the image\n        has multiple channels, the same rotation will be applied to each channel.\n\n        Arguments\n        ---------\n        values : a list or tuple\n            the values from which the rotation value will be sampled\n\n        p : a list or tuple the same length as `values`\n            the probabilities of sampling any given value. Must sum to 1.\n\n        interp : string in {\'bilinear\', \'nearest\'} or list of strings\n            type of interpolation to use. You can provide a different\n            type of interpolation for each input, e.g. if you have two\n            inputs then you can say `interp=[\'bilinear\',\'nearest\']\n\n        lazy    : boolean\n            if true, only create the affine transform matrix and return that\n            if false, perform the transform on the tensor and return the tensor\n        """"""\n        if isinstance(values, (list, tuple)):\n            values = th.FloatTensor(values)\n        self.values = values\n        if p is None:\n            p = th.ones(len(values)) / len(values)\n        else:\n            if abs(1.0-sum(p)) > 1e-3:\n                raise ValueError(\'Probs must sum to 1\')\n        self.p = p\n        self.interp = interp\n        self.lazy = lazy\n\n    def __call__(self, *inputs):\n        degree = th_random_choice(self.values, p=self.p)\n\n        if self.lazy:\n            return Rotate(degree, lazy=True)(inputs[0])\n        else:\n            outputs = Rotate(degree,\n                             interp=self.interp)(*inputs)\n            return outputs\n\n\nclass Rotate(object):\n\n    def __init__(self, \n                 value,\n                 interp=\'bilinear\',\n                 lazy=False):\n        """"""\n        Randomly rotate an image between (-degrees, degrees). If the image\n        has multiple channels, the same rotation will be applied to each channel.\n\n        Arguments\n        ---------\n        rotation_range : integer or float\n            image will be rotated between (-degrees, degrees) degrees\n\n        interp : string in {\'bilinear\', \'nearest\'} or list of strings\n            type of interpolation to use. You can provide a different\n            type of interpolation for each input, e.g. if you have two\n            inputs then you can say `interp=[\'bilinear\',\'nearest\']\n\n        lazy    : boolean\n            if true, only create the affine transform matrix and return that\n            if false, perform the transform on the tensor and return the tensor\n        """"""\n        self.value = value\n        self.interp = interp\n        self.lazy = lazy\n\n    def __call__(self, *inputs):\n        if not isinstance(self.interp, (tuple,list)):\n            interp = [self.interp]*len(inputs)\n        else:\n            interp = self.interp\n\n        theta = math.pi / 180 * self.value\n        rotation_matrix = th.FloatTensor([[math.cos(theta), -math.sin(theta), 0],\n                                          [math.sin(theta), math.cos(theta), 0],\n                                          [0, 0, 1]])\n        if self.lazy:\n            return rotation_matrix\n        else:\n            outputs = []\n            for idx, _input in enumerate(inputs):\n                input_tf = th_affine2d(_input,\n                                       rotation_matrix,\n                                       mode=interp[idx],\n                                       center=True)\n                outputs.append(input_tf)\n            return outputs if idx >= 1 else outputs[0]\n\n\nclass RandomTranslate(object):\n\n    def __init__(self, \n                 translation_range,\n                 interp=\'bilinear\',\n                 lazy=False):\n        """"""\n        Randomly translate an image some fraction of total height and/or\n        some fraction of total width. If the image has multiple channels,\n        the same translation will be applied to each channel.\n\n        Arguments\n        ---------\n        translation_range : two floats between [0, 1) \n            first value:\n                fractional bounds of total height to shift image\n                image will be horizontally shifted between \n                (-height_range * height_dimension, height_range * height_dimension)\n            second value:\n                fractional bounds of total width to shift image \n                Image will be vertically shifted between \n                (-width_range * width_dimension, width_range * width_dimension)\n\n        interp : string in {\'bilinear\', \'nearest\'} or list of strings\n            type of interpolation to use. You can provide a different\n            type of interpolation for each input, e.g. if you have two\n            inputs then you can say `interp=[\'bilinear\',\'nearest\']\n\n        lazy    : boolean\n            if true, only create the affine transform matrix and return that\n            if false, perform the transform on the tensor and return the tensor\n        """"""\n        if isinstance(translation_range, float):\n            translation_range = (translation_range, translation_range)\n        self.height_range = translation_range[0]\n        self.width_range = translation_range[1]\n        self.interp = interp\n        self.lazy = lazy\n\n    def __call__(self, *inputs):\n        # height shift\n        random_height = random.uniform(-self.height_range, self.height_range)\n        # width shift\n        random_width = random.uniform(-self.width_range, self.width_range)\n\n        if self.lazy:\n            return Translate([random_height, random_width], \n                             lazy=True)(inputs[0])\n        else:\n            outputs = Translate([random_height, random_width],\n                                 interp=self.interp)(*inputs)\n            return outputs\n\n\nclass RandomChoiceTranslate(object):\n\n    def __init__(self,\n                 values,\n                 p=None,\n                 interp=\'bilinear\',\n                 lazy=False):\n        """"""\n        Randomly translate an image some fraction of total height and/or\n        some fraction of total width from a list of potential values. \n        If the image has multiple channels,\n        the same translation will be applied to each channel.\n\n        Arguments\n        ---------\n        values : a list or tuple\n            the values from which the translation value will be sampled\n\n        p : a list or tuple the same length as `values`\n            the probabilities of sampling any given value. Must sum to 1.\n\n        interp : string in {\'bilinear\', \'nearest\'} or list of strings\n            type of interpolation to use. You can provide a different\n            type of interpolation for each input, e.g. if you have two\n            inputs then you can say `interp=[\'bilinear\',\'nearest\']\n\n        lazy    : boolean\n            if true, only create the affine transform matrix and return that\n            if false, perform the transform on the tensor and return the tensor\n        """"""\n        if isinstance(values, (list, tuple)):\n            values = th.FloatTensor(values)\n        self.values = values\n        if p is None:\n            p = th.ones(len(values)) / len(values)\n        else:\n            if abs(1.0-sum(p)) > 1e-3:\n                raise ValueError(\'Probs must sum to 1\')\n        self.p = p\n        self.interp = interp\n        self.lazy = lazy\n\n    def __call__(self, *inputs):\n        random_height = th_random_choice(self.values, p=self.p)\n        random_width = th_random_choice(self.values, p=self.p)\n\n        if self.lazy:\n            return Translate([random_height, random_width],\n                             lazy=True)(inputs[0])\n        else:\n            outputs = Translate([random_height, random_width],\n                                interp=self.interp)(*inputs)\n            return outputs\n\n\nclass Translate(object):\n\n    def __init__(self, \n                 value, \n                 interp=\'bilinear\',\n                 lazy=False):\n        """"""\n        Arguments\n        ---------\n        value : float or 2-tuple of float\n            if single value, both horizontal and vertical translation\n            will be this value * total height/width. Thus, value should\n            be a fraction of total height/width with range (-1, 1)\n\n        interp : string in {\'bilinear\', \'nearest\'} or list of strings\n            type of interpolation to use. You can provide a different\n            type of interpolation for each input, e.g. if you have two\n            inputs then you can say `interp=[\'bilinear\',\'nearest\']\n\n        """"""\n        if not isinstance(value, (tuple,list)):\n            value = (value, value)\n\n        if value[0] > 1 or value[0] < -1:\n            raise ValueError(\'Translation must be between -1 and 1\')\n        if value[1] > 1 or value[1] < -1:\n            raise ValueError(\'Translation must be between -1 and 1\')\n\n        self.height_range = value[0]\n        self.width_range = value[1]\n        self.interp = interp\n        self.lazy = lazy\n\n    def __call__(self, *inputs):\n        if not isinstance(self.interp, (tuple,list)):\n            interp = [self.interp]*len(inputs)\n        else:\n            interp = self.interp\n\n        tx = self.height_range * inputs[0].size(1)\n        ty = self.width_range * inputs[0].size(2)\n\n        translation_matrix = th.FloatTensor([[1, 0, tx],\n                                             [0, 1, ty],\n                                             [0, 0, 1]])\n        if self.lazy:\n            return translation_matrix\n        else:\n            outputs = []\n            for idx, _input in enumerate(inputs):\n                input_tf = th_affine2d(_input,\n                                       translation_matrix,\n                                       mode=interp[idx],\n                                       center=True)\n                outputs.append(input_tf)\n            return outputs if idx >= 1 else outputs[0]\n\n\nclass RandomShear(object):\n\n    def __init__(self, \n                 shear_range,\n                 interp=\'bilinear\',\n                 lazy=False):\n        """"""\n        Randomly shear an image with radians (-shear_range, shear_range)\n\n        Arguments\n        ---------\n        shear_range : float\n            radian bounds on the shear transform\n        \n        interp : string in {\'bilinear\', \'nearest\'} or list of strings\n            type of interpolation to use. You can provide a different\n            type of interpolation for each input, e.g. if you have two\n            inputs then you can say `interp=[\'bilinear\',\'nearest\']\n\n        lazy    : boolean\n            if false, perform the transform on the tensor and return the tensor\n            if true, only create the affine transform matrix and return that\n        """"""\n        self.shear_range = shear_range\n        self.interp = interp\n        self.lazy = lazy\n\n    def __call__(self, *inputs):\n        shear = random.uniform(-self.shear_range, self.shear_range)\n        if self.lazy:\n            return Shear(shear, \n                         lazy=True)(inputs[0])\n        else:\n            outputs = Shear(shear,\n                            interp=self.interp)(*inputs)\n            return outputs\n\n\nclass RandomChoiceShear(object):\n\n    def __init__(self,\n                 values,\n                 p=None,\n                 interp=\'bilinear\',\n                 lazy=False):\n        """"""\n        Randomly shear an image with a value sampled from a list of values.\n\n        Arguments\n        ---------\n        values : a list or tuple\n            the values from which the rotation value will be sampled\n\n        p : a list or tuple the same length as `values`\n            the probabilities of sampling any given value. Must sum to 1.\n\n        interp : string in {\'bilinear\', \'nearest\'} or list of strings\n            type of interpolation to use. You can provide a different\n            type of interpolation for each input, e.g. if you have two\n            inputs then you can say `interp=[\'bilinear\',\'nearest\']\n\n        lazy    : boolean\n            if false, perform the transform on the tensor and return the tensor\n            if true, only create the affine transform matrix and return that\n        """"""\n        if isinstance(values, (list, tuple)):\n            values = th.FloatTensor(values)\n        self.values = values\n        if p is None:\n            p = th.ones(len(values)) / len(values)\n        else:\n            if abs(1.0-sum(p)) > 1e-3:\n                raise ValueError(\'Probs must sum to 1\')\n        self.p = p\n        self.interp = interp\n        self.lazy = lazy\n\n    def __call__(self, *inputs):\n        shear = th_random_choice(self.values, p=self.p)\n\n        if self.lazy:\n            return Shear(shear, \n                         lazy=True)(inputs[0])\n        else:\n            outputs = Shear(shear,\n                            interp=self.interp)(*inputs)\n            return outputs \n\n\nclass Shear(object):\n\n    def __init__(self,\n                 value,\n                 interp=\'bilinear\',\n                 lazy=False):\n        self.value = value\n        self.interp = interp\n        self.lazy = lazy\n\n    def __call__(self, *inputs):\n        if not isinstance(self.interp, (tuple,list)):\n            interp = [self.interp]*len(inputs)\n        else:\n            interp = self.interp\n\n        theta = (math.pi * self.value) / 180\n        shear_matrix = th.FloatTensor([[1, -math.sin(theta), 0],\n                                        [0, math.cos(theta), 0],\n                                        [0, 0, 1]])\n        if self.lazy:\n            return shear_matrix\n        else:\n            outputs = []\n            for idx, _input in enumerate(inputs):\n                input_tf = th_affine2d(_input,\n                                       shear_matrix,\n                                       mode=interp[idx],\n                                       center=True)\n                outputs.append(input_tf)\n            return outputs if idx >= 1 else outputs[0]\n\n\nclass RandomSquareZoom(object):\n\n    def __init__(self,\n                 zoom_range,\n                 interp=\'bilinear\',\n                 lazy=False):\n        """"""\n        Randomly zoom in and/or out on an image\n        Arguments\n        ---------\n        zoom_range : tuple or list with 2 values, both between (0, infinity)\n            lower and upper bounds on percent zoom.\n            Anything less than 1.0 will zoom in on the image,\n            anything greater than 1.0 will zoom out on the image.\n            e.g. (0.7, 1.0) will only zoom in,\n                 (1.0, 1.4) will only zoom out,\n                 (0.7, 1.4) will randomly zoom in or out\n        interp : string in {\'bilinear\', \'nearest\'} or list of strings\n            type of interpolation to use. You can provide a different\n            type of interpolation for each input, e.g. if you have two\n            inputs then you can say `interp=[\'bilinear\',\'nearest\']\n        lazy    : boolean\n            if false, perform the transform on the tensor and return the tensor\n            if true, only create the affine transform matrix and return that\n        """"""\n        if not isinstance(zoom_range, list) and not isinstance(zoom_range, tuple):\n            raise ValueError(\'zoom_range must be tuple or list with 2 values\')\n        self.zoom_range = zoom_range\n        self.interp = interp\n        self.lazy = lazy\n\n    def __call__(self, *inputs):\n        zx = random.uniform(self.zoom_range[0], self.zoom_range[1])\n        zy = zx\n        if self.lazy:\n            return Zoom([zx, zy], lazy=True)(inputs[0])\n        else:\n            outputs = Zoom([zx, zy],\n                           interp=self.interp)(*inputs)\n        return outputs\n\nclass RandomZoom(object):\n\n    def __init__(self, \n                 zoom_range,\n                 interp=\'bilinear\',\n                 lazy=False):\n        """"""\n        Randomly zoom in and/or out on an image \n\n        Arguments\n        ---------\n        zoom_range : tuple or list with 2 values, both between (0, infinity)\n            lower and upper bounds on percent zoom. \n            Anything less than 1.0 will zoom in on the image, \n            anything greater than 1.0 will zoom out on the image.\n            e.g. (0.7, 1.0) will only zoom in, \n                 (1.0, 1.4) will only zoom out,\n                 (0.7, 1.4) will randomly zoom in or out\n\n        interp : string in {\'bilinear\', \'nearest\'} or list of strings\n            type of interpolation to use. You can provide a different\n            type of interpolation for each input, e.g. if you have two\n            inputs then you can say `interp=[\'bilinear\',\'nearest\']\n\n        lazy    : boolean\n            if false, perform the transform on the tensor and return the tensor\n            if true, only create the affine transform matrix and return that\n        """"""\n        if not isinstance(zoom_range, list) and not isinstance(zoom_range, tuple):\n            raise ValueError(\'zoom_range must be tuple or list with 2 values\')\n        self.zoom_range = zoom_range\n        self.interp = interp\n        self.lazy = lazy\n\n    def __call__(self, *inputs):\n        zx = random.uniform(self.zoom_range[0], self.zoom_range[1])\n        zy = random.uniform(self.zoom_range[0], self.zoom_range[1])\n\n        if self.lazy:\n            return Zoom([zx, zy], lazy=True)(inputs[0])\n        else:\n            outputs = Zoom([zx, zy], \n                           interp=self.interp)(*inputs)\n            return outputs\n\n\nclass RandomChoiceZoom(object):\n\n    def __init__(self, \n                 values,\n                 p=None,\n                 interp=\'bilinear\',\n                 lazy=False):\n        """"""\n        Randomly zoom in and/or out on an image with a value sampled from\n        a list of values\n\n        Arguments\n        ---------\n        values : a list or tuple\n            the values from which the applied zoom value will be sampled\n\n        p : a list or tuple the same length as `values`\n            the probabilities of sampling any given value. Must sum to 1.\n\n        interp : string in {\'bilinear\', \'nearest\'} or list of strings\n            type of interpolation to use. You can provide a different\n            type of interpolation for each input, e.g. if you have two\n            inputs then you can say `interp=[\'bilinear\',\'nearest\']\n\n        lazy    : boolean\n            if false, perform the transform on the tensor and return the tensor\n            if true, only create the affine transform matrix and return that\n        """"""\n        if isinstance(values, (list, tuple)):\n            values = th.FloatTensor(values)\n        self.values = values\n        if p is None:\n            p = th.ones(len(values)) / len(values)\n        else:\n            if abs(1.0-sum(p)) > 1e-3:\n                raise ValueError(\'Probs must sum to 1\')\n        self.p = p\n        self.interp = interp\n        self.lazy = lazy\n\n    def __call__(self, *inputs):\n        zx = th_random_choice(self.values, p=self.p)\n        zy = th_random_choice(self.values, p=self.p)\n\n        if self.lazy:\n            return Zoom([zx, zy], lazy=True)(inputs[0])\n        else:\n            outputs = Zoom([zx, zy], \n                           interp=self.interp)(*inputs)\n            return outputs\n\n\nclass Zoom(object):\n\n    def __init__(self,\n                 value,\n                 interp=\'bilinear\',\n                 lazy=False):\n        """"""\n        Arguments\n        ---------\n        value : float\n            Fractional zoom.\n            =1 : no zoom\n            >1 : zoom-in (value-1)%\n            <1 : zoom-out (1-value)%\n\n        interp : string in {\'bilinear\', \'nearest\'} or list of strings\n            type of interpolation to use. You can provide a different\n            type of interpolation for each input, e.g. if you have two\n            inputs then you can say `interp=[\'bilinear\',\'nearest\']\n\n        lazy: boolean\n            If true, just return transformed\n        """"""\n\n        if not isinstance(value, (tuple,list)):\n            value = (value, value)\n        self.value = value\n        self.interp = interp\n        self.lazy = lazy\n\n    def __call__(self, *inputs):\n        if not isinstance(self.interp, (tuple,list)):\n            interp = [self.interp]*len(inputs)\n        else:\n            interp = self.interp\n\n        zx, zy = self.value\n        zoom_matrix = th.FloatTensor([[zx, 0, 0],\n                                      [0, zy, 0],\n                                      [0, 0,  1]])        \n\n        if self.lazy:\n            return zoom_matrix\n        else:\n            outputs = []\n            for idx, _input in enumerate(inputs):\n                input_tf = th_affine2d(_input,\n                                       zoom_matrix,\n                                       mode=interp[idx],\n                                       center=True)\n                outputs.append(input_tf)\n            return outputs if idx >= 1 else outputs[0]\n\n\n'"
pywick/transforms/distortion_transforms.py,0,"b'""""""\nTransforms to distort local or global information of an image\n""""""\n\n\nimport torch as th\nimport numpy as np\nimport random\n\n\nclass Scramble(object):\n    """"""\n    Create blocks of an image and scramble them\n    """"""\n    def __init__(self, blocksize):\n        self.blocksize = blocksize\n\n    def __call__(self, *inputs):\n        outputs = []\n        for idx, _input in enumerate(inputs):\n            size = _input.size()\n            img_height = size[1]\n            img_width = size[2]\n\n            x_blocks = int(img_height/self.blocksize) # number of x blocks\n            y_blocks = int(img_width/self.blocksize)\n            ind = th.randperm(x_blocks*y_blocks)\n\n            new = th.zeros(_input.size())\n            count = 0\n            for i in range(x_blocks):\n                for j in range (y_blocks):\n                    row = int(ind[count] / x_blocks)\n                    column = ind[count] % x_blocks\n                    new[:, i*self.blocksize:(i+1)*self.blocksize, j*self.blocksize:(j+1)*self.blocksize] = \\\n                    _input[:, row*self.blocksize:(row+1)*self.blocksize, column*self.blocksize:(column+1)*self.blocksize]\n                    count += 1\n            outputs.append(new)\n        return outputs if idx >= 1 else outputs[0]\n \n\nclass RandomChoiceScramble(object):\n\n    def __init__(self, blocksizes):\n        self.blocksizes = blocksizes\n\n    def __call__(self, *inputs):\n        blocksize = random.choice(self.blocksizes)\n        outputs = Scramble(blocksize=blocksize)(*inputs)\n        return outputs\n\n\ndef _blur_image(image, H):\n    # break image up into its color components\n    size = image.shape\n    imr = image[0,:,:]\n    img = image[1,:,:]\n    imb = image[2,:,:]\n\n    # compute Fourier transform and frequqnecy spectrum\n    Fim1r = np.fft.fftshift(np.fft.fft2(imr))\n    Fim1g  = np.fft.fftshift(np.fft.fft2(img))\n    Fim1b  = np.fft.fftshift(np.fft.fft2(imb))\n    \n    # Apply the lowpass filter to the Fourier spectrum of the image\n    filtered_imager = np.multiply(H, Fim1r)\n    filtered_imageg = np.multiply(H, Fim1g)\n    filtered_imageb = np.multiply(H, Fim1b)\n    \n    newim = np.zeros(size)\n\n    # convert the result to the spatial domain.\n    newim[0,:,:] = np.absolute(np.real(np.fft.ifft2(filtered_imager)))\n    newim[1,:,:] = np.absolute(np.real(np.fft.ifft2(filtered_imageg)))\n    newim[2,:,:] = np.absolute(np.real(np.fft.ifft2(filtered_imageb)))\n\n    return newim.astype(\'uint8\')\n\ndef _butterworth_filter(rows, cols, thresh, order):\n    # X and Y matrices with ranges normalised to +/- 0.5\n    array1 = np.ones(rows)\n    array2 = np.ones(cols)\n    array3 = np.arange(1,rows+1)\n    array4 = np.arange(1,cols+1)\n\n    x = np.outer(array1, array4)\n    y = np.outer(array3, array2)\n\n    x = x - float(cols/2) - 1\n    y = y - float(rows/2) - 1\n\n    x = x / cols\n    y = y / rows\n\n    radius = np.sqrt(np.square(x) + np.square(y))\n\n    matrix1 = radius/thresh\n    matrix2 = np.power(matrix1, 2*order)\n    f = np.reciprocal(1 + matrix2)\n\n    return f\n\n\nclass Blur(object):\n    """"""\n    Blur an image with a Butterworth filter with a frequency\n    cutoff matching local block size\n    """"""\n    def __init__(self, threshold, order=5):\n        """"""\n        scramble blocksize of 128 => filter threshold of 64\n        scramble blocksize of 64 => filter threshold of 32\n        scramble blocksize of 32 => filter threshold of 16\n        scramble blocksize of 16 => filter threshold of 8\n        scramble blocksize of 8 => filter threshold of 4\n        """"""\n        self.threshold = threshold\n        self.order = order\n\n    def __call__(self, *inputs):\n        """"""\n        inputs should have values between 0 and 255\n        """"""\n        outputs = []\n        for idx, _input in enumerate(inputs):\n            rows = _input.size(1)\n            cols = _input.size(2)\n            fc = self.threshold # threshold\n            fs = 128.0 # max frequency\n            n  = self.order # filter order\n            fc_rad = (fc/fs)*0.5\n            H = _butterworth_filter(rows, cols, fc_rad, n)\n            _input_blurred = _blur_image(_input.numpy().astype(\'uint8\'), H)\n            _input_blurred = th.from_numpy(_input_blurred).float()\n            outputs.append(_input_blurred)\n\n        return outputs if idx >= 1 else outputs[0]\n\n\nclass RandomChoiceBlur(object):\n\n    def __init__(self, thresholds, order=5):\n        """"""\n        thresholds = [64.0, 32.0, 16.0, 8.0, 4.0]\n        """"""\n        self.thresholds = thresholds\n        self.order = order\n\n    def __call__(self, *inputs):\n        threshold = random.choice(self.thresholds)\n        outputs = Blur(threshold=threshold, order=self.order)(*inputs)\n        return outputs\n\n\n\n\n\n\n'"
pywick/transforms/image_transforms.py,0,"b'""""""\nTransforms very specific to images such as \ncolor, lighting, contrast, brightness, etc transforms\n\nNOTE: Most of these transforms assume your image intensity\nis between 0 and 1, and are torch tensors (NOT numpy or PIL)\n""""""\n\nimport random\n\nimport torch as th\n\nfrom ..utils import th_random_choice\n\n\ndef _blend(img1, img2, alpha):\n    """"""\n    Weighted sum of two images\n\n    Arguments\n    ---------\n    img1 : torch tensor\n    img2 : torch tensor\n    alpha : float between 0 and 1\n        how much weight to put on img1 and 1-alpha weight\n        to put on img2\n    """"""\n    return img1.mul(alpha).add(1 - alpha, img2)\n\n\nclass Grayscale(object):\n\n    def __init__(self, keep_channels=False):\n        """"""\n        Convert RGB image to grayscale\n\n        Arguments\n        ---------\n        keep_channels : boolean\n            If true, will keep all 3 channels and they will be the same\n            If false, will just return 1 grayscale channel\n        """"""\n        self.keep_channels = keep_channels\n        if keep_channels:\n            self.channels = 3\n        else:\n            self.channels = 1\n\n    def __call__(self, *inputs):\n        outputs = []\n        for idx, _input in enumerate(inputs):\n            _input_dst = _input[0]*0.299 + _input[1]*0.587 + _input[2]*0.114\n            _input_gs = _input_dst.repeat(self.channels,1,1)\n            outputs.append(_input_gs)\n        return outputs if idx >= 1 else outputs[0]\n\nclass RandomGrayscale(object):\n\n    def __init__(self, p=0.5):\n        """"""\n        Randomly convert RGB image(s) to Grayscale w/ some probability,\n        NOTE: Always retains the 3 channels if image is grayscaled\n\n        p : a float\n            probability that image will be grayscaled\n        """"""\n        self.p = p\n\n    def __call__(self, *inputs):\n        pval = random.random()\n        if pval < self.p:\n            outputs = Grayscale(keep_channels=True)(*inputs)\n        else:\n            outputs = inputs\n        return outputs\n\n# ----------------------------------------------------\n# ----------------------------------------------------\n\nclass Gamma(object):\n\n    def __init__(self, value):\n        """"""\n        Performs Gamma Correction on the input image. Also known as \n        Power Law Transform. This function transforms the input image \n        pixelwise according \n        to the equation Out = In**gamma after scaling each \n        pixel to the range 0 to 1.\n\n        Arguments\n        ---------\n        value : float\n            <1 : image will tend to be lighter\n            =1 : image will stay the same\n            >1 : image will tend to be darker\n        """"""\n        self.value = value\n\n    def __call__(self, *inputs):\n        outputs = []\n        for idx, _input in enumerate(inputs):\n            _input = th.pow(_input, self.value)\n            outputs.append(_input)\n        return outputs if idx >= 1 else outputs[0]\n\nclass RandomGamma(object):\n\n    def __init__(self, min_val, max_val):\n        """"""\n        Performs Gamma Correction on the input image with some\n        randomly selected gamma value between min_val and max_val. \n        Also known as Power Law Transform. This function transforms \n        the input image pixelwise according to the equation \n        Out = In**gamma after scaling each pixel to the range 0 to 1.\n\n        Arguments\n        ---------\n        min_val : float\n            min range\n        max_val : float\n            max range\n\n        NOTE:\n        for values:\n            <1 : image will tend to be lighter\n            =1 : image will stay the same\n            >1 : image will tend to be darker\n        """"""\n        self.values = (min_val, max_val)\n\n    def __call__(self, *inputs):\n        value = random.uniform(self.values[0], self.values[1])\n        outputs = Gamma(value)(*inputs)\n        return outputs\n\nclass RandomChoiceGamma(object):\n\n    def __init__(self, values, p=None):\n        """"""\n        Performs Gamma Correction on the input image with some\n        gamma value selected in the list of given values.\n        Also known as Power Law Transform. This function transforms \n        the input image pixelwise according to the equation \n        Out = In**gamma after scaling each pixel to the range 0 to 1.\n\n        Arguments\n        ---------\n        values : list of floats\n            gamma values to sampled from\n        p : list of floats - same length as `values`\n            if None, values will be sampled uniformly.\n            Must sum to 1.\n\n        NOTE:\n        for values:\n            <1 : image will tend to be lighter\n            =1 : image will stay the same\n            >1 : image will tend to be darker\n        """"""\n        self.values = values\n        self.p = p\n\n    def __call__(self, *inputs):\n        value = th_random_choice(self.values, p=self.p)\n        outputs = Gamma(value)(*inputs)\n        return outputs\n\n# ----------------------------------------------------\n# ----------------------------------------------------\n\nclass Brightness(object):\n    def __init__(self, value):\n        """"""\n        Alter the Brightness of an image\n\n        Arguments\n        ---------\n        value : brightness factor\n            =-1 = completely black\n            <0 = darker\n            0 = no change\n            >0 = brighter\n            =1 = completely white\n        """"""\n        self.value = max(min(value,1.0),-1.0)\n\n    def __call__(self, *inputs):\n        outputs = []\n        for idx, _input in enumerate(inputs):\n            _input = th.clamp(_input.float().add(self.value).type(_input.type()), 0, 1)\n            outputs.append(_input)\n        return outputs if idx >= 1 else outputs[0]\n\nclass RandomBrightness(object):\n\n    def __init__(self, min_val, max_val):\n        """"""\n        Alter the Brightness of an image with a value randomly selected\n        between `min_val` and `max_val`\n\n        Arguments\n        ---------\n        min_val : float\n            min range\n        max_val : float\n            max range\n        """"""\n        self.values = (min_val, max_val)\n\n    def __call__(self, *inputs):\n        value = random.uniform(self.values[0], self.values[1])\n        outputs = Brightness(value)(*inputs)\n        return outputs\n\nclass RandomChoiceBrightness(object):\n\n    def __init__(self, values, p=None):\n        """"""\n        Alter the Brightness of an image with a value randomly selected\n        from the list of given values with given probabilities\n\n        Arguments\n        ---------\n        values : list of floats\n            brightness values to sampled from\n        p : list of floats - same length as `values`\n            if None, values will be sampled uniformly.\n            Must sum to 1.\n        """"""\n        self.values = values\n        self.p = p\n\n    def __call__(self, *inputs):\n        value = th_random_choice(self.values, p=self.p)\n        outputs = Brightness(value)(*inputs)\n        return outputs\n\n# ----------------------------------------------------\n# ----------------------------------------------------\n\nclass Saturation(object):\n\n    def __init__(self, value):\n        """"""\n        Alter the Saturation of image\n\n        Arguments\n        ---------\n        value : float\n            =-1 : gray\n            <0 : colors are more muted\n            =0 : image stays the same\n            >0 : colors are more pure\n            =1 : most saturated\n        """"""\n        self.value = max(min(value,1.0),-1.0)\n\n    def __call__(self, *inputs):\n        outputs = []\n        for idx, _input in enumerate(inputs):\n            _in_gs = Grayscale(keep_channels=True)(_input)\n            alpha = 1.0 + self.value\n            _in = th.clamp(_blend(_input, _in_gs, alpha), 0, 1)\n            outputs.append(_in)\n        return outputs if idx >= 1 else outputs[0]\n\nclass RandomSaturation(object):\n\n    def __init__(self, min_val, max_val):\n        """"""\n        Alter the Saturation of an image with a value randomly selected\n        between `min_val` and `max_val`\n\n        Arguments\n        ---------\n        min_val : float\n            min range\n        max_val : float\n            max range\n        """"""\n        self.values = (min_val, max_val)\n\n    def __call__(self, *inputs):\n        value = random.uniform(self.values[0], self.values[1])\n        outputs = Saturation(value)(*inputs)\n        return outputs\n\nclass RandomChoiceSaturation(object):\n\n    def __init__(self, values, p=None):\n        """"""\n        Alter the Saturation of an image with a value randomly selected\n        from the list of given values with given probabilities\n\n        Arguments\n        ---------\n        values : list of floats\n            saturation values to sampled from\n        p : list of floats - same length as `values`\n            if None, values will be sampled uniformly.\n            Must sum to 1.\n\n        """"""\n        self.values = values\n        self.p = p\n\n    def __call__(self, *inputs):\n        value = th_random_choice(self.values, p=self.p)\n        outputs = Saturation(value)(*inputs)\n        return outputs\n\n# ----------------------------------------------------\n# ----------------------------------------------------\n\nclass Contrast(object):\n    """"""\n\n    """"""\n    def __init__(self, value):\n        """"""\n        Adjust Contrast of image.\n\n        Contrast is adjusted independently for each channel of each image.\n\n        For each channel, this Op computes the mean of the image pixels \n        in the channel and then adjusts each component x of each pixel to \n        (x - mean) * contrast_factor + mean.\n\n        Arguments\n        ---------\n        value : float\n            smaller value: less contrast\n            ZERO: channel means\n            larger positive value: greater contrast\n            larger negative value: greater inverse contrast\n        """"""\n        self.value = value\n\n    def __call__(self, *inputs):\n        outputs = []\n        for idx, _input in enumerate(inputs):\n            channel_means = _input.mean(1, keepdim=True).mean(2, keepdim=True)\n            channel_means = channel_means.expand_as(_input)\n            _input = th.clamp((_input - channel_means) * self.value + channel_means,0,1)\n            outputs.append(_input)\n        return outputs if idx >= 1 else outputs[0]\n\nclass RandomContrast(object):\n\n    def __init__(self, min_val, max_val):\n        """"""\n        Alter the Contrast of an image with a value randomly selected\n        between `min_val` and `max_val`\n\n        Arguments\n        ---------\n        min_val : float\n            min range\n        max_val : float\n            max range\n        """"""\n        self.values = (min_val, max_val)\n\n    def __call__(self, *inputs):\n        value = random.uniform(self.values[0], self.values[1])\n        outputs = Contrast(value)(*inputs)\n        return outputs\n\nclass RandomChoiceContrast(object):\n\n    def __init__(self, values, p=None):\n        """"""\n        Alter the Contrast of an image with a value randomly selected\n        from the list of given values with given probabilities\n\n        Arguments\n        ---------\n        values : list of floats\n            contrast values to sampled from\n        p : list of floats - same length as `values`\n            if None, values will be sampled uniformly.\n            Must sum to 1.\n\n        """"""\n        self.values = values\n        self.p = p\n\n    def __call__(self, *inputs):\n        value = th_random_choice(self.values, p=self.p)\n        outputs = Contrast(value)(*inputs)\n        return outputs\n\n# ----------------------------------------------------\n# ----------------------------------------------------\n\ndef rgb_to_hsv(x):\n    """"""\n    Convert from RGB to HSV\n    """"""\n    hsv = th.zeros(*x.size())\n    c_min = x.min(0)\n    c_max = x.max(0)\n\n    delta = c_max[0] - c_min[0]\n\n    # set H\n    r_idx = c_max[1].eq(0)\n    hsv[0][r_idx] = ((x[1][r_idx] - x[2][r_idx]) / delta[r_idx]) % 6\n    g_idx = c_max[1].eq(1)\n    hsv[0][g_idx] = 2 + ((x[2][g_idx] - x[0][g_idx]) / delta[g_idx])\n    b_idx = c_max[1].eq(2)\n    hsv[0][b_idx] = 4 + ((x[0][b_idx] - x[1][b_idx]) / delta[b_idx])\n    hsv[0] = hsv[0].mul(60)\n\n    # set S\n    hsv[1] = delta / c_max[0]\n\n    # set V - good\n    hsv[2] = c_max[0]\n\n    return hsv\n'"
pywick/transforms/tensor_transforms.py,3,"b'\nimport os\nimport random\nimport math\nimport numpy as np\n\nimport torch as th\n\n\nclass Compose(object):\n    """"""\n    Composes (chains) several transforms together.\n\n    :param transforms: (list of transforms) to apply sequentially\n    """"""\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, *inputs):\n        for transform in self.transforms:\n            if not isinstance(inputs, (list,tuple)):\n                inputs = [inputs]\n            inputs = transform(*inputs)\n        return inputs\n\n\nclass RandomChoiceCompose(object):\n    """"""\n    Randomly choose to apply one transform from a collection of transforms\n\n    e.g. to randomly apply EITHER 0-1 or -1-1 normalization to an input:\n        >>> transform = RandomChoiceCompose([RangeNormalize(0,1),\n                                             RangeNormalize(-1,1)])\n        >>> x_norm = transform(x) # only one of the two normalizations is applied\n\n    :param transforms: (list of transforms) to choose from at random\n    """"""\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, *inputs):\n        tform = random.choice(self.transforms)\n        outputs = tform(*inputs)\n        return outputs\n\n\nclass ToTensor(object):\n    """"""\n    Converts a numpy array to torch.Tensor\n    """"""\n    def __call__(self, *inputs):\n        outputs = []\n        for idx, _input in enumerate(inputs):\n            _input = th.from_numpy(_input)\n            outputs.append(_input)\n        return outputs if idx >= 1 else outputs[0]\n\n\nclass ToFile(object):\n    """"""\n    Saves an image to file. Useful as a pass-through transform\n    when wanting to observe how augmentation affects the data\n\n    NOTE: Only supports saving to Numpy currently\n\n    :param root: (string):\n            path to main directory in which images will be saved\n    """"""\n    def __init__(self, root):\n        if root.startswith(\'~\'):\n            root = os.path.expanduser(root)\n        self.root = root\n        self.counter = 0\n\n    def __call__(self, *inputs):\n        for idx, _input in inputs:\n            fpath = os.path.join(self.root, \'img_%i_%i.npy\'%(self.counter, idx))\n            np.save(fpath, _input.numpy())\n        self.counter += 1\n        return inputs\n\n\nclass ToNumpyType(object):\n    """"""\n    Converts an object to a specific numpy type (with the idea to be passed to ToTensor() next)\n\n    :param type:  (one of `{numpy.double, numpy.float, numpy.int64, numpy.int32, and numpy.uint8})\n    """"""\n\n    def __init__(self, type):\n        self.type = type\n\n    def __call__(self, input):\n        if isinstance(input, list):     # handle a simple list\n            return np.array(input, dtype=self.type)\n        else:                           # handle ndarray (that is of a different type than desired)\n            return input.astype(self.type)\n\n\nclass ChannelsLast(object):\n    """"""\n    Transposes a tensor so that the channel dim is last\n    `HWC` and `DHWC` are aliases for this transform.\n\n    :param safe_check: (bool):\n        if true, will check if channels are already last and, if so,\n        will just return the inputs\n    """"""\n    def __init__(self, safe_check=False):\n        self.safe_check = safe_check\n\n    def __call__(self, *inputs):\n        ndim = inputs[0].dim()\n        if self.safe_check:\n            # check if channels are already last\n            if inputs[0].size(-1) < inputs[0].size(0):\n                return inputs\n        plist = list(range(1,ndim))+[0]\n\n        outputs = []\n        for idx, _input in enumerate(inputs):\n            _input = _input.permute(*plist)\n            outputs.append(_input)\n        return outputs if idx >= 1 else outputs[0]\n\nHWC = ChannelsLast\nDHWC = ChannelsLast\n\n\nclass ChannelsFirst(object):\n    """"""\n    Transposes a tensor so that the channel dim is first.\n    `CHW` and `CDHW` are aliases for this transform.\n\n    :param safe_check: (bool):\n        if true, will check if channels are already first and, if so,\n        will just return the inputs\n    """"""\n    def __init__(self, safe_check=False):\n        self.safe_check = safe_check\n\n    def __call__(self, *inputs):\n        ndim = inputs[0].dim()\n        if self.safe_check:\n            # check if channels are already first\n            if inputs[0].size(0) < inputs[0].size(-1):\n                return inputs\n        plist = [ndim-1] + list(range(0,ndim-1))\n\n        outputs = []\n        for idx, _input in enumerate(inputs):\n            _input = _input.permute(*plist)\n            outputs.append(_input)\n        return outputs if idx >= 1 else outputs[0]\n\nCHW = ChannelsFirst\nCDHW = ChannelsFirst\n\n\nclass TypeCast(object):\n    """"""\n    Cast a torch.Tensor to a different type\n    param dtype: (string or torch.*Tensor literal or list) of such\n            data type to which input(s) will be cast.\n            If list, it should be the same length as inputs.\n    """"""\n    def __init__(self, dtype=\'float\'):\n        if isinstance(dtype, (list,tuple)):\n            dtypes = []\n            for dt in dtype:\n                if isinstance(dt, str):\n                    if dt == \'byte\':\n                        dt = th.ByteTensor\n                    elif dt == \'double\':\n                        dt = th.DoubleTensor\n                    elif dt == \'float\':\n                        dt = th.FloatTensor\n                    elif dt == \'int\':\n                        dt = th.IntTensor\n                    elif dt == \'long\':\n                        dt = th.LongTensor\n                    elif dt == \'short\':\n                        dt = th.ShortTensor\n                dtypes.append(dt)\n            self.dtype = dtypes\n        else:\n            if isinstance(dtype, str):\n                if dtype == \'byte\':\n                    dtype = th.ByteTensor\n                elif dtype == \'double\':\n                    dtype = th.DoubleTensor\n                elif dtype == \'float\':\n                    dtype = th.FloatTensor\n                elif dtype == \'int\':\n                    dtype = th.IntTensor\n                elif dtype == \'long\':\n                    dtype = th.LongTensor\n                elif dtype == \'short\':\n                    dtype = th.ShortTensor\n            self.dtype = dtype\n\n    def __call__(self, *inputs):\n        if not isinstance(self.dtype, (tuple,list)):\n            dtypes = [self.dtype]*len(inputs)\n        else:\n            dtypes = self.dtype\n        \n        outputs = []\n        for idx, _input in enumerate(inputs):\n            _input = _input.type(dtypes[idx])\n            outputs.append(_input)\n        return outputs if idx >= 1 else outputs[0]\n\n\nclass AddChannel(object):\n    """"""Adds a dummy channel to an image, also known as expanding an axis or unsqueezing a dim\n    This will make an image of size (28, 28) to now be\n    of size (1, 28, 28), for example.\n\n    param axis: (int): dimension to be expanded to singleton\n    """"""\n    def __init__(self, axis=0):\n        self.axis = axis\n\n    def __call__(self, *inputs):\n        outputs = []\n        for idx, _input in enumerate(inputs):\n            _input = _input.unsqueeze(self.axis)\n            outputs.append(_input)\n        return outputs if idx >= 1 else outputs[0]\n\nExpandAxis = AddChannel\nUnsqueeze = AddChannel\n\n\nclass Transpose(object):\n    """"""\n    Swaps two dimensions of a tensor\n\n    :param dim1: (int):\n        first dim to switch\n    :param dim2: (int):\n        second dim to switch\n    """"""\n\n    def __init__(self, dim1, dim2):\n\n        self.dim1 = dim1\n        self.dim2 = dim2\n\n    def __call__(self, *inputs):\n        outputs = []\n        for idx, _input in enumerate(inputs):\n            _input = th.transpose(_input, self.dim1, self.dim2)\n            outputs.append(_input)\n        return outputs if idx >= 1 else outputs[0]\n\n\nclass RangeNormalize(object):\n    """"""\n    Given min_val: (R, G, B) and max_val: (R,G,B),\n    will normalize each channel of the th.*Tensor to\n    the provided min and max values.\n\n    Works by calculating :\n        a = (max\'-min\')/(max-min)\\n\n        b = max\' - a * max\\n\n        new_value = a * value + b\n    where min\' & max\' are given values, \n    and min & max are observed min/max for each channel\n    \n    :param min_val: (float or integer):\n        Lower bound of normalized tensor\n    :param max_val: (float or integer):\n        Upper bound of normalized tensor\n\n    Example:\n        >>> x = th.rand(3,5,5)\n        >>> rn = RangeNormalize((0,0,10),(1,1,11))\n        >>> x_norm = rn(x)\n\n    Also works with just one value for min/max:\n        >>> x = th.rand(3,5,5)\n        >>> rn = RangeNormalize(0,1)\n        >>> x_norm = rn(x)\n    """"""\n    def __init__(self, min_val, max_val):\n        """"""\n        Normalize a tensor between a min and max value\n        """"""\n        self.min_val = min_val\n        self.max_val = max_val\n\n    def __call__(self, *inputs):\n        outputs = []\n        for idx, _input in enumerate(inputs):\n            _min_val = _input.min()\n            _max_val = _input.max()\n            a = (self.max_val - self.min_val) / (_max_val - _min_val)\n            b = self.max_val- a * _max_val\n            _input = _input.mul(a).add(b)\n            outputs.append(_input)\n        return outputs if idx >= 1 else outputs[0]\n\n\nclass StdNormalize(object):\n    """"""\n    Normalize torch tensor to have zero mean and unit std deviation\n    """"""\n    def __call__(self, *inputs):\n        outputs = []\n        for idx, _input in enumerate(inputs):\n            _input = _input.sub(_input.mean()).div(_input.std())\n            outputs.append(_input)\n        return outputs if idx >= 1 else outputs[0]\n\n\nclass Slice2D(object):\n    """"""\n    Take a random 2D slice from a 3D image along\n    a given axis. This image should not have a 4th channel dim.\n\n    :param axis: (int `in {0, 1, 2}`):\n        the axis on which to take slices\n\n    :param reject_zeros: (bool):\n        whether to reject slices that are all zeros\n    """"""\n\n    def __init__(self, axis=0, reject_zeros=False):\n\n        self.axis = axis\n        self.reject_zeros = reject_zeros\n\n    def __call__(self, x, y=None):\n        while True:\n            keep_slice  = random.randint(0,x.size(self.axis)-1)\n            if self.axis == 0:\n                slice_x = x[keep_slice,:,:]\n                if y is not None:\n                    slice_y = y[keep_slice,:,:]\n            elif self.axis == 1:\n                slice_x = x[:,keep_slice,:]\n                if y is not None:\n                    slice_y = y[:,keep_slice,:]\n            elif self.axis == 2:\n                slice_x = x[:,:,keep_slice]\n                if y is not None:\n                    slice_y = y[:,:,keep_slice]\n\n            if not self.reject_zeros:\n                break\n            else:\n                if y is not None and th.sum(slice_y) > 0:\n                        break\n                elif th.sum(slice_x) > 0:\n                        break\n        if y is not None:\n            return slice_x, slice_y\n        else:\n            return slice_x\n\n\nclass RandomCrop(object):\n    """"""\n    Randomly crop a torch tensor\n\n    :param size: (tuple or list):\n        dimensions of the crop\n    """"""\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, *inputs):\n        h_idx = random.randint(0,inputs[0].size(1)-self.size[0])\n        w_idx = random.randint(0,inputs[1].size(2)-self.size[1])\n        outputs = []\n        for idx, _input in enumerate(inputs):\n            _input = _input[:, h_idx:(h_idx+self.size[0]),w_idx:(w_idx+self.size[1])]\n            outputs.append(_input)\n        return outputs if idx >= 1 else outputs[0]\n\n\nclass SpecialCrop(object):\n    """"""\n    Perform a special crop - one of the four corners or center crop\n\n    :param size: (tuple or list):\n        dimensions of the crop\n\n    :param crop_type: (int in `{0,1,2,3,4}`):\n        0 = center crop\n        1 = top left crop\n        2 = top right crop\n        3 = bottom right crop\n        4 = bottom left crop\n    """"""\n    def __init__(self, size, crop_type=0):\n        if crop_type not in {0, 1, 2, 3, 4}:\n            raise ValueError(\'crop_type must be in {0, 1, 2, 3, 4}\')\n        self.size = size\n        self.crop_type = crop_type\n\n    def __call__(self, x, y=None):\n        if self.crop_type == 0:\n            # center crop\n            x_diff  = (x.size(1)-self.size[0])/2.\n            y_diff  = (x.size(2)-self.size[1])/2.\n            ct_x    = [int(math.ceil(x_diff)),x.size(1)-int(math.floor(x_diff))]\n            ct_y    = [int(math.ceil(y_diff)),x.size(2)-int(math.floor(y_diff))]\n            indices = [ct_x,ct_y]\n        elif self.crop_type == 1:\n            # top left crop\n            tl_x = [0, self.size[0]]\n            tl_y = [0, self.size[1]]\n            indices = [tl_x,tl_y]\n        elif self.crop_type == 2:\n            # top right crop\n            tr_x = [0, self.size[0]]\n            tr_y = [x.size(2)-self.size[1], x.size(2)]\n            indices = [tr_x,tr_y]\n        elif self.crop_type == 3:\n            # bottom right crop\n            br_x = [x.size(1)-self.size[0],x.size(1)]\n            br_y = [x.size(2)-self.size[1],x.size(2)]\n            indices = [br_x,br_y]\n        elif self.crop_type == 4:\n            # bottom left crop\n            bl_x = [x.size(1)-self.size[0], x.size(1)]\n            bl_y = [0, self.size[1]]\n            indices = [bl_x,bl_y]\n\n        x = x[:,indices[0][0]:indices[0][1],indices[1][0]:indices[1][1]]\n\n        if y is not None:\n            y = y[:,indices[0][0]:indices[0][1],indices[1][0]:indices[1][1]]\n            return x, y\n        else:\n            return x\n\n\nclass Pad(object):\n\n    """"""\n    Pads an image to the given size\n\n    Arguments\n    ---------\n    :param size: (tuple or list):\n        size of crop\n    """"""\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, x, y=None):\n        x = x.numpy()\n        shape_diffs = [int(np.ceil((i_s - d_s))) for d_s,i_s in zip(x.shape,self.size)]\n        shape_diffs = np.maximum(shape_diffs,0)\n        pad_sizes = [(int(np.ceil(s/2.)),int(np.floor(s/2.))) for s in shape_diffs]\n        x = np.pad(x, pad_sizes, mode=\'constant\')\n        if y is not None:\n            y = y.numpy()\n            y = np.pad(y, pad_sizes, mode=\'constant\')\n            return th.from_numpy(x), th.from_numpy(y)\n        else:\n            return th.from_numpy(x)\n\n\nclass PadNumpy(object):\n\n    """"""\n    Pads a Numpy image to the given size\n    Return a Numpy image / image pair\n    Arguments\n    ---------\n    :param size: (tuple or list):\n        size of crop\n    """"""\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, x, y=None):\n        shape_diffs = [int(np.ceil((i_s - d_s))) for d_s,i_s in zip(x.shape,self.size)]\n        shape_diffs = np.maximum(shape_diffs,0)\n        pad_sizes = [(int(np.ceil(s/2.)),int(np.floor(s/2.))) for s in shape_diffs]\n        x = np.pad(x, pad_sizes, mode=\'constant\')\n        if y is not None:\n            y = np.pad(y, pad_sizes, mode=\'constant\')\n            return x, y\n        else:\n            return x\n\n\nclass RandomFlip(object):\n\n    """"""\n    Randomly flip an image horizontally and/or vertically with\n    some probability.\n\n    :param h: (bool):\n        whether to horizontally flip w/ probability p\n    :param v: (bool):\n        whether to vertically flip w/ probability p\n    :param p: (float between [0,1]):\n        probability with which to apply allowed flipping operations\n    """"""\n    def __init__(self, h=True, v=False, p=0.5):\n        self.horizontal = h\n        self.vertical = v\n        self.p = p\n\n    def __call__(self, x, y=None):\n        x = x.numpy()\n        if y is not None:\n            y = y.numpy()\n        # horizontal flip with p = self.p\n        if self.horizontal:\n            if random.random() < self.p:\n                x = x.swapaxes(2, 0)\n                x = x[::-1, ...]\n                x = x.swapaxes(0, 2)\n                if y is not None:\n                    y = y.swapaxes(2, 0)\n                    y = y[::-1, ...]\n                    y = y.swapaxes(0, 2)\n        # vertical flip with p = self.p\n        if self.vertical:\n            if random.random() < self.p:\n                x = x.swapaxes(1, 0)\n                x = x[::-1, ...]\n                x = x.swapaxes(0, 1)\n                if y is not None:\n                    y = y.swapaxes(1, 0)\n                    y = y[::-1, ...]\n                    y = y.swapaxes(0, 1)\n        if y is None:\n            # must copy because torch doesnt current support neg strides\n            return th.from_numpy(x.copy())\n        else:\n            return th.from_numpy(x.copy()),th.from_numpy(y.copy())\n\n\nclass RandomOrder(object):\n    """"""\n    Randomly permute the channels of an image\n    """"""\n    def __call__(self, *inputs):\n        order = th.randperm(inputs[0].dim())\n        outputs = []\n        for idx, _input in enumerate(inputs):\n            _input = _input.index_select(0, order)\n            outputs.append(_input)\n        return outputs if idx >= 1 else outputs[0]\n\n'"
pywick/datasets/tnt/__init__.py,0,b''
pywick/datasets/tnt/batchdataset.py,0,"b'import math\nfrom pywick.datasets.tnt.dataset import Dataset\nfrom . import transform\n\n\nclass BatchDataset(Dataset):\n    """"""\n    Dataset which batches the data from a given dataset.\n\n    Given a `dataset`, `BatchDataset` merges samples from this dataset to\n    form a new sample which can be interpreted as a batch of size `batchsize`.\n\n    The `merge` function controls how the batching is performed. By default\n    the occurrences are supposed to be tensors, and they aggregated along the\n    first dimension.\n\n    It is often important to shuffle examples while performing the batch\n    operation. `perm(idx, size)` is a function which returns the shuffled index\n    of the sample at position `idx` in the underlying dataset. For convenience,\n    the `size` of the underlying dataset is also passed to the function. By\n    default, the function is the identity.\n\n    The underlying dataset size might or might not be always divisible by\n    `batchsize`.  The optional `policy` string specify how to handle corner\n    cases.\n\n    Purpose: the concept of batch is problem dependent. In *torchnet*, it is up\n    to the user to interpret a sample as a batch or not. When one wants to\n    assemble samples from an existing dataset into a batch, then\n    `BatchDataset` is suited for the job. Sometimes it is however more\n    convenient to write a dataset from scratch providing ""batched"" samples.\n\n    Args:\n        dataset (Dataset): Dataset to be batched.\n        batchsize (int): Size of the batch.\n        perm (function, optional): Function used to shuffle the dataset before\n            batching. `perm(idx, size)` should return the shuffled index of\n            `idx`th sample. By default, the function is the identity.\n        merge (function, optional): Function to control batching behaviour.\n             `transform.makebatch(merge)` is used to make the batch. Default is\n             None.\n        policy (str, optional): Policy to handle the corner cases when the\n            underlying dataset size is not divisible by `batchsize`. One of\n            (`include-last`, `skip-last`, `divisible-only`).\n\n            - `include-last` makes sure all samples of the underlying dataset\n               will be seen, batches will be of size equal or inferior to\n               `batchsize`.\n            - `skip-last` will skip last examples of the underlying dataset if\n               its size is not properly divisible. Batches will be always of\n               size equal to `batchsize`.\n            - `divisible-only` will raise an error if the underlying dataset\n               has not a size divisible by `batchsize`.\n        filter (function, optional): Function to filter the sample before\n            batching. If `filter(sample)` is True, then sample is included for\n            batching. Otherwise, it is excluded. By default, `filter(sample)`\n            returns True for any `sample`.\n\n    """"""\n\n    def __init__(self,\n                 dataset,\n                 batchsize,\n                 perm=lambda idx, size: idx,\n                 merge=None,\n                 policy=\'include-last\',\n                 filter=lambda sample: True):\n        super(BatchDataset, self).__init__()\n        self.dataset = dataset\n        self.perm = perm\n        self.batchsize = batchsize\n        self.policy = policy\n        self.filter = filter\n        self.makebatch = transform.makebatch(merge)\n        len(self)\n\n    def __len__(self):\n        if self.policy == \'include-last\':\n            return int(math.ceil(float(len(self.dataset) / self.batchsize)))\n        elif self.policy == \'skip-last\':\n            return int(math.floor(float(len(self.dataset) / self.batchsize)))\n        elif self.policy == \'divisible-only\':\n            assert len(self.dataset) % self.batchsize == 0, \\\n                \'dataset size is not divisible by batch size\'\n            return len(self.dataset) / self.batchsize\n        else:\n            assert False, \'invalid policy (include-last | skip-last | \\\n                divisible-only expected)\'\n\n    def __getitem__(self, idx):\n        super(BatchDataset, self).__getitem__(idx)\n        maxidx = len(self.dataset)\n\n        samples = []\n        for i in range(0, self.batchsize):\n            j = idx * self.batchsize + i\n            if j >= maxidx:\n                break\n\n            j = self.perm(j, maxidx)\n            sample = self.dataset[j]\n\n            if self.filter(sample):\n                samples.append(sample)\n\n        samples = self.makebatch(samples)\n        return samples\n'"
pywick/datasets/tnt/concatdataset.py,0,"b'from pywick.datasets.tnt.dataset import Dataset\nimport numpy as np\n\n\nclass ConcatDataset(Dataset):\n    """"""\n    Dataset to concatenate multiple datasets.\n\n    Purpose: useful to assemble different existing datasets, possibly\n    large-scale datasets as the concatenation operation is done in an\n    on-the-fly manner.\n\n    Args:\n        datasets (iterable): List of datasets to be concatenated\n    """"""\n\n    def __init__(self, datasets):\n        super(ConcatDataset, self).__init__()\n\n        self.datasets = list(datasets)\n        assert len(datasets) > 0, \'datasets should not be an empty iterable\'\n        self.cum_sizes = np.cumsum([len(x) for x in self.datasets])\n\n    def __len__(self):\n        return self.cum_sizes[-1]\n\n    def __getitem__(self, idx):\n        super(ConcatDataset, self).__getitem__(idx)\n        dataset_index = self.cum_sizes.searchsorted(idx, \'right\')\n\n        if dataset_index == 0:\n            dataset_idx = idx\n        else:\n            dataset_idx = idx - self.cum_sizes[dataset_index - 1]\n\n        return self.datasets[dataset_index][dataset_idx]\n'"
pywick/datasets/tnt/dataset.py,1,"b'from pywick.datasets.tnt.batchdataset import BatchDataset\nfrom pywick.datasets.transformdataset import TransformDataset\nfrom pywick.datasets.shuffledataset import ShuffleDataset\nfrom pywick.datasets.multipartitiondataset import MultiPartitionDataset\nfrom torch.utils.data import DataLoader\n\n\nclass Dataset(object):\n    def __init__(self):\n        pass\n\n    def __len__(self):\n        pass\n\n    def __getitem__(self, idx):\n        if idx >= len(self):\n            raise IndexError(""CustomRange index out of range"")\n        pass\n\n    def batch(self, *args, **kwargs):\n        return BatchDataset(self, *args, **kwargs)\n\n    def transform(self, *args, **kwargs):\n        return TransformDataset(self, *args, **kwargs)\n\n    def shuffle(self, *args, **kwargs):\n        return ShuffleDataset(self, *args, **kwargs)\n\n    def parallel(self, *args, **kwargs):\n        return DataLoader(self, *args, **kwargs)\n\n    def partition(self, *args, **kwargs):\n        return MultiPartitionDataset(self, *args, **kwargs)\n'"
pywick/datasets/tnt/listdataset.py,0,"b'from pywick.datasets.tnt.dataset import Dataset\n\n\nclass ListDataset(Dataset):\n    """"""\n    Dataset which loads data from a list using given function.\n\n    Considering a `elem_list` (can be an iterable or a `string` ) i-th sample\n    of a dataset will be returned by `load(elem_list[i])`, where `load()`\n    is a function provided by the user.\n\n    If `path` is provided, `elem_list` is assumed to be a list of strings, and\n    each element `elem_list[i]` will prefixed by `path/` when fed to `load()`.\n\n    Purpose: many low or medium-scale datasets can be seen as a list of files\n    (for example representing input samples). For this list of file, a target\n    can be often inferred in a simple manner.\n\n    Args:\n        elem_list (iterable/str): List of arguments which will be passed to\n            `load` function. It can also be a path to file with each line\n            containing the arguments to `load`\n        load (function, optional): Function which loads the data.\n            i-th sample is returned by `load(elem_list[i])`. By default `load`\n            is identity i.e, `lambda x: x`\n        path (str, optional): Defaults to None. If a string is provided,\n            `elem_list` is assumed to be a list of strings, and each element\n            `elem_list[i]` will prefixed by this string when fed to `load()`.\n\n    """"""\n\n    def __init__(self, elem_list, load=lambda x: x, path=None):\n        super(ListDataset, self).__init__()\n\n        if isinstance(elem_list, str):\n            with open(elem_list) as f:\n                self.list = [line.replace(\'\\n\', \'\') for line in f]\n        else:\n            # just assume iterable\n            self.list = elem_list\n\n        self.path = path\n        self.load = load\n\n    def __len__(self):\n        return len(self.list)\n\n    def __getitem__(self, idx):\n        super(ListDataset, self).__getitem__(idx)\n\n        if self.path is not None:\n            return self.load(""%s/%s"" % (self.path, self.list[idx]))\n        else:\n            return self.load(self.list[idx])\n'"
pywick/datasets/tnt/multipartitiondataset.py,0,"b'from pywick.datasets.tnt.dataset import Dataset\nimport numpy as np\n\nclass MultiPartitionDataset(Dataset):\n    """"""\n    Dataset to partition a given dataset.\n\n    Partition a given `dataset`, according to the specified `partitions`. Use\n    the method `select()` to select the current partition in use.\n\n    The `partitions` is a dictionary where a key is a user-chosen string\n    naming the partition, and value is a number representing the weight (as a\n    number between 0 and 1) or the size (in number of samples) of the\n    corresponding partition.\n\n    Partioning is achieved linearly (no shuffling). See `ShuffleDataset` if you\n    want to shuffle the dataset before partitioning.\n\n    Args:\n        dataset (Dataset): Dataset to be split.\n        partitions (dict): Dictionary where key is a user-chosen string\n            naming the partition, and value is a number representing the weight\n            (as a number between 0 and 1) or the size (in number of samples)\n            of the corresponding partition.\n        initial_partition (str, optional): Initial parition to be selected.\n\n    """"""\n\n    def __init__(self, dataset, partitions, initial_partition=None):\n        super(MultiPartitionDataset, self).__init__()\n\n        self.dataset = dataset\n        self.partitions = partitions\n\n        # A few assertions\n        assert isinstance(partitions, dict), \'partitions must be a dict\'\n        assert len(partitions) >= 2, \\\n            \'MultiPartitionDataset should have at least two partitions\'\n        assert min(partitions.values()) >= 0, \\\n            \'partition sizes cannot be negative\'\n        assert max(partitions.values()) > 0, \'all partitions cannot be empty\'\n\n        self.partition_names = list(self.partitions.keys())\n        self.partition_index = {partition: i for i, partition in\n                                enumerate(self.partition_names)}\n\n        self.partition_sizes = [self.partitions[parition] for parition in\n                                self.partition_names]\n        # if partition sizes are fractions, convert to sizes:\n        if sum(self.partition_sizes) <= 1:\n            self.partition_sizes = [round(x * len(dataset)) for x in\n                                    self.partition_sizes]\n        else:\n            for x in self.partition_sizes:\n                assert x == int(x), (\'partition sizes should be integer\'\n                                     \' numbers, or sum up to <= 1 \')\n\n        self.partition_cum_sizes = np.cumsum(self.partition_sizes)\n\n        if initial_partition is not None:\n            self.select(initial_partition)\n\n    def select(self, partition):\n        """"""\n        Select the parition.\n\n        Args:\n            partition (str): Partition to be selected.\n        """"""\n        self.current_partition_idx = self.partition_index[partition]\n\n    def __len__(self):\n        try:\n            return self.partition_sizes[self.current_partition_idx]\n        except AttributeError:\n            raise ValueError(""Select a partition before accessing data."")\n\n    def __getitem__(self, idx):\n        super(MultiPartitionDataset, self).__getitem__(idx)\n        try:\n            if self.current_partition_idx == 0:\n                return self.dataset[idx]\n            else:\n                offset = self.partition_cum_sizes[self.current_partition_idx - 1]\n                return self.dataset[int(offset) + idx]\n        except AttributeError:\n            raise ValueError(""Select a partition before accessing data."")\n'"
pywick/datasets/tnt/resampledataset.py,0,"b'from pywick.datasets.tnt.dataset import Dataset\n\n\nclass ResampleDataset(Dataset):\n    """"""\n    Dataset which resamples a given dataset.\n\n    Given a `dataset`, creates a new dataset which will (re-)sample from this\n    underlying dataset using the provided `sampler(dataset, idx)` function.\n\n    If `size` is provided, then the newly created dataset will have the\n    specified `size`, which might be different than the underlying dataset\n    size. If `size` is not provided, then the new dataset will have the same\n    size as the underlying one.\n\n    Purpose: shuffling data, re-weighting samples, getting a subset of the\n    data. Note that an important sub-class `ShuffleDataset` is provided for\n    convenience.\n\n    Args:\n        dataset (Dataset): Dataset to be resampled.\n        sampler (function, optional): Function used for sampling. `idx`th\n            sample is returned by `dataset[sampler(dataset, idx)]`. By default\n            `sampler(dataset, idx)` is the identity, simply returning `idx`.\n            `sampler(dataset, idx)` must return an index in the range\n            acceptable for the underlying `dataset`.\n        size (int, optional): Desired size of the dataset after resampling. By\n            default, the new dataset will have the same size as the underlying\n            one.\n    """"""\n\n    def __init__(self, dataset, sampler=lambda ds, idx: idx, size=None):\n        super(ResampleDataset, self).__init__()\n        self.dataset = dataset\n        self.sampler = sampler\n        self.size = size\n\n    def __len__(self):\n        return (self.size and self.size > 0) and self.size or len(self.dataset)\n\n    def __getitem__(self, idx):\n        super(ResampleDataset, self).__getitem__(idx)\n        idx = self.sampler(self.dataset, idx)\n\n        if idx < 0 or idx >= len(self.dataset):\n            raise IndexError(\'out of range\')\n\n        return self.dataset[idx]\n'"
pywick/datasets/tnt/shuffledataset.py,4,"b'from pywick.datasets.tnt.resampledataset import ResampleDataset\nimport torch\n\n\nclass ShuffleDataset(ResampleDataset):\n    """"""\n    Dataset which shuffles a given dataset.\n\n    `ShuffleDataset` is a sub-class of `ResampleDataset` provided for\n    convenience. It samples uniformly from the given `dataset` with, or without\n    `replacement`. The chosen partition can be redrawn by calling `resample()`\n\n    If `replacement` is `true`, then the specified `size` may be larger than\n    the underlying `dataset`.\n    If `size` is not provided, then the new dataset size will be equal to the\n    underlying `dataset` size.\n\n    Purpose: the easiest way to shuffle a dataset!\n\n    Args:\n        dataset (Dataset): Dataset to be shuffled.\n        size (int, optional): Desired size of the shuffled dataset. If\n            `replacement` is `true`, then can be larger than the `len(dataset)`.\n            By default, the new dataset will have the same size as `dataset`.\n        replacement (bool, optional): True if uniform sampling is to be done\n            with replacement. False otherwise. Defaults to false.\n\n    Raises:\n        ValueError: If `size` is larger than the size of the underlying dataset\n            and `replacement` is False.\n    """"""\n\n    def __init__(self, dataset, size=None, replacement=False):\n        if size and not replacement and size > len(dataset):\n            raise ValueError(\'size cannot be larger than underlying dataset \\\n                    size when sampling without replacement\')\n\n        super(ShuffleDataset, self).__init__(dataset,\n                                             lambda dataset, idx: self.perm[idx],\n                                             size)\n        self.replacement = replacement\n        self.resample()\n\n    def resample(self, seed=None):\n        """"""Resample the dataset.\n\n        Args:\n            seed (int, optional): Seed for resampling. By default no seed is\n            used.\n        """"""\n        if seed is not None:\n            gen = torch.manual_seed(seed)\n        else:\n            gen = torch.default_generator\n\n        if self.replacement:\n            self.perm = torch.LongTensor(len(self)).random_(len(self.dataset), generator=gen)\n        else:\n            self.perm = torch.randperm(len(self.dataset), generator=gen).narrow(0, 0, len(self))\n'"
pywick/datasets/tnt/splitdataset.py,0,"b'from pywick.datasets.tnt.dataset import Dataset\nimport numpy as np\n\n\nclass SplitDataset(Dataset):\n    """"""\n    Dataset to partition a given dataset.\n\n    Partition a given `dataset`, according to the specified `partitions`. Use\n    the method `select()` to select the current partition in use.\n\n    The `partitions` is a dictionary where a key is a user-chosen string\n    naming the partition, and value is a number representing the weight (as a\n    number between 0 and 1) or the size (in number of samples) of the\n    corresponding partition.\n\n    Partioning is achieved linearly (no shuffling). See `ShuffleDataset` if you\n    want to shuffle the dataset before partitioning.\n\n    Args:\n        dataset (Dataset): Dataset to be split.\n        partitions (dict): Dictionary where key is a user-chosen string\n            naming the partition, and value is a number representing the weight\n            (as a number between 0 and 1) or the size (in number of samples)\n            of the corresponding partition.\n        initial_partition (str, optional): Initial parition to be selected.\n\n    """"""\n\n    def __init__(self, dataset, partitions, initial_partition=None):\n        super(SplitDataset, self).__init__()\n\n        self.dataset = dataset\n        self.partitions = partitions\n\n        # A few assertions\n        assert isinstance(partitions, dict), \'partitions must be a dict\'\n        assert len(partitions) >= 2, \\\n            \'SplitDataset should have at least two partitions\'\n        assert min(partitions.values()) >= 0, \\\n            \'partition sizes cannot be negative\'\n        assert max(partitions.values()) > 0, \'all partitions cannot be empty\'\n\n        self.partition_names = sorted(list(self.partitions.keys()))\n        self.partition_index = {partition: i for i, partition in\n                                enumerate(self.partition_names)}\n\n        self.partition_sizes = [self.partitions[parition] for parition in\n                                self.partition_names]\n        # if partition sizes are fractions, convert to sizes:\n        if sum(self.partition_sizes) <= 1:\n            self.partition_sizes = [round(x * len(dataset)) for x in\n                                    self.partition_sizes]\n        else:\n            for x in self.partition_sizes:\n                assert x == int(x), (\'partition sizes should be integer\'\n                                     \' numbers, or sum up to <= 1 \')\n\n        self.partition_cum_sizes = np.cumsum(self.partition_sizes)\n\n        if initial_partition is not None:\n            self.select(initial_partition)\n\n    def select(self, partition):\n        """"""\n        Select the parition.\n\n        Args:\n            partition (str): Partition to be selected.\n        """"""\n        self.current_partition_idx = self.partition_index[partition]\n\n    def __len__(self):\n        try:\n            return self.partition_sizes[self.current_partition_idx]\n        except AttributeError:\n            raise ValueError(""Select a partition before accessing data."")\n\n    def __getitem__(self, idx):\n        super(SplitDataset, self).__getitem__(idx)\n        try:\n            if self.current_partition_idx == 0:\n                return self.dataset[idx]\n            else:\n                offset = self.partition_cum_sizes[self.current_partition_idx - 1]\n                return self.dataset[int(offset) + idx]\n        except AttributeError:\n            raise ValueError(""Select a partition before accessing data."")\n'"
pywick/datasets/tnt/table.py,2,"b'import torch\n\n\ndef canmergetensor(tbl):\n    if not isinstance(tbl, list):\n        return False\n\n    if torch.is_tensor(tbl[0]):\n        sz = tbl[0].numel()\n        for v in tbl:\n            if v.numel() != sz:\n                return False\n        return True\n    return False\n\n\ndef mergetensor(tbl):\n    sz = [len(tbl)] + list(tbl[0].size())\n    res = tbl[0].new(torch.Size(sz))\n    for i,v in enumerate(tbl):\n        res[i].copy_(v)\n    return res\n'"
pywick/datasets/tnt/transform.py,0,"b""from six import iteritems\nfrom pywick.datasets.tnt.table import canmergetensor as canmerge\nfrom pywick.datasets.tnt.table import mergetensor as mergetensor\n\n\ndef compose(transforms):\n    assert isinstance(transforms, list)\n    for tr in transforms:\n        assert callable(tr), 'list of functions expected'\n\n    def composition(z):\n        for tr in transforms:\n            z = tr(z)\n        return z\n    return composition\n\n\ndef tablemergekeys():\n    def mergekeys(tbl):\n        mergetbl = {}\n        if isinstance(tbl, dict):\n            for idx, elem in tbl.items():\n                for key, value in elem.items():\n                    if not key in mergetbl:\n                        mergetbl[key] = {}\n                    mergetbl[key][idx] = value\n        elif isinstance(tbl, list):\n            for elem in tbl:\n                for key, value in elem.items():\n                    if not key in mergetbl:\n                        mergetbl[key] = []\n                    mergetbl[key].append(value)\n        return mergetbl\n    return mergekeys\n\ntableapply = lambda f: lambda d: dict(\n    map(lambda kv: (kv[0], f(kv[1])), iteritems(d)))\n\n\ndef makebatch(merge=None):\n    if merge:\n        makebatch = compose([tablemergekeys(), merge])\n    else:\n        makebatch = compose([\n            tablemergekeys(),\n            tableapply(lambda field: mergetensor(field)\n                       if canmerge(field) else field)\n        ])\n\n    return lambda samples: makebatch(samples)\n"""
pywick/datasets/tnt/transformdataset.py,0,"b'from pywick.datasets.tnt.dataset import Dataset\n\n\nclass TransformDataset(Dataset):\n    """"""\n    Dataset which transforms a given dataset with a given function.\n\n    Given a function `transform`, and a `dataset`, `TransformDataset` applies\n    the function in an on-the-fly manner when querying a sample with\n    `__getitem__(idx)` and therefore returning `transform[dataset[idx]]`.\n\n    `transform` can also be a dict with functions as values. In this case, it\n    is assumed that `dataset[idx]` is a dict which has all the keys in\n    `transform`. Then, `transform[key]` is applied to dataset[idx][key] for\n    each key in `transform`\n\n    The size of the new dataset is equal to the size of the underlying\n    `dataset`.\n\n    Purpose: when performing pre-processing operations, it is convenient to be\n    able to perform on-the-fly transformations to a dataset.\n\n    Args:\n        dataset (Dataset): Dataset which has to be transformed.\n        transforms (function/dict): Function or dict with function as values.\n            These functions will be applied to data.\n    """"""\n\n    def __init__(self, dataset, transforms):\n        super(TransformDataset, self).__init__()\n\n        assert isinstance(transforms, dict) or callable(transforms), \\\n            \'expected a dict of transforms or a function\'\n        if isinstance(transforms, dict):\n            for k, v in transforms.items():\n                assert callable(v), str(k) + \' is not a function\'\n\n        self.dataset = dataset\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        super(TransformDataset, self).__getitem__(idx)\n        z = self.dataset[idx]\n\n        if isinstance(self.transforms, dict):\n            for k, transform in self.transforms.items():\n                z[k] = transform(z[k])\n        else:\n            z = self.transforms(z)\n\n        return z\n'"
pywick/models/classification/__init__.py,0,"b'""""""\nBelow you will find all the latest image classification models.\nBy convention, model names starting with lowercase are pretrained on imagenet while uppercase are not (vanilla). To load one of the pretrained\nmodels with your own number of classes use the ``models.model_utils.get_model(...)`` function and specify the name of the model\nexactly like the pretrained model method name (e.g. if the method name reads ``pywick.models.classification.dpn.dualpath.dpn68`` then use\n`dpn68` as the model name for ``models.model_utils.get_model(...)``.\n""""""\n\nfrom .dpn.dualpath import *                                 # dpnXX = pretrained on imagenet, DPN = not pretrained\nfrom .bn_inception import *                                  # bninception = pretrained on imagenet, BNInception not pretrained\nfrom .fbresnet import *                                     # only fbresnet152 pretrained\nfrom .inception_resv2_wide import InceptionResV2            # InceptionResV2 not pretrained\nfrom .inceptionresnet_v2 import *                            # inceptionresnetv2 = pretrained on imagenet, InceptionResNetV2 not pretrained\nfrom .inception_v4 import *                                  # inceptionv4 = pretrained on imagenet, InceptionV4 not pretrained\nfrom .nasnet import *                                       # nasnetalarge = pretrained on NASNetALarge, InceptionV4 not pretrained\nfrom .nasnet_mobile import *                                # nasnetamobile = pretrained on imagenet, NASNetAMobile not pretrained\nfrom .pnasnet import *                                      # pnasnet5large = pretrained on imagenet, PNASNet5Large not pretrained\nfrom .poly_net import *                                      # polynet = pretrained on imagenet, PolyNet not pretrained\nfrom .pyramid_resnet import *                               # pyresnetxx = pretrained on imagenet, PyResNet not pretrained\nfrom .resnet_preact import *                                # not pretrained\nfrom .resnet_swish import *                                 # not pretrained\nfrom .resnext import *                                      # resnextxxx = pretrained on imagenet, ResNeXt not pretrained\nfrom .senet import *                                        # SENet not pretrained, all others pretrained\nfrom .wideresnet import *                                   # models have not been vetted\nfrom .xception1 import *                                     # xception = pretrained on imagenet, Xception not pretrained\n\nfrom .testnets import *\n'"
pywick/models/classification/bn_inception.py,13,"b'# Source: https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/bninception.py (License: BSD-3-Clause)\n# Pretrained: Yes\n\nr""""""\nImplementation of BNInception as described in this `paper <https://arxiv.org/pdf/1502.03167.pdf>`_.\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\n\n__all__ = [\'BNInception\', \'bninception\']\n\npretrained_settings = {\n    \'bninception\': {\n        \'imagenet\': {\n            # Was ported using python2 (may trigger warning)\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/bn_inception-52deb4733.pth\',\n            # \'url\': \'http://yjxiong.me/others/bn_inception-9f5701afb96c8044.pth\',\n            \'input_space\': \'BGR\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 255],\n            \'mean\': [104, 117, 128],\n            \'std\': [1, 1, 1],\n            \'num_classes\': 1000\n        }\n    }\n}\n\nclass BNInception(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(BNInception, self).__init__()\n        inplace = True\n        self.conv1_7x7_s2 = nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n        self.conv1_7x7_s2_bn = nn.BatchNorm2d(64, affine=True)\n        self.conv1_relu_7x7 = nn.ReLU (inplace)\n        self.pool1_3x3_s2 = nn.MaxPool2d ((3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=True)\n        self.conv2_3x3_reduce = nn.Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n        self.conv2_3x3_reduce_bn = nn.BatchNorm2d(64, affine=True)\n        self.conv2_relu_3x3_reduce = nn.ReLU (inplace)\n        self.conv2_3x3 = nn.Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.conv2_3x3_bn = nn.BatchNorm2d(192, affine=True)\n        self.conv2_relu_3x3 = nn.ReLU (inplace)\n        self.pool2_3x3_s2 = nn.MaxPool2d ((3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=True)\n        self.inception_3a_1x1 = nn.Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_3a_1x1_bn = nn.BatchNorm2d(64, affine=True)\n        self.inception_3a_relu_1x1 = nn.ReLU (inplace)\n        self.inception_3a_3x3_reduce = nn.Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_3a_3x3_reduce_bn = nn.BatchNorm2d(64, affine=True)\n        self.inception_3a_relu_3x3_reduce = nn.ReLU (inplace)\n        self.inception_3a_3x3 = nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_3a_3x3_bn = nn.BatchNorm2d(64, affine=True)\n        self.inception_3a_relu_3x3 = nn.ReLU (inplace)\n        self.inception_3a_double_3x3_reduce = nn.Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_3a_double_3x3_reduce_bn = nn.BatchNorm2d(64, affine=True)\n        self.inception_3a_relu_double_3x3_reduce = nn.ReLU (inplace)\n        self.inception_3a_double_3x3_1 = nn.Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_3a_double_3x3_1_bn = nn.BatchNorm2d(96, affine=True)\n        self.inception_3a_relu_double_3x3_1 = nn.ReLU (inplace)\n        self.inception_3a_double_3x3_2 = nn.Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_3a_double_3x3_2_bn = nn.BatchNorm2d(96, affine=True)\n        self.inception_3a_relu_double_3x3_2 = nn.ReLU (inplace)\n        self.inception_3a_pool = nn.AvgPool2d (3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n        self.inception_3a_pool_proj = nn.Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_3a_pool_proj_bn = nn.BatchNorm2d(32, affine=True)\n        self.inception_3a_relu_pool_proj = nn.ReLU (inplace)\n        self.inception_3b_1x1 = nn.Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_3b_1x1_bn = nn.BatchNorm2d(64, affine=True)\n        self.inception_3b_relu_1x1 = nn.ReLU (inplace)\n        self.inception_3b_3x3_reduce = nn.Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_3b_3x3_reduce_bn = nn.BatchNorm2d(64, affine=True)\n        self.inception_3b_relu_3x3_reduce = nn.ReLU (inplace)\n        self.inception_3b_3x3 = nn.Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_3b_3x3_bn = nn.BatchNorm2d(96, affine=True)\n        self.inception_3b_relu_3x3 = nn.ReLU (inplace)\n        self.inception_3b_double_3x3_reduce = nn.Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_3b_double_3x3_reduce_bn = nn.BatchNorm2d(64, affine=True)\n        self.inception_3b_relu_double_3x3_reduce = nn.ReLU (inplace)\n        self.inception_3b_double_3x3_1 = nn.Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_3b_double_3x3_1_bn = nn.BatchNorm2d(96, affine=True)\n        self.inception_3b_relu_double_3x3_1 = nn.ReLU (inplace)\n        self.inception_3b_double_3x3_2 = nn.Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_3b_double_3x3_2_bn = nn.BatchNorm2d(96, affine=True)\n        self.inception_3b_relu_double_3x3_2 = nn.ReLU (inplace)\n        self.inception_3b_pool = nn.AvgPool2d (3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n        self.inception_3b_pool_proj = nn.Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_3b_pool_proj_bn = nn.BatchNorm2d(64, affine=True)\n        self.inception_3b_relu_pool_proj = nn.ReLU (inplace)\n        self.inception_3c_3x3_reduce = nn.Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_3c_3x3_reduce_bn = nn.BatchNorm2d(128, affine=True)\n        self.inception_3c_relu_3x3_reduce = nn.ReLU (inplace)\n        self.inception_3c_3x3 = nn.Conv2d(128, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.inception_3c_3x3_bn = nn.BatchNorm2d(160, affine=True)\n        self.inception_3c_relu_3x3 = nn.ReLU (inplace)\n        self.inception_3c_double_3x3_reduce = nn.Conv2d(320, 64, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_3c_double_3x3_reduce_bn = nn.BatchNorm2d(64, affine=True)\n        self.inception_3c_relu_double_3x3_reduce = nn.ReLU (inplace)\n        self.inception_3c_double_3x3_1 = nn.Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_3c_double_3x3_1_bn = nn.BatchNorm2d(96, affine=True)\n        self.inception_3c_relu_double_3x3_1 = nn.ReLU (inplace)\n        self.inception_3c_double_3x3_2 = nn.Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.inception_3c_double_3x3_2_bn = nn.BatchNorm2d(96, affine=True)\n        self.inception_3c_relu_double_3x3_2 = nn.ReLU (inplace)\n        self.inception_3c_pool = nn.MaxPool2d ((3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=True)\n        self.inception_4a_1x1 = nn.Conv2d(576, 224, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4a_1x1_bn = nn.BatchNorm2d(224, affine=True)\n        self.inception_4a_relu_1x1 = nn.ReLU (inplace)\n        self.inception_4a_3x3_reduce = nn.Conv2d(576, 64, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4a_3x3_reduce_bn = nn.BatchNorm2d(64, affine=True)\n        self.inception_4a_relu_3x3_reduce = nn.ReLU (inplace)\n        self.inception_4a_3x3 = nn.Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4a_3x3_bn = nn.BatchNorm2d(96, affine=True)\n        self.inception_4a_relu_3x3 = nn.ReLU (inplace)\n        self.inception_4a_double_3x3_reduce = nn.Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4a_double_3x3_reduce_bn = nn.BatchNorm2d(96, affine=True)\n        self.inception_4a_relu_double_3x3_reduce = nn.ReLU (inplace)\n        self.inception_4a_double_3x3_1 = nn.Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4a_double_3x3_1_bn = nn.BatchNorm2d(128, affine=True)\n        self.inception_4a_relu_double_3x3_1 = nn.ReLU (inplace)\n        self.inception_4a_double_3x3_2 = nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4a_double_3x3_2_bn = nn.BatchNorm2d(128, affine=True)\n        self.inception_4a_relu_double_3x3_2 = nn.ReLU (inplace)\n        self.inception_4a_pool = nn.AvgPool2d (3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n        self.inception_4a_pool_proj = nn.Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4a_pool_proj_bn = nn.BatchNorm2d(128, affine=True)\n        self.inception_4a_relu_pool_proj = nn.ReLU (inplace)\n        self.inception_4b_1x1 = nn.Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4b_1x1_bn = nn.BatchNorm2d(192, affine=True)\n        self.inception_4b_relu_1x1 = nn.ReLU (inplace)\n        self.inception_4b_3x3_reduce = nn.Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4b_3x3_reduce_bn = nn.BatchNorm2d(96, affine=True)\n        self.inception_4b_relu_3x3_reduce = nn.ReLU (inplace)\n        self.inception_4b_3x3 = nn.Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4b_3x3_bn = nn.BatchNorm2d(128, affine=True)\n        self.inception_4b_relu_3x3 = nn.ReLU (inplace)\n        self.inception_4b_double_3x3_reduce = nn.Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4b_double_3x3_reduce_bn = nn.BatchNorm2d(96, affine=True)\n        self.inception_4b_relu_double_3x3_reduce = nn.ReLU (inplace)\n        self.inception_4b_double_3x3_1 = nn.Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4b_double_3x3_1_bn = nn.BatchNorm2d(128, affine=True)\n        self.inception_4b_relu_double_3x3_1 = nn.ReLU (inplace)\n        self.inception_4b_double_3x3_2 = nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4b_double_3x3_2_bn = nn.BatchNorm2d(128, affine=True)\n        self.inception_4b_relu_double_3x3_2 = nn.ReLU (inplace)\n        self.inception_4b_pool = nn.AvgPool2d (3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n        self.inception_4b_pool_proj = nn.Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4b_pool_proj_bn = nn.BatchNorm2d(128, affine=True)\n        self.inception_4b_relu_pool_proj = nn.ReLU (inplace)\n        self.inception_4c_1x1 = nn.Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4c_1x1_bn = nn.BatchNorm2d(160, affine=True)\n        self.inception_4c_relu_1x1 = nn.ReLU (inplace)\n        self.inception_4c_3x3_reduce = nn.Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4c_3x3_reduce_bn = nn.BatchNorm2d(128, affine=True)\n        self.inception_4c_relu_3x3_reduce = nn.ReLU (inplace)\n        self.inception_4c_3x3 = nn.Conv2d(128, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4c_3x3_bn = nn.BatchNorm2d(160, affine=True)\n        self.inception_4c_relu_3x3 = nn.ReLU (inplace)\n        self.inception_4c_double_3x3_reduce = nn.Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4c_double_3x3_reduce_bn = nn.BatchNorm2d(128, affine=True)\n        self.inception_4c_relu_double_3x3_reduce = nn.ReLU (inplace)\n        self.inception_4c_double_3x3_1 = nn.Conv2d(128, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4c_double_3x3_1_bn = nn.BatchNorm2d(160, affine=True)\n        self.inception_4c_relu_double_3x3_1 = nn.ReLU (inplace)\n        self.inception_4c_double_3x3_2 = nn.Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4c_double_3x3_2_bn = nn.BatchNorm2d(160, affine=True)\n        self.inception_4c_relu_double_3x3_2 = nn.ReLU (inplace)\n        self.inception_4c_pool = nn.AvgPool2d (3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n        self.inception_4c_pool_proj = nn.Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4c_pool_proj_bn = nn.BatchNorm2d(128, affine=True)\n        self.inception_4c_relu_pool_proj = nn.ReLU (inplace)\n        self.inception_4d_1x1 = nn.Conv2d(608, 96, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4d_1x1_bn = nn.BatchNorm2d(96, affine=True)\n        self.inception_4d_relu_1x1 = nn.ReLU (inplace)\n        self.inception_4d_3x3_reduce = nn.Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4d_3x3_reduce_bn = nn.BatchNorm2d(128, affine=True)\n        self.inception_4d_relu_3x3_reduce = nn.ReLU (inplace)\n        self.inception_4d_3x3 = nn.Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4d_3x3_bn = nn.BatchNorm2d(192, affine=True)\n        self.inception_4d_relu_3x3 = nn.ReLU (inplace)\n        self.inception_4d_double_3x3_reduce = nn.Conv2d(608, 160, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4d_double_3x3_reduce_bn = nn.BatchNorm2d(160, affine=True)\n        self.inception_4d_relu_double_3x3_reduce = nn.ReLU (inplace)\n        self.inception_4d_double_3x3_1 = nn.Conv2d(160, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4d_double_3x3_1_bn = nn.BatchNorm2d(192, affine=True)\n        self.inception_4d_relu_double_3x3_1 = nn.ReLU (inplace)\n        self.inception_4d_double_3x3_2 = nn.Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4d_double_3x3_2_bn = nn.BatchNorm2d(192, affine=True)\n        self.inception_4d_relu_double_3x3_2 = nn.ReLU (inplace)\n        self.inception_4d_pool = nn.AvgPool2d (3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n        self.inception_4d_pool_proj = nn.Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4d_pool_proj_bn = nn.BatchNorm2d(128, affine=True)\n        self.inception_4d_relu_pool_proj = nn.ReLU (inplace)\n        self.inception_4e_3x3_reduce = nn.Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4e_3x3_reduce_bn = nn.BatchNorm2d(128, affine=True)\n        self.inception_4e_relu_3x3_reduce = nn.ReLU (inplace)\n        self.inception_4e_3x3 = nn.Conv2d(128, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.inception_4e_3x3_bn = nn.BatchNorm2d(192, affine=True)\n        self.inception_4e_relu_3x3 = nn.ReLU (inplace)\n        self.inception_4e_double_3x3_reduce = nn.Conv2d(608, 192, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4e_double_3x3_reduce_bn = nn.BatchNorm2d(192, affine=True)\n        self.inception_4e_relu_double_3x3_reduce = nn.ReLU (inplace)\n        self.inception_4e_double_3x3_1 = nn.Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4e_double_3x3_1_bn = nn.BatchNorm2d(256, affine=True)\n        self.inception_4e_relu_double_3x3_1 = nn.ReLU (inplace)\n        self.inception_4e_double_3x3_2 = nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.inception_4e_double_3x3_2_bn = nn.BatchNorm2d(256, affine=True)\n        self.inception_4e_relu_double_3x3_2 = nn.ReLU (inplace)\n        self.inception_4e_pool = nn.MaxPool2d ((3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=True)\n        self.inception_5a_1x1 = nn.Conv2d(1056, 352, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_5a_1x1_bn = nn.BatchNorm2d(352, affine=True)\n        self.inception_5a_relu_1x1 = nn.ReLU (inplace)\n        self.inception_5a_3x3_reduce = nn.Conv2d(1056, 192, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_5a_3x3_reduce_bn = nn.BatchNorm2d(192, affine=True)\n        self.inception_5a_relu_3x3_reduce = nn.ReLU (inplace)\n        self.inception_5a_3x3 = nn.Conv2d(192, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_5a_3x3_bn = nn.BatchNorm2d(320, affine=True)\n        self.inception_5a_relu_3x3 = nn.ReLU (inplace)\n        self.inception_5a_double_3x3_reduce = nn.Conv2d(1056, 160, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_5a_double_3x3_reduce_bn = nn.BatchNorm2d(160, affine=True)\n        self.inception_5a_relu_double_3x3_reduce = nn.ReLU (inplace)\n        self.inception_5a_double_3x3_1 = nn.Conv2d(160, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_5a_double_3x3_1_bn = nn.BatchNorm2d(224, affine=True)\n        self.inception_5a_relu_double_3x3_1 = nn.ReLU (inplace)\n        self.inception_5a_double_3x3_2 = nn.Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_5a_double_3x3_2_bn = nn.BatchNorm2d(224, affine=True)\n        self.inception_5a_relu_double_3x3_2 = nn.ReLU (inplace)\n        self.inception_5a_pool = nn.AvgPool2d (3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n        self.inception_5a_pool_proj = nn.Conv2d(1056, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_5a_pool_proj_bn = nn.BatchNorm2d(128, affine=True)\n        self.inception_5a_relu_pool_proj = nn.ReLU (inplace)\n        self.inception_5b_1x1 = nn.Conv2d(1024, 352, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_5b_1x1_bn = nn.BatchNorm2d(352, affine=True)\n        self.inception_5b_relu_1x1 = nn.ReLU (inplace)\n        self.inception_5b_3x3_reduce = nn.Conv2d(1024, 192, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_5b_3x3_reduce_bn = nn.BatchNorm2d(192, affine=True)\n        self.inception_5b_relu_3x3_reduce = nn.ReLU (inplace)\n        self.inception_5b_3x3 = nn.Conv2d(192, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_5b_3x3_bn = nn.BatchNorm2d(320, affine=True)\n        self.inception_5b_relu_3x3 = nn.ReLU (inplace)\n        self.inception_5b_double_3x3_reduce = nn.Conv2d(1024, 192, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_5b_double_3x3_reduce_bn = nn.BatchNorm2d(192, affine=True)\n        self.inception_5b_relu_double_3x3_reduce = nn.ReLU (inplace)\n        self.inception_5b_double_3x3_1 = nn.Conv2d(192, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_5b_double_3x3_1_bn = nn.BatchNorm2d(224, affine=True)\n        self.inception_5b_relu_double_3x3_1 = nn.ReLU (inplace)\n        self.inception_5b_double_3x3_2 = nn.Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_5b_double_3x3_2_bn = nn.BatchNorm2d(224, affine=True)\n        self.inception_5b_relu_double_3x3_2 = nn.ReLU (inplace)\n        self.inception_5b_pool = nn.MaxPool2d ((3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), ceil_mode=True)\n        self.inception_5b_pool_proj = nn.Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_5b_pool_proj_bn = nn.BatchNorm2d(128, affine=True)\n        self.inception_5b_relu_pool_proj = nn.ReLU (inplace)\n        self.last_linear = nn.Linear (1024, num_classes)\n\n    def features(self, input):\n        conv1_7x7_s2_out = self.conv1_7x7_s2(input)\n        conv1_7x7_s2_bn_out = self.conv1_7x7_s2_bn(conv1_7x7_s2_out)\n        conv1_relu_7x7_out = self.conv1_relu_7x7(conv1_7x7_s2_bn_out)\n        pool1_3x3_s2_out = self.pool1_3x3_s2(conv1_relu_7x7_out)\n        conv2_3x3_reduce_out = self.conv2_3x3_reduce(pool1_3x3_s2_out)\n        conv2_3x3_reduce_bn_out = self.conv2_3x3_reduce_bn(conv2_3x3_reduce_out)\n        conv2_relu_3x3_reduce_out = self.conv2_relu_3x3_reduce(conv2_3x3_reduce_bn_out)\n        conv2_3x3_out = self.conv2_3x3(conv2_relu_3x3_reduce_out)\n        conv2_3x3_bn_out = self.conv2_3x3_bn(conv2_3x3_out)\n        conv2_relu_3x3_out = self.conv2_relu_3x3(conv2_3x3_bn_out)\n        pool2_3x3_s2_out = self.pool2_3x3_s2(conv2_relu_3x3_out)\n        inception_3a_1x1_out = self.inception_3a_1x1(pool2_3x3_s2_out)\n        inception_3a_1x1_bn_out = self.inception_3a_1x1_bn(inception_3a_1x1_out)\n        inception_3a_relu_1x1_out = self.inception_3a_relu_1x1(inception_3a_1x1_bn_out)\n        inception_3a_3x3_reduce_out = self.inception_3a_3x3_reduce(pool2_3x3_s2_out)\n        inception_3a_3x3_reduce_bn_out = self.inception_3a_3x3_reduce_bn(inception_3a_3x3_reduce_out)\n        inception_3a_relu_3x3_reduce_out = self.inception_3a_relu_3x3_reduce(inception_3a_3x3_reduce_bn_out)\n        inception_3a_3x3_out = self.inception_3a_3x3(inception_3a_relu_3x3_reduce_out)\n        inception_3a_3x3_bn_out = self.inception_3a_3x3_bn(inception_3a_3x3_out)\n        inception_3a_relu_3x3_out = self.inception_3a_relu_3x3(inception_3a_3x3_bn_out)\n        inception_3a_double_3x3_reduce_out = self.inception_3a_double_3x3_reduce(pool2_3x3_s2_out)\n        inception_3a_double_3x3_reduce_bn_out = self.inception_3a_double_3x3_reduce_bn(inception_3a_double_3x3_reduce_out)\n        inception_3a_relu_double_3x3_reduce_out = self.inception_3a_relu_double_3x3_reduce(inception_3a_double_3x3_reduce_bn_out)\n        inception_3a_double_3x3_1_out = self.inception_3a_double_3x3_1(inception_3a_relu_double_3x3_reduce_out)\n        inception_3a_double_3x3_1_bn_out = self.inception_3a_double_3x3_1_bn(inception_3a_double_3x3_1_out)\n        inception_3a_relu_double_3x3_1_out = self.inception_3a_relu_double_3x3_1(inception_3a_double_3x3_1_bn_out)\n        inception_3a_double_3x3_2_out = self.inception_3a_double_3x3_2(inception_3a_relu_double_3x3_1_out)\n        inception_3a_double_3x3_2_bn_out = self.inception_3a_double_3x3_2_bn(inception_3a_double_3x3_2_out)\n        inception_3a_relu_double_3x3_2_out = self.inception_3a_relu_double_3x3_2(inception_3a_double_3x3_2_bn_out)\n        inception_3a_pool_out = self.inception_3a_pool(pool2_3x3_s2_out)\n        inception_3a_pool_proj_out = self.inception_3a_pool_proj(inception_3a_pool_out)\n        inception_3a_pool_proj_bn_out = self.inception_3a_pool_proj_bn(inception_3a_pool_proj_out)\n        inception_3a_relu_pool_proj_out = self.inception_3a_relu_pool_proj(inception_3a_pool_proj_bn_out)\n        inception_3a_output_out = torch.cat([inception_3a_relu_1x1_out,inception_3a_relu_3x3_out,inception_3a_relu_double_3x3_2_out ,inception_3a_relu_pool_proj_out], 1)\n        inception_3b_1x1_out = self.inception_3b_1x1(inception_3a_output_out)\n        inception_3b_1x1_bn_out = self.inception_3b_1x1_bn(inception_3b_1x1_out)\n        inception_3b_relu_1x1_out = self.inception_3b_relu_1x1(inception_3b_1x1_bn_out)\n        inception_3b_3x3_reduce_out = self.inception_3b_3x3_reduce(inception_3a_output_out)\n        inception_3b_3x3_reduce_bn_out = self.inception_3b_3x3_reduce_bn(inception_3b_3x3_reduce_out)\n        inception_3b_relu_3x3_reduce_out = self.inception_3b_relu_3x3_reduce(inception_3b_3x3_reduce_bn_out)\n        inception_3b_3x3_out = self.inception_3b_3x3(inception_3b_relu_3x3_reduce_out)\n        inception_3b_3x3_bn_out = self.inception_3b_3x3_bn(inception_3b_3x3_out)\n        inception_3b_relu_3x3_out = self.inception_3b_relu_3x3(inception_3b_3x3_bn_out)\n        inception_3b_double_3x3_reduce_out = self.inception_3b_double_3x3_reduce(inception_3a_output_out)\n        inception_3b_double_3x3_reduce_bn_out = self.inception_3b_double_3x3_reduce_bn(inception_3b_double_3x3_reduce_out)\n        inception_3b_relu_double_3x3_reduce_out = self.inception_3b_relu_double_3x3_reduce(inception_3b_double_3x3_reduce_bn_out)\n        inception_3b_double_3x3_1_out = self.inception_3b_double_3x3_1(inception_3b_relu_double_3x3_reduce_out)\n        inception_3b_double_3x3_1_bn_out = self.inception_3b_double_3x3_1_bn(inception_3b_double_3x3_1_out)\n        inception_3b_relu_double_3x3_1_out = self.inception_3b_relu_double_3x3_1(inception_3b_double_3x3_1_bn_out)\n        inception_3b_double_3x3_2_out = self.inception_3b_double_3x3_2(inception_3b_relu_double_3x3_1_out)\n        inception_3b_double_3x3_2_bn_out = self.inception_3b_double_3x3_2_bn(inception_3b_double_3x3_2_out)\n        inception_3b_relu_double_3x3_2_out = self.inception_3b_relu_double_3x3_2(inception_3b_double_3x3_2_bn_out)\n        inception_3b_pool_out = self.inception_3b_pool(inception_3a_output_out)\n        inception_3b_pool_proj_out = self.inception_3b_pool_proj(inception_3b_pool_out)\n        inception_3b_pool_proj_bn_out = self.inception_3b_pool_proj_bn(inception_3b_pool_proj_out)\n        inception_3b_relu_pool_proj_out = self.inception_3b_relu_pool_proj(inception_3b_pool_proj_bn_out)\n        inception_3b_output_out = torch.cat([inception_3b_relu_1x1_out,inception_3b_relu_3x3_out,inception_3b_relu_double_3x3_2_out,inception_3b_relu_pool_proj_out], 1)\n        inception_3c_3x3_reduce_out = self.inception_3c_3x3_reduce(inception_3b_output_out)\n        inception_3c_3x3_reduce_bn_out = self.inception_3c_3x3_reduce_bn(inception_3c_3x3_reduce_out)\n        inception_3c_relu_3x3_reduce_out = self.inception_3c_relu_3x3_reduce(inception_3c_3x3_reduce_bn_out)\n        inception_3c_3x3_out = self.inception_3c_3x3(inception_3c_relu_3x3_reduce_out)\n        inception_3c_3x3_bn_out = self.inception_3c_3x3_bn(inception_3c_3x3_out)\n        inception_3c_relu_3x3_out = self.inception_3c_relu_3x3(inception_3c_3x3_bn_out)\n        inception_3c_double_3x3_reduce_out = self.inception_3c_double_3x3_reduce(inception_3b_output_out)\n        inception_3c_double_3x3_reduce_bn_out = self.inception_3c_double_3x3_reduce_bn(inception_3c_double_3x3_reduce_out)\n        inception_3c_relu_double_3x3_reduce_out = self.inception_3c_relu_double_3x3_reduce(inception_3c_double_3x3_reduce_bn_out)\n        inception_3c_double_3x3_1_out = self.inception_3c_double_3x3_1(inception_3c_relu_double_3x3_reduce_out)\n        inception_3c_double_3x3_1_bn_out = self.inception_3c_double_3x3_1_bn(inception_3c_double_3x3_1_out)\n        inception_3c_relu_double_3x3_1_out = self.inception_3c_relu_double_3x3_1(inception_3c_double_3x3_1_bn_out)\n        inception_3c_double_3x3_2_out = self.inception_3c_double_3x3_2(inception_3c_relu_double_3x3_1_out)\n        inception_3c_double_3x3_2_bn_out = self.inception_3c_double_3x3_2_bn(inception_3c_double_3x3_2_out)\n        inception_3c_relu_double_3x3_2_out = self.inception_3c_relu_double_3x3_2(inception_3c_double_3x3_2_bn_out)\n        inception_3c_pool_out = self.inception_3c_pool(inception_3b_output_out)\n        inception_3c_output_out = torch.cat([inception_3c_relu_3x3_out,inception_3c_relu_double_3x3_2_out,inception_3c_pool_out], 1)\n        inception_4a_1x1_out = self.inception_4a_1x1(inception_3c_output_out)\n        inception_4a_1x1_bn_out = self.inception_4a_1x1_bn(inception_4a_1x1_out)\n        inception_4a_relu_1x1_out = self.inception_4a_relu_1x1(inception_4a_1x1_bn_out)\n        inception_4a_3x3_reduce_out = self.inception_4a_3x3_reduce(inception_3c_output_out)\n        inception_4a_3x3_reduce_bn_out = self.inception_4a_3x3_reduce_bn(inception_4a_3x3_reduce_out)\n        inception_4a_relu_3x3_reduce_out = self.inception_4a_relu_3x3_reduce(inception_4a_3x3_reduce_bn_out)\n        inception_4a_3x3_out = self.inception_4a_3x3(inception_4a_relu_3x3_reduce_out)\n        inception_4a_3x3_bn_out = self.inception_4a_3x3_bn(inception_4a_3x3_out)\n        inception_4a_relu_3x3_out = self.inception_4a_relu_3x3(inception_4a_3x3_bn_out)\n        inception_4a_double_3x3_reduce_out = self.inception_4a_double_3x3_reduce(inception_3c_output_out)\n        inception_4a_double_3x3_reduce_bn_out = self.inception_4a_double_3x3_reduce_bn(inception_4a_double_3x3_reduce_out)\n        inception_4a_relu_double_3x3_reduce_out = self.inception_4a_relu_double_3x3_reduce(inception_4a_double_3x3_reduce_bn_out)\n        inception_4a_double_3x3_1_out = self.inception_4a_double_3x3_1(inception_4a_relu_double_3x3_reduce_out)\n        inception_4a_double_3x3_1_bn_out = self.inception_4a_double_3x3_1_bn(inception_4a_double_3x3_1_out)\n        inception_4a_relu_double_3x3_1_out = self.inception_4a_relu_double_3x3_1(inception_4a_double_3x3_1_bn_out)\n        inception_4a_double_3x3_2_out = self.inception_4a_double_3x3_2(inception_4a_relu_double_3x3_1_out)\n        inception_4a_double_3x3_2_bn_out = self.inception_4a_double_3x3_2_bn(inception_4a_double_3x3_2_out)\n        inception_4a_relu_double_3x3_2_out = self.inception_4a_relu_double_3x3_2(inception_4a_double_3x3_2_bn_out)\n        inception_4a_pool_out = self.inception_4a_pool(inception_3c_output_out)\n        inception_4a_pool_proj_out = self.inception_4a_pool_proj(inception_4a_pool_out)\n        inception_4a_pool_proj_bn_out = self.inception_4a_pool_proj_bn(inception_4a_pool_proj_out)\n        inception_4a_relu_pool_proj_out = self.inception_4a_relu_pool_proj(inception_4a_pool_proj_bn_out)\n        inception_4a_output_out = torch.cat([inception_4a_relu_1x1_out,inception_4a_relu_3x3_out,inception_4a_relu_double_3x3_2_out,inception_4a_relu_pool_proj_out], 1)\n        inception_4b_1x1_out = self.inception_4b_1x1(inception_4a_output_out)\n        inception_4b_1x1_bn_out = self.inception_4b_1x1_bn(inception_4b_1x1_out)\n        inception_4b_relu_1x1_out = self.inception_4b_relu_1x1(inception_4b_1x1_bn_out)\n        inception_4b_3x3_reduce_out = self.inception_4b_3x3_reduce(inception_4a_output_out)\n        inception_4b_3x3_reduce_bn_out = self.inception_4b_3x3_reduce_bn(inception_4b_3x3_reduce_out)\n        inception_4b_relu_3x3_reduce_out = self.inception_4b_relu_3x3_reduce(inception_4b_3x3_reduce_bn_out)\n        inception_4b_3x3_out = self.inception_4b_3x3(inception_4b_relu_3x3_reduce_out)\n        inception_4b_3x3_bn_out = self.inception_4b_3x3_bn(inception_4b_3x3_out)\n        inception_4b_relu_3x3_out = self.inception_4b_relu_3x3(inception_4b_3x3_bn_out)\n        inception_4b_double_3x3_reduce_out = self.inception_4b_double_3x3_reduce(inception_4a_output_out)\n        inception_4b_double_3x3_reduce_bn_out = self.inception_4b_double_3x3_reduce_bn(inception_4b_double_3x3_reduce_out)\n        inception_4b_relu_double_3x3_reduce_out = self.inception_4b_relu_double_3x3_reduce(inception_4b_double_3x3_reduce_bn_out)\n        inception_4b_double_3x3_1_out = self.inception_4b_double_3x3_1(inception_4b_relu_double_3x3_reduce_out)\n        inception_4b_double_3x3_1_bn_out = self.inception_4b_double_3x3_1_bn(inception_4b_double_3x3_1_out)\n        inception_4b_relu_double_3x3_1_out = self.inception_4b_relu_double_3x3_1(inception_4b_double_3x3_1_bn_out)\n        inception_4b_double_3x3_2_out = self.inception_4b_double_3x3_2(inception_4b_relu_double_3x3_1_out)\n        inception_4b_double_3x3_2_bn_out = self.inception_4b_double_3x3_2_bn(inception_4b_double_3x3_2_out)\n        inception_4b_relu_double_3x3_2_out = self.inception_4b_relu_double_3x3_2(inception_4b_double_3x3_2_bn_out)\n        inception_4b_pool_out = self.inception_4b_pool(inception_4a_output_out)\n        inception_4b_pool_proj_out = self.inception_4b_pool_proj(inception_4b_pool_out)\n        inception_4b_pool_proj_bn_out = self.inception_4b_pool_proj_bn(inception_4b_pool_proj_out)\n        inception_4b_relu_pool_proj_out = self.inception_4b_relu_pool_proj(inception_4b_pool_proj_bn_out)\n        inception_4b_output_out = torch.cat([inception_4b_relu_1x1_out,inception_4b_relu_3x3_out,inception_4b_relu_double_3x3_2_out,inception_4b_relu_pool_proj_out], 1)\n        inception_4c_1x1_out = self.inception_4c_1x1(inception_4b_output_out)\n        inception_4c_1x1_bn_out = self.inception_4c_1x1_bn(inception_4c_1x1_out)\n        inception_4c_relu_1x1_out = self.inception_4c_relu_1x1(inception_4c_1x1_bn_out)\n        inception_4c_3x3_reduce_out = self.inception_4c_3x3_reduce(inception_4b_output_out)\n        inception_4c_3x3_reduce_bn_out = self.inception_4c_3x3_reduce_bn(inception_4c_3x3_reduce_out)\n        inception_4c_relu_3x3_reduce_out = self.inception_4c_relu_3x3_reduce(inception_4c_3x3_reduce_bn_out)\n        inception_4c_3x3_out = self.inception_4c_3x3(inception_4c_relu_3x3_reduce_out)\n        inception_4c_3x3_bn_out = self.inception_4c_3x3_bn(inception_4c_3x3_out)\n        inception_4c_relu_3x3_out = self.inception_4c_relu_3x3(inception_4c_3x3_bn_out)\n        inception_4c_double_3x3_reduce_out = self.inception_4c_double_3x3_reduce(inception_4b_output_out)\n        inception_4c_double_3x3_reduce_bn_out = self.inception_4c_double_3x3_reduce_bn(inception_4c_double_3x3_reduce_out)\n        inception_4c_relu_double_3x3_reduce_out = self.inception_4c_relu_double_3x3_reduce(inception_4c_double_3x3_reduce_bn_out)\n        inception_4c_double_3x3_1_out = self.inception_4c_double_3x3_1(inception_4c_relu_double_3x3_reduce_out)\n        inception_4c_double_3x3_1_bn_out = self.inception_4c_double_3x3_1_bn(inception_4c_double_3x3_1_out)\n        inception_4c_relu_double_3x3_1_out = self.inception_4c_relu_double_3x3_1(inception_4c_double_3x3_1_bn_out)\n        inception_4c_double_3x3_2_out = self.inception_4c_double_3x3_2(inception_4c_relu_double_3x3_1_out)\n        inception_4c_double_3x3_2_bn_out = self.inception_4c_double_3x3_2_bn(inception_4c_double_3x3_2_out)\n        inception_4c_relu_double_3x3_2_out = self.inception_4c_relu_double_3x3_2(inception_4c_double_3x3_2_bn_out)\n        inception_4c_pool_out = self.inception_4c_pool(inception_4b_output_out)\n        inception_4c_pool_proj_out = self.inception_4c_pool_proj(inception_4c_pool_out)\n        inception_4c_pool_proj_bn_out = self.inception_4c_pool_proj_bn(inception_4c_pool_proj_out)\n        inception_4c_relu_pool_proj_out = self.inception_4c_relu_pool_proj(inception_4c_pool_proj_bn_out)\n        inception_4c_output_out = torch.cat([inception_4c_relu_1x1_out,inception_4c_relu_3x3_out,inception_4c_relu_double_3x3_2_out,inception_4c_relu_pool_proj_out], 1)\n        inception_4d_1x1_out = self.inception_4d_1x1(inception_4c_output_out)\n        inception_4d_1x1_bn_out = self.inception_4d_1x1_bn(inception_4d_1x1_out)\n        inception_4d_relu_1x1_out = self.inception_4d_relu_1x1(inception_4d_1x1_bn_out)\n        inception_4d_3x3_reduce_out = self.inception_4d_3x3_reduce(inception_4c_output_out)\n        inception_4d_3x3_reduce_bn_out = self.inception_4d_3x3_reduce_bn(inception_4d_3x3_reduce_out)\n        inception_4d_relu_3x3_reduce_out = self.inception_4d_relu_3x3_reduce(inception_4d_3x3_reduce_bn_out)\n        inception_4d_3x3_out = self.inception_4d_3x3(inception_4d_relu_3x3_reduce_out)\n        inception_4d_3x3_bn_out = self.inception_4d_3x3_bn(inception_4d_3x3_out)\n        inception_4d_relu_3x3_out = self.inception_4d_relu_3x3(inception_4d_3x3_bn_out)\n        inception_4d_double_3x3_reduce_out = self.inception_4d_double_3x3_reduce(inception_4c_output_out)\n        inception_4d_double_3x3_reduce_bn_out = self.inception_4d_double_3x3_reduce_bn(inception_4d_double_3x3_reduce_out)\n        inception_4d_relu_double_3x3_reduce_out = self.inception_4d_relu_double_3x3_reduce(inception_4d_double_3x3_reduce_bn_out)\n        inception_4d_double_3x3_1_out = self.inception_4d_double_3x3_1(inception_4d_relu_double_3x3_reduce_out)\n        inception_4d_double_3x3_1_bn_out = self.inception_4d_double_3x3_1_bn(inception_4d_double_3x3_1_out)\n        inception_4d_relu_double_3x3_1_out = self.inception_4d_relu_double_3x3_1(inception_4d_double_3x3_1_bn_out)\n        inception_4d_double_3x3_2_out = self.inception_4d_double_3x3_2(inception_4d_relu_double_3x3_1_out)\n        inception_4d_double_3x3_2_bn_out = self.inception_4d_double_3x3_2_bn(inception_4d_double_3x3_2_out)\n        inception_4d_relu_double_3x3_2_out = self.inception_4d_relu_double_3x3_2(inception_4d_double_3x3_2_bn_out)\n        inception_4d_pool_out = self.inception_4d_pool(inception_4c_output_out)\n        inception_4d_pool_proj_out = self.inception_4d_pool_proj(inception_4d_pool_out)\n        inception_4d_pool_proj_bn_out = self.inception_4d_pool_proj_bn(inception_4d_pool_proj_out)\n        inception_4d_relu_pool_proj_out = self.inception_4d_relu_pool_proj(inception_4d_pool_proj_bn_out)\n        inception_4d_output_out = torch.cat([inception_4d_relu_1x1_out,inception_4d_relu_3x3_out,inception_4d_relu_double_3x3_2_out,inception_4d_relu_pool_proj_out], 1)\n        inception_4e_3x3_reduce_out = self.inception_4e_3x3_reduce(inception_4d_output_out)\n        inception_4e_3x3_reduce_bn_out = self.inception_4e_3x3_reduce_bn(inception_4e_3x3_reduce_out)\n        inception_4e_relu_3x3_reduce_out = self.inception_4e_relu_3x3_reduce(inception_4e_3x3_reduce_bn_out)\n        inception_4e_3x3_out = self.inception_4e_3x3(inception_4e_relu_3x3_reduce_out)\n        inception_4e_3x3_bn_out = self.inception_4e_3x3_bn(inception_4e_3x3_out)\n        inception_4e_relu_3x3_out = self.inception_4e_relu_3x3(inception_4e_3x3_bn_out)\n        inception_4e_double_3x3_reduce_out = self.inception_4e_double_3x3_reduce(inception_4d_output_out)\n        inception_4e_double_3x3_reduce_bn_out = self.inception_4e_double_3x3_reduce_bn(inception_4e_double_3x3_reduce_out)\n        inception_4e_relu_double_3x3_reduce_out = self.inception_4e_relu_double_3x3_reduce(inception_4e_double_3x3_reduce_bn_out)\n        inception_4e_double_3x3_1_out = self.inception_4e_double_3x3_1(inception_4e_relu_double_3x3_reduce_out)\n        inception_4e_double_3x3_1_bn_out = self.inception_4e_double_3x3_1_bn(inception_4e_double_3x3_1_out)\n        inception_4e_relu_double_3x3_1_out = self.inception_4e_relu_double_3x3_1(inception_4e_double_3x3_1_bn_out)\n        inception_4e_double_3x3_2_out = self.inception_4e_double_3x3_2(inception_4e_relu_double_3x3_1_out)\n        inception_4e_double_3x3_2_bn_out = self.inception_4e_double_3x3_2_bn(inception_4e_double_3x3_2_out)\n        inception_4e_relu_double_3x3_2_out = self.inception_4e_relu_double_3x3_2(inception_4e_double_3x3_2_bn_out)\n        inception_4e_pool_out = self.inception_4e_pool(inception_4d_output_out)\n        inception_4e_output_out = torch.cat([inception_4e_relu_3x3_out,inception_4e_relu_double_3x3_2_out,inception_4e_pool_out], 1)\n        inception_5a_1x1_out = self.inception_5a_1x1(inception_4e_output_out)\n        inception_5a_1x1_bn_out = self.inception_5a_1x1_bn(inception_5a_1x1_out)\n        inception_5a_relu_1x1_out = self.inception_5a_relu_1x1(inception_5a_1x1_bn_out)\n        inception_5a_3x3_reduce_out = self.inception_5a_3x3_reduce(inception_4e_output_out)\n        inception_5a_3x3_reduce_bn_out = self.inception_5a_3x3_reduce_bn(inception_5a_3x3_reduce_out)\n        inception_5a_relu_3x3_reduce_out = self.inception_5a_relu_3x3_reduce(inception_5a_3x3_reduce_bn_out)\n        inception_5a_3x3_out = self.inception_5a_3x3(inception_5a_relu_3x3_reduce_out)\n        inception_5a_3x3_bn_out = self.inception_5a_3x3_bn(inception_5a_3x3_out)\n        inception_5a_relu_3x3_out = self.inception_5a_relu_3x3(inception_5a_3x3_bn_out)\n        inception_5a_double_3x3_reduce_out = self.inception_5a_double_3x3_reduce(inception_4e_output_out)\n        inception_5a_double_3x3_reduce_bn_out = self.inception_5a_double_3x3_reduce_bn(inception_5a_double_3x3_reduce_out)\n        inception_5a_relu_double_3x3_reduce_out = self.inception_5a_relu_double_3x3_reduce(inception_5a_double_3x3_reduce_bn_out)\n        inception_5a_double_3x3_1_out = self.inception_5a_double_3x3_1(inception_5a_relu_double_3x3_reduce_out)\n        inception_5a_double_3x3_1_bn_out = self.inception_5a_double_3x3_1_bn(inception_5a_double_3x3_1_out)\n        inception_5a_relu_double_3x3_1_out = self.inception_5a_relu_double_3x3_1(inception_5a_double_3x3_1_bn_out)\n        inception_5a_double_3x3_2_out = self.inception_5a_double_3x3_2(inception_5a_relu_double_3x3_1_out)\n        inception_5a_double_3x3_2_bn_out = self.inception_5a_double_3x3_2_bn(inception_5a_double_3x3_2_out)\n        inception_5a_relu_double_3x3_2_out = self.inception_5a_relu_double_3x3_2(inception_5a_double_3x3_2_bn_out)\n        inception_5a_pool_out = self.inception_5a_pool(inception_4e_output_out)\n        inception_5a_pool_proj_out = self.inception_5a_pool_proj(inception_5a_pool_out)\n        inception_5a_pool_proj_bn_out = self.inception_5a_pool_proj_bn(inception_5a_pool_proj_out)\n        inception_5a_relu_pool_proj_out = self.inception_5a_relu_pool_proj(inception_5a_pool_proj_bn_out)\n        inception_5a_output_out = torch.cat([inception_5a_relu_1x1_out,inception_5a_relu_3x3_out,inception_5a_relu_double_3x3_2_out,inception_5a_relu_pool_proj_out], 1)\n        inception_5b_1x1_out = self.inception_5b_1x1(inception_5a_output_out)\n        inception_5b_1x1_bn_out = self.inception_5b_1x1_bn(inception_5b_1x1_out)\n        inception_5b_relu_1x1_out = self.inception_5b_relu_1x1(inception_5b_1x1_bn_out)\n        inception_5b_3x3_reduce_out = self.inception_5b_3x3_reduce(inception_5a_output_out)\n        inception_5b_3x3_reduce_bn_out = self.inception_5b_3x3_reduce_bn(inception_5b_3x3_reduce_out)\n        inception_5b_relu_3x3_reduce_out = self.inception_5b_relu_3x3_reduce(inception_5b_3x3_reduce_bn_out)\n        inception_5b_3x3_out = self.inception_5b_3x3(inception_5b_relu_3x3_reduce_out)\n        inception_5b_3x3_bn_out = self.inception_5b_3x3_bn(inception_5b_3x3_out)\n        inception_5b_relu_3x3_out = self.inception_5b_relu_3x3(inception_5b_3x3_bn_out)\n        inception_5b_double_3x3_reduce_out = self.inception_5b_double_3x3_reduce(inception_5a_output_out)\n        inception_5b_double_3x3_reduce_bn_out = self.inception_5b_double_3x3_reduce_bn(inception_5b_double_3x3_reduce_out)\n        inception_5b_relu_double_3x3_reduce_out = self.inception_5b_relu_double_3x3_reduce(inception_5b_double_3x3_reduce_bn_out)\n        inception_5b_double_3x3_1_out = self.inception_5b_double_3x3_1(inception_5b_relu_double_3x3_reduce_out)\n        inception_5b_double_3x3_1_bn_out = self.inception_5b_double_3x3_1_bn(inception_5b_double_3x3_1_out)\n        inception_5b_relu_double_3x3_1_out = self.inception_5b_relu_double_3x3_1(inception_5b_double_3x3_1_bn_out)\n        inception_5b_double_3x3_2_out = self.inception_5b_double_3x3_2(inception_5b_relu_double_3x3_1_out)\n        inception_5b_double_3x3_2_bn_out = self.inception_5b_double_3x3_2_bn(inception_5b_double_3x3_2_out)\n        inception_5b_relu_double_3x3_2_out = self.inception_5b_relu_double_3x3_2(inception_5b_double_3x3_2_bn_out)\n        inception_5b_pool_out = self.inception_5b_pool(inception_5a_output_out)\n        inception_5b_pool_proj_out = self.inception_5b_pool_proj(inception_5b_pool_out)\n        inception_5b_pool_proj_bn_out = self.inception_5b_pool_proj_bn(inception_5b_pool_proj_out)\n        inception_5b_relu_pool_proj_out = self.inception_5b_relu_pool_proj(inception_5b_pool_proj_bn_out)\n        inception_5b_output_out = torch.cat([inception_5b_relu_1x1_out,inception_5b_relu_3x3_out,inception_5b_relu_double_3x3_2_out,inception_5b_relu_pool_proj_out], 1)\n        return inception_5b_output_out\n\n    def logits(self, features):\n        adaptiveAvgPoolWidth = features.shape[2]\n        x = F.avg_pool2d(features, kernel_size=adaptiveAvgPoolWidth)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\ndef bninception(pretrained=\'imagenet\'):\n    r""""""Pretrained BNInception\n    """"""\n    model = BNInception()\n    if pretrained:\n        settings = pretrained_settings[\'bninception\'][pretrained]\n        model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n\n    return model\n\n\nif __name__ == \'__main__\':\n\n    model = bninception()\n'"
pywick/models/classification/fbresnet.py,3,"b'""""""`Facebook implementation <https://github.com/facebook/fb.resnet.torch>`_ of ResNet""""""\n\n# Source: https://github.com/Cadene/pretrained-models.pytorch/blob/0819c4f43a70fcd40234b03ff02f87599cd8ace6/pretrainedmodels/models/fbresnet.py\n# Note this is the version with adaptive capabilities so it can accept differently-sized images\n\n\nfrom __future__ import print_function, division, absolute_import\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport torch.utils.model_zoo as model_zoo\n\n\n__all__ = [\'FBResNet\', \'FBResNet18\', \'FBResNet34\', \'FBResNet50\', \'FBResNet101\', \'fbresnet152\']\n\npretrained_settings = {\n    \'fbresnet152\': {\n        \'imagenet\': {\n            # \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/fbresnet152-2e20f6b4.pth\',        # old version?\n            \'url\': \'http://pretorched-x.csail.mit.edu/models/fbresnet152-3ade0e00.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    }\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=True)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=True)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=True)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=True)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass FBResNet(nn.Module):\n    def __init__(self, block, layers, num_classes=1000):\n        self.inplanes = 64\n        # Special attributs\n        self.input_space = None\n        self.input_size = (299, 299, 3)\n        self.mean = None\n        self.std = None\n        super(FBResNet, self).__init__()\n        # Modules\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                                bias=True)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=True),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def features(self, input):\n        x = self.conv1(input)\n        self.conv1_input = x.clone()\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def logits(self, features):\n        adaptiveAvgPoolWidth = features.shape[2]\n        x = F.avg_pool2d(features, kernel_size=adaptiveAvgPoolWidth)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n\ndef FBResNet18(num_classes=1000):\n    """"""Constructs a ResNet-18 model.\n\n    Args:\n        num_classes\n    """"""\n    model = FBResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n    return model\n\n\ndef FBResNet34(num_classes=1000):\n    """"""Constructs a ResNet-34 model.\n\n    Args:\n        num_classes\n    """"""\n    model = FBResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes)\n    return model\n\n\ndef FBResNet50(num_classes=1000):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        num_classes\n    """"""\n    model = FBResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes)\n    return model\n\n\ndef FBResNet101(num_classes=1000):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        num_classes\n    """"""\n    model = FBResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes)\n    return model\n\n\ndef fbresnet152(num_classes=1000, pretrained=\'imagenet\'):\n    """"""Constructs a ResNet-152 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = FBResNet(Bottleneck, [3, 8, 36, 3], num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'fbresnet152\'][pretrained]\n        assert num_classes == settings[\'num_classes\'], \\\n            ""num_classes should be {}, but is {}"".format(settings[\'num_classes\'], num_classes)\n        model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n    return model\n\n\n'"
pywick/models/classification/inception_resv2_wide.py,9,"b'# Source: https://github.com/pytorch/vision/pull/159/commits/881380c63edc995f540157bc026b1ad181ff0e85#diff-68398f4672bf28b96202308cf30204f6\n# Pretrained: No\n\n""""""Inception Resnet V2 Wide implementation""""""\n\nimport torch\nimport torch.nn as nn\n\n\nclass BasicConv2d(nn.Module):\n\n    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n                              padding=padding, bias=False)\n        self.bn = nn.BatchNorm2d(out_planes, eps=0.001, momentum=0, affine=True)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n\nclass StemBlock(nn.Module):\n    \'\'\'\n    input 299*299*3\n    output 35*35*384\n    \'\'\'\n    def __init__(self):\n        super(StemBlock, self).__init__()\n        self.model_a = nn.Sequential(\n            BasicConv2d(3, 32, kernel_size=3, stride=2),\n            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(32, 64, kernel_size=3, stride=1)\n        )\n        self.branch_a0 = nn.MaxPool2d(3, stride=2)\n        self.branch_a1 = BasicConv2d(64, 96, kernel_size=3, stride=2)\n        self.branch_b0 = nn.Sequential(\n            BasicConv2d(160, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1)\n        )\n        self.branch_b1 = nn.Sequential(\n            BasicConv2d(160, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 64, kernel_size=(1, 7), stride=1, padding=(0, 3)),\n            BasicConv2d(64, 64, kernel_size=(7, 1), stride=1, padding=(3, 0)),\n            BasicConv2d(64, 96, kernel_size=3, stride=1)\n        )\n        self.branch_c0 = BasicConv2d(192, 192, kernel_size=3, stride=2)\n        self.branch_c1 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x = self.model_a(x)\n        x_0 = self.branch_a0(x)\n        x_1 = self.branch_a1(x)\n        x = torch.cat((x_0, x_1), 1)\n        x_0 = self.branch_b0(x)\n        x_1 = self.branch_b1(x)\n        x = torch.cat((x_0, x_1), 1)\n        x_0 = self.branch_c0(x)\n        x_1 = self.branch_c1(x)\n        x = torch.cat((x_0, x_1), 1)\n        return x\n\n\nclass InceptionResA(nn.Module):\n    \'\'\'\n    input 35*35*384\n    output 35*35*384\n    \'\'\'\n\n    def __init__(self, scale=1.0):\n        super(InceptionResA, self).__init__()\n        self.relu = nn.ReLU(inplace=False)\n        self.scale = scale\n        self.branch_0 = BasicConv2d(384, 32, kernel_size=1, stride=1)\n        self.branch_1 = nn.Sequential(\n            BasicConv2d(384, 32, kernel_size=1, stride=1),\n            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1)\n        )\n        self.branch_2 = nn.Sequential(\n            BasicConv2d(384, 32, kernel_size=1, stride=1),\n            BasicConv2d(32, 48, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(48, 64, kernel_size=3, stride=1, padding=1)\n        )\n        self.branch_all = BasicConv2d(128, 384, kernel_size=1, stride=1)\n\n    def forward(self, x):\n        x = self.relu(x)\n        x_0 = self.branch_0(x)\n        x_1 = self.branch_1(x)\n        x_2 = self.branch_2(x)\n        x_new = torch.cat((x_0, x_1, x_2), 1)\n        x_new = self.branch_all(x_new)\n        x = x + x_new * self.scale\n        return x\n\n\nclass ReductionA(nn.Module):\n    \'\'\'\n    input 35*35*384\n    output 17*17*1152\n    \'\'\'\n    def __init__(self):\n        super(ReductionA, self).__init__()\n        self.branch_0 = nn.MaxPool2d(3, stride=2)\n        self.branch_1 = BasicConv2d(384, 384, kernel_size=3, stride=2)\n        self.branch_2 = nn.Sequential(\n            BasicConv2d(384, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(256, 384, kernel_size=3, stride=2)\n        )\n\n    def forward(self, x):\n        x_0 = self.branch_0(x)\n        x_1 = self.branch_1(x)\n        x_2 = self.branch_2(x)\n        return torch.cat((x_0, x_1, x_2), 1)\n\n\nclass InceptionResB(nn.Module):\n    \'\'\'\n    input 17*17*1152\n    output 17*17*1152\n    \'\'\'\n    def __init__(self, scale=1.0):\n        super(InceptionResB, self).__init__()\n        self.relu = nn.ReLU(inplace=False)\n        self.scale = scale\n        self.branch_0 = BasicConv2d(1152, 192, kernel_size=1, stride=1)\n        self.branch_1 = nn.Sequential(\n            BasicConv2d(1152, 128, kernel_size=1, stride=1),\n            BasicConv2d(128, 160, kernel_size=(1, 7), stride=1, padding=(0, 3)),\n            BasicConv2d(160, 192, kernel_size=(7, 1), stride=1, padding=(3, 0))\n        )\n        self.branch_all = BasicConv2d(384, 1152, kernel_size=1, stride=1)\n\n    def forward(self, x):\n        x = self.relu(x)\n        x_0 = self.branch_0(x)\n        x_1 = self.branch_1(x)\n        x_new = torch.cat((x_0, x_1), 1)\n        x_new = self.branch_all(x_new)\n        x = x + x_new * self.scale\n        return x\n\n\nclass ReductionB(nn.Module):\n    \'\'\'\n    input 17*17*1152\n    ouput 8*8*2144\n    \'\'\'\n    def __init__(self):\n        super(ReductionB, self).__init__()\n        self.branch_0 = nn.MaxPool2d(3, stride=2)\n        self.branch_1 = nn.Sequential(\n            BasicConv2d(1152, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 384, kernel_size=3, stride=2)\n        )\n        self.branch_2 = nn.Sequential(\n            BasicConv2d(1152, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 288, kernel_size=3, stride=2)\n        )\n        self.branch_3 = nn.Sequential(\n            BasicConv2d(1152, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 288, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(288, 320, kernel_size=3, stride=2)\n        )\n\n    def forward(self, x):\n        x_0 = self.branch_0(x)\n        x_1 = self.branch_1(x)\n        x_2 = self.branch_2(x)\n        x_3 = self.branch_3(x)\n        return torch.cat((x_0, x_1, x_2, x_3), 1)\n\n\nclass InceptionResC(nn.Module):\n    \'\'\'\n    input 8*8*2144\n    output 8*8*2144\n    \'\'\'\n    def __init__(self, scale=1.0):\n        super(InceptionResC, self).__init__()\n        self.scale = scale\n        self.relu = nn.ReLU(inplace=False)\n        self.branch_0 = BasicConv2d(2144, 192, kernel_size=1, stride=1)\n        self.branch_1 = nn.Sequential(\n            BasicConv2d(2144, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 224, kernel_size=(1, 3), stride=1, padding=(0, 1)),\n            BasicConv2d(224, 256, kernel_size=(3, 1), stride=1, padding=(1, 0))\n        )\n        self.branch_all = BasicConv2d(448, 2144, kernel_size=1, stride=1)\n\n    def forward(self, x):\n        x = self.relu(x)\n        x_0 = self.branch_0(x)\n        x_1 = self.branch_1(x)\n        x_new = torch.cat((x_0, x_1), 1)\n        x_new = self.branch_all(x_new)\n        x = x + x_new * self.scale\n        return x\n\n\nclass InceptionResV2(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(InceptionResV2, self).__init__()\n        self.stem = StemBlock()\n        self.inception_resA5 = nn.Sequential(\n            InceptionResA(),\n            InceptionResA(),\n            InceptionResA(),\n            InceptionResA(),\n            InceptionResA()\n        )\n        self.reductionA = ReductionA()\n        self.inception_resB10 = nn.Sequential(\n            InceptionResB(),\n            InceptionResB(),\n            InceptionResB(),\n            InceptionResB(),\n            InceptionResB(),\n            InceptionResB(),\n            InceptionResB(),\n            InceptionResB(),\n            InceptionResB(),\n            InceptionResB()\n        )\n        self.reductionB = ReductionB()\n        self.inception_resC5 = nn.Sequential(\n            InceptionResC(),\n            InceptionResC(),\n            InceptionResC(),\n            InceptionResC(),\n            InceptionResC()\n        )\n        self.avg_pool = nn.AvgPool2d(8, count_include_pad=False)\n        self.dropout = nn.Dropout2d(p=0.8)\n        self.last_linear = nn.Linear(2144, num_classes)\n\n    def forward(self, x):\n        x = self.stem(x)\n        x = self.inception_resA5(x)\n        x = self.reductionA(x)\n        x = self.inception_resB10(x)\n        x = self.reductionB(x)\n        x = self.inception_resC5(x)\n        x = self.avg_pool(x)\n        x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x'"
pywick/models/classification/inception_v4.py,13,"b""# Source: https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/inceptionv4.py (License: BSD-3-Clause)\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\n\n__all__ = ['InceptionV4', 'inceptionv4']\n\npretrained_settings = {\n    'inceptionv4': {\n        'imagenet': {\n            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/inceptionv4-8e4777a0.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 299, 299],\n            'input_range': [0, 1],\n            'mean': [0.5, 0.5, 0.5],\n            'std': [0.5, 0.5, 0.5],\n            'num_classes': 1000\n        },\n        'imagenet+background': {\n            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/inceptionv4-8e4777a0.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 299, 299],\n            'input_range': [0, 1],\n            'mean': [0.5, 0.5, 0.5],\n            'std': [0.5, 0.5, 0.5],\n            'num_classes': 1001\n        }\n    }\n}\n\n\nclass BasicConv2d(nn.Module):\n    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_planes, out_planes,\n                              kernel_size=kernel_size, stride=stride,\n                              padding=padding, bias=False)  # verify bias false\n        self.bn = nn.BatchNorm2d(out_planes,\n                                 eps=0.001,  # value found in tensorflow\n                                 momentum=0.1,  # default pytorch value\n                                 affine=True)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n\nclass Mixed_3a(nn.Module):\n    def __init__(self):\n        super(Mixed_3a, self).__init__()\n        self.maxpool = nn.MaxPool2d(3, stride=2)\n        self.conv = BasicConv2d(64, 96, kernel_size=3, stride=2)\n\n    def forward(self, x):\n        x0 = self.maxpool(x)\n        x1 = self.conv(x)\n        out = torch.cat((x0, x1), 1)\n        return out\n\n\nclass Mixed_4a(nn.Module):\n    def __init__(self):\n        super(Mixed_4a, self).__init__()\n\n        self.branch0 = nn.Sequential(\n            BasicConv2d(160, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1)\n        )\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(160, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 64, kernel_size=(1, 7), stride=1, padding=(0, 3)),\n            BasicConv2d(64, 64, kernel_size=(7, 1), stride=1, padding=(3, 0)),\n            BasicConv2d(64, 96, kernel_size=(3, 3), stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        out = torch.cat((x0, x1), 1)\n        return out\n\n\nclass Mixed_5a(nn.Module):\n    def __init__(self):\n        super(Mixed_5a, self).__init__()\n        self.conv = BasicConv2d(192, 192, kernel_size=3, stride=2)\n        self.maxpool = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.conv(x)\n        x1 = self.maxpool(x)\n        out = torch.cat((x0, x1), 1)\n        return out\n\n\nclass Inception_A(nn.Module):\n    def __init__(self):\n        super(Inception_A, self).__init__()\n        self.branch0 = BasicConv2d(384, 96, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(384, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(384, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(96, 96, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            BasicConv2d(384, 96, kernel_size=1, stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Reduction_A(nn.Module):\n    def __init__(self):\n        super(Reduction_A, self).__init__()\n        self.branch0 = BasicConv2d(384, 384, kernel_size=3, stride=2)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(384, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 224, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(224, 256, kernel_size=3, stride=2)\n        )\n\n        self.branch2 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        return out\n\n\nclass Inception_B(nn.Module):\n    def __init__(self):\n        super(Inception_B, self).__init__()\n        self.branch0 = BasicConv2d(1024, 384, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(1024, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 224, kernel_size=(1, 7), stride=1, padding=(0, 3)),\n            BasicConv2d(224, 256, kernel_size=(7, 1), stride=1, padding=(3, 0))\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(1024, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 192, kernel_size=(7, 1), stride=1, padding=(3, 0)),\n            BasicConv2d(192, 224, kernel_size=(1, 7), stride=1, padding=(0, 3)),\n            BasicConv2d(224, 224, kernel_size=(7, 1), stride=1, padding=(3, 0)),\n            BasicConv2d(224, 256, kernel_size=(1, 7), stride=1, padding=(0, 3))\n        )\n\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            BasicConv2d(1024, 128, kernel_size=1, stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Reduction_B(nn.Module):\n    def __init__(self):\n        super(Reduction_B, self).__init__()\n\n        self.branch0 = nn.Sequential(\n            BasicConv2d(1024, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 192, kernel_size=3, stride=2)\n        )\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(1024, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 256, kernel_size=(1, 7), stride=1, padding=(0, 3)),\n            BasicConv2d(256, 320, kernel_size=(7, 1), stride=1, padding=(3, 0)),\n            BasicConv2d(320, 320, kernel_size=3, stride=2)\n        )\n\n        self.branch2 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        return out\n\n\nclass Inception_C(nn.Module):\n    def __init__(self):\n        super(Inception_C, self).__init__()\n\n        self.branch0 = BasicConv2d(1536, 256, kernel_size=1, stride=1)\n\n        self.branch1_0 = BasicConv2d(1536, 384, kernel_size=1, stride=1)\n        self.branch1_1a = BasicConv2d(384, 256, kernel_size=(1, 3), stride=1, padding=(0, 1))\n        self.branch1_1b = BasicConv2d(384, 256, kernel_size=(3, 1), stride=1, padding=(1, 0))\n\n        self.branch2_0 = BasicConv2d(1536, 384, kernel_size=1, stride=1)\n        self.branch2_1 = BasicConv2d(384, 448, kernel_size=(3, 1), stride=1, padding=(1, 0))\n        self.branch2_2 = BasicConv2d(448, 512, kernel_size=(1, 3), stride=1, padding=(0, 1))\n        self.branch2_3a = BasicConv2d(512, 256, kernel_size=(1, 3), stride=1, padding=(0, 1))\n        self.branch2_3b = BasicConv2d(512, 256, kernel_size=(3, 1), stride=1, padding=(1, 0))\n\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            BasicConv2d(1536, 256, kernel_size=1, stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n\n        x1_0 = self.branch1_0(x)\n        x1_1a = self.branch1_1a(x1_0)\n        x1_1b = self.branch1_1b(x1_0)\n        x1 = torch.cat((x1_1a, x1_1b), 1)\n\n        x2_0 = self.branch2_0(x)\n        x2_1 = self.branch2_1(x2_0)\n        x2_2 = self.branch2_2(x2_1)\n        x2_3a = self.branch2_3a(x2_2)\n        x2_3b = self.branch2_3b(x2_2)\n        x2 = torch.cat((x2_3a, x2_3b), 1)\n\n        x3 = self.branch3(x)\n\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass InceptionV4(nn.Module):\n    def __init__(self, num_classes=1001):\n        super(InceptionV4, self).__init__()\n        # Special attributes\n        self.input_space = None\n        self.input_size = (299, 299, 3)\n        self.mean = None\n        self.std = None\n        # Modules\n        self.features = nn.Sequential(\n            BasicConv2d(3, 32, kernel_size=3, stride=2),\n            BasicConv2d(32, 32, kernel_size=3, stride=1),\n            BasicConv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            Mixed_3a(),\n            Mixed_4a(),\n            Mixed_5a(),\n            Inception_A(),\n            Inception_A(),\n            Inception_A(),\n            Inception_A(),\n            Reduction_A(),  # Mixed_6a\n            Inception_B(),\n            Inception_B(),\n            Inception_B(),\n            Inception_B(),\n            Inception_B(),\n            Inception_B(),\n            Inception_B(),\n            Reduction_B(),  # Mixed_7a\n            Inception_C(),\n            Inception_C(),\n            Inception_C()\n        )\n        self.last_linear = nn.Linear(1536, num_classes)\n\n    def logits(self, features):\n        #Allows image of any size to be processed\n        adaptiveAvgPoolWidth = features.shape[2]\n        x = F.avg_pool2d(features, kernel_size=adaptiveAvgPoolWidth)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n\ndef inceptionv4(pretrained='imagenet'):\n    # both 'imagenet'&'imagenet+background' are loaded from same parameters\n    model = InceptionV4(num_classes=1001)\n    if pretrained:\n        settings = pretrained_settings['inceptionv4'][pretrained]\n        model.load_state_dict(model_zoo.load_url(settings['url']))\n\n        model.input_space = settings['input_space']\n        model.input_size = settings['input_size']\n        model.input_range = settings['input_range']\n        model.mean = settings['mean']\n        model.std = settings['std']\n\n    if pretrained == 'imagenet':\n        new_last_linear = nn.Linear(1536, settings['num_classes'])\n        new_last_linear.weight.data = model.last_linear.weight.data[1:]\n        new_last_linear.bias.data = model.last_linear.bias.data[1:]\n        model.last_linear = new_last_linear\n\n    return model\n\n\n'''\nTEST\nRun this code with:\n```\ncd $HOME/pretrained-models.pytorch\npython -m pretrainedmodels.inceptionv4\n```\n'''\nif __name__ == '__main__':\n    assert inceptionv4(num_classes=10, pretrained=None)\n    print('success')\n    assert inceptionv4(num_classes=1000, pretrained='imagenet')\n    print('success')\n    assert inceptionv4(num_classes=1001, pretrained='imagenet+background')\n    print('success')\n\n    # fail\n    assert inceptionv4(num_classes=1001, pretrained='imagenet')"""
pywick/models/classification/inceptionresnet_v2.py,8,"b'# Source: https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/inceptionresnetv2.py (License: BSD-3-Clause)\n# Pretrained: Yes\n\nr""""""InceptionResNetV2 model architecture from the\n`""InceptionV4, Inception-ResNet..."" <https://arxiv.org/abs/1602.07261>`_ paper.\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\n__all__ = [\'InceptionResNetV2\', \'inceptionresnetv2\']\n\npretrained_settings = {\n    \'inceptionresnetv2\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/inceptionresnetv2-520b38e4.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 299, 299],\n            \'input_range\': [0, 1],\n            \'mean\': [0.5, 0.5, 0.5],\n            \'std\': [0.5, 0.5, 0.5],\n            \'num_classes\': 1000\n        },\n        \'imagenet+background\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/inceptionresnetv2-520b38e4.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 299, 299],\n            \'input_range\': [0, 1],\n            \'mean\': [0.5, 0.5, 0.5],\n            \'std\': [0.5, 0.5, 0.5],\n            \'num_classes\': 1001\n        }\n    }\n}\n\n\nclass BasicConv2d(nn.Module):\n    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_planes, out_planes,\n                              kernel_size=kernel_size, stride=stride,\n                              padding=padding, bias=False)  # verify bias false\n        self.bn = nn.BatchNorm2d(out_planes,\n                                 eps=0.001,  # value found in tensorflow\n                                 momentum=0.1,  # default pytorch value\n                                 affine=True)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n\nclass Mixed_5b(nn.Module):\n    def __init__(self):\n        super(Mixed_5b, self).__init__()\n\n        self.branch0 = BasicConv2d(192, 96, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(192, 48, kernel_size=1, stride=1),\n            BasicConv2d(48, 64, kernel_size=5, stride=1, padding=2)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(192, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(96, 96, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            BasicConv2d(192, 64, kernel_size=1, stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Block35(nn.Module):\n    def __init__(self, scale=1.0):\n        super(Block35, self).__init__()\n\n        self.scale = scale\n\n        self.branch0 = BasicConv2d(320, 32, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(320, 32, kernel_size=1, stride=1),\n            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(320, 32, kernel_size=1, stride=1),\n            BasicConv2d(32, 48, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(48, 64, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.conv2d = nn.Conv2d(128, 320, kernel_size=1, stride=1)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        out = self.relu(out)\n        return out\n\n\nclass Mixed_6a(nn.Module):\n    def __init__(self):\n        super(Mixed_6a, self).__init__()\n\n        self.branch0 = BasicConv2d(320, 384, kernel_size=3, stride=2)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(320, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(256, 384, kernel_size=3, stride=2)\n        )\n\n        self.branch2 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        return out\n\n\nclass Block17(nn.Module):\n    def __init__(self, scale=1.0):\n        super(Block17, self).__init__()\n\n        self.scale = scale\n\n        self.branch0 = BasicConv2d(1088, 192, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(1088, 128, kernel_size=1, stride=1),\n            BasicConv2d(128, 160, kernel_size=(1, 7), stride=1, padding=(0, 3)),\n            BasicConv2d(160, 192, kernel_size=(7, 1), stride=1, padding=(3, 0))\n        )\n\n        self.conv2d = nn.Conv2d(384, 1088, kernel_size=1, stride=1)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        out = torch.cat((x0, x1), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        out = self.relu(out)\n        return out\n\n\nclass Mixed_7a(nn.Module):\n    def __init__(self):\n        super(Mixed_7a, self).__init__()\n\n        self.branch0 = nn.Sequential(\n            BasicConv2d(1088, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 384, kernel_size=3, stride=2)\n        )\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(1088, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 288, kernel_size=3, stride=2)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(1088, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 288, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(288, 320, kernel_size=3, stride=2)\n        )\n\n        self.branch3 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Block8(nn.Module):\n    def __init__(self, scale=1.0, noReLU=False):\n        super(Block8, self).__init__()\n\n        self.scale = scale\n        self.noReLU = noReLU\n\n        self.branch0 = BasicConv2d(2080, 192, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(2080, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 224, kernel_size=(1, 3), stride=1, padding=(0, 1)),\n            BasicConv2d(224, 256, kernel_size=(3, 1), stride=1, padding=(1, 0))\n        )\n\n        self.conv2d = nn.Conv2d(448, 2080, kernel_size=1, stride=1)\n        if not self.noReLU:\n            self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        out = torch.cat((x0, x1), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        if not self.noReLU:\n            out = self.relu(out)\n        return out\n\n\nclass InceptionResNetV2(nn.Module):\n    def __init__(self, num_classes=1001):\n        super(InceptionResNetV2, self).__init__()\n        # Special attributs\n        self.input_space = None\n        self.input_size = (299, 299, 3)\n        self.mean = None\n        self.std = None\n        # Modules\n        self.conv2d_1a = BasicConv2d(3, 32, kernel_size=3, stride=2)\n        self.conv2d_2a = BasicConv2d(32, 32, kernel_size=3, stride=1)\n        self.conv2d_2b = BasicConv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.maxpool_3a = nn.MaxPool2d(3, stride=2)\n        self.conv2d_3b = BasicConv2d(64, 80, kernel_size=1, stride=1)\n        self.conv2d_4a = BasicConv2d(80, 192, kernel_size=3, stride=1)\n        self.maxpool_5a = nn.MaxPool2d(3, stride=2)\n        self.mixed_5b = Mixed_5b()\n        self.repeat = nn.Sequential(\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17)\n        )\n        self.mixed_6a = Mixed_6a()\n        self.repeat_1 = nn.Sequential(\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10)\n        )\n        self.mixed_7a = Mixed_7a()\n        self.repeat_2 = nn.Sequential(\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20)\n        )\n        self.block8 = Block8(noReLU=True)\n        self.conv2d_7b = BasicConv2d(2080, 1536, kernel_size=1, stride=1)\n        self.avgpool_1a = nn.AvgPool2d(8, count_include_pad=False)\n        self.last_linear = nn.Linear(1536, num_classes)\n\n    def features(self, input):\n        x = self.conv2d_1a(input)\n        x = self.conv2d_2a(x)\n        x = self.conv2d_2b(x)\n        x = self.maxpool_3a(x)\n        x = self.conv2d_3b(x)\n        x = self.conv2d_4a(x)\n        x = self.maxpool_5a(x)\n        x = self.mixed_5b(x)\n        x = self.repeat(x)\n        x = self.mixed_6a(x)\n        x = self.repeat_1(x)\n        x = self.mixed_7a(x)\n        x = self.repeat_2(x)\n        x = self.block8(x)\n        x = self.conv2d_7b(x)\n        return x\n\n    def logits(self, features):\n        x = self.avgpool_1a(features)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n\ndef inceptionresnetv2(pretrained=\'imagenet\'):\n\n    # both \'imagenet\'&\'imagenet+background\' are loaded from same parameters\n    model = InceptionResNetV2(num_classes=1001)\n\n    if pretrained:\n        settings = pretrained_settings[\'inceptionresnetv2\'][pretrained]\n        model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n\n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n\n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n\n    if pretrained == \'imagenet\':\n        new_last_linear = nn.Linear(1536, 1000)\n        new_last_linear.weight.data = model.last_linear.weight.data[1:]\n        new_last_linear.bias.data = model.last_linear.bias.data[1:]\n        model.last_linear = new_last_linear\n\n    return model\n\n\n\'\'\'\nTEST\nRun this code with:\n```\ncd $HOME/pretrained-models.pytorch\npython -m pretrainedmodels.inceptionresnetv2\n```\n\'\'\'\nif __name__ == \'__main__\':\n    assert inceptionresnetv2(num_classes=10, pretrained=None)\n    print(\'success\')\n    assert inceptionresnetv2(num_classes=1000, pretrained=\'imagenet\')\n    print(\'success\')\n    assert inceptionresnetv2(num_classes=1001, pretrained=\'imagenet+background\')\n    print(\'success\')\n\n    # fail\n    assert inceptionresnetv2(num_classes=1001, pretrained=\'imagenet\')'"
pywick/models/classification/nasnet.py,10,"b'r""""""NASNetALarge model architecture from the\n`""NASNet"" <https://arxiv.org/abs/1707.07012>`_ paper.\n""""""\n\n# Source: https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/nasnet.py (License: BSD-3-Clause)\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\n__all__ = [\'nasnetalarge\', \'NASNetALarge\']\n\npretrained_settings = {\n    \'nasnetalarge\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/nasnetalarge-a1897284.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 331, 331], # resize 354\n            \'input_range\': [0, 1],\n            \'mean\': [0.5, 0.5, 0.5],\n            \'std\': [0.5, 0.5, 0.5],\n            \'num_classes\': 1000\n        },\n        \'imagenet+background\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/nasnetalarge-a1897284.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 331, 331], # resize 354\n            \'input_range\': [0, 1],\n            \'mean\': [0.5, 0.5, 0.5],\n            \'std\': [0.5, 0.5, 0.5],\n            \'num_classes\': 1001\n        }\n    }\n}\n\nclass MaxPoolPad(nn.Module):\n\n    def __init__(self):\n        super(MaxPoolPad, self).__init__()\n        self.pad = nn.ZeroPad2d((1, 0, 1, 0))\n        self.pool = nn.MaxPool2d(3, stride=2, padding=1)\n\n    def forward(self, x):\n        x = self.pad(x)\n        x = self.pool(x)\n        x = x[:, :, 1:, 1:]\n        return x\n\n\nclass AvgPoolPad(nn.Module):\n\n    def __init__(self, stride=2, padding=1):\n        super(AvgPoolPad, self).__init__()\n        self.pad = nn.ZeroPad2d((1, 0, 1, 0))\n        self.pool = nn.AvgPool2d(3, stride=stride, padding=padding, count_include_pad=False)\n\n    def forward(self, x):\n        x = self.pad(x)\n        x = self.pool(x)\n        x = x[:, :, 1:, 1:]\n        return x\n\n\nclass SeparableConv2d(nn.Module):\n\n    def __init__(self, in_channels, out_channels, dw_kernel, dw_stride, dw_padding, bias=False):\n        super(SeparableConv2d, self).__init__()\n        self.depthwise_conv2d = nn.Conv2d(in_channels, in_channels, dw_kernel,\n                                          stride=dw_stride,\n                                          padding=dw_padding,\n                                          bias=bias,\n                                          groups=in_channels)\n        self.pointwise_conv2d = nn.Conv2d(in_channels, out_channels, 1, stride=1, bias=bias)\n\n    def forward(self, x):\n        x = self.depthwise_conv2d(x)\n        x = self.pointwise_conv2d(x)\n        return x\n\n\nclass BranchSeparables(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=False):\n        super(BranchSeparables, self).__init__()\n        self.relu = nn.ReLU()\n        self.separable_1 = SeparableConv2d(in_channels, in_channels, kernel_size, stride, padding, bias=bias)\n        self.bn_sep_1 = nn.BatchNorm2d(in_channels, eps=0.001, momentum=0.1, affine=True)\n        self.relu1 = nn.ReLU()\n        self.separable_2 = SeparableConv2d(in_channels, out_channels, kernel_size, 1, padding, bias=bias)\n        self.bn_sep_2 = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.1, affine=True)\n\n    def forward(self, x):\n        x = self.relu(x)\n        x = self.separable_1(x)\n        x = self.bn_sep_1(x)\n        x = self.relu1(x)\n        x = self.separable_2(x)\n        x = self.bn_sep_2(x)\n        return x\n\n\nclass BranchSeparablesStem(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=False):\n        super(BranchSeparablesStem, self).__init__()\n        self.relu = nn.ReLU()\n        self.separable_1 = SeparableConv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias)\n        self.bn_sep_1 = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.1, affine=True)\n        self.relu1 = nn.ReLU()\n        self.separable_2 = SeparableConv2d(out_channels, out_channels, kernel_size, 1, padding, bias=bias)\n        self.bn_sep_2 = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.1, affine=True)\n\n    def forward(self, x):\n        x = self.relu(x)\n        x = self.separable_1(x)\n        x = self.bn_sep_1(x)\n        x = self.relu1(x)\n        x = self.separable_2(x)\n        x = self.bn_sep_2(x)\n        return x\n\n\nclass BranchSeparablesReduction(BranchSeparables):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, z_padding=1, bias=False):\n        BranchSeparables.__init__(self, in_channels, out_channels, kernel_size, stride, padding, bias)\n        self.padding = nn.ZeroPad2d((z_padding, 0, z_padding, 0))\n\n    def forward(self, x):\n        x = self.relu(x)\n        x = self.padding(x)\n        x = self.separable_1(x)\n        x = x[:, :, 1:, 1:].contiguous()\n        x = self.bn_sep_1(x)\n        x = self.relu1(x)\n        x = self.separable_2(x)\n        x = self.bn_sep_2(x)\n        return x\n\n\nclass CellStem0(nn.Module):\n\n    def __init__(self):\n        super(CellStem0, self).__init__()\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(96, 42, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(42, eps=0.001, momentum=0.1, affine=True))\n\n        self.comb_iter_0_left = BranchSeparables(42, 42, 5, 2, 2)\n        self.comb_iter_0_right = BranchSeparablesStem(96, 42, 7, 2, 3, bias=False)\n\n        self.comb_iter_1_left = nn.MaxPool2d(3, stride=2, padding=1)\n        self.comb_iter_1_right = BranchSeparablesStem(96, 42, 7, 2, 3, bias=False)\n\n        self.comb_iter_2_left = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)\n        self.comb_iter_2_right = BranchSeparablesStem(96, 42, 5, 2, 2, bias=False)\n\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparables(42, 42, 3, 1, 1, bias=False)\n        self.comb_iter_4_right = nn.MaxPool2d(3, stride=2, padding=1)\n\n    def forward(self, x):\n        x1 = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x1)\n        x_comb_iter_0_right = self.comb_iter_0_right(x)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x1)\n        x_comb_iter_1_right = self.comb_iter_1_right(x)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x1)\n        x_comb_iter_2_right = self.comb_iter_2_right(x)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x1)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass CellStem1(nn.Module):\n\n    def __init__(self):\n        super(CellStem1, self).__init__()\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(168, 84, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(84, eps=0.001, momentum=0.1, affine=True))\n\n        self.relu = nn.ReLU()\n        self.path_1 = nn.Sequential()\n        self.path_1.add_module(\'avgpool\', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n        self.path_1.add_module(\'conv\', nn.Conv2d(96, 42, 1, stride=1, bias=False))\n        self.path_2 = nn.ModuleList()\n        self.path_2.add_module(\'pad\', nn.ZeroPad2d((0, 1, 0, 1)))\n        self.path_2.add_module(\'avgpool\', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n        self.path_2.add_module(\'conv\', nn.Conv2d(96, 42, 1, stride=1, bias=False))\n\n        self.final_path_bn = nn.BatchNorm2d(84, eps=0.001, momentum=0.1, affine=True)\n\n        self.comb_iter_0_left = BranchSeparables(84, 84, 5, 2, 2, bias=False)\n        self.comb_iter_0_right = BranchSeparables(84, 84, 7, 2, 3, bias=False)\n\n        self.comb_iter_1_left = nn.MaxPool2d(3, stride=2, padding=1)\n        self.comb_iter_1_right = BranchSeparables(84, 84, 7, 2, 3, bias=False)\n\n        self.comb_iter_2_left = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)\n        self.comb_iter_2_right = BranchSeparables(84, 84, 5, 2, 2, bias=False)\n\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparables(84, 84, 3, 1, 1, bias=False)\n        self.comb_iter_4_right = nn.MaxPool2d(3, stride=2, padding=1)\n\n    def forward(self, x_conv0, x_stem_0):\n        x_left = self.conv_1x1(x_stem_0)\n\n        x_relu = self.relu(x_conv0)\n        # path 1\n        x_path1 = self.path_1(x_relu)\n        # path 2\n        x_path2 = self.path_2.pad(x_relu)\n        x_path2 = x_path2[:, :, 1:, 1:]\n        x_path2 = self.path_2.avgpool(x_path2)\n        x_path2 = self.path_2.conv(x_path2)\n        # final path\n        x_right = self.final_path_bn(torch.cat([x_path1, x_path2], 1))\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_left)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_right)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_left)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_right)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_left)\n        x_comb_iter_2_right = self.comb_iter_2_right(x_right)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x_left)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass FirstCell(nn.Module):\n\n    def __init__(self, in_channels_left, out_channels_left, in_channels_right, out_channels_right):\n        super(FirstCell, self).__init__()\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(in_channels_right, out_channels_right, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_right, eps=0.001, momentum=0.1, affine=True))\n\n        self.relu = nn.ReLU()\n        self.path_1 = nn.Sequential()\n        self.path_1.add_module(\'avgpool\', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n        self.path_1.add_module(\'conv\', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n        self.path_2 = nn.ModuleList()\n        self.path_2.add_module(\'pad\', nn.ZeroPad2d((0, 1, 0, 1)))\n        self.path_2.add_module(\'avgpool\', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n        self.path_2.add_module(\'conv\', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n\n        self.final_path_bn = nn.BatchNorm2d(out_channels_left * 2, eps=0.001, momentum=0.1, affine=True)\n\n        self.comb_iter_0_left = BranchSeparables(out_channels_right, out_channels_right, 5, 1, 2, bias=False)\n        self.comb_iter_0_right = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n\n        self.comb_iter_1_left = BranchSeparables(out_channels_right, out_channels_right, 5, 1, 2, bias=False)\n        self.comb_iter_1_right = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n\n        self.comb_iter_2_left = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_3_left = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n\n    def forward(self, x, x_prev):\n        x_relu = self.relu(x_prev)\n        # path 1\n        x_path1 = self.path_1(x_relu)\n        # path 2\n        x_path2 = self.path_2.pad(x_relu)\n        x_path2 = x_path2[:, :, 1:, 1:]\n        x_path2 = self.path_2.avgpool(x_path2)\n        x_path2 = self.path_2.conv(x_path2)\n        # final path\n        x_left = self.final_path_bn(torch.cat([x_path1, x_path2], 1))\n\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_left)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2 = x_comb_iter_2_left + x_left\n\n        x_comb_iter_3_left = self.comb_iter_3_left(x_left)\n        x_comb_iter_3_right = self.comb_iter_3_right(x_left)\n        x_comb_iter_3 = x_comb_iter_3_left + x_comb_iter_3_right\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_right\n\n        x_out = torch.cat([x_left, x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass NormalCell(nn.Module):\n\n    def __init__(self, in_channels_left, out_channels_left, in_channels_right, out_channels_right):\n        super(NormalCell, self).__init__()\n        self.conv_prev_1x1 = nn.Sequential()\n        self.conv_prev_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_prev_1x1.add_module(\'conv\', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n        self.conv_prev_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_left, eps=0.001, momentum=0.1, affine=True))\n\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(in_channels_right, out_channels_right, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_right, eps=0.001, momentum=0.1, affine=True))\n\n        self.comb_iter_0_left = BranchSeparables(out_channels_right, out_channels_right, 5, 1, 2, bias=False)\n        self.comb_iter_0_right = BranchSeparables(out_channels_left, out_channels_left, 3, 1, 1, bias=False)\n\n        self.comb_iter_1_left = BranchSeparables(out_channels_left, out_channels_left, 5, 1, 2, bias=False)\n        self.comb_iter_1_right = BranchSeparables(out_channels_left, out_channels_left, 3, 1, 1, bias=False)\n\n        self.comb_iter_2_left = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_3_left = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n\n    def forward(self, x, x_prev):\n        x_left = self.conv_prev_1x1(x_prev)\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_left)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2 = x_comb_iter_2_left + x_left\n\n        x_comb_iter_3_left = self.comb_iter_3_left(x_left)\n        x_comb_iter_3_right = self.comb_iter_3_right(x_left)\n        x_comb_iter_3 = x_comb_iter_3_left + x_comb_iter_3_right\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_right\n\n        x_out = torch.cat([x_left, x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass ReductionCell0(nn.Module):\n\n    def __init__(self, in_channels_left, out_channels_left, in_channels_right, out_channels_right):\n        super(ReductionCell0, self).__init__()\n        self.conv_prev_1x1 = nn.Sequential()\n        self.conv_prev_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_prev_1x1.add_module(\'conv\', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n        self.conv_prev_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_left, eps=0.001, momentum=0.1, affine=True))\n\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(in_channels_right, out_channels_right, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_right, eps=0.001, momentum=0.1, affine=True))\n\n        self.comb_iter_0_left = BranchSeparablesReduction(out_channels_right, out_channels_right, 5, 2, 2, bias=False)\n        self.comb_iter_0_right = BranchSeparablesReduction(out_channels_right, out_channels_right, 7, 2, 3, bias=False)\n\n        self.comb_iter_1_left = MaxPoolPad()\n        self.comb_iter_1_right = BranchSeparablesReduction(out_channels_right, out_channels_right, 7, 2, 3, bias=False)\n\n        self.comb_iter_2_left = AvgPoolPad()\n        self.comb_iter_2_right = BranchSeparablesReduction(out_channels_right, out_channels_right, 5, 2, 2, bias=False)\n\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparablesReduction(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n        self.comb_iter_4_right = MaxPoolPad()\n\n    def forward(self, x, x_prev):\n        x_left = self.conv_prev_1x1(x_prev)\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_right)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2_right = self.comb_iter_2_right(x_left)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass ReductionCell1(nn.Module):\n\n    def __init__(self, in_channels_left, out_channels_left, in_channels_right, out_channels_right):\n        super(ReductionCell1, self).__init__()\n        self.conv_prev_1x1 = nn.Sequential()\n        self.conv_prev_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_prev_1x1.add_module(\'conv\', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n        self.conv_prev_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_left, eps=0.001, momentum=0.1, affine=True))\n\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(in_channels_right, out_channels_right, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_right, eps=0.001, momentum=0.1, affine=True))\n\n        self.comb_iter_0_left = BranchSeparables(out_channels_right, out_channels_right, 5, 2, 2, bias=False)\n        self.comb_iter_0_right = BranchSeparables(out_channels_right, out_channels_right, 7, 2, 3, bias=False)\n\n        self.comb_iter_1_left = nn.MaxPool2d(3, stride=2, padding=1)\n        self.comb_iter_1_right = BranchSeparables(out_channels_right, out_channels_right, 7, 2, 3, bias=False)\n\n        self.comb_iter_2_left = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)\n        self.comb_iter_2_right = BranchSeparables(out_channels_right, out_channels_right, 5, 2, 2, bias=False)\n\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n        self.comb_iter_4_right = nn.MaxPool2d(3, stride=2, padding=1)\n\n    def forward(self, x, x_prev):\n        x_left = self.conv_prev_1x1(x_prev)\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_right)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2_right = self.comb_iter_2_right(x_left)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass NASNetALarge(nn.Module):\n\n    def __init__(self, num_classes=1001):\n        super(NASNetALarge, self).__init__()\n        self.num_classes = num_classes\n\n        self.conv0 = nn.Sequential()\n        self.conv0.add_module(\'conv\', nn.Conv2d(in_channels=3, out_channels=96, kernel_size=3, padding=0, stride=2,\n                                                bias=False))\n        self.conv0.add_module(\'bn\', nn.BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True))\n\n        self.cell_stem_0 = CellStem0()\n        self.cell_stem_1 = CellStem1()\n\n        self.cell_0 = FirstCell(in_channels_left=168, out_channels_left=84,\n                                in_channels_right=336, out_channels_right=168)\n        self.cell_1 = NormalCell(in_channels_left=336, out_channels_left=168,\n                                 in_channels_right=1008, out_channels_right=168)\n        self.cell_2 = NormalCell(in_channels_left=1008, out_channels_left=168,\n                                 in_channels_right=1008, out_channels_right=168)\n        self.cell_3 = NormalCell(in_channels_left=1008, out_channels_left=168,\n                                 in_channels_right=1008, out_channels_right=168)\n        self.cell_4 = NormalCell(in_channels_left=1008, out_channels_left=168,\n                                 in_channels_right=1008, out_channels_right=168)\n        self.cell_5 = NormalCell(in_channels_left=1008, out_channels_left=168,\n                                 in_channels_right=1008, out_channels_right=168)\n\n        self.reduction_cell_0 = ReductionCell0(in_channels_left=1008, out_channels_left=336,\n                                               in_channels_right=1008, out_channels_right=336)\n\n        self.cell_6 = FirstCell(in_channels_left=1008, out_channels_left=168,\n                                in_channels_right=1344, out_channels_right=336)\n        self.cell_7 = NormalCell(in_channels_left=1344, out_channels_left=336,\n                                 in_channels_right=2016, out_channels_right=336)\n        self.cell_8 = NormalCell(in_channels_left=2016, out_channels_left=336,\n                                 in_channels_right=2016, out_channels_right=336)\n        self.cell_9 = NormalCell(in_channels_left=2016, out_channels_left=336,\n                                 in_channels_right=2016, out_channels_right=336)\n        self.cell_10 = NormalCell(in_channels_left=2016, out_channels_left=336,\n                                  in_channels_right=2016, out_channels_right=336)\n        self.cell_11 = NormalCell(in_channels_left=2016, out_channels_left=336,\n                                  in_channels_right=2016, out_channels_right=336)\n\n        self.reduction_cell_1 = ReductionCell1(in_channels_left=2016, out_channels_left=672,\n                                               in_channels_right=2016, out_channels_right=672)\n\n        self.cell_12 = FirstCell(in_channels_left=2016, out_channels_left=336,\n                                 in_channels_right=2688, out_channels_right=672)\n        self.cell_13 = NormalCell(in_channels_left=2688, out_channels_left=672,\n                                  in_channels_right=4032, out_channels_right=672)\n        self.cell_14 = NormalCell(in_channels_left=4032, out_channels_left=672,\n                                  in_channels_right=4032, out_channels_right=672)\n        self.cell_15 = NormalCell(in_channels_left=4032, out_channels_left=672,\n                                  in_channels_right=4032, out_channels_right=672)\n        self.cell_16 = NormalCell(in_channels_left=4032, out_channels_left=672,\n                                  in_channels_right=4032, out_channels_right=672)\n        self.cell_17 = NormalCell(in_channels_left=4032, out_channels_left=672,\n                                  in_channels_right=4032, out_channels_right=672)\n\n        self.relu = nn.ReLU()\n        self.avg_pool = nn.AvgPool2d(11, stride=1, padding=0)\n        self.dropout = nn.Dropout()\n        self.last_linear = nn.Linear(4032, self.num_classes)\n\n    def features(self, input):\n        x_conv0 = self.conv0(input)\n        x_stem_0 = self.cell_stem_0(x_conv0)\n        x_stem_1 = self.cell_stem_1(x_conv0, x_stem_0)\n\n        x_cell_0 = self.cell_0(x_stem_1, x_stem_0)\n        x_cell_1 = self.cell_1(x_cell_0, x_stem_1)\n        x_cell_2 = self.cell_2(x_cell_1, x_cell_0)\n        x_cell_3 = self.cell_3(x_cell_2, x_cell_1)\n        x_cell_4 = self.cell_4(x_cell_3, x_cell_2)\n        x_cell_5 = self.cell_5(x_cell_4, x_cell_3)\n\n        x_reduction_cell_0 = self.reduction_cell_0(x_cell_5, x_cell_4)\n\n        x_cell_6 = self.cell_6(x_reduction_cell_0, x_cell_4)\n        x_cell_7 = self.cell_7(x_cell_6, x_reduction_cell_0)\n        x_cell_8 = self.cell_8(x_cell_7, x_cell_6)\n        x_cell_9 = self.cell_9(x_cell_8, x_cell_7)\n        x_cell_10 = self.cell_10(x_cell_9, x_cell_8)\n        x_cell_11 = self.cell_11(x_cell_10, x_cell_9)\n\n        x_reduction_cell_1 = self.reduction_cell_1(x_cell_11, x_cell_10)\n\n        x_cell_12 = self.cell_12(x_reduction_cell_1, x_cell_10)\n        x_cell_13 = self.cell_13(x_cell_12, x_reduction_cell_1)\n        x_cell_14 = self.cell_14(x_cell_13, x_cell_12)\n        x_cell_15 = self.cell_15(x_cell_14, x_cell_13)\n        x_cell_16 = self.cell_16(x_cell_15, x_cell_14)\n        x_cell_17 = self.cell_17(x_cell_16, x_cell_15)\n        return x_cell_17\n\n    def logits(self, features):\n        x = self.relu(features)\n        x = self.avg_pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.dropout(x)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n\ndef nasnetalarge(pretrained=\'imagenet\'):\n    """"""\n    pretrained NASNet\n    """"""\n\n    # both \'imagenet\'&\'imagenet+background\' are loaded from same parameters\n    model = NASNetALarge(num_classes=1001)\n\n    if pretrained:\n        settings = pretrained_settings[\'nasnetalarge\'][pretrained]\n        model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n\n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n\n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n\n    if pretrained == \'imagenet\':\n        new_last_linear = nn.Linear(model.last_linear.in_features, settings[\'num_classes\'])\n        new_last_linear.weight.data = model.last_linear.weight.data[1:]\n        new_last_linear.bias.data = model.last_linear.bias.data[1:]\n        model.last_linear = new_last_linear\n\n    return model\n\n'"
pywick/models/classification/nasnet_mobile.py,11,"b'# Source: https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/nasnet_mobile.py (License: BSD-3-Clause)\n\n""""""\nNASNet Mobile following the paper:\n`Learning Transferable Architectures for Scalable Image Recognition <https://arxiv.org/abs/1707.07012>`_\n""""""\n\n# Thanks to Anastasiia (https://github.com/DagnyT) for the great help, support and motivation!\n#\n#\n# ------------------------------------------------------------------------------------\n#       Architecture       | Top-1 Acc | Top-5 Acc |  Multiply-Adds |  Params (M)\n# ------------------------------------------------------------------------------------\n# |   NASNet-A (4 @ 1056)  |   74.08%  |   91.74%  |       564 M    |     5.3        |\n# ------------------------------------------------------------------------------------\n# References: [Learning Transferable Architectures for Scalable Image Recognition](https://arxiv.org/abs/1707.07012)\n\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\n__all__ = [\'nasnetamobile\', \'NASNetAMobile\']\n\npretrained_settings = {\n    \'nasnetamobile\': {\n        \'imagenet\': {\n            #\'url\': \'https://github.com/veronikayurchuk/pretrained-models.pytorch/releases/download/v1.0/nasnetmobile-7e03cead.pth.tar\',\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/nasnetamobile-7e03cead.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224], # resize 256\n            \'input_range\': [0, 1],\n            \'mean\': [0.5, 0.5, 0.5],\n            \'std\': [0.5, 0.5, 0.5],\n            \'num_classes\': 1000\n        },\n        # \'imagenet+background\': {\n        #     # \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/nasnetalarge-a1897284.pth\',\n        #     \'input_space\': \'RGB\',\n        #     \'input_size\': [3, 224, 224], # resize 256\n        #     \'input_range\': [0, 1],\n        #     \'mean\': [0.5, 0.5, 0.5],\n        #     \'std\': [0.5, 0.5, 0.5],\n        #     \'num_classes\': 1001\n        # }\n    }\n}\n\n\nclass MaxPoolPad(nn.Module):\n\n    def __init__(self):\n        super(MaxPoolPad, self).__init__()\n        self.pad = nn.ZeroPad2d((1, 0, 1, 0))\n        self.pool = nn.MaxPool2d(3, stride=2, padding=1)\n\n    def forward(self, x):\n        x = self.pad(x)\n        x = self.pool(x)\n        x = x[:, :, 1:, 1:].contiguous()\n        return x\n\n\nclass AvgPoolPad(nn.Module):\n\n    def __init__(self, stride=2, padding=1):\n        super(AvgPoolPad, self).__init__()\n        self.pad = nn.ZeroPad2d((1, 0, 1, 0))\n        self.pool = nn.AvgPool2d(3, stride=stride, padding=padding, count_include_pad=False)\n\n    def forward(self, x):\n        x = self.pad(x)\n        x = self.pool(x)\n        x = x[:, :, 1:, 1:].contiguous()\n        return x\n\n\nclass SeparableConv2d(nn.Module):\n\n    def __init__(self, in_channels, out_channels, dw_kernel, dw_stride, dw_padding, bias=False):\n        super(SeparableConv2d, self).__init__()\n        self.depthwise_conv2d = nn.Conv2d(in_channels, in_channels, dw_kernel,\n                                          stride=dw_stride,\n                                          padding=dw_padding,\n                                          bias=bias,\n                                          groups=in_channels)\n        self.pointwise_conv2d = nn.Conv2d(in_channels, out_channels, 1, stride=1, bias=bias)\n\n    def forward(self, x):\n        x = self.depthwise_conv2d(x)\n        x = self.pointwise_conv2d(x)\n        return x\n\n\nclass BranchSeparables(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, name=None, bias=False):\n        super(BranchSeparables, self).__init__()\n        self.relu = nn.ReLU()\n        self.separable_1 = SeparableConv2d(in_channels, in_channels, kernel_size, stride, padding, bias=bias)\n        self.bn_sep_1 = nn.BatchNorm2d(in_channels, eps=0.001, momentum=0.1, affine=True)\n        self.relu1 = nn.ReLU()\n        self.separable_2 = SeparableConv2d(in_channels, out_channels, kernel_size, 1, padding, bias=bias)\n        self.bn_sep_2 = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.1, affine=True)\n        self.name = name\n\n    def forward(self, x):\n        x = self.relu(x)\n        if self.name == \'specific\':\n            x = nn.ZeroPad2d((1, 0, 1, 0))(x)\n        x = self.separable_1(x)\n        if self.name == \'specific\':\n            x = x[:, :, 1:, 1:].contiguous()\n\n        x = self.bn_sep_1(x)\n        x = self.relu1(x)\n        x = self.separable_2(x)\n        x = self.bn_sep_2(x)\n        return x\n\n\nclass BranchSeparablesStem(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=False):\n        super(BranchSeparablesStem, self).__init__()\n        self.relu = nn.ReLU()\n        self.separable_1 = SeparableConv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias)\n        self.bn_sep_1 = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.1, affine=True)\n        self.relu1 = nn.ReLU()\n        self.separable_2 = SeparableConv2d(out_channels, out_channels, kernel_size, 1, padding, bias=bias)\n        self.bn_sep_2 = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.1, affine=True)\n\n    def forward(self, x):\n        x = self.relu(x)\n        x = self.separable_1(x)\n        x = self.bn_sep_1(x)\n        x = self.relu1(x)\n        x = self.separable_2(x)\n        x = self.bn_sep_2(x)\n        return x\n\n\nclass BranchSeparablesReduction(BranchSeparables):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, z_padding=1, bias=False):\n        BranchSeparables.__init__(self, in_channels, out_channels, kernel_size, stride, padding, bias)\n        self.padding = nn.ZeroPad2d((z_padding, 0, z_padding, 0))\n\n    def forward(self, x):\n        x = self.relu(x)\n        x = self.padding(x)\n        x = self.separable_1(x)\n        x = x[:, :, 1:, 1:].contiguous()\n        x = self.bn_sep_1(x)\n        x = self.relu1(x)\n        x = self.separable_2(x)\n        x = self.bn_sep_2(x)\n        return x\n\n\nclass CellStem0(nn.Module):\n    def __init__(self, stem_filters, num_filters=42):\n        super(CellStem0, self).__init__()\n        self.num_filters = num_filters\n        self.stem_filters = stem_filters\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(self.stem_filters, self.num_filters, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(self.num_filters, eps=0.001, momentum=0.1, affine=True))\n\n        self.comb_iter_0_left = BranchSeparables(self.num_filters, self.num_filters, 5, 2, 2)\n        self.comb_iter_0_right = BranchSeparablesStem(self.stem_filters, self.num_filters, 7, 2, 3, bias=False)\n\n        self.comb_iter_1_left = nn.MaxPool2d(3, stride=2, padding=1)\n        self.comb_iter_1_right = BranchSeparablesStem(self.stem_filters, self.num_filters, 7, 2, 3, bias=False)\n\n        self.comb_iter_2_left = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)\n        self.comb_iter_2_right = BranchSeparablesStem(self.stem_filters, self.num_filters, 5, 2, 2, bias=False)\n\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparables(self.num_filters, self.num_filters, 3, 1, 1, bias=False)\n        self.comb_iter_4_right = nn.MaxPool2d(3, stride=2, padding=1)\n\n    def forward(self, x):\n        x1 = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x1)\n        x_comb_iter_0_right = self.comb_iter_0_right(x)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x1)\n        x_comb_iter_1_right = self.comb_iter_1_right(x)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x1)\n        x_comb_iter_2_right = self.comb_iter_2_right(x)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x1)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass CellStem1(nn.Module):\n\n    def __init__(self, stem_filters, num_filters):\n        super(CellStem1, self).__init__()\n        self.num_filters = num_filters\n        self.stem_filters = stem_filters\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(2*self.num_filters, self.num_filters, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(self.num_filters, eps=0.001, momentum=0.1, affine=True))\n\n        self.relu = nn.ReLU()\n        self.path_1 = nn.Sequential()\n        self.path_1.add_module(\'avgpool\', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n        self.path_1.add_module(\'conv\', nn.Conv2d(self.stem_filters, self.num_filters//2, 1, stride=1, bias=False))\n        self.path_2 = nn.ModuleList()\n        self.path_2.add_module(\'pad\', nn.ZeroPad2d((0, 1, 0, 1)))\n        self.path_2.add_module(\'avgpool\', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n        self.path_2.add_module(\'conv\', nn.Conv2d(self.stem_filters, self.num_filters//2, 1, stride=1, bias=False))\n\n        self.final_path_bn = nn.BatchNorm2d(self.num_filters, eps=0.001, momentum=0.1, affine=True)\n\n        self.comb_iter_0_left = BranchSeparables(self.num_filters, self.num_filters, 5, 2, 2, name=\'specific\', bias=False)\n        self.comb_iter_0_right = BranchSeparables(self.num_filters, self.num_filters, 7, 2, 3, name=\'specific\', bias=False)\n\n        # self.comb_iter_1_left = nn.MaxPool2d(3, stride=2, padding=1)\n        self.comb_iter_1_left = MaxPoolPad()\n        self.comb_iter_1_right = BranchSeparables(self.num_filters, self.num_filters, 7, 2, 3, name=\'specific\', bias=False)\n\n        # self.comb_iter_2_left = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)\n        self.comb_iter_2_left = AvgPoolPad()\n        self.comb_iter_2_right = BranchSeparables(self.num_filters, self.num_filters, 5, 2, 2, name=\'specific\', bias=False)\n\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparables(self.num_filters, self.num_filters, 3, 1, 1, name=\'specific\', bias=False)\n        # self.comb_iter_4_right = nn.MaxPool2d(3, stride=2, padding=1)\n        self.comb_iter_4_right = MaxPoolPad()\n\n    def forward(self, x_conv0, x_stem_0):\n        x_left = self.conv_1x1(x_stem_0)\n\n        x_relu = self.relu(x_conv0)\n        # path 1\n        x_path1 = self.path_1(x_relu)\n        # path 2\n        x_path2 = self.path_2.pad(x_relu)\n        x_path2 = x_path2[:, :, 1:, 1:]\n        x_path2 = self.path_2.avgpool(x_path2)\n        x_path2 = self.path_2.conv(x_path2)\n        # final path\n        x_right = self.final_path_bn(torch.cat([x_path1, x_path2], 1))\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_left)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_right)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_left)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_right)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_left)\n        x_comb_iter_2_right = self.comb_iter_2_right(x_right)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x_left)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass FirstCell(nn.Module):\n\n    def __init__(self, in_channels_left, out_channels_left, in_channels_right, out_channels_right):\n        super(FirstCell, self).__init__()\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(in_channels_right, out_channels_right, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_right, eps=0.001, momentum=0.1, affine=True))\n\n        self.relu = nn.ReLU()\n        self.path_1 = nn.Sequential()\n        self.path_1.add_module(\'avgpool\', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n        self.path_1.add_module(\'conv\', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n        self.path_2 = nn.ModuleList()\n        self.path_2.add_module(\'pad\', nn.ZeroPad2d((0, 1, 0, 1)))\n        self.path_2.add_module(\'avgpool\', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n        self.path_2.add_module(\'conv\', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n\n        self.final_path_bn = nn.BatchNorm2d(out_channels_left * 2, eps=0.001, momentum=0.1, affine=True)\n\n        self.comb_iter_0_left = BranchSeparables(out_channels_right, out_channels_right, 5, 1, 2, bias=False)\n        self.comb_iter_0_right = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n\n        self.comb_iter_1_left = BranchSeparables(out_channels_right, out_channels_right, 5, 1, 2, bias=False)\n        self.comb_iter_1_right = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n\n        self.comb_iter_2_left = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_3_left = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n\n    def forward(self, x, x_prev):\n        x_relu = self.relu(x_prev)\n        # path 1\n        x_path1 = self.path_1(x_relu)\n        # path 2\n        x_path2 = self.path_2.pad(x_relu)\n        x_path2 = x_path2[:, :, 1:, 1:]\n        x_path2 = self.path_2.avgpool(x_path2)\n        x_path2 = self.path_2.conv(x_path2)\n        # final path\n        x_left = self.final_path_bn(torch.cat([x_path1, x_path2], 1))\n\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_left)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2 = x_comb_iter_2_left + x_left\n\n        x_comb_iter_3_left = self.comb_iter_3_left(x_left)\n        x_comb_iter_3_right = self.comb_iter_3_right(x_left)\n        x_comb_iter_3 = x_comb_iter_3_left + x_comb_iter_3_right\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_right\n\n        x_out = torch.cat([x_left, x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass NormalCell(nn.Module):\n\n    def __init__(self, in_channels_left, out_channels_left, in_channels_right, out_channels_right):\n        super(NormalCell, self).__init__()\n        self.conv_prev_1x1 = nn.Sequential()\n        self.conv_prev_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_prev_1x1.add_module(\'conv\', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n        self.conv_prev_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_left, eps=0.001, momentum=0.1, affine=True))\n\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(in_channels_right, out_channels_right, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_right, eps=0.001, momentum=0.1, affine=True))\n\n        self.comb_iter_0_left = BranchSeparables(out_channels_right, out_channels_right, 5, 1, 2, bias=False)\n        self.comb_iter_0_right = BranchSeparables(out_channels_left, out_channels_left, 3, 1, 1, bias=False)\n\n        self.comb_iter_1_left = BranchSeparables(out_channels_left, out_channels_left, 5, 1, 2, bias=False)\n        self.comb_iter_1_right = BranchSeparables(out_channels_left, out_channels_left, 3, 1, 1, bias=False)\n\n        self.comb_iter_2_left = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_3_left = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n\n    def forward(self, x, x_prev):\n        x_left = self.conv_prev_1x1(x_prev)\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_left)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2 = x_comb_iter_2_left + x_left\n\n        x_comb_iter_3_left = self.comb_iter_3_left(x_left)\n        x_comb_iter_3_right = self.comb_iter_3_right(x_left)\n        x_comb_iter_3 = x_comb_iter_3_left + x_comb_iter_3_right\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_right\n\n        x_out = torch.cat([x_left, x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass ReductionCell0(nn.Module):\n\n    def __init__(self, in_channels_left, out_channels_left, in_channels_right, out_channels_right):\n        super(ReductionCell0, self).__init__()\n        self.conv_prev_1x1 = nn.Sequential()\n        self.conv_prev_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_prev_1x1.add_module(\'conv\', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n        self.conv_prev_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_left, eps=0.001, momentum=0.1, affine=True))\n\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(in_channels_right, out_channels_right, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_right, eps=0.001, momentum=0.1, affine=True))\n\n        self.comb_iter_0_left = BranchSeparablesReduction(out_channels_right, out_channels_right, 5, 2, 2, bias=False)\n        self.comb_iter_0_right = BranchSeparablesReduction(out_channels_right, out_channels_right, 7, 2, 3, bias=False)\n\n        self.comb_iter_1_left = MaxPoolPad()\n        self.comb_iter_1_right = BranchSeparablesReduction(out_channels_right, out_channels_right, 7, 2, 3, bias=False)\n\n        self.comb_iter_2_left = AvgPoolPad()\n        self.comb_iter_2_right = BranchSeparablesReduction(out_channels_right, out_channels_right, 5, 2, 2, bias=False)\n\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparablesReduction(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n        self.comb_iter_4_right = MaxPoolPad()\n\n    def forward(self, x, x_prev):\n        x_left = self.conv_prev_1x1(x_prev)\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_right)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2_right = self.comb_iter_2_right(x_left)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass ReductionCell1(nn.Module):\n\n    def __init__(self, in_channels_left, out_channels_left, in_channels_right, out_channels_right):\n        super(ReductionCell1, self).__init__()\n        self.conv_prev_1x1 = nn.Sequential()\n        self.conv_prev_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_prev_1x1.add_module(\'conv\', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n        self.conv_prev_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_left, eps=0.001, momentum=0.1, affine=True))\n\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(in_channels_right, out_channels_right, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_right, eps=0.001, momentum=0.1, affine=True))\n\n        self.comb_iter_0_left = BranchSeparables(out_channels_right, out_channels_right, 5, 2, 2, name=\'specific\', bias=False)\n        self.comb_iter_0_right = BranchSeparables(out_channels_right, out_channels_right, 7, 2, 3, name=\'specific\', bias=False)\n\n        # self.comb_iter_1_left = nn.MaxPool2d(3, stride=2, padding=1)\n        self.comb_iter_1_left = MaxPoolPad()\n        self.comb_iter_1_right = BranchSeparables(out_channels_right, out_channels_right, 7, 2, 3, name=\'specific\', bias=False)\n\n        # self.comb_iter_2_left = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)\n        self.comb_iter_2_left = AvgPoolPad()\n        self.comb_iter_2_right = BranchSeparables(out_channels_right, out_channels_right, 5, 2, 2, name=\'specific\', bias=False)\n\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, name=\'specific\', bias=False)\n        # self.comb_iter_4_right = nn.MaxPool2d(3, stride=2, padding=1)\n        self.comb_iter_4_right =MaxPoolPad()\n\n    def forward(self, x, x_prev):\n        x_left = self.conv_prev_1x1(x_prev)\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_right)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2_right = self.comb_iter_2_right(x_left)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass NASNetAMobile(nn.Module):\n    """"""NASNetAMobile (4 @ 1056) """"""\n\n    def __init__(self, num_classes=1001, stem_filters=32, penultimate_filters=1056, filters_multiplier=2):\n        super(NASNetAMobile, self).__init__()\n        self.num_classes = num_classes\n        self.stem_filters = stem_filters\n        self.penultimate_filters = penultimate_filters\n        self.filters_multiplier = filters_multiplier\n\n        filters = self.penultimate_filters // 24\n        # 24 is default value for the architecture\n\n        self.conv0 = nn.Sequential()\n        self.conv0.add_module(\'conv\', nn.Conv2d(in_channels=3, out_channels=self.stem_filters, kernel_size=3, padding=0, stride=2,\n                                                bias=False))\n        self.conv0.add_module(\'bn\', nn.BatchNorm2d(self.stem_filters, eps=0.001, momentum=0.1, affine=True))\n\n        self.cell_stem_0 = CellStem0(self.stem_filters, num_filters=filters // (filters_multiplier ** 2))\n        self.cell_stem_1 = CellStem1(self.stem_filters, num_filters=filters // filters_multiplier)\n\n        self.cell_0 = FirstCell(in_channels_left=filters, out_channels_left=filters//2, # 1, 0.5\n                                in_channels_right=2*filters, out_channels_right=filters) # 2, 1\n        self.cell_1 = NormalCell(in_channels_left=2*filters, out_channels_left=filters, # 2, 1\n                                 in_channels_right=6*filters, out_channels_right=filters) # 6, 1\n        self.cell_2 = NormalCell(in_channels_left=6*filters, out_channels_left=filters, # 6, 1\n                                 in_channels_right=6*filters, out_channels_right=filters) # 6, 1\n        self.cell_3 = NormalCell(in_channels_left=6*filters, out_channels_left=filters, # 6, 1\n                                 in_channels_right=6*filters, out_channels_right=filters) # 6, 1\n\n        self.reduction_cell_0 = ReductionCell0(in_channels_left=6*filters, out_channels_left=2*filters, # 6, 2\n                                               in_channels_right=6*filters, out_channels_right=2*filters) # 6, 2\n\n        self.cell_6 = FirstCell(in_channels_left=6*filters, out_channels_left=filters, # 6, 1\n                                in_channels_right=8*filters, out_channels_right=2*filters) # 8, 2\n        self.cell_7 = NormalCell(in_channels_left=8*filters, out_channels_left=2*filters, # 8, 2\n                                 in_channels_right=12*filters, out_channels_right=2*filters) # 12, 2\n        self.cell_8 = NormalCell(in_channels_left=12*filters, out_channels_left=2*filters, # 12, 2\n                                 in_channels_right=12*filters, out_channels_right=2*filters) # 12, 2\n        self.cell_9 = NormalCell(in_channels_left=12*filters, out_channels_left=2*filters, # 12, 2\n                                 in_channels_right=12*filters, out_channels_right=2*filters) # 12, 2\n\n        self.reduction_cell_1 = ReductionCell1(in_channels_left=12*filters, out_channels_left=4*filters, # 12, 4\n                                               in_channels_right=12*filters, out_channels_right=4*filters) # 12, 4\n\n        self.cell_12 = FirstCell(in_channels_left=12*filters, out_channels_left=2*filters, # 12, 2\n                                 in_channels_right=16*filters, out_channels_right=4*filters) # 16, 4\n        self.cell_13 = NormalCell(in_channels_left=16*filters, out_channels_left=4*filters, # 16, 4\n                                  in_channels_right=24*filters, out_channels_right=4*filters) # 24, 4\n        self.cell_14 = NormalCell(in_channels_left=24*filters, out_channels_left=4*filters, # 24, 4\n                                  in_channels_right=24*filters, out_channels_right=4*filters) # 24, 4\n        self.cell_15 = NormalCell(in_channels_left=24*filters, out_channels_left=4*filters, # 24, 4\n                                  in_channels_right=24*filters, out_channels_right=4*filters) # 24, 4\n\n        self.relu = nn.ReLU()\n        self.avg_pool = nn.AvgPool2d(7, stride=1, padding=0)\n        self.dropout = nn.Dropout()\n        self.last_linear = nn.Linear(24*filters, self.num_classes)\n\n    def features(self, input):\n        x_conv0 = self.conv0(input)\n        x_stem_0 = self.cell_stem_0(x_conv0)\n        x_stem_1 = self.cell_stem_1(x_conv0, x_stem_0)\n\n        x_cell_0 = self.cell_0(x_stem_1, x_stem_0)\n        x_cell_1 = self.cell_1(x_cell_0, x_stem_1)\n        x_cell_2 = self.cell_2(x_cell_1, x_cell_0)\n        x_cell_3 = self.cell_3(x_cell_2, x_cell_1)\n\n        x_reduction_cell_0 = self.reduction_cell_0(x_cell_3, x_cell_2)\n\n        x_cell_6 = self.cell_6(x_reduction_cell_0, x_cell_3)\n        x_cell_7 = self.cell_7(x_cell_6, x_reduction_cell_0)\n        x_cell_8 = self.cell_8(x_cell_7, x_cell_6)\n        x_cell_9 = self.cell_9(x_cell_8, x_cell_7)\n\n        x_reduction_cell_1 = self.reduction_cell_1(x_cell_9, x_cell_8)\n\n        x_cell_12 = self.cell_12(x_reduction_cell_1, x_cell_9)\n        x_cell_13 = self.cell_13(x_cell_12, x_reduction_cell_1)\n        x_cell_14 = self.cell_14(x_cell_13, x_cell_12)\n        x_cell_15 = self.cell_15(x_cell_14, x_cell_13)\n        return x_cell_15\n\n    def logits(self, features):\n        x = self.relu(features)\n        x = self.avg_pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.dropout(x)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n\ndef nasnetamobile(pretrained=\'imagenet\'):\n    """"""Pretrained version of NASNet_mobile""""""\n    model = NASNetAMobile(num_classes=1000) # both \'imagenet\'&\'imagenet+background\' are loaded from same parameters\n    if pretrained:\n        settings = pretrained_settings[\'nasnetamobile\'][pretrained]\n        model.load_state_dict(model_zoo.load_url(settings[\'url\'], map_location=None))\n\n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n\n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n    else:\n        settings = pretrained_settings[\'nasnetamobile\'][\'imagenet\']\n        model = NASNetAMobile(num_classes=1000)\n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n\n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n    return model\n\n\nif __name__ == ""__main__"":\n\n    model = NASNetAMobile()\n    input = torch.randn(2, 3, 224, 224)\n    output = model(input)\n\n    print(output.size())\n'"
pywick/models/classification/pnasnet.py,4,"b'# Source: https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/pnasnet.py (License: BSD-3-Clause)\n\nr""""""PNASNet-5 model architecture from the\n`""Progressive Neural Architecture Search""\n<https://arxiv.org/abs/1712.00559>`_ paper.\n""""""\n\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\n__all__ = [\'pnasnet5large\', \'PNASNet5Large\']\n\npretrained_settings = {\n    \'pnasnet5large\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/pnasnet5large-bf079911.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 331, 331],\n            \'input_range\': [0, 1],\n            \'mean\': [0.5, 0.5, 0.5],\n            \'std\': [0.5, 0.5, 0.5],\n            \'num_classes\': 1000\n        },\n        \'imagenet+background\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/pnasnet5large-bf079911.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 331, 331],\n            \'input_range\': [0, 1],\n            \'mean\': [0.5, 0.5, 0.5],\n            \'std\': [0.5, 0.5, 0.5],\n            \'num_classes\': 1001\n        }\n    }\n}\n\n\nclass MaxPool(nn.Module):\n\n    def __init__(self, kernel_size, stride=1, padding=1, zero_pad=False):\n        super(MaxPool, self).__init__()\n        self.zero_pad = nn.ZeroPad2d((1, 0, 1, 0)) if zero_pad else None\n        self.pool = nn.MaxPool2d(kernel_size, stride=stride, padding=padding)\n\n    def forward(self, x):\n        if self.zero_pad:\n            x = self.zero_pad(x)\n        x = self.pool(x)\n        if self.zero_pad:\n            x = x[:, :, 1:, 1:]\n        return x\n\n\nclass SeparableConv2d(nn.Module):\n\n    def __init__(self, in_channels, out_channels, dw_kernel_size, dw_stride,\n                 dw_padding):\n        super(SeparableConv2d, self).__init__()\n        self.depthwise_conv2d = nn.Conv2d(in_channels, in_channels,\n                                          kernel_size=dw_kernel_size,\n                                          stride=dw_stride, padding=dw_padding,\n                                          groups=in_channels, bias=False)\n        self.pointwise_conv2d = nn.Conv2d(in_channels, out_channels,\n                                          kernel_size=1, bias=False)\n\n    def forward(self, x):\n        x = self.depthwise_conv2d(x)\n        x = self.pointwise_conv2d(x)\n        return x\n\n\nclass BranchSeparables(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 stem_cell=False, zero_pad=False):\n        super(BranchSeparables, self).__init__()\n        padding = kernel_size // 2\n        middle_channels = out_channels if stem_cell else in_channels\n        self.zero_pad = nn.ZeroPad2d((1, 0, 1, 0)) if zero_pad else None\n        self.relu_1 = nn.ReLU()\n        self.separable_1 = SeparableConv2d(in_channels, middle_channels,\n                                           kernel_size, dw_stride=stride,\n                                           dw_padding=padding)\n        self.bn_sep_1 = nn.BatchNorm2d(middle_channels, eps=0.001)\n        self.relu_2 = nn.ReLU()\n        self.separable_2 = SeparableConv2d(middle_channels, out_channels,\n                                           kernel_size, dw_stride=1,\n                                           dw_padding=padding)\n        self.bn_sep_2 = nn.BatchNorm2d(out_channels, eps=0.001)\n\n    def forward(self, x):\n        x = self.relu_1(x)\n        if self.zero_pad:\n            x = self.zero_pad(x)\n        x = self.separable_1(x)\n        if self.zero_pad:\n            x = x[:, :, 1:, 1:].contiguous()\n        x = self.bn_sep_1(x)\n        x = self.relu_2(x)\n        x = self.separable_2(x)\n        x = self.bn_sep_2(x)\n        return x\n\n\nclass ReluConvBn(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1):\n        super(ReluConvBn, self).__init__()\n        self.relu = nn.ReLU()\n        self.conv = nn.Conv2d(in_channels, out_channels,\n                              kernel_size=kernel_size, stride=stride,\n                              bias=False)\n        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n\n    def forward(self, x):\n        x = self.relu(x)\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n\n\nclass FactorizedReduction(nn.Module):\n\n    def __init__(self, in_channels, out_channels):\n        super(FactorizedReduction, self).__init__()\n        self.relu = nn.ReLU()\n        self.path_1 = nn.Sequential(OrderedDict([\n            (\'avgpool\', nn.AvgPool2d(1, stride=2, count_include_pad=False)),\n            (\'conv\', nn.Conv2d(in_channels, out_channels // 2,\n                               kernel_size=1, bias=False)),\n        ]))\n        self.path_2 = nn.Sequential(OrderedDict([\n            (\'pad\', nn.ZeroPad2d((0, 1, 0, 1))),\n            (\'avgpool\', nn.AvgPool2d(1, stride=2, count_include_pad=False)),\n            (\'conv\', nn.Conv2d(in_channels, out_channels // 2,\n                               kernel_size=1, bias=False)),\n        ]))\n        self.final_path_bn = nn.BatchNorm2d(out_channels, eps=0.001)\n\n    def forward(self, x):\n        x = self.relu(x)\n\n        x_path1 = self.path_1(x)\n\n        x_path2 = self.path_2.pad(x)\n        x_path2 = x_path2[:, :, 1:, 1:]\n        x_path2 = self.path_2.avgpool(x_path2)\n        x_path2 = self.path_2.conv(x_path2)\n\n        out = self.final_path_bn(torch.cat([x_path1, x_path2], 1))\n        return out\n\n\nclass CellBase(nn.Module):\n\n    def cell_forward(self, x_left, x_right):\n        x_comb_iter_0_left = self.comb_iter_0_left(x_left)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_right)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_right)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2_right = self.comb_iter_2_right(x_right)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_left = self.comb_iter_3_left(x_comb_iter_2)\n        x_comb_iter_3_right = self.comb_iter_3_right(x_right)\n        x_comb_iter_3 = x_comb_iter_3_left + x_comb_iter_3_right\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_left)\n        if self.comb_iter_4_right:\n            x_comb_iter_4_right = self.comb_iter_4_right(x_right)\n        else:\n            x_comb_iter_4_right = x_right\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = torch.cat(\n            [x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3,\n             x_comb_iter_4], 1)\n        return x_out\n\n\nclass CellStem0(CellBase):\n\n    def __init__(self, in_channels_left, out_channels_left, in_channels_right,\n                 out_channels_right):\n        super(CellStem0, self).__init__()\n        self.conv_1x1 = ReluConvBn(in_channels_right, out_channels_right,\n                                   kernel_size=1)\n        self.comb_iter_0_left = BranchSeparables(in_channels_left,\n                                                 out_channels_left,\n                                                 kernel_size=5, stride=2,\n                                                 stem_cell=True)\n        self.comb_iter_0_right = nn.Sequential(OrderedDict([\n            (\'max_pool\', MaxPool(3, stride=2)),\n            (\'conv\', nn.Conv2d(in_channels_left, out_channels_left,\n                               kernel_size=1, bias=False)),\n            (\'bn\', nn.BatchNorm2d(out_channels_left, eps=0.001)),\n        ]))\n        self.comb_iter_1_left = BranchSeparables(out_channels_right,\n                                                 out_channels_right,\n                                                 kernel_size=7, stride=2)\n        self.comb_iter_1_right = MaxPool(3, stride=2)\n        self.comb_iter_2_left = BranchSeparables(out_channels_right,\n                                                 out_channels_right,\n                                                 kernel_size=5, stride=2)\n        self.comb_iter_2_right = BranchSeparables(out_channels_right,\n                                                  out_channels_right,\n                                                  kernel_size=3, stride=2)\n        self.comb_iter_3_left = BranchSeparables(out_channels_right,\n                                                 out_channels_right,\n                                                 kernel_size=3)\n        self.comb_iter_3_right = MaxPool(3, stride=2)\n        self.comb_iter_4_left = BranchSeparables(in_channels_right,\n                                                 out_channels_right,\n                                                 kernel_size=3, stride=2,\n                                                 stem_cell=True)\n        self.comb_iter_4_right = ReluConvBn(out_channels_right,\n                                            out_channels_right,\n                                            kernel_size=1, stride=2)\n\n    def forward(self, x_left):\n        x_right = self.conv_1x1(x_left)\n        x_out = self.cell_forward(x_left, x_right)\n        return x_out\n\n\nclass Cell(CellBase):\n\n    def __init__(self, in_channels_left, out_channels_left, in_channels_right,\n                 out_channels_right, is_reduction=False, zero_pad=False,\n                 match_prev_layer_dimensions=False):\n        super(Cell, self).__init__()\n\n        # If `is_reduction` is set to `True` stride 2 is used for\n        # convolutional and pooling layers to reduce the spatial size of\n        # the output of a cell approximately by a factor of 2.\n        stride = 2 if is_reduction else 1\n\n        # If `match_prev_layer_dimensions` is set to `True`\n        # `FactorizedReduction` is used to reduce the spatial size\n        # of the left input of a cell approximately by a factor of 2.\n        self.match_prev_layer_dimensions = match_prev_layer_dimensions\n        if match_prev_layer_dimensions:\n            self.conv_prev_1x1 = FactorizedReduction(in_channels_left,\n                                                     out_channels_left)\n        else:\n            self.conv_prev_1x1 = ReluConvBn(in_channels_left,\n                                            out_channels_left, kernel_size=1)\n\n        self.conv_1x1 = ReluConvBn(in_channels_right, out_channels_right,\n                                   kernel_size=1)\n        self.comb_iter_0_left = BranchSeparables(out_channels_left,\n                                                 out_channels_left,\n                                                 kernel_size=5, stride=stride,\n                                                 zero_pad=zero_pad)\n        self.comb_iter_0_right = MaxPool(3, stride=stride, zero_pad=zero_pad)\n        self.comb_iter_1_left = BranchSeparables(out_channels_right,\n                                                 out_channels_right,\n                                                 kernel_size=7, stride=stride,\n                                                 zero_pad=zero_pad)\n        self.comb_iter_1_right = MaxPool(3, stride=stride, zero_pad=zero_pad)\n        self.comb_iter_2_left = BranchSeparables(out_channels_right,\n                                                 out_channels_right,\n                                                 kernel_size=5, stride=stride,\n                                                 zero_pad=zero_pad)\n        self.comb_iter_2_right = BranchSeparables(out_channels_right,\n                                                  out_channels_right,\n                                                  kernel_size=3, stride=stride,\n                                                  zero_pad=zero_pad)\n        self.comb_iter_3_left = BranchSeparables(out_channels_right,\n                                                 out_channels_right,\n                                                 kernel_size=3)\n        self.comb_iter_3_right = MaxPool(3, stride=stride, zero_pad=zero_pad)\n        self.comb_iter_4_left = BranchSeparables(out_channels_left,\n                                                 out_channels_left,\n                                                 kernel_size=3, stride=stride,\n                                                 zero_pad=zero_pad)\n        if is_reduction:\n            self.comb_iter_4_right = ReluConvBn(out_channels_right,\n                                                out_channels_right,\n                                                kernel_size=1, stride=stride)\n        else:\n            self.comb_iter_4_right = None\n\n    def forward(self, x_left, x_right):\n        x_left = self.conv_prev_1x1(x_left)\n        x_right = self.conv_1x1(x_right)\n        x_out = self.cell_forward(x_left, x_right)\n        return x_out\n\n\nclass PNASNet5Large(nn.Module):\n    def __init__(self, num_classes=1001):\n        super().__init__()\n        self.num_classes = num_classes\n        self.conv_0 = nn.Sequential(OrderedDict([\n            (\'conv\', nn.Conv2d(3, 96, kernel_size=3, stride=2, bias=False)),\n            (\'bn\', nn.BatchNorm2d(96, eps=0.001))\n        ]))\n        self.cell_stem_0 = CellStem0(in_channels_left=96, out_channels_left=54,\n                                     in_channels_right=96,\n                                     out_channels_right=54)\n        self.cell_stem_1 = Cell(in_channels_left=96, out_channels_left=108,\n                                in_channels_right=270, out_channels_right=108,\n                                match_prev_layer_dimensions=True,\n                                is_reduction=True)\n        self.cell_0 = Cell(in_channels_left=270, out_channels_left=216,\n                           in_channels_right=540, out_channels_right=216,\n                           match_prev_layer_dimensions=True)\n        self.cell_1 = Cell(in_channels_left=540, out_channels_left=216,\n                           in_channels_right=1080, out_channels_right=216)\n        self.cell_2 = Cell(in_channels_left=1080, out_channels_left=216,\n                           in_channels_right=1080, out_channels_right=216)\n        self.cell_3 = Cell(in_channels_left=1080, out_channels_left=216,\n                           in_channels_right=1080, out_channels_right=216)\n        self.cell_4 = Cell(in_channels_left=1080, out_channels_left=432,\n                           in_channels_right=1080, out_channels_right=432,\n                           is_reduction=True, zero_pad=True)\n        self.cell_5 = Cell(in_channels_left=1080, out_channels_left=432,\n                           in_channels_right=2160, out_channels_right=432,\n                           match_prev_layer_dimensions=True)\n        self.cell_6 = Cell(in_channels_left=2160, out_channels_left=432,\n                           in_channels_right=2160, out_channels_right=432)\n        self.cell_7 = Cell(in_channels_left=2160, out_channels_left=432,\n                           in_channels_right=2160, out_channels_right=432)\n        self.cell_8 = Cell(in_channels_left=2160, out_channels_left=864,\n                           in_channels_right=2160, out_channels_right=864,\n                           is_reduction=True)\n        self.cell_9 = Cell(in_channels_left=2160, out_channels_left=864,\n                           in_channels_right=4320, out_channels_right=864,\n                           match_prev_layer_dimensions=True)\n        self.cell_10 = Cell(in_channels_left=4320, out_channels_left=864,\n                            in_channels_right=4320, out_channels_right=864)\n        self.cell_11 = Cell(in_channels_left=4320, out_channels_left=864,\n                            in_channels_right=4320, out_channels_right=864)\n        self.relu = nn.ReLU()\n        self.avg_pool = nn.AvgPool2d(11, stride=1, padding=0)\n        self.dropout = nn.Dropout(0.5)\n        self.last_linear = nn.Linear(4320, num_classes)\n\n    def features(self, x):\n        x_conv_0 = self.conv_0(x)\n        x_stem_0 = self.cell_stem_0(x_conv_0)\n        x_stem_1 = self.cell_stem_1(x_conv_0, x_stem_0)\n        x_cell_0 = self.cell_0(x_stem_0, x_stem_1)\n        x_cell_1 = self.cell_1(x_stem_1, x_cell_0)\n        x_cell_2 = self.cell_2(x_cell_0, x_cell_1)\n        x_cell_3 = self.cell_3(x_cell_1, x_cell_2)\n        x_cell_4 = self.cell_4(x_cell_2, x_cell_3)\n        x_cell_5 = self.cell_5(x_cell_3, x_cell_4)\n        x_cell_6 = self.cell_6(x_cell_4, x_cell_5)\n        x_cell_7 = self.cell_7(x_cell_5, x_cell_6)\n        x_cell_8 = self.cell_8(x_cell_6, x_cell_7)\n        x_cell_9 = self.cell_9(x_cell_7, x_cell_8)\n        x_cell_10 = self.cell_10(x_cell_8, x_cell_9)\n        x_cell_11 = self.cell_11(x_cell_9, x_cell_10)\n        return x_cell_11\n\n    def logits(self, features):\n        x = self.relu(features)\n        x = self.avg_pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.dropout(x)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n\ndef pnasnet5large(pretrained=\'imagenet\'):\n    """"""Pretrained PNASNet""""""\n\n    # both \'imagenet\'&\'imagenet+background\' are loaded from same parameters\n    model = PNASNet5Large(num_classes=1001)\n\n    if pretrained:\n        settings = pretrained_settings[\'pnasnet5large\'][pretrained]\n        model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n\n    if pretrained == \'imagenet\':\n        new_last_linear = nn.Linear(model.last_linear.in_features, settings[\'num_classes\'])\n        new_last_linear.weight.data = model.last_linear.weight.data[1:]\n        new_last_linear.bias.data = model.last_linear.bias.data[1:]\n        model.last_linear = new_last_linear\n\n    model.input_space = settings[\'input_space\']\n    model.input_size = settings[\'input_size\']\n    model.input_range = settings[\'input_range\']\n\n    model.mean = settings[\'mean\']\n    model.std = settings[\'std\']\n    return model\n'"
pywick/models/classification/poly_net.py,12,"b'""""""\nPolyNet architecture from the paper `PolyNet: A Pursuit of Structural Diversity in Very Deep Networks <https://arxiv.org/abs/1611.05725>`_.\n""""""\n\n# Source: https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/polynet.py (License: BSD-3-Clause)\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils import model_zoo\n\n__all__ = [\'PolyNet\', \'polynet\']\n\npretrained_settings = {\n    \'polynet\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/polynet-f71d82a5.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 331, 331],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        },\n    }\n}\n\n\nclass BasicConv2d(nn.Module):\n\n    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0,\n                 output_relu=True):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size,\n                              stride=stride, padding=padding, bias=False)\n        self.bn = nn.BatchNorm2d(out_planes)\n        self.relu = nn.ReLU() if output_relu else None\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        if self.relu:\n            x = self.relu(x)\n        return x\n\n\nclass PolyConv2d(nn.Module):\n    """"""A block that is used inside poly-N (poly-2, poly-3, and so on) modules.\n    The Convolution layer is shared between all Inception blocks inside\n    a poly-N module. BatchNorm layers are not shared between Inception blocks\n    and therefore the number of BatchNorm layers is equal to the number of\n    Inception blocks inside a poly-N module.\n    """"""\n\n    def __init__(self, in_planes, out_planes, kernel_size, num_blocks,\n                 stride=1, padding=0):\n        super(PolyConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size,\n                              stride=stride, padding=padding, bias=False)\n        self.bn_blocks = nn.ModuleList([\n            nn.BatchNorm2d(out_planes) for _ in range(num_blocks)\n        ])\n        self.relu = nn.ReLU()\n\n    def forward(self, x, block_index):\n        x = self.conv(x)\n        bn = self.bn_blocks[block_index]\n        x = bn(x)\n        x = self.relu(x)\n        return x\n\n\nclass Stem(nn.Module):\n\n    def __init__(self):\n        super(Stem, self).__init__()\n        self.conv1 = nn.Sequential(\n            BasicConv2d(3, 32, kernel_size=3, stride=2),\n            BasicConv2d(32, 32, kernel_size=3),\n            BasicConv2d(32, 64, kernel_size=3, padding=1),\n        )\n        self.conv1_pool_branch = nn.MaxPool2d(3, stride=2)\n        self.conv1_branch = BasicConv2d(64, 96, kernel_size=3, stride=2)\n        self.conv2_short = nn.Sequential(\n            BasicConv2d(160, 64, kernel_size=1),\n            BasicConv2d(64, 96, kernel_size=3),\n        )\n        self.conv2_long = nn.Sequential(\n            BasicConv2d(160, 64, kernel_size=1),\n            BasicConv2d(64, 64, kernel_size=(7, 1), padding=(3, 0)),\n            BasicConv2d(64, 64, kernel_size=(1, 7), padding=(0, 3)),\n            BasicConv2d(64, 96, kernel_size=3),\n        )\n        self.conv2_pool_branch = nn.MaxPool2d(3, stride=2)\n        self.conv2_branch = BasicConv2d(192, 192, kernel_size=3, stride=2)\n\n    def forward(self, x):\n        x = self.conv1(x)\n\n        x0 = self.conv1_pool_branch(x)\n        x1 = self.conv1_branch(x)\n        x = torch.cat((x0, x1), 1)\n\n        x0 = self.conv2_short(x)\n        x1 = self.conv2_long(x)\n        x = torch.cat((x0, x1), 1)\n\n        x0 = self.conv2_pool_branch(x)\n        x1 = self.conv2_branch(x)\n        out = torch.cat((x0, x1), 1)\n        return out\n\n\nclass BlockA(nn.Module):\n    """"""Inception-ResNet-A block.""""""\n\n    def __init__(self):\n        super(BlockA, self).__init__()\n        self.path0 = nn.Sequential(\n            BasicConv2d(384, 32, kernel_size=1),\n            BasicConv2d(32, 48, kernel_size=3, padding=1),\n            BasicConv2d(48, 64, kernel_size=3, padding=1),\n        )\n        self.path1 = nn.Sequential(\n            BasicConv2d(384, 32, kernel_size=1),\n            BasicConv2d(32, 32, kernel_size=3, padding=1),\n        )\n        self.path2 = BasicConv2d(384, 32, kernel_size=1)\n        self.conv2d = BasicConv2d(128, 384, kernel_size=1, output_relu=False)\n\n    def forward(self, x):\n        x0 = self.path0(x)\n        x1 = self.path1(x)\n        x2 = self.path2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        out = self.conv2d(out)\n        return out\n\n\nclass BlockB(nn.Module):\n    """"""Inception-ResNet-B block.""""""\n\n    def __init__(self):\n        super(BlockB, self).__init__()\n        self.path0 = nn.Sequential(\n            BasicConv2d(1152, 128, kernel_size=1),\n            BasicConv2d(128, 160, kernel_size=(1, 7), padding=(0, 3)),\n            BasicConv2d(160, 192, kernel_size=(7, 1), padding=(3, 0)),\n        )\n        self.path1 = BasicConv2d(1152, 192, kernel_size=1)\n        self.conv2d = BasicConv2d(384, 1152, kernel_size=1, output_relu=False)\n\n    def forward(self, x):\n        x0 = self.path0(x)\n        x1 = self.path1(x)\n        out = torch.cat((x0, x1), 1)\n        out = self.conv2d(out)\n        return out\n\n\nclass BlockC(nn.Module):\n    """"""Inception-ResNet-C block.""""""\n\n    def __init__(self):\n        super(BlockC, self).__init__()\n        self.path0 = nn.Sequential(\n            BasicConv2d(2048, 192, kernel_size=1),\n            BasicConv2d(192, 224, kernel_size=(1, 3), padding=(0, 1)),\n            BasicConv2d(224, 256, kernel_size=(3, 1), padding=(1, 0)),\n        )\n        self.path1 = BasicConv2d(2048, 192, kernel_size=1)\n        self.conv2d = BasicConv2d(448, 2048, kernel_size=1, output_relu=False)\n\n    def forward(self, x):\n        x0 = self.path0(x)\n        x1 = self.path1(x)\n        out = torch.cat((x0, x1), 1)\n        out = self.conv2d(out)\n        return out\n\n\nclass ReductionA(nn.Module):\n    """"""A dimensionality reduction block that is placed after stage-a\n    Inception-ResNet blocks.\n    """"""\n\n    def __init__(self):\n        super(ReductionA, self).__init__()\n        self.path0 = nn.Sequential(\n            BasicConv2d(384, 256, kernel_size=1),\n            BasicConv2d(256, 256, kernel_size=3, padding=1),\n            BasicConv2d(256, 384, kernel_size=3, stride=2),\n        )\n        self.path1 = BasicConv2d(384, 384, kernel_size=3, stride=2)\n        self.path2 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.path0(x)\n        x1 = self.path1(x)\n        x2 = self.path2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        return out\n\n\nclass ReductionB(nn.Module):\n    """"""A dimensionality reduction block that is placed after stage-b\n    Inception-ResNet blocks.\n    """"""\n    def __init__(self):\n        super(ReductionB, self).__init__()\n        self.path0 = nn.Sequential(\n            BasicConv2d(1152, 256, kernel_size=1),\n            BasicConv2d(256, 256, kernel_size=3, padding=1),\n            BasicConv2d(256, 256, kernel_size=3, stride=2),\n        )\n        self.path1 = nn.Sequential(\n            BasicConv2d(1152, 256, kernel_size=1),\n            BasicConv2d(256, 256, kernel_size=3, stride=2),\n        )\n        self.path2 = nn.Sequential(\n            BasicConv2d(1152, 256, kernel_size=1),\n            BasicConv2d(256, 384, kernel_size=3, stride=2),\n        )\n        self.path3 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.path0(x)\n        x1 = self.path1(x)\n        x2 = self.path2(x)\n        x3 = self.path3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass InceptionResNetBPoly(nn.Module):\n    """"""Base class for constructing poly-N Inception-ResNet-B modules.\n    When `num_blocks` is equal to 1, a module will have only a first-order path\n    and will be equal to a standard Inception-ResNet-B block.\n    When `num_blocks` is equal to 2, a module will have first-order and\n    second-order paths and will be called Inception-ResNet-B poly-2 module.\n    Increasing value of the `num_blocks` parameter will produce a higher order\n    Inception-ResNet-B poly-N modules.\n    """"""\n\n    def __init__(self, scale, num_blocks):\n        super(InceptionResNetBPoly, self).__init__()\n        assert num_blocks >= 1, \'num_blocks should be greater or equal to 1\'\n        self.scale = scale\n        self.num_blocks = num_blocks\n        self.path0_1x1 = PolyConv2d(1152, 128, kernel_size=1,\n                                    num_blocks=self.num_blocks)\n        self.path0_1x7 = PolyConv2d(128, 160, kernel_size=(1, 7),\n                                    num_blocks=self.num_blocks, padding=(0, 3))\n        self.path0_7x1 = PolyConv2d(160, 192, kernel_size=(7, 1),\n                                    num_blocks=self.num_blocks, padding=(3, 0))\n        self.path1 = PolyConv2d(1152, 192, kernel_size=1,\n                                num_blocks=self.num_blocks)\n        # conv2d blocks are not shared between Inception-ResNet-B blocks\n        self.conv2d_blocks = nn.ModuleList([\n            BasicConv2d(384, 1152, kernel_size=1, output_relu=False)\n            for _ in range(self.num_blocks)\n        ])\n        self.relu = nn.ReLU()\n\n    def forward_block(self, x, block_index):\n        x0 = self.path0_1x1(x, block_index)\n        x0 = self.path0_1x7(x0, block_index)\n        x0 = self.path0_7x1(x0, block_index)\n        x1 = self.path1(x, block_index)\n        out = torch.cat((x0, x1), 1)\n        conv2d_block = self.conv2d_blocks[block_index]\n        out = conv2d_block(out)\n        return out\n\n    def forward(self, x):\n        out = x\n        for block_index in range(self.num_blocks):\n            x = self.forward_block(x, block_index)\n            out = out + x * self.scale\n            x = self.relu(x)\n        out = self.relu(out)\n        return out\n\n\nclass InceptionResNetCPoly(nn.Module):\n    """"""Base class for constructing poly-N Inception-ResNet-C modules.\n    When `num_blocks` is equal to 1, a module will have only a first-order path\n    and will be equal to a standard Inception-ResNet-C block.\n    When `num_blocks` is equal to 2, a module will have first-order and\n    second-order paths and will be called Inception-ResNet-C poly-2 module.\n    Increasing value of the `num_blocks` parameter will produce a higher order\n    Inception-ResNet-C poly-N modules.\n    """"""\n\n    def __init__(self, scale, num_blocks):\n        super(InceptionResNetCPoly, self).__init__()\n        assert num_blocks >= 1, \'num_blocks should be greater or equal to 1\'\n        self.scale = scale\n        self.num_blocks = num_blocks\n        self.path0_1x1 = PolyConv2d(2048, 192, kernel_size=1,\n                                    num_blocks=self.num_blocks)\n        self.path0_1x3 = PolyConv2d(192, 224, kernel_size=(1, 3),\n                                    num_blocks=self.num_blocks, padding=(0, 1))\n        self.path0_3x1 = PolyConv2d(224, 256, kernel_size=(3, 1),\n                                    num_blocks=self.num_blocks, padding=(1, 0))\n        self.path1 = PolyConv2d(2048, 192, kernel_size=1,\n                                num_blocks=self.num_blocks)\n        # conv2d blocks are not shared between Inception-ResNet-C blocks\n        self.conv2d_blocks = nn.ModuleList([\n            BasicConv2d(448, 2048, kernel_size=1, output_relu=False)\n            for _ in range(self.num_blocks)\n        ])\n        self.relu = nn.ReLU()\n\n    def forward_block(self, x, block_index):\n        x0 = self.path0_1x1(x, block_index)\n        x0 = self.path0_1x3(x0, block_index)\n        x0 = self.path0_3x1(x0, block_index)\n        x1 = self.path1(x, block_index)\n        out = torch.cat((x0, x1), 1)\n        conv2d_block = self.conv2d_blocks[block_index]\n        out = conv2d_block(out)\n        return out\n\n    def forward(self, x):\n        out = x\n        for block_index in range(self.num_blocks):\n            x = self.forward_block(x, block_index)\n            out = out + x * self.scale\n            x = self.relu(x)\n        out = self.relu(out)\n        return out\n\n\nclass MultiWay(nn.Module):\n    """"""Base class for constructing N-way modules (2-way, 3-way, and so on).""""""\n\n    def __init__(self, scale, block_cls, num_blocks):\n        super(MultiWay, self).__init__()\n        assert num_blocks >= 1, \'num_blocks should be greater or equal to 1\'\n        self.scale = scale\n        self.blocks = nn.ModuleList([block_cls() for _ in range(num_blocks)])\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        out = x\n        for block in self.blocks:\n            out = out + block(x) * self.scale\n        out = self.relu(out)\n        return out\n\n\n# Some helper classes to simplify the construction of PolyNet\n\nclass InceptionResNetA2Way(MultiWay):\n\n    def __init__(self, scale):\n        super(InceptionResNetA2Way, self).__init__(scale, block_cls=BlockA,\n                                                   num_blocks=2)\n\n\nclass InceptionResNetB2Way(MultiWay):\n\n    def __init__(self, scale):\n        super(InceptionResNetB2Way, self).__init__(scale, block_cls=BlockB,\n                                                   num_blocks=2)\n\n\nclass InceptionResNetC2Way(MultiWay):\n\n    def __init__(self, scale):\n        super(InceptionResNetC2Way, self).__init__(scale, block_cls=BlockC,\n                                                   num_blocks=2)\n\n\nclass InceptionResNetBPoly3(InceptionResNetBPoly):\n\n    def __init__(self, scale):\n        super(InceptionResNetBPoly3, self).__init__(scale, num_blocks=3)\n\n\nclass InceptionResNetCPoly3(InceptionResNetCPoly):\n\n    def __init__(self, scale):\n        super(InceptionResNetCPoly3, self).__init__(scale, num_blocks=3)\n\n\nclass PolyNet(nn.Module):\n\n    def __init__(self, num_classes=1000):\n        super(PolyNet, self).__init__()\n        self.stem = Stem()\n        self.stage_a = nn.Sequential(\n            InceptionResNetA2Way(scale=1),\n            InceptionResNetA2Way(scale=0.992308),\n            InceptionResNetA2Way(scale=0.984615),\n            InceptionResNetA2Way(scale=0.976923),\n            InceptionResNetA2Way(scale=0.969231),\n            InceptionResNetA2Way(scale=0.961538),\n            InceptionResNetA2Way(scale=0.953846),\n            InceptionResNetA2Way(scale=0.946154),\n            InceptionResNetA2Way(scale=0.938462),\n            InceptionResNetA2Way(scale=0.930769),\n        )\n        self.reduction_a = ReductionA()\n        self.stage_b = nn.Sequential(\n            InceptionResNetBPoly3(scale=0.923077),\n            InceptionResNetB2Way(scale=0.915385),\n            InceptionResNetBPoly3(scale=0.907692),\n            InceptionResNetB2Way(scale=0.9),\n            InceptionResNetBPoly3(scale=0.892308),\n            InceptionResNetB2Way(scale=0.884615),\n            InceptionResNetBPoly3(scale=0.876923),\n            InceptionResNetB2Way(scale=0.869231),\n            InceptionResNetBPoly3(scale=0.861538),\n            InceptionResNetB2Way(scale=0.853846),\n            InceptionResNetBPoly3(scale=0.846154),\n            InceptionResNetB2Way(scale=0.838462),\n            InceptionResNetBPoly3(scale=0.830769),\n            InceptionResNetB2Way(scale=0.823077),\n            InceptionResNetBPoly3(scale=0.815385),\n            InceptionResNetB2Way(scale=0.807692),\n            InceptionResNetBPoly3(scale=0.8),\n            InceptionResNetB2Way(scale=0.792308),\n            InceptionResNetBPoly3(scale=0.784615),\n            InceptionResNetB2Way(scale=0.776923),\n        )\n        self.reduction_b = ReductionB()\n        self.stage_c = nn.Sequential(\n            InceptionResNetCPoly3(scale=0.769231),\n            InceptionResNetC2Way(scale=0.761538),\n            InceptionResNetCPoly3(scale=0.753846),\n            InceptionResNetC2Way(scale=0.746154),\n            InceptionResNetCPoly3(scale=0.738462),\n            InceptionResNetC2Way(scale=0.730769),\n            InceptionResNetCPoly3(scale=0.723077),\n            InceptionResNetC2Way(scale=0.715385),\n            InceptionResNetCPoly3(scale=0.707692),\n            InceptionResNetC2Way(scale=0.7),\n        )\n        self.avg_pool = nn.AvgPool2d(9, stride=1)\n        self.dropout = nn.Dropout(0.2)\n        self.last_linear = nn.Linear(2048, num_classes)\n\n    def features(self, x):\n        x = self.stem(x)\n        x = self.stage_a(x)\n        x = self.reduction_a(x)\n        x = self.stage_b(x)\n        x = self.reduction_b(x)\n        x = self.stage_c(x)\n        return x\n\n    def logits(self, x):\n        x = self.avg_pool(x)\n        x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.logits(x)\n        return x\n\n\ndef polynet(pretrained=\'imagenet\'):\n    """"""Pretrained PolyNet model""""""\n    model = PolyNet(num_classes=1000)\n\n    if pretrained == \'imagenet\':\n        settings = pretrained_settings[\'polynet\'][pretrained]\n        model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n\n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n    return model\n'"
pywick/models/classification/pyramid_resnet.py,7,"b'""""""\nImplementation from paper: `Deep Pyramidal Residual Networks <https://arxiv.org/abs/1610.02915>`_.\nNot pretrained.\n""""""\n\nimport os\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n__all__ = [\'PyResNet18\', \'PyResNet34\', \'PyResNet\']\n\ndef make_conv_bn_relu(in_channels, out_channels, kernel_size=3, stride=1, padding=1, groups=1):\n    return [\n        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, bias=False),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(inplace=True),\n    ]\n\ndef make_linear_bn_relu(in_channels, out_channels):\n    return [\n        nn.Linear(in_channels, out_channels, bias=False),\n        nn.BatchNorm1d(out_channels),\n        nn.ReLU(inplace=True),\n    ]\n\n\ndef make_max_flat(out):\n    flat = F.adaptive_max_pool2d(out,output_size=1)  ##nn.AdaptiveMaxPool2d(1)(out)\n    flat = flat.view(flat.size(0), -1)\n    return flat\n\n\ndef make_avg_flat(out):\n    flat = F.adaptive_avg_pool2d(out,output_size=1)\n    flat = flat.view(flat.size(0), -1)\n    return flat\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1   = nn.BatchNorm2d(planes)\n        self.relu  = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2   = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\n\n\n\nclass PyResNet(nn.Module):\n\n    def __init__(self, block, layers, in_shape=(3,256,256), num_classes=17):\n        self.inplanes = 64\n\n        super(PyResNet, self).__init__()\n        in_channels, height, width = in_shape\n\n        # self.conv0 = nn.Sequential(\n        #     *make_conv_bn_relu(in_channels, 64, kernel_size=7, stride=2, padding=3, groups=1)\n        # )\n        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1  = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.layer1 = self.make_layer(block, 64, layers[0])\n        self.layer2 = self.make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self.make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self.make_layer(block, 512, layers[3], stride=2)\n\n        self.fc2 = nn.Sequential(\n             *make_linear_bn_relu(128 * block.expansion, 512),\n              nn.Linear(512, num_classes),\n        )\n        self.fc3 = nn.Sequential(\n             *make_linear_bn_relu(256 * block.expansion, 512),\n              nn.Linear(512, num_classes),\n        )\n        self.fc4 = nn.Sequential(\n             *make_linear_bn_relu(512 * block.expansion, 512),\n              nn.Linear(512, num_classes),\n        )\n\n        # self.fc = nn.Sequential(\n        #     *make_linear_bn_relu((128+256+512) * block.expansion, 1024),\n        #     nn.Linear(1024, num_classes)\n        # )\n        #\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        #x = self.conv0(x)\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n\n        x = self.layer1(x) # 64, 64x64\n\n        x = self.layer2(x) #128, 32x32\n        flat2 = make_max_flat(x)  ##make_avg_flat\n\n        x = self.layer3(x) #256, 16x16\n        flat3 = make_max_flat(x)\n\n        x = self.layer4(x) #512,  8x8\n        flat4 = make_max_flat(x)\n\n        # x = torch.cat([flat2,flat3,flat4,],1)\n        # x = self.fc(x)\n        x = self.fc2(flat2) + self.fc3(flat3) + self.fc4(flat4)\n\n\n        logit = x\n        prob  = torch.sigmoid(logit)\n        return logit, prob\n\n\ndef PyResNet18(pretrained=None, **kwargs):\n    """"""Not Pretrained""""""\n    if pretrained:\n        raise NotImplementedError()\n    model = PyResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    return model\n\n\ndef PyResNet34(pretrained=None, **kwargs):\n    """"""Not Pretrained""""""\n    if pretrained:\n        raise NotImplementedError()\n    model = PyResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    return model\n\n\n\n\n########################################################################################\nif __name__ == \'__main__\':\n    print( \'%s: calling main function ... \' % os.path.basename(__file__))\n\n    # https://discuss.pytorch.org/t/print-autograd-graph/692/8\n    batch_size  = 1\n    num_classes = 17\n    C,H,W = 3,256,256\n\n    inputs = torch.randn(batch_size,C,H,W)\n    labels = torch.randn(batch_size,num_classes)\n    in_shape = inputs.size()[1:]\n\n    if 1:\n        net = PyResNet34(in_shape=in_shape, num_classes=num_classes).cuda().train()\n\n        x = inputs\n        logits, probs = net.forward(x.cuda())\n\n        loss = nn.MultiLabelSoftMarginLoss()(logits, labels.cuda())\n        loss.backward()\n\n        print(type(net))\n        print(net)\n\n        print(\'probs\')\n        print(probs)\n\n        #input(\'Press ENTER to continue.\')\n\n'"
pywick/models/classification/resnet_preact.py,4,"b'# Source: https://github.com/hysts/pytorch_resnet_preact (License: MIT)\n\n""""""\n`Preact_Resnet models <https://github.com/hysts/pytorch_resnet_preact>`_. Not pretrained.\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict\n\n__all__ = [\'PreactResnet110\', \'PreactResnet164_bottleneck\']\n\ndef PreactResnet110(num_classes):\n    model_config = OrderedDict([\n            (\'arch\', \'resnet_preact\'),\n            (\'block_type\', \'basic\'),\n            (\'depth\', 110),\n            (\'base_channels\', 16),\n            (\'remove_first_relu\', True),\n            (\'add_last_bn\', True),\n            (\'preact_stage\', [True, True, True]),\n            (\'input_shape\', (1, 3, 32, 32)),\n            (\'n_classes\', num_classes)\n    ])\n    return Network(model_config)\n\n\ndef PreactResnet164_bottleneck(num_classes):\n    model_config = OrderedDict([\n            (\'arch\', \'resnet_preact\'),\n            (\'block_type\', \'bottleneck\'),\n            (\'depth\', 164),\n            (\'base_channels\', 16),\n            (\'remove_first_relu\', True),\n            (\'add_last_bn\', True),\n            (\'preact_stage\', [True, True, True]),\n            (\'input_shape\', (1, 3, 32, 32)),\n            (\'n_classes\', num_classes)\n    ])\n    return Network(model_config)\n\n\ndef initialize_weights(module):\n    if isinstance(module, nn.Conv2d):\n        nn.init.kaiming_normal_(module.weight.data, mode=\'fan_out\')\n    elif isinstance(module, nn.BatchNorm2d):\n        module.weight.data.fill_(1)\n        module.bias.data.zero_()\n    elif isinstance(module, nn.Linear):\n        module.bias.data.zero_()\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 stride,\n                 remove_first_relu,\n                 add_last_bn,\n                 preact=False):\n        super(BasicBlock, self).__init__()\n\n        self._remove_first_relu = remove_first_relu\n        self._add_last_bn = add_last_bn\n        self._preact = preact\n\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        self.conv1 = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=stride,  # downsample with first conv\n            padding=1,\n            bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(\n            out_channels,\n            out_channels,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias=False)\n\n        if add_last_bn:\n            self.bn3 = nn.BatchNorm2d(out_channels)\n\n        self.shortcut = nn.Sequential()\n        if in_channels != out_channels:\n            self.shortcut.add_module(\n                \'conv\',\n                nn.Conv2d(\n                    in_channels,\n                    out_channels,\n                    kernel_size=1,\n                    stride=stride,  # downsample\n                    padding=0,\n                    bias=False))\n\n    def forward(self, x):\n        if self._preact:\n            x = F.relu(\n                self.bn1(x), inplace=True)  # shortcut after preactivation\n            y = self.conv1(x)\n        else:\n            # preactivation only for residual path\n            y = self.bn1(x)\n            if not self._remove_first_relu:\n                y = F.relu(y, inplace=True)\n            y = self.conv1(y)\n\n        y = F.relu(self.bn2(y), inplace=True)\n        y = self.conv2(y)\n\n        if self._add_last_bn:\n            y = self.bn3(y)\n\n        y += self.shortcut(x)\n        return y\n\n\nclass BottleneckBlock(nn.Module):\n    expansion = 4\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 stride,\n                 remove_first_relu,\n                 add_last_bn,\n                 preact=False):\n        super(BottleneckBlock, self).__init__()\n\n        self._remove_first_relu = remove_first_relu\n        self._add_last_bn = add_last_bn\n        self._preact = preact\n\n        bottleneck_channels = out_channels // self.expansion\n\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        self.conv1 = nn.Conv2d(\n            in_channels,\n            bottleneck_channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=False)\n        self.bn2 = nn.BatchNorm2d(bottleneck_channels)\n        self.conv2 = nn.Conv2d(\n            bottleneck_channels,\n            bottleneck_channels,\n            kernel_size=3,\n            stride=stride,  # downsample with 3x3 conv\n            padding=1,\n            bias=False)\n        self.bn3 = nn.BatchNorm2d(bottleneck_channels)\n        self.conv3 = nn.Conv2d(\n            bottleneck_channels,\n            out_channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=False)\n\n        if add_last_bn:\n            self.bn4 = nn.BatchNorm2d(out_channels)\n\n        self.shortcut = nn.Sequential()  # identity\n        if in_channels != out_channels:\n            self.shortcut.add_module(\n                \'conv\',\n                nn.Conv2d(\n                    in_channels,\n                    out_channels,\n                    kernel_size=1,\n                    stride=stride,  # downsample\n                    padding=0,\n                    bias=False))\n\n    def forward(self, x):\n        if self._preact:\n            x = F.relu(\n                self.bn1(x), inplace=True)  # shortcut after preactivation\n            y = self.conv1(x)\n        else:\n            # preactivation only for residual path\n            y = self.bn1(x)\n            if not self._remove_first_relu:\n                y = F.relu(y, inplace=True)\n            y = self.conv1(y)\n\n        y = F.relu(self.bn2(y), inplace=True)\n        y = self.conv2(y)\n        y = F.relu(self.bn3(y), inplace=True)\n        y = self.conv3(y)\n\n        if self._add_last_bn:\n            y = self.bn4(y)\n\n        y += self.shortcut(x)\n        return y\n\n\nclass Network(nn.Module):\n    def __init__(self, config):\n        super(Network, self).__init__()\n\n        input_shape = config[\'input_shape\']\n        n_classes = config[\'n_classes\']\n\n        base_channels = config[\'base_channels\']\n        self._remove_first_relu = config[\'remove_first_relu\']\n        self._add_last_bn = config[\'add_last_bn\']\n        block_type = config[\'block_type\']\n        depth = config[\'depth\']\n        preact_stage = config[\'preact_stage\']\n\n        assert block_type in [\'basic\', \'bottleneck\']\n        if block_type == \'basic\':\n            block = BasicBlock\n            n_blocks_per_stage = (depth - 2) // 6\n            assert n_blocks_per_stage * 6 + 2 == depth\n        else:\n            block = BottleneckBlock\n            n_blocks_per_stage = (depth - 2) // 9\n            assert n_blocks_per_stage * 9 + 2 == depth\n\n        n_channels = [\n            base_channels,\n            base_channels * 2 * block.expansion,\n            base_channels * 4 * block.expansion,\n        ]\n\n        self.conv = nn.Conv2d(\n            input_shape[1],\n            n_channels[0],\n            kernel_size=(3, 3),\n            stride=1,\n            padding=1,\n            bias=False)\n\n        self.stage1 = self._make_stage(\n            n_channels[0],\n            n_channels[0],\n            n_blocks_per_stage,\n            block,\n            stride=1,\n            preact=preact_stage[0])\n        self.stage2 = self._make_stage(\n            n_channels[0],\n            n_channels[1],\n            n_blocks_per_stage,\n            block,\n            stride=2,\n            preact=preact_stage[1])\n        self.stage3 = self._make_stage(\n            n_channels[1],\n            n_channels[2],\n            n_blocks_per_stage,\n            block,\n            stride=2,\n            preact=preact_stage[2])\n        self.bn = nn.BatchNorm2d(n_channels[2])\n\n        # compute conv feature size\n        with torch.no_grad():\n            self.feature_size = self._forward_conv(\n                torch.zeros(*input_shape)).view(-1).shape[0]\n\n        self.fc = nn.Linear(self.feature_size, n_classes)\n\n        # initialize weights\n        self.apply(initialize_weights)\n\n    def _make_stage(self, in_channels, out_channels, n_blocks, block, stride,\n                    preact):\n        stage = nn.Sequential()\n        for index in range(n_blocks):\n            block_name = \'block{}\'.format(index + 1)\n            if index == 0:\n                stage.add_module(\n                    block_name,\n                    block(\n                        in_channels,\n                        out_channels,\n                        stride=stride,\n                        remove_first_relu=self._remove_first_relu,\n                        add_last_bn=self._add_last_bn,\n                        preact=preact))\n            else:\n                stage.add_module(\n                    block_name,\n                    block(\n                        out_channels,\n                        out_channels,\n                        stride=1,\n                        remove_first_relu=self._remove_first_relu,\n                        add_last_bn=self._add_last_bn,\n                        preact=False))\n        return stage\n\n    def _forward_conv(self, x):\n        x = self.conv(x)\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = F.relu(\n            self.bn(x),\n            inplace=True)  # apply BN and ReLU before average pooling\n        x = F.adaptive_avg_pool2d(x, output_size=1)\n        return x\n\n    def forward(self, x):\n        x = self._forward_conv(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n'"
pywick/models/classification/resnet_swish.py,3,"b'""""""Resnet model combined with Swish activation function""""""\n\n# Source: https://github.com/tzing/resnet-swish\n\nimport torch.nn as nn\nimport torch\nimport math\n\n__all__ = [\'ResNet_swish\', \'ResNet18_swish\', \'ResNet34_swish\', \'ResNet50_swish\', \'ResNet101_swish\', \'ResNet152_swish\']\n\n\nclass Swish(nn.Module):\n\n    def __init__(self, inplace=False):\n        super().__init__()\n\n        self.inplace = True\n\n    def forward(self, x):\n        if self.inplace:\n            x.mul_(torch.sigmoid(x))\n            return x\n        else:\n            return x * torch.sigmoid(x)\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.act = Swish(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.act(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.act(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.act = Swish(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.act(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.act(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.act(out)\n\n        return out\n\n\nclass ResNet_swish(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000):\n        self.inplanes = 64\n        super(ResNet_swish, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.act = Swish(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(7)\n        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.act(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n\n        return x\n\n\ndef ResNet18_swish(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model. Not pretrained.""""""\n    model = ResNet_swish(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        raise NotImplementedError()\n    return model\n\n\ndef ResNet34_swish(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model. Not pretrained.""""""\n    model = ResNet_swish(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        raise NotImplementedError()\n    return model\n\n\ndef ResNet50_swish(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model. Not pretrained.""""""\n    model = ResNet_swish(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        raise NotImplementedError()\n    return model\n\n\ndef ResNet101_swish(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model. Not pretrained.""""""\n    model = ResNet_swish(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        raise NotImplementedError()\n    return model\n\n\ndef ResNet152_swish(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model. Not pretrained.""""""\n    model = ResNet_swish(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        raise NotImplementedError()\n    return model'"
pywick/models/classification/resnext.py,2,"b'# Source: https://raw.githubusercontent.com/Cadene/pretrained-models.pytorch/master/pretrainedmodels/models/resnext.py (License: BSD-3-Clause)\n# Pretrained: Yes\n\n""""""\nImplementation of paper: `Aggregated Residual Transformations for Deep Neural Networks <https://arxiv.org/abs/1611.05431>`_.\n""""""\n\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\nfrom .resnext_features import resnext50_32x4d_features\nfrom .resnext_features import resnext101_32x4d_features\nfrom .resnext_features import resnext101_64x4d_features\n\n__all__ = [\'ResNeXt50_32x4d\', \'resnext50_32x4d\',\n           \'ResNeXt101_32x4d\', \'resnext101_32x4d\',\n           \'ResNeXt101_64x4d\', \'resnext101_64x4d\']\n\npretrained_settings = {\n    \'resnext50_32x4d\': {\n        \'imagenet\': {\n            \'url\': \'https://github.com/barrh/pretrained-models.pytorch/releases/download/v0.7.4.1/resnext50_32x4d-b86d1c04b9.pt\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'resnext101_32x4d\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/resnext101_32x4d-29e315fa.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'resnext101_64x4d\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/resnext101_64x4d-e77a0586.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    }\n}\n\nclass ResNeXt50_32x4d(nn.Module):\n\n    def __init__(self, num_classes=1000):\n        super(ResNeXt50_32x4d, self).__init__()\n        self.num_classes = num_classes\n        self.features = resnext50_32x4d_features\n        self.avg_pool = nn.AvgPool2d((7, 7), (1, 1))\n        self.last_linear = nn.Linear(2048, num_classes)\n\n    def logits(self, input):\n        x = self.avg_pool(input)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\nclass ResNeXt101_32x4d(nn.Module):\n\n    def __init__(self, num_classes=1000):\n        super(ResNeXt101_32x4d, self).__init__()\n        self.num_classes = num_classes\n        self.features = resnext101_32x4d_features\n        self.avg_pool = nn.AvgPool2d((7, 7), (1, 1))\n        self.last_linear = nn.Linear(2048, num_classes)\n\n    def logits(self, input):\n        x = self.avg_pool(input)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n\nclass ResNeXt101_64x4d(nn.Module):\n\n    def __init__(self, num_classes=1000):\n        super(ResNeXt101_64x4d, self).__init__()\n        self.num_classes = num_classes\n        self.features = resnext101_64x4d_features\n        self.avg_pool = nn.AvgPool2d((7, 7), (1, 1))\n        self.last_linear = nn.Linear(2048, num_classes)\n\n    def logits(self, input):\n        x = self.avg_pool(input)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\ndef resnext50_32x4d(num_classes=1000, pretrained=\'imagenet\'):\n    """"""Pretrained Resnext50_32x4d model""""""\n    model = ResNeXt50_32x4d(num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'resnext50_32x4d\'][pretrained]\n        assert num_classes == settings[\'num_classes\'], \\\n            ""num_classes should be {}, but is {}"".format(settings[\'num_classes\'], num_classes)\n        model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n    return model\n\n\ndef resnext101_32x4d(pretrained=\'imagenet\'):\n    """"""Pretrained Resnext101_32x4d model""""""\n    model = ResNeXt101_32x4d(num_classes=1000)\n    if pretrained:\n        settings = pretrained_settings[\'resnext101_32x4d\'][pretrained]\n        model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n    return model\n\ndef resnext101_64x4d(pretrained=\'imagenet\'):\n    """"""Pretrained ResNeXt101_64x4d model""""""\n    model = ResNeXt101_64x4d(num_classes=1000)\n    if pretrained:\n        settings = pretrained_settings[\'resnext101_64x4d\'][pretrained]\n        model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n    return model'"
pywick/models/classification/senet.py,2,"b'# Source: https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/senet.py (License: BSD-3-Clause)\n# Pretrained: Yes\n\n""""""\nSENet implementation as described in: `Squeeze-and-Excitation Networks <https://arxiv.org/pdf/1709.01507.pdf>`_.\n""""""\n\nfrom collections import OrderedDict\n\nimport torch.nn as nn\nfrom torch.utils import model_zoo\n\n__all__ = [\'SENet\', \'senet154\', \'se_resnet50\', \'se_resnet101\', \'se_resnet152\',\n           \'se_resnext50_32x4d\', \'se_resnext101_32x4d\']\n\npretrained_settings = {\n    \'senet154\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/senet154-c7b49a05.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnet50\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet50-ce0d4300.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnet101\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet101-7e38fcc6.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnet152\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet152-d17c99b7.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnext50_32x4d\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext50_32x4d-a260b3a4.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnext101_32x4d\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext101_32x4d-3b2fe3d8.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n}\n\n\nclass SEModule(nn.Module):\n\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        # Is below same as self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))?\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1, padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1, padding=0)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        module_input = x\n        x = self.avg_pool(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return module_input * x\n\n\nclass Bottleneck(nn.Module):\n    """"""\n    Base class for bottlenecks that implements `forward()` method.\n    """"""\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = self.se_module(out) + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBottleneck(Bottleneck):\n    """"""\n    Bottleneck for SENet154.\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes * 2)\n        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3, stride=stride, padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes * 4)\n        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNetBottleneck(Bottleneck):\n    """"""\n    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n    (the latter is used in the torchvision implementation of ResNet).\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEResNetBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False, stride=stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNeXtBottleneck(Bottleneck):\n    """"""\n    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None, base_width=4):\n        super(SEResNeXtBottleneck, self).__init__()\n        width = int((planes * (float(base_width) / 64)) * groups)\n        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False, stride=1)\n        self.bn1 = nn.BatchNorm2d(width)\n        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride, padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(width)\n        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SENet(nn.Module):\n\n    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n                 downsample_padding=1, num_classes=1000):\n        """"""\n        Parameters\n        ----------\n        block (nn.Module): Bottleneck class.\n            - For SENet154: SEBottleneck\n            - For SE-ResNet models: SEResNetBottleneck\n            - For SE-ResNeXt models:  SEResNeXtBottleneck\n        layers (list of ints): Number of residual blocks for 4 layers of the\n            network (layer1...layer4).\n        groups (int): Number of groups for the 3x3 convolution in each\n            bottleneck block.\n            - For SENet154: 64\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models:  32\n        reduction (int): Reduction ratio for Squeeze-and-Excitation modules.\n            - For all models: 16\n        dropout_p (float or None): Drop probability for the Dropout layer.\n            If `None` the Dropout layer is not used.\n            - For SENet154: 0.2\n            - For SE-ResNet models: None\n            - For SE-ResNeXt models: None\n        inplanes (int):  Number of input channels for layer1.\n            - For SENet154: 128\n            - For SE-ResNet models: 64\n            - For SE-ResNeXt models: 64\n        input_3x3 (bool): If `True`, use three 3x3 convolutions instead of\n            a single 7x7 convolution in layer0.\n            - For SENet154: True\n            - For SE-ResNet models: False\n            - For SE-ResNeXt models: False\n        downsample_kernel_size (int): Kernel size for downsampling convolutions\n            in layer2, layer3 and layer4.\n            - For SENet154: 3\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models: 1\n        downsample_padding (int): Padding for downsampling convolutions in\n            layer2, layer3 and layer4.\n            - For SENet154: 1\n            - For SE-ResNet models: 0\n            - For SE-ResNeXt models: 0\n        num_classes (int): Number of outputs in `last_linear` layer.\n            - For all models: 1000\n        """"""\n        super(SENet, self).__init__()\n        self.inplanes = inplanes\n        if input_3x3:\n            layer0_modules = [\n                (\'conv1\', nn.Conv2d(3, 64, 3, stride=2, padding=1, bias=False)),\n                (\'bn1\', nn.BatchNorm2d(64)),\n                (\'relu1\', nn.ReLU(inplace=True)),\n                (\'conv2\', nn.Conv2d(64, 64, 3, stride=1, padding=1, bias=False)),\n                (\'bn2\', nn.BatchNorm2d(64)),\n                (\'relu2\', nn.ReLU(inplace=True)),\n                (\'conv3\', nn.Conv2d(64, inplanes, 3, stride=1, padding=1, bias=False)),\n                (\'bn3\', nn.BatchNorm2d(inplanes)),\n                (\'relu3\', nn.ReLU(inplace=True)),\n            ]\n        else:\n            layer0_modules = [\n                (\'conv1\', nn.Conv2d(3, inplanes, kernel_size=7, stride=2, padding=3, bias=False)),\n                (\'bn1\', nn.BatchNorm2d(inplanes)),\n                (\'relu1\', nn.ReLU(inplace=True)),\n            ]\n        # To preserve compatibility with Caffe weights `ceil_mode=True`\n        # is used instead of `padding=1`.\n        layer0_modules.append((\'pool\', nn.MaxPool2d(3, stride=2, ceil_mode=True)))\n        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n        self.layer1 = self._make_layer(\n            block,\n            planes=64,\n            blocks=layers[0],\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=1,\n            downsample_padding=0\n        )\n        self.layer2 = self._make_layer(\n            block,\n            planes=128,\n            blocks=layers[1],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer3 = self._make_layer(\n            block,\n            planes=256,\n            blocks=layers[2],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer4 = self._make_layer(\n            block,\n            planes=512,\n            blocks=layers[3],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1, downsample_kernel_size=1, downsample_padding=0):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=downsample_kernel_size, stride=stride, padding=downsample_padding, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n                            downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups, reduction))\n\n        return nn.Sequential(*layers)\n\n    def features(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def logits(self, x):\n        x = self.avg_pool(x)\n        if self.dropout is not None:\n            x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.logits(x)\n        return x\n\n\ndef initialize_pretrained_model(model, num_classes, settings):\n    assert num_classes == settings[\'num_classes\'], \'num_classes should be {}, but is {}\'.format(settings[\'num_classes\'], num_classes)\n    model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n    model.input_space = settings[\'input_space\']\n    model.input_size = settings[\'input_size\']\n    model.input_range = settings[\'input_range\']\n    model.mean = settings[\'mean\']\n    model.std = settings[\'std\']\n\n\ndef senet154(num_classes=1000, pretrained=\'imagenet\'):\n    """"""Pretrained SENet154 model""""""\n    model = SENet(SEBottleneck, [3, 8, 36, 3], groups=64, reduction=16, dropout_p=0.2, num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'senet154\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet50(num_classes=1000, pretrained=\'imagenet\'):\n    """"""Pretrained SEResNet50 model""""""\n    model = SENet(SEResNetBottleneck, [3, 4, 6, 3], groups=1, reduction=16, dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0, num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnet50\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet101(num_classes=1000, pretrained=\'imagenet\'):\n    """"""Pretrained SEResNet101 model""""""\n    model = SENet(SEResNetBottleneck, [3, 4, 23, 3], groups=1, reduction=16, dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0, num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnet101\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet152(num_classes=1000, pretrained=\'imagenet\'):\n    """"""Pretrained SEResNet152 model""""""\n    model = SENet(SEResNetBottleneck, [3, 8, 36, 3], groups=1, reduction=16, dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0, num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnet152\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnext50_32x4d(num_classes=1000, pretrained=\'imagenet\'):\n    """"""Pretrained SEResNext50 model""""""\n    model = SENet(SEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16, dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0, num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnext50_32x4d\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnext101_32x4d(num_classes=1000, pretrained=\'imagenet\'):\n    """"""Pretrained SEResNext101 model""""""\n    model = SENet(SEResNeXtBottleneck, [3, 4, 23, 3], groups=32, reduction=16, dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0, num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnext101_32x4d\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n'"
pywick/models/classification/wideresnet.py,4,"b'# Source: https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/wideresnet.py (License: BSD-3-Clause)\n\n""""""\nImplementation of WideResNet as described in: `Wide Residual Networks <https://arxiv.org/abs/1605.07146>`_.\n""""""\n\nfrom __future__ import print_function, division, absolute_import\nimport re\nimport os\nfrom os.path import expanduser\nimport hickle as hkl\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n__all__ = [\'WideResNet\', \'wideresnet50\']\n\nmodel_urls = {\n    \'wideresnet50\': \'https://s3.amazonaws.com/pytorch/h5models/wide-resnet-50-2-export.hkl\'\n}\n\ndef define_model(params):\n    def conv2d(input, params, base, stride=1, pad=0):\n        return F.conv2d(input, params[base + \'.weight\'],\n                        params[base + \'.bias\'], stride, pad)\n\n    def group(input, params, base, stride, n):\n        o = input\n        for i in range(0,n):\n            b_base = (\'%s.block%d.conv\') % (base, i)\n            x = o\n            o = conv2d(x, params, b_base + \'0\')\n            o = F.relu(o)\n            o = conv2d(o, params, b_base + \'1\', stride=i==0 and stride or 1, pad=1)\n            o = F.relu(o)\n            o = conv2d(o, params, b_base + \'2\')\n            if i == 0:\n                o += conv2d(x, params, b_base + \'_dim\', stride=stride)\n            else:\n                o += x\n            o = F.relu(o)\n        return o\n\n    # determine network size by parameters\n    blocks = [sum([re.match(\'group%d.block\\d+.conv0.weight\'%j, k) is not None\n                   for k in params.keys()]) for j in range(4)]\n\n    def f(input, params, pooling_classif=True):\n        o = F.conv2d(input, params[\'conv0.weight\'], params[\'conv0.bias\'], 2, 3)\n        o = F.relu(o)\n        o = F.max_pool2d(o, 3, 2, 1)\n        o_g0 = group(o, params, \'group0\', 1, blocks[0])\n        o_g1 = group(o_g0, params, \'group1\', 2, blocks[1])\n        o_g2 = group(o_g1, params, \'group2\', 2, blocks[2])\n        o_g3 = group(o_g2, params, \'group3\', 2, blocks[3])\n        if pooling_classif:\n            o = F.avg_pool2d(o_g3, 7, 1, 0)\n            o = o.view(o.size(0), -1)\n            o = F.linear(o, params[\'fc.weight\'], params[\'fc.bias\'])\n        return o\n\n    return f\n\n\nclass WideResNet(nn.Module):\n\n    def __init__(self, pooling, f, params):\n        super(WideResNet, self).__init__()\n        self.pooling = pooling\n        self.f = f\n        self.params = params\n\n    def forward(self, x):\n        x = self.f(x, self.params, self.pooling)\n        return x\n\n\ndef wideresnet50(pooling):\n    """"""Pretrained WideResnet50 model""""""\n    dir_models = os.path.join(expanduser(""~""), \'.torch/wideresnet\')\n    path_hkl = os.path.join(dir_models, \'wideresnet50.hkl\')\n    if os.path.isfile(path_hkl):\n        params = hkl.load(path_hkl)\n        # convert numpy arrays to torch Variables\n        for k,v in sorted(params.items()):\n            print(k, v.shape)\n            params[k] = Variable(torch.from_numpy(v), requires_grad=True)\n    else:\n        os.system(\'mkdir -p \' + dir_models)\n        os.system(\'wget {} -O {}\'.format(model_urls[\'wideresnet50\'], path_hkl))\n    f = define_model(params)\n    model = WideResNet(pooling, f, params)\n    return model\n\n\n'"
pywick/models/classification/xception1.py,3,"b'# Source: https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/xception.py (License: BSD-3-Clause)\n# Pretrained: Yes\n\n""""""\nPorted to pytorch thanks to [tstandley](https://github.com/tstandley/Xception-PyTorch)\n\n@author: tstandley\nAdapted by cadene\n\nCreates an Xception Model as defined in:\n\nFrancois Chollet\n`Xception: Deep Learning with Depthwise Separable Convolutions <https://arxiv.org/pdf/1610.02357.pdf>`_.\n""""""\n\n# This weights ported from the Keras implementation. Achieves the following performance on the validation set:\n#\n# Loss:0.9173 Prec@1:78.892 Prec@5:94.292\n#\n# REMEMBER to set your image size to 3x299x299 for both test and validation\n#\n# normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n#                                   std=[0.5, 0.5, 0.5])\n#\n# The resize parameter of the validation transform should be 333, and make sure to center crop at 299x299\n\nfrom __future__ import print_function, division, absolute_import\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\n\n__all__ = [\'Xception\', \'xception\']\n\npretrained_settings = {\n    \'xception\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/xception-43020ad28.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 299, 299],\n            \'input_range\': [0, 1],\n            \'mean\': [0.5, 0.5, 0.5],\n            \'std\': [0.5, 0.5, 0.5],\n            \'num_classes\': 1000,\n            \'scale\': 0.8975 # The resize parameter of the validation transform should be 333, and make sure to center crop at 299x299\n        }\n    }\n}\n\n\nclass SeparableConv2d(nn.Module):\n    def __init__(self,in_channels,out_channels,kernel_size=1,stride=1,padding=0,dilation=1,bias=False):\n        super(SeparableConv2d,self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels,in_channels,kernel_size,stride,padding,dilation,groups=in_channels,bias=bias)\n        self.pointwise = nn.Conv2d(in_channels,out_channels,1,1,0,1,1,bias=bias)\n\n    def forward(self,x):\n        x = self.conv1(x)\n        x = self.pointwise(x)\n        return x\n\n\nclass Block(nn.Module):\n    def __init__(self,in_filters,out_filters,reps,strides=1,start_with_relu=True,grow_first=True):\n        super(Block, self).__init__()\n\n        if out_filters != in_filters or strides!=1:\n            self.skip = nn.Conv2d(in_filters,out_filters,1,stride=strides, bias=False)\n            self.skipbn = nn.BatchNorm2d(out_filters)\n        else:\n            self.skip=None\n\n        rep=[]\n\n        filters=in_filters\n        if grow_first:\n            rep.append(nn.ReLU(inplace=True))\n            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n            rep.append(nn.BatchNorm2d(out_filters))\n            filters = out_filters\n\n        for i in range(reps-1):\n            rep.append(nn.ReLU(inplace=True))\n            rep.append(SeparableConv2d(filters,filters,3,stride=1,padding=1,bias=False))\n            rep.append(nn.BatchNorm2d(filters))\n\n        if not grow_first:\n            rep.append(nn.ReLU(inplace=True))\n            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n            rep.append(nn.BatchNorm2d(out_filters))\n\n        if not start_with_relu:\n            rep = rep[1:]\n        else:\n            rep[0] = nn.ReLU(inplace=False)\n\n        if strides != 1:\n            rep.append(nn.MaxPool2d(3,strides,1))\n        self.rep = nn.Sequential(*rep)\n\n    def forward(self,inp):\n        x = self.rep(inp)\n\n        if self.skip is not None:\n            skip = self.skip(inp)\n            skip = self.skipbn(skip)\n        else:\n            skip = inp\n\n        x+=skip\n        return x\n\n\nclass Xception(nn.Module):\n    def __init__(self, num_classes=1000):\n        """""" Constructor\n        Args:\n            num_classes: number of classes\n        """"""\n        super(Xception, self).__init__()\n        self.num_classes = num_classes\n\n        self.conv1 = nn.Conv2d(3, 32, 3,2, 0, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.relu1 = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(32,64,3,bias=False)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.relu2 = nn.ReLU(inplace=True)\n        #do relu here\n\n        self.block1=Block(64,128,2,2,start_with_relu=False,grow_first=True)\n        self.block2=Block(128,256,2,2,start_with_relu=True,grow_first=True)\n        self.block3=Block(256,728,2,2,start_with_relu=True,grow_first=True)\n\n        self.block4=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n        self.block5=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n        self.block6=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n        self.block7=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n\n        self.block8=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n        self.block9=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n        self.block10=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n        self.block11=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n\n        self.block12=Block(728,1024,2,2,start_with_relu=True,grow_first=False)\n\n        self.conv3 = SeparableConv2d(1024,1536,3,1,1)\n        self.bn3 = nn.BatchNorm2d(1536)\n        self.relu3 = nn.ReLU(inplace=True)\n\n        #do relu here\n        self.conv4 = SeparableConv2d(1536,2048,3,1,1)\n        self.bn4 = nn.BatchNorm2d(2048)\n\n        self.fc = nn.Linear(2048, num_classes)\n\n        # #------- init weights --------\n        # for m in self.modules():\n        #     if isinstance(m, nn.Conv2d):\n        #         n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        #         m.weight.data.normal_(0, math.sqrt(2. / n))\n        #     elif isinstance(m, nn.BatchNorm2d):\n        #         m.weight.data.fill_(1)\n        #         m.bias.data.zero_()\n        # #-----------------------------\n\n    def features(self, input):\n        x = self.conv1(input)\n        x = self.bn1(x)\n        x = self.relu1(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu2(x)\n\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.block4(x)\n        x = self.block5(x)\n        x = self.block6(x)\n        x = self.block7(x)\n        x = self.block8(x)\n        x = self.block9(x)\n        x = self.block10(x)\n        x = self.block11(x)\n        x = self.block12(x)\n\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.relu3(x)\n\n        x = self.conv4(x)\n        x = self.bn4(x)\n        return x\n\n    def logits(self, features):\n        x = nn.ReLU(inplace=True)(features)\n\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n\ndef xception(pretrained=\'imagenet\'):\n    """"""Pretrained Xception model.""""""\n    model = Xception(num_classes=1000)\n    if pretrained:\n        settings = pretrained_settings[\'xception\'][pretrained]\n        model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n\n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n\n    # TODO: ugly\n    model.last_linear = model.fc\n    del model.fc\n    return model\n'"
pywick/models/localization/__init__.py,0,b''
pywick/models/localization/fpn.py,2,"b""# Source: https://github.com/kuangliu/pytorch-fpn\n\n'''FPN in PyTorch.\n\nImplementation of `Feature Pyramid Networks for Object Detection <http://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf>`_.\n'''\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass FPN(nn.Module):\n    def __init__(self, block, num_blocks):\n        super(FPN, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n\n        # Bottom-up layers\n        self.layer1 = self._make_layer(block,  64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n\n        # Top layer\n        self.toplayer = nn.Conv2d(2048, 256, kernel_size=1, stride=1, padding=0)  # Reduce channels\n\n        # Smooth layers\n        self.smooth1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n        self.smooth2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n        self.smooth3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n\n        # Lateral layers\n        self.latlayer1 = nn.Conv2d(1024, 256, kernel_size=1, stride=1, padding=0)\n        self.latlayer2 = nn.Conv2d( 512, 256, kernel_size=1, stride=1, padding=0)\n        self.latlayer3 = nn.Conv2d( 256, 256, kernel_size=1, stride=1, padding=0)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def _upsample_add(self, x, y):\n        '''Upsample and add two feature maps.\n\n        Args:\n          x: (Tensor) top feature map to be upsampled.\n          y: (Tensor) lateral feature map.\n\n        Returns:\n          (Tensor) added feature map.\n\n        Note in PyTorch, when input size is odd, the upsampled feature map\n        with `F.interpolate(..., scale_factor=2, mode='nearest')`\n        maybe not equal to the lateral feature map size.\n\n        e.g.\n        original input size: [N,_,15,15] ->\n        conv2d feature map size: [N,_,8,8] ->\n        upsampled feature map size: [N,_,16,16]\n\n        So we choose bilinear upsample which supports arbitrary output sizes.\n        '''\n        _,_,H,W = y.size()\n        return F.interpolate(x, size=(H,W), mode='bilinear') + y\n\n    def forward(self, x):\n        # Bottom-up\n        c1 = F.relu(self.bn1(self.conv1(x)))\n        c1 = F.max_pool2d(c1, kernel_size=3, stride=2, padding=1)\n        c2 = self.layer1(c1)\n        c3 = self.layer2(c2)\n        c4 = self.layer3(c3)\n        c5 = self.layer4(c4)\n        # Top-down\n        p5 = self.toplayer(c5)\n        p4 = self._upsample_add(p5, self.latlayer1(c4))\n        p3 = self._upsample_add(p4, self.latlayer2(c3))\n        p2 = self._upsample_add(p3, self.latlayer3(c2))\n        # Smooth\n        p4 = self.smooth1(p4)\n        p3 = self.smooth2(p3)\n        p2 = self.smooth3(p2)\n        return p2 #, p3, p4, p5\n\n\ndef FPN101():\n    # return FPN(Bottleneck, [2,4,23,3])\n    return FPN(Bottleneck, [2,2,2,2])\n\n"""
pywick/models/localization/retina_fpn.py,3,"b""# Source: https://github.com/kuangliu/pytorch-fpn\n\n'''RetinaFPN in PyTorch.\n\nImplementation of `Focal Loss for Dense Object Detection <http://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf>`_.\n'''\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch import Tensor\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass RetinaFPN(nn.Module):\n    def __init__(self, block, num_blocks):\n        super(RetinaFPN, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n\n        # Bottom-up layers\n        self.layer2 = self._make_layer(block,  64, num_blocks[0], stride=1)\n        self.layer3 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer4 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer5 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.conv6 = nn.Conv2d(2048, 256, kernel_size=3, stride=2, padding=1)\n        self.conv7 = nn.Conv2d( 256, 256, kernel_size=3, stride=2, padding=1)\n\n        # Top layer\n        self.toplayer = nn.Conv2d(2048, 256, kernel_size=1, stride=1, padding=0)  # Reduce channels\n\n        # Smooth layers\n        self.smooth1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n        self.smooth2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n\n        # Lateral layers\n        self.latlayer1 = nn.Conv2d(1024, 256, kernel_size=1, stride=1, padding=0)\n        self.latlayer2 = nn.Conv2d( 512, 256, kernel_size=1, stride=1, padding=0)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def _upsample_add(self, x, y):\n        '''Upsample and add two feature maps.\n\n        Args:\n          x: (Tensor) top feature map to be upsampled.\n          y: (Tensor) lateral feature map.\n\n        Returns:\n          (Tensor) added feature map.\n\n        Note in PyTorch, when input size is odd, the upsampled feature map\n        with `F.interpolate(..., scale_factor=2, mode='nearest')`\n        maybe not equal to the lateral feature map size.\n\n        e.g.\n        original input size: [N,_,15,15] ->\n        conv2d feature map size: [N,_,8,8] ->\n        upsampled feature map size: [N,_,16,16]\n\n        So we choose bilinear upsample which supports arbitrary output sizes.\n        '''\n        _,_,H,W = y.size()\n        return F.interpolate(x, size=(H,W), mode='bilinear') + y\n\n    def forward(self, x):\n        # Bottom-up\n        c1 = F.relu(self.bn1(self.conv1(x)))\n        c1 = F.max_pool2d(c1, kernel_size=3, stride=2, padding=1)\n        c2 = self.layer2(c1)\n        c3 = self.layer3(c2)\n        c4 = self.layer4(c3)\n        c5 = self.layer5(c4)\n        p6 = self.conv6(c5)\n        p7 = self.conv7(F.relu(p6))\n        # Top-down\n        p5 = self.toplayer(c5)\n        p4 = self._upsample_add(p5, self.latlayer1(c4))\n        p3 = self._upsample_add(p4, self.latlayer2(c3))\n        # Smooth\n        p4 = self.smooth1(p4)\n        p3 = self.smooth2(p3)\n        return p3, p4, p5, p6, p7\n\n\ndef RetinaFPN101():\n    # return RetinaFPN(Bottleneck, [2,4,23,3])\n    return RetinaFPN(Bottleneck, [2,2,2,2])\n\n\ndef test():\n    net = RetinaFPN101()\n    fms = net(torch.randn(1,3,600,900))\n    for fm in fms:\n        print(fm.size())\n\ntest()\n"""
pywick/models/segmentation/__init__.py,0,"b'""""""\nBelow you will find all the latest image segmentation models. To get a list of specific model names that are available programmatically, call the ``pywick.models.model_utils.get_supported_models(...)`` method.\nTo load one of these models with your own number of classes you have two options:\n1. You can always load the model directly from the API. Most models allow you to customize *number of classes* as well as *pretrained* options.\n2. You can use the ``pywick.models.model_utils.get_model(...)`` method and pass the name of the model\nthat you want as a string. Note: Some models allow you to customize additional parameters. You can take a look at the ``pywick.models.model_utils.get_model(...)`` method\nor at the definition of the model to see what\'s possible. ``pywick.models.model_utils.get_model(...)`` takes in a ``**kwargs`` argument that you can populate with whatever parameters you\'d like\nto pass to the model you are creating.\n""""""\n\nfrom .bisenet import *\nfrom .carvana_unet import *\nfrom ..classification import resnext101_64x4d\nfrom .danet import *\nfrom .deeplab_v2_res import *\nfrom .deeplab_v3 import *\nfrom .deeplab_v3_plus import *\nfrom .denseaspp import *\nfrom .drn_seg import *\nfrom .duc_hdc import *\nfrom .dunet import *\nfrom .enet import *\nfrom .fcn8s import *\nfrom .fcn16s import *\nfrom .fcn32s import *\nfrom .frrn1 import *\nfrom .fusionnet import *\nfrom .gcnnets import *\nfrom .lexpsp import *\nfrom .mnas_linknets import *\nfrom .ocnet import *\nfrom .refinenet import *\nfrom .resnet_gcn import *\nfrom .seg_net import *\nfrom .testnets import *\nfrom .tiramisu import *\nfrom .u_net import *\nfrom .unet_dilated import *\nfrom .unet_res import *\nfrom .unet_stack import *'"
pywick/models/segmentation/bisenet.py,4,"b'# Source: https://github.com/Tramac/awesome-semantic-segmentation-pytorch/blob/master/core/models/bisenet.py (License: Apache 2.0)\n\n""""""\nImplementation of `BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation <https://arxiv.org/pdf/1808.00897>`_\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom pywick.models.segmentation.da_basenets.resnet import resnet18\n\n__all__ = [\'BiSeNet\', \'BiSeNet_Resnet18\']\n\n\nclass BiSeNet(nn.Module):\n    def __init__(self, num_classes, pretrained=True, backbone=\'resnet18\', aux=False, **kwargs):\n        super(BiSeNet, self).__init__()\n        self.aux = aux\n        self.spatial_path = SpatialPath(3, 128, **kwargs)\n        self.context_path = ContextPath(backbone=backbone, pretrained=pretrained, **kwargs)\n        self.ffm = FeatureFusion(256, 256, 4, **kwargs)\n        self.head = _BiSeHead(256, 64, num_classes, **kwargs)\n        if aux:\n            self.auxlayer1 = _BiSeHead(128, 256, num_classes, **kwargs)\n            self.auxlayer2 = _BiSeHead(128, 256, num_classes, **kwargs)\n\n        self.__setattr__(\'exclusive\',\n                         [\'spatial_path\', \'context_path\', \'ffm\', \'head\', \'auxlayer1\', \'auxlayer2\'] if aux else [\n                             \'spatial_path\', \'context_path\', \'ffm\', \'head\'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        spatial_out = self.spatial_path(x)\n        context_out = self.context_path(x)\n        fusion_out = self.ffm(spatial_out, context_out[-1])\n        outputs = []\n        x = self.head(fusion_out)\n        x = F.interpolate(x, size, mode=\'bilinear\', align_corners=True)\n        outputs.append(x)\n\n        if self.aux:\n            auxout1 = self.auxlayer1(context_out[0])\n            auxout1 = F.interpolate(auxout1, size, mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout1)\n            auxout2 = self.auxlayer2(context_out[1])\n            auxout2 = F.interpolate(auxout2, size, mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout2)\n            return tuple(outputs)\n\n        else:\n            return outputs[0]\n\n\nclass _BiSeHead(nn.Module):\n    def __init__(self, in_channels, inter_channels, nclass, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(_BiSeHead, self).__init__()\n        self.block = nn.Sequential(\n            _ConvBNReLU(in_channels, inter_channels, 3, 1, 1, norm_layer=norm_layer, **kwargs),\n            nn.Dropout(0.1),\n            nn.Conv2d(inter_channels, nclass, 1)\n        )\n\n    def forward(self, x):\n        x = self.block(x)\n        return x\n\n\nclass _ConvBNReLU(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, dilation=1,\n                 groups=1, norm_layer=nn.BatchNorm2d, bias=False, **kwargs):\n        super(_ConvBNReLU, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n        self.bn = norm_layer(out_channels)\n        self.relu = nn.ReLU(True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n\nclass SpatialPath(nn.Module):\n    """"""Spatial path""""""\n\n    def __init__(self, in_channels, out_channels, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(SpatialPath, self).__init__()\n        inter_channels = 64\n        self.conv7x7 = _ConvBNReLU(in_channels, inter_channels, 7, 2, 3, norm_layer=norm_layer, **kwargs)\n        self.conv3x3_1 = _ConvBNReLU(inter_channels, inter_channels, 3, 2, 1, norm_layer=norm_layer, **kwargs)\n        self.conv3x3_2 = _ConvBNReLU(inter_channels, inter_channels, 3, 2, 1, norm_layer=norm_layer, **kwargs)\n        self.conv1x1 = _ConvBNReLU(inter_channels, out_channels, 1, 1, 0, norm_layer=norm_layer, **kwargs)\n\n    def forward(self, x):\n        x = self.conv7x7(x)\n        x = self.conv3x3_1(x)\n        x = self.conv3x3_2(x)\n        x = self.conv1x1(x)\n\n        return x\n\n\nclass _GlobalAvgPooling(nn.Module):\n    def __init__(self, in_channels, out_channels, norm_layer, **kwargs):\n        super(_GlobalAvgPooling, self).__init__()\n        self.gap = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            norm_layer(out_channels),\n            nn.ReLU(True)\n        )\n\n    def forward(self, x):\n        size = x.size()[2:]\n        pool = self.gap(x)\n        out = F.interpolate(pool, size, mode=\'bilinear\', align_corners=True)\n        return out\n\n\nclass AttentionRefinmentModule(nn.Module):\n    def __init__(self, in_channels, out_channels, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(AttentionRefinmentModule, self).__init__()\n        self.conv3x3 = _ConvBNReLU(in_channels, out_channels, 3, 1, 1, norm_layer=norm_layer, **kwargs)\n        self.channel_attention = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            _ConvBNReLU(out_channels, out_channels, 1, 1, 0, norm_layer=norm_layer, **kwargs),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.conv3x3(x)\n        attention = self.channel_attention(x)\n        x = x * attention\n        return x\n\n\nclass ContextPath(nn.Module):\n    def __init__(self, pretrained=True, backbone=\'resnet18\', norm_layer=nn.BatchNorm2d, **kwargs):\n        super(ContextPath, self).__init__()\n        if backbone == \'resnet18\':\n            pretrained = resnet18(pretrained=pretrained, **kwargs)\n        else:\n            raise RuntimeError(\'unknown backbone: {}\'.format(backbone))\n        self.conv1 = pretrained.conv1\n        self.bn1 = pretrained.bn1\n        self.relu = pretrained.relu\n        self.maxpool = pretrained.maxpool\n        self.layer1 = pretrained.layer1\n        self.layer2 = pretrained.layer2\n        self.layer3 = pretrained.layer3\n        self.layer4 = pretrained.layer4\n\n        inter_channels = 128\n        self.global_context = _GlobalAvgPooling(512, inter_channels, norm_layer, **kwargs)\n\n        self.arms = nn.ModuleList(\n            [AttentionRefinmentModule(512, inter_channels, norm_layer, **kwargs),\n             AttentionRefinmentModule(256, inter_channels, norm_layer, **kwargs)]\n        )\n        self.refines = nn.ModuleList(\n            [_ConvBNReLU(inter_channels, inter_channels, 3, 1, 1, norm_layer=norm_layer, **kwargs),\n             _ConvBNReLU(inter_channels, inter_channels, 3, 1, 1, norm_layer=norm_layer, **kwargs)]\n        )\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)\n\n        context_blocks = []\n        context_blocks.append(x)\n        x = self.layer2(x)\n        context_blocks.append(x)\n        c3 = self.layer3(x)\n        context_blocks.append(c3)\n        c4 = self.layer4(c3)\n        context_blocks.append(c4)\n        context_blocks.reverse()\n\n        global_context = self.global_context(c4)\n        last_feature = global_context\n        context_outputs = []\n        for i, (feature, arm, refine) in enumerate(zip(context_blocks[:2], self.arms, self.refines)):\n            feature = arm(feature)\n            feature += last_feature\n            last_feature = F.interpolate(feature, size=context_blocks[i + 1].size()[2:],\n                                         mode=\'bilinear\', align_corners=True)\n            last_feature = refine(last_feature)\n            context_outputs.append(last_feature)\n\n        return context_outputs\n\n\nclass FeatureFusion(nn.Module):\n    def __init__(self, in_channels, out_channels, reduction=1, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(FeatureFusion, self).__init__()\n        self.conv1x1 = _ConvBNReLU(in_channels, out_channels, 1, 1, 0, norm_layer=norm_layer, **kwargs)\n        self.channel_attention = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            _ConvBNReLU(out_channels, out_channels // reduction, 1, 1, 0, norm_layer=norm_layer, **kwargs),\n            _ConvBNReLU(out_channels // reduction, out_channels, 1, 1, 0, norm_layer=norm_layer, **kwargs),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x1, x2):\n        fusion = torch.cat([x1, x2], dim=1)\n        out = self.conv1x1(fusion)\n        attention = self.channel_attention(out)\n        out = out + out * attention\n        return out\n\n\ndef get_bisenet(num_classes=1, backbone=\'resnet18\', pretrained=True, **kwargs):\n    model = BiSeNet(num_classes=num_classes, backbone=backbone, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef BiSeNet_Resnet18(num_classes=1, **kwargs):\n    return get_bisenet(num_classes=num_classes, backbone=\'resnet18\', **kwargs)\n\n\nif __name__ == \'__main__\':\n    img = torch.randn(2, 3, 224, 224)\n    model = BiSeNet(19, backbone=\'resnet18\')\n    print(model.exclusive)\n'"
pywick/models/segmentation/carvana_unet.py,9,"b'""""""\nImplementation of `U-net: Convolutional networks for biomedical image segmentation <https://arxiv.org/pdf/1505.04597>`_\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n__all__ = [\'UNet128\', \'UNet256\', \'UNet512\', \'UNet1024\']\n\nBN_EPS = 1e-4\n\n## ==== EXPERIMENTAL - ought to try ===== ##\n# Source: https://github.com/qbit-/unet/blob/master/model/unet_work.py\nclass Conv3BN(nn.Module):\n    """"""A module which applies the following actions:\n        - convolution with 3x3 kernel;\n        - batch normalization (if enabled);\n        - ELU.\n    Attributes:\n        in_ch: Number of input channels.\n        out_ch: Number of output channels.\n        bn: A boolean indicating if Batch Normalization is enabled or not.\n    """"""\n\n    def __init__(self, in_ch: int, out_ch: int, bn=True):\n        super(Conv3BN, self).__init__()\n        self.conv = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n        self.bn = nn.BatchNorm2d(out_ch) if bn else None\n        self.activation = nn.ELU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.bn is not None:\n            x = self.bn(x)\n        x = self.activation(x)\n        return x\n## == END == ##\n\nclass ConvBnRelu2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, dilation=1, stride=1, groups=1, is_bn=True, is_relu=True):\n        super(ConvBnRelu2d, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, stride=stride, dilation=dilation, groups=groups, bias=False)\n        self.bn   = nn.BatchNorm2d(out_channels, eps=BN_EPS)\n        self.relu = nn.ReLU(inplace=True)\n        if is_bn   is False: self.bn  =None\n        if is_relu is False: self.relu=None\n\n\n    def forward(self,x):\n        x = self.conv(x)\n        if self.bn   is not None: x = self.bn(x)\n        if self.relu is not None: x = self.relu(x)\n        return x\n\n\n    def merge_bn(self):\n        if self.bn == None: return\n\n        assert(self.conv.bias==None)\n        conv_weight     = self.conv.weight.data\n        bn_weight       = self.bn.weight.data\n        bn_bias         = self.bn.bias.data\n        bn_running_mean = self.bn.running_mean\n        bn_running_var  = self.bn.running_var\n        bn_eps          = self.bn.eps\n\n        #https://github.com/sanghoon/pva-faster-rcnn/issues/5\n        #https://github.com/sanghoon/pva-faster-rcnn/commit/39570aab8c6513f0e76e5ab5dba8dfbf63e9c68c\n\n        N,C,KH,KW = conv_weight.size()\n        std = 1/(torch.sqrt(bn_running_var+bn_eps))\n        std_bn_weight =(std*bn_weight).repeat(C*KH*KW,1).t().contiguous().view(N,C,KH,KW )\n        conv_weight_hat = std_bn_weight*conv_weight\n        conv_bias_hat   = (bn_bias - bn_weight*std*bn_running_mean)\n\n        self.bn   = None\n        self.conv = nn.Conv2d(in_channels=self.conv.in_channels, out_channels=self.conv.out_channels, kernel_size=self.conv.kernel_size,\n                              padding=self.conv.padding, stride=self.conv.stride, dilation=self.conv.dilation, groups=self.conv.groups,\n                              bias=True)\n        self.conv.weight.data = conv_weight_hat #fill in\n        self.conv.bias.data   = conv_bias_hat\n\n\n\nclass ConvResidual (nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(ConvResidual, self).__init__()\n\n        self.block = nn.Sequential(\n            ConvBnRelu2d(in_channels,  out_channels, kernel_size=3, padding=1,  stride=1 ),\n            ConvBnRelu2d(out_channels, out_channels, kernel_size=3, padding=1,  stride=1, is_relu=False),\n        )\n        self.shortcut = None\n        if in_channels!=out_channels or stride!=1:\n           self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0, stride=stride,  bias=True)\n\n    def forward(self, x):\n        r = x if self.shortcut is None else self.shortcut(x)\n        x = self.block(x)\n        x = F.relu(x+r, inplace=True)\n        return x\n\n\n\n## -----------------------------------------------------------------------------------------------------------\n\n## origainl 3x3 stack filters used in UNet\nclass StackEncoder (nn.Module):\n    def __init__(self, x_channels, y_channels, kernel_size=3):\n        super(StackEncoder, self).__init__()\n        padding=(kernel_size-1)//2\n        self.encode = nn.Sequential(\n            ConvBnRelu2d(x_channels, y_channels, kernel_size=kernel_size, padding=padding, dilation=1, stride=1, groups=1),\n            ConvBnRelu2d(y_channels, y_channels, kernel_size=kernel_size, padding=padding, dilation=1, stride=1, groups=1),\n        )\n\n    def forward(self,x):\n        y = self.encode(x)\n        y_small = F.max_pool2d(y, kernel_size=2, stride=2)\n        return y, y_small\n\n\nclass StackDecoder (nn.Module):\n    def __init__(self, x_big_channels, x_channels, y_channels, kernel_size=3):\n        super(StackDecoder, self).__init__()\n        padding=(kernel_size-1)//2\n\n        self.decode = nn.Sequential(\n            ConvBnRelu2d(x_big_channels+x_channels, y_channels, kernel_size=kernel_size, padding=padding, dilation=1, stride=1, groups=1),\n            ConvBnRelu2d(y_channels, y_channels, kernel_size=kernel_size, padding=padding, dilation=1, stride=1, groups=1),\n            ConvBnRelu2d(y_channels, y_channels, kernel_size=kernel_size, padding=padding, dilation=1, stride=1, groups=1),\n        )\n\n    def forward(self, x_big, x):\n        N,C,H,W = x_big.size()\n        y = F.interpolate(x, size=(H,W),mode=\'bilinear\')\n        y = torch.cat([y,x_big],1)\n        y = self.decode(y)\n        return  y\n##---------------------------------------------------------------\n\n\n## origainl 3x3 stack filters used in UNet\nclass ResStackEncoder (nn.Module):\n    def __init__(self, x_channels, y_channels):\n        super(ResStackEncoder, self).__init__()\n        self.encode = ConvResidual(x_channels, y_channels)\n\n    def forward(self,x):\n        y = self.encode(x)\n        y_small = F.max_pool2d(y, kernel_size=2, stride=2)\n        return y, y_small\n\n\nclass ResStackDecoder (nn.Module):\n    def __init__(self, x_big_channels, x_channels, y_channels, kernel_size=3):\n        super(ResStackDecoder, self).__init__()\n        padding=(kernel_size-1)//2\n\n        self.decode = nn.Sequential(\n            ConvBnRelu2d(x_big_channels+x_channels, y_channels, kernel_size=kernel_size, padding=padding, dilation=1, stride=1, groups=1),\n            ConvResidual(y_channels, y_channels)\n        )\n\n    def forward(self, x_big, x):\n        N,C,H,W = x_big.size()\n        y = F.interpolate(x, size=(H,W), mode=\'bilinear\')\n        #y = F.interpolate(x, scale_factor=2,mode=\'bilinear\')\n        y = torch.cat([y,x_big],1)\n        y = self.decode(y)\n        return  y\n\n\n##---------------------------------------------------------------\n\n\n# baseline 128x128, 256x256, 512x512, 1024x1024 for experiments -----------------------------------------------\n\n# 1024x1024\nclass UNet1024 (nn.Module):\n    def __init__(self, in_shape=(3, 1024, 1024), **kwargs):\n        super(UNet1024, self).__init__()\n        C,H,W = in_shape\n        #assert(C==3)\n\n        #1024\n        self.down1 = StackEncoder(  C,   24, kernel_size=3)   #512\n        self.down2 = StackEncoder( 24,   64, kernel_size=3)   #256\n        self.down3 = StackEncoder( 64,  128, kernel_size=3)   #128\n        self.down4 = StackEncoder(128,  256, kernel_size=3)   # 64\n        self.down5 = StackEncoder(256,  512, kernel_size=3)   # 32\n        self.down6 = StackEncoder(512,  768, kernel_size=3)   # 16\n\n        self.center = nn.Sequential(\n            ConvBnRelu2d(768, 768, kernel_size=3, padding=1, stride=1 ),\n        )\n\n        # 8\n        # x_big_channels, x_channels, y_channels\n        self.up6 = StackDecoder(768,  768, 512, kernel_size=3)  # 16\n        self.up5 = StackDecoder( 512, 512, 256, kernel_size=3)  # 32\n        self.up4 = StackDecoder( 256, 256, 128, kernel_size=3)  # 64\n        self.up3 = StackDecoder( 128, 128,  64, kernel_size=3)  #128\n        self.up2 = StackDecoder(  64,  64,  24, kernel_size=3)  #256\n        self.up1 = StackDecoder(  24,  24,  24, kernel_size=3)  #512\n        self.classify = nn.Conv2d(24, 1, kernel_size=1, padding=0, stride=1, bias=True)\n\n\n    def forward(self, x):\n\n        out = x                       #;print(\'x    \',x.size())\n                                      #\n        down1,out = self.down1(out)  ##;print(\'down1\',down1.size())  #256\n        down2,out = self.down2(out)   #;print(\'down2\',down2.size())  #128\n        down3,out = self.down3(out)   #;print(\'down3\',down3.size())  #64\n        down4,out = self.down4(out)   #;print(\'down4\',down4.size())  #32\n        down5,out = self.down5(out)   #;print(\'down5\',down5.size())  #16\n        down6,out = self.down6(out)   #;print(\'down6\',down6.size())  #8\n        pass                          #;print(\'out  \',out.size())\n\n        out = self.center(out)\n        out = self.up6(down6, out)\n        out = self.up5(down5, out)\n        out = self.up4(down4, out)\n        out = self.up3(down3, out)\n        out = self.up2(down2, out)\n        out = self.up1(down1, out)\n        #1024\n\n        out = self.classify(out)\n        # out = torch.squeeze(out, dim=1)\n        return out\n\n\n# 512x512\nclass UNet512 (nn.Module):\n    def __init__(self, in_shape=(3, 512, 512), **kwargs):\n        super(UNet512, self).__init__()\n        C,H,W = in_shape\n        #assert(C==3)\n\n        #1024\n        self.down2 = StackEncoder(  C,   64, kernel_size=3)   #256\n        self.down3 = StackEncoder( 64,  128, kernel_size=3)   #128\n        self.down4 = StackEncoder(128,  256, kernel_size=3)   #64\n        self.down5 = StackEncoder(256,  512, kernel_size=3)   #32\n        self.down6 = StackEncoder(512, 1024, kernel_size=3)   #16\n\n        self.center = nn.Sequential(\n            ConvBnRelu2d(1024, 1024, kernel_size=3, padding=1, stride=1 ),\n            #ConvBnRelu2d(2048, 1024, kernel_size=3, padding=1, stride=1 ),\n        )\n\n        # 16\n        # x_big_channels, x_channels, y_channels\n        self.up6 = StackDecoder(1024,1024, 512, kernel_size=3)  # 16\n        self.up5 = StackDecoder( 512, 512, 256, kernel_size=3)  # 32\n        self.up4 = StackDecoder( 256, 256, 128, kernel_size=3)  # 64\n        self.up3 = StackDecoder( 128, 128,  64, kernel_size=3)  #128\n        self.up2 = StackDecoder(  64,  64,  32, kernel_size=3)  #256\n        self.classify = nn.Conv2d(32, 1, kernel_size=1, padding=0, stride=1, bias=True)\n\n\n    def forward(self, x):\n\n        out = x                       #;print(\'x    \',x.size())\n        down2,out = self.down2(out)   #;print(\'down2\',down2.size())\n        down3,out = self.down3(out)   #;print(\'down3\',down3.size())\n        down4,out = self.down4(out)   #;print(\'down4\',down4.size())\n        down5,out = self.down5(out)   #;print(\'down5\',down5.size())\n        down6,out = self.down6(out)   #;print(\'down6\',down6.size())\n        pass                          #;print(\'out  \',out.size())\n\n        out = self.center(out)\n        out = self.up6(down6, out)\n        out = self.up5(down5, out)\n        out = self.up4(down4, out)\n        out = self.up3(down3, out)\n        out = self.up2(down2, out)\n\n        out = self.classify(out)\n        # out = torch.squeeze(out, dim=1)\n        return out\n\n\n\n# 256x256\nclass UNet256 (nn.Module):\n    def __init__(self, in_shape=(3, 256, 256), **kwargs):\n        super(UNet256, self).__init__()\n        C,H,W = in_shape\n        #assert(C==3)\n\n        #256\n        self.down2 = StackEncoder(  C,   64, kernel_size=3)   #128\n        self.down3 = StackEncoder( 64,  128, kernel_size=3)   # 64\n        self.down4 = StackEncoder(128,  256, kernel_size=3)   # 32\n        self.down5 = StackEncoder(256,  512, kernel_size=3)   # 16\n        self.down6 = StackEncoder(512, 1024, kernel_size=3)   #  8\n\n        self.center = nn.Sequential(\n            #ConvBnRelu2d( 512, 1024, kernel_size=3, padding=1, stride=1 ),\n            ConvBnRelu2d(1024, 1024, kernel_size=3, padding=1, stride=1 ),\n        )\n\n        # 8\n        # x_big_channels, x_channels, y_channels\n        self.up6 = StackDecoder(1024,1024, 512, kernel_size=3)  # 16\n        self.up5 = StackDecoder( 512, 512, 256, kernel_size=3)  # 32\n        self.up4 = StackDecoder( 256, 256, 128, kernel_size=3)  # 64\n        self.up3 = StackDecoder( 128, 128,  64, kernel_size=3)  #128\n        self.up2 = StackDecoder(  64,  64,  32, kernel_size=3)  #256\n        self.classify = nn.Conv2d(32, 1, kernel_size=1, padding=0, stride=1, bias=True)\n\n\n    def forward(self, x):\n\n        out = x                       #;print(\'x    \',x.size())\n                                      #\n        down2,out = self.down2(out)   #;print(\'down2\',down2.size())  #128\n        down3,out = self.down3(out)   #;print(\'down3\',down3.size())  #64\n        down4,out = self.down4(out)   #;print(\'down4\',down4.size())  #32\n        down5,out = self.down5(out)   #;print(\'down5\',down5.size())  #16\n        down6,out = self.down6(out)   #;print(\'down6\',down6.size())  #8\n        pass                          #;print(\'out  \',out.size())\n\n        out = self.center(out)\n        out = self.up6(down6, out)\n        out = self.up5(down5, out)\n        out = self.up4(down4, out)\n        out = self.up3(down3, out)\n        out = self.up2(down2, out)\n\n        out = self.classify(out)\n        # out = torch.squeeze(out, dim=1)\n        return out\n\n\n\n# 128x128\nclass UNet128 (nn.Module):\n    def __init__(self, in_shape=(3, 128, 128), **kwargs):\n        super(UNet128, self).__init__()\n        C,H,W = in_shape\n        #assert(C==3)\n\n        #128\n        self.down3 = StackEncoder( C,   128, kernel_size=3)   # 64\n        self.down4 = StackEncoder(128,  256, kernel_size=3)   # 32\n        self.down5 = StackEncoder(256,  512, kernel_size=3)   # 16\n        self.down6 = StackEncoder(512, 1024, kernel_size=3)   #  8\n\n        self.center = nn.Sequential(\n            ConvBnRelu2d(1024, 1024, kernel_size=3, padding=1, stride=1 ),\n        )\n\n        # 8\n        # x_big_channels, x_channels, y_channels\n        self.up6 = StackDecoder(1024,1024, 512, kernel_size=3)  # 16\n        self.up5 = StackDecoder( 512, 512, 256, kernel_size=3)  # 32\n        self.up4 = StackDecoder( 256, 256, 128, kernel_size=3)  # 64\n        self.up3 = StackDecoder( 128, 128,  64, kernel_size=3)  #128\n        self.classify = nn.Conv2d(64, 1, kernel_size=1, padding=0, stride=1, bias=True)\n\n\n    def forward(self, x):\n\n        out = x                       #;print(\'x    \',x.size())\n        down3,out = self.down3(out)   #;print(\'down3\',down3.size())  #64\n        down4,out = self.down4(out)   #;print(\'down4\',down4.size())  #32\n        down5,out = self.down5(out)   #;print(\'down5\',down5.size())  #16\n        down6,out = self.down6(out)   #;print(\'down6\',down6.size())  #8\n        pass                          #;print(\'out  \',out.size())\n\n        out = self.center(out)\n        out = self.up6(down6, out)\n        out = self.up5(down5, out)\n        out = self.up4(down4, out)\n        out = self.up3(down3, out)\n        out = self.classify(out)\n        # out = torch.squeeze(out, dim=1)\n        return out\n'"
pywick/models/segmentation/config.py,0,"b""import os\n\n# here (https://github.com/pytorch/vision/tree/master/torchvision/models) to find the download link of pretrained models\n\nroot = '/models/pytorch'\nres50_path = os.path.join(root, 'resnet50-19c8e357.pth')\nres101_path = os.path.join(root, 'resnet101-5d3b4d8f.pth')\nres152_path = os.path.join(root, 'resnet152-b121ed2d.pth')\ninception_v3_path = os.path.join(root, 'inception_v3_google-1a9a5a14.pth')\nvgg19_bn_path = os.path.join(root, 'vgg19_bn-c79401a0.pth')\nvgg16_path = os.path.join(root, 'vgg16-397923af.pth')\ndense201_path = os.path.join(root, 'densenet201-4c113574.pth')\n\n'''\nvgg16 trained using caffe\nvisit this (https://github.com/jcjohnson/pytorch-vgg) to download the converted vgg16\n'''\nvgg16_caffe_path = os.path.join(root, 'vgg16-caffe.pth')\n"""
pywick/models/segmentation/danet.py,10,"b'# Source: https://github.com/Tramac/awesome-semantic-segmentation-pytorch/blob/master/core/models/danet.py (License: Apache 2.0)\n\n""""""\nImplementation of `Dual Attention Network for Scene Segmentation <https://arxiv.org/pdf/1809.02983.pdf>`_\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom pywick.models.segmentation.da_basenets.segbase import SegBaseModel\n\n__all__ = [\'DANet\', \'DANet_Resnet50\', \'DANet_Resnet101\', \'DANet_Resnet152\']\n\n\nclass DANet(SegBaseModel):\n    r""""""Pyramid Scene Parsing Network\n\n    Parameters\n    ----------\n    nclass : int\n        Number of categories for the training dataset.\n    backbone : string\n        Pre-trained dilated backbone network type (default:\'resnet50\'; \'resnet50\',\n        \'resnet101\' or \'resnet152\').\n    norm_layer : object\n        Normalization layer used in backbone network (default: :class:`mxnet.gluon.nn.BatchNorm`;\n        for Synchronized Cross-GPU BachNormalization).\n    aux : bool\n        Auxiliary loss.\n    Reference:\n        Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang,and Hanqing Lu.\n        ""Dual Attention Network for Scene Segmentation."" *CVPR*, 2019\n    """"""\n\n    def __init__(self, num_classes, pretrained=True, backbone=\'resnet101\', aux=False, **kwargs):\n        super(DANet, self).__init__(num_classes, pretrained=pretrained, aux=aux, backbone=backbone, **kwargs)\n        self.head = _DAHead(2048, num_classes, aux, **kwargs)\n\n        self.__setattr__(\'exclusive\', [\'head\'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        _, _, c3, c4 = self.base_forward(x)\n        outputs = []\n        x = self.head(c4)\n        x0 = F.interpolate(x[0], size, mode=\'bilinear\', align_corners=True)\n        outputs.append(x0)\n\n        if self.aux:\n            x1 = F.interpolate(x[1], size, mode=\'bilinear\', align_corners=True)\n            x2 = F.interpolate(x[2], size, mode=\'bilinear\', align_corners=True)\n            outputs.append(x1)\n            outputs.append(x2)\n            return outputs\n        else:\n            return outputs[0]\n\n\nclass _PositionAttentionModule(nn.Module):\n    """""" Position attention module""""""\n\n    def __init__(self, in_channels, **kwargs):\n        super(_PositionAttentionModule, self).__init__()\n        self.conv_b = nn.Conv2d(in_channels, in_channels // 8, 1)\n        self.conv_c = nn.Conv2d(in_channels, in_channels // 8, 1)\n        self.conv_d = nn.Conv2d(in_channels, in_channels, 1)\n        self.alpha = nn.Parameter(torch.zeros(1))\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x):\n        batch_size, _, height, width = x.size()\n        feat_b = self.conv_b(x).view(batch_size, -1, height * width).permute(0, 2, 1)\n        feat_c = self.conv_c(x).view(batch_size, -1, height * width)\n        attention_s = self.softmax(torch.bmm(feat_b, feat_c))\n        feat_d = self.conv_d(x).view(batch_size, -1, height * width)\n        feat_e = torch.bmm(feat_d, attention_s.permute(0, 2, 1)).view(batch_size, -1, height, width)\n        out = self.alpha * feat_e + x\n\n        return out\n\n\nclass _ChannelAttentionModule(nn.Module):\n    """"""Channel attention module""""""\n\n    def __init__(self, **kwargs):\n        super(_ChannelAttentionModule, self).__init__()\n        self.beta = nn.Parameter(torch.zeros(1))\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x):\n        batch_size, _, height, width = x.size()\n        feat_a = x.view(batch_size, -1, height * width)\n        feat_a_transpose = x.view(batch_size, -1, height * width).permute(0, 2, 1)\n        attention = torch.bmm(feat_a, feat_a_transpose)\n        attention_new = torch.max(attention, dim=-1, keepdim=True)[0].expand_as(attention) - attention\n        attention = self.softmax(attention_new)\n\n        feat_e = torch.bmm(attention, feat_a).view(batch_size, -1, height, width)\n        out = self.beta * feat_e + x\n\n        return out\n\n\nclass _DAHead(nn.Module):\n    def __init__(self, in_channels, nclass, aux=True, norm_layer=nn.BatchNorm2d, norm_kwargs=None, **kwargs):\n        super(_DAHead, self).__init__()\n        self.aux = aux\n        inter_channels = in_channels // 4\n        self.conv_p1 = nn.Sequential(\n            nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n            norm_layer(inter_channels, **({} if norm_kwargs is None else norm_kwargs)),\n            nn.ReLU(True)\n        )\n        self.conv_c1 = nn.Sequential(\n            nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n            norm_layer(inter_channels, **({} if norm_kwargs is None else norm_kwargs)),\n            nn.ReLU(True)\n        )\n        self.pam = _PositionAttentionModule(inter_channels, **kwargs)\n        self.cam = _ChannelAttentionModule(**kwargs)\n        self.conv_p2 = nn.Sequential(\n            nn.Conv2d(inter_channels, inter_channels, 3, padding=1, bias=False),\n            norm_layer(inter_channels, **({} if norm_kwargs is None else norm_kwargs)),\n            nn.ReLU(True)\n        )\n        self.conv_c2 = nn.Sequential(\n            nn.Conv2d(inter_channels, inter_channels, 3, padding=1, bias=False),\n            norm_layer(inter_channels, **({} if norm_kwargs is None else norm_kwargs)),\n            nn.ReLU(True)\n        )\n        self.out = nn.Sequential(\n            nn.Dropout(0.1),\n            nn.Conv2d(inter_channels, nclass, 1)\n        )\n        if aux:\n            self.conv_p3 = nn.Sequential(\n                nn.Dropout(0.1),\n                nn.Conv2d(inter_channels, nclass, 1)\n            )\n            self.conv_c3 = nn.Sequential(\n                nn.Dropout(0.1),\n                nn.Conv2d(inter_channels, nclass, 1)\n            )\n\n    def forward(self, x):\n        feat_p = self.conv_p1(x)\n        feat_p = self.pam(feat_p)\n        feat_p = self.conv_p2(feat_p)\n\n        feat_c = self.conv_c1(x)\n        feat_c = self.cam(feat_c)\n        feat_c = self.conv_c2(feat_c)\n\n        feat_fusion = feat_p + feat_c\n\n        outputs = []\n        fusion_out = self.out(feat_fusion)\n        outputs.append(fusion_out)\n        if self.aux:\n            p_out = self.conv_p3(feat_p)\n            c_out = self.conv_c3(feat_c)\n            outputs.append(p_out)\n            outputs.append(c_out)\n\n        return tuple(outputs)\n\n\ndef get_danet(num_classes=1, backbone=\'resnet50\', pretrained=True, **kwargs):\n    r""""""Dual Attention Network\n\n    Parameters\n    ----------\n    num_classes : int\n        Number of classes\n    pretrained : bool (default True)\n        This will load pretrained backbone network, that was trained on ImageNet.\n    """"""\n\n    model = DANet(num_classes=num_classes, backbone=backbone, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef DANet_Resnet50(num_classes=1, **kwargs):\n    return get_danet(num_classes=num_classes, backbone=\'resnet50\', **kwargs)\n\n\ndef DANet_Resnet101(num_classes=1, **kwargs):\n    return get_danet(num_classes=num_classes, backbone=\'resnet101\', **kwargs)\n\n\ndef DANet_Resnet152(num_classes=1, **kwargs):\n    return get_danet(num_classes=num_classes, backbone=\'resnet152\', **kwargs)\n\n\nif __name__ == \'__main__\':\n    img = torch.randn(2, 3, 480, 480)\n    model = DANet_Resnet50()\n    outputs = model(img)\n'"
pywick/models/segmentation/deeplab_v2_res.py,3,"b'# Source: https://github.com/doiken23/DeepLab_pytorch\n\n""""""\nDeepLab v2 - DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs <https://arxiv.org/abs/1606.00915>`_\n""""""\n\nimport math\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\n\nmodel_url = \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\'\n\n__all__ = [\'DeepLabv2_ASPP\', \'DeepLabv2_FOV\']\n\nclass Atrous_Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, rate=1, downsample=None):\n        super(Atrous_Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               dilation=rate, padding=rate, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Atrous_ResNet_features(nn.Module):\n\n    def __init__(self, block, layers, pretrained=False):\n        super(Atrous_ResNet_features, self).__init__()\n        self.inplanes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0], stride=1, rate=1)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, rate=1)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, rate=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, rate=4)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n        if pretrained:\n            print(\'load the pre-trained model.\')\n            resnet = models.resnet101(pretrained)\n            self.conv1 = resnet.conv1\n            self.bn1 = resnet.bn1\n            self.layer1 = resnet.layer1\n            self.layer2 = resnet.layer2\n\n    def _make_layer(self, block, planes, blocks, stride=1, rate=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, rate, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, stride=1, rate=rate))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        return x\n\n\nclass Atrous_module(nn.Module):\n    def __init__(self, inplanes, num_classes, rate):\n        super(Atrous_module, self).__init__()\n        planes = inplanes\n        self.atrous_convolution = nn.Conv2d(inplanes, planes, kernel_size=3,\n                                            stride=1, padding=rate, dilation=rate)\n        self.fc1 = nn.Conv2d(planes, planes, kernel_size=1, stride=1)\n        self.fc2 = nn.Conv2d(planes, num_classes, kernel_size=1, stride=1)\n\n    def forward(self, x):\n        x = self.atrous_convolution(x)\n        x = self.fc1(x)\n        x = self.fc2(x)\n\n        return x\n\n\nclass DeepLabv2_ASPP(nn.Module):\n    """"""\n    DeeplabV2 Resnet implementation with ASPP.\n    """"""\n    def __init__(self, num_classes, small=True, pretrained=False, **kwargs):\n        super(DeepLabv2_ASPP, self).__init__()\n        block = Atrous_Bottleneck\n        self.resnet_features = Atrous_ResNet_features(block, [3, 4, 23, 3], pretrained)\n\n        if small:\n            rates = [2, 4, 8, 12]\n        else:\n            rates = [6, 12, 18, 24]\n        self.aspp1 = Atrous_module(2048, num_classes, rate=rates[0])\n        self.aspp2 = Atrous_module(2048, num_classes, rate=rates[1])\n        self.aspp3 = Atrous_module(2048, num_classes, rate=rates[2])\n        self.aspp4 = Atrous_module(2048, num_classes, rate=rates[3])\n\n    def forward(self, x):\n        x = self.resnet_features(x)\n        x1 = self.aspp1(x)\n        x2 = self.aspp2(x)\n        x3 = self.aspp3(x)\n        x4 = self.aspp4(x)\n\n        x = x1 + x2 + x3 + x4\n        x = F.interpolate(x, scale_factor=8, mode=\'bilinear\')\n\n        return x\n\n\nclass DeepLabv2_FOV(nn.Module):\n    """"""\n        DeeplabV2 Resnet implementation with FOV.\n        """"""\n    def __init__(self, num_classes, pretrained=True, **kwargs):\n        super(DeepLabv2_FOV, self).__init__()\n        block = Atrous_Bottleneck\n        self.resnet_features = Atrous_ResNet_features(block, [3, 4, 23, 3], pretrained)\n\n        self.atrous = Atrous_module(2048, num_classes, rate=12)\n\n    def forward(self, x):\n        x = self.resnet_features(x)\n        x = self.atrous(x)\n        x = F.interpolate(x, scale_factor=8, mode=\'bilinear\')\n\n        return x\n'"
pywick/models/segmentation/deeplab_v3.py,4,"b'# Source: https://github.com/doiken23/DeepLab_pytorch\n\n""""""\nDeepLab v3 - `Rethinking Atrous Convolution for Semantic Image Segmentation <https://arxiv.org/abs/1706.05587>`_\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nimport math\n\n__all__ = [\'DeepLabv3\']\n\nmodel_url = \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\'\n\nclass Atrous_Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, rate=1, downsample=None):\n        super(Atrous_Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               dilation=rate, padding=rate, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Atrous_ResNet_features(nn.Module):\n\n    def __init__(self, block, layers, pretrained=False):\n        super(Atrous_ResNet_features, self).__init__()\n        self.inplanes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0], stride=1, rate=1)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, rate=1)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, rate=1)\n        self.layer4 = self._make_MG_unit(block, 512, stride=1, rate=2)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n        if pretrained:\n            print(\'load the pre-trained model.\')\n            resnet = models.resnet101(pretrained)\n            self.conv1 = resnet.conv1\n            self.bn1 = resnet.bn1\n            self.layer1 = resnet.layer1\n            self.layer2 = resnet.layer2\n\n    def _make_layer(self, block, planes, blocks, stride=1, rate=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, rate, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def _make_MG_unit(self, block, planes, blocks=[1, 2, 4], stride=1, rate=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, rate=blocks[0] * rate, downsample=downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, len(blocks)):\n            layers.append(block(self.inplanes, planes, stride=1, rate=blocks[i] * rate))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        return x\n\n\nclass Atrous_module(nn.Module):\n    def __init__(self, inplanes, planes, rate):\n        super(Atrous_module, self).__init__()\n        self.atrous_convolution = nn.Conv2d(inplanes, planes, kernel_size=3,\n                                            stride=1, padding=rate, dilation=rate)\n        self.batch_norm = nn.BatchNorm2d(planes)\n\n    def forward(self, x):\n        x = self.atrous_convolution(x)\n        x = self.batch_norm(x)\n\n        return x\n\n\nclass DeepLabv3(nn.Module):\n    def __init__(self, num_classes, small=True, pretrained=True, **kwargs):\n        super(DeepLabv3, self).__init__()\n        block = Atrous_Bottleneck\n        self.resnet_features = Atrous_ResNet_features(block, [3, 4, 23], pretrained)\n\n        rates = [1, 6, 12, 18]\n        self.aspp1 = Atrous_module(2048, 256, rate=rates[0])\n        self.aspp2 = Atrous_module(2048, 256, rate=rates[1])\n        self.aspp3 = Atrous_module(2048, 256, rate=rates[2])\n        self.aspp4 = Atrous_module(2048, 256, rate=rates[3])\n        self.image_pool = nn.Sequential(nn.AdaptiveMaxPool2d(1),\n                                        nn.Conv2d(2048, 256, kernel_size=1))\n\n        self.fc1 = nn.Sequential(nn.Conv2d(1280, 256, kernel_size=1),\n                                 nn.BatchNorm2d(256))\n        self.fc2 = nn.Conv2d(256, num_classes, kernel_size=1)\n\n    def forward(self, x):\n        x = self.resnet_features(x)\n        x1 = self.aspp1(x)\n        x2 = self.aspp2(x)\n        x3 = self.aspp3(x)\n        x4 = self.aspp4(x)\n        x5 = self.image_pool(x)\n        x5 = F.interpolate(x5, size=x4.size()[2:], mode=\'nearest\')\n\n        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        x = F.interpolate(x, scale_factor=(16, 16), mode=\'bilinear\')\n\n        return x\n'"
pywick/models/segmentation/deeplab_v3_plus.py,5,"b'# Source: https://github.com/doiken23/DeepLab_pytorch\n\n""""""\nDeepLab v3+ `Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation <https://arxiv.org/abs/1802.02611>`_\n""""""\n\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\n\nmodel_url = \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\'\n\n__all__ = [\'DeepLabv3_plus\']\n\nclass Atrous_Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, rate=1, downsample=None):\n        super(Atrous_Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               dilation=rate, padding=rate, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Atrous_ResNet_features(nn.Module):\n\n    def __init__(self, block, layers, pretrained=False):\n        super(Atrous_ResNet_features, self).__init__()\n        self.inplanes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0], stride=1, rate=1)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, rate=1)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, rate=1)\n        self.layer4 = self._make_MG_unit(block, 512, stride=1, rate=2)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n        if pretrained:\n            print(\'load the pre-trained model.\')\n            resnet = models.resnet101(pretrained)\n            self.conv1 = resnet.conv1\n            self.bn1 = resnet.bn1\n            self.layer1 = resnet.layer1\n            self.layer2 = resnet.layer2\n\n    def _make_layer(self, block, planes, blocks, stride=1, rate=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, rate, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def _make_MG_unit(self, block, planes, blocks=[1, 2, 4], stride=1, rate=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, rate=blocks[0] * rate, downsample=downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, len(blocks)):\n            layers.append(block(self.inplanes, planes, stride=1, rate=blocks[i] * rate))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        conv2 = x\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        return x, conv2\n\n\nclass Atrous_module(nn.Module):\n    def __init__(self, inplanes, planes, rate):\n        super(Atrous_module, self).__init__()\n        self.atrous_convolution = nn.Conv2d(inplanes, planes, kernel_size=3,\n                                            stride=1, padding=rate, dilation=rate)\n        self.batch_norm = nn.BatchNorm2d(planes)\n\n    def forward(self, x):\n        x = self.atrous_convolution(x)\n        x = self.batch_norm(x)\n\n        return x\n\n\nclass DeepLabv3_plus(nn.Module):\n    def __init__(self, num_classes, small=True, pretrained=True, **kwargs):\n        super(DeepLabv3_plus, self).__init__()\n        block = Atrous_Bottleneck\n        self.resnet_features = Atrous_ResNet_features(block, [3, 4, 23], pretrained)\n\n        rates = [1, 6, 12, 18]\n        self.aspp1 = Atrous_module(2048, 256, rate=rates[0])\n        self.aspp2 = Atrous_module(2048, 256, rate=rates[1])\n        self.aspp3 = Atrous_module(2048, 256, rate=rates[2])\n        self.aspp4 = Atrous_module(2048, 256, rate=rates[3])\n        self.image_pool = nn.Sequential(nn.AdaptiveMaxPool2d(1),\n                                        nn.Conv2d(2048, 256, kernel_size=1))\n\n        self.fc1 = nn.Sequential(nn.Conv2d(1280, 256, kernel_size=1),\n                                 nn.BatchNorm2d(256))\n\n        self.reduce_conv2 = nn.Sequential(nn.Conv2d(256, 48, kernel_size=1),\n                                          nn.BatchNorm2d(48))\n        self.last_conv = nn.Sequential(nn.Conv2d(304, 256, kernel_size=3, stride=1, padding=1),\n                                       nn.BatchNorm2d(256),\n                                       nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n                                       nn.BatchNorm2d(256),\n                                       nn.Conv2d(256, num_classes, kernel_size=1, stride=1))\n\n    def forward(self, x):\n        x, conv2 = self.resnet_features(x)\n        x1 = self.aspp1(x)\n        x2 = self.aspp2(x)\n        x3 = self.aspp3(x)\n        x4 = self.aspp4(x)\n        x5 = self.image_pool(x)\n        x5 = F.interpolate(x5, size=x4.size()[2:], mode=\'nearest\')\n\n        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n        x = self.fc1(x)\n        x = F.interpolate(x, scale_factor=(4, 4), mode=\'bilinear\')\n\n        low_lebel_features = self.reduce_conv2(conv2)\n\n        x = torch.cat((x, low_lebel_features), dim=1)\n        x = self.last_conv(x)\n        x = F.interpolate(x, scale_factor=(4, 4), mode=\'bilinear\')\n\n        return x\n'"
pywick/models/segmentation/denseaspp.py,8,"b'# Source: https://github.com/Tramac/awesome-semantic-segmentation-pytorch/blob/master/core/models/denseaspp.py (License: Apache 2.0)\n\n""""""\nImplementation of `DenseASPP for Semantic Segmentation in Street Scenes <http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_DenseASPP_for_Semantic_CVPR_2018_paper.pdf>`_\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom pywick.models.segmentation.da_basenets.densenet import *\nfrom pywick.models.segmentation.da_basenets.fcn import _FCNHead\n\n__all__ = [\'DenseASPP\', \'DenseASPP_121\', \'DenseASPP_161\', \'DenseASPP_169\', \'DenseASPP_201\']\n\n\nclass DenseASPP(nn.Module):\n    def __init__(self, num_classes, pretrained=True, backbone=\'densenet161\', aux=False, dilate_scale=8, **kwargs):\n        super(DenseASPP, self).__init__()\n        self.nclass = num_classes\n        self.aux = aux\n        self.dilate_scale = dilate_scale\n        if backbone == \'densenet121\':\n            self.pretrained = dilated_densenet121(dilate_scale, pretrained=pretrained, **kwargs)\n        elif backbone == \'densenet161\':\n            self.pretrained = dilated_densenet161(dilate_scale, pretrained=pretrained, **kwargs)\n        elif backbone == \'densenet169\':\n            self.pretrained = dilated_densenet169(dilate_scale, pretrained=pretrained, **kwargs)\n        elif backbone == \'densenet201\':\n            self.pretrained = dilated_densenet201(dilate_scale, pretrained=pretrained, **kwargs)\n        else:\n            raise RuntimeError(\'unknown backbone: {}\'.format(backbone))\n        in_channels = self.pretrained.num_features\n\n        self.head = _DenseASPPHead(in_channels, num_classes, **kwargs)\n\n        if aux:\n            self.auxlayer = _FCNHead(in_channels, num_classes, **kwargs)\n\n        self.__setattr__(\'exclusive\', [\'head\', \'auxlayer\'] if aux else [\'head\'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        features = self.pretrained.features(x)\n        if self.dilate_scale > 8:\n            features = F.interpolate(features, scale_factor=2, mode=\'bilinear\', align_corners=True)\n        outputs = []\n        x = self.head(features)\n        x = F.interpolate(x, size, mode=\'bilinear\', align_corners=True)\n        outputs.append(x)\n\n        if self.aux:\n            auxout = self.auxlayer(features)\n            auxout = F.interpolate(auxout, size, mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout)\n            return tuple(outputs)\n        else:\n            return outputs[0]\n\n\nclass _DenseASPPHead(nn.Module):\n    def __init__(self, in_channels, nclass, norm_layer=nn.BatchNorm2d, norm_kwargs=None, **kwargs):\n        super(_DenseASPPHead, self).__init__()\n        self.dense_aspp_block = _DenseASPPBlock(in_channels, 256, 64, norm_layer, norm_kwargs)\n        self.block = nn.Sequential(\n            nn.Dropout(0.1),\n            nn.Conv2d(in_channels + 5 * 64, nclass, 1)\n        )\n\n    def forward(self, x):\n        x = self.dense_aspp_block(x)\n        return self.block(x)\n\n\nclass _DenseASPPConv(nn.Sequential):\n    def __init__(self, in_channels, inter_channels, out_channels, atrous_rate,\n                 drop_rate=0.1, norm_layer=nn.BatchNorm2d, norm_kwargs=None):\n        super(_DenseASPPConv, self).__init__()\n        self.add_module(\'conv1\', nn.Conv2d(in_channels, inter_channels, 1)),\n        self.add_module(\'bn1\', norm_layer(inter_channels, **({} if norm_kwargs is None else norm_kwargs))),\n        self.add_module(\'relu1\', nn.ReLU(True)),\n        self.add_module(\'conv2\', nn.Conv2d(inter_channels, out_channels, 3, dilation=atrous_rate, padding=atrous_rate)),\n        self.add_module(\'bn2\', norm_layer(out_channels, **({} if norm_kwargs is None else norm_kwargs))),\n        self.add_module(\'relu2\', nn.ReLU(True)),\n        self.drop_rate = drop_rate\n\n    def forward(self, x):\n        features = super(_DenseASPPConv, self).forward(x)\n        if self.drop_rate > 0:\n            features = F.dropout(features, p=self.drop_rate, training=self.training)\n        return features\n\n\nclass _DenseASPPBlock(nn.Module):\n    def __init__(self, in_channels, inter_channels1, inter_channels2,\n                 norm_layer=nn.BatchNorm2d, norm_kwargs=None):\n        super(_DenseASPPBlock, self).__init__()\n        self.aspp_3 = _DenseASPPConv(in_channels, inter_channels1, inter_channels2, 3, 0.1,\n                                     norm_layer, norm_kwargs)\n        self.aspp_6 = _DenseASPPConv(in_channels + inter_channels2 * 1, inter_channels1, inter_channels2, 6, 0.1,\n                                     norm_layer, norm_kwargs)\n        self.aspp_12 = _DenseASPPConv(in_channels + inter_channels2 * 2, inter_channels1, inter_channels2, 12, 0.1,\n                                      norm_layer, norm_kwargs)\n        self.aspp_18 = _DenseASPPConv(in_channels + inter_channels2 * 3, inter_channels1, inter_channels2, 18, 0.1,\n                                      norm_layer, norm_kwargs)\n        self.aspp_24 = _DenseASPPConv(in_channels + inter_channels2 * 4, inter_channels1, inter_channels2, 24, 0.1,\n                                      norm_layer, norm_kwargs)\n\n    def forward(self, x):\n        aspp3 = self.aspp_3(x)\n        x = torch.cat([aspp3, x], dim=1)\n\n        aspp6 = self.aspp_6(x)\n        x = torch.cat([aspp6, x], dim=1)\n\n        aspp12 = self.aspp_12(x)\n        x = torch.cat([aspp12, x], dim=1)\n\n        aspp18 = self.aspp_18(x)\n        x = torch.cat([aspp18, x], dim=1)\n\n        aspp24 = self.aspp_24(x)\n        x = torch.cat([aspp24, x], dim=1)\n\n        return x\n\n\ndef get_denseaspp(num_classes=1, backbone=\'densenet169\', pretrained=True, **kwargs):\n    r""""""DenseASPP\n\n    Parameters\n    ----------\n    dataset : str, default citys\n        The dataset that model pretrained on. (pascal_voc, ade20k)\n    pretrained : bool or str\n        Boolean value controls whether to load the default pretrained weights for model.\n        String value represents the hashtag for a certain version of pretrained weights.\n    root : str, default \'~/.torch/models\'\n        Location for keeping the model parameters.\n    pretrained_base : bool or str, default True\n        This will load pretrained backbone network, that was trained on ImageNet.\n\n   """"""\n\n    return DenseASPP(num_classes=num_classes, pretrained=pretrained, backbone=backbone, **kwargs)\n\n\ndef DenseASPP_121(num_classes=1, **kwargs):\n    return get_denseaspp(num_classes=num_classes, backbone=\'densenet121\', **kwargs)\n\n\ndef DenseASPP_161(num_classes=1, **kwargs):\n    return get_denseaspp(num_classes=num_classes, backbone=\'densenet161\', **kwargs)\n\n\ndef DenseASPP_169(num_classes=1, **kwargs):\n    return get_denseaspp(num_classes=num_classes, backbone=\'densenet169\', **kwargs)\n\n\ndef DenseASPP_201(num_classes=1, **kwargs):\n    return get_denseaspp(num_classes=num_classes, backbone=\'densenet201\', **kwargs)\n\n\nif __name__ == \'__main__\':\n    img = torch.randn(2, 3, 480, 480)\n    model = DenseASPP_121()\n    outputs = model(img)\n'"
pywick/models/segmentation/drn.py,2,"b'# Source: https://github.com/fyu/drn/blob/master/drn.py (BSD 3-Clause)\n# Pretrained: yes\n\n""""""\nImplementation of `Dilated Residual Networks <http://openaccess.thecvf.com/content_cvpr_2017/papers/Yu_Dilated_Residual_Networks_CVPR_2017_paper.pdf>`_\n""""""\n\nimport torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\n\nBatchNorm = nn.BatchNorm2d\n\n\n# __all__ = [\'DRN\', \'drn26\', \'drn42\', \'drn58\']\n\n\nwebroot = \'https://tigress-web.princeton.edu/~fy/drn/models/\'\n\nmodel_urls = {\n    \'drn-c-26\': webroot + \'drn_c_26-ddedf421.pth\',\n    \'drn-c-42\': webroot + \'drn_c_42-9d336e8c.pth\',\n    \'drn-c-58\': webroot + \'drn_c_58-0a53a92c.pth\',\n    \'drn-d-22\': webroot + \'drn_d_22-4bd2f8ea.pth\',\n    \'drn-d-38\': webroot + \'drn_d_38-eebb45f0.pth\',\n    \'drn-d-54\': webroot + \'drn_d_54-0e0534ff.pth\',\n    \'drn-d-105\': webroot + \'drn_d_105-12b40979.pth\'\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1, padding=1, dilation=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=padding, bias=False, dilation=dilation)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None,\n                 dilation=(1, 1), residual=True):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride,\n                             padding=dilation[0], dilation=dilation[0])\n        self.bn1 = BatchNorm(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes,\n                             padding=dilation[1], dilation=dilation[1])\n        self.bn2 = BatchNorm(planes)\n        self.downsample = downsample\n        self.stride = stride\n        self.residual = residual\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n        if self.residual:\n            out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None,\n                 dilation=(1, 1), residual=True):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = BatchNorm(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=dilation[1], bias=False,\n                               dilation=dilation[1])\n        self.bn2 = BatchNorm(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = BatchNorm(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass DRN(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000,\n                 channels=(16, 32, 64, 128, 256, 512, 512, 512),\n                 out_map=False, out_middle=False, pool_size=28, arch=\'D\'):\n        super(DRN, self).__init__()\n        self.inplanes = channels[0]\n        self.out_map = out_map\n        self.out_dim = channels[-1]\n        self.out_middle = out_middle\n        self.arch = arch\n\n        if arch == \'C\':\n            self.conv1 = nn.Conv2d(3, channels[0], kernel_size=7, stride=1,\n                                   padding=3, bias=False)\n            self.bn1 = BatchNorm(channels[0])\n            self.relu = nn.ReLU(inplace=True)\n\n            self.layer1 = self._make_layer(\n                BasicBlock, channels[0], layers[0], stride=1)\n            self.layer2 = self._make_layer(\n                BasicBlock, channels[1], layers[1], stride=2)\n        elif arch == \'D\':\n            self.layer0 = nn.Sequential(\n                nn.Conv2d(3, channels[0], kernel_size=7, stride=1, padding=3,\n                          bias=False),\n                BatchNorm(channels[0]),\n                nn.ReLU(inplace=True)\n            )\n\n            self.layer1 = self._make_conv_layers(\n                channels[0], layers[0], stride=1)\n            self.layer2 = self._make_conv_layers(\n                channels[1], layers[1], stride=2)\n\n        self.layer3 = self._make_layer(block, channels[2], layers[2], stride=2)\n        self.layer4 = self._make_layer(block, channels[3], layers[3], stride=2)\n        self.layer5 = self._make_layer(block, channels[4], layers[4], dilation=2,\n                                       new_level=False)\n        self.layer6 = None if layers[5] == 0 else \\\n            self._make_layer(block, channels[5], layers[5], dilation=4,\n                             new_level=False)\n\n        if arch == \'C\':\n            self.layer7 = None if layers[6] == 0 else \\\n                self._make_layer(BasicBlock, channels[6], layers[6], dilation=2,\n                                 new_level=False, residual=False)\n            self.layer8 = None if layers[7] == 0 else \\\n                self._make_layer(BasicBlock, channels[7], layers[7], dilation=1,\n                                 new_level=False, residual=False)\n        elif arch == \'D\':\n            self.layer7 = None if layers[6] == 0 else \\\n                self._make_conv_layers(channels[6], layers[6], dilation=2)\n            self.layer8 = None if layers[7] == 0 else \\\n                self._make_conv_layers(channels[7], layers[7], dilation=1)\n\n        if num_classes > 0:\n            self.avgpool = nn.AvgPool2d(pool_size)\n            self.fc = nn.Conv2d(self.out_dim, num_classes, kernel_size=1,\n                                stride=1, padding=0, bias=True)\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, BatchNorm):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1,\n                    new_level=True, residual=True):\n        assert dilation == 1 or dilation % 2 == 0\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                BatchNorm(planes * block.expansion),\n            )\n\n        layers = list()\n        layers.append(block(\n            self.inplanes, planes, stride, downsample,\n            dilation=(1, 1) if dilation == 1 else (\n                dilation // 2 if new_level else dilation, dilation),\n            residual=residual))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, residual=residual,\n                                dilation=(dilation, dilation)))\n\n        return nn.Sequential(*layers)\n\n    def _make_conv_layers(self, channels, convs, stride=1, dilation=1):\n        modules = []\n        for i in range(convs):\n            modules.extend([\n                nn.Conv2d(self.inplanes, channels, kernel_size=3,\n                          stride=stride if i == 0 else 1,\n                          padding=dilation, bias=False, dilation=dilation),\n                BatchNorm(channels),\n                nn.ReLU(inplace=True)])\n            self.inplanes = channels\n        return nn.Sequential(*modules)\n\n    def forward(self, x):\n        y = list()\n\n        if self.arch == \'C\':\n            x = self.conv1(x)\n            x = self.bn1(x)\n            x = self.relu(x)\n        elif self.arch == \'D\':\n            x = self.layer0(x)\n\n        x = self.layer1(x)\n        y.append(x)\n        x = self.layer2(x)\n        y.append(x)\n\n        x = self.layer3(x)\n        y.append(x)\n\n        x = self.layer4(x)\n        y.append(x)\n\n        x = self.layer5(x)\n        y.append(x)\n\n        if self.layer6 is not None:\n            x = self.layer6(x)\n            y.append(x)\n\n        if self.layer7 is not None:\n            x = self.layer7(x)\n            y.append(x)\n\n        if self.layer8 is not None:\n            x = self.layer8(x)\n            y.append(x)\n\n        if self.out_map:\n            x = self.fc(x)\n        else:\n            x = self.avgpool(x)\n            x = self.fc(x)\n            x = x.view(x.size(0), -1)\n\n        if self.out_middle:\n            return x, y\n        else:\n            return x\n\n\ndef drn_c_26(pretrained=False, **kwargs):\n    model = DRN(BasicBlock, [1, 1, 2, 2, 2, 2, 1, 1], arch=\'C\', **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'drn-c-26\']))\n    return model\n\n\ndef drn_c_42(pretrained=False, **kwargs):\n    model = DRN(BasicBlock, [1, 1, 3, 4, 6, 3, 1, 1], arch=\'C\', **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'drn-c-42\']))\n    return model\n\n\ndef drn_c_58(pretrained=False, **kwargs):\n    model = DRN(Bottleneck, [1, 1, 3, 4, 6, 3, 1, 1], arch=\'C\', **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'drn-c-58\']))\n    return model\n\n\ndef drn_d_22(pretrained=False, **kwargs):\n    model = DRN(BasicBlock, [1, 1, 2, 2, 2, 2, 1, 1], arch=\'D\', **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'drn-d-22\']))\n    return model\n\n\ndef drn_d_24(pretrained=False, **kwargs):\n    model = DRN(BasicBlock, [1, 1, 2, 2, 2, 2, 2, 2], arch=\'D\', **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'drn-d-24\']))\n    return model\n\n\ndef drn_d_38(pretrained=False, **kwargs):\n    model = DRN(BasicBlock, [1, 1, 3, 4, 6, 3, 1, 1], arch=\'D\', **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'drn-d-38\']))\n    return model\n\n\ndef drn_d_40(pretrained=False, **kwargs):\n    model = DRN(BasicBlock, [1, 1, 3, 4, 6, 3, 2, 2], arch=\'D\', **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'drn-d-40\']))\n    return model\n\n\ndef drn_d_54(pretrained=False, **kwargs):\n    model = DRN(Bottleneck, [1, 1, 3, 4, 6, 3, 1, 1], arch=\'D\', **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'drn-d-54\']))\n    return model\n\n\ndef drn_d_56(pretrained=False, **kwargs):\n    model = DRN(Bottleneck, [1, 1, 3, 4, 6, 3, 2, 2], arch=\'D\', **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'drn-d-56\']))\n    return model\n\n\ndef drn_d_105(pretrained=False, **kwargs):\n    model = DRN(Bottleneck, [1, 1, 3, 4, 23, 3, 1, 1], arch=\'D\', **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'drn-d-105\']))\n    return model\n\n\ndef drn_d_107(pretrained=False, **kwargs):\n    model = DRN(Bottleneck, [1, 1, 3, 4, 23, 3, 2, 2], arch=\'D\', **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'drn-d-107\']))\n    return model'"
pywick/models/segmentation/drn_seg.py,2,"b""from .drn import *\nimport torch.nn as nn\nimport math\nimport torch.nn.functional as F\n\n__all__ = ['DRNSeg']\n\nclass DRNSeg(nn.Module):\n    def __init__(self, num_classes, pretrained=True, model_name=None, use_torch_up=False, **kwargs):\n        super(DRNSeg, self).__init__()\n\n        if model_name == 'DRN_C_42':\n            model = drn_c_42(pretrained=pretrained, num_classes=1000)\n        elif model_name == 'DRN_C_58':\n            model = drn_c_58(pretrained=pretrained, num_classes=1000)\n        elif model_name == 'DRN_D_38':\n            model = drn_d_38(pretrained=pretrained, num_classes=1000)\n        elif model_name == 'DRN_D_54':\n            model = drn_d_54(pretrained=pretrained, num_classes=1000)\n        elif model_name == 'DRN_D_105':\n            model = drn_d_105(pretrained=pretrained, num_classes=1000)\n        else:\n            raise Exception('model_name must be supplied to DRNSeg constructor.')\n\n        self.base = nn.Sequential(*list(model.children())[:-2])\n\n        self.seg = nn.Conv2d(model.out_dim, num_classes, kernel_size=1, bias=True)\n        self.softmax = nn.LogSoftmax()\n        m = self.seg\n        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n        m.bias.data.zero_()\n        self.use_torch_up = use_torch_up\n\n        up = nn.ConvTranspose2d(num_classes, num_classes, 16, stride=8, padding=4,\n                                output_padding=0, groups=num_classes,\n                                bias=False)\n        fill_up_weights(up)\n        up.weight.requires_grad = False\n        self.up = up\n\n    def forward(self, x):\n        base = self.base(x)\n        final = self.seg(base)\n        if self.use_torch_up:\n            return F.interpolate(final, x.size()[2:], mode='bilinear')\n        else:\n            return self.up(final)\n\n    def optim_parameters(self, memo=None):\n        for param in self.base.parameters():\n            yield param\n        for param in self.seg.parameters():\n            yield param\n\ndef fill_up_weights(up):\n    w = up.weight.data\n    f = math.ceil(w.size(2) / 2)\n    c = (2 * f - 1 - f % 2) / (2. * f)\n    for i in range(w.size(2)):\n        for j in range(w.size(3)):\n            w[0, 0, i, j] = \\\n                (1 - math.fabs(i / f - c)) * (1 - math.fabs(j / f - c))\n    for c in range(1, w.size(0)):\n        w[c, 0, :, :] = w[0, 0, :, :]"""
pywick/models/segmentation/duc_hdc.py,2,"b'# Source: https://github.com/zijundeng/pytorch-semantic-segmentation/tree/master/models (MIT)\n\n""""""\nImplementation of: `Understanding Convolution for Semantic Segmentation <https://arxiv.org/pdf/1702.08502.pdf>`_\n""""""\nimport torch\nfrom torch import nn\nfrom torchvision import models\n\nfrom .config import res152_path\n\n__all__ = [\'ResNetDUC\', \'ResNetDUCHDC\']\n\nclass _DenseUpsamplingConvModule(nn.Module):\n    def __init__(self, down_factor, in_dim, num_classes):\n        super(_DenseUpsamplingConvModule, self).__init__()\n        upsample_dim = (down_factor ** 2) * num_classes\n        self.conv = nn.Conv2d(in_dim, upsample_dim, kernel_size=3, padding=1)\n        self.bn = nn.BatchNorm2d(upsample_dim)\n        self.relu = nn.ReLU(inplace=True)\n        self.pixel_shuffle = nn.PixelShuffle(down_factor)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.pixel_shuffle(x)\n        return x\n\n\nclass ResNetDUC(nn.Module):\n    # the size of image should be multiple of 8\n    def __init__(self, num_classes, pretrained=True, **kwargs):\n        super(ResNetDUC, self).__init__()\n        resnet = models.resnet152()\n        if pretrained:\n            resnet.load_state_dict(torch.load(res152_path))\n        self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n        self.layer1 = resnet.layer1\n        self.layer2 = resnet.layer2\n        self.layer3 = resnet.layer3\n        self.layer4 = resnet.layer4\n\n        for n, m in self.layer3.named_modules():\n            if \'conv2\' in n:\n                m.dilation = (2, 2)\n                m.padding = (2, 2)\n                m.stride = (1, 1)\n            elif \'downsample.0\' in n:\n                m.stride = (1, 1)\n        for n, m in self.layer4.named_modules():\n            if \'conv2\' in n:\n                m.dilation = (4, 4)\n                m.padding = (4, 4)\n                m.stride = (1, 1)\n            elif \'downsample.0\' in n:\n                m.stride = (1, 1)\n\n        self.duc = _DenseUpsamplingConvModule(8, 2048, num_classes)\n\n    def forward(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.duc(x)\n        return x\n\n\nclass ResNetDUCHDC(nn.Module):\n    # the size of image should be multiple of 8\n    def __init__(self, num_classes, pretrained=True, **kwargs):\n        super(ResNetDUCHDC, self).__init__()\n        resnet = models.resnet152()\n        if pretrained:\n            resnet.load_state_dict(torch.load(res152_path))\n        self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n        self.layer1 = resnet.layer1\n        self.layer2 = resnet.layer2\n        self.layer3 = resnet.layer3\n        self.layer4 = resnet.layer4\n\n        for n, m in self.layer3.named_modules():\n            if \'conv2\' in n or \'downsample.0\' in n:\n                m.stride = (1, 1)\n        for n, m in self.layer4.named_modules():\n            if \'conv2\' in n or \'downsample.0\' in n:\n                m.stride = (1, 1)\n        layer3_group_config = [1, 2, 5, 9]\n        for idx in range(len(self.layer3)):\n            self.layer3[idx].conv2.dilation = (layer3_group_config[idx % 4], layer3_group_config[idx % 4])\n            self.layer3[idx].conv2.padding = (layer3_group_config[idx % 4], layer3_group_config[idx % 4])\n        layer4_group_config = [5, 9, 17]\n        for idx in range(len(self.layer4)):\n            self.layer4[idx].conv2.dilation = (layer4_group_config[idx], layer4_group_config[idx])\n            self.layer4[idx].conv2.padding = (layer4_group_config[idx], layer4_group_config[idx])\n\n        self.duc = _DenseUpsamplingConvModule(8, 2048, num_classes)\n\n    def forward(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.duc(x)\n        return x\n'"
pywick/models/segmentation/dunet.py,4,"b'# Source: https://github.com/Tramac/awesome-semantic-segmentation-pytorch/blob/master/core/models/dunet.py (License: Apache 2.0)\n\n""""""\nImplementation of `Decoders Matter for Semantic Segmentation: Data-Dependent Decoding Enables Flexible Feature Aggregation <https://arxiv.org/pdf/1903.02120>`_\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom pywick.models.segmentation.da_basenets.segbase import SegBaseModel\nfrom pywick.models.segmentation.da_basenets.fcn import _FCNHead\n\n__all__ = [\'DUNet\', \'DUNet_Resnet50\', \'DUNet_Resnet101\', \'DUNet_Resnet152\']\n\n\n# The model may be wrong because lots of details missing in paper.\nclass DUNet(SegBaseModel):\n    """"""Decoders Matter for Semantic Segmentation\n\n    Reference:\n        Zhi Tian, Tong He, Chunhua Shen, and Youliang Yan.\n        ""Decoders Matter for Semantic Segmentation:\n        Data-Dependent Decoding Enables Flexible Feature Aggregation."" CVPR, 2019\n    """"""\n\n    def __init__(self, num_classes, pretrained=True, backbone=\'resnet101\', aux=False, **kwargs):\n        super(DUNet, self).__init__(num_classes, pretrained=pretrained, aux=aux, backbone=backbone, **kwargs)\n        self.head = _DUHead(2144, **kwargs)\n        self.dupsample = DUpsampling(256, num_classes, scale_factor=8, **kwargs)\n        if aux:\n            self.auxlayer = _FCNHead(1024, 256, **kwargs)\n            self.aux_dupsample = DUpsampling(256, num_classes, scale_factor=8, **kwargs)\n\n        self.__setattr__(\'exclusive\', [\'dupsample\', \'head\', \'auxlayer\', \'aux_dupsample\'] if aux else [\'dupsample\', \'head\'])\n\n    def forward(self, x):\n        c1, c2, c3, c4 = self.base_forward(x)\n        outputs = []\n        x = self.head(c2, c3, c4)\n        x = self.dupsample(x)\n        outputs.append(x)\n\n        if self.aux:\n            auxout = self.auxlayer(c3)\n            auxout = self.aux_dupsample(auxout)\n            outputs.append(auxout)\n            return tuple(outputs)\n        else:\n            return outputs[0]\n\n\nclass FeatureFused(nn.Module):\n    """"""Module for fused features""""""\n\n    def __init__(self, inter_channels=48, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(FeatureFused, self).__init__()\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(512, inter_channels, 1, bias=False),\n            norm_layer(inter_channels),\n            nn.ReLU(True)\n        )\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(1024, inter_channels, 1, bias=False),\n            norm_layer(inter_channels),\n            nn.ReLU(True)\n        )\n\n    def forward(self, c2, c3, c4):\n        size = c4.size()[2:]\n        c2 = self.conv2(F.interpolate(c2, size, mode=\'bilinear\', align_corners=True))\n        c3 = self.conv3(F.interpolate(c3, size, mode=\'bilinear\', align_corners=True))\n        fused_feature = torch.cat([c4, c3, c2], dim=1)\n        return fused_feature\n\n\nclass _DUHead(nn.Module):\n    def __init__(self, in_channels, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(_DUHead, self).__init__()\n        self.fuse = FeatureFused(norm_layer=norm_layer, **kwargs)\n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels, 256, 3, padding=1, bias=False),\n            norm_layer(256),\n            nn.ReLU(True),\n            nn.Conv2d(256, 256, 3, padding=1, bias=False),\n            norm_layer(256),\n            nn.ReLU(True)\n        )\n\n    def forward(self, c2, c3, c4):\n        fused_feature = self.fuse(c2, c3, c4)\n        out = self.block(fused_feature)\n        return out\n\n\nclass DUpsampling(nn.Module):\n    """"""DUsampling module""""""\n\n    def __init__(self, in_channels, out_channels, scale_factor=2, **kwargs):\n        super(DUpsampling, self).__init__()\n        self.scale_factor = scale_factor\n        self.conv_w = nn.Conv2d(in_channels, out_channels * scale_factor * scale_factor, 1, bias=False)\n\n    def forward(self, x):\n        x = self.conv_w(x)\n        n, c, h, w = x.size()\n\n        # N, C, H, W --> N, W, H, C\n        x = x.permute(0, 3, 2, 1).contiguous()\n\n        # N, W, H, C --> N, W, H * scale, C // scale\n        x = x.view(n, w, h * self.scale_factor, c // self.scale_factor)\n\n        # N, W, H * scale, C // scale --> N, H * scale, W, C // scale\n        x = x.permute(0, 2, 1, 3).contiguous()\n\n        # N, H * scale, W, C // scale --> N, H * scale, W * scale, C // (scale ** 2)\n        x = x.view(n, h * self.scale_factor, w * self.scale_factor, c // (self.scale_factor * self.scale_factor))\n\n        # N, H * scale, W * scale, C // (scale ** 2) -- > N, C // (scale ** 2), H * scale, W * scale\n        x = x.permute(0, 3, 1, 2)\n\n        return x\n\n\ndef get_dunet(num_classes=1, backbone=\'resnet50\', pretrained=True, **kwargs):\n    r""""""Decoders Matter for Semantic Segmentation\n\n        Parameters\n        ----------\n        num_classes : int (default: 1) - number of classes\n        backbone : str - type of backbone to use (one of `{resnet50, resnet101, resnet152}`)\n        pretrained : bool (default: True) - whether to load pretrained backbone network, that was trained on ImageNet.\n        """"""\n    model = DUNet(num_classes=num_classes, backbone=backbone, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef DUNet_Resnet50(num_classes=1, **kwargs):\n    return get_dunet(num_classes=num_classes, backbone=\'resnet50\', **kwargs)\n\n\ndef DUNet_Resnet101(num_classes=1, **kwargs):\n    return get_dunet(num_classes=num_classes, backbone=\'resnet101\', **kwargs)\n\n\ndef DUNet_Resnet152(num_classes=1, **kwargs):\n    return get_dunet(num_classes=num_classes, backbone=\'resnet152\', **kwargs)\n\n\nif __name__ == \'__main__\':\n    img = torch.randn(2, 3, 256, 256)\n    model = DUNet_Resnet50()\n    outputs = model(img)\n'"
pywick/models/segmentation/enet.py,4,"b'# Source: https://github.com/davidtvs/PyTorch-ENet (MIT)\n\n""""""\nImplementation of `ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation <https://arxiv.org/abs/1606.02147>`_\n""""""\n\nimport torch.nn as nn\nimport torch\n\n__all__ = [\'ENet\']\n\nclass InitialBlock(nn.Module):\n    """"""The initial block is composed of two branches:\n    1. a main branch which performs a regular convolution with stride 2;\n    2. an extension branch which performs max-pooling.\n\n    Doing both operations in parallel and concatenating their results\n    allows for efficient downsampling and expansion. The main branch\n    outputs 13 feature maps while the extension branch outputs 3, for a\n    total of 16 feature maps after concatenation.\n\n    Keyword arguments:\n    - in_channels (int): the number of input channels.\n    - out_channels (int): the number output channels.\n    - kernel_size (int, optional): the kernel size of the filters used in\n    the convolution layer. Default: 3.\n    - padding (int, optional): zero-padding added to both sides of the\n    input. Default: 0.\n    - bias (bool, optional): Adds a learnable bias to the output if\n    ``True``. Default: False.\n    - relu (bool, optional): When ``True`` ReLU is used as the activation\n    function; otherwise, PReLU is used. Default: True.\n\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size=3,\n                 padding=0,\n                 bias=False,\n                 relu=True):\n        super().__init__()\n\n        if relu:\n            activation = nn.ReLU()\n        else:\n            activation = nn.PReLU()\n\n        # Main branch - As stated above the number of output channels for this\n        # branch is the total minus 3, since the remaining channels come from\n        # the extension branch\n        self.main_branch = nn.Conv2d(\n            in_channels,\n            out_channels - 3,\n            kernel_size=kernel_size,\n            stride=2,\n            padding=padding,\n            bias=bias)\n\n        # Extension branch\n        self.ext_branch = nn.MaxPool2d(kernel_size, stride=2, padding=padding)\n\n        # Initialize batch normalization to be used after concatenation\n        self.batch_norm = nn.BatchNorm2d(out_channels)\n\n        # PReLU layer to apply after concatenating the branches\n        self.out_prelu = activation\n\n    def forward(self, x):\n        main = self.main_branch(x)\n        ext = self.ext_branch(x)\n\n        # Concatenate branches\n        out = torch.cat((main, ext), 1)\n\n        # Apply batch normalization\n        out = self.batch_norm(out)\n\n        return self.out_prelu(out)\n\n\nclass RegularBottleneck(nn.Module):\n    """"""Regular bottlenecks are the main building block of ENet.\n    Main branch:\n    1. Shortcut connection.\n\n    Extension branch:\n    1. 1x1 convolution which decreases the number of channels by\n    ``internal_ratio``, also called a projection;\n    2. regular, dilated or asymmetric convolution;\n    3. 1x1 convolution which increases the number of channels back to\n    ``channels``, also called an expansion;\n    4. dropout as a regularizer.\n\n    Keyword arguments:\n    - channels (int): the number of input and output channels.\n    - internal_ratio (int, optional): a scale factor applied to\n    ``channels`` used to compute the number of\n    channels after the projection. eg. given ``channels`` equal to 128 and\n    internal_ratio equal to 2 the number of channels after the projection\n    is 64. Default: 4.\n    - kernel_size (int, optional): the kernel size of the filters used in\n    the convolution layer described above in item 2 of the extension\n    branch. Default: 3.\n    - padding (int, optional): zero-padding added to both sides of the\n    input. Default: 0.\n    - dilation (int, optional): spacing between kernel elements for the\n    convolution described in item 2 of the extension branch. Default: 1.\n    asymmetric (bool, optional): flags if the convolution described in\n    item 2 of the extension branch is asymmetric or not. Default: False.\n    - dropout_prob (float, optional): probability of an element to be\n    zeroed. Default: 0 (no dropout).\n    - bias (bool, optional): Adds a learnable bias to the output if\n    ``True``. Default: False.\n    - relu (bool, optional): When ``True`` ReLU is used as the activation\n    function; otherwise, PReLU is used. Default: True.\n\n    """"""\n\n    def __init__(self,\n                 channels,\n                 internal_ratio=4,\n                 kernel_size=3,\n                 padding=0,\n                 dilation=1,\n                 asymmetric=False,\n                 dropout_prob=0,\n                 bias=False,\n                 relu=True):\n        super().__init__()\n\n        # Check in the internal_scale parameter is within the expected range\n        # [1, channels]\n        if internal_ratio <= 1 or internal_ratio > channels:\n            raise RuntimeError(""Value out of range. Expected value in the ""\n                               ""interval [1, {0}], got internal_scale={1}.""\n                               .format(channels, internal_ratio))\n\n        internal_channels = channels // internal_ratio\n\n        if relu:\n            activation = nn.ReLU()\n        else:\n            activation = nn.PReLU()\n\n        # Main branch - shortcut connection\n\n        # Extension branch - 1x1 convolution, followed by a regular, dilated or\n        # asymmetric convolution, followed by another 1x1 convolution, and,\n        # finally, a regularizer (spatial dropout). Number of channels is constant.\n\n        # 1x1 projection convolution\n        self.ext_conv1 = nn.Sequential(\n            nn.Conv2d(\n                channels,\n                internal_channels,\n                kernel_size=1,\n                stride=1,\n                bias=bias), nn.BatchNorm2d(internal_channels), activation)\n\n        # If the convolution is asymmetric we split the main convolution in\n        # two. Eg. for a 5x5 asymmetric convolution we have two convolution:\n        # the first is 5x1 and the second is 1x5.\n        if asymmetric:\n            self.ext_conv2 = nn.Sequential(\n                nn.Conv2d(\n                    internal_channels,\n                    internal_channels,\n                    kernel_size=(kernel_size, 1),\n                    stride=1,\n                    padding=(padding, 0),\n                    dilation=dilation,\n                    bias=bias), nn.BatchNorm2d(internal_channels), activation,\n                nn.Conv2d(\n                    internal_channels,\n                    internal_channels,\n                    kernel_size=(1, kernel_size),\n                    stride=1,\n                    padding=(0, padding),\n                    dilation=dilation,\n                    bias=bias), nn.BatchNorm2d(internal_channels), activation)\n        else:\n            self.ext_conv2 = nn.Sequential(\n                nn.Conv2d(\n                    internal_channels,\n                    internal_channels,\n                    kernel_size=kernel_size,\n                    stride=1,\n                    padding=padding,\n                    dilation=dilation,\n                    bias=bias), nn.BatchNorm2d(internal_channels), activation)\n\n        # 1x1 expansion convolution\n        self.ext_conv3 = nn.Sequential(\n            nn.Conv2d(\n                internal_channels,\n                channels,\n                kernel_size=1,\n                stride=1,\n                bias=bias), nn.BatchNorm2d(channels), activation)\n\n        self.ext_regul = nn.Dropout2d(p=dropout_prob)\n\n        # PReLU layer to apply after adding the branches\n        self.out_prelu = activation\n\n    def forward(self, x):\n        # Main branch shortcut\n        main = x\n\n        # Extension branch\n        ext = self.ext_conv1(x)\n        ext = self.ext_conv2(ext)\n        ext = self.ext_conv3(ext)\n        ext = self.ext_regul(ext)\n\n        # Add main and extension branches\n        out = main + ext\n\n        return self.out_prelu(out)\n\n\nclass DownsamplingBottleneck(nn.Module):\n    """"""Downsampling bottlenecks further downsample the feature map size.\n\n    Main branch:\n    1. max pooling with stride 2; indices are saved to be used for\n    unpooling later.\n\n    Extension branch:\n    1. 2x2 convolution with stride 2 that decreases the number of channels\n    by ``internal_ratio``, also called a projection;\n    2. regular convolution (by default, 3x3);\n    3. 1x1 convolution which increases the number of channels to\n    ``out_channels``, also called an expansion;\n    4. dropout as a regularizer.\n\n    Keyword arguments:\n    - in_channels (int): the number of input channels.\n    - out_channels (int): the number of output channels.\n    - internal_ratio (int, optional): a scale factor applied to ``channels``\n    used to compute the number of channels after the projection. eg. given\n    ``channels`` equal to 128 and internal_ratio equal to 2 the number of\n    channels after the projection is 64. Default: 4.\n    - kernel_size (int, optional): the kernel size of the filters used in\n    the convolution layer described above in item 2 of the extension branch.\n    Default: 3.\n    - padding (int, optional): zero-padding added to both sides of the\n    input. Default: 0.\n    - dilation (int, optional): spacing between kernel elements for the\n    convolution described in item 2 of the extension branch. Default: 1.\n    - asymmetric (bool, optional): flags if the convolution described in\n    item 2 of the extension branch is asymmetric or not. Default: False.\n    - return_indices (bool, optional):  if ``True``, will return the max\n    indices along with the outputs. Useful when unpooling later.\n    - dropout_prob (float, optional): probability of an element to be\n    zeroed. Default: 0 (no dropout).\n    - bias (bool, optional): Adds a learnable bias to the output if\n    ``True``. Default: False.\n    - relu (bool, optional): When ``True`` ReLU is used as the activation\n    function; otherwise, PReLU is used. Default: True.\n\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 internal_ratio=4,\n                 kernel_size=3,\n                 padding=0,\n                 return_indices=False,\n                 dropout_prob=0,\n                 bias=False,\n                 relu=True):\n        super().__init__()\n\n        # Store parameters that are needed later\n        self.return_indices = return_indices\n\n        # Check in the internal_scale parameter is within the expected range\n        # [1, channels]\n        if internal_ratio <= 1 or internal_ratio > in_channels:\n            raise RuntimeError(""Value out of range. Expected value in the ""\n                               ""interval [1, {0}], got internal_scale={1}. ""\n                               .format(in_channels, internal_ratio))\n\n        internal_channels = in_channels // internal_ratio\n\n        if relu:\n            activation = nn.ReLU()\n        else:\n            activation = nn.PReLU()\n\n        # Main branch - max pooling followed by feature map (channels) padding\n        self.main_max1 = nn.MaxPool2d(\n            kernel_size,\n            stride=2,\n            padding=padding,\n            return_indices=return_indices)\n\n        # Extension branch - 2x2 convolution, followed by a regular, dilated or\n        # asymmetric convolution, followed by another 1x1 convolution. Number\n        # of channels is doubled.\n\n        # 2x2 projection convolution with stride 2\n        self.ext_conv1 = nn.Sequential(\n            nn.Conv2d(\n                in_channels,\n                internal_channels,\n                kernel_size=2,\n                stride=2,\n                bias=bias), nn.BatchNorm2d(internal_channels), activation)\n\n        # Convolution\n        self.ext_conv2 = nn.Sequential(\n            nn.Conv2d(\n                internal_channels,\n                internal_channels,\n                kernel_size=kernel_size,\n                stride=1,\n                padding=padding,\n                bias=bias), nn.BatchNorm2d(internal_channels), activation)\n\n        # 1x1 expansion convolution\n        self.ext_conv3 = nn.Sequential(\n            nn.Conv2d(\n                internal_channels,\n                out_channels,\n                kernel_size=1,\n                stride=1,\n                bias=bias), nn.BatchNorm2d(out_channels), activation)\n\n        self.ext_regul = nn.Dropout2d(p=dropout_prob)\n\n        # PReLU layer to apply after concatenating the branches\n        self.out_prelu = activation\n\n    def forward(self, x):\n        # Main branch shortcut\n        if self.return_indices:\n            main, max_indices = self.main_max1(x)\n        else:\n            main = self.main_max1(x)\n\n        # Extension branch\n        ext = self.ext_conv1(x)\n        ext = self.ext_conv2(ext)\n        ext = self.ext_conv3(ext)\n        ext = self.ext_regul(ext)\n\n        # Main branch channel padding\n        n, ch_ext, h, w = ext.size()\n        ch_main = main.size()[1]\n        padding = torch.zeros(n, ch_ext - ch_main, h, w)\n\n        # Before concatenating, check if main is on the CPU or GPU and\n        # convert padding accordingly\n        if main.is_cuda:\n            padding = padding.cuda()\n\n        # Concatenate\n        main = torch.cat((main, padding), 1)\n\n        # Add main and extension branches\n        out = main + ext\n\n        return self.out_prelu(out), max_indices\n\n\nclass UpsamplingBottleneck(nn.Module):\n    """"""The upsampling bottlenecks upsample the feature map resolution using max\n    pooling indices stored from the corresponding downsampling bottleneck.\n\n    Main branch:\n    1. 1x1 convolution with stride 1 that decreases the number of channels by\n    ``internal_ratio``, also called a projection;\n    2. max unpool layer using the max pool indices from the corresponding\n    downsampling max pool layer.\n\n    Extension branch:\n    1. 1x1 convolution with stride 1 that decreases the number of channels by\n    ``internal_ratio``, also called a projection;\n    2. transposed convolution (by default, 3x3);\n    3. 1x1 convolution which increases the number of channels to\n    ``out_channels``, also called an expansion;\n    4. dropout as a regularizer.\n\n    Keyword arguments:\n    - in_channels (int): the number of input channels.\n    - out_channels (int): the number of output channels.\n    - internal_ratio (int, optional): a scale factor applied to ``in_channels``\n     used to compute the number of channels after the projection. eg. given\n     ``in_channels`` equal to 128 and ``internal_ratio`` equal to 2 the number\n     of channels after the projection is 64. Default: 4.\n    - kernel_size (int, optional): the kernel size of the filters used in the\n    convolution layer described above in item 2 of the extension branch.\n    Default: 3.\n    - padding (int, optional): zero-padding added to both sides of the input.\n    Default: 0.\n    - dropout_prob (float, optional): probability of an element to be zeroed.\n    Default: 0 (no dropout).\n    - bias (bool, optional): Adds a learnable bias to the output if ``True``.\n    Default: False.\n    - relu (bool, optional): When ``True`` ReLU is used as the activation\n    function; otherwise, PReLU is used. Default: True.\n\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 internal_ratio=4,\n                 kernel_size=3,\n                 padding=0,\n                 dropout_prob=0,\n                 bias=False,\n                 relu=True):\n        super().__init__()\n\n        # Check in the internal_scale parameter is within the expected range\n        # [1, channels]\n        if internal_ratio <= 1 or internal_ratio > in_channels:\n            raise RuntimeError(""Value out of range. Expected value in the ""\n                               ""interval [1, {0}], got internal_scale={1}. ""\n                               .format(in_channels, internal_ratio))\n\n        internal_channels = in_channels // internal_ratio\n\n        if relu:\n            activation = nn.ReLU()\n        else:\n            activation = nn.PReLU()\n\n        # Main branch - max pooling followed by feature map (channels) padding\n        self.main_conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias),\n            nn.BatchNorm2d(out_channels))\n\n        # Remember that the stride is the same as the kernel_size, just like\n        # the max pooling layers\n        self.main_unpool1 = nn.MaxUnpool2d(kernel_size=2)\n\n        # Extension branch - 1x1 convolution, followed by a regular, dilated or\n        # asymmetric convolution, followed by another 1x1 convolution. Number\n        # of channels is doubled.\n\n        # 1x1 projection convolution with stride 1\n        self.ext_conv1 = nn.Sequential(\n            nn.Conv2d(\n                in_channels, internal_channels, kernel_size=1, bias=bias),\n            nn.BatchNorm2d(internal_channels), activation)\n\n        # Transposed convolution\n        self.ext_conv2 = nn.Sequential(\n            nn.ConvTranspose2d(\n                internal_channels,\n                internal_channels,\n                kernel_size=kernel_size,\n                stride=2,\n                padding=padding,\n                output_padding=1,\n                bias=bias), nn.BatchNorm2d(internal_channels), activation)\n\n        # 1x1 expansion convolution\n        self.ext_conv3 = nn.Sequential(\n            nn.Conv2d(\n                internal_channels, out_channels, kernel_size=1, bias=bias),\n            nn.BatchNorm2d(out_channels), activation)\n\n        self.ext_regul = nn.Dropout2d(p=dropout_prob)\n\n        # PReLU layer to apply after concatenating the branches\n        self.out_prelu = activation\n\n    def forward(self, x, max_indices):\n        # Main branch shortcut\n        main = self.main_conv1(x)\n        main = self.main_unpool1(main, max_indices)\n        # Extension branch\n        ext = self.ext_conv1(x)\n        ext = self.ext_conv2(ext)\n        ext = self.ext_conv3(ext)\n        ext = self.ext_regul(ext)\n\n        # Add main and extension branches\n        out = main + ext\n\n        return self.out_prelu(out)\n\n\nclass ENet(nn.Module):\n    """"""Generate the ENet model.\n\n    :param num_classes: (int): the number of classes to segment.\n    :param encoder_relu: (bool, optional): When ``True`` ReLU is used as the\n        activation function in the encoder blocks/layers; otherwise, PReLU\n        is used. Default: False.\n    :param decoder_relu: (bool, optional): When ``True`` ReLU is used as the\n        activation function in the decoder blocks/layers; otherwise, PReLU\n        is used. Default: True.\n    """"""\n\n    def __init__(self, num_classes, encoder_relu=False, decoder_relu=True, **kwargs):\n        super().__init__()\n\n        self.initial_block = InitialBlock(3, 16, padding=1, relu=encoder_relu)\n\n        # Stage 1 - Encoder\n        self.downsample1_0 = DownsamplingBottleneck(\n            16,\n            64,\n            padding=1,\n            return_indices=True,\n            dropout_prob=0.01,\n            relu=encoder_relu)\n        self.regular1_1 = RegularBottleneck(\n            64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n        self.regular1_2 = RegularBottleneck(\n            64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n        self.regular1_3 = RegularBottleneck(\n            64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n        self.regular1_4 = RegularBottleneck(\n            64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n\n        # Stage 2 - Encoder\n        self.downsample2_0 = DownsamplingBottleneck(\n            64,\n            128,\n            padding=1,\n            return_indices=True,\n            dropout_prob=0.1,\n            relu=encoder_relu)\n        self.regular2_1 = RegularBottleneck(\n            128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n        self.dilated2_2 = RegularBottleneck(\n            128, dilation=2, padding=2, dropout_prob=0.1, relu=encoder_relu)\n        self.asymmetric2_3 = RegularBottleneck(\n            128,\n            kernel_size=5,\n            padding=2,\n            asymmetric=True,\n            dropout_prob=0.1,\n            relu=encoder_relu)\n        self.dilated2_4 = RegularBottleneck(\n            128, dilation=4, padding=4, dropout_prob=0.1, relu=encoder_relu)\n        self.regular2_5 = RegularBottleneck(\n            128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n        self.dilated2_6 = RegularBottleneck(\n            128, dilation=8, padding=8, dropout_prob=0.1, relu=encoder_relu)\n        self.asymmetric2_7 = RegularBottleneck(\n            128,\n            kernel_size=5,\n            asymmetric=True,\n            padding=2,\n            dropout_prob=0.1,\n            relu=encoder_relu)\n        self.dilated2_8 = RegularBottleneck(\n            128, dilation=16, padding=16, dropout_prob=0.1, relu=encoder_relu)\n\n        # Stage 3 - Encoder\n        self.regular3_0 = RegularBottleneck(\n            128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n        self.dilated3_1 = RegularBottleneck(\n            128, dilation=2, padding=2, dropout_prob=0.1, relu=encoder_relu)\n        self.asymmetric3_2 = RegularBottleneck(\n            128,\n            kernel_size=5,\n            padding=2,\n            asymmetric=True,\n            dropout_prob=0.1,\n            relu=encoder_relu)\n        self.dilated3_3 = RegularBottleneck(\n            128, dilation=4, padding=4, dropout_prob=0.1, relu=encoder_relu)\n        self.regular3_4 = RegularBottleneck(\n            128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n        self.dilated3_5 = RegularBottleneck(\n            128, dilation=8, padding=8, dropout_prob=0.1, relu=encoder_relu)\n        self.asymmetric3_6 = RegularBottleneck(\n            128,\n            kernel_size=5,\n            asymmetric=True,\n            padding=2,\n            dropout_prob=0.1,\n            relu=encoder_relu)\n        self.dilated3_7 = RegularBottleneck(\n            128, dilation=16, padding=16, dropout_prob=0.1, relu=encoder_relu)\n\n        # Stage 4 - Decoder\n        self.upsample4_0 = UpsamplingBottleneck(\n            128, 64, padding=1, dropout_prob=0.1, relu=decoder_relu)\n        self.regular4_1 = RegularBottleneck(\n            64, padding=1, dropout_prob=0.1, relu=decoder_relu)\n        self.regular4_2 = RegularBottleneck(\n            64, padding=1, dropout_prob=0.1, relu=decoder_relu)\n\n        # Stage 5 - Decoder\n        self.upsample5_0 = UpsamplingBottleneck(\n            64, 16, padding=1, dropout_prob=0.1, relu=decoder_relu)\n        self.regular5_1 = RegularBottleneck(\n            16, padding=1, dropout_prob=0.1, relu=decoder_relu)\n        self.transposed_conv = nn.ConvTranspose2d(\n            16,\n            num_classes,\n            kernel_size=3,\n            stride=2,\n            padding=1,\n            output_padding=1,\n            bias=False)\n\n    def forward(self, x):\n        # Initial block\n        x = self.initial_block(x)\n\n        # Stage 1 - Encoder\n        x, max_indices1_0 = self.downsample1_0(x)\n        x = self.regular1_1(x)\n        x = self.regular1_2(x)\n        x = self.regular1_3(x)\n        x = self.regular1_4(x)\n\n        # Stage 2 - Encoder\n        x, max_indices2_0 = self.downsample2_0(x)\n        x = self.regular2_1(x)\n        x = self.dilated2_2(x)\n        x = self.asymmetric2_3(x)\n        x = self.dilated2_4(x)\n        x = self.regular2_5(x)\n        x = self.dilated2_6(x)\n        x = self.asymmetric2_7(x)\n        x = self.dilated2_8(x)\n\n        # Stage 3 - Encoder\n        x = self.regular3_0(x)\n        x = self.dilated3_1(x)\n        x = self.asymmetric3_2(x)\n        x = self.dilated3_3(x)\n        x = self.regular3_4(x)\n        x = self.dilated3_5(x)\n        x = self.asymmetric3_6(x)\n        x = self.dilated3_7(x)\n\n        # Stage 4 - Decoder\n        x = self.upsample4_0(x, max_indices2_0)\n        x = self.regular4_1(x)\n        x = self.regular4_2(x)\n\n        # Stage 5 - Decoder\n        x = self.upsample5_0(x, max_indices1_0)\n        x = self.regular5_1(x)\n        x = self.transposed_conv(x)\n\n        return x\n'"
pywick/models/segmentation/fcn16s.py,1,"b'# Source: https://github.com/zijundeng/pytorch-semantic-segmentation/tree/master/models (MIT)\n\n""""""\nImplementation of `Fully Convolutional Networks for Semantic Segmentation <http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf>`_\n""""""\n\nimport torch\nfrom torch import nn\nfrom torchvision import models\n\nfrom .fcn_utils import get_upsampling_weight\nfrom .config import vgg16_caffe_path\n\n__all__ = [\'FCN16VGG\']\n\nclass FCN16VGG(nn.Module):\n    def __init__(self, num_classes, pretrained=True, **kwargs):\n        super(FCN16VGG, self).__init__()\n        vgg = models.vgg16()\n        if pretrained:\n            vgg.load_state_dict(torch.load(vgg16_caffe_path))\n        features, classifier = list(vgg.features.children()), list(vgg.classifier.children())\n\n        features[0].padding = (100, 100)\n\n        for f in features:\n            if \'MaxPool\' in f.__class__.__name__:\n                f.ceil_mode = True\n            elif \'ReLU\' in f.__class__.__name__:\n                f.inplace = True\n\n        self.features4 = nn.Sequential(*features[: 24])\n        self.features5 = nn.Sequential(*features[24:])\n\n        self.score_pool4 = nn.Conv2d(512, num_classes, kernel_size=1)\n        self.score_pool4.weight.data.zero_()\n        self.score_pool4.bias.data.zero_()\n\n        fc6 = nn.Conv2d(512, 4096, kernel_size=7)\n        fc6.weight.data.copy_(classifier[0].weight.data.view(4096, 512, 7, 7))\n        fc6.bias.data.copy_(classifier[0].bias.data)\n        fc7 = nn.Conv2d(4096, 4096, kernel_size=1)\n        fc7.weight.data.copy_(classifier[3].weight.data.view(4096, 4096, 1, 1))\n        fc7.bias.data.copy_(classifier[3].bias.data)\n        score_fr = nn.Conv2d(4096, num_classes, kernel_size=1)\n        score_fr.weight.data.zero_()\n        score_fr.bias.data.zero_()\n        self.score_fr = nn.Sequential(\n            fc6, nn.ReLU(inplace=True), nn.Dropout(), fc7, nn.ReLU(inplace=True), nn.Dropout(), score_fr\n        )\n\n        self.upscore2 = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=4, stride=2, bias=False)\n        self.upscore16 = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=32, stride=16, bias=False)\n        self.upscore2.weight.data.copy_(get_upsampling_weight(num_classes, num_classes, 4))\n        self.upscore16.weight.data.copy_(get_upsampling_weight(num_classes, num_classes, 32))\n\n    def forward(self, x):\n        x_size = x.size()\n        pool4 = self.features4(x)\n        pool5 = self.features5(pool4)\n\n        score_fr = self.score_fr(pool5)\n        upscore2 = self.upscore2(score_fr)\n\n        score_pool4 = self.score_pool4(0.01 * pool4)\n        upscore16 = self.upscore16(score_pool4[:, :, 5: (5 + upscore2.size()[2]), 5: (5 + upscore2.size()[3])]\n                                   + upscore2)\n        return upscore16[:, :, 27: (27 + x_size[2]), 27: (27 + x_size[3])].contiguous()\n'"
pywick/models/segmentation/fcn32s.py,1,"b'# Source: https://github.com/zijundeng/pytorch-semantic-segmentation/tree/master/models (MIT)\n\n""""""\nImplementation of `Fully Convolutional Networks for Semantic Segmentation <http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf>`_\n""""""\n\nimport torch\nfrom torch import nn\nfrom torchvision import models\n\nfrom .fcn_utils import get_upsampling_weight\nfrom .config import vgg16_caffe_path\n\n__all__ = [\'FCN32VGG\']\n\n\nclass FCN32VGG(nn.Module):\n    def __init__(self, num_classes, pretrained=True, **kwargs):\n        super(FCN32VGG, self).__init__()\n        vgg = models.vgg16()\n        if pretrained:\n            vgg.load_state_dict(torch.load(vgg16_caffe_path))\n        features, classifier = list(vgg.features.children()), list(vgg.classifier.children())\n\n        features[0].padding = (100, 100)\n\n        for f in features:\n            if \'MaxPool\' in f.__class__.__name__:\n                f.ceil_mode = True\n            elif \'ReLU\' in f.__class__.__name__:\n                f.inplace = True\n\n        self.features5 = nn.Sequential(*features)\n\n        fc6 = nn.Conv2d(512, 4096, kernel_size=7)\n        fc6.weight.data.copy_(classifier[0].weight.data.view(4096, 512, 7, 7))\n        fc6.bias.data.copy_(classifier[0].bias.data)\n        fc7 = nn.Conv2d(4096, 4096, kernel_size=1)\n        fc7.weight.data.copy_(classifier[3].weight.data.view(4096, 4096, 1, 1))\n        fc7.bias.data.copy_(classifier[3].bias.data)\n        score_fr = nn.Conv2d(4096, num_classes, kernel_size=1)\n        score_fr.weight.data.zero_()\n        score_fr.bias.data.zero_()\n        self.score_fr = nn.Sequential(\n            fc6, nn.ReLU(inplace=True), nn.Dropout(), fc7, nn.ReLU(inplace=True), nn.Dropout(), score_fr\n        )\n\n        self.upscore = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=64, stride=32, bias=False)\n        self.upscore.weight.data.copy_(get_upsampling_weight(num_classes, num_classes, 64))\n\n    def forward(self, x):\n        x_size = x.size()\n        pool5 = self.features5(x)\n        score_fr = self.score_fr(pool5)\n        upscore = self.upscore(score_fr)\n        return upscore[:, :, 19: (19 + x_size[2]), 19: (19 + x_size[3])].contiguous()\n'"
pywick/models/segmentation/fcn8s.py,2,"b'# Source: https://github.com/zijundeng/pytorch-semantic-segmentation/tree/master/models (MIT)\n\n""""""\nImplementation of `Fully Convolutional Networks for Semantic Segmentation <http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf>`_\n""""""\n\nimport torch\nfrom torch import nn\nfrom torchvision import models\n\nfrom .fcn_utils import get_upsampling_weight\nfrom .config import vgg16_path, vgg16_caffe_path\n\n__all__ = [\'FCN8s\']\n\n# This is implemented in full accordance with the original one (https://github.com/shelhamer/fcn.berkeleyvision.org)\nclass FCN8s(nn.Module):\n    def __init__(self, num_classes, pretrained=True, caffe=False, **kwargs):\n        super(FCN8s, self).__init__()\n        vgg = models.vgg16()\n        if pretrained:\n            if caffe:\n                # load the pretrained vgg16 used by the paper\'s author\n                vgg.load_state_dict(torch.load(vgg16_caffe_path))\n            else:\n                vgg.load_state_dict(torch.load(vgg16_path))\n        features, classifier = list(vgg.features.children()), list(vgg.classifier.children())\n\n        \'\'\'\n        100 padding for 2 reasons:\n            1) support very small input size\n            2) allow cropping in order to match size of different layers\' feature maps\n        Note that the cropped part corresponds to a part of the 100 padding\n        Spatial information of different layers\' feature maps cannot be align exactly because of cropping, which is bad\n        \'\'\'\n        features[0].padding = (100, 100)\n\n        for f in features:\n            if \'MaxPool\' in f.__class__.__name__:\n                f.ceil_mode = True\n            elif \'ReLU\' in f.__class__.__name__:\n                f.inplace = True\n\n        self.features3 = nn.Sequential(*features[: 17])\n        self.features4 = nn.Sequential(*features[17: 24])\n        self.features5 = nn.Sequential(*features[24:])\n\n        self.score_pool3 = nn.Conv2d(256, num_classes, kernel_size=1)\n        self.score_pool4 = nn.Conv2d(512, num_classes, kernel_size=1)\n        self.score_pool3.weight.data.zero_()\n        self.score_pool3.bias.data.zero_()\n        self.score_pool4.weight.data.zero_()\n        self.score_pool4.bias.data.zero_()\n\n        fc6 = nn.Conv2d(512, 4096, kernel_size=7)\n        fc6.weight.data.copy_(classifier[0].weight.data.view(4096, 512, 7, 7))\n        fc6.bias.data.copy_(classifier[0].bias.data)\n        fc7 = nn.Conv2d(4096, 4096, kernel_size=1)\n        fc7.weight.data.copy_(classifier[3].weight.data.view(4096, 4096, 1, 1))\n        fc7.bias.data.copy_(classifier[3].bias.data)\n        score_fr = nn.Conv2d(4096, num_classes, kernel_size=1)\n        score_fr.weight.data.zero_()\n        score_fr.bias.data.zero_()\n        self.score_fr = nn.Sequential(\n            fc6, nn.ReLU(inplace=True), nn.Dropout(), fc7, nn.ReLU(inplace=True), nn.Dropout(), score_fr\n        )\n\n        self.upscore2 = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=4, stride=2, bias=False)\n        self.upscore_pool4 = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=4, stride=2, bias=False)\n        self.upscore8 = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=16, stride=8, bias=False)\n        self.upscore2.weight.data.copy_(get_upsampling_weight(num_classes, num_classes, 4))\n        self.upscore_pool4.weight.data.copy_(get_upsampling_weight(num_classes, num_classes, 4))\n        self.upscore8.weight.data.copy_(get_upsampling_weight(num_classes, num_classes, 16))\n\n    def forward(self, x):\n        x_size = x.size()\n        pool3 = self.features3(x)\n        pool4 = self.features4(pool3)\n        pool5 = self.features5(pool4)\n\n        score_fr = self.score_fr(pool5)\n        upscore2 = self.upscore2(score_fr)\n\n        score_pool4 = self.score_pool4(0.01 * pool4)\n        upscore_pool4 = self.upscore_pool4(score_pool4[:, :, 5: (5 + upscore2.size()[2]), 5: (5 + upscore2.size()[3])]\n                                           + upscore2)\n\n        score_pool3 = self.score_pool3(0.0001 * pool3)\n        upscore8 = self.upscore8(score_pool3[:, :, 9: (9 + upscore_pool4.size()[2]), 9: (9 + upscore_pool4.size()[3])]\n                                 + upscore_pool4)\n        return upscore8[:, :, 31: (31 + x_size[2]), 31: (31 + x_size[3])].contiguous()\n'"
pywick/models/segmentation/fcn_utils.py,14,"b""# Source: https://github.com/zijundeng/pytorch-semantic-segmentation/tree/master/models (MIT)\n\nimport os\nfrom math import ceil\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n\ndef check_mkdir(dir_name):\n    if not os.path.exists(dir_name):\n        os.mkdir(dir_name)\n\n\ndef initialize_weights(*models):\n    for model in models:\n        for module in model.modules():\n            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n                nn.init.kaiming_normal_(module.weight)\n                if module.bias is not None:\n                    module.bias.data.zero_()\n            elif isinstance(module, nn.BatchNorm2d):\n                module.weight.data.fill_(1)\n                module.bias.data.zero_()\n\n\ndef get_upsampling_weight(in_channels, out_channels, kernel_size):\n    factor = (kernel_size + 1) // 2\n    if kernel_size % 2 == 1:\n        center = factor - 1\n    else:\n        center = factor - 0.5\n    og = np.ogrid[:kernel_size, :kernel_size]\n    filt = (1 - abs(og[0] - center) / factor) * (1 - abs(og[1] - center) / factor)\n    weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size), dtype=np.float64)\n    weight[list(range(in_channels)), list(range(out_channels)), :, :] = filt\n    return torch.from_numpy(weight).float()\n\n\nclass CrossEntropyLoss2d(nn.Module):\n    def __init__(self, weight=None, size_average=True, ignore_index=255):\n        super(CrossEntropyLoss2d, self).__init__()\n        self.nll_loss = nn.NLLLoss2d(weight, size_average, ignore_index)\n\n    def forward(self, inputs, targets):\n        return self.nll_loss(F.log_softmax(inputs), targets)\n\n\nclass FocalLoss2d(nn.Module):\n    def __init__(self, gamma=2, weight=None, size_average=True, ignore_index=255):\n        super(FocalLoss2d, self).__init__()\n        self.gamma = gamma\n        self.nll_loss = nn.NLLLoss2d(weight, size_average, ignore_index)\n\n    def forward(self, inputs, targets):\n        return self.nll_loss((1 - F.softmax(inputs)) ** self.gamma * F.log_softmax(inputs), targets)\n\n\ndef _fast_hist(label_pred, label_true, num_classes):\n    mask = (label_true >= 0) & (label_true < num_classes)\n    hist = np.bincount(\n        num_classes * label_true[mask].astype(int) +\n        label_pred[mask], minlength=num_classes ** 2).reshape(num_classes, num_classes)\n    return hist\n\n\ndef evaluate(predictions, gts, num_classes):\n    hist = np.zeros((num_classes, num_classes))\n    for lp, lt in zip(predictions, gts):\n        hist += _fast_hist(lp.flatten(), lt.flatten(), num_classes)\n    # axis 0: gt, axis 1: prediction\n    acc = np.diag(hist).sum() / hist.sum()\n    acc_cls = np.diag(hist) / hist.sum(axis=1)\n    acc_cls = np.nanmean(acc_cls)\n    iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n    mean_iu = np.nanmean(iu)\n    freq = hist.sum(axis=1) / hist.sum()\n    fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n    return acc, acc_cls, mean_iu, fwavacc\n\n\nclass AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\nclass PolyLR(object):\n    def __init__(self, optimizer, curr_iter, max_iter, lr_decay):\n        self.max_iter = float(max_iter)\n        self.init_lr_groups = []\n        for p in optimizer.param_groups:\n            self.init_lr_groups.append(p['lr'])\n        self.param_groups = optimizer.param_groups\n        self.curr_iter = curr_iter\n        self.lr_decay = lr_decay\n\n    def step(self):\n        for idx, p in enumerate(self.param_groups):\n            p['lr'] = self.init_lr_groups[idx] * (1 - self.curr_iter / self.max_iter) ** self.lr_decay\n\n\n# just a try, not recommend to use\nclass Conv2dDeformable(nn.Module):\n    def __init__(self, regular_filter, cuda=True):\n        super(Conv2dDeformable, self).__init__()\n        assert isinstance(regular_filter, nn.Conv2d)\n        self.regular_filter = regular_filter\n        self.offset_filter = nn.Conv2d(regular_filter.in_channels, 2 * regular_filter.in_channels, kernel_size=3,\n                                       padding=1, bias=False)\n        self.offset_filter.weight.data.normal_(0, 0.0005)\n        self.input_shape = None\n        self.grid_w = None\n        self.grid_h = None\n        self.cuda = cuda\n\n    def forward(self, x):\n        x_shape = x.size()  # (b, c, h, w)\n        offset = self.offset_filter(x)  # (b, 2*c, h, w)\n        offset_w, offset_h = torch.split(offset, self.regular_filter.in_channels, 1)  # (b, c, h, w)\n        offset_w = offset_w.contiguous().view(-1, int(x_shape[2]), int(x_shape[3]))  # (b*c, h, w)\n        offset_h = offset_h.contiguous().view(-1, int(x_shape[2]), int(x_shape[3]))  # (b*c, h, w)\n        if not self.input_shape or self.input_shape != x_shape:\n            self.input_shape = x_shape\n            grid_w, grid_h = np.meshgrid(np.linspace(-1, 1, x_shape[3]), np.linspace(-1, 1, x_shape[2]))  # (h, w)\n            grid_w = torch.Tensor(grid_w)\n            grid_h = torch.Tensor(grid_h)\n            if self.cuda:\n                grid_w = grid_w.cuda()\n                grid_h = grid_h.cuda()\n            self.grid_w = nn.Parameter(grid_w)\n            self.grid_h = nn.Parameter(grid_h)\n        offset_w = offset_w + self.grid_w  # (b*c, h, w)\n        offset_h = offset_h + self.grid_h  # (b*c, h, w)\n        x = x.contiguous().view(-1, int(x_shape[2]), int(x_shape[3])).unsqueeze(1)  # (b*c, 1, h, w)\n        x = F.grid_sample(x, torch.stack((offset_h, offset_w), 3))  # (b*c, h, w)\n        x = x.contiguous().view(-1, int(x_shape[1]), int(x_shape[2]), int(x_shape[3]))  # (b, c, h, w)\n        x = self.regular_filter(x)\n        return x\n\n\ndef sliced_forward(single_forward):\n    def _pad(x, crop_size):\n        h, w = x.size()[2:]\n        pad_h = max(crop_size - h, 0)\n        pad_w = max(crop_size - w, 0)\n        x = F.pad(x, (0, pad_w, 0, pad_h))\n        return x, pad_h, pad_w\n\n    def wrapper(self, x):\n        batch_size, _, ori_h, ori_w = x.size()\n        if self.training and self.use_aux:\n            outputs_all_scales = torch.zeros((batch_size, self.num_classes, ori_h, ori_w)).cuda()\n            aux_all_scales = torch.zeros((batch_size, self.num_classes, ori_h, ori_w)).cuda()\n            for s in self.scales:\n                new_size = (int(ori_h * s), int(ori_w * s))\n                scaled_x = F.interpolate(x, size=new_size, mode='bilinear')\n                scaled_x = scaled_x.cuda()\n                scaled_h, scaled_w = scaled_x.size()[2:]\n                long_size = max(scaled_h, scaled_w)\n                print(scaled_x.size())\n\n                if long_size > self.crop_size:\n                    count = torch.zeros((scaled_h, scaled_w))\n                    outputs = torch.zeros((batch_size, self.num_classes, scaled_h, scaled_w)).cuda()\n                    aux_outputs = torch.zeros((batch_size, self.num_classes, scaled_h, scaled_w)).cuda()\n                    stride = int(ceil(self.crop_size * self.stride_rate))\n                    h_step_num = int(ceil((scaled_h - self.crop_size) / stride)) + 1\n                    w_step_num = int(ceil((scaled_w - self.crop_size) / stride)) + 1\n                    for yy in range(h_step_num):\n                        for xx in range(w_step_num):\n                            sy, sx = yy * stride, xx * stride\n                            ey, ex = sy + self.crop_size, sx + self.crop_size\n                            x_sub = scaled_x[:, :, sy: ey, sx: ex]\n                            x_sub, pad_h, pad_w = _pad(x_sub, self.crop_size)\n                            print(x_sub.size())\n                            outputs_sub, aux_sub = single_forward(self, x_sub)\n\n                            if sy + self.crop_size > scaled_h:\n                                outputs_sub = outputs_sub[:, :, : -pad_h, :]\n                                aux_sub = aux_sub[:, :, : -pad_h, :]\n\n                            if sx + self.crop_size > scaled_w:\n                                outputs_sub = outputs_sub[:, :, :, : -pad_w]\n                                aux_sub = aux_sub[:, :, :, : -pad_w]\n\n                            outputs[:, :, sy: ey, sx: ex] = outputs_sub\n                            aux_outputs[:, :, sy: ey, sx: ex] = aux_sub\n\n                            count[sy: ey, sx: ex] += 1\n                    count = count.cuda()\n                    outputs = (outputs / count)\n                    aux_outputs = (outputs / count)\n                else:\n                    scaled_x, pad_h, pad_w = _pad(scaled_x, self.crop_size)\n                    outputs, aux_outputs = single_forward(self, scaled_x)\n                    outputs = outputs[:, :, : -pad_h, : -pad_w]\n                    aux_outputs = aux_outputs[:, :, : -pad_h, : -pad_w]\n                outputs_all_scales += outputs\n                aux_all_scales += aux_outputs\n            return outputs_all_scales / len(self.scales), aux_all_scales\n        else:\n            outputs_all_scales = torch.zeros((batch_size, self.num_classes, ori_h, ori_w)).cuda()\n            for s in self.scales:\n                new_size = (int(ori_h * s), int(ori_w * s))\n                scaled_x = F.interpolate(x, size=new_size, mode='bilinear')\n                scaled_h, scaled_w = scaled_x.size()[2:]\n                long_size = max(scaled_h, scaled_w)\n\n                if long_size > self.crop_size:\n                    count = torch.zeros((scaled_h, scaled_w))\n                    outputs = torch.zeros((batch_size, self.num_classes, scaled_h, scaled_w)).cuda()\n                    stride = int(ceil(self.crop_size * self.stride_rate))\n                    h_step_num = int(ceil((scaled_h - self.crop_size) / stride)) + 1\n                    w_step_num = int(ceil((scaled_w - self.crop_size) / stride)) + 1\n                    for yy in range(h_step_num):\n                        for xx in range(w_step_num):\n                            sy, sx = yy * stride, xx * stride\n                            ey, ex = sy + self.crop_size, sx + self.crop_size\n                            x_sub = scaled_x[:, :, sy: ey, sx: ex]\n                            x_sub, pad_h, pad_w = _pad(x_sub, self.crop_size)\n\n                            outputs_sub = single_forward(self, x_sub)\n\n                            if sy + self.crop_size > scaled_h:\n                                outputs_sub = outputs_sub[:, :, : -pad_h, :]\n\n                            if sx + self.crop_size > scaled_w:\n                                outputs_sub = outputs_sub[:, :, :, : -pad_w]\n\n                            outputs[:, :, sy: ey, sx: ex] = outputs_sub\n\n                            count[sy: ey, sx: ex] += 1\n                    count = count.cuda()\n                    outputs = (outputs / count)\n                else:\n                    scaled_x, pad_h, pad_w = _pad(scaled_x, self.crop_size)\n                    outputs = single_forward(self, scaled_x)\n                    outputs = outputs[:, :, : -pad_h, : -pad_w]\n                outputs_all_scales += outputs\n            return outputs_all_scales\n\n    return wrapper\n"""
pywick/models/segmentation/frrn1.py,6,"b'# Source: https://github.com/meetshah1995/pytorch-semseg (MIT)\n\n""""""\nImplementation of `Full Resolution Residual Networks for Semantic Segmentation in Street Scenes <https://arxiv.org/abs/1611.08323>`_\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n__all__ = [\'frrn\']\n\nfrrn_specs_dic = {\n    ""A"": {\n        ""encoder"": [[3, 96, 2], [4, 192, 4], [2, 384, 8], [2, 384, 16]],\n        ""decoder"": [[2, 192, 8], [2, 192, 4], [2, 96, 2]],\n    },\n    ""B"": {\n        ""encoder"": [[3, 96, 2], [4, 192, 4], [2, 384, 8], [2, 384, 16], [2, 384, 32]],\n        ""decoder"": [[2, 192, 16], [2, 192, 8], [2, 192, 4], [2, 96, 2]],\n    },\n}\n\nclass conv2DBatchNorm(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        n_filters,\n        k_size,\n        stride,\n        padding,\n        bias=True,\n        dilation=1,\n        with_bn=True,\n    ):\n        super(conv2DBatchNorm, self).__init__()\n\n        if dilation > 1:\n            conv_mod = nn.Conv2d(\n                int(in_channels),\n                int(n_filters),\n                kernel_size=k_size,\n                padding=padding,\n                stride=stride,\n                bias=bias,\n                dilation=dilation,\n            )\n\n        else:\n            conv_mod = nn.Conv2d(\n                int(in_channels),\n                int(n_filters),\n                kernel_size=k_size,\n                padding=padding,\n                stride=stride,\n                bias=bias,\n                dilation=1,\n            )\n\n        if with_bn:\n            self.cb_unit = nn.Sequential(conv_mod, nn.BatchNorm2d(int(n_filters)))\n        else:\n            self.cb_unit = nn.Sequential(conv_mod)\n\n    def forward(self, inputs):\n        outputs = self.cb_unit(inputs)\n        return outputs\n\n\nclass deconv2DBatchNorm(nn.Module):\n    def __init__(self, in_channels, n_filters, k_size, stride, padding, bias=True):\n        super(deconv2DBatchNorm, self).__init__()\n\n        self.dcb_unit = nn.Sequential(\n            nn.ConvTranspose2d(\n                int(in_channels),\n                int(n_filters),\n                kernel_size=k_size,\n                padding=padding,\n                stride=stride,\n                bias=bias,\n            ),\n            nn.BatchNorm2d(int(n_filters)),\n        )\n\n    def forward(self, inputs):\n        outputs = self.dcb_unit(inputs)\n        return outputs\n\n\nclass conv2DBatchNormRelu(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        n_filters,\n        k_size,\n        stride,\n        padding,\n        bias=True,\n        dilation=1,\n        with_bn=True,\n    ):\n        super(conv2DBatchNormRelu, self).__init__()\n\n        if dilation > 1:\n            conv_mod = nn.Conv2d(\n                int(in_channels),\n                int(n_filters),\n                kernel_size=k_size,\n                padding=padding,\n                stride=stride,\n                bias=bias,\n                dilation=dilation,\n            )\n\n        else:\n            conv_mod = nn.Conv2d(\n                int(in_channels),\n                int(n_filters),\n                kernel_size=k_size,\n                padding=padding,\n                stride=stride,\n                bias=bias,\n                dilation=1,\n            )\n\n        if with_bn:\n            self.cbr_unit = nn.Sequential(\n                conv_mod, nn.BatchNorm2d(int(n_filters)), nn.ReLU(inplace=True)\n            )\n        else:\n            self.cbr_unit = nn.Sequential(conv_mod, nn.ReLU(inplace=True))\n\n    def forward(self, inputs):\n        outputs = self.cbr_unit(inputs)\n        return outputs\n\n\nclass deconv2DBatchNormRelu(nn.Module):\n    def __init__(self, in_channels, n_filters, k_size, stride, padding, bias=True):\n        super(deconv2DBatchNormRelu, self).__init__()\n\n        self.dcbr_unit = nn.Sequential(\n            nn.ConvTranspose2d(\n                int(in_channels),\n                int(n_filters),\n                kernel_size=k_size,\n                padding=padding,\n                stride=stride,\n                bias=bias,\n            ),\n            nn.BatchNorm2d(int(n_filters)),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, inputs):\n        outputs = self.dcbr_unit(inputs)\n        return outputs\n\n\nclass RU(nn.Module):\n    """"""\n    Residual Unit for FRRN\n    """"""\n\n    def __init__(self, channels, kernel_size=3, strides=1):\n        super(RU, self).__init__()\n\n        self.conv1 = conv2DBatchNormRelu(\n            channels, channels, k_size=kernel_size, stride=strides, padding=1\n        )\n        self.conv2 = conv2DBatchNorm(\n            channels, channels, k_size=kernel_size, stride=strides, padding=1\n        )\n\n    def forward(self, x):\n        incoming = x\n        x = self.conv1(x)\n        x = self.conv2(x)\n        return x + incoming\n\n\nclass FRRU(nn.Module):\n    """"""\n    Full Resolution Residual Unit for FRRN\n    """"""\n\n    def __init__(self, prev_channels, out_channels, scale):\n        super(FRRU, self).__init__()\n        self.scale = scale\n        self.prev_channels = prev_channels\n        self.out_channels = out_channels\n\n        self.conv1 = conv2DBatchNormRelu(\n            prev_channels + 32, out_channels, k_size=3, stride=1, padding=1\n        )\n        self.conv2 = conv2DBatchNormRelu(\n            out_channels, out_channels, k_size=3, stride=1, padding=1\n        )\n        self.conv_res = nn.Conv2d(out_channels, 32, kernel_size=1, stride=1, padding=0)\n\n    def forward(self, y, z):\n        x = torch.cat([y, nn.MaxPool2d(self.scale, self.scale)(z)], dim=1)\n        y_prime = self.conv1(x)\n        y_prime = self.conv2(y_prime)\n\n        x = self.conv_res(y_prime)\n        upsample_size = torch.Size([_s * self.scale for _s in y_prime.shape[-2:]])\n        x = F.upsample(x, size=upsample_size, mode=""nearest"")\n        z_prime = z + x\n\n        return y_prime, z_prime\n\n\nclass frrn(nn.Module):\n    """"""\n    Full Resolution Residual Networks for Semantic Segmentation\n    URL: https://arxiv.org/abs/1611.08323\n\n    References:\n    1) Original Author\'s code: https://github.com/TobyPDE/FRRN\n    2) TF implementation by @kiwonjoon: https://github.com/hiwonjoon/tf-frrn\n    """"""\n\n    def __init__(self, num_classes=21, model_type=None, **kwargs):\n        super(frrn, self).__init__()\n        self.n_classes = num_classes\n        self.model_type = model_type\n        self.K = 64 * 512\n\n        self.conv1 = conv2DBatchNormRelu(3, 48, 5, 1, 2)\n\n        self.up_residual_units = []\n        self.down_residual_units = []\n        for i in range(3):\n            self.up_residual_units.append(RU(channels=48, kernel_size=3, strides=1))\n            self.down_residual_units.append(RU(channels=48, kernel_size=3, strides=1))\n\n        self.up_residual_units = nn.ModuleList(self.up_residual_units)\n        self.down_residual_units = nn.ModuleList(self.down_residual_units)\n\n        self.split_conv = nn.Conv2d(\n            48, 32, kernel_size=1, padding=0, stride=1, bias=True\n        )\n\n        # each spec is as (n_blocks, channels, scale)\n        self.encoder_frru_specs = frrn_specs_dic[self.model_type][""encoder""]\n\n        self.decoder_frru_specs = frrn_specs_dic[self.model_type][""decoder""]\n\n        # encoding\n        prev_channels = 48\n        self.encoding_frrus = {}\n        for n_blocks, channels, scale in self.encoder_frru_specs:\n            for block in range(n_blocks):\n                key = ""_"".join(\n                    map(str, [""encoding_frru"", n_blocks, channels, scale, block])\n                )\n                setattr(\n                    self,\n                    key,\n                    FRRU(\n                        prev_channels=prev_channels, out_channels=channels, scale=scale\n                    ),\n                )\n            prev_channels = channels\n\n        # decoding\n        self.decoding_frrus = {}\n        for n_blocks, channels, scale in self.decoder_frru_specs:\n            # pass through decoding FRRUs\n            for block in range(n_blocks):\n                key = ""_"".join(\n                    map(str, [""decoding_frru"", n_blocks, channels, scale, block])\n                )\n                setattr(\n                    self,\n                    key,\n                    FRRU(\n                        prev_channels=prev_channels, out_channels=channels, scale=scale\n                    ),\n                )\n            prev_channels = channels\n\n        self.merge_conv = nn.Conv2d(\n            prev_channels + 32, 48, kernel_size=1, padding=0, stride=1, bias=True\n        )\n\n        self.classif_conv = nn.Conv2d(\n            48, self.n_classes, kernel_size=1, padding=0, stride=1, bias=True\n        )\n\n    def forward(self, x):\n\n        # pass to initial conv\n        x = self.conv1(x)\n\n        # pass through residual units\n        for i in range(3):\n            x = self.up_residual_units[i](x)\n\n        # divide stream\n        y = x\n        z = self.split_conv(x)\n\n        prev_channels = 48\n        # encoding\n        for n_blocks, channels, scale in self.encoder_frru_specs:\n            # maxpool bigger feature map\n            y_pooled = F.max_pool2d(y, stride=2, kernel_size=2, padding=0)\n            # pass through encoding FRRUs\n            for block in range(n_blocks):\n                key = ""_"".join(\n                    map(str, [""encoding_frru"", n_blocks, channels, scale, block])\n                )\n                y, z = getattr(self, key)(y_pooled, z)\n            prev_channels = channels\n\n        # decoding\n        for n_blocks, channels, scale in self.decoder_frru_specs:\n            # bilinear upsample smaller feature map\n            upsample_size = torch.Size([_s * 2 for _s in y.size()[-2:]])\n            y_upsampled = F.upsample(y, size=upsample_size, mode=""bilinear"")\n            # pass through decoding FRRUs\n            for block in range(n_blocks):\n                key = ""_"".join(\n                    map(str, [""decoding_frru"", n_blocks, channels, scale, block])\n                )\n                # print(""Incoming FRRU Size: "", key, y_upsampled.shape, z.shape)\n                y, z = getattr(self, key)(y_upsampled, z)\n                # print(""Outgoing FRRU Size: "", key, y.shape, z.shape)\n            prev_channels = channels\n\n        # merge streams\n        x = torch.cat([F.upsample(y, scale_factor=2, mode=""bilinear""), z], dim=1)\n        x = self.merge_conv(x)\n\n        # pass through residual units\n        for i in range(3):\n            x = self.down_residual_units[i](x)\n\n        # final 1x1 conv to get classification\n        x = self.classif_conv(x)\n\n        return x\n'"
pywick/models/segmentation/fusionnet.py,3,"b'# Source: https://github.com/saeedizadi/binseg_pytoch (Apache-2.0)\n\n""""""\nImplementation of `FusionNet: A deep fully residual convolutional neural network for image segmentation in connectomics <https://arxiv.org/abs/1612.05360>`_\n""""""\n\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\nimport numpy as np\n\n__all__ = [\'FusionNet\']\n\ndef initialize_weights(method=\'kaiming\', *models):\n    for model in models:\n        for module in model.modules():\n\n            if isinstance(module, nn.Conv2d) or isinstance(module, nn.ConvTranspose2d) or isinstance(module, nn.Linear):\n                if method == \'kaiming\':\n                    init.kaiming_normal_(module.weight.data, np.sqrt(2.0))\n                elif method == \'xavier\':\n                    init.xavier_normal(module.weight.data, np.sqrt(2.0))\n                elif method == \'orthogonal\':\n                    init.orthogonal(module.weight.data, np.sqrt(2.0))\n                elif method == \'normal\':\n                    init.normal(module.weight.data,mean=0, std=0.02)\n                if module.bias is not None:\n                    init.constant(module.bias.data,0)\n\nclass ResBlock(nn.Module):\n    def __init__(self, in_channels):\n        super(ResBlock, self).__init__()\n\n        self.layer = nn.Sequential(nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),\n                                   nn.ReLU(inplace=True),\n                                   # nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),\n                                   # nn.ReLU(inplace=True),\n                                   nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1))\n\n    def forward(self,x):\n        conv = self.layer(x)\n        # The last relu must be applied after the sumation\n        return F.relu(x.expand_as(conv)+ conv)\n\nclass ConvResConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n        super(ConvResConv, self).__init__()\n\n        # Note that the block do not return ReLU version of the output. Reason: ReLU should take place after summation\n        self.layer = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding),\n                                   nn.ReLU(inplace=True),\n                                   ResBlock(out_channels),\n                                   nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n\n    def forward(self,x):\n        return self.layer(x)\n\n\nclass DeconvBN(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DeconvBN, self).__init__()\n        self.layer = nn.Sequential(nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2))\n\n    def forward(self,x):\n        return self.layer(x)\n\nclass FusionNet(nn.Module):\n    def __init__(self, num_classes, **kwargs):\n        super(FusionNet, self).__init__()\n\n        #Assuming input of size 240x320\n        self.enc1 = ConvResConv(3, 64)\n        self.enc2 = ConvResConv(64, 128)\n        self.enc3 = ConvResConv(128, 256)\n        self.enc4 = ConvResConv(256, 512)\n\n        self.middle = ConvResConv(512, 1024)\n\n        self.dec1 = ConvResConv(512, 512)\n        self.dec2 = ConvResConv(256, 256)\n        self.dec3 = ConvResConv(128, 128)\n        self.dec4 = nn.Sequential(nn.Conv2d(64, 64, 3, padding=1))\n\n        self.deconvbn1024_512 = DeconvBN(1024,512)\n        self.deconvbn512_256 = DeconvBN(512, 256)\n        self.deconvbn256_128 = DeconvBN(256, 128)\n        self.deconvbn128_64 = DeconvBN(128, 64)\n\n        self.final = nn.Conv2d(64, num_classes, kernel_size=1, stride=1)\n        self.activation = nn.Sigmoid()\n        initialize_weights(self)\n\n\n    def forward(self,x):\n\n        enc1 = self.enc1(x) ## 240x320x64 --> No Relu\n        enc2 = self.enc2(self._do_downsample(F.relu(enc1))) ## 120x160x128 --> No relu\n        enc3 = self.enc3(self._do_downsample(F.relu(enc2))) ## 60x80x256 -->\n        enc4 = self.enc4(self._do_downsample(F.relu(enc3))) ## 30x40x512 --> conv4\n        middle = self.deconvbn1024_512(self.middle(self._do_downsample(F.relu(enc4)))) ## 30x40x512 --> no relu\n\n\n        dec1 = self.deconvbn512_256(self.dec1(F.relu(middle+enc4))) ## 60x80x256\n        dec2 = self.deconvbn256_128(self.dec2(F.relu(dec1 + enc3)))  ## 120x160x128\n        dec3 = self.deconvbn128_64(self.dec3(F.relu(dec2 + enc2)))  ## 240x320x64\n        dec4 = self.dec4(F.relu(dec3 + enc1))  ## 240x320x64\n\n        output = self.final(dec4)\n        return self.activation(output)\n\n    def _do_downsample(self, x, kernel_size=2, stride=2):\n        return F.max_pool2d(x, kernel_size=kernel_size, stride=stride)'"
pywick/models/segmentation/lex_extractors.py,14,"b'from collections import OrderedDict\nimport math\nimport re\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils import model_zoo\nfrom torchvision.models.densenet import DenseNet as Orig_DenseNet\nfrom torchvision.models.squeezenet import squeezenet1_1\n\n\ndef load_weights_sequential(target, source_state):\n    new_dict = OrderedDict()\n    for (k1, v1), (k2, v2) in zip(target.state_dict().items(), source_state.items()):\n        new_dict[k1] = v2\n    target.load_state_dict(new_dict)\n\n\'\'\'\n    Implementation of dilated ResNet-101 with deep supervision. Downsampling is changed to 8x\n\'\'\'\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n    \'densenet121\': \'https://download.pytorch.org/models/densenet121-a639ec97.pth\',\n    \'densenet169\': \'https://download.pytorch.org/models/densenet169-b2777c0a.pth\',\n    \'densenet201\': \'https://download.pytorch.org/models/densenet201-c1103571.pth\',\n    \'densenet161\': \'https://download.pytorch.org/models/densenet161-8d451a50.pth\'\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1, dilation=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, dilation=dilation, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride=stride, dilation=dilation)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes, stride=1, dilation=dilation)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, dilation=dilation,\n                               padding=dilation, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers=(3, 4, 23, 3)):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = [block(self.inplanes, planes, stride, downsample)]\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dilation=dilation))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x_3 = self.layer3(x)\n        x = self.layer4(x_3)\n\n        return x, x_3\n\n\n\'\'\'\n    Implementation of DenseNet with deep supervision. Downsampling is changed to 8x \n\'\'\'\n\ndef densenet121(pretrained=False, **kwargs):\n    r""""""Densenet-121 model from\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = Orig_DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 24, 16), **kwargs)\n    if pretrained:\n        # \'.\'s are no longer allowed in module names, but pervious _DenseLayer\n        # has keys \'norm.1\', \'relu.1\', \'conv.1\', \'norm.2\', \'relu.2\', \'conv.2\'.\n        # They are also in the checkpoints in model_urls. This pattern is used\n        # to find such keys.\n        pattern = re.compile(\n            r\'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$\')\n        state_dict = model_zoo.load_url(model_urls[\'densenet121\'])\n        for key in list(state_dict.keys()):\n            res = pattern.match(key)\n            if res:\n                new_key = res.group(1) + res.group(2)\n                state_dict[new_key] = state_dict[key]\n                del state_dict[key]\n        model.load_state_dict(state_dict)\n    return model\n\n\nclass _DenseLayer(nn.Sequential):\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n        super(_DenseLayer, self).__init__()\n        self.add_module(\'norm1\', nn.BatchNorm2d(num_input_features)),\n        self.add_module(\'relu1\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv1\', nn.Conv2d(num_input_features, bn_size *\n                                            growth_rate, kernel_size=1, stride=1, bias=False)),\n        self.add_module(\'norm2\', nn.BatchNorm2d(bn_size * growth_rate)),\n        self.add_module(\'relu2\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv2\', nn.Conv2d(bn_size * growth_rate, growth_rate,\n                                            kernel_size=3, stride=1, padding=1, bias=False)),\n        self.drop_rate = drop_rate\n\n    def forward(self, x):\n        new_features = super(_DenseLayer, self).forward(x)\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n        return torch.cat([x, new_features], 1)\n\n\nclass _DenseBlock(nn.Sequential):\n    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n        super(_DenseBlock, self).__init__()\n        for i in range(num_layers):\n            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)\n            self.add_module(\'denselayer%d\' % (i + 1), layer)\n\n\nclass _Transition(nn.Sequential):\n    def __init__(self, num_input_features, num_output_features, downsample=True):\n        super(_Transition, self).__init__()\n        self.add_module(\'norm\', nn.BatchNorm2d(num_input_features))\n        self.add_module(\'relu\', nn.ReLU(inplace=True))\n        self.add_module(\'conv\', nn.Conv2d(num_input_features, num_output_features,\n                                          kernel_size=1, stride=1, bias=False))\n        if downsample:\n            self.add_module(\'pool\', nn.AvgPool2d(kernel_size=2, stride=2))\n        else:\n            self.add_module(\'pool\', nn.AvgPool2d(kernel_size=1, stride=1))  # compatibility hack\n\n\nclass DenseNet(nn.Module):\n    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n                 num_init_features=64, bn_size=4, drop_rate=0, pretrained=True):\n\n        super(DenseNet, self).__init__()\n\n        # First convolution\n        self.start_features = nn.Sequential(OrderedDict([\n            (\'conv0\', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n            (\'norm0\', nn.BatchNorm2d(num_init_features)),\n            (\'relu0\', nn.ReLU(inplace=True)),\n            (\'pool0\', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n        ]))\n\n        # Each denseblock\n        num_features = num_init_features\n\n        init_weights = list(densenet121(pretrained=True).features.children())\n        start = 0\n        for i, c in enumerate(self.start_features.children()):\n            if pretrained:\n                c.load_state_dict(init_weights[i].state_dict())\n            start += 1\n        self.blocks = nn.ModuleList()\n        for i, num_layers in enumerate(block_config):\n            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n            if pretrained:\n                block.load_state_dict(init_weights[start].state_dict())\n            start += 1\n            self.blocks.append(block)\n            setattr(self, \'denseblock%d\' % (i + 1), block)\n\n            num_features = num_features + num_layers * growth_rate\n            if i != len(block_config) - 1:\n                downsample = i < 1\n                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2,\n                                    downsample=downsample)\n                if pretrained:\n                    trans.load_state_dict(init_weights[start].state_dict())\n                start += 1\n                self.blocks.append(trans)\n                setattr(self, \'transition%d\' % (i + 1), trans)\n                num_features = num_features // 2\n\n    def forward(self, x):\n        out = self.start_features(x)\n        deep_features = None\n        for i, block in enumerate(self.blocks):\n            out = block(out)\n            if i == 5:\n                deep_features = out\n\n        return out, deep_features\n\n\nclass Fire(nn.Module):\n\n    def __init__(self, inplanes, squeeze_planes,\n                 expand1x1_planes, expand3x3_planes, dilation=1):\n        super(Fire, self).__init__()\n        self.inplanes = inplanes\n        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n                                   kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n                                   kernel_size=3, padding=dilation, dilation=dilation)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\n\nclass SqueezeNet(nn.Module):\n\n    def __init__(self, pretrained=False):\n        super(SqueezeNet, self).__init__()\n\n        self.feat_1 = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(inplace=True)\n        )\n        self.feat_2 = nn.Sequential(\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n            Fire(64, 16, 64, 64),\n            Fire(128, 16, 64, 64)\n        )\n        self.feat_3 = nn.Sequential(\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n            Fire(128, 32, 128, 128, 2),\n            Fire(256, 32, 128, 128, 2)\n        )\n        self.feat_4 = nn.Sequential(\n            Fire(256, 48, 192, 192, 4),\n            Fire(384, 48, 192, 192, 4),\n            Fire(384, 64, 256, 256, 4),\n            Fire(512, 64, 256, 256, 4)\n        )\n        if pretrained:\n            weights = squeezenet1_1(pretrained=True).features.state_dict()\n            load_weights_sequential(self, weights)\n\n    def forward(self, x):\n        f1 = self.feat_1(x)\n        f2 = self.feat_2(f1)\n        f3 = self.feat_3(f2)\n        f4 = self.feat_4(f3)\n        return f4, f3\n\n\n\'\'\'\n    Handy methods for construction\n\'\'\'\n\n\ndef squeezenet(pretrained=True):\n    return SqueezeNet(pretrained)\n\n\ndef densenet(pretrained=True):\n    return DenseNet(pretrained=pretrained)\n\n\ndef resnet18(pretrained=True):\n    model = ResNet(BasicBlock, [2, 2, 2, 2])\n    if pretrained:\n        load_weights_sequential(model, model_zoo.load_url(model_urls[\'resnet18\']))\n    return model\n\n\ndef resnet34(pretrained=True):\n    model = ResNet(BasicBlock, [3, 4, 6, 3])\n    if pretrained:\n        load_weights_sequential(model, model_zoo.load_url(model_urls[\'resnet34\']))\n    return model\n\n\ndef resnet50(pretrained=True):\n    model = ResNet(Bottleneck, [3, 4, 6, 3])\n    if pretrained:\n        load_weights_sequential(model, model_zoo.load_url(model_urls[\'resnet50\']))\n    return model\n\n\ndef resnet101(pretrained=True):\n    model = ResNet(Bottleneck, [3, 4, 23, 3])\n    if pretrained:\n        load_weights_sequential(model, model_zoo.load_url(model_urls[\'resnet101\']))\n    return model\n\n\ndef resnet152(pretrained=True):\n    model = ResNet(Bottleneck, [3, 8, 36, 3])\n    if pretrained:\n        load_weights_sequential(model, model_zoo.load_url(model_urls[\'resnet152\']))\n    return model\n'"
pywick/models/segmentation/lexpsp.py,1,"b'# Source: https://github.com/Lextal/pspnet-pytorch\n\n""""""\nImplementation of `Pyramid Scene Parsing Network <https://arxiv.org/pdf/1612.01105>`_\n""""""\n\nfrom .lex_extractors import *\n\n__all__ = [\'PSPNet\']\n\nextractor_models = {\n    \'resnet18\': resnet18,\n    \'resnet34\': resnet34,\n    \'resnet50\': resnet50,\n    \'resnet101\': resnet101,\n    \'resnet152\': resnet152,\n    \'densenet121\': densenet\n}\n\nclass PSPModule(nn.Module):\n    def __init__(self, features, out_features=1024, sizes=(1, 2, 3, 6)):\n        super().__init__()\n        self.stages = []\n        self.stages = nn.ModuleList([self._make_stage(features, size) for size in sizes])\n        self.bottleneck = nn.Conv2d(features * (len(sizes) + 1), out_features, kernel_size=1)\n        self.relu = nn.ReLU()\n\n    def _make_stage(self, features, size):\n        prior = nn.AdaptiveAvgPool2d(output_size=(size, size))\n        conv = nn.Conv2d(features, features, kernel_size=1, bias=False)\n        return nn.Sequential(prior, conv)\n\n    def forward(self, feats):\n        h, w = feats.size(2), feats.size(3)\n        priors = [F.upsample(input=stage(feats), size=(h, w), mode=\'bilinear\') for stage in self.stages] + [feats]\n        bottle = self.bottleneck(torch.cat(priors, 1))\n        return self.relu(bottle)\n\n\nclass PSPUpsample(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.PReLU()\n        )\n\n    def forward(self, x):\n        h, w = 2 * x.size(2), 2 * x.size(3)\n        p = F.upsample(input=x, size=(h, w), mode=\'bilinear\')\n        return self.conv(p)\n\n\nclass PSPNet(nn.Module):\n    def __init__(self, num_classes=18, pretrained=True, backend=\'densenet121\', sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024, **kwargs):\n        super().__init__()\n        self.feats = extractor_models[backend](pretrained=pretrained)\n        self.psp = PSPModule(psp_size, 1024, sizes)\n        self.drop_1 = nn.Dropout2d(p=0.3)\n\n        self.up_1 = PSPUpsample(1024, 256)\n        self.up_2 = PSPUpsample(256, 64)\n        self.up_3 = PSPUpsample(64, 64)\n\n        self.drop_2 = nn.Dropout2d(p=0.15)\n        self.final = nn.Conv2d(64, num_classes, kernel_size=1)\n        # self.final = nn.Sequential(\n        #     nn.Conv2d(64, num_classes, kernel_size=1),\n        #     nn.LogSoftmax()\n        # )\n\n        self.classifier = nn.Sequential(\n            nn.Linear(deep_features_size, 256),\n            nn.ReLU(),\n            nn.Linear(256, num_classes)\n        )\n\n    def forward(self, x):\n        f, class_f = self.feats(x)\n        p = self.psp(f)\n        p = self.drop_1(p)\n\n        p = self.up_1(p)\n        p = self.drop_2(p)\n\n        p = self.up_2(p)\n        p = self.drop_2(p)\n\n        p = self.up_3(p)\n        p = self.drop_2(p)\n\n        # auxiliary = F.adaptive_max_pool2d(input=class_f, output_size=(1, 1)).view(-1, class_f.size(1))\n\n        return self.final(p)  #, self.classifier(auxiliary)\n'"
pywick/models/segmentation/ocnet.py,12,"b'# Source: https://github.com/Tramac/awesome-semantic-segmentation-pytorch/blob/master/core/models/ocnet.py (License: Apache 2.0)\n\n""""""\nImplementation of `OCNet: Object Context Network for Scene Parsing <https://arxiv.org/pdf/1809.00916>`_\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom pywick.models.segmentation.da_basenets.segbase import SegBaseModel\nfrom pywick.models.segmentation.da_basenets.fcn import _FCNHead\n\n__all__ = [\'OCNet\', \'OCNet_Base_Resnet101\', \'OCNet_Pyramid_Resnet101\', \'OCNet_ASP_Resnet101\', \'OCNet_Base_Resnet152\', \'OCNet_Pyramid_Resnet152\', \'OCNet_ASP_Resnet152\']\n\n\nclass OCNet(SegBaseModel):\n    r""""""OCNet\n\n    Parameters\n    ----------\n    nclass : int\n        Number of categories for the training dataset.\n    backbone : string\n        Pre-trained dilated backbone network type (default:\'resnet50\'; \'resnet50\',\n        \'resnet101\' or \'resnet152\').\n    norm_layer : object\n        Normalization layer used in backbone network (default: :class:`nn.BatchNorm`;\n        for Synchronized Cross-GPU BachNormalization).\n    aux : bool\n        Auxiliary loss.\n    Reference:\n        Yuhui Yuan, Jingdong Wang. ""OCNet: Object Context Network for Scene Parsing.""\n        arXiv preprint arXiv:1809.00916 (2018).\n    """"""\n\n    def __init__(self, num_classes, pretrained=True, backbone=\'resnet101\', oc_arch=\'base\', aux=False, **kwargs):\n        super(OCNet, self).__init__(num_classes, pretrained=pretrained, aux=aux, backbone=backbone, **kwargs)\n        self.head = _OCHead(num_classes, oc_arch, **kwargs)\n        if self.aux:\n            self.auxlayer = _FCNHead(1024, num_classes, **kwargs)\n\n        self.__setattr__(\'exclusive\', [\'head\', \'auxlayer\'] if aux else [\'head\'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        _, _, c3, c4 = self.base_forward(x)\n        outputs = []\n        x = self.head(c4)\n        x = F.interpolate(x, size, mode=\'bilinear\', align_corners=True)\n        outputs.append(x)\n\n        if self.aux:\n            auxout = self.auxlayer(c3)\n            auxout = F.interpolate(auxout, size, mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout)\n            return tuple(outputs)\n        else:\n            return outputs[0]\n\n\nclass _OCHead(nn.Module):\n    def __init__(self, nclass, oc_arch, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(_OCHead, self).__init__()\n        if oc_arch == \'base\':\n            self.context = nn.Sequential(\n                nn.Conv2d(2048, 512, 3, 1, padding=1, bias=False),\n                norm_layer(512),\n                nn.ReLU(True),\n                BaseOCModule(512, 512, 256, 256, scales=([1]), norm_layer=norm_layer, **kwargs))\n        elif oc_arch == \'pyramid\':\n            self.context = nn.Sequential(\n                nn.Conv2d(2048, 512, 3, 1, padding=1, bias=False),\n                norm_layer(512),\n                nn.ReLU(True),\n                PyramidOCModule(512, 512, 256, 512, scales=([1, 2, 3, 6]), norm_layer=norm_layer, **kwargs))\n        elif oc_arch == \'asp\':\n            self.context = ASPOCModule(2048, 512, 256, 512, norm_layer=norm_layer, **kwargs)\n        else:\n            raise ValueError(""Unknown OC architecture!"")\n\n        self.out = nn.Conv2d(512, nclass, 1)\n\n    def forward(self, x):\n        x = self.context(x)\n        return self.out(x)\n\n\nclass BaseAttentionBlock(nn.Module):\n    """"""The basic implementation for self-attention block/non-local block.""""""\n\n    def __init__(self, in_channels, out_channels, key_channels, value_channels,\n                 scale=1, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(BaseAttentionBlock, self).__init__()\n        self.scale = scale\n        self.key_channels = key_channels\n        self.value_channels = value_channels\n        if scale > 1:\n            self.pool = nn.MaxPool2d(scale)\n\n        self.f_value = nn.Conv2d(in_channels, value_channels, 1)\n        self.f_key = nn.Sequential(\n            nn.Conv2d(in_channels, key_channels, 1),\n            norm_layer(key_channels),\n            nn.ReLU(True)\n        )\n        self.f_query = self.f_key\n        self.W = nn.Conv2d(value_channels, out_channels, 1)\n        nn.init.constant_(self.W.weight, 0)\n        nn.init.constant_(self.W.bias, 0)\n\n    def forward(self, x):\n        batch_size, c, w, h = x.size()\n        if self.scale > 1:\n            x = self.pool(x)\n\n        value = self.f_value(x).view(batch_size, self.value_channels, -1).permute(0, 2, 1)\n        query = self.f_query(x).view(batch_size, self.key_channels, -1).permute(0, 2, 1)\n        key = self.f_key(x).view(batch_size, self.key_channels, -1)\n\n        sim_map = torch.bmm(query, key) * (self.key_channels ** -.5)\n        sim_map = F.softmax(sim_map, dim=-1)\n\n        context = torch.bmm(sim_map, value).permute(0, 2, 1).contiguous()\n        context = context.view(batch_size, self.value_channels, *x.size()[2:])\n        context = self.W(context)\n        if self.scale > 1:\n            context = F.interpolate(context, size=(w, h), mode=\'bilinear\', align_corners=True)\n\n        return context\n\n\nclass BaseOCModule(nn.Module):\n    """"""Base-OC""""""\n\n    def __init__(self, in_channels, out_channels, key_channels, value_channels,\n                 scales=([1]), norm_layer=nn.BatchNorm2d, concat=True, **kwargs):\n        super(BaseOCModule, self).__init__()\n        self.stages = nn.ModuleList([\n            BaseAttentionBlock(in_channels, out_channels, key_channels, value_channels, scale, norm_layer, **kwargs)\n            for scale in scales])\n        in_channels = in_channels * 2 if concat else in_channels\n        self.project = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 1),\n            norm_layer(out_channels),\n            nn.ReLU(True),\n            nn.Dropout2d(0.05)\n        )\n        self.concat = concat\n\n    def forward(self, x):\n        priors = [stage(x) for stage in self.stages]\n        context = priors[0]\n        for i in range(1, len(priors)):\n            context += priors[i]\n        if self.concat:\n            context = torch.cat([context, x], 1)\n        out = self.project(context)\n        return out\n\n\nclass PyramidAttentionBlock(nn.Module):\n    """"""The basic implementation for pyramid self-attention block/non-local block""""""\n\n    def __init__(self, in_channels, out_channels, key_channels, value_channels,\n                 scale=1, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(PyramidAttentionBlock, self).__init__()\n        self.scale = scale\n        self.value_channels = value_channels\n        self.key_channels = key_channels\n\n        self.f_value = nn.Conv2d(in_channels, value_channels, 1)\n        self.f_key = nn.Sequential(\n            nn.Conv2d(in_channels, key_channels, 1),\n            norm_layer(key_channels),\n            nn.ReLU(True)\n        )\n        self.f_query = self.f_key\n        self.W = nn.Conv2d(value_channels, out_channels, 1)\n        nn.init.constant_(self.W.weight, 0)\n        nn.init.constant_(self.W.bias, 0)\n\n    def forward(self, x):\n        batch_size, c, w, h = x.size()\n\n        local_x = list()\n        local_y = list()\n        step_w, step_h = w // self.scale, h // self.scale\n        for i in range(self.scale):\n            for j in range(self.scale):\n                start_x, start_y = step_w * i, step_h * j\n                end_x, end_y = min(start_x + step_w, w), min(start_y + step_h, h)\n                if i == (self.scale - 1):\n                    end_x = w\n                if j == (self.scale - 1):\n                    end_y = h\n                local_x += [start_x, end_x]\n                local_y += [start_y, end_y]\n\n        value = self.f_value(x)\n        query = self.f_query(x)\n        key = self.f_key(x)\n\n        local_list = list()\n        local_block_cnt = (self.scale ** 2) * 2\n        for i in range(0, local_block_cnt, 2):\n            value_local = value[:, :, local_x[i]:local_x[i + 1], local_y[i]:local_y[i + 1]]\n            query_local = query[:, :, local_x[i]:local_x[i + 1], local_y[i]:local_y[i + 1]]\n            key_local = key[:, :, local_x[i]:local_x[i + 1], local_y[i]:local_y[i + 1]]\n\n            w_local, h_local = value_local.size(2), value_local.size(3)\n            value_local = value_local.contiguous().view(batch_size, self.value_channels, -1).permute(0, 2, 1)\n            query_local = query_local.contiguous().view(batch_size, self.key_channels, -1).permute(0, 2, 1)\n            key_local = key_local.contiguous().view(batch_size, self.key_channels, -1)\n\n            sim_map = torch.bmm(query_local, key_local) * (self.key_channels ** -.5)\n            sim_map = F.softmax(sim_map, dim=-1)\n\n            context_local = torch.bmm(sim_map, value_local).permute(0, 2, 1).contiguous()\n            context_local = context_local.view(batch_size, self.value_channels, w_local, h_local)\n            local_list.append(context_local)\n\n        context_list = list()\n        for i in range(0, self.scale):\n            row_tmp = list()\n            for j in range(self.scale):\n                row_tmp.append(local_list[j + i * self.scale])\n            context_list.append(torch.cat(row_tmp, 3))\n\n        context = torch.cat(context_list, 2)\n        context = self.W(context)\n\n        return context\n\n\nclass PyramidOCModule(nn.Module):\n    """"""Pyramid-OC""""""\n\n    def __init__(self, in_channels, out_channels, key_channels, value_channels,\n                 scales=([1]), norm_layer=nn.BatchNorm2d, **kwargs):\n        super(PyramidOCModule, self).__init__()\n        self.stages = nn.ModuleList([\n            PyramidAttentionBlock(in_channels, out_channels, key_channels, value_channels, scale, norm_layer, **kwargs)\n            for scale in scales])\n        self.up_dr = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels * len(scales), 1),\n            norm_layer(in_channels * len(scales)),\n            nn.ReLU(True)\n        )\n        self.project = nn.Sequential(\n            nn.Conv2d(in_channels * len(scales) * 2, out_channels, 1),\n            norm_layer(out_channels),\n            nn.ReLU(True),\n            nn.Dropout2d(0.05)\n        )\n\n    def forward(self, x):\n        priors = [stage(x) for stage in self.stages]\n        context = [self.up_dr(x)]\n        for i in range(len(priors)):\n            context += [priors[i]]\n        context = torch.cat(context, 1)\n        out = self.project(context)\n        return out\n\n\nclass ASPOCModule(nn.Module):\n    """"""ASP-OC""""""\n\n    def __init__(self, in_channels, out_channels, key_channels, value_channels,\n                 atrous_rates=(12, 24, 36), norm_layer=nn.BatchNorm2d, **kwargs):\n        super(ASPOCModule, self).__init__()\n        self.context = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n            norm_layer(out_channels),\n            nn.ReLU(True),\n            BaseOCModule(out_channels, out_channels, key_channels, value_channels, ([2]), norm_layer, False, **kwargs))\n\n        rate1, rate2, rate3 = tuple(atrous_rates)\n        self.b1 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, padding=rate1, dilation=rate1, bias=False),\n            norm_layer(out_channels),\n            nn.ReLU(True))\n        self.b2 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, padding=rate2, dilation=rate2, bias=False),\n            norm_layer(out_channels),\n            nn.ReLU(True))\n        self.b3 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, padding=rate3, dilation=rate3, bias=False),\n            norm_layer(out_channels),\n            nn.ReLU(True))\n        self.b4 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            norm_layer(out_channels),\n            nn.ReLU(True))\n\n        self.project = nn.Sequential(\n            nn.Conv2d(out_channels * 5, out_channels, 1, bias=False),\n            norm_layer(out_channels),\n            nn.ReLU(True),\n            nn.Dropout2d(0.1)\n        )\n\n    def forward(self, x):\n        feat1 = self.context(x)\n        feat2 = self.b1(x)\n        feat3 = self.b2(x)\n        feat4 = self.b3(x)\n        feat5 = self.b4(x)\n        out = torch.cat((feat1, feat2, feat3, feat4, feat5), dim=1)\n        out = self.project(out)\n        return out\n\n\ndef get_ocnet(num_classes=1, backbone=\'resnet50\', oc_arch=\'base\', pretrained=True, **kwargs):\n    model = OCNet(num_classes=num_classes, backbone=backbone, oc_arch=oc_arch, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef OCNet_Base_Resnet101(num_classes=1, **kwargs):\n    return get_ocnet(num_classes=num_classes, backbone=\'resnet101\', oc_arch=\'base\', **kwargs)\n\n\ndef OCNet_Pyramid_Resnet101(num_classes=1, **kwargs):\n    return get_ocnet(num_classes=num_classes, backbone=\'resnet101\', oc_arch=\'pyramid\', **kwargs)\n\n\ndef OCNet_ASP_Resnet101(num_classes=1, **kwargs):\n    return get_ocnet(num_classes=num_classes, backbone=\'resnet101\', oc_arch=\'asp\', **kwargs)\n\ndef OCNet_Base_Resnet152(num_classes=1, **kwargs):\n    return get_ocnet(num_classes=num_classes, backbone=\'resnet152\', oc_arch=\'base\', **kwargs)\n\n\ndef OCNet_Pyramid_Resnet152(num_classes=1, **kwargs):\n    return get_ocnet(num_classes=num_classes, backbone=\'resnet152\', oc_arch=\'pyramid\', **kwargs)\n\n\ndef OCNet_ASP_Resnet152(num_classes=1, **kwargs):\n    return get_ocnet(num_classes=num_classes, backbone=\'resnet152\', oc_arch=\'asp\', **kwargs)\n\n\nif __name__ == \'__main__\':\n    img = torch.randn(1, 3, 256, 256)\n    model = OCNet_ASP_Resnet101()\n    outputs = model(img)\n'"
pywick/models/segmentation/resnet_gcn.py,3,"b'# Another implementation of GCN\n# Source: https://github.com/saeedizadi/binseg_pytoch/tree/master/models (Apache-2.0)\n\n""""""\nImplementation of `Large Kernel Matters <https://arxiv.org/pdf/1703.02719>`_ with Resnet backend.\n""""""\n\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\nimport torchvision.models as models\nimport numpy as np\n\n__all__ = [\'ResnetGCN\']\n\ndef initialize_weights(method=\'kaiming\', *models):\n    for model in models:\n        for module in model.modules():\n\n            if isinstance(module, nn.Conv2d) or isinstance(module, nn.ConvTranspose2d) or isinstance(module, nn.Linear):\n                if method == \'kaiming\':\n                    init.kaiming_normal_(module.weight.data, np.sqrt(2.0))\n                elif method == \'xavier\':\n                    init.xavier_normal(module.weight.data, np.sqrt(2.0))\n                elif method == \'orthogonal\':\n                    init.orthogonal(module.weight.data, np.sqrt(2.0))\n                elif method == \'normal\':\n                    init.normal(module.weight.data,mean=0, std=0.02)\n                if module.bias is not None:\n                    init.constant(module.bias.data,0)\n\n\nclass GlobalConvolutionBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, k):\n\n        super(GlobalConvolutionBlock, self).__init__()\n        self.left = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=(k[0],1), padding=(k[0]//2,0)),\n                                  nn.Conv2d(out_channels, out_channels, kernel_size=(1,k[1]), padding=(0,k[1]//2)))\n\n        self.right = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=(1,k[1]), padding=(0,k[1]//2)),\n                                   nn.Conv2d(out_channels, out_channels, kernel_size=(k[0],1), padding=(k[0]//2,0)))\n\n\n    def forward(self,x):\n        left = self.left(x)\n        right = self.right(x)\n        return left + right\n\n\n\nclass BoundaryRefine(nn.Module):\n    def __init__(self, in_channels):\n        super(BoundaryRefine, self).__init__()\n        self.layer = nn.Sequential(nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),\n                                   nn.BatchNorm2d(in_channels),\n                                   nn.ReLU(inplace=True),\n                                   nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),\n                                   nn.BatchNorm2d(in_channels))\n\n    def forward(self,x):\n        convs = self.layer(x)\n        return x.expand_as(convs)+convs\n\nclass ResnetGCN(nn.Module):\n    def __init__(self, num_classes, pretrained=True, **kwargs):\n        super(ResnetGCN, self).__init__()\n\n        resent = models.resnet101(pretrained=pretrained)\n        self.layer0 = nn.Sequential(resent.conv1, resent.bn1, resent.relu, resent.maxpool)\n        self.layer1 = resent.layer1\n        self.layer2 = resent.layer2\n        self.layer3 = resent.layer3\n        self.layer4 = resent.layer4\n\n        #Assuming input of size 240x320\n        ks = 7\n        self.gcn256 = GlobalConvolutionBlock(256, num_classes, (59,79))\n        self.br256 = BoundaryRefine(num_classes)\n        self.gcn512 = GlobalConvolutionBlock(512, num_classes, (29,39))\n        self.br512 = BoundaryRefine(num_classes)\n        self.gcn1024 = GlobalConvolutionBlock(1024, num_classes, (13,19))\n        self.br1024 = BoundaryRefine(num_classes)\n        self.gcn2048 = GlobalConvolutionBlock(2048, num_classes, (7,9))\n        self.br2048 = BoundaryRefine(num_classes)\n\n\n        self.br1 = BoundaryRefine(num_classes)\n        self.br2 = BoundaryRefine(num_classes)\n        self.br3 = BoundaryRefine(num_classes)\n        self.br4 = BoundaryRefine(num_classes)\n        self.br5 = BoundaryRefine(num_classes)\n\n        self.activation = nn.Sigmoid()\n\n        self.deconv1 = nn.ConvTranspose2d(1,1,2,stride=2)\n        self.deconv2 = nn.ConvTranspose2d(1, 1, 2, stride=2)\n\n        initialize_weights(self.gcn256,self.gcn512,self.gcn1024, self.gcn2048,\n                           self.br5,self.br4,self.br3, self.br2, self.br1,\n                           self.br256, self.br512, self.br1024, self.br2048,\n                           self.deconv1, self.deconv2)\n\n    def forward(self,x):\n\n        # Assuming input of size 240x320\n\n        x = self.layer0(x) ## 120x160x64\n\n        layer1 = self.layer1(x) ## 60x80x256\n        layer2 = self.layer2(layer1) ## 30x40x512\n        layer3 = self.layer3(layer2) ## 15x 20x1024\n        layer4 = self.layer4(layer3) ## 7x10x2048\n\n        enc1 = self.br256(self.gcn256(layer1))\n        enc2 = self.br512(self.gcn512(layer2))\n        enc3 = self.br1024(self.gcn1024(layer3))\n        enc4 = self.br2048(self.gcn2048(layer4)) ## 8x10x1\n\n        dec1 = self.br1(F.interpolate(enc4, size=enc3.size()[2:], mode=\'bilinear\')+ enc3)\n        dec2 = self.br2(F.interpolate(dec1, enc2.size()[2:], mode=\'bilinear\') + enc2)\n        dec3 = self.br3(F.interpolate(dec2, enc1.size()[2:], mode=\'bilinear\') + enc1)\n        dec4 = self.br4(self.deconv1(dec3))\n\n        score_map = self.br5(self.deconv2(dec4))\n\n        return self.activation(score_map)\n\n\n\n    def _do_upsample(self, num_classes=1, kernel_size=2, stride=2):\n        return nn.ConvTranspose2d(num_classes, num_classes, kernel_size=kernel_size, stride=stride)\n'"
pywick/models/segmentation/seg_net.py,5,"b'# Source: https://github.com/zijundeng/pytorch-semantic-segmentation/tree/master/models (MIT)\n\n""""""\nImplementation of `Segnet: A deep convolutional encoder-decoder architecture for image segmentation <https://arxiv.org/pdf/1511.00561>`_\n""""""\n\nimport torch\nfrom torch import nn\nfrom torchvision import models\n\nfrom .fcn_utils import initialize_weights\nfrom .config import vgg19_bn_path\n\n__all__ = [\'SegNet\']\n\nclass _DecoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, num_conv_layers):\n        super(_DecoderBlock, self).__init__()\n        middle_channels = in_channels / 2\n        layers = [\n            nn.ConvTranspose2d(in_channels, in_channels, kernel_size=2, stride=2),\n            nn.Conv2d(in_channels, middle_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(middle_channels),\n            nn.ReLU(inplace=True)\n        ]\n        layers += [\n                      nn.Conv2d(middle_channels, middle_channels, kernel_size=3, padding=1),\n                      nn.BatchNorm2d(middle_channels),\n                      nn.ReLU(inplace=True),\n                  ] * (num_conv_layers - 2)\n        layers += [\n            nn.Conv2d(middle_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        ]\n        self.decode = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.decode(x)\n\n\nclass SegNet(nn.Module):\n    def __init__(self, num_classes, pretrained=True, **kwargs):\n        super(SegNet, self).__init__()\n        vgg = models.vgg19_bn()\n        if pretrained:\n            vgg.load_state_dict(torch.load(vgg19_bn_path))\n        features = list(vgg.features.children())\n        self.enc1 = nn.Sequential(*features[0:7])\n        self.enc2 = nn.Sequential(*features[7:14])\n        self.enc3 = nn.Sequential(*features[14:27])\n        self.enc4 = nn.Sequential(*features[27:40])\n        self.enc5 = nn.Sequential(*features[40:])\n\n        self.dec5 = nn.Sequential(\n            *([nn.ConvTranspose2d(512, 512, kernel_size=2, stride=2)] +\n              [nn.Conv2d(512, 512, kernel_size=3, padding=1),\n               nn.BatchNorm2d(512),\n               nn.ReLU(inplace=True)] * 4)\n        )\n        self.dec4 = _DecoderBlock(1024, 256, 4)\n        self.dec3 = _DecoderBlock(512, 128, 4)\n        self.dec2 = _DecoderBlock(256, 64, 2)\n        self.dec1 = _DecoderBlock(128, num_classes, 2)\n        initialize_weights(self.dec5, self.dec4, self.dec3, self.dec2, self.dec1)\n\n    def forward(self, x):\n        enc1 = self.enc1(x)\n        enc2 = self.enc2(enc1)\n        enc3 = self.enc3(enc2)\n        enc4 = self.enc4(enc3)\n        enc5 = self.enc5(enc4)\n\n        dec5 = self.dec5(enc5)\n        dec4 = self.dec4(torch.cat([enc4, dec5], 1))\n        dec3 = self.dec3(torch.cat([enc3, dec4], 1))\n        dec2 = self.dec2(torch.cat([enc2, dec3], 1))\n        dec1 = self.dec1(torch.cat([enc1, dec2], 1))\n        return dec1\n'"
pywick/models/segmentation/tiramisu.py,5,"b'# Source: https://github.com/bfortuner/pytorch_tiramisu/blob/master/models/tiramisu.py (MIT)\n\n""""""\nImplementation of `The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation <https://arxiv.org/pdf/1611.09326>`_\n""""""\n\nimport torch\nimport torch.nn as nn\n\n__all__ = [\'FCDenseNet\', \'Tiramisu57\', \'Tiramisu67\', \'Tiramisu103\']\n\nclass DenseLayer(nn.Sequential):\n    def __init__(self, in_channels, growth_rate):\n        super().__init__()\n        self.add_module(\'norm\', nn.BatchNorm2d(in_channels))\n        self.add_module(\'relu\', nn.ReLU(True))\n        self.add_module(\'conv\', nn.Conv2d(in_channels, growth_rate, kernel_size=3, stride=1, padding=1, bias=True))\n        self.add_module(\'drop\', nn.Dropout2d(0.2))\n\n    def forward(self, x):\n        return super().forward(x)\n\n\nclass DenseBlock(nn.Module):\n    def __init__(self, in_channels, growth_rate, n_layers, upsample=False):\n        super().__init__()\n        self.upsample = upsample\n        self.layers = nn.ModuleList([DenseLayer(\n            in_channels + i*growth_rate, growth_rate)\n            for i in range(n_layers)])\n\n    def forward(self, x):\n        if self.upsample:\n            new_features = []\n            #we pass all previous activations into each dense layer normally\n            #But we only store each dense layer\'s output in the new_features array\n            for layer in self.layers:\n                out = layer(x)\n                x = torch.cat([x, out], 1)\n                new_features.append(out)\n            return torch.cat(new_features,1)\n        else:\n            for layer in self.layers:\n                out = layer(x)\n                x = torch.cat([x, out], 1) # 1 = channel axis\n            return x\n\n\nclass TransitionDown(nn.Sequential):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.add_module(\'norm\', nn.BatchNorm2d(num_features=in_channels))\n        self.add_module(\'relu\', nn.ReLU(inplace=True))\n        self.add_module(\'conv\', nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0, bias=True))\n        self.add_module(\'drop\', nn.Dropout2d(0.2))\n        self.add_module(\'maxpool\', nn.MaxPool2d(2))\n\n    def forward(self, x):\n        return super().forward(x)\n\n\nclass TransitionUp(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.convTrans = nn.ConvTranspose2d( in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=2, padding=0, bias=True)\n\n    def forward(self, x, skip):\n        out = self.convTrans(x)\n        out = center_crop(out, skip.size(2), skip.size(3))\n        out = torch.cat([out, skip], 1)\n        return out\n\n\nclass Bottleneck(nn.Sequential):\n    def __init__(self, in_channels, growth_rate, n_layers):\n        super().__init__()\n        self.add_module(\'bottleneck\', DenseBlock(in_channels, growth_rate, n_layers, upsample=True))\n\n    def forward(self, x):\n        return super().forward(x)\n\n\ndef center_crop(layer, max_height, max_width):\n    _, _, h, w = layer.size()\n    xy1 = (w - max_width) // 2\n    xy2 = (h - max_height) // 2\n    return layer[:, :, xy2:(xy2 + max_height), xy1:(xy1 + max_width)]\n\n\n\nclass FCDenseNet(nn.Module):\n    def __init__(self, in_channels=3, down_blocks=(5,5,5,5,5),\n                 up_blocks=(5,5,5,5,5), bottleneck_layers=5,\n                 growth_rate=16, out_chans_first_conv=48, num_classes=12, **kwargs):\n        super().__init__()\n        self.down_blocks = down_blocks\n        self.up_blocks = up_blocks\n        cur_channels_count = 0\n        skip_connection_channel_counts = []\n\n        ## First Convolution ##\n\n        self.add_module(\'firstconv\', nn.Conv2d(in_channels=in_channels,\n                  out_channels=out_chans_first_conv, kernel_size=3,\n                  stride=1, padding=1, bias=True))\n        cur_channels_count = out_chans_first_conv\n\n        #####################\n        # Downsampling path #\n        #####################\n\n        self.denseBlocksDown = nn.ModuleList([])\n        self.transDownBlocks = nn.ModuleList([])\n        for i in range(len(down_blocks)):\n            self.denseBlocksDown.append(\n                DenseBlock(cur_channels_count, growth_rate, down_blocks[i]))\n            cur_channels_count += (growth_rate*down_blocks[i])\n            skip_connection_channel_counts.insert(0,cur_channels_count)\n            self.transDownBlocks.append(TransitionDown(cur_channels_count))\n\n        #####################\n        #     Bottleneck    #\n        #####################\n\n        self.add_module(\'bottleneck\',Bottleneck(cur_channels_count,\n                                     growth_rate, bottleneck_layers))\n        prev_block_channels = growth_rate*bottleneck_layers\n        cur_channels_count += prev_block_channels\n\n        #######################\n        #   Upsampling path   #\n        #######################\n\n        self.transUpBlocks = nn.ModuleList([])\n        self.denseBlocksUp = nn.ModuleList([])\n        for i in range(len(up_blocks)-1):\n            self.transUpBlocks.append(TransitionUp(prev_block_channels, prev_block_channels))\n            cur_channels_count = prev_block_channels + skip_connection_channel_counts[i]\n\n            self.denseBlocksUp.append(DenseBlock(\n                cur_channels_count, growth_rate, up_blocks[i],\n                    upsample=True))\n            prev_block_channels = growth_rate*up_blocks[i]\n            cur_channels_count += prev_block_channels\n\n        ## Final DenseBlock ##\n\n        self.transUpBlocks.append(TransitionUp(\n            prev_block_channels, prev_block_channels))\n        cur_channels_count = prev_block_channels + skip_connection_channel_counts[-1]\n\n        self.denseBlocksUp.append(DenseBlock(\n            cur_channels_count, growth_rate, up_blocks[-1],\n                upsample=False))\n        cur_channels_count += growth_rate*up_blocks[-1]\n\n        ## Softmax ##\n\n        self.finalConv = nn.Conv2d(in_channels=cur_channels_count,\n                                   out_channels=num_classes, kernel_size=1, stride=1,\n                                   padding=0, bias=True)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, x):\n        out = self.firstconv(x)\n\n        skip_connections = []\n        for i in range(len(self.down_blocks)):\n            out = self.denseBlocksDown[i](out)\n            skip_connections.append(out)\n            out = self.transDownBlocks[i](out)\n\n        out = self.bottleneck(out)\n        for i in range(len(self.up_blocks)):\n            skip = skip_connections.pop()\n            out = self.transUpBlocks[i](out, skip)\n            out = self.denseBlocksUp[i](out)\n\n        out = self.finalConv(out)\n        out = self.softmax(out)\n        return out\n\n\ndef Tiramisu57(num_classes, **kwargs):\n    return FCDenseNet(\n        in_channels=3, down_blocks=(4, 4, 4, 4, 4),\n        up_blocks=(4, 4, 4, 4, 4), bottleneck_layers=4,\n        growth_rate=12, out_chans_first_conv=48, num_classes=num_classes, **kwargs)\n\n\ndef Tiramisu67(num_classes, **kwargs):\n    return FCDenseNet(\n        in_channels=3, down_blocks=(5, 5, 5, 5, 5),\n        up_blocks=(5, 5, 5, 5, 5), bottleneck_layers=5,\n        growth_rate=16, out_chans_first_conv=48, num_classes=num_classes, **kwargs)\n\n\ndef Tiramisu103(num_classes, **kwargs):\n    return FCDenseNet(\n        in_channels=3, down_blocks=(4,5,7,10,12),\n        up_blocks=(12,10,7,5,4), bottleneck_layers=15,\n        growth_rate=16, out_chans_first_conv=48, num_classes=num_classes, **kwargs)\n'"
pywick/models/segmentation/u_net.py,5,"b'# Source: https://github.com/zijundeng/pytorch-semantic-segmentation/tree/master/models (MIT)\n\n""""""\nImplementation of `U-net: Convolutional networks for biomedical image segmentation <https://arxiv.org/pdf/1505.04597>`_\n""""""\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom .fcn_utils import initialize_weights\n\n__all__ = [\'UNet\']\n\nclass _EncoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, dropout=False):\n        super(_EncoderBlock, self).__init__()\n        layers = [\n            nn.Conv2d(in_channels, out_channels, kernel_size=3),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        ]\n        if dropout:\n            layers.append(nn.Dropout())\n        layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n        self.encode = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.encode(x)\n\n\nclass _DecoderBlock(nn.Module):\n    def __init__(self, in_channels, middle_channels, out_channels):\n        super(_DecoderBlock, self).__init__()\n        self.decode = nn.Sequential(\n            nn.Conv2d(in_channels, middle_channels, kernel_size=3),\n            nn.BatchNorm2d(middle_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(middle_channels, middle_channels, kernel_size=3),\n            nn.BatchNorm2d(middle_channels),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=2, stride=2),\n        )\n\n    def forward(self, x):\n        return self.decode(x)\n\n\nclass UNet(nn.Module):\n    """"""\n    Basic Unet\n    """"""\n    def __init__(self, num_classes, **kwargs):\n        super(UNet, self).__init__()\n        self.enc1 = _EncoderBlock(3, 64)\n        self.enc2 = _EncoderBlock(64, 128)\n        self.enc3 = _EncoderBlock(128, 256)\n        self.enc4 = _EncoderBlock(256, 512, dropout=True)\n        self.center = _DecoderBlock(512, 1024, 512)\n        self.dec4 = _DecoderBlock(1024, 512, 256)\n        self.dec3 = _DecoderBlock(512, 256, 128)\n        self.dec2 = _DecoderBlock(256, 128, 64)\n        self.dec1 = nn.Sequential(\n            nn.Conv2d(128, 64, kernel_size=3),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n        )\n        self.final = nn.Conv2d(64, num_classes, kernel_size=1)\n        initialize_weights(self)\n\n    def forward(self, x):\n        enc1 = self.enc1(x)\n        enc2 = self.enc2(enc1)\n        enc3 = self.enc3(enc2)\n        enc4 = self.enc4(enc3)\n        center = self.center(enc4)\n        dec4 = self.dec4(torch.cat([center, F.interpolate(enc4, center.size()[2:], mode=\'bilinear\')], 1))\n        dec3 = self.dec3(torch.cat([dec4, F.interpolate(enc3, dec4.size()[2:], mode=\'bilinear\')], 1))\n        dec2 = self.dec2(torch.cat([dec3, F.interpolate(enc2, dec3.size()[2:], mode=\'bilinear\')], 1))\n        dec1 = self.dec1(torch.cat([dec2, F.interpolate(enc1, dec2.size()[2:], mode=\'bilinear\')], 1))\n        final = self.final(dec1)\n        return F.interpolate(final, x.size()[2:], mode=\'bilinear\')\n'"
pywick/models/segmentation/unet_dilated.py,11,"b'# Source: https://github.com/Hsuxu/carvana-pytorch-uNet/blob/master/model.py\n\n""""""\nImplementation of `U-net: Convolutional networks for biomedical image segmentation <https://arxiv.org/pdf/1505.04597>`_ with dilation convolution operation\n""""""\n\nimport torch\nimport torch.nn as nn\n\n__all__ = [\'UNetDilated\']\n\n\nclass Conv_transition(nn.Module):\n    \'\'\'\n    resnet block contains inception\n    \'\'\'\n\n    def __init__(self, kernel_size, in_channels, out_channels):\n        super(Conv_transition, self).__init__()\n        if not kernel_size:\n            kernel_size = [1, 3, 5]\n        paddings = [int(a / 2) for a in kernel_size]\n        # self.Conv0=nn.Conv2d(in_channels,out_channels,3,stride=1,padding=1)\n        self.Conv1 = nn.Conv2d(in_channels, out_channels, kernel_size[0], stride=1, padding=paddings[0])\n        self.Conv2 = nn.Conv2d(in_channels, out_channels, kernel_size[1], stride=1, padding=paddings[1])\n        self.Conv3 = nn.Conv2d(in_channels, out_channels, kernel_size[2], stride=1, padding=paddings[2])\n        self.Conv_f = nn.Conv2d(3 * out_channels, out_channels, 3, stride=1, padding=1)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.act = nn.PReLU()\n\n    def forward(self, x):\n        # x = self.Conv0(x)\n        x1 = self.act(self.Conv1(x))\n        x2 = self.act(self.Conv2(x))\n        x3 = self.act(self.Conv3(x))\n        x = torch.cat([x1, x2, x3], dim=1)\n        return self.act(self.bn(self.Conv_f(x)))\n\n\nclass Dense_layer(nn.Module):\n    """"""\n    an two-layer\n    """"""\n\n    def __init__(self, in_channels, growth_rate):\n        super(Dense_layer, self).__init__()\n        # self.bn0=nn.BatchNorm2d(in_channels)\n        self.Conv0 = nn.Conv2d(in_channels, in_channels + growth_rate, 3, stride=1, padding=1)\n        self.bn1 = nn.BatchNorm2d(in_channels + growth_rate)\n        self.Conv1 = nn.Conv2d(in_channels + growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)\n\n        self.bn2 = nn.BatchNorm2d(in_channels + growth_rate)\n        self.Conv2 = nn.Conv2d(in_channels + growth_rate, in_channels, kernel_size=3, stride=1, padding=1)\n        self.bn3 = nn.BatchNorm2d(in_channels)\n\n        # self.Conv1=nn.Conv2d(in_channels+growth_rate,growth_rate,kernel_size=3,stride=1,padding=1,bias=False)\n\n        self.act = nn.PReLU()\n\n    def forward(self, x):\n        x1 = self.act(self.bn1(self.Conv0(x)))\n        x1 = self.act(self.bn2(torch.cat([self.Conv1(x1), x], dim=1)))\n\n        return self.act(self.bn3(self.Conv2(x1)))\n\n\nclass Fire_Down(nn.Module):\n    def __init__(self, kernel_size, in_channels, inner_channels, out_channels):\n        super(Fire_Down, self).__init__()\n        dilations = [1, 3, 5]\n        self.Conv1 = nn.Conv2d(in_channels, inner_channels, kernel_size=kernel_size, stride=1, padding=dilations[0],\n                               dilation=dilations[0])\n        self.Conv4 = nn.Conv2d(in_channels, inner_channels, kernel_size=kernel_size, stride=1, padding=dilations[1],\n                               dilation=dilations[1])\n        self.Conv8 = nn.Conv2d(in_channels, inner_channels, kernel_size=kernel_size, stride=1, padding=dilations[2],\n                               dilation=dilations[2])\n        self.Conv_f3 = nn.Conv2d(3 * inner_channels, out_channels, kernel_size=kernel_size, stride=2, padding=1)\n        self.Conv_f1 = nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, padding=0)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.act = nn.PReLU()\n\n    def forward(self, x):\n        x1 = self.act(self.Conv1(x))\n        x2 = self.act(self.Conv4(x))\n        x3 = self.act(self.Conv8(x))\n        x = torch.cat([x1, x2, x3], dim=1)\n        x = self.act(self.Conv_f3(x))\n        return self.act(self.bn1(self.Conv_f1(x)))\n\n\nclass Fire_Up(nn.Module):\n    def __init__(self, kernel_size, in_channels, inner_channels, out_channels, out_padding=(1, 1)):\n        super(Fire_Up, self).__init__()\n        padds = int(kernel_size / 2)\n        self.Conv1 = nn.Conv2d(in_channels, inner_channels, kernel_size=3, stride=1, padding=1)\n        if not out_padding:\n            out_padding = (1, 1)\n        # self.ConvT1=nn.ConvTranspose2d(inner_channels,out_channels,kernel_size=1,stride=2,padding=0,output_padding=out_padding)\n        self.ConvT4 = nn.ConvTranspose2d(inner_channels, out_channels, kernel_size=kernel_size, stride=2, padding=padds,\n                                         output_padding=out_padding)\n        # self.ConvT8=nn.ConvTranspose2d(inner_channels,out_channels,kernel_size=5,stride=2,padding=2,output_padding=out_padding)\n        self.Conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1, stride=1)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.act = nn.PReLU()\n\n    def forward(self, x):\n        x = self.act(self.Conv1(x))\n        # x1=self.act(self.ConvT1(x))\n        x = self.act(self.ConvT4(x))\n        # x8=self.act(self.ConvT8(x))\n        # x=torch.cat([x1,x4],dim=1)\n        x = self.act(self.bn1(self.Conv2(x)))\n        return x\n\n\nclass UNetDilated(nn.Module):\n    """"""\n    Unet utilizing dilation\n    """"""\n    def __init__(self, num_classes, **kwargs):\n        super(UNetDilated, self).__init__()\n        self.Conv0 = self._transition(3, 8)  # 1918\n        self.down1 = self._down_block(8, 16, 16)  # 959\n        self.down2 = self._down_block(16, 16, 32)  # 480\n        self.down3 = self._down_block(32, 32, 64)  # 240\n        self.down4 = self._down_block(64, 64, 96)  # 120\n        self.down5 = self._down_block(96, 96, 128)  # 60\n        self.tran0 = self._transition(128, 256)\n        self.db0 = self._dense_block(256, 32)\n\n        self.up1 = self._up_block(256, 96, 96)  # 120\n        self.db1 = self._dense_block(96, 32)\n        self.conv1 = nn.Conv2d(96 * 2, 96, 3, stride=1, padding=1)\n        self.bn1 = nn.BatchNorm2d(96)\n\n        self.up2 = self._up_block(96, 64, 64)  # 240\n        self.db2 = self._dense_block(64, 24)\n        self.conv2 = nn.Conv2d(64 * 2, 64, 3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)\n\n        self.up3 = self._up_block(64, 32, 32)  # 480\n        self.db3 = self._dense_block(32, 10)\n        self.conv3 = nn.Conv2d(32 * 2, 32, 3, stride=1, padding=1)\n        self.bn3 = nn.BatchNorm2d(32)\n\n        self.up4 = self._up_block(32, 16, 16) #, output_padding=(1, 0))  # 959\n        self.db4 = self._dense_block(16, 8)\n        self.conv4 = nn.Conv2d(16 * 2, 16, 3, stride=1, padding=1)\n        self.bn4 = nn.BatchNorm2d(16)\n\n        self.up5 = self._up_block(16, 16, 16)  # 1918\n        self.db5 = self._dense_block(16, 4)\n        self.conv5 = nn.Conv2d(16, num_classes, 3, stride=1, padding=1)\n\n        self.clss = nn.LogSoftmax()\n        self.act = nn.PReLU()\n\n    def forward(self, x):\n\n        x1 = self.Conv0(x)\n        down1 = self.down1(x1)\n        down2 = self.down2(down1)\n        down3 = self.down3(down2)\n        down4 = self.down4(down3)\n        down5 = self.down5(down4)\n        down5 = self.tran0(down5)\n        down5 = self.db0(down5)\n\n        ## TODO Problem here:\n        # self.up1(down5).data.shape    =>  torch.Size([2, 96, 64, 44])\n        #                       -- MISMATCH WITH --\n        # down4.data.shape              =>  torch.Size([2, 96, 64, 43])\n\n        up1 = self.act(self.bn1(self.conv1(torch.cat([self.db1(self.up1(down5)), down4], dim=1))))\n        del down5, down4\n\n        up2 = self.act(self.bn2(self.conv2(torch.cat([self.db2(self.up2(up1)), down3], dim=1))))\n        del down3\n\n        up3 = self.act(self.bn3(self.conv3(torch.cat([self.db3(self.up3(up2)), down2], dim=1))))\n        del down2\n\n        up4 = self.act(self.bn4(self.conv4(torch.cat([self.db4(self.up4(up3)), down1], dim=1))))\n        del down1\n\n        up5 = self.up5(up4)\n        # up5=self.conv5(up5)\n\n        # return self.clss(self.conv5(up5))\n        return self.conv5(up5)\n\n\n    def _transition(self, in_channels, out_channels):\n        layers = []\n        layers.append(Conv_transition([1, 3, 5], in_channels, out_channels))\n        return nn.Sequential(*layers)\n\n    def _down_block(self, in_channels, inner_channels, out_channels):\n        layers = []\n        layers.append(Fire_Down(3, in_channels, inner_channels, out_channels))\n        return nn.Sequential(*layers)\n\n    def _up_block(self, in_channels, inner_channels, out_channels, output_padding=(1, 1)):\n        layers = []\n        layers.append(Fire_Up(3, in_channels, inner_channels, out_channels, output_padding))\n        return nn.Sequential(*layers)\n\n    def _dense_block(self, in_channels, growth_rate):\n        layers = []\n        layers.append(Dense_layer(in_channels, growth_rate))\n        return nn.Sequential(*layers)'"
pywick/models/segmentation/unet_res.py,7,"b'""""""\nImplementation of `U-net: Convolutional networks for biomedical image segmentation <https://arxiv.org/pdf/1505.04597>`_\n""""""\n\n# Source: https://github.com/saeedizadi/binseg_pytoch (Apache-2.0)\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\nimport numpy as np\n\n__all__ = [\'UNetRes\']\n\ndef initialize_weights(method=\'kaiming\', *models):\n    for model in models:\n        for module in model.modules():\n\n            if isinstance(module, nn.Conv2d) or isinstance(module, nn.ConvTranspose2d) or isinstance(module, nn.Linear):\n                if method == \'kaiming\':\n                    init.kaiming_normal_(module.weight.data, np.sqrt(2.0))\n                elif method == \'xavier\':\n                    init.xavier_normal(module.weight.data, np.sqrt(2.0))\n                elif method == \'orthogonal\':\n                    init.orthogonal(module.weight.data, np.sqrt(2.0))\n                elif method == \'normal\':\n                    init.normal(module.weight.data,mean=0, std=0.02)\n                if module.bias is not None:\n                    init.constant(module.bias.data,0)\n\n\nclass UnetEncoder(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(UnetEncoder, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n        self.layer = nn.Sequential(nn.Conv2d(self.in_channels, self.out_channels, kernel_size=3, padding=1),\n                                   nn.BatchNorm2d(self.out_channels),\n                                   nn.ReLU(),\n                                   nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, padding=1),\n                                   nn.BatchNorm2d(self.out_channels),\n                                   nn.ReLU())\n\n    def forward(self, x):\n        return self.layer(x)\n\n\nclass UnetDecoder(nn.Module):\n    def __init__(self, in_channels, featrures, out_channels):\n        super(UnetDecoder, self).__init__()\n        self.in_channels = in_channels\n        self.features = featrures\n        self.out_channels = out_channels\n\n        self.layer = nn.Sequential(nn.Conv2d(self.in_channels, self.features, kernel_size=3, padding=1),\n                                   nn.BatchNorm2d(self.features),\n                                   nn.ReLU(),\n                                   nn.Conv2d(self.features, self.features, kernel_size=3, padding=1),\n                                   nn.BatchNorm2d(self.features),\n                                   nn.ReLU(),\n                                   nn.ConvTranspose2d(self.features, self.out_channels, kernel_size=2, stride=2),\n                                   nn.BatchNorm2d(self.out_channels),\n                                   nn.ReLU())\n\n    def forward(self, x):\n        return self.layer(x)\n\n\nclass UNet(nn.Module):\n    def __init__(self, num_classes):\n        super(UNet, self).__init__()\n        self.num_classes = num_classes\n        self.down1 = UnetEncoder(3, 64)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n        self.down2 = UnetEncoder(64, 128)\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n        self.down3 = UnetEncoder(128, 256)\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n        self.down4 = UnetEncoder(256, 512)\n        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n        self.center = nn.Sequential(nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n                                    nn.BatchNorm2d(1024),\n                                    nn.ReLU(),\n                                    nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n                                    nn.BatchNorm2d(1024),\n                                    nn.ReLU(),\n                                    nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2),\n                                    nn.BatchNorm2d(512),\n                                    nn.ReLU())\n\n        self.up1 = UnetDecoder(1024, 512, 256)\n        self.up2 = UnetDecoder(512, 256, 128)\n        self.up3 = UnetDecoder(256, 128, 64)\n\n        self.up4 = nn.Sequential(nn.Conv2d(128, 64, 3, padding=1),\n                                 # nn.BatchNorm2d(64),\n                                 # nn.ReLU(),\n                                 nn.Conv2d(64, 64, 3, padding=1))\n        # nn.BatchNorm2d(64),\n        # nn.ReLU())\n\n        self.output = nn.Conv2d(64, self.num_classes, kernel_size=1, stride=1)\n        self.final = nn.Sigmoid()\n\n        # Initialize weights\n        initialize_weights(self)\n\n    def forward(self, x):\n        en1 = self.down1(x)\n        po1 = self.pool1(en1)\n        en2 = self.down2(po1)\n        po2 = self.pool2(en2)\n        en3 = self.down3(po2)\n        po3 = self.pool3(en3)\n        en4 = self.down4(po3)\n        po4 = self.pool4(en4)\n\n        c1 = self.center(po4)\n\n        dec1 = self.up1(torch.cat([c1, F.interpolate(en4, c1.size()[2:], mode=""bilinear"")], 1))\n        dec2 = self.up2(torch.cat([dec1, F.interpolate(en3, dec1.size()[2:], mode=""bilinear"")], 1))\n        dec3 = self.up3(torch.cat([dec2, F.interpolate(en2, dec2.size()[2:], mode=""bilinear"")], 1))\n        dec4 = self.up4(torch.cat([dec3, F.interpolate(en1, dec3.size()[2:], mode=""bilinear"")], 1))\n\n        out = self.output(dec4)\n        return self.final(out)\n\n\n# The improved version of UNet model which replaces all poolings with convolution, skip conenction goes through convolutions, and residual convlutions\nclass Conv2dX2_Res(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n        super(Conv2dX2_Res, self).__init__()\n\n        self.layer = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding),\n                                   nn.BatchNorm2d(out_channels),\n                                   nn.ReLU(inplace=True),\n                                   nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=padding))\n\n    def forward(self, x):\n        conv = self.layer(x)\n        return F.relu(x.expand_as(conv) + conv)\n\n\nclass PassConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0):\n        super(PassConv, self).__init__()\n\n        self.layer = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n            nn.ReLU(inplace=True))\n\n    def forward(self, x):\n        return self.layer(x)\n\n\nclass DeconvX2_Res(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=2, stride=2):\n        super(DeconvX2_Res, self).__init__()\n\n        self.convx2_res = Conv2dX2_Res(in_channels, in_channels, kernel_size=3, padding=1)\n        self.upsample = nn.Sequential(\n            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride),\n            nn.ReLU(inplace=True))\n\n    def forward(self, x):\n        convx2_res = self.convx2_res(x)\n        return self.upsample(convx2_res)\n\n\nclass UNetRes(nn.Module):\n    def __init__(self, num_classes, **kwargs):\n        super(UNetRes, self).__init__()\n\n        # Assuming Input as 240x320x3\n        self.enc1 = nn.Sequential(nn.Conv2d(3, 64, 3, padding=1),\n                                  nn.ReLU(inplace=True),\n                                  nn.Conv2d(64, 64, 3, padding=1),\n                                  nn.ReLU(inplace=True))\n        self.pool1 = nn.Conv2d(64, 128, kernel_size=2, stride=2)  # Conv as Pool\n        self.enc2 = Conv2dX2_Res(128, 128, 3, padding=1)\n        self.pool2 = nn.Conv2d(128, 256, kernel_size=2, stride=2)  # Conv as Pool\n        self.enc3 = Conv2dX2_Res(256, 256, 3, padding=1)\n        self.pool3 = nn.Conv2d(256, 512, kernel_size=2, stride=2)  # Conv as Pool\n        self.enc4 = Conv2dX2_Res(512, 512, 3, padding=1)\n        self.pool4 = nn.Conv2d(512, 1024, kernel_size=2, stride=2)  # Conv as Pool\n\n        self.middle = nn.Sequential(Conv2dX2_Res(1024, 1024, 3, padding=1),\n                                    nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2),\n                                    nn.ReLU(inplace=True))\n\n        self.pass_enc4 = PassConv(512, 512)\n        self.pass_enc3 = PassConv(256, 256)\n        self.pass_enc2 = PassConv(128, 128)\n        self.pass_enc1 = PassConv(64, 64)\n\n        self.dec1 = DeconvX2_Res(512, 256, 2, stride=2)\n        self.dec2 = DeconvX2_Res(256, 128, 2, stride=2)\n        self.dec3 = DeconvX2_Res(128, 64, 2, stride=2)\n\n        self.dec4 = nn.Sequential(nn.Conv2d(64, 64, 3, padding=1), nn.Conv2d(64, num_classes, kernel_size=1, stride=1))\n\n        self.activation = nn.Sigmoid()\n\n        initialize_weights(self)\n\n    def forward(self, x):\n        en1 = self.enc1(x)  ##240x320x64\n\n        en2 = self.enc2(self.pool1(en1))  ## 120x160x128\n        en3 = self.enc3(self.pool2(en2))  ## 60x80x256\n        en4 = self.enc4(self.pool3(en3))  ## 30x40x512\n\n        middle = self.middle(self.pool4(en4))  ## 30x40x512\n\n        # pass_en4 = self.pass_enc4(en4) ## 30x40x512\n        # dec1 = self.dec1(pass_en4+middle) ## 60x80x256\n        dec1 = self.dec1(en4 + middle)  ## 60x80x256\n\n        # pass_enc3 = self.pass_enc3(en3) ## 60x80x256\n        # dec2 = self.dec2(pass_enc3+dec1) ## 120x160x128\n        dec2 = self.dec2(en3 + dec1)  ## 120x160x128\n\n        # pass_enc2 = self.pass_enc2(en2) ## 120x160x128\n        # dec3 = self.dec3(pass_enc2+dec2) ## 240x320x64\n        dec3 = self.dec3(en2 + dec2)  ## 240x320x64\n\n        # pass_enc1 = self.pass_enc1(enc1) ## 240x320x64\n        # dec4 = self.dec4(pass_enc1+dec3) ## 240x320x1\n        dec4 = self.dec4(en1 + dec3)  ## 240x320x1\n\n        return self.activation(dec4)'"
pywick/models/segmentation/unet_stack.py,5,"b'# Source: https://github.com/doodledood/carvana-image-masking-challenge/models (MIT)\n\n""""""\nImplementation of stacked `U-net: Convolutional networks for biomedical image segmentation <https://arxiv.org/pdf/1505.04597>`_\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n__all__ = [\'UNet960\', \'UNet_stack\']\n\nclass ConvBNReluStack(nn.Module):\n    def __init__(self, in_dim, out_dim, kernel_size=3, stride=1, padding=1, **kwargs):\n        super(ConvBNReluStack, self).__init__()\n\n        in_dim = int(in_dim)\n        out_dim = int(out_dim)\n\n        self.conv = nn.Conv2d(in_dim, out_dim, kernel_size, stride=stride, padding=padding)\n        # nn.init.xavier_normal(self.conv.weight.data)\n\n        self.bn = nn.BatchNorm2d(out_dim)\n        self.activation = nn.PReLU()  # nn.LeakyReLU(0.2)\n\n    def forward(self, inputs_):\n        x = self.conv(inputs_)\n        x = self.bn(x)\n        x = self.activation(x)\n\n        return x\n\n\nclass UNetDownStack(nn.Module):\n    def __init__(self, input_dim, filters, pool=True):\n        super(UNetDownStack, self).__init__()\n\n        self.stack1 = ConvBNReluStack(input_dim, filters, 1, stride=1, padding=0)\n        self.stack3 = ConvBNReluStack(input_dim, filters, 3, stride=1, padding=1)\n        self.stack5 = ConvBNReluStack(input_dim, filters, 5, stride=1, padding=2)\n        self.stack_pool = nn.AvgPool2d(3, stride=1, padding=1)\n        self.reducer = ConvBNReluStack(filters * 3 + input_dim, filters, kernel_size=1, stride=1, padding=0)\n\n        # self.pool = ConvBNReluStack(filters, filters, kernel_size, stride=2, padding=1) if pool else None\n        self.pool = nn.MaxPool2d(2, stride=2) if pool else None\n        # ConvBNReluStack(filters, filters, kernel_size, stride=2, padding=1) if pool else None\n        # nn.MaxPool2d(2, stride=2) if pool else None\n\n    def forward(self, inputs_):\n        x1 = self.stack1(inputs_)\n        x3 = self.stack3(inputs_)\n        x5 = self.stack5(inputs_)\n        x_pool = self.stack_pool(inputs_)\n\n        x = torch.cat([x1, x3, x5, x_pool], dim=1)\n        x = self.reducer(x)\n\n        if self.pool:\n            return x, self.pool(x)\n\n        return x\n\n\nclass UNetUpStack(nn.Module):\n    def __init__(self, input_dim, filters, kernel_size=3):\n        super(UNetUpStack, self).__init__()\n\n        self.scale_factor = 2\n        self.stack1 = ConvBNReluStack(input_dim, filters, 1, stride=1, padding=0)\n        self.stack3 = ConvBNReluStack(input_dim, filters, 3, stride=1, padding=1)\n        self.stack5 = ConvBNReluStack(input_dim, filters, 5, stride=1, padding=2)\n        self.stack_pool = nn.AvgPool2d(3, stride=1, padding=1)\n        self.reducer = ConvBNReluStack(filters * 3 + input_dim, filters, kernel_size=1, stride=1, padding=0)\n\n    def forward(self, inputs_, down):\n        x = F.interpolate(inputs_, scale_factor=self.scale_factor)\n        x = torch.cat([x, down], dim=1)\n\n        x1 = self.stack1(x)\n        x3 = self.stack3(x)\n        x5 = self.stack5(x)\n        x_pool = self.stack_pool(x)\n\n        x = torch.cat([x1, x3, x5, x_pool], dim=1)\n        x = self.reducer(x)\n\n        return x\n\n\nclass UNet_stack(nn.Module):\n    def get_n_stacks(self, input_size, **kwargs):\n        n_stacks = 0\n        width, height = input_size, input_size\n        while width % 2 == 0 and height % 2 == 0:\n            n_stacks += 1\n            width = width // 2\n            height = height // 2\n\n        return n_stacks\n\n    def __init__(self, input_size=512, filters=12, kernel_size=3, max_stacks=6, **kwargs):\n        super(UNet_stack, self).__init__()\n        self.n_stacks = min(self.get_n_stacks((input_size, input_size)), max_stacks)\n\n        # dynamically create stacks\n        self.down1 = UNetDownStack(3, filters)\n        prev_filters = filters\n        for i in range(2, self.n_stacks + 1):\n            n = i\n            layer = UNetDownStack(prev_filters, prev_filters * 2)\n            layer_name = \'down\' + str(n)\n            setattr(self, layer_name, layer)\n            prev_filters *= 2\n\n        self.center = UNetDownStack(prev_filters, prev_filters * 2, pool=False)\n\n        prev_filters = prev_filters * 3\n        for i in range(self.n_stacks):\n            n = self.n_stacks - i\n            layer = UNetUpStack(prev_filters, prev_filters // 3, kernel_size)\n            layer_name = \'up\' + str(n)\n            setattr(self, layer_name, layer)\n            prev_filters = prev_filters // 2\n\n        self.classify = nn.Conv2d(prev_filters * 2 // 3, 1, kernel_size, stride=1, padding=1)\n        # nn.init.xavier_normal(self.classify.weight.data)\n\n    def forward(self, inputs_):\n        down1, down1_pool = self.down1(inputs_)\n\n        downs = [down1]\n\n        # execute down nodes\n        prev_down_pool = down1_pool\n        for i in range(2, self.n_stacks + 1):\n            layer_name = \'down\' + str(i)\n            layer = getattr(self, layer_name)\n            down, prev_down_pool = layer(prev_down_pool)\n            downs.append(down)\n\n        center = self.center(prev_down_pool)\n\n        # excute up nodes\n        prev = center\n        for i in range(self.n_stacks):\n            n = self.n_stacks - i\n            matching_down = downs.pop()\n            layer_name = \'up\' + str(n)\n            layer = getattr(self, layer_name)\n            prev = layer(prev, matching_down)\n\n        x = self.classify(prev)\n\n        return x\n\n\nclass UNet960(nn.Module):\n    def __init__(self, filters=12, kernel_size=3, **kwargs):\n        super(UNet960, self).__init__()\n\n        # 960\n        self.down1 = UNetDownStack(3, filters)\n        # 480\n        self.down2 = UNetDownStack(filters, filters * 2)\n        # 240\n        self.down3 = UNetDownStack(filters * 2, filters * 4)\n        # 120\n        self.down4 = UNetDownStack(filters * 4, filters * 8)\n        # 60\n        self.down5 = UNetDownStack(filters * 8, filters * 16)\n        # 30\n        self.down6 = UNetDownStack(filters * 16, filters * 32)\n        # 15\n        self.center = UNetDownStack(filters * 32, filters * 64, pool=False)\n        # 15\n        self.up6 = UNetUpStack(filters * 96, filters * 32, kernel_size)\n        # 30\n        self.up5 = UNetUpStack(filters * 48, filters * 16, kernel_size)\n        # 60\n        self.up4 = UNetUpStack(filters * 24, filters * 8, kernel_size)\n        # 120\n        self.up3 = UNetUpStack(filters * 12, filters * 4, kernel_size)\n        # 240\n        self.up2 = UNetUpStack(filters * 6, filters * 2, kernel_size)\n        # 480\n        self.up1 = UNetUpStack(filters * 3, filters, kernel_size)\n        # 960\n        self.classify = nn.Conv2d(filters, 1, kernel_size, stride=1, padding=1)\n\n    def forward(self, inputs_):\n        down1, down1_pool = self.down1(inputs_)\n        down2, down2_pool = self.down2(down1_pool)\n        down3, down3_pool = self.down3(down2_pool)\n        down4, down4_pool = self.down4(down3_pool)\n        down5, down5_pool = self.down5(down4_pool)\n        down6, down6_pool = self.down6(down5_pool)\n\n        center = self.center(down6_pool)\n\n        up6 = self.up6(center, down6)\n        up5 = self.up5(up6, down5)\n        up4 = self.up4(up5, down4)\n        up3 = self.up3(up4, down3)\n        up2 = self.up2(up3, down2)\n        up1 = self.up1(up2, down1)\n\n        x = self.classify(up1)\n\n        return x\n'"
tests/integration/fit_complex/multi_input_multi_target.py,2,"b""\nimport os\n\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets\nfrom pywick import callbacks as cbks\nfrom pywick import constraints as cons\nfrom pywick import regularizers as regs\nfrom pywick.modules import ModuleTrainer\n\nROOT = '/data/mnist'\ndataset = datasets.MNIST(ROOT, train=True, download=True)\nx_train, y_train = th.load(os.path.join(dataset.root, 'processed/training.pt'))\nx_test, y_test = th.load(os.path.join(dataset.root, 'processed/test.pt'))\n\nx_train = x_train.float()\ny_train = y_train.long()\nx_test = x_test.float()\ny_test = y_test.long()\n\nx_train = x_train / 255.\nx_test = x_test / 255.\nx_train = x_train.unsqueeze(1)\nx_test = x_test.unsqueeze(1)\n\n# only train on a subset\nx_train = x_train[:1000]\ny_train = y_train[:1000]\nx_test = x_test[:100]\ny_test = y_test[:100]\n\n\n# Define your model EXACTLY as if you were using nn.Module\nclass Network(nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n        self.fc1 = nn.Linear(1600, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x, y, z):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n        x = x.view(-1, 1600)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x), F.log_softmax(x), F.log_softmax(x)\n\n# with one loss function given\nmodel = Network()\ntrainer = ModuleTrainer(model)\n\nregularizers = [regs.L1Regularizer(1e-4, 'fc*'), regs.L2Regularizer(1e-5, 'conv*')]\nconstraints = [cons.UnitNorm(5, 'batch', 'fc*'),\n               cons.MaxNorm(5, 0, 'batch', 'conv*')]\ncallbacks = [cbks.ReduceLROnPlateau(monitor='loss', verbose=1)]\n\ntrainer.compile(loss='nll_loss',\n                optimizer='adadelta',\n                regularizers=regularizers,\n                constraints=constraints,\n                callbacks=callbacks)\n\ntrainer.fit([x_train, x_train, x_train], \n            [y_train, y_train, y_train],\n            num_epoch=3, \n            batch_size=128,\n            verbose=1)\n\nyp1, yp2, yp3 = trainer.predict([x_train, x_train, x_train])\nprint(yp1.size(), yp2.size(), yp3.size())\n\neval_loss = trainer.evaluate([x_train, x_train, x_train],\n                             [y_train, y_train, y_train])\nprint(eval_loss)\n\n# With multiple loss functions given\nmodel = Network()\ntrainer = ModuleTrainer(model)\n\ntrainer.compile(loss=['nll_loss', 'nll_loss', 'nll_loss'],\n                optimizer='adadelta',\n                regularizers=regularizers,\n                constraints=constraints,\n                callbacks=callbacks)\n\ntrainer.fit([x_train, x_train, x_train], \n            [y_train, y_train, y_train],\n            num_epoch=3, \n            batch_size=128,\n            verbose=1)\n\n# should raise exception for giving multiple loss functions \n# but not giving a loss function for every input\ntry:\n    model = Network()\n    trainer = ModuleTrainer(model)\n\n    trainer.compile(loss=['nll_loss', 'nll_loss'],\n                    optimizer='adadelta',\n                    regularizers=regularizers,\n                    constraints=constraints,\n                    callbacks=callbacks)\n\n    trainer.fit([x_train, x_train, x_train], \n                [y_train, y_train, y_train],\n                num_epoch=3, \n                batch_size=128,\n                verbose=1)\nexcept:\n    print('Exception correctly caught')\n\n"""
tests/integration/fit_loader_simple/single_input_multi_target.py,3,"b""\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nfrom pywick.modules import ModuleTrainer\nfrom pywick.datasets.TensorDataset import TensorDataset\n\nimport os\nfrom torchvision import datasets\nROOT = '/data/mnist'\ndataset = datasets.MNIST(ROOT, train=True, download=True)\nx_train, y_train = th.load(os.path.join(dataset.root, 'processed/training.pt'))\nx_test, y_test = th.load(os.path.join(dataset.root, 'processed/test.pt'))\n\nx_train = x_train.float()\ny_train = y_train.long()\nx_test = x_test.float()\ny_test = y_test.long()\n\nx_train = x_train / 255.\nx_test = x_test / 255.\nx_train = x_train.unsqueeze(1)\nx_test = x_test.unsqueeze(1)\n\n# only train on a subset\nx_train = x_train[:1000]\ny_train = y_train[:1000]\nx_test = x_test[:1000]\ny_test = y_test[:1000]\n\ntrain_data = TensorDataset(x_train, [y_train, y_train])\ntrain_loader = DataLoader(train_data, batch_size=128)\n\n# Define your model EXACTLY as if you were using nn.Module\nclass Network(nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n        self.fc1 = nn.Linear(1600, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n        x = x.view(-1, 1600)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x), F.log_softmax(x)\n\n\n# one loss function for multiple targets\nmodel = Network()\ntrainer = ModuleTrainer(model)\ntrainer.compile(loss='nll_loss',\n                optimizer='adadelta')\n\ntrainer.fit_loader(train_loader,\n                    num_epoch=3, \n                    verbose=1)\nypred1, ypred2 = trainer.predict(x_train)\nprint(ypred1.size(), ypred2.size())\n\neval_loss = trainer.evaluate(x_train, [y_train, y_train])\nprint(eval_loss)\n# multiple loss functions\nmodel = Network()\ntrainer = ModuleTrainer(model)\ntrainer.compile(loss=['nll_loss', 'nll_loss'],\n                optimizer='adadelta')\ntrainer.fit_loader(train_loader,\n                   num_epoch=3, \n                   verbose=1)\n\n\n\n"""
tests/integration/fit_loader_simple/single_input_single_target.py,3,"b""\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nfrom pywick.modules import ModuleTrainer\nfrom pywick.datasets.TensorDataset import TensorDataset\n\nimport os\nfrom torchvision import datasets\nROOT = '/data/mnist'\ndataset = datasets.MNIST(ROOT, train=True, download=True)\nx_train, y_train = th.load(os.path.join(dataset.root, 'processed/training.pt'))\nx_test, y_test = th.load(os.path.join(dataset.root, 'processed/test.pt'))\n\nx_train = x_train.float()\ny_train = y_train.long()\nx_test = x_test.float()\ny_test = y_test.long()\n\nx_train = x_train / 255.\nx_test = x_test / 255.\nx_train = x_train.unsqueeze(1)\nx_test = x_test.unsqueeze(1)\n\n# only train on a subset\nx_train = x_train[:1000]\ny_train = y_train[:1000]\nx_test = x_test[:1000]\ny_test = y_test[:1000]\n\ntrain_data = TensorDataset(x_train, y_train)\ntrain_loader = DataLoader(train_data, batch_size=128)\n\n# Define your model EXACTLY as if you were using nn.Module\nclass Network(nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n        self.fc1 = nn.Linear(1600, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n        x = x.view(-1, 1600)\n        x = F.relu(self.fc1(x))\n        #x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x)\n\n\nmodel = Network()\ntrainer = ModuleTrainer(model)\n\ntrainer.compile(loss='nll_loss',\n                optimizer='adadelta')\n\ntrainer.fit_loader(train_loader,\n                   num_epoch=3,\n                   verbose=1)\n\nypred = trainer.predict(x_train)\nprint(ypred.size())\n\neval_loss = trainer.evaluate(x_train, y_train)\nprint(eval_loss)\n\nprint(trainer.history)\n#print(trainer.history['loss'])\n\n"""
tests/integration/fit_simple/simple_multi_input_multi_target.py,2,"b""\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom pywick.modules import ModuleTrainer\n\nimport os\nfrom torchvision import datasets\nROOT = '/data/mnist'\ndataset = datasets.MNIST(ROOT, train=True, download=True)\nx_train, y_train = th.load(os.path.join(dataset.root, 'processed/training.pt'))\nx_test, y_test = th.load(os.path.join(dataset.root, 'processed/test.pt'))\n\nx_train = x_train.float()\ny_train = y_train.long()\nx_test = x_test.float()\ny_test = y_test.long()\n\nx_train = x_train / 255.\nx_test = x_test / 255.\nx_train = x_train.unsqueeze(1)\nx_test = x_test.unsqueeze(1)\n\n# only train on a subset\nx_train = x_train[:1000]\ny_train = y_train[:1000]\nx_test = x_test[:100]\ny_test = y_test[:100]\n\n\n# Define your model EXACTLY as if you were using nn.Module\nclass Network(nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n        self.fc1 = nn.Linear(1600, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x, y, z):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n        x = x.view(-1, 1600)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x), F.log_softmax(x), F.log_softmax(x)\n\n# with one loss function given\nmodel = Network()\ntrainer = ModuleTrainer(model)\n\ntrainer.compile(loss='nll_loss',\n                optimizer='adadelta')\n\ntrainer.fit([x_train, x_train, x_train], \n            [y_train, y_train, y_train],\n            num_epoch=3, \n            batch_size=128,\n            verbose=1)\n\nyp1, yp2, yp3 = trainer.predict([x_train, x_train, x_train])\nprint(yp1.size(), yp2.size(), yp3.size())\n\neval_loss = trainer.evaluate([x_train, x_train, x_train],\n                             [y_train, y_train, y_train])\nprint(eval_loss)\n\n# With multiple loss functions given\nmodel = Network()\ntrainer = ModuleTrainer(model)\n\ntrainer.compile(loss=['nll_loss', 'nll_loss', 'nll_loss'],\n                optimizer='adadelta')\n\ntrainer.fit([x_train, x_train, x_train], \n            [y_train, y_train, y_train],\n            num_epoch=3, \n            batch_size=128,\n            verbose=1)\n\n# should raise exception for giving multiple loss functions \n# but not giving a loss function for every input\ntry:\n    model = Network()\n    trainer = ModuleTrainer(model)\n\n    trainer.compile(loss=['nll_loss', 'nll_loss'],\n                    optimizer='adadelta')\n\n    trainer.fit([x_train, x_train, x_train], \n                [y_train, y_train, y_train],\n                num_epoch=3, \n                batch_size=128,\n                verbose=1)\nexcept:\n    print('Exception correctly caught')\n\n"""
tests/integration/fit_simple/simple_multi_input_no_target.py,2,"b""\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom pywick.modules import ModuleTrainer\n\nimport os\nfrom torchvision import datasets\nROOT = '/data/mnist'\ndataset = datasets.MNIST(ROOT, train=True, download=True)\nx_train, y_train = th.load(os.path.join(dataset.root, 'processed/training.pt'))\nx_test, y_test = th.load(os.path.join(dataset.root, 'processed/test.pt'))\n\nx_train = x_train.float()\ny_train = y_train.long()\nx_test = x_test.float()\ny_test = y_test.long()\n\nx_train = x_train / 255.\nx_test = x_test / 255.\nx_train = x_train.unsqueeze(1)\nx_test = x_test.unsqueeze(1)\n\n# only train on a subset\nx_train = x_train[:1000]\ny_train = y_train[:1000]\nx_test = x_test[:1000]\ny_test = y_test[:1000]\n\n\n# Define your model EXACTLY as if you were using nn.Module\nclass Network(nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n        self.fc1 = nn.Linear(1600, 128)\n        self.fc2 = nn.Linear(128, 1)\n\n    def forward(self, x, y, z):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n        x = x.view(-1, 1600)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return th.abs(10 - x)\n\n\nmodel = Network()\ntrainer = ModuleTrainer(model)\n\ntrainer.compile(loss='unconstrained_sum',\n                optimizer='adadelta')\n\ntrainer.fit([x_train, x_train, x_train],\n            num_epoch=3, \n            batch_size=128,\n            verbose=1)\n\nypred = trainer.predict([x_train, x_train, x_train])\nprint(ypred.size())\n\neval_loss = trainer.evaluate([x_train, x_train, x_train])\nprint(eval_loss)\n\n"""
tests/integration/fit_simple/simple_multi_input_single_target.py,2,"b""\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom pywick.modules import ModuleTrainer\n\nimport os\nfrom torchvision import datasets\nROOT = '/data/mnist'\ndataset = datasets.MNIST(ROOT, train=True, download=True)\nx_train, y_train = th.load(os.path.join(dataset.root, 'processed/training.pt'))\nx_test, y_test = th.load(os.path.join(dataset.root, 'processed/test.pt'))\n\nx_train = x_train.float()\ny_train = y_train.long()\nx_test = x_test.float()\ny_test = y_test.long()\n\nx_train = x_train / 255.\nx_test = x_test / 255.\nx_train = x_train.unsqueeze(1)\nx_test = x_test.unsqueeze(1)\n\n# only train on a subset\nx_train = x_train[:1000]\ny_train = y_train[:1000]\nx_test = x_test[:100]\ny_test = y_test[:100]\n\n\n# Define your model EXACTLY as if you were using nn.Module\nclass Network(nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n        self.fc1 = nn.Linear(1600, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x, y, z):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n        x = x.view(-1, 1600)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x)\n\n\nmodel = Network()\ntrainer = ModuleTrainer(model)\n\ntrainer.compile(loss='nll_loss',\n                optimizer='adadelta')\n\ntrainer.fit([x_train, x_train, x_train], y_train,\n            val_data=([x_test, x_test, x_test], y_test),\n            num_epoch=3, \n            batch_size=128,\n            verbose=1)\n\nypred = trainer.predict([x_train, x_train, x_train])\nprint(ypred.size())\n\neval_loss = trainer.evaluate([x_train, x_train, x_train], y_train)\nprint(eval_loss)"""
tests/integration/fit_simple/single_input_multi_target.py,2,"b""\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom pywick.modules import ModuleTrainer\n\nimport os\nfrom torchvision import datasets\nROOT = '/data/mnist'\ndataset = datasets.MNIST(ROOT, train=True, download=True)\nx_train, y_train = th.load(os.path.join(dataset.root, 'processed/training.pt'))\nx_test, y_test = th.load(os.path.join(dataset.root, 'processed/test.pt'))\n\nx_train = x_train.float()\ny_train = y_train.long()\nx_test = x_test.float()\ny_test = y_test.long()\n\nx_train = x_train / 255.\nx_test = x_test / 255.\nx_train = x_train.unsqueeze(1)\nx_test = x_test.unsqueeze(1)\n\n# only train on a subset\nx_train = x_train[:1000]\ny_train = y_train[:1000]\nx_test = x_test[:1000]\ny_test = y_test[:1000]\n\n\n# Define your model EXACTLY as if you were using nn.Module\nclass Network(nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n        self.fc1 = nn.Linear(1600, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n        x = x.view(-1, 1600)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x), F.log_softmax(x)\n\n\n# one loss function for multiple targets\nmodel = Network()\ntrainer = ModuleTrainer(model)\ntrainer.compile(loss='nll_loss',\n                optimizer='adadelta')\n\ntrainer.fit(x_train, \n            [y_train, y_train], \n            num_epoch=3, \n            batch_size=128,\n            verbose=1)\nypred1, ypred2 = trainer.predict(x_train)\nprint(ypred1.size(), ypred2.size())\n\neval_loss = trainer.evaluate(x_train, [y_train, y_train])\nprint(eval_loss)\n# multiple loss functions\nmodel = Network()\ntrainer = ModuleTrainer(model)\ntrainer.compile(loss=['nll_loss', 'nll_loss'],\n                optimizer='adadelta')\ntrainer.fit(x_train, \n            [y_train, y_train], \n            num_epoch=3, \n            batch_size=128,\n            verbose=1)\n\n\n\n"""
tests/integration/fit_simple/single_input_no_target.py,2,"b""\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom pywick.modules import ModuleTrainer\n\nimport os\nfrom torchvision import datasets\nROOT = '/data/mnist'\ndataset = datasets.MNIST(ROOT, train=True, download=True)\nx_train, y_train = th.load(os.path.join(dataset.root, 'processed/training.pt'))\nx_test, y_test = th.load(os.path.join(dataset.root, 'processed/test.pt'))\n\nx_train = x_train.float()\ny_train = y_train.long()\nx_test = x_test.float()\ny_test = y_test.long()\n\nx_train = x_train / 255.\nx_test = x_test / 255.\nx_train = x_train.unsqueeze(1)\nx_test = x_test.unsqueeze(1)\n\n# only train on a subset\nx_train = x_train[:1000]\ny_train = y_train[:1000]\nx_test = x_test[:1000]\ny_test = y_test[:1000]\n\n\n# Define your model EXACTLY as if you were using nn.Module\nclass Network(nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n        self.fc1 = nn.Linear(1600, 128)\n        self.fc2 = nn.Linear(128, 1)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n        x = x.view(-1, 1600)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return th.abs(10 - x)\n\n\nmodel = Network()\ntrainer = ModuleTrainer(model)\n\ntrainer.compile(loss='unconstrained_sum',\n                optimizer='adadelta')\n\ntrainer.fit(x_train,\n            num_epoch=3, \n            batch_size=128,\n            verbose=1)\n\nypred = trainer.predict(x_train)\nprint(ypred.size())\n\neval_loss = trainer.evaluate(x_train, None)\nprint(eval_loss)"""
tests/integration/fit_simple/single_input_single_target.py,2,"b"" \nimport os\n\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets\nfrom pywick import regularizers as reg\nfrom pywick.modules import ModuleTrainer\n\nROOT = '/data/mnist'\ndataset = datasets.MNIST(ROOT, train=True, download=True)\nx_train, y_train = th.load(os.path.join(dataset.root, 'processed/training.pt'))\nx_test, y_test = th.load(os.path.join(dataset.root, 'processed/test.pt'))\n\nx_train = x_train.float()\ny_train = y_train.long()\nx_test = x_test.float()\ny_test = y_test.long()\n\nx_train = x_train / 255.\nx_test = x_test / 255.\nx_train = x_train.unsqueeze(1)\nx_test = x_test.unsqueeze(1)\n\n# only train on a subset\nx_train = x_train[:1000]\ny_train = y_train[:1000]\nx_test = x_test[:1000]\ny_test = y_test[:1000]\n\n\n# Define your model EXACTLY as if you were using nn.Module\nclass Network(nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n        self.fc1 = nn.Linear(1600, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n        x = x.view(-1, 1600)\n        x = F.relu(self.fc1(x))\n        #x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x)\n\n\nmodel = Network()\ntrainer = ModuleTrainer(model)\n\ntrainer.compile(loss='nll_loss',\n                optimizer='adadelta',\n                regularizers=[reg.L1Regularizer(1e-4)])\n\ntrainer.fit(x_train, y_train, \n            val_data=(x_test, y_test),\n            num_epoch=3, \n            batch_size=128,\n            verbose=1)\n\nypred = trainer.predict(x_train)\nprint(ypred.size())\n\neval_loss = trainer.evaluate(x_train, y_train)\nprint(eval_loss)\n\nprint(trainer.history)\n#print(trainer.history['loss'])\n\n"""
tests/unit/transforms/test_affine_transforms.py,0,"b'""""""\nTest affine transforms\n\nTransforms:\n    - Affine + RandomAffine\n    - AffineCompose\n    - Rotate + RandomRotate\n    - Translate + RandomTranslate\n    - Shear + RandomShear\n    - Zoom + RandomZoom\n""""""\n\n#import pytest\n\nimport torch as th\n\nfrom pywick.transforms import (RandomAffine, Affine,\n                               RandomRotate, RandomChoiceRotate, Rotate,\n                               RandomTranslate, RandomChoiceTranslate, Translate,\n                               RandomShear, RandomChoiceShear, Shear,\n                               RandomZoom, RandomChoiceZoom, Zoom)\n\n# ----------------------------------------------------\n# ----------------------------------------------------\n\n## DATA SET ##\ndef gray2d_setup():\n    images = {}\n\n    x = th.zeros(1,30,30)\n    x[:,10:21,10:21] = 1\n    images[\'gray_01\'] = x\n\n    x = th.zeros(1,30,40)\n    x[:,10:21,10:21] = 1\n    images[\'gray_02\'] = x\n\n    return images\n\ndef multi_gray2d_setup():\n    old_imgs = gray2d_setup()\n    images = {}\n    for k,v in old_imgs.items():\n        images[k+\'_2imgs\'] = [v,v]\n        images[k+\'_3imgs\'] = [v,v,v]\n        images[k+\'_4imgs\'] = [v,v,v,v]\n    return images\n\ndef color2d_setup():\n    images = {}\n\n    x = th.zeros(3,30,30)\n    x[:,10:21,10:21] = 1\n    images[\'color_01\'] = x\n\n    x = th.zeros(3,30,40)\n    x[:,10:21,10:21] = 1\n    images[\'color_02\'] = x\n\n    return images\n\ndef multi_color2d_setup():\n    old_imgs = color2d_setup()\n    images = {}\n    for k,v in old_imgs.items():\n        images[k+\'_2imgs\'] = [v,v]\n        images[k+\'_3imgs\'] = [v,v,v]\n        images[k+\'_4imgs\'] = [v,v,v,v]\n    return images\n\n\n# ----------------------------------------------------\n# ----------------------------------------------------\n\n\ndef Affine_setup():\n    tforms = {}\n    tforms[\'random_affine\'] = RandomAffine(rotation_range=30, \n                                           translation_range=0.1)\n    tforms[\'affine\'] = Affine(th.FloatTensor([[0.9,0,0],[0,0.9,0]]))\n    return tforms\n\ndef Rotate_setup():\n    tforms = {}\n    tforms[\'random_rotate\'] = RandomRotate(30)\n    tforms[\'random_choice_rotate\'] = RandomChoiceRotate([30,40,50])\n    tforms[\'rotate\'] = Rotate(30)\n    return tforms\n\ndef Translate_setup():\n    tforms = {}\n    tforms[\'random_translate\'] = RandomTranslate(0.1)\n    tforms[\'random_choice_translate\'] = RandomChoiceTranslate([0.1,0.2])\n    tforms[\'translate\'] = Translate(0.3)\n    return tforms\n\ndef Shear_setup():\n    tforms = {}\n    tforms[\'random_shear\'] = RandomShear(30)\n    tforms[\'random_choice_shear\'] = RandomChoiceShear([20,30,40])\n    tforms[\'shear\'] = Shear(25)\n    return tforms\n\ndef Zoom_setup():\n    tforms = {}\n    tforms[\'random_zoom\'] = RandomZoom((0.8,1.2))\n    tforms[\'random_choice_zoom\'] = RandomChoiceZoom([0.8,0.9,1.1,1.2])\n    tforms[\'zoom\'] = Zoom(0.9)\n    return tforms\n\n# ----------------------------------------------------\n# ----------------------------------------------------\n\ndef test_affine_transforms_runtime(verbose=1):\n    """"""\n    Test that there are no runtime errors\n    """"""\n    ### MAKE TRANSFORMS ###\n    tforms = {}\n    tforms.update(Affine_setup())\n    tforms.update(Rotate_setup())\n    tforms.update(Translate_setup())\n    tforms.update(Shear_setup())\n    tforms.update(Zoom_setup())\n\n    ### MAKE DATA\n    images = {}\n    images.update(gray2d_setup())\n    images.update(multi_gray2d_setup())\n    images.update(color2d_setup())\n    images.update(multi_color2d_setup())\n\n    successes = []\n    failures = []\n    for im_key, im_val in images.items():\n        for tf_key, tf_val in tforms.items():\n            try:\n                if isinstance(im_val, (tuple,list)):\n                    tf_val(*im_val)\n                else:\n                    tf_val(im_val)\n                successes.append((im_key, tf_key))\n            except:\n                failures.append((im_key, tf_key))\n\n    if verbose > 0:\n        for k, v in failures:\n            print(\'%s - %s\' % (k, v))\n\n    print(\'# SUCCESSES: \', len(successes))\n    print(\'# FAILURES: \' , len(failures))\n\n\nif __name__==\'__main__\':\n    test_affine_transforms_runtime()\n\n\n\n\n\n\n\n\n'"
tests/unit/transforms/test_image_transforms.py,0,"b'""""""\nTests for pywick/transforms/image_transforms.py\n""""""\n\n\nimport torch as th\n\nfrom pywick.transforms import (Grayscale, RandomGrayscale,\n                               Gamma, RandomGamma, RandomChoiceGamma,\n                               Brightness, RandomBrightness, RandomChoiceBrightness,\n                               Saturation, RandomSaturation, RandomChoiceSaturation,\n                               Contrast, RandomContrast, RandomChoiceContrast)\n\n# ----------------------------------------------------\n# ----------------------------------------------------\n\n## DATA SET ##\ndef gray2d_setup():\n    images = {}\n\n    x = th.zeros(1,30,30)\n    x[:,10:21,10:21] = 1\n    images[\'gray_01\'] = x\n\n    x = th.zeros(1,30,40)\n    x[:,10:21,10:21] = 1\n    images[\'gray_02\'] = x\n\n    return images\n\ndef multi_gray2d_setup():\n    old_imgs = gray2d_setup()\n    images = {}\n    for k,v in old_imgs.items():\n        images[k+\'_2imgs\'] = [v,v]\n        images[k+\'_3imgs\'] = [v,v,v]\n        images[k+\'_4imgs\'] = [v,v,v,v]\n    return images\n\ndef color2d_setup():\n    images = {}\n\n    x = th.zeros(3,30,30)\n    x[:,10:21,10:21] = 1\n    images[\'color_01\'] = x\n\n    x = th.zeros(3,30,40)\n    x[:,10:21,10:21] = 1\n    images[\'color_02\'] = x\n\n    return images\n\ndef multi_color2d_setup():\n    old_imgs = color2d_setup()\n    images = {}\n    for k,v in old_imgs.items():\n        images[k+\'_2imgs\'] = [v,v]\n        images[k+\'_3imgs\'] = [v,v,v]\n        images[k+\'_4imgs\'] = [v,v,v,v]\n    return images\n\n# ----------------------------------------------------\n# ----------------------------------------------------\n\n## TFORMS SETUP ###\ndef Grayscale_setup():\n    tforms = {}\n    tforms[\'grayscale_keepchannels\'] = Grayscale(keep_channels=True)\n    tforms[\'grayscale_dontkeepchannels\'] = Grayscale(keep_channels=False)\n\n    tforms[\'random_grayscale_nop\'] = RandomGrayscale()\n    tforms[\'random_grayscale_p_01\'] = RandomGrayscale(0)\n    tforms[\'random_grayscale_p_02\'] = RandomGrayscale(0.5)\n    tforms[\'random_grayscale_p_03\'] = RandomGrayscale(1)\n\n    return tforms\n\ndef Gamma_setup():\n    tforms = {}\n    tforms[\'gamma_<1\'] = Gamma(value=0.5)\n    tforms[\'gamma_=1\'] = Gamma(value=1.0)\n    tforms[\'gamma_>1\'] = Gamma(value=1.5)\n    tforms[\'random_gamma_01\'] = RandomGamma(0.5,1.5)\n    tforms[\'random_gamma_02\'] = RandomGamma(0.5,1.0)\n    tforms[\'random_gamma_03\'] = RandomGamma(1.0,1.5)\n    tforms[\'random_choice_gamma_01\'] = RandomChoiceGamma([0.5,1.0])\n    tforms[\'random_choice_gamma_02\'] = RandomChoiceGamma([0.5,1.0],p=[0.5,0.5])\n    tforms[\'random_choice_gamma_03\'] = RandomChoiceGamma([0.5,1.0],p=[0.2,0.8])\n\n    return tforms\n\ndef Brightness_setup():\n    tforms = {}\n    tforms[\'brightness_=-1\'] = Brightness(value=-1)\n    tforms[\'brightness_<0\'] = Brightness(value=-0.5)\n    tforms[\'brightness_=0\'] = Brightness(value=0)\n    tforms[\'brightness_>0\'] = Brightness(value=0.5)\n    tforms[\'brightness_=1\'] = Brightness(value=1)\n\n    tforms[\'random_brightness_01\'] = RandomBrightness(-1,-0.5)\n    tforms[\'random_brightness_02\'] = RandomBrightness(-0.5,0)\n    tforms[\'random_brightness_03\'] = RandomBrightness(0,0.5)\n    tforms[\'random_brightness_04\'] = RandomBrightness(0.5,1)\n\n    tforms[\'random_choice_brightness_01\'] = RandomChoiceBrightness([-1,0,1])\n    tforms[\'random_choice_brightness_02\'] = RandomChoiceBrightness([-1,0,1],p=[0.2,0.5,0.3])\n    tforms[\'random_choice_brightness_03\'] = RandomChoiceBrightness([0,0,0,0],p=[0.25,0.5,0.25,0.25])\n\n    return tforms\n\ndef Saturation_setup():\n    tforms = {}\n    tforms[\'saturation_=-1\'] = Saturation(-1)\n    tforms[\'saturation_<0\'] = Saturation(-0.5)\n    tforms[\'saturation_=0\'] = Saturation(0)\n    tforms[\'saturation_>0\'] = Saturation(0.5)\n    tforms[\'saturation_=1\'] = Saturation(1)\n\n    tforms[\'random_saturation_01\'] = RandomSaturation(-1,-0.5)\n    tforms[\'random_saturation_02\'] = RandomSaturation(-0.5,0)\n    tforms[\'random_saturation_03\'] = RandomSaturation(0,0.5)\n    tforms[\'random_saturation_04\'] = RandomSaturation(0.5,1)\n\n    tforms[\'random_choice_saturation_01\'] = RandomChoiceSaturation([-1,0,1])\n    tforms[\'random_choice_saturation_02\'] = RandomChoiceSaturation([-1,0,1],p=[0.2,0.5,0.3])\n    tforms[\'random_choice_saturation_03\'] = RandomChoiceSaturation([0,0,0,0],p=[0.25,0.5,0.25,0.25])\n    \n    return tforms\n\ndef Contrast_setup():\n    tforms = {}\n    tforms[\'contrast_<<0\'] = Contrast(-10)\n    tforms[\'contrast_<0\'] = Contrast(-1)\n    tforms[\'contrast_=0\'] = Contrast(0)\n    tforms[\'contrast_>0\'] = Contrast(1)\n    tforms[\'contrast_>>0\'] = Contrast(10)\n\n    tforms[\'random_contrast_01\'] = RandomContrast(-10,-1)\n    tforms[\'random_contrast_02\'] = RandomContrast(-1,0)\n    tforms[\'random_contrast_03\'] = RandomContrast(0,1)\n    tforms[\'random_contrast_04\'] = RandomContrast(1,10)\n\n    tforms[\'random_choice_saturation_01\'] = RandomChoiceContrast([-1,0,1])\n    tforms[\'random_choice_saturation_02\'] = RandomChoiceContrast([-10,0,10],p=[0.2,0.5,0.3])\n    tforms[\'random_choice_saturation_03\'] = RandomChoiceContrast([1,1],p=[0.5,0.5])\n    \n    return tforms\n\n# ----------------------------------------------------\n# ----------------------------------------------------\n\ndef test_image_transforms_runtime(verbose=1):\n    """"""\n    Test that there are no runtime errors\n    """"""\n    ### MAKE TRANSFORMS ###\n    tforms = {}\n    tforms.update(Gamma_setup())\n    tforms.update(Brightness_setup())\n    tforms.update(Saturation_setup())\n    tforms.update(Contrast_setup())\n\n    ### MAKE DATA ###\n    images = {}\n    images.update(gray2d_setup())\n    images.update(multi_gray2d_setup())\n    images.update(color2d_setup())\n    images.update(multi_color2d_setup())\n\n    successes = []\n    failures = []\n    for im_key, im_val in images.items():\n        for tf_key, tf_val in tforms.items():\n            try:\n                if isinstance(im_val, (tuple,list)):\n                    tf_val(*im_val)\n                else:\n                    tf_val(im_val)\n                successes.append((im_key, tf_key))\n            except:\n                failures.append((im_key, tf_key))\n\n    if verbose > 0:\n        for k, v in failures:\n            print(\'%s - %s\' % (k, v))\n\n    print(\'# SUCCESSES: \', len(successes))\n    print(\'# FAILURES: \' , len(failures))\n\n\nif __name__==\'__main__\':\n    test_image_transforms_runtime()\n'"
tests/unit/transforms/test_tensor_transforms.py,0,"b'""""""\nTests for pywick/transforms/image_transforms.py\n""""""\n\n\nimport torch as th\n\nfrom pywick.transforms import (ToTensor,\n                               ToFile,\n                               ChannelsLast, HWC,\n                               ChannelsFirst, CHW,\n                               TypeCast,\n                               AddChannel,\n                               Transpose,\n                               RangeNormalize,\n                               StdNormalize,\n                               RandomCrop,\n                               SpecialCrop,\n                               Pad,\n                               RandomFlip,\n                               RandomOrder)\n\n# ----------------------------------------------------\n\n## DATA SET ##\ndef gray2d_setup():\n    images = {}\n\n    x = th.zeros(1,30,30)\n    x[:,10:21,10:21] = 1\n    images[\'gray_01\'] = x\n\n    x = th.zeros(1,30,40)\n    x[:,10:21,10:21] = 1\n    images[\'gray_02\'] = x\n    return images\n\ndef multi_gray2d_setup():\n    old_imgs = gray2d_setup()\n    images = {}\n    for k,v in old_imgs.items():\n        images[k+\'_2imgs\'] = [v,v]\n        images[k+\'_3imgs\'] = [v,v,v]\n        images[k+\'_4imgs\'] = [v,v,v,v]\n    return images\n\ndef color2d_setup():\n    images = {}\n\n    x = th.zeros(3,30,30)\n    x[:,10:21,10:21] = 1\n    images[\'color_01\'] = x\n\n    x = th.zeros(3,30,40)\n    x[:,10:21,10:21] = 1\n    images[\'color_02\'] = x\n\n    return images\n\ndef multi_color2d_setup():\n    old_imgs = color2d_setup()\n    images = {}\n    for k,v in old_imgs.items():\n        images[k+\'_2imgs\'] = [v,v]\n        images[k+\'_3imgs\'] = [v,v,v]\n        images[k+\'_4imgs\'] = [v,v,v,v]\n    return images\n# ----------------------------------------------------\n# ----------------------------------------------------\n\n## TFORMS SETUP ###\ndef ToTensor_setup():\n    tforms = {}\n\n    tforms[\'totensor\'] = ToTensor()\n\n    return tforms\n\n\ndef ToFile_setup():\n    tforms = {}\n\n    ROOT = \'~/desktop/data/\'\n    tforms[\'tofile_npy\'] = ToFile(root=ROOT, fmt=\'npy\')\n    tforms[\'tofile_pth\'] = ToFile(root=ROOT, fmt=\'pth\')\n    tforms[\'tofile_jpg\'] = ToFile(root=ROOT, fmt=\'jpg\')\n    tforms[\'tofile_png\'] = ToFile(root=ROOT, fmt=\'png\')\n\n    return tforms\n\ndef ChannelsLast_setup():\n    tforms = {}\n\n    tforms[\'channels_last\'] = ChannelsLast()\n    tforms[\'hwc\'] = HWC()\n\n    return tforms\n\ndef ChannelsFirst_setup():\n    tforms = {}\n\n    tforms[\'channels_first\'] = ChannelsFirst()\n    tforms[\'chw\'] = CHW()\n\n    return tforms\n\ndef TypeCast_setup():\n    tforms = {}\n\n    tforms[\'byte\'] = TypeCast(\'byte\')\n    tforms[\'double\'] = TypeCast(\'double\')\n    tforms[\'float\'] = TypeCast(\'float\')\n    tforms[\'int\'] = TypeCast(\'int\')\n    tforms[\'long\'] = TypeCast(\'long\')\n    tforms[\'short\'] = TypeCast(\'short\')\n\n    return tforms\n\ndef AddChannel_setup():\n    tforms = {}\n\n    tforms[\'addchannel_axis0\'] = AddChannel(axis=0)\n    tforms[\'addchannel_axis1\'] = AddChannel(axis=1)\n    tforms[\'addchannel_axis2\'] = AddChannel(axis=2)\n\n    return tforms\n\ndef Transpose_setup():\n    tforms = {}\n\n    tforms[\'transpose_01\'] = Transpose(0, 1)\n    tforms[\'transpose_02\'] = Transpose(0, 2)\n    tforms[\'transpose_10\'] = Transpose(1, 0)\n    tforms[\'transpose_12\'] = Transpose(1, 2)\n    tforms[\'transpose_20\'] = Transpose(2, 0)\n    tforms[\'transpose_21\'] = Transpose(2, 1)\n\n    return tforms\n\ndef RangeNormalize_setup():\n    tforms = {}\n\n    tforms[\'rangenorm_01\'] = RangeNormalize(0, 1)\n    tforms[\'rangenorm_-11\'] = RangeNormalize(-1, 1)\n    tforms[\'rangenorm_-33\'] = RangeNormalize(-3, 3)\n    tforms[\'rangenorm_02\'] = RangeNormalize(0, 2)\n\n    return tforms\n\ndef StdNormalize_setup():\n    tforms = {}\n\n    tforms[\'stdnorm\'] = StdNormalize()\n\n    return tforms\n\ndef RandomCrop_setup():\n    tforms = {}\n\n    tforms[\'randomcrop_1010\'] = RandomCrop((10,10))\n    tforms[\'randomcrop_510\'] = RandomCrop((5,10))\n    tforms[\'randomcrop_105\'] = RandomCrop((10,5))\n    tforms[\'randomcrop_99\'] = RandomCrop((9,9))\n    tforms[\'randomcrop_79\'] = RandomCrop((7,9))\n    tforms[\'randomcrop_97\'] = RandomCrop((9,7))\n\n    return tforms\n\ndef SpecialCrop_setup():\n    tforms = {}\n\n    tforms[\'specialcrop_0_1010\'] = SpecialCrop((10,10),0)\n    tforms[\'specialcrop_0_510\'] = SpecialCrop((5,10),0)\n    tforms[\'specialcrop_0_105\'] = SpecialCrop((10,5),0)\n    tforms[\'specialcrop_0_99\'] = SpecialCrop((9,9),0)\n    tforms[\'specialcrop_0_79\'] = SpecialCrop((7,9),0)\n    tforms[\'specialcrop_0_97\'] = SpecialCrop((9,7),0)\n\n    tforms[\'specialcrop_1_1010\'] = SpecialCrop((10,10),1)\n    tforms[\'specialcrop_1_510\'] = SpecialCrop((5,10),1)\n    tforms[\'specialcrop_1_105\'] = SpecialCrop((10,5),1)\n    tforms[\'specialcrop_1_99\'] = SpecialCrop((9,9),1)\n    tforms[\'specialcrop_1_79\'] = SpecialCrop((7,9),1)\n    tforms[\'specialcrop_1_97\'] = SpecialCrop((9,7),1)\n\n    tforms[\'specialcrop_2_1010\'] = SpecialCrop((10,10),2)\n    tforms[\'specialcrop_2_510\'] = SpecialCrop((5,10),2)\n    tforms[\'specialcrop_2_105\'] = SpecialCrop((10,5),2)\n    tforms[\'specialcrop_2_99\'] = SpecialCrop((9,9),2)\n    tforms[\'specialcrop_2_79\'] = SpecialCrop((7,9),2)\n    tforms[\'specialcrop_2_97\'] = SpecialCrop((9,7),2)\n\n    tforms[\'specialcrop_3_1010\'] = SpecialCrop((10,10),3)\n    tforms[\'specialcrop_3_510\'] = SpecialCrop((5,10),3)\n    tforms[\'specialcrop_3_105\'] = SpecialCrop((10,5),3)\n    tforms[\'specialcrop_3_99\'] = SpecialCrop((9,9),3)\n    tforms[\'specialcrop_3_79\'] = SpecialCrop((7,9),3)\n    tforms[\'specialcrop_3_97\'] = SpecialCrop((9,7),3)\n\n    tforms[\'specialcrop_4_1010\'] = SpecialCrop((10,10),4)\n    tforms[\'specialcrop_4_510\'] = SpecialCrop((5,10),4)\n    tforms[\'specialcrop_4_105\'] = SpecialCrop((10,5),4)\n    tforms[\'specialcrop_4_99\'] = SpecialCrop((9,9),4)\n    tforms[\'specialcrop_4_79\'] = SpecialCrop((7,9),4)\n    tforms[\'specialcrop_4_97\'] = SpecialCrop((9,7),4)\n    return tforms\n\ndef Pad_setup():\n    tforms = {}\n\n    tforms[\'pad_4040\'] = Pad((40,40))\n    tforms[\'pad_3040\'] = Pad((30,40))\n    tforms[\'pad_4030\'] = Pad((40,30))\n    tforms[\'pad_3939\'] = Pad((39,39))\n    tforms[\'pad_3941\'] = Pad((39,41))\n    tforms[\'pad_4139\'] = Pad((41,39))\n    tforms[\'pad_4138\'] = Pad((41,38))\n    tforms[\'pad_3841\'] = Pad((38,41))\n\n    return tforms\n\ndef RandomFlip_setup():\n    tforms = {}\n\n    tforms[\'randomflip_h_01\'] = RandomFlip(h=True, v=False)\n    tforms[\'randomflip_h_02\'] = RandomFlip(h=True, v=False, p=0)\n    tforms[\'randomflip_h_03\'] = RandomFlip(h=True, v=False, p=1)\n    tforms[\'randomflip_h_04\'] = RandomFlip(h=True, v=False, p=0.3)\n    tforms[\'randomflip_v_01\'] = RandomFlip(h=False, v=True)\n    tforms[\'randomflip_v_02\'] = RandomFlip(h=False, v=True, p=0)\n    tforms[\'randomflip_v_03\'] = RandomFlip(h=False, v=True, p=1)\n    tforms[\'randomflip_v_04\'] = RandomFlip(h=False, v=True, p=0.3)\n    tforms[\'randomflip_hv_01\'] = RandomFlip(h=True, v=True)\n    tforms[\'randomflip_hv_02\'] = RandomFlip(h=True, v=True, p=0)\n    tforms[\'randomflip_hv_03\'] = RandomFlip(h=True, v=True, p=1)\n    tforms[\'randomflip_hv_04\'] = RandomFlip(h=True, v=True, p=0.3)\n    return tforms\n\ndef RandomOrder_setup():\n    tforms = {}\n\n    tforms[\'randomorder\'] = RandomOrder()\n\n    return tforms\n\n# ----------------------------------------------------\n# ----------------------------------------------------\n\ndef test_image_transforms_runtime(verbose=1):\n    ### MAKE TRANSFORMS ###\n    tforms = {}\n    tforms.update(ToTensor_setup())\n    #tforms.update(ToFile_setup())\n    tforms.update(ChannelsLast_setup())\n    tforms.update(ChannelsFirst_setup())\n    tforms.update(TypeCast_setup())\n    tforms.update(AddChannel_setup())\n    tforms.update(Transpose_setup())\n    tforms.update(RangeNormalize_setup())\n    tforms.update(StdNormalize_setup())\n    tforms.update(RandomCrop_setup())\n    tforms.update(SpecialCrop_setup())\n    tforms.update(Pad_setup())\n    tforms.update(RandomFlip_setup())\n    tforms.update(RandomOrder_setup())\n\n\n    ### MAKE DATA\n    images = {}\n    images.update(gray2d_setup())\n    images.update(multi_gray2d_setup())\n    images.update(color2d_setup())\n    images.update(multi_color2d_setup())\n\n    successes =[]\n    failures = []\n    for im_key, im_val in images.items():\n        for tf_key, tf_val in tforms.items():\n            try:\n                if isinstance(im_val, (tuple,list)):\n                    tf_val(*im_val)\n                else:\n                    tf_val(im_val)\n                successes.append((im_key, tf_key))\n            except:\n                failures.append((im_key, tf_key))\n\n    if verbose > 0:\n        for k, v in failures:\n            print(\'%s - %s\' % (k, v))\n\n    print(\'# SUCCESSES: \', len(successes))\n    print(\'# FAILURES: \' , len(failures))\n\n\nif __name__==\'__main__\':\n    test_image_transforms_runtime()\n'"
pywick/models/classification/dpn/__init__.py,0,b'from .dualpath import *'
pywick/models/classification/dpn/adaptive_avgmax_pool.py,6,"b'# Source: https://github.com/rwightman/pytorch-dpn-pretrained (License: Apache 2.0)\n# Pretrained: Yes\n\n"""""" PyTorch selectable adaptive pooling\nAdaptive pooling with the ability to select the type of pooling from:\n    * \'avg\' - Average pooling\n    * \'max\' - Max pooling\n    * \'avgmax\' - Sum of average and max pooling re-scaled by 0.5\n    * \'avgmaxc\' - Concatenation of average and max pooling along feature dim, doubles feature dim\n\nBoth a functional and a nn.Module version of the pooling is provided.\n\nAuthor: Ross Wightman (rwightman)\n""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef pooling_factor(pool_type=\'avg\'):\n    return 2 if pool_type == \'avgmaxc\' else 1\n\n\ndef adaptive_avgmax_pool2d(x, pool_type=\'avg\', padding=0, count_include_pad=False):\n    """"""Selectable global pooling function with dynamic input kernel size\n    """"""\n    if pool_type == \'avgmaxc\':\n        x = torch.cat([\n            F.avg_pool2d(\n                x, kernel_size=(x.size(2), x.size(3)), padding=padding, count_include_pad=count_include_pad),\n            F.max_pool2d(x, kernel_size=(x.size(2), x.size(3)), padding=padding)\n        ], dim=1)\n    elif pool_type == \'avgmax\':\n        x_avg = F.avg_pool2d(\n                x, kernel_size=(x.size(2), x.size(3)), padding=padding, count_include_pad=count_include_pad)\n        x_max = F.max_pool2d(x, kernel_size=(x.size(2), x.size(3)), padding=padding)\n        x = 0.5 * (x_avg + x_max)\n    elif pool_type == \'max\':\n        x = F.max_pool2d(x, kernel_size=(x.size(2), x.size(3)), padding=padding)\n    else:\n        if pool_type != \'avg\':\n            print(\'Invalid pool type %s specified. Defaulting to average pooling.\' % pool_type)\n        x = F.avg_pool2d(\n            x, kernel_size=(x.size(2), x.size(3)), padding=padding, count_include_pad=count_include_pad)\n    return x\n\n\nclass AdaptiveAvgMaxPool2d(torch.nn.Module):\n    """"""Selectable global pooling layer with dynamic input kernel size\n    """"""\n    def __init__(self, output_size=1, pool_type=\'avg\'):\n        super(AdaptiveAvgMaxPool2d, self).__init__()\n        self.output_size = output_size\n        self.pool_type = pool_type\n        if pool_type == \'avgmaxc\' or pool_type == \'avgmax\':\n            self.pool = nn.ModuleList([nn.AdaptiveAvgPool2d(output_size), nn.AdaptiveMaxPool2d(output_size)])\n        elif pool_type == \'max\':\n            self.pool = nn.AdaptiveMaxPool2d(output_size)\n        else:\n            if pool_type != \'avg\':\n                print(\'Invalid pool type %s specified. Defaulting to average pooling.\' % pool_type)\n            self.pool = nn.AdaptiveAvgPool2d(output_size)\n\n    def forward(self, x):\n        if self.pool_type == \'avgmaxc\':\n            x = torch.cat([p(x) for p in self.pool], dim=1)\n        elif self.pool_type == \'avgmax\':\n            x = 0.5 * torch.sum(torch.stack([p(x) for p in self.pool]), 0).squeeze(dim=0)\n        else:\n            x = self.pool(x)\n        return x\n\n    def factor(self):\n        return pooling_factor(self.pool_type)\n\n    def __repr__(self):\n        return self.__class__.__name__ + \' (\' \\\n               + \'output_size=\' + str(self.output_size) \\\n               + \', pool_type=\' + self.pool_type + \')\'\n'"
pywick/models/classification/dpn/convert_from_mxnet.py,2,"b""# Source: https://github.com/rwightman/pytorch-dpn-pretrained (License: Apache 2.0)\n# Pretrained: Yes\n\nimport os\nimport argparse\nimport torch\nfrom .model_factory import create_model\n\ntry:\n    import mxnet\n    has_mxnet = True\nexcept ImportError:\n    has_mxnet = False\n\n\ndef _convert_bn(k):\n    aux = False\n    if k == 'bias':\n        add = 'beta'\n    elif k == 'weight':\n        add = 'gamma'\n    elif k == 'running_mean':\n        aux = True\n        add = 'moving_mean'\n    elif k == 'running_var':\n        aux = True\n        add = 'moving_var'\n    else:\n        assert False, 'Unknown key: %s' % k\n    return aux, add\n\n\ndef convert_from_mxnet(model, checkpoint_prefix, debug=False):\n    _, mxnet_weights, mxnet_aux = mxnet.model.load_checkpoint(checkpoint_prefix, 0)\n    remapped_state = {}\n    for state_key in model.state_dict().keys():\n        k = state_key.split('.')\n        aux = False\n        mxnet_key = ''\n        if k[-1] == 'num_batches_tracked':\n            continue\n        if k[0] == 'features':\n            if k[1] == 'conv1_1':\n                # input block\n                mxnet_key += 'conv1_x_1__'\n                if k[2] == 'bn':\n                    mxnet_key += 'relu-sp__bn_'\n                    aux, key_add = _convert_bn(k[3])\n                    mxnet_key += key_add\n                else:\n                    assert k[3] == 'weight'\n                    mxnet_key += 'conv_' + k[3]\n            elif k[1] == 'conv5_bn_ac':\n                # bn + ac at end of features block\n                mxnet_key += 'conv5_x_x__relu-sp__bn_'\n                assert k[2] == 'bn'\n                aux, key_add = _convert_bn(k[3])\n                mxnet_key += key_add\n            else:\n                # middle blocks\n                if model.b and 'c1x1_c' in k[2]:\n                    bc_block = True  # b-variant split c-block special treatment\n                else:\n                    bc_block = False\n                ck = k[1].split('_')\n                mxnet_key += ck[0] + '_x__' + ck[1] + '_'\n                ck = k[2].split('_')\n                mxnet_key += ck[0] + '-' + ck[1]\n                if ck[1] == 'w' and len(ck) > 2:\n                    mxnet_key += '(s/2)' if ck[2] == 's2' else '(s/1)'\n                mxnet_key += '__'\n                if k[3] == 'bn':\n                    mxnet_key += 'bn_' if bc_block else 'bn__bn_'\n                    aux, key_add = _convert_bn(k[4])\n                    mxnet_key += key_add\n                else:\n                    ki = 3 if bc_block else 4\n                    assert k[ki] == 'weight'\n                    mxnet_key += 'conv_' + k[ki]\n        elif k[0] == 'classifier':\n            if 'fc6-1k_weight' in mxnet_weights:\n                mxnet_key += 'fc6-1k_'\n            else:\n                mxnet_key += 'fc6_'\n            mxnet_key += k[1]\n        else:\n            assert False, 'Unexpected token'\n\n        if debug:\n            print(mxnet_key, '=> ', state_key, end=' ')\n\n        mxnet_array = mxnet_aux[mxnet_key] if aux else mxnet_weights[mxnet_key]\n        torch_tensor = torch.from_numpy(mxnet_array.asnumpy())\n        if k[0] == 'classifier' and k[1] == 'weight':\n            torch_tensor = torch_tensor.view(torch_tensor.size() + (1, 1))\n        remapped_state[state_key] = torch_tensor\n\n        if debug:\n            print(list(torch_tensor.size()), torch_tensor.mean(), torch_tensor.std())\n\n    model.load_state_dict(remapped_state)\n\n    return model\n\nparser = argparse.ArgumentParser(description='MXNet to PyTorch DPN conversion')\nparser.add_argument('checkpoint_path', metavar='DIR', help='path to mxnet checkpoints')\nparser.add_argument('--model', '-m', metavar='MODEL', default='dpn92',\n                    help='model architecture (default: dpn92)')\n\n\ndef main():\n    args = parser.parse_args()\n    if 'dpn' not in args.model:\n        print('Error: Can only convert DPN models.')\n        exit(1)\n    if not has_mxnet:\n        print('Error: Cannot import MXNet module. Please install.')\n        exit(1)\n\n    model = create_model(args.model, num_classes=1000, pretrained=False)\n\n    model_prefix = args.model\n    if model_prefix in ['dpn107', 'dpn68b', 'dpn92']:\n        model_prefix += '-extra'\n    checkpoint_base = os.path.join(args.checkpoint_path, model_prefix)\n    convert_from_mxnet(model, checkpoint_base)\n\n    output_checkpoint = os.path.join(args.checkpoint_path, model_prefix + '.pth')\n    torch.save(model.state_dict(), output_checkpoint)\n\n\nif __name__ == '__main__':\n    main()\n"""
pywick/models/classification/dpn/dualpath.py,6,"b'"""""" PyTorch implementation of `Dual Path Networks <https://arxiv.org/abs/1707.01629/>`_.\nBased on original `MXNet implementation <https://github.com/cypw/DPNs>`_ with\nmany ideas from another PyTorch `implementation <https://github.com/oyam/pytorch-DPNs>`_.\n\nThis implementation is compatible with the pretrained weights\nfrom cypw\'s MXNet implementation.\n""""""\n\n# Source: https://github.com/rwightman/pytorch-dpn-pretrained (License: Apache 2.0)\n# Pretrained: Yes\n\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\nfrom collections import OrderedDict\n\nfrom .adaptive_avgmax_pool import adaptive_avgmax_pool2d\nfrom .convert_from_mxnet import convert_from_mxnet, has_mxnet\n\n__all__ = [\'DPN\', \'dpn68\', \'dpn68b\', \'dpn98\', \'dpn131\', \'dpn107\']  # dpn92 not pretrained\n\ndpnroot = \'https://s3.amazonaws.com/dpn-pytorch-weights/\'\ndrnroot = \'https://tigress-web.princeton.edu/~fy/drn/models/\'\ncadeneroot = \'http://data.lip6.fr/cadene/pretrainedmodels/\'\n\nmodel_urls = {\n    \'dpn68\': cadeneroot + \'dpn68-66bebafa7.pth\',\n    \'dpn68b-extra\': cadeneroot + \'dpn68b_extra-84854c156.pth\',\n    \'dpn92\': \'\',\n    \'dpn92-extra\': cadeneroot + \'dpn92_extra-b040e4a9b.pth\',\n    \'dpn98\': cadeneroot + \'dpn98-5b90dec4d.pth\',\n    \'dpn131\': cadeneroot + \'dpn131-71dfe43e0.pth\',\n    \'dpn107-extra\': cadeneroot + \'dpn107_extra-1ac7121e2.pth\'\n}\n\n\ndef dpn68(num_classes=1000, pretrained=False, test_time_pool=True):\n    """"""Pretrained DPN68 model""""""\n    model = DPN(\n        small=True, num_init_features=10, k_r=128, groups=32,\n        k_sec=(3, 4, 12, 3), inc_sec=(16, 32, 32, 64),\n        num_classes=num_classes, test_time_pool=test_time_pool)\n    if pretrained:\n        if model_urls[\'dpn68\']:\n            state_dict = model_zoo.load_url(model_urls[\'dpn68\'])\n            if state_dict.get(\'classifier.weight\') is not None:\n                state_dict[\'last_linear.weight\'] = state_dict.pop(\'classifier.weight\')\n            if state_dict.get(\'classifier.bias\') is not None:\n                state_dict[\'last_linear.bias\'] = state_dict.pop(\'classifier.bias\')\n            model.load_state_dict(state_dict)\n        elif has_mxnet and os.path.exists(\'./pretrained/\'):\n            convert_from_mxnet(model, checkpoint_prefix=\'./pretrained/dpn68\')\n        else:\n            assert False, ""Unable to load a pretrained model""\n    return model\n\n\ndef dpn68b(num_classes=1000, pretrained=False, test_time_pool=True):\n    """"""Pretrained DPN68b model""""""\n    model = DPN(\n        small=True, num_init_features=10, k_r=128, groups=32,\n        b=True, k_sec=(3, 4, 12, 3), inc_sec=(16, 32, 32, 64),\n        num_classes=num_classes, test_time_pool=test_time_pool)\n    if pretrained:\n        if model_urls[\'dpn68b-extra\']:\n            state_dict = model_zoo.load_url(model_urls[\'dpn68b-extra\'])\n            if state_dict.get(\'classifier.weight\') is not None:\n                state_dict[\'last_linear.weight\'] = state_dict.pop(\'classifier.weight\')\n            if state_dict.get(\'classifier.bias\') is not None:\n                state_dict[\'last_linear.bias\'] = state_dict.pop(\'classifier.bias\')\n            model.load_state_dict(state_dict)\n        elif has_mxnet and os.path.exists(\'./pretrained/\'):\n            convert_from_mxnet(model, checkpoint_prefix=\'./pretrained/dpn68-extra\')\n        else:\n            assert False, ""Unable to load a pretrained model""\n    return model\n\n\ndef dpn92(num_classes=1000, pretrained=False, test_time_pool=True, extra=True):\n    """"""Pretrained DPN92 model""""""\n    model = DPN(\n        num_init_features=64, k_r=96, groups=32,\n        k_sec=(3, 4, 20, 3), inc_sec=(16, 32, 24, 128),\n        num_classes=num_classes, test_time_pool=test_time_pool)\n    if pretrained:\n        # there are both imagenet 5k trained, 1k finetuned \'extra\' weights\n        # and normal imagenet 1k trained weights for dpn92\n        key = \'dpn92\'\n        if extra:\n            key += \'-extra\'\n        if model_urls[key]:\n            state_dict = model_zoo.load_url(model_urls[\'dpn92\'])\n            if state_dict.get(\'classifier.weight\') is not None:\n                state_dict[\'last_linear.weight\'] = state_dict.pop(\'classifier.weight\')\n            if state_dict.get(\'classifier.bias\') is not None:\n                state_dict[\'last_linear.bias\'] = state_dict.pop(\'classifier.bias\')\n            model.load_state_dict(state_dict)\n        elif has_mxnet and os.path.exists(\'./pretrained/\'):\n            convert_from_mxnet(model, checkpoint_prefix=\'./pretrained/\' + key)\n        else:\n            assert False, ""Unable to load a pretrained model""\n    return model\n\n\ndef dpn98(num_classes=1000, pretrained=False, test_time_pool=True):\n    """"""Pretrained DPN98 model""""""\n    model = DPN(\n        num_init_features=96, k_r=160, groups=40,\n        k_sec=(3, 6, 20, 3), inc_sec=(16, 32, 32, 128),\n        num_classes=num_classes, test_time_pool=test_time_pool)\n    if pretrained:\n        if model_urls[\'dpn98\']:\n            state_dict = model_zoo.load_url(model_urls[\'dpn98\'])\n            if state_dict.get(\'classifier.weight\') is not None:\n                state_dict[\'last_linear.weight\'] = state_dict.pop(\'classifier.weight\')\n            if state_dict.get(\'classifier.bias\') is not None:\n                state_dict[\'last_linear.bias\'] = state_dict.pop(\'classifier.bias\')\n            model.load_state_dict(state_dict)\n        elif has_mxnet and os.path.exists(\'./pretrained/\'):\n            convert_from_mxnet(model, checkpoint_prefix=\'./pretrained/dpn98\')\n        else:\n            assert False, ""Unable to load a pretrained model""\n    return model\n\n\ndef dpn131(num_classes=1000, pretrained=False, test_time_pool=True):\n    """"""Pretrained DPN131 model""""""\n    model = DPN(\n        num_init_features=128, k_r=160, groups=40,\n        k_sec=(4, 8, 28, 3), inc_sec=(16, 32, 32, 128),\n        num_classes=num_classes, test_time_pool=test_time_pool)\n    if pretrained:\n        if model_urls[\'dpn131\']:\n            state_dict = model_zoo.load_url(model_urls[\'dpn131\'])\n            if state_dict.get(\'classifier.weight\') is not None:\n                state_dict[\'last_linear.weight\'] = state_dict.pop(\'classifier.weight\')\n            if state_dict.get(\'classifier.bias\') is not None:\n                state_dict[\'last_linear.bias\'] = state_dict.pop(\'classifier.bias\')\n            model.load_state_dict(state_dict)\n        elif has_mxnet and os.path.exists(\'./pretrained/\'):\n            convert_from_mxnet(model, checkpoint_prefix=\'./pretrained/dpn131\')\n        else:\n            assert False, ""Unable to load a pretrained model""\n    return model\n\n\ndef dpn107(num_classes=1000, pretrained=False, test_time_pool=True):\n    """"""Pretrained DPN107 model""""""\n    model = DPN(\n        num_init_features=128, k_r=200, groups=50,\n        k_sec=(4, 8, 20, 3), inc_sec=(20, 64, 64, 128),\n        num_classes=num_classes, test_time_pool=test_time_pool)\n    if pretrained:\n        if model_urls[\'dpn107-extra\']:\n            state_dict = model_zoo.load_url(model_urls[\'dpn107-extra\'])\n            if state_dict.get(\'classifier.weight\') is not None:\n                state_dict[\'last_linear.weight\'] = state_dict.pop(\'classifier.weight\')\n            if state_dict.get(\'classifier.bias\') is not None:\n                state_dict[\'last_linear.bias\'] = state_dict.pop(\'classifier.bias\')\n            model.load_state_dict(state_dict)\n        elif has_mxnet and os.path.exists(\'./pretrained/\'):\n            convert_from_mxnet(model, checkpoint_prefix=\'./pretrained/dpn107-extra\')\n        else:\n            assert False, ""Unable to load a pretrained model""\n    return model\n\n\nclass CatBnAct(nn.Module):\n    def __init__(self, in_chs, activation_fn=nn.ReLU(inplace=True)):\n        super(CatBnAct, self).__init__()\n        self.bn = nn.BatchNorm2d(in_chs, eps=0.001)\n        self.act = activation_fn\n\n    def forward(self, x):\n        x = torch.cat(x, dim=1) if isinstance(x, tuple) else x\n        return self.act(self.bn(x))\n\n\nclass BnActConv2d(nn.Module):\n    def __init__(self, in_chs, out_chs, kernel_size, stride,\n                 padding=0, groups=1, activation_fn=nn.ReLU(inplace=True)):\n        super(BnActConv2d, self).__init__()\n        self.bn = nn.BatchNorm2d(in_chs, eps=0.001)\n        self.act = activation_fn\n        self.conv = nn.Conv2d(in_chs, out_chs, kernel_size, stride, padding, groups=groups, bias=False)\n\n    def forward(self, x):\n        return self.conv(self.act(self.bn(x)))\n\n\nclass InputBlock(nn.Module):\n    def __init__(self, num_init_features, kernel_size=7,\n                 padding=3, activation_fn=nn.ReLU(inplace=True)):\n        super(InputBlock, self).__init__()\n        self.conv = nn.Conv2d(\n            3, num_init_features, kernel_size=kernel_size, stride=2, padding=padding, bias=False)\n        self.bn = nn.BatchNorm2d(num_init_features, eps=0.001)\n        self.act = activation_fn\n        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.act(x)\n        x = self.pool(x)\n        return x\n\n\nclass DualPathBlock(nn.Module):\n    def __init__(\n            self, in_chs, num_1x1_a, num_3x3_b, num_1x1_c, inc, groups, block_type=\'normal\', b=False):\n        super(DualPathBlock, self).__init__()\n        self.num_1x1_c = num_1x1_c\n        self.inc = inc\n        self.b = b\n        if block_type == \'proj\':\n            self.key_stride = 1\n            self.has_proj = True\n        elif block_type == \'down\':\n            self.key_stride = 2\n            self.has_proj = True\n        else:\n            assert block_type is \'normal\'\n            self.key_stride = 1\n            self.has_proj = False\n\n        if self.has_proj:\n            # Using different member names here to allow easier parameter key matching for conversion\n            if self.key_stride == 2:\n                self.c1x1_w_s2 = BnActConv2d(\n                    in_chs=in_chs, out_chs=num_1x1_c + 2 * inc, kernel_size=1, stride=2)\n            else:\n                self.c1x1_w_s1 = BnActConv2d(\n                    in_chs=in_chs, out_chs=num_1x1_c + 2 * inc, kernel_size=1, stride=1)\n        self.c1x1_a = BnActConv2d(in_chs=in_chs, out_chs=num_1x1_a, kernel_size=1, stride=1)\n        self.c3x3_b = BnActConv2d(\n            in_chs=num_1x1_a, out_chs=num_3x3_b, kernel_size=3,\n            stride=self.key_stride, padding=1, groups=groups)\n        if b:\n            self.c1x1_c = CatBnAct(in_chs=num_3x3_b)\n            self.c1x1_c1 = nn.Conv2d(num_3x3_b, num_1x1_c, kernel_size=1, bias=False)\n            self.c1x1_c2 = nn.Conv2d(num_3x3_b, inc, kernel_size=1, bias=False)\n        else:\n            self.c1x1_c = BnActConv2d(in_chs=num_3x3_b, out_chs=num_1x1_c + inc, kernel_size=1, stride=1)\n\n    def forward(self, x):\n        x_in = torch.cat(x, dim=1) if isinstance(x, tuple) else x\n        if self.has_proj:\n            if self.key_stride == 2:\n                x_s = self.c1x1_w_s2(x_in)\n            else:\n                x_s = self.c1x1_w_s1(x_in)\n            x_s1 = x_s[:, :self.num_1x1_c, :, :]\n            x_s2 = x_s[:, self.num_1x1_c:, :, :]\n        else:\n            x_s1 = x[0]\n            x_s2 = x[1]\n        x_in = self.c1x1_a(x_in)\n        x_in = self.c3x3_b(x_in)\n        if self.b:\n            x_in = self.c1x1_c(x_in)\n            out1 = self.c1x1_c1(x_in)\n            out2 = self.c1x1_c2(x_in)\n        else:\n            x_in = self.c1x1_c(x_in)\n            out1 = x_in[:, :self.num_1x1_c, :, :]\n            out2 = x_in[:, self.num_1x1_c:, :, :]\n        resid = x_s1 + out1\n        dense = torch.cat([x_s2, out2], dim=1)\n        return resid, dense\n\n\nclass DPN(nn.Module):\n    def __init__(self, small=False, num_init_features=64, k_r=96, groups=32,\n                 b=False, k_sec=(3, 4, 20, 3), inc_sec=(16, 32, 24, 128),\n                 num_classes=1000, test_time_pool=False):\n        super(DPN, self).__init__()\n        self.test_time_pool = test_time_pool\n        self.b = b\n        bw_factor = 1 if small else 4\n\n        blocks = OrderedDict()\n\n        # conv1\n        if small:\n            blocks[\'conv1_1\'] = InputBlock(num_init_features, kernel_size=3, padding=1)\n        else:\n            blocks[\'conv1_1\'] = InputBlock(num_init_features, kernel_size=7, padding=3)\n\n        # conv2\n        bw = 64 * bw_factor\n        inc = inc_sec[0]\n        r = (k_r * bw) // (64 * bw_factor)\n        blocks[\'conv2_1\'] = DualPathBlock(num_init_features, r, r, bw, inc, groups, \'proj\', b)\n        in_chs = bw + 3 * inc\n        for i in range(2, k_sec[0] + 1):\n            blocks[\'conv2_\' + str(i)] = DualPathBlock(in_chs, r, r, bw, inc, groups, \'normal\', b)\n            in_chs += inc\n\n        # conv3\n        bw = 128 * bw_factor\n        inc = inc_sec[1]\n        r = (k_r * bw) // (64 * bw_factor)\n        blocks[\'conv3_1\'] = DualPathBlock(in_chs, r, r, bw, inc, groups, \'down\', b)\n        in_chs = bw + 3 * inc\n        for i in range(2, k_sec[1] + 1):\n            blocks[\'conv3_\' + str(i)] = DualPathBlock(in_chs, r, r, bw, inc, groups, \'normal\', b)\n            in_chs += inc\n\n        # conv4\n        bw = 256 * bw_factor\n        inc = inc_sec[2]\n        r = (k_r * bw) // (64 * bw_factor)\n        blocks[\'conv4_1\'] = DualPathBlock(in_chs, r, r, bw, inc, groups, \'down\', b)\n        in_chs = bw + 3 * inc\n        for i in range(2, k_sec[2] + 1):\n            blocks[\'conv4_\' + str(i)] = DualPathBlock(in_chs, r, r, bw, inc, groups, \'normal\', b)\n            in_chs += inc\n\n        # conv5\n        bw = 512 * bw_factor\n        inc = inc_sec[3]\n        r = (k_r * bw) // (64 * bw_factor)\n        blocks[\'conv5_1\'] = DualPathBlock(in_chs, r, r, bw, inc, groups, \'down\', b)\n        in_chs = bw + 3 * inc\n        for i in range(2, k_sec[3] + 1):\n            blocks[\'conv5_\' + str(i)] = DualPathBlock(in_chs, r, r, bw, inc, groups, \'normal\', b)\n            in_chs += inc\n        blocks[\'conv5_bn_ac\'] = CatBnAct(in_chs)\n\n        self.features = nn.Sequential(blocks)\n\n        # Using 1x1 conv for the FC layer to allow the extra pooling scheme\n        self.last_linear = nn.Conv2d(in_chs, num_classes, kernel_size=1, bias=True)\n\n    def forward(self, x):\n        x = self.features(x)\n        if not self.training and self.test_time_pool:\n            x = F.avg_pool2d(x, kernel_size=7, stride=1)\n            out = self.last_linear(x)\n            # The extra test time pool should be pooling an img_size//32 - 6 size patch\n            out = adaptive_avgmax_pool2d(out, pool_type=\'avgmax\')\n        else:\n            x = adaptive_avgmax_pool2d(x, pool_type=\'avg\')\n            out = self.last_linear(x)\n        return out.view(out.size(0), -1)\n'"
pywick/models/classification/dpn/model_factory.py,0,"b'# Source: https://github.com/rwightman/pytorch-dpn-pretrained (License: Apache 2.0)\n# Pretrained: Yes\n\nimport math\nfrom .dualpath import *\nfrom torchvision.models.resnet import resnet18, resnet34, resnet50, resnet101, resnet152\nfrom torchvision.models.densenet import densenet121, densenet169, densenet161, densenet201\nfrom torchvision.models.inception import inception_v3\nimport torchvision.transforms as transforms\nfrom PIL import Image\n\n\ndef create_model(model_name, num_classes=1000, pretrained=False, **kwargs):\n    if \'test_time_pool\' in kwargs:\n        test_time_pool = kwargs.pop(\'test_time_pool\')\n    else:\n        test_time_pool = True\n    if \'extra\' in kwargs:\n        extra = kwargs.pop(\'extra\')\n    else:\n        extra = True\n    if model_name == \'dpn68\':\n        model = dpn68(num_classes=num_classes, pretrained=pretrained, test_time_pool=test_time_pool)\n    elif model_name == \'dpn68b\':\n        model = dpn68b(num_classes=num_classes, pretrained=pretrained, test_time_pool=test_time_pool)\n    elif model_name == \'dpn92\':\n        model = dpn92(num_classes=num_classes, pretrained=pretrained, test_time_pool=test_time_pool, extra=extra)\n    elif model_name == \'dpn98\':\n        model = dpn98(num_classes=num_classes, pretrained=pretrained, test_time_pool=test_time_pool)\n    elif model_name == \'dpn131\':\n        model = dpn131(num_classes=num_classes, pretrained=pretrained, test_time_pool=test_time_pool)\n    elif model_name == \'dpn107\':\n        model = dpn107(num_classes=num_classes, pretrained=pretrained, test_time_pool=test_time_pool)\n    elif model_name == \'resnet18\':\n        model = resnet18(num_classes=num_classes, pretrained=pretrained, **kwargs)\n    elif model_name == \'resnet34\':\n        model = resnet34(num_classes=num_classes, pretrained=pretrained, **kwargs)\n    elif model_name == \'resnet50\':\n        model = resnet50(num_classes=num_classes, pretrained=pretrained, **kwargs)\n    elif model_name == \'resnet101\':\n        model = resnet101(num_classes=num_classes, pretrained=pretrained, **kwargs)\n    elif model_name == \'resnet152\':\n        model = resnet152(num_classes=num_classes, pretrained=pretrained, **kwargs)\n    elif model_name == \'densenet121\':\n        model = densenet121(num_classes=num_classes, pretrained=pretrained, **kwargs)\n    elif model_name == \'densenet161\':\n        model = densenet161(num_classes=num_classes, pretrained=pretrained, **kwargs)\n    elif model_name == \'densenet169\':\n        model = densenet169(num_classes=num_classes, pretrained=pretrained, **kwargs)\n    elif model_name == \'densenet201\':\n        model = densenet201(num_classes=num_classes, pretrained=pretrained, **kwargs)\n    elif model_name == \'inception_v3\':\n        model = inception_v3(num_classes=num_classes, pretrained=pretrained, transform_input=False, **kwargs)\n    else:\n        assert False, ""Unknown model architecture (%s)"" % model_name\n    return model\n\n\nclass LeNormalize(object):\n    """"""Normalize to -1..1 in Google Inception style\n    """"""\n    def __call__(self, tensor):\n        for t in tensor:\n            t.sub_(0.5).mul_(2.0)\n        return tensor\n\n\nDEFAULT_CROP_PCT = 0.875\n\n\ndef get_transforms_eval(model_name, img_size=224, crop_pct=None):\n    crop_pct = crop_pct or DEFAULT_CROP_PCT\n    if \'dpn\' in model_name:\n        if crop_pct is None:\n            # Use default 87.5% crop for model\'s native img_size\n            # but use 100% crop for larger than native as it\n            # improves test time results across all models.\n            if img_size == 224:\n                scale_size = int(math.floor(img_size / DEFAULT_CROP_PCT))\n            else:\n                scale_size = img_size\n        else:\n            scale_size = int(math.floor(img_size / crop_pct))\n        normalize = transforms.Normalize(\n            mean=[124 / 255, 117 / 255, 104 / 255],\n            std=[1 / (.0167 * 255)] * 3)\n    elif \'inception\' in model_name:\n        scale_size = int(math.floor(img_size / crop_pct))\n        normalize = LeNormalize()\n    else:\n        scale_size = int(math.floor(img_size / crop_pct))\n        normalize = transforms.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225])\n\n    return transforms.Compose([\n        transforms.Scale(scale_size, Image.BICUBIC),\n        transforms.CenterCrop(img_size),\n        transforms.ToTensor(),normalize])\n'"
pywick/models/classification/resnext_features/__init__.py,0,b'from .resnext101_32x4d_features import resnext101_32x4d_features\nfrom .resnext101_64x4d_features import resnext101_64x4d_features\nfrom .resnext50_32x4d_features import resnext50_32x4d_features'
pywick/models/classification/resnext_features/resnext101_32x4d_features.py,1,"b'# Source: https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/resnext_features/resnext101_32x4d_features.py\n\nfrom __future__ import print_function, division, absolute_import\nfrom functools import reduce\nimport torch.nn as nn\n\n\nclass LambdaBase(nn.Sequential):\n    def __init__(self, *args):\n        super(LambdaBase, self).__init__(*args)\n\n    def forward_prepare(self, input):\n        output = []\n        for module in self._modules.values():\n            output.append(module(input))\n        return output if output else input\n\nclass Lambda(LambdaBase):\n    def __init__(self, *args):\n        super(Lambda, self).__init__(*args)\n        self.lambda_func = identity\n\n    def forward(self, input):\n        return self.lambda_func(self.forward_prepare(input))\n\nclass LambdaMap(LambdaBase):\n    def __init__(self, *args):\n        super(LambdaMap, self).__init__(*args)\n        self.lambda_func = identity\n\n    def forward(self, input):\n        return list(map(self.lambda_func,self.forward_prepare(input)))\n\nclass LambdaReduce(LambdaBase):\n    def __init__(self, *args):\n        super(LambdaReduce, self).__init__(*args)\n        self.lambda_func = add\n\n    def forward(self, input):\n        return reduce(self.lambda_func,self.forward_prepare(input))\n\ndef identity(x): return x\n\ndef add(x, y): return x + y\n\nresnext101_32x4d_features = nn.Sequential( # Sequential,\n    nn.Conv2d(3,64,(7, 7),(2, 2),(3, 3),1,1,bias=False),\n    nn.BatchNorm2d(64),\n    nn.ReLU(),\n    nn.MaxPool2d((3, 3),(2, 2),(1, 1)),\n    nn.Sequential( # Sequential,\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(64,128,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(128),\n                        nn.ReLU(),\n                        nn.Conv2d(128,128,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(128),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(128,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(256),\n                ),\n                nn.Sequential( # Sequential,\n                    nn.Conv2d(64,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(256),\n                ),\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(256,128,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(128),\n                        nn.ReLU(),\n                        nn.Conv2d(128,128,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(128),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(128,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(256),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(256,128,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(128),\n                        nn.ReLU(),\n                        nn.Conv2d(128,128,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(128),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(128,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(256),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n    ),\n    nn.Sequential( # Sequential,\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(256,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                        nn.Conv2d(256,256,(3, 3),(2, 2),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(256,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(512),\n                ),\n                nn.Sequential( # Sequential,\n                    nn.Conv2d(256,512,(1, 1),(2, 2),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(512),\n                ),\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(512,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                        nn.Conv2d(256,256,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(256,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(512),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(512,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                        nn.Conv2d(256,256,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(256,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(512),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(512,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                        nn.Conv2d(256,256,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(256,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(512),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n    ),\n    nn.Sequential( # Sequential,\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(512,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(2, 2),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                nn.Sequential( # Sequential,\n                    nn.Conv2d(512,1024,(1, 1),(2, 2),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n    ),\n    nn.Sequential( # Sequential,\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024,1024,(3, 3),(2, 2),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024,2048,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(2048),\n                ),\n                nn.Sequential( # Sequential,\n                    nn.Conv2d(1024,2048,(1, 1),(2, 2),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(2048),\n                ),\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(2048,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024,2048,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(2048),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(2048,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024,2048,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(2048),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n    )\n)\n'"
pywick/models/classification/resnext_features/resnext101_64x4d_features.py,1,"b'# Source: https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/resnext_features/resnext101_64x4d_features.py\n\nfrom __future__ import print_function, division, absolute_import\nimport torch.nn as nn\nfrom functools import reduce\n\nclass LambdaBase(nn.Sequential):\n    def __init__(self, *args):\n        super(LambdaBase, self).__init__(*args)\n\n    def forward_prepare(self, input):\n        output = []\n        for module in self._modules.values():\n            output.append(module(input))\n        return output if output else input\n\nclass Lambda(LambdaBase):\n    def __init__(self, *args):\n        super(Lambda, self).__init__(*args)\n        self.lambda_func = identity\n\n    def forward(self, input):\n        return self.lambda_func(self.forward_prepare(input))\n\nclass LambdaMap(LambdaBase):\n    def __init__(self, *args):\n        super(LambdaMap, self).__init__(*args)\n        self.lambda_func = identity\n\n    def forward(self, input):\n        return list(map(self.lambda_func,self.forward_prepare(input)))\n\nclass LambdaReduce(LambdaBase):\n    def __init__(self, *args):\n        super(LambdaReduce, self).__init__(*args)\n        self.lambda_func = add\n\n    def forward(self, input):\n        return reduce(self.lambda_func,self.forward_prepare(input))\n\ndef identity(x): return x\n\ndef add(x, y): return x + y\n\nresnext101_64x4d_features = nn.Sequential(#Sequential,\n    nn.Conv2d(3, 64, (7, 7), (2, 2), (3, 3), 1, 1, bias = False),\n    nn.BatchNorm2d(64),\n    nn.ReLU(),\n    nn.MaxPool2d((3, 3), (2, 2), (1, 1)),\n    nn.Sequential(#Sequential,\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(64, 256, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                        nn.Conv2d(256, 256, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(256, 256, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(256),\n                ),\n                nn.Sequential(#Sequential,\n                    nn.Conv2d(64, 256, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(256),\n                ),\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(256, 256, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                        nn.Conv2d(256, 256, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(256, 256, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(256),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(256, 256, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                        nn.Conv2d(256, 256, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(256, 256, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(256),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n    ),\n    nn.Sequential(#Sequential,\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(256, 512, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512, 512, (3, 3), (2, 2), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512, 512, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(512),\n                ),\n                nn.Sequential(#Sequential,\n                    nn.Conv2d(256, 512, (1, 1), (2, 2), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(512),\n                ),\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(512, 512, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512, 512, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512, 512, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(512),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(512, 512, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512, 512, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512, 512, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(512),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(512, 512, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512, 512, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512, 512, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(512),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n    ),\n    nn.Sequential(#Sequential,\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(512, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (2, 2), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                nn.Sequential(#Sequential,\n                    nn.Conv2d(512, 1024, (1, 1), (2, 2), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n    ),\n    nn.Sequential(#Sequential,\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 2048, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(2048),\n                        nn.ReLU(),\n                        nn.Conv2d(2048, 2048, (3, 3), (2, 2), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(2048),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(2048, 2048, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(2048),\n                ),\n                nn.Sequential(#Sequential,\n                    nn.Conv2d(1024, 2048, (1, 1), (2, 2), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(2048),\n                ),\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(2048, 2048, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(2048),\n                        nn.ReLU(),\n                        nn.Conv2d(2048, 2048, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(2048),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(2048, 2048, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(2048),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(2048, 2048, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(2048),\n                        nn.ReLU(),\n                        nn.Conv2d(2048, 2048, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(2048),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(2048, 2048, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(2048),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n    )\n)\n'"
pywick/models/classification/resnext_features/resnext50_32x4d_features.py,1,"b'from __future__ import print_function, division, absolute_import\nimport torch.nn as nn\nfrom functools import reduce\n\nclass LambdaBase(nn.Sequential):\n    def __init__(self, *args):\n        super(LambdaBase, self).__init__(*args)\n\n    def forward_prepare(self, input):\n        output = []\n        for module in self._modules.values():\n            output.append(module(input))\n        return output if output else input\n\nclass Lambda(LambdaBase):\n    def __init__(self, *args):\n        super(Lambda, self).__init__(*args)\n        self.lambda_func = identity\n\n    def forward(self, input):\n        return self.lambda_func(self.forward_prepare(input))\n\nclass LambdaMap(LambdaBase):\n    def __init__(self, *args):\n        super(LambdaMap, self).__init__(*args)\n        self.lambda_func = identity\n\n    def forward(self, input):\n        return list(map(self.lambda_func,self.forward_prepare(input)))\n\nclass LambdaReduce(LambdaBase):\n    def __init__(self, *args):\n        super(LambdaReduce, self).__init__(*args)\n        self.lambda_func = add\n\n    def forward(self, input):\n        return reduce(self.lambda_func,self.forward_prepare(input))\n\ndef identity(x): return x\n\ndef add(x, y): return x + y\n\nresnext50_32x4d_features = nn.Sequential( # Sequential,\n    nn.Conv2d(3,64,(7, 7),(2, 2),(3, 3),1,1,bias=False),\n    nn.BatchNorm2d(64),\n    nn.ReLU(),\n    nn.MaxPool2d((3, 3),(2, 2),(1, 1)),\n    nn.Sequential( # Sequential,\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(64,128,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(128),\n                        nn.ReLU(),\n                        nn.Conv2d(128,128,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(128),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(128,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(256),\n                ),\n                nn.Sequential( # Sequential,\n                    nn.Conv2d(64,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(256),\n                ),\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(256,128,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(128),\n                        nn.ReLU(),\n                        nn.Conv2d(128,128,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(128),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(128,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(256),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(256,128,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(128),\n                        nn.ReLU(),\n                        nn.Conv2d(128,128,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(128),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(128,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(256),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n    ),\n    nn.Sequential( # Sequential,\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(256,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                        nn.Conv2d(256,256,(3, 3),(2, 2),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(256,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(512),\n                ),\n                nn.Sequential( # Sequential,\n                    nn.Conv2d(256,512,(1, 1),(2, 2),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(512),\n                ),\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(512,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                        nn.Conv2d(256,256,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(256,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(512),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(512,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                        nn.Conv2d(256,256,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(256,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(512),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(512,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                        nn.Conv2d(256,256,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(256,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(512),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n    ),\n    nn.Sequential( # Sequential,\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(512,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(2, 2),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                nn.Sequential( # Sequential,\n                    nn.Conv2d(512,1024,(1, 1),(2, 2),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n    ),\n    nn.Sequential( # Sequential,\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024,1024,(3, 3),(2, 2),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024,2048,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(2048),\n                ),\n                nn.Sequential( # Sequential,\n                    nn.Conv2d(1024,2048,(1, 1),(2, 2),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(2048),\n                ),\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(2048,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024,2048,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(2048),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(2048,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024,2048,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(2048),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n    )\n)'"
pywick/models/classification/testnets/__init__.py,0,"b'from .se_densenet_full import se_densenet121, se_densenet161, se_densenet169, se_densenet201'"
pywick/models/classification/testnets/large_densenet.py,2,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Source: https://github.com/Ontheway361/stanford_cs231n/blob/master/basenet/zoo/densenet.py\n\n""""""\nCreated on 2019/06/03\nauthor: lujie\n""""""\n\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n\n__all__ = [\'lg_densenet264\']\n\nclass _DenseLayer(nn.Sequential):\n\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n\n        super(_DenseLayer, self).__init__()\n\n        self.add_module(\'norm1\', nn.BatchNorm2d(num_input_features)),\n        self.add_module(\'relu1\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv1\', nn.Conv2d(num_input_features, bn_size * growth_rate, kernel_size=1, stride=1, bias=False)),\n        self.add_module(\'norm2\', nn.BatchNorm2d(bn_size * growth_rate)),\n        self.add_module(\'relu2\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv2\', nn.Conv2d(bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)),\n        self.drop_rate = drop_rate\n\n    def forward(self, x):\n\n        new_features = super(_DenseLayer, self).forward(x)\n\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n\n        return torch.cat([x, new_features], 1)\n\n\nclass _DenseBlock(nn.Sequential):\n\n    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n\n        super(_DenseBlock, self).__init__()\n\n        for i in range(num_layers):\n\n            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)\n            self.add_module(\'denselayer%d\' % (i + 1), layer)\n\n\nclass _Transition(nn.Sequential):\n\n    def __init__(self, num_input_features, num_output_features):\n\n        super(_Transition, self).__init__()\n\n        self.add_module(\'norm\', nn.BatchNorm2d(num_input_features))\n        self.add_module(\'relu\', nn.ReLU(inplace=True))\n        self.add_module(\'conv\', nn.Conv2d(num_input_features, num_output_features, kernel_size=1, stride=1, bias=False))\n        self.add_module(\'pool\', nn.AvgPool2d(kernel_size=2, stride=2))\n\n\nclass DenseNet(nn.Module):\n\n    def __init__(self, num_classes=10, mode=\'A\',  num_init_features=64, growth_rate=32, bn_size=4, drop_rate=0, **kwargs):\n        \'\'\'\n        input_args :\n            growth_rate (int) - how many filters to add each layer (`k` in paper)\n            block_config (list of 4 ints) - how many layers in each pooling block\n            num_init_features (int) - the number of filters to learn in the first convolution layer\n            bn_size (int) - multiplicative factor for number of bottle neck layers\n              (i.e. bn_size * k features in the bottleneck layer)\n            drop_rate (float) - dropout rate after each dense layer\n            num_classes (int) - number of classification classes\n        \'\'\'\n\n        super(DenseNet, self).__init__()\n\n        # First convolution\n        self.features = nn.Sequential(OrderedDict([\n            (\'conv0\', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n            (\'norm0\', nn.BatchNorm2d(num_init_features)),\n            (\'relu0\', nn.ReLU(inplace=True)),\n            (\'pool0\', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n        ]))\n\n        # Each denseblock\n        block_dict = {\n            \'A\' : (6, 12, 24, 16),    # DenseNet-121\n            \'B\' : (6, 12, 32, 32),    # DenseNet-169\n            \'C\' : (6, 12, 48, 32),    # DenseNet-201\n            \'D\' : (6, 12, 64, 48),    # DenseNet-264\n        }\n        block_config = block_dict[mode]\n\n        num_features = num_init_features\n        for i, num_layers in enumerate(block_config):\n\n            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features, bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n            self.features.add_module(\'denseblock%d\' % (i + 1), block)\n            num_features = num_features + num_layers * growth_rate\n\n            if i != len(block_config) - 1:\n\n                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)\n                self.features.add_module(\'transition%d\' % (i + 1), trans)\n                num_features = num_features // 2\n\n        # Final batch norm\n        self.features.add_module(\'norm5\', nn.BatchNorm2d(num_features))\n\n        # Linear layer\n        self.classifier = nn.Linear(num_features, num_classes)\n\n        # Official init from torch repo.\n        self._initialize_weights()\n\n\n    def _initialize_weights(self):\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.constant_(m.bias, 0)\n\n\n    def forward(self, x):\n\n        features = self.features(x)\n        out = F.relu(features, inplace=True)\n        out = F.adaptive_avg_pool2d(out, (1, 1)).view(features.size(0), -1)\n        out = self.classifier(out)\n\n        return out\n\n\ndef lg_densenet264(pretrained=True, **kwargs):\n    r""""""Densenet-264 model""""""\n    model = DenseNet(num_classes=10, mode=\'D\',  num_init_features=64, growth_rate=32, bn_size=4, drop_rate=0, **kwargs)\n    return model\n'"
pywick/models/classification/testnets/opt_densenset.py,4,"b'# Source: https://github.com/prigoyal/pytorch_memonger (GPL 3.0)\n\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.checkpoint import checkpoint_sequential\n\n__all__ = [\'OptDenseNet\', \'opt_densenet100\', \'opt_densenet121\', \'opt_densenet169\', \'opt_densenet201\', \'opt_densenet161\', \'opt_densenet264\']\n\ndef opt_densenet100(pretrained=False, **kwargs):\n    r""""""Densenet-100 model""""""\n    model = OptDenseNet(num_init_features=64, growth_rate=12, block_config=(6, 12, 24, 16), **kwargs)\n    return model\n\ndef opt_densenet121(pretrained=False, **kwargs):\n    r""""""Densenet-121 model""""""\n    model = OptDenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 24, 16), **kwargs)\n    return model\n\n\ndef opt_densenet161(pretrained=False, **kwargs):\n    r""""""Densenet-161 model""""""\n    model = OptDenseNet(num_init_features=96, growth_rate=48, block_config=(6, 12, 36, 24), **kwargs)\n    return model\n\n\ndef opt_densenet169(pretrained=False, **kwargs):\n    r""""""Densenet-169 model""""""\n    model = OptDenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 32, 32), **kwargs)\n    return model\n\n\ndef opt_densenet201(pretrained=False, **kwargs):\n    r""""""Densenet-201 model""""""\n    model = OptDenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 48, 32), **kwargs)\n    return model\n\n\ndef opt_densenet264(pretrained=False, **kwargs):\n    r""""""Densenet-264 model""""""\n    model = OptDenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 64, 48), **kwargs)\n    return model\n\n\nclass _DenseLayer(nn.Sequential):\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n        super(_DenseLayer, self).__init__()\n        self.add_module(\'norm_1\', nn.BatchNorm2d(num_input_features)),\n        self.add_module(\'relu_1\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv_1\', nn.Conv2d(num_input_features, bn_size *\n                        growth_rate, kernel_size=1, stride=1, bias=False)),\n        self.add_module(\'norm_2\', nn.BatchNorm2d(bn_size * growth_rate)),\n        self.add_module(\'relu_2\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv_2\', nn.Conv2d(bn_size * growth_rate, growth_rate,\n                        kernel_size=3, stride=1, padding=1, bias=False)),\n        self.drop_rate = drop_rate\n\n    def forward(self, x):\n        new_features = super(_DenseLayer, self).forward(x)\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n        return torch.cat([x, new_features], 1)\n\n\nclass _Transition(nn.Sequential):\n    def __init__(self, num_input_features, num_output_features):\n        super(_Transition, self).__init__()\n        self.add_module(\'norm\', nn.BatchNorm2d(num_input_features))\n        self.add_module(\'relu\', nn.ReLU(inplace=True))\n        self.add_module(\'conv\', nn.Conv2d(num_input_features, num_output_features,\n                                          kernel_size=1, stride=1, bias=False))\n        self.add_module(\'pool\', nn.AvgPool2d(kernel_size=2, stride=2))\n\n\nclass OptDenseNet(nn.Module):\n    r""""""Optimized Densenet-BC model""""""\n    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16), num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000, **kwargs):\n        super(OptDenseNet, self).__init__()\n\n        # First convolution\n        self.features = nn.Sequential(OrderedDict([\n            (\'conv0\', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n            (\'norm0\', nn.BatchNorm2d(num_init_features)),\n            (\'relu0\', nn.ReLU(inplace=True)),\n            (\'pool0\', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n        ]))\n\n        num_features = num_init_features\n        for i, num_layers in enumerate(block_config):\n            num_input_features = num_features\n            for j in range(num_layers):\n                layer = _DenseLayer(\n                    num_input_features + j * growth_rate, growth_rate, bn_size, drop_rate)\n                self.features.add_module(\'denseblock{}_layer{}\'.format((i + 1), (j + 1)), layer)\n\n            num_features = num_features + num_layers * growth_rate\n            if i != len(block_config) - 1:\n                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)\n                self.features.add_module(\'transition%d\' % (i + 1), trans)\n                num_features = num_features // 2\n\n        self.features.add_module(\'norm5\', nn.BatchNorm2d(num_features))\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x, chunks=None):\n        modules = [module for k, module in self._modules.items()][0]\n        input_var = x.detach()\n        input_var.requires_grad = True\n        input_var = checkpoint_sequential(modules, chunks, input_var)\n        input_var = F.relu(input_var, inplace=True)\n        input_var = F.avg_pool2d(input_var, kernel_size=7, stride=1).view(input_var.size(0), -1)\n        input_var = self.classifier(input_var)\n        return input_var\n'"
pywick/models/classification/testnets/pnn.py,7,"b'# Source: https://github.com/juefeix/pnn.pytorch.update/blob/master/models.py (License: Apache 2.0)\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef act_fn(act):\n    if act == \'relu\':\n        act_ = nn.ReLU(inplace=False)\n    elif act == \'lrelu\':\n        act_ = nn.LeakyReLU(inplace=True)\n    elif act == \'prelu\':\n        act_ = nn.PReLU()\n    elif act == \'rrelu\':\n        act_ = nn.RReLU(inplace=True)\n    elif act == \'elu\':\n        act_ = nn.ELU(inplace=True)\n    elif act == \'selu\':\n        act_ = nn.SELU(inplace=True)\n    elif act == \'tanh\':\n        act_ = nn.Tanh()\n    elif act == \'sigmoid\':\n        act_ = nn.Sigmoid()\n    else:\n        print(\'\\n\\nActivation function {} is not supported/understood\\n\\n\'.format(act))\n        act_ = None\n    return act_\n\n"""""" ****************** Modified (Michael Klachko) PNN Implementation ******************* """"""\n\n\nclass PerturbLayerFirst(nn.Module):  # (2) Felix added this\n    def __init__(self, in_channels=None, out_channels=None, nmasks=None, level=None, filter_size=None,\n                 debug=False, use_act=False, stride=1, act=None, unique_masks=False, mix_maps=None,\n                 train_masks=False, noise_type=\'uniform\', input_size=None):\n        super(PerturbLayerFirst, self).__init__()\n        # self.nmasks = nmasks              #per input channel\n        self.nmasks = nmasks\n        self.unique_masks = unique_masks  # same set or different sets of nmasks per input channel\n        self.train_masks = train_masks  # whether to treat noise masks as regular trainable parameters of the model\n        self.level = level  # noise magnitude\n        self.filter_size = filter_size  # if filter_size=0, layers=(perturb, conv_1x1) else layers=(conv_NxN), N=filter_size\n        self.use_act = use_act  # whether to use activation immediately after perturbing input (set it to False for the first layer)\n        # self.act = act_fn(act)            #relu, prelu, rrelu, elu, selu, tanh, sigmoid (see utils)\n        self.act = act_fn(\'sigmoid\')  # (6) Felix modified this, to sigmoid, only first layer, all-layer noise\n        self.debug = debug  # print input, mask, output values for each batch\n        self.noise_type = noise_type  # normal or uniform\n        self.in_channels = in_channels\n        self.input_size = input_size  # input image resolution (28 for MNIST, 32 for CIFAR), needed to construct masks\n        self.mix_maps = mix_maps  # whether to apply second 1x1 convolution after perturbation, to mix output feature maps\n\n        if filter_size == 1:\n            padding = 0\n            bias = True\n        elif filter_size == 3 or filter_size == 5:\n            padding = 1\n            bias = False\n        elif filter_size == 7:\n            stride = 2\n            padding = 3\n            bias = False\n\n        if self.filter_size > 0:\n            self.noise = None\n            self.layers = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=filter_size, padding=padding, stride=stride, bias=bias),\n                nn.BatchNorm2d(out_channels),\n                self.act\n            )\n        else:  # Felix: noise layer definitions here\n            noise_channels = in_channels if self.unique_masks else 1\n            shape = (1, noise_channels, self.nmasks, input_size, input_size)  # can\'t dynamically reshape masks in forward if we want to train them\n\n            self.noise = nn.Parameter(torch.Tensor(*shape), requires_grad=self.train_masks)\n            if noise_type == ""uniform"":\n                self.noise.data.uniform_(-1, 1)\n                # Felix added: Gaussian blue the noise masks\n                # self.noise = gaussian_filter(self.noise, sigma=2)\n            elif self.noise_type == \'normal\':\n                self.noise.data.normal_()\n            else:\n                print(\'\\n\\nNoise type {} is not supported / understood\\n\\n\'.format(self.noise_type))\n\n            if nmasks != 1:\n                if out_channels % in_channels != 0:\n                    print(\'\\n\\n\\nnfilters must be divisible by 3 if using multiple noise masks per input channel\\n\\n\\n\')\n                groups = in_channels\n            else:\n                groups = 1\n\n            self.layers = nn.Sequential(\n                nn.BatchNorm2d(in_channels * self.nmasks),  # (1) Felix modified this\n                self.act,\n                nn.Conv2d(in_channels * self.nmasks, out_channels, kernel_size=1, stride=1, groups=groups),\n                nn.BatchNorm2d(out_channels),\n                self.act,\n            )\n            if self.mix_maps:\n                self.mix_layers = nn.Sequential(\n                    nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, groups=1),\n                    nn.BatchNorm2d(out_channels),\n                    self.act,\n                )\n\n    def forward(self, x):\n        if self.filter_size > 0:\n            return self.layers(x)  # image, conv, batchnorm, relu\n        else:\n            y = torch.add(x.unsqueeze(2), self.noise * self.level)\n            # (10, 3, 1, 32, 32) + (1, 3, 128, 32, 32) --> (10, 3, 128, 32, 32)\n\n            y = y.view(-1, self.in_channels * self.nmasks, self.input_size, self.input_size)\n            y = self.layers(y)\n\n            if self.mix_maps:\n                y = self.mix_layers(y)\n\n            return y  # image, perturb, (relu?), conv1x1, batchnorm, relu + mix_maps (conv1x1, batchnorm relu)\n\n\nclass PerturbLayer(nn.Module):\n    def __init__(self, in_channels=None, out_channels=None, nmasks=None, level=None, filter_size=None,\n                 debug=False, use_act=False, stride=1, act=None, unique_masks=False, mix_maps=None,\n                 train_masks=False, noise_type=\'uniform\', input_size=None):\n        super(PerturbLayer, self).__init__()\n        self.nmasks = nmasks  # per input channel\n        self.unique_masks = unique_masks  # same set or different sets of nmasks per input channel\n        self.train_masks = train_masks  # whether to treat noise masks as regular trainable parameters of the model\n        self.level = level  # noise magnitude\n        self.filter_size = filter_size  # if filter_size=0, layers=(perturb, conv_1x1) else layers=(conv_NxN), N=filter_size\n        self.use_act = use_act  # whether to use activation immediately after perturbing input (set it to False for the first layer)\n        self.act = act_fn(act)  # relu, prelu, rrelu, elu, selu, tanh, sigmoid (see utils)\n        self.debug = debug  # print input, mask, output values for each batch\n        self.noise_type = noise_type  # normal or uniform\n        self.in_channels = in_channels\n        self.input_size = input_size  # input image resolution (28 for MNIST, 32 for CIFAR), needed to construct masks\n        self.mix_maps = mix_maps  # whether to apply second 1x1 convolution after perturbation, to mix output feature maps\n\n        if filter_size == 1:\n            padding = 0\n            bias = True\n        elif filter_size == 3 or filter_size == 5:\n            padding = 1\n            bias = False\n        elif filter_size == 7:\n            stride = 2\n            padding = 3\n            bias = False\n\n        if self.filter_size > 0:\n            self.noise = None\n            self.layers = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=filter_size, padding=padding, stride=stride, bias=bias),\n                nn.BatchNorm2d(out_channels),\n                self.act\n            )\n        else:\n            noise_channels = in_channels if self.unique_masks else 1\n            shape = (1, noise_channels, self.nmasks, input_size, input_size)  # can\'t dynamically reshape masks in forward if we want to train them\n            self.noise = nn.Parameter(torch.Tensor(*shape), requires_grad=self.train_masks)\n            if noise_type == ""uniform"":\n                self.noise.data.uniform_(-1, 1)\n            elif self.noise_type == \'normal\':\n                self.noise.data.normal_()\n            else:\n                print(\'\\n\\nNoise type {} is not supported / understood\\n\\n\'.format(self.noise_type))\n\n            if nmasks != 1:\n                if out_channels % in_channels != 0:\n                    print(\'\\n\\n\\nnfilters must be divisible by 3 if using multiple noise masks per input channel\\n\\n\\n\')\n                groups = in_channels\n            else:\n                groups = 1\n\n            self.layers = nn.Sequential(\n                # self.act,      #TODO orig code uses ReLU here\n                # nn.BatchNorm2d(out_channels), #TODO: orig code uses BN here\n                nn.Conv2d(in_channels * self.nmasks, out_channels, kernel_size=1, stride=1, groups=groups),\n                nn.BatchNorm2d(out_channels),\n                self.act,\n            )\n            if self.mix_maps:\n                self.mix_layers = nn.Sequential(\n                    nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, groups=1),\n                    nn.BatchNorm2d(out_channels),\n                    self.act,\n                )\n\n    def forward(self, x):\n        if self.filter_size > 0:\n            return self.layers(x)  # image, conv, batchnorm, relu\n        else:\n            y = torch.add(x.unsqueeze(2), self.noise * self.level)  # (10, 3, 1, 32, 32) + (1, 3, 128, 32, 32) --> (10, 3, 128, 32, 32)\n\n            if self.use_act:\n                y = self.act(y)\n\n            y = y.view(-1, self.in_channels * self.nmasks, self.input_size, self.input_size)\n            y = self.layers(y)\n\n            if self.mix_maps:\n                y = self.mix_layers(y)\n\n            return y  # image, perturb, (relu?), conv1x1, batchnorm, relu + mix_maps (conv1x1, batchnorm relu)\n\n\nclass PerturbBasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_channels=None, out_channels=None, stride=1, shortcut=None, nmasks=None, train_masks=False,\n                 level=None, use_act=False, filter_size=None, act=None, unique_masks=False, noise_type=None,\n                 input_size=None, pool_type=None, mix_maps=None):\n        super(PerturbBasicBlock, self).__init__()\n        self.shortcut = shortcut\n        if pool_type == \'max\':\n            pool = nn.MaxPool2d\n        elif pool_type == \'avg\':\n            pool = nn.AvgPool2d\n        else:\n            print(\'\\n\\nPool Type {} is not supported/understood\\n\\n\'.format(pool_type))\n            return\n        self.layers = nn.Sequential(\n            PerturbLayer(in_channels=in_channels, out_channels=out_channels, nmasks=nmasks, input_size=input_size,\n                         level=level, filter_size=filter_size, use_act=use_act, train_masks=train_masks,\n                         act=act, unique_masks=unique_masks, noise_type=noise_type, mix_maps=mix_maps),\n            pool(stride, stride),\n            PerturbLayer(in_channels=out_channels, out_channels=out_channels, nmasks=nmasks, input_size=input_size // stride,\n                         level=level, filter_size=filter_size, use_act=use_act, train_masks=train_masks,\n                         act=act, unique_masks=unique_masks, noise_type=noise_type, mix_maps=mix_maps),\n        )\n\n    def forward(self, x):\n        residual = x\n        y = self.layers(x)\n        if self.shortcut:\n            residual = self.shortcut(x)\n        y += residual\n        y = F.relu(y)\n        return y\n\n\nclass PerturbResNet(nn.Module):\n    def __init__(self, block, nblocks=None, avgpool=None, nfilters=None, nclasses=None, nmasks=None, input_size=32,\n                 level=None, filter_size=None, first_filter_size=None, use_act=False, train_masks=False, mix_maps=None,\n                 act=None, scale_noise=1, unique_masks=False, debug=False, noise_type=None, pool_type=None):\n        super(PerturbResNet, self).__init__()\n        self.nfilters = nfilters\n        self.unique_masks = unique_masks\n        self.noise_type = noise_type\n        self.train_masks = train_masks\n        self.pool_type = pool_type\n        self.mix_maps = mix_maps\n        self.act = act_fn(act)  # (7) Felix added this\n\n        # layers = [PerturbLayer(in_channels=3, out_channels=nfilters, nmasks=nmasks, level=level*scale_noise,\n        # debug=debug, filter_size=first_filter_size, use_act=use_act, train_masks=train_masks, input_size=input_size,\n        # act=act, unique_masks=self.unique_masks, noise_type=self.noise_type, mix_maps=mix_maps)]\n\n        layers = [PerturbLayerFirst(in_channels=3, out_channels=3 * nfilters, nmasks=nfilters * 5, level=level * scale_noise * 20,\n                                    debug=debug, filter_size=first_filter_size, use_act=use_act, train_masks=train_masks, input_size=input_size,\n                                    act=act, unique_masks=self.unique_masks, noise_type=self.noise_type, mix_maps=mix_maps)]  # scale noise 20x at 1st layer\n        # 256x3  X (5) = 3840\n        # 128x3  X (5) = 1920\n\n        # (3) Felix modified this\n\n        if first_filter_size == 7:\n            layers.append(nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n\n        # self.pre_layers = nn.Sequential(*layers)\n        self.pre_layers = nn.Sequential(*layers,  # (4) Felix modified this, not really necessary.\n                                        nn.Conv2d(self.nfilters * 3 * 1, self.nfilters, kernel_size=1, stride=1, bias=False),\n                                        # mapping 10*nfilters back to nfilters with 1x1 conv\n                                        nn.BatchNorm2d(self.nfilters),\n                                        self.act\n                                        )\n\n        self.layer1 = self._make_layer(block, 1 * nfilters, nblocks[0], stride=1, level=level, nmasks=nmasks, use_act=True,\n                                       filter_size=filter_size, act=act, input_size=input_size)\n        self.layer2 = self._make_layer(block, 2 * nfilters, nblocks[1], stride=2, level=level, nmasks=nmasks, use_act=True,\n                                       filter_size=filter_size, act=act, input_size=input_size)\n        self.layer3 = self._make_layer(block, 4 * nfilters, nblocks[2], stride=2, level=level, nmasks=nmasks, use_act=True,\n                                       filter_size=filter_size, act=act, input_size=input_size // 2)\n        self.layer4 = self._make_layer(block, 8 * nfilters, nblocks[3], stride=2, level=level, nmasks=nmasks, use_act=True,\n                                       filter_size=filter_size, act=act, input_size=input_size // 4)\n        self.avgpool = nn.AvgPool2d(avgpool, stride=1)\n        self.linear = nn.Linear(8 * nfilters * block.expansion, nclasses)\n\n    def _make_layer(self, block, out_channels, nblocks, stride=1, level=0.2, nmasks=None, use_act=False,\n                    filter_size=None, act=None, input_size=None):\n        shortcut = None\n        if stride != 1 or self.nfilters != out_channels * block.expansion:\n            shortcut = nn.Sequential(\n                nn.Conv2d(self.nfilters, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels * block.expansion),\n            )\n        layers = []\n        layers.append(block(self.nfilters, out_channels, stride, shortcut, level=level, nmasks=nmasks, use_act=use_act,\n                            filter_size=filter_size, act=act, unique_masks=self.unique_masks, noise_type=self.noise_type,\n                            train_masks=self.train_masks, input_size=input_size, pool_type=self.pool_type, mix_maps=self.mix_maps))\n        self.nfilters = out_channels * block.expansion\n        for i in range(1, nblocks):\n            layers.append(block(self.nfilters, out_channels, level=level, nmasks=nmasks, use_act=use_act,\n                                train_masks=self.train_masks, filter_size=filter_size, act=act, unique_masks=self.unique_masks,\n                                noise_type=self.noise_type, input_size=input_size // stride, pool_type=self.pool_type, mix_maps=self.mix_maps))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.pre_layers(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.linear(x)\n        return x\n\n\nclass LeNet(nn.Module):\n    def __init__(self, nfilters=None, nclasses=None, nmasks=None, level=None, filter_size=None, linear=128, input_size=28,\n                 debug=False, scale_noise=1, act=\'relu\', use_act=False, first_filter_size=None, pool_type=None,\n                 dropout=None, unique_masks=False, train_masks=False, noise_type=\'uniform\', mix_maps=None):\n        super(LeNet, self).__init__()\n        if filter_size == 5:\n            n = 5\n        else:\n            n = 4\n\n        if input_size == 32:\n            first_channels = 3\n        elif input_size == 28:\n            first_channels = 1\n\n        if pool_type == \'max\':\n            pool = nn.MaxPool2d\n        elif pool_type == \'avg\':\n            pool = nn.AvgPool2d\n        else:\n            print(\'\\n\\nPool Type {} is not supported/understood\\n\\n\'.format(pool_type))\n            return\n\n        self.linear1 = nn.Linear(nfilters * n * n, linear)\n        self.linear2 = nn.Linear(linear, nclasses)\n        self.dropout = nn.Dropout(p=dropout)\n        self.act = act_fn(act)\n        self.batch_norm = nn.BatchNorm1d(linear)\n\n        self.first_layers = nn.Sequential(\n            PerturbLayer(in_channels=first_channels, out_channels=nfilters, nmasks=nmasks, level=level * scale_noise,\n                         filter_size=first_filter_size, use_act=use_act, act=act, unique_masks=unique_masks,\n                         train_masks=train_masks, noise_type=noise_type, input_size=input_size, mix_maps=mix_maps),\n            pool(kernel_size=3, stride=2, padding=1),\n\n            PerturbLayer(in_channels=nfilters, out_channels=nfilters, nmasks=nmasks, level=level, filter_size=filter_size,\n                         use_act=True, act=act, unique_masks=unique_masks, debug=debug, train_masks=train_masks,\n                         noise_type=noise_type, input_size=input_size // 2, mix_maps=mix_maps),\n            pool(kernel_size=3, stride=2, padding=1),\n\n            PerturbLayer(in_channels=nfilters, out_channels=nfilters, nmasks=nmasks, level=level, filter_size=filter_size,\n                         use_act=True, act=act, unique_masks=unique_masks, train_masks=train_masks, noise_type=noise_type,\n                         input_size=input_size // 4, mix_maps=mix_maps),\n            pool(kernel_size=3, stride=2, padding=1),\n        )\n\n        self.last_layers = nn.Sequential(\n            self.dropout,\n            self.linear1,\n            self.batch_norm,\n            self.act,\n            self.dropout,\n            self.linear2,\n        )\n\n    def forward(self, x):\n        x = self.first_layers(x)\n        x = x.view(x.size(0), -1)\n        x = self.last_layers(x)\n        return x\n\n\n\ndef perturb_resnet18(nfilters, avgpool=4, nclasses=10, nmasks=32, level=0.1, filter_size=0, first_filter_size=0,\n                     pool_type=None, input_size=None, scale_noise=1, act=\'relu\', use_act=True, dropout=0.5,\n                     unique_masks=False, debug=False, noise_type=\'uniform\', train_masks=False, mix_maps=None):\n    return PerturbResNet(PerturbBasicBlock, [2, 2, 2, 2], nfilters=nfilters, avgpool=avgpool, nclasses=nclasses, pool_type=pool_type,\n                         scale_noise=scale_noise, nmasks=nmasks, level=level, filter_size=filter_size, train_masks=train_masks,\n                         first_filter_size=first_filter_size, act=act, use_act=use_act, unique_masks=unique_masks,\n                         debug=debug, noise_type=noise_type, input_size=input_size, mix_maps=mix_maps)\n'"
pywick/models/classification/testnets/se_densenet_full.py,12,"b'# Source: https://github.com/zhouyuangan/SE_DenseNet/blob/master/se_densenet_full.py (License: MIT)\n\nimport re\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\nfrom collections import OrderedDict\nfrom .se_module import SELayer\n\n\n__all__ = [\'SEDenseNet\', \'se_densenet121\', \'se_densenet169\', \'se_densenet201\', \'se_densenet161\']\n\n\nmodel_urls = {\n    \'densenet121\': \'https://download.pytorch.org/models/densenet121-a639ec97.pth\',\n    \'densenet169\': \'https://download.pytorch.org/models/densenet169-b2777c0a.pth\',\n    \'densenet201\': \'https://download.pytorch.org/models/densenet201-c1103571.pth\',\n    \'densenet161\': \'https://download.pytorch.org/models/densenet161-8d451a50.pth\',\n}\n\n\ndef se_densenet121(pretrained=False, is_strict=False, **kwargs):\n    r""""""Densenet-121 model from\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = SEDenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 24, 16),\n                     **kwargs)\n    if pretrained:\n        # \'.\'s are no longer allowed in module names, but pervious _DenseLayer\n        # has keys \'norm.1\', \'relu.1\', \'conv.1\', \'norm.2\', \'relu.2\', \'conv.2\'.\n        # They are also in the checkpoints in model_urls. This pattern is used\n        # to find such keys.\n        pattern = re.compile(\n            r\'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$\')\n        state_dict = model_zoo.load_url(model_urls[\'densenet121\'])\n        for key in list(state_dict.keys()):\n            res = pattern.match(key)\n            if res:\n                new_key = res.group(1) + res.group(2)\n                state_dict[new_key] = state_dict[key]\n                del state_dict[key]\n        model.load_state_dict(state_dict, strict=is_strict)\n    return model\n\n\ndef se_densenet169(pretrained=False, **kwargs):\n    r""""""Densenet-169 model from\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = SEDenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 32, 32),\n                     **kwargs)\n    if pretrained:\n        # \'.\'s are no longer allowed in module names, but pervious _DenseLayer\n        # has keys \'norm.1\', \'relu.1\', \'conv.1\', \'norm.2\', \'relu.2\', \'conv.2\'.\n        # They are also in the checkpoints in model_urls. This pattern is used\n        # to find such keys.\n        pattern = re.compile(\n            r\'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$\')\n        state_dict = model_zoo.load_url(model_urls[\'densenet169\'])\n        for key in list(state_dict.keys()):\n            res = pattern.match(key)\n            if res:\n                new_key = res.group(1) + res.group(2)\n                state_dict[new_key] = state_dict[key]\n                del state_dict[key]\n        model.load_state_dict(state_dict, strict=False)\n    return model\n\n\ndef se_densenet201(pretrained=False, **kwargs):\n    r""""""Densenet-201 model from\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = SEDenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 48, 32),\n                     **kwargs)\n    if pretrained:\n        # \'.\'s are no longer allowed in module names, but pervious _DenseLayer\n        # has keys \'norm.1\', \'relu.1\', \'conv.1\', \'norm.2\', \'relu.2\', \'conv.2\'.\n        # They are also in the checkpoints in model_urls. This pattern is used\n        # to find such keys.\n        pattern = re.compile(\n            r\'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$\')\n        state_dict = model_zoo.load_url(model_urls[\'densenet201\'])\n        for key in list(state_dict.keys()):\n            res = pattern.match(key)\n            if res:\n                new_key = res.group(1) + res.group(2)\n                state_dict[new_key] = state_dict[key]\n                del state_dict[key]\n        model.load_state_dict(state_dict, strict=False)\n    return model\n\n\ndef se_densenet161(pretrained=False, **kwargs):\n    r""""""Densenet-161 model from\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = SEDenseNet(num_init_features=96, growth_rate=48, block_config=(6, 12, 36, 24),\n                     **kwargs)\n    if pretrained:\n        # \'.\'s are no longer allowed in module names, but pervious _DenseLayer\n        # has keys \'norm.1\', \'relu.1\', \'conv.1\', \'norm.2\', \'relu.2\', \'conv.2\'.\n        # They are also in the checkpoints in model_urls. This pattern is used\n        # to find such keys.\n        pattern = re.compile(\n            r\'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$\')\n        state_dict = model_zoo.load_url(model_urls[\'densenet161\'])\n        for key in list(state_dict.keys()):\n            res = pattern.match(key)\n            if res:\n                new_key = res.group(1) + res.group(2)\n                state_dict[new_key] = state_dict[key]\n                del state_dict[key]\n        model.load_state_dict(state_dict, strict=False)\n    return model\n\n\nclass _DenseLayer(nn.Sequential):\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n        super(_DenseLayer, self).__init__()\n        # Add SELayer at here, like SE-PRE block in original paper illustrates\n        self.add_module(""selayer"", SELayer(channel=num_input_features)),\n\n        self.add_module(\'norm1\', nn.BatchNorm2d(num_input_features)),\n        self.add_module(\'relu1\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv1\', nn.Conv2d(num_input_features, bn_size *\n                        growth_rate, kernel_size=1, stride=1, bias=False)),\n        self.add_module(\'norm2\', nn.BatchNorm2d(bn_size * growth_rate)),\n        self.add_module(\'relu2\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv2\', nn.Conv2d(bn_size * growth_rate, growth_rate,\n                        kernel_size=3, stride=1, padding=1, bias=False)),\n        self.drop_rate = drop_rate\n\n    def forward(self, x):\n        new_features = super(_DenseLayer, self).forward(x)\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n        return torch.cat([x, new_features], 1)\n\n\nclass _DenseBlock(nn.Sequential):\n    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n        super(_DenseBlock, self).__init__()\n        for i in range(num_layers):\n            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)\n            self.add_module(\'denselayer%d\' % (i + 1), layer)\n\n\nclass _Transition(nn.Sequential):\n    def __init__(self, num_input_features, num_output_features):\n        super(_Transition, self).__init__()\n        self.add_module(""selayer"", SELayer(channel=num_input_features))\n        self.add_module(\'norm\', nn.BatchNorm2d(num_input_features))\n        self.add_module(\'relu\', nn.ReLU(inplace=True))\n        self.add_module(\'conv\', nn.Conv2d(num_input_features, num_output_features,\n                                          kernel_size=1, stride=1, bias=False))\n        self.add_module(\'pool\', nn.AvgPool2d(kernel_size=2, stride=2))\n\n\nclass SEDenseNet(nn.Module):\n    r""""""Densenet-BC model class, based on\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`_\n\n    Args:\n        growth_rate (int) - how many filters to add each layer (`k` in paper)\n        block_config (list of 4 ints) - how many layers in each pooling block\n        num_init_features (int) - the number of filters to learn in the first convolution layer\n        bn_size (int) - multiplicative factor for number of bottle neck layers\n          (i.e. bn_size * k features in the bottleneck layer)\n        drop_rate (float) - dropout rate after each dense layer\n        num_classes (int) - number of classification classes\n    """"""\n\n    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000):\n\n        super(SEDenseNet, self).__init__()\n\n        # First convolution\n        self.features = nn.Sequential(OrderedDict([\n            (\'conv0\', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n            (\'norm0\', nn.BatchNorm2d(num_init_features)),\n            (\'relu0\', nn.ReLU(inplace=True)),\n            (\'pool0\', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n        ]))\n\n        # Add SELayer at first convolution\n        # self.features.add_module(""SELayer_0a"", SELayer(channel=num_init_features))\n\n        # Each denseblock\n        num_features = num_init_features\n        for i, num_layers in enumerate(block_config):\n            # Add a SELayer\n            # self.features.add_module(""SELayer_%da"" % (i + 1), SELayer(channel=num_features))\n\n            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n            self.features.add_module(\'denseblock%d\' % (i + 1), block)\n\n            num_features = num_features + num_layers * growth_rate\n\n            if i != len(block_config) - 1:\n                # Add a SELayer behind each transition block\n                # self.features.add_module(""SELayer_%db"" % (i + 1), SELayer(channel=num_features))\n\n                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)\n                self.features.add_module(\'transition%d\' % (i + 1), trans)\n                num_features = num_features // 2\n\n        # Final batch norm\n        self.features.add_module(\'norm5\', nn.BatchNorm2d(num_features))\n\n        # Add SELayer\n        # self.features.add_module(""SELayer_0b"", SELayer(channel=num_features))\n\n        # Linear layer\n        self.classifier = nn.Linear(num_features, num_classes)\n\n        # Official init from torch repo.\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        features = self.features(x)\n        out = F.relu(features, inplace=True)\n        out = F.avg_pool2d(out, kernel_size=7, stride=1).view(features.size(0), -1)\n        out = self.classifier(out)\n        return out\n\n\n\ndef test_se_densenet(pretrained=False):\n    X = torch.Tensor(32, 3, 224, 224)\n\n    if pretrained:\n        model = se_densenet121(pretrained=pretrained)\n        net_state_dict = {key: value for key, value in model_zoo.load_url(""https://download.pytorch.org/models/densenet121-a639ec97.pth"").items()}\n        model.load_state_dict(net_state_dict, strict=False)\n\n    else:\n        model = se_densenet121(pretrained=pretrained)\n\n    print(model)\n    if torch.cuda.is_available():\n        X = X.cuda()\n        model = model.cuda()\n    model.eval()\n    with torch.no_grad():\n        output = model(X)\n        print(output.shape)\n'"
pywick/models/classification/testnets/se_efficient_densenet.py,15,"b'# Source: https://github.com/zhouyuangan/SE_DenseNet/blob/master/se_densenet_full.py (License: MIT)\n\n""""""This implementation is based on the DenseNet-BC implementation in torchvision\n1. using pytorch.utils.checkpoint to reduce memory comsumption\n2. add senet module\n""""""\n\nimport re\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as cp\nimport torch.utils.model_zoo as model_zoo\nfrom .se_module import SELayer\n\n__all__ = [\'DenseNet\', \'densenet121\', \'densenet169\', \'densenet201\', \'densenet161\']\n\nmodel_urls = {\n    \'densenet121\': \'https://download.pytorch.org/models/densenet121-a639ec97.pth\',\n    \'densenet169\': \'https://download.pytorch.org/models/densenet169-b2777c0a.pth\',\n    \'densenet201\': \'https://download.pytorch.org/models/densenet201-c1103571.pth\',\n    \'densenet161\': \'https://download.pytorch.org/models/densenet161-8d451a50.pth\',\n}\n\n\ndef densenet121(pretrained=False, is_strict=False, is_efficient=True, **kwargs):\n    r""""""Densenet-121 model from\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 24, 16),\n                     num_classes=4096, efficient=is_efficient, **kwargs)\n    if pretrained:\n        # \'.\'s are no longer allowed in module names, but pervious _DenseLayer\n        # has keys \'norm.1\', \'relu.1\', \'conv.1\', \'norm.2\', \'relu.2\', \'conv.2\'.\n        # They are also in the checkpoints in model_urls. This pattern is used\n        # to find such keys.\n        pattern = re.compile(\n            r\'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$\')\n        state_dict = {key: value for key, value in model_zoo.load_url(model_urls[\'densenet121\']).items() if ""classifier"" not in key}\n        for key in list(state_dict.keys()):\n            res = pattern.match(key)\n            if res:\n                new_key = res.group(1) + res.group(2)\n                state_dict[new_key] = state_dict[key]\n                del state_dict[key]\n        model.load_state_dict(state_dict, strict=is_strict)\n    return model\n\n\ndef densenet169(pretrained=False, is_strict=False, is_efficient=True, **kwargs):\n    r""""""Densenet-169 model from\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 32, 32),\n                     efficient=is_efficient, **kwargs)\n    if pretrained:\n        # \'.\'s are no longer allowed in module names, but pervious _DenseLayer\n        # has keys \'norm.1\', \'relu.1\', \'conv.1\', \'norm.2\', \'relu.2\', \'conv.2\'.\n        # They are also in the checkpoints in model_urls. This pattern is used\n        # to find such keys.\n        pattern = re.compile(\n            r\'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$\')\n        state_dict = model_zoo.load_url(model_urls[\'densenet169\'])\n        for key in list(state_dict.keys()):\n            res = pattern.match(key)\n            if res:\n                new_key = res.group(1) + res.group(2)\n                state_dict[new_key] = state_dict[key]\n                del state_dict[key]\n        model.load_state_dict(state_dict, strict=is_strict)\n    return model\n\n\ndef densenet201(pretrained=False, is_strict=False, is_efficient=True, **kwargs):\n    r""""""Densenet-201 model from\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 48, 32),\n                     efficient=is_efficient, **kwargs)\n    if pretrained:\n        # \'.\'s are no longer allowed in module names, but pervious _DenseLayer\n        # has keys \'norm.1\', \'relu.1\', \'conv.1\', \'norm.2\', \'relu.2\', \'conv.2\'.\n        # They are also in the checkpoints in model_urls. This pattern is used\n        # to find such keys.\n        pattern = re.compile(\n            r\'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$\')\n        state_dict = model_zoo.load_url(model_urls[\'densenet201\'])\n        for key in list(state_dict.keys()):\n            res = pattern.match(key)\n            if res:\n                new_key = res.group(1) + res.group(2)\n                state_dict[new_key] = state_dict[key]\n                del state_dict[key]\n        model.load_state_dict(state_dict, strict=is_strict)\n    return model\n\n\ndef densenet161(pretrained=False, is_strict=False, is_efficient=True, **kwargs):\n    r""""""Densenet-161 model from\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = DenseNet(num_init_features=96, growth_rate=48, block_config=(6, 12, 36, 24),\n                     efficient=is_efficient, **kwargs)\n    if pretrained:\n        # \'.\'s are no longer allowed in module names, but pervious _DenseLayer\n        # has keys \'norm.1\', \'relu.1\', \'conv.1\', \'norm.2\', \'relu.2\', \'conv.2\'.\n        # They are also in the checkpoints in model_urls. This pattern is used\n        # to find such keys.\n        pattern = re.compile(\n            r\'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$\')\n        state_dict = model_zoo.load_url(model_urls[\'densenet161\'])\n        for key in list(state_dict.keys()):\n            res = pattern.match(key)\n            if res:\n                new_key = res.group(1) + res.group(2)\n                state_dict[new_key] = state_dict[key]\n                del state_dict[key]\n        model.load_state_dict(state_dict, strict=is_strict)\n    return model\n\n\ndef _bn_function_factory(norm, relu, conv):\n    def bn_function(*inputs):\n        concated_features = torch.cat(inputs, 1)\n        bottleneck_output = conv(relu(norm(concated_features)))\n        return bottleneck_output\n\n    return bn_function\n\n\nclass _DenseLayer(nn.Module):\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate,\n                 efficient=False):\n        super(_DenseLayer, self).__init__()\n        # Add SELayer at here, like SE-PRE block in original paper illustrates\n        self.add_module(""selayer"", SELayer(channel=num_input_features)),\n\n        self.add_module(\'norm1\', nn.BatchNorm2d(num_input_features)),\n        self.add_module(\'relu1\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv1\', nn.Conv2d(num_input_features, bn_size *\n                                           growth_rate, kernel_size=1, stride=1, bias=False)),\n        self.add_module(\'norm2\', nn.BatchNorm2d(bn_size * growth_rate)),\n        self.add_module(\'relu2\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv2\', nn.Conv2d(bn_size * growth_rate, growth_rate,\n                                           kernel_size=3, stride=1, padding=1, bias=False)),\n        self.drop_rate = drop_rate\n        self.efficient = efficient\n\n    def forward(self, *prev_features):\n        """"""\xe5\x8e\x9f\xe6\x9c\x89\xe7\x9a\x84\xe4\xb8\xa4\xe6\xac\xa1BN\xe5\xb1\x82\xe9\x9c\x80\xe8\xa6\x81\xe6\xb6\x88\xe8\x80\x97\xe7\x9a\x84\xe4\xb8\xa4\xe5\x9d\x97\xe6\x98\xbe\xe5\xad\x98\xe7\xa9\xba\xe9\x97\xb4\xef\xbc\x8c\n        \xe9\x80\x9a\xe8\xbf\x87\xe4\xbd\xbf\xe7\x94\xa8checkpoint\xef\xbc\x8c\xe5\xae\x9e\xe7\x8e\xb0\xe4\xba\x86\xe5\x8f\xaa\xe5\xbc\x80\xe8\xbe\x9f\xe4\xb8\x80\xe5\x9d\x97\xe7\xa9\xba\xe9\x97\xb4\xe7\x94\xa8\xe6\x9d\xa5\xe5\xad\x98\xe5\x82\xa8\xe4\xb8\xad\xe9\x97\xb4\xe7\x89\xb9\xe5\xbe\x81\n        """"""\n        bn_function = _bn_function_factory(self.norm1, self.relu1, self.conv1)\n        # requires_grad is True means that model is in train status\n        # checkpoint implement shared memory storage function\n\n        if self.efficient and any(prev_feature.requires_grad for prev_feature in prev_features):\n            bottleneck_output = cp.checkpoint(bn_function, *prev_features)\n        else:\n            bottleneck_output = bn_function(*prev_features)\n\n        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features,\n                                     p=self.drop_rate,\n                                     training=self.training\n                                     )\n        return new_features\n\n\nclass _DenseBlock(nn.Module):\n    def __init__(self,\n                 num_layers,\n                 num_input_features,\n                 bn_size,\n                 growth_rate,\n                 drop_rate,\n                 efficient=False):\n        super(_DenseBlock, self).__init__()\n        for i in range(num_layers):\n            layer = _DenseLayer(\n                num_input_features + i * growth_rate,\n                growth_rate=growth_rate,\n                bn_size=bn_size,\n                drop_rate=drop_rate,\n                efficient=efficient,\n            )\n\n            self.add_module(\'denselayer%d\' % (i + 1), layer)\n\n    def forward(self, init_features):\n        features = [init_features]\n        for name, layer in self.named_children():\n            new_features = layer(*features)\n            features.append(new_features)\n        return torch.cat(features, 1)\n\n\nclass _Transition(nn.Sequential):\n    def __init__(self, num_input_features, num_output_features):\n        super(_Transition, self).__init__()\n        self.add_module(\'norm\', nn.BatchNorm2d(num_input_features))\n        self.add_module(\'relu\', nn.ReLU(inplace=True))\n        self.add_module(\'conv\', nn.Conv2d(num_input_features, num_output_features,\n                                          kernel_size=1, stride=1, bias=False))\n        self.add_module(\'pool\', nn.AvgPool2d(kernel_size=2, stride=2))\n\n\nclass DenseNet(nn.Module):\n    r""""""Densenet-BC model class, based on\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`\n    Args:\n        growth_rate (int) - how many filters to add each layer (`k` in paper)\n        block_config (list of 3 or 4 ints) - how many layers in each pooling block\n        num_init_features (int) - the number of filters to learn in the first convolution layer\n        bn_size (int) - multiplicative factor for number of bottle neck layers\n            (i.e. bn_size * k features in the bottleneck layer)\n        drop_rate (float) - dropout rate after each dense layer\n        num_classes (int) - number of classification classes\n        small_inputs (bool) - set to True if images are 32x32. Otherwise assumes images are larger.\n        efficient (bool) - set to True to use checkpointing. Much more memory efficient, but slower.\n    """"""\n\n    def __init__(self, growth_rate=12, block_config=(16, 16, 16), compression=0.5,\n                 num_init_features=24, bn_size=4, drop_rate=0,\n                 num_classes=4096, efficient=True):\n\n        super(DenseNet, self).__init__()\n        assert 0 < compression <= 1, \'compression of densenet should be between 0 and 1\'\n\n        # First convolution\n        self.features = nn.Sequential(OrderedDict([\n            (\'conv0\', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n            (\'norm0\', nn.BatchNorm2d(num_init_features)),\n            (\'relu0\', nn.ReLU(inplace=True)),\n            (\'pool0\', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n        ]))\n\n        # Each denseblock\n        num_features = num_init_features\n\n        for i, num_layers in enumerate(block_config):\n            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate, efficient=efficient,\n                                )\n\n            self.features.add_module(\'denseblock%d\' % (i + 1), block)\n\n            num_features = num_features + num_layers * growth_rate\n\n            if i != len(block_config) - 1:\n                # Add a SELayer behind each transition block\n                self.features.add_module(""SELayer_%da"" % (i + 1), SELayer(channel=num_features))\n\n                trans = _Transition(num_input_features=num_features,\n                                    num_output_features=int(num_features * compression))\n                self.features.add_module(\'transition%d\' % (i + 1), trans)\n                num_features = num_features // 2\n\n        # Final batch norm\n        self.features.add_module(\'norm5\', nn.BatchNorm2d(num_features))\n\n        # Linear layer\n        self.classifier = nn.Linear(num_features, num_classes)\n\n        # Official init from torch repo.\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        features = self.features(x)\n        out = F.relu(features, inplace=True)\n        out = F.avg_pool2d(out, kernel_size=7).view(features.size(0), -1)\n        out = self.classifier(out)\n        return out\n\n\nif __name__ == ""__main__"":\n    # X = torch.Tensor(1, 3, 224, 224)\n    X = torch.zeros([1, 3, 224, 224])\n    net = densenet121(pretrained=True, is_strict=False, is_efficient=True)\n    print(net)\n\n    if torch.cuda.is_available():\n        X = X.cuda()\n        net = net.cuda()\n\n    net.eval()\n    with torch.no_grad():\n        output = net(X)\n        print(output.shape)\n\n    # print(net)'"
pywick/models/classification/testnets/se_module.py,0,"b'# Source: https://github.com/zhouyuangan/SE_DenseNet/blob/master/se_module.py (License: MIT)\n\nfrom torch import nn\n\n\nclass SELayer(nn.Module):\n    def __init__(self, channel, reduction=16):\n        assert channel > reduction, ""Make sure your input channel bigger than reduction which equals to {}"".format(reduction)\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n                nn.Linear(channel, channel // reduction),\n                nn.ReLU(inplace=True),\n                nn.Linear(channel // reduction, channel),\n                nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y'"
pywick/models/segmentation/da_basenets/__init__.py,0,b'from . import *'
pywick/models/segmentation/da_basenets/basic.py,4,"b'""""""Basic Module for Semantic Segmentation""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n__all__ = [\'_ConvBNPReLU\', \'_ConvBN\', \'_BNPReLU\', \'_ConvBNReLU\', \'_DepthwiseConv\', \'InvertedResidual\']\n\n\nclass _ConvBNReLU(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0,\n                 dilation=1, groups=1, relu6=False, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(_ConvBNReLU, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias=False)\n        self.bn = norm_layer(out_channels)\n        self.relu = nn.ReLU6(True) if relu6 else nn.ReLU(True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n\nclass _ConvBNPReLU(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0,\n                 dilation=1, groups=1, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(_ConvBNPReLU, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias=False)\n        self.bn = norm_layer(out_channels)\n        self.prelu = nn.PReLU(out_channels)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.prelu(x)\n        return x\n\n\nclass _ConvBN(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0,\n                 dilation=1, groups=1, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(_ConvBN, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias=False)\n        self.bn = norm_layer(out_channels)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n\n\nclass _BNPReLU(nn.Module):\n    def __init__(self, out_channels, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(_BNPReLU, self).__init__()\n        self.bn = norm_layer(out_channels)\n        self.prelu = nn.PReLU(out_channels)\n\n    def forward(self, x):\n        x = self.bn(x)\n        x = self.prelu(x)\n        return x\n\n\n# -----------------------------------------------------------------\n#                      For PSPNet\n# -----------------------------------------------------------------\nclass _PSPModule(nn.Module):\n    def __init__(self, in_channels, sizes=(1, 2, 3, 6), **kwargs):\n        super(_PSPModule, self).__init__()\n        out_channels = int(in_channels / 4)\n        self.avgpools = nn.ModuleList()\n        self.convs = nn.ModuleList()\n        for size in sizes:\n            self.avgpool.append(nn.AdaptiveAvgPool2d(size))\n            self.convs.append(_ConvBNReLU(in_channels, out_channels, 1, **kwargs))\n\n    def forward(self, x):\n        size = x.size()[2:]\n        feats = [x]\n        for (avgpool, conv) in enumerate(zip(self.avgpools, self.convs)):\n            feats.append(F.interpolate(conv(avgpool(x)), size, mode=\'bilinear\', align_corners=True))\n        return torch.cat(feats, dim=1)\n\n\n# -----------------------------------------------------------------\n#                      For MobileNet\n# -----------------------------------------------------------------\nclass _DepthwiseConv(nn.Module):\n    """"""conv_dw in MobileNet""""""\n\n    def __init__(self, in_channels, out_channels, stride, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(_DepthwiseConv, self).__init__()\n        self.conv = nn.Sequential(\n            _ConvBNReLU(in_channels, in_channels, 3, stride, 1, groups=in_channels, norm_layer=norm_layer),\n            _ConvBNReLU(in_channels, out_channels, 1, norm_layer=norm_layer))\n\n    def forward(self, x):\n        return self.conv(x)\n\n\n# -----------------------------------------------------------------\n#                      For MobileNetV2\n# -----------------------------------------------------------------\nclass InvertedResidual(nn.Module):\n    def __init__(self, in_channels, out_channels, stride, expand_ratio, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(InvertedResidual, self).__init__()\n        assert stride in [1, 2]\n        self.use_res_connect = stride == 1 and in_channels == out_channels\n\n        layers = list()\n        inter_channels = int(round(in_channels * expand_ratio))\n        if expand_ratio != 1:\n            # pw\n            layers.append(_ConvBNReLU(in_channels, inter_channels, 1, relu6=True, norm_layer=norm_layer))\n        layers.extend([\n            # dw\n            _ConvBNReLU(inter_channels, inter_channels, 3, stride, 1,\n                        groups=inter_channels, relu6=True, norm_layer=norm_layer),\n            # pw-linear\n            nn.Conv2d(inter_channels, out_channels, 1, bias=False),\n            norm_layer(out_channels)])\n        self.conv = nn.Sequential(*layers)\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nif __name__ == \'__main__\':\n    x = torch.randn(1, 32, 64, 64)\n    model = InvertedResidual(32, 64, 2, 1)\n    out = model(x)\n'"
pywick/models/segmentation/da_basenets/densenet.py,9,"b'import re\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\n\nfrom collections import OrderedDict\n\n__all__ = [\'DenseNet\', \'densenet121\', \'densenet161\', \'densenet169\', \'densenet201\',\n           \'dilated_densenet121\', \'dilated_densenet161\', \'dilated_densenet169\', \'dilated_densenet201\']\n\nmodel_urls = {\n    \'densenet121\': \'https://download.pytorch.org/models/densenet121-a639ec97.pth\',\n    \'densenet169\': \'https://download.pytorch.org/models/densenet169-b2777c0a.pth\',\n    \'densenet201\': \'https://download.pytorch.org/models/densenet201-c1103571.pth\',\n    \'densenet161\': \'https://download.pytorch.org/models/densenet161-8d451a50.pth\',\n}\n\n\nclass _DenseLayer(nn.Sequential):\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, dilation=1, norm_layer=nn.BatchNorm2d):\n        super(_DenseLayer, self).__init__()\n        self.add_module(\'norm1\', norm_layer(num_input_features)),\n        self.add_module(\'relu1\', nn.ReLU(True)),\n        self.add_module(\'conv1\', nn.Conv2d(num_input_features, bn_size * growth_rate, 1, 1, bias=False)),\n        self.add_module(\'norm2\', norm_layer(bn_size * growth_rate)),\n        self.add_module(\'relu2\', nn.ReLU(True)),\n        self.add_module(\'conv2\', nn.Conv2d(bn_size * growth_rate, growth_rate, 3, 1, dilation, dilation, bias=False)),\n        self.drop_rate = drop_rate\n\n    def forward(self, x):\n        new_features = super(_DenseLayer, self).forward(x)\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n        return torch.cat([x, new_features], 1)\n\n\nclass _DenseBlock(nn.Sequential):\n    def __init__(self, num_layers, num_input_features, bn_size,\n                 growth_rate, drop_rate, dilation=1, norm_layer=nn.BatchNorm2d):\n        super(_DenseBlock, self).__init__()\n        for i in range(num_layers):\n            layer = _DenseLayer(num_input_features + i * growth_rate,\n                                growth_rate, bn_size, drop_rate, dilation, norm_layer)\n            self.add_module(\'denselayer%d\' % (i + 1), layer)\n\n\nclass _Transition(nn.Sequential):\n    def __init__(self, num_input_features, num_output_features, norm_layer=nn.BatchNorm2d):\n        super(_Transition, self).__init__()\n        self.add_module(\'norm\', norm_layer(num_input_features))\n        self.add_module(\'relu\', nn.ReLU(True))\n        self.add_module(\'conv\', nn.Conv2d(num_input_features, num_output_features, 1, 1, bias=False))\n        self.add_module(\'pool\', nn.AvgPool2d(2, 2))\n\n\n# Net\nclass DenseNet(nn.Module):\n\n    def __init__(self, growth_rate=12, block_config=(6, 12, 24, 16), num_init_features=64,\n                 bn_size=4, drop_rate=0, num_classes=1000, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(DenseNet, self).__init__()\n\n        # First convolution\n        self.features = nn.Sequential(OrderedDict([\n            (\'conv0\', nn.Conv2d(3, num_init_features, 7, 2, 3, bias=False)),\n            (\'norm0\', norm_layer(num_init_features)),\n            (\'relu0\', nn.ReLU(True)),\n            (\'pool0\', nn.MaxPool2d(3, 2, 1)),\n        ]))\n\n        # Each denseblock\n        num_features = num_init_features\n        for i, num_layers in enumerate(block_config):\n            block = _DenseBlock(num_layers, num_features, bn_size, growth_rate, drop_rate, norm_layer=norm_layer)\n            self.features.add_module(\'denseblock%d\' % (i + 1), block)\n            num_features = num_features + num_layers * growth_rate\n            if i != len(block_config) - 1:\n                trans = _Transition(num_features, num_features // 2, norm_layer=norm_layer)\n                self.features.add_module(\'transition%d\' % (i + 1), trans)\n                num_features = num_features // 2\n        self.num_features = num_features\n\n        # Final batch norm\n        self.features.add_module(\'norm5\', norm_layer(num_features))\n\n        # Linear layer\n        self.classifier = nn.Linear(num_features, num_classes)\n\n        # Official init from torch repo.\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        features = self.features(x)\n        out = F.relu(features, True)\n        out = F.adaptive_avg_pool2d(out, (1, 1)).view(features.size(0), -1)\n        out = self.classifier(out)\n        return out\n\n\nclass DilatedDenseNet(DenseNet):\n    def __init__(self, growth_rate=12, block_config=(6, 12, 24, 16), num_init_features=64,\n                 bn_size=4, drop_rate=0, num_classes=1000, dilate_scale=8, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(DilatedDenseNet, self).__init__(growth_rate, block_config, num_init_features,\n                                              bn_size, drop_rate, num_classes, norm_layer)\n        assert (dilate_scale == 8 or dilate_scale == 16), ""dilate_scale can only set as 8 or 16""\n        from functools import partial\n        if dilate_scale == 8:\n            self.features.denseblock3.apply(partial(self._conv_dilate, dilate=2))\n            self.features.denseblock4.apply(partial(self._conv_dilate, dilate=4))\n            del self.features.transition2.pool\n            del self.features.transition3.pool\n        elif dilate_scale == 16:\n            self.features.denseblock4.apply(partial(self._conv_dilate, dilate=2))\n            del self.features.transition3.pool\n\n    def _conv_dilate(self, m, dilate):\n        classname = m.__class__.__name__\n        if classname.find(\'Conv\') != -1:\n            if m.kernel_size == (3, 3):\n                m.padding = (dilate, dilate)\n                m.dilation = (dilate, dilate)\n\n\n# Specification\ndensenet_spec = {121: (64, 32, [6, 12, 24, 16]),\n                 161: (96, 48, [6, 12, 36, 24]),\n                 169: (64, 32, [6, 12, 32, 32]),\n                 201: (64, 32, [6, 12, 48, 32])}\n\n\n# Constructor\ndef get_densenet(num_layers, pretrained=False, **kwargs):\n    r""""""Densenet-BC model from the\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`_ paper.\n\n    Parameters\n    ----------\n    num_layers : int\n        Number of layers for the variant of densenet. Options are 121, 161, 169, 201.\n    pretrained : bool or str\n        Boolean value controls whether to load the default pretrained weights for model.\n        String value represents the hashtag for a certain version of pretrained weights.\n    root : str, default $TORCH_HOME/models\n        Location for keeping the model parameters.\n    """"""\n    num_init_features, growth_rate, block_config = densenet_spec[num_layers]\n    model = DenseNet(growth_rate, block_config, num_init_features, **kwargs)\n    if pretrained:\n        # \'.\'s are no longer allowed in module names, but pervious _DenseLayer\n        # has keys \'norm.1\', \'relu.1\', \'conv.1\', \'norm.2\', \'relu.2\', \'conv.2\'.\n        # They are also in the checkpoints in model_urls. This pattern is used\n        # to find such keys.\n        pattern = re.compile(\n            r\'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$\')\n        state_dict = model_zoo.load_url(model_urls[\'densenet%d\' % num_layers])\n        for key in list(state_dict.keys()):\n            res = pattern.match(key)\n            if res:\n                new_key = res.group(1) + res.group(2)\n                state_dict[new_key] = state_dict[key]\n                del state_dict[key]\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef get_dilated_densenet(num_layers, dilate_scale, pretrained=False, **kwargs):\n    num_init_features, growth_rate, block_config = densenet_spec[num_layers]\n    model = DilatedDenseNet(growth_rate, block_config, num_init_features, dilate_scale=dilate_scale)\n    if pretrained:\n        # \'.\'s are no longer allowed in module names, but pervious _DenseLayer\n        # has keys \'norm.1\', \'relu.1\', \'conv.1\', \'norm.2\', \'relu.2\', \'conv.2\'.\n        # They are also in the checkpoints in model_urls. This pattern is used\n        # to find such keys.\n        pattern = re.compile(\n            r\'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$\')\n        state_dict = model_zoo.load_url(model_urls[\'densenet%d\' % num_layers])\n        for key in list(state_dict.keys()):\n            res = pattern.match(key)\n            if res:\n                new_key = res.group(1) + res.group(2)\n                state_dict[new_key] = state_dict[key]\n                del state_dict[key]\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef densenet121(**kwargs):\n    return get_densenet(121, **kwargs)\n\n\ndef densenet161(**kwargs):\n    return get_densenet(161, **kwargs)\n\n\ndef densenet169(**kwargs):\n    return get_densenet(169, **kwargs)\n\n\ndef densenet201(**kwargs):\n    return get_densenet(201, **kwargs)\n\n\ndef dilated_densenet121(dilate_scale, **kwargs):\n    return get_dilated_densenet(121, dilate_scale, **kwargs)\n\n\ndef dilated_densenet161(dilate_scale, **kwargs):\n    return get_dilated_densenet(161, dilate_scale, **kwargs)\n\n\ndef dilated_densenet169(dilate_scale, **kwargs):\n    return get_dilated_densenet(169, dilate_scale, **kwargs)\n\n\ndef dilated_densenet201(dilate_scale, **kwargs):\n    return get_dilated_densenet(201, dilate_scale, **kwargs)\n\n\nif __name__ == \'__main__\':\n    img = torch.randn(2, 3, 224, 224)\n    model = dilated_densenet121(8)\n    outputs = model(img)\n'"
pywick/models/segmentation/da_basenets/download.py,0,"b'""""""Download files with progress bar.""""""\nimport os\nimport hashlib\nimport requests\nfrom tqdm import tqdm\n\ndef check_sha1(filename, sha1_hash):\n    """"""Check whether the sha1 hash of the file content matches the expected hash.\n    Parameters\n    ----------\n    filename : str\n        Path to the file.\n    sha1_hash : str\n        Expected sha1 hash in hexadecimal digits.\n    Returns\n    -------\n    bool\n        Whether the file content matches the expected hash.\n    """"""\n    sha1 = hashlib.sha1()\n    with open(filename, \'rb\') as f:\n        while True:\n            data = f.read(1048576)\n            if not data:\n                break\n            sha1.update(data)\n\n    sha1_file = sha1.hexdigest()\n    l = min(len(sha1_file), len(sha1_hash))\n    return sha1.hexdigest()[0:l] == sha1_hash[0:l]\n\ndef download(url, path=None, overwrite=False, sha1_hash=None):\n    """"""Download an given URL\n    Parameters\n    ----------\n    url : str\n        URL to download\n    path : str, optional\n        Destination path to store downloaded file. By default stores to the\n        current directory with same name as in url.\n    overwrite : bool, optional\n        Whether to overwrite destination file if already exists.\n    sha1_hash : str, optional\n        Expected sha1 hash in hexadecimal digits. Will ignore existing file when hash is specified\n        but doesn\'t match.\n    Returns\n    -------\n    str\n        The file path of the downloaded file.\n    """"""\n    if path is None:\n        fname = url.split(\'/\')[-1]\n    else:\n        path = os.path.expanduser(path)\n        if os.path.isdir(path):\n            fname = os.path.join(path, url.split(\'/\')[-1])\n        else:\n            fname = path\n\n    if overwrite or not os.path.exists(fname) or (sha1_hash and not check_sha1(fname, sha1_hash)):\n        dirname = os.path.dirname(os.path.abspath(os.path.expanduser(fname)))\n        if not os.path.exists(dirname):\n            os.makedirs(dirname)\n\n        print(\'Downloading %s from %s...\'%(fname, url))\n        r = requests.get(url, stream=True)\n        if r.status_code != 200:\n            raise RuntimeError(""Failed downloading url %s""%url)\n        total_length = r.headers.get(\'content-length\')\n        with open(fname, \'wb\') as f:\n            if total_length is None: # no content length header\n                for chunk in r.iter_content(chunk_size=1024):\n                    if chunk: # filter out keep-alive new chunks\n                        f.write(chunk)\n            else:\n                total_length = int(total_length)\n                for chunk in tqdm(r.iter_content(chunk_size=1024),\n                                  total=int(total_length / 1024. + 0.5),\n                                  unit=\'KB\', unit_scale=False, dynamic_ncols=True):\n                    f.write(chunk)\n\n        if sha1_hash and not check_sha1(fname, sha1_hash):\n            raise UserWarning(\'File {} is downloaded but the content hash does not match. \' \\\n                              \'The repo may be outdated or download may be incomplete. \' \\\n                              \'If the ""repo_url"" is overridden, consider switching to \' \\\n                              \'the default repo.\'.format(fname))\n\n    return fname'"
pywick/models/segmentation/da_basenets/fcn.py,5,"b'import torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .vgg import vgg16\n\n__all__ = [\'FCN8s\', \'FCN16s\', \'FCN32s\']\n\n\nclass FCN32s(nn.Module):\n    """"""There are some difference from original fcn""""""\n\n    def __init__(self, num_classes, backbone=\'vgg16\', aux=False, pretrained=True, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(FCN32s, self).__init__()\n        self.aux = aux\n        if backbone == \'vgg16\':\n            self.pretrained = vgg16(pretrained=pretrained).features\n        else:\n            raise RuntimeError(\'unknown backbone: {}\'.format(backbone))\n        self.head = _FCNHead(512, num_classes, norm_layer)\n        if aux:\n            self.auxlayer = _FCNHead(512, num_classes, norm_layer)\n\n        self.__setattr__(\'exclusive\', [\'head\', \'auxlayer\'] if aux else [\'head\'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        pool5 = self.pretrained(x)\n\n        outputs = []\n        out = self.head(pool5)\n        out = F.interpolate(out, size, mode=\'bilinear\', align_corners=True)\n        outputs.append(out)\n\n        if self.aux:\n            auxout = self.auxlayer(pool5)\n            auxout = F.interpolate(auxout, size, mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout)\n\n        return tuple(outputs)\n\n\nclass FCN16s(nn.Module):\n    def __init__(self, num_classes, backbone=\'vgg16\', aux=False, pretrained=True, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(FCN16s, self).__init__()\n        self.aux = aux\n        if backbone == \'vgg16\':\n            self.pretrained = vgg16(pretrained=pretrained).features\n        else:\n            raise RuntimeError(\'unknown backbone: {}\'.format(backbone))\n        self.pool4 = nn.Sequential(*self.pretrained[:24])\n        self.pool5 = nn.Sequential(*self.pretrained[24:])\n        self.head = _FCNHead(512, num_classes, norm_layer)\n        self.score_pool4 = nn.Conv2d(512, num_classes, 1)\n        if aux:\n            self.auxlayer = _FCNHead(512, num_classes, norm_layer)\n\n        self.__setattr__(\'exclusive\', [\'head\', \'score_pool4\', \'auxlayer\'] if aux else [\'head\', \'score_pool4\'])\n\n    def forward(self, x):\n        pool4 = self.pool4(x)\n        pool5 = self.pool5(pool4)\n\n        outputs = []\n        score_fr = self.head(pool5)\n\n        score_pool4 = self.score_pool4(pool4)\n\n        upscore2 = F.interpolate(score_fr, score_pool4.size()[2:], mode=\'bilinear\', align_corners=True)\n        fuse_pool4 = upscore2 + score_pool4\n\n        out = F.interpolate(fuse_pool4, x.size()[2:], mode=\'bilinear\', align_corners=True)\n        outputs.append(out)\n\n        if self.aux:\n            auxout = self.auxlayer(pool5)\n            auxout = F.interpolate(auxout, x.size()[2:], mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout)\n\n        return tuple(outputs)\n\n\nclass FCN8s(nn.Module):\n    def __init__(self, num_classes, backbone=\'vgg16\', aux=False, pretrained=True, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(FCN8s, self).__init__()\n        self.aux = aux\n        if backbone == \'vgg16\':\n            self.pretrained = vgg16(pretrained=pretrained).features\n        else:\n            raise RuntimeError(\'unknown backbone: {}\'.format(backbone))\n        self.pool3 = nn.Sequential(*self.pretrained[:17])\n        self.pool4 = nn.Sequential(*self.pretrained[17:24])\n        self.pool5 = nn.Sequential(*self.pretrained[24:])\n        self.head = _FCNHead(512, num_classes, norm_layer)\n        self.score_pool3 = nn.Conv2d(256, num_classes, 1)\n        self.score_pool4 = nn.Conv2d(512, num_classes, 1)\n        if aux:\n            self.auxlayer = _FCNHead(512, num_classes, norm_layer)\n\n        self.__setattr__(\'exclusive\',\n                         [\'head\', \'score_pool3\', \'score_pool4\', \'auxlayer\'] if aux else [\'head\', \'score_pool3\', \'score_pool4\'])\n\n    def forward(self, x):\n        pool3 = self.pool3(x)\n        pool4 = self.pool4(pool3)\n        pool5 = self.pool5(pool4)\n\n        outputs = []\n        score_fr = self.head(pool5)\n\n        score_pool4 = self.score_pool4(pool4)\n        score_pool3 = self.score_pool3(pool3)\n\n        upscore2 = F.interpolate(score_fr, score_pool4.size()[2:], mode=\'bilinear\', align_corners=True)\n        fuse_pool4 = upscore2 + score_pool4\n\n        upscore_pool4 = F.interpolate(fuse_pool4, score_pool3.size()[2:], mode=\'bilinear\', align_corners=True)\n        fuse_pool3 = upscore_pool4 + score_pool3\n\n        out = F.interpolate(fuse_pool3, x.size()[2:], mode=\'bilinear\', align_corners=True)\n        outputs.append(out)\n\n        if self.aux:\n            auxout = self.auxlayer(pool5)\n            auxout = F.interpolate(auxout, x.size()[2:], mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout)\n\n        return tuple(outputs)\n\n\nclass _FCNHead(nn.Module):\n    def __init__(self, in_channels, channels, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(_FCNHead, self).__init__()\n        inter_channels = in_channels // 4\n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n            norm_layer(inter_channels),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.1),\n            nn.Conv2d(inter_channels, channels, 1)\n        )\n\n    def forward(self, x):\n        return self.block(x)\n\n\n# def get_fcn32s(dataset=\'pascal_voc\', backbone=\'vgg16\', pretrained=False, root=\'~/.torch/models\',\n#                pretrained_base=True, **kwargs):\n#     acronyms = {\n#         \'pascal_voc\': \'pascal_voc\',\n#         \'pascal_aug\': \'pascal_aug\',\n#         \'ade20k\': \'ade\',\n#         \'coco\': \'coco\',\n#         \'citys\': \'citys\',\n#     }\n#     from ..data.dataloader import datasets\n#     model = FCN32s(datasets[dataset].NUM_CLASS, backbone=backbone, pretrained_base=pretrained_base, **kwargs)\n#     if pretrained:\n#         from .model_store import get_model_file\n#         model.load_state_dict(torch.load(get_model_file(\'fcn32s_%s_%s\' % (backbone, acronyms[dataset]), root=root)))\n#     return model\n#\n#\n# def get_fcn16s(dataset=\'pascal_voc\', backbone=\'vgg16\', pretrained=False, root=\'~/.torch/models\',\n#                pretrained_base=True, **kwargs):\n#     acronyms = {\n#         \'pascal_voc\': \'pascal_voc\',\n#         \'pascal_aug\': \'pascal_aug\',\n#         \'ade20k\': \'ade\',\n#         \'coco\': \'coco\',\n#         \'citys\': \'citys\',\n#     }\n#     from ..data.dataloader import datasets\n#     model = FCN16s(datasets[dataset].NUM_CLASS, backbone=backbone, pretrained_base=pretrained_base, **kwargs)\n#     if pretrained:\n#         from .model_store import get_model_file\n#         model.load_state_dict(torch.load(get_model_file(\'fcn16s_%s_%s\' % (backbone, acronyms[dataset]), root=root)))\n#     return model\n#\n#\n# def get_fcn8s(dataset=\'pascal_voc\', backbone=\'vgg16\', pretrained=False, root=\'~/.torch/models\',\n#               pretrained_base=True, **kwargs):\n#     acronyms = {\n#         \'pascal_voc\': \'pascal_voc\',\n#         \'pascal_aug\': \'pascal_aug\',\n#         \'ade20k\': \'ade\',\n#         \'coco\': \'coco\',\n#         \'citys\': \'citys\',\n#     }\n#     from ..data.dataloader import datasets\n#     model = FCN8s(datasets[dataset].NUM_CLASS, backbone=backbone, pretrained_base=pretrained_base, **kwargs)\n#     if pretrained:\n#         from .model_store import get_model_file\n#         model.load_state_dict(torch.load(get_model_file(\'fcn8s_%s_%s\' % (backbone, acronyms[dataset]), root=root)))\n#     return model\n#\n#\n# def get_fcn32s_vgg16_voc(**kwargs):\n#     return get_fcn32s(\'pascal_voc\', \'vgg16\', **kwargs)\n#\n#\n# def get_fcn16s_vgg16_voc(**kwargs):\n#     return get_fcn16s(\'pascal_voc\', \'vgg16\', **kwargs)\n#\n#\n# def get_fcn8s_vgg16_voc(**kwargs):\n#     return get_fcn8s(\'pascal_voc\', \'vgg16\', **kwargs)\n#\n#\n# if __name__ == \'__main__\':\n#     model = FCN16s(21)\n#     print(model)\n'"
pywick/models/segmentation/da_basenets/jpu.py,4,"b'""""""Joint Pyramid Upsampling""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n__all__ = [\'JPU\']\n\n\nclass SeparableConv2d(nn.Module):\n    def __init__(self, inplanes, planes, kernel_size=3, stride=1, padding=1,\n                 dilation=1, bias=False, norm_layer=nn.BatchNorm2d):\n        super(SeparableConv2d, self).__init__()\n        self.conv = nn.Conv2d(inplanes, inplanes, kernel_size, stride, padding, dilation, groups=inplanes, bias=bias)\n        self.bn = norm_layer(inplanes)\n        self.pointwise = nn.Conv2d(inplanes, planes, 1, bias=bias)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.pointwise(x)\n        return x\n\n\n# copy from: https://github.com/wuhuikai/FastFCN/blob/master/encoding/nn/customize.py\nclass JPU(nn.Module):\n    def __init__(self, in_channels, width=512, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(JPU, self).__init__()\n\n        self.conv5 = nn.Sequential(\n            nn.Conv2d(in_channels[-1], width, 3, padding=1, bias=False),\n            norm_layer(width),\n            nn.ReLU(True))\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(in_channels[-2], width, 3, padding=1, bias=False),\n            norm_layer(width),\n            nn.ReLU(True))\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(in_channels[-3], width, 3, padding=1, bias=False),\n            norm_layer(width),\n            nn.ReLU(True))\n\n        self.dilation1 = nn.Sequential(\n            SeparableConv2d(3 * width, width, 3, padding=1, dilation=1, bias=False),\n            norm_layer(width),\n            nn.ReLU(True))\n        self.dilation2 = nn.Sequential(\n            SeparableConv2d(3 * width, width, 3, padding=2, dilation=2, bias=False),\n            norm_layer(width),\n            nn.ReLU(True))\n        self.dilation3 = nn.Sequential(\n            SeparableConv2d(3 * width, width, 3, padding=4, dilation=4, bias=False),\n            norm_layer(width),\n            nn.ReLU(True))\n        self.dilation4 = nn.Sequential(\n            SeparableConv2d(3 * width, width, 3, padding=8, dilation=8, bias=False),\n            norm_layer(width),\n            nn.ReLU(True))\n\n    def forward(self, *inputs):\n        feats = [self.conv5(inputs[-1]), self.conv4(inputs[-2]), self.conv3(inputs[-3])]\n        size = feats[-1].size()[2:]\n        feats[-2] = F.interpolate(feats[-2], size, mode=\'bilinear\', align_corners=True)\n        feats[-3] = F.interpolate(feats[-3], size, mode=\'bilinear\', align_corners=True)\n        feat = torch.cat(feats, dim=1)\n        feat = torch.cat([self.dilation1(feat), self.dilation2(feat), self.dilation3(feat), self.dilation4(feat)],\n                         dim=1)\n\n        return inputs[0], inputs[1], inputs[2], feat\n'"
pywick/models/segmentation/da_basenets/model_store.py,0,"b'""""""Model store which provides pretrained models.""""""\nfrom __future__ import print_function\n\nimport os\nimport zipfile\n\nfrom .download import download, check_sha1\n\n__all__ = [\'get_model_file\', \'get_resnet_file\']\n\n_model_sha1 = {name: checksum for checksum, name in [\n    (\'25c4b50959ef024fcc050213a06b614899f94b3d\', \'resnet50\'),\n    (\'2a57e44de9c853fa015b172309a1ee7e2d0e4e2a\', \'resnet101\'),\n    (\'0d43d698c66aceaa2bc0309f55efdd7ff4b143af\', \'resnet152\'),\n]}\n\nencoding_repo_url = \'https://hangzh.s3.amazonaws.com/\'\n_url_format = \'{repo_url}encoding/models/{file_name}.zip\'\n\n\ndef short_hash(name):\n    if name not in _model_sha1:\n        raise ValueError(\'Pretrained model for {name} is not available.\'.format(name=name))\n    return _model_sha1[name][:8]\n\n\ndef get_resnet_file(name, root=\'~/.torch/models\'):\n    file_name = \'{name}-{short_hash}\'.format(name=name, short_hash=short_hash(name))\n    root = os.path.expanduser(root)\n\n    file_path = os.path.join(root, file_name + \'.pth\')\n    sha1_hash = _model_sha1[name]\n    if os.path.exists(file_path):\n        if check_sha1(file_path, sha1_hash):\n            return file_path\n        else:\n            print(\'Mismatch in the content of model file {} detected.\' +\n                  \' Downloading again.\'.format(file_path))\n    else:\n        print(\'Model file {} is not found. Downloading.\'.format(file_path))\n\n    if not os.path.exists(root):\n        os.makedirs(root)\n\n    zip_file_path = os.path.join(root, file_name + \'.zip\')\n    repo_url = os.environ.get(\'ENCODING_REPO\', encoding_repo_url)\n    if repo_url[-1] != \'/\':\n        repo_url = repo_url + \'/\'\n    download(_url_format.format(repo_url=repo_url, file_name=file_name),\n             path=zip_file_path,\n             overwrite=True)\n    with zipfile.ZipFile(zip_file_path) as zf:\n        zf.extractall(root)\n    os.remove(zip_file_path)\n\n    if check_sha1(file_path, sha1_hash):\n        return file_path\n    else:\n        raise ValueError(\'Downloaded file has different hash. Please try again.\')\n\n\ndef get_model_file(name, root=\'~/.torch/models\'):\n    root = os.path.expanduser(root)\n    file_path = os.path.join(root, name + \'.pth\')\n    if os.path.exists(file_path):\n        return file_path\n    else:\n        raise ValueError(\'Model file is not found. Downloading or trainning.\')\n'"
pywick/models/segmentation/da_basenets/resnet.py,8,"b'import torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\', \'resnet152\']\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    """"""1x1 convolution""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, norm_layer=nn.BatchNorm2d):\n        super(BasicBlock, self).__init__()\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, norm_layer=nn.BatchNorm2d):\n        super(Bottleneck, self).__init__()\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, planes)\n        self.bn1 = norm_layer(planes)\n        self.conv2 = conv3x3(planes, planes, stride)\n        self.bn2 = norm_layer(planes)\n        self.conv3 = conv1x1(planes, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(ResNet, self).__init__()\n        self.inplanes = 64\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = norm_layer(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0], norm_layer=norm_layer)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, norm_layer=norm_layer)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, norm_layer=norm_layer)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, norm_layer=norm_layer)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, norm_layer=nn.BatchNorm2d):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet18\']))\n    return model\n\n\ndef resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet34\']))\n    return model\n\n\ndef resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\']))\n    return model\n\n\ndef resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet101\']))\n    return model\n\n\ndef resnet152(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet152\']))\n    return model\n\n\nif __name__ == \'__main__\':\n    import torch\n    img = torch.randn(4, 3, 224, 224)\n    model = resnet50(True)\n    output = model(img)'"
pywick/models/segmentation/da_basenets/resnetv1b.py,11,"b'import torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\n__all__ = [\'ResNetV1b\', \'resnet18_v1b\', \'resnet34_v1b\', \'resnet50_v1b\',\n           \'resnet101_v1b\', \'resnet152_v1b\', \'resnet152_v1s\', \'resnet101_v1s\', \'resnet50_v1s\']\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\nclass BasicBlockV1b(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None,\n                 previous_dilation=1, norm_layer=nn.BatchNorm2d):\n        super(BasicBlockV1b, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, 3, stride,\n                               dilation, dilation, bias=False)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(True)\n        self.conv2 = nn.Conv2d(planes, planes, 3, 1, previous_dilation,\n                               dilation=previous_dilation, bias=False)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass BottleneckV1b(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None,\n                 previous_dilation=1, norm_layer=nn.BatchNorm2d):\n        super(BottleneckV1b, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n        self.bn1 = norm_layer(planes)\n        self.conv2 = nn.Conv2d(planes, planes, 3, stride,\n                               dilation, dilation, bias=False)\n        self.bn2 = norm_layer(planes)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNetV1b(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000, dilated=True, deep_stem=False,\n                 zero_init_residual=False, norm_layer=nn.BatchNorm2d, **kwargs):\n        self.inplanes = 128 if deep_stem else 64\n        super(ResNetV1b, self).__init__()\n        if deep_stem:\n            self.conv1 = nn.Sequential(\n                nn.Conv2d(3, 64, 3, 2, 1, bias=False),\n                norm_layer(64),\n                nn.ReLU(True),\n                nn.Conv2d(64, 64, 3, 1, 1, bias=False),\n                norm_layer(64),\n                nn.ReLU(True),\n                nn.Conv2d(64, 128, 3, 1, 1, bias=False)\n            )\n        else:\n            self.conv1 = nn.Conv2d(3, 64, 7, 2, 3, bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(True)\n        self.maxpool = nn.MaxPool2d(3, 2, 1)\n        self.layer1 = self._make_layer(block, 64, layers[0], norm_layer=norm_layer)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, norm_layer=norm_layer)\n        if dilated:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2, norm_layer=norm_layer)\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4, norm_layer=norm_layer)\n        else:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=2, norm_layer=norm_layer)\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=2, norm_layer=norm_layer)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, BottleneckV1b):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlockV1b):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, norm_layer=nn.BatchNorm2d):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion, 1, stride, bias=False),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        if dilation in (1, 2):\n            layers.append(block(self.inplanes, planes, stride, dilation=1,\n                                downsample=downsample, previous_dilation=dilation))\n        elif dilation == 4:\n            layers.append(block(self.inplanes, planes, stride, dilation=2,\n                                downsample=downsample, previous_dilation=dilation))\n        else:\n            raise RuntimeError(""=> unknown dilation size: {}"".format(dilation))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dilation=dilation, previous_dilation=dilation))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef resnet18_v1b(pretrained=False, **kwargs):\n    model = ResNetV1b(BasicBlockV1b, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        old_dict = model_zoo.load_url(model_urls[\'resnet18\'])\n        model_dict = model.state_dict()\n        old_dict = {k: v for k, v in old_dict.items() if (k in model_dict)}\n        model_dict.update(old_dict)\n        model.load_state_dict(model_dict)\n    return model\n\n\ndef resnet34_v1b(pretrained=False, **kwargs):\n    model = ResNetV1b(BasicBlockV1b, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        old_dict = model_zoo.load_url(model_urls[\'resnet34\'])\n        model_dict = model.state_dict()\n        old_dict = {k: v for k, v in old_dict.items() if (k in model_dict)}\n        model_dict.update(old_dict)\n        model.load_state_dict(model_dict)\n    return model\n\n\ndef resnet50_v1b(pretrained=False, **kwargs):\n    model = ResNetV1b(BottleneckV1b, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        old_dict = model_zoo.load_url(model_urls[\'resnet50\'])\n        model_dict = model.state_dict()\n        old_dict = {k: v for k, v in old_dict.items() if (k in model_dict)}\n        model_dict.update(old_dict)\n        model.load_state_dict(model_dict)\n    return model\n\n\ndef resnet101_v1b(pretrained=False, **kwargs):\n    model = ResNetV1b(BottleneckV1b, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        old_dict = model_zoo.load_url(model_urls[\'resnet101\'])\n        model_dict = model.state_dict()\n        old_dict = {k: v for k, v in old_dict.items() if (k in model_dict)}\n        model_dict.update(old_dict)\n        model.load_state_dict(model_dict)\n    return model\n\n\ndef resnet152_v1b(pretrained=False, **kwargs):\n    model = ResNetV1b(BottleneckV1b, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        old_dict = model_zoo.load_url(model_urls[\'resnet152\'])\n        model_dict = model.state_dict()\n        old_dict = {k: v for k, v in old_dict.items() if (k in model_dict)}\n        model_dict.update(old_dict)\n        model.load_state_dict(model_dict)\n    return model\n\n\ndef resnet50_v1s(pretrained=False, model_root=\'~/.torch/models\', **kwargs):\n    model = ResNetV1b(BottleneckV1b, [3, 4, 6, 3], deep_stem=True, **kwargs)\n    if pretrained:\n        from .model_store import get_resnet_file\n        model.load_state_dict(torch.load(get_resnet_file(\'resnet50\', root=model_root)), strict=False)\n    return model\n\n\ndef resnet101_v1s(pretrained=False, root=\'~/.torch/models\', **kwargs):\n    model = ResNetV1b(BottleneckV1b, [3, 4, 23, 3], deep_stem=True, **kwargs)\n    if pretrained:\n        from .model_store import get_resnet_file\n        model.load_state_dict(torch.load(get_resnet_file(\'resnet101\', root=root)), strict=False)\n    return model\n\n\ndef resnet152_v1s(pretrained=False, root=\'~/.torch/models\', **kwargs):\n    model = ResNetV1b(BottleneckV1b, [3, 8, 36, 3], deep_stem=True, **kwargs)\n    if pretrained:\n        from .model_store import get_resnet_file\n        model.load_state_dict(torch.load(get_resnet_file(\'resnet152\', root=root)), strict=False)\n    return model\n\n\nif __name__ == \'__main__\':\n    import torch\n\n    img = torch.randn(4, 3, 224, 224)\n    model = resnet50_v1b(True)\n    output = model(img)\n'"
pywick/models/segmentation/da_basenets/segbase.py,1,"b'""""""Base Model for Semantic Segmentation""""""\nimport torch.nn as nn\nfrom pywick.models.segmentation.da_basenets.resnetv1b import resnet50_v1s, resnet101_v1s, resnet152_v1s\n\n__all__ = [\'SegBaseModel\']\n\n\nclass SegBaseModel(nn.Module):\n    r""""""Base Model for Semantic Segmentation\n\n    Parameters\n    ----------\n    backbone : string\n        Pre-trained dilated backbone network type (default:\'resnet50\'; \'resnet50\',\n        \'resnet101\' or \'resnet152\').\n    """"""\n\n    def __init__(self, num_classes, pretrained=True, aux=False, backbone=\'resnet101\', **kwargs):\n        super(SegBaseModel, self).__init__()\n        self.aux = aux\n        self.nclass = num_classes\n        if backbone == \'resnet50\':\n            self.pretrained = resnet50_v1s(pretrained=pretrained, **kwargs)\n        elif backbone == \'resnet101\':\n            self.pretrained = resnet101_v1s(pretrained=pretrained, **kwargs)\n        elif backbone == \'resnet152\':\n            self.pretrained = resnet152_v1s(pretrained=pretrained, **kwargs)\n        else:\n            raise RuntimeError(\'unknown backbone: {}\'.format(backbone))\n\n    def base_forward(self, x):\n        """"""forwarding pre-trained network""""""\n        x = self.pretrained.conv1(x)\n        x = self.pretrained.bn1(x)\n        x = self.pretrained.relu(x)\n        x = self.pretrained.maxpool(x)\n        c1 = self.pretrained.layer1(x)\n        c2 = self.pretrained.layer2(c1)\n        c3 = self.pretrained.layer3(c2)\n        c4 = self.pretrained.layer4(c3)\n        return c1, c2, c3, c4\n\n    def evaluate(self, x):\n        """"""evaluating network with inputs and targets""""""\n        return self.forward(x)[0]\n\n    def demo(self, x):\n        pred = self.forward(x)\n        if self.aux:\n            pred = pred[0]\n        return pred\n'"
pywick/models/segmentation/da_basenets/vgg.py,11,"b'import torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\n__all__ = [\n    \'VGG\', \'vgg11\', \'vgg11_bn\', \'vgg13\', \'vgg13_bn\', \'vgg16\', \'vgg16_bn\',\n    \'vgg19_bn\', \'vgg19\',\n]\n\nmodel_urls = {\n    \'vgg11\': \'https://download.pytorch.org/models/vgg11-bbd30ac9.pth\',\n    \'vgg13\': \'https://download.pytorch.org/models/vgg13-c768596a.pth\',\n    \'vgg16\': \'https://download.pytorch.org/models/vgg16-397923af.pth\',\n    \'vgg19\': \'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\',\n    \'vgg11_bn\': \'https://download.pytorch.org/models/vgg11_bn-6002323d.pth\',\n    \'vgg13_bn\': \'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth\',\n    \'vgg16_bn\': \'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\',\n    \'vgg19_bn\': \'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth\',\n}\n\n\nclass VGG(nn.Module):\n    def __init__(self, features, num_classes=1000, init_weights=True):\n        super(VGG, self).__init__()\n        self.features = features\n        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, num_classes)\n        )\n        if init_weights:\n            self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)\n\n\ndef make_layers(cfg, batch_norm=False):\n    layers = []\n    in_channels = 3\n    for v in cfg:\n        if v == \'M\':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += (conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True))\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = v\n    return nn.Sequential(*layers)\n\n\ncfg = {\n    \'A\': [64, \'M\', 128, \'M\', 256, 256, \'M\', 512, 512, \'M\', 512, 512, \'M\'],\n    \'B\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, \'M\', 512, 512, \'M\', 512, 512, \'M\'],\n    \'D\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, \'M\', 512, 512, 512, \'M\', 512, 512, 512, \'M\'],\n    \'E\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, 256, \'M\', 512, 512, 512, 512, \'M\', 512, 512, 512, 512, \'M\'],\n}\n\n\ndef vgg11(pretrained=False, **kwargs):\n    """"""VGG 11-layer model (configuration ""A"")\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    if pretrained:\n        kwargs[\'init_weights\'] = False\n    model = VGG(make_layers(cfg[\'A\']), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg11\']))\n    return model\n\n\ndef vgg11_bn(pretrained=False, **kwargs):\n    """"""VGG 11-layer model (configuration ""A"") with batch normalization\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    if pretrained:\n        kwargs[\'init_weights\'] = False\n    model = VGG(make_layers(cfg[\'A\'], batch_norm=True), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg11_bn\']))\n    return model\n\n\ndef vgg13(pretrained=False, **kwargs):\n    """"""VGG 13-layer model (configuration ""B"")\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    if pretrained:\n        kwargs[\'init_weights\'] = False\n    model = VGG(make_layers(cfg[\'B\']), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg13\']))\n    return model\n\n\ndef vgg13_bn(pretrained=False, **kwargs):\n    """"""VGG 13-layer model (configuration ""B"") with batch normalization\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    if pretrained:\n        kwargs[\'init_weights\'] = False\n    model = VGG(make_layers(cfg[\'B\'], batch_norm=True), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg13_bn\']))\n    return model\n\n\ndef vgg16(pretrained=False, **kwargs):\n    """"""VGG 16-layer model (configuration ""D"")\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    if pretrained:\n        kwargs[\'init_weights\'] = False\n    model = VGG(make_layers(cfg[\'D\']), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg16\']))\n    return model\n\n\ndef vgg16_bn(pretrained=False, **kwargs):\n    """"""VGG 16-layer model (configuration ""D"") with batch normalization\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    if pretrained:\n        kwargs[\'init_weights\'] = False\n    model = VGG(make_layers(cfg[\'D\'], batch_norm=True), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg16_bn\']))\n    return model\n\n\ndef vgg19(pretrained=False, **kwargs):\n    """"""VGG 19-layer model (configuration ""E"")\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    if pretrained:\n        kwargs[\'init_weights\'] = False\n    model = VGG(make_layers(cfg[\'E\']), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg19\']))\n    return model\n\n\ndef vgg19_bn(pretrained=False, **kwargs):\n    """"""VGG 19-layer model (configuration \'E\') with batch normalization\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    if pretrained:\n        kwargs[\'init_weights\'] = False\n    model = VGG(make_layers(cfg[\'E\'], batch_norm=True), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg19_bn\']))\n    return model\n\n\nif __name__ == \'__main__\':\n    img = torch.randn((4, 3, 480, 480))\n    model = vgg16(pretrained=False)\n    out = model(img)\n'"
pywick/models/segmentation/gcnnets/__init__.py,0,b'from .gcn import *\nfrom .gcn_nasnet import *\nfrom .gcn_densenet import *\nfrom .gcn_psp import *\nfrom .gcn_resnext import *'
pywick/models/segmentation/gcnnets/gcn.py,1,"b""# Source: https://github.com/flixpar/VisDa/tree/master/models\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torchvision import models\nimport os\nfrom math import floor\n\n__all__ = ['GCN']\n\nclass _GlobalConvModule(nn.Module):\n    def __init__(self, in_dim, out_dim, kernel_size):\n        super(_GlobalConvModule, self).__init__()\n\n        pad0 = floor((kernel_size[0] - 1) / 2)\n        pad1 = floor((kernel_size[1] - 1) / 2)\n\n        self.conv_l1 = nn.Conv2d(in_dim, out_dim, kernel_size=(kernel_size[0], 1), padding=(pad0, 0))\n        self.conv_l2 = nn.Conv2d(out_dim, out_dim, kernel_size=(1, kernel_size[1]), padding=(0, pad1))\n        self.conv_r1 = nn.Conv2d(in_dim, out_dim, kernel_size=(1, kernel_size[1]), padding=(0, pad1))\n        self.conv_r2 = nn.Conv2d(out_dim, out_dim, kernel_size=(kernel_size[0], 1), padding=(pad0, 0))\n\n    def forward(self, x):\n        x_l = self.conv_l1(x)\n        x_l = self.conv_l2(x_l)\n        x_r = self.conv_r1(x)\n        x_r = self.conv_r2(x_r)\n        x = x_l + x_r\n        return x\n\n\nclass _BoundaryRefineModule(nn.Module):\n    def __init__(self, dim):\n        super(_BoundaryRefineModule, self).__init__()\n        self.relu = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        residual = self.conv1(x)\n        residual = self.relu(residual)\n        residual = self.conv2(residual)\n        out = x + residual\n        return out\n\n\nclass GCN(nn.Module):\n    def __init__(self, num_classes, pretrained=True, k=7):\n        super(GCN, self).__init__()\n\n        self.K = k\n\n        resnet = models.resnet152(pretrained=pretrained)\n\n        self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu)\n        self.layer1 = nn.Sequential(resnet.maxpool, resnet.layer1)\n        self.layer2 = resnet.layer2\n        self.layer3 = resnet.layer3\n        self.layer4 = resnet.layer4\n\n        self.gcm1 = _GlobalConvModule(2048, num_classes, (self.K, self.K))\n        self.gcm2 = _GlobalConvModule(1024, num_classes, (self.K, self.K))\n        self.gcm3 = _GlobalConvModule(512, num_classes, (self.K, self.K))\n        self.gcm4 = _GlobalConvModule(256, num_classes, (self.K, self.K))\n\n        self.brm1 = _BoundaryRefineModule(num_classes)\n        self.brm2 = _BoundaryRefineModule(num_classes)\n        self.brm3 = _BoundaryRefineModule(num_classes)\n        self.brm4 = _BoundaryRefineModule(num_classes)\n        self.brm5 = _BoundaryRefineModule(num_classes)\n        self.brm6 = _BoundaryRefineModule(num_classes)\n        self.brm7 = _BoundaryRefineModule(num_classes)\n        self.brm8 = _BoundaryRefineModule(num_classes)\n        self.brm9 = _BoundaryRefineModule(num_classes)\n\n        initialize_weights(self.gcm1, self.gcm2, self.gcm3, self.gcm4, self.brm1, self.brm2, self.brm3,\n                           self.brm4, self.brm5, self.brm6, self.brm7, self.brm8, self.brm9)\n\n    def forward(self, x):\n        size = x.size()[2:]\n        fm0 = self.layer0(x)\n        fm1 = self.layer1(fm0)\n        fm2 = self.layer2(fm1)\n        fm3 = self.layer3(fm2)\n        fm4 = self.layer4(fm3)\n\n        gcfm1 = self.brm1(self.gcm1(fm4))\n        gcfm2 = self.brm2(self.gcm2(fm3))\n        gcfm3 = self.brm3(self.gcm3(fm2))\n        gcfm4 = self.brm4(self.gcm4(fm1))\n\n        fs1 = self.brm5(F.interpolate(gcfm1, size=fm3.size()[2:], mode='bilinear') + gcfm2)\n        fs2 = self.brm6(F.interpolate(fs1, size=fm2.size()[2:], mode='bilinear') + gcfm3)\n        fs3 = self.brm7(F.interpolate(fs2, size=fm1.size()[2:], mode='bilinear') + gcfm4)\n        fs4 = self.brm8(F.interpolate(fs3, size=fm0.size()[2:], mode='bilinear'))\n        out = self.brm9(F.interpolate(fs4, size=size, mode='bilinear'))\n\n        return out\n\n\ndef initialize_weights(*models):\n    for model in models:\n        for module in model.modules():\n            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n                nn.init.kaiming_normal_(module.weight)\n                if module.bias is not None:\n                    module.bias.data.zero_()\n            elif isinstance(module, nn.BatchNorm2d):\n                module.weight.data.fill_(1)\n                module.bias.data.zero_()\n"""
pywick/models/segmentation/gcnnets/gcn_densenet.py,1,"b'# Source: https://github.com/flixpar/VisDa/tree/master/models\n\n""""""\nImplementation of `Large Kernel Matters <https://arxiv.org/pdf/1703.02719>`_ with Densenet backend\n""""""\n\nfrom math import floor\n\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torchvision import models\n\n__all__ = [\'GCN_Densenet\']\n\nclass _GlobalConvModule(nn.Module):\n    def __init__(self, in_dim, out_dim, kernel_size):\n        super(_GlobalConvModule, self).__init__()\n\n        pad0 = floor((kernel_size[0] - 1) / 2)\n        pad1 = floor((kernel_size[1] - 1) / 2)\n\n        self.conv_l1 = nn.Conv2d(in_dim, out_dim, kernel_size=(kernel_size[0], 1), padding=(pad0, 0))\n        self.conv_l2 = nn.Conv2d(out_dim, out_dim, kernel_size=(1, kernel_size[1]), padding=(0, pad1))\n        self.conv_r1 = nn.Conv2d(in_dim, out_dim, kernel_size=(1, kernel_size[1]), padding=(0, pad1))\n        self.conv_r2 = nn.Conv2d(out_dim, out_dim, kernel_size=(kernel_size[0], 1), padding=(pad0, 0))\n\n    def forward(self, x):\n        x_l = self.conv_l1(x)\n        x_l = self.conv_l2(x_l)\n        x_r = self.conv_r1(x)\n        x_r = self.conv_r2(x_r)\n        x = x_l + x_r\n        return x\n\n\nclass _BoundaryRefineModule(nn.Module):\n    def __init__(self, dim):\n        super(_BoundaryRefineModule, self).__init__()\n        self.relu = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        residual = self.conv1(x)\n        residual = self.relu(residual)\n        residual = self.conv2(residual)\n        out = x + residual\n        return out\n\n\nclass GCN_Densenet(nn.Module):\n    def __init__(self, num_classes, pretrained=True, k=7, **kwargs):\n        super(GCN_Densenet, self).__init__()\n\n        self.K = k\n\n        densenet = models.densenet161(pretrained=pretrained)\n\n        self.layer0 = nn.Sequential(\n            densenet.features.conv0,\n            densenet.features.norm0,\n            densenet.features.relu0,\n        )\n\n        self.layer1 = nn.Sequential(\n            densenet.features.pool0,\n            densenet.features.denseblock1,\n        )\n\n        self.layer2 = nn.Sequential(\n            densenet.features.transition1,\n            densenet.features.denseblock2,\n        )\n\n        self.layer3 = nn.Sequential(\n            densenet.features.transition2,\n            densenet.features.denseblock3,\n        )\n\n        self.layer4 = nn.Sequential(\n            densenet.features.transition3,\n            densenet.features.denseblock4,\n        )\n\n        self.gcm1 = _GlobalConvModule(2208, num_classes, (self.K, self.K))\n        self.gcm2 = _GlobalConvModule(2112, num_classes, (self.K, self.K))\n        self.gcm3 = _GlobalConvModule(768, num_classes, (self.K, self.K))\n        self.gcm4 = _GlobalConvModule(384, num_classes, (self.K, self.K))\n\n        self.brm1 = _BoundaryRefineModule(num_classes)\n        self.brm2 = _BoundaryRefineModule(num_classes)\n        self.brm3 = _BoundaryRefineModule(num_classes)\n        self.brm4 = _BoundaryRefineModule(num_classes)\n        self.brm5 = _BoundaryRefineModule(num_classes)\n        self.brm6 = _BoundaryRefineModule(num_classes)\n        self.brm7 = _BoundaryRefineModule(num_classes)\n        self.brm8 = _BoundaryRefineModule(num_classes)\n        self.brm9 = _BoundaryRefineModule(num_classes)\n\n        initialize_weights(self.gcm1, self.gcm2, self.gcm3, self.gcm4, self.brm1, self.brm2, self.brm3,\n                           self.brm4, self.brm5, self.brm6, self.brm7, self.brm8, self.brm9)\n\n    def forward(self, x):\n        size = x.size()[2:]\n        fm0 = self.layer0(x)\n        fm1 = self.layer1(fm0)\n        fm2 = self.layer2(fm1)\n        fm3 = self.layer3(fm2)\n        fm4 = self.layer4(fm3)\n\n        gcfm1 = self.brm1(self.gcm1(fm4))\n        gcfm2 = self.brm2(self.gcm2(fm3))\n        gcfm3 = self.brm3(self.gcm3(fm2))\n        gcfm4 = self.brm4(self.gcm4(fm1))\n\n        fs1 = self.brm5(F.upsample(gcfm1, size=fm3.size()[2:], mode=\'bilinear\') + gcfm2)\n        fs2 = self.brm6(F.upsample(fs1, size=fm2.size()[2:], mode=\'bilinear\') + gcfm3)\n        fs3 = self.brm7(F.upsample(fs2, size=fm1.size()[2:], mode=\'bilinear\') + gcfm4)\n        fs4 = self.brm8(F.upsample(fs3, size=fm0.size()[2:], mode=\'bilinear\'))\n        out = self.brm9(F.upsample(fs4, size=size, mode=\'bilinear\'))\n\n        return out\n\n\ndef initialize_weights(*models):\n    for model in models:\n        for module in model.modules():\n            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n                nn.init.kaiming_normal_(module.weight)\n                if module.bias is not None:\n                    module.bias.data.zero_()\n            elif isinstance(module, nn.BatchNorm2d):\n                module.weight.data.fill_(1)\n                module.bias.data.zero_()\n'"
pywick/models/segmentation/gcnnets/gcn_nasnet.py,12,"b'# Source: https://github.com/flixpar/VisDa/tree/master/models\n\n""""""\nImplementation of `Large Kernel Matters <https://arxiv.org/pdf/1703.02719>`_ with NASNet backend\n""""""\n\nfrom math import floor\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\n\n__all__ = [\'GCN_NASNet\']\n\n################### GCN ######################\n\nclass _GlobalConvModule(nn.Module):\n    def __init__(self, in_dim, out_dim, kernel_size):\n        super(_GlobalConvModule, self).__init__()\n\n        pad0 = floor((kernel_size[0] - 1) / 2)\n        pad1 = floor((kernel_size[1] - 1) / 2)\n\n        self.conv_l1 = nn.Conv2d(in_dim, out_dim, kernel_size=(kernel_size[0], 1), padding=(pad0, 0))\n        self.conv_l2 = nn.Conv2d(out_dim, out_dim, kernel_size=(1, kernel_size[1]), padding=(0, pad1))\n        self.conv_r1 = nn.Conv2d(in_dim, out_dim, kernel_size=(1, kernel_size[1]), padding=(0, pad1))\n        self.conv_r2 = nn.Conv2d(out_dim, out_dim, kernel_size=(kernel_size[0], 1), padding=(pad0, 0))\n\n    def forward(self, x):\n        x_l = self.conv_l1(x)\n        x_l = self.conv_l2(x_l)\n        x_r = self.conv_r1(x)\n        x_r = self.conv_r2(x_r)\n        x = x_l + x_r\n        return x\n\n\nclass _BoundaryRefineModule(nn.Module):\n    def __init__(self, dim):\n        super(_BoundaryRefineModule, self).__init__()\n        self.relu = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        residual = self.conv1(x)\n        residual = self.relu(residual)\n        residual = self.conv2(residual)\n        out = x + residual\n        return out\n\n\nclass _LearnedBilinearDeconvModule(nn.Module):\n    def __init__(self, channels):\n        super(_LearnedBilinearDeconvModule, self).__init__()\n        self.deconv = nn.ConvTranspose2d(channels, channels, kernel_size=4, stride=2, padding=1)\n        self.deconv.weight.data = self.make_bilinear_weights(4, channels)\n        self.deconv.bias.data.zero_()\n\n    def forward(self, x):\n        out = self.deconv(x)\n        return out\n\n    def make_bilinear_weights(self, size, num_channels):\n        factor = (size + 1) // 2\n        if size % 2 == 1:\n            center = factor - 1\n        else:\n            center = factor - 0.5\n        og = np.ogrid[:size, :size]\n        filt = (1 - abs(og[0] - center) / factor) * (1 - abs(og[1] - center) / factor)\n        filt = torch.from_numpy(filt)\n        w = torch.zeros(num_channels, num_channels, size, size)\n        for i in range(num_channels):\n            w[i, i] = filt\n        return w\n\n\nclass GCN_NASNet(nn.Module):\n    def __init__(self, num_classes, pretrained=True, k=7):\n        super(GCN_NASNet, self).__init__()\n\n        self.K = k\n\n        model = NASNetALarge(num_classes=1001)\n        if pretrained:\n            model.load_state_dict(torch.utils.model_zoo.load_url(\'http://data.lip6.fr/cadene/pretrainedmodels/nasnetalarge-a1897284.pth\'))\n        self.nasnet = model\n\n        self.gcm1 = _GlobalConvModule(4032, num_classes, (self.K, self.K))\n        self.gcm2 = _GlobalConvModule(2016, num_classes, (self.K, self.K))\n        self.gcm3 = _GlobalConvModule(1008, num_classes, (self.K, self.K))\n        self.gcm4 = _GlobalConvModule(num_classes, num_classes, (self.K, self.K))\n\n        self.brm1 = _BoundaryRefineModule(num_classes)\n        self.brm2 = _BoundaryRefineModule(num_classes)\n        self.brm3 = _BoundaryRefineModule(num_classes)\n        self.brm4 = _BoundaryRefineModule(num_classes)\n        self.brm5 = _BoundaryRefineModule(num_classes)\n        self.brm6 = _BoundaryRefineModule(num_classes)\n        self.brm7 = _BoundaryRefineModule(num_classes)\n        self.brm8 = _BoundaryRefineModule(num_classes)\n\n        self.deconv = _LearnedBilinearDeconvModule(num_classes)\n\n        initialize_weights(self.gcm1, self.gcm2, self.gcm3, self.gcm4, self.brm1, self.brm2, self.brm3, self.brm4, self.brm5, self.brm6, self.brm7, self.brm8)\n\n    def forward(self, x):\n        x_conv0 = self.nasnet.conv0(x)\n        x_stem_0 = self.nasnet.cell_stem_0(x_conv0)\n        x_stem_1 = self.nasnet.cell_stem_1(x_conv0, x_stem_0)\n\n        x_cell_0 = self.nasnet.cell_0(x_stem_1, x_stem_0)\n        x_cell_1 = self.nasnet.cell_1(x_cell_0, x_stem_1)\n        x_cell_2 = self.nasnet.cell_2(x_cell_1, x_cell_0)\n        x_cell_3 = self.nasnet.cell_3(x_cell_2, x_cell_1)\n        x_cell_4 = self.nasnet.cell_4(x_cell_3, x_cell_2)\n        x_cell_5 = self.nasnet.cell_5(x_cell_4, x_cell_3)\n\n        x_reduction_cell_0 = self.nasnet.reduction_cell_0(x_cell_5, x_cell_4)\n\n        x_cell_6 = self.nasnet.cell_6(x_reduction_cell_0, x_cell_4)\n        x_cell_7 = self.nasnet.cell_7(x_cell_6, x_reduction_cell_0)\n        x_cell_8 = self.nasnet.cell_8(x_cell_7, x_cell_6)\n        x_cell_9 = self.nasnet.cell_9(x_cell_8, x_cell_7)\n        x_cell_10 = self.nasnet.cell_10(x_cell_9, x_cell_8)\n        x_cell_11 = self.nasnet.cell_11(x_cell_10, x_cell_9)\n\n        x_reduction_cell_1 = self.nasnet.reduction_cell_1(x_cell_11, x_cell_10)\n\n        x_cell_12 = self.nasnet.cell_12(x_reduction_cell_1, x_cell_10)\n        x_cell_13 = self.nasnet.cell_13(x_cell_12, x_reduction_cell_1)\n        x_cell_14 = self.nasnet.cell_14(x_cell_13, x_cell_12)\n        x_cell_15 = self.nasnet.cell_15(x_cell_14, x_cell_13)\n        x_cell_16 = self.nasnet.cell_16(x_cell_15, x_cell_14)\n        x_cell_17 = self.nasnet.cell_17(x_cell_16, x_cell_15)\n\n        gcfm1 = self.brm1(self.gcm1(x_cell_17))\n        gcfm2 = self.brm2(self.gcm2(x_cell_11))\n        gcfm3 = self.brm3(self.gcm3(x_cell_5))\n\n        fs1 = self.brm4(self.deconv(gcfm1) + gcfm2)\n        fs2 = self.brm5(self.deconv(fs1) + gcfm3)\n        fs3 = self.brm6(self.deconv(fs2))\n        fs4 = self.brm7(self.deconv(fs3))\n        out = self.brm8(self.deconv(self.gcm4(fs4)))\n\n        return out\n\n\ndef initialize_weights(*models):\n    for model in models:\n        for module in model.modules():\n            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n                nn.init.kaiming_normal_(module.weight)\n                if module.bias is not None:\n                    module.bias.data.zero_()\n            elif isinstance(module, nn.BatchNorm2d):\n                module.weight.data.fill_(1)\n                module.bias.data.zero_()\n\n\n################## NASNet ############################\n\nclass MaxPoolPad(nn.Module):\n\n    def __init__(self):\n        super(MaxPoolPad, self).__init__()\n        self.pad = nn.ZeroPad2d((1, 0, 1, 0))\n        self.pool = nn.MaxPool2d(3, stride=2, padding=1)\n\n    def forward(self, x):\n        x = self.pad(x)\n        x = self.pool(x)\n        x = x[:, :, 1:, 1:]\n        return x\n\n\nclass AvgPoolPad(nn.Module):\n\n    def __init__(self, stride=2, padding=1):\n        super(AvgPoolPad, self).__init__()\n        self.pad = nn.ZeroPad2d((1, 0, 1, 0))\n        self.pool = nn.AvgPool2d(3, stride=stride, padding=padding, count_include_pad=False)\n\n    def forward(self, x):\n        x = self.pad(x)\n        x = self.pool(x)\n        x = x[:, :, 1:, 1:]\n        return x\n\n\nclass SeparableConv2d(nn.Module):\n\n    def __init__(self, in_channels, out_channels, dw_kernel, dw_stride, dw_padding, bias=False):\n        super(SeparableConv2d, self).__init__()\n        self.depthwise_conv2d = nn.Conv2d(in_channels, in_channels, dw_kernel,\n                                          stride=dw_stride,\n                                          padding=dw_padding,\n                                          bias=bias,\n                                          groups=in_channels)\n        self.pointwise_conv2d = nn.Conv2d(in_channels, out_channels, 1, stride=1, bias=bias)\n\n    def forward(self, x):\n        x = self.depthwise_conv2d(x)\n        x = self.pointwise_conv2d(x)\n        return x\n\n\nclass BranchSeparables(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=False):\n        super(BranchSeparables, self).__init__()\n        self.relu = nn.ReLU()\n        self.separable_1 = SeparableConv2d(in_channels, in_channels, kernel_size, stride, padding, bias=bias)\n        self.bn_sep_1 = nn.BatchNorm2d(in_channels, eps=0.001, momentum=0.1, affine=True)\n        self.relu1 = nn.ReLU()\n        self.separable_2 = SeparableConv2d(in_channels, out_channels, kernel_size, 1, padding, bias=bias)\n        self.bn_sep_2 = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.1, affine=True)\n\n    def forward(self, x):\n        x = self.relu(x)\n        x = self.separable_1(x)\n        x = self.bn_sep_1(x)\n        x = self.relu1(x)\n        x = self.separable_2(x)\n        x = self.bn_sep_2(x)\n        return x\n\n\nclass BranchSeparablesStem(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=False):\n        super(BranchSeparablesStem, self).__init__()\n        self.relu = nn.ReLU()\n        self.separable_1 = SeparableConv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias)\n        self.bn_sep_1 = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.1, affine=True)\n        self.relu1 = nn.ReLU()\n        self.separable_2 = SeparableConv2d(out_channels, out_channels, kernel_size, 1, padding, bias=bias)\n        self.bn_sep_2 = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.1, affine=True)\n\n    def forward(self, x):\n        x = self.relu(x)\n        x = self.separable_1(x)\n        x = self.bn_sep_1(x)\n        x = self.relu1(x)\n        x = self.separable_2(x)\n        x = self.bn_sep_2(x)\n        return x\n\n\nclass BranchSeparablesReduction(BranchSeparables):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, z_padding=1, bias=False):\n        BranchSeparables.__init__(self, in_channels, out_channels, kernel_size, stride, padding, bias)\n        self.padding = nn.ZeroPad2d((z_padding, 0, z_padding, 0))\n\n    def forward(self, x):\n        x = self.relu(x)\n        x = self.padding(x)\n        x = self.separable_1(x)\n        x = x[:, :, 1:, 1:].contiguous()\n        x = self.bn_sep_1(x)\n        x = self.relu1(x)\n        x = self.separable_2(x)\n        x = self.bn_sep_2(x)\n        return x\n\n\nclass CellStem0(nn.Module):\n    def __init__(self, stem_filters, num_filters=42):\n        super(CellStem0, self).__init__()\n        self.num_filters = num_filters\n        self.stem_filters = stem_filters\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(self.stem_filters, self.num_filters, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(self.num_filters, eps=0.001, momentum=0.1, affine=True))\n\n        self.comb_iter_0_left = BranchSeparables(self.num_filters, self.num_filters, 5, 2, 2)\n        self.comb_iter_0_right = BranchSeparablesStem(self.stem_filters, self.num_filters, 7, 2, 3, bias=False)\n\n        self.comb_iter_1_left = nn.MaxPool2d(3, stride=2, padding=1)\n        self.comb_iter_1_right = BranchSeparablesStem(self.stem_filters, self.num_filters, 7, 2, 3, bias=False)\n\n        self.comb_iter_2_left = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)\n        self.comb_iter_2_right = BranchSeparablesStem(self.stem_filters, self.num_filters, 5, 2, 2, bias=False)\n\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparables(self.num_filters, self.num_filters, 3, 1, 1, bias=False)\n        self.comb_iter_4_right = nn.MaxPool2d(3, stride=2, padding=1)\n\n    def forward(self, x):\n        x1 = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x1)\n        x_comb_iter_0_right = self.comb_iter_0_right(x)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x1)\n        x_comb_iter_1_right = self.comb_iter_1_right(x)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x1)\n        x_comb_iter_2_right = self.comb_iter_2_right(x)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x1)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass CellStem1(nn.Module):\n\n    def __init__(self, stem_filters, num_filters):\n        super(CellStem1, self).__init__()\n        self.num_filters = num_filters\n        self.stem_filters = stem_filters\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(2*self.num_filters, self.num_filters, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(self.num_filters, eps=0.001, momentum=0.1, affine=True))\n\n        self.relu = nn.ReLU()\n        self.path_1 = nn.Sequential()\n        self.path_1.add_module(\'avgpool\', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n        self.path_1.add_module(\'conv\', nn.Conv2d(self.stem_filters, self.num_filters//2, 1, stride=1, bias=False))\n        self.path_2 = nn.ModuleList()\n        self.path_2.add_module(\'pad\', nn.ZeroPad2d((0, 1, 0, 1)))\n        self.path_2.add_module(\'avgpool\', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n        self.path_2.add_module(\'conv\', nn.Conv2d(self.stem_filters, self.num_filters//2, 1, stride=1, bias=False))\n\n        self.final_path_bn = nn.BatchNorm2d(self.num_filters, eps=0.001, momentum=0.1, affine=True)\n\n        self.comb_iter_0_left = BranchSeparables(self.num_filters, self.num_filters, 5, 2, 2, bias=False)\n        self.comb_iter_0_right = BranchSeparables(self.num_filters, self.num_filters, 7, 2, 3, bias=False)\n\n        self.comb_iter_1_left = nn.MaxPool2d(3, stride=2, padding=1)\n        self.comb_iter_1_right = BranchSeparables(self.num_filters, self.num_filters, 7, 2, 3, bias=False)\n\n        self.comb_iter_2_left = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)\n        self.comb_iter_2_right = BranchSeparables(self.num_filters, self.num_filters, 5, 2, 2, bias=False)\n\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparables(self.num_filters, self.num_filters, 3, 1, 1, bias=False)\n        self.comb_iter_4_right = nn.MaxPool2d(3, stride=2, padding=1)\n\n    def forward(self, x_conv0, x_stem_0):\n        x_left = self.conv_1x1(x_stem_0)\n\n        x_relu = self.relu(x_conv0)\n        # path 1\n        x_path1 = self.path_1(x_relu)\n        # path 2\n        x_path2 = self.path_2.pad(x_relu)\n        x_path2 = x_path2[:, :, 1:, 1:]\n        x_path2 = self.path_2.avgpool(x_path2)\n        x_path2 = self.path_2.conv(x_path2)\n        # final path\n        x_right = self.final_path_bn(torch.cat([x_path1, x_path2], 1))\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_left)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_right)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_left)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_right)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_left)\n        x_comb_iter_2_right = self.comb_iter_2_right(x_right)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x_left)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass FirstCell(nn.Module):\n\n    def __init__(self, in_channels_left, out_channels_left, in_channels_right, out_channels_right):\n        super(FirstCell, self).__init__()\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(in_channels_right, out_channels_right, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_right, eps=0.001, momentum=0.1, affine=True))\n\n        self.relu = nn.ReLU()\n        self.path_1 = nn.Sequential()\n        self.path_1.add_module(\'avgpool\', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n        self.path_1.add_module(\'conv\', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n        self.path_2 = nn.ModuleList()\n        self.path_2.add_module(\'pad\', nn.ZeroPad2d((0, 1, 0, 1)))\n        self.path_2.add_module(\'avgpool\', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n        self.path_2.add_module(\'conv\', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n\n        self.final_path_bn = nn.BatchNorm2d(out_channels_left * 2, eps=0.001, momentum=0.1, affine=True)\n\n        self.comb_iter_0_left = BranchSeparables(out_channels_right, out_channels_right, 5, 1, 2, bias=False)\n        self.comb_iter_0_right = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n\n        self.comb_iter_1_left = BranchSeparables(out_channels_right, out_channels_right, 5, 1, 2, bias=False)\n        self.comb_iter_1_right = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n\n        self.comb_iter_2_left = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_3_left = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n\n    def forward(self, x, x_prev):\n        x_relu = self.relu(x_prev)\n        # path 1\n        x_path1 = self.path_1(x_relu)\n        # path 2\n        x_path2 = self.path_2.pad(x_relu)\n        x_path2 = x_path2[:, :, 1:, 1:]\n        x_path2 = self.path_2.avgpool(x_path2)\n        x_path2 = self.path_2.conv(x_path2)\n        # final path\n        x_left = self.final_path_bn(torch.cat([x_path1, x_path2], 1))\n\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_left)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2 = x_comb_iter_2_left + x_left\n\n        x_comb_iter_3_left = self.comb_iter_3_left(x_left)\n        x_comb_iter_3_right = self.comb_iter_3_right(x_left)\n        x_comb_iter_3 = x_comb_iter_3_left + x_comb_iter_3_right\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_right\n\n        x_out = torch.cat([x_left, x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass NormalCell(nn.Module):\n\n    def __init__(self, in_channels_left, out_channels_left, in_channels_right, out_channels_right):\n        super(NormalCell, self).__init__()\n        self.conv_prev_1x1 = nn.Sequential()\n        self.conv_prev_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_prev_1x1.add_module(\'conv\', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n        self.conv_prev_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_left, eps=0.001, momentum=0.1, affine=True))\n\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(in_channels_right, out_channels_right, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_right, eps=0.001, momentum=0.1, affine=True))\n\n        self.comb_iter_0_left = BranchSeparables(out_channels_right, out_channels_right, 5, 1, 2, bias=False)\n        self.comb_iter_0_right = BranchSeparables(out_channels_left, out_channels_left, 3, 1, 1, bias=False)\n\n        self.comb_iter_1_left = BranchSeparables(out_channels_left, out_channels_left, 5, 1, 2, bias=False)\n        self.comb_iter_1_right = BranchSeparables(out_channels_left, out_channels_left, 3, 1, 1, bias=False)\n\n        self.comb_iter_2_left = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_3_left = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n\n    def forward(self, x, x_prev):\n        x_left = self.conv_prev_1x1(x_prev)\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_left)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2 = x_comb_iter_2_left + x_left\n\n        x_comb_iter_3_left = self.comb_iter_3_left(x_left)\n        x_comb_iter_3_right = self.comb_iter_3_right(x_left)\n        x_comb_iter_3 = x_comb_iter_3_left + x_comb_iter_3_right\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_right\n\n        x_out = torch.cat([x_left, x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass ReductionCell0(nn.Module):\n\n    def __init__(self, in_channels_left, out_channels_left, in_channels_right, out_channels_right):\n        super(ReductionCell0, self).__init__()\n        self.conv_prev_1x1 = nn.Sequential()\n        self.conv_prev_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_prev_1x1.add_module(\'conv\', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n        self.conv_prev_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_left, eps=0.001, momentum=0.1, affine=True))\n\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(in_channels_right, out_channels_right, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_right, eps=0.001, momentum=0.1, affine=True))\n\n        self.comb_iter_0_left = BranchSeparablesReduction(out_channels_right, out_channels_right, 5, 2, 2, bias=False)\n        self.comb_iter_0_right = BranchSeparablesReduction(out_channels_right, out_channels_right, 7, 2, 3, bias=False)\n\n        self.comb_iter_1_left = MaxPoolPad()\n        self.comb_iter_1_right = BranchSeparablesReduction(out_channels_right, out_channels_right, 7, 2, 3, bias=False)\n\n        self.comb_iter_2_left = AvgPoolPad()\n        self.comb_iter_2_right = BranchSeparablesReduction(out_channels_right, out_channels_right, 5, 2, 2, bias=False)\n\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparablesReduction(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n        self.comb_iter_4_right = MaxPoolPad()\n\n    def forward(self, x, x_prev):\n        x_left = self.conv_prev_1x1(x_prev)\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_right)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2_right = self.comb_iter_2_right(x_left)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass ReductionCell1(nn.Module):\n\n    def __init__(self, in_channels_left, out_channels_left, in_channels_right, out_channels_right):\n        super(ReductionCell1, self).__init__()\n        self.conv_prev_1x1 = nn.Sequential()\n        self.conv_prev_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_prev_1x1.add_module(\'conv\', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n        self.conv_prev_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_left, eps=0.001, momentum=0.1, affine=True))\n\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(in_channels_right, out_channels_right, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_right, eps=0.001, momentum=0.1, affine=True))\n\n        self.comb_iter_0_left = BranchSeparables(out_channels_right, out_channels_right, 5, 2, 2, bias=False)\n        self.comb_iter_0_right = BranchSeparables(out_channels_right, out_channels_right, 7, 2, 3, bias=False)\n\n        self.comb_iter_1_left = nn.MaxPool2d(3, stride=2, padding=1)\n        self.comb_iter_1_right = BranchSeparables(out_channels_right, out_channels_right, 7, 2, 3, bias=False)\n\n        self.comb_iter_2_left = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)\n        self.comb_iter_2_right = BranchSeparables(out_channels_right, out_channels_right, 5, 2, 2, bias=False)\n\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n        self.comb_iter_4_right = nn.MaxPool2d(3, stride=2, padding=1)\n\n    def forward(self, x, x_prev):\n        x_left = self.conv_prev_1x1(x_prev)\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_right)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2_right = self.comb_iter_2_right(x_left)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass NASNetALarge(nn.Module):\n    """"""NASNetALarge (6 @ 4032) """"""\n\n    def __init__(self, num_classes=1001, stem_filters=96, penultimate_filters=4032, filters_multiplier=2):\n        super(NASNetALarge, self).__init__()\n        self.num_classes = num_classes\n        self.stem_filters = stem_filters\n        self.penultimate_filters = penultimate_filters\n        self.filters_multiplier = filters_multiplier\n\n        filters = self.penultimate_filters // 24\n        # 24 is default value for the architecture\n\n        self.conv0 = nn.Sequential()\n        self.conv0.add_module(\'conv\', nn.Conv2d(in_channels=3, out_channels=self.stem_filters, kernel_size=3, padding=0, stride=2,\n                                                bias=False))\n        self.conv0.add_module(\'bn\', nn.BatchNorm2d(self.stem_filters, eps=0.001, momentum=0.1, affine=True))\n\n        self.cell_stem_0 = CellStem0(self.stem_filters, num_filters=filters // (filters_multiplier ** 2))\n        self.cell_stem_1 = CellStem1(self.stem_filters, num_filters=filters // filters_multiplier)\n\n        self.cell_0 = FirstCell(in_channels_left=filters, out_channels_left=filters//2,\n                                in_channels_right=2*filters, out_channels_right=filters)\n        self.cell_1 = NormalCell(in_channels_left=2*filters, out_channels_left=filters,\n                                 in_channels_right=6*filters, out_channels_right=filters)\n        self.cell_2 = NormalCell(in_channels_left=6*filters, out_channels_left=filters,\n                                 in_channels_right=6*filters, out_channels_right=filters)\n        self.cell_3 = NormalCell(in_channels_left=6*filters, out_channels_left=filters,\n                                 in_channels_right=6*filters, out_channels_right=filters)\n        self.cell_4 = NormalCell(in_channels_left=6*filters, out_channels_left=filters,\n                                 in_channels_right=6*filters, out_channels_right=filters)\n        self.cell_5 = NormalCell(in_channels_left=6*filters, out_channels_left=filters,\n                                 in_channels_right=6*filters, out_channels_right=filters)\n\n        self.reduction_cell_0 = ReductionCell0(in_channels_left=6*filters, out_channels_left=2*filters,\n                                               in_channels_right=6*filters, out_channels_right=2*filters)\n\n        self.cell_6 = FirstCell(in_channels_left=6*filters, out_channels_left=filters,\n                                in_channels_right=8*filters, out_channels_right=2*filters)\n        self.cell_7 = NormalCell(in_channels_left=8*filters, out_channels_left=2*filters,\n                                 in_channels_right=12*filters, out_channels_right=2*filters)\n        self.cell_8 = NormalCell(in_channels_left=12*filters, out_channels_left=2*filters,\n                                 in_channels_right=12*filters, out_channels_right=2*filters)\n        self.cell_9 = NormalCell(in_channels_left=12*filters, out_channels_left=2*filters,\n                                 in_channels_right=12*filters, out_channels_right=2*filters)\n        self.cell_10 = NormalCell(in_channels_left=12*filters, out_channels_left=2*filters,\n                                  in_channels_right=12*filters, out_channels_right=2*filters)\n        self.cell_11 = NormalCell(in_channels_left=12*filters, out_channels_left=2*filters,\n                                  in_channels_right=12*filters, out_channels_right=2*filters)\n\n        self.reduction_cell_1 = ReductionCell1(in_channels_left=12*filters, out_channels_left=4*filters,\n                                               in_channels_right=12*filters, out_channels_right=4*filters)\n\n        self.cell_12 = FirstCell(in_channels_left=12*filters, out_channels_left=2*filters,\n                                 in_channels_right=16*filters, out_channels_right=4*filters)\n        self.cell_13 = NormalCell(in_channels_left=16*filters, out_channels_left=4*filters,\n                                  in_channels_right=24*filters, out_channels_right=4*filters)\n        self.cell_14 = NormalCell(in_channels_left=24*filters, out_channels_left=4*filters,\n                                  in_channels_right=24*filters, out_channels_right=4*filters)\n        self.cell_15 = NormalCell(in_channels_left=24*filters, out_channels_left=4*filters,\n                                  in_channels_right=24*filters, out_channels_right=4*filters)\n        self.cell_16 = NormalCell(in_channels_left=24*filters, out_channels_left=4*filters,\n                                  in_channels_right=24*filters, out_channels_right=4*filters)\n        self.cell_17 = NormalCell(in_channels_left=24*filters, out_channels_left=4*filters,\n                                  in_channels_right=24*filters, out_channels_right=4*filters)\n\n        self.relu = nn.ReLU()\n        self.avg_pool = nn.AvgPool2d(11, stride=1, padding=0)\n        self.dropout = nn.Dropout()\n        self.last_linear = nn.Linear(24*filters, self.num_classes)\n\n    def features(self, input):\n        x_conv0 = self.conv0(input)\n        x_stem_0 = self.cell_stem_0(x_conv0)\n        x_stem_1 = self.cell_stem_1(x_conv0, x_stem_0)\n\n        x_cell_0 = self.cell_0(x_stem_1, x_stem_0)\n        x_cell_1 = self.cell_1(x_cell_0, x_stem_1)\n        x_cell_2 = self.cell_2(x_cell_1, x_cell_0)\n        x_cell_3 = self.cell_3(x_cell_2, x_cell_1)\n        x_cell_4 = self.cell_4(x_cell_3, x_cell_2)\n        x_cell_5 = self.cell_5(x_cell_4, x_cell_3)\n\n        x_reduction_cell_0 = self.reduction_cell_0(x_cell_5, x_cell_4)\n\n        x_cell_6 = self.cell_6(x_reduction_cell_0, x_cell_4)\n        x_cell_7 = self.cell_7(x_cell_6, x_reduction_cell_0)\n        x_cell_8 = self.cell_8(x_cell_7, x_cell_6)\n        x_cell_9 = self.cell_9(x_cell_8, x_cell_7)\n        x_cell_10 = self.cell_10(x_cell_9, x_cell_8)\n        x_cell_11 = self.cell_11(x_cell_10, x_cell_9)\n\n        x_reduction_cell_1 = self.reduction_cell_1(x_cell_11, x_cell_10)\n\n        x_cell_12 = self.cell_12(x_reduction_cell_1, x_cell_10)\n        x_cell_13 = self.cell_13(x_cell_12, x_reduction_cell_1)\n        x_cell_14 = self.cell_14(x_cell_13, x_cell_12)\n        x_cell_15 = self.cell_15(x_cell_14, x_cell_13)\n        x_cell_16 = self.cell_16(x_cell_15, x_cell_14)\n        x_cell_17 = self.cell_17(x_cell_16, x_cell_15)\n        return x_cell_17\n\n    def logits(self, features):\n        x = self.relu(features)\n        x = self.avg_pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.dropout(x)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n'"
pywick/models/segmentation/gcnnets/gcn_psp.py,3,"b'# Source: https://github.com/flixpar/VisDa/tree/master/models\n\n""""""\nImplementation of `Large Kernel Matters <https://arxiv.org/pdf/1703.02719>`_ with PSP backend\n""""""\n\nfrom math import floor\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torchvision import models\n\n__all__ = [\'GCN_PSP\']\n\nclass _GlobalConvModule(nn.Module):\n    def __init__(self, in_dim, out_dim, kernel_size):\n        super(_GlobalConvModule, self).__init__()\n\n        pad0 = floor((kernel_size[0] - 1) / 2)\n        pad1 = floor((kernel_size[1] - 1) / 2)\n\n        self.conv_l1 = nn.Conv2d(in_dim, out_dim, kernel_size=(kernel_size[0], 1), padding=(pad0, 0))\n        self.conv_l2 = nn.Conv2d(out_dim, out_dim, kernel_size=(1, kernel_size[1]), padding=(0, pad1))\n        self.conv_r1 = nn.Conv2d(in_dim, out_dim, kernel_size=(1, kernel_size[1]), padding=(0, pad1))\n        self.conv_r2 = nn.Conv2d(out_dim, out_dim, kernel_size=(kernel_size[0], 1), padding=(pad0, 0))\n\n    def forward(self, x):\n        x_l = self.conv_l1(x)\n        x_l = self.conv_l2(x_l)\n        x_r = self.conv_r1(x)\n        x_r = self.conv_r2(x_r)\n        x = x_l + x_r\n        return x\n\n\nclass _BoundaryRefineModule(nn.Module):\n    def __init__(self, dim):\n        super(_BoundaryRefineModule, self).__init__()\n        self.relu = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        residual = self.conv1(x)\n        residual = self.relu(residual)\n        residual = self.conv2(residual)\n        out = x + residual\n        return out\n\n\nclass _PyramidSpatialPoolingModule(nn.Module):\n    def __init__(self, in_channels, down_channels, out_size, levels=(1, 2, 3, 6)):\n        super(_PyramidSpatialPoolingModule, self).__init__()\n\n        self.out_channels = len(levels) * down_channels\n\n        self.layers = nn.ModuleList()\n        for level in levels:\n            layer = nn.Sequential(\n                nn.AdaptiveAvgPool2d(level),\n                nn.Conv2d(in_channels, down_channels, kernel_size=1, padding=0, bias=False),\n                nn.BatchNorm2d(down_channels),\n                nn.ReLU(inplace=True),\n                nn.Upsample(size=out_size, mode=\'bilinear\')\n            )\n            self.layers.append(layer)\n\n    def forward(self, x):\n        features = [layer(x) for layer in self.layers]\n        out = torch.cat(features, 1)\n\n        return out\n\n\nclass GCN_PSP(nn.Module):\n    def __init__(self, num_classes, pretrained=True, k=7, input_size=512):\n        super(GCN_PSP, self).__init__()\n\n        self.K = k\n        self.input_size = input_size\n\n        resnet = models.resnet152(pretrained=pretrained)\n\n        self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu)\n        self.layer1 = nn.Sequential(resnet.maxpool, resnet.layer1)\n        self.layer2 = resnet.layer2\n        self.layer3 = resnet.layer3\n        self.layer4 = resnet.layer4\n\n        self.gcm1 = _GlobalConvModule(2048, num_classes, (self.K, self.K))\n        self.gcm2 = _GlobalConvModule(1024, num_classes, (self.K, self.K))\n        self.gcm3 = _GlobalConvModule(512, num_classes, (self.K, self.K))\n        self.gcm4 = _GlobalConvModule(256, num_classes, (self.K, self.K))\n\n        self.brm1 = _BoundaryRefineModule(num_classes)\n        self.brm2 = _BoundaryRefineModule(num_classes)\n        self.brm3 = _BoundaryRefineModule(num_classes)\n        self.brm4 = _BoundaryRefineModule(num_classes)\n        self.brm5 = _BoundaryRefineModule(num_classes)\n        self.brm6 = _BoundaryRefineModule(num_classes)\n        self.brm7 = _BoundaryRefineModule(num_classes)\n        self.brm8 = _BoundaryRefineModule(num_classes)\n        self.brm9 = _BoundaryRefineModule(num_classes)\n\n        self.psp = _PyramidSpatialPoolingModule(num_classes, 10, input_size, levels=(1, 2, 3, 6, 8))\n        self.final = nn.Sequential(\n            nn.Conv2d(num_classes + self.psp.out_channels, num_classes, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(num_classes),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_classes, num_classes, kernel_size=1, padding=0)\n        )\n\n        initialize_weights(self.gcm1, self.gcm2, self.gcm3, self.gcm4, self.brm1, self.brm2, self.brm3,\n                           self.brm4, self.brm5, self.brm6, self.brm7, self.brm8, self.brm9,\n                           self.psp, self.final)\n\n    def forward(self, x):\n        fm0 = self.layer0(x)\n        fm1 = self.layer1(fm0)\n        fm2 = self.layer2(fm1)\n        fm3 = self.layer3(fm2)\n        fm4 = self.layer4(fm3)\n\n        gcfm1 = self.brm1(self.gcm1(fm4))\n        gcfm2 = self.brm2(self.gcm2(fm3))\n        gcfm3 = self.brm3(self.gcm3(fm2))\n        gcfm4 = self.brm4(self.gcm4(fm1))\n\n        fs1 = self.brm5(F.interpolate(gcfm1, fm3.size()[2:], mode=\'bilinear\') + gcfm2)\n        fs2 = self.brm6(F.interpolate(fs1, fm2.size()[2:], mode=\'bilinear\') + gcfm3)\n        fs3 = self.brm7(F.interpolate(fs2, fm1.size()[2:], mode=\'bilinear\') + gcfm4)\n        fs4 = self.brm8(F.interpolate(fs3, fm0.size()[2:], mode=\'bilinear\'))\n        fs5 = self.brm9(F.interpolate(fs4, self.input_size, mode=\'bilinear\'))\n\n        ppm = torch.cat([self.psp(fs5), fs5], 1)\n        out = self.final(ppm)\n\n        return out\n\n\ndef initialize_weights(*models):\n    for model in models:\n        for module in model.modules():\n            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n                nn.init.kaiming_normal_(module.weight)\n                if module.bias is not None:\n                    module.bias.data.zero_()\n            elif isinstance(module, nn.BatchNorm2d):\n                module.weight.data.fill_(1)\n                module.bias.data.zero_()\n'"
pywick/models/segmentation/gcnnets/gcn_resnext.py,5,"b'# Source: https://github.com/flixpar/VisDa/tree/master/models\n\n\n""""""\nImplementation of `Large Kernel Matters <https://arxiv.org/pdf/1703.02719>`_ with Resnext backend\n""""""\n\nfrom .. import resnext101_64x4d\nfrom math import floor\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\n__all__ = [\'GCN_Resnext\']\n\n################## GCN Modules #####################\n\nclass _GlobalConvModule(nn.Module):\n    def __init__(self, in_dim, out_dim, kernel_size):\n        super(_GlobalConvModule, self).__init__()\n\n        pad0 = floor((kernel_size[0] - 1) / 2)\n        pad1 = floor((kernel_size[1] - 1) / 2)\n\n        self.conv_l1 = nn.Conv2d(in_dim, out_dim, kernel_size=(kernel_size[0], 1), padding=(pad0, 0))\n        self.conv_l2 = nn.Conv2d(out_dim, out_dim, kernel_size=(1, kernel_size[1]), padding=(0, pad1))\n        self.conv_r1 = nn.Conv2d(in_dim, out_dim, kernel_size=(1, kernel_size[1]), padding=(0, pad1))\n        self.conv_r2 = nn.Conv2d(out_dim, out_dim, kernel_size=(kernel_size[0], 1), padding=(pad0, 0))\n\n    def forward(self, x):\n        x_l = self.conv_l1(x)\n        x_l = self.conv_l2(x_l)\n        x_r = self.conv_r1(x)\n        x_r = self.conv_r2(x_r)\n        x = x_l + x_r\n        return x\n\n\nclass _BoundaryRefineModule(nn.Module):\n    def __init__(self, dim):\n        super(_BoundaryRefineModule, self).__init__()\n        self.relu = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        residual = self.conv1(x)\n        residual = self.relu(residual)\n        residual = self.conv2(residual)\n        out = x + residual\n        return out\n\n\nclass _DeconvModule(nn.Module):\n    def __init__(self, channels):\n        super(_DeconvModule, self).__init__()\n        self.deconv = nn.ConvTranspose2d(channels, channels, kernel_size=4, stride=2, padding=1)\n        self.deconv.weight.data = self.make_bilinear_weights(4, channels)\n        self.deconv.bias.data.zero_()\n\n    def forward(self, x):\n        out = self.deconv(x)\n        return out\n\n    def make_bilinear_weights(self, size, num_channels):\n        factor = (size + 1) // 2\n        if size % 2 == 1:\n            center = factor - 1\n        else:\n            center = factor - 0.5\n        og = np.ogrid[:size, :size]\n        filt = (1 - abs(og[0] - center) / factor) * (1 - abs(og[1] - center) / factor)\n        filt = torch.from_numpy(filt)\n        w = torch.zeros(num_channels, num_channels, size, size)\n        for i in range(num_channels):\n            w[i, i] = filt\n        return w\n\n\nclass _PyramidSpatialPoolingModule(nn.Module):\n    def __init__(self, in_channels, down_channels, out_size, levels=(1, 2, 3, 6)):\n        super(_PyramidSpatialPoolingModule, self).__init__()\n\n        self.out_channels = len(levels) * down_channels\n\n        self.layers = nn.ModuleList()\n        for level in levels:\n            layer = nn.Sequential(\n                nn.AdaptiveAvgPool2d(level),\n                nn.Conv2d(in_channels, down_channels, kernel_size=1, padding=0, bias=False),\n                nn.BatchNorm2d(down_channels),\n                nn.ReLU(inplace=True),\n                nn.Upsample(size=out_size, mode=\'bilinear\')\n            )\n            self.layers.append(layer)\n\n    def forward(self, x):\n        features = [layer(x) for layer in self.layers]\n        out = torch.cat(features, 1)\n\n        return out\n\n########################### ResNeXt ###########################\n\nclass LambdaBase(nn.Sequential):\n\tdef __init__(self, fn, *args):\n\t\tsuper(LambdaBase, self).__init__(*args)\n\t\tself.lambda_func = fn\n\n\tdef forward_prepare(self, input):\n\t\toutput = []\n\t\tfor module in self._modules.values():\n\t\t\toutput.append(module(input))\n\t\treturn output if output else input\n\nclass Lambda(LambdaBase):\n    def forward(self, input):\n        return self.lambda_func(self.forward_prepare(input))\n\nclass ResNeXt(nn.Module):\n\n    def __init__(self, pretrained=True):\n        super(ResNeXt, self).__init__()\n\n        if pretrained:\n            self.resnext = resnext101_64x4d()\n        else:\n            self.resnext = resnext101_64x4d(pretrained=None)\n\n        self.layer0 = nn.Sequential(\n            self.resnext.features[0],\n            self.resnext.features[1],\n            self.resnext.features[2],\n            self.resnext.features[3]\n        )\n\n        self.layer1 = self.resnext.features[4]\n        self.layer2 = self.resnext.features[5]\n        self.layer3 = self.resnext.features[6]\n        self.layer4 = self.resnext.features[7]\n\n        self.layer5 = nn.Sequential(\n            nn.AvgPool2d((7, 7), (1, 1)),\n            Lambda(lambda x: x.view(x.size(0), -1)),  # View,\n            nn.Sequential(Lambda(lambda x: x.view(1, -1) if 1 == len(x.size()) else x), nn.Linear(2048, 1000))\n        )\n\n    def forward(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n\n        return x\n\n\n############################## GCN #################################\n\nclass GCN_Resnext(nn.Module):\n\n    def __init__(self, num_classes, pretrained=True, k=7, input_size=512, **kwargs):\n        super(GCN_Resnext, self).__init__()\n\n        self.num_classes = num_classes\n        self.K = k\n        num_imd_feats = 40\n\n        self.resnext = ResNeXt(pretrained)\n\n        self.gcm1 = _GlobalConvModule(2048, num_imd_feats, (self.K, self.K))\n        self.gcm2 = _GlobalConvModule(1024, num_imd_feats, (self.K, self.K))\n        self.gcm3 = _GlobalConvModule(512, num_imd_feats, (self.K, self.K))\n        self.gcm4 = _GlobalConvModule(256, num_imd_feats, (self.K, self.K))\n\n        self.brm1 = _BoundaryRefineModule(num_imd_feats)\n        self.brm2 = _BoundaryRefineModule(num_imd_feats)\n        self.brm3 = _BoundaryRefineModule(num_imd_feats)\n        self.brm4 = _BoundaryRefineModule(num_imd_feats)\n        self.brm5 = _BoundaryRefineModule(num_imd_feats)\n        self.brm6 = _BoundaryRefineModule(num_imd_feats)\n        self.brm7 = _BoundaryRefineModule(num_imd_feats)\n        self.brm8 = _BoundaryRefineModule(num_imd_feats)\n        self.brm9 = _BoundaryRefineModule(num_imd_feats)\n\n        self.deconv = _DeconvModule(num_imd_feats)\n\n        self.psp_module = _PyramidSpatialPoolingModule(num_imd_feats, 30, input_size, levels=(1, 2, 3, 6))\n        self.final = nn.Sequential(\n            nn.Conv2d(num_imd_feats + self.psp_module.out_channels, num_imd_feats, kernel_size=3, padding=1),\n            nn.BatchNorm2d(num_imd_feats),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_imd_feats, num_classes, kernel_size=1, padding=0)\n        )\n\n        self.initialize_weights(self.gcm1, self.gcm2, self.gcm3, self.gcm4)\n        self.initialize_weights(self.brm1, self.brm2, self.brm3, self.brm4, self.brm5, self.brm6, self.brm7, self.brm8, self.brm9)\n        self.initialize_weights(self.psp_module, self.final)\n\n    def forward(self, x):\n\n        fm0 = self.resnext.layer0(x)\n        fm1 = self.resnext.layer1(fm0)\n        fm2 = self.resnext.layer2(fm1)\n        fm3 = self.resnext.layer3(fm2)\n        fm4 = self.resnext.layer4(fm3)\n\n        gcfm1 = self.brm1(self.gcm1(fm4))\n        gcfm2 = self.brm2(self.gcm2(fm3))\n        gcfm3 = self.brm3(self.gcm3(fm2))\n        gcfm4 = self.brm4(self.gcm4(fm1))\n\n        fs1 = self.brm5(self.deconv(gcfm1) + gcfm2)\n        fs2 = self.brm6(self.deconv(fs1) + gcfm3)\n        fs3 = self.brm7(self.deconv(fs2) + gcfm4)\n        fs4 = self.brm8(self.deconv(fs3))\n        fs5 = self.brm9(self.deconv(fs4))\n\n        p = torch.cat([self.psp_module(fs5), fs5], 1)\n        out = self.final(p)\n\n        return out\n\n    def initialize_weights(self, *models):\n        for model in models:\n            for module in model.modules():\n                if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n                    nn.init.kaiming_normal_(module.weight)\n                    if module.bias is not None:\n                        module.bias.data.zero_()\n                elif isinstance(module, nn.BatchNorm2d):\n                    module.weight.data.fill_(1)\n                    module.bias.data.zero_()\n'"
pywick/models/segmentation/mnas_linknets/__init__.py,0,b'from .linknet import *'
pywick/models/segmentation/mnas_linknets/decoder.py,0,"b'from torch import nn\n\nnonlinearity = nn.ReLU\n\ndef conv3x3(in_, out):\n    return nn.Conv2d(in_, out, 3, padding=1)\n\nclass ConvRelu(nn.Module):\n    def __init__(self, in_, out):\n        super().__init__()\n        self.conv = conv3x3(in_, out)\n        self.activation = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.activation(x)\n        return x\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, middle_channels, out_channels):\n        super().__init__()\n\n        self.block = nn.Sequential(\n            ConvRelu(in_channels, middle_channels),\n            nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.block(x)\n    \nclass DecoderBlockV2(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 middle_channels,\n                 out_channels,\n                 is_deconv=True):\n        \n        super(DecoderBlockV2, self).__init__()\n        self.in_channels = in_channels\n\n        if is_deconv:\n            """"""\n                Paramaters for Deconvolution were chosen to avoid artifacts, following\n                link https://distill.pub/2016/deconv-checkerboard/\n            """"""\n\n            self.block = nn.Sequential(\n                ConvRelu(in_channels, middle_channels),\n                nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=4, stride=2,\n                                   padding=1),\n                nn.ReLU(inplace=True)\n            )\n        else:\n            self.block = nn.Sequential(\n                nn.Upsample(scale_factor=2, mode=\'bilinear\'),\n                ConvRelu(in_channels, middle_channels),\n                ConvRelu(middle_channels, out_channels),\n            )\n\n    def forward(self, x):\n        return self.block(x)    \n    \nclass DecoderBlockLinkNet(nn.Module):\n    def __init__(self,\n                 in_channels=512,\n                 n_filters=256,\n                 kernel_size=3,\n                 is_deconv = False,\n                ):\n        super().__init__()\n\n        if kernel_size == 3:\n            conv_padding = 1\n        elif kernel_size == 1:\n            conv_padding = 0\n            \n        # B, C, H, W -> B, C/4, H, W\n        self.conv1 = nn.Conv2d(in_channels,\n                               in_channels // 4,\n                               kernel_size,\n                               padding = 1)\n        self.norm1 = nn.BatchNorm2d(in_channels // 4)\n        self.relu1 = nonlinearity(inplace=True)\n\n        # B, C/4, H, W -> B, C/4, H, W\n        if is_deconv == True:\n            self.deconv2 = nn.ConvTranspose2d(in_channels // 4,\n                                              in_channels // 4,\n                                              3,\n                                              stride=2,\n                                              padding=1,\n                                              output_padding=conv_padding)\n        else:\n            self.deconv2 = nn.Upsample(scale_factor=2)\n        \n        self.norm2 = nn.BatchNorm2d(in_channels // 4)\n        self.relu2 = nonlinearity(inplace=True)\n\n        # B, C/4, H, W -> B, C, H, W\n        self.conv3 = nn.Conv2d(in_channels // 4,\n                               n_filters,\n                               kernel_size,\n                               padding = conv_padding)\n        self.norm3 = nn.BatchNorm2d(n_filters)\n        self.relu3 = nonlinearity(inplace=True)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.norm1(x)\n        x = self.relu1(x)\n        x = self.deconv2(x)\n        x = self.norm2(x)\n        x = self.relu2(x)\n        x = self.conv3(x)\n        x = self.norm3(x)\n        x = self.relu3(x)\n        return x\n\nclass DecoderBlockLinkNetV2(nn.Module):\n    def __init__(self,\n                 in_channels=512,\n                 n_filters=256,\n                 kernel_size=4,\n                 is_deconv=False,\n                 is_upsample=True,\n                 ):\n        super().__init__()\n\n        \n        self.is_upsample = is_upsample\n        \n        if kernel_size == 3:\n            conv_stride = 1\n        elif kernel_size == 1:\n            conv_stride = 1\n        elif kernel_size == 4:\n            conv_stride = 2\n\n        # B, C, H, W -> B, C/4, H, W\n        self.conv1 = nn.Conv2d(in_channels,\n                               in_channels // 4,\n                               3,\n                               padding=1)\n        self.norm1 = nn.BatchNorm2d(in_channels // 4)\n        self.relu1 = nonlinearity(inplace=True)\n\n        # B, C/4, H, W -> B, C/4, H, W\n        if is_deconv == True:\n            self.deconv2 = nn.ConvTranspose2d(in_channels // 4,\n                                              in_channels // 4,\n                                              kernel_size,\n                                              stride=conv_stride,\n                                              padding=1)\n        else:\n            self.deconv2 = nn.Upsample(scale_factor=2)\n\n        self.norm2 = nn.BatchNorm2d(in_channels // 4)\n        self.relu2 = nonlinearity(inplace=True)\n\n        # B, C/4, H, W -> B, C, H, W\n        self.conv3 = nn.Conv2d(in_channels // 4,\n                               n_filters,\n                               3,\n                               padding=1)\n        self.norm3 = nn.BatchNorm2d(n_filters)\n        self.relu3 = nonlinearity(inplace=True)\n\n    def forward(self, x):\n        if self.is_upsample:\n            x = self.conv1(x)\n            x = self.norm1(x)\n            x = self.relu1(x)\n            x = self.deconv2(x)\n            x = self.norm2(x)\n            x = self.relu2(x)\n            x = self.conv3(x)\n            x = self.norm3(x)\n            x = self.relu3(x)\n        else:\n            x = self.conv1(x)\n            x = self.norm1(x)\n            x = self.relu1(x)\n            x = self.norm2(x)\n            x = self.relu2(x)\n            x = self.conv3(x)\n            x = self.norm3(x)\n            x = self.relu3(x)\n        return x\n\nclass DecoderBlockLinkNetInceptionV2(nn.Module):\n    def __init__(self,\n                 in_channels=512,\n                 out_channels=512,\n                 n_filters=256,\n                 last_padding=0,\n                 kernel_size=3,\n                 is_deconv = False\n                ):\n        super().__init__()\n\n        if kernel_size == 3:\n            conv_stride = 1              \n        elif kernel_size == 1:\n            conv_stride = 1              \n        elif kernel_size == 4:\n            conv_stride = 2              \n        \n        # B, C, H, W -> B, out_channels, H, W\n        self.conv1 = nn.Conv2d(in_channels,\n                               out_channels,\n                               3,\n                               padding = 2)\n        \n        self.norm1 = nn.BatchNorm2d(out_channels)\n        self.relu1 = nonlinearity(inplace=True)\n\n        # B, out_channels, H, W -> B, out_channels, H, W\n        if is_deconv == True:\n            self.deconv2 = nn.ConvTranspose2d(out_channels,\n                                              out_channels,\n                                              kernel_size,\n                                              stride=conv_stride,\n                                              padding=1)\n        else:\n            self.deconv2 = nn.Upsample(scale_factor=2)\n            \n        self.norm2 = nn.BatchNorm2d(out_channels)\n        self.relu2 = nonlinearity(inplace=True)\n\n        # B, out_channels, H, W -> B, C, H, W\n        self.conv3 = nn.Conv2d(out_channels,\n                               n_filters,\n                               3,\n                               padding = 1+last_padding)\n        \n        self.norm3 = nn.BatchNorm2d(n_filters)\n        self.relu3 = nonlinearity(inplace=True)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.norm1(x)\n        x = self.relu1(x)\n        x = self.deconv2(x)\n        x = self.norm2(x)\n        x = self.relu2(x)\n        x = self.conv3(x)\n        x = self.norm3(x)\n        x = self.relu3(x)\n        return x    '"
pywick/models/segmentation/mnas_linknets/inception4.py,12,"b'""""""\nInception V4 architecture as described in:\n`Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning <http://arxiv.org/abs/1602.07261>`_.\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\nimport os\nimport sys\n\n__all__ = [\'InceptionV4\', \'inceptionv4\']\n\npretrained_settings = {\n    \'inceptionv4\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/inceptionv4-8e4777a0.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 299, 299],\n            \'input_range\': [0, 1],\n            \'mean\': [0.5, 0.5, 0.5],\n            \'std\': [0.5, 0.5, 0.5],\n            \'num_classes\': 1000\n        },\n        \'imagenet+background\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/inceptionv4-8e4777a0.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 299, 299],\n            \'input_range\': [0, 1],\n            \'mean\': [0.5, 0.5, 0.5],\n            \'std\': [0.5, 0.5, 0.5],\n            \'num_classes\': 1001\n        }\n    }\n}\n\n\nclass BasicConv2d(nn.Module):\n\n    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_planes, out_planes,\n                              kernel_size=kernel_size, stride=stride,\n                              padding=padding, bias=False) # verify bias false\n        self.bn = nn.BatchNorm2d(out_planes,\n                                 eps=0.001, # value found in tensorflow\n                                 momentum=0.1, # default pytorch value\n                                 affine=True)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n\nclass Mixed_3a(nn.Module):\n\n    def __init__(self):\n        super(Mixed_3a, self).__init__()\n        self.maxpool = nn.MaxPool2d(3, stride=2)\n        self.conv = BasicConv2d(64, 96, kernel_size=3, stride=2)\n\n    def forward(self, x):\n        x0 = self.maxpool(x)\n        x1 = self.conv(x)\n        out = torch.cat((x0, x1), 1)\n        return out\n\n\nclass Mixed_4a(nn.Module):\n\n    def __init__(self):\n        super(Mixed_4a, self).__init__()\n\n        self.branch0 = nn.Sequential(\n            BasicConv2d(160, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1)\n        )\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(160, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 64, kernel_size=(1,7), stride=1, padding=(0,3)),\n            BasicConv2d(64, 64, kernel_size=(7,1), stride=1, padding=(3,0)),\n            BasicConv2d(64, 96, kernel_size=(3,3), stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        out = torch.cat((x0, x1), 1)\n        return out\n\n\nclass Mixed_5a(nn.Module):\n\n    def __init__(self):\n        super(Mixed_5a, self).__init__()\n        self.conv = BasicConv2d(192, 192, kernel_size=3, stride=2)\n        self.maxpool = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.conv(x)\n        x1 = self.maxpool(x)\n        out = torch.cat((x0, x1), 1)\n        return out\n\n\nclass Inception_A(nn.Module):\n\n    def __init__(self):\n        super(Inception_A, self).__init__()\n        self.branch0 = BasicConv2d(384, 96, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(384, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(384, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(96, 96, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            BasicConv2d(384, 96, kernel_size=1, stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Reduction_A(nn.Module):\n\n    def __init__(self):\n        super(Reduction_A, self).__init__()\n        self.branch0 = BasicConv2d(384, 384, kernel_size=3, stride=2)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(384, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 224, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(224, 256, kernel_size=3, stride=2)\n        )\n        \n        self.branch2 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        return out\n\n\nclass Inception_B(nn.Module):\n\n    def __init__(self):\n        super(Inception_B, self).__init__()\n        self.branch0 = BasicConv2d(1024, 384, kernel_size=1, stride=1)\n        \n        self.branch1 = nn.Sequential(\n            BasicConv2d(1024, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 224, kernel_size=(1,7), stride=1, padding=(0,3)),\n            BasicConv2d(224, 256, kernel_size=(7,1), stride=1, padding=(3,0))\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(1024, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 192, kernel_size=(7,1), stride=1, padding=(3,0)),\n            BasicConv2d(192, 224, kernel_size=(1,7), stride=1, padding=(0,3)),\n            BasicConv2d(224, 224, kernel_size=(7,1), stride=1, padding=(3,0)),\n            BasicConv2d(224, 256, kernel_size=(1,7), stride=1, padding=(0,3))\n        )\n\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            BasicConv2d(1024, 128, kernel_size=1, stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Reduction_B(nn.Module):\n\n    def __init__(self):\n        super(Reduction_B, self).__init__()\n\n        self.branch0 = nn.Sequential(\n            BasicConv2d(1024, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 192, kernel_size=3, stride=2)\n        )\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(1024, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 256, kernel_size=(1,7), stride=1, padding=(0,3)),\n            BasicConv2d(256, 320, kernel_size=(7,1), stride=1, padding=(3,0)),\n            BasicConv2d(320, 320, kernel_size=3, stride=2)\n        )\n\n        self.branch2 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        return out\n\n\nclass Inception_C(nn.Module):\n\n    def __init__(self):\n        super(Inception_C, self).__init__()\n\n        self.branch0 = BasicConv2d(1536, 256, kernel_size=1, stride=1)\n        \n        self.branch1_0 = BasicConv2d(1536, 384, kernel_size=1, stride=1)\n        self.branch1_1a = BasicConv2d(384, 256, kernel_size=(1,3), stride=1, padding=(0,1))\n        self.branch1_1b = BasicConv2d(384, 256, kernel_size=(3,1), stride=1, padding=(1,0))\n        \n        self.branch2_0 = BasicConv2d(1536, 384, kernel_size=1, stride=1)\n        self.branch2_1 = BasicConv2d(384, 448, kernel_size=(3,1), stride=1, padding=(1,0))\n        self.branch2_2 = BasicConv2d(448, 512, kernel_size=(1,3), stride=1, padding=(0,1))\n        self.branch2_3a = BasicConv2d(512, 256, kernel_size=(1,3), stride=1, padding=(0,1))\n        self.branch2_3b = BasicConv2d(512, 256, kernel_size=(3,1), stride=1, padding=(1,0))\n        \n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            BasicConv2d(1536, 256, kernel_size=1, stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        \n        x1_0 = self.branch1_0(x)\n        x1_1a = self.branch1_1a(x1_0)\n        x1_1b = self.branch1_1b(x1_0)\n        x1 = torch.cat((x1_1a, x1_1b), 1)\n\n        x2_0 = self.branch2_0(x)\n        x2_1 = self.branch2_1(x2_0)\n        x2_2 = self.branch2_2(x2_1)\n        x2_3a = self.branch2_3a(x2_2)\n        x2_3b = self.branch2_3b(x2_2)\n        x2 = torch.cat((x2_3a, x2_3b), 1)\n\n        x3 = self.branch3(x)\n\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass InceptionV4(nn.Module):\n    def __init__(self, num_classes=1001):\n        super(InceptionV4, self).__init__()\n        # Special attributs\n        self.input_space = None\n        self.input_size = (299, 299, 3)\n        self.mean = None\n        self.std = None\n        # Modules\n        self.features = nn.Sequential(\n            BasicConv2d(3, 32, kernel_size=3, stride=2),\n            BasicConv2d(32, 32, kernel_size=3, stride=1),\n            BasicConv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            Mixed_3a(),\n            Mixed_4a(),\n            Mixed_5a(),\n            Inception_A(),\n            Inception_A(),\n            Inception_A(),\n            Inception_A(),\n            Reduction_A(), # Mixed_6a\n            Inception_B(),\n            Inception_B(),\n            Inception_B(),\n            Inception_B(),\n            Inception_B(),\n            Inception_B(),\n            Inception_B(),\n            Reduction_B(), # Mixed_7a\n            Inception_C(),\n            Inception_C(),\n            Inception_C()\n        )\n        self.avg_pool = nn.AvgPool2d(8, count_include_pad=False)\n        self.last_linear = nn.Linear(1536, num_classes)\n\n    def logits(self, features):\n        x = self.avg_pool(features)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x) \n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n\ndef inceptionv4(pretrained=\'imagenet\'):\n    """"""Pretrained InceptionV4 model""""""\n    settings = pretrained_settings[\'inceptionv4\'][pretrained]\n\n    # both \'imagenet\'&\'imagenet+background\' are loaded from same parameters\n    model = InceptionV4(num_classes=1001)\n    model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n\n    if pretrained == \'imagenet\':\n        new_last_linear = nn.Linear(1536, settings[\'num_classes\'])\n        new_last_linear.weight.data = model.last_linear.weight.data[1:]\n        new_last_linear.bias.data = model.last_linear.bias.data[1:]\n        model.last_linear = new_last_linear\n\n    model.input_space = settings[\'input_space\']\n    model.input_size = settings[\'input_size\']\n    model.input_range = settings[\'input_range\']\n    model.mean = settings[\'mean\']\n    model.std = settings[\'std\']\n    return model\n'"
pywick/models/segmentation/mnas_linknets/inception_resnet.py,8,"b'import torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\n__all__ = [\'InceptionResNetV2\', \'inceptionresnetv2\']\n\npretrained_settings = {\n    \'inceptionresnetv2\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/inceptionresnetv2-520b38e4.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 299, 299],\n            \'input_range\': [0, 1],\n            \'mean\': [0.5, 0.5, 0.5],\n            \'std\': [0.5, 0.5, 0.5],\n            \'num_classes\': 1000\n        },\n        \'imagenet+background\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/inceptionresnetv2-520b38e4.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 299, 299],\n            \'input_range\': [0, 1],\n            \'mean\': [0.5, 0.5, 0.5],\n            \'std\': [0.5, 0.5, 0.5],\n            \'num_classes\': 1001\n        }\n    }\n}\n\n\nclass BasicConv2d(nn.Module):\n\n    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_planes, out_planes,\n                              kernel_size=kernel_size, stride=stride,\n                              padding=padding, bias=False) # verify bias false\n        self.bn = nn.BatchNorm2d(out_planes,\n                                 eps=0.001, # value found in tensorflow\n                                 momentum=0.1, # default pytorch value\n                                 affine=True)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n\nclass Mixed_5b(nn.Module):\n\n    def __init__(self):\n        super(Mixed_5b, self).__init__()\n\n        self.branch0 = BasicConv2d(192, 96, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(192, 48, kernel_size=1, stride=1),\n            BasicConv2d(48, 64, kernel_size=5, stride=1, padding=2)\n        ) \n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(192, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(96, 96, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            BasicConv2d(192, 64, kernel_size=1, stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Block35(nn.Module):\n\n    def __init__(self, scale=1.0):\n        super(Block35, self).__init__()\n\n        self.scale = scale\n\n        self.branch0 = BasicConv2d(320, 32, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(320, 32, kernel_size=1, stride=1),\n            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(320, 32, kernel_size=1, stride=1),\n            BasicConv2d(32, 48, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(48, 64, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.conv2d = nn.Conv2d(128, 320, kernel_size=1, stride=1)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        out = self.relu(out)\n        return out\n\n\nclass Mixed_6a(nn.Module):\n\n    def __init__(self):\n        super(Mixed_6a, self).__init__()\n        \n        self.branch0 = BasicConv2d(320, 384, kernel_size=3, stride=2)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(320, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(256, 384, kernel_size=3, stride=2)\n        )\n\n        self.branch2 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        return out\n\n\nclass Block17(nn.Module):\n\n    def __init__(self, scale=1.0):\n        super(Block17, self).__init__()\n\n        self.scale = scale\n\n        self.branch0 = BasicConv2d(1088, 192, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(1088, 128, kernel_size=1, stride=1),\n            BasicConv2d(128, 160, kernel_size=(1,7), stride=1, padding=(0,3)),\n            BasicConv2d(160, 192, kernel_size=(7,1), stride=1, padding=(3,0))\n        )\n\n        self.conv2d = nn.Conv2d(384, 1088, kernel_size=1, stride=1)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        out = torch.cat((x0, x1), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        out = self.relu(out)\n        return out\n\n\nclass Mixed_7a(nn.Module):\n\n    def __init__(self):\n        super(Mixed_7a, self).__init__()\n        \n        self.branch0 = nn.Sequential(\n            BasicConv2d(1088, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 384, kernel_size=3, stride=2)\n        )\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(1088, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 288, kernel_size=3, stride=2)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(1088, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 288, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(288, 320, kernel_size=3, stride=2)\n        )\n\n        self.branch3 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Block8(nn.Module):\n\n    def __init__(self, scale=1.0, noReLU=False):\n        super(Block8, self).__init__()\n\n        self.scale = scale\n        self.noReLU = noReLU\n\n        self.branch0 = BasicConv2d(2080, 192, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(2080, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 224, kernel_size=(1,3), stride=1, padding=(0,1)),\n            BasicConv2d(224, 256, kernel_size=(3,1), stride=1, padding=(1,0))\n        )\n\n        self.conv2d = nn.Conv2d(448, 2080, kernel_size=1, stride=1)\n        if not self.noReLU:\n            self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        out = torch.cat((x0, x1), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        if not self.noReLU:\n            out = self.relu(out)\n        return out\n\n\nclass InceptionResNetV2(nn.Module):\n\n    def __init__(self, num_classes=1001):\n        super(InceptionResNetV2, self).__init__()\n        # Special attributs\n        self.input_space = None\n        self.input_size = (299, 299, 3)\n        self.mean = None\n        self.std = None\n        # Modules\n        self.conv2d_1a = BasicConv2d(3, 32, kernel_size=3, stride=2)\n        self.conv2d_2a = BasicConv2d(32, 32, kernel_size=3, stride=1)\n        self.conv2d_2b = BasicConv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.maxpool_3a = nn.MaxPool2d(3, stride=2)\n        self.conv2d_3b = BasicConv2d(64, 80, kernel_size=1, stride=1)\n        self.conv2d_4a = BasicConv2d(80, 192, kernel_size=3, stride=1)\n        self.maxpool_5a = nn.MaxPool2d(3, stride=2)\n        self.mixed_5b = Mixed_5b()\n        self.repeat = nn.Sequential(\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17)\n        )\n        self.mixed_6a = Mixed_6a()\n        self.repeat_1 = nn.Sequential(\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10)\n        )\n        self.mixed_7a = Mixed_7a()\n        self.repeat_2 = nn.Sequential(\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20)\n        )\n        self.block8 = Block8(noReLU=True)\n        self.conv2d_7b = BasicConv2d(2080, 1536, kernel_size=1, stride=1)\n        self.avgpool_1a = nn.AvgPool2d(8, count_include_pad=False)\n        self.last_linear = nn.Linear(1536, num_classes)\n\n    def features(self, input):\n        x = self.conv2d_1a(input)\n        x = self.conv2d_2a(x)\n        x = self.conv2d_2b(x)\n        x = self.maxpool_3a(x)\n        x = self.conv2d_3b(x)\n        x = self.conv2d_4a(x)\n        x = self.maxpool_5a(x)\n        x = self.mixed_5b(x)\n        x = self.repeat(x)\n        x = self.mixed_6a(x)\n        x = self.repeat_1(x)\n        x = self.mixed_7a(x)\n        x = self.repeat_2(x)\n        x = self.block8(x)\n        x = self.conv2d_7b(x)\n        return x\n\n    def logits(self, features):\n        x = self.avgpool_1a(features)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x) \n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\ndef inceptionresnetv2(num_classes=1001, pretrained=\'imagenet\'):\n    r""""""InceptionResNetV2 model architecture from the\n    `""InceptionV4, Inception-ResNet..."" <https://arxiv.org/abs/1602.07261>`_ paper.\n    """"""\n    if pretrained:\n        settings = pretrained_settings[\'inceptionresnetv2\'][pretrained]\n        assert num_classes == settings[\'num_classes\'], \\\n            ""num_classes should be {}, but is {}"".format(settings[\'num_classes\'], num_classes)\n\n        # both \'imagenet\'&\'imagenet+background\' are loaded from same parameters\n        model = InceptionResNetV2(num_classes=1001)\n        model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n        \n        if pretrained == \'imagenet\':\n            new_last_linear = nn.Linear(1536, 1000)\n            new_last_linear.weight.data = model.last_linear.weight.data[1:]\n            new_last_linear.bias.data = model.last_linear.bias.data[1:]\n            model.last_linear = new_last_linear\n        \n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n        \n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n    else:\n        model = InceptionResNetV2(num_classes=num_classes)\n    return model'"
pywick/models/segmentation/mnas_linknets/linknet.py,2,"b'# Source: https://github.com/snakers4/mnasnet-pytorch/blob/master/src/models/linknet.py\n\n""""""\nImplementation of `LinkNet: Exploiting Encoder Representations for Efficient Semantic Segmentation <https://arxiv.org/abs/1707.03718>`_\n""""""\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\nfrom .resnext import resnext101_32x4d\nfrom .inception_resnet import inceptionresnetv2\nfrom .inception4 import inceptionv4\nfrom .decoder import DecoderBlockLinkNetV2 as DecoderBlock\nfrom .decoder import DecoderBlockLinkNetInceptionV2 as DecoderBlockInception\n\n__all__ = [\'LinkCeption\', \'LinkDenseNet121\', \'LinkDenseNet161\', \'LinkInceptionResNet\', \'LinkNet18\', \'LinkNet34\', \'LinkNet50\', \'LinkNet101\', \'LinkNet152\', \'LinkNeXt\', \'CoarseLinkNet50\']\n\nnonlinearity = nn.ReLU\n\n\nclass BasicConv2d(nn.Module):\n    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_planes, out_planes,\n                              kernel_size=kernel_size, stride=stride,\n                              padding=padding, bias=False)  # verify bias false\n        self.bn = nn.BatchNorm2d(out_planes,\n                                 eps=0.001,  # value found in tensorflow\n                                 momentum=0.1,  # default pytorch value\n                                 affine=True)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n\nclass LinkNet18(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 pretrained=True,\n                 num_channels=3,\n                 is_deconv=False,\n                 decoder_kernel_size=4,\n                 **kwargs\n                 ):\n        super().__init__()\n\n        filters = [64, 128, 256, 512]\n        resnet = models.resnet18(pretrained=pretrained)\n\n        self.mean = (0.485, 0.456, 0.406)\n        self.std = (0.229, 0.224, 0.225)\n\n        if num_channels == 3:\n            self.firstconv = resnet.conv1\n        else:\n            self.firstconv = nn.Conv2d(num_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n\n        self.firstbn = resnet.bn1\n        self.firstrelu = resnet.relu\n        self.firstmaxpool = resnet.maxpool\n        self.encoder1 = resnet.layer1\n        self.encoder2 = resnet.layer2\n        self.encoder3 = resnet.layer3\n        self.encoder4 = resnet.layer4\n\n        # Decoder\n        self.decoder4 = DecoderBlock(in_channels=filters[3],\n                                     n_filters=filters[2],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n        self.decoder3 = DecoderBlock(in_channels=filters[2],\n                                     n_filters=filters[1],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n        self.decoder2 = DecoderBlock(in_channels=filters[1],\n                                     n_filters=filters[0],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n        self.decoder1 = DecoderBlock(in_channels=filters[0],\n                                     n_filters=filters[0],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n\n        # Final Classifier\n        self.finaldeconv1 = nn.ConvTranspose2d(filters[0], 32, 3, stride=2)\n        self.finalrelu1 = nonlinearity(inplace=True)\n        self.finalconv2 = nn.Conv2d(32, 32, 3)\n        self.finalrelu2 = nonlinearity(inplace=True)\n        self.finalconv3 = nn.Conv2d(32, num_classes, 2, padding=1)\n\n    def freeze(self):\n        self.require_encoder_grad(False)\n\n    def unfreeze(self):\n        self.require_encoder_grad(True)\n\n    def require_encoder_grad(self, requires_grad):\n        blocks = [self.firstconv,\n                  self.encoder1,\n                  self.encoder2,\n                  self.encoder3,\n                  self.encoder4]\n\n        for block in blocks:\n            for p in block.parameters():\n                p.requires_grad = requires_grad\n\n    # noinspection PyCallingNonCallable\n    def forward(self, x):\n        # Encoder\n        x = self.firstconv(x)\n        x = self.firstbn(x)\n        x = self.firstrelu(x)\n        x = self.firstmaxpool(x)\n        e1 = self.encoder1(x)\n        e2 = self.encoder2(e1)\n        e3 = self.encoder3(e2)\n        e4 = self.encoder4(e3)\n\n        # Decoder with Skip Connections\n        d4 = self.decoder4(e4) + e3\n        d3 = self.decoder3(d4) + e2\n        d2 = self.decoder2(d3) + e1\n        d1 = self.decoder1(d2)\n\n        # Final Classification\n        f1 = self.finaldeconv1(d1)\n        f2 = self.finalrelu1(f1)\n        f3 = self.finalconv2(f2)\n        f4 = self.finalrelu2(f3)\n        f5 = self.finalconv3(f4)\n\n        return f5\n\n\nclass LinkNet34(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 pretrained=True,\n                 num_channels=3,\n                 is_deconv=False,\n                 decoder_kernel_size=4,\n                 **kwargs\n                 ):\n        super().__init__()\n\n        filters = [64, 128, 256, 512]\n        resnet = models.resnet34(pretrained=pretrained)\n\n        self.mean = (0.485, 0.456, 0.406)\n        self.std = (0.229, 0.224, 0.225)\n\n        if num_channels == 3:\n            self.firstconv = resnet.conv1\n        else:\n            self.firstconv = nn.Conv2d(num_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n\n        self.firstbn = resnet.bn1\n        self.firstrelu = resnet.relu\n        self.firstmaxpool = resnet.maxpool\n        self.encoder1 = resnet.layer1\n        self.encoder2 = resnet.layer2\n        self.encoder3 = resnet.layer3\n        self.encoder4 = resnet.layer4\n\n        # Decoder\n        self.decoder4 = DecoderBlock(in_channels=filters[3],\n                                     n_filters=filters[2],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n        self.decoder3 = DecoderBlock(in_channels=filters[2],\n                                     n_filters=filters[1],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n        self.decoder2 = DecoderBlock(in_channels=filters[1],\n                                     n_filters=filters[0],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n        self.decoder1 = DecoderBlock(in_channels=filters[0],\n                                     n_filters=filters[0],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n\n        # Final Classifier\n        self.finaldeconv1 = nn.ConvTranspose2d(filters[0], 32, 3, stride=2)\n        self.finalrelu1 = nonlinearity(inplace=True)\n        self.finalconv2 = nn.Conv2d(32, 32, 3)\n        self.finalrelu2 = nonlinearity(inplace=True)\n        self.finalconv3 = nn.Conv2d(32, num_classes, 2, padding=1)\n\n    def freeze(self):\n        self.require_encoder_grad(False)\n\n    def unfreeze(self):\n        self.require_encoder_grad(True)\n\n    def require_encoder_grad(self, requires_grad):\n        blocks = [self.firstconv,\n                  self.encoder1,\n                  self.encoder2,\n                  self.encoder3,\n                  self.encoder4]\n\n        for block in blocks:\n            for p in block.parameters():\n                p.requires_grad = requires_grad\n\n    # noinspection PyCallingNonCallable\n    def forward(self, x):\n        # Encoder\n        x = self.firstconv(x)\n        x = self.firstbn(x)\n        x = self.firstrelu(x)\n        x = self.firstmaxpool(x)\n        e1 = self.encoder1(x)\n        e2 = self.encoder2(e1)\n        e3 = self.encoder3(e2)\n        e4 = self.encoder4(e3)\n\n        # Decoder with Skip Connections\n        d4 = self.decoder4(e4) + e3\n        d3 = self.decoder3(d4) + e2\n        d2 = self.decoder2(d3) + e1\n        d1 = self.decoder1(d2)\n\n        # Final Classification\n        f1 = self.finaldeconv1(d1)\n        f2 = self.finalrelu1(f1)\n        f3 = self.finalconv2(f2)\n        f4 = self.finalrelu2(f3)\n        f5 = self.finalconv3(f4)\n\n        return f5\n\n\nclass LinkNet50(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 pretrained=True,\n                 num_channels=3,\n                 is_deconv=False,\n                 decoder_kernel_size=4,\n                 **kwargs\n                 ):\n        super().__init__()\n\n        filters = [256, 512, 1024, 2048]\n        resnet = models.resnet50(pretrained=pretrained)\n\n        self.mean = (0.485, 0.456, 0.406)\n        self.std = (0.229, 0.224, 0.225)\n\n        # self.firstconv = resnet.conv1\n        # assert num_channels == 3, ""num channels not used now. to use changle first conv layer to support num channels other then 3""\n        # try to use 8-channels as first input\n        if num_channels == 3:\n            self.firstconv = resnet.conv1\n        else:\n            self.firstconv = nn.Conv2d(num_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n\n        self.firstbn = resnet.bn1\n        self.firstrelu = resnet.relu\n        self.firstmaxpool = resnet.maxpool\n        self.encoder1 = resnet.layer1\n        self.encoder2 = resnet.layer2\n        self.encoder3 = resnet.layer3\n        self.encoder4 = resnet.layer4\n\n        # Decoder\n        self.decoder4 = DecoderBlock(in_channels=filters[3],\n                                     n_filters=filters[2],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n        self.decoder3 = DecoderBlock(in_channels=filters[2],\n                                     n_filters=filters[1],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n        self.decoder2 = DecoderBlock(in_channels=filters[1],\n                                     n_filters=filters[0],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n        self.decoder1 = DecoderBlock(in_channels=filters[0],\n                                     n_filters=filters[0],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n        # Final Classifier\n        self.finaldeconv1 = nn.ConvTranspose2d(filters[0], 32, 3, stride=2)\n        self.finalrelu1 = nonlinearity(inplace=True)\n        self.finalconv2 = nn.Conv2d(32, 32, 3)\n        self.finalrelu2 = nonlinearity(inplace=True)\n        self.finalconv3 = nn.Conv2d(32, num_classes, 2, padding=1)\n\n    def freeze(self):\n        self.require_encoder_grad(False)\n\n    def unfreeze(self):\n        self.require_encoder_grad(True)\n\n    def require_encoder_grad(self, requires_grad):\n        blocks = [self.firstconv,\n                  self.encoder1,\n                  self.encoder2,\n                  self.encoder3,\n                  self.encoder4]\n\n        for block in blocks:\n            for p in block.parameters():\n                p.requires_grad = requires_grad\n\n                # noinspection PyCallingNonCallable\n\n    def forward(self, x):\n        # Encoder\n        x = self.firstconv(x)\n        x = self.firstbn(x)\n        x = self.firstrelu(x)\n        x = self.firstmaxpool(x)\n        e1 = self.encoder1(x)\n        e2 = self.encoder2(e1)\n        e3 = self.encoder3(e2)\n        e4 = self.encoder4(e3)\n\n        # Decoder with Skip Connections\n        d4 = self.decoder4(e4) + e3\n        d3 = self.decoder3(d4) + e2\n        d2 = self.decoder2(d3) + e1\n        d1 = self.decoder1(d2)\n\n        # Final Classification\n        f1 = self.finaldeconv1(d1)\n        f2 = self.finalrelu1(f1)\n        f3 = self.finalconv2(f2)\n        f4 = self.finalrelu2(f3)\n        f5 = self.finalconv3(f4)\n\n        return f5\n\n\nclass LinkNet101(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 pretrained=True,\n                 num_channels=3,\n                 is_deconv=False,\n                 decoder_kernel_size=4,\n                 **kwargs\n                 ):\n        super().__init__()\n\n        filters = [256, 512, 1024, 2048]\n        resnet = models.resnet101(pretrained=pretrained)\n\n        self.mean = (0.485, 0.456, 0.406)\n        self.std = (0.229, 0.224, 0.225)\n\n        # self.firstconv = resnet.conv1\n        # assert num_channels == 3, ""num channels not used now. to use changle first conv layer to support num channels other then 3""\n        # try to use 8-channels as first input\n        if num_channels == 3:\n            self.firstconv = resnet.conv1\n        else:\n            self.firstconv = nn.Conv2d(num_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n\n        self.firstbn = resnet.bn1\n        self.firstrelu = resnet.relu\n        self.firstmaxpool = resnet.maxpool\n        self.encoder1 = resnet.layer1\n        self.encoder2 = resnet.layer2\n        self.encoder3 = resnet.layer3\n        self.encoder4 = resnet.layer4\n\n        # Decoder\n        self.decoder4 = DecoderBlock(in_channels=filters[3],\n                                     n_filters=filters[2],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n        self.decoder3 = DecoderBlock(in_channels=filters[2],\n                                     n_filters=filters[1],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n        self.decoder2 = DecoderBlock(in_channels=filters[1],\n                                     n_filters=filters[0],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n        self.decoder1 = DecoderBlock(in_channels=filters[0],\n                                     n_filters=filters[0],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n        # Final Classifier\n        self.finaldeconv1 = nn.ConvTranspose2d(filters[0], 32, 3, stride=2)\n        self.finalrelu1 = nonlinearity(inplace=True)\n        self.finalconv2 = nn.Conv2d(32, 32, 3)\n        self.finalrelu2 = nonlinearity(inplace=True)\n        self.finalconv3 = nn.Conv2d(32, num_classes, 2, padding=1)\n\n    def freeze(self):\n        self.require_encoder_grad(False)\n\n    def unfreeze(self):\n        self.require_encoder_grad(True)\n\n    def require_encoder_grad(self, requires_grad):\n        blocks = [self.firstconv,\n                  self.encoder1,\n                  self.encoder2,\n                  self.encoder3,\n                  self.encoder4]\n\n        for block in blocks:\n            for p in block.parameters():\n                p.requires_grad = requires_grad\n\n                # noinspection PyCallingNonCallable\n\n    def forward(self, x):\n        # Encoder\n        x = self.firstconv(x)\n        x = self.firstbn(x)\n        x = self.firstrelu(x)\n        x = self.firstmaxpool(x)\n        e1 = self.encoder1(x)\n        e2 = self.encoder2(e1)\n        e3 = self.encoder3(e2)\n        e4 = self.encoder4(e3)\n\n        # Decoder with Skip Connections\n        d4 = self.decoder4(e4) + e3\n        d3 = self.decoder3(d4) + e2\n        d2 = self.decoder2(d3) + e1\n        d1 = self.decoder1(d2)\n\n        # Final Classification\n        f1 = self.finaldeconv1(d1)\n        f2 = self.finalrelu1(f1)\n        f3 = self.finalconv2(f2)\n        f4 = self.finalrelu2(f3)\n        f5 = self.finalconv3(f4)\n\n        return f5\n\n\nclass LinkNeXt(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 pretrained=True,\n                 num_channels=3,\n                 is_deconv=False,\n                 decoder_kernel_size=4,\n                 **kwargs\n                 ):\n        super().__init__()\n\n        filters = [256, 512, 1024, 2048]\n        # only pretrained\n        resnet = resnext101_32x4d(num_classes=1000, pretrained=\'imagenet\')\n\n        self.mean = (0.485, 0.456, 0.406)\n        self.std = (0.229, 0.224, 0.225)\n\n        self.stem = resnet.stem\n        self.encoder1 = resnet.layer1\n        self.encoder2 = resnet.layer2\n        self.encoder3 = resnet.layer3\n        self.encoder4 = resnet.layer4\n\n        # Decoder\n        self.decoder4 = DecoderBlock(in_channels=filters[3],\n                                     n_filters=filters[2],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n        self.decoder3 = DecoderBlock(in_channels=filters[2],\n                                     n_filters=filters[1],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n        self.decoder2 = DecoderBlock(in_channels=filters[1],\n                                     n_filters=filters[0],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n        self.decoder1 = DecoderBlock(in_channels=filters[0],\n                                     n_filters=filters[0],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n\n        # Final Classifier\n        self.finaldeconv1 = nn.ConvTranspose2d(filters[0], 32, 3, stride=2)\n        self.finalrelu1 = nonlinearity(inplace=True)\n        self.finalconv2 = nn.Conv2d(32, 32, 3)\n        self.finalrelu2 = nonlinearity(inplace=True)\n        self.finalconv3 = nn.Conv2d(32, num_classes, 2, padding=1)\n\n    def freeze(self):\n        self.require_encoder_grad(False)\n\n    def unfreeze(self):\n        self.require_encoder_grad(True)\n\n    def require_encoder_grad(self, requires_grad):\n        blocks = [self.stem,\n                  self.encoder1,\n                  self.encoder2,\n                  self.encoder3,\n                  self.encoder4]\n\n        for block in blocks:\n            for p in block.parameters():\n                p.requires_grad = requires_grad\n\n    # noinspection PyCallingNonCallable\n    def forward(self, x):\n        # Encoder\n        x = self.stem(x)\n        e1 = self.encoder1(x)\n        e2 = self.encoder2(e1)\n        e3 = self.encoder3(e2)\n        e4 = self.encoder4(e3)\n\n        # Decoder with Skip Connections\n        d4 = self.decoder4(e4) + e3\n        # d4 = e3\n        d3 = self.decoder3(d4) + e2\n        d2 = self.decoder2(d3) + e1\n        d1 = self.decoder1(d2)\n\n        # Final Classification\n        f1 = self.finaldeconv1(d1)\n        f2 = self.finalrelu1(f1)\n        f3 = self.finalconv2(f2)\n        f4 = self.finalrelu2(f3)\n        f5 = self.finalconv3(f4)\n\n        # return F.sigmoid(f5)\n        return f5\n\n\nclass LinkNet152(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 pretrained=True,\n                 num_channels=3,\n                 is_deconv=False,\n                 decoder_kernel_size=3,\n                 **kwargs\n                 ):\n        super().__init__()\n\n        filters = [256, 512, 1024, 2048]\n        resnet = models.resnet152(pretrained=pretrained)\n\n        self.mean = (0.485, 0.456, 0.406)\n        self.std = (0.229, 0.224, 0.225)\n\n        # self.firstconv = resnet.conv1\n        # assert num_channels == 3, ""num channels not used now. to use changle first conv layer to support num channels other then 3""\n        # try to use 8-channels as first input\n        if num_channels == 3:\n            self.firstconv = resnet.conv1\n        else:\n            self.firstconv = nn.Conv2d(num_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n\n        self.firstbn = resnet.bn1\n        self.firstrelu = resnet.relu\n        self.firstmaxpool = resnet.maxpool\n        self.encoder1 = resnet.layer1\n        self.encoder2 = resnet.layer2\n        self.encoder3 = resnet.layer3\n        self.encoder4 = resnet.layer4\n\n        # Decoder\n        self.decoder4 = DecoderBlock(in_channels=filters[3],\n                                     n_filters=filters[2],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n        self.decoder3 = DecoderBlock(in_channels=filters[2],\n                                     n_filters=filters[1],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n        self.decoder2 = DecoderBlock(in_channels=filters[1],\n                                     n_filters=filters[0],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n        self.decoder1 = DecoderBlock(in_channels=filters[0],\n                                     n_filters=filters[0],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n        # Final Classifier\n        self.finaldeconv1 = nn.ConvTranspose2d(filters[0], 32, 3, stride=2)\n        self.finalrelu1 = nonlinearity(inplace=True)\n        self.finalconv2 = nn.Conv2d(32, 32, 3)\n        self.finalrelu2 = nonlinearity(inplace=True)\n        self.finalconv3 = nn.Conv2d(32, num_classes, 2, padding=1)\n\n    def freeze(self):\n        self.require_encoder_grad(False)\n\n    def unfreeze(self):\n        self.require_encoder_grad(True)\n\n    def require_encoder_grad(self, requires_grad):\n        blocks = [self.firstconv,\n                  self.encoder1,\n                  self.encoder2,\n                  self.encoder3,\n                  self.encoder4]\n\n        for block in blocks:\n            for p in block.parameters():\n                p.requires_grad = requires_grad\n\n                # noinspection PyCallingNonCallable\n\n    def forward(self, x):\n        # Encoder\n        x = self.firstconv(x)\n        x = self.firstbn(x)\n        x = self.firstrelu(x)\n        x = self.firstmaxpool(x)\n        e1 = self.encoder1(x)\n        e2 = self.encoder2(e1)\n        e3 = self.encoder3(e2)\n        e4 = self.encoder4(e3)\n\n        # Decoder with Skip Connections\n        d4 = self.decoder4(e4) + e3\n        d3 = self.decoder3(d4) + e2\n        d2 = self.decoder2(d3) + e1\n        d1 = self.decoder1(d2)\n\n        # Final Classification\n        f1 = self.finaldeconv1(d1)\n        f2 = self.finalrelu1(f1)\n        f3 = self.finalconv2(f2)\n        f4 = self.finalrelu2(f3)\n        f5 = self.finalconv3(f4)\n\n        return f5\n\n\nclass LinkCeption(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 pretrained=True,\n                 num_channels=3,\n                 is_deconv=False,\n                 decoder_kernel_size=4,\n                 **kwargs\n                 ):\n        super().__init__()\n\n        self.mean = (0.5, 0.5, 0.5)\n        self.std = (0.5, 0.5, 0.5)\n\n        filters = [64, 384, 384, 1024, 1536]\n\n        # only pre-trained\n        inception = inceptionv4(pretrained=\'imagenet\')\n\n        if num_channels == 3:\n            self.stem1 = nn.Sequential(\n                inception.features[0],\n                inception.features[1],\n                inception.features[2],\n            )\n        else:\n            self.stem1 = nn.Sequential(\n                BasicConv2d(num_channels, 32, kernel_size=3, stride=2),\n                inception.features[1],\n                inception.features[2],\n            )\n\n        self.stem2 = nn.Sequential(\n            inception.features[3],\n            inception.features[4],\n            inception.features[5],\n        )\n\n        self.block1 = nn.Sequential(\n            inception.features[6],\n            inception.features[7],\n            inception.features[8],\n            inception.features[9],\n        )\n\n        self.tr1 = inception.features[10]\n\n        self.block2 = nn.Sequential(\n            inception.features[11],\n            inception.features[12],\n            inception.features[13],\n            inception.features[14],\n            inception.features[15],\n            inception.features[16],\n            inception.features[17],\n        )\n\n        self.tr2 = inception.features[18]\n\n        self.block3 = nn.Sequential(\n            inception.features[19],\n            inception.features[20],\n            inception.features[21]\n        )\n\n        # Decoder\n        self.decoder4 = DecoderBlockInception(in_channels=filters[4],\n                                              out_channels=filters[3],\n                                              n_filters=filters[3],\n                                              last_padding=0,\n                                              kernel_size=decoder_kernel_size,\n                                              is_deconv=is_deconv)\n        self.decoder3 = DecoderBlockInception(in_channels=filters[3],\n                                              out_channels=filters[2],\n                                              n_filters=filters[2],\n                                              last_padding=0,\n                                              kernel_size=decoder_kernel_size,\n                                              is_deconv=is_deconv)\n        self.decoder2 = DecoderBlockInception(in_channels=filters[2],\n                                              out_channels=filters[1],\n                                              n_filters=filters[1],\n                                              last_padding=0,\n                                              kernel_size=decoder_kernel_size,\n                                              is_deconv=is_deconv)\n        self.decoder1 = DecoderBlockInception(in_channels=filters[1],\n                                              out_channels=filters[0],\n                                              n_filters=filters[0],\n                                              last_padding=0,\n                                              kernel_size=decoder_kernel_size,\n                                              is_deconv=is_deconv)\n\n        # Final Classifier\n        self.finaldeconv1 = nn.ConvTranspose2d(filters[0], 32, 1, stride=2)\n        self.finalnorm1 = nn.BatchNorm2d(32)\n        self.finalrelu1 = nonlinearity(inplace=True)\n        self.finalconv2 = nn.Conv2d(32, 32, 3)\n        self.finalnorm2 = nn.BatchNorm2d(32)\n        self.finalrelu2 = nonlinearity(inplace=True)\n        self.finalconv3 = nn.Conv2d(32, num_classes, 2, padding=0)\n\n    def freeze(self):\n        self.require_encoder_grad(False)\n\n    def unfreeze(self):\n        self.require_encoder_grad(True)\n\n    def require_encoder_grad(self, requires_grad):\n        blocks = [self.stem1,\n                  self.stem2,\n                  self.block1,\n                  self.tr1,\n                  self.block2,\n                  self.tr2,\n                  self.block3]\n\n        for block in blocks:\n            for p in block.parameters():\n                p.requires_grad = requires_grad\n\n                # noinspection PyCallingNonCallable\n\n    def forward(self, x):\n        final_shape = x.shape[2:]\n\n        # Encoder\n        x = self.stem1(x)\n        e1 = self.stem2(x)\n        e2 = self.block1(e1)\n        e3 = self.tr1(e2)\n        e3 = self.block2(e3)\n        e4 = self.tr2(e3)\n        e4 = self.block3(e4)\n\n        # Decoder with Skip Connections\n        d4 = self.decoder4(e4)[:, :, 0:e3.size(2), 0:e3.size(3)] + e3\n        d3 = self.decoder3(d4)[:, :, 0:e2.size(2), 0:e2.size(3)] + e2\n        d2 = self.decoder2(d3)[:, :, 0:self.decoder2(e1).size(2), 0:self.decoder2(e1).size(3)] + self.decoder2(e1)\n        d1 = self.decoder1(d2)\n\n        # Final Classification\n        f1 = self.finaldeconv1(d1)\n        f1 = self.finalnorm1(f1)\n        f2 = self.finalrelu1(f1)\n        f2 = self.finalnorm2(f2)\n        f3 = self.finalconv2(f2)\n        f4 = self.finalrelu2(f3)\n        f5 = self.finalconv3(f4)\n\n        out = F.interpolate(f5, size=final_shape, mode=""bilinear"")\n\n        return out\n\n\nclass LinkInceptionResNet(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 pretrained=True,\n                 num_channels=3,\n                 is_deconv=False,\n                 decoder_kernel_size=3,\n                 **kwargs\n                 ):\n        super().__init__()\n\n        self.mean = (0.485, 0.456, 0.406)\n        self.std = (0.229, 0.224, 0.225)\n\n        filters = [64, 192, 320, 1088, 2080]\n\n        # only pre-trained\n        ir = inceptionresnetv2(pretrained=\'imagenet\', num_classes=1000)\n\n        if num_channels == 3:\n            self.stem1 = nn.Sequential(\n                ir.conv2d_1a,\n                ir.conv2d_2a,\n                ir.conv2d_2b,\n            )\n        else:\n            self.stem1 = nn.Sequential(\n                BasicConv2d(num_channels, 32, kernel_size=3, stride=2),\n                ir.conv2d_2a,\n                ir.conv2d_2b,\n            )\n\n        self.maxpool_3a = ir.maxpool_3a\n\n        self.stem2 = nn.Sequential(\n            ir.conv2d_3b,\n            ir.conv2d_4a,\n        )\n\n        self.maxpool_5a = ir.maxpool_5a\n        self.mixed_5b = ir.mixed_5b\n\n        self.mixed_6a = ir.mixed_6a\n        self.mixed_7a = ir.mixed_7a\n        self.skip1 = ir.repeat\n        self.skip2 = ir.repeat_1\n        self.skip3 = ir.repeat_2\n\n        # Decoder\n        self.decoder3 = DecoderBlockInception(in_channels=filters[4],\n                                              out_channels=filters[3],\n                                              n_filters=filters[3],\n                                              last_padding=0,\n                                              kernel_size=decoder_kernel_size,\n                                              is_deconv=is_deconv)\n        self.decoder2 = DecoderBlockInception(in_channels=filters[3],\n                                              out_channels=filters[2],\n                                              n_filters=filters[2],\n                                              last_padding=0,\n                                              kernel_size=decoder_kernel_size,\n                                              is_deconv=is_deconv)\n        self.decoder1 = DecoderBlockInception(in_channels=filters[2],\n                                              out_channels=filters[1],\n                                              n_filters=filters[1],\n                                              last_padding=0,\n                                              kernel_size=decoder_kernel_size,\n                                              is_deconv=is_deconv)\n        self.decoder0 = DecoderBlockInception(in_channels=filters[1],\n                                              out_channels=filters[0],\n                                              n_filters=filters[0],\n                                              last_padding=2,\n                                              kernel_size=decoder_kernel_size,\n                                              is_deconv=is_deconv)\n\n        # Final Classifier\n        self.finaldeconv1 = nn.ConvTranspose2d(filters[0], 32, 3, stride=2)\n        self.finalnorm1 = nn.BatchNorm2d(32)\n        self.finalrelu1 = nonlinearity(inplace=True)\n        self.finalconv2 = nn.Conv2d(32, 32, 3)\n        self.finalnorm2 = nn.BatchNorm2d(32)\n        self.finalrelu2 = nonlinearity(inplace=True)\n        self.finalconv3 = nn.Conv2d(32, num_classes, 2, padding=1)\n\n    def freeze(self):\n        self.require_encoder_grad(False)\n\n    def unfreeze(self):\n        self.require_encoder_grad(True)\n\n    def require_encoder_grad(self, requires_grad):\n        blocks = [self.stem1,\n                  self.stem2,\n                  self.mixed_5b,\n                  self.mixed_6a,\n                  self.mixed_7a,\n                  self.skip1,\n                  self.skip2,\n                  self.skip3]\n\n        for block in blocks:\n            for p in block.parameters():\n                p.requires_grad = requires_grad\n\n    # noinspection PyCallingNonCallable\n    def forward(self, x):\n\n        # Encoder\n        x = self.stem1(x)\n        x1 = self.maxpool_3a(x)\n        x1 = self.stem2(x1)\n        x2 = self.maxpool_3a(x1)\n        x2 = self.mixed_5b(x2)\n\n        e1 = self.skip1(x2)\n        e1_resume = self.mixed_6a(e1)\n        e2 = self.skip2(e1_resume)\n        e2_resume = self.mixed_7a(e2)\n        e3 = self.skip3(e2_resume)\n\n        # Decoder with Skip Connections\n        d3 = self.decoder3(e3)[:, :, 0:e2.size(2), 0:e2.size(3)] + e2\n        d2 = self.decoder2(d3)[:, :, 0:e1.size(2), 0:e1.size(3)] + e1\n        d1 = self.decoder1(d2)[:, :, 0:x1.size(2), 0:x1.size(3)] + x1\n        d0 = self.decoder0(d1)\n\n        # Final Classification\n        f1 = self.finaldeconv1(d0)\n        f2 = self.finalrelu1(f1)\n        f3 = self.finalconv2(f2)\n        f4 = self.finalrelu2(f3)\n        f5 = self.finalconv3(f4)\n\n        return f5\n\n\nclass LinkDenseNet161(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 pretrained=True,\n                 num_channels=3,\n                 is_deconv=False,\n                 decoder_kernel_size=4,\n                 **kwargs\n                 ):\n        super().__init__()\n\n        filters = [384, 768, 2112, 2208]\n        densenet = models.densenet161(pretrained=pretrained)\n\n        self.mean = (0.485, 0.456, 0.406)\n        self.std = (0.229, 0.224, 0.225)\n\n        if num_channels == 3:\n            self.firstconv = densenet.features.conv0\n        else:\n            self.firstconv = nn.Conv2d(num_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n\n        self.stem = nn.Sequential(\n            self.firstconv,\n            densenet.features.norm0,\n            densenet.features.relu0,\n            densenet.features.pool0,\n        )\n\n        self.encoder1 = nn.Sequential(densenet.features.denseblock1)\n        self.encoder2 = nn.Sequential(densenet.features.transition1,\n                                      densenet.features.denseblock2)\n        self.encoder3 = nn.Sequential(densenet.features.transition2,\n                                      densenet.features.denseblock3)\n        self.encoder4 = nn.Sequential(densenet.features.transition3,\n                                      densenet.features.denseblock4)\n\n        # Decoder\n        self.decoder4 = DecoderBlock(in_channels=filters[3],\n                                     n_filters=filters[2],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n        self.decoder3 = DecoderBlock(in_channels=filters[2],\n                                     n_filters=filters[1],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n        self.decoder2 = DecoderBlock(in_channels=filters[1],\n                                     n_filters=filters[0],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n        self.decoder1 = DecoderBlock(in_channels=filters[0],\n                                     n_filters=filters[0],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n\n        # Final Classifier\n        self.finaldeconv1 = nn.ConvTranspose2d(filters[0], 32, 3, stride=2)\n        self.finalrelu1 = nonlinearity(inplace=True)\n        self.finalconv2 = nn.Conv2d(32, 32, 3)\n        self.finalrelu2 = nonlinearity(inplace=True)\n        self.finalconv3 = nn.Conv2d(32, num_classes, 2, padding=1)\n\n    def require_encoder_grad(self, requires_grad):\n        blocks = [self.stem,\n                  self.encoder1,\n                  self.encoder2,\n                  self.encoder3,\n                  self.encoder4]\n\n        for block in blocks:\n            for p in block.parameters():\n                p.requires_grad = requires_grad\n\n    def freeze(self):\n        self.require_encoder_grad(False)\n\n    def unfreeze(self):\n        self.require_encoder_grad(True)\n\n        # noinspection PyCallingNonCallable\n\n    def forward(self, x):\n        # Encoder\n        x = self.stem(x)\n        e1 = self.encoder1(x)\n        e2 = self.encoder2(e1)\n        e3 = self.encoder3(e2)\n        e4 = self.encoder4(e3)\n\n        # Decoder with Skip Connections\n        d4 = self.decoder4(e4) + e3\n        d3 = self.decoder3(d4) + e2\n        d2 = self.decoder2(d3) + e1\n        d1 = self.decoder1(d2)\n\n        # Final Classification\n        f1 = self.finaldeconv1(d1)\n        f2 = self.finalrelu1(f1)\n        f3 = self.finalconv2(f2)\n        f4 = self.finalrelu2(f3)\n        f5 = self.finalconv3(f4)\n\n        return f5\n\n\nclass LinkDenseNet121(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 pretrained=True,\n                 num_channels=3,\n                 is_deconv=False,\n                 decoder_kernel_size=4,\n                 **kwargs\n                 ):\n        super().__init__()\n\n        filters = [256, 512, 1024, 1024]\n        densenet = models.densenet121(pretrained=pretrained)\n\n        self.mean = (0.485, 0.456, 0.406)\n        self.std = (0.229, 0.224, 0.225)\n\n        if num_channels == 3:\n            self.firstconv = densenet.features.conv0\n        else:\n            self.firstconv = nn.Conv2d(num_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n\n        self.stem = nn.Sequential(\n            self.firstconv,\n            densenet.features.norm0,\n            densenet.features.relu0,\n            densenet.features.pool0,\n        )\n\n        self.encoder1 = nn.Sequential(densenet.features.denseblock1)\n        self.encoder2 = nn.Sequential(densenet.features.transition1,\n                                      densenet.features.denseblock2)\n        self.encoder3 = nn.Sequential(densenet.features.transition2,\n                                      densenet.features.denseblock3)\n        self.encoder4 = nn.Sequential(densenet.features.transition3,\n                                      densenet.features.denseblock4)\n\n        # Decoder\n        self.decoder4 = DecoderBlock(in_channels=filters[3],\n                                     n_filters=filters[2],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n        self.decoder3 = DecoderBlock(in_channels=filters[2],\n                                     n_filters=filters[1],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n        self.decoder2 = DecoderBlock(in_channels=filters[1],\n                                     n_filters=filters[0],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n        self.decoder1 = DecoderBlock(in_channels=filters[0],\n                                     n_filters=filters[0],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n\n        # Final Classifier\n        self.finaldeconv1 = nn.ConvTranspose2d(filters[0], 32, 3, stride=2)\n        self.finalrelu1 = nonlinearity(inplace=True)\n        self.finalconv2 = nn.Conv2d(32, 32, 3)\n        self.finalrelu2 = nonlinearity(inplace=True)\n        self.finalconv3 = nn.Conv2d(32, num_classes, 2, padding=1)\n\n    def require_encoder_grad(self, requires_grad):\n        blocks = [self.stem,\n                  self.encoder1,\n                  self.encoder2,\n                  self.encoder3,\n                  self.encoder4]\n\n        for block in blocks:\n            for p in block.parameters():\n                p.requires_grad = requires_grad\n\n    def freeze(self):\n        self.require_encoder_grad(False)\n\n    def unfreeze(self):\n        self.require_encoder_grad(True)\n\n        # noinspection PyCallingNonCallable\n\n    def forward(self, x):\n        # Encoder\n        x = self.stem(x)\n        e1 = self.encoder1(x)\n        e2 = self.encoder2(e1)\n        e3 = self.encoder3(e2)\n        e4 = self.encoder4(e3)\n\n        # Decoder with Skip Connections\n        d4 = self.decoder4(e4) + e3\n        d3 = self.decoder3(d4) + e2\n        d2 = self.decoder2(d3) + e1\n        d1 = self.decoder1(d2)\n\n        # Final Classification\n        f1 = self.finaldeconv1(d1)\n        f2 = self.finalrelu1(f1)\n        f3 = self.finalconv2(f2)\n        f4 = self.finalrelu2(f3)\n        f5 = self.finalconv3(f4)\n\n        return f5\n\n\nclass CoarseLinkNet50(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 pretrained=True,\n                 num_channels=3,\n                 is_deconv=False,\n                 decoder_kernel_size=4,\n                 **kwargs\n                 ):\n        super().__init__()\n\n        filters = [256, 512, 1024, 2048]\n        resnet = models.resnet50(pretrained=pretrained)\n\n        self.mean = (0.485, 0.456, 0.406)\n        self.std = (0.229, 0.224, 0.225)\n\n        # self.firstconv = resnet.conv1\n        # assert num_channels == 3, ""num channels not used now. to use changle first conv layer to support num channels other then 3""\n        # try to use 8-channels as first input\n        if num_channels == 3:\n            self.firstconv = resnet.conv1\n        else:\n            self.firstconv = nn.Conv2d(num_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n\n        self.firstbn = resnet.bn1\n        self.firstrelu = resnet.relu\n        self.firstmaxpool = resnet.maxpool\n        self.encoder1 = resnet.layer1\n        self.encoder2 = resnet.layer2\n        self.encoder3 = resnet.layer3\n        self.encoder4 = resnet.layer4\n\n        # Decoder\n        self.decoder4 = DecoderBlock(in_channels=filters[3],\n                                     n_filters=filters[2],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n        self.decoder3 = DecoderBlock(in_channels=filters[2],\n                                     n_filters=filters[1],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n        self.decoder2 = DecoderBlock(in_channels=filters[1],\n                                     n_filters=filters[0],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n        self.decoder1 = DecoderBlock(in_channels=filters[0],\n                                     n_filters=filters[0],\n                                     kernel_size=decoder_kernel_size,\n                                     is_deconv=is_deconv)\n        # Final Classifier\n        self.finalconv1 = nn.Conv2d(filters[0], 32, 2, padding=1)\n        self.finalrelu1 = nonlinearity(inplace=True)\n        self.finalconv2 = nn.Conv2d(32, num_classes, 2, padding=1)\n\n    def freeze(self):\n        self.require_encoder_grad(False)\n\n    def unfreeze(self):\n        self.require_encoder_grad(True)\n\n    def require_encoder_grad(self, requires_grad):\n        blocks = [self.firstconv,\n                  self.encoder1,\n                  self.encoder2,\n                  self.encoder3,\n                  self.encoder4]\n\n        for block in blocks:\n            for p in block.parameters():\n                p.requires_grad = requires_grad\n\n                # noinspection PyCallingNonCallable\n\n    def forward(self, x):\n        # Encoder\n        x = self.firstconv(x)\n        x = self.firstbn(x)\n        x = self.firstrelu(x)\n        x = self.firstmaxpool(x)\n        e1 = self.encoder1(x)\n        e2 = self.encoder2(e1)\n        e3 = self.encoder3(e2)\n        e4 = self.encoder4(e3)\n\n        # Decoder with Skip Connections\n        d4 = self.decoder4(e4) + e3\n        d3 = self.decoder3(d4) + e2\n        d2 = self.decoder2(d3) + e1\n        d1 = self.decoder1(d2)\n\n        # Final Classification\n        f1 = self.finalconv1(d1)\n        f2 = self.finalrelu1(f1)\n        f3 = self.finalconv2(f2)\n\n        return f3'"
pywick/models/segmentation/mnas_linknets/resnext.py,2,"b'import torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\nfrom .resnext101_32x4d_features import resnext101_32x4d_features,resnext101_32x4d_features_blob\n\n__all__ = [\'ResNeXt101_32x4d\', \'resnext101_32x4d\']\n\npretrained_settings = {\n    \'resnext101_32x4d\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/resnext101_32x4d-29e315fa.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'resnext101_64x4d\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/resnext101_64x4d-e77a0586.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    }\n}\n\nclass ResNeXt101_32x4d_blob(nn.Module):\n\n    def __init__(self, num_classes=1000):\n        super(ResNeXt101_32x4d_blob, self).__init__()\n        self.num_classes = num_classes\n        \n        resnext = resnext101_32x4d_features_blob()\n        \n        self.features = resnext.resnext101_32x4d_features\n        self.avg_pool = nn.AvgPool2d((7, 7), (1, 1))\n        self.last_linear = nn.Linear(2048, num_classes)\n\n    def logits(self, input):\n        x = self.avg_pool(input)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\nclass ResNeXt101_32x4d(nn.Module):\n\n    def __init__(self, num_classes=1000):\n        super(ResNeXt101_32x4d, self).__init__()\n        self.num_classes = num_classes\n        \n        resnext = resnext101_32x4d_features()\n        \n        self.stem = resnext.resnext101_32x4d_stem\n        self.layer1 = resnext.resnext101_32x4d_layer1\n        self.layer2 = resnext.resnext101_32x4d_layer2\n        self.layer3 = resnext.resnext101_32x4d_layer3\n        self.layer4 = resnext.resnext101_32x4d_layer4\n        \n        self.avg_pool = nn.AvgPool2d((7, 7), (1, 1))\n        self.last_linear = nn.Linear(2048, num_classes)\n\n    def logits(self, input):\n        x = self.avg_pool(input)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.stem(input)\n        x = self.layer1(x)\n        x = self.layer2(x)       \n        x = self.layer3(x)       \n        x = self.layer4(x)      \n        x = self.logits(x)\n        return x\n\ndef resnext101_32x4d(num_classes=1000, pretrained=\'imagenet\'):\n    model = ResNeXt101_32x4d(num_classes=num_classes)\n    model_blob = ResNeXt101_32x4d_blob(num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'resnext101_32x4d\'][pretrained]\n        assert num_classes == settings[\'num_classes\'], \\\n            ""num_classes should be {}, but is {}"".format(settings[\'num_classes\'], num_classes)\n        model_blob.load_state_dict(model_zoo.load_url(settings[\'url\']))\n        \n        model.stem = nn.Sequential( \n            model_blob.features[0],\n            model_blob.features[1],\n            model_blob.features[2],\n            model_blob.features[3],\n        )\n        \n        model.layer1 = nn.Sequential( \n            model_blob.features[4],\n        )      \n        model.layer2 = nn.Sequential( \n            model_blob.features[5],\n        ) \n        model.layer3 = nn.Sequential( \n            model_blob.features[6],\n        ) \n        model.layer4 = nn.Sequential( \n            model_blob.features[7],\n        )         \n        # finish here\n         \n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n    return model\n\n# def resnext101_64x4d(num_classes=1000, pretrained=\'imagenet\'):\n#     model = ResNeXt101_64x4d(num_classes=num_classes)\n#     if pretrained is not None:\n#         settings = pretrained_settings[\'resnext101_64x4d\'][pretrained]\n#         assert num_classes == settings[\'num_classes\'], \\\n#             ""num_classes should be {}, but is {}"".format(settings[\'num_classes\'], num_classes)\n#         model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n#         model.input_space = settings[\'input_space\']\n#         model.input_size = settings[\'input_size\']\n#         model.input_range = settings[\'input_range\']\n#         model.mean = settings[\'mean\']\n#         model.std = settings[\'std\']\n#     return model'"
pywick/models/segmentation/mnas_linknets/resnext101_32x4d_features.py,2,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom functools import reduce\n\nclass LambdaBase(nn.Sequential):\n    def __init__(self, fn, *args):\n        super(LambdaBase, self).__init__(*args)\n        self.lambda_func = fn\n\n    def forward_prepare(self, input):\n        output = []\n        for module in self._modules.values():\n            output.append(module(input))\n        return output if output else input\n\nclass Lambda(LambdaBase):\n    def forward(self, input):\n        return self.lambda_func(self.forward_prepare(input))\n\nclass LambdaMap(LambdaBase):\n    def forward(self, input):\n        return list(map(self.lambda_func,self.forward_prepare(input)))\n\nclass LambdaReduce(LambdaBase):\n    def forward(self, input):\n        return reduce(self.lambda_func,self.forward_prepare(input))\n\nclass resnext101_32x4d_features_blob(nn.Module):\n\n    def __init__(self):\n        super(resnext101_32x4d_features_blob, self).__init__()\n        \n        self.resnext101_32x4d_features = nn.Sequential( # Sequential,\n            nn.Conv2d(3,64,(7, 7),(2, 2),(3, 3),1,1,bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d((3, 3),(2, 2),(1, 1)),\n            nn.Sequential( # Sequential,\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(64,128,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(128),\n                                nn.ReLU(),\n                                nn.Conv2d(128,128,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(128),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(128,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(256),\n                        ),\n                        nn.Sequential( # Sequential,\n                            nn.Conv2d(64,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(256),\n                        ),\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(256,128,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(128),\n                                nn.ReLU(),\n                                nn.Conv2d(128,128,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(128),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(128,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(256),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(256,128,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(128),\n                                nn.ReLU(),\n                                nn.Conv2d(128,128,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(128),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(128,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(256),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n            ),\n            nn.Sequential( # Sequential,\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(256,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(256),\n                                nn.ReLU(),\n                                nn.Conv2d(256,256,(3, 3),(2, 2),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(256),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(256,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(512),\n                        ),\n                        nn.Sequential( # Sequential,\n                            nn.Conv2d(256,512,(1, 1),(2, 2),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(512),\n                        ),\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(512,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(256),\n                                nn.ReLU(),\n                                nn.Conv2d(256,256,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(256),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(256,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(512),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(512,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(256),\n                                nn.ReLU(),\n                                nn.Conv2d(256,256,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(256),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(256,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(512),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(512,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(256),\n                                nn.ReLU(),\n                                nn.Conv2d(256,256,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(256),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(256,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(512),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n            ),\n            nn.Sequential( # Sequential,\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(512,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(2, 2),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        nn.Sequential( # Sequential,\n                            nn.Conv2d(512,1024,(1, 1),(2, 2),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n            ),\n            nn.Sequential( # Sequential,\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(1024),\n                                nn.ReLU(),\n                                nn.Conv2d(1024,1024,(3, 3),(2, 2),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(1024),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(1024,2048,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(2048),\n                        ),\n                        nn.Sequential( # Sequential,\n                            nn.Conv2d(1024,2048,(1, 1),(2, 2),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(2048),\n                        ),\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(2048,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(1024),\n                                nn.ReLU(),\n                                nn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(1024),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(1024,2048,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(2048),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(2048,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(1024),\n                                nn.ReLU(),\n                                nn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(1024),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(1024,2048,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(2048),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n            )\n        )\n        \n    def forward(self, input):\n        x = self.resnext101_32x4d_features(input)\n        return x        \n    \nclass resnext101_32x4d_features(nn.Module):\n\n    def __init__(self):\n        super(resnext101_32x4d_features, self).__init__()\n        \n        self.resnext101_32x4d_stem = nn.Sequential( # Sequential,\n            nn.Conv2d(3,64,(7, 7),(2, 2),(3, 3),1,1,bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d((3, 3),(2, 2),(1, 1))\n        )\n            \n        self.resnext101_32x4d_layer1 = nn.Sequential( # Sequential,\n            nn.Sequential( # Sequential,\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(64,128,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(128),\n                                nn.ReLU(),\n                                nn.Conv2d(128,128,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(128),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(128,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(256),\n                        ),\n                        nn.Sequential( # Sequential,\n                            nn.Conv2d(64,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(256),\n                        ),\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(256,128,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(128),\n                                nn.ReLU(),\n                                nn.Conv2d(128,128,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(128),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(128,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(256),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(256,128,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(128),\n                                nn.ReLU(),\n                                nn.Conv2d(128,128,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(128),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(128,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(256),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n            )      \n        )\n        \n        self.resnext101_32x4d_layer2 = nn.Sequential( # Sequential,\n            nn.Sequential( # Sequential,\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(256,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(256),\n                                nn.ReLU(),\n                                nn.Conv2d(256,256,(3, 3),(2, 2),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(256),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(256,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(512),\n                        ),\n                        nn.Sequential( # Sequential,\n                            nn.Conv2d(256,512,(1, 1),(2, 2),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(512),\n                        ),\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(512,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(256),\n                                nn.ReLU(),\n                                nn.Conv2d(256,256,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(256),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(256,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(512),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(512,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(256),\n                                nn.ReLU(),\n                                nn.Conv2d(256,256,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(256),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(256,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(512),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(512,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(256),\n                                nn.ReLU(),\n                                nn.Conv2d(256,256,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(256),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(256,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(512),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n            )            \n        )\n        \n        self.resnext101_32x4d_layer3 = nn.Sequential( # Sequential,\n            nn.Sequential( # Sequential,\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(512,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(2, 2),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        nn.Sequential( # Sequential,\n                            nn.Conv2d(512,1024,(1, 1),(2, 2),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                                nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(512),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(1024),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n            )            \n        )\n        \n        self.resnext101_32x4d_layer4 = nn.Sequential( # Sequential,\n            nn.Sequential( # Sequential,\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(1024),\n                                nn.ReLU(),\n                                nn.Conv2d(1024,1024,(3, 3),(2, 2),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(1024),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(1024,2048,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(2048),\n                        ),\n                        nn.Sequential( # Sequential,\n                            nn.Conv2d(1024,2048,(1, 1),(2, 2),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(2048),\n                        ),\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(2048,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(1024),\n                                nn.ReLU(),\n                                nn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(1024),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(1024,2048,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(2048),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n                nn.Sequential( # Sequential,\n                    LambdaMap(lambda x: x, # ConcatTable,\n                        nn.Sequential( # Sequential,\n                            nn.Sequential( # Sequential,\n                                nn.Conv2d(2048,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                                nn.BatchNorm2d(1024),\n                                nn.ReLU(),\n                                nn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                                nn.BatchNorm2d(1024),\n                                nn.ReLU(),\n                            ),\n                            nn.Conv2d(1024,2048,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                            nn.BatchNorm2d(2048),\n                        ),\n                        Lambda(lambda x: x), # Identity,\n                    ),\n                    LambdaReduce(lambda x,y: x+y), # CAddTable,\n                    nn.ReLU(),\n                ),\n            )            \n        )        \n\n        \n    def forward(self, input):\n        x = self.resnext101_32x4d_stem(input)\n        x = self.resnext101_32x4d_layer1(x)\n        x = self.resnext101_32x4d_layer2(x)\n        x = self.resnext101_32x4d_layer3(x)\n        x = self.resnext101_32x4d_layer4(x)\n        return x   '"
pywick/models/segmentation/refinenet/__init__.py,0,b'from .refinenet import *'
pywick/models/segmentation/refinenet/blocks.py,1,"b'import torch.nn as nn\n\n\nclass ResidualConvUnit(nn.Module):\n    def __init__(self, features):\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(\n            features, features, kernel_size=3, stride=1, padding=1, bias=True)\n        self.conv2 = nn.Conv2d(\n            features, features, kernel_size=3, stride=1, padding=1, bias=False)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n\n        out = self.relu(x)\n        out = self.conv1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n\n        return out + x\n\n\nclass MultiResolutionFusion(nn.Module):\n    def __init__(self, out_feats, *shapes):\n        super().__init__()\n\n        _, max_size = max(shapes, key=lambda x: x[1])\n\n        self.scale_factors = []\n        for i, shape in enumerate(shapes):\n            feat, size = shape\n            if max_size % size != 0:\n                raise ValueError(""max_size not divisble by shape {}"".format(i))\n\n            self.scale_factors.append(max_size // size)\n            self.add_module(\n                ""resolve{}"".format(i),\n                nn.Conv2d(\n                    feat,\n                    out_feats,\n                    kernel_size=3,\n                    stride=1,\n                    padding=1,\n                    bias=False))\n\n    def forward(self, *xs):\n\n        output = self.resolve0(xs[0])\n        if self.scale_factors[0] != 1:\n            output = nn.functional.interpolate(\n                output,\n                scale_factor=self.scale_factors[0],\n                mode=\'bilinear\',\n                align_corners=True)\n\n        for i, x in enumerate(xs[1:], 1):\n            output += self.__getattr__(""resolve{}"".format(i))(x)\n            if self.scale_factors[i] != 1:\n                output = nn.functional.interpolate(\n                    output,\n                    scale_factor=self.scale_factors[i],\n                    mode=\'bilinear\',\n                    align_corners=True)\n\n        return output\n\n\nclass ChainedResidualPool(nn.Module):\n    def __init__(self, feats):\n        super().__init__()\n\n        self.relu = nn.ReLU(inplace=True)\n        for i in range(1, 4):\n            self.add_module(\n                ""block{}"".format(i),\n                nn.Sequential(\n                    nn.MaxPool2d(kernel_size=5, stride=1, padding=2),\n                    nn.Conv2d(\n                        feats,\n                        feats,\n                        kernel_size=3,\n                        stride=1,\n                        padding=1,\n                        bias=False)))\n\n    def forward(self, x):\n        x = self.relu(x)\n        path = x\n\n        for i in range(1, 4):\n            path = self.__getattr__(""block{}"".format(i))(path)\n            x = x + path\n\n        return x\n\n\nclass ChainedResidualPoolImproved(nn.Module):\n    def __init__(self, feats):\n        super().__init__()\n\n        self.relu = nn.ReLU(inplace=True)\n        for i in range(1, 5):\n            self.add_module(\n                ""block{}"".format(i),\n                nn.Sequential(\n                    nn.Conv2d(\n                        feats,\n                        feats,\n                        kernel_size=3,\n                        stride=1,\n                        padding=1,\n                        bias=False),\n                    nn.MaxPool2d(kernel_size=5, stride=1, padding=2)))\n\n    def forward(self, x):\n        x = self.relu(x)\n        path = x\n\n        for i in range(1, 5):\n            path = self.__getattr__(""block{}"".format(i))(path)\n            x += path\n\n        return x\n\n\nclass BaseRefineNetBlock(nn.Module):\n    def __init__(self, features, residual_conv_unit, multi_resolution_fusion,\n                 chained_residual_pool, *shapes):\n        super().__init__()\n\n        for i, shape in enumerate(shapes):\n            feats = shape[0]\n            self.add_module(\n                ""rcu{}"".format(i),\n                nn.Sequential(\n                    residual_conv_unit(feats), residual_conv_unit(feats)))\n\n        if len(shapes) != 1:\n            self.mrf = multi_resolution_fusion(features, *shapes)\n        else:\n            self.mrf = None\n\n        self.crp = chained_residual_pool(features)\n        self.output_conv = residual_conv_unit(features)\n\n    def forward(self, *xs):\n        rcu_xs = []\n\n        for i, x in enumerate(xs):\n            rcu_xs.append(self.__getattr__(""rcu{}"".format(i))(x))\n\n        if self.mrf is not None:\n            out = self.mrf(*rcu_xs)\n        else:\n            out = rcu_xs[0]\n\n        out = self.crp(out)\n        return self.output_conv(out)\n\n\nclass RefineNetBlock(BaseRefineNetBlock):\n    def __init__(self, features, *shapes):\n        super().__init__(features, ResidualConvUnit, MultiResolutionFusion,\n                         ChainedResidualPool, *shapes)\n\n\nclass RefineNetBlockImprovedPooling(nn.Module):\n    def __init__(self, features, *shapes):\n        super().__init__(features, ResidualConvUnit, MultiResolutionFusion,\n                         ChainedResidualPoolImproved, *shapes)\n'"
pywick/models/segmentation/refinenet/refinenet.py,2,"b'# Source: https://github.com/thomasjpfan/pytorch_refinenet (License: MIT)\n\n""""""\nImplementation of `RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation <https://arxiv.org/abs/1611.06612>`_.\n""""""\n\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.nn.functional as F\n\nfrom .blocks import (RefineNetBlock, ResidualConvUnit,\n                      RefineNetBlockImprovedPooling)\n\n__all__ = [\'RefineNet4Cascade\', \'RefineNet4CascadePoolingImproved\']\n\nclass BaseRefineNet4Cascade(nn.Module):\n    def __init__(self,\n                 input_shape,\n                 refinenet_block,\n                 num_classes=1,\n                 features=256,\n                 resnet_factory=models.resnet101,\n                 pretrained=True,\n                 freeze_resnet=False,\n                 **kwargs):\n        """"""Multi-path 4-Cascaded RefineNet for image segmentation\n\n        Args:\n            input_shape ((int, int)): (channel, size) assumes input has\n                equal height and width\n            refinenet_block (block): RefineNet Block\n            num_classes (int, optional): number of classes\n            features (int, optional): number of features in refinenet\n            resnet_factory (func, optional): A Resnet model from torchvision.\n                Default: models.resnet101\n            pretrained (bool, optional): Use pretrained version of resnet\n                Default: True\n            freeze_resnet (bool, optional): Freeze resnet model\n                Default: True\n\n        Raises:\n            ValueError: size of input_shape not divisible by 32\n        """"""\n        super().__init__()\n\n        input_channel, input_size = input_shape\n\n        if input_size % 32 != 0:\n            raise ValueError(""{} not divisble by 32"".format(input_shape))\n\n        resnet = resnet_factory(pretrained=pretrained)\n\n        self.layer1 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu,\n                                    resnet.maxpool, resnet.layer1)\n\n        self.layer2 = resnet.layer2\n        self.layer3 = resnet.layer3\n        self.layer4 = resnet.layer4\n\n        if freeze_resnet:\n            layers = [self.layer1, self.layer2, self.layer3, self.layer4]\n            for layer in layers:\n                for param in layer.parameters():\n                    param.requires_grad = False\n\n        self.layer1_rn = nn.Conv2d(\n            256, features, kernel_size=3, stride=1, padding=1, bias=False)\n        self.layer2_rn = nn.Conv2d(\n            512, features, kernel_size=3, stride=1, padding=1, bias=False)\n        self.layer3_rn = nn.Conv2d(\n            1024, features, kernel_size=3, stride=1, padding=1, bias=False)\n        self.layer4_rn = nn.Conv2d(\n            2048, 2 * features, kernel_size=3, stride=1, padding=1, bias=False)\n\n        self.refinenet4 = RefineNetBlock(2 * features,\n                                         (2 * features, input_size // 32))\n        self.refinenet3 = RefineNetBlock(features,\n                                         (2 * features, input_size // 32),\n                                         (features, input_size // 16))\n        self.refinenet2 = RefineNetBlock(features,\n                                         (features, input_size // 16),\n                                         (features, input_size // 8))\n        self.refinenet1 = RefineNetBlock(features, (features, input_size // 8),\n                                         (features, input_size // 4))\n\n        self.output_conv = nn.Sequential(\n            ResidualConvUnit(features), ResidualConvUnit(features),\n            nn.Conv2d(\n                features,\n                num_classes,\n                kernel_size=1,\n                stride=1,\n                padding=0,\n                bias=True))\n\n    def forward(self, x):\n        size = x.size()[2:]\n        layer_1 = self.layer1(x)\n        layer_2 = self.layer2(layer_1)\n        layer_3 = self.layer3(layer_2)\n        layer_4 = self.layer4(layer_3)\n\n        layer_1_rn = self.layer1_rn(layer_1)\n        layer_2_rn = self.layer2_rn(layer_2)\n        layer_3_rn = self.layer3_rn(layer_3)\n        layer_4_rn = self.layer4_rn(layer_4)\n\n        path_4 = self.refinenet4(layer_4_rn)\n        path_3 = self.refinenet3(path_4, layer_3_rn)\n        path_2 = self.refinenet2(path_3, layer_2_rn)\n        path_1 = self.refinenet1(path_2, layer_1_rn)\n        out_conv = self.output_conv(path_1)\n        out = F.interpolate(out_conv, size, mode=\'bilinear\', align_corners=True)\n        return out\n\n\nclass RefineNet4CascadePoolingImproved(BaseRefineNet4Cascade):\n    def __init__(self,\n                 num_classes=1,\n                 pretrained=True,\n                 input_shape=(1, 512),\n                 features=256,\n                 resnet_factory=models.resnet101,\n                 freeze_resnet=False,\n                 **kwargs):\n        """"""Multi-path 4-Cascaded RefineNet for image segmentation with improved pooling\n\n        Args:\n            input_shape ((int, int)): (channel, size) assumes input has\n                equal height and width\n            refinenet_block (block): RefineNet Block\n            num_classes (int, optional): number of classes\n            features (int, optional): number of features in refinenet\n            resnet_factory (func, optional): A Resnet model from torchvision.\n                Default: models.resnet101\n            pretrained (bool, optional): Use pretrained version of resnet\n                Default: True\n            freeze_resnet (bool, optional): Freeze resnet model\n                Default: True\n\n        Raises:\n            ValueError: size of input_shape not divisible by 32\n        """"""\n        super().__init__(\n            input_shape,\n            RefineNetBlockImprovedPooling,\n            num_classes=num_classes,\n            features=features,\n            resnet_factory=resnet_factory,\n            pretrained=pretrained,\n            freeze_resnet=freeze_resnet,\n            **kwargs)\n\n\nclass RefineNet4Cascade(BaseRefineNet4Cascade):\n    def __init__(self,\n                 num_classes=1,\n                 pretrained=True,\n                 input_shape=(1, 512),\n                 features=256,\n                 resnet_factory=models.resnet101,\n                 freeze_resnet=False,\n                 **kwargs):\n        """"""Multi-path 4-Cascaded RefineNet for image segmentation\n\n        Args:\n            input_shape ((int, int)): (channel, size) assumes input has\n                equal height and width\n            refinenet_block (block): RefineNet Block\n            num_classes (int, optional): number of classes\n            features (int, optional): number of features in refinenet\n            resnet_factory (func, optional): A Resnet model from torchvision.\n                Default: models.resnet101\n            pretrained (bool, optional): Use pretrained version of resnet\n                Default: True\n            freeze_resnet (bool, optional): Freeze resnet model\n                Default: True\n\n        Raises:\n            ValueError: size of input_shape not divisible by 32\n        """"""\n        super().__init__(\n            input_shape,\n            RefineNetBlock,\n            num_classes=num_classes,\n            features=features,\n            resnet_factory=resnet_factory,\n            pretrained=pretrained,\n            freeze_resnet=freeze_resnet,\n            **kwargs)\n'"
pywick/models/segmentation/testnets/Unet_nested.py,3,"b""import torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import init\nfrom .Unet_nested_layers import unetConv2, unetUp, unetConv2_dilation\n\n\ndef weights_init_normal(m):\n    classname = m.__class__.__name__\n    #print(classname)\n    if classname.find('Conv') != -1:\n        init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('Linear') != -1:\n        init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        init.normal_(m.weight.data, 1.0, 0.02)\n        init.constant_(m.bias.data, 0.0)\n\n\ndef weights_init_xavier(m):\n    classname = m.__class__.__name__\n    #print(classname)\n    if classname.find('Conv') != -1:\n        init.xavier_normal_(m.weight.data, gain=1)\n    elif classname.find('Linear') != -1:\n        init.xavier_normal_(m.weight.data, gain=1)\n    elif classname.find('BatchNorm') != -1:\n        init.normal_(m.weight.data, 1.0, 0.02)\n        init.constant_(m.bias.data, 0.0)\n\n\ndef weights_init_kaiming(m):\n    classname = m.__class__.__name__\n    #print(classname)\n    if classname.find('Conv') != -1:\n        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n    elif classname.find('Linear') != -1:\n        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n    elif classname.find('BatchNorm') != -1:\n        init.normal_(m.weight.data, 1.0, 0.02)\n        init.constant_(m.bias.data, 0.0)\n\n\ndef weights_init_orthogonal(m):\n    classname = m.__class__.__name__\n    #print(classname)\n    if classname.find('Conv') != -1:\n        init.orthogonal_(m.weight.data, gain=1)\n    elif classname.find('Linear') != -1:\n        init.orthogonal_(m.weight.data, gain=1)\n    elif classname.find('BatchNorm') != -1:\n        init.normal_(m.weight.data, 1.0, 0.02)\n        init.constant_(m.bias.data, 0.0)\n\n\ndef init_weights(net, init_type='normal'):\n    #print('initialization method [%s]' % init_type)\n    if init_type == 'normal':\n        net.apply(weights_init_normal)\n    elif init_type == 'xavier':\n        net.apply(weights_init_xavier)\n    elif init_type == 'kaiming':\n        net.apply(weights_init_kaiming)\n    elif init_type == 'orthogonal':\n        net.apply(weights_init_orthogonal)\n    else:\n        raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n\nclass UNet(nn.Module):\n\n    def __init__(self, in_channels=3, num_classes=4, feature_scale=4, is_deconv=True, is_batchnorm=True):\n        super(UNet, self).__init__()\n        self.is_deconv = is_deconv\n        self.in_channels = in_channels\n        self.is_batchnorm = is_batchnorm\n        self.feature_scale = feature_scale\n\n        filters = [64, 128, 256, 512, 1024]\n        filters = [int(x / self.feature_scale) for x in filters]\n\n        # downsampling\n        self.conv1 = unetConv2(self.in_channels, filters[0], self.is_batchnorm)\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n\n        self.conv2 = unetConv2(filters[0], filters[1], self.is_batchnorm)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n\n        self.conv3 = unetConv2(filters[1], filters[2], self.is_batchnorm)\n        self.maxpool3 = nn.MaxPool2d(kernel_size=2)\n\n        self.conv4 = unetConv2(filters[2], filters[3], self.is_batchnorm)\n        self.maxpool4 = nn.MaxPool2d(kernel_size=2)\n\n        self.center = unetConv2(filters[3], filters[4], self.is_batchnorm)\n\n        self.cls = nn.Sequential(\n            nn.Dropout(p=0.5),\n            nn.Conv2d(256, 3, 1),\n            nn.AdaptiveMaxPool2d(1),\n            nn.Sigmoid())\n\n        # upsampling\n        self.up_concat4 = unetUp(filters[4], filters[3], self.is_deconv)\n        self.up_concat3 = unetUp(filters[3], filters[2], self.is_deconv)\n        self.up_concat2 = unetUp(filters[2], filters[1], self.is_deconv)\n        self.up_concat1 = unetUp(filters[1], filters[0], self.is_deconv)\n\n        # final conv (without any concat)\n        self.final_1 = nn.Conv2d(filters[0], num_classes, 1)\n        # self.final_2 = nn.Conv2d(filters[0], n_classes, 1)\n        # self.final_3 = nn.Conv2d(filters[0], n_classes, 1)\n\n        # initialise weights\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                init_weights(m, init_type='kaiming')\n            elif isinstance(m, nn.BatchNorm2d):\n                init_weights(m, init_type='kaiming')\n\n    def forward(self, inputs):\n        conv1 = self.conv1(inputs)  # 16*512*512\n        maxpool1 = self.maxpool1(conv1)  # 16*256*256\n\n        conv2 = self.conv2(maxpool1)  # 32*256*256\n        maxpool2 = self.maxpool2(conv2)  # 32*128*128\n\n        conv3 = self.conv3(maxpool2)  # 64*128*128\n        maxpool3 = self.maxpool3(conv3)  # 64*64*64\n\n        conv4 = self.conv4(maxpool3)  # 128*64*64\n        maxpool4 = self.maxpool4(conv4)  # 128*32*32\n\n        center = self.center(maxpool4)  # 256*32*32\n        cls_branch = self.cls(center).squeeze()\n        up4 = self.up_concat4(center, conv4)  # 128*64*64\n        up3 = self.up_concat3(up4, conv3)  # 64*128*128\n        up2 = self.up_concat2(up3, conv2)  # 32*256*256\n        up1 = self.up_concat1(up2, conv1)  # 16*512*512\n\n        final_1 = self.final_1(up1)\n        # final_2 = self.final_2(up1)\n        # final_3 = self.final_3(up1)\n\n        # return F.log_softmax(final_1, dim=1), cls_branch\n        return final_1\n\n\nclass UNet_Nested(nn.Module):\n\n    def __init__(self, in_channels=3, n_classes=4, feature_scale=4, is_deconv=True, is_batchnorm=True, is_ds=True):\n        super(UNet_Nested, self).__init__()\n        self.is_deconv = is_deconv\n        self.in_channels = in_channels\n        self.is_batchnorm = is_batchnorm\n        self.is_ds = is_ds\n        self.feature_scale = feature_scale\n\n        filters = [64, 128, 256, 512, 1024]\n        filters = [int(x / self.feature_scale) for x in filters]\n\n        # downsampling\n        self.conv00 = unetConv2(self.in_channels, filters[0], self.is_batchnorm)\n        self.maxpool0 = nn.MaxPool2d(kernel_size=2)\n        self.conv10 = unetConv2(filters[0], filters[1], self.is_batchnorm)\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n        self.conv20 = unetConv2(filters[1], filters[2], self.is_batchnorm)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n        self.conv30 = unetConv2(filters[2], filters[3], self.is_batchnorm)\n        self.maxpool3 = nn.MaxPool2d(kernel_size=2)\n        self.conv40 = unetConv2(filters[3], filters[4], self.is_batchnorm)\n\n        self.cls = nn.Sequential(\n            nn.Dropout(p=0.5),\n            nn.Conv2d(256, 3, 1),\n            nn.AdaptiveMaxPool2d(1),\n            nn.Sigmoid())\n\n        # upsampling\n        self.up_concat01 = unetUp(filters[1], filters[0], self.is_deconv)\n        self.up_concat11 = unetUp(filters[2], filters[1], self.is_deconv)\n        self.up_concat21 = unetUp(filters[3], filters[2], self.is_deconv)\n        self.up_concat31 = unetUp(filters[4], filters[3], self.is_deconv)\n\n        self.up_concat02 = unetUp(filters[1], filters[0], self.is_deconv, 3)\n        self.up_concat12 = unetUp(filters[2], filters[1], self.is_deconv, 3)\n        self.up_concat22 = unetUp(filters[3], filters[2], self.is_deconv, 3)\n\n        self.up_concat03 = unetUp(filters[1], filters[0], self.is_deconv, 4)\n        self.up_concat13 = unetUp(filters[2], filters[1], self.is_deconv, 4)\n\n        self.up_concat04 = unetUp(filters[1], filters[0], self.is_deconv, 5)\n\n        # final conv (without any concat)\n        self.final_1 = nn.Conv2d(filters[0], n_classes, 1)\n        self.final_2 = nn.Conv2d(filters[0], n_classes, 1)\n        self.final_3 = nn.Conv2d(filters[0], n_classes, 1)\n        self.final_4 = nn.Conv2d(filters[0], n_classes, 1)\n\n        # initialise weights\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                init_weights(m, init_type='kaiming')\n            elif isinstance(m, nn.BatchNorm2d):\n                init_weights(m, init_type='kaiming')\n\n    def forward(self, inputs):\n        # column : 0\n        X_00 = self.conv00(inputs)  # 16*512*512\n        maxpool0 = self.maxpool0(X_00)  # 16*256*256\n        X_10 = self.conv10(maxpool0)  # 32*256*256\n        maxpool1 = self.maxpool1(X_10)  # 32*128*128\n        X_20 = self.conv20(maxpool1)  # 64*128*128\n        maxpool2 = self.maxpool2(X_20)  # 64*64*64\n        X_30 = self.conv30(maxpool2)  # 128*64*64\n        maxpool3 = self.maxpool3(X_30)  # 128*32*32\n        X_40 = self.conv40(maxpool3)  # 256*32*32\n        cls_branch = self.cls(X_40).squeeze()\n        # column : 1\n        X_01 = self.up_concat01(X_10, X_00)\n        X_11 = self.up_concat11(X_20, X_10)\n        X_21 = self.up_concat21(X_30, X_20)\n        X_31 = self.up_concat31(X_40, X_30)\n        # column : 2\n        X_02 = self.up_concat02(X_11, X_00, X_01)\n        X_12 = self.up_concat12(X_21, X_10, X_11)\n        X_22 = self.up_concat22(X_31, X_20, X_21)\n        # column : 3\n        X_03 = self.up_concat03(X_12, X_00, X_01, X_02)\n        X_13 = self.up_concat13(X_22, X_10, X_11, X_12)\n        # column : 4\n        X_04 = self.up_concat04(X_13, X_00, X_01, X_02, X_03)\n\n        # final layer\n        final_1 = self.final_1(X_01)\n        final_2 = self.final_2(X_02)\n        final_3 = self.final_3(X_03)\n        final_4 = self.final_4(X_04)\n\n        final = (final_1 + final_2 + final_3 + final_4) / 4\n\n        if self.is_ds:\n            # return F.log_softmax(final, dim=1), cls_branch\n            return final\n        else:\n            # return F.log_softmax(final_4), cls_branch\n            return final_4\n\n\nclass UNet_Nested_dilated(nn.Module):\n\n    def __init__(self, num_classes=4, in_channels=3, feature_scale=4, is_deconv=True, is_batchnorm=True, is_ds=True, **kwargs):\n        super(UNet_Nested_dilated, self).__init__()\n        self.is_deconv = is_deconv\n        self.in_channels = in_channels\n        self.is_batchnorm = is_batchnorm\n        self.is_ds = is_ds\n        self.feature_scale = feature_scale\n\n        filters = [64, 128, 256, 512, 1024]\n        filters = [int(x / self.feature_scale) for x in filters]\n\n        # downsampling\n        self.conv00 = unetConv2(self.in_channels, filters[0], self.is_batchnorm)\n        self.maxpool0 = nn.MaxPool2d(kernel_size=2)\n        self.conv10 = unetConv2(filters[0], filters[1], self.is_batchnorm)\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n        self.conv20 = unetConv2(filters[1], filters[2], self.is_batchnorm)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n        self.conv30 = unetConv2(filters[2], filters[3], self.is_batchnorm)\n        self.maxpool3 = nn.MaxPool2d(kernel_size=2)\n        self.conv40 = unetConv2(filters[3], filters[4], self.is_batchnorm)\n\n        self.dilated = unetConv2_dilation(filters[4], filters[4], self.is_batchnorm)\n\n        self.cls = nn.Sequential(\n            nn.Dropout(p=0.5),\n            nn.Conv2d(256, 3, 1),\n            nn.AdaptiveMaxPool2d(1),\n            nn.Sigmoid())\n\n        # upsampling\n        self.up_concat01 = unetUp(filters[1], filters[0], self.is_deconv)\n        self.up_concat11 = unetUp(filters[2], filters[1], self.is_deconv)\n        self.up_concat21 = unetUp(filters[3], filters[2], self.is_deconv)\n        self.up_concat31 = unetUp(filters[4], filters[3], self.is_deconv)\n\n        self.up_concat02 = unetUp(filters[1], filters[0], self.is_deconv, 3)\n        self.up_concat12 = unetUp(filters[2], filters[1], self.is_deconv, 3)\n        self.up_concat22 = unetUp(filters[3], filters[2], self.is_deconv, 3)\n\n        self.up_concat03 = unetUp(filters[1], filters[0], self.is_deconv, 4)\n        self.up_concat13 = unetUp(filters[2], filters[1], self.is_deconv, 4)\n\n        self.up_concat04 = unetUp(filters[1], filters[0], self.is_deconv, 5)\n\n        # final conv (without any concat)\n        self.final_1 = nn.Conv2d(filters[0], num_classes, 1)\n        self.final_2 = nn.Conv2d(filters[0], num_classes, 1)\n        self.final_3 = nn.Conv2d(filters[0], num_classes, 1)\n        self.final_4 = nn.Conv2d(filters[0], num_classes, 1)\n\n        # initialise weights\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                init_weights(m, init_type='kaiming')\n            elif isinstance(m, nn.BatchNorm2d):\n                init_weights(m, init_type='kaiming')\n\n    def forward(self, inputs):\n        # column : 0\n        X_00 = self.conv00(inputs)  # 16*512*512\n        maxpool0 = self.maxpool0(X_00)  # 16*256*256\n        X_10 = self.conv10(maxpool0)  # 32*256*256\n        maxpool1 = self.maxpool1(X_10)  # 32*128*128\n        X_20 = self.conv20(maxpool1)  # 64*128*128\n        maxpool2 = self.maxpool2(X_20)  # 64*64*64\n        X_30 = self.conv30(maxpool2)  # 128*64*64\n        maxpool3 = self.maxpool3(X_30)  # 128*32*32\n        X_40 = self.conv40(maxpool3)  # 256*32*32\n        X_40_d = self.dilated(X_40)\n        cls_branch = self.cls(X_40_d).squeeze()\n        # column : 1\n        X_01 = self.up_concat01(X_10, X_00)\n        X_11 = self.up_concat11(X_20, X_10)\n        X_21 = self.up_concat21(X_30, X_20)\n        X_31 = self.up_concat31(X_40_d, X_30)\n        # column : 2\n        X_02 = self.up_concat02(X_11, X_00, X_01)\n        X_12 = self.up_concat12(X_21, X_10, X_11)\n        X_22 = self.up_concat22(X_31, X_20, X_21)\n        # column : 3\n        X_03 = self.up_concat03(X_12, X_00, X_01, X_02)\n        X_13 = self.up_concat13(X_22, X_10, X_11, X_12)\n        # column : 4\n        X_04 = self.up_concat04(X_13, X_00, X_01, X_02, X_03)\n\n        # final layer\n        final_1 = self.final_1(X_01)\n        final_2 = self.final_2(X_02)\n        final_3 = self.final_3(X_03)\n        final_4 = self.final_4(X_04)\n\n        final = (final_1 + final_2 + final_3 + final_4) / 4\n\n        if self.is_ds:\n            # return F.log_softmax(final), cls_branch\n            return final\n        else:\n            # return F.log_softmax(final_4), cls_branch\n            return final_4\n"""
pywick/models/segmentation/testnets/Unet_nested_layers.py,7,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import init\n\ndef weights_init_normal(m):\n    classname = m.__class__.__name__\n    #print(classname)\n    if classname.find('Conv') != -1:\n        init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('Linear') != -1:\n        init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        init.normal_(m.weight.data, 1.0, 0.02)\n        init.constant_(m.bias.data, 0.0)\n\n\ndef weights_init_xavier(m):\n    classname = m.__class__.__name__\n    #print(classname)\n    if classname.find('Conv') != -1:\n        init.xavier_normal_(m.weight.data, gain=1)\n    elif classname.find('Linear') != -1:\n        init.xavier_normal_(m.weight.data, gain=1)\n    elif classname.find('BatchNorm') != -1:\n        init.normal_(m.weight.data, 1.0, 0.02)\n        init.constant_(m.bias.data, 0.0)\n\n\ndef weights_init_kaiming(m):\n    classname = m.__class__.__name__\n    #print(classname)\n    if classname.find('Conv') != -1:\n        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n    elif classname.find('Linear') != -1:\n        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n    elif classname.find('BatchNorm') != -1:\n        init.normal_(m.weight.data, 1.0, 0.02)\n        init.constant_(m.bias.data, 0.0)\n\n\ndef weights_init_orthogonal(m):\n    classname = m.__class__.__name__\n    #print(classname)\n    if classname.find('Conv') != -1:\n        init.orthogonal_(m.weight.data, gain=1)\n    elif classname.find('Linear') != -1:\n        init.orthogonal_(m.weight.data, gain=1)\n    elif classname.find('BatchNorm') != -1:\n        init.normal_(m.weight.data, 1.0, 0.02)\n        init.constant_(m.bias.data, 0.0)\n\n\ndef init_weights(net, init_type='normal'):\n    #print('initialization method [%s]' % init_type)\n    if init_type == 'normal':\n        net.apply(weights_init_normal)\n    elif init_type == 'xavier':\n        net.apply(weights_init_xavier)\n    elif init_type == 'kaiming':\n        net.apply(weights_init_kaiming)\n    elif init_type == 'orthogonal':\n        net.apply(weights_init_orthogonal)\n    else:\n        raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n\n\nclass unetConv2(nn.Module):\n    def __init__(self, in_size, out_size, is_batchnorm, n=2, ks=3, stride=1, padding=1):\n        super(unetConv2, self).__init__()\n        self.n = n\n        self.ks = ks\n        self.stride = stride\n        self.padding = padding\n        s = stride\n        p = padding\n        if is_batchnorm:\n            for i in range(1, n + 1):\n                conv = nn.Sequential(nn.Conv2d(in_size, out_size, ks, s, p),\n                                     nn.BatchNorm2d(out_size),\n                                     nn.ReLU(inplace=True), )\n                setattr(self, 'conv%d' % i, conv)\n                in_size = out_size\n\n        else:\n            for i in range(1, n + 1):\n                conv = nn.Sequential(nn.Conv2d(in_size, out_size, ks, s, p),\n                                     nn.ReLU(inplace=True), )\n                setattr(self, 'conv%d' % i, conv)\n                in_size = out_size\n\n        # initialise the blocks\n        for m in self.children():\n            init_weights(m, init_type='kaiming')\n\n    def forward(self, inputs):\n        x = inputs\n        for i in range(1, self.n + 1):\n            conv = getattr(self, 'conv%d' % i)\n            x = conv(x)\n\n        return x\n\n\nclass unetConv2_res(nn.Module):\n    def __init__(self, in_size, out_size, is_batchnorm, n=2, ks=3, stride=1, padding=1):\n        super(unetConv2_res, self).__init__()\n        self.n = n\n        self.ks = ks\n        self.stride = stride\n        self.padding = padding\n        s = stride\n        p = padding\n        self.conv0 = nn.Conv2d(in_size, out_size, 1)\n        if is_batchnorm:\n            for i in range(1, n + 1):\n                conv = nn.Sequential(nn.Conv2d(in_size, out_size, ks, s, p),\n                                     nn.BatchNorm2d(out_size),\n                                     nn.ReLU(inplace=True), )\n                setattr(self, 'conv%d' % i, conv)\n                in_size = out_size\n\n        else:\n            for i in range(1, n + 1):\n                conv = nn.Sequential(nn.Conv2d(in_size, out_size, ks, s, p),\n                                     nn.ReLU(inplace=True), )\n                setattr(self, 'conv%d' % i, conv)\n                in_size = out_size\n\n        # initialise the blocks\n        for m in self.children():\n            init_weights(m, init_type='kaiming')\n\n    def forward(self, inputs):\n        inputs_ori = self.conv0(inputs)\n        x = inputs\n        for i in range(1, self.n + 1):\n            conv = getattr(self, 'conv%d' % i)\n            x = conv(x)\n\n        return x + inputs_ori\n\n\nclass unetUp(nn.Module):\n    def __init__(self, in_size, out_size, is_deconv, n_concat=2):\n        super(unetUp, self).__init__()\n        self.conv = unetConv2(in_size + (n_concat - 2) * out_size, out_size, False)\n        if is_deconv:\n            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=4, stride=2, padding=1)\n        else:\n            self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n\n        # initialise the blocks\n        for m in self.children():\n            if m.__class__.__name__.find('unetConv2') != -1: continue\n            init_weights(m, init_type='kaiming')\n\n    def forward(self, inputs0, *input):\n        # print(self.n_concat)\n        # print(input)\n        outputs0 = self.up(inputs0)\n        for i in range(len(input)):\n            outputs0 = torch.cat([outputs0, input[i]], 1)\n        return self.conv(outputs0)\n\n\nclass UnetConv3(nn.Module):\n    def __init__(self, in_size, out_size, is_batchnorm, kernel_size=(3, 3, 3), padding_size=(1, 1, 1), init_stride=(1, 1, 1)):\n        super(UnetConv3, self).__init__()\n\n        if is_batchnorm:\n            self.conv1 = nn.Sequential(nn.Conv3d(in_size, out_size, kernel_size, init_stride, padding_size),\n                                       nn.BatchNorm3d(out_size),\n                                       nn.ReLU(inplace=True), )\n            self.conv2 = nn.Sequential(nn.Conv3d(out_size, out_size, kernel_size, 1, padding_size),\n                                       nn.BatchNorm3d(out_size),\n                                       nn.ReLU(inplace=True), )\n        else:\n            self.conv1 = nn.Sequential(nn.Conv3d(in_size, out_size, kernel_size, init_stride, padding_size),\n                                       nn.ReLU(inplace=True), )\n            self.conv2 = nn.Sequential(nn.Conv3d(out_size, out_size, kernel_size, 1, padding_size),\n                                       nn.ReLU(inplace=True), )\n\n        # initialise the blocks\n        for m in self.children():\n            init_weights(m, init_type='kaiming')\n\n    def forward(self, inputs):\n        outputs = self.conv1(inputs)\n        outputs = self.conv2(outputs)\n        return outputs\n\n\nclass UnetUp3(nn.Module):\n    def __init__(self, in_size, out_size, is_deconv, is_batchnorm=True):\n        super(UnetUp3, self).__init__()\n        if is_deconv:\n            self.conv = UnetConv3(in_size, out_size, is_batchnorm)\n            self.up = nn.ConvTranspose3d(in_size, out_size, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n        else:\n            self.conv = UnetConv3(in_size + out_size, out_size, is_batchnorm)\n            self.up = nn.Upsample(scale_factor=(2, 2, 2), mode='trilinear')\n\n        # initialise the blocks\n        for m in self.children():\n            if m.__class__.__name__.find('UnetConv3') != -1: continue\n            init_weights(m, init_type='kaiming')\n\n    def forward(self, inputs1, inputs2):\n        outputs2 = self.up(inputs2)\n        offset = outputs2.size()[2] - inputs1.size()[2]\n        padding = 2 * [offset // 2, offset // 2, 0]\n        outputs1 = F.pad(inputs1, padding)\n        return self.conv(torch.cat([outputs1, outputs2], 1))\n\n\nclass Upold(nn.Module):\n    def __init__(self, in_size, out_size, is_deconv):\n        super(Upold, self).__init__()\n        self.conv = unetConv2(in_size, out_size, False)\n        if is_deconv:\n            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=4, stride=2, padding=1)\n        else:\n            self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n\n        # initialise the blocks\n        for m in self.children():\n            if m.__class__.__name__.find('unetConv2') != -1: continue\n            init_weights(m, init_type='kaiming')\n\n    def forward(self, inputs1, inputs2):\n        outputs2 = self.up(inputs2)\n        offset = outputs2.size()[2] - inputs1.size()[2]\n        padding = 2 * [offset // 2, offset // 2]\n        outputs1 = F.pad(inputs1, padding)\n        return self.conv(torch.cat([outputs1, outputs2], 1))\n\n\nclass unetConv2_SELU(nn.Module):\n    def __init__(self, in_size, out_size, is_batchnorm, n=2, ks=3, stride=1, padding=1):\n        super(unetConv2_SELU, self).__init__()\n        self.n = n\n        self.ks = ks\n        self.stride = stride\n        self.padding = padding\n        s = stride\n        p = padding\n        if is_batchnorm:\n            for i in range(1, n + 1):\n                conv = nn.Sequential(nn.Conv2d(in_size, out_size, ks, s, p),\n                                     nn.BatchNorm2d(out_size),\n                                     nn.SELU(inplace=True), )\n                setattr(self, 'conv%d' % i, conv)\n                in_size = out_size\n\n        else:\n            for i in range(1, n + 1):\n                conv = nn.Sequential(nn.Conv2d(in_size, out_size, ks, s, p),\n                                     nn.SELU(inplace=True), )\n                setattr(self, 'conv%d' % i, conv)\n                in_size = out_size\n\n        # initialise the blocks\n        for m in self.children():\n            init_weights(m, init_type='kaiming')\n\n    def forward(self, inputs):\n        x = inputs\n        for i in range(1, self.n + 1):\n            conv = getattr(self, 'conv%d' % i)\n            x = conv(x)\n\n        return x\n\n\nclass unetUp_SELU(nn.Module):\n    def __init__(self, in_size, out_size, is_deconv):\n        super(unetUp_SELU, self).__init__()\n        self.conv = unetConv2_SELU(in_size, out_size, False)\n        if is_deconv:\n            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=4, stride=2, padding=1)\n        else:\n            self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n\n        # initialise the blocks\n        for m in self.children():\n            if m.__class__.__name__.find('unetConv2') != -1: continue\n            init_weights(m, init_type='kaiming')\n\n    def forward(self, inputs1, inputs2):\n        outputs2 = self.up(inputs2)\n        offset = outputs2.size()[2] - inputs1.size()[2]\n        padding = 2 * [offset // 2, offset // 2]\n        outputs1 = F.pad(inputs1, padding)\n        return self.conv(torch.cat([outputs1, outputs2], 1))\n\n\nclass unetConv2_dilation(nn.Module):\n    def __init__(self, in_size, out_size, is_batchnorm=True, n=4, ks=3, stride=1):\n        super(unetConv2_dilation, self).__init__()\n        self.n = n\n        self.ks = ks\n        self.stride = stride\n        s = stride\n        if is_batchnorm:\n            for i in range(1, n + 1):\n                conv = nn.Sequential(nn.Conv2d(in_size, out_size, ks, s, 2 ** (i - 1), 2 ** (i - 1)),\n                                     nn.BatchNorm2d(out_size),\n                                     nn.ReLU(inplace=True), )\n                setattr(self, 'conv%d' % i, conv)\n                in_size = out_size\n\n        else:\n            for i in range(1, n + 1):\n                conv = nn.Sequential(nn.Conv2d(in_size, out_size, ks, s), nn.ReLU(inplace=True), )\n                setattr(self, 'conv%d' % i, conv)\n                in_size = out_size\n\n        # initialise the blocks\n        for m in self.children():\n            init_weights(m, init_type='kaiming')\n\n    def forward(self, inputs):\n        output = inputs\n        # print(output.shape)\n        x_0 = inputs\n        conv = getattr(self, 'conv1')\n        x_1 = conv(x_0)\n        conv = getattr(self, 'conv2')\n        x_2 = conv(x_1)\n        conv = getattr(self, 'conv3')\n        x_3 = conv(x_2)\n        conv = getattr(self, 'conv4')\n        x_4 = conv(x_3)\n\n        return x_0 + x_1 + x_2 + x_3 + x_4\n\n\nclass unetConv2_dilation2(nn.Module):\n    def __init__(self, in_size, out_size, is_batchnorm=True, n=3, ks=3, stride=1):\n        super(unetConv2_dilation2, self).__init__()\n        self.n = n\n        self.ks = ks\n        self.stride = stride\n        s = stride\n        if is_batchnorm:\n            for i in range(1, n + 1):\n                conv = nn.Sequential(nn.Conv2d(in_size, out_size, ks, s, 2 ** (i - 1), 2 ** (i - 1)),\n                                     nn.BatchNorm2d(out_size),\n                                     nn.ReLU(inplace=True), )\n                setattr(self, 'conv%d' % i, conv)\n                in_size = out_size\n\n        else:\n            for i in range(1, n + 1):\n                conv = nn.Sequential(nn.Conv2d(in_size, out_size, ks, s), nn.ReLU(inplace=True), )\n                setattr(self, 'conv%d' % i, conv)\n                in_size = out_size\n\n        # initialise the blocks\n        for m in self.children():\n            init_weights(m, init_type='kaiming')\n\n    def forward(self, inputs):\n        output = inputs\n        # print(output.shape)\n        x_0 = inputs\n        conv = getattr(self, 'conv1')\n        x_1 = conv(x_0)\n        conv = getattr(self, 'conv2')\n        x_2 = conv(x_1)\n        conv = getattr(self, 'conv3')\n        x_3 = conv(x_2)\n\n        return x_0 + x_1 + x_2 + x_3\n\n\nclass SELayer(nn.Module):\n    def __init__(self, channel, reduction=2, is_bn=True, is_cse=True, is_sse=True):\n        super(SELayer, self).__init__()\n        self.is_cse = is_cse\n        self.is_sse = is_sse\n        self.is_bn = is_bn\n\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel),\n            nn.Sigmoid()\n        )\n\n        self.sse1 = nn.Conv2d(channel, 1, 1)\n        self.sse2 = nn.Sigmoid()\n        self.bn = nn.BatchNorm2d(channel)\n\n    def forward(self, x):\n\n        b, c, _, _ = x.size()\n        y_c = self.avg_pool(x).view(b, c)\n        y_c = self.fc(y_c).view(b, c, 1, 1)\n        if self.is_bn:\n            out_c = self.bn(x * y_c)\n        else:\n            out_c = x * y_c\n\n        y_s = self.sse2(self.sse1(x))\n        if self.is_bn:\n            out_s = self.bn(x * y_s)\n        else:\n            out_s = x * y_s\n\n        if self.is_cse and not self.is_sse:\n            return out_c\n        elif self.is_sse and not self.is_cse:\n            return out_s\n        else:\n            return out_c + out_s\n\n\nclass Downsample(nn.Module):\n    def __init__(self, in_size, out_size, is_batchnorm, ks=4, stride=2, padding=1):\n        super(Downsample, self).__init__()\n        self.ks = ks\n        self.stride = stride\n        self.padding = padding\n        s = stride\n        p = padding\n        if is_batchnorm:\n            self.conv = nn.Sequential(nn.Conv2d(in_size, out_size, ks, s, p),\n                                      nn.BatchNorm2d(out_size),\n                                      nn.ReLU(inplace=True), )\n\n\n        else:\n            self.conv = nn.Sequential(nn.Conv2d(in_size, out_size, ks, s, p),\n                                      nn.ReLU(inplace=True), )\n\n        # initialise the blocks\n        for m in self.children():\n            init_weights(m, init_type='kaiming')\n\n    def forward(self, inputs):\n        x = self.conv(inputs)\n        return x\n\n\nclass Atrous_module(nn.Module):\n    def __init__(self, inplanes, planes, rate):\n        super(Atrous_module, self).__init__()\n        self.atrous_convolution = nn.Conv2d(inplanes, planes, kernel_size=3,\n                                            stride=1, padding=rate, dilation=rate)\n        self.batch_norm = nn.BatchNorm2d(planes)\n\n    def forward(self, x):\n        x = self.atrous_convolution(x)\n        x = self.batch_norm(x)\n\n        return x\n\n"""
pywick/models/segmentation/testnets/__init__.py,0,"b'from .autofocusNN import *\nfrom .dabnet import *\nfrom .deeplabv3 import DeepLabV3 as TEST_DLV3\nfrom .deeplabv2 import DeepLabV2 as TEST_DLV2\nfrom .deeplabv3_xception import DeepLabv3_plus as TEST_DLV3_Xception\nfrom .deeplabv3_xception import create_DLX_V3_pretrained as TEST_DLX_V3\nfrom .deeplabv3_resnet import create_DLR_V3_pretrained as TEST_DLR_V3\nfrom .difnet import DifNet101, DifNet152\nfrom .encnet import EncNet as TEST_EncNet, encnet_resnet50 as TEST_EncNet_Res50, encnet_resnet101 as TEST_EncNet_Res101, encnet_resnet152 as TEST_EncNet_Res152\nfrom .exfuse import UnetExFuse\nfrom .lg_kernel_exfuse import GCNFuse\nfrom .psanet import *\nfrom .psp_saeed import PSPNet as TEST_PSPNet2\nfrom .tkcnet.tkcnet import TKCNet_Resnet101\nfrom .tiramisu_test import FCDenseNet57 as TEST_Tiramisu57\nfrom .Unet_nested import UNet_Nested_dilated as TEST_Unet_nested_dilated\nfrom .unet_plus_plus import NestNet as Unet_Plus_Plus'"
pywick/models/segmentation/testnets/autofocusNN.py,4,"b'# Source: https://github.com/yaq007/Autofocus-Layer (MIT)\n\n""""""\nAutofocus Layer - `Autofocus Layer for Semantic Segmentation <https://arxiv.org/abs/1805.08403>`_.\n\nOnly applicable to 3D targets\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n__all__ = [\'AFN1\', \'AFN2\', \'AFN3\', \'AFN4\', \'AFN5\', \'AFN6\', \'AFN_ASPP_c\', \'AFN_ASPP_s\', \'get_AFN\']\n\n\nclass ModelBuilder():\n    def build_net(self, arch=\'AFN1\', num_input=4, num_classes=5, num_branches=4, padding_list=[0, 4, 8, 12], dilation_list=[2, 6, 10, 14], **kwargs):\n        # parameters in the architecture\n        channels = [num_input - 1, 30, 30, 40, 40, 40, 40, 50, 50, num_classes]\n        kernel_size = 3\n\n        # Baselines\n        if arch == \'Basic\':\n            network = Basic(channels, kernel_size)\n            return network\n        elif arch == \'ASPP_c\':\n            network = ASPP_c(dilation_list, channels, kernel_size, num_branches)\n            return network\n        elif arch == \'ASPP_s\':\n            network = ASPP_s(dilation_list, channels, kernel_size, num_branches)\n            return network\n\n        # Autofocus Neural Networks\n        elif arch == \'AFN1\':\n            blocks = [BasicBlock, BasicBlock, Autofocus_single]\n        elif arch == \'AFN2\':\n            blocks = [BasicBlock, BasicBlock, Autofocus]\n        elif arch == \'AFN3\':\n            blocks = [BasicBlock, Autofocus_single, Autofocus]\n        elif arch == \'AFN4\':\n            blocks = [BasicBlock, Autofocus, Autofocus]\n        elif arch == \'AFN5\':\n            blocks = [Autofocus_single, Autofocus, Autofocus]\n        elif arch == \'AFN6\':\n            blocks = [Autofocus, Autofocus, Autofocus]\n        else:\n            raise Exception(\'Architecture undefined\')\n\n        network = AFN(blocks, padding_list, dilation_list, channels, kernel_size, num_branches)\n        return network\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, inplanes1, outplanes1, outplanes2, kernel=3, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv3d(inplanes1, outplanes1, kernel_size=kernel, dilation=2)\n        self.bn1 = nn.BatchNorm3d(outplanes1)\n        self.conv2 = nn.Conv3d(outplanes1, outplanes2, kernel_size=kernel, dilation=2)\n        self.bn2 = nn.BatchNorm3d(outplanes2)\n        self.relu = nn.ReLU(inplace=True)\n        if inplanes1 == outplanes2:\n            self.downsample = downsample\n        else:\n            self.downsample = nn.Sequential(nn.Conv3d(inplanes1, outplanes2, kernel_size=1), nn.BatchNorm3d(outplanes2))\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                m.weight.data.normal_(0, 0.01)\n            elif isinstance(m, nn.BatchNorm3d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        residual = x[:, :, 4:-4, 4:-4, 4:-4]\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n\n        if self.downsample is not None:\n            residual = self.downsample(residual)\n\n        x += residual\n        x = self.relu(x)\n        return x\n\n\nclass Autofocus_single(nn.Module):\n    def __init__(self, inplanes1, outplanes1, outplanes2, padding_list, dilation_list, num_branches, kernel=3):\n        super(Autofocus_single, self).__init__()\n        self.padding_list = padding_list\n        self.dilation_list = dilation_list\n        self.num_branches = num_branches\n        self.conv1 = nn.Conv3d(inplanes1, outplanes1, kernel_size=kernel, dilation=2)\n        self.bn1 = nn.BatchNorm3d(outplanes1)\n\n        self.bn_list2 = nn.ModuleList()\n        for i in range(len(self.padding_list)):\n            self.bn_list2.append(nn.BatchNorm3d(outplanes2))\n\n        self.conv2 = nn.Conv3d(outplanes1, outplanes2, kernel_size=kernel, dilation=self.dilation_list[0])\n        self.convatt1 = nn.Conv3d(outplanes1, int(outplanes1 / 2), kernel_size=kernel)\n        self.convatt2 = nn.Conv3d(int(outplanes1 / 2), self.num_branches, kernel_size=1)\n\n        self.relu = nn.ReLU(inplace=True)\n        if inplanes1 == outplanes2:\n            self.downsample = None\n        else:\n            self.downsample = nn.Sequential(nn.Conv3d(inplanes1, outplanes2, kernel_size=1), nn.BatchNorm3d(outplanes2))\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                m.weight.data.normal_(0, 0.01)\n            elif isinstance(m, nn.BatchNorm3d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        residual = x[:, :, 4:-4, 4:-4, 4:-4]\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        # compute attention weights for the second layer\n        feature = x.detach()\n        att = self.relu(self.convatt1(feature))\n        att = self.convatt2(att)\n        # att = torch.sigmoid(att)\n        att = F.softmax(att, dim=1)\n        att = att[:, :, 1:-1, 1:-1, 1:-1]\n\n        # linear combination of different dilation rates\n        x1 = self.conv2(x)\n        shape = x1.size()\n        x1 = self.bn_list2[0](x1) * att[:, 0:1, :, :, :].expand(shape)\n\n        # sharing weights in parallel convolutions\n        for i in range(1, self.num_branches):\n            x2 = F.conv3d(x, self.conv2.weight, padding=self.padding_list[i], dilation=self.dilation_list[i])\n            x2 = self.bn_list2[i](x2)\n            x1 += x2 * att[:, i:(i + 1), :, :, :].expand(shape)\n\n        if self.downsample is not None:\n            residual = self.downsample(residual)\n\n        x = x1 + residual\n        x = self.relu(x)\n        return x\n\n\nclass Autofocus(nn.Module):\n    def __init__(self, inplanes1, outplanes1, outplanes2, padding_list, dilation_list, num_branches, kernel=3):\n        super(Autofocus, self).__init__()\n        self.padding_list = padding_list\n        self.dilation_list = dilation_list\n        self.num_branches = num_branches\n\n        self.conv1 = nn.Conv3d(inplanes1, outplanes1, kernel_size=kernel, dilation=self.dilation_list[0])\n        self.convatt11 = nn.Conv3d(inplanes1, int(inplanes1 / 2), kernel_size=kernel)\n        self.convatt12 = nn.Conv3d(int(inplanes1 / 2), self.num_branches, kernel_size=1)\n        self.bn_list1 = nn.ModuleList()\n        for i in range(self.num_branches):\n            self.bn_list1.append(nn.BatchNorm3d(outplanes1))\n\n        self.conv2 = nn.Conv3d(outplanes1, outplanes2, kernel_size=kernel, dilation=self.dilation_list[0])\n        self.convatt21 = nn.Conv3d(outplanes1, int(outplanes1 / 2), kernel_size=kernel)\n        self.convatt22 = nn.Conv3d(int(outplanes1 / 2), self.num_branches, kernel_size=1)\n        self.bn_list2 = nn.ModuleList()\n        for i in range(self.num_branches):\n            self.bn_list2.append(nn.BatchNorm3d(outplanes2))\n\n        self.relu = nn.ReLU(inplace=True)\n        if inplanes1 == outplanes2:\n            self.downsample = None\n        else:\n            self.downsample = nn.Sequential(nn.Conv3d(inplanes1, outplanes2, kernel_size=1), nn.BatchNorm3d(outplanes2))\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                m.weight.data.normal_(0, 0.01)\n            elif isinstance(m, nn.BatchNorm3d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        residual = x[:, :, 4:-4, 4:-4, 4:-4]\n        # compute attention weights in the first autofocus convolutional layer\n        feature = x.detach()\n        att = self.relu(self.convatt11(feature))\n        att = self.convatt12(att)\n        att = F.softmax(att, dim=1)\n        att = att[:, :, 1:-1, 1:-1, 1:-1]\n\n        # linear combination of different rates\n        x1 = self.conv1(x)\n        shape = x1.size()\n        x1 = self.bn_list1[0](x1) * att[:, 0:1, :, :, :].expand(shape)\n\n        for i in range(1, self.num_branches):\n            x2 = F.conv3d(x, self.conv1.weight, padding=self.padding_list[i], dilation=self.dilation_list[i])\n            x2 = self.bn_list1[i](x2)\n            x1 += x2 * att[:, i:(i + 1), :, :, :].expand(shape)\n\n        x = self.relu(x1)\n\n        # compute attention weights for the second autofocus layer\n        feature2 = x.detach()\n        att2 = self.relu(self.convatt21(feature2))\n        att2 = self.convatt22(att2)\n        att2 = F.softmax(att2, dim=1)\n        att2 = att2[:, :, 1:-1, 1:-1, 1:-1]\n\n        # linear combination of different rates\n        x21 = self.conv2(x)\n        shape = x21.size()\n        x21 = self.bn_list2[0](x21) * att2[:, 0:1, :, :, :].expand(shape)\n\n        for i in range(1, self.num_branches):\n            x22 = F.conv3d(x, self.conv2.weight, padding=self.padding_list[i], dilation=self.dilation_list[i])\n            x22 = self.bn_list2[i](x22)\n            x21 += x22 * att2[:, i:(i + 1), :, :, :].expand(shape)\n\n        if self.downsample is not None:\n            residual = self.downsample(residual)\n\n        x = x21 + residual\n        x = self.relu(x)\n        return x\n\n\nclass Basic(nn.Module):\n    def __init__(self, channels, kernel_size):\n        super(Basic, self).__init__()\n\n        # parameters in the architecture\n        self.channels = channels\n        self.kernel_size = kernel_size\n\n        # network architecture\n        self.conv1 = nn.Conv3d(self.channels[0], self.channels[1], kernel_size=self.kernel_size)\n        self.bn1 = nn.BatchNorm3d(self.channels[1])\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv3d(self.channels[1], self.channels[2], kernel_size=self.kernel_size)\n        self.bn2 = nn.BatchNorm3d(self.channels[2])\n\n        self.layer3 = BasicBlock(self.channels[2], self.channels[3], self.channels[4])\n        self.layer4 = BasicBlock(self.channels[4], self.channels[5], self.channels[6])\n        self.layer5 = BasicBlock(self.channels[6], self.channels[7], self.channels[8])\n\n        self.fc = nn.Conv3d(self.channels[8], self.channels[9], kernel_size=1)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                m.weight.data.normal_(0, 0.01)\n            elif isinstance(m, nn.BatchNorm3d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n\n        x = self.fc(x)\n        return x\n\n\nclass ASPP_c(nn.Module):\n    def __init__(self, dilation_list, channels, kernel_size, num_branches):\n        super(ASPP_c, self).__init__()\n\n        # parameters in the architecture\n        channels.insert(-1, 30)\n        self.channels = channels\n        self.kernel_size = kernel_size\n        self.padding_list = dilation_list\n        self.dilation_list = dilation_list\n        self.num_branches = num_branches\n\n        # network architecture\n        self.conv1 = nn.Conv3d(self.channels[0], self.channels[1], kernel_size=self.kernel_size)\n        self.bn1 = nn.BatchNorm3d(self.channels[1])\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv3d(self.channels[1], self.channels[2], kernel_size=self.kernel_size)\n        self.bn2 = nn.BatchNorm3d(self.channels[2])\n\n        self.layer3 = BasicBlock(self.channels[2], self.channels[3], self.channels[4])\n        self.layer4 = BasicBlock(self.channels[4], self.channels[5], self.channels[6])\n        self.layer5 = BasicBlock(self.channels[6], self.channels[7], self.channels[8])\n\n        self.aspp = nn.ModuleList()\n        for i in range(self.num_branches):\n            self.aspp.append(nn.Conv3d(self.channels[8], self.channels[9], kernel_size=self.kernel_size, padding=self.padding_list[i], dilation=self.dilation_list[i]))\n\n        self.bn9 = nn.BatchNorm3d(4 * self.channels[9])\n        self.fc9 = nn.Conv3d(4 * self.channels[9], self.channels[10], kernel_size=1)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                m.weight.data.normal_(0, 0.01)\n            elif isinstance(m, nn.BatchNorm3d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n\n        aspp_out = []\n        for aspp_scale in self.aspp:\n            aspp_out.append(aspp_scale(x))\n        aspp_out = torch.cat(aspp_out, 1)\n\n        out = self.bn9(aspp_out)\n        out = self.relu(out)\n        out = self.fc9(out)\n        return out\n\n\nclass ASPP_s(nn.Module):\n    def __init__(self, dilation_list, channels, kernel_size, num_branches):\n        super(ASPP_s, self).__init__()\n\n        # parameters in the architecture\n        channels.insert(-1, 120)\n        self.channels = channels\n        self.kernel_size = kernel_size\n        self.padding_list = dilation_list\n        self.dilation_list = dilation_list\n        self.num_branches = num_branches\n\n        # network architecture\n        self.conv1 = nn.Conv3d(self.channels[0], self.channels[1], kernel_size=self.kernel_size)\n        self.bn1 = nn.BatchNorm3d(self.channels[1])\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv3d(self.channels[1], self.channels[2], kernel_size=self.kernel_size)\n        self.bn2 = nn.BatchNorm3d(self.channels[2])\n\n        self.layer3 = BasicBlock(self.channels[2], self.channels[3], self.channels[4])\n        self.layer4 = BasicBlock(self.channels[4], self.channels[5], self.channels[6])\n        self.layer5 = BasicBlock(self.channels[6], self.channels[7], self.channels[8])\n\n        self.conv_list = nn.ModuleList()\n        self.bn_list = nn.ModuleList()\n        self.last_list = nn.ModuleList()\n        for i in range(self.num_branches):\n            self.conv_list.append(nn.Conv3d(self.channels[8], self.channels[9], kernel_size=self.kernel_size, padding=self.padding_list[i], dilation=self.dilation_list[i]))\n            self.bn_list.append(nn.BatchNorm3d(self.channels[9]))\n            self.last_list.append(nn.Conv3d(self.channels[9], self.channels[10], kernel_size=1))\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                m.weight.data.normal_(0, 0.01)\n            elif isinstance(m, nn.BatchNorm3d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n\n        out = self.conv_list[0](x)\n        out = self.relu(self.bn_list[0](out))\n        out = self.last_list[0](out)\n        for i in range(1, self.num_branches):\n            out1 = self.conv_list[i](x)\n            out1 = self.relu(self.bn_list[i](out1))\n            out += self.last_list[i](out1)\n\n        return out\n\n\nclass AFN(nn.Module):\n    def __init__(self, blocks, padding_list, dilation_list, channels, kernel_size, num_branches):\n        super(AFN, self).__init__()\n\n        # parameters in the architecture\n        self.channels = channels\n        self.kernel_size = kernel_size\n        self.padding_list = padding_list\n        self.dilation_list = dilation_list\n        self.num_branches = num_branches\n        self.blocks = blocks\n\n        # network architecture\n        self.conv1 = nn.Conv3d(self.channels[0], self.channels[1], kernel_size=self.kernel_size)\n        self.bn1 = nn.BatchNorm3d(self.channels[1])\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv3d(self.channels[1], self.channels[2], kernel_size=self.kernel_size)\n        self.bn2 = nn.BatchNorm3d(self.channels[2])\n\n        self.layers = nn.ModuleList()\n        for i in range(len(blocks)):\n            block = blocks[i]\n            index = int(2 * i + 2)\n            if block == BasicBlock:\n                self.layers.append(block(self.channels[index], self.channels[index + 1], self.channels[index + 2]))\n            else:\n                self.layers.append(block(self.channels[index], self.channels[index + 1], self.channels[index + 2], self.padding_list, self.dilation_list, self.num_branches))\n\n        self.fc = nn.Conv3d(self.channels[8], self.channels[9], kernel_size=1)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                m.weight.data.normal_(0, 0.01)\n            elif isinstance(m, nn.BatchNorm3d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n\n        for layer in self.layers:\n            x = layer(x)\n\n        x = self.fc(x)\n        return x\n\n\ndef get_AFN(num_classes=1, arch=\'AFN4\', **kwargs):\n    return ModelBuilder().build_net(arch=arch, num_classes=num_classes, **kwargs)\n\ndef AFN_ASPP_c(num_classes=1, **kwargs):\n    return get_AFN(arch=\'ASPP_c\', num_classes=num_classes, **kwargs)\n\ndef AFN_ASPP_s(num_classes=1, **kwargs):\n    return get_AFN(arch=\'ASPP_s\', num_classes=num_classes, **kwargs)\n\ndef AFN1(num_classes=1, **kwargs):\n    return get_AFN(arch=\'AFN1\', num_classes=num_classes, **kwargs)\n\ndef AFN2(num_classes=1, **kwargs):\n    return get_AFN(arch=\'AFN2\', num_classes=num_classes, **kwargs)\n\ndef AFN3(num_classes=1, **kwargs):\n    return get_AFN(arch=\'AFN3\', num_classes=num_classes, **kwargs)\n\ndef AFN4(num_classes=1, **kwargs):\n    return get_AFN(arch=\'AFN4\', num_classes=num_classes, **kwargs)\n\ndef AFN5(num_classes=1, **kwargs):\n    return get_AFN(arch=\'AFN5\', num_classes=num_classes, **kwargs)\n\ndef AFN6(num_classes=1, **kwargs):\n    return get_AFN(arch=\'AFN6\', num_classes=num_classes, **kwargs)\n'"
pywick/models/segmentation/testnets/dabnet.py,6,"b'# Source: https://github.com/Reagan1311/DABNet (MIT)\n\n""""""\nDABNet - `Depth-wise Asymmetric Bottleneck for Real-time Semantic Segmentation <https://arxiv.org/abs/1907.11357>`_.\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n__all__ = [""DABNet""]\n\n\nclass Conv(nn.Module):\n    def __init__(self, nIn, nOut, kSize, stride, padding, dilation=(1, 1), groups=1, bn_acti=False, bias=False):\n        super().__init__()\n\n        self.bn_acti = bn_acti\n\n        self.conv = nn.Conv2d(nIn, nOut, kernel_size=kSize,\n                              stride=stride, padding=padding,\n                              dilation=dilation, groups=groups, bias=bias)\n\n        if self.bn_acti:\n            self.bn_prelu = BNPReLU(nOut)\n\n    def forward(self, input):\n        output = self.conv(input)\n\n        if self.bn_acti:\n            output = self.bn_prelu(output)\n\n        return output\n\n\nclass BNPReLU(nn.Module):\n    def __init__(self, nIn):\n        super().__init__()\n        self.bn = nn.BatchNorm2d(nIn, eps=1e-3)\n        self.acti = nn.PReLU(nIn)\n\n    def forward(self, input):\n        output = self.bn(input)\n        output = self.acti(output)\n\n        return output\n\n\nclass DABModule(nn.Module):\n    def __init__(self, nIn, d=1, kSize=3, dkSize=3):\n        super().__init__()\n\n        self.bn_relu_1 = BNPReLU(nIn)\n        self.conv3x3 = Conv(nIn, nIn // 2, kSize, 1, padding=1, bn_acti=True)\n\n        self.dconv3x1 = Conv(nIn // 2, nIn // 2, (dkSize, 1), 1,\n                             padding=(1, 0), groups=nIn // 2, bn_acti=True)\n        self.dconv1x3 = Conv(nIn // 2, nIn // 2, (1, dkSize), 1,\n                             padding=(0, 1), groups=nIn // 2, bn_acti=True)\n        self.ddconv3x1 = Conv(nIn // 2, nIn // 2, (dkSize, 1), 1,\n                              padding=(1 * d, 0), dilation=(d, 1), groups=nIn // 2, bn_acti=True)\n        self.ddconv1x3 = Conv(nIn // 2, nIn // 2, (1, dkSize), 1,\n                              padding=(0, 1 * d), dilation=(1, d), groups=nIn // 2, bn_acti=True)\n\n        self.bn_relu_2 = BNPReLU(nIn // 2)\n        self.conv1x1 = Conv(nIn // 2, nIn, 1, 1, padding=0, bn_acti=False)\n\n    def forward(self, input):\n        output = self.bn_relu_1(input)\n        output = self.conv3x3(output)\n\n        br1 = self.dconv3x1(output)\n        br1 = self.dconv1x3(br1)\n        br2 = self.ddconv3x1(output)\n        br2 = self.ddconv1x3(br2)\n\n        output = br1 + br2\n        output = self.bn_relu_2(output)\n        output = self.conv1x1(output)\n\n        return output + input\n\n\nclass DownSamplingBlock(nn.Module):\n    def __init__(self, nIn, nOut):\n        super().__init__()\n        self.nIn = nIn\n        self.nOut = nOut\n\n        if self.nIn < self.nOut:\n            nConv = nOut - nIn\n        else:\n            nConv = nOut\n\n        self.conv3x3 = Conv(nIn, nConv, kSize=3, stride=2, padding=1)\n        self.max_pool = nn.MaxPool2d(2, stride=2)\n        self.bn_prelu = BNPReLU(nOut)\n\n    def forward(self, input):\n        output = self.conv3x3(input)\n\n        if self.nIn < self.nOut:\n            max_pool = self.max_pool(input)\n            output = torch.cat([output, max_pool], 1)\n\n        output = self.bn_prelu(output)\n\n        return output\n\n\nclass InputInjection(nn.Module):\n    def __init__(self, ratio):\n        super().__init__()\n        self.pool = nn.ModuleList()\n        for i in range(0, ratio):\n            self.pool.append(nn.AvgPool2d(3, stride=2, padding=1))\n\n    def forward(self, input):\n        for pool in self.pool:\n            input = pool(input)\n\n        return input\n\n\nclass DABNet(nn.Module):\n    def __init__(self, num_classes=19, block_1=3, block_2=6, **kwargs):\n        super().__init__()\n        self.init_conv = nn.Sequential(\n            Conv(3, 32, 3, 2, padding=1, bn_acti=True),\n            Conv(32, 32, 3, 1, padding=1, bn_acti=True),\n            Conv(32, 32, 3, 1, padding=1, bn_acti=True),\n        )\n\n        self.down_1 = InputInjection(1)  # down-sample the image 1 times\n        self.down_2 = InputInjection(2)  # down-sample the image 2 times\n        self.down_3 = InputInjection(3)  # down-sample the image 3 times\n\n        self.bn_prelu_1 = BNPReLU(32 + 3)\n\n        # DAB Block 1\n        self.downsample_1 = DownSamplingBlock(32 + 3, 64)\n        self.DAB_Block_1 = nn.Sequential()\n        for i in range(0, block_1):\n            self.DAB_Block_1.add_module(""DAB_Module_1_"" + str(i), DABModule(64, d=2))\n        self.bn_prelu_2 = BNPReLU(128 + 3)\n\n        # DAB Block 2\n        dilation_block_2 = [4, 4, 8, 8, 16, 16]\n        self.downsample_2 = DownSamplingBlock(128 + 3, 128)\n        self.DAB_Block_2 = nn.Sequential()\n        for i in range(0, block_2):\n            self.DAB_Block_2.add_module(""DAB_Module_2_"" + str(i),\n                                        DABModule(128, d=dilation_block_2[i]))\n        self.bn_prelu_3 = BNPReLU(256 + 3)\n\n        self.classifier = nn.Sequential(Conv(259, num_classes, 1, 1, padding=0))\n\n    def forward(self, input):\n\n        output0 = self.init_conv(input)\n\n        down_1 = self.down_1(input)\n        down_2 = self.down_2(input)\n        down_3 = self.down_3(input)\n\n        output0_cat = self.bn_prelu_1(torch.cat([output0, down_1], 1))\n\n        # DAB Block 1\n        output1_0 = self.downsample_1(output0_cat)\n        output1 = self.DAB_Block_1(output1_0)\n        output1_cat = self.bn_prelu_2(torch.cat([output1, output1_0, down_2], 1))\n\n        # DAB Block 2\n        output2_0 = self.downsample_2(output1_cat)\n        output2 = self.DAB_Block_2(output2_0)\n        output2_cat = self.bn_prelu_3(torch.cat([output2, output2_0, down_3], 1))\n\n        out = self.classifier(output2_cat)\n        out = F.interpolate(out, input.size()[2:], mode=\'bilinear\', align_corners=False)\n\n        return out\n'"
pywick/models/segmentation/testnets/deeplabv2.py,3,"b'#!/usr/bin/env python\n# coding: utf-8\n#\n# Author:   Kazuto Nakashima\n# URL:      http://kazuto1011.github.io\n# Created:  2017-11-19\n\n# Source: https://github.com/kazuto1011/deeplab-pytorch/tree/master/libs/models\n\n""""""\nDeepLab v2 - `DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs <https://arxiv.org/abs/1606.00915>`_.\n""""""\n\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .resnet import _ConvBatchNormReLU, _ResBlock\nfrom .msc import MSC\n\n\ndef DeepLabV2_ResNet101_MSC(n_classes):\n    return MSC(\n        scale=DeepLabV2(\n            num_classes=n_classes, n_blocks=[3, 4, 23, 3], pyramids=[6, 12, 18, 24]\n        ),\n        pyramids=[0.5, 0.75],\n)\n\nclass _ASPPModule(nn.Module):\n    """"""Atrous Spatial Pyramid Pooling""""""\n\n    def __init__(self, in_channels, out_channels, pyramids):\n        super(_ASPPModule, self).__init__()\n        self.stages = nn.Module()\n        for i, (dilation, padding) in enumerate(zip(pyramids, pyramids)):\n            self.stages.add_module(\n                ""c{}"".format(i),\n                nn.Conv2d(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    kernel_size=3,\n                    stride=1,\n                    padding=padding,\n                    dilation=dilation,\n                    bias=True,\n                ),\n            )\n\n        for m in self.stages.children():\n            nn.init.normal_(m.weight, mean=0, std=0.01)\n            nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        h = 0\n        for stage in self.stages.children():\n            h += stage(x)\n        return h\n\n\nclass DeepLabV2(nn.Sequential):\n\n    def __init__(self, num_classes, n_blocks, pyramids, **kwargs):\n        super(DeepLabV2, self).__init__()\n        self.add_module(\n            ""layer1"",\n            nn.Sequential(\n                OrderedDict(\n                    [\n                        (""conv1"", _ConvBatchNormReLU(3, 64, 7, 2, 3, 1)),\n                        (""pool"", nn.MaxPool2d(3, 2, 1, ceil_mode=True)),\n                    ]\n                )\n            ),\n        )\n        self.add_module(""layer2"", _ResBlock(n_blocks[0], 64, 64, 256, 1, 1))\n        self.add_module(""layer3"", _ResBlock(n_blocks[1], 256, 128, 512, 2, 1))\n        self.add_module(""layer4"", _ResBlock(n_blocks[2], 512, 256, 1024, 1, 2))\n        self.add_module(""layer5"", _ResBlock(n_blocks[3], 1024, 512, 2048, 1, 4))\n        self.add_module(""aspp"", _ASPPModule(2048, num_classes, pyramids))\n\n    def forward(self, x):\n        # return super(DeepLabV2, self).forward(x)\n        logits = super(DeepLabV2, self).forward(x)\n        logits = F.interpolate(logits, size=x.shape[2:], mode=""bilinear"", align_corners=True)\n        return logits\n\n    def freeze_bn(self):\n        for m in self.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                m.eval()\n\n\nif __name__ == ""__main__"":\n    model = DeepLabV2(num_classes=21, n_blocks=[3, 4, 23, 3], pyramids=[6, 12, 18, 24])\n    model.freeze_bn()\n    model.eval()\n    print(list(model.named_children()))\n    image = torch.randn(1, 3, 513, 513)\n    print(model(image)[0].size())\n'"
pywick/models/segmentation/testnets/deeplabv3.py,3,"b'#!/usr/bin/env python\n# coding: utf-8\n#\n# Author:   Kazuto Nakashima\n# URL:      http://kazuto1011.github.io\n# Created:  2018-03-26\n\n# Source: https://github.com/kazuto1011/deeplab-pytorch/tree/master/libs/models\n\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .resnet import _ConvBatchNormReLU, _ResBlock\nfrom .msc import MSC\n\n\ndef init_weights(model):\n    for m in model.modules():\n        if isinstance(m, nn.Conv2d):\n            nn.init.kaiming_normal_(m.weight)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.Linear):\n            nn.init.kaiming_normal_(m.weight)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.BatchNorm2d):\n            nn.init.constant_(m.weight, 1)\n            if m.bias is not None:\n                nn.init.constant_(m.weight, 1)\n\n\ndef DeepLabV3_ResNet101_MSC(n_classes, output_stride):\n    if output_stride == 16:\n        pyramids = [6, 12, 18]\n    elif output_stride == 8:\n        pyramids = [12, 24, 36]\n    else:\n        NotImplementedError\n\n    return MSC(\n        scale=DeepLabV3(\n            n_classes=n_classes,\n            n_blocks=[3, 4, 23, 3],\n            pyramids=pyramids,\n            grids=[1, 2, 4],\n            output_stride=output_stride,\n        ),\n        pyramids=[0.5, 0.75],\n    )\n\nclass _ASPPModule(nn.Module):\n    """"""Atrous Spatial Pyramid Pooling with image pool""""""\n\n    def __init__(self, in_channels, out_channels, pyramids):\n        super(_ASPPModule, self).__init__()\n        self.stages = nn.Module()\n        self.stages.add_module(\n            ""c0"", _ConvBatchNormReLU(in_channels, out_channels, 1, 1, 0, 1)\n        )\n        for i, (dilation, padding) in enumerate(zip(pyramids, pyramids)):\n            self.stages.add_module(\n                ""c{}"".format(i + 1),\n                _ConvBatchNormReLU(in_channels, out_channels, 3, 1, padding, dilation),\n            )\n        self.imagepool = nn.Sequential(\n            OrderedDict(\n                [\n                    (""pool"", nn.AdaptiveAvgPool2d(1)),\n                    (""conv"", _ConvBatchNormReLU(in_channels, out_channels, 1, 1, 0, 1)),\n                ]\n            )\n        )\n\n    def forward(self, x):\n        h = self.imagepool(x)\n        h = [F.interpolate(h, size=x.shape[2:], mode=""bilinear"")]\n        for stage in self.stages.children():\n            h += [stage(x)]\n        h = torch.cat(h, dim=1)\n        return h\n\n\nclass DeepLabV3(nn.Sequential):\n    """"""DeepLab v3""""""\n\n    def __init__(self, n_classes, n_blocks, pyramids, grids, output_stride):\n        super(DeepLabV3, self).__init__()\n\n        if output_stride == 8:\n            stride = [1, 2, 1, 1]\n            dilation = [1, 1, 2, 2]\n        elif output_stride == 16:\n            stride = [1, 2, 2, 1]\n            dilation = [1, 1, 1, 2]\n\n        self.add_module(\n            ""layer1"",\n            nn.Sequential(\n                OrderedDict(\n                    [\n                        (""conv1"", _ConvBatchNormReLU(3, 64, 7, 2, 3, 1)),\n                        (""pool"", nn.MaxPool2d(3, 2, 1, ceil_mode=True)),\n                    ]\n                )\n            ),\n        )\n        self.add_module(\n            ""layer2"", _ResBlock(n_blocks[0], 64, 64, 256, stride[0], dilation[0])\n        )\n        self.add_module(\n            ""layer3"", _ResBlock(n_blocks[1], 256, 128, 512, stride[1], dilation[1])\n        )\n        self.add_module(\n            ""layer4"", _ResBlock(n_blocks[2], 512, 256, 1024, stride[2], dilation[2])\n        )\n        self.add_module(\n            ""layer5"",\n            _ResBlock(n_blocks[3], 1024, 512, 2048, stride[3], dilation[3], mg=grids),\n        )\n        self.add_module(""aspp"", _ASPPModule(2048, 256, pyramids))\n        self.add_module(""fc1"", _ConvBatchNormReLU(256 * (len(pyramids) + 2), 256, 1, 1, 0, 1))\n        self.add_module(""fc2"", nn.Conv2d(256, n_classes, kernel_size=1))\n\n    def forward(self, x):\n        # return super(DeepLabV3, self).forward(x)\n        logits = super(DeepLabV3, self).forward(x)\n        logits = F.interpolate(logits, size=x.shape[2:], mode=""bilinear"", align_corners=True)\n        return logits\n\n    def freeze_bn(self):\n        for m in self.named_modules():\n            if isinstance(m[1], nn.BatchNorm2d):\n                m[1].eval()\n\n'"
pywick/models/segmentation/testnets/deeplabv3_resnet.py,11,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, rate=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               dilation=rate, padding=rate, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.rate = rate\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass ResNet(nn.Module):\n\n    def __init__(self, nInputChannels, block, layers, os=16, pretrained=False):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        if os == 16:\n            strides = [1, 2, 2, 1]\n            rates = [1, 1, 1, 2]\n            blocks = [1, 2, 4]\n        elif os == 8:\n            strides = [1, 2, 1, 1]\n            rates = [1, 1, 2, 2]\n            blocks = [1, 2, 1]\n        else:\n            raise NotImplementedError\n\n        # Modules\n        self.conv1 = nn.Conv2d(nInputChannels, 64, kernel_size=7, stride=2, padding=3,\n                                bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(block, 64, layers[0], stride=strides[0], rate=rates[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=strides[1], rate=rates[1])\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=strides[2], rate=rates[2])\n        self.layer4 = self._make_MG_unit(block, 512, blocks=blocks, stride=strides[3], rate=rates[3])\n\n        self._init_weight()\n\n        if pretrained:\n            self._load_pretrained_model()\n\n    def _make_layer(self, block, planes, blocks, stride=1, rate=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, rate, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def _make_MG_unit(self, block, planes, blocks=[1,2,4], stride=1, rate=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, rate=blocks[0]*rate, downsample=downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, len(blocks)):\n            layers.append(block(self.inplanes, planes, stride=1, rate=blocks[i]*rate))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, input):\n        x = self.conv1(input)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        low_level_feat = x\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x, low_level_feat\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                # m.weight.data.normal_(0, math.sqrt(2. / n))\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _load_pretrained_model(self):\n        pretrain_dict = model_zoo.load_url(\'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\')\n        model_dict = {}\n        state_dict = self.state_dict()\n        for k, v in pretrain_dict.items():\n            if k in state_dict:\n                model_dict[k] = v\n        state_dict.update(model_dict)\n        self.load_state_dict(state_dict)\n\ndef ResNet101(nInputChannels=3, os=16, pretrained=False):\n    model = ResNet(nInputChannels, Bottleneck, [3, 4, 23, 3], os, pretrained=pretrained)\n    return model\n\n\nclass ASPP_module(nn.Module):\n    def __init__(self, inplanes, planes, rate):\n        super(ASPP_module, self).__init__()\n        if rate == 1:\n            kernel_size = 1\n            padding = 0\n        else:\n            kernel_size = 3\n            padding = rate\n        self.atrous_convolution = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,\n                                            stride=1, padding=padding, dilation=rate, bias=False)\n        self.bn = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU()\n\n        self._init_weight()\n\n    def forward(self, x):\n        x = self.atrous_convolution(x)\n        x = self.bn(x)\n\n        return self.relu(x)\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                # m.weight.data.normal_(0, math.sqrt(2. / n))\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n\nclass DeepLabv3_plus(nn.Module):\n    def __init__(self, num_classes=21, pretrained=False, nInputChannels=3, os=16, _print=True, **kwargs):\n        if _print:\n            print(""Constructing DeepLabv3+ model..."")\n            print(""Number of classes: {}"".format(num_classes))\n            print(""Output stride: {}"".format(os))\n            print(""Number of Input Channels: {}"".format(nInputChannels))\n        super(DeepLabv3_plus, self).__init__()\n\n        # Atrous Conv\n        self.resnet_features = ResNet101(nInputChannels, os, pretrained=pretrained)\n\n        # ASPP\n        if os == 16:\n            rates = [1, 6, 12, 18]\n        elif os == 8:\n            rates = [1, 12, 24, 36]\n        else:\n            raise NotImplementedError\n\n        self.aspp1 = ASPP_module(2048, 256, rate=rates[0])\n        self.aspp2 = ASPP_module(2048, 256, rate=rates[1])\n        self.aspp3 = ASPP_module(2048, 256, rate=rates[2])\n        self.aspp4 = ASPP_module(2048, 256, rate=rates[3])\n\n        self.relu = nn.ReLU()\n\n        self.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n                                             nn.Conv2d(2048, 256, 1, stride=1, bias=False),\n                                             nn.BatchNorm2d(256),\n                                             nn.ReLU())\n\n        self.conv1 = nn.Conv2d(1280, 256, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(256)\n\n        # adopt [1x1, 48] for channel reduction.\n        self.conv2 = nn.Conv2d(256, 48, 1, bias=False)\n        self.bn2 = nn.BatchNorm2d(48)\n\n        self.last_linear = nn.Sequential(nn.Conv2d(304, 256, kernel_size=3, stride=1, padding=1, bias=False),\n                                         nn.BatchNorm2d(256),\n                                         nn.ReLU(),\n                                         nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),\n                                         nn.BatchNorm2d(256),\n                                         nn.ReLU(),\n                                         nn.Conv2d(256, num_classes, kernel_size=1, stride=1))\n\n    def forward(self, input):\n        x, low_level_features = self.resnet_features(input)\n        x1 = self.aspp1(x)\n        x2 = self.aspp2(x)\n        x3 = self.aspp3(x)\n        x4 = self.aspp4(x)\n        x5 = self.global_avg_pool(x)\n        x5 = F.upsample(x5, size=x4.size()[2:], mode=\'bilinear\', align_corners=True)\n\n        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = F.upsample(x, size=(int(math.ceil(input.size()[-2]/4)),\n                                int(math.ceil(input.size()[-1]/4))), mode=\'bilinear\', align_corners=True)\n\n        low_level_features = self.conv2(low_level_features)\n        low_level_features = self.bn2(low_level_features)\n        low_level_features = self.relu(low_level_features)\n\n\n        x = torch.cat((x, low_level_features), dim=1)\n        x = self.last_linear(x)\n        x = F.upsample(x, size=input.size()[2:], mode=\'bilinear\', align_corners=True)\n\n        return x\n\n    def freeze_bn(self):\n        for m in self.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                m.eval()\n\n    def __init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                # m.weight.data.normal_(0, math.sqrt(2. / n))\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\ndef get_1x_lr_params(model):\n    """"""\n    This generator returns all the parameters of the net except for\n    the last classification layer. Note that for each batchnorm layer,\n    requires_grad is set to False in deeplab_resnet.py, therefore this function does not return\n    any batchnorm parameter\n    """"""\n    b = [model.resnet_features]\n    for i in range(len(b)):\n        for k in b[i].parameters():\n            if k.requires_grad:\n                yield k\n\n\ndef get_10x_lr_params(model):\n    """"""\n    This generator returns all the parameters for the last layer of the net,\n    which does the classification of pixel into classes\n    """"""\n    b = [model.aspp1, model.aspp2, model.aspp3, model.aspp4, model.conv1, model.conv2, model.last_conv]\n    for j in range(len(b)):\n        for k in b[j].parameters():\n            if k.requires_grad:\n                yield k\n\ndef create_DLR_V3_pretrained(num_classes=1, **kwargs):\n    """"""\n    Creates a pretrained version of the DeepLab Resnet model but substitutes num_classes for output instead of default 21.\n    :param num_classes:\n    :return:\n    """"""\n    model = DeepLabv3_plus(num_classes=21, pretrained=True, os=8, nInputChannels=3, print=True, **kwargs)\n    last_linear = nn.Sequential(nn.Conv2d(304, 256, kernel_size=3, stride=1, padding=1, bias=False),\n                                     nn.BatchNorm2d(256),\n                                     nn.ReLU(),\n                                     nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),\n                                     nn.BatchNorm2d(256),\n                                     nn.ReLU(),\n                                     nn.Conv2d(256, num_classes, kernel_size=1, stride=1))\n    model.last_linear = last_linear\n\nif __name__ == ""__main__"":\n    model = DeepLabv3_plus(nInputChannels=3, num_classes=21, os=16, pretrained=True, _print=True)\n    model.eval()\n    image = torch.randn(1, 3, 512, 512)\n    with torch.no_grad():\n        output = model.forward(image)\n    print(output.size())\n\n\n\n\n\n\n'"
pywick/models/segmentation/testnets/deeplabv3_xception.py,10,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\n\n\n\n\nclass SeparableConv2d(nn.Module):\n    def __init__(self, inplanes, planes, kernel_size=3, stride=1, padding=0, dilation=1, bias=False):\n        super(SeparableConv2d, self).__init__()\n\n        self.conv1 = nn.Conv2d(inplanes, inplanes, kernel_size, stride, padding, dilation,\n                               groups=inplanes, bias=bias)\n        self.pointwise = nn.Conv2d(inplanes, planes, 1, 1, 0, 1, 1, bias=bias)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.pointwise(x)\n        return x\n\n\ndef fixed_padding(inputs, kernel_size, rate):\n    kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n    pad_total = kernel_size_effective - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n    padded_inputs = F.pad(inputs, (pad_beg, pad_end, pad_beg, pad_end))\n    return padded_inputs\n\n\nclass SeparableConv2d_same(nn.Module):\n    def __init__(self, inplanes, planes, kernel_size=3, stride=1, dilation=1, bias=False):\n        super(SeparableConv2d_same, self).__init__()\n\n        self.conv1 = nn.Conv2d(inplanes, inplanes, kernel_size, stride, 0, dilation,\n                               groups=inplanes, bias=bias)\n        self.pointwise = nn.Conv2d(inplanes, planes, 1, 1, 0, 1, 1, bias=bias)\n\n    def forward(self, x):\n        x = fixed_padding(x, self.conv1.kernel_size[0], rate=self.conv1.dilation[0])\n        x = self.conv1(x)\n        x = self.pointwise(x)\n        return x\n\n\nclass Block(nn.Module):\n    def __init__(self, inplanes, planes, reps, stride=1, dilation=1, start_with_relu=True, grow_first=True, is_last=False):\n        super(Block, self).__init__()\n\n        if planes != inplanes or stride != 1:\n            self.skip = nn.Conv2d(inplanes, planes, 1, stride=stride, bias=False)\n            self.skipbn = nn.BatchNorm2d(planes)\n        else:\n            self.skip = None\n\n        self.relu = nn.ReLU(inplace=True)\n        rep = []\n\n        filters = inplanes\n        if grow_first:\n            rep.append(self.relu)\n            rep.append(SeparableConv2d_same(inplanes, planes, 3, stride=1, dilation=dilation))\n            rep.append(nn.BatchNorm2d(planes))\n            filters = planes\n\n        for i in range(reps - 1):\n            rep.append(self.relu)\n            rep.append(SeparableConv2d_same(filters, filters, 3, stride=1, dilation=dilation))\n            rep.append(nn.BatchNorm2d(filters))\n\n        if not grow_first:\n            rep.append(self.relu)\n            rep.append(SeparableConv2d_same(inplanes, planes, 3, stride=1, dilation=dilation))\n            rep.append(nn.BatchNorm2d(planes))\n\n        if not start_with_relu:\n            rep = rep[1:]\n\n        if stride != 1:\n            rep.append(SeparableConv2d_same(planes, planes, 3, stride=2))\n\n        if is_last:\n            rep.append(SeparableConv2d_same(planes, planes, 3, stride=1))\n\n\n        self.rep = nn.Sequential(*rep)\n\n    def forward(self, inp):\n        x = self.rep(inp)\n\n        if self.skip is not None:\n            skip = self.skip(inp)\n            skip = self.skipbn(skip)\n        else:\n            skip = inp\n\n        x += skip\n\n        return x\n\n\nclass Xception(nn.Module):\n    """"""\n    Modified Alighed Xception\n    """"""\n    def __init__(self, inplanes=3, os=16, pretrained=False):\n        super(Xception, self).__init__()\n\n        if os == 16:\n            entry_block3_stride = 2\n            middle_block_rate = 1\n            exit_block_rates = (1, 2)\n        elif os == 8:\n            entry_block3_stride = 1\n            middle_block_rate = 2\n            exit_block_rates = (2, 4)\n        else:\n            raise NotImplementedError\n\n\n        # Entry flow\n        self.conv1 = nn.Conv2d(inplanes, 32, 3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(64)\n\n        self.block1 = Block(64, 128, reps=2, stride=2, start_with_relu=False)\n        self.block2 = Block(128, 256, reps=2, stride=2, start_with_relu=True, grow_first=True)\n        self.block3 = Block(256, 728, reps=2, stride=entry_block3_stride, start_with_relu=True, grow_first=True)\n\n        # Middle flow\n        self.block4  = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n        self.block5  = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n        self.block6  = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n        self.block7  = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n        self.block8  = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n        self.block9  = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n        self.block10 = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n        self.block11 = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n        self.block12 = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n        self.block13 = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n        self.block14 = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n        self.block15 = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n        self.block16 = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n        self.block17 = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n        self.block18 = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n        self.block19 = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n\n        # Exit flow\n        self.block20 = Block(728, 1024, reps=2, stride=1, dilation=exit_block_rates[0],\n                             start_with_relu=True, grow_first=False, is_last=True)\n\n        self.conv3 = SeparableConv2d_same(1024, 1536, 3, stride=1, dilation=exit_block_rates[1])\n        self.bn3 = nn.BatchNorm2d(1536)\n\n        self.conv4 = SeparableConv2d_same(1536, 1536, 3, stride=1, dilation=exit_block_rates[1])\n        self.bn4 = nn.BatchNorm2d(1536)\n\n        self.conv5 = SeparableConv2d_same(1536, 2048, 3, stride=1, dilation=exit_block_rates[1])\n        self.bn5 = nn.BatchNorm2d(2048)\n\n        # Init weights\n        self.__init_weight()\n\n        # Load pretrained model\n        if pretrained:\n            self.__load_xception_pretrained()\n\n    def forward(self, x):\n        # Entry flow\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n\n        x = self.block1(x)\n        low_level_feat = x\n        x = self.block2(x)\n        x = self.block3(x)\n\n        # Middle flow\n        x = self.block4(x)\n        x = self.block5(x)\n        x = self.block6(x)\n        x = self.block7(x)\n        x = self.block8(x)\n        x = self.block9(x)\n        x = self.block10(x)\n        x = self.block11(x)\n        x = self.block12(x)\n        x = self.block13(x)\n        x = self.block14(x)\n        x = self.block15(x)\n        x = self.block16(x)\n        x = self.block17(x)\n        x = self.block18(x)\n        x = self.block19(x)\n\n        # Exit flow\n        x = self.block20(x)\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.relu(x)\n\n        x = self.conv4(x)\n        x = self.bn4(x)\n        x = self.relu(x)\n\n        x = self.conv5(x)\n        x = self.bn5(x)\n        x = self.relu(x)\n\n        return x, low_level_feat\n\n    def __init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                # m.weight.data.normal_(0, math.sqrt(2. / n))\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def __load_xception_pretrained(self):\n        # pretrain_dict = model_zoo.load_url(\'http://data.lip6.fr/cadene/pretrainedmodels/xception-b5690688.pth\')\n        pretrain_dict = model_zoo.load_url(\'http://data.lip6.fr/cadene/pretrainedmodels/xception-43020ad28.pth\')\n        model_dict = {}\n        state_dict = self.state_dict()\n\n        for k, v in pretrain_dict.items():\n            if k in state_dict:\n                # if \'pointwise\' in k:\n                #     v = v.unsqueeze(-1).unsqueeze(-1)\n                if k.startswith(\'block12\'):\n                    model_dict[k.replace(\'block12\', \'block20\')] = v\n                elif k.startswith(\'block11\'):\n                    model_dict[k.replace(\'block11\', \'block12\')] = v\n                    model_dict[k.replace(\'block11\', \'block13\')] = v\n                    model_dict[k.replace(\'block11\', \'block14\')] = v\n                    model_dict[k.replace(\'block11\', \'block15\')] = v\n                    model_dict[k.replace(\'block11\', \'block16\')] = v\n                    model_dict[k.replace(\'block11\', \'block17\')] = v\n                    model_dict[k.replace(\'block11\', \'block18\')] = v\n                    model_dict[k.replace(\'block11\', \'block19\')] = v\n                elif k.startswith(\'conv3\'):\n                    model_dict[k] = v\n                elif k.startswith(\'bn3\'):\n                    model_dict[k] = v\n                    model_dict[k.replace(\'bn3\', \'bn4\')] = v\n                elif k.startswith(\'conv4\'):\n                    model_dict[k.replace(\'conv4\', \'conv5\')] = v\n                elif k.startswith(\'bn4\'):\n                    model_dict[k.replace(\'bn4\', \'bn5\')] = v\n                else:\n                    model_dict[k] = v\n        state_dict.update(model_dict)\n        self.load_state_dict(state_dict)\n\nclass ASPP_module(nn.Module):\n    def __init__(self, inplanes, planes, rate):\n        super(ASPP_module, self).__init__()\n        if rate == 1:\n            kernel_size = 1\n            padding = 0\n        else:\n            kernel_size = 3\n            padding = rate\n        self.atrous_convolution = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,\n                                            stride=1, padding=padding, dilation=rate, bias=False)\n        self.bn = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU()\n\n        self.__init_weight()\n\n    def forward(self, x):\n        x = self.atrous_convolution(x)\n        x = self.bn(x)\n\n        return self.relu(x)\n\n    def __init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                # m.weight.data.normal_(0, math.sqrt(2. / n))\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n\nclass DeepLabv3_plus(nn.Module):\n    def __init__(self, num_classes=21, pretrained=False, nInputChannels=3, os=16, _print=True, **kwargs):\n        if _print:\n            print(""Constructing DeepLabv3+ model..."")\n            print(""Number of classes: {}"".format(num_classes))\n            print(""Output stride: {}"".format(os))\n            print(""Number of Input Channels: {}"".format(nInputChannels))\n        super(DeepLabv3_plus, self).__init__()\n\n        # Atrous Conv\n        self.xception_features = Xception(nInputChannels, os, pretrained)\n\n        # ASPP\n        if os == 16:\n            rates = [1, 6, 12, 18]\n        elif os == 8:\n            rates = [1, 12, 24, 36]\n        else:\n            raise NotImplementedError\n\n        self.aspp1 = ASPP_module(2048, 256, rate=rates[0])\n        self.aspp2 = ASPP_module(2048, 256, rate=rates[1])\n        self.aspp3 = ASPP_module(2048, 256, rate=rates[2])\n        self.aspp4 = ASPP_module(2048, 256, rate=rates[3])\n\n        self.relu = nn.ReLU()\n\n        self.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n                                             nn.Conv2d(2048, 256, 1, stride=1, bias=False),\n                                             nn.BatchNorm2d(256),\n                                             nn.ReLU()\n                                             )\n\n        self.conv1 = nn.Conv2d(1280, 256, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(256)\n\n        # adopt [1x1, 48] for channel reduction.\n        self.conv2 = nn.Conv2d(128, 48, 1, bias=False)\n        self.bn2 = nn.BatchNorm2d(48)\n\n        self.last_linear = nn.Sequential(nn.Conv2d(304, 256, kernel_size=3, stride=1, padding=1, bias=False),\n                                         nn.BatchNorm2d(256),\n                                         nn.ReLU(),\n                                         nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),\n                                         nn.BatchNorm2d(256),\n                                         nn.ReLU(),\n                                         nn.Conv2d(256, num_classes, kernel_size=1, stride=1))\n\n    def forward(self, input):\n        x, low_level_features = self.xception_features(input)\n        x1 = self.aspp1(x)\n        x2 = self.aspp2(x)\n        x3 = self.aspp3(x)\n        x4 = self.aspp4(x)\n        x5 = self.global_avg_pool(x)\n        x5 = F.interpolate(x5, size=x4.size()[2:], mode=\'bilinear\', align_corners=True)\n\n        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = F.interpolate(x, size=(int(math.ceil(input.size()[-2]/4)),\n                                int(math.ceil(input.size()[-1]/4))), mode=\'bilinear\', align_corners=True)\n\n        low_level_features = self.conv2(low_level_features)\n        low_level_features = self.bn2(low_level_features)\n        low_level_features = self.relu(low_level_features)\n\n\n        x = torch.cat((x, low_level_features), dim=1)\n        x = self.last_linear(x)\n        x = F.interpolate(x, size=input.size()[2:], mode=\'bilinear\', align_corners=True)\n\n        return x\n\n    def freeze_bn(self):\n        for m in self.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                m.eval()\n\n    def __init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                # torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\ndef get_1x_lr_params(model):\n    """"""\n    This generator returns all the parameters of the net except for\n    the last classification layer. Note that for each batchnorm layer,\n    requires_grad is set to False in deeplab_resnet.py, therefore this function does not return\n    any batchnorm parameter\n    """"""\n    b = [model.xception_features]\n    for i in range(len(b)):\n        for k in b[i].parameters():\n            if k.requires_grad:\n                yield k\n\n\ndef get_10x_lr_params(model):\n    """"""\n    This generator returns all the parameters for the last layer of the net,\n    which does the classification of pixel into classes\n    """"""\n    b = [model.aspp1, model.aspp2, model.aspp3, model.aspp4, model.conv1, model.conv2, model.last_conv]\n    for j in range(len(b)):\n        for k in b[j].parameters():\n            if k.requires_grad:\n                yield k\n\n\ndef create_DLX_V3_pretrained(num_classes=1, **kwargs):\n    """"""\n    Creates a pretrained version of the DeepLab Xception model but substitutes num_classes for output instead of default 21.\n    :param num_classes:\n    :return:\n    """"""\n    model = DeepLabv3_plus(num_classes=21, pretrained=True, nInputChannels=3, os=8, _print=True, **kwargs)\n    last_linear = nn.Sequential(nn.Conv2d(304, 256, kernel_size=3, stride=1, padding=1, bias=False),\n                                     nn.BatchNorm2d(256),\n                                     nn.ReLU(),\n                                     nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),\n                                     nn.BatchNorm2d(256),\n                                     nn.ReLU(),\n                                     nn.Conv2d(256, num_classes, kernel_size=1, stride=1))\n    model.last_linear = last_linear\n\nif __name__ == ""__main__"":\n    model = DeepLabv3_plus(nInputChannels=3, num_classes=21, os=8, pretrained=True, _print=True)\n    model.eval()\n    image = torch.randn(1, 3, 512, 512)\n    with torch.no_grad():\n        output = model.forward(image)\n    print(output.size())\n\n\n\n\n\n\n'"
pywick/models/segmentation/testnets/densenet_se_seg.py,6,"b""# Source: https://github.com/areum-lee/SENet_Segmentation/blob/master/2d_densenetse_model.py\n\nimport torch\nimport torch.functional as F\nimport torch.nn as nn\n\ndef center_crop(layer, max_height, max_width):\n    batch_size, n_channels, layer_height, layer_width = layer.size()\n    xy1 = (layer_width - max_width) // 2\n    xy2 = (layer_height - max_height) // 2\n    return layer[:, :, xy2:(xy2 + max_height), xy1:(xy1 + max_width)]\n\n\nclass DenseLayer(nn.Sequential):\n    def __init__(self, in_channels, growth_rate):\n        super(DenseLayer, self).__init__()\n        self.add_module('norm', nn.BatchNorm2d(num_features=in_channels))\n        self.add_module('relu', nn.ReLU(inplace=True))\n\n        self.add_module('conv', nn.Conv2d(in_channels=in_channels,\n                                          out_channels=growth_rate, kernel_size=3, stride=1,\n                                          padding=1, bias=True))\n\n        self.add_module('drop', nn.Dropout2d(0.2))\n\n    def forward(self, x):\n        out = super(DenseLayer, self).forward(x)\n        return out\n\n\nclass DenseBlock(nn.Module):\n    def __init__(self, in_channels, growth_rate, n_layers, upsample=False):\n        super(DenseBlock, self).__init__()\n        self.upsample = upsample\n        self.layers = nn.ModuleList([DenseLayer(\n            in_channels + i * growth_rate, growth_rate)\n            for i in range(n_layers)])\n\n        self.SE_upsample1 = nn.Conv2d(growth_rate * n_layers, growth_rate * n_layers // 16, kernel_size=1)\n        self.SE_upsample2 = nn.Conv2d(growth_rate * n_layers // 16, growth_rate * n_layers, kernel_size=1)\n        self.SE1 = nn.Conv2d((in_channels + growth_rate * n_layers), (in_channels + growth_rate * n_layers) // 16, kernel_size=1)\n        self.SE2 = nn.Conv2d((in_channels + growth_rate * n_layers) // 16, (in_channels + growth_rate * n_layers), kernel_size=1)\n\n    def forward(self, x):\n        if self.upsample:\n            new_features = []\n            for layer in self.layers:\n                out = layer(x)\n                x = torch.cat([x, out], 1)\n                new_features.append(out)\n            out = torch.cat(new_features, 1)\n            fm_size = out.size()[2]\n            scale_weight = F.avg_pool2d(out, fm_size)\n            scale_weight = F.relu(self.SE_upsample1(scale_weight))\n            scale_weight = F.sigmoid(self.SE_upsample2(scale_weight))\n            out = out * scale_weight.expand_as(out)\n            return out\n        else:\n            for layer in self.layers:\n                out = layer(x)\n                x = torch.cat([x, out], 1)  # 1 = channel axis\n            fm_size = x.size()[2]\n            scale_weight = F.avg_pool2d(x, fm_size)\n            scale_weight = F.relu(self.SE1(scale_weight))\n            scale_weight = F.sigmoid(self.SE2(scale_weight))\n            x = x * scale_weight.expand_as(x)\n            return x\n\n\nclass TransitionDown(nn.Sequential):\n    def __init__(self, in_channels):\n        super(TransitionDown, self).__init__()\n        self.add_module('norm', nn.BatchNorm2d(num_features=in_channels))\n        self.add_module('relu', nn.ReLU(inplace=True))\n        self.add_module('conv', nn.Conv2d(in_channels=in_channels,\n                                          out_channels=in_channels, kernel_size=1, stride=1,\n                                          padding=0, bias=True))\n        self.add_module('drop', nn.Dropout2d(0.2))\n        self.add_module('maxpool', nn.MaxPool2d(2))\n\n    def forward(self, x):\n        out = super(TransitionDown, self).forward(x)\n\n        return out\n\n\nclass TransitionUp(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(TransitionUp, self).__init__()\n        self.convTrans = nn.ConvTranspose2d(in_channels=in_channels,\n                                            out_channels=out_channels, kernel_size=3, stride=2,\n                                            padding=0,\n                                            bias=True)  # crop = 'valid' means padding=0. Padding has reverse effect for transpose conv (reduces output size)\n\n    def forward(self, x, skip):\n        out = self.convTrans(x)\n        out = center_crop(out, skip.size(2), skip.size(3))\n        out = torch.cat([out, skip], 1)\n\n        return out\n\n\nclass Bottleneck(nn.Sequential):\n    def __init__(self, in_channels, growth_rate, n_layers):\n        super(Bottleneck, self).__init__()\n        self.add_module('bottleneck', DenseBlock(in_channels, growth_rate, n_layers, upsample=True))\n\n    def forward(self, x):\n        out = super(Bottleneck, self).forward(x)\n        return out\n\n\nclass FCDenseNet(nn.Module):\n    def __init__(self, in_channels=3, down_blocks=(5, 5, 5, 5, 5),\n                 up_blocks=(5, 5, 5, 5, 5), bottleneck_layers=5,\n                 growth_rate=16, out_chans_first_conv=48, n_classes=12):\n        super(FCDenseNet, self).__init__()\n        self.down_blocks = down_blocks\n        self.up_blocks = up_blocks\n\n        cur_channels_count = 0\n        skip_connection_channel_counts = []\n\n        #####################\n        # First Convolution #\n        #####################\n\n        self.add_module('firstconv', nn.Conv2d(in_channels=in_channels,\n                                               out_channels=out_chans_first_conv, kernel_size=3,\n                                               stride=1, padding=1, bias=True))\n        cur_channels_count = out_chans_first_conv\n\n        #####################\n        # Downsampling path #\n        #####################\n\n        self.denseBlocksDown = nn.ModuleList([])\n        self.transDownBlocks = nn.ModuleList([])\n        for i in range(len(down_blocks)):\n            self.denseBlocksDown.append(\n                DenseBlock(cur_channels_count, growth_rate, down_blocks[i]))\n            cur_channels_count += (growth_rate * down_blocks[i])\n            skip_connection_channel_counts.insert(0, cur_channels_count)\n            self.transDownBlocks.append(TransitionDown(cur_channels_count))\n\n        #####################\n        #     Bottleneck    #\n        #####################\n\n        self.add_module('bottleneck', Bottleneck(cur_channels_count,\n                                                 growth_rate, bottleneck_layers))\n        prev_block_channels = growth_rate * bottleneck_layers\n        cur_channels_count += prev_block_channels\n\n        #######################\n        #   Upsampling path   #\n        #######################\n\n        self.transUpBlocks = nn.ModuleList([])\n        self.denseBlocksUp = nn.ModuleList([])\n        for i in range(len(up_blocks) - 1):\n            self.transUpBlocks.append(TransitionUp(prev_block_channels, prev_block_channels))\n            cur_channels_count = prev_block_channels + skip_connection_channel_counts[i]\n\n            self.denseBlocksUp.append(DenseBlock(\n                cur_channels_count, growth_rate, up_blocks[i],\n                upsample=True))\n            prev_block_channels = growth_rate * up_blocks[i]\n            cur_channels_count += prev_block_channels\n\n        # One final dense block\n        self.transUpBlocks.append(TransitionUp(\n            prev_block_channels, prev_block_channels))\n        cur_channels_count = prev_block_channels + skip_connection_channel_counts[-1]\n\n        self.denseBlocksUp.append(DenseBlock(\n            cur_channels_count, growth_rate, up_blocks[-1],\n            upsample=False))\n        cur_channels_count += growth_rate * up_blocks[-1]\n\n        #####################\n        #      Softmax      #\n        #####################\n\n        self.finalConv = nn.Conv2d(in_channels=cur_channels_count,\n                                   out_channels=n_classes, kernel_size=1, stride=1,\n                                   padding=0, bias=True)\n        self.softmax = nn.LogSoftmax()\n\n    def forward(self, x):\n        out = self.firstconv(x)\n\n        skip_connections = []\n        for i in range(len(self.down_blocks)):\n            out = self.denseBlocksDown[i](out)\n            skip_connections.append(out)\n            out = self.transDownBlocks[i](out)\n\n        out = self.bottleneck(out)\n        for i in range(len(self.up_blocks)):\n            skip = skip_connections.pop()\n            out = self.transUpBlocks[i](out, skip)\n            out = self.denseBlocksUp[i](out)\n\n        out = self.finalConv(out)\n        out = self.softmax(out)\n        return out\n\n\ndef FCDenseNet57(n_classes):\n    return FCDenseNet(in_channels=2, down_blocks=(4, 4, 4, 4, 4),\n                      up_blocks=(4, 4, 4, 4, 4), bottleneck_layers=4,\n                      growth_rate=4, out_chans_first_conv=48, n_classes=n_classes)\n\n\ndef FCDenseNet67(n_classes):\n    return FCDenseNet(in_channels=2, down_blocks=(5, 5, 5, 5, 5),\n                      up_blocks=(5, 5, 5, 5, 5), bottleneck_layers=5,\n                      growth_rate=16, out_chans_first_conv=48, n_classes=n_classes)\n\n\ndef FCDenseNet103(n_classes):\n    return FCDenseNet(in_channels=2, down_blocks=(4, 5, 7, 10, 12),\n                      up_blocks=(12, 10, 7, 5, 4), bottleneck_layers=15,\n                      growth_rate=16, out_chans_first_conv=48, n_classes=n_classes)"""
pywick/models/segmentation/testnets/difnet.py,14,"b'# Source: https://github.com/sdujump/DifNet\n\nimport math\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.parameter import Parameter\nimport torch.nn.functional as F\n\naffine_par = True\n\n__all__ = [\'DifNet\', \'DifNet101\', \'DifNet152\']\n\ndef conv3x3(in_planes, out_planes, stride=1, padding=1, dilation=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=padding, dilation=dilation, bias=False)\n\n\nclass Mask(nn.Module):\n    def __init__(self, inplanes=21):\n        super(Mask, self).__init__()\n        self.sig = nn.Sigmoid()\n        self.relu = nn.ReLU(inplace=True)\n        self.bn0 = nn.BatchNorm2d(inplanes)\n        self.bn1 = nn.BatchNorm2d(8)\n        self.bn2 = nn.BatchNorm2d(4)\n        self.conv1 = nn.Conv2d(inplanes, 8, kernel_size=5, stride=1, padding=2, bias=False)\n        self.conv2 = nn.Conv2d(8, 4, kernel_size=5, stride=1, padding=2, bias=False)\n        self.conv3 = nn.Conv2d(4, 1, kernel_size=5, stride=1, padding=2, bias=False)\n\n    def forward(self, x):\n        x = self.relu(self.bn0(x))\n        x = self.relu(self.bn1(self.conv1(x)))\n        x = self.relu(self.bn2(self.conv2(x)))\n        x = self.sig(self.conv3(x))\n        return x\n\n\nclass Diffuse(nn.Module):\n    def __init__(self, inplanes, outplanes=64, clamp=False):\n        super(Diffuse, self).__init__()\n        self.alpha = Parameter(torch.Tensor(1))\n        self.beta = Parameter(torch.Tensor(1))\n        self.alpha.data.fill_(0)\n        self.beta.data.fill_(0)\n        self.clamp = clamp\n        self.softmax = nn.Softmax(2)\n        self.conv = nn.Conv2d(in_channels=inplanes, out_channels=outplanes, kernel_size=1, stride=1, padding=0)\n        self.bn = nn.BatchNorm2d(outplanes)\n\n    def forward(self, F, pred, seed):\n        b, c, h, w = pred.size()\n\n        F = self.bn(self.conv(F))\n        F = nn.functional.adaptive_max_pool2d(F, (h, w))\n        F = F.view(b, -1, h * w)\n        W = torch.bmm(F.transpose(1, 2), F)\n        P = self.softmax(W)\n\n        if self.clamp:\n            self.alpha.data = torch.clamp(self.alpha.data, 0, 1)\n            self.beta.data = torch.clamp(self.beta.data, 0, 1)\n\n        pred_vec = pred.view(b, c, -1)\n        out_vec = torch.bmm(P, pred_vec.transpose(1, 2)).transpose(1, 2).contiguous()\n        out = (1 / (1 + torch.exp(self.beta))) * ((1 / (1 + torch.exp(self.alpha))) * out_vec.view(b, c, h, w) + (torch.exp(self.alpha) / (1 + torch.exp(self.alpha))) * seed) + (\n                    torch.exp(self.beta) / (1 + torch.exp(self.beta))) * pred\n        return out, P\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride=stride)\n        self.bn1 = nn.BatchNorm2d(planes, affine=affine_par)\n        self.relu = nn.ReLU(inplace=True)\n\n        padding = dilation\n        self.conv2 = conv3x3(planes, planes, stride=1, padding=padding, dilation=dilation)\n        self.bn2 = nn.BatchNorm2d(planes, affine=affine_par)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False)  # change\n        self.bn1 = nn.BatchNorm2d(planes, affine=affine_par)\n\n        padding = dilation\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=padding, bias=False, dilation=dilation)\n        self.bn2 = nn.BatchNorm2d(planes, affine=affine_par)\n\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4, affine=affine_par)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Classifier_Module(nn.Module):\n\n    def __init__(self, dilation_series, padding_series, num_classes, inplane):\n        super(Classifier_Module, self).__init__()\n        self.conv2d_list = nn.ModuleList()\n        for dilation, padding in zip(dilation_series, padding_series):\n            self.conv2d_list.append(nn.Conv2d(inplane, num_classes, kernel_size=3, stride=1, padding=padding, dilation=dilation, bias=True))\n\n        for m in self.conv2d_list:\n            m.weight.data.normal_(0, 0.01)\n\n    def forward(self, x):\n        out = self.conv2d_list[0](x)\n        for i in range(len(self.conv2d_list) - 1):\n            out += self.conv2d_list[i + 1](x)\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes, isseed=True):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64, affine=affine_par)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=True)  # change\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4)\n        if isseed:\n            if block.__name__ == \'Bottleneck\':\n                self.layer5 = self._make_pred_layer(Classifier_Module, [6, 12, 18, 24], [6, 12, 18, 24], num_classes, 2048)\n            else:\n                self.layer5 = self._make_pred_layer(Classifier_Module, [6, 12, 18, 24], [6, 12, 18, 24], num_classes, 512)\n        self.isseed = isseed\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion or dilation == 2 or dilation == 4:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion, affine=affine_par))\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, dilation=dilation, downsample=downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dilation=dilation))\n\n        return nn.Sequential(*layers)\n\n    def _make_pred_layer(self, block, dilation_series, padding_series, num_classes, inplane):\n        return block(dilation_series, padding_series, num_classes, inplane)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x1 = self.layer1(x)\n        x2 = self.layer2(x1)\n        x3 = self.layer3(x2)\n        x4 = self.layer4(x3)\n\n        if self.isseed:\n            out = self.layer5(x4)\n        else:\n            out = (x1, x2, x3, x4)\n        return out\n\n\nclass DifNet(nn.Module):\n    def __init__(self, num_classes, layers, **kwargs):\n        super(DifNet, self).__init__()\n        if layers <= 34:\n            self.diffuse0 = Diffuse(3)\n            self.diffuse1 = Diffuse(64)\n            self.diffuse2 = Diffuse(128)\n            self.diffuse3 = Diffuse(256)\n            self.diffuse4 = Diffuse(512)\n        else:\n            self.diffuse0 = Diffuse(3)\n            self.diffuse1 = Diffuse(64 * 4)\n            self.diffuse2 = Diffuse(128 * 4)\n            self.diffuse3 = Diffuse(256 * 4)\n            self.diffuse4 = Diffuse(512 * 4)\n\n        if layers == 18:\n            self.model_sed = ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n            self.model_dif = ResNet(BasicBlock, [2, 2, 2, 2], num_classes, isseed=False)\n        elif layers == 34:\n            self.model_sed = ResNet(BasicBlock, [3, 4, 6, 3], num_classes)\n            self.model_dif = ResNet(BasicBlock, [3, 4, 6, 3], num_classes, isseed=False)\n        elif layers == 50:\n            self.model_sed = ResNet(Bottleneck, [3, 4, 6, 3], num_classes)\n            self.model_dif = ResNet(Bottleneck, [3, 4, 6, 3], num_classes, isseed=False)\n        elif layers == 101:\n            self.model_sed = ResNet(Bottleneck, [3, 4, 23, 3], num_classes)\n            self.model_dif = ResNet(Bottleneck, [3, 4, 23, 3], num_classes, isseed=False)\n        elif layers == 152:\n            self.model_sed = ResNet(Bottleneck, [3, 8, 36, 3], num_classes)\n            self.model_dif = ResNet(Bottleneck, [3, 8, 36, 3], num_classes, isseed=False)\n        elif layers == 1850:\n            self.model_sed = ResNet(BasicBlock, [3, 2, 2, 2], num_classes)\n            self.model_dif = ResNet(Bottleneck, [3, 4, 6, 3], num_classes, isseed=False)\n        else:\n            print(\'unsupport layer number: {}\'.format(layers))\n            exit()\n        self.mask_layer = Mask(inplanes=num_classes)\n\n    def get_alpha(self):\n        return torch.stack((self.diffuse0.alpha.data, self.diffuse1.alpha.data, self.diffuse2.alpha.data, self.diffuse3.alpha.data, self.diffuse4.alpha.data)).t()\n\n    def get_beta(self):\n        return torch.stack((self.diffuse0.beta.data, self.diffuse1.beta.data, self.diffuse2.beta.data, self.diffuse3.beta.data, self.diffuse4.beta.data)).t()\n\n    def forward(self, x):\n        sed = self.model_sed(x)\n        sed_out = sed.clone()\n        mask = self.mask_layer(sed)\n        sed = sed * mask\n\n        dif = self.model_dif(x)\n\n        pred0, P0 = self.diffuse0(x, sed, sed)\n        pred1, P1 = self.diffuse1(dif[0], pred0, sed)\n        pred2, P2 = self.diffuse2(dif[1], pred1, sed)\n        pred3, P3 = self.diffuse3(dif[2], pred2, sed)\n        pred4, P4 = self.diffuse4(dif[3], pred3, sed)\n\n        # return mask, sed_out, (pred0,pred1,pred2,pred3,pred4), torch.stack((P0,P1,P2,P3,P4))\n        return F.interpolate(pred4, size=x.shape[2:], mode=""bilinear"", align_corners=True)        # mask, sed_out, pred4\n\n\ndef DifNet152(num_classes=1, **kwargs):\n    difnet = DifNet(num_classes=num_classes, layers=152, **kwargs)\n    return difnet\n\n\ndef DifNet101(num_classes=1, **kwargs):\n    difnet = DifNet(num_classes=num_classes, layers=101, **kwargs)\n    return difnet\n'"
pywick/models/segmentation/testnets/dilated_resnet.py,7,"b'# Source: https://github.com/BloodAxe/segmentation-networks-benchmark/blob/master/lib/models/dilated_resnet.py\n\n""""""Dilated ResNet""""""\nimport math\nimport torch.utils.model_zoo as model_zoo\nimport torch.nn as nn\n\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    """"""ResNet BasicBlock\n    """"""\n    expansion = 1\n    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None, previous_dilation=1,\n                 norm_layer=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n                               padding=dilation, dilation=dilation, bias=False)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,\n                               padding=previous_dilation, dilation=previous_dilation, bias=False)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    """"""ResNet Bottleneck\n    """"""\n    # pylint: disable=unused-argument\n    expansion = 4\n    def __init__(self, inplanes, planes, stride=1, dilation=1,\n                 downsample=None, previous_dilation=1, norm_layer=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = norm_layer(planes)\n        self.conv2 = nn.Conv2d(\n            planes, planes, kernel_size=3, stride=stride,\n            padding=dilation, dilation=dilation, bias=False)\n        self.bn2 = norm_layer(planes)\n        self.conv3 = nn.Conv2d(\n            planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = norm_layer(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.dilation = dilation\n        self.stride = stride\n\n    def _sum_each(self, x, y):\n        assert(len(x) == len(y))\n        z = []\n        for i in range(len(x)):\n            z.append(x[i]+y[i])\n        return z\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass DilatedResNet(nn.Module):\n    """"""Dilated Pre-trained ResNet Model, which preduces the stride of 8 featuremaps at conv5.\n\n    Parameters\n    ----------\n    block : Block\n        Class for the residual block. Options are BasicBlockV1, BottleneckV1.\n    layers : list of int\n        Numbers of layers in each block\n    classes : int, default 1000\n        Number of classification classes.\n    dilated : bool, default False\n        Applying dilation strategy to pretrained ResNet yielding a stride-8 model,\n        typically used in Semantic Segmentation.\n    norm_layer : object\n        Normalization layer used in backbone network (default: :class:`mxnet.gluon.nn.BatchNorm`;\n        for Synchronized Cross-GPU BachNormalization).\n\n    Reference:\n\n        - He, Kaiming, et al. ""Deep residual learning for image recognition."" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n\n        - Yu, Fisher, and Vladlen Koltun. ""Multi-scale context aggregation by dilated convolutions.""\n    """"""\n    # pylint: disable=unused-variable\n    def __init__(self, block, layers, num_classes=1000, dilated=True, norm_layer=nn.BatchNorm2d):\n        self.inplanes = 64\n        super(DilatedResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = norm_layer(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0], norm_layer=norm_layer)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, norm_layer=norm_layer)\n        if dilated:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=1,\n                                           dilation=2, norm_layer=norm_layer)\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=1,\n                                           dilation=4, norm_layer=norm_layer)\n        else:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n                                           norm_layer=norm_layer)\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n                                           norm_layer=norm_layer)\n        self.avgpool = nn.AvgPool2d(7)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, norm_layer):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, norm_layer=None):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        if dilation == 1 or dilation == 2:\n            layers.append(block(self.inplanes, planes, stride, dilation=1,\n                                downsample=downsample, previous_dilation=dilation, norm_layer=norm_layer))\n        elif dilation == 4:\n            layers.append(block(self.inplanes, planes, stride, dilation=2,\n                                downsample=downsample, previous_dilation=dilation, norm_layer=norm_layer))\n        else:\n            raise RuntimeError(""=> unknown dilation size: {}"".format(dilation))\n\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dilation=dilation, previous_dilation=dilation,\n                                norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = DilatedResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet18\']))\n    return model\n\n\ndef dilated_resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = DilatedResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet34\']))\n    return model\n\n\ndef resnet50(pretrained=False, root=\'~/.torch/models\', **kwargs):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = DilatedResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\']))\n    return model\n\n\ndef resnet101(pretrained=False, root=\'~/.torch/models\', **kwargs):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = DilatedResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet101\']))\n    return model\n\n\ndef resnet152(pretrained=False, root=\'~/.encoding/models\', **kwargs):\n    """"""Constructs a ResNet-152 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = DilatedResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet152\']))\n    return model'"
pywick/models/segmentation/testnets/encnet.py,6,"b'# Source: https://github.com/Tramac/awesome-semantic-segmentation-pytorch/blob/master/core/models/encnet.py (License: Apache 2.0)\n\n""""""\nImplementation of `Context Encoding for Semantic Segmentation <https://arxiv.org/pdf/1803.08904v1>`_\n\nse_loss is the Semantic Encoding Loss from the paper. It computes probabilities of contexts appearing together.\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom pywick.models.segmentation.da_basenets.segbase import SegBaseModel\nfrom pywick.models.segmentation.da_basenets.fcn import _FCNHead\n\n__all__ = [\'EncNet\', \'EncModule\', \'get_encnet\', \'encnet_resnet50\', \'encnet_resnet101\', \'encnet_resnet152\']\n\n\nclass EncNet(SegBaseModel):\n    def __init__(self, num_classes, pretrained=True, backbone=\'resnet101\', aux=False, se_loss=True, lateral=False, **kwargs):\n        super(EncNet, self).__init__(num_classes, pretrained=pretrained, aux=aux, backbone=backbone, **kwargs)\n        self.head = _EncHead(2048, num_classes, se_loss=se_loss, lateral=lateral, **kwargs)\n        if aux:\n            self.auxlayer = _FCNHead(1024, num_classes, **kwargs)\n\n        self.__setattr__(\'exclusive\', [\'head\', \'auxlayer\'] if aux else [\'head\'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        features = self.base_forward(x)\n\n        x = list(self.head(*features))\n        x[0] = F.interpolate(x[0], size, mode=\'bilinear\', align_corners=True)\n        if self.aux:\n            auxout = self.auxlayer(features[2])\n            auxout = F.interpolate(auxout, size, mode=\'bilinear\', align_corners=True)\n            x.append(auxout)\n            return tuple(x)\n        else:\n            return x[0]\n\n\nclass _EncHead(nn.Module):\n    def __init__(self, in_channels, nclass, se_loss=True, lateral=True, norm_layer=nn.BatchNorm2d, norm_kwargs=None, **kwargs):\n        super(_EncHead, self).__init__()\n        self.lateral = lateral\n        self.conv5 = nn.Sequential(\n            nn.Conv2d(in_channels, 512, 3, padding=1, bias=False),\n            norm_layer(512, **({} if norm_kwargs is None else norm_kwargs)),\n            nn.ReLU(True)\n        )\n        if lateral:\n            self.connect = nn.ModuleList([\n                nn.Sequential(\n                    nn.Conv2d(512, 512, 1, bias=False),\n                    norm_layer(512, **({} if norm_kwargs is None else norm_kwargs)),\n                    nn.ReLU(True)),\n                nn.Sequential(\n                    nn.Conv2d(1024, 512, 1, bias=False),\n                    norm_layer(512, **({} if norm_kwargs is None else norm_kwargs)),\n                    nn.ReLU(True)),\n            ])\n            self.fusion = nn.Sequential(\n                nn.Conv2d(3 * 512, 512, 3, padding=1, bias=False),\n                norm_layer(512, **({} if norm_kwargs is None else norm_kwargs)),\n                nn.ReLU(True)\n            )\n        self.encmodule = EncModule(512, nclass, ncodes=32, se_loss=se_loss,\n                                   norm_layer=norm_layer, norm_kwargs=norm_kwargs, **kwargs)\n        self.conv6 = nn.Sequential(\n            nn.Dropout(0.1, False),\n            nn.Conv2d(512, nclass, 1)\n        )\n\n    def forward(self, *inputs):\n        feat = self.conv5(inputs[-1])\n        if self.lateral:\n            c2 = self.connect[0](inputs[1])\n            c3 = self.connect[1](inputs[2])\n            feat = self.fusion(torch.cat([feat, c2, c3], 1))\n        outs = list(self.encmodule(feat))\n        outs[0] = self.conv6(outs[0])\n        return tuple(outs)\n\n\nclass EncModule(nn.Module):\n    def __init__(self, in_channels, nclass, ncodes=32, se_loss=True,\n                 norm_layer=nn.BatchNorm2d, norm_kwargs=None, **kwargs):\n        super(EncModule, self).__init__()\n        self.se_loss = se_loss\n        self.encoding = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels, 1, bias=False),\n            norm_layer(in_channels, **({} if norm_kwargs is None else norm_kwargs)),\n            nn.ReLU(True),\n            Encoding(D=in_channels, K=ncodes),\n            nn.BatchNorm1d(ncodes),\n            nn.ReLU(True),\n            Mean(dim=1)\n        )\n        self.fc = nn.Sequential(\n            nn.Linear(in_channels, in_channels),\n            nn.Sigmoid()\n        )\n        if self.se_loss:\n            self.selayer = nn.Linear(in_channels, nclass)\n\n    def forward(self, x):\n        en = self.encoding(x)\n        b, c, _, _ = x.size()\n        gamma = self.fc(en)\n        y = gamma.view(b, c, 1, 1)\n        outputs = [F.relu_(x + x * y)]\n        if self.se_loss:\n            outputs.append(self.selayer(en))\n        return tuple(outputs)\n\n\nclass Encoding(nn.Module):\n    def __init__(self, D, K):\n        super(Encoding, self).__init__()\n        # init codewords and smoothing factor\n        self.D, self.K = D, K\n        self.codewords = nn.Parameter(torch.Tensor(K, D), requires_grad=True)\n        self.scale = nn.Parameter(torch.Tensor(K), requires_grad=True)\n        self.reset_params()\n\n    def reset_params(self):\n        std1 = 1. / ((self.K * self.D) ** (1 / 2))\n        self.codewords.data.uniform_(-std1, std1)\n        self.scale.data.uniform_(-1, 0)\n\n    def forward(self, X):\n        # input X is a 4D tensor\n        assert (X.size(1) == self.D)\n        B, D = X.size(0), self.D\n        if X.dim() == 3:\n            # BxDxN -> BxNxD\n            X = X.transpose(1, 2).contiguous()\n        elif X.dim() == 4:\n            # BxDxHxW -> Bx(HW)xD\n            X = X.view(B, D, -1).transpose(1, 2).contiguous()\n        else:\n            raise RuntimeError(\'Encoding Layer unknown input dims!\')\n        # assignment weights BxNxK\n        A = F.softmax(self.scale_l2(X, self.codewords, self.scale), dim=2)\n        # aggregate\n        E = self.aggregate(A, X, self.codewords)\n        return E\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'(\' \\\n               + \'N x\' + str(self.D) + \'=>\' + str(self.K) + \'x\' \\\n               + str(self.D) + \')\'\n\n    @staticmethod\n    def scale_l2(X, C, S):\n        S = S.view(1, 1, C.size(0), 1)\n        X = X.unsqueeze(2).expand(X.size(0), X.size(1), C.size(0), C.size(1))\n        C = C.unsqueeze(0).unsqueeze(0)\n        SL = S * (X - C)\n        SL = SL.pow(2).sum(3)\n        return SL\n\n    @staticmethod\n    def aggregate(A, X, C):\n        A = A.unsqueeze(3)\n        X = X.unsqueeze(2).expand(X.size(0), X.size(1), C.size(0), C.size(1))\n        C = C.unsqueeze(0).unsqueeze(0)\n        E = A * (X - C)\n        E = E.sum(1)\n        return E\n\n\nclass Mean(nn.Module):\n    def __init__(self, dim, keep_dim=False):\n        super(Mean, self).__init__()\n        self.dim = dim\n        self.keep_dim = keep_dim\n\n    def forward(self, input):\n        return input.mean(self.dim, self.keep_dim)\n\n\ndef get_encnet(num_classes=1, backbone=\'resnet50\', pretrained=True, **kwargs):\n    return EncNet(num_classes=num_classes, backbone=backbone, pretrained=pretrained, **kwargs)\n\n\ndef encnet_resnet50(num_classes=1, **kwargs):\n    return get_encnet(num_classes=num_classes, backbone=\'resnet50\', **kwargs)\n\n\ndef encnet_resnet101(num_classes=1, **kwargs):\n    return get_encnet(num_classes=num_classes, backbone=\'resnet101\', **kwargs)\n\n\ndef encnet_resnet152(num_classes=1, **kwargs):\n    return get_encnet(num_classes=num_classes, backbone=\'resnet152\', **kwargs)\n\n\nif __name__ == \'__main__\':\n    img = torch.randn(2, 3, 224, 224)\n    model = encnet_resnet50()\n    outputs = model(img)\n'"
pywick/models/segmentation/testnets/esp_net.py,12,"b'# Source: https://github.com/sacmehta/ESPNet\n# Note: currently missing references to pretrained files\n\nimport torch\nimport torch.nn as nn\n\n__author__ = ""Sachin Mehta""\n\n\nclass CBR(nn.Module):\n    \'\'\'\n    This class defines the convolution layer with batch normalization and PReLU activation\n    \'\'\'\n\n    def __init__(self, nIn, nOut, kSize, stride=1):\n        \'\'\'\n\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: stride rate for down-sampling. Default is 1\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1) / 2)\n        # self.conv = nn.Conv2d(nIn, nOut, kSize, stride=stride, padding=padding, bias=False)\n        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False)\n        # self.conv1 = nn.Conv2d(nOut, nOut, (1, kSize), stride=1, padding=(0, padding), bias=False)\n        self.bn = nn.BatchNorm2d(nOut, eps=1e-03)\n        self.act = nn.PReLU(nOut)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        output = self.conv(input)\n        # output = self.conv1(output)\n        output = self.bn(output)\n        output = self.act(output)\n        return output\n\n\nclass BR(nn.Module):\n    \'\'\'\n        This class groups the batch normalization and PReLU activation\n    \'\'\'\n\n    def __init__(self, nOut):\n        \'\'\'\n        :param nOut: output feature maps\n        \'\'\'\n        super().__init__()\n        self.bn = nn.BatchNorm2d(nOut, eps=1e-03)\n        self.act = nn.PReLU(nOut)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: normalized and thresholded feature map\n        \'\'\'\n        output = self.bn(input)\n        output = self.act(output)\n        return output\n\n\nclass CB(nn.Module):\n    \'\'\'\n       This class groups the convolution and batch normalization\n    \'\'\'\n\n    def __init__(self, nIn, nOut, kSize, stride=1):\n        \'\'\'\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: optinal stide for down-sampling\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1) / 2)\n        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False)\n        self.bn = nn.BatchNorm2d(nOut, eps=1e-03)\n\n    def forward(self, input):\n        \'\'\'\n\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        output = self.conv(input)\n        output = self.bn(output)\n        return output\n\n\nclass C(nn.Module):\n    \'\'\'\n    This class is for a convolutional layer.\n    \'\'\'\n\n    def __init__(self, nIn, nOut, kSize, stride=1):\n        \'\'\'\n\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: optional stride rate for down-sampling\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1) / 2)\n        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        output = self.conv(input)\n        return output\n\n\nclass CDilated(nn.Module):\n    \'\'\'\n    This class defines the dilated convolution.\n    \'\'\'\n\n    def __init__(self, nIn, nOut, kSize, stride=1, d=1):\n        \'\'\'\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: optional stride rate for down-sampling\n        :param d: optional dilation rate\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1) / 2) * d\n        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False, dilation=d)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        output = self.conv(input)\n        return output\n\n\nclass DownSamplerB(nn.Module):\n    def __init__(self, nIn, nOut):\n        super().__init__()\n        n = int(nOut / 5)\n        n1 = nOut - 4 * n\n        self.c1 = C(nIn, n, 3, 2)\n        self.d1 = CDilated(n, n1, 3, 1, 1)\n        self.d2 = CDilated(n, n, 3, 1, 2)\n        self.d4 = CDilated(n, n, 3, 1, 4)\n        self.d8 = CDilated(n, n, 3, 1, 8)\n        self.d16 = CDilated(n, n, 3, 1, 16)\n        self.bn = nn.BatchNorm2d(nOut, eps=1e-3)\n        self.act = nn.PReLU(nOut)\n\n    def forward(self, input):\n        output1 = self.c1(input)\n        d1 = self.d1(output1)\n        d2 = self.d2(output1)\n        d4 = self.d4(output1)\n        d8 = self.d8(output1)\n        d16 = self.d16(output1)\n\n        add1 = d2\n        add2 = add1 + d4\n        add3 = add2 + d8\n        add4 = add3 + d16\n\n        combine = torch.cat([d1, add1, add2, add3, add4], 1)\n        # combine_in_out = input + combine\n        output = self.bn(combine)\n        output = self.act(output)\n        return output\n\n\nclass DilatedParllelResidualBlockB(nn.Module):\n    \'\'\'\n    This class defines the ESP block, which is based on the following principle\n        Reduce ---> Split ---> Transform --> Merge\n    \'\'\'\n\n    def __init__(self, nIn, nOut, add=True):\n        \'\'\'\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param add: if true, add a residual connection through identity operation. You can use projection too as\n                in ResNet paper, but we avoid to use it if the dimensions are not the same because we do not want to\n                increase the module complexity\n        \'\'\'\n        super().__init__()\n        n = int(nOut / 5)\n        n1 = nOut - 4 * n\n        self.c1 = C(nIn, n, 1, 1)\n        self.d1 = CDilated(n, n1, 3, 1, 1)  # dilation rate of 2^0\n        self.d2 = CDilated(n, n, 3, 1, 2)  # dilation rate of 2^1\n        self.d4 = CDilated(n, n, 3, 1, 4)  # dilation rate of 2^2\n        self.d8 = CDilated(n, n, 3, 1, 8)  # dilation rate of 2^3\n        self.d16 = CDilated(n, n, 3, 1, 16)  # dilation rate of 2^4\n        self.bn = BR(nOut)\n        self.add = add\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        # reduce\n        output1 = self.c1(input)\n        # split and transform\n        d1 = self.d1(output1)\n        d2 = self.d2(output1)\n        d4 = self.d4(output1)\n        d8 = self.d8(output1)\n        d16 = self.d16(output1)\n\n        # heirarchical fusion for de-gridding\n        add1 = d2\n        add2 = add1 + d4\n        add3 = add2 + d8\n        add4 = add3 + d16\n\n        # merge\n        combine = torch.cat([d1, add1, add2, add3, add4], 1)\n\n        # if residual version\n        if self.add:\n            combine = input + combine\n        output = self.bn(combine)\n        return output\n\n\nclass InputProjectionA(nn.Module):\n    \'\'\'\n    This class projects the input image to the same spatial dimensions as the feature map.\n    For example, if the input image is 512 x512 x3 and spatial dimensions of feature map size are 56x56xF, then\n    this class will generate an output of 56x56x3\n    \'\'\'\n\n    def __init__(self, samplingTimes):\n        \'\'\'\n        :param samplingTimes: The rate at which you want to down-sample the image\n        \'\'\'\n        super().__init__()\n        self.pool = nn.ModuleList()\n        for i in range(0, samplingTimes):\n            # pyramid-based approach for down-sampling\n            self.pool.append(nn.AvgPool2d(3, stride=2, padding=1))\n\n    def forward(self, input):\n        \'\'\'\n        :param input: Input RGB Image\n        :return: down-sampled image (pyramid-based approach)\n        \'\'\'\n        for pool in self.pool:\n            input = pool(input)\n        return input\n\n\nclass ESPNet_Encoder(nn.Module):\n    \'\'\'\n    This class defines the ESPNet-C network in the paper\n    \'\'\'\n\n    def __init__(self, classes=20, p=5, q=3):\n        \'\'\'\n        :param classes: number of classes in the dataset. Default is 20 for the cityscapes\n        :param p: depth multiplier\n        :param q: depth multiplier\n        \'\'\'\n        super().__init__()\n        self.level1 = CBR(3, 16, 3, 2)\n        self.sample1 = InputProjectionA(1)\n        self.sample2 = InputProjectionA(2)\n\n        self.b1 = BR(16 + 3)\n        self.level2_0 = DownSamplerB(16 + 3, 64)\n\n        self.level2 = nn.ModuleList()\n        for i in range(0, p):\n            self.level2.append(DilatedParllelResidualBlockB(64, 64))\n        self.b2 = BR(128 + 3)\n\n        self.level3_0 = DownSamplerB(128 + 3, 128)\n        self.level3 = nn.ModuleList()\n        for i in range(0, q):\n            self.level3.append(DilatedParllelResidualBlockB(128, 128))\n        self.b3 = BR(256)\n\n        self.classifier = C(256, classes, 1, 1)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: Receives the input RGB image\n        :return: the transformed feature map with spatial dimensions 1/8th of the input image\n        \'\'\'\n        output0 = self.level1(input)\n        inp1 = self.sample1(input)\n        inp2 = self.sample2(input)\n\n        output0_cat = self.b1(torch.cat([output0, inp1], 1))\n        output1_0 = self.level2_0(output0_cat)  # down-sampled\n\n        for i, layer in enumerate(self.level2):\n            if i == 0:\n                output1 = layer(output1_0)\n            else:\n                output1 = layer(output1)\n\n        output1_cat = self.b2(torch.cat([output1, output1_0, inp2], 1))\n\n        output2_0 = self.level3_0(output1_cat)  # down-sampled\n        for i, layer in enumerate(self.level3):\n            if i == 0:\n                output2 = layer(output2_0)\n            else:\n                output2 = layer(output2)\n\n        output2_cat = self.b3(torch.cat([output2_0, output2], 1))\n\n        classifier = self.classifier(output2_cat)\n\n        return classifier\n\n\nclass ESPNet(nn.Module):\n    \'\'\'\n    This class defines the ESPNet network\n    \'\'\'\n\n    def __init__(self, num_classes=20, p=2, q=3, encoderFile=None):\n        \'\'\'\n        :param num_classes: number of classes in the dataset. Default is 20 for the cityscapes\n        :param p: depth multiplier\n        :param q: depth multiplier\n        :param encoderFile: pretrained encoder weights. Recall that we first trained the ESPNet-C and then attached the\n                            RUM-based light weight decoder. See paper for more details.\n        \'\'\'\n        super().__init__()\n        self.encoder = ESPNet_Encoder(num_classes, p, q)\n        if encoderFile != None:\n            self.encoder.load_state_dict(torch.load(encoderFile))\n            print(\'Encoder loaded!\')\n        # load the encoder modules\n        self.modules = []\n        for i, m in enumerate(self.encoder.children()):\n            self.modules.append(m)\n\n        # light-weight decoder\n        self.level3_C = C(128 + 3, num_classes, 1, 1)\n        self.br = nn.BatchNorm2d(num_classes, eps=1e-03)\n        self.conv = CBR(19 + num_classes, num_classes, 3, 1)\n\n        self.up_l3 = nn.Sequential(nn.ConvTranspose2d(num_classes, num_classes, 2, stride=2, padding=0, output_padding=0, bias=False))\n        self.combine_l2_l3 = nn.Sequential(BR(2 * num_classes), DilatedParllelResidualBlockB(2 * num_classes, num_classes, add=False))\n\n        self.up_l2 = nn.Sequential(nn.ConvTranspose2d(num_classes, num_classes, 2, stride=2, padding=0, output_padding=0, bias=False), BR(num_classes))\n\n        self.classifier = nn.ConvTranspose2d(num_classes, num_classes, 2, stride=2, padding=0, output_padding=0, bias=False)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: RGB image\n        :return: transformed feature map\n        \'\'\'\n        output0 = self.modules[0](input)\n        inp1 = self.modules[1](input)\n        inp2 = self.modules[2](input)\n\n        output0_cat = self.modules[3](torch.cat([output0, inp1], 1))\n        output1_0 = self.modules[4](output0_cat)  # down-sampled\n\n        for i, layer in enumerate(self.modules[5]):\n            if i == 0:\n                output1 = layer(output1_0)\n            else:\n                output1 = layer(output1)\n\n        output1_cat = self.modules[6](torch.cat([output1, output1_0, inp2], 1))\n\n        output2_0 = self.modules[7](output1_cat)  # down-sampled\n        for i, layer in enumerate(self.modules[8]):\n            if i == 0:\n                output2 = layer(output2_0)\n            else:\n                output2 = layer(output2)\n\n        output2_cat = self.modules[9](torch.cat([output2_0, output2], 1))  # concatenate for feature map width expansion\n\n        output2_c = self.up_l3(self.br(self.modules[10](output2_cat)))  # RUM\n\n        output1_C = self.level3_C(output1_cat)  # project to C-dimensional space\n        comb_l2_l3 = self.up_l2(self.combine_l2_l3(torch.cat([output1_C, output2_c], 1)))  # RUM\n\n        concat_features = self.conv(torch.cat([comb_l2_l3, output0_cat], 1))\n\n        classifier = self.classifier(concat_features)\n        return classifier\n'"
pywick/models/segmentation/testnets/fc_densenet.py,2,"b'from typing import Optional, Sequence, Union\n\nfrom torch.nn import Module, Conv2d, BatchNorm2d, Linear, init\nfrom torch.nn import functional as F\n\nfrom .transition_down import TransitionDown\nfrom .transition_up import TransitionUp\nfrom .dense_block import DenseBlock\n\n\nclass FCDenseNet(Module):\n    r""""""\n    The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation\n    https://arxiv.org/abs/1611.09326\n\n    In this paper, we extend DenseNets to deal with the problem of semantic segmentation. We achieve state-of-the-art\n    results on urban scene benchmark datasets such as CamVid and Gatech, without any further post-processing module nor\n    pretraining. Moreover, due to smart construction of the model, our approach has much less parameters than currently\n    published best entries for these datasets.\n    """"""\n\n    def __init__(self,\n                 in_channels: int = 3,\n                 out_channels: int = 1000,\n                 initial_num_features: int = 48,\n                 dropout: float = 0.2,\n\n                 down_dense_growth_rates: Union[int, Sequence[int]] = 16,\n                 down_dense_bottleneck_ratios: Union[Optional[int], Sequence[Optional[int]]] = None,\n                 down_dense_num_layers: Union[int, Sequence[int]] = (4, 5, 7, 10, 12),\n                 down_transition_compression_factors: Union[float, Sequence[float]] = 1.0,\n\n                 middle_dense_growth_rate: int = 16,\n                 middle_dense_bottleneck: Optional[int] = None,\n                 middle_dense_num_layers: int = 15,\n\n                 up_dense_growth_rates: Union[int, Sequence[int]] = 16,\n                 up_dense_bottleneck_ratios: Union[Optional[int], Sequence[Optional[int]]] = None,\n                 up_dense_num_layers: Union[int, Sequence[int]] = (12, 10, 7, 5, 4)):\n        super(FCDenseNet, self).__init__()\n\n        # region Parameters handling\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n        if type(down_dense_growth_rates) == int:\n            down_dense_growth_rates = (down_dense_growth_rates,) * 5\n        if down_dense_bottleneck_ratios is None or type(down_dense_bottleneck_ratios) == int:\n            down_dense_bottleneck_ratios = (down_dense_bottleneck_ratios,) * 5\n        if type(down_dense_num_layers) == int:\n            down_dense_num_layers = (down_dense_num_layers,) * 5\n        if type(down_transition_compression_factors) == float:\n            down_transition_compression_factors = (down_transition_compression_factors,) * 5\n\n        if type(up_dense_growth_rates) == int:\n            up_dense_growth_rates = (up_dense_growth_rates,) * 5\n        if up_dense_bottleneck_ratios is None or type(up_dense_bottleneck_ratios) == int:\n            up_dense_bottleneck_ratios = (up_dense_bottleneck_ratios,) * 5\n        if type(up_dense_num_layers) == int:\n            up_dense_num_layers = (up_dense_num_layers,) * 5\n        # endregion\n\n        # region First convolution\n        # The Lasagne implementation uses convolution with \'same\' padding, the PyTorch equivalent is padding=1\n        self.features = Conv2d(in_channels, initial_num_features, kernel_size=3, padding=1, bias=False)\n        current_channels = self.features.out_channels\n        # endregion\n\n        # region Downward path\n        # Pairs of Dense Blocks with input concatenation and TransitionDown layers\n        down_dense_params = [\n            {\n                \'concat_input\': True,\n                \'growth_rate\': gr,\n                \'num_layers\': nl,\n                \'dense_layer_params\': {\n                    \'dropout\': dropout,\n                    \'bottleneck_ratio\': br\n                }\n            }\n            for gr, nl, br in\n            zip(down_dense_growth_rates, down_dense_num_layers, down_dense_bottleneck_ratios)\n        ]\n        down_transition_params = [\n            {\n                \'dropout\': dropout,\n                \'compression\': c\n            } for c in down_transition_compression_factors\n        ]\n        skip_connections_channels = []\n\n        self.down_dense = Module()\n        self.down_trans = Module()\n        down_pairs_params = zip(down_dense_params, down_transition_params)\n        for i, (dense_params, transition_params) in enumerate(down_pairs_params):\n            block = DenseBlock(current_channels, **dense_params)\n            current_channels = block.out_channels\n            self.down_dense.add_module(f\'block_{i}\', block)\n\n            skip_connections_channels.append(block.out_channels)\n\n            transition = TransitionDown(current_channels, **transition_params)\n            current_channels = transition.out_channels\n            self.down_trans.add_module(f\'trans_{i}\', transition)\n        # endregion\n\n        # region Middle block\n        # Renamed from ""bottleneck"" in the paper, to avoid confusion with the Bottleneck of DenseLayers\n        self.middle = DenseBlock(\n            current_channels,\n            middle_dense_growth_rate,\n            middle_dense_num_layers,\n            concat_input=True,\n            dense_layer_params={\n                \'dropout\': dropout,\n                \'bottleneck_ratio\': middle_dense_bottleneck\n            })\n        current_channels = self.middle.out_channels\n        # endregion\n\n        # region Upward path\n        # Pairs of TransitionUp layers and Dense Blocks without input concatenation\n        up_transition_params = [\n            {\n                \'skip_channels\': sc,\n            } for sc in reversed(skip_connections_channels)\n        ]\n        up_dense_params = [\n            {\n                \'concat_input\': False,\n                \'growth_rate\': gr,\n                \'num_layers\': nl,\n                \'dense_layer_params\': {\n                    \'dropout\': dropout,\n                    \'bottleneck_ratio\': br\n                }\n            }\n            for gr, nl, br in\n            zip(up_dense_growth_rates, up_dense_num_layers, up_dense_bottleneck_ratios)\n        ]\n\n        self.up_dense = Module()\n        self.up_trans = Module()\n        up_pairs_params = zip(up_transition_params, up_dense_params)\n        for i, (transition_params_up, dense_params_up) in enumerate(up_pairs_params):\n            transition = TransitionUp(current_channels, **transition_params_up)\n            current_channels = transition.out_channels\n            self.up_trans.add_module(f\'trans_{i}\', transition)\n\n            block = DenseBlock(current_channels, **dense_params_up)\n            current_channels = block.out_channels\n            self.up_dense.add_module(f\'block_{i}\', block)\n        # endregion\n\n        # region Final convolution\n        self.final = Conv2d(current_channels, out_channels, kernel_size=1, bias=False)\n        # endregion\n\n        # region Weight initialization\n        for module in self.modules():\n            if isinstance(module, Conv2d):\n                init.kaiming_normal_(module.weight)\n            elif isinstance(module, BatchNorm2d):\n                module.reset_parameters()\n            elif isinstance(module, Linear):\n                init.xavier_uniform(module.weight)\n                init.constant(module.bias, 0)\n        # endregion\n\n    def forward(self, x):\n        res = self.features(x)\n\n        skip_tensors = []\n        for dense, trans in zip(self.down_dense.children(), self.down_trans.children()):\n            res = dense(res)\n            skip_tensors.append(res)\n            res = trans(res)\n\n        res = self.middle(res)\n\n        for skip, trans, dense in zip(reversed(skip_tensors), self.up_trans.children(), self.up_dense.children()):\n            res = trans(res, skip)\n            res = dense(res)\n\n        res = self.final(res)\n\n        return res\n\n    def predict(self, x):\n        logits = self(x)\n        return F.softmax(logits)\n'"
pywick/models/segmentation/testnets/flatten.py,1,"b'from torch.nn import Module\n\n\nclass Flatten(Module):\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n'"
pywick/models/segmentation/testnets/msc.py,3,"b'#!/usr/bin/env python\n# coding: utf-8\n#\n# Author:   Kazuto Nakashima\n# URL:      http://kazuto1011.github.io\n# Created:  2018-03-26\n\n# Source: https://raw.githubusercontent.com/kazuto1011/deeplab-pytorch/master/libs/models/msc.py\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass MSC(nn.Module):\n    """"""Multi-scale inputs""""""\n\n    def __init__(self, scale, pyramids=[0.5, 0.75]):\n        super(MSC, self).__init__()\n        self.scale = scale\n        self.pyramids = pyramids\n\n    def forward(self, x):\n        # Original\n        logits = self.scale(x)\n        interp = lambda l: F.interpolate(l, size=logits.shape[2:], mode=""bilinear"", align_corners=True)\n\n        # Scaled\n        logits_pyramid = []\n        for p in self.pyramids:\n            size = [int(s * p) for s in x.shape[2:]]\n            h = F.interpolate(x, size=size, mode=""bilinear"", align_corners=True)\n            logits_pyramid.append(self.scale(h))\n\n        # Pixel-wise max\n        logits_all = [logits] + [interp(l) for l in logits_pyramid]\n        logits_max = torch.max(torch.stack(logits_all), dim=0)[0]\n\n        if self.training:\n            return [logits] + logits_pyramid + [logits_max]\n        else:\n            return logits_max\n'"
pywick/models/segmentation/testnets/psanet.py,6,"b'# Source: https://github.com/Tramac/awesome-semantic-segmentation-pytorch/blob/master/core/models/psanet.py (License: Apache 2.0)\n\n""""""\nImplementation of `PSANet: Point-wise Spatial AttentionNetwork for Scene Parsing <https://hszhao.github.io/papers/eccv18_psanet.pdf>`_\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom pywick.models.segmentation.da_basenets.basic import _ConvBNReLU\nfrom pywick.models.segmentation.da_basenets.segbase import SegBaseModel\nfrom pywick.models.segmentation.da_basenets.fcn import _FCNHead\n\n__all__ = [\'PSANet\', \'get_psanet\', \'PSANet_Resnet50\', \'PSANet_Resnet101\', \'PSANet_Resnet152\']\n\n\nclass PSANet(SegBaseModel):\n    r""""""PSANet\n    Parameters\n    ----------\n    :param num_classes : int\n        Number of categories for the training dataset.\n    :param backbone : string\n        Pre-trained dilated backbone network type (default:\'resnet50\'; \'resnet50\',\n        \'resnet101\' or \'resnet152\').\n    :param norm_layer : object\n        Normalization layer used in backbone network (default: :class:`nn.BatchNorm`;\n        for Synchronized Cross-GPU BachNormalization).\n    :param aux : (bool, default=False)  Whether to use auxiliary loss.\n    """"""\n\n    def __init__(self, num_classes, pretrained=True, backbone=\'resnet101\', aux=False, **kwargs):\n        super(PSANet, self).__init__(num_classes, pretrained=pretrained, aux=aux, backbone=backbone, **kwargs)\n        self.head = _PSAHead(num_classes, **kwargs)\n        if aux:\n            self.auxlayer = _FCNHead(1024, num_classes, **kwargs)\n\n        self.__setattr__(\'exclusive\', [\'head\', \'auxlayer\'] if aux else [\'head\'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        _, _, c3, c4 = self.base_forward(x)\n        outputs = list()\n        x = self.head(c4)\n        x = F.interpolate(x, size, mode=\'bilinear\', align_corners=True)\n        outputs.append(x)\n\n        if self.aux:\n            auxout = self.auxlayer(c3)\n            auxout = F.interpolate(auxout, size, mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout)\n            return tuple(outputs)\n        else:\n            return outputs[0]\n\n\nclass _PSAHead(nn.Module):\n    def __init__(self, nclass, input_size, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(_PSAHead, self).__init__()\n        psa_out_channels = (input_size // 8) ** 2\n        # self.psa = _PointwiseSpatialAttention(2048, 3600, norm_layer) # <-- original definition. Does not work. Why 3600?\n        self.psa = _PointwiseSpatialAttention(2048, psa_out_channels, norm_layer)\n\n        self.conv_post = _ConvBNReLU(1024, 2048, 1, norm_layer=norm_layer)\n        self.project = nn.Sequential(\n            _ConvBNReLU(4096, 512, 3, padding=1, norm_layer=norm_layer),\n            nn.Dropout2d(0.1, False),\n            nn.Conv2d(512, nclass, 1))\n\n    def forward(self, x):\n        global_feature = self.psa(x)\n        out = self.conv_post(global_feature)\n        out = torch.cat([x, out], dim=1)\n        out = self.project(out)\n\n        return out\n\n\nclass _PointwiseSpatialAttention(nn.Module):\n    def __init__(self, in_channels, out_channels, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(_PointwiseSpatialAttention, self).__init__()\n        reduced_channels = 512\n        self.collect_attention = _AttentionGeneration(in_channels, reduced_channels, out_channels, norm_layer)\n        self.distribute_attention = _AttentionGeneration(in_channels, reduced_channels, out_channels, norm_layer)\n\n    def forward(self, x):\n        collect_fm = self.collect_attention(x)\n        distribute_fm = self.distribute_attention(x)\n        psa_fm = torch.cat([collect_fm, distribute_fm], dim=1)\n        return psa_fm\n\n\nclass _AttentionGeneration(nn.Module):\n    def __init__(self, in_channels, reduced_channels, out_channels, norm_layer, **kwargs):\n        super(_AttentionGeneration, self).__init__()\n        self.conv_reduce = _ConvBNReLU(in_channels, reduced_channels, 1, norm_layer=norm_layer)\n        self.attention = nn.Sequential(\n            _ConvBNReLU(reduced_channels, reduced_channels, 1, norm_layer=norm_layer),\n            nn.Conv2d(reduced_channels, out_channels, 1, bias=False))\n\n        self.reduced_channels = reduced_channels\n\n    def forward(self, x):\n        reduce_x = self.conv_reduce(x)\n        attention = self.attention(reduce_x)\n        n, c, h, w = attention.size()\n        attention = attention.view(n, c, -1)\n        reduce_x = reduce_x.view(n, self.reduced_channels, -1)\n        fm = torch.bmm(reduce_x, torch.softmax(attention, dim=1))\n        fm = fm.view(n, self.reduced_channels, h, w)\n\n        return fm\n\n\ndef get_psanet(num_classes=1, backbone=\'resnet50\', pretrained=True, **kwargs):\n    r""""""PS Attention Network\n\n        Parameters\n        ----------\n        num_classes : int\n            Number of classes\n        pretrained : bool, default True\n            This will load pretrained backbone network, that was trained on ImageNet.\n        """"""\n\n    model = PSANet(num_classes=num_classes, backbone=backbone, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef PSANet_Resnet50(num_classes=1, **kwargs):\n    return get_psanet(num_classes=num_classes, backbone=\'resnet50\', **kwargs)\n\n\ndef PSANet_Resnet101(num_classes=1, backbone=\'resnet101\', **kwargs):\n    return get_psanet(num_classes=num_classes, backbone=backbone, **kwargs)\n\n\ndef PSANet_Resnet152(num_classes=1, backbone=\'resnet152\', **kwargs):\n    return get_psanet(num_classes=num_classes, backbone=backbone, **kwargs)\n\n\nif __name__ == \'__main__\':\n    model = PSANet_Resnet50()\n    img = torch.randn(1, 3, 480, 480)\n    output = model(img)\n'"
pywick/models/segmentation/testnets/psp_saeed.py,4,"b""# Source: https://github.com/saeedizadi/binseg_pytoch/blob/master/models/pspnet.py\n\nimport torch\nimport torch.nn.init as init\nimport torch.nn as nn\nfrom torchvision import models\nimport torch.nn.functional as F\nimport numpy as np\n\nimport math\n\ndef initialize_weights(method='kaiming', *models):\n    for model in models:\n        for module in model.modules():\n\n            if isinstance(module, nn.Conv2d) or isinstance(module, nn.ConvTranspose2d) or isinstance(module, nn.Linear):\n                if method == 'kaiming':\n                    init.kaiming_normal_(module.weight.data, np.sqrt(2.0))\n                elif method == 'xavier':\n                    init.xavier_normal(module.weight.data, np.sqrt(2.0))\n                elif method == 'orthogonal':\n                    init.orthogonal(module.weight.data, np.sqrt(2.0))\n                elif method == 'normal':\n                    init.normal(module.weight.data,mean=0, std=0.02)\n                if module.bias is not None:\n                    init.constant(module.bias.data,0)\n\nclass PyramidPoolingModule(nn.Module):\n    def __init__(self, in_size, in_channels, out_channels, setting):\n        super(PyramidPoolingModule, self).__init__()\n\n        self.features = []\n\n        for s in setting:\n            pool_size = int(math.ceil(float(in_size[0])/s)),int(math.ceil(float(in_size[1])/s))\n            self.features.append(nn.Sequential(nn.AvgPool2d(kernel_size=pool_size,stride=pool_size, ceil_mode=True),\n                                          nn.Conv2d(in_channels, out_channels,kernel_size=1, bias=False),\n                                          nn.BatchNorm2d(out_channels),\n                                          nn.ReLU(inplace=True),\n                                          nn.UpsamplingBilinear2d(size=in_size)))\n\n        self.features = nn.ModuleList(modules=self.features)\n\n    def forward(self,x):\n        out = []\n        out.append(x)\n\n        for m in self.features:\n            out.append(m(x))\n\n        out = torch.cat(out, 1)\n\n        return out\n\n\nclass PSPNet(nn.Module):\n    def __init__(self, num_classes, pretrained=True, **kwargs):\n        super(PSPNet, self).__init__()\n\n        feats = list(models.resnet101(pretrained=pretrained).modules())\n        resent = models.resnet101(pretrained=pretrained)\n\n        self.layer0 = nn.Sequential(resent.conv1, resent.bn1, resent.relu, resent.maxpool)\n        self.layer1 = resent.layer1\n        self.layer2 = resent.layer2\n        self.layer3 = resent.layer3\n        self.layer4 = resent.layer4\n\n\n        for n, m in self.layer3.named_modules():\n            if 'conv2' in n:\n                m.dilation = (2,2)\n                m.padding = (2,2)\n                m.stride = (1,1)\n            if 'downsample.0' in n:\n                m.stride = (1,1)\n\n        for n, m in self.layer4.named_modules():\n            if 'conv2' in n:\n                m.dilation = (4,4)\n                m.padding = (4,4)\n                m.stride = (1,1)\n            if 'downsample.0' in n:\n                m.stride = (1,1)\n\n\n        #NOte that the size of input image is assumed to be 240hx320w\n        self.ppm = PyramidPoolingModule(in_size=(30,40), in_channels=2048, out_channels=512, setting=(1,2,3,6))\n\n        #4x512 = 4096\n        self.final = nn.Sequential(nn.Conv2d(4096, 512, kernel_size=1, stride=1, bias=False),\n                                   nn.BatchNorm2d(512),\n                                   nn.ReLU(inplace=True),\n                                   nn.Conv2d(512, num_classes, kernel_size=1))\n\n        self.activation = nn.Sigmoid()\n        initialize_weights(self.ppm, self.final)\n\n    def forward(self,x):\n\n        input_size = x.size()\n        x = self.layer0(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.ppm(x)\n        x = self.final(x)\n\n        upsample = F.interpolate(x, input_size[2:], mode='bilinear')\n\n        return upsample\n        # return self.activation(upsample)\n\n\n\n"""
pywick/models/segmentation/testnets/resnet.py,3,"b'#!/usr/bin/env python\n# coding: utf-8\n#\n# Author:   Kazuto Nakashima\n# URL:      http://kazuto1011.github.io\n# Created:  2017-11-19\n\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\n\n\nclass _ConvBatchNormReLU(nn.Sequential):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding,\n        dilation,\n        relu=True,\n    ):\n        super(_ConvBatchNormReLU, self).__init__()\n        self.add_module(\n            ""conv"",\n            nn.Conv2d(\n                in_channels=in_channels,\n                out_channels=out_channels,\n                kernel_size=kernel_size,\n                stride=stride,\n                padding=padding,\n                dilation=dilation,\n                bias=False,\n            ),\n        )\n        self.add_module(\n            ""bn"",\n            nn.BatchNorm2d(\n                num_features=out_channels, eps=1e-5, momentum=0.999, affine=True\n            ),\n        )\n\n        if relu:\n            self.add_module(""relu"", nn.ReLU())\n\n    def forward(self, x):\n        return super(_ConvBatchNormReLU, self).forward(x)\n\n\nclass _Bottleneck(nn.Sequential):\n    """"""Bottleneck Unit""""""\n\n    def __init__(\n        self, in_channels, mid_channels, out_channels, stride, dilation, downsample\n    ):\n        super(_Bottleneck, self).__init__()\n        self.reduce = _ConvBatchNormReLU(in_channels, mid_channels, 1, stride, 0, 1)\n        self.conv3x3 = _ConvBatchNormReLU(\n            mid_channels, mid_channels, 3, 1, dilation, dilation\n        )\n        self.increase = _ConvBatchNormReLU(\n            mid_channels, out_channels, 1, 1, 0, 1, relu=False\n        )\n        self.downsample = downsample\n        if self.downsample:\n            self.proj = _ConvBatchNormReLU(\n                in_channels, out_channels, 1, stride, 0, 1, relu=False\n            )\n\n    def forward(self, x):\n        h = self.reduce(x)\n        h = self.conv3x3(h)\n        h = self.increase(h)\n        if self.downsample:\n            h += self.proj(x)\n        else:\n            h += x\n        return F.relu(h)\n\n\nclass _ResBlock(nn.Sequential):\n    """"""Residual Block""""""\n\n    def __init__(\n        self,\n        n_layers,\n        in_channels,\n        mid_channels,\n        out_channels,\n        stride,\n        dilation,\n        mg=None,\n    ):\n        super(_ResBlock, self).__init__()\n\n        if mg is None:\n            mg = [1 for _ in range(n_layers)]\n        else:\n            assert n_layers == len(mg), ""{} values expected, but got: mg={}"".format(\n                n_layers, mg\n            )\n\n        self.add_module(\n            ""block1"",\n            _Bottleneck(\n                in_channels, mid_channels, out_channels, stride, dilation * mg[0], True\n            ),\n        )\n        for i, g in zip(range(2, n_layers + 1), mg[1:]):\n            self.add_module(\n                ""block"" + str(i),\n                _Bottleneck(\n                    out_channels, mid_channels, out_channels, 1, dilation * g, False\n                ),\n            )\n\n    def __call__(self, x):\n        return super(_ResBlock, self).forward(x)\n'"
pywick/models/segmentation/testnets/tiramisu_test.py,5,"b""# Source: https://github.com/BloodAxe/segmentation-networks-benchmark/blob/master/lib/models/tiramisu.py\n\nimport torch\nimport torch.nn as nn\n\n\nclass DenseLayer(nn.Sequential):\n    def __init__(self, in_channels, growth_rate):\n        super().__init__()\n        self.add_module('norm', nn.BatchNorm2d(in_channels))\n        self.add_module('relu', nn.ReLU(True))\n        self.add_module('conv', nn.Conv2d(in_channels, growth_rate, kernel_size=3,\n                                          stride=1, padding=1, bias=True))\n        self.add_module('drop', nn.Dropout2d(0.2))\n\n    def forward(self, x):\n        return super().forward(x)\n\n\nclass DenseBlock(nn.Module):\n    def __init__(self, in_channels, growth_rate, n_layers, upsample=False):\n        super().__init__()\n        self.upsample = upsample\n        self.layers = nn.ModuleList([DenseLayer(\n            in_channels + i * growth_rate, growth_rate)\n            for i in range(n_layers)])\n\n    def forward(self, x):\n        if self.upsample:\n            new_features = []\n            # we pass all previous activations into each dense layer normally\n            # But we only store each dense layer's output in the new_features array\n            for layer in self.layers:\n                out = layer(x)\n                x = torch.cat([x, out], 1)\n                new_features.append(out)\n            return torch.cat(new_features, 1)\n        else:\n            for layer in self.layers:\n                out = layer(x)\n                x = torch.cat([x, out], 1)  # 1 = channel axis\n            return x\n\n\nclass TransitionDown(nn.Sequential):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.add_module('norm', nn.BatchNorm2d(num_features=in_channels))\n        self.add_module('relu', nn.ReLU(inplace=True))\n        self.add_module('conv', nn.Conv2d(in_channels, in_channels,\n                                          kernel_size=1, stride=1,\n                                          padding=0, bias=True))\n        self.add_module('drop', nn.Dropout2d(0.2))\n        self.add_module('maxpool', nn.MaxPool2d(2))\n\n    def forward(self, x):\n        return super().forward(x)\n\n\nclass TransitionUp(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.convTrans = nn.ConvTranspose2d(\n            in_channels=in_channels, out_channels=out_channels,\n            kernel_size=3, stride=2, padding=0, bias=True)\n\n    def forward(self, x, skip):\n        out = self.convTrans(x)\n        out = center_crop(out, skip.size(2), skip.size(3))\n        out = torch.cat([out, skip], 1)\n        return out\n\n\nclass Bottleneck(nn.Sequential):\n    def __init__(self, in_channels, growth_rate, n_layers):\n        super().__init__()\n        self.add_module('bottleneck', DenseBlock(\n            in_channels, growth_rate, n_layers, upsample=True))\n\n    def forward(self, x):\n        return super().forward(x)\n\n\ndef center_crop(layer, max_height, max_width):\n    _, _, h, w = layer.size()\n    xy1 = (w - max_width) // 2\n    xy2 = (h - max_height) // 2\n    return layer[:, :, xy2:(xy2 + max_height), xy1:(xy1 + max_width)]\n\n\nclass FCDenseNet(nn.Module):\n    def __init__(self, num_classes=12, in_channels=3, down_blocks=(5, 5, 5, 5, 5),\n                 up_blocks=(5, 5, 5, 5, 5), bottleneck_layers=5,\n                 growth_rate=16, out_chans_first_conv=48, **kwargs):\n        super().__init__()\n        self.num_classes = num_classes\n        self.down_blocks = down_blocks\n        self.up_blocks = up_blocks\n        cur_channels_count = 0\n        skip_connection_channel_counts = []\n\n        ## First Convolution ##\n\n        self.add_module('firstconv', nn.Conv2d(in_channels=in_channels,\n                                               out_channels=out_chans_first_conv, kernel_size=3,\n                                               stride=1, padding=1, bias=True))\n        cur_channels_count = out_chans_first_conv\n\n        #####################\n        # Downsampling path #\n        #####################\n\n        self.denseBlocksDown = nn.ModuleList([])\n        self.transDownBlocks = nn.ModuleList([])\n        for i in range(len(down_blocks)):\n            self.denseBlocksDown.append(\n                DenseBlock(cur_channels_count, growth_rate, down_blocks[i]))\n            cur_channels_count += (growth_rate * down_blocks[i])\n            skip_connection_channel_counts.insert(0, cur_channels_count)\n            self.transDownBlocks.append(TransitionDown(cur_channels_count))\n\n        #####################\n        #     Bottleneck    #\n        #####################\n\n        self.add_module('bottleneck', Bottleneck(cur_channels_count,\n                                                 growth_rate, bottleneck_layers))\n        prev_block_channels = growth_rate * bottleneck_layers\n        cur_channels_count += prev_block_channels\n\n        #######################\n        #   Upsampling path   #\n        #######################\n\n        self.transUpBlocks = nn.ModuleList([])\n        self.denseBlocksUp = nn.ModuleList([])\n        for i in range(len(up_blocks) - 1):\n            self.transUpBlocks.append(TransitionUp(prev_block_channels, prev_block_channels))\n            cur_channels_count = prev_block_channels + skip_connection_channel_counts[i]\n\n            self.denseBlocksUp.append(DenseBlock(\n                cur_channels_count, growth_rate, up_blocks[i],\n                upsample=True))\n            prev_block_channels = growth_rate * up_blocks[i]\n            cur_channels_count += prev_block_channels\n\n        ## Final DenseBlock ##\n\n        self.transUpBlocks.append(TransitionUp(\n            prev_block_channels, prev_block_channels))\n        cur_channels_count = prev_block_channels + skip_connection_channel_counts[-1]\n\n        self.denseBlocksUp.append(DenseBlock(\n            cur_channels_count, growth_rate, up_blocks[-1],\n            upsample=False))\n        cur_channels_count += growth_rate * up_blocks[-1]\n\n        ## Softmax ##\n\n        self.finalConv = nn.Conv2d(in_channels=cur_channels_count,\n                                   out_channels=num_classes, kernel_size=1, stride=1,\n                                   padding=0, bias=True)\n\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, x):\n        out = self.firstconv(x)\n\n        skip_connections = []\n        for i in range(len(self.down_blocks)):\n            out = self.denseBlocksDown[i](out)\n            skip_connections.append(out)\n            out = self.transDownBlocks[i](out)\n\n        out = self.bottleneck(out)\n        for i in range(len(self.up_blocks)):\n            skip = skip_connections.pop()\n            out = self.transUpBlocks[i](out, skip)\n            out = self.denseBlocksUp[i](out)\n\n        out = self.finalConv(out)\n        return out\n\n\ndef FCDenseNet57(num_classes, **kwargs):\n    return FCDenseNet(\n        in_channels=3, down_blocks=(4, 4, 4, 4, 4),\n        up_blocks=(4, 4, 4, 4, 4), bottleneck_layers=4,\n        growth_rate=12, out_chans_first_conv=48, num_classes=num_classes, **kwargs)\n\n\ndef FCDenseNet67(num_classes, **kwargs):\n    return FCDenseNet(\n        in_channels=3, down_blocks=(5, 5, 5, 5, 5),\n        up_blocks=(5, 5, 5, 5, 5), bottleneck_layers=5,\n        growth_rate=16, out_chans_first_conv=48, num_classes=num_classes, **kwargs)\n\n\ndef FCDenseNet103(num_classes, **kwargs):\n    return FCDenseNet(\n        in_channels=3, down_blocks=(4, 5, 7, 10, 12),\n        up_blocks=(12, 10, 7, 5, 4), bottleneck_layers=15,\n        growth_rate=16, out_chans_first_conv=48, num_classes=num_classes, **kwargs)\n"""
pywick/models/segmentation/testnets/unet_plus_plus.py,12,"b'# Source: https://github.com/keng000/pytorch_unet_plus_plus/blob/master/unet_plus_plus.py (License: MIT)\n\nimport torch\nimport torch.nn as nn\n\n\nclass ConvSamePad2d(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, bias: bool = True):\n        super().__init__()\n\n        left_top_pad = right_bottom_pad = kernel_size // 2\n        if kernel_size % 2 == 0:\n            right_bottom_pad -= 1\n\n        self.layer = nn.Sequential(\n            nn.ReflectionPad2d((left_top_pad, right_bottom_pad, left_top_pad, right_bottom_pad)),\n            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, bias=bias)\n        )\n\n    def forward(self, inputs):\n        return self.layer(inputs)\n\n\nclass StandardUnit(nn.Module):\n    def __init__(self, in_channels, out_channels, drop_rate=0.5):\n        super().__init__()\n        self.layer = nn.Sequential(\n            ConvSamePad2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3),\n            nn.Dropout2d(p=drop_rate),\n            ConvSamePad2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3),\n            nn.Dropout2d(p=drop_rate)\n        )\n\n    def forward(self, inputs):\n        return self.layer(inputs)\n\n\nclass Final1x1ConvLayer(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n\n        self.layer = nn.Sequential(\n            ConvSamePad2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, inputs):\n        return self.layer(inputs)\n\n\nclass NestNet(nn.Module):\n    def __init__(self, num_classes, in_channels=3, deep_supervision=True, **kwargs):\n        super().__init__()\n        self.deep_supervision = deep_supervision\n\n        filters = [32, 64, 128, 256, 512]\n\n        # j == 0\n        self.x_00 = StandardUnit(in_channels=in_channels, out_channels=filters[0])\n        self.pool0 = nn.MaxPool2d(kernel_size=2)\n\n        self.x_01 = StandardUnit(in_channels=filters[0] * 2, out_channels=filters[0])\n        self.x_02 = StandardUnit(in_channels=filters[0] * 3, out_channels=filters[0])\n        self.x_03 = StandardUnit(in_channels=filters[0] * 4, out_channels=filters[0])\n        self.x_04 = StandardUnit(in_channels=filters[0] * 5, out_channels=filters[0])\n\n        self.up_10_to_01 = nn.ConvTranspose2d(in_channels=filters[1], out_channels=filters[0], kernel_size=2, stride=2)\n        self.up_11_to_02 = nn.ConvTranspose2d(in_channels=filters[1], out_channels=filters[0], kernel_size=2, stride=2)\n        self.up_12_to_03 = nn.ConvTranspose2d(in_channels=filters[1], out_channels=filters[0], kernel_size=2, stride=2)\n        self.up_13_to_04 = nn.ConvTranspose2d(in_channels=filters[1], out_channels=filters[0], kernel_size=2, stride=2)\n\n        # j == 1\n        self.x_10 = StandardUnit(in_channels=filters[0], out_channels=filters[1])\n        self.pool1 = nn.MaxPool2d(kernel_size=2)\n\n        self.x_11 = StandardUnit(in_channels=filters[1] * 2, out_channels=filters[1])\n        self.x_12 = StandardUnit(in_channels=filters[1] * 3, out_channels=filters[1])\n        self.x_13 = StandardUnit(in_channels=filters[1] * 4, out_channels=filters[1])\n\n        self.up_20_to_11 = nn.ConvTranspose2d(in_channels=filters[2], out_channels=filters[1], kernel_size=2, stride=2)\n        self.up_21_to_12 = nn.ConvTranspose2d(in_channels=filters[2], out_channels=filters[1], kernel_size=2, stride=2)\n        self.up_22_to_13 = nn.ConvTranspose2d(in_channels=filters[2], out_channels=filters[1], kernel_size=2, stride=2)\n\n        # j == 2\n        self.x_20 = StandardUnit(in_channels=filters[1], out_channels=filters[2])\n        self.pool2 = nn.MaxPool2d(kernel_size=2)\n\n        self.x_21 = StandardUnit(in_channels=filters[2] * 2, out_channels=filters[2])\n        self.x_22 = StandardUnit(in_channels=filters[2] * 3, out_channels=filters[2])\n\n        self.up_30_to_21 = nn.ConvTranspose2d(in_channels=filters[3], out_channels=filters[2], kernel_size=2, stride=2)\n        self.up_31_to_22 = nn.ConvTranspose2d(in_channels=filters[3], out_channels=filters[2], kernel_size=2, stride=2)\n\n        # j == 3\n        self.x_30 = StandardUnit(in_channels=filters[2], out_channels=filters[3])\n        self.pool3 = nn.MaxPool2d(kernel_size=2)\n\n        self.x_31 = StandardUnit(in_channels=filters[3] * 2, out_channels=filters[3])\n\n        self.up_40_to_31 = nn.ConvTranspose2d(in_channels=filters[4], out_channels=filters[3], kernel_size=2, stride=2)\n\n        # j == 4\n        self.x_40 = StandardUnit(in_channels=filters[3], out_channels=filters[4])\n\n        # 1x1 conv layer\n        self.final_1x1_x01 = Final1x1ConvLayer(in_channels=filters[0], out_channels=num_classes)\n        self.final_1x1_x02 = Final1x1ConvLayer(in_channels=filters[0], out_channels=num_classes)\n        self.final_1x1_x03 = Final1x1ConvLayer(in_channels=filters[0], out_channels=num_classes)\n        self.final_1x1_x04 = Final1x1ConvLayer(in_channels=filters[0], out_channels=num_classes)\n\n    def forward(self, inputs, L=4):\n        if not (1 <= L <= 4):\n            raise ValueError(""the model pruning factor `L` should be 1 <= L <= 4"")\n\n        x_00_output = self.x_00(inputs)\n        x_10_output = self.x_10(self.pool0(x_00_output))\n        x_10_up_sample = self.up_10_to_01(x_10_output)\n        x_01_output = self.x_01(torch.cat([x_00_output, x_10_up_sample], 1))\n        nestnet_output_1 = self.final_1x1_x01(x_01_output)\n\n        if L == 1:\n            return nestnet_output_1\n\n        x_20_output = self.x_20(self.pool1(x_10_output))\n        x_20_up_sample = self.up_20_to_11(x_20_output)\n        x_11_output = self.x_11(torch.cat([x_10_output, x_20_up_sample], 1))\n        x_11_up_sample = self.up_11_to_02(x_11_output)\n        x_02_output = self.x_02(torch.cat([x_00_output, x_01_output, x_11_up_sample], 1))\n        nestnet_output_2 = self.final_1x1_x01(x_02_output)\n\n        if L == 2:\n            if self.deep_supervision:\n                # return the average of output layers\n                return (nestnet_output_1 + nestnet_output_2) / 2\n            else:\n                return nestnet_output_2\n\n        x_30_output = self.x_30(self.pool2(x_20_output))\n        x_30_up_sample = self.up_30_to_21(x_30_output)\n        x_21_output = self.x_21(torch.cat([x_20_output, x_30_up_sample], 1))\n        x_21_up_sample = self.up_21_to_12(x_21_output)\n        x_12_output = self.x_12(torch.cat([x_10_output, x_11_output, x_21_up_sample], 1))\n        x_12_up_sample = self.up_12_to_03(x_12_output)\n        x_03_output = self.x_03(torch.cat([x_00_output, x_01_output, x_02_output, x_12_up_sample], 1))\n        nestnet_output_3 = self.final_1x1_x01(x_03_output)\n\n        if L == 3:\n            # return the average of output layers\n            if self.deep_supervision:\n                return (nestnet_output_1 + nestnet_output_2 + nestnet_output_3) / 3\n            else:\n                return nestnet_output_3\n\n        x_40_output = self.x_40(self.pool3(x_30_output))\n        x_40_up_sample = self.up_40_to_31(x_40_output)\n        x_31_output = self.x_31(torch.cat([x_30_output, x_40_up_sample], 1))\n        x_31_up_sample = self.up_31_to_22(x_31_output)\n        x_22_output = self.x_22(torch.cat([x_20_output, x_21_output, x_31_up_sample], 1))\n        x_22_up_sample = self.up_22_to_13(x_22_output)\n        x_13_output = self.x_13(torch.cat([x_10_output, x_11_output, x_12_output, x_22_up_sample], 1))\n        x_13_up_sample = self.up_13_to_04(x_13_output)\n        x_04_output = self.x_04(torch.cat([x_00_output, x_01_output, x_02_output, x_03_output, x_13_up_sample], 1))\n        nestnet_output_4 = self.final_1x1_x01(x_04_output)\n\n        if L == 4:\n            if self.deep_supervision:\n                # return the average of output layers\n                return (nestnet_output_1 + nestnet_output_2 + nestnet_output_3 + nestnet_output_4) / 4\n            else:\n                return nestnet_output_4\n\n\nif __name__ == \'__main__\':\n    inputs = torch.rand((3, 1, 96, 96)).cuda()\n\n    unet_plus_plus = NestNet(in_channels=1, num_classes=3).cuda()\n\n    from datetime import datetime\n\n    st = datetime.now()\n    output = unet_plus_plus(inputs, L=1)\n    print(f""{(datetime.now() - st).total_seconds(): .4f}s"")\n'"
pywick/models/segmentation/testnets/unet_se.py,3,"b""# Source: https://github.com/areum-lee/SENet_Segmentation\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass UNetDec(nn.Module):\n\n    def __init__(self, in_channels, out_channels):\n        super(UNetDec, self).__init__()\n\n        self.up = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(out_channels),\n            nn.ConvTranspose2d(out_channels, out_channels, 2, stride=2),\n            nn.ReLU(inplace=True),\n        )\n\n        self.SE1 = nn.Conv2d(in_channels, in_channels // 16, kernel_size=1)\n        self.SE2 = nn.Conv2d(in_channels // 16, in_channels, kernel_size=1)\n\n    def forward(self, x):\n        fm_size = x.size()[2]\n        scale_weight = F.avg_pool2d(x, fm_size)\n        scale_weight = F.relu(self.SE1(scale_weight))\n        scale_weight = F.sigmoid(self.SE2(scale_weight))\n        x = x * scale_weight.expand_as(x)\n        out = self.up(x)\n        return out\n\n\nclass Dilated_UNetEnc(nn.Module):\n\n    def __init__(self, in_channels, out_channels, dropout=False):\n        super(Dilated_UNetEnc, self).__init__()\n\n        layers = [\n            nn.Conv2d(in_channels, out_channels, 3, padding=1, dilation=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, padding=1, dilation=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, stride=2, ceil_mode=True)\n\n        ]\n        if dropout:\n            layers += [nn.Dropout(.5)]\n\n        self.down = nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.down(x)\n\n        return out\n\n\nclass Dilated_Bottleneck_block(nn.Module):\n    def __init__(self, in_channels, out_channels, dilation_rate, dropout=False):\n        super(Dilated_Bottleneck_block, self).__init__()\n\n        layers = [\n            nn.Conv2d(in_channels, out_channels, 3, padding=dilation_rate, dilation=dilation_rate, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)]\n\n        if dropout:\n            layers += [nn.Dropout(.5)]\n\n        self.center = nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.center(x)\n\n        return out\n\n\n# Example: model = Dilated_UNet(inChannel=2, num_classes=2, init_features=32, network_depth=3, bottleneck_layers=3))\nclass Dilated_UNet(nn.Module):\n\n    def __init__(self, inChannel, num_classes, init_features, network_depth, bottleneck_layers):\n        super(Dilated_UNet, self).__init__()\n\n        self.network_depth = network_depth\n        self.bottleneck_layers = bottleneck_layers\n        skip_connection_channel_counts = []\n\n        self.add_module('firstconv', nn.Conv2d(in_channels=inChannel,\n                                               out_channels=init_features, kernel_size=3,\n                                               stride=1, padding=1, bias=True))\n\n        self.encodingBlocks = nn.ModuleList([])\n        features = init_features\n\n        for i in range(self.network_depth):\n            self.encodingBlocks.append(Dilated_UNetEnc(features, 2 * features))\n\n            skip_connection_channel_counts.insert(0, 2 * features)\n            features *= 2\n        final_encoding_channels = skip_connection_channel_counts[0]\n\n        self.bottleNecks = nn.ModuleList([])\n        for i in range(self.bottleneck_layers):\n            dilation_factor = 1\n            self.bottleNecks.append(Dilated_Bottleneck_block(final_encoding_channels,\n                                                             final_encoding_channels, dilation_rate=dilation_factor))\n\n        self.decodingBlocks = nn.ModuleList([])\n        for i in range(self.network_depth):\n            if i == 0:\n                prev_deconv_channels = final_encoding_channels\n            self.decodingBlocks.append(UNetDec(prev_deconv_channels + skip_connection_channel_counts[i],\n                                               skip_connection_channel_counts[i]))\n            prev_deconv_channels = skip_connection_channel_counts[i]\n\n        self.final = nn.Conv2d(64, num_classes, 1)\n\n    def forward(self, x):\n        out = self.firstconv(x)\n\n        skip_connections = []\n        for i in range(self.network_depth):\n            out = self.encodingBlocks[i](out)\n            skip_connections.append(out)\n\n        for i in range(self.bottleneck_layers):\n            out = self.bottleNecks[i](out)\n\n        for i in range(self.network_depth):\n            skip = skip_connections.pop()\n            out = self.decodingBlocks[i](torch.cat([out, skip], 1))\n\n        out = self.final(out)\n        return out"""
pywick/models/segmentation/testnets/exfuse/ExFuseLayer.py,3,"b'import torch\nimport torch.nn as nn\n\n\n# Conv-Norm-Activation\nclass CNA(nn.Module):\n    def __init__(self, in_c, out_c, kernel_size=3, stride=1, padding=1, norm=nn.InstanceNorm2d, act=nn.ReLU):\n        super(CNA, self).__init__()\n        self.layer = nn.Sequential(nn.Conv2d(in_c, out_c, kernel_size, stride, padding), norm(out_c), act(True))\n\n    def forward(self, x):\n        return self.layer(x)\n\n\n# UpConv-Norm-Activation\nclass UpCNA(nn.Module):\n    def __init__(self, in_c, out_c, kernel_size=4, stride=2, padding=1, norm=nn.InstanceNorm2d, act=nn.ReLU):\n        super(UpCNA, self).__init__()\n        self.layer = nn.Sequential(nn.ConvTranspose2d(in_c, out_c, kernel_size, stride, padding),\n                                   norm(out_c), act(True))\n\n    def forward(self, x):\n        return self.layer(x)\n\n\n# Semantic Embedding Branch, Fig 4\nclass SEB_dw(nn.Module):\n    def __init__(self, low_feature, high_feature,\n                 norm=nn.InstanceNorm2d, up_scale=2):\n        super(SEB_dw, self).__init__()\n        self.conv = CNA(high_feature, low_feature, norm=norm)\n        self.up = nn.UpsamplingBilinear2d(scale_factor=up_scale)\n\n    def forward(self, low_feature, high_feature):\n        high_feature = self.conv(high_feature)\n        high_feature = self.up(high_feature)\n        return low_feature * high_feature # element wise mul\n\n\n# Orignal Paper Impl\nclass SEB(nn.Module):\n    def __init__(self, low_feature, high_features,\n                 norm=nn.InstanceNorm2d, up_scale=2):\n        super(SEB, self).__init__()\n        self.sebs = []\n        for c in range(len(high_features) - 1, 0, -1):\n            self.sebs.append(nn.Sequential(CNA(high_features[c], high_features[c - 1], norm=norm),\n                                           nn.UpsamplingBilinear2d(scale_factor=up_scale)))\n\n    def forward(self, low_feature, *high_features):\n        high_features = reversed(high_features)\n        \n        low_feature = self.seb[0](high_features[0]) * high_features[1]\n        for c in range(1, len(high_features)):\n            high_feature = self.sebs[c](high_features[c])\n            low_feature *= high_feature\n            \n        return low_feature  # element wise mul\n\n\n# Global Convolution Network\n# https://github.com/ycszen/pytorch-segmentation\n# https://arxiv.org/pdf/1703.02719.pdf\nclass GCN(nn.Module):\n    def __init__(self, in_c, out_c, ks=7, norm=nn.InstanceNorm2d):\n        super(GCN, self).__init__()\n        self.conv_l1 = CNA(in_c, out_c, kernel_size=(ks, 1), padding=(ks // 2, 0), norm=norm)\n        self.conv_l2 = CNA(out_c, out_c, kernel_size=(1, ks), padding=(0, ks // 2), norm=norm)\n\n        self.conv_r1 = CNA(in_c, out_c, kernel_size=(1, ks), padding=(0, ks // 2), norm=norm)\n        self.conv_r2 = CNA(out_c, out_c, kernel_size=(ks, 1), padding=(ks // 2, 0), norm=norm)\n\n    def forward(self, x):\n        x_l = self.conv_l1(x)\n        x_l = self.conv_l2(x_l)\n\n        x_r = self.conv_r1(x)\n        x_r = self.conv_r2(x_r)\n\n        return x_l + x_r\n\n\n# Explicit Channel Resolution Embedding\nclass ECRE(nn.Module):\n    def __init__(self, in_c, up_scale=2, norm=nn.InstanceNorm2d):\n        super(ECRE, self).__init__()\n        self.ecre = nn.Sequential(CNA(in_c, in_c * up_scale * up_scale, norm=norm), nn.PixelShuffle(up_scale))\n\n    def forward(self, input_):\n        return self.ecre(input_)\n\n\n# Densely Adjacent Prediction\nclass DAP(nn.Module):\n    def __init__(self, in_c, k=3, norm=nn.InstanceNorm2d):\n        super(DAP, self).__init__()\n        self.k2 = k * k\n        self.conv = CNA(in_c, in_c * k * k, norm=norm)\n        self.padd = nn.ZeroPad2d(k // 2)\n\n    def forward(self, input_):\n        batch, input_c, max_i, max_j = input_.shape\n        x = self.conv(input_)\n        x = self.padd(x)\n\n        # It works only k=3.\n        # TODO : Make beautiful\n        a = [(0, max_i,     0, max_j), (0, max_i,     1, max_j + 1), (0, max_i,     2, max_j + 2),\n             (1, max_i + 1, 0, max_j), (1, max_i + 1, 1, max_j + 1), (1, max_i + 1, 2, max_j + 2),\n             (2, max_i + 2, 0, max_j), (2, max_i + 2, 1, max_j + 1), (2, max_i + 2, 2, max_j + 2)]\n\n        R = torch.zeros([batch, input_c, self.k2, max_i * max_j]).cuda()\n        for dap_c in range(input_c):\n            for c, (s_i, e_i, s_j, e_j) in enumerate(a):\n                R[:, dap_c, c] = x[:, c, s_i:e_i, s_j:e_j].contiguous().view(batch, -1)\n\n        R = torch.mean(R, 2).reshape(batch, input_c, max_i, max_j)\n        return R\n\n\n# Each Step of Framework\nclass ExFuseLevel(nn.Module):\n    def __init__(self, in_c, out_c=21, norm=nn.InstanceNorm2d):\n        super(ExFuseLevel, self).__init__()\n        self.seb = SEB(in_c * 2, in_c, norm=norm)\n        self.gcn = GCN(in_c, out_c, norm=norm)\n        self.upconv = nn.Sequential(nn.ConvTranspose2d(out_c, out_c, 3, 2, 1, output_padding=1),\n                                    norm(out_c),\n                                    nn.ReLU(True))\n\n    def forward(self, low_level, high_level, prev_feature):\n        level = self.seb(low_level, high_level)\n        level = self.gcn(level)\n\n        return self.upconv(level + prev_feature)\n\n\n# Each Step of Framework\nclass UnetExFuseLevel(nn.Module):\n    def __init__(self, in_c, out_c=21, norm=nn.InstanceNorm2d):\n        super(UnetExFuseLevel, self).__init__()\n        self.seb = SEB(in_c * 2, in_c, norm=norm)\n        self.gcn = GCN(in_c, in_c, norm=norm)\n        self.upconv = nn.Sequential(nn.ConvTranspose2d(in_c, out_c, 3, 2, 1, output_padding=1),\n                                    norm(out_c),\n                                    nn.ReLU(True))\n\n    def forward(self, low_level, high_level, prev_feature):\n        level = self.seb(low_level, high_level)\n        level = self.gcn(level)\n\n        return self.upconv(level + prev_feature)\n'"
pywick/models/segmentation/testnets/exfuse/UnetExFuse.py,3,"b'# Source: https://github.com/rplab-snu/nucleus_segmentation\n\n""""""\nImplementation of `ExFuse: Enhancing Feature Fusion for SemanticSegmentation <https://arxiv.org/abs/1804.03821>`_\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .unet_layer import UnetConv2D, UnetUpConv2D, weights_init_kaiming, ConvBNReLU\nfrom .ExFuseLayer import SEB, GCN, ECRE, DAP, UnetExFuseLevel\n\n\n__all__ = [\'UnetExFuse\', \'UnetGCN\', \'UnetGCNECRE\', \'UnetGCNECRE_v2\', \'UnetGCNSEB\']\n\n\nclass UnetGCN(nn.Module):\n\n    def __init__(self, feature_scale=4, n_classes=1, is_deconv=True, norm=nn.InstanceNorm2d, is_pool=True):\n        super(UnetGCN, self).__init__()\n        filters = [64, 128, 256, 512, 1024]\n        filters = [x // feature_scale for x in filters]\n\n        # downsampling\n        self.conv1 = UnetConv2D(1, filters[0], norm)\n        self.gcn1 = GCN(filters[0], filters[0])\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2) if is_pool else ConvBNReLU(filters[0], filters[0], norm, stride=2)\n\n        self.conv2 = UnetConv2D(filters[0], filters[1], norm)\n        self.gcn2 = GCN(filters[1], filters[1])\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2) if is_pool else ConvBNReLU(filters[1], filters[1], norm, stride=2)\n\n        self.conv3 = UnetConv2D(filters[1], filters[2], norm)\n        self.gcn3 = GCN(filters[2], filters[2])\n        self.maxpool3 = nn.MaxPool2d(kernel_size=2) if is_pool else ConvBNReLU(filters[2], filters[2], norm, stride=2)\n\n        self.conv4 = UnetConv2D(filters[2], filters[3], norm)\n        self.gcn4 = GCN(filters[3], filters[3])\n        self.maxpool4 = nn.MaxPool2d(kernel_size=2) if is_pool else ConvBNReLU(filters[3], filters[3], norm, stride=2)\n\n        self.center = UnetConv2D(filters[3], filters[4], norm)\n\n        # upsampling\n        self.up_concat4 = UnetUpConv2D(filters[4], filters[3], norm, is_deconv)\n        self.up_concat3 = UnetUpConv2D(filters[3], filters[2], norm, is_deconv)\n        self.up_concat2 = UnetUpConv2D(filters[2], filters[1], norm, is_deconv)\n        self.up_concat1 = UnetUpConv2D(filters[1], filters[0], norm, is_deconv)\n\n        # final conv (without any concat)\n        self.final = nn.Conv2d(filters[0], n_classes, 1)\n\n        # initialise weights\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                m.apply(weights_init_kaiming)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.apply(weights_init_kaiming)\n\n    def forward(self, inputs):\n        conv1 = self.conv1(inputs)\n        conv1 = self.gcn1(conv1)\n        maxpool1 = self.maxpool1(conv1)\n\n        conv2 = self.conv2(maxpool1)\n        conv2 = self.gcn2(conv2)\n        maxpool2 = self.maxpool2(conv2)\n\n        conv3 = self.conv3(maxpool2)\n        conv3 = self.gcn3(conv3)\n        maxpool3 = self.maxpool3(conv3)\n\n        conv4 = self.conv4(maxpool3)\n        conv4 = self.gcn4(conv4)\n        maxpool4 = self.maxpool4(conv4)\n\n        center = self.center(maxpool4)\n\n        up4 = self.up_concat4(conv4, center)\n        up3 = self.up_concat3(conv3, up4)\n        up2 = self.up_concat2(conv2, up3)\n        up1 = self.up_concat1(conv1, up2)\n\n        final = self.final(up1)\n        return final\n\n\nclass UnetGCNSEB(nn.Module):\n\n    def __init__(self, feature_scale=4, n_classes=1, is_deconv=True, norm=nn.InstanceNorm2d, is_pool=True):\n        super(UnetGCNSEB, self).__init__()\n        filters = [64, 128, 256, 512, 1024]\n        filters = [x // feature_scale for x in filters]\n\n        # downsampling\n        self.conv1 = UnetConv2D(1, filters[0], norm)\n        self.gcn1 = GCN(filters[0], filters[0])\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2) if is_pool else ConvBNReLU(filters[0], filters[0], norm, stride=2)\n\n        self.conv2 = UnetConv2D(filters[0], filters[1], norm)\n        self.gcn2 = GCN(filters[1], filters[1])\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2) if is_pool else ConvBNReLU(filters[1], filters[1], norm, stride=2)\n\n        self.conv3 = UnetConv2D(filters[1], filters[2], norm)\n        self.gcn3 = GCN(filters[2], filters[2])\n        self.maxpool3 = nn.MaxPool2d(kernel_size=2) if is_pool else ConvBNReLU(filters[2], filters[2], norm, stride=2)\n\n        self.conv4 = UnetConv2D(filters[2], filters[3], norm)\n        self.gcn4 = GCN(filters[3], filters[3])\n        self.maxpool4 = nn.MaxPool2d(kernel_size=2) if is_pool else ConvBNReLU(filters[3], filters[3], norm, stride=2)\n        self.center = UnetConv2D(filters[3], filters[4], norm)\n\n        # upsampling\n        self.up_concat4 = SEB(filters[4], filters[3])\n        self.up_concat3 = SEB(filters[3], filters[2])\n        self.up_concat2 = SEB(filters[2], filters[1])\n        self.up_concat1 = SEB(filters[1], filters[0])\n\n        # final conv (without any concat)\n        self.final = nn.Conv2d(filters[0], 1, 1)\n\n        # initialise weights\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                m.apply(weights_init_kaiming)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.apply(weights_init_kaiming)\n\n    def forward(self, inputs):\n        conv1 = self.conv1(inputs)\n        conv1 = self.gcn1(conv1)\n        maxpool1 = self.maxpool1(conv1)\n\n        conv2 = self.conv2(maxpool1)\n        conv2 = self.gcn2(conv2)\n        maxpool2 = self.maxpool2(conv2)\n\n        conv3 = self.conv3(maxpool2)\n        conv3 = self.gcn3(conv3)\n        maxpool3 = self.maxpool3(conv3)\n\n        conv4 = self.conv4(maxpool3)\n        conv4 = self.gcn4(conv4)\n        maxpool4 = self.maxpool4(conv4)\n\n        center = self.center(maxpool4)\n\n        up4 = self.up_concat4(conv4, center)\n        up3 = self.up_concat3(conv3, up4)\n        up2 = self.up_concat2(conv2, up3)\n        up1 = self.up_concat1(conv1, up2)\n\n        final = self.final(up1)\n        return final\n\n\nclass UnetUpECRE(nn.Module):\n    def __init__(self, in_size, out_size, norm, is_deconv=False):\n        super(UnetUpECRE, self).__init__()\n\n        self.conv = UnetConv2D(in_size + out_size, out_size, norm)\n        self.up = ECRE(in_size)\n\n        # initialise the blocks\n        for m in self.children():\n            if m.__class__.__name__.find(\'UnetConv2D\') != -1:\n                continue\n            m.apply(weights_init_kaiming)\n\n    def forward(self, input1, input2):\n        output2 = self.up(input2)\n        offset = output2.size()[2] - input1.size()[2]\n        padding = [offset // 2] * 4\n        output1 = F.pad(input1, padding)\n        output = torch.cat([output1, output2], 1)\n        return self.conv(output), output2\n\n\nclass UnetGCNECRE(nn.Module):\n\n    def __init__(self, feature_scale=4, n_classes=1, is_deconv=True, norm=nn.InstanceNorm2d, is_pool=True):\n        super(UnetGCNECRE, self).__init__()\n        filters = [64, 128, 256, 512, 1024]\n        filters = [x // feature_scale for x in filters]\n\n        # downsampling\n        self.conv1 = UnetConv2D(1, filters[0], norm)\n        self.gcn1 = GCN(filters[0], filters[0])\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2) if is_pool else ConvBNReLU(filters[0], filters[0], norm, stride=2)\n\n        self.conv2 = UnetConv2D(filters[0], filters[1], norm)\n        self.gcn2 = GCN(filters[1], filters[1])\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2) if is_pool else ConvBNReLU(filters[1], filters[1], norm, stride=2)\n\n        self.conv3 = UnetConv2D(filters[1], filters[2], norm)\n        self.gcn3 = GCN(filters[2], filters[2])\n        self.maxpool3 = nn.MaxPool2d(kernel_size=2) if is_pool else ConvBNReLU(filters[2], filters[2], norm, stride=2)\n\n        self.conv4 = UnetConv2D(filters[2], filters[3], norm)\n        self.gcn4 = GCN(filters[3], filters[3])\n        self.maxpool4 = nn.MaxPool2d(kernel_size=2) if is_pool else ConvBNReLU(filters[3], filters[3], norm, stride=2)\n        self.center = UnetConv2D(filters[3], filters[4], norm)\n\n        # upsampling\n        self.up_concat4 = UnetUpECRE(filters[4], filters[3], norm)\n        self.up_concat3 = UnetUpECRE(filters[3], filters[2], norm)\n        self.up_concat2 = UnetUpECRE(filters[2], filters[1], norm)\n        self.up_concat1 = UnetUpECRE(filters[1], filters[0], norm)\n\n        # final conv (without any concat)\n        self.final = nn.Conv2d(filters[0], n_classes, 1)\n\n        # For aux loss\n        self.ecre4 = ConvBNReLU(filters[4], 1, norm, stride=2)\n        self.ecre3 = ConvBNReLU(filters[3], 1, norm, stride=2)\n        self.ecre2 = ConvBNReLU(filters[2], 1, norm, stride=2)\n        self.ecre1 = ConvBNReLU(filters[1], 1, norm, stride=2)\n\n        # initialise weights\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                m.apply(weights_init_kaiming)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.apply(weights_init_kaiming)\n\n    def forward(self, inputs):\n        conv1 = self.conv1(inputs)\n        conv1 = self.gcn1(conv1)\n        maxpool1 = self.maxpool1(conv1)\n\n        conv2 = self.conv2(maxpool1)\n        conv2 = self.gcn2(conv2)\n        maxpool2 = self.maxpool2(conv2)\n\n        conv3 = self.conv3(maxpool2)\n        conv3 = self.gcn3(conv3)\n        maxpool3 = self.maxpool3(conv3)\n\n        conv4 = self.conv4(maxpool3)\n        conv4 = self.gcn4(conv4)\n        maxpool4 = self.maxpool4(conv4)\n\n        center = self.center(maxpool4)\n\n        up4, ecre4 = self.up_concat4(conv4, center)\n        up3, ecre3 = self.up_concat3(conv3, up4)\n        up2, ecre2 = self.up_concat2(conv2, up3)\n        up1, ecre1 = self.up_concat1(conv1, up2)\n\n        final = self.final(up1)\n        return final # , self.ecre4(ecre4), self.ecre3(ecre3), self.ecre2(ecre2), self.ecre1(ecre1) \n\n\nclass UnetGCNECRE_v2(UnetGCNECRE):\n    # Move GCN Module in forward\n    def __init__(self, feature_scale=4, n_classes=1, is_deconv=True, norm=nn.InstanceNorm2d, is_pool=True):\n        super(UnetGCNECRE_v2, self).__init__(feature_scale=feature_scale, n_classes=n_classes, is_deconv=is_deconv, norm=norm, is_pool=is_pool)\n\n    def forward(self, inputs):\n        conv1 = self.conv1(inputs)\n        gcn1 = self.gcn1(conv1)\n        maxpool1 = self.maxpool1(conv1)\n\n        conv2 = self.conv2(maxpool1)\n        gcn2 = self.gcn2(conv2)\n        maxpool2 = self.maxpool2(conv2)\n\n        conv3 = self.conv3(maxpool2)\n        gcn3 = self.gcn3(conv3)\n        maxpool3 = self.maxpool3(conv3)\n\n        conv4 = self.conv4(maxpool3)\n        gcn4 = self.gcn4(conv4)\n        maxpool4 = self.maxpool4(conv4)\n\n        center = self.center(maxpool4)\n\n        up4 = self.up_concat4(gcn4, center)\n        up3 = self.up_concat3(gcn3, up4)\n        up2 = self.up_concat2(gcn2, up3)\n        up1 = self.up_concat1(gcn1, up2)\n\n        final = self.final(up1)\n        return final\n\n\nclass UnetGCNECRE_v3(UnetGCNECRE):\n    # Add Auxiliary Supervision Loss\n    # For Exfuse trainer\n    def __init__(self, feature_scale=4, n_classes=1,\n                 is_deconv=True, norm=nn.InstanceNorm2d, is_pool=True):\n        super(UnetGCNECRE_v3, self).__init__(feature_scale=feature_scale, n_classes=n_classes,\n                                             is_deconv=is_deconv, norm=norm, is_pool=is_pool)\n\n    def forward(self, inputs):\n        conv1 = self.conv1(inputs)\n        gcn1 = self.gcn1(conv1)\n        maxpool1 = self.maxpool1(conv1)\n\n        conv2 = self.conv2(maxpool1)\n        gcn2 = self.gcn2(conv2)\n        maxpool2 = self.maxpool2(conv2)\n\n        conv3 = self.conv3(maxpool2)\n        gcn3 = self.gcn3(conv3)\n        maxpool3 = self.maxpool3(conv3)\n\n        conv4 = self.conv4(maxpool3)\n        gcn4 = self.gcn4(conv4)\n        maxpool4 = self.maxpool4(conv4)\n\n        center = self.center(maxpool4)\n\n        up4 = self.up_concat4(gcn4, center)\n        up3 = self.up_concat3(gcn3, up4)\n        up2 = self.up_concat2(gcn2, up3)\n        up1 = self.up_concat1(gcn1, up2)\n\n        final = self.final(up1)\n        return final\n\n\nclass UnetExFuse(nn.Module):\n\n    def __init__(self, num_classes=1, pretrained=False, feature_scale=4, is_deconv=True, norm=nn.InstanceNorm2d, is_pool=True, **kwargs):\n        super(UnetExFuse, self).__init__()\n        filters = [64, 128, 256, 512, 1024]\n        filters = [x // feature_scale for x in filters]\n\n        # downsampling\n        self.conv1 = UnetConv2D(1, filters[0], norm)\n        self.gcn1 = GCN(filters[0], filters[0])\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2) if is_pool else ConvBNReLU(filters[0], filters[0], norm, stride=2)\n\n        self.conv2 = UnetConv2D(filters[0], filters[1], norm)\n        self.gcn2 = GCN(filters[1], filters[1])\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2) if is_pool else ConvBNReLU(filters[1], filters[1], norm, stride=2)\n\n        self.conv3 = UnetConv2D(filters[1], filters[2], norm)\n        self.gcn3 = GCN(filters[2], filters[2])\n        self.maxpool3 = nn.MaxPool2d(kernel_size=2) if is_pool else ConvBNReLU(filters[2], filters[2], norm, stride=2)\n\n        self.conv4 = UnetConv2D(filters[2], filters[3], norm)\n        self.gcn4 = GCN(filters[3], filters[3])\n        self.maxpool4 = nn.MaxPool2d(kernel_size=2) if is_pool else ConvBNReLU(filters[3], filters[3], norm, stride=2)\n\n        self.center = UnetConv2D(filters[3], filters[4], norm)\n\n        # upsampling\n        self.up_concat4 = UnetUpConv2D(filters[4], filters[3], norm, is_deconv)\n        self.level4 = UnetExFuseLevel(filters[3], filters[2])\n        self.level3 = UnetExFuseLevel(filters[2], filters[1])\n        self.level2 = UnetExFuseLevel(filters[1], filters[0])\n        self.final = nn.Sequential(DAP(filters[0]), nn.Conv2d(filters[0], 1, 1))\n        # initialise weights\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                m.apply(weights_init_kaiming)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.apply(weights_init_kaiming)\n\n    def forward(self, inputs):\n        conv1 = self.conv1(inputs)\n        conv1 = self.gcn1(conv1)\n        maxpool1 = self.maxpool1(conv1)\n\n        conv2 = self.conv2(maxpool1)\n        conv2 = self.gcn2(conv2)\n        maxpool2 = self.maxpool2(conv2)\n\n        conv3 = self.conv3(maxpool2)\n        conv3 = self.gcn3(conv3)\n        maxpool3 = self.maxpool3(conv3)\n\n        conv4 = self.conv4(maxpool3)\n        conv4 = self.gcn4(conv4)\n        maxpool4 = self.maxpool4(conv4)\n\n        center = self.center(maxpool4)\n\n        up4 = self.up_concat4(conv4, center)\n        l4 = self.level4(conv4, center, up4)\n        l3 = self.level3(conv3, conv4, l4)\n        l2 = self.level2(conv2, conv3, l3)\n        final = self.final(l2)\n        return final\n'"
pywick/models/segmentation/testnets/exfuse/__init__.py,0,"b'""""""\nSource: https://github.com/rplab-snu/nucleus_segmentation\n""""""'"
pywick/models/segmentation/testnets/exfuse/unet_layer.py,3,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef weights_init_kaiming(m):\n    classname = m.__class__.__name__\n    #print(classname)\n    if classname.find('Conv') != -1:\n        nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n    elif classname.find('Linear') != -1:\n        nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0.0)\n\nclass ConvBNReLU(nn.Module):\n    def __init__(self, in_size, out_size, norm, kernel_size=3, stride=1, padding=1, act=nn.ReLU):\n        super(ConvBNReLU, self).__init__()\n        self.conv1 = nn.Sequential(nn.Conv2d(in_size, out_size, kernel_size, stride, padding),\n                                   norm(out_size),\n                                   act(inplace=True),)\n        # initialise the blocks\n        for m in self.children():\n            m.apply(weights_init_kaiming)\n        \n    def forward(self, inputs):\n        return self.conv1(inputs)\n\nclass UnetConv2D(nn.Module):\n    def __init__(self, in_size, out_size, norm, kernel_size=3, stride=1, padding=1, act=nn.ReLU):\n        super(UnetConv2D, self).__init__()\n        self.conv1 = ConvBNReLU(in_size, out_size, norm, kernel_size, stride, padding, act)\n        self.conv2 = ConvBNReLU(out_size, out_size, norm, kernel_size, 1, padding, act)\n\n\n    def forward(self, inputs):\n        x = self.conv1(inputs)\n        return self.conv2(x)\n\nclass UnetUpConv2D(nn.Module):\n    def __init__(self, in_size, out_size, norm, is_deconv=True, act=nn.ReLU):\n        super(UnetUpConv2D, self).__init__()\n\n        self.conv = UnetConv2D(in_size, out_size, norm, act=act)\n        if is_deconv:\n            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=4, stride=2, padding=1)\n        else:\n            self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n\n        # initialise the blocks\n        for m in self.children():\n            if m.__class__.__name__.find('UnetConv2D') != -1: \n                continue\n            m.apply(weights_init_kaiming)\n\n    def forward(self, input1, input2):\n        output2 = self.up(input2)\n        offset  = output2.size()[2] - input1.size()[2]\n        padding = [offset // 2] * 4\n        output1 = F.pad(input1, padding)\n        output  = torch.cat([output1, output2], 1)\n        return self.conv(output)\n"""
pywick/models/segmentation/testnets/lg_kernel_exfuse/__init__.py,0,b'from .large_kernel_exfuse import *'
pywick/models/segmentation/testnets/lg_kernel_exfuse/deeplab_resnet.py,9,"b'#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n# Author: Xiangtai(lxtpku@pku.edu.cn)\n# this file contains the baseline resnet(encoder), baseline pspnet(decoder) segmentation models\n# and mul grid bottleneck modules which can be used for deeplab models\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom . import seg_resnet as resnet\nfrom . import seg_resnext as resnext\n\n\n# this is for encoder part\n# resnet encoder\nclass Resnet(nn.Module):\n    def __init__(self, orig_resnet, **kwargs):\n        super(Resnet, self).__init__()\n\n        # take pretrained resnet, except AvgPool and FC\n        self.conv1 = orig_resnet.conv1\n        self.bn1 = orig_resnet.bn1\n        self.relu1 = orig_resnet.relu1\n        self.conv2 = orig_resnet.conv2\n        self.bn2 = orig_resnet.bn2\n        self.relu2 = orig_resnet.relu2\n        self.conv3 = orig_resnet.conv3\n        self.bn3 = orig_resnet.bn3\n        self.relu3 = orig_resnet.relu3\n        self.maxpool = orig_resnet.maxpool\n        self.layer1 = orig_resnet.layer1\n        self.layer2 = orig_resnet.layer2\n        self.layer3 = orig_resnet.layer3\n        self.layer4 = orig_resnet.layer4\n\n    def forward(self, x, return_feature_maps=False):\n        conv_out = []\n\n        x = self.relu1(self.bn1(self.conv1(x)))\n        x = self.relu2(self.bn2(self.conv2(x)))\n        x = self.relu3(self.bn3(self.conv3(x)))\n        x = self.maxpool(x)\n\n        x = self.layer1(x); conv_out.append(x);\n        x = self.layer2(x); conv_out.append(x);\n        x = self.layer3(x); conv_out.append(x);\n        x = self.layer4(x); conv_out.append(x);\n\n        if return_feature_maps:\n            return conv_out\n        return x\n\n# dilated resnet encoder\nclass ResnetDilated(nn.Module):\n    def __init__(self, orig_resnet, dilate_scale=8, **kwargs):\n        super(ResnetDilated, self).__init__()\n        from functools import partial\n\n        if dilate_scale == 8:\n            orig_resnet.layer3.apply(partial(self._nostride_dilate, dilate=2))\n            orig_resnet.layer4.apply(partial(self._nostride_dilate, dilate=4))\n        elif dilate_scale == 16:\n            orig_resnet.layer4.apply(partial(self._nostride_dilate, dilate=2))\n\n        # take pretrained resnet, except AvgPool and FC\n        self.conv1 = orig_resnet.conv1\n        self.bn1 = orig_resnet.bn1\n        self.relu1 = orig_resnet.relu1\n        self.conv2 = orig_resnet.conv2\n        self.bn2 = orig_resnet.bn2\n        self.relu2 = orig_resnet.relu2\n        self.conv3 = orig_resnet.conv3\n        self.bn3 = orig_resnet.bn3\n        self.relu3 = orig_resnet.relu3\n        self.maxpool = orig_resnet.maxpool\n        self.layer1 = orig_resnet.layer1\n        self.layer2 = orig_resnet.layer2\n        self.layer3 = orig_resnet.layer3\n        self.layer4 = orig_resnet.layer4\n\n    def _nostride_dilate(self, m, dilate):\n        classname = m.__class__.__name__\n        if classname.find(\'Conv\') != -1:\n            # the convolution with stride\n            if m.stride == (2, 2):\n                m.stride = (1, 1)\n                if m.kernel_size == (3, 3):\n                    m.dilation = (dilate // 2, dilate // 2)\n                    m.padding = (dilate // 2, dilate // 2)\n            # other convoluions\n            else:\n                if m.kernel_size == (3, 3):\n                    m.dilation = (dilate, dilate)\n                    m.padding = (dilate, dilate)\n\n    def forward(self, x, return_feature_maps=False):\n        conv_out = []\n\n        x = self.relu1(self.bn1(self.conv1(x)))\n        x = self.relu2(self.bn2(self.conv2(x)))\n        x = self.relu3(self.bn3(self.conv3(x)))\n        x = self.maxpool(x)\n\n        x = self.layer1(x); conv_out.append(x);\n        x = self.layer2(x); conv_out.append(x);\n        x = self.layer3(x); conv_out.append(x);\n        x = self.layer4(x); conv_out.append(x);\n\n        if return_feature_maps:\n            return conv_out\n        return x\n\n\n# this is for decoder part\n# last conv, bilinear upsample\nclass C1BilinearDeepSup(nn.Module):\n    def __init__(self, num_class=150, fc_dim=2048, use_softmax=False):\n        super(C1BilinearDeepSup, self).__init__()\n        self.use_softmax = use_softmax\n\n        self.cbr = conv3x3_bn_relu(fc_dim, fc_dim // 4, 1)\n        self.cbr_deepsup = conv3x3_bn_relu(fc_dim // 2, fc_dim // 4, 1)\n\n        # last conv\n        self.conv_last = nn.Conv2d(fc_dim // 4, num_class, 1, 1, 0)\n        self.conv_last_deepsup = nn.Conv2d(fc_dim // 4, num_class, 1, 1, 0)\n\n    def forward(self, conv_out, segSize=None):\n        conv5 = conv_out[-1]\n\n        x = self.cbr(conv5)\n        x = self.conv_last(x)\n\n        if self.use_softmax:  # is True during inference\n            x = nn.functional.upsample(x, size=segSize, mode=\'bilinear\')\n            x = nn.functional.softmax(x, dim=1)\n            return x\n\n        # deep sup\n        conv4 = conv_out[-2]\n        _ = self.cbr_deepsup(conv4)\n        _ = self.conv_last_deepsup(_)\n\n        x = nn.functional.log_softmax(x, dim=1)\n        _ = nn.functional.log_softmax(_, dim=1)\n\n        return (x, _)\n\n\n# last conv, bilinear upsample\nclass C1Bilinear(nn.Module):\n    def __init__(self, num_class=150, fc_dim=2048, use_softmax=False):\n        super(C1Bilinear, self).__init__()\n        self.use_softmax = use_softmax\n\n        self.cbr = conv3x3_bn_relu(fc_dim, fc_dim // 4, 1)\n\n        # last conv\n        self.conv_last = nn.Conv2d(fc_dim // 4, num_class, 1, 1, 0)\n\n    def forward(self, conv_out, segSize=None):\n        conv5 = conv_out[-1]\n        x = self.cbr(conv5)\n        x = self.conv_last(x)\n\n        if self.use_softmax: # is True during inference\n            x = nn.functional.upsample(x, size=segSize, mode=\'bilinear\')\n            x = nn.functional.softmax(x, dim=1)\n        else:\n            x = nn.functional.log_softmax(x, dim=1)\n\n        return x\n\n\n# pyramid pooling, bilinear upsample\nclass PPMBilinear(nn.Module):\n    def __init__(self, num_class=150, fc_dim=4096,\n                 use_softmax=False, pool_scales=(1, 2, 3, 6)):\n        super(PPMBilinear, self).__init__()\n        self.use_softmax = use_softmax\n\n        self.ppm = []\n        for scale in pool_scales:\n            self.ppm.append(nn.Sequential(\n                nn.AdaptiveAvgPool2d(scale),\n                nn.Conv2d(fc_dim, 512, kernel_size=1, bias=False),\n                nn.BatchNorm2d(512),\n                nn.ReLU(inplace=True)\n            ))\n        self.ppm = nn.ModuleList(self.ppm)\n\n        self.conv_last = nn.Sequential(\n            nn.Conv2d(fc_dim+len(pool_scales)*512, 512,\n                      kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(0.1),\n            nn.Conv2d(512, num_class, kernel_size=1)\n        )\n\n    def forward(self, conv_out, segSize=None):\n        conv5 = conv_out[-1]\n\n        input_size = conv5.size()\n        ppm_out = [conv5]\n        for pool_scale in self.ppm:\n            ppm_out.append(nn.functional.upsample(\n                pool_scale(conv5),\n                (input_size[2], input_size[3]),\n                mode=\'bilinear\'))\n        ppm_out = torch.cat(ppm_out, 1)\n\n        x = self.conv_last(ppm_out)\n\n        if self.use_softmax:  # is True during inference\n            x = nn.functional.upsample(x, size=segSize, mode=\'bilinear\')\n            x = nn.functional.softmax(x, dim=1)\n        else:\n            x = nn.functional.log_softmax(x, dim=1)\n        return x\n\n\n# pyramid pooling, bilinear upsample\nclass PPMBilinearDeepsup(nn.Module):\n    def __init__(self, num_class=150, fc_dim=4096,\n                 use_softmax=False, pool_scales=(1, 2, 3, 6)):\n        super(PPMBilinearDeepsup, self).__init__()\n        self.use_softmax = use_softmax\n\n        self.ppm = []\n        for scale in pool_scales:\n            self.ppm.append(nn.Sequential(\n                nn.AdaptiveAvgPool2d(scale),\n                nn.Conv2d(fc_dim, 512, kernel_size=1, bias=False),\n                nn.BatchNorm2d(512),\n                nn.ReLU(inplace=True)\n            ))\n        self.ppm = nn.ModuleList(self.ppm)\n        self.cbr_deepsup = conv3x3_bn_relu(fc_dim // 2, fc_dim // 4, 1)\n\n        self.conv_last = nn.Sequential(\n            nn.Conv2d(fc_dim+len(pool_scales)*512, 512,\n                      kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(0.1),\n            nn.Conv2d(512, num_class, kernel_size=1)\n        )\n        self.conv_last_deepsup = nn.Conv2d(fc_dim // 4, num_class, 1, 1, 0)\n        self.dropout_deepsup = nn.Dropout2d(0.1)\n\n    def forward(self, conv_out, segSize=None):\n        conv5 = conv_out[-1]\n\n        input_size = conv5.size()\n        ppm_out = [conv5]\n        for pool_scale in self.ppm:\n            ppm_out.append(nn.functional.upsample(\n                pool_scale(conv5),\n                (input_size[2], input_size[3]),\n                mode=\'bilinear\'))\n        ppm_out = torch.cat(ppm_out, 1)\n\n        x = self.conv_last(ppm_out)\n\n        if self.use_softmax:  # is True during inference\n            x = nn.functional.upsample(x, size=segSize, mode=\'bilinear\')\n            x = nn.functional.softmax(x, dim=1)\n            return x\n\n        # deep sup\n        conv4 = conv_out[-2]\n        _ = self.cbr_deepsup(conv4)\n        _ = self.dropout_deepsup(_)\n        _ = self.conv_last_deepsup(_)\n\n        x = nn.functional.log_softmax(x, dim=1)\n        _ = nn.functional.log_softmax(_, dim=1)\n\n        return (x, _)\n\n\n# upernet\nclass UPerNet(nn.Module):\n    def __init__(self, num_class=150, fc_dim=4096,\n                 use_softmax=False, pool_scales=(1, 2, 3, 6),\n                 fpn_inplanes=(256,512,1024,2048), fpn_dim=256):\n        super(UPerNet, self).__init__()\n        self.use_softmax = use_softmax\n\n        # PPM Module\n        self.ppm_pooling = []\n        self.ppm_conv = []\n\n        for scale in pool_scales:\n            self.ppm_pooling.append(nn.AdaptiveAvgPool2d(scale))\n            self.ppm_conv.append(nn.Sequential(\n                nn.Conv2d(fc_dim, 512, kernel_size=1, bias=False),\n                nn.BatchNorm2d(512),\n                nn.ReLU(inplace=True)\n            ))\n        self.ppm_pooling = nn.ModuleList(self.ppm_pooling)\n        self.ppm_conv = nn.ModuleList(self.ppm_conv)\n        self.ppm_last_conv = conv3x3_bn_relu(fc_dim + len(pool_scales)*512, fpn_dim, 1)\n\n        # FPN Module\n        self.fpn_in = []\n        for fpn_inplane in fpn_inplanes[:-1]: # skip the top layer\n            self.fpn_in.append(nn.Sequential(\n                nn.Conv2d(fpn_inplane, fpn_dim, kernel_size=1, bias=False),\n                nn.BatchNorm2d(fpn_dim),\n                nn.ReLU(inplace=True)\n            ))\n        self.fpn_in = nn.ModuleList(self.fpn_in)\n\n        self.fpn_out = []\n        for i in range(len(fpn_inplanes) - 1): # skip the top layer\n            self.fpn_out.append(nn.Sequential(\n                conv3x3_bn_relu(fpn_dim, fpn_dim, 1),\n            ))\n        self.fpn_out = nn.ModuleList(self.fpn_out)\n\n        self.conv_last = nn.Sequential(\n            conv3x3_bn_relu(len(fpn_inplanes) * fpn_dim, fpn_dim, 1),\n            nn.Conv2d(fpn_dim, num_class, kernel_size=1)\n        )\n\n    def forward(self, conv_out, segSize=None):\n        conv5 = conv_out[-1]\n\n        input_size = conv5.size()\n        ppm_out = [conv5]\n        for pool_scale, pool_conv in zip(self.ppm_pooling, self.ppm_conv):\n            ppm_out.append(pool_conv(nn.functional.upsample(\n                pool_scale(conv5),\n                (input_size[2], input_size[3]),\n                mode=\'bilinear\')))\n        ppm_out = torch.cat(ppm_out, 1)\n        f = self.ppm_last_conv(ppm_out)\n\n        fpn_feature_list = [f]\n        for i in reversed(range(len(conv_out) - 1)):\n            conv_x = conv_out[i]\n            conv_x = self.fpn_in[i](conv_x) # lateral branch\n\n            f = nn.functional.upsample(f, size=conv_x.size()[2:], mode=\'bilinear\') # top-down branch\n            f = conv_x + f\n\n            fpn_feature_list.append(self.fpn_out[i](f))\n\n        fpn_feature_list.reverse() # [P2 - P5]\n        output_size = fpn_feature_list[0].size()[2:]\n        fusion_list = [fpn_feature_list[0]]\n        for i in range(1, len(fpn_feature_list)):\n            fusion_list.append(nn.functional.upsample(\n                fpn_feature_list[i],\n                output_size,\n                mode=\'bilinear\'))\n        fusion_out = torch.cat(fusion_list, 1)\n        x = self.conv_last(fusion_out)\n\n        if self.use_softmax:  # is True during inference\n            x = nn.functional.upsample(x, size=segSize, mode=\'bilinear\')\n            x = nn.functional.softmax(x, dim=1)\n            return x\n\n        x = nn.functional.log_softmax(x, dim=1)\n\n        return x\n\n\n\ndef conv3x3(in_planes, out_planes, stride=1, has_bias=False):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=has_bias)\n\n\ndef conv3x3_bn_relu(in_planes, out_planes, stride=1):\n    return nn.Sequential(\n            conv3x3(in_planes, out_planes, stride),\n        nn.BatchNorm2d(out_planes),\n            nn.ReLU(inplace=True),\n            )\n\n# this is used to build the different models, both encoder and decoder\nclass ModelBuilder():\n    # custom weights initialization\n    def weights_init(self, m):\n        classname = m.__class__.__name__\n        if classname.find(\'Conv\') != -1:\n            nn.init.kaiming_normal(m.weight.data)\n        elif classname.find(\'BatchNorm\') != -1:\n            m.weight.data.fill_(1.)\n            m.bias.data.fill_(1e-4)\n        elif classname.find(\'Linear\') != -1:\n            m.weight.data.normal_(0.0, 0.0001)\n\n    def build_encoder(self, arch=\'resnet50_dilated8\', fc_dim=512, weights=\'\', **kwargs):\n        pretrained = True if len(weights) == 0 else False\n        if arch == \'resnet34\':\n            raise NotImplementedError\n            orig_resnet = resnet.__dict__[\'resnet34\'](pretrained=pretrained)\n            net_encoder = Resnet(orig_resnet)\n        elif arch == \'resnet34_dilated8\':\n            raise NotImplementedError\n            orig_resnet = resnet.__dict__[\'resnet34\'](pretrained=pretrained)\n            net_encoder = ResnetDilated(orig_resnet,\n                                        dilate_scale=8)\n        elif arch == \'resnet34_dilated16\':\n            raise NotImplementedError\n            orig_resnet = resnet.__dict__[\'resnet34\'](pretrained=pretrained)\n            net_encoder = ResnetDilated(orig_resnet,\n                                        dilate_scale=16)\n        elif arch == \'resnet50\':\n            orig_resnet = resnet.resnet50(**kwargs)\n            net_encoder = Resnet(orig_resnet)\n        elif arch == \'resnet50_dilated8\':\n            orig_resnet = resnet.resnet50(**kwargs)\n            net_encoder = ResnetDilated(orig_resnet, dilate_scale=8)\n        elif arch == \'resnet50_dilated16\':\n            orig_resnet = resnet.resnet50(**kwargs)\n            net_encoder = ResnetDilated(orig_resnet, dilate_scale=16)\n        elif arch == \'resnet101\':\n            orig_resnet = resnet.resnet101(**kwargs)\n            net_encoder = Resnet(orig_resnet)\n        elif arch == \'resnet101_dilated8\':\n            orig_resnet = resnet.resnet101(**kwargs)\n            net_encoder = ResnetDilated(orig_resnet, dilate_scale=8)\n        elif arch == \'resnet101_dilated16\':\n            orig_resnet = resnet.resnet101(**kwargs)\n            net_encoder = ResnetDilated(orig_resnet, dilate_scale=16)\n        elif arch == \'resnext101\':\n            orig_resnext = resnext.resnext101(**kwargs)\n            net_encoder = Resnet(orig_resnext) # we can still use class Resnet\n        else:\n            raise Exception(\'Architecture undefined!\')\n\n        # net_encoder.apply(self.weights_init)\n        if len(weights) > 0:\n            print(\'Loading weights for net_encoder\')\n            net_encoder.load_state_dict(\n                torch.load(weights, map_location=lambda storage, loc: storage), strict=False)\n        return net_encoder\n\n    def build_decoder(self, arch=\'ppm_bilinear_deepsup\', fc_dim=512, num_classes=150, weights=\'\', use_softmax=False):\n        if arch == \'c1_bilinear_deepsup\':\n            net_decoder = C1BilinearDeepSup(\n                num_class=num_classes, fc_dim=fc_dim, use_softmax=use_softmax)\n        elif arch == \'c1_bilinear\':\n            net_decoder = C1Bilinear(\n                num_class=num_classes,\n                fc_dim=fc_dim,\n                use_softmax=use_softmax)\n        elif arch == \'ppm_bilinear\':\n            net_decoder = PPMBilinear(\n                num_class=num_classes,\n                fc_dim=fc_dim,\n                use_softmax=use_softmax)\n        elif arch == \'ppm_bilinear_deepsup\':\n            net_decoder = PPMBilinearDeepsup(\n                num_class=num_classes,\n                fc_dim=fc_dim,\n                use_softmax=use_softmax)\n        elif arch == \'upernet_lite\':\n            net_decoder = UPerNet(\n                num_class=num_classes,\n                fc_dim=fc_dim,\n                use_softmax=use_softmax,\n                fpn_dim=256)\n        elif arch == \'upernet\':\n            net_decoder = UPerNet(\n                num_class=num_classes,\n                fc_dim=fc_dim,\n                use_softmax=use_softmax,\n                fpn_dim=512)\n        else:\n            raise Exception(\'Architecture undefined!\')\n\n        net_decoder.apply(self.weights_init)\n        if len(weights) > 0:\n            print(\'Loading weights for net_decoder\')\n            net_decoder.load_state_dict(\n                torch.load(weights, map_location=lambda storage, loc: storage), strict=False)\n        return net_decoder\n\n\n\n# this is used to build deeplabv3, deeplabv3+\n\nclass _ConvBatchNormReluBlock(nn.Sequential):\n\tdef __init__(self, inplanes, outplanes, kernel_size, stride, padding, dilation, relu=True):\n\t\tsuper(_ConvBatchNormReluBlock, self).__init__()\n\t\tself.add_module(""cov"", nn.Conv2d(in_channels=inplanes,out_channels=outplanes,\n\t\t\t\t\t\t\tkernel_size=kernel_size, stride=stride, padding = padding,\n\t\t\t\t\t\t\tdilation = dilation, bias=False))\n\t\tself.add_module(""bn"", nn.BatchNorm2d(num_features=outplanes, momentum=0.999, affine=True))\n\t\tif relu:\n\t\t\tself.add_module(""relu"", nn.ReLU())\n\tdef forward(self, x):\n\t\treturn super(_ConvBatchNormReluBlock, self).forward(x)\n\nclass _ResidualBlockMulGrid(nn.Sequential):\n\t""""""\n\t\tResidual Block with multi-grid , note: best model-> (1, 2, 1)\n\t""""""\n\tdef __init__(self, layers, inplanes, midplanes, outplanes, stride, dilation, mulgrid=[1,2,1]):\n\t\tsuper(_ResidualBlockMulGrid, self).__init__()\n\t\tself.add_module(""block1"", _Bottleneck(inplanes, midplanes, outplanes, stride, dilation * mulgrid[0], True))\n\t\tself.add_module(""block2"", _Bottleneck(outplanes, midplanes, outplanes, stride, dilation * mulgrid[1], False))\n\t\tself.add_module(""block3"", _Bottleneck(outplanes, midplanes, outplanes, stride, dilation * mulgrid[2], False))\n\tdef forward(self, x):\n\t\treturn super(_ResidualBlockMulGrid, self).forward(x)\n\nclass _Bottleneck(nn.Sequential):\n\tdef __init__(self, inplanes, midplanes, outplanes, stride, dilation, downsample):\n\t\tsuper(_Bottleneck, self).__init__()\n\t\tself.reduce = _ConvBatchNormReluBlock(inplanes, midplanes, 1, stride, 0, 1)\n\t\tself.conv3x3 = _ConvBatchNormReluBlock(midplanes, midplanes, 3, 1, dilation, dilation)\n\t\tself.increase = _ConvBatchNormReluBlock(midplanes, outplanes, 1, 1, 0, 1, relu=False)\n\t\tself.downsample = downsample\n\t\tif self.downsample:\n\t\t\tself.proj = _ConvBatchNormReluBlock(inplanes, outplanes, 1, stride, 0, 1, relu=False)\n\tdef forward(self, x):\n\t\th = self.reduce(x)\n\t\th = self.conv3x3(h)\n\t\th = self.increase(h)\n\t\tif self.downsample:\n\t\t\th += self.proj(x)\n\t\telse:\n\t\t\th += x\n\t\treturn F.relu(h)\n\n\n\n\n\nif __name__ == \'__main__\':\n    # test for model builder\n    builder = ModelBuilder()\n    net_encoder = builder.build_encoder(\n        arch =""resnet101_dilated8""\n    ).cuda()\n    test_input = torch.autograd.Variable(torch.randn(1, 3, 1024, 512), volatile=True).cuda()\n    out = net_encoder.forward(test_input)\n    print (out[0].size())'"
pywick/models/segmentation/testnets/lg_kernel_exfuse/large_kernel.py,1,"b'#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n# Implementation of Large Kernel Matters Paper (face++)\n# Author: Xiangtai(lxtpku@pku.edu.cn)\n\nimport torch\nfrom torch import nn\n\n\nfrom .deeplab_resnet import ModelBuilder\n\n\nclass _BoundaryRefineModule(nn.Module):\n    def __init__(self, dim):\n        super(_BoundaryRefineModule, self).__init__()\n        self.relu = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        residual = self.conv1(x)\n        residual = self.relu(residual)\n        residual = self.conv2(residual)\n        out = x + residual\n        return out\n\n\nclass _GlobalConvModule(nn.Module):\n    def __init__(self, in_dim, out_dim, kernel_size):\n        super(_GlobalConvModule, self).__init__()\n        pad0 = int((kernel_size[0] - 1) / 2)\n        pad1 = int((kernel_size[1] - 1) / 2)\n        # kernel size had better be odd number so as to avoid alignment error\n        self.conv_l1 = nn.Conv2d(in_dim, out_dim, kernel_size=(kernel_size[0], 1), padding=(pad0, 0))\n        self.conv_l2 = nn.Conv2d(out_dim, out_dim, kernel_size=(1, kernel_size[1]), padding=(0, pad1))\n        self.conv_r1 = nn.Conv2d(in_dim, out_dim, kernel_size=(1, kernel_size[1]), padding=(0, pad1))\n        self.conv_r2 = nn.Conv2d(out_dim, out_dim, kernel_size=(kernel_size[0], 1), padding=(pad0, 0))\n\n    def forward(self, x):\n        x_l = self.conv_l1(x)\n        x_l = self.conv_l2(x_l)\n        x_r = self.conv_r1(x)\n        x_r = self.conv_r2(x_r)\n        x = x_l + x_r\n        return x\n\n\nclass GCN(nn.Module):\n    def __init__(self, num_classes, kernel_size=7):\n        super(GCN, self).__init__()\n        self.resnet_features = ModelBuilder().build_encoder(""resnet101"")\n        self.layer0 = nn.Sequential(self.resnet_features.conv1, self.resnet_features.bn1,\n                                    self.resnet_features.relu1, self.resnet_features.conv3,\n                                    self.resnet_features.bn3, self.resnet_features.relu3\n                                    )\n        self.layer1 = nn.Sequential(self.resnet_features.maxpool, self.resnet_features.layer1)\n        self.layer2 = self.resnet_features.layer2\n        self.layer3 = self.resnet_features.layer3\n        self.layer4 = self.resnet_features.layer4\n\n        self.gcm1 = _GlobalConvModule(2048, num_classes, (kernel_size, kernel_size))\n        self.gcm2 = _GlobalConvModule(1024, num_classes, (kernel_size, kernel_size))\n        self.gcm3 = _GlobalConvModule(512, num_classes, (kernel_size, kernel_size))\n        self.gcm4 = _GlobalConvModule(256, num_classes, (kernel_size, kernel_size))\n\n        self.brm1 = _BoundaryRefineModule(num_classes)\n        self.brm2 = _BoundaryRefineModule(num_classes)\n        self.brm3 = _BoundaryRefineModule(num_classes)\n        self.brm4 = _BoundaryRefineModule(num_classes)\n        self.brm5 = _BoundaryRefineModule(num_classes)\n        self.brm6 = _BoundaryRefineModule(num_classes)\n        self.brm7 = _BoundaryRefineModule(num_classes)\n        self.brm8 = _BoundaryRefineModule(num_classes)\n        self.brm9 = _BoundaryRefineModule(num_classes)\n\n        self.deconv1 = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=4, stride=2, padding=1, bias=False)\n        self.deconv2 = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=4, stride=2, padding=1, bias=False)\n        self.deconv3 = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=4, stride=2, padding=1, bias=False)\n        self.deconv4 = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=4, stride=2, padding=1, bias=False)\n        self.deconv5 = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=4, stride=2, padding=1, bias=False)\n\n    def forward(self, x):\n        # suppose input = x , if x 512\n        f0 = self.layer0(x)  # 256\n        f1 = self.layer1(f0)  # 128\n        f2 = self.layer2(f1)  # 64\n        f3 = self.layer3(f2)  # 32\n        f4 = self.layer4(f3)  # 16\n\n        gcfm1 = self.brm1(self.gcm1(f4))  # 16\n        gcfm2 = self.brm2(self.gcm2(f3))  # 32\n        gcfm3 = self.brm3(self.gcm3(f2))  # 64\n        gcfm4 = self.brm4(self.gcm4(f1))  # 128\n\n        fs1 = self.brm5(self.deconv1(gcfm1) + gcfm2)  # 32\n        fs2 = self.brm6(self.deconv2(fs1) + gcfm3)  # 64\n        fs3 = self.brm7(self.deconv3(fs2) + gcfm4)  # 128\n        fs4 = self.brm8(self.deconv4(fs3))  # 256\n        out = self.brm9(self.deconv5(fs4))\n\n        return out\n\n    def freeze_bn(self):\n        for m in self.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                m.eval()\n\nif __name__ == \'__main__\':\n    model = GCN(20).cuda()\n    model.freeze_bn()\n    model.eval()\n    image = torch.autograd.Variable(torch.randn(1, 3, 512, 512), volatile=True).cuda()\n    print (model(image).size())'"
pywick/models/segmentation/testnets/lg_kernel_exfuse/large_kernel_exfuse.py,3,"b'# Source: https://github.com/lxtGH/fuse_seg_pytorch\n\n#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n# Implementation of ExFuse: Enhancing Feature Fusion for Semantic Segmentation Paper (face++)\n# Author: Xiangtai(lxtpku@pku.edu.cn)\n# ###########\n# backbone GCN framework(large_kernel.py) and ResNext101 (Resnet) as pretrained model\n# Layer Rearrangement (LR) (0.8%):  re-arrange the layer in the resnet model\n# Semantic Supervision (SS) (1.1%): used when training the model on the ImageNet\n# assign auxiliary supervisions directly to the early stages of the encoder network\n# Semantic Embedding Branch (SEB) (0.7%)\n# Explicit Channel Resolution Embedding (ECRE) (0.5%)\n# Densely Adjacent Prediction (0.6%)\n\n# ###########\n\nimport torch\nfrom torch import nn\n\nfrom .deeplab_resnet import ModelBuilder\n\nfrom .large_kernel import _GlobalConvModule\n\n__all__ = [\'GCNFuse\']\n\n\nclass SEB(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(SEB, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n        self.upsample = nn.Upsample(scale_factor=2, mode=""bilinear"")\n\n    def forward(self, x):\n        x1, x2 = x\n        return x1 * self.upsample(self.conv(x2))\n\n\nclass GCNFuse(nn.Module):\n    """"""\n    :param kernel_size: (int) Must be an ODD number!!\n    """"""\n    def __init__(self, num_classes=1, backbone=\'resnet101\', kernel_size=7, dap_k=3, **kwargs):\n        super(GCNFuse, self).__init__()\n        self.num_classes = num_classes\n        self.resnet_features = ModelBuilder().build_encoder(arch=backbone, **kwargs)\n        self.layer0 = nn.Sequential(self.resnet_features.conv1, self.resnet_features.bn1,\n                                    self.resnet_features.relu1, self.resnet_features.conv3,\n                                    self.resnet_features.bn3, self.resnet_features.relu3\n                                    )\n        self.layer1 = nn.Sequential(self.resnet_features.maxpool, self.resnet_features.layer1)\n        self.layer2 = self.resnet_features.layer2\n        self.layer3 = self.resnet_features.layer3\n        self.layer4 = self.resnet_features.layer4\n\n        self.gcm1 = _GlobalConvModule(2048, num_classes * 4, (kernel_size, kernel_size))\n        self.gcm2 = _GlobalConvModule(1024, num_classes, (kernel_size, kernel_size))\n        self.gcm3 = _GlobalConvModule(512, num_classes * dap_k**2, (kernel_size, kernel_size))\n        self.gcm4 = _GlobalConvModule(256, num_classes * dap_k**2, (kernel_size, kernel_size))\n\n        self.deconv1 = nn.ConvTranspose2d(num_classes, num_classes * dap_k**2, kernel_size=4, stride=2, padding=1, bias=False)\n        self.deconv2 = nn.ConvTranspose2d(num_classes, num_classes * dap_k**2, kernel_size=4, stride=2, padding=1, bias=False)\n        self.deconv3 = nn.ConvTranspose2d(num_classes * dap_k**2, num_classes * dap_k**2, kernel_size=4, stride=2, padding=1, bias=False)\n        self.deconv4 = nn.ConvTranspose2d(num_classes * dap_k**2, num_classes * dap_k**2, kernel_size=4, stride=2, padding=1, bias=False)\n        self.deconv5 = nn.ConvTranspose2d(num_classes * dap_k**2, num_classes * dap_k**2, kernel_size=4, stride=2, padding=1, bias=False)\n\n        self.ecre = nn.PixelShuffle(2)\n\n        self.seb1 = SEB(2048, 1024)\n        self.seb2 = SEB(3072, 512)\n        self.seb3 = SEB(3584, 256)\n\n        self.upsample2 = nn.Upsample(scale_factor=2, mode=""bilinear"")\n        self.upsample4 = nn.Upsample(scale_factor=4, mode=""bilinear"")\n\n        self.DAP = nn.Sequential(\n            nn.PixelShuffle(dap_k),\n            nn.AvgPool2d((dap_k, dap_k))\n        )\n\n    def forward(self, x):\n        # suppose input = x , if x 512\n        f0 = self.layer0(x)  # 256\n        f1 = self.layer1(f0)  # 128\n        # print (f1.size())\n        f2 = self.layer2(f1)  # 64\n        # print (f2.size())\n        f3 = self.layer3(f2)  # 32\n        # print (f3.size())\n        f4 = self.layer4(f3)  # 16\n        # print (f4.size())\n        x = self.gcm1(f4)\n        out1 = self.ecre(x)\n        seb1 = self.seb1([f3, f4])\n        gcn1 = self.gcm2(seb1)\n\n        seb2 = self.seb2([f2, torch.cat([f3, self.upsample2(f4)], dim=1)])\n        gcn2 = self.gcm3(seb2)\n\n        seb3 = self.seb3([f1, torch.cat([f2, self.upsample2(f3), self.upsample4(f4)], dim=1)])\n        gcn3 = self.gcm4(seb3)\n\n        y = self.deconv2(gcn1 + out1)\n        y = self.deconv3(gcn2 + y)\n        y = self.deconv4(gcn3 + y)\n        y = self.deconv5(y)\n        y = self.DAP(y)\n        return y\n\n    def freeze_bn(self):\n        for m in self.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                m.eval()\n\n\nif __name__ == \'__main__\':\n    model = GCNFuse(20).cuda()\n    model.freeze_bn()\n    model.eval()\n    image = torch.autograd.Variable(torch.randn(1, 3, 512, 512), volatile=True).cuda()\n    res1, res2 = model(image)\n    print (res1.size(), res2.size())'"
pywick/models/segmentation/testnets/lg_kernel_exfuse/seg_resnet.py,2,"b'#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n#resnet50 and resnet 101\nimport os\nimport sys\nimport torch\nimport torch.nn as nn\nimport math\n\ntry:\n    from urllib import urlretrieve\nexcept ImportError:\n    from urllib.request import urlretrieve\n\n\n__all__ = [\'ResNet\', \'resnet50\', \'resnet101\'] # resnet101 is coming soon!\n\n\nmodel_urls = {\n    \'resnet50\': \'http://sceneparsing.csail.mit.edu/model/pretrained_resnet/resnet50-imagenet.pth\',\n    \'resnet101\': \'http://sceneparsing.csail.mit.edu/model/pretrained_resnet/resnet101-imagenet.pth\'\n    # \'resnet101\': \'http://sceneparsing.csail.mit.edu/model/pretrained_resnet/resnet101-2a57e44d.pth\'\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000, **kwargs):\n        self.inplanes = 128\n        super(ResNet, self).__init__()\n        self.conv1 = conv3x3(3, 64, stride=2)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(64, 64)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv3 = conv3x3(64, 128)\n        self.bn3 = nn.BatchNorm2d(128)\n        self.relu3 = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(7, stride=1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.relu1(self.bn1(self.conv1(x)))\n        x = self.relu2(self.bn2(self.conv2(x)))\n        x = self.relu3(self.bn3(self.conv3(x)))\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\n\ndef resnet50(pretrained=True, **kwargs):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on Places\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(load_url(model_urls[\'resnet50\'], **kwargs), strict=False)\n    return model\n\n\ndef resnet101(pretrained=True, **kwargs):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on Places\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(load_url(url=model_urls[\'resnet101\'], **kwargs), strict=False)\n    return model\n\n\ndef load_url(url, model_root=\'~/.torch/models\', map_location=\'cpu\', **kwargs):\n    model_root = os.path.expanduser(model_root)\n    if not os.path.exists(model_root):\n        os.makedirs(model_root)\n    filename = url.split(\'/\')[-1]\n    cached_file = os.path.join(model_root, filename)\n    if not os.path.exists(cached_file):\n        sys.stderr.write(\'Downloading: ""{}"" to {}\\n\'.format(url, cached_file))\n        urlretrieve(url, cached_file)\n    return torch.load(cached_file, map_location=map_location)\n\n\nif __name__ == \'__main__\':\n    res = resnet101(pretrained=True)\n    print (res)'"
pywick/models/segmentation/testnets/lg_kernel_exfuse/seg_resnext.py,2,"b'# synchronized batchnorm version of resnext101\nimport os\nimport sys\nimport torch\nimport torch.nn as nn\nimport math\n\ntry:\n    from urllib import urlretrieve\nexcept ImportError:\n    from urllib.request import urlretrieve\n\n\n__all__ = [\'ResNeXt\', \'resnext101\'] # support resnext 101\n\n# can not used for now\nmodel_urls = {\n    # \'resnext101\': \'http://sceneparsing.csail.mit.edu/model/pretrained_resnet/resnext101-imagenet.pth\'\n    \'resnext101\': \'http://data.lip6.fr/cadene/pretrainedmodels/resnext101_64x4d-e77a0586.pth\'\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass GroupBottleneck(nn.Module):\n    expansion = 2\n\n    def __init__(self, inplanes, planes, stride=1, groups=1, downsample=None):\n        super(GroupBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 2, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 2)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNeXt(nn.Module):\n\n    def __init__(self, block, layers, groups=32, num_classes=1000, **kwargs):\n        self.inplanes = 128\n        super(ResNeXt, self).__init__()\n        self.conv1 = conv3x3(3, 64, stride=2)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(64, 64)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv3 = conv3x3(64, 128)\n        self.bn3 = nn.BatchNorm2d(128)\n        self.relu3 = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(block, 128, layers[0], groups=groups)\n        self.layer2 = self._make_layer(block, 256, layers[1], stride=2, groups=groups)\n        self.layer3 = self._make_layer(block, 512, layers[2], stride=2, groups=groups)\n        self.layer4 = self._make_layer(block, 1024, layers[3], stride=2, groups=groups)\n        self.avgpool = nn.AvgPool2d(7, stride=1)\n        self.fc = nn.Linear(1024 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels // m.groups\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, groups=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, groups, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups=groups))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.relu1(self.bn1(self.conv1(x)))\n        x = self.relu2(self.bn2(self.conv2(x)))\n        x = self.relu3(self.bn3(self.conv3(x)))\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef resnext101(pretrained=True, **kwargs):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on Places\n    """"""\n    model = ResNeXt(GroupBottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(load_url(model_urls[\'resnext101\'], **kwargs), strict=False)\n    return model\n\n\ndef load_url(url, model_root=\'/models/pytorch\', map_location=None, **kwargs):\n    model_root = os.path.expanduser(model_root)\n    if not os.path.exists(model_root):\n        os.makedirs(model_root)\n    filename = url.split(\'/\')[-1]\n    cached_file = os.path.join(model_root, filename)\n    if not os.path.exists(cached_file):\n        sys.stderr.write(\'Downloading: ""{}"" to {}\\n\'.format(url, cached_file))\n        urlretrieve(url, cached_file)\n    return torch.load(cached_file, map_location=map_location)\n\nif __name__ == \'__main__\':\n    res = resnext101(pretrained=True)\n    print (res)'"
pywick/models/segmentation/testnets/mixnet/__init__.py,0,"b'""""""\nSource: https://github.com/zsef123/MixNet-PyTorch\n""""""'"
pywick/models/segmentation/testnets/mixnet/layers.py,3,"b'import torch\nimport torch.nn as nn\n\n\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\nclass Flatten(nn.Module):\n    def forward(self, x):\n        return x.view(x.shape[0], -1)\n\n\nclass SEModule(nn.Module):\n    def __init__(self, ch, squeeze_ch):\n        super().__init__()\n        self.se = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(ch, squeeze_ch, 1, 1, 0, bias=True),\n            Swish(),\n            nn.Conv2d(squeeze_ch, ch, 1, 1, 0, bias=True),\n        )\n\n    def forward(self, x):\n        return x * torch.sigmoid(self.se(x))\n'"
pywick/models/segmentation/testnets/mixnet/mdconv.py,4,"b'# https://github.com/tensorflow/tpu/blob/master/models/official/mnasnet/mixnet/custom_layers.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef _split_channels(total_filters, num_groups):\n    """"""\n    https://github.com/tensorflow/tpu/blob/master/models/official/mnasnet/mixnet/custom_layers.py#L33\n    """"""\n    split = [total_filters // num_groups for _ in range(num_groups)]\n    split[0] += total_filters - sum(split)\n    return split\n\n\nclass MDConv(nn.Module):\n    def __init__(self, in_channels, kernel_sizes, stride, dilatied=False, bias=False):\n        super().__init__()\n\n        if not isinstance(kernel_sizes, list):\n            kernel_sizes = [kernel_sizes]\n\n        self.in_channels  = _split_channels(in_channels, len(kernel_sizes))\n\n        self.convs = nn.ModuleList()\n        for ch, k in zip(self.in_channels, kernel_sizes):\n            dilation = 1\n            if stride[0] == 1 and dilatied:\n                dilation, stride = (k - 1) // 2, 3\n                print(""Use dilated conv with dilation rate = {}"".format(dilation))\n            pad = ((stride[0] - 1) + dilation * (k - 1)) // 2\n\n            conv = nn.Conv2d(ch, ch, k, stride, pad, dilation,\n                             groups=ch, bias=bias)\n            self.convs.append(conv)\n\n    def forward(self, x):\n        xs = torch.split(x, self.in_channels, 1)\n        return torch.cat([conv(x) for conv, x in zip(self.convs, xs)], 1)'"
pywick/models/segmentation/testnets/mixnet/mixnet.py,1,"b'import torch.nn as nn\n\nfrom .layers import Flatten\nfrom .layers import SEModule\nfrom .layers import Swish\nfrom .mdconv import MDConv\nfrom .utils import MixnetDecoder\nfrom .utils import round_filters\n\n\nclass MixBlock(nn.Module):\n    def __init__(self, dw_ksize, expand_ksize, project_ksize,\n                 in_channels, out_channels, expand_ratio, id_skip,\n                 strides, se_ratio, swish, dilated):\n        super().__init__()\n\n        self.id_skip = id_skip and all(s == 1 for s in strides) and in_channels == out_channels\n\n        act_fn = lambda : Swish() if swish else nn.ReLU(True)\n\n        layers = []\n        expaned_ch = in_channels * expand_ratio\n        if expand_ratio != 1:\n            expand = nn.Sequential(\n                nn.Conv2d(in_channels, expaned_ch, expand_ksize, bias=False),\n                nn.BatchNorm2d(expaned_ch),\n                act_fn(),\n            )\n            layers.append(expand)\n\n        depthwise = nn.Sequential(\n            MDConv(expaned_ch, dw_ksize, strides, bias=False),\n            nn.BatchNorm2d(expaned_ch),\n            act_fn(),\n        )\n        layers.append(depthwise)\n\n        if se_ratio > 0:\n            se = SEModule(expaned_ch, int(expaned_ch * se_ratio))\n            layers.append(se)\n\n        project = nn.Sequential(\n            nn.Conv2d(expaned_ch, out_channels, project_ksize, bias=False),\n            nn.BatchNorm2d(out_channels),\n        )\n        layers.append(project)\n\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.layers(x)\n        if self.id_skip:\n            out = out + x\n        return out\n\n\nclass MixModule(nn.Module):\n    def __init__(self, dw_ksize, expand_ksize, project_ksize, num_repeat,\n                 in_channels, out_channels, expand_ratio, id_skip,\n                 strides, se_ratio, swish, dilated):\n        super().__init__()\n        layers = [MixBlock(dw_ksize, expand_ksize, project_ksize,\n                           in_channels, out_channels, expand_ratio, id_skip,\n                           strides, se_ratio, swish, dilated)]\n\n        for _ in range(num_repeat - 1):\n            layers.append(MixBlock(dw_ksize, expand_ksize, project_ksize,\n                                   in_channels, out_channels, expand_ratio, id_skip,\n                                   [1, 1], se_ratio, swish, dilated))\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.layers(x)\n\n\nclass MixNet(nn.Module):\n    def __init__(self, stem, blocks_args, head, dropout_rate, num_classes=1000, **kwargs):\n        super().__init__()\n\n        self.stem = nn.Sequential(\n            nn.Conv2d(3, stem, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(stem),\n            nn.ReLU(True)\n        )\n\n        self.blocks = nn.Sequential(*[MixModule(*args) for args in blocks_args])\n\n        self.classifier = nn.Sequential(\n            nn.Conv2d(blocks_args[-1].out_channels, head, 1, bias=False),\n            nn.BatchNorm2d(head),\n            nn.ReLU(True),\n            nn.AdaptiveAvgPool2d(1),\n            Flatten(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(head, num_classes)\n        )\n\n        self.init_weights()\n\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=""fan_out"", nonlinearity=\'relu\')\n            elif isinstance(m, nn.Linear):\n                nn.init.kaiming_uniform_(m.weight, mode=\'fan_in\', nonlinearity=\'linear\')\n\n    def forward(self, x):\n        # print(""Input : "", x.shape)\n        stem = self.stem(x)\n        # print(""Stem : "", x.shape)\n        feature = self.blocks(stem)\n        # print(""feature : "", feature.shape)\n        out = self.classifier(feature)\n        return out\n\n\ndef mixnet_s(depth_multiplier=1, depth_divisor=8, min_depth=None, num_classes=1000):\n    """"""\n    Creates mixnet-s model.\n\n    Args:\n        depth_multiplier: depth_multiplier to number of filters per layer.\n    """"""\n    stem = round_filters(16,   depth_multiplier, depth_divisor, min_depth)\n    head = round_filters(1536, depth_multiplier, depth_divisor, min_depth)\n    dropout = 0.2\n\n    blocks_args = [\n        \'r1_k3_a1_p1_s11_e1_i16_o16\',\n        \'r1_k3_a1.1_p1.1_s22_e6_i16_o24\',\n        \'r1_k3_a1.1_p1.1_s11_e3_i24_o24\',\n\n        \'r1_k3.5.7_a1_p1_s22_e6_i24_o40_se0.5_sw\',\n        \'r3_k3.5_a1.1_p1.1_s11_e6_i40_o40_se0.5_sw\',\n\n        \'r1_k3.5.7_a1_p1.1_s22_e6_i40_o80_se0.25_sw\',\n        \'r2_k3.5_a1_p1.1_s11_e6_i80_o80_se0.25_sw\',\n\n        \'r1_k3.5.7_a1.1_p1.1_s11_e6_i80_o120_se0.5_sw\',\n        \'r2_k3.5.7.9_a1.1_p1.1_s11_e3_i120_o120_se0.5_sw\',\n\n        \'r1_k3.5.7.9.11_a1_p1_s22_e6_i120_o200_se0.5_sw\',\n        \'r2_k3.5.7.9_a1_p1.1_s11_e6_i200_o200_se0.5_sw\',\n    ]\n\n    blocks_args = MixnetDecoder.decode(blocks_args, depth_multiplier, depth_divisor, min_depth)\n    print(""-----------"")\n    print(""Mixnet S"")\n    for a in blocks_args:\n        print(a)\n    print(""-----------"")\n    return MixNet(stem, blocks_args, head, dropout, num_classes=num_classes)\n\n\nif __name__ == ""__main__"":\n    mixnet_s()\n'"
pywick/models/segmentation/testnets/mixnet/utils.py,0,"b'import re\nfrom collections import namedtuple\n\nBlockArgs = namedtuple(\'BlockArgs\', [\n    \'dw_ksize\', \'expand_ksize\', \'project_ksize\', \'num_repeat\',\n    \'in_channels\', \'out_channels\', \'expand_ratio\', \'id_skip\',\n    \'strides\', \'se_ratio\', \'swish\', \'dilated\',\n])\n\n\ndef round_filters(filters, depth_multiplier, depth_divisor, min_depth):\n    """"""Round number of filters based on depth depth_multiplier.\n    TODO : ref link\n    """"""\n    if not depth_multiplier:\n        return filters\n\n    filters *= depth_multiplier\n    min_depth = min_depth or depth_divisor\n    new_filters = max(min_depth, int(filters + depth_divisor / 2) // depth_divisor * depth_divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_filters < 0.9 * filters:\n        new_filters += depth_divisor\n    return new_filters\n\n\nclass MixnetDecoder:\n    """"""A class of Mixnet decoder to get model configuration.""""""\n\n    @staticmethod\n    def _decode_block_string(block_string, depth_multiplier, depth_divisor, min_depth):\n        """"""Gets a mixnet block through a string notation of arguments.\n\n        E.g. r2_k3_a1_p1_s2_e1_i32_o16_se0.25_noskip: r - number of repeat blocks,\n        k - kernel size, s - strides (1-9), e - expansion ratio, i - input filters,\n        o - output filters, se - squeeze/excitation ratio\n\n        Args:\n        block_string: a string, a string representation of block arguments.\n\n        Returns:\n        A BlockArgs instance.\n        Raises:\n        ValueError: if the strides option is not correctly specified.\n        """"""\n        assert isinstance(block_string, str)\n\n        ops = block_string.split(\'_\')\n        options = {}\n        for op in ops:\n            splits = re.split(r\'(\\d.*)\', op)\n            if len(splits) >= 2:\n                key, value = splits[:2]\n                options[key] = value\n\n        if \'s\' not in options or len(options[\'s\']) != 2:\n            raise ValueError(\'Strides options should be a pair of integers.\')\n\n        def _parse_ksize(ss):\n            ks = [int(k) for k in ss.split(\'.\')]\n            return ks if len(ks) > 1 else ks[0]\n\n        return BlockArgs(num_repeat=int(options[\'r\']),\n                         dw_ksize=_parse_ksize(options[\'k\']),\n                         expand_ksize=_parse_ksize(options[\'a\']),\n                         project_ksize=_parse_ksize(options[\'p\']),\n                         strides=[int(options[\'s\'][0]), int(options[\'s\'][1])],\n                         expand_ratio=int(options[\'e\']),\n                         in_channels=round_filters(int(options[\'i\']), depth_multiplier, depth_divisor, min_depth),\n                         out_channels=round_filters(int(options[\'o\']), depth_multiplier, depth_divisor, min_depth),\n                         id_skip=(\'noskip\' not in block_string),\n                         se_ratio=float(options[\'se\']) if \'se\' in options else 0,\n                         swish=(\'sw\' in block_string),\n                         dilated=(\'dilated\' in block_string)\n                         )\n\n    @staticmethod\n    def _encode_block_string(block):\n        """"""Encodes a Mixnet block to a string.""""""\n\n        def _encode_ksize(arr):\n            return \'.\'.join([str(k) for k in arr])\n\n        args = [\n            \'r%d\' % block.num_repeat,\n            \'k%s\' % _encode_ksize(block.dw_ksize),\n            \'a%s\' % _encode_ksize(block.expand_ksize),\n            \'p%s\' % _encode_ksize(block.project_ksize),\n            \'s%d%d\' % (block.strides[0], block.strides[1]),\n            \'e%s\' % block.expand_ratio,\n            \'i%d\' % block.in_channels,\n            \'o%d\' % block.out_channels\n        ]\n\n        if (block.se_ratio is not None and block.se_ratio > 0 and block.se_ratio <= 1):\n            args.append(\'se%s\' % block.se_ratio)\n        if block.id_skip is False:\n            args.append(\'noskip\')\n        if block.swish:\n            args.append(\'sw\')\n        if block.dilated:\n            args.append(\'dilated\')\n        return \'_\'.join(args)\n\n    @staticmethod\n    def decode(string_list, depth_multiplier, depth_divisor, min_depth):\n        """"""Decodes a list of string notations to specify blocks inside the network.\n\n        Args:\n        string_list: a list of strings, each string is a notation of Mixnet\n        block.build_model_base\n\n        Returns:\n        A list of namedtuples to represent Mixnet blocks arguments.\n        """"""\n        assert isinstance(string_list, list)\n        blocks_args = []\n        for block_string in string_list:\n            blocks_args.append(MixnetDecoder._decode_block_string(block_string, depth_multiplier, depth_divisor, min_depth))\n        return blocks_args\n\n    @staticmethod\n    def encode(blocks_args):\n        """"""Encodes a list of Mixnet Blocks to a list of strings.\n\n        Args:\n        blocks_args: A list of namedtuples to represent Mixnet blocks arguments.\n        Returns:\n        a list of strings, each string is a notation of Mixnet block.\n        """"""\n        block_strings = []\n        for block in blocks_args:\n            block_strings.append(MixnetDecoder._encode_block_string(block))\n        return block_strings\n'"
pywick/models/segmentation/testnets/tkcnet/__init__.py,0,b''
pywick/models/segmentation/testnets/tkcnet/base.py,4,"b'###########################################################################\n# Created by: Hang Zhang \n# Email: zhang.hang@rutgers.edu \n# Copyright (c) 2017\n###########################################################################\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .resnet import *\n\nup_kwargs = {\'mode\': \'bilinear\', \'align_corners\': True}\n\n__all__ = [\'BaseNet\']\n\nclass BaseNet(nn.Module):\n    def __init__(self, nclass, backbone, aux, se_loss, dilated=True, norm_layer=None,\n                 base_size=576, crop_size=608, mean=[.485, .456, .406],\n                 std=[.229, .224, .225], root=\'./pretrain_models\',\n                 multi_grid=False, multi_dilation=None, **kwargs):\n        super(BaseNet, self).__init__()\n        self.nclass = nclass\n        self.aux = aux\n        self.se_loss = se_loss\n        self.mean = mean\n        self.std = std\n        self.base_size = base_size\n        self.crop_size = crop_size\n        # copying modules from pretrained models\n        if backbone == \'resnet50\':\n            self.pretrained = resnet50(pretrained=True, dilated=dilated, norm_layer=norm_layer, root=root,\n                                       multi_grid=multi_grid, multi_dilation=multi_dilation)\n        elif backbone == \'resnet101\':\n            self.pretrained = resnet101(pretrained=True, dilated=dilated, norm_layer=norm_layer, root=root, multi_grid=multi_grid,\n                                        multi_dilation=multi_dilation)\n        else:\n            raise RuntimeError(\'unknown backbone: {}\'.format(backbone))\n        # bilinear upsample options\n        self._up_kwargs = up_kwargs\n\n    def base_forward(self, x):\n        #print(""in BaseNet.base_forward(), input_size: "", x.size())\n        x = self.pretrained.conv1(x)\n        x = self.pretrained.bn1(x)\n        x = self.pretrained.relu(x)\n        x = self.pretrained.maxpool(x)\n        c1 = self.pretrained.layer1(x)\n        c2 = self.pretrained.layer2(c1)\n        c3 = self.pretrained.layer3(c2)\n        c4 = self.pretrained.layer4(c3)\n        return c1, c2, c3, c4\n\ndef module_inference(module, image, flip=True):\n    #print(""in model_inference input_size: "", image.size())\n    output = module.evaluate(image)\n    if flip:\n        fimg = flip_image(image)\n        foutput = module.evaluate(fimg)\n        output += flip_image(foutput)\n    return output.exp()\n\ndef resize_image(img, h, w, **up_kwargs):\n    return F.upsample(img, (h, w), **up_kwargs)\n\ndef pad_image(img, mean, std, crop_size):\n    b,c,h,w = img.size()\n    assert(c==3)\n    padh = crop_size - h if h < crop_size else 0\n    padw = crop_size - w if w < crop_size else 0\n    pad_values = -np.array(mean) / np.array(std)\n    img_pad = img.new().resize_(b,c,h+padh,w+padw)\n    for i in range(c):\n        # note that pytorch pad params is in reversed orders\n        img_pad[:,i,:,:] = F.pad(img[:,i,:,:], (0, padw, 0, padh), value=pad_values[i])\n    assert(img_pad.size(2)>=crop_size and img_pad.size(3)>=crop_size)\n    return img_pad\n\ndef crop_image(img, h0, h1, w0, w1):\n    return img[:,:,h0:h1,w0:w1]\n\ndef flip_image(img):\n    assert(img.dim()==4)\n    with torch.cuda.device_of(img):\n        idx = torch.arange(img.size(3)-1, -1, -1).type_as(img).long()\n    return img.index_select(3, idx)\n'"
pywick/models/segmentation/testnets/tkcnet/files.py,1,"b'import os\nimport requests\nimport errno\nimport shutil\nimport hashlib\nfrom tqdm import tqdm\nimport torch\n\n__all__ = [\'save_checkpoint\', \'download\', \'mkdir\', \'check_sha1\']\n\ndef save_checkpoint(state, args, is_best, filename=\'checkpoint.pth.tar\'):\n    """"""Saves checkpoint to disk""""""\n    if args.model == \'acnet\':\n        directory = ""%s/model/%s_model_%s_%s_thvflag%s_%s_gpu%dbs%depochs%d/%s/""%(args.dataset, args.model, args.backbone, args.context_head, args.thv_learn_flag, args.dataset, args.gpu_nums, args.batch_size, args.epochs,args.checkname)\n    else:\n        directory = ""%s/model/%s_model_%s_%s_gpu%dbs%depochs%d/%s/""%(args.dataset, args.model, args.backbone,args.dataset, args.gpu_nums, args.batch_size, args.epochs,args.checkname)\n\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n    filename = directory + filename\n    print(""saving file:"", filename)\n    torch.save(state, filename)\n    if is_best:\n        shutil.copyfile(filename, directory + \'model_best.pth.tar\')\n\n\ndef download(url, path=None, overwrite=False, sha1_hash=None):\n    """"""Download an given URL\n    Parameters\n    ----------\n    url : str\n        URL to download\n    path : str, optional\n        Destination path to store downloaded file. By default stores to the\n        current directory with same name as in url.\n    overwrite : bool, optional\n        Whether to overwrite destination file if already exists.\n    sha1_hash : str, optional\n        Expected sha1 hash in hexadecimal digits. Will ignore existing file when hash is specified\n        but doesn\'t match.\n    Returns\n    -------\n    str\n        The file path of the downloaded file.\n    """"""\n    if path is None:\n        fname = url.split(\'/\')[-1]\n    else:\n        path = os.path.expanduser(path)\n        if os.path.isdir(path):\n            fname = os.path.join(path, url.split(\'/\')[-1])\n        else:\n            fname = path\n\n    if overwrite or not os.path.exists(fname) or (sha1_hash and not check_sha1(fname, sha1_hash)):\n        dirname = os.path.dirname(os.path.abspath(os.path.expanduser(fname)))\n        if not os.path.exists(dirname):\n            os.makedirs(dirname)\n\n        print(\'Downloading %s from %s...\'%(fname, url))\n        r = requests.get(url, stream=True)\n        if r.status_code != 200:\n            raise RuntimeError(""Failed downloading url %s""%url)\n        total_length = r.headers.get(\'content-length\')\n        with open(fname, \'wb\') as f:\n            if total_length is None: # no content length header\n                for chunk in r.iter_content(chunk_size=1024):\n                    if chunk: # filter out keep-alive new chunks\n                        f.write(chunk)\n            else:\n                total_length = int(total_length)\n                for chunk in tqdm(r.iter_content(chunk_size=1024),\n                                  total=int(total_length / 1024. + 0.5),\n                                  unit=\'KB\', unit_scale=False, dynamic_ncols=True):\n                    f.write(chunk)\n\n        if sha1_hash and not check_sha1(fname, sha1_hash):\n            raise UserWarning(\'File {} is downloaded but the content hash does not match. \' \\\n                              \'The repo may be outdated or download may be incomplete. \' \\\n                              \'If the ""repo_url"" is overridden, consider switching to \' \\\n                              \'the default repo.\'.format(fname))\n\n    return fname\n\n\ndef check_sha1(filename, sha1_hash):\n    """"""Check whether the sha1 hash of the file content matches the expected hash.\n    Parameters\n    ----------\n    filename : str\n        Path to the file.\n    sha1_hash : str\n        Expected sha1 hash in hexadecimal digits.\n    Returns\n    -------\n    bool\n        Whether the file content matches the expected hash.\n    """"""\n    sha1 = hashlib.sha1()\n    with open(filename, \'rb\') as f:\n        while True:\n            data = f.read(1048576)\n            if not data:\n                break\n            sha1.update(data)\n\n    return sha1.hexdigest() == sha1_hash\n\n\ndef mkdir(path):\n    """"""make dir exists okay""""""\n    try:\n        os.makedirs(path)\n    except OSError as exc:  # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else:\n            raise\n'"
pywick/models/segmentation/testnets/tkcnet/model_store.py,0,"b'""""""Model store which provides pretrained models.""""""\nfrom __future__ import print_function\n__all__ = [\'get_model_file\', \'purge\']\nimport os\nimport zipfile\n\nfrom .files import download, check_sha1\n\n_model_sha1 = {name: checksum for checksum, name in [\n    (\'853f2fb07aeb2927f7696e166b215609a987fd44\', \'resnet50\'),\n    (\'5be5422ad7cb6a2e5f5a54070d0aa9affe69a9a4\', \'resnet101\'),\n    (\'6cb047cda851de6aa31963e779fae5f4c299056a\', \'deepten_minc\'),\n    (\'fc8c0b795abf0133700c2d4265d2f9edab7eb6cc\', \'fcn_resnet50_ade\'),\n    (\'eeed8e582f0fdccdba8579e7490570adc6d85c7c\', \'fcn_resnet50_pcontext\'),\n    (\'54f70c772505064e30efd1ddd3a14e1759faa363\', \'psp_resnet50_ade\'),\n    (\'558e8904e123813f23dc0347acba85224650fe5f\', \'encnet_resnet50_ade\'),\n    (\'7846a2f065e90ce70d268ba8ada1a92251587734\', \'encnet_resnet50_pcontext\'),\n    (\'6f7c372259988bc2b6d7fc0007182e7835c31a11\', \'encnet_resnet101_pcontext\'),\n    ]}\n\nencoding_repo_url = \'https://hangzh.s3.amazonaws.com/\'\n_url_format = \'{repo_url}encoding/models/{file_name}.zip\'\n\ndef short_hash(name):\n    if name not in _model_sha1:\n        raise ValueError(\'Pretrained model for {name} is not available.\'.format(name=name))\n    return _model_sha1[name][:8]\n\ndef get_model_file(name, root=\'./pretrain_models\'):\n    r""""""Return location for the pretrained on local file system.\n\n    This function will download from online model zoo when model cannot be found or has mismatch.\n    The root directory will be created if it doesn\'t exist.\n\n    Parameters\n    ----------\n    name : str\n        Name of the model.\n    root : str, default \'./pretrain_models\'\n        Location for keeping the model parameters.\n\n    Returns\n    -------\n    file_path\n        Path to the requested pretrained model file.\n    """"""\n    file_name = \'{name}-{short_hash}\'.format(name=name, short_hash=short_hash(name))\n    root = os.path.expanduser(root)\n    file_path = os.path.join(root, file_name+\'.pth\')\n    sha1_hash = _model_sha1[name]\n    if os.path.exists(file_path):\n        if check_sha1(file_path, sha1_hash):\n            print(""===>> loading pretrain model file: "",file_path)\n            return file_path\n        else:\n            print(\'Mismatch in the content of model file detected. Downloading again.\')\n    else:\n        print(\'Model file is not found. Downloading.\')\n\n    if not os.path.exists(root):\n        os.makedirs(root)\n\n    zip_file_path = os.path.join(root, file_name+\'.zip\')\n    repo_url = os.environ.get(\'ENCODING_REPO\', encoding_repo_url)\n    if repo_url[-1] != \'/\':\n        repo_url = repo_url + \'/\'\n    download(_url_format.format(repo_url=repo_url, file_name=file_name),\n             path=zip_file_path,\n             overwrite=True)\n    with zipfile.ZipFile(zip_file_path) as zf:\n        zf.extractall(root)\n    os.remove(zip_file_path)\n\n    if check_sha1(file_path, sha1_hash):\n        return file_path\n    else:\n        raise ValueError(\'Downloaded file has different hash. Please try again.\')\n\ndef purge(root=\'./pretrain_models\'):\n    r""""""Purge all pretrained model files in local file store.\n\n    Parameters\n    ----------\n    root : str, default \'./pretrain_models\'\n        Location for keeping the model parameters.\n    """"""\n    root = os.path.expanduser(root)\n    files = os.listdir(root)\n    for f in files:\n        if f.endswith("".pth""):\n            os.remove(os.path.join(root, f))\n\ndef pretrained_model_list():\n    return list(_model_sha1.keys())\n'"
pywick/models/segmentation/testnets/tkcnet/resnet.py,6,"b'""""""Dilated ResNet""""""\nimport math\nimport torch\nimport torch.utils.model_zoo as model_zoo\nimport torch.nn as nn\n\n__all__ = [\'ResNet\', \'resnet50\', \'resnet101\', \'BasicBlock\', \'Bottleneck\']\n\nmodel_urls = {\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    """"""ResNet BasicBlock\n    """"""\n    expansion = 1\n    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None, previous_dilation=1,\n                 norm_layer=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n                               padding=dilation, dilation=dilation, bias=False)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,\n                               padding=previous_dilation, dilation=previous_dilation, bias=False)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    """"""ResNet Bottleneck\n    """"""\n    # pylint: disable=unused-argument\n    expansion = 4\n    def __init__(self, inplanes, planes, stride=1, dilation=1,\n                 downsample=None, previous_dilation=1, norm_layer=None, r2_factor=None):\n        super(Bottleneck, self).__init__()\n        self.r2_factor = r2_factor\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = norm_layer(planes)\n        if self.r2_factor:\n            self.avg_pool = nn.AvgPool2d(r2_factor, stride=1, padding =1)\n        self.conv2 = nn.Conv2d(\n            planes, planes, kernel_size=3, stride=stride,\n            padding=dilation, dilation=dilation, bias=False)\n        self.bn2 = norm_layer(planes)\n        self.conv3 = nn.Conv2d(\n            planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = norm_layer(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.dilation = dilation\n        self.stride = stride\n\n    def _sum_each(self, x, y):\n        assert(len(x) == len(y))\n        z = []\n        for i in range(len(x)):\n            z.append(x[i]+y[i])\n        return z\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        if self.r2_factor:\n            out = self.avg_pool(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n    """"""Dilated Pre-trained ResNet Model, which preduces the stride of 8 featuremaps at conv5.\n\n    Parameters\n    ----------\n    block : Block\n        Class for the residual block. Options are BasicBlockV1, BottleneckV1.\n    layers : list of int\n        Numbers of layers in each block\n    classes : int, default 1000\n        Number of classification classes.\n    dilated : bool, default False\n        Applying dilation strategy to pretrained ResNet yielding a stride-8 model,\n        typically used in Semantic Segmentation.\n    norm_layer : object\n        Normalization layer used in backbone network (default: :class:`mxnet.gluon.nn.BatchNorm`;\n        for Synchronized Cross-GPU BachNormalization).\n\n    Reference:\n\n        - He, Kaiming, et al. ""Deep residual learning for image recognition."" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n\n        - Yu, Fisher, and Vladlen Koltun. ""Multi-scale context aggregation by dilated convolutions.""\n    """"""\n    # pylint: disable=unused-variable\n    def __init__(self, block, layers, num_classes=1000, dilated=True, norm_layer=nn.BatchNorm2d, multi_grid=False, multi_dilation=None):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = norm_layer(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0], norm_layer=norm_layer)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, norm_layer=norm_layer)\n        if dilated:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=1,\n                                           dilation=2, norm_layer=norm_layer) # r1=2, r1=1\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=1,\n                                           dilation=4, norm_layer=norm_layer, r2_factor=3) #r1=4, r2=3\n        else:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n                                           norm_layer=norm_layer)\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n                                           norm_layer=norm_layer)\n        self.avgpool = nn.AvgPool2d(7)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, norm_layer):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, norm_layer=None, r2_factor=None):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        if dilation == 1 or dilation == 2:\n            layers.append(block(self.inplanes, planes, stride, dilation=1,\n                            downsample=downsample, previous_dilation=dilation, norm_layer=norm_layer))\n        elif dilation == 4:\n            layers.append(block(self.inplanes, planes, stride, dilation = 4, downsample = downsample, \n                                previous_dilation = dilation, norm_layer = norm_layer, r2_factor = r2_factor))\n        else:\n            raise RuntimeError(""=> unknown dilation size: {}"".format(dilation))\n        \n        self.inplanes = planes * block.expansion\n            \n        for i in range(1, blocks):\n            if r2_factor:\n                layers.append(block(self.inplanes, planes, dilation=dilation, previous_dilation=dilation,norm_layer=norm_layer, r2_factor=3))\n            else:\n                layers.append(block(self.inplanes, planes, dilation=dilation, previous_dilation=dilation,norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\ndef resnet50(pretrained=False, root=\'./pretrain_models\', **kwargs):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        print(""===>> pretrained==True, returns a model pre-trained on ImageNet"")\n        from .model_store import get_model_file\n        model.load_state_dict(torch.load(\n            get_model_file(\'resnet50\', root=root)), strict=False)\n    return model\n\n\ndef resnet101(pretrained=False, root=\'./pretrain_models\', **kwargs):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    #Remove the following lines of comments\n    #if u want to train from a pretrained model\n    if pretrained:\n       print(""===>> pretrained==True, returns a model pre-trained on ImageNet"")\n       from .model_store import get_model_file\n       model.load_state_dict(torch.load(\n           get_model_file(\'resnet101\', root=root)), strict=False)\n    return model\n'"
pywick/models/segmentation/testnets/tkcnet/tkcnet.py,3,"b'# Source: https://github.com/wutianyiRosun/TKCN (MIT)\n\n###########################################################################\n# Created by: Tianyi Wu \n# Email: wutianyi@ict.ac.cn\n# Copyright (c) 2018\n###########################################################################\nfrom __future__ import division\nimport torch\nimport torch.nn as nn\nfrom torch.nn.functional import upsample\nfrom .base import BaseNet\n\n\n__all__ = [\'TKCNet\', \'get_tkcnet\', \'TKCNet_Resnet101\']\n\nclass TKCNet(BaseNet):\n    """"""Tree-structured Kronecker Convolutional Networks for Semantic Segmentation, \n      Note that:\n        In our pytorch implementation of TKCN: for KConv(r_1,r_2), we use AvgPool2d(kernel_size = r_2, stride=1) \n        and Conv2d( kernel_size =3, dilation = r_1) to approximate it.\n        The original codes (caffe)  will be relesed later .\n\n    Parameters\n    ----------\n    nclass : int\n        Number of categories for the training dataset.\n    backbone : string\n        Pre-trained dilated backbone network type (default:\'resnet50\'; \'resnet50\',\n        \'resnet101\' or \'resnet152\').\n    norm_layer : object\n\n    """"""\n    def __init__(self, num_classes, backbone, aux=False, se_loss=False, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(TKCNet, self).__init__(num_classes, backbone, aux, se_loss, norm_layer=norm_layer, **kwargs)\n        self.head = TFAHead(2048, num_classes, norm_layer, r1=[10, 20, 30], r2=[7, 15, 25])\n\n    def forward(self, x):\n        #print(""in tkcnet.forward(): input_size: "", x.size())  #input.size == crop_size\n        imsize = x.size()[2:]\n        _, _, c3, c4 = self.base_forward(x)\n        x = self.head(c4)\n        out = upsample(x, imsize, **self._up_kwargs)\n        out1 = [out]\n\n        if self.aux:\n            return tuple(out1)\n        else:\n            return out1[0]\n        \nclass TFAHead(nn.Module):\n    """"""\n       input:\n        x: B x C x H x W  (C = 2048)\n       output: B x nClass x H x W\n    """"""\n    def __init__(self, in_channels, out_channels, norm_layer, r1, r2):\n        super(TFAHead, self).__init__()\n        # TFA module\n        inter_channels = in_channels // 4  # 2048-->512\n        self.TFA_level_1 = self._make_level(2048, inter_channels, r1[0], r2[0], norm_layer)\n        self.TFA_level_list = nn.ModuleList()\n        for i in range(1, len(r1)):\n            self.TFA_level_list.append( self._make_level(inter_channels, inter_channels, r1[i], r2[i], norm_layer))\n\n        # segmentation subnetwork \n        self.conv51 = nn.Sequential(nn.Conv2d(in_channels + inter_channels*len(r1), inter_channels, 3, padding=1, bias=False),\n                                        norm_layer(inter_channels),\n                                        nn.ReLU())\n        self.conv6 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(inter_channels, out_channels, 1))\n    \n    def _make_level(self, inChannel, outChannel, r1, r2, norm_layer):\n        avg_agg = nn.AvgPool2d(r2, stride =1, padding= r2 // 2)\n        conv = nn.Sequential( nn.Conv2d(inChannel, outChannel, kernel_size= 3, stride= 1, padding = r1, dilation = r1 ),\n                              norm_layer(outChannel),\n                              nn.ReLU())\n        return nn.Sequential(avg_agg, conv)\n\n\n    def forward(self, x):\n        TFA_out_list = []\n        TFA_out_list.append(x)\n        level_1_out = self.TFA_level_1(x)\n        TFA_out_list.append(level_1_out)\n        for i, layer in enumerate(self.TFA_level_list):\n            if i==0:\n                output1 = layer(level_1_out)\n                TFA_out_list.append(output1)\n            else:\n                output1 = layer(output1)\n                TFA_out_list.append(output1)\n        TFA_out= torch.cat( TFA_out_list, 1)\n        \n        out = self.conv51(TFA_out) # B x 4096 x H x W  --> B x 512 x H x W\n        out = self.conv6(out)  # B x nClass x H x W\n        return out\n\n\ndef get_tkcnet(num_classes=1, backbone=\'resnet101\', pretrained=True, **kwargs):\n    """"""TKCN model from the paper `""Tree-structured Kronocker Convolutional Network for Semantic Segmentation""\n    """"""\n\n    model = TKCNet(num_classes=num_classes, backbone=backbone, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef TKCNet_Resnet101(num_classes=1, **kwargs):\n    return get_tkcnet(num_classes=num_classes, backbone=\'resnet101\', **kwargs)\n\n\n'"
