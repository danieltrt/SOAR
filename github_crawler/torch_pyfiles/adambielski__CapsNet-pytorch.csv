file_path,api_count,code
net.py,16,"b""from __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\n\n\ndef squash(x):\n    lengths2 = x.pow(2).sum(dim=2)\n    lengths = lengths2.sqrt()\n    x = x * (lengths2 / (1 + lengths2) / lengths).view(x.size(0), x.size(1), 1)\n    return x\n\n\nclass AgreementRouting(nn.Module):\n    def __init__(self, input_caps, output_caps, n_iterations):\n        super(AgreementRouting, self).__init__()\n        self.n_iterations = n_iterations\n        self.b = nn.Parameter(torch.zeros((input_caps, output_caps)))\n\n    def forward(self, u_predict):\n        batch_size, input_caps, output_caps, output_dim = u_predict.size()\n\n        c = F.softmax(self.b)\n        s = (c.unsqueeze(2) * u_predict).sum(dim=1)\n        v = squash(s)\n\n        if self.n_iterations > 0:\n            b_batch = self.b.expand((batch_size, input_caps, output_caps))\n            for r in range(self.n_iterations):\n                v = v.unsqueeze(1)\n                b_batch = b_batch + (u_predict * v).sum(-1)\n\n                c = F.softmax(b_batch.view(-1, output_caps)).view(-1, input_caps, output_caps, 1)\n                s = (c * u_predict).sum(dim=1)\n                v = squash(s)\n\n        return v\n\n\nclass CapsLayer(nn.Module):\n    def __init__(self, input_caps, input_dim, output_caps, output_dim, routing_module):\n        super(CapsLayer, self).__init__()\n        self.input_dim = input_dim\n        self.input_caps = input_caps\n        self.output_dim = output_dim\n        self.output_caps = output_caps\n        self.weights = nn.Parameter(torch.Tensor(input_caps, input_dim, output_caps * output_dim))\n        self.routing_module = routing_module\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.input_caps)\n        self.weights.data.uniform_(-stdv, stdv)\n\n    def forward(self, caps_output):\n        caps_output = caps_output.unsqueeze(2)\n        u_predict = caps_output.matmul(self.weights)\n        u_predict = u_predict.view(u_predict.size(0), self.input_caps, self.output_caps, self.output_dim)\n        v = self.routing_module(u_predict)\n        return v\n\n\nclass PrimaryCapsLayer(nn.Module):\n    def __init__(self, input_channels, output_caps, output_dim, kernel_size, stride):\n        super(PrimaryCapsLayer, self).__init__()\n        self.conv = nn.Conv2d(input_channels, output_caps * output_dim, kernel_size=kernel_size, stride=stride)\n        self.input_channels = input_channels\n        self.output_caps = output_caps\n        self.output_dim = output_dim\n\n    def forward(self, input):\n        out = self.conv(input)\n        N, C, H, W = out.size()\n        out = out.view(N, self.output_caps, self.output_dim, H, W)\n\n        # will output N x OUT_CAPS x OUT_DIM\n        out = out.permute(0, 1, 3, 4, 2).contiguous()\n        out = out.view(out.size(0), -1, out.size(4))\n        out = squash(out)\n        return out\n\n\nclass CapsNet(nn.Module):\n    def __init__(self, routing_iterations, n_classes=10):\n        super(CapsNet, self).__init__()\n        self.conv1 = nn.Conv2d(1, 256, kernel_size=9, stride=1)\n        self.primaryCaps = PrimaryCapsLayer(256, 32, 8, kernel_size=9, stride=2)  # outputs 6*6\n        self.num_primaryCaps = 32 * 6 * 6\n        routing_module = AgreementRouting(self.num_primaryCaps, n_classes, routing_iterations)\n        self.digitCaps = CapsLayer(self.num_primaryCaps, 8, n_classes, 16, routing_module)\n\n    def forward(self, input):\n        x = self.conv1(input)\n        x = F.relu(x)\n        x = self.primaryCaps(x)\n        x = self.digitCaps(x)\n        probs = x.pow(2).sum(dim=2).sqrt()\n        return x, probs\n\n\nclass ReconstructionNet(nn.Module):\n    def __init__(self, n_dim=16, n_classes=10):\n        super(ReconstructionNet, self).__init__()\n        self.fc1 = nn.Linear(n_dim * n_classes, 512)\n        self.fc2 = nn.Linear(512, 1024)\n        self.fc3 = nn.Linear(1024, 784)\n        self.n_dim = n_dim\n        self.n_classes = n_classes\n\n    def forward(self, x, target):\n        mask = Variable(torch.zeros((x.size()[0], self.n_classes)), requires_grad=False)\n        if next(self.parameters()).is_cuda:\n            mask = mask.cuda()\n        mask.scatter_(1, target.view(-1, 1), 1.)\n        mask = mask.unsqueeze(2)\n        x = x * mask\n        x = x.view(-1, self.n_dim * self.n_classes)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.sigmoid(self.fc3(x))\n        return x\n\n\nclass CapsNetWithReconstruction(nn.Module):\n    def __init__(self, capsnet, reconstruction_net):\n        super(CapsNetWithReconstruction, self).__init__()\n        self.capsnet = capsnet\n        self.reconstruction_net = reconstruction_net\n\n    def forward(self, x, target):\n        x, probs = self.capsnet(x)\n        reconstruction = self.reconstruction_net(x, target)\n        return reconstruction, probs\n\n\nclass MarginLoss(nn.Module):\n    def __init__(self, m_pos, m_neg, lambda_):\n        super(MarginLoss, self).__init__()\n        self.m_pos = m_pos\n        self.m_neg = m_neg\n        self.lambda_ = lambda_\n\n    def forward(self, lengths, targets, size_average=True):\n        t = torch.zeros(lengths.size()).long()\n        if targets.is_cuda:\n            t = t.cuda()\n        t = t.scatter_(1, targets.data.view(-1, 1), 1)\n        targets = Variable(t)\n        losses = targets.float() * F.relu(self.m_pos - lengths).pow(2) + \\\n                 self.lambda_ * (1. - targets.float()) * F.relu(lengths - self.m_neg).pow(2)\n        return losses.mean() if size_average else losses.sum()\n\n\nif __name__ == '__main__':\n\n    import argparse\n    import torch.optim as optim\n    from torchvision import datasets, transforms\n    from torch.autograd import Variable\n\n    # Training settings\n    parser = argparse.ArgumentParser(description='CapsNet with MNIST')\n    parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n                        help='input batch size for training (default: 64)')\n    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n                        help='input batch size for testing (default: 1000)')\n    parser.add_argument('--epochs', type=int, default=250, metavar='N',\n                        help='number of epochs to train (default: 10)')\n    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n                        help='learning rate (default: 0.01)')\n    parser.add_argument('--no-cuda', action='store_true', default=False,\n                        help='disables CUDA training')\n    parser.add_argument('--seed', type=int, default=1, metavar='S',\n                        help='random seed (default: 1)')\n    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n                        help='how many batches to wait before logging training status')\n    parser.add_argument('--routing_iterations', type=int, default=3)\n    parser.add_argument('--with_reconstruction', action='store_true', default=False)\n    args = parser.parse_args()\n    args.cuda = not args.no_cuda and torch.cuda.is_available()\n\n    torch.manual_seed(args.seed)\n    if args.cuda:\n        torch.cuda.manual_seed(args.seed)\n\n    kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n\n    train_loader = torch.utils.data.DataLoader(\n        datasets.MNIST('../data', train=True, download=True,\n                       transform=transforms.Compose([\n                           transforms.Pad(2), transforms.RandomCrop(28),\n                           transforms.ToTensor()\n                       ])),\n        batch_size=args.batch_size, shuffle=True, **kwargs)\n\n    test_loader = torch.utils.data.DataLoader(\n        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n            transforms.ToTensor()\n        ])),\n        batch_size=args.test_batch_size, shuffle=False, **kwargs)\n\n    model = CapsNet(args.routing_iterations)\n\n    if args.with_reconstruction:\n        reconstruction_model = ReconstructionNet(16, 10)\n        reconstruction_alpha = 0.0005\n        model = CapsNetWithReconstruction(model, reconstruction_model)\n\n    if args.cuda:\n        model.cuda()\n\n    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n\n    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=15, min_lr=1e-6)\n\n    loss_fn = MarginLoss(0.9, 0.1, 0.5)\n\n\n    def train(epoch):\n        model.train()\n        for batch_idx, (data, target) in enumerate(train_loader):\n            if args.cuda:\n                data, target = data.cuda(), target.cuda()\n            data, target = Variable(data), Variable(target, requires_grad=False)\n            optimizer.zero_grad()\n            if args.with_reconstruction:\n                output, probs = model(data, target)\n                reconstruction_loss = F.mse_loss(output, data.view(-1, 784))\n                margin_loss = loss_fn(probs, target)\n                loss = reconstruction_alpha * reconstruction_loss + margin_loss\n            else:\n                output, probs = model(data)\n                loss = loss_fn(probs, target)\n            loss.backward()\n            optimizer.step()\n            if batch_idx % args.log_interval == 0:\n                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                    epoch, batch_idx * len(data), len(train_loader.dataset),\n                           100. * batch_idx / len(train_loader), loss.data[0]))\n\n    def test():\n        model.eval()\n        test_loss = 0\n        correct = 0\n        for data, target in test_loader:\n            if args.cuda:\n                data, target = data.cuda(), target.cuda()\n            data, target = Variable(data, volatile=True), Variable(target)\n\n            if args.with_reconstruction:\n                output, probs = model(data, target)\n                reconstruction_loss = F.mse_loss(output, data.view(-1, 784), size_average=False).data[0]\n                test_loss += loss_fn(probs, target, size_average=False).data[0]\n                test_loss += reconstruction_alpha * reconstruction_loss\n            else:\n                output, probs = model(data)\n                test_loss += loss_fn(probs, target, size_average=False).data[0]\n\n            pred = probs.data.max(1, keepdim=True)[1]  # get the index of the max probability\n            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n\n        test_loss /= len(test_loader.dataset)\n        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n            test_loss, correct, len(test_loader.dataset),\n            100. * correct / len(test_loader.dataset)))\n        return test_loss\n\n\n    for epoch in range(1, args.epochs + 1):\n        train(epoch)\n        test_loss = test()\n        scheduler.step(test_loss)\n        torch.save(model.state_dict(),\n                   '{:03d}_model_dict_{}routing_reconstruction{}.pth'.format(epoch, args.routing_iterations,\n                                                                             args.with_reconstruction))\n"""
