file_path,api_count,code
datasets.py,2,"b'import numpy as np\nfrom PIL import Image\n\nfrom torch.utils.data import Dataset\nfrom torch.utils.data.sampler import BatchSampler\n\n\nclass SiameseMNIST(Dataset):\n    """"""\n    Train: For each sample creates randomly a positive or a negative pair\n    Test: Creates fixed pairs for testing\n    """"""\n\n    def __init__(self, mnist_dataset):\n        self.mnist_dataset = mnist_dataset\n\n        self.train = self.mnist_dataset.train\n        self.transform = self.mnist_dataset.transform\n\n        if self.train:\n            self.train_labels = self.mnist_dataset.train_labels\n            self.train_data = self.mnist_dataset.train_data\n            self.labels_set = set(self.train_labels.numpy())\n            self.label_to_indices = {label: np.where(self.train_labels.numpy() == label)[0]\n                                     for label in self.labels_set}\n        else:\n            # generate fixed pairs for testing\n            self.test_labels = self.mnist_dataset.test_labels\n            self.test_data = self.mnist_dataset.test_data\n            self.labels_set = set(self.test_labels.numpy())\n            self.label_to_indices = {label: np.where(self.test_labels.numpy() == label)[0]\n                                     for label in self.labels_set}\n\n            random_state = np.random.RandomState(29)\n\n            positive_pairs = [[i,\n                               random_state.choice(self.label_to_indices[self.test_labels[i].item()]),\n                               1]\n                              for i in range(0, len(self.test_data), 2)]\n\n            negative_pairs = [[i,\n                               random_state.choice(self.label_to_indices[\n                                                       np.random.choice(\n                                                           list(self.labels_set - set([self.test_labels[i].item()]))\n                                                       )\n                                                   ]),\n                               0]\n                              for i in range(1, len(self.test_data), 2)]\n            self.test_pairs = positive_pairs + negative_pairs\n\n    def __getitem__(self, index):\n        if self.train:\n            target = np.random.randint(0, 2)\n            img1, label1 = self.train_data[index], self.train_labels[index].item()\n            if target == 1:\n                siamese_index = index\n                while siamese_index == index:\n                    siamese_index = np.random.choice(self.label_to_indices[label1])\n            else:\n                siamese_label = np.random.choice(list(self.labels_set - set([label1])))\n                siamese_index = np.random.choice(self.label_to_indices[siamese_label])\n            img2 = self.train_data[siamese_index]\n        else:\n            img1 = self.test_data[self.test_pairs[index][0]]\n            img2 = self.test_data[self.test_pairs[index][1]]\n            target = self.test_pairs[index][2]\n\n        img1 = Image.fromarray(img1.numpy(), mode=\'L\')\n        img2 = Image.fromarray(img2.numpy(), mode=\'L\')\n        if self.transform is not None:\n            img1 = self.transform(img1)\n            img2 = self.transform(img2)\n        return (img1, img2), target\n\n    def __len__(self):\n        return len(self.mnist_dataset)\n\n\nclass TripletMNIST(Dataset):\n    """"""\n    Train: For each sample (anchor) randomly chooses a positive and negative samples\n    Test: Creates fixed triplets for testing\n    """"""\n\n    def __init__(self, mnist_dataset):\n        self.mnist_dataset = mnist_dataset\n        self.train = self.mnist_dataset.train\n        self.transform = self.mnist_dataset.transform\n\n        if self.train:\n            self.train_labels = self.mnist_dataset.train_labels\n            self.train_data = self.mnist_dataset.train_data\n            self.labels_set = set(self.train_labels.numpy())\n            self.label_to_indices = {label: np.where(self.train_labels.numpy() == label)[0]\n                                     for label in self.labels_set}\n\n        else:\n            self.test_labels = self.mnist_dataset.test_labels\n            self.test_data = self.mnist_dataset.test_data\n            # generate fixed triplets for testing\n            self.labels_set = set(self.test_labels.numpy())\n            self.label_to_indices = {label: np.where(self.test_labels.numpy() == label)[0]\n                                     for label in self.labels_set}\n\n            random_state = np.random.RandomState(29)\n\n            triplets = [[i,\n                         random_state.choice(self.label_to_indices[self.test_labels[i].item()]),\n                         random_state.choice(self.label_to_indices[\n                                                 np.random.choice(\n                                                     list(self.labels_set - set([self.test_labels[i].item()]))\n                                                 )\n                                             ])\n                         ]\n                        for i in range(len(self.test_data))]\n            self.test_triplets = triplets\n\n    def __getitem__(self, index):\n        if self.train:\n            img1, label1 = self.train_data[index], self.train_labels[index].item()\n            positive_index = index\n            while positive_index == index:\n                positive_index = np.random.choice(self.label_to_indices[label1])\n            negative_label = np.random.choice(list(self.labels_set - set([label1])))\n            negative_index = np.random.choice(self.label_to_indices[negative_label])\n            img2 = self.train_data[positive_index]\n            img3 = self.train_data[negative_index]\n        else:\n            img1 = self.test_data[self.test_triplets[index][0]]\n            img2 = self.test_data[self.test_triplets[index][1]]\n            img3 = self.test_data[self.test_triplets[index][2]]\n\n        img1 = Image.fromarray(img1.numpy(), mode=\'L\')\n        img2 = Image.fromarray(img2.numpy(), mode=\'L\')\n        img3 = Image.fromarray(img3.numpy(), mode=\'L\')\n        if self.transform is not None:\n            img1 = self.transform(img1)\n            img2 = self.transform(img2)\n            img3 = self.transform(img3)\n        return (img1, img2, img3), []\n\n    def __len__(self):\n        return len(self.mnist_dataset)\n\n\nclass BalancedBatchSampler(BatchSampler):\n    """"""\n    BatchSampler - from a MNIST-like dataset, samples n_classes and within these classes samples n_samples.\n    Returns batches of size n_classes * n_samples\n    """"""\n\n    def __init__(self, labels, n_classes, n_samples):\n        self.labels = labels\n        self.labels_set = list(set(self.labels.numpy()))\n        self.label_to_indices = {label: np.where(self.labels.numpy() == label)[0]\n                                 for label in self.labels_set}\n        for l in self.labels_set:\n            np.random.shuffle(self.label_to_indices[l])\n        self.used_label_indices_count = {label: 0 for label in self.labels_set}\n        self.count = 0\n        self.n_classes = n_classes\n        self.n_samples = n_samples\n        self.n_dataset = len(self.labels)\n        self.batch_size = self.n_samples * self.n_classes\n\n    def __iter__(self):\n        self.count = 0\n        while self.count + self.batch_size < self.n_dataset:\n            classes = np.random.choice(self.labels_set, self.n_classes, replace=False)\n            indices = []\n            for class_ in classes:\n                indices.extend(self.label_to_indices[class_][\n                               self.used_label_indices_count[class_]:self.used_label_indices_count[\n                                                                         class_] + self.n_samples])\n                self.used_label_indices_count[class_] += self.n_samples\n                if self.used_label_indices_count[class_] + self.n_samples > len(self.label_to_indices[class_]):\n                    np.random.shuffle(self.label_to_indices[class_])\n                    self.used_label_indices_count[class_] = 0\n            yield indices\n            self.count += self.n_classes * self.n_samples\n\n    def __len__(self):\n        return self.n_dataset // self.batch_size\n'"
losses.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass ContrastiveLoss(nn.Module):\n    """"""\n    Contrastive loss\n    Takes embeddings of two samples and a target label == 1 if samples are from the same class and label == 0 otherwise\n    """"""\n\n    def __init__(self, margin):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin\n        self.eps = 1e-9\n\n    def forward(self, output1, output2, target, size_average=True):\n        distances = (output2 - output1).pow(2).sum(1)  # squared distances\n        losses = 0.5 * (target.float() * distances +\n                        (1 + -1 * target).float() * F.relu(self.margin - (distances + self.eps).sqrt()).pow(2))\n        return losses.mean() if size_average else losses.sum()\n\n\nclass TripletLoss(nn.Module):\n    """"""\n    Triplet loss\n    Takes embeddings of an anchor sample, a positive sample and a negative sample\n    """"""\n\n    def __init__(self, margin):\n        super(TripletLoss, self).__init__()\n        self.margin = margin\n\n    def forward(self, anchor, positive, negative, size_average=True):\n        distance_positive = (anchor - positive).pow(2).sum(1)  # .pow(.5)\n        distance_negative = (anchor - negative).pow(2).sum(1)  # .pow(.5)\n        losses = F.relu(distance_positive - distance_negative + self.margin)\n        return losses.mean() if size_average else losses.sum()\n\n\nclass OnlineContrastiveLoss(nn.Module):\n    """"""\n    Online Contrastive loss\n    Takes a batch of embeddings and corresponding labels.\n    Pairs are generated using pair_selector object that take embeddings and targets and return indices of positive\n    and negative pairs\n    """"""\n\n    def __init__(self, margin, pair_selector):\n        super(OnlineContrastiveLoss, self).__init__()\n        self.margin = margin\n        self.pair_selector = pair_selector\n\n    def forward(self, embeddings, target):\n        positive_pairs, negative_pairs = self.pair_selector.get_pairs(embeddings, target)\n        if embeddings.is_cuda:\n            positive_pairs = positive_pairs.cuda()\n            negative_pairs = negative_pairs.cuda()\n        positive_loss = (embeddings[positive_pairs[:, 0]] - embeddings[positive_pairs[:, 1]]).pow(2).sum(1)\n        negative_loss = F.relu(\n            self.margin - (embeddings[negative_pairs[:, 0]] - embeddings[negative_pairs[:, 1]]).pow(2).sum(\n                1).sqrt()).pow(2)\n        loss = torch.cat([positive_loss, negative_loss], dim=0)\n        return loss.mean()\n\n\nclass OnlineTripletLoss(nn.Module):\n    """"""\n    Online Triplets loss\n    Takes a batch of embeddings and corresponding labels.\n    Triplets are generated using triplet_selector object that take embeddings and targets and return indices of\n    triplets\n    """"""\n\n    def __init__(self, margin, triplet_selector):\n        super(OnlineTripletLoss, self).__init__()\n        self.margin = margin\n        self.triplet_selector = triplet_selector\n\n    def forward(self, embeddings, target):\n\n        triplets = self.triplet_selector.get_triplets(embeddings, target)\n\n        if embeddings.is_cuda:\n            triplets = triplets.cuda()\n\n        ap_distances = (embeddings[triplets[:, 0]] - embeddings[triplets[:, 1]]).pow(2).sum(1)  # .pow(.5)\n        an_distances = (embeddings[triplets[:, 0]] - embeddings[triplets[:, 2]]).pow(2).sum(1)  # .pow(.5)\n        losses = F.relu(ap_distances - an_distances + self.margin)\n\n        return losses.mean(), len(triplets)\n'"
metrics.py,0,"b'import numpy as np\n\n\nclass Metric:\n    def __init__(self):\n        pass\n\n    def __call__(self, outputs, target, loss):\n        raise NotImplementedError\n\n    def reset(self):\n        raise NotImplementedError\n\n    def value(self):\n        raise NotImplementedError\n\n    def name(self):\n        raise NotImplementedError\n\n\nclass AccumulatedAccuracyMetric(Metric):\n    """"""\n    Works with classification model\n    """"""\n\n    def __init__(self):\n        self.correct = 0\n        self.total = 0\n\n    def __call__(self, outputs, target, loss):\n        pred = outputs[0].data.max(1, keepdim=True)[1]\n        self.correct += pred.eq(target[0].data.view_as(pred)).cpu().sum()\n        self.total += target[0].size(0)\n        return self.value()\n\n    def reset(self):\n        self.correct = 0\n        self.total = 0\n\n    def value(self):\n        return 100 * float(self.correct) / self.total\n\n    def name(self):\n        return \'Accuracy\'\n\n\nclass AverageNonzeroTripletsMetric(Metric):\n    \'\'\'\n    Counts average number of nonzero triplets found in minibatches\n    \'\'\'\n\n    def __init__(self):\n        self.values = []\n\n    def __call__(self, outputs, target, loss):\n        self.values.append(loss[1])\n        return self.value()\n\n    def reset(self):\n        self.values = []\n\n    def value(self):\n        return np.mean(self.values)\n\n    def name(self):\n        return \'Average nonzero triplets\'\n'"
networks.py,2,"b'import torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass EmbeddingNet(nn.Module):\n    def __init__(self):\n        super(EmbeddingNet, self).__init__()\n        self.convnet = nn.Sequential(nn.Conv2d(1, 32, 5), nn.PReLU(),\n                                     nn.MaxPool2d(2, stride=2),\n                                     nn.Conv2d(32, 64, 5), nn.PReLU(),\n                                     nn.MaxPool2d(2, stride=2))\n\n        self.fc = nn.Sequential(nn.Linear(64 * 4 * 4, 256),\n                                nn.PReLU(),\n                                nn.Linear(256, 256),\n                                nn.PReLU(),\n                                nn.Linear(256, 2)\n                                )\n\n    def forward(self, x):\n        output = self.convnet(x)\n        output = output.view(output.size()[0], -1)\n        output = self.fc(output)\n        return output\n\n    def get_embedding(self, x):\n        return self.forward(x)\n\n\nclass EmbeddingNetL2(EmbeddingNet):\n    def __init__(self):\n        super(EmbeddingNetL2, self).__init__()\n\n    def forward(self, x):\n        output = super(EmbeddingNetL2, self).forward(x)\n        output /= output.pow(2).sum(1, keepdim=True).sqrt()\n        return output\n\n    def get_embedding(self, x):\n        return self.forward(x)\n\n\nclass ClassificationNet(nn.Module):\n    def __init__(self, embedding_net, n_classes):\n        super(ClassificationNet, self).__init__()\n        self.embedding_net = embedding_net\n        self.n_classes = n_classes\n        self.nonlinear = nn.PReLU()\n        self.fc1 = nn.Linear(2, n_classes)\n\n    def forward(self, x):\n        output = self.embedding_net(x)\n        output = self.nonlinear(output)\n        scores = F.log_softmax(self.fc1(output), dim=-1)\n        return scores\n\n    def get_embedding(self, x):\n        return self.nonlinear(self.embedding_net(x))\n\n\nclass SiameseNet(nn.Module):\n    def __init__(self, embedding_net):\n        super(SiameseNet, self).__init__()\n        self.embedding_net = embedding_net\n\n    def forward(self, x1, x2):\n        output1 = self.embedding_net(x1)\n        output2 = self.embedding_net(x2)\n        return output1, output2\n\n    def get_embedding(self, x):\n        return self.embedding_net(x)\n\n\nclass TripletNet(nn.Module):\n    def __init__(self, embedding_net):\n        super(TripletNet, self).__init__()\n        self.embedding_net = embedding_net\n\n    def forward(self, x1, x2, x3):\n        output1 = self.embedding_net(x1)\n        output2 = self.embedding_net(x2)\n        output3 = self.embedding_net(x3)\n        return output1, output2, output3\n\n    def get_embedding(self, x):\n        return self.embedding_net(x)\n'"
trainer.py,1,"b'import torch\nimport numpy as np\n\n\ndef fit(train_loader, val_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, metrics=[],\n        start_epoch=0):\n    """"""\n    Loaders, model, loss function and metrics should work together for a given task,\n    i.e. The model should be able to process data output of loaders,\n    loss function should process target output of loaders and outputs from the model\n\n    Examples: Classification: batch loader, classification model, NLL loss, accuracy metric\n    Siamese network: Siamese loader, siamese model, contrastive loss\n    Online triplet learning: batch loader, embedding model, online triplet loss\n    """"""\n    for epoch in range(0, start_epoch):\n        scheduler.step()\n\n    for epoch in range(start_epoch, n_epochs):\n        scheduler.step()\n\n        # Train stage\n        train_loss, metrics = train_epoch(train_loader, model, loss_fn, optimizer, cuda, log_interval, metrics)\n\n        message = \'Epoch: {}/{}. Train set: Average loss: {:.4f}\'.format(epoch + 1, n_epochs, train_loss)\n        for metric in metrics:\n            message += \'\\t{}: {}\'.format(metric.name(), metric.value())\n\n        val_loss, metrics = test_epoch(val_loader, model, loss_fn, cuda, metrics)\n        val_loss /= len(val_loader)\n\n        message += \'\\nEpoch: {}/{}. Validation set: Average loss: {:.4f}\'.format(epoch + 1, n_epochs,\n                                                                                 val_loss)\n        for metric in metrics:\n            message += \'\\t{}: {}\'.format(metric.name(), metric.value())\n\n        print(message)\n\n\ndef train_epoch(train_loader, model, loss_fn, optimizer, cuda, log_interval, metrics):\n    for metric in metrics:\n        metric.reset()\n\n    model.train()\n    losses = []\n    total_loss = 0\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        target = target if len(target) > 0 else None\n        if not type(data) in (tuple, list):\n            data = (data,)\n        if cuda:\n            data = tuple(d.cuda() for d in data)\n            if target is not None:\n                target = target.cuda()\n\n\n        optimizer.zero_grad()\n        outputs = model(*data)\n\n        if type(outputs) not in (tuple, list):\n            outputs = (outputs,)\n\n        loss_inputs = outputs\n        if target is not None:\n            target = (target,)\n            loss_inputs += target\n\n        loss_outputs = loss_fn(*loss_inputs)\n        loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs\n        losses.append(loss.item())\n        total_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n\n        for metric in metrics:\n            metric(outputs, target, loss_outputs)\n\n        if batch_idx % log_interval == 0:\n            message = \'Train: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\'.format(\n                batch_idx * len(data[0]), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), np.mean(losses))\n            for metric in metrics:\n                message += \'\\t{}: {}\'.format(metric.name(), metric.value())\n\n            print(message)\n            losses = []\n\n    total_loss /= (batch_idx + 1)\n    return total_loss, metrics\n\n\ndef test_epoch(val_loader, model, loss_fn, cuda, metrics):\n    with torch.no_grad():\n        for metric in metrics:\n            metric.reset()\n        model.eval()\n        val_loss = 0\n        for batch_idx, (data, target) in enumerate(val_loader):\n            target = target if len(target) > 0 else None\n            if not type(data) in (tuple, list):\n                data = (data,)\n            if cuda:\n                data = tuple(d.cuda() for d in data)\n                if target is not None:\n                    target = target.cuda()\n\n            outputs = model(*data)\n\n            if type(outputs) not in (tuple, list):\n                outputs = (outputs,)\n            loss_inputs = outputs\n            if target is not None:\n                target = (target,)\n                loss_inputs += target\n\n            loss_outputs = loss_fn(*loss_inputs)\n            loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs\n            val_loss += loss.item()\n\n            for metric in metrics:\n                metric(outputs, target, loss_outputs)\n\n    return val_loss, metrics\n'"
utils.py,8,"b'from itertools import combinations\n\nimport numpy as np\nimport torch\n\n\ndef pdist(vectors):\n    distance_matrix = -2 * vectors.mm(torch.t(vectors)) + vectors.pow(2).sum(dim=1).view(1, -1) + vectors.pow(2).sum(\n        dim=1).view(-1, 1)\n    return distance_matrix\n\n\nclass PairSelector:\n    """"""\n    Implementation should return indices of positive pairs and negative pairs that will be passed to compute\n    Contrastive Loss\n    return positive_pairs, negative_pairs\n    """"""\n\n    def __init__(self):\n        pass\n\n    def get_pairs(self, embeddings, labels):\n        raise NotImplementedError\n\n\nclass AllPositivePairSelector(PairSelector):\n    """"""\n    Discards embeddings and generates all possible pairs given labels.\n    If balance is True, negative pairs are a random sample to match the number of positive samples\n    """"""\n    def __init__(self, balance=True):\n        super(AllPositivePairSelector, self).__init__()\n        self.balance = balance\n\n    def get_pairs(self, embeddings, labels):\n        labels = labels.cpu().data.numpy()\n        all_pairs = np.array(list(combinations(range(len(labels)), 2)))\n        all_pairs = torch.LongTensor(all_pairs)\n        positive_pairs = all_pairs[(labels[all_pairs[:, 0]] == labels[all_pairs[:, 1]]).nonzero()]\n        negative_pairs = all_pairs[(labels[all_pairs[:, 0]] != labels[all_pairs[:, 1]]).nonzero()]\n        if self.balance:\n            negative_pairs = negative_pairs[torch.randperm(len(negative_pairs))[:len(positive_pairs)]]\n\n        return positive_pairs, negative_pairs\n\n\nclass HardNegativePairSelector(PairSelector):\n    """"""\n    Creates all possible positive pairs. For negative pairs, pairs with smallest distance are taken into consideration,\n    matching the number of positive pairs.\n    """"""\n\n    def __init__(self, cpu=True):\n        super(HardNegativePairSelector, self).__init__()\n        self.cpu = cpu\n\n    def get_pairs(self, embeddings, labels):\n        if self.cpu:\n            embeddings = embeddings.cpu()\n        distance_matrix = pdist(embeddings)\n\n        labels = labels.cpu().data.numpy()\n        all_pairs = np.array(list(combinations(range(len(labels)), 2)))\n        all_pairs = torch.LongTensor(all_pairs)\n        positive_pairs = all_pairs[(labels[all_pairs[:, 0]] == labels[all_pairs[:, 1]]).nonzero()]\n        negative_pairs = all_pairs[(labels[all_pairs[:, 0]] != labels[all_pairs[:, 1]]).nonzero()]\n\n        negative_distances = distance_matrix[negative_pairs[:, 0], negative_pairs[:, 1]]\n        negative_distances = negative_distances.cpu().data.numpy()\n        top_negatives = np.argpartition(negative_distances, len(positive_pairs))[:len(positive_pairs)]\n        top_negative_pairs = negative_pairs[torch.LongTensor(top_negatives)]\n\n        return positive_pairs, top_negative_pairs\n\n\nclass TripletSelector:\n    """"""\n    Implementation should return indices of anchors, positive and negative samples\n    return np array of shape [N_triplets x 3]\n    """"""\n\n    def __init__(self):\n        pass\n\n    def get_triplets(self, embeddings, labels):\n        raise NotImplementedError\n\n\nclass AllTripletSelector(TripletSelector):\n    """"""\n    Returns all possible triplets\n    May be impractical in most cases\n    """"""\n\n    def __init__(self):\n        super(AllTripletSelector, self).__init__()\n\n    def get_triplets(self, embeddings, labels):\n        labels = labels.cpu().data.numpy()\n        triplets = []\n        for label in set(labels):\n            label_mask = (labels == label)\n            label_indices = np.where(label_mask)[0]\n            if len(label_indices) < 2:\n                continue\n            negative_indices = np.where(np.logical_not(label_mask))[0]\n            anchor_positives = list(combinations(label_indices, 2))  # All anchor-positive pairs\n\n            # Add all negatives for all positive pairs\n            temp_triplets = [[anchor_positive[0], anchor_positive[1], neg_ind] for anchor_positive in anchor_positives\n                             for neg_ind in negative_indices]\n            triplets += temp_triplets\n\n        return torch.LongTensor(np.array(triplets))\n\n\ndef hardest_negative(loss_values):\n    hard_negative = np.argmax(loss_values)\n    return hard_negative if loss_values[hard_negative] > 0 else None\n\n\ndef random_hard_negative(loss_values):\n    hard_negatives = np.where(loss_values > 0)[0]\n    return np.random.choice(hard_negatives) if len(hard_negatives) > 0 else None\n\n\ndef semihard_negative(loss_values, margin):\n    semihard_negatives = np.where(np.logical_and(loss_values < margin, loss_values > 0))[0]\n    return np.random.choice(semihard_negatives) if len(semihard_negatives) > 0 else None\n\n\nclass FunctionNegativeTripletSelector(TripletSelector):\n    """"""\n    For each positive pair, takes the hardest negative sample (with the greatest triplet loss value) to create a triplet\n    Margin should match the margin used in triplet loss.\n    negative_selection_fn should take array of loss_values for a given anchor-positive pair and all negative samples\n    and return a negative index for that pair\n    """"""\n\n    def __init__(self, margin, negative_selection_fn, cpu=True):\n        super(FunctionNegativeTripletSelector, self).__init__()\n        self.cpu = cpu\n        self.margin = margin\n        self.negative_selection_fn = negative_selection_fn\n\n    def get_triplets(self, embeddings, labels):\n        if self.cpu:\n            embeddings = embeddings.cpu()\n        distance_matrix = pdist(embeddings)\n        distance_matrix = distance_matrix.cpu()\n\n        labels = labels.cpu().data.numpy()\n        triplets = []\n\n        for label in set(labels):\n            label_mask = (labels == label)\n            label_indices = np.where(label_mask)[0]\n            if len(label_indices) < 2:\n                continue\n            negative_indices = np.where(np.logical_not(label_mask))[0]\n            anchor_positives = list(combinations(label_indices, 2))  # All anchor-positive pairs\n            anchor_positives = np.array(anchor_positives)\n\n            ap_distances = distance_matrix[anchor_positives[:, 0], anchor_positives[:, 1]]\n            for anchor_positive, ap_distance in zip(anchor_positives, ap_distances):\n                loss_values = ap_distance - distance_matrix[torch.LongTensor(np.array([anchor_positive[0]])), torch.LongTensor(negative_indices)] + self.margin\n                loss_values = loss_values.data.cpu().numpy()\n                hard_negative = self.negative_selection_fn(loss_values)\n                if hard_negative is not None:\n                    hard_negative = negative_indices[hard_negative]\n                    triplets.append([anchor_positive[0], anchor_positive[1], hard_negative])\n\n        if len(triplets) == 0:\n            triplets.append([anchor_positive[0], anchor_positive[1], negative_indices[0]])\n\n        triplets = np.array(triplets)\n\n        return torch.LongTensor(triplets)\n\n\ndef HardestNegativeTripletSelector(margin, cpu=False): return FunctionNegativeTripletSelector(margin=margin,\n                                                                                 negative_selection_fn=hardest_negative,\n                                                                                 cpu=cpu)\n\n\ndef RandomNegativeTripletSelector(margin, cpu=False): return FunctionNegativeTripletSelector(margin=margin,\n                                                                                negative_selection_fn=random_hard_negative,\n                                                                                cpu=cpu)\n\n\ndef SemihardNegativeTripletSelector(margin, cpu=False): return FunctionNegativeTripletSelector(margin=margin,\n                                                                                  negative_selection_fn=lambda x: semihard_negative(x, margin),\n                                                                                  cpu=cpu)\n'"
