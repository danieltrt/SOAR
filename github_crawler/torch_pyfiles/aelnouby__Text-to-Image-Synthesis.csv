file_path,api_count,code
convert_cub_to_hd5_script.py,1,"b'import os\nfrom os.path import join, isfile\nimport numpy as np\nimport h5py\nfrom glob import glob\nfrom torch.utils.serialization import load_lua  \nfrom PIL import Image \nimport yaml\nimport io\nimport pdb\n\nwith open(\'config.yaml\', \'r\') as f:\n\tconfig = yaml.load(f)\n\nimages_path = config[\'birds_images_path\']\nembedding_path = config[\'birds_embedding_path\']\ntext_path = config[\'birds_text_path\']\ndatasetDir = config[\'birds_dataset_path\']\n\nval_classes = open(config[\'val_split_path\']).read().splitlines()\ntrain_classes = open(config[\'train_split_path\']).read().splitlines()\ntest_classes = open(config[\'test_split_path\']).read().splitlines()\n\nf = h5py.File(datasetDir, \'w\')\ntrain = f.create_group(\'train\')\nvalid = f.create_group(\'valid\')\ntest = f.create_group(\'test\')\n\nfor _class in sorted(os.listdir(embedding_path)):\n\tsplit = \'\'\n\tif _class in train_classes:\n\t\tsplit = train\n\telif _class in val_classes:\n\t\tsplit = valid\n\telif _class in test_classes:\n\t\tsplit = test\n\n\tdata_path = os.path.join(embedding_path, _class)\n\ttxt_path = os.path.join(text_path, _class)\n\tfor example, txt_file in zip(sorted(glob(data_path + ""/*.t7"")), sorted(glob(txt_path + ""/*.txt""))):\n\t\texample_data = load_lua(example)\n\t\timg_path = example_data[\'img\']\n\t\tembeddings = example_data[\'txt\'].numpy()\n\t\texample_name = img_path.split(\'/\')[-1][:-4]\n\n\t\tf = open(txt_file, ""r"")\n\t\ttxt = f.readlines()\n\t\tf.close()\n\n\t\timg_path = os.path.join(images_path, img_path)\n\t\timg = open(img_path, \'rb\').read()\n\n\t\ttxt_choice = np.random.choice(range(10), 5)\n\n\t\tembeddings = embeddings[txt_choice]\n\t\ttxt = np.array(txt)\n\t\ttxt = txt[txt_choice]\n\t\tdt = h5py.special_dtype(vlen=str)\n\n\t\tfor c, e in enumerate(embeddings):\n\t\t\tex = split.create_group(example_name + \'_\' + str(c))\n\t\t\tex.create_dataset(\'name\', data=example_name)\n\t\t\tex.create_dataset(\'img\', data=np.void(img))\n\t\t\tex.create_dataset(\'embeddings\', data=e)\n\t\t\tex.create_dataset(\'class\', data=_class)\n\t\t\tex.create_dataset(\'txt\', data=txt[c].astype(object), dtype=dt)\n\n\t\tprint(example_name)\n\n\n\n'"
convert_flowers_to_hd5_script.py,1,"b'import os\nfrom os.path import join, isfile\nimport numpy as np\nimport h5py\nfrom glob import glob\nfrom torch.utils.serialization import load_lua\nfrom PIL import Image\nimport yaml\nimport io\nimport pdb\n\nwith open(\'config.yaml\', \'r\') as f:\n\tconfig = yaml.load(f)\n\nimages_path = config[\'flowers_images_path\']\nembedding_path = config[\'flowers_embedding_path\']\ntext_path = config[\'flowers_text_path\']\ndatasetDir = config[\'flowers_dataset_path\']\n\nval_classes = open(config[\'flowers_val_split_path\']).read().splitlines()\ntrain_classes = open(config[\'flowers_train_split_path\']).read().splitlines()\ntest_classes = open(config[\'flowers_test_split_path\']).read().splitlines()\n\nf = h5py.File(datasetDir, \'w\')\ntrain = f.create_group(\'train\')\nvalid = f.create_group(\'valid\')\ntest = f.create_group(\'test\')\n\nfor _class in sorted(os.listdir(embedding_path)):\n\tsplit = \'\'\n\tif _class in train_classes:\n\t\tsplit = train\n\telif _class in val_classes:\n\t\tsplit = valid\n\telif _class in test_classes:\n\t\tsplit = test\n\n\tdata_path = os.path.join(embedding_path, _class)\n\ttxt_path = os.path.join(text_path, _class)\n\tfor example, txt_file in zip(sorted(glob(data_path + ""/*.t7"")), sorted(glob(txt_path + ""/*.txt""))):\n\t\texample_data = load_lua(example)\n\t\timg_path = example_data[\'img\']\n\t\tembeddings = example_data[\'txt\'].numpy()\n\t\texample_name = img_path.split(\'/\')[-1][:-4]\n\n\t\tf = open(txt_file, ""r"")\n\t\ttxt = f.readlines()\n\t\tf.close()\n\n\t\timg_path = os.path.join(images_path, img_path)\n\t\timg = open(img_path, \'rb\').read()\n\n\t\ttxt_choice = np.random.choice(range(10), 5)\n\n\t\tembeddings = embeddings[txt_choice]\n\t\ttxt = np.array(txt)\n\t\ttxt = txt[txt_choice]\n\t\tdt = h5py.special_dtype(vlen=str)\n\n\t\tfor c, e in enumerate(embeddings):\n\t\t\tex = split.create_group(example_name + \'_\' + str(c))\n\t\t\tex.create_dataset(\'name\', data=example_name)\n\t\t\tex.create_dataset(\'img\', data=np.void(img))\n\t\t\tex.create_dataset(\'embeddings\', data=e)\n\t\t\tex.create_dataset(\'class\', data=_class)\n\t\t\tex.create_dataset(\'txt\', data=txt[c].astype(object), dtype=dt)\n\n\t\tprint(example_name, txt[1], _class)\n\n\n\n'"
loss_estimator.py,7,"b'import torch\nfrom torch import  nn\nimport numpy as np\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\nclass generator_loss(torch.nn.Module):\n    def __init__(self):\n        super(generator_loss, self).__init__()\n        self.estimator = nn.BCELoss()\n\n    def forward(self, fake):\n        batch_size = fake.size()[0]\n        self.labels = Variable(torch.FloatTensor(batch_size).cuda().fill_(1))\n        return self.estimator(fake, self.labels)\n\nclass discriminator_loss(torch.nn.Module):\n    def __init__(self):\n        super(discriminator_loss, self).__init__()\n        self.estimator = nn.BCELoss()\n\n    def forward(self, real, wrong, fake):\n        batch_size = real.size()[0]\n        self.real_labels = Variable(torch.FloatTensor(batch_size).cuda().fill_(1))\n        self.fake_labels = Variable(torch.FloatTensor(batch_size).cuda().fill_(0))\n        return self.estimator(real, self.real_labels) + 0.5 * (self.estimator(wrong, self.fake_labels) + self.estimator(fake, self.fake_labels))\n\n'"
runtime.py,0,"b'from trainer import Trainer\nimport argparse\nfrom PIL import Image\nimport os\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""--type"", default=\'gan\')\nparser.add_argument(""--lr"", default=0.0002, type=float)\nparser.add_argument(""--l1_coef"", default=50, type=float)\nparser.add_argument(""--l2_coef"", default=100, type=float)\nparser.add_argument(""--diter"", default=5, type=int)\nparser.add_argument(""--cls"", default=False, action=\'store_true\')\nparser.add_argument(""--vis_screen"", default=\'gan\')\nparser.add_argument(""--save_path"", default=\'\')\nparser.add_argument(""--inference"", default=False, action=\'store_true\')\nparser.add_argument(\'--pre_trained_disc\', default=None)\nparser.add_argument(\'--pre_trained_gen\', default=None)\nparser.add_argument(\'--dataset\', default=\'flowers\')\nparser.add_argument(\'--split\', default=0, type=int)\nparser.add_argument(\'--batch_size\', default=64, type=int)\nparser.add_argument(\'--num_workers\', default=8, type=int)\nparser.add_argument(\'--epochs\', default=200, type=int)\nargs = parser.parse_args()\n\ntrainer = Trainer(type=args.type,\n                  dataset=args.dataset,\n                  split=args.split,\n                  lr=args.lr,\n                  diter=args.diter,\n                  vis_screen=args.vis_screen,\n                  save_path=args.save_path,\n                  l1_coef=args.l1_coef,\n                  l2_coef=args.l2_coef,\n                  pre_trained_disc=args.pre_trained_disc,\n                  pre_trained_gen=args.pre_trained_gen,\n                  batch_size=args.batch_size,\n                  num_workers=args.num_workers,\n                  epochs=args.epochs\n                  )\n\nif not args.inference:\n    trainer.train(args.cls)\nelse:\n    trainer.predict()\n\n'"
trainer.py,36,"b'import numpy as np\nimport torch\nimport yaml\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\n\nfrom txt2image_dataset import Text2ImageDataset\nfrom models.gan_factory import gan_factory\nfrom utils import Utils, Logger\nfrom PIL import Image\nimport os\n\nclass Trainer(object):\n    def __init__(self, type, dataset, split, lr, diter, vis_screen, save_path, l1_coef, l2_coef, pre_trained_gen, pre_trained_disc, batch_size, num_workers, epochs):\n        with open(\'config.yaml\', \'r\') as f:\n            config = yaml.load(f)\n\n        self.generator = torch.nn.DataParallel(gan_factory.generator_factory(type).cuda())\n        self.discriminator = torch.nn.DataParallel(gan_factory.discriminator_factory(type).cuda())\n\n        if pre_trained_disc:\n            self.discriminator.load_state_dict(torch.load(pre_trained_disc))\n        else:\n            self.discriminator.apply(Utils.weights_init)\n\n        if pre_trained_gen:\n            self.generator.load_state_dict(torch.load(pre_trained_gen))\n        else:\n            self.generator.apply(Utils.weights_init)\n\n        if dataset == \'birds\':\n            self.dataset = Text2ImageDataset(config[\'birds_dataset_path\'], split=split)\n        elif dataset == \'flowers\':\n            self.dataset = Text2ImageDataset(config[\'flowers_dataset_path\'], split=split)\n        else:\n            print(\'Dataset not supported, please select either birds or flowers.\')\n            exit()\n\n        self.noise_dim = 100\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.lr = lr\n        self.beta1 = 0.5\n        self.num_epochs = epochs\n        self.DITER = diter\n\n        self.l1_coef = l1_coef\n        self.l2_coef = l2_coef\n\n        self.data_loader = DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True,\n                                num_workers=self.num_workers)\n\n        self.optimD = torch.optim.Adam(self.discriminator.parameters(), lr=self.lr, betas=(self.beta1, 0.999))\n        self.optimG = torch.optim.Adam(self.generator.parameters(), lr=self.lr, betas=(self.beta1, 0.999))\n\n        self.logger = Logger(vis_screen)\n        self.checkpoints_path = \'checkpoints\'\n        self.save_path = save_path\n        self.type = type\n\n    def train(self, cls=False):\n\n        if self.type == \'wgan\':\n            self._train_wgan(cls)\n        elif self.type == \'gan\':\n            self._train_gan(cls)\n        elif self.type == \'vanilla_wgan\':\n            self._train_vanilla_wgan()\n        elif self.type == \'vanilla_gan\':\n            self._train_vanilla_gan()\n\n    def _train_wgan(self, cls):\n        one = torch.FloatTensor([1])\n        mone = one * -1\n\n        one = Variable(one).cuda()\n        mone = Variable(mone).cuda()\n\n        gen_iteration = 0\n        for epoch in range(self.num_epochs):\n            iterator = 0\n            data_iterator = iter(self.data_loader)\n\n            while iterator < len(self.data_loader):\n\n                if gen_iteration < 25 or gen_iteration % 500 == 0:\n                    d_iter_count = 100\n                else:\n                    d_iter_count = self.DITER\n\n                d_iter = 0\n\n                # Train the discriminator\n                while d_iter < d_iter_count and iterator < len(self.data_loader):\n                    d_iter += 1\n\n                    for p in self.discriminator.parameters():\n                        p.requires_grad = True\n\n                    self.discriminator.zero_grad()\n\n                    sample = next(data_iterator)\n                    iterator += 1\n\n                    right_images = sample[\'right_images\']\n                    right_embed = sample[\'right_embed\']\n                    wrong_images = sample[\'wrong_images\']\n\n                    right_images = Variable(right_images.float()).cuda()\n                    right_embed = Variable(right_embed.float()).cuda()\n                    wrong_images = Variable(wrong_images.float()).cuda()\n\n                    outputs, _ = self.discriminator(right_images, right_embed)\n                    real_loss = torch.mean(outputs)\n                    real_loss.backward(mone)\n\n                    if cls:\n                        outputs, _ = self.discriminator(wrong_images, right_embed)\n                        wrong_loss = torch.mean(outputs)\n                        wrong_loss.backward(one)\n\n                    noise = Variable(torch.randn(right_images.size(0), self.noise_dim), volatile=True).cuda()\n                    noise = noise.view(noise.size(0), self.noise_dim, 1, 1)\n\n                    fake_images = Variable(self.generator(right_embed, noise).data)\n                    outputs, _ = self.discriminator(fake_images, right_embed)\n                    fake_loss = torch.mean(outputs)\n                    fake_loss.backward(one)\n\n                    ## NOTE: Pytorch had a bug with gradient penalty at the time of this project development\n                    ##       , uncomment the next two lines and remove the params clamping below if you want to try gradient penalty\n                    # gp = Utils.compute_GP(self.discriminator, right_images.data, right_embed, fake_images.data, LAMBDA=10)\n                    # gp.backward()\n\n                    d_loss = real_loss - fake_loss\n\n                    if cls:\n                        d_loss = d_loss - wrong_loss\n\n                    self.optimD.step()\n\n                    for p in self.discriminator.parameters():\n                        p.data.clamp_(-0.01, 0.01)\n\n                # Train Generator\n                for p in self.discriminator.parameters():\n                    p.requires_grad = False\n                self.generator.zero_grad()\n                noise = Variable(torch.randn(right_images.size(0), 100)).cuda()\n                noise = noise.view(noise.size(0), 100, 1, 1)\n                fake_images = self.generator(right_embed, noise)\n                outputs, _ = self.discriminator(fake_images, right_embed)\n\n                g_loss = torch.mean(outputs)\n                g_loss.backward(mone)\n                g_loss = - g_loss\n                self.optimG.step()\n\n                gen_iteration += 1\n\n                self.logger.draw(right_images, fake_images)\n                self.logger.log_iteration_wgan(epoch, gen_iteration, d_loss, g_loss, real_loss, fake_loss)\n                \n            self.logger.plot_epoch(gen_iteration)\n\n            if (epoch+1) % 50 == 0:\n                Utils.save_checkpoint(self.discriminator, self.generator, self.checkpoints_path, epoch)\n\n    def _train_gan(self, cls):\n        criterion = nn.BCELoss()\n        l2_loss = nn.MSELoss()\n        l1_loss = nn.L1Loss()\n        iteration = 0\n\n        for epoch in range(self.num_epochs):\n            for sample in self.data_loader:\n                iteration += 1\n                right_images = sample[\'right_images\']\n                right_embed = sample[\'right_embed\']\n                wrong_images = sample[\'wrong_images\']\n\n                right_images = Variable(right_images.float()).cuda()\n                right_embed = Variable(right_embed.float()).cuda()\n                wrong_images = Variable(wrong_images.float()).cuda()\n\n                real_labels = torch.ones(right_images.size(0))\n                fake_labels = torch.zeros(right_images.size(0))\n\n                # ======== One sided label smoothing ==========\n                # Helps preventing the discriminator from overpowering the\n                # generator adding penalty when the discriminator is too confident\n                # =============================================\n                smoothed_real_labels = torch.FloatTensor(Utils.smooth_label(real_labels.numpy(), -0.1))\n\n                real_labels = Variable(real_labels).cuda()\n                smoothed_real_labels = Variable(smoothed_real_labels).cuda()\n                fake_labels = Variable(fake_labels).cuda()\n\n                # Train the discriminator\n                self.discriminator.zero_grad()\n                outputs, activation_real = self.discriminator(right_images, right_embed)\n                real_loss = criterion(outputs, smoothed_real_labels)\n                real_score = outputs\n\n                if cls:\n                    outputs, _ = self.discriminator(wrong_images, right_embed)\n                    wrong_loss = criterion(outputs, fake_labels)\n                    wrong_score = outputs\n\n                noise = Variable(torch.randn(right_images.size(0), 100)).cuda()\n                noise = noise.view(noise.size(0), 100, 1, 1)\n                fake_images = self.generator(right_embed, noise)\n                outputs, _ = self.discriminator(fake_images, right_embed)\n                fake_loss = criterion(outputs, fake_labels)\n                fake_score = outputs\n\n                d_loss = real_loss + fake_loss\n\n                if cls:\n                    d_loss = d_loss + wrong_loss\n\n                d_loss.backward()\n                self.optimD.step()\n\n                # Train the generator\n                self.generator.zero_grad()\n                noise = Variable(torch.randn(right_images.size(0), 100)).cuda()\n                noise = noise.view(noise.size(0), 100, 1, 1)\n                fake_images = self.generator(right_embed, noise)\n                outputs, activation_fake = self.discriminator(fake_images, right_embed)\n                _, activation_real = self.discriminator(right_images, right_embed)\n\n                activation_fake = torch.mean(activation_fake, 0)\n                activation_real = torch.mean(activation_real, 0)\n\n\n                #======= Generator Loss function============\n                # This is a customized loss function, the first term is the regular cross entropy loss\n                # The second term is feature matching loss, this measure the distance between the real and generated\n                # images statistics by comparing intermediate layers activations\n                # The third term is L1 distance between the generated and real images, this is helpful for the conditional case\n                # because it links the embedding feature vector directly to certain pixel values.\n                #===========================================\n                g_loss = criterion(outputs, real_labels) \\\n                         + self.l2_coef * l2_loss(activation_fake, activation_real.detach()) \\\n                         + self.l1_coef * l1_loss(fake_images, right_images)\n\n                g_loss.backward()\n                self.optimG.step()\n\n                if iteration % 5 == 0:\n                    self.logger.log_iteration_gan(epoch,d_loss, g_loss, real_score, fake_score)\n                    self.logger.draw(right_images, fake_images)\n\n            self.logger.plot_epoch_w_scores(epoch)\n\n            if (epoch) % 10 == 0:\n                Utils.save_checkpoint(self.discriminator, self.generator, self.checkpoints_path, self.save_path, epoch)\n\n    def _train_vanilla_wgan(self):\n        one = Variable(torch.FloatTensor([1])).cuda()\n        mone = one * -1\n        gen_iteration = 0\n\n        for epoch in range(self.num_epochs):\n         iterator = 0\n         data_iterator = iter(self.data_loader)\n\n         while iterator < len(self.data_loader):\n\n             if gen_iteration < 25 or gen_iteration % 500 == 0:\n                 d_iter_count = 100\n             else:\n                 d_iter_count = self.DITER\n\n             d_iter = 0\n\n             # Train the discriminator\n             while d_iter < d_iter_count and iterator < len(self.data_loader):\n                 d_iter += 1\n\n                 for p in self.discriminator.parameters():\n                     p.requires_grad = True\n\n                 self.discriminator.zero_grad()\n\n                 sample = next(data_iterator)\n                 iterator += 1\n\n                 right_images = sample[\'right_images\']\n                 right_images = Variable(right_images.float()).cuda()\n\n                 outputs, _ = self.discriminator(right_images)\n                 real_loss = torch.mean(outputs)\n                 real_loss.backward(mone)\n\n                 noise = Variable(torch.randn(right_images.size(0), self.noise_dim), volatile=True).cuda()\n                 noise = noise.view(noise.size(0), self.noise_dim, 1, 1)\n\n                 fake_images = Variable(self.generator(noise).data)\n                 outputs, _ = self.discriminator(fake_images)\n                 fake_loss = torch.mean(outputs)\n                 fake_loss.backward(one)\n\n                 ## NOTE: Pytorch had a bug with gradient penalty at the time of this project development\n                 ##       , uncomment the next two lines and remove the params clamping below if you want to try gradient penalty\n                 # gp = Utils.compute_GP(self.discriminator, right_images.data, right_embed, fake_images.data, LAMBDA=10)\n                 # gp.backward()\n\n                 d_loss = real_loss - fake_loss\n                 self.optimD.step()\n\n                 for p in self.discriminator.parameters():\n                     p.data.clamp_(-0.01, 0.01)\n\n             # Train Generator\n             for p in self.discriminator.parameters():\n                 p.requires_grad = False\n\n             self.generator.zero_grad()\n             noise = Variable(torch.randn(right_images.size(0), 100)).cuda()\n             noise = noise.view(noise.size(0), 100, 1, 1)\n             fake_images = self.generator(noise)\n             outputs, _ = self.discriminator(fake_images)\n\n             g_loss = torch.mean(outputs)\n             g_loss.backward(mone)\n             g_loss = - g_loss\n             self.optimG.step()\n\n             gen_iteration += 1\n\n             self.logger.draw(right_images, fake_images)\n             self.logger.log_iteration_wgan(epoch, gen_iteration, d_loss, g_loss, real_loss, fake_loss)\n\n         self.logger.plot_epoch(gen_iteration)\n\n         if (epoch + 1) % 50 == 0:\n             Utils.save_checkpoint(self.discriminator, self.generator, self.checkpoints_path, epoch)\n\n    def _train_vanilla_gan(self):\n        criterion = nn.BCELoss()\n        l2_loss = nn.MSELoss()\n        l1_loss = nn.L1Loss()\n        iteration = 0\n\n        for epoch in range(self.num_epochs):\n            for sample in self.data_loader:\n                iteration += 1\n                right_images = sample[\'right_images\']\n\n                right_images = Variable(right_images.float()).cuda()\n\n                real_labels = torch.ones(right_images.size(0))\n                fake_labels = torch.zeros(right_images.size(0))\n\n                # ======== One sided label smoothing ==========\n                # Helps preventing the discriminator from overpowering the\n                # generator adding penalty when the discriminator is too confident\n                # =============================================\n                smoothed_real_labels = torch.FloatTensor(Utils.smooth_label(real_labels.numpy(), -0.1))\n\n                real_labels = Variable(real_labels).cuda()\n                smoothed_real_labels = Variable(smoothed_real_labels).cuda()\n                fake_labels = Variable(fake_labels).cuda()\n\n\n                # Train the discriminator\n                self.discriminator.zero_grad()\n                outputs, activation_real = self.discriminator(right_images)\n                real_loss = criterion(outputs, smoothed_real_labels)\n                real_score = outputs\n\n                noise = Variable(torch.randn(right_images.size(0), 100)).cuda()\n                noise = noise.view(noise.size(0), 100, 1, 1)\n                fake_images = self.generator(noise)\n                outputs, _ = self.discriminator(fake_images)\n                fake_loss = criterion(outputs, fake_labels)\n                fake_score = outputs\n\n                d_loss = real_loss + fake_loss\n\n                d_loss.backward()\n                self.optimD.step()\n\n                # Train the generator\n                self.generator.zero_grad()\n                noise = Variable(torch.randn(right_images.size(0), 100)).cuda()\n                noise = noise.view(noise.size(0), 100, 1, 1)\n                fake_images = self.generator(noise)\n                outputs, activation_fake = self.discriminator(fake_images)\n                _, activation_real = self.discriminator(right_images)\n\n                activation_fake = torch.mean(activation_fake, 0)\n                activation_real = torch.mean(activation_real, 0)\n\n                # ======= Generator Loss function============\n                # This is a customized loss function, the first term is the regular cross entropy loss\n                # The second term is feature matching loss, this measure the distance between the real and generated\n                # images statistics by comparing intermediate layers activations\n                # The third term is L1 distance between the generated and real images, this is helpful for the conditional case\n                # because it links the embedding feature vector directly to certain pixel values.\n                g_loss = criterion(outputs, real_labels) \\\n                         + self.l2_coef * l2_loss(activation_fake, activation_real.detach()) \\\n                         + self.l1_coef * l1_loss(fake_images, right_images)\n\n                g_loss.backward()\n                self.optimG.step()\n\n                if iteration % 5 == 0:\n                    self.logger.log_iteration_gan(epoch, d_loss, g_loss, real_score, fake_score)\n                    self.logger.draw(right_images, fake_images)\n\n            self.logger.plot_epoch_w_scores(iteration)\n\n            if (epoch) % 50 == 0:\n                Utils.save_checkpoint(self.discriminator, self.generator, self.checkpoints_path, epoch)\n\n    def predict(self):\n        for sample in self.data_loader:\n            right_images = sample[\'right_images\']\n            right_embed = sample[\'right_embed\']\n            txt = sample[\'txt\']\n\n            if not os.path.exists(\'results/{0}\'.format(self.save_path)):\n                os.makedirs(\'results/{0}\'.format(self.save_path))\n\n            right_images = Variable(right_images.float()).cuda()\n            right_embed = Variable(right_embed.float()).cuda()\n\n            # Train the generator\n            noise = Variable(torch.randn(right_images.size(0), 100)).cuda()\n            noise = noise.view(noise.size(0), 100, 1, 1)\n            fake_images = self.generator(right_embed, noise)\n\n            self.logger.draw(right_images, fake_images)\n\n            for image, t in zip(fake_images, txt):\n                im = Image.fromarray(image.data.mul_(127.5).add_(127.5).byte().permute(1, 2, 0).cpu().numpy())\n                im.save(\'results/{0}/{1}.jpg\'.format(self.save_path, t.replace(""/"", """")[:100]))\n                print(t)\n\n\n\n\n\n\n\n'"
txt2image_dataset.py,7,"b""import os\nimport io\nfrom torch.utils.data import Dataset, DataLoader\nimport h5py\nimport numpy as np\nimport pdb\nfrom PIL import Image\nimport torch\nfrom torch.autograd import Variable\nimport pdb\nimport torch.nn.functional as F\n\nclass Text2ImageDataset(Dataset):\n\n    def __init__(self, datasetFile, transform=None, split=0):\n        self.datasetFile = datasetFile\n        self.transform = transform\n        self.dataset = None\n        self.dataset_keys = None\n        self.split = 'train' if split == 0 else 'valid' if split == 1 else 'test'\n        self.h5py2int = lambda x: int(np.array(x))\n\n    def __len__(self):\n        f = h5py.File(self.datasetFile, 'r')\n        self.dataset_keys = [str(k) for k in f[self.split].keys()]\n        length = len(f[self.split])\n        f.close()\n\n        return length\n\n    def __getitem__(self, idx):\n        if self.dataset is None:\n            self.dataset = h5py.File(self.datasetFile, mode='r')\n            self.dataset_keys = [str(k) for k in self.dataset[self.split].keys()]\n\n        example_name = self.dataset_keys[idx]\n        example = self.dataset[self.split][example_name]\n\n        # pdb.set_trace()\n\n        right_image = bytes(np.array(example['img']))\n        right_embed = np.array(example['embeddings'], dtype=float)\n        wrong_image = bytes(np.array(self.find_wrong_image(example['class'])))\n        inter_embed = np.array(self.find_inter_embed())\n\n        right_image = Image.open(io.BytesIO(right_image)).resize((64, 64))\n        wrong_image = Image.open(io.BytesIO(wrong_image)).resize((64, 64))\n\n        right_image = self.validate_image(right_image)\n        wrong_image = self.validate_image(wrong_image)\n\n        txt = np.array(example['txt']).astype(str)\n\n        sample = {\n                'right_images': torch.FloatTensor(right_image),\n                'right_embed': torch.FloatTensor(right_embed),\n                'wrong_images': torch.FloatTensor(wrong_image),\n                'inter_embed': torch.FloatTensor(inter_embed),\n                'txt': str(txt)\n                 }\n\n        sample['right_images'] = sample['right_images'].sub_(127.5).div_(127.5)\n        sample['wrong_images'] =sample['wrong_images'].sub_(127.5).div_(127.5)\n\n        return sample\n\n    def find_wrong_image(self, category):\n        idx = np.random.randint(len(self.dataset_keys))\n        example_name = self.dataset_keys[idx]\n        example = self.dataset[self.split][example_name]\n        _category = example['class']\n\n        if _category != category:\n            return example['img']\n\n        return self.find_wrong_image(category)\n\n    def find_inter_embed(self):\n        idx = np.random.randint(len(self.dataset_keys))\n        example_name = self.dataset_keys[idx]\n        example = self.dataset[self.split][example_name]\n        return example['embeddings']\n\n\n    def validate_image(self, img):\n        img = np.array(img, dtype=float)\n        if len(img.shape) < 3:\n            rgb = np.empty((64, 64, 3), dtype=np.float32)\n            rgb[:, :, 0] = img\n            rgb[:, :, 1] = img\n            rgb[:, :, 2] = img\n            img = rgb\n\n        return img.transpose(2, 0, 1)\n\n"""
utils.py,9,"b'import numpy as np\nfrom torch import nn\nfrom torch import  autograd\nimport torch\nfrom visualize import VisdomPlotter\nimport os\nimport pdb\n\nclass Concat_embed(nn.Module):\n\n    def __init__(self, embed_dim, projected_embed_dim):\n        super(Concat_embed, self).__init__()\n        self.projection = nn.Sequential(\n            nn.Linear(in_features=embed_dim, out_features=projected_embed_dim),\n            nn.BatchNorm1d(num_features=projected_embed_dim),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True)\n            )\n\n    def forward(self, inp, embed):\n        projected_embed = self.projection(embed)\n        replicated_embed = projected_embed.repeat(4, 4, 1, 1).permute(2,  3, 0, 1)\n        hidden_concat = torch.cat([inp, replicated_embed], 1)\n\n        return hidden_concat\n\n\nclass minibatch_discriminator(nn.Module):\n    def __init__(self, num_channels, B_dim, C_dim):\n        super(minibatch_discriminator, self).__init__()\n        self.B_dim = B_dim\n        self.C_dim =C_dim\n        self.num_channels = num_channels\n        T_init = torch.randn(num_channels * 4 * 4, B_dim * C_dim) * 0.1\n        self.T_tensor = nn.Parameter(T_init, requires_grad=True)\n\n    def forward(self, inp):\n        inp = inp.view(-1, self.num_channels * 4 * 4)\n        M = inp.mm(self.T_tensor)\n        M = M.view(-1, self.B_dim, self.C_dim)\n\n        op1 = M.unsqueeze(3)\n        op2 = M.permute(1, 2, 0).unsqueeze(0)\n\n        output = torch.sum(torch.abs(op1 - op2), 2)\n        output = torch.sum(torch.exp(-output), 2)\n        output = output.view(M.size(0), -1)\n\n        output = torch.cat((inp, output), 1)\n\n        return output\n\n\nclass Utils(object):\n\n    @staticmethod\n    def smooth_label(tensor, offset):\n        return tensor + offset\n\n    @staticmethod\n\n    # based on:  https://github.com/caogang/wgan-gp/blob/master/gan_cifar10.py\n    def compute_GP(netD, real_data, real_embed, fake_data, LAMBDA):\n        BATCH_SIZE = real_data.size(0)\n        alpha = torch.rand(BATCH_SIZE, 1)\n        alpha = alpha.expand(BATCH_SIZE, int(real_data.nelement() / BATCH_SIZE)).contiguous().view(BATCH_SIZE, 3, 64, 64)\n        alpha = alpha.cuda()\n\n        interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n\n        interpolates = interpolates.cuda()\n\n        interpolates = autograd.Variable(interpolates, requires_grad=True)\n\n        disc_interpolates, _ = netD(interpolates, real_embed)\n\n        gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n                                  grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n                                  create_graph=True, retain_graph=True, only_inputs=True)[0]\n\n        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n\n        return gradient_penalty\n\n    @staticmethod\n    def save_checkpoint(netD, netG, dir_path, subdir_path, epoch):\n        path =  os.path.join(dir_path, subdir_path)\n        if not os.path.exists(path):\n            os.makedirs(path)\n\n        torch.save(netD.state_dict(), \'{0}/disc_{1}.pth\'.format(path, epoch))\n        torch.save(netG.state_dict(), \'{0}/gen_{1}.pth\'.format(path, epoch))\n\n    @staticmethod\n    def weights_init(m):\n        classname = m.__class__.__name__\n        if classname.find(\'Conv\') != -1:\n            m.weight.data.normal_(0.0, 0.02)\n        elif classname.find(\'BatchNorm\') != -1:\n            m.weight.data.normal_(1.0, 0.02)\n            m.bias.data.fill_(0)\n\n\nclass Logger(object):\n    def __init__(self, vis_screen):\n        self.viz = VisdomPlotter(env_name=vis_screen)\n        self.hist_D = []\n        self.hist_G = []\n        self.hist_Dx = []\n        self.hist_DGx = []\n\n    def log_iteration_wgan(self, epoch, gen_iteration, d_loss, g_loss, real_loss, fake_loss):\n        print(""Epoch: %d, Gen_iteration: %d, d_loss= %f, g_loss= %f, real_loss= %f, fake_loss = %f"" %\n              (epoch, gen_iteration, d_loss.data.cpu().mean(), g_loss.data.cpu().mean(), real_loss, fake_loss))\n        self.hist_D.append(d_loss.data.cpu().mean())\n        self.hist_G.append(g_loss.data.cpu().mean())\n\n    def log_iteration_gan(self, epoch, d_loss, g_loss, real_score, fake_score):\n        print(""Epoch: %d, d_loss= %f, g_loss= %f, D(X)= %f, D(G(X))= %f"" % (\n            epoch, d_loss.data.cpu().mean(), g_loss.data.cpu().mean(), real_score.data.cpu().mean(),\n            fake_score.data.cpu().mean()))\n        self.hist_D.append(d_loss.data.cpu().mean())\n        self.hist_G.append(g_loss.data.cpu().mean())\n        self.hist_Dx.append(real_score.data.cpu().mean())\n        self.hist_DGx.append(fake_score.data.cpu().mean())\n\n    def plot_epoch(self, epoch):\n        self.viz.plot(\'Discriminator\', \'train\', epoch, np.array(self.hist_D).mean())\n        self.viz.plot(\'Generator\', \'train\', epoch, np.array(self.hist_G).mean())\n        self.hist_D = []\n        self.hist_G = []\n\n    def plot_epoch_w_scores(self, epoch):\n        self.viz.plot(\'Discriminator\', \'train\', epoch, np.array(self.hist_D).mean())\n        self.viz.plot(\'Generator\', \'train\', epoch, np.array(self.hist_G).mean())\n        self.viz.plot(\'D(X)\', \'train\', epoch, np.array(self.hist_Dx).mean())\n        self.viz.plot(\'D(G(X))\', \'train\', epoch, np.array(self.hist_DGx).mean())\n        self.hist_D = []\n        self.hist_G = []\n        self.hist_Dx = []\n        self.hist_DGx = []\n\n    def draw(self, right_images, fake_images):\n        self.viz.draw(\'generated images\', fake_images.data.cpu().numpy()[:64] * 128 + 128)\n        self.viz.draw(\'real images\', right_images.data.cpu().numpy()[:64] * 128 + 128)\n'"
visualize.py,0,"b'from visdom import Visdom\nimport numpy as np\nimport torchvision\nfrom PIL import ImageDraw, Image, ImageFont\nimport torch\nimport pdb\n\nclass VisdomPlotter(object):\n\n    """"""Plots to Visdom""""""\n\n    def __init__(self, env_name=\'gan\'):\n        self.viz = Visdom()\n        self.env = env_name\n        self.plots = {}\n\n    def plot(self, var_name, split_name, x, y, xlabel=\'epoch\'):\n        if var_name not in self.plots:\n            self.plots[var_name] = self.viz.line(X=np.array([x,x]), Y=np.array([y,y]), env=self.env, opts=dict(\n                legend=[split_name],\n                title=var_name,\n                xlabel=xlabel,\n                ylabel=var_name\n            ))\n        else:\n            self.viz.updateTrace(X=np.array([x]), Y=np.array([y]), env=self.env, win=self.plots[var_name], name=split_name)\n\n    def draw(self, var_name, images):\n        if var_name not in self.plots:\n            self.plots[var_name] = self.viz.images(images, env=self.env)\n        else:\n            self.viz.images(images, env=self.env, win=self.plots[var_name])'"
models/gan.py,2,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\n\nclass generator(nn.Module):\n\tdef __init__(self):\n\t\tsuper(generator, self).__init__()\n\t\tself.image_size = 64\n\t\tself.num_channels = 3\n\t\tself.noise_dim = 100\n\t\tself.ngf = 64\n\n\t\t# based on: https://github.com/pytorch/examples/blob/master/dcgan/main.py\n\t\tself.netG = nn.Sequential(\n\t\t\tnn.ConvTranspose2d(self.noise_dim, self.ngf * 8, 4, 1, 0, bias=False),\n\t\t\tnn.BatchNorm2d(self.ngf * 8),\n\t\t\tnn.ReLU(True),\n\t\t\t# state size. (ngf*8) x 4 x 4\n\t\t\tnn.ConvTranspose2d(self.ngf * 8, self.ngf * 4, 4, 2, 1, bias=False),\n\t\t\tnn.BatchNorm2d(self.ngf * 4),\n\t\t\tnn.ReLU(True),\n\t\t\t# state size. (ngf*4) x 8 x 8\n\t\t\tnn.ConvTranspose2d(self.ngf * 4, self.ngf * 2, 4, 2, 1, bias=False),\n\t\t\tnn.BatchNorm2d(self.ngf * 2),\n\t\t\tnn.ReLU(True),\n\t\t\t# state size. (ngf*2) x 16 x 16\n\t\t\tnn.ConvTranspose2d(self.ngf * 2,self.ngf, 4, 2, 1, bias=False),\n\t\t\tnn.BatchNorm2d(self.ngf),\n\t\t\tnn.ReLU(True),\n\t\t\t# state size. (ngf) x 32 x 32\n\t\t\tnn.ConvTranspose2d(self.ngf, self.num_channels, 4, 2, 1, bias=False),\n\t\t\tnn.Tanh()\n\t\t\t # state size. (num_channels) x 64 x 64\n\t\t\t)\n\n\tdef forward(self, z):\n\t\treturn self.netG(z)\n\n\nclass discriminator(nn.Module):\n\tdef __init__(self):\n\t\tsuper(discriminator, self).__init__()\n\t\tself.image_size = 64\n\t\tself.num_channels = 3\n\t\tself.ndf = 64\n\n\t\tself.B_dim = 128\n\t\tself.C_dim = 16\n\n\t\tself.netD_1 = nn.Sequential(\n\t\t\t# input is (nc) x 64 x 64\n\t\t\tnn.Conv2d(self.num_channels, self.ndf, 4, 2, 1, bias=False),\n\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t# state size. (ndf) x 32 x 32\n\t\t\tnn.Conv2d(self.ndf, self.ndf * 2, 4, 2, 1, bias=False),\n\t\t\tnn.BatchNorm2d(self.ndf * 2),\n\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t# state size. (ndf*2) x 16 x 16\n\t\t\tnn.Conv2d(self.ndf * 2, self.ndf * 4, 4, 2, 1, bias=False),\n\t\t\tnn.BatchNorm2d(self.ndf * 4),\n\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t# state size. (ndf*4) x 8 x 8\n\t\t\tnn.Conv2d(self.ndf * 4, self.ndf * 8, 4, 2, 1, bias=False),\n\t\t\tnn.BatchNorm2d(self.ndf * 8),\n\t\t\tnn.LeakyReLU(0.2, inplace=True)\n\t\t)\n\n\t\tself.netD_2 = nn.Sequential(\n\t\t\t# state size. (ndf*8) x 4 x 4\n\t\t\tnn.Conv2d(self.ndf * 8, 1, 4, 1, 0, bias=False),\n\t\t\tnn.Sigmoid()\n\t\t\t)\n\n\tdef forward(self, inp):\n\t\tx_intermediate = self.netD_1(inp)\n\t\toutput =  self.netD_2(x_intermediate)\n\t\treturn output.view(-1, 1).squeeze(1), x_intermediate\n'"
models/gan_cls.py,3,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\nfrom utils import Concat_embed\nimport pdb\n\nclass generator(nn.Module):\n\tdef __init__(self):\n\t\tsuper(generator, self).__init__()\n\t\tself.image_size = 64\n\t\tself.num_channels = 3\n\t\tself.noise_dim = 100\n\t\tself.embed_dim = 1024\n\t\tself.projected_embed_dim = 128\n\t\tself.latent_dim = self.noise_dim + self.projected_embed_dim\n\t\tself.ngf = 64\n\n\t\tself.projection = nn.Sequential(\n\t\t\tnn.Linear(in_features=self.embed_dim, out_features=self.projected_embed_dim),\n\t\t\tnn.BatchNorm1d(num_features=self.projected_embed_dim),\n\t\t\tnn.LeakyReLU(negative_slope=0.2, inplace=True)\n\t\t\t)\n\n\t\t# based on: https://github.com/pytorch/examples/blob/master/dcgan/main.py\n\t\tself.netG = nn.Sequential(\n\t\t\tnn.ConvTranspose2d(self.latent_dim, self.ngf * 8, 4, 1, 0, bias=False),\n\t\t\tnn.BatchNorm2d(self.ngf * 8),\n\t\t\tnn.ReLU(True),\n\t\t\t# state size. (ngf*8) x 4 x 4\n\t\t\tnn.ConvTranspose2d(self.ngf * 8, self.ngf * 4, 4, 2, 1, bias=False),\n\t\t\tnn.BatchNorm2d(self.ngf * 4),\n\t\t\tnn.ReLU(True),\n\t\t\t# state size. (ngf*4) x 8 x 8\n\t\t\tnn.ConvTranspose2d(self.ngf * 4, self.ngf * 2, 4, 2, 1, bias=False),\n\t\t\tnn.BatchNorm2d(self.ngf * 2),\n\t\t\tnn.ReLU(True),\n\t\t\t# state size. (ngf*2) x 16 x 16\n\t\t\tnn.ConvTranspose2d(self.ngf * 2,self.ngf, 4, 2, 1, bias=False),\n\t\t\tnn.BatchNorm2d(self.ngf),\n\t\t\tnn.ReLU(True),\n\t\t\t# state size. (ngf) x 32 x 32\n\t\t\tnn.ConvTranspose2d(self.ngf, self.num_channels, 4, 2, 1, bias=False),\n\t\t\tnn.Tanh()\n\t\t\t # state size. (num_channels) x 64 x 64\n\t\t\t)\n\n\n\tdef forward(self, embed_vector, z):\n\n\t\tprojected_embed = self.projection(embed_vector).unsqueeze(2).unsqueeze(3)\n\t\tlatent_vector = torch.cat([projected_embed, z], 1)\n\t\toutput = self.netG(latent_vector)\n\n\t\treturn output\n\nclass discriminator(nn.Module):\n\tdef __init__(self):\n\t\tsuper(discriminator, self).__init__()\n\t\tself.image_size = 64\n\t\tself.num_channels = 3\n\t\tself.embed_dim = 1024\n\t\tself.projected_embed_dim = 128\n\t\tself.ndf = 64\n\t\tself.B_dim = 128\n\t\tself.C_dim = 16\n\n\t\tself.netD_1 = nn.Sequential(\n\t\t\t# input is (nc) x 64 x 64\n\t\t\tnn.Conv2d(self.num_channels, self.ndf, 4, 2, 1, bias=False),\n\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t# state size. (ndf) x 32 x 32\n\t\t\tnn.Conv2d(self.ndf, self.ndf * 2, 4, 2, 1, bias=False),\n\t\t\tnn.BatchNorm2d(self.ndf * 2),\n\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t# state size. (ndf*2) x 16 x 16\n\t\t\tnn.Conv2d(self.ndf * 2, self.ndf * 4, 4, 2, 1, bias=False),\n\t\t\tnn.BatchNorm2d(self.ndf * 4),\n\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t# state size. (ndf*4) x 8 x 8\n\t\t\tnn.Conv2d(self.ndf * 4, self.ndf * 8, 4, 2, 1, bias=False),\n\t\t\tnn.BatchNorm2d(self.ndf * 8),\n\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t)\n\n\t\tself.projector = Concat_embed(self.embed_dim, self.projected_embed_dim)\n\n\t\tself.netD_2 = nn.Sequential(\n\t\t\t# state size. (ndf*8) x 4 x 4\n\t\t\tnn.Conv2d(self.ndf * 8 + self.projected_embed_dim, 1, 4, 1, 0, bias=False),\n\t\t\tnn.Sigmoid()\n\t\t\t)\t\n\n\tdef forward(self, inp, embed):\n\t\tx_intermediate = self.netD_1(inp)\n\t\tx = self.projector(x_intermediate, embed)\n\t\tx = self.netD_2(x)\n\n\t\treturn x.view(-1, 1).squeeze(1) , x_intermediate'"
models/gan_factory.py,0,"b""from models import gan, gan_cls, wgan_cls, wgan\n\nclass gan_factory(object):\n\n    @staticmethod\n    def generator_factory(type):\n        if type == 'gan':\n            return gan_cls.generator()\n        elif type == 'wgan':\n            return wgan_cls.generator()\n        elif type == 'vanilla_gan':\n            return gan.generator()\n        elif type == 'vanilla_wgan':\n            return wgan.generator()\n\n    @staticmethod\n    def discriminator_factory(type):\n        if type == 'gan':\n            return gan_cls.discriminator()\n        elif type == 'wgan':\n            return wgan_cls.discriminator()\n        elif type == 'vanilla_gan':\n            return gan.discriminator()\n        elif type == 'vanilla_wgan':\n            return wgan.discriminator()\n"""
models/wgan.py,2,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\nfrom utils import Concat_embed, minibatch_discriminator\nimport pdb\n\nclass generator(nn.Module):\n\tdef __init__(self):\n\t\tsuper(generator, self).__init__()\n\t\tself.image_size = 64\n\t\tself.num_channels = 3\n\t\tself.noise_dim = 100\n\t\tself.ngf = 64\n\n\t\t# based on: https://github.com/pytorch/examples/blob/master/dcgan/main.py\n\t\tself.netG = nn.Sequential(\n\t\t\tnn.ConvTranspose2d(self.noise_dim, self.ngf * 8, 4, 1, 0, bias=False),\n\t\t\tnn.BatchNorm2d(self.ngf * 8),\n\t\t\tnn.ReLU(True),\n\t\t\t# state size. (ngf*8) x 4 x 4\n\t\t\tnn.ConvTranspose2d(self.ngf * 8, self.ngf * 4, 4, 2, 1, bias=False),\n\t\t\tnn.BatchNorm2d(self.ngf * 4),\n\t\t\tnn.ReLU(True),\n\t\t\t# state size. (ngf*4) x 8 x 8\n\t\t\tnn.ConvTranspose2d(self.ngf * 4, self.ngf * 2, 4, 2, 1, bias=False),\n\t\t\tnn.BatchNorm2d(self.ngf * 2),\n\t\t\tnn.ReLU(True),\n\t\t\t# state size. (ngf*2) x 16 x 16\n\t\t\tnn.ConvTranspose2d(self.ngf * 2,self.ngf, 4, 2, 1, bias=False),\n\t\t\tnn.BatchNorm2d(self.ngf),\n\t\t\tnn.ReLU(True),\n\t\t\t# state size. (ngf) x 32 x 32\n\t\t\tnn.ConvTranspose2d(self.ngf, self.num_channels, 4, 2, 1, bias=False),\n\t\t\tnn.Tanh()\n\t\t\t # state size. (num_channels) x 64 x 64\n\t\t\t)\n\n\tdef forward(self, z):\n\n\t\toutput = self.netG(z)\n\t\treturn output\n\nclass discriminator(nn.Module):\n\tdef __init__(self, improved = False):\n\t\tsuper(discriminator, self).__init__()\n\t\tself.image_size = 64\n\t\tself.num_channels = 3\n\t\tself.ndf = 64\n\n\t\tif improved:\n\t\t\tself.netD_1 = nn.Sequential(\n\t\t\t\t# input is (nc) x 64 x 64\n\t\t\t\tnn.Conv2d(self.num_channels, self.ndf, 4, 2, 1, bias=False),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\t# state size. (ndf) x 32 x 32\n\t\t\t\tnn.Conv2d(self.ndf, self.ndf * 2, 4, 2, 1, bias=False),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\t# state size. (ndf*2) x 16 x 16\n\t\t\t\tnn.Conv2d(self.ndf * 2, self.ndf * 4, 4, 2, 1, bias=False),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\t# state size. (ndf*4) x 8 x 8\n\t\t\t\tnn.Conv2d(self.ndf * 4, self.ndf * 8, 4, 2, 1, bias=False),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\t# state size. (ndf*8) x 4 x 4\n\t\t\t)\n\t\telse:\n\t\t\tself.netD_1 = nn.Sequential(\n\t\t\t\t# input is (nc) x 64 x 64\n\t\t\t\tnn.Conv2d(self.num_channels, self.ndf, 4, 2, 1, bias=False),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\t# state size. (ndf) x 32 x 32\n\t\t\t\tnn.Conv2d(self.ndf, self.ndf * 2, 4, 2, 1, bias=False),\n\t\t\t\tnn.BatchNorm2d(self.ndf * 2),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\t# state size. (ndf*2) x 16 x 16\n\t\t\t\tnn.Conv2d(self.ndf * 2, self.ndf * 4, 4, 2, 1, bias=False),\n\t\t\t\tnn.BatchNorm2d(self.ndf * 4),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\t# state size. (ndf*4) x 8 x 8\n\t\t\t\tnn.Conv2d(self.ndf * 4, self.ndf * 8, 4, 2, 1, bias=False),\n\t\t\t\tnn.BatchNorm2d(self.ndf * 8),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\t# state size. (ndf*8) x 4 x 4\n\t\t\t)\n\n\n\t\tself.netD_2 = nn.Sequential(\n\t\t\t# nn.Conv2d(self.ndf * 8, 1, 4, 1, 0, bias=False),\n\t\t\tnn.Conv2d(self.ndf * 8, 1, 4, 1, 0, bias=False)\n\t\t\t)\n\n\tdef forward(self, inp):\n\t\tx_intermediate = self.netD_1(inp)\n\t\tx = self.netD_2(x_intermediate)\n\t\tx = x.mean(0)\n\n\t\treturn x.view(1) , x_intermediate'"
models/wgan_cls.py,3,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\nfrom utils import Concat_embed, minibatch_discriminator\nimport pdb\n\nclass generator(nn.Module):\n\tdef __init__(self):\n\t\tsuper(generator, self).__init__()\n\t\tself.image_size = 64\n\t\tself.num_channels = 3\n\t\tself.noise_dim = 100\n\t\tself.embed_dim = 1024\n\t\tself.projected_embed_dim = 128\n\t\tself.latent_dim = self.noise_dim + self.projected_embed_dim\n\t\tself.ngf = 64\n\n\t\tself.projection = nn.Sequential(\n\t\t\tnn.Linear(in_features=self.embed_dim, out_features=self.projected_embed_dim),\n\t\t\tnn.BatchNorm1d(num_features=self.projected_embed_dim),\n\t\t\tnn.LeakyReLU(negative_slope=0.2, inplace=True)\n\t\t\t)\n\n\t\t# based on: https://github.com/pytorch/examples/blob/master/dcgan/main.py\n\t\tself.netG = nn.Sequential(\n\t\t\tnn.ConvTranspose2d(self.latent_dim, self.ngf * 8, 4, 1, 0, bias=False),\n\t\t\tnn.BatchNorm2d(self.ngf * 8),\n\t\t\tnn.ReLU(True),\n\t\t\t# state size. (ngf*8) x 4 x 4\n\t\t\tnn.ConvTranspose2d(self.ngf * 8, self.ngf * 4, 4, 2, 1, bias=False),\n\t\t\tnn.BatchNorm2d(self.ngf * 4),\n\t\t\tnn.ReLU(True),\n\t\t\t# state size. (ngf*4) x 8 x 8\n\t\t\tnn.ConvTranspose2d(self.ngf * 4, self.ngf * 2, 4, 2, 1, bias=False),\n\t\t\tnn.BatchNorm2d(self.ngf * 2),\n\t\t\tnn.ReLU(True),\n\t\t\t# state size. (ngf*2) x 16 x 16\n\t\t\tnn.ConvTranspose2d(self.ngf * 2,self.ngf, 4, 2, 1, bias=False),\n\t\t\tnn.BatchNorm2d(self.ngf),\n\t\t\tnn.ReLU(True),\n\t\t\t# state size. (ngf) x 32 x 32\n\t\t\tnn.ConvTranspose2d(self.ngf, self.num_channels, 4, 2, 1, bias=False),\n\t\t\tnn.Tanh()\n\t\t\t # state size. (num_channels) x 64 x 64\n\t\t\t)\n\n\tdef forward(self, embed_vector, z):\n\n\t\tprojected_embed = self.projection(embed_vector).unsqueeze(2).unsqueeze(3)\n\t\tlatent_vector = torch.cat([projected_embed, z], 1)\n\t\toutput = self.netG(latent_vector)\n\n\t\treturn output\n\nclass discriminator(nn.Module):\n\tdef __init__(self, improved = False):\n\t\tsuper(discriminator, self).__init__()\n\t\tself.image_size = 64\n\t\tself.num_channels = 3\n\t\tself.embed_dim = 1024\n\t\tself.projected_embed_dim = 128\n\t\tself.ndf = 64\n\n\t\tif improved:\n\t\t\tself.netD_1 = nn.Sequential(\n\t\t\t\t# input is (nc) x 64 x 64\n\t\t\t\tnn.Conv2d(self.num_channels, self.ndf, 4, 2, 1, bias=False),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\t# state size. (ndf) x 32 x 32\n\t\t\t\tnn.Conv2d(self.ndf, self.ndf * 2, 4, 2, 1, bias=False),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\t# state size. (ndf*2) x 16 x 16\n\t\t\t\tnn.Conv2d(self.ndf * 2, self.ndf * 4, 4, 2, 1, bias=False),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\t# state size. (ndf*4) x 8 x 8\n\t\t\t\tnn.Conv2d(self.ndf * 4, self.ndf * 8, 4, 2, 1, bias=False),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\t# state size. (ndf*8) x 4 x 4\n\t\t\t)\n\t\telse:\n\t\t\tself.netD_1 = nn.Sequential(\n\t\t\t\t# input is (nc) x 64 x 64\n\t\t\t\tnn.Conv2d(self.num_channels, self.ndf, 4, 2, 1, bias=False),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\t# state size. (ndf) x 32 x 32\n\t\t\t\tnn.Conv2d(self.ndf, self.ndf * 2, 4, 2, 1, bias=False),\n\t\t\t\tnn.BatchNorm2d(self.ndf * 2),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\t# state size. (ndf*2) x 16 x 16\n\t\t\t\tnn.Conv2d(self.ndf * 2, self.ndf * 4, 4, 2, 1, bias=False),\n\t\t\t\tnn.BatchNorm2d(self.ndf * 4),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\t# state size. (ndf*4) x 8 x 8\n\t\t\t\tnn.Conv2d(self.ndf * 4, self.ndf * 8, 4, 2, 1, bias=False),\n\t\t\t\tnn.BatchNorm2d(self.ndf * 8),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\t# state size. (ndf*8) x 4 x 4\n\t\t\t)\n\n\t\tself.projector = Concat_embed(self.embed_dim, self.projected_embed_dim)\n\n\t\tself.netD_2 = nn.Sequential(\n\t\t\t# nn.Conv2d(self.ndf * 8, 1, 4, 1, 0, bias=False),\n\t\t\tnn.Conv2d(self.ndf * 8 + self.projected_embed_dim, 1, 4, 1, 0, bias=False)\n\t\t\t)\n\n\tdef forward(self, inp, embed):\n\t\tx_intermediate = self.netD_1(inp)\n\t\tx = self.projector(x_intermediate, embed)\n\t\tx = self.netD_2(x)\n\t\tx = x.mean(0)\n\n\t\treturn x.view(1) , x_intermediate'"
