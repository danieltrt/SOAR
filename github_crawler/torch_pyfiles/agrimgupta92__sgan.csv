file_path,api_count,code
scripts/evaluate_model.py,5,"b""import argparse\nimport os\nimport torch\n\nfrom attrdict import AttrDict\n\nfrom sgan.data.loader import data_loader\nfrom sgan.models import TrajectoryGenerator\nfrom sgan.losses import displacement_error, final_displacement_error\nfrom sgan.utils import relative_to_abs, get_dset_path\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--model_path', type=str)\nparser.add_argument('--num_samples', default=20, type=int)\nparser.add_argument('--dset_type', default='test', type=str)\n\n\ndef get_generator(checkpoint):\n    args = AttrDict(checkpoint['args'])\n    generator = TrajectoryGenerator(\n        obs_len=args.obs_len,\n        pred_len=args.pred_len,\n        embedding_dim=args.embedding_dim,\n        encoder_h_dim=args.encoder_h_dim_g,\n        decoder_h_dim=args.decoder_h_dim_g,\n        mlp_dim=args.mlp_dim,\n        num_layers=args.num_layers,\n        noise_dim=args.noise_dim,\n        noise_type=args.noise_type,\n        noise_mix_type=args.noise_mix_type,\n        pooling_type=args.pooling_type,\n        pool_every_timestep=args.pool_every_timestep,\n        dropout=args.dropout,\n        bottleneck_dim=args.bottleneck_dim,\n        neighborhood_size=args.neighborhood_size,\n        grid_size=args.grid_size,\n        batch_norm=args.batch_norm)\n    generator.load_state_dict(checkpoint['g_state'])\n    generator.cuda()\n    generator.train()\n    return generator\n\n\ndef evaluate_helper(error, seq_start_end):\n    sum_ = 0\n    error = torch.stack(error, dim=1)\n\n    for (start, end) in seq_start_end:\n        start = start.item()\n        end = end.item()\n        _error = error[start:end]\n        _error = torch.sum(_error, dim=0)\n        _error = torch.min(_error)\n        sum_ += _error\n    return sum_\n\n\ndef evaluate(args, loader, generator, num_samples):\n    ade_outer, fde_outer = [], []\n    total_traj = 0\n    with torch.no_grad():\n        for batch in loader:\n            batch = [tensor.cuda() for tensor in batch]\n            (obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel,\n             non_linear_ped, loss_mask, seq_start_end) = batch\n\n            ade, fde = [], []\n            total_traj += pred_traj_gt.size(1)\n\n            for _ in range(num_samples):\n                pred_traj_fake_rel = generator(\n                    obs_traj, obs_traj_rel, seq_start_end\n                )\n                pred_traj_fake = relative_to_abs(\n                    pred_traj_fake_rel, obs_traj[-1]\n                )\n                ade.append(displacement_error(\n                    pred_traj_fake, pred_traj_gt, mode='raw'\n                ))\n                fde.append(final_displacement_error(\n                    pred_traj_fake[-1], pred_traj_gt[-1], mode='raw'\n                ))\n\n            ade_sum = evaluate_helper(ade, seq_start_end)\n            fde_sum = evaluate_helper(fde, seq_start_end)\n\n            ade_outer.append(ade_sum)\n            fde_outer.append(fde_sum)\n        ade = sum(ade_outer) / (total_traj * args.pred_len)\n        fde = sum(fde_outer) / (total_traj)\n        return ade, fde\n\n\ndef main(args):\n    if os.path.isdir(args.model_path):\n        filenames = os.listdir(args.model_path)\n        filenames.sort()\n        paths = [\n            os.path.join(args.model_path, file_) for file_ in filenames\n        ]\n    else:\n        paths = [args.model_path]\n\n    for path in paths:\n        checkpoint = torch.load(path)\n        generator = get_generator(checkpoint)\n        _args = AttrDict(checkpoint['args'])\n        path = get_dset_path(_args.dataset_name, args.dset_type)\n        _, loader = data_loader(_args, path)\n        ade, fde = evaluate(_args, loader, generator, args.num_samples)\n        print('Dataset: {}, Pred Len: {}, ADE: {:.2f}, FDE: {:.2f}'.format(\n            _args.dataset_name, _args.pred_len, ade, fde))\n\n\nif __name__ == '__main__':\n    args = parser.parse_args()\n    main(args)\n"""
scripts/print_args.py,1,"b'import argparse\nimport torch\n\n""""""\nTiny utility to print the command-line args used for a checkpoint\n""""""\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--checkpoint\')\n\n\ndef main(args):\n\tcheckpoint = torch.load(args.checkpoint, map_location=\'cpu\')\n\tfor k, v in checkpoint[\'args\'].items():\n\t\tprint(k, v)\n\n\nif __name__ == \'__main__\':\n\targs = parser.parse_args()\n\tmain(args)\n'"
scripts/train.py,32,"b'import argparse\nimport gc\nimport logging\nimport os\nimport sys\nimport time\n\nfrom collections import defaultdict\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom sgan.data.loader import data_loader\nfrom sgan.losses import gan_g_loss, gan_d_loss, l2_loss\nfrom sgan.losses import displacement_error, final_displacement_error\n\nfrom sgan.models import TrajectoryGenerator, TrajectoryDiscriminator\nfrom sgan.utils import int_tuple, bool_flag, get_total_norm\nfrom sgan.utils import relative_to_abs, get_dset_path\n\ntorch.backends.cudnn.benchmark = True\n\nparser = argparse.ArgumentParser()\nFORMAT = \'[%(levelname)s: %(filename)s: %(lineno)4d]: %(message)s\'\nlogging.basicConfig(level=logging.INFO, format=FORMAT, stream=sys.stdout)\nlogger = logging.getLogger(__name__)\n\n# Dataset options\nparser.add_argument(\'--dataset_name\', default=\'zara1\', type=str)\nparser.add_argument(\'--delim\', default=\' \')\nparser.add_argument(\'--loader_num_workers\', default=4, type=int)\nparser.add_argument(\'--obs_len\', default=8, type=int)\nparser.add_argument(\'--pred_len\', default=8, type=int)\nparser.add_argument(\'--skip\', default=1, type=int)\n\n# Optimization\nparser.add_argument(\'--batch_size\', default=64, type=int)\nparser.add_argument(\'--num_iterations\', default=10000, type=int)\nparser.add_argument(\'--num_epochs\', default=200, type=int)\n\n# Model Options\nparser.add_argument(\'--embedding_dim\', default=64, type=int)\nparser.add_argument(\'--num_layers\', default=1, type=int)\nparser.add_argument(\'--dropout\', default=0, type=float)\nparser.add_argument(\'--batch_norm\', default=0, type=bool_flag)\nparser.add_argument(\'--mlp_dim\', default=1024, type=int)\n\n# Generator Options\nparser.add_argument(\'--encoder_h_dim_g\', default=64, type=int)\nparser.add_argument(\'--decoder_h_dim_g\', default=128, type=int)\nparser.add_argument(\'--noise_dim\', default=None, type=int_tuple)\nparser.add_argument(\'--noise_type\', default=\'gaussian\')\nparser.add_argument(\'--noise_mix_type\', default=\'ped\')\nparser.add_argument(\'--clipping_threshold_g\', default=0, type=float)\nparser.add_argument(\'--g_learning_rate\', default=5e-4, type=float)\nparser.add_argument(\'--g_steps\', default=1, type=int)\n\n# Pooling Options\nparser.add_argument(\'--pooling_type\', default=\'pool_net\')\nparser.add_argument(\'--pool_every_timestep\', default=1, type=bool_flag)\n\n# Pool Net Option\nparser.add_argument(\'--bottleneck_dim\', default=1024, type=int)\n\n# Social Pooling Options\nparser.add_argument(\'--neighborhood_size\', default=2.0, type=float)\nparser.add_argument(\'--grid_size\', default=8, type=int)\n\n# Discriminator Options\nparser.add_argument(\'--d_type\', default=\'local\', type=str)\nparser.add_argument(\'--encoder_h_dim_d\', default=64, type=int)\nparser.add_argument(\'--d_learning_rate\', default=5e-4, type=float)\nparser.add_argument(\'--d_steps\', default=2, type=int)\nparser.add_argument(\'--clipping_threshold_d\', default=0, type=float)\n\n# Loss Options\nparser.add_argument(\'--l2_loss_weight\', default=0, type=float)\nparser.add_argument(\'--best_k\', default=1, type=int)\n\n# Output\nparser.add_argument(\'--output_dir\', default=os.getcwd())\nparser.add_argument(\'--print_every\', default=5, type=int)\nparser.add_argument(\'--checkpoint_every\', default=100, type=int)\nparser.add_argument(\'--checkpoint_name\', default=\'checkpoint\')\nparser.add_argument(\'--checkpoint_start_from\', default=None)\nparser.add_argument(\'--restore_from_checkpoint\', default=1, type=int)\nparser.add_argument(\'--num_samples_check\', default=5000, type=int)\n\n# Misc\nparser.add_argument(\'--use_gpu\', default=1, type=int)\nparser.add_argument(\'--timing\', default=0, type=int)\nparser.add_argument(\'--gpu_num\', default=""0"", type=str)\n\n\ndef init_weights(m):\n    classname = m.__class__.__name__\n    if classname.find(\'Linear\') != -1:\n        nn.init.kaiming_normal_(m.weight)\n\n\ndef get_dtypes(args):\n    long_dtype = torch.LongTensor\n    float_dtype = torch.FloatTensor\n    if args.use_gpu == 1:\n        long_dtype = torch.cuda.LongTensor\n        float_dtype = torch.cuda.FloatTensor\n    return long_dtype, float_dtype\n\n\ndef main(args):\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.gpu_num\n    train_path = get_dset_path(args.dataset_name, \'train\')\n    val_path = get_dset_path(args.dataset_name, \'val\')\n\n    long_dtype, float_dtype = get_dtypes(args)\n\n    logger.info(""Initializing train dataset"")\n    train_dset, train_loader = data_loader(args, train_path)\n    logger.info(""Initializing val dataset"")\n    _, val_loader = data_loader(args, val_path)\n\n    iterations_per_epoch = len(train_dset) / args.batch_size / args.d_steps\n    if args.num_epochs:\n        args.num_iterations = int(iterations_per_epoch * args.num_epochs)\n\n    logger.info(\n        \'There are {} iterations per epoch\'.format(iterations_per_epoch)\n    )\n\n    generator = TrajectoryGenerator(\n        obs_len=args.obs_len,\n        pred_len=args.pred_len,\n        embedding_dim=args.embedding_dim,\n        encoder_h_dim=args.encoder_h_dim_g,\n        decoder_h_dim=args.decoder_h_dim_g,\n        mlp_dim=args.mlp_dim,\n        num_layers=args.num_layers,\n        noise_dim=args.noise_dim,\n        noise_type=args.noise_type,\n        noise_mix_type=args.noise_mix_type,\n        pooling_type=args.pooling_type,\n        pool_every_timestep=args.pool_every_timestep,\n        dropout=args.dropout,\n        bottleneck_dim=args.bottleneck_dim,\n        neighborhood_size=args.neighborhood_size,\n        grid_size=args.grid_size,\n        batch_norm=args.batch_norm)\n\n    generator.apply(init_weights)\n    generator.type(float_dtype).train()\n    logger.info(\'Here is the generator:\')\n    logger.info(generator)\n\n    discriminator = TrajectoryDiscriminator(\n        obs_len=args.obs_len,\n        pred_len=args.pred_len,\n        embedding_dim=args.embedding_dim,\n        h_dim=args.encoder_h_dim_d,\n        mlp_dim=args.mlp_dim,\n        num_layers=args.num_layers,\n        dropout=args.dropout,\n        batch_norm=args.batch_norm,\n        d_type=args.d_type)\n\n    discriminator.apply(init_weights)\n    discriminator.type(float_dtype).train()\n    logger.info(\'Here is the discriminator:\')\n    logger.info(discriminator)\n\n    g_loss_fn = gan_g_loss\n    d_loss_fn = gan_d_loss\n\n    optimizer_g = optim.Adam(generator.parameters(), lr=args.g_learning_rate)\n    optimizer_d = optim.Adam(\n        discriminator.parameters(), lr=args.d_learning_rate\n    )\n\n    # Maybe restore from checkpoint\n    restore_path = None\n    if args.checkpoint_start_from is not None:\n        restore_path = args.checkpoint_start_from\n    elif args.restore_from_checkpoint == 1:\n        restore_path = os.path.join(args.output_dir,\n                                    \'%s_with_model.pt\' % args.checkpoint_name)\n\n    if restore_path is not None and os.path.isfile(restore_path):\n        logger.info(\'Restoring from checkpoint {}\'.format(restore_path))\n        checkpoint = torch.load(restore_path)\n        generator.load_state_dict(checkpoint[\'g_state\'])\n        discriminator.load_state_dict(checkpoint[\'d_state\'])\n        optimizer_g.load_state_dict(checkpoint[\'g_optim_state\'])\n        optimizer_d.load_state_dict(checkpoint[\'d_optim_state\'])\n        t = checkpoint[\'counters\'][\'t\']\n        epoch = checkpoint[\'counters\'][\'epoch\']\n        checkpoint[\'restore_ts\'].append(t)\n    else:\n        # Starting from scratch, so initialize checkpoint data structure\n        t, epoch = 0, 0\n        checkpoint = {\n            \'args\': args.__dict__,\n            \'G_losses\': defaultdict(list),\n            \'D_losses\': defaultdict(list),\n            \'losses_ts\': [],\n            \'metrics_val\': defaultdict(list),\n            \'metrics_train\': defaultdict(list),\n            \'sample_ts\': [],\n            \'restore_ts\': [],\n            \'norm_g\': [],\n            \'norm_d\': [],\n            \'counters\': {\n                \'t\': None,\n                \'epoch\': None,\n            },\n            \'g_state\': None,\n            \'g_optim_state\': None,\n            \'d_state\': None,\n            \'d_optim_state\': None,\n            \'g_best_state\': None,\n            \'d_best_state\': None,\n            \'best_t\': None,\n            \'g_best_nl_state\': None,\n            \'d_best_state_nl\': None,\n            \'best_t_nl\': None,\n        }\n    t0 = None\n    while t < args.num_iterations:\n        gc.collect()\n        d_steps_left = args.d_steps\n        g_steps_left = args.g_steps\n        epoch += 1\n        logger.info(\'Starting epoch {}\'.format(epoch))\n        for batch in train_loader:\n            if args.timing == 1:\n                torch.cuda.synchronize()\n                t1 = time.time()\n\n            # Decide whether to use the batch for stepping on discriminator or\n            # generator; an iteration consists of args.d_steps steps on the\n            # discriminator followed by args.g_steps steps on the generator.\n            if d_steps_left > 0:\n                step_type = \'d\'\n                losses_d = discriminator_step(args, batch, generator,\n                                              discriminator, d_loss_fn,\n                                              optimizer_d)\n                checkpoint[\'norm_d\'].append(\n                    get_total_norm(discriminator.parameters()))\n                d_steps_left -= 1\n            elif g_steps_left > 0:\n                step_type = \'g\'\n                losses_g = generator_step(args, batch, generator,\n                                          discriminator, g_loss_fn,\n                                          optimizer_g)\n                checkpoint[\'norm_g\'].append(\n                    get_total_norm(generator.parameters())\n                )\n                g_steps_left -= 1\n\n            if args.timing == 1:\n                torch.cuda.synchronize()\n                t2 = time.time()\n                logger.info(\'{} step took {}\'.format(step_type, t2 - t1))\n\n            # Skip the rest if we are not at the end of an iteration\n            if d_steps_left > 0 or g_steps_left > 0:\n                continue\n\n            if args.timing == 1:\n                if t0 is not None:\n                    logger.info(\'Interation {} took {}\'.format(\n                        t - 1, time.time() - t0\n                    ))\n                t0 = time.time()\n\n            # Maybe save loss\n            if t % args.print_every == 0:\n                logger.info(\'t = {} / {}\'.format(t + 1, args.num_iterations))\n                for k, v in sorted(losses_d.items()):\n                    logger.info(\'  [D] {}: {:.3f}\'.format(k, v))\n                    checkpoint[\'D_losses\'][k].append(v)\n                for k, v in sorted(losses_g.items()):\n                    logger.info(\'  [G] {}: {:.3f}\'.format(k, v))\n                    checkpoint[\'G_losses\'][k].append(v)\n                checkpoint[\'losses_ts\'].append(t)\n\n            # Maybe save a checkpoint\n            if t > 0 and t % args.checkpoint_every == 0:\n                checkpoint[\'counters\'][\'t\'] = t\n                checkpoint[\'counters\'][\'epoch\'] = epoch\n                checkpoint[\'sample_ts\'].append(t)\n\n                # Check stats on the validation set\n                logger.info(\'Checking stats on val ...\')\n                metrics_val = check_accuracy(\n                    args, val_loader, generator, discriminator, d_loss_fn\n                )\n                logger.info(\'Checking stats on train ...\')\n                metrics_train = check_accuracy(\n                    args, train_loader, generator, discriminator,\n                    d_loss_fn, limit=True\n                )\n\n                for k, v in sorted(metrics_val.items()):\n                    logger.info(\'  [val] {}: {:.3f}\'.format(k, v))\n                    checkpoint[\'metrics_val\'][k].append(v)\n                for k, v in sorted(metrics_train.items()):\n                    logger.info(\'  [train] {}: {:.3f}\'.format(k, v))\n                    checkpoint[\'metrics_train\'][k].append(v)\n\n                min_ade = min(checkpoint[\'metrics_val\'][\'ade\'])\n                min_ade_nl = min(checkpoint[\'metrics_val\'][\'ade_nl\'])\n\n                if metrics_val[\'ade\'] == min_ade:\n                    logger.info(\'New low for avg_disp_error\')\n                    checkpoint[\'best_t\'] = t\n                    checkpoint[\'g_best_state\'] = generator.state_dict()\n                    checkpoint[\'d_best_state\'] = discriminator.state_dict()\n\n                if metrics_val[\'ade_nl\'] == min_ade_nl:\n                    logger.info(\'New low for avg_disp_error_nl\')\n                    checkpoint[\'best_t_nl\'] = t\n                    checkpoint[\'g_best_nl_state\'] = generator.state_dict()\n                    checkpoint[\'d_best_nl_state\'] = discriminator.state_dict()\n\n                # Save another checkpoint with model weights and\n                # optimizer state\n                checkpoint[\'g_state\'] = generator.state_dict()\n                checkpoint[\'g_optim_state\'] = optimizer_g.state_dict()\n                checkpoint[\'d_state\'] = discriminator.state_dict()\n                checkpoint[\'d_optim_state\'] = optimizer_d.state_dict()\n                checkpoint_path = os.path.join(\n                    args.output_dir, \'%s_with_model.pt\' % args.checkpoint_name\n                )\n                logger.info(\'Saving checkpoint to {}\'.format(checkpoint_path))\n                torch.save(checkpoint, checkpoint_path)\n                logger.info(\'Done.\')\n\n                # Save a checkpoint with no model weights by making a shallow\n                # copy of the checkpoint excluding some items\n                checkpoint_path = os.path.join(\n                    args.output_dir, \'%s_no_model.pt\' % args.checkpoint_name)\n                logger.info(\'Saving checkpoint to {}\'.format(checkpoint_path))\n                key_blacklist = [\n                    \'g_state\', \'d_state\', \'g_best_state\', \'g_best_nl_state\',\n                    \'g_optim_state\', \'d_optim_state\', \'d_best_state\',\n                    \'d_best_nl_state\'\n                ]\n                small_checkpoint = {}\n                for k, v in checkpoint.items():\n                    if k not in key_blacklist:\n                        small_checkpoint[k] = v\n                torch.save(small_checkpoint, checkpoint_path)\n                logger.info(\'Done.\')\n\n            t += 1\n            d_steps_left = args.d_steps\n            g_steps_left = args.g_steps\n            if t >= args.num_iterations:\n                break\n\n\ndef discriminator_step(\n    args, batch, generator, discriminator, d_loss_fn, optimizer_d\n):\n    batch = [tensor.cuda() for tensor in batch]\n    (obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, non_linear_ped,\n     loss_mask, seq_start_end) = batch\n    losses = {}\n    loss = torch.zeros(1).to(pred_traj_gt)\n\n    generator_out = generator(obs_traj, obs_traj_rel, seq_start_end)\n\n    pred_traj_fake_rel = generator_out\n    pred_traj_fake = relative_to_abs(pred_traj_fake_rel, obs_traj[-1])\n\n    traj_real = torch.cat([obs_traj, pred_traj_gt], dim=0)\n    traj_real_rel = torch.cat([obs_traj_rel, pred_traj_gt_rel], dim=0)\n    traj_fake = torch.cat([obs_traj, pred_traj_fake], dim=0)\n    traj_fake_rel = torch.cat([obs_traj_rel, pred_traj_fake_rel], dim=0)\n\n    scores_fake = discriminator(traj_fake, traj_fake_rel, seq_start_end)\n    scores_real = discriminator(traj_real, traj_real_rel, seq_start_end)\n\n    # Compute loss with optional gradient penalty\n    data_loss = d_loss_fn(scores_real, scores_fake)\n    losses[\'D_data_loss\'] = data_loss.item()\n    loss += data_loss\n    losses[\'D_total_loss\'] = loss.item()\n\n    optimizer_d.zero_grad()\n    loss.backward()\n    if args.clipping_threshold_d > 0:\n        nn.utils.clip_grad_norm_(discriminator.parameters(),\n                                 args.clipping_threshold_d)\n    optimizer_d.step()\n\n    return losses\n\n\ndef generator_step(\n    args, batch, generator, discriminator, g_loss_fn, optimizer_g\n):\n    batch = [tensor.cuda() for tensor in batch]\n    (obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, non_linear_ped,\n     loss_mask, seq_start_end) = batch\n    losses = {}\n    loss = torch.zeros(1).to(pred_traj_gt)\n    g_l2_loss_rel = []\n\n    loss_mask = loss_mask[:, args.obs_len:]\n\n    for _ in range(args.best_k):\n        generator_out = generator(obs_traj, obs_traj_rel, seq_start_end)\n\n        pred_traj_fake_rel = generator_out\n        pred_traj_fake = relative_to_abs(pred_traj_fake_rel, obs_traj[-1])\n\n        if args.l2_loss_weight > 0:\n            g_l2_loss_rel.append(args.l2_loss_weight * l2_loss(\n                pred_traj_fake_rel,\n                pred_traj_gt_rel,\n                loss_mask,\n                mode=\'raw\'))\n\n    g_l2_loss_sum_rel = torch.zeros(1).to(pred_traj_gt)\n    if args.l2_loss_weight > 0:\n        g_l2_loss_rel = torch.stack(g_l2_loss_rel, dim=1)\n        for start, end in seq_start_end.data:\n            _g_l2_loss_rel = g_l2_loss_rel[start:end]\n            _g_l2_loss_rel = torch.sum(_g_l2_loss_rel, dim=0)\n            _g_l2_loss_rel = torch.min(_g_l2_loss_rel) / torch.sum(\n                loss_mask[start:end])\n            g_l2_loss_sum_rel += _g_l2_loss_rel\n        losses[\'G_l2_loss_rel\'] = g_l2_loss_sum_rel.item()\n        loss += g_l2_loss_sum_rel\n\n    traj_fake = torch.cat([obs_traj, pred_traj_fake], dim=0)\n    traj_fake_rel = torch.cat([obs_traj_rel, pred_traj_fake_rel], dim=0)\n\n    scores_fake = discriminator(traj_fake, traj_fake_rel, seq_start_end)\n    discriminator_loss = g_loss_fn(scores_fake)\n\n    loss += discriminator_loss\n    losses[\'G_discriminator_loss\'] = discriminator_loss.item()\n    losses[\'G_total_loss\'] = loss.item()\n\n    optimizer_g.zero_grad()\n    loss.backward()\n    if args.clipping_threshold_g > 0:\n        nn.utils.clip_grad_norm_(\n            generator.parameters(), args.clipping_threshold_g\n        )\n    optimizer_g.step()\n\n    return losses\n\n\ndef check_accuracy(\n    args, loader, generator, discriminator, d_loss_fn, limit=False\n):\n    d_losses = []\n    metrics = {}\n    g_l2_losses_abs, g_l2_losses_rel = ([],) * 2\n    disp_error, disp_error_l, disp_error_nl = ([],) * 3\n    f_disp_error, f_disp_error_l, f_disp_error_nl = ([],) * 3\n    total_traj, total_traj_l, total_traj_nl = 0, 0, 0\n    loss_mask_sum = 0\n    generator.eval()\n    with torch.no_grad():\n        for batch in loader:\n            batch = [tensor.cuda() for tensor in batch]\n            (obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel,\n             non_linear_ped, loss_mask, seq_start_end) = batch\n            linear_ped = 1 - non_linear_ped\n            loss_mask = loss_mask[:, args.obs_len:]\n\n            pred_traj_fake_rel = generator(\n                obs_traj, obs_traj_rel, seq_start_end\n            )\n            pred_traj_fake = relative_to_abs(pred_traj_fake_rel, obs_traj[-1])\n\n            g_l2_loss_abs, g_l2_loss_rel = cal_l2_losses(\n                pred_traj_gt, pred_traj_gt_rel, pred_traj_fake,\n                pred_traj_fake_rel, loss_mask\n            )\n            ade, ade_l, ade_nl = cal_ade(\n                pred_traj_gt, pred_traj_fake, linear_ped, non_linear_ped\n            )\n\n            fde, fde_l, fde_nl = cal_fde(\n                pred_traj_gt, pred_traj_fake, linear_ped, non_linear_ped\n            )\n\n            traj_real = torch.cat([obs_traj, pred_traj_gt], dim=0)\n            traj_real_rel = torch.cat([obs_traj_rel, pred_traj_gt_rel], dim=0)\n            traj_fake = torch.cat([obs_traj, pred_traj_fake], dim=0)\n            traj_fake_rel = torch.cat([obs_traj_rel, pred_traj_fake_rel], dim=0)\n\n            scores_fake = discriminator(traj_fake, traj_fake_rel, seq_start_end)\n            scores_real = discriminator(traj_real, traj_real_rel, seq_start_end)\n\n            d_loss = d_loss_fn(scores_real, scores_fake)\n            d_losses.append(d_loss.item())\n\n            g_l2_losses_abs.append(g_l2_loss_abs.item())\n            g_l2_losses_rel.append(g_l2_loss_rel.item())\n            disp_error.append(ade.item())\n            disp_error_l.append(ade_l.item())\n            disp_error_nl.append(ade_nl.item())\n            f_disp_error.append(fde.item())\n            f_disp_error_l.append(fde_l.item())\n            f_disp_error_nl.append(fde_nl.item())\n\n            loss_mask_sum += torch.numel(loss_mask.data)\n            total_traj += pred_traj_gt.size(1)\n            total_traj_l += torch.sum(linear_ped).item()\n            total_traj_nl += torch.sum(non_linear_ped).item()\n            if limit and total_traj >= args.num_samples_check:\n                break\n\n    metrics[\'d_loss\'] = sum(d_losses) / len(d_losses)\n    metrics[\'g_l2_loss_abs\'] = sum(g_l2_losses_abs) / loss_mask_sum\n    metrics[\'g_l2_loss_rel\'] = sum(g_l2_losses_rel) / loss_mask_sum\n\n    metrics[\'ade\'] = sum(disp_error) / (total_traj * args.pred_len)\n    metrics[\'fde\'] = sum(f_disp_error) / total_traj\n    if total_traj_l != 0:\n        metrics[\'ade_l\'] = sum(disp_error_l) / (total_traj_l * args.pred_len)\n        metrics[\'fde_l\'] = sum(f_disp_error_l) / total_traj_l\n    else:\n        metrics[\'ade_l\'] = 0\n        metrics[\'fde_l\'] = 0\n    if total_traj_nl != 0:\n        metrics[\'ade_nl\'] = sum(disp_error_nl) / (\n            total_traj_nl * args.pred_len)\n        metrics[\'fde_nl\'] = sum(f_disp_error_nl) / total_traj_nl\n    else:\n        metrics[\'ade_nl\'] = 0\n        metrics[\'fde_nl\'] = 0\n\n    generator.train()\n    return metrics\n\n\ndef cal_l2_losses(\n    pred_traj_gt, pred_traj_gt_rel, pred_traj_fake, pred_traj_fake_rel,\n    loss_mask\n):\n    g_l2_loss_abs = l2_loss(\n        pred_traj_fake, pred_traj_gt, loss_mask, mode=\'sum\'\n    )\n    g_l2_loss_rel = l2_loss(\n        pred_traj_fake_rel, pred_traj_gt_rel, loss_mask, mode=\'sum\'\n    )\n    return g_l2_loss_abs, g_l2_loss_rel\n\n\ndef cal_ade(pred_traj_gt, pred_traj_fake, linear_ped, non_linear_ped):\n    ade = displacement_error(pred_traj_fake, pred_traj_gt)\n    ade_l = displacement_error(pred_traj_fake, pred_traj_gt, linear_ped)\n    ade_nl = displacement_error(pred_traj_fake, pred_traj_gt, non_linear_ped)\n    return ade, ade_l, ade_nl\n\n\ndef cal_fde(\n    pred_traj_gt, pred_traj_fake, linear_ped, non_linear_ped\n):\n    fde = final_displacement_error(pred_traj_fake[-1], pred_traj_gt[-1])\n    fde_l = final_displacement_error(\n        pred_traj_fake[-1], pred_traj_gt[-1], linear_ped\n    )\n    fde_nl = final_displacement_error(\n        pred_traj_fake[-1], pred_traj_gt[-1], non_linear_ped\n    )\n    return fde, fde_l, fde_nl\n\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n    main(args)\n'"
sgan/losses.py,11,"b'import torch\nimport random\n\n\ndef bce_loss(input, target):\n    """"""\n    Numerically stable version of the binary cross-entropy loss function.\n    As per https://github.com/pytorch/pytorch/issues/751\n    See the TensorFlow docs for a derivation of this formula:\n    https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits\n    Input:\n    - input: PyTorch Tensor of shape (N, ) giving scores.\n    - target: PyTorch Tensor of shape (N,) containing 0 and 1 giving targets.\n\n    Output:\n    - A PyTorch Tensor containing the mean BCE loss over the minibatch of\n      input data.\n    """"""\n    neg_abs = -input.abs()\n    loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\n    return loss.mean()\n\n\ndef gan_g_loss(scores_fake):\n    """"""\n    Input:\n    - scores_fake: Tensor of shape (N,) containing scores for fake samples\n\n    Output:\n    - loss: Tensor of shape (,) giving GAN generator loss\n    """"""\n    y_fake = torch.ones_like(scores_fake) * random.uniform(0.7, 1.2)\n    return bce_loss(scores_fake, y_fake)\n\n\ndef gan_d_loss(scores_real, scores_fake):\n    """"""\n    Input:\n    - scores_real: Tensor of shape (N,) giving scores for real samples\n    - scores_fake: Tensor of shape (N,) giving scores for fake samples\n\n    Output:\n    - loss: Tensor of shape (,) giving GAN discriminator loss\n    """"""\n    y_real = torch.ones_like(scores_real) * random.uniform(0.7, 1.2)\n    y_fake = torch.zeros_like(scores_fake) * random.uniform(0, 0.3)\n    loss_real = bce_loss(scores_real, y_real)\n    loss_fake = bce_loss(scores_fake, y_fake)\n    return loss_real + loss_fake\n\n\ndef l2_loss(pred_traj, pred_traj_gt, loss_mask, random=0, mode=\'average\'):\n    """"""\n    Input:\n    - pred_traj: Tensor of shape (seq_len, batch, 2). Predicted trajectory.\n    - pred_traj_gt: Tensor of shape (seq_len, batch, 2). Groud truth\n    predictions.\n    - loss_mask: Tensor of shape (batch, seq_len)\n    - mode: Can be one of sum, average, raw\n    Output:\n    - loss: l2 loss depending on mode\n    """"""\n    seq_len, batch, _ = pred_traj.size()\n    loss = (loss_mask.unsqueeze(dim=2) *\n            (pred_traj_gt.permute(1, 0, 2) - pred_traj.permute(1, 0, 2))**2)\n    if mode == \'sum\':\n        return torch.sum(loss)\n    elif mode == \'average\':\n        return torch.sum(loss) / torch.numel(loss_mask.data)\n    elif mode == \'raw\':\n        return loss.sum(dim=2).sum(dim=1)\n\n\ndef displacement_error(pred_traj, pred_traj_gt, consider_ped=None, mode=\'sum\'):\n    """"""\n    Input:\n    - pred_traj: Tensor of shape (seq_len, batch, 2). Predicted trajectory.\n    - pred_traj_gt: Tensor of shape (seq_len, batch, 2). Ground truth\n    predictions.\n    - consider_ped: Tensor of shape (batch)\n    - mode: Can be one of sum, raw\n    Output:\n    - loss: gives the eculidian displacement error\n    """"""\n    seq_len, _, _ = pred_traj.size()\n    loss = pred_traj_gt.permute(1, 0, 2) - pred_traj.permute(1, 0, 2)\n    loss = loss**2\n    if consider_ped is not None:\n        loss = torch.sqrt(loss.sum(dim=2)).sum(dim=1) * consider_ped\n    else:\n        loss = torch.sqrt(loss.sum(dim=2)).sum(dim=1)\n    if mode == \'sum\':\n        return torch.sum(loss)\n    elif mode == \'raw\':\n        return loss\n\n\ndef final_displacement_error(\n    pred_pos, pred_pos_gt, consider_ped=None, mode=\'sum\'\n):\n    """"""\n    Input:\n    - pred_pos: Tensor of shape (batch, 2). Predicted last pos.\n    - pred_pos_gt: Tensor of shape (seq_len, batch, 2). Groud truth\n    last pos\n    - consider_ped: Tensor of shape (batch)\n    Output:\n    - loss: gives the eculidian displacement error\n    """"""\n    loss = pred_pos_gt - pred_pos\n    loss = loss**2\n    if consider_ped is not None:\n        loss = torch.sqrt(loss.sum(dim=1)) * consider_ped\n    else:\n        loss = torch.sqrt(loss.sum(dim=1))\n    if mode == \'raw\':\n        return loss\n    else:\n        return torch.sum(loss)\n'"
sgan/models.py,22,"b'import torch\nimport torch.nn as nn\n\n\ndef make_mlp(dim_list, activation=\'relu\', batch_norm=True, dropout=0):\n    layers = []\n    for dim_in, dim_out in zip(dim_list[:-1], dim_list[1:]):\n        layers.append(nn.Linear(dim_in, dim_out))\n        if batch_norm:\n            layers.append(nn.BatchNorm1d(dim_out))\n        if activation == \'relu\':\n            layers.append(nn.ReLU())\n        elif activation == \'leakyrelu\':\n            layers.append(nn.LeakyReLU())\n        if dropout > 0:\n            layers.append(nn.Dropout(p=dropout))\n    return nn.Sequential(*layers)\n\n\ndef get_noise(shape, noise_type):\n    if noise_type == \'gaussian\':\n        return torch.randn(*shape).cuda()\n    elif noise_type == \'uniform\':\n        return torch.rand(*shape).sub_(0.5).mul_(2.0).cuda()\n    raise ValueError(\'Unrecognized noise type ""%s""\' % noise_type)\n\n\nclass Encoder(nn.Module):\n    """"""Encoder is part of both TrajectoryGenerator and\n    TrajectoryDiscriminator""""""\n    def __init__(\n        self, embedding_dim=64, h_dim=64, mlp_dim=1024, num_layers=1,\n        dropout=0.0\n    ):\n        super(Encoder, self).__init__()\n\n        self.mlp_dim = 1024\n        self.h_dim = h_dim\n        self.embedding_dim = embedding_dim\n        self.num_layers = num_layers\n\n        self.encoder = nn.LSTM(\n            embedding_dim, h_dim, num_layers, dropout=dropout\n        )\n\n        self.spatial_embedding = nn.Linear(2, embedding_dim)\n\n    def init_hidden(self, batch):\n        return (\n            torch.zeros(self.num_layers, batch, self.h_dim).cuda(),\n            torch.zeros(self.num_layers, batch, self.h_dim).cuda()\n        )\n\n    def forward(self, obs_traj):\n        """"""\n        Inputs:\n        - obs_traj: Tensor of shape (obs_len, batch, 2)\n        Output:\n        - final_h: Tensor of shape (self.num_layers, batch, self.h_dim)\n        """"""\n        # Encode observed Trajectory\n        batch = obs_traj.size(1)\n        obs_traj_embedding = self.spatial_embedding(obs_traj.view(-1, 2))\n        obs_traj_embedding = obs_traj_embedding.view(\n            -1, batch, self.embedding_dim\n        )\n        state_tuple = self.init_hidden(batch)\n        output, state = self.encoder(obs_traj_embedding, state_tuple)\n        final_h = state[0]\n        return final_h\n\n\nclass Decoder(nn.Module):\n    """"""Decoder is part of TrajectoryGenerator""""""\n    def __init__(\n        self, seq_len, embedding_dim=64, h_dim=128, mlp_dim=1024, num_layers=1,\n        pool_every_timestep=True, dropout=0.0, bottleneck_dim=1024,\n        activation=\'relu\', batch_norm=True, pooling_type=\'pool_net\',\n        neighborhood_size=2.0, grid_size=8\n    ):\n        super(Decoder, self).__init__()\n\n        self.seq_len = seq_len\n        self.mlp_dim = mlp_dim\n        self.h_dim = h_dim\n        self.embedding_dim = embedding_dim\n        self.pool_every_timestep = pool_every_timestep\n\n        self.decoder = nn.LSTM(\n            embedding_dim, h_dim, num_layers, dropout=dropout\n        )\n\n        if pool_every_timestep:\n            if pooling_type == \'pool_net\':\n                self.pool_net = PoolHiddenNet(\n                    embedding_dim=self.embedding_dim,\n                    h_dim=self.h_dim,\n                    mlp_dim=mlp_dim,\n                    bottleneck_dim=bottleneck_dim,\n                    activation=activation,\n                    batch_norm=batch_norm,\n                    dropout=dropout\n                )\n            elif pooling_type == \'spool\':\n                self.pool_net = SocialPooling(\n                    h_dim=self.h_dim,\n                    activation=activation,\n                    batch_norm=batch_norm,\n                    dropout=dropout,\n                    neighborhood_size=neighborhood_size,\n                    grid_size=grid_size\n                )\n\n            mlp_dims = [h_dim + bottleneck_dim, mlp_dim, h_dim]\n            self.mlp = make_mlp(\n                mlp_dims,\n                activation=activation,\n                batch_norm=batch_norm,\n                dropout=dropout\n            )\n\n        self.spatial_embedding = nn.Linear(2, embedding_dim)\n        self.hidden2pos = nn.Linear(h_dim, 2)\n\n    def forward(self, last_pos, last_pos_rel, state_tuple, seq_start_end):\n        """"""\n        Inputs:\n        - last_pos: Tensor of shape (batch, 2)\n        - last_pos_rel: Tensor of shape (batch, 2)\n        - state_tuple: (hh, ch) each tensor of shape (num_layers, batch, h_dim)\n        - seq_start_end: A list of tuples which delimit sequences within batch\n        Output:\n        - pred_traj: tensor of shape (self.seq_len, batch, 2)\n        """"""\n        batch = last_pos.size(0)\n        pred_traj_fake_rel = []\n        decoder_input = self.spatial_embedding(last_pos_rel)\n        decoder_input = decoder_input.view(1, batch, self.embedding_dim)\n\n        for _ in range(self.seq_len):\n            output, state_tuple = self.decoder(decoder_input, state_tuple)\n            rel_pos = self.hidden2pos(output.view(-1, self.h_dim))\n            curr_pos = rel_pos + last_pos\n\n            if self.pool_every_timestep:\n                decoder_h = state_tuple[0]\n                pool_h = self.pool_net(decoder_h, seq_start_end, curr_pos)\n                decoder_h = torch.cat(\n                    [decoder_h.view(-1, self.h_dim), pool_h], dim=1)\n                decoder_h = self.mlp(decoder_h)\n                decoder_h = torch.unsqueeze(decoder_h, 0)\n                state_tuple = (decoder_h, state_tuple[1])\n\n            embedding_input = rel_pos\n\n            decoder_input = self.spatial_embedding(embedding_input)\n            decoder_input = decoder_input.view(1, batch, self.embedding_dim)\n            pred_traj_fake_rel.append(rel_pos.view(batch, -1))\n            last_pos = curr_pos\n\n        pred_traj_fake_rel = torch.stack(pred_traj_fake_rel, dim=0)\n        return pred_traj_fake_rel, state_tuple[0]\n\n\nclass PoolHiddenNet(nn.Module):\n    """"""Pooling module as proposed in our paper""""""\n    def __init__(\n        self, embedding_dim=64, h_dim=64, mlp_dim=1024, bottleneck_dim=1024,\n        activation=\'relu\', batch_norm=True, dropout=0.0\n    ):\n        super(PoolHiddenNet, self).__init__()\n\n        self.mlp_dim = 1024\n        self.h_dim = h_dim\n        self.bottleneck_dim = bottleneck_dim\n        self.embedding_dim = embedding_dim\n\n        mlp_pre_dim = embedding_dim + h_dim\n        mlp_pre_pool_dims = [mlp_pre_dim, 512, bottleneck_dim]\n\n        self.spatial_embedding = nn.Linear(2, embedding_dim)\n        self.mlp_pre_pool = make_mlp(\n            mlp_pre_pool_dims,\n            activation=activation,\n            batch_norm=batch_norm,\n            dropout=dropout)\n\n    def repeat(self, tensor, num_reps):\n        """"""\n        Inputs:\n        -tensor: 2D tensor of any shape\n        -num_reps: Number of times to repeat each row\n        Outpus:\n        -repeat_tensor: Repeat each row such that: R1, R1, R2, R2\n        """"""\n        col_len = tensor.size(1)\n        tensor = tensor.unsqueeze(dim=1).repeat(1, num_reps, 1)\n        tensor = tensor.view(-1, col_len)\n        return tensor\n\n    def forward(self, h_states, seq_start_end, end_pos):\n        """"""\n        Inputs:\n        - h_states: Tensor of shape (num_layers, batch, h_dim)\n        - seq_start_end: A list of tuples which delimit sequences within batch\n        - end_pos: Tensor of shape (batch, 2)\n        Output:\n        - pool_h: Tensor of shape (batch, bottleneck_dim)\n        """"""\n        pool_h = []\n        for _, (start, end) in enumerate(seq_start_end):\n            start = start.item()\n            end = end.item()\n            num_ped = end - start\n            curr_hidden = h_states.view(-1, self.h_dim)[start:end]\n            curr_end_pos = end_pos[start:end]\n            # Repeat -> H1, H2, H1, H2\n            curr_hidden_1 = curr_hidden.repeat(num_ped, 1)\n            # Repeat position -> P1, P2, P1, P2\n            curr_end_pos_1 = curr_end_pos.repeat(num_ped, 1)\n            # Repeat position -> P1, P1, P2, P2\n            curr_end_pos_2 = self.repeat(curr_end_pos, num_ped)\n            curr_rel_pos = curr_end_pos_1 - curr_end_pos_2\n            curr_rel_embedding = self.spatial_embedding(curr_rel_pos)\n            mlp_h_input = torch.cat([curr_rel_embedding, curr_hidden_1], dim=1)\n            curr_pool_h = self.mlp_pre_pool(mlp_h_input)\n            curr_pool_h = curr_pool_h.view(num_ped, num_ped, -1).max(1)[0]\n            pool_h.append(curr_pool_h)\n        pool_h = torch.cat(pool_h, dim=0)\n        return pool_h\n\n\nclass SocialPooling(nn.Module):\n    """"""Current state of the art pooling mechanism:\n    http://cvgl.stanford.edu/papers/CVPR16_Social_LSTM.pdf""""""\n    def __init__(\n        self, h_dim=64, activation=\'relu\', batch_norm=True, dropout=0.0,\n        neighborhood_size=2.0, grid_size=8, pool_dim=None\n    ):\n        super(SocialPooling, self).__init__()\n        self.h_dim = h_dim\n        self.grid_size = grid_size\n        self.neighborhood_size = neighborhood_size\n        if pool_dim:\n            mlp_pool_dims = [grid_size * grid_size * h_dim, pool_dim]\n        else:\n            mlp_pool_dims = [grid_size * grid_size * h_dim, h_dim]\n\n        self.mlp_pool = make_mlp(\n            mlp_pool_dims,\n            activation=activation,\n            batch_norm=batch_norm,\n            dropout=dropout\n        )\n\n    def get_bounds(self, ped_pos):\n        top_left_x = ped_pos[:, 0] - self.neighborhood_size / 2\n        top_left_y = ped_pos[:, 1] + self.neighborhood_size / 2\n        bottom_right_x = ped_pos[:, 0] + self.neighborhood_size / 2\n        bottom_right_y = ped_pos[:, 1] - self.neighborhood_size / 2\n        top_left = torch.stack([top_left_x, top_left_y], dim=1)\n        bottom_right = torch.stack([bottom_right_x, bottom_right_y], dim=1)\n        return top_left, bottom_right\n\n    def get_grid_locations(self, top_left, other_pos):\n        cell_x = torch.floor(\n            ((other_pos[:, 0] - top_left[:, 0]) / self.neighborhood_size) *\n            self.grid_size)\n        cell_y = torch.floor(\n            ((top_left[:, 1] - other_pos[:, 1]) / self.neighborhood_size) *\n            self.grid_size)\n        grid_pos = cell_x + cell_y * self.grid_size\n        return grid_pos\n\n    def repeat(self, tensor, num_reps):\n        """"""\n        Inputs:\n        -tensor: 2D tensor of any shape\n        -num_reps: Number of times to repeat each row\n        Outpus:\n        -repeat_tensor: Repeat each row such that: R1, R1, R2, R2\n        """"""\n        col_len = tensor.size(1)\n        tensor = tensor.unsqueeze(dim=1).repeat(1, num_reps, 1)\n        tensor = tensor.view(-1, col_len)\n        return tensor\n\n    def forward(self, h_states, seq_start_end, end_pos):\n        """"""\n        Inputs:\n        - h_states: Tesnsor of shape (num_layers, batch, h_dim)\n        - seq_start_end: A list of tuples which delimit sequences within batch.\n        - end_pos: Absolute end position of obs_traj (batch, 2)\n        Output:\n        - pool_h: Tensor of shape (batch, h_dim)\n        """"""\n        pool_h = []\n        for _, (start, end) in enumerate(seq_start_end):\n            start = start.item()\n            end = end.item()\n            num_ped = end - start\n            grid_size = self.grid_size * self.grid_size\n            curr_hidden = h_states.view(-1, self.h_dim)[start:end]\n            curr_hidden_repeat = curr_hidden.repeat(num_ped, 1)\n            curr_end_pos = end_pos[start:end]\n            curr_pool_h_size = (num_ped * grid_size) + 1\n            curr_pool_h = curr_hidden.new_zeros((curr_pool_h_size, self.h_dim))\n            # curr_end_pos = curr_end_pos.data\n            top_left, bottom_right = self.get_bounds(curr_end_pos)\n\n            # Repeat position -> P1, P2, P1, P2\n            curr_end_pos = curr_end_pos.repeat(num_ped, 1)\n            # Repeat bounds -> B1, B1, B2, B2\n            top_left = self.repeat(top_left, num_ped)\n            bottom_right = self.repeat(bottom_right, num_ped)\n\n            grid_pos = self.get_grid_locations(\n                    top_left, curr_end_pos).type_as(seq_start_end)\n            # Make all positions to exclude as non-zero\n            # Find which peds to exclude\n            x_bound = ((curr_end_pos[:, 0] >= bottom_right[:, 0]) +\n                       (curr_end_pos[:, 0] <= top_left[:, 0]))\n            y_bound = ((curr_end_pos[:, 1] >= top_left[:, 1]) +\n                       (curr_end_pos[:, 1] <= bottom_right[:, 1]))\n\n            within_bound = x_bound + y_bound\n            within_bound[0::num_ped + 1] = 1  # Don\'t include the ped itself\n            within_bound = within_bound.view(-1)\n\n            # This is a tricky way to get scatter add to work. Helps me avoid a\n            # for loop. Offset everything by 1. Use the initial 0 position to\n            # dump all uncessary adds.\n            grid_pos += 1\n            total_grid_size = self.grid_size * self.grid_size\n            offset = torch.arange(\n                0, total_grid_size * num_ped, total_grid_size\n            ).type_as(seq_start_end)\n\n            offset = self.repeat(offset.view(-1, 1), num_ped).view(-1)\n            grid_pos += offset\n            grid_pos[within_bound != 0] = 0\n            grid_pos = grid_pos.view(-1, 1).expand_as(curr_hidden_repeat)\n\n            curr_pool_h = curr_pool_h.scatter_add(0, grid_pos,\n                                                  curr_hidden_repeat)\n            curr_pool_h = curr_pool_h[1:]\n            pool_h.append(curr_pool_h.view(num_ped, -1))\n\n        pool_h = torch.cat(pool_h, dim=0)\n        pool_h = self.mlp_pool(pool_h)\n        return pool_h\n\n\nclass TrajectoryGenerator(nn.Module):\n    def __init__(\n        self, obs_len, pred_len, embedding_dim=64, encoder_h_dim=64,\n        decoder_h_dim=128, mlp_dim=1024, num_layers=1, noise_dim=(0, ),\n        noise_type=\'gaussian\', noise_mix_type=\'ped\', pooling_type=None,\n        pool_every_timestep=True, dropout=0.0, bottleneck_dim=1024,\n        activation=\'relu\', batch_norm=True, neighborhood_size=2.0, grid_size=8\n    ):\n        super(TrajectoryGenerator, self).__init__()\n\n        if pooling_type and pooling_type.lower() == \'none\':\n            pooling_type = None\n\n        self.obs_len = obs_len\n        self.pred_len = pred_len\n        self.mlp_dim = mlp_dim\n        self.encoder_h_dim = encoder_h_dim\n        self.decoder_h_dim = decoder_h_dim\n        self.embedding_dim = embedding_dim\n        self.noise_dim = noise_dim\n        self.num_layers = num_layers\n        self.noise_type = noise_type\n        self.noise_mix_type = noise_mix_type\n        self.pooling_type = pooling_type\n        self.noise_first_dim = 0\n        self.pool_every_timestep = pool_every_timestep\n        self.bottleneck_dim = 1024\n\n        self.encoder = Encoder(\n            embedding_dim=embedding_dim,\n            h_dim=encoder_h_dim,\n            mlp_dim=mlp_dim,\n            num_layers=num_layers,\n            dropout=dropout\n        )\n\n        self.decoder = Decoder(\n            pred_len,\n            embedding_dim=embedding_dim,\n            h_dim=decoder_h_dim,\n            mlp_dim=mlp_dim,\n            num_layers=num_layers,\n            pool_every_timestep=pool_every_timestep,\n            dropout=dropout,\n            bottleneck_dim=bottleneck_dim,\n            activation=activation,\n            batch_norm=batch_norm,\n            pooling_type=pooling_type,\n            grid_size=grid_size,\n            neighborhood_size=neighborhood_size\n        )\n\n        if pooling_type == \'pool_net\':\n            self.pool_net = PoolHiddenNet(\n                embedding_dim=self.embedding_dim,\n                h_dim=encoder_h_dim,\n                mlp_dim=mlp_dim,\n                bottleneck_dim=bottleneck_dim,\n                activation=activation,\n                batch_norm=batch_norm\n            )\n        elif pooling_type == \'spool\':\n            self.pool_net = SocialPooling(\n                h_dim=encoder_h_dim,\n                activation=activation,\n                batch_norm=batch_norm,\n                dropout=dropout,\n                neighborhood_size=neighborhood_size,\n                grid_size=grid_size\n            )\n\n        if self.noise_dim[0] == 0:\n            self.noise_dim = None\n        else:\n            self.noise_first_dim = noise_dim[0]\n\n        # Decoder Hidden\n        if pooling_type:\n            input_dim = encoder_h_dim + bottleneck_dim\n        else:\n            input_dim = encoder_h_dim\n\n        if self.mlp_decoder_needed():\n            mlp_decoder_context_dims = [\n                input_dim, mlp_dim, decoder_h_dim - self.noise_first_dim\n            ]\n\n            self.mlp_decoder_context = make_mlp(\n                mlp_decoder_context_dims,\n                activation=activation,\n                batch_norm=batch_norm,\n                dropout=dropout\n            )\n\n    def add_noise(self, _input, seq_start_end, user_noise=None):\n        """"""\n        Inputs:\n        - _input: Tensor of shape (_, decoder_h_dim - noise_first_dim)\n        - seq_start_end: A list of tuples which delimit sequences within batch.\n        - user_noise: Generally used for inference when you want to see\n        relation between different types of noise and outputs.\n        Outputs:\n        - decoder_h: Tensor of shape (_, decoder_h_dim)\n        """"""\n        if not self.noise_dim:\n            return _input\n\n        if self.noise_mix_type == \'global\':\n            noise_shape = (seq_start_end.size(0), ) + self.noise_dim\n        else:\n            noise_shape = (_input.size(0), ) + self.noise_dim\n\n        if user_noise is not None:\n            z_decoder = user_noise\n        else:\n            z_decoder = get_noise(noise_shape, self.noise_type)\n\n        if self.noise_mix_type == \'global\':\n            _list = []\n            for idx, (start, end) in enumerate(seq_start_end):\n                start = start.item()\n                end = end.item()\n                _vec = z_decoder[idx].view(1, -1)\n                _to_cat = _vec.repeat(end - start, 1)\n                _list.append(torch.cat([_input[start:end], _to_cat], dim=1))\n            decoder_h = torch.cat(_list, dim=0)\n            return decoder_h\n\n        decoder_h = torch.cat([_input, z_decoder], dim=1)\n\n        return decoder_h\n\n    def mlp_decoder_needed(self):\n        if (\n            self.noise_dim or self.pooling_type or\n            self.encoder_h_dim != self.decoder_h_dim\n        ):\n            return True\n        else:\n            return False\n\n    def forward(self, obs_traj, obs_traj_rel, seq_start_end, user_noise=None):\n        """"""\n        Inputs:\n        - obs_traj: Tensor of shape (obs_len, batch, 2)\n        - obs_traj_rel: Tensor of shape (obs_len, batch, 2)\n        - seq_start_end: A list of tuples which delimit sequences within batch.\n        - user_noise: Generally used for inference when you want to see\n        relation between different types of noise and outputs.\n        Output:\n        - pred_traj_rel: Tensor of shape (self.pred_len, batch, 2)\n        """"""\n        batch = obs_traj_rel.size(1)\n        # Encode seq\n        final_encoder_h = self.encoder(obs_traj_rel)\n        # Pool States\n        if self.pooling_type:\n            end_pos = obs_traj[-1, :, :]\n            pool_h = self.pool_net(final_encoder_h, seq_start_end, end_pos)\n            # Construct input hidden states for decoder\n            mlp_decoder_context_input = torch.cat(\n                [final_encoder_h.view(-1, self.encoder_h_dim), pool_h], dim=1)\n        else:\n            mlp_decoder_context_input = final_encoder_h.view(\n                -1, self.encoder_h_dim)\n\n        # Add Noise\n        if self.mlp_decoder_needed():\n            noise_input = self.mlp_decoder_context(mlp_decoder_context_input)\n        else:\n            noise_input = mlp_decoder_context_input\n        decoder_h = self.add_noise(\n            noise_input, seq_start_end, user_noise=user_noise)\n        decoder_h = torch.unsqueeze(decoder_h, 0)\n\n        decoder_c = torch.zeros(\n            self.num_layers, batch, self.decoder_h_dim\n        ).cuda()\n\n        state_tuple = (decoder_h, decoder_c)\n        last_pos = obs_traj[-1]\n        last_pos_rel = obs_traj_rel[-1]\n        # Predict Trajectory\n\n        decoder_out = self.decoder(\n            last_pos,\n            last_pos_rel,\n            state_tuple,\n            seq_start_end,\n        )\n        pred_traj_fake_rel, final_decoder_h = decoder_out\n\n        return pred_traj_fake_rel\n\n\nclass TrajectoryDiscriminator(nn.Module):\n    def __init__(\n        self, obs_len, pred_len, embedding_dim=64, h_dim=64, mlp_dim=1024,\n        num_layers=1, activation=\'relu\', batch_norm=True, dropout=0.0,\n        d_type=\'local\'\n    ):\n        super(TrajectoryDiscriminator, self).__init__()\n\n        self.obs_len = obs_len\n        self.pred_len = pred_len\n        self.seq_len = obs_len + pred_len\n        self.mlp_dim = mlp_dim\n        self.h_dim = h_dim\n        self.d_type = d_type\n\n        self.encoder = Encoder(\n            embedding_dim=embedding_dim,\n            h_dim=h_dim,\n            mlp_dim=mlp_dim,\n            num_layers=num_layers,\n            dropout=dropout\n        )\n\n        real_classifier_dims = [h_dim, mlp_dim, 1]\n        self.real_classifier = make_mlp(\n            real_classifier_dims,\n            activation=activation,\n            batch_norm=batch_norm,\n            dropout=dropout\n        )\n        if d_type == \'global\':\n            mlp_pool_dims = [h_dim + embedding_dim, mlp_dim, h_dim]\n            self.pool_net = PoolHiddenNet(\n                embedding_dim=embedding_dim,\n                h_dim=h_dim,\n                mlp_dim=mlp_pool_dims,\n                bottleneck_dim=h_dim,\n                activation=activation,\n                batch_norm=batch_norm\n            )\n\n    def forward(self, traj, traj_rel, seq_start_end=None):\n        """"""\n        Inputs:\n        - traj: Tensor of shape (obs_len + pred_len, batch, 2)\n        - traj_rel: Tensor of shape (obs_len + pred_len, batch, 2)\n        - seq_start_end: A list of tuples which delimit sequences within batch\n        Output:\n        - scores: Tensor of shape (batch,) with real/fake scores\n        """"""\n        final_h = self.encoder(traj_rel)\n        # Note: In case of \'global\' option we are using start_pos as opposed to\n        # end_pos. The intution being that hidden state has the whole\n        # trajectory and relative postion at the start when combined with\n        # trajectory information should help in discriminative behavior.\n        if self.d_type == \'local\':\n            classifier_input = final_h.squeeze()\n        else:\n            classifier_input = self.pool_net(\n                final_h.squeeze(), seq_start_end, traj[0]\n            )\n        scores = self.real_classifier(classifier_input)\n        return scores\n'"
sgan/utils.py,5,"b'import os\nimport time\nimport torch\nimport numpy as np\nimport inspect\nfrom contextlib import contextmanager\nimport subprocess\n\n\ndef int_tuple(s):\n    return tuple(int(i) for i in s.split(\',\'))\n\n\ndef find_nan(variable, var_name):\n    variable_n = variable.data.cpu().numpy()\n    if np.isnan(variable_n).any():\n        exit(\'%s has nan\' % var_name)\n\n\ndef bool_flag(s):\n    if s == \'1\':\n        return True\n    elif s == \'0\':\n        return False\n    msg = \'Invalid value ""%s"" for bool flag (should be 0 or 1)\'\n    raise ValueError(msg % s)\n\n\ndef lineno():\n    return str(inspect.currentframe().f_back.f_lineno)\n\n\ndef get_total_norm(parameters, norm_type=2):\n    if norm_type == float(\'inf\'):\n        total_norm = max(p.grad.data.abs().max() for p in parameters)\n    else:\n        total_norm = 0\n        for p in parameters:\n            try:\n                param_norm = p.grad.data.norm(norm_type)\n                total_norm += param_norm**norm_type\n                total_norm = total_norm**(1. / norm_type)\n            except:\n                continue\n    return total_norm\n\n\n@contextmanager\ndef timeit(msg, should_time=True):\n    if should_time:\n        torch.cuda.synchronize()\n        t0 = time.time()\n    yield\n    if should_time:\n        torch.cuda.synchronize()\n        t1 = time.time()\n        duration = (t1 - t0) * 1000.0\n        print(\'%s: %.2f ms\' % (msg, duration))\n\n\ndef get_gpu_memory():\n    torch.cuda.synchronize()\n    opts = [\n        \'nvidia-smi\', \'-q\', \'--gpu=\' + str(1), \'|\', \'grep\', \'""Used GPU Memory""\'\n    ]\n    cmd = str.join(\' \', opts)\n    ps = subprocess.Popen(\n        cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n    output = ps.communicate()[0].decode(\'utf-8\')\n    output = output.split(""\\n"")[0].split("":"")\n    consumed_mem = int(output[1].strip().split("" "")[0])\n    return consumed_mem\n\n\ndef get_dset_path(dset_name, dset_type):\n    _dir = os.path.dirname(__file__)\n    _dir = _dir.split(""/"")[:-1]\n    _dir = ""/"".join(_dir)\n    return os.path.join(_dir, \'datasets\', dset_name, dset_type)\n\n\ndef relative_to_abs(rel_traj, start_pos):\n    """"""\n    Inputs:\n    - rel_traj: pytorch tensor of shape (seq_len, batch, 2)\n    - start_pos: pytorch tensor of shape (batch, 2)\n    Outputs:\n    - abs_traj: pytorch tensor of shape (seq_len, batch, 2)\n    """"""\n    # batch, seq_len, 2\n    rel_traj = rel_traj.permute(1, 0, 2)\n    displacement = torch.cumsum(rel_traj, dim=1)\n    start_pos = torch.unsqueeze(start_pos, dim=1)\n    abs_traj = displacement + start_pos\n    return abs_traj.permute(1, 0, 2)\n'"
sgan/data/__init__.py,0,"b'from .trajectories import seq_collate, TrajectoryDataset\n'"
sgan/data/loader.py,1,"b'from torch.utils.data import DataLoader\n\nfrom sgan.data.trajectories import TrajectoryDataset, seq_collate\n\n\ndef data_loader(args, path):\n    dset = TrajectoryDataset(\n        path,\n        obs_len=args.obs_len,\n        pred_len=args.pred_len,\n        skip=args.skip,\n        delim=args.delim)\n\n    loader = DataLoader(\n        dset,\n        batch_size=args.batch_size,\n        shuffle=True,\n        num_workers=args.loader_num_workers,\n        collate_fn=seq_collate)\n    return dset, loader\n'"
sgan/data/trajectories.py,18,"b'import logging\nimport os\nimport math\n\nimport numpy as np\n\nimport torch\nfrom torch.utils.data import Dataset\n\nlogger = logging.getLogger(__name__)\n\n\ndef seq_collate(data):\n    (obs_seq_list, pred_seq_list, obs_seq_rel_list, pred_seq_rel_list,\n     non_linear_ped_list, loss_mask_list) = zip(*data)\n\n    _len = [len(seq) for seq in obs_seq_list]\n    cum_start_idx = [0] + np.cumsum(_len).tolist()\n    seq_start_end = [[start, end]\n                     for start, end in zip(cum_start_idx, cum_start_idx[1:])]\n\n    # Data format: batch, input_size, seq_len\n    # LSTM input format: seq_len, batch, input_size\n    obs_traj = torch.cat(obs_seq_list, dim=0).permute(2, 0, 1)\n    pred_traj = torch.cat(pred_seq_list, dim=0).permute(2, 0, 1)\n    obs_traj_rel = torch.cat(obs_seq_rel_list, dim=0).permute(2, 0, 1)\n    pred_traj_rel = torch.cat(pred_seq_rel_list, dim=0).permute(2, 0, 1)\n    non_linear_ped = torch.cat(non_linear_ped_list)\n    loss_mask = torch.cat(loss_mask_list, dim=0)\n    seq_start_end = torch.LongTensor(seq_start_end)\n    out = [\n        obs_traj, pred_traj, obs_traj_rel, pred_traj_rel, non_linear_ped,\n        loss_mask, seq_start_end\n    ]\n\n    return tuple(out)\n\n\ndef read_file(_path, delim=\'\\t\'):\n    data = []\n    if delim == \'tab\':\n        delim = \'\\t\'\n    elif delim == \'space\':\n        delim = \' \'\n    with open(_path, \'r\') as f:\n        for line in f:\n            line = line.strip().split(delim)\n            line = [float(i) for i in line]\n            data.append(line)\n    return np.asarray(data)\n\n\ndef poly_fit(traj, traj_len, threshold):\n    """"""\n    Input:\n    - traj: Numpy array of shape (2, traj_len)\n    - traj_len: Len of trajectory\n    - threshold: Minimum error to be considered for non linear traj\n    Output:\n    - int: 1 -> Non Linear 0-> Linear\n    """"""\n    t = np.linspace(0, traj_len - 1, traj_len)\n    res_x = np.polyfit(t, traj[0, -traj_len:], 2, full=True)[1]\n    res_y = np.polyfit(t, traj[1, -traj_len:], 2, full=True)[1]\n    if res_x + res_y >= threshold:\n        return 1.0\n    else:\n        return 0.0\n\n\nclass TrajectoryDataset(Dataset):\n    """"""Dataloder for the Trajectory datasets""""""\n    def __init__(\n        self, data_dir, obs_len=8, pred_len=12, skip=1, threshold=0.002,\n        min_ped=1, delim=\'\\t\'\n    ):\n        """"""\n        Args:\n        - data_dir: Directory containing dataset files in the format\n        <frame_id> <ped_id> <x> <y>\n        - obs_len: Number of time-steps in input trajectories\n        - pred_len: Number of time-steps in output trajectories\n        - skip: Number of frames to skip while making the dataset\n        - threshold: Minimum error to be considered for non linear traj\n        when using a linear predictor\n        - min_ped: Minimum number of pedestrians that should be in a seqeunce\n        - delim: Delimiter in the dataset files\n        """"""\n        super(TrajectoryDataset, self).__init__()\n\n        self.data_dir = data_dir\n        self.obs_len = obs_len\n        self.pred_len = pred_len\n        self.skip = skip\n        self.seq_len = self.obs_len + self.pred_len\n        self.delim = delim\n\n        all_files = os.listdir(self.data_dir)\n        all_files = [os.path.join(self.data_dir, _path) for _path in all_files]\n        num_peds_in_seq = []\n        seq_list = []\n        seq_list_rel = []\n        loss_mask_list = []\n        non_linear_ped = []\n        for path in all_files:\n            data = read_file(path, delim)\n            frames = np.unique(data[:, 0]).tolist()\n            frame_data = []\n            for frame in frames:\n                frame_data.append(data[frame == data[:, 0], :])\n            num_sequences = int(\n                math.ceil((len(frames) - self.seq_len + 1) / skip))\n\n            for idx in range(0, num_sequences * self.skip + 1, skip):\n                curr_seq_data = np.concatenate(\n                    frame_data[idx:idx + self.seq_len], axis=0)\n                peds_in_curr_seq = np.unique(curr_seq_data[:, 1])\n                curr_seq_rel = np.zeros((len(peds_in_curr_seq), 2,\n                                         self.seq_len))\n                curr_seq = np.zeros((len(peds_in_curr_seq), 2, self.seq_len))\n                curr_loss_mask = np.zeros((len(peds_in_curr_seq),\n                                           self.seq_len))\n                num_peds_considered = 0\n                _non_linear_ped = []\n                for _, ped_id in enumerate(peds_in_curr_seq):\n                    curr_ped_seq = curr_seq_data[curr_seq_data[:, 1] ==\n                                                 ped_id, :]\n                    curr_ped_seq = np.around(curr_ped_seq, decimals=4)\n                    pad_front = frames.index(curr_ped_seq[0, 0]) - idx\n                    pad_end = frames.index(curr_ped_seq[-1, 0]) - idx + 1\n                    if pad_end - pad_front != self.seq_len:\n                        continue\n                    curr_ped_seq = np.transpose(curr_ped_seq[:, 2:])\n                    curr_ped_seq = curr_ped_seq\n                    # Make coordinates relative\n                    rel_curr_ped_seq = np.zeros(curr_ped_seq.shape)\n                    rel_curr_ped_seq[:, 1:] = \\\n                        curr_ped_seq[:, 1:] - curr_ped_seq[:, :-1]\n                    _idx = num_peds_considered\n                    curr_seq[_idx, :, pad_front:pad_end] = curr_ped_seq\n                    curr_seq_rel[_idx, :, pad_front:pad_end] = rel_curr_ped_seq\n                    # Linear vs Non-Linear Trajectory\n                    _non_linear_ped.append(\n                        poly_fit(curr_ped_seq, pred_len, threshold))\n                    curr_loss_mask[_idx, pad_front:pad_end] = 1\n                    num_peds_considered += 1\n\n                if num_peds_considered > min_ped:\n                    non_linear_ped += _non_linear_ped\n                    num_peds_in_seq.append(num_peds_considered)\n                    loss_mask_list.append(curr_loss_mask[:num_peds_considered])\n                    seq_list.append(curr_seq[:num_peds_considered])\n                    seq_list_rel.append(curr_seq_rel[:num_peds_considered])\n\n        self.num_seq = len(seq_list)\n        seq_list = np.concatenate(seq_list, axis=0)\n        seq_list_rel = np.concatenate(seq_list_rel, axis=0)\n        loss_mask_list = np.concatenate(loss_mask_list, axis=0)\n        non_linear_ped = np.asarray(non_linear_ped)\n\n        # Convert numpy -> Torch Tensor\n        self.obs_traj = torch.from_numpy(\n            seq_list[:, :, :self.obs_len]).type(torch.float)\n        self.pred_traj = torch.from_numpy(\n            seq_list[:, :, self.obs_len:]).type(torch.float)\n        self.obs_traj_rel = torch.from_numpy(\n            seq_list_rel[:, :, :self.obs_len]).type(torch.float)\n        self.pred_traj_rel = torch.from_numpy(\n            seq_list_rel[:, :, self.obs_len:]).type(torch.float)\n        self.loss_mask = torch.from_numpy(loss_mask_list).type(torch.float)\n        self.non_linear_ped = torch.from_numpy(non_linear_ped).type(torch.float)\n        cum_start_idx = [0] + np.cumsum(num_peds_in_seq).tolist()\n        self.seq_start_end = [\n            (start, end)\n            for start, end in zip(cum_start_idx, cum_start_idx[1:])\n        ]\n\n    def __len__(self):\n        return self.num_seq\n\n    def __getitem__(self, index):\n        start, end = self.seq_start_end[index]\n        out = [\n            self.obs_traj[start:end, :], self.pred_traj[start:end, :],\n            self.obs_traj_rel[start:end, :], self.pred_traj_rel[start:end, :],\n            self.non_linear_ped[start:end], self.loss_mask[start:end, :]\n        ]\n        return out\n'"
