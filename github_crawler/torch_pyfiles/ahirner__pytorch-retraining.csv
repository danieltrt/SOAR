file_path,api_count,code
CLR_preview.py,4,"b'#Preview of CLR scheduler as submitted in:\n#https://github.com/pytorch/pytorch/pull/2016\n\nimport numpy as np\nfrom torch.optim.optimizer import Optimizer\n\nclass CyclicLR(object):\n    """"""Sets the learning rate of each parameter group according to\n    cyclical learning rate policy (CLR). The policy cycles the learning\n    rate between two boundaries with a constant frequency, as detailed in\n    the paper `Cyclical Learning Rates for Training Neural Networks`_.\n    The distance between the two boundaries can be scaled on a per-iteration\n    or per-cycle basis.\n    Cyclical learning rate policy changes the learning rate after every batch.\n    `batch_step` should be called after a batch has been used for training.\n    To resume training, save `last_batch_iteration` and use it to instantiate `CycleLR`.\n    This class has three built-in policies, as put forth in the paper:\n    ""triangular"":\n        A basic triangular cycle w/ no amplitude scaling.\n    ""triangular2"":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    ""exp_range"":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each\n        cycle iteration.\n    This implementation was adapted from the github repo: `bckenstler/CLR`_\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        base_lr (float or list): Initial learning rate which is the\n            lower boundary in the cycle for eachparam groups.\n            Default: 0.001\n        max_lr (float or list): Upper boundaries in the cycle for\n            each parameter group. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore\n            max_lr may not actually be reached depending on\n            scaling function. Default: 0.006\n        step_size (int): Number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch. Default: 2000\n        mode (str): One of {triangular, triangular2, exp_range}.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n            Default: \'triangular\'\n        gamma (float): Constant in \'exp_range\' scaling function:\n            gamma**(cycle iterations)\n            Default: 1.0\n        scale_fn (function): Custom scaling policy defined by a single\n            argument lambda function, where\n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored\n            Default: None\n        scale_mode (str): {\'cycle\', \'iterations\'}.\n            Defines whether scale_fn is evaluated on\n            cycle number or cycle iterations (training\n            iterations since start of cycle).\n            Default: \'cycle\'\n        last_batch_iteration (int): The index of the last batch. Default: -1\n    Example:\n        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n        >>> scheduler = torch.optim.CyclicLR(optimizer)\n        >>> data_loader = torch.utils.data.DataLoader(...)\n        >>> for epoch in range(10):\n        >>>     for batch in data_loader:\n        >>>         scheduler.batch_step()\n        >>>         train_batch(...)\n    .. _Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186\n    .. _bckenstler/CLR: https://github.com/bckenstler/CLR\n    """"""\n\n    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n                 step_size=2000, mode=\'triangular\', gamma=1.,\n                 scale_fn=None, scale_mode=\'cycle\', last_batch_iteration=-1):\n\n        if not isinstance(optimizer, Optimizer):\n            raise TypeError(\'{} is not an Optimizer\'.format(\n                type(optimizer).__name__))\n        self.optimizer = optimizer\n\n        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n            if len(base_lr) != len(optimizer.param_groups):\n                raise ValueError(""expected {} base_lr, got {}"".format(\n                    len(optimizer.param_groups), len(base_lr)))\n            self.base_lrs = list(base_lr)\n        else:\n            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n\n        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n            if len(max_lr) != len(optimizer.param_groups):\n                raise ValueError(""expected {} max_lr, got {}"".format(\n                    len(optimizer.param_groups), len(max_lr)))\n            self.max_lrs = list(max_lr)\n        else:\n            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n\n        self.step_size = step_size\n\n        if mode not in [\'triangular\', \'triangular2\', \'exp_range\'] \\\n                and scale_fn is None:\n            raise ValueError(\'mode is invalid and scale_fn is None\')\n\n        self.mode = mode\n        self.gamma = gamma\n\n        if scale_fn is None:\n            if self.mode == \'triangular\':\n                self.scale_fn = self._triangular_scale_fn\n                self.scale_mode = \'cycle\'\n            elif self.mode == \'triangular2\':\n                self.scale_fn = self._triangular2_scale_fn\n                self.scale_mode = \'cycle\'\n            elif self.mode == \'exp_range\':\n                self.scale_fn = self._exp_range_scale_fn\n                self.scale_mode = \'iterations\'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n\n        self.batch_step(last_batch_iteration + 1)\n        self.last_batch_iteration = last_batch_iteration\n\n    def batch_step(self, batch_iteration=None):\n        if batch_iteration is None:\n            batch_iteration = self.last_batch_iteration + 1\n        self.last_batch_iteration = batch_iteration\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n            param_group[\'lr\'] = lr\n\n    def _triangular_scale_fn(self, x):\n        return 1.\n\n    def _triangular2_scale_fn(self, x):\n        return 1 / (2. ** (x - 1))\n\n    def _exp_range_scale_fn(self, x):\n        return self.gamma**(x)\n\n    def get_lr(self):\n        step_size = float(self.step_size)\n        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n\n        lrs = []\n        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n        for param_group, base_lr, max_lr in param_lrs:\n            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n            if self.scale_mode == \'cycle\':\n                lr = base_lr + base_height * self.scale_fn(cycle)\n            else:\n                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n            lrs.append(lr)\n        return lrs'"
retrain.py,27,"b'import csv\nimport time\nimport os\nfrom glob import glob\n\nimport torch\nimport torch.optim as optim\nfrom torch import nn\nfrom torch.autograd import Variable\n\nimport torchvision\nimport torchvision.models as models\nimport torch.utils.model_zoo as model_zoo\nimport torchvision.transforms as transforms\nfrom torchvision import datasets\n\nfrom itertools import accumulate\nfrom functools import reduce\n\nfrom CLR_preview import CyclicLR\n\nmodel_urls = {\n    \'alexnet\': \'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\',\n    \'densenet121\': \'https://download.pytorch.org/models/densenet121-241335ed.pth\',\n    \'densenet169\': \'https://download.pytorch.org/models/densenet169-6f0f7f60.pth\',\n    \'densenet201\': \'https://download.pytorch.org/models/densenet201-4c113574.pth\',\n    \'densenet161\': \'https://download.pytorch.org/models/densenet161-17b70270.pth\',\n    #truncated _google to match module name\n    \'inception_v3\': \'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\',\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',    \n    \'squeezenet1_0\': \'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth\',\n    \'squeezenet1_1\': \'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth\',\n    \'vgg11\': \'https://download.pytorch.org/models/vgg11-bbd30ac9.pth\',\n    \'vgg13\': \'https://download.pytorch.org/models/vgg13-c768596a.pth\',\n    \'vgg16\': \'https://download.pytorch.org/models/vgg16-397923af.pth\',\n    \'vgg19\': \'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\',    \n}\n\nmodel_names = model_urls.keys()\n\ninput_sizes = {\n    \'alexnet\' : (224,224),\n    \'densenet\': (224,224),\n    \'resnet\' : (224,224),\n    \'inception\' : (299,299),\n    \'squeezenet\' : (224,224),#not 255,255 acc. to https://github.com/pytorch/pytorch/issues/1120\n    \'vgg\' : (224,224)\n}\n\n# ### Configuration\nmodels_to_test = [\'alexnet\', \'densenet169\', \'inception_v3\', \\\n                  \'resnet34\', \'squeezenet1_1\', \'vgg13\']\n#Todo: inception_v3 (sometimes) fails with tensor size mismatch\n#when training from scratch\nmodels_to_test = [\'alexnet\', \'densenet169\', \\\n                  \'resnet34\', \'squeezenet1_1\', \'vgg13\']\n\n#Todo: Argparse\ndata_dir = \'xxx\'\n\ntrain_subfolder = os.path.join(data_dir, \'train\')\nclasses = [d.split(train_subfolder, 1)[1] for d in \\\n           glob(os.path.join(train_subfolder, \'**\'))]\n\nbatch_size = 8\nepoch_multiplier = 4 #per class and times 1(shallow), 2(deep), 4(from_scratch)\nuse_gpu = torch.cuda.is_available()\nuse_clr = True\n\n#Assume 50 examples per class and CLR authors\' middle ground\nclr_stepsize = (len(classes)*50//batch_size)*4\n\n\nprint(""Shootout of model(s) %s with batch_size %d running on CUDA %s "" % \\\n            ("", "".join(models_to_test), batch_size, use_gpu) + \\\n            ""with CLR %s for %d classes on data in %s."" % \\\n            (use_clr, len(classes), data_dir))\n\n\n# ### Generic pretrained model loading\n\n\n#We solve the dimensionality mismatch between\n#final layers in the constructed vs pretrained\n#modules at the data level.\ndef diff_states(dict_canonical, dict_subset):\n    names1, names2 = (list(dict_canonical.keys()), list(dict_subset.keys()))\n    \n    #Sanity check that param names overlap\n    #Note that params are not necessarily in the same order\n    #for every pretrained model\n    not_in_1 = [n for n in names1 if n not in names2]\n    not_in_2 = [n for n in names2 if n not in names1]\n    assert len(not_in_1) == 0\n    assert len(not_in_2) == 0\n\n    for name, v1 in dict_canonical.items():\n        v2 = dict_subset[name]\n        assert hasattr(v2, \'size\')\n        if v1.size() != v2.size():\n            yield (name, v1)                \n\ndef load_model_merged(name, num_classes):\n    \n    # Densenets don\'t (yet) pass on num_classes, hack it in\n    if ""densenet"" in name:\n        if name == \'densenet169\':\n            model = models.DenseNet(num_init_features=64, growth_rate=32, \\\n                                    block_config=(6, 12, 32, 32),\n                                    num_classes=num_classes)\n\n        elif name == \'densenet121\':\n            model = models.DenseNet(num_init_features=64, growth_rate=32, \\\n                                    block_config=(6, 12, 24, 16),\n                                    num_classes=num_classes)\n\n        elif name == \'densenet201\':\n            model = models.DenseNet(num_init_features=64, growth_rate=32, \\\n                                    block_config=(6, 12, 48, 32),\n                                    num_classes=num_classes)\n\n        elif name == \'densenet161\':\n            model = models.DenseNet(num_init_features=96, growth_rate=48, \\\n                                    block_config=(6, 12, 36, 24),\n                                    num_classes=num_classes)\n        else:\n            raise ValueError(\n                ""Cirumventing missing num_classes kwargs not implemented for %s"" % name)\n    else:\n        model = models.__dict__[name](num_classes=num_classes)\n\n\n    pretrained_state = model_zoo.load_url(model_urls[name])\n\n    #Diff\n    diff = [s for s in diff_states(model.state_dict(), pretrained_state)]\n    print(""Replacing the following state from initialized"", name, "":"",           [d[0] for d in diff])\n    \n    for name, value in diff:\n        pretrained_state[name] = value\n    \n    assert len([s for s in diff_states(model.state_dict(), pretrained_state)]) == 0\n    \n    #Merge\n    model.load_state_dict(pretrained_state)\n    return model, diff\n\n\ndef filtered_params(net, param_list=None):\n    def in_param_list(s):\n        for p in param_list:\n            if s.endswith(p):\n                return True\n        return False    \n    #Caution: DataParallel prefixes \'.module\' to every parameter name\n    params = net.named_parameters() if param_list is None     else (p for p in net.named_parameters() if           in_param_list(p[0]) and p[1].requires_grad)\n    return params\n\n\n#Todo: split function into separate test and train data\n#To get the tutorial data (bee vs. ants), go to:\n#http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\ndef get_data(resize):\n\n    data_transforms = {\n        \'train\': transforms.Compose([\n            transforms.RandomSizedCrop(max(resize)),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]),\n        \'val\': transforms.Compose([\n            #Higher scale-up for inception\n            transforms.Scale(int(max(resize)/224*256)),\n            transforms.CenterCrop(max(resize)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]),\n    }\n\n    dsets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n             for x in [\'train\', \'val\']}\n    dset_loaders = {x: torch.utils.data.DataLoader(dsets[x], batch_size=batch_size,\n                                                   shuffle=True)\n                    for x in [\'train\', \'val\']}\n    \n    return dset_loaders[\'train\'], dset_loaders[\'val\']\n\n\ndef train(net, trainloader, epochs, param_list=None, CLR=False):\n    #Todo: DRY\n    def in_param_list(s):\n        for p in param_list:\n            if s.endswith(p):\n                return True\n        return False\n\n    criterion = nn.CrossEntropyLoss()\n    if use_gpu:\n        criterion = criterion.cuda()\n    \n    #If finetuning model, turn off grad for other params and make sure to turn on others\n    for p in net.named_parameters():\n        p[1].requires_grad = (param_list is None) or in_param_list(p[0])\n\n    params = (p for p in filtered_params(net, param_list))\n\n    #Optimizer as in tutorial\n    optimizer = optim.SGD((p[1] for p in params), lr=0.001, momentum=0.9)\n    if CLR:\n            \n        global clr_stepsize\n        clr_wrapper = CyclicLR(optimizer, step_size=clr_stepsize)\n    \n    losses = []\n    for epoch in range(epochs):\n\n        running_loss = 0.0\n        for i, data in enumerate(trainloader, 0):\n            # get the inputs\n            inputs, labels = data\n            if use_gpu:\n                inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda(async=True))\n            else:\n                inputs, labels = Variable(inputs), Variable(labels)\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            outputs = net(inputs)\n            \n            loss = None\n            # for nets that have multiple outputs such as inception\n            if isinstance(outputs, tuple):\n                loss = sum((criterion(o,labels) for o in outputs))\n            else:\n                loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            if CLR:\n                clr_wrapper.batch_step()\n            \n            # print statistics\n            running_loss += loss.data[0]\n            if i % 30 == 29:\n                avg_loss = running_loss / 30\n                losses.append(avg_loss)\n                \n                lrs = [p[\'lr\'] for p in optimizer.param_groups]\n                    \n                print(\'[%d, %5d] loss: %.3f\' %\n                      (epoch + 1, i + 1, avg_loss), lrs)\n                running_loss = 0.0\n\n    print(\'Finished Training\')\n    return losses\n\n\ndef train_stats(m, trainloader, epochs, param_list=None, CLR=False):\n    """"""\n    Get stats for training and evaluation in a structured way\n    If param_list is None all relevant parameters are tuned,\n    otherwise, only parameters that have been constructed for custom\n    num_classes\n    """"""\n    stats = {}\n    params = filtered_params(m, param_list)    \n    counts = 0,0\n    for counts in enumerate(accumulate((reduce(lambda d1,d2: d1*d2, p[1].size()) for p in params)) ):\n        pass\n    stats[\'variables_optimized\'] = counts[0] + 1\n    stats[\'params_optimized\'] = counts[1]\n    \n    before = time.time()\n    losses = train(m, trainloader, epochs, param_list=param_list, CLR=CLR)\n    stats[\'training_time\'] = time.time() - before\n\n    stats[\'training_loss\'] = losses[-1] if len(losses) else float(\'nan\')\n    stats[\'training_losses\'] = losses\n    \n    return stats\n\ndef evaluate_stats(net, testloader):\n    stats = {}\n    correct = 0\n    total = 0\n    \n    before = time.time()\n    for i, data in enumerate(testloader, 0):\n        images, labels = data\n\n        if use_gpu:\n            images, labels = (images.cuda()), (labels.cuda(async=True))\n\n        outputs = net(Variable(images))\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum()\n    accuracy = correct / total\n    stats[\'accuracy\'] = accuracy\n    stats[\'eval_time\'] = time.time() - before\n    \n    print(\'Accuracy on test images: %f\' % accuracy)\n    return stats\n\n\ndef train_eval(net, trainloader, testloader, epochs, param_list=None, CLR=False):\n    print(""Training..."" if not param_list else ""Retraining..."")\n    stats_train = train_stats(net, trainloader, epochs, param_list=param_list, CLR=CLR)\n    \n    print(""Evaluating..."")\n    net = net.eval()\n    stats_eval = evaluate_stats(net, testloader)\n    \n    return {**stats_train, **stats_eval}\n\n\nif __name__==\'__main__\':\n    stats = []\n    t = 0.0\n    num_classes = len(classes)\n\n    #Retraining shallow\n    epochs = num_classes * epoch_multiplier * 1\n    print(""RETRAINING %d epochs"" % epochs)\n\n    for name in models_to_test:\n        print("""")\n        print(""Targeting %s with %d classes"" % (name, num_classes))\n        print(""------------------------------------------"")\n        model_pretrained, diff = load_model_merged(name, num_classes)\n        final_params = [d[0] for d in diff]\n\n        resize = [s[1] for s in input_sizes.items() if s[0] in name][0]\n        print(""Resizing input images to max of"", resize)\n        trainloader, testloader = get_data(resize)\n\n        if use_gpu:\n            print(""Transfering models to GPU(s)"")\n            model_pretrained = torch.nn.DataParallel(model_pretrained).cuda()\n\n        pretrained_stats = train_eval(model_pretrained, \n                                      trainloader, testloader, epochs,\n                                      final_params, use_clr)\n        pretrained_stats[\'name\'] = name\n        pretrained_stats[\'retrained\'] = True\n        pretrained_stats[\'shallow_retrain\'] = True\n        stats.append(pretrained_stats)\n\n        print("""")\n\n    #Training from scratch\n    epochs = num_classes * epoch_multiplier * 4\n    print(""TRAINING %d epochs from scratch"" % epochs)\n    \n    for name in models_to_test:\n        print("""")    \n        print(""Targeting %s with %d classes"" % (name, num_classes))\n        print(""------------------------------------------"")\n        model_blank = models.__dict__[name](num_classes=num_classes)\n\n        resize = [s[1] for s in input_sizes.items() if s[0] in name][0]\n        print(""Resizing input images to max of"", resize)\n        trainloader, testloader = get_data(resize)\n\n        if use_gpu:\n            print(""Transfering models to GPU(s)"")\n            model_blank = torch.nn.DataParallel(model_blank).cuda()    \n\n        blank_stats = train_eval(model_pretrained, trainloader, testloader, epochs, None,\n                                 CLR=use_clr)\n        blank_stats[\'name\'] = name\n        blank_stats[\'retrained\'] = False\n        blank_stats[\'shallow_retrain\'] = False\n        stats.append(blank_stats)\n\n        print("""")\n\n    #Retraining deep\n    epochs = num_classes * epoch_multiplier * 2\n    print(""RETRAINING %d epochs deeply"" % epochs)\n\n    for name in models_to_test:\n        print("""")\n        print(""Targeting %s with %d classes"" % (name, num_classes))\n        print(""------------------------------------------"")\n        model_pretrained, diff = load_model_merged(name, num_classes)\n\n        resize = [s[1] for s in input_sizes.items() if s[0] in name][0]\n        print(""Resizing input images to max of"", resize)\n        trainloader, testloader = get_data(resize)\n\n        if use_gpu:\n            print(""Transfering models to GPU(s)"")\n            model_pretrained = torch.nn.DataParallel(model_pretrained).cuda()\n\n        pretrained_stats = train_eval(model_pretrained, trainloader, testloader, \n                                      epochs, None,CLR=use_clr)\n        pretrained_stats[\'name\'] = name\n        pretrained_stats[\'retrained\'] = True\n        pretrained_stats[\'shallow_retrain\'] = False\n        stats.append(pretrained_stats)\n\n        print("""")\n\n\n    for s in stats:\n        t += s[\'eval_time\'] + s[\'training_time\']\n    print(""Total time for training and evaluation"", t)\n    print(""FINISHED"")\n\n    #Export\n    with open(data_dir+(\'_clr\' if use_clr else \'\')+\'.csv\', \'w\') as csvfile:\n        fieldnames = stats[0].keys()\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        writer.writeheader()\n        for s in stats:\n            writer.writerow(s)\n\n'"
