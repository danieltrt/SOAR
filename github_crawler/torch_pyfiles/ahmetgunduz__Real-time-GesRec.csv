file_path,api_count,code
calculate_FLOP.py,1,"b'import torch.nn as nn\nfrom thop import profile\nfrom models import squeezenet, shufflenetv2, shufflenet, mobilenet, mobilenetv2, c3d, resnext, resnet\n\n# %%%%%%%%--------------------- SELECT THE MODEL BELOW ---------------------%%%%%%%%\n\n# model = shufflenet.get_model(groups=3, width_mult=0.5, num_classes=27)#1\n# model = shufflenetv2.get_model( width_mult=0.25, num_classes=27, sample_size = 112)#2\n# model = mobilenet.get_model( width_mult=0.5, num_classes=27, sample_size = 112)#3\n# model = mobilenetv2.get_model( width_mult=0.2, num_classes=27, sample_size = 112)#4\n# model = shufflenet.get_model(groups=3, width_mult=1.0, num_classes=27)#5\n# model = shufflenetv2.get_model( width_mult=1.0, num_classes=27, sample_size = 112)#6\n# model = mobilenet.get_model( width_mult=1.0, num_classes=27, sample_size = 112)#7\n# model = mobilenetv2.get_model( width_mult=0.45, num_classes=27, sample_size = 112)#8\n# model = shufflenet.get_model(groups=3, width_mult=1.5, num_classes=27)#9\n# model = shufflenetv2.get_model( width_mult=1.5, num_classes=27, sample_size = 112)#10\n# model = mobilenet.get_model( width_mult=1.5, num_classes=27, sample_size = 112)#11\n# model = mobilenetv2.get_model( width_mult=0.7, num_classes=27, sample_size = 112)#12\n# model = shufflenet.get_model(groups=3, width_mult=2.0, num_classes=27)#13\n# model = shufflenetv2.get_model( width_mult=2.0, num_classes=27, sample_size = 112)#14\n# model = mobilenet.get_model( width_mult=2.0, num_classes=27, sample_size = 112)#15\n# model = mobilenetv2.get_model( width_mult=1.0, num_classes=27, sample_size = 112)#16\n# model = squeezenet.get_model( version=1.1, num_classes=27, sample_size = 112, sample_duration = 16)\n# model = resnext.resnet101( num_classes=27, shortcut_type=\'B\', cardinality=32, sample_size=112, sample_duration=32)\n# model = resnet.resnet18( num_classes=27, shortcut_type=\'A\', sample_size=112, sample_duration=16)\n# model = resnet.resnet50( num_classes=27, shortcut_type=\'A\', sample_size=112, sample_duration=16)\n# model = resnet.resnet101( num_classes=27, shortcut_type=\'A\', sample_size=112, sample_duration=16)\nmodel = c3d.get_model( num_classes=27, sample_size=112, sample_duration=32)\n#model = model.cuda()\n#model = nn.DataParallel(model)\t\nprint(model)\n\nflops, prms = profile(model, input_size=(1, 3, 64, 224, 224))\nprint(""Total number of FLOPs: "", flops)\n\npytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(""Total number of trainable parameters: "", pytorch_total_params)\n\n'"
dataset.py,0,"b'from datasets.kinetics import Kinetics\nfrom datasets.ucf101 import UCF101\nfrom datasets.jester import Jester\nfrom datasets.egogesture import EgoGesture\nfrom datasets.nv import NV\nfrom datasets.egogesture_online import EgoGestureOnline\nfrom datasets.nv_online import NVOnline\n\ndef get_training_set(opt, spatial_transform, temporal_transform,\n                     target_transform):\n    assert opt.dataset in [\'kinetics\', \'jester\', \'ucf101\', \'egogesture\', \'nvgesture\']\n\n    if opt.train_validate:\n        subset = [\'training\', \'validation\']\n    else:\n        subset = \'training\'\n\n    if opt.dataset == \'kinetics\':\n        training_data = Kinetics(\n            opt.video_path,\n            opt.annotation_path,\n            \'training\',\n            spatial_transform=spatial_transform,\n            temporal_transform=temporal_transform,\n            target_transform=target_transform,\n            sample_duration=opt.sample_duration)\n    elif opt.dataset == \'jester\':\n        training_data = Jester(\n            opt.video_path,\n            opt.annotation_path,\n            \'training\',\n            spatial_transform=spatial_transform,\n            temporal_transform=temporal_transform,\n            target_transform=target_transform,\n            sample_duration=opt.sample_duration)\n    elif opt.dataset == \'ucf101\':\n        training_data = UCF101(\n            opt.video_path,\n            opt.annotation_path,\n            \'training\',\n            spatial_transform=spatial_transform,\n            temporal_transform=temporal_transform,\n            target_transform=target_transform,\n            sample_duration=opt.sample_duration)\n    elif opt.dataset == \'egogesture\':\n        training_data = EgoGesture(\n            opt.video_path,\n            opt.annotation_path,\n            subset,\n            spatial_transform=spatial_transform,\n            temporal_transform=temporal_transform,\n            target_transform=target_transform,\n            sample_duration=opt.sample_duration,\n            modality=opt.modality)\n    elif opt.dataset == \'nvgesture\':\n        training_data = NV(\n            opt.video_path,\n            opt.annotation_path,\n            subset,\n            spatial_transform=spatial_transform,\n            temporal_transform=temporal_transform,\n            target_transform=target_transform,\n            sample_duration=opt.sample_duration,\n            modality=opt.modality)\n    return training_data\n\n\ndef get_validation_set(opt, spatial_transform, temporal_transform,\n                       target_transform):\n    assert opt.dataset in [\'kinetics\', \'jester\', \'ucf101\', \'egogesture\', \'nvgesture\']\n\n    if opt.dataset == \'kinetics\':\n        validation_data = Kinetics(\n            opt.video_path,\n            opt.annotation_path,\n            \'validation\',\n            opt.n_val_samples,\n            spatial_transform,\n            temporal_transform,\n            target_transform,\n            sample_duration=opt.sample_duration)\n    elif opt.dataset == \'jester\':\n        validation_data = Jester(\n            opt.video_path,\n            opt.annotation_path,\n            \'validation\',\n            opt.n_val_samples,\n            spatial_transform,\n            temporal_transform,\n            target_transform,\n            sample_duration=opt.sample_duration)\n    elif opt.dataset == \'ucf101\':\n        validation_data = UCF101(\n            opt.video_path,\n            opt.annotation_path,\n            \'validation\',\n            opt.n_val_samples,\n            spatial_transform,\n            temporal_transform,\n            target_transform,\n            sample_duration=opt.sample_duration)\n    elif opt.dataset == \'egogesture\':\n        validation_data = EgoGesture(\n            opt.video_path,\n            opt.annotation_path,\n            \'testing\',\n            opt.n_val_samples,\n            spatial_transform,\n            temporal_transform,\n            target_transform,\n            modality=opt.modality,\n            sample_duration=opt.sample_duration)\n    elif opt.dataset == \'nvgesture\':\n        validation_data = NV(\n            opt.video_path,\n            opt.annotation_path,\n            \'validation\',\n            spatial_transform=spatial_transform,\n            temporal_transform=temporal_transform,\n            target_transform=target_transform,\n            sample_duration=opt.sample_duration,\n            modality=opt.modality)\n    return validation_data\n\n\ndef get_test_set(opt, spatial_transform, temporal_transform, target_transform):\n    assert opt.dataset in [\'kinetics\', \'jester\', \'ucf101\', \'egogesture\', \'nvgesture\']\n    assert opt.test_subset in [\'val\', \'test\']\n\n    if opt.test_subset == \'val\':\n        subset = \'validation\'\n    elif opt.test_subset == \'test\':\n        subset = \'testing\'\n    if opt.dataset == \'kinetics\':\n        test_data = Kinetics(\n            opt.video_path,\n            opt.annotation_path,\n            subset,\n            0,\n            spatial_transform,\n            temporal_transform,\n            target_transform,\n            sample_duration=opt.sample_duration)\n    elif opt.dataset == \'jester\':\n        test_data = Jester(\n            opt.video_path,\n            opt.annotation_path,\n            subset,\n            0,\n            spatial_transform,\n            temporal_transform,\n            target_transform,\n            sample_duration=opt.sample_duration)\n    elif opt.dataset == \'ucf101\':\n        test_data = UCF101(\n            opt.video_path,\n            opt.annotation_path,\n            subset,\n            0,\n            spatial_transform,\n            temporal_transform,\n            target_transform,\n            sample_duration=opt.sample_duration)\n    elif opt.dataset == \'egogesture\':\n        test_data = EgoGesture(\n            opt.video_path,\n            opt.annotation_path,\n            subset,\n            opt.n_val_samples,\n            spatial_transform,\n            temporal_transform,\n            target_transform,\n            modality=opt.modality,\n            sample_duration=opt.sample_duration)\n    elif opt.dataset == \'nvgesture\':\n        test_data = NV(\n            opt.video_path,\n            opt.annotation_path,\n            \'validation\',\n            spatial_transform=spatial_transform,\n            temporal_transform=temporal_transform,\n            target_transform=target_transform,\n            sample_duration=opt.sample_duration,\n            modality=opt.modality)\n    return test_data\n\ndef get_online_data(opt, spatial_transform, temporal_transform, target_transform):\n    assert opt.dataset in [ \'egogesture\', \'nvgesture\']\n    whole_path = opt.whole_path\n    if opt.dataset == \'egogesture\':\n        online_data = EgoGestureOnline(\n            opt.annotation_path,  \n            opt.video_path,\n            opt.whole_path,  \n            opt.n_val_samples,\n            spatial_transform,\n            temporal_transform,\n            target_transform,\n            modality=""RGB-D"",\n            stride_len = opt.stride_len,\n            sample_duration=opt.sample_duration)\n    if opt.dataset == \'nvgesture\':\n        online_data = NVOnline(\n            opt.annotation_path,  \n            opt.video_path,\n            opt.whole_path,  \n            opt.n_val_samples,\n            spatial_transform,\n            temporal_transform,\n            target_transform,\n            modality=""RGB-D"",\n            stride_len = opt.stride_len,\n            sample_duration=opt.sample_duration)\n    \n    return online_data\n'"
inference.py,7,"b'import argparse\nimport time\nimport os\nimport sys\nimport json\nimport shutil\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\nfrom sklearn.metrics import confusion_matrix\nfrom torch.nn import functional as F\n\nfrom opts import parse_opts\nfrom model import generate_model\nfrom dataset import get_training_set, get_validation_set, get_test_set\nfrom mean import get_mean, get_std\nfrom spatial_transforms import *\nfrom temporal_transforms import *\nfrom target_transforms import ClassLabel, VideoID\nfrom target_transforms import Compose as TargetCompose\nfrom dataset import get_training_set, get_validation_set, get_test_set\nfrom utils import Logger\nfrom train import train_epoch\nfrom validation import val_epoch\nimport test\nfrom utils import AverageMeter\n\n\n""""""\ndef calculate_accuracy(outputs, targets, topk=(1,)):\n    maxk = max(topk)\n    batch_size = targets.size(0)\n\n    _, pred = outputs.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(targets.view(1, -1).expand_as(pred))\n    ret = []\n    for k in topk:\n        correct_k = correct[:k].float().sum().data[0]\n        ret.append(correct_k / batch_size)\n\n    return ret\n""""""\n\n\n\ndef calculate_accuracy(outputs, targets, topk=(1,)):\n    maxk = max(topk)\n    batch_size = targets.size(0)\n\n    _, pred = outputs.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(targets.view(1, -1).expand_as(pred))\n    ret = []\n    for k in topk:\n        correct_k = correct[:k].float().sum().data[0]\n        ret.append(correct_k / batch_size)\n\n    return ret\n\n\nopt = parse_opts()\nif opt.root_path != \'\':\n    opt.video_path = os.path.join(opt.root_path, opt.video_path)\n    opt.annotation_path = os.path.join(opt.root_path, opt.annotation_path)\n    opt.result_path = os.path.join(opt.root_path, opt.result_path)\n    if opt.resume_path:\n        opt.resume_path = os.path.join(opt.root_path, opt.resume_path)\n    if opt.pretrain_path:\n        opt.pretrain_path = os.path.join(opt.root_path, opt.pretrain_path)\nopt.scales = [opt.initial_scale]\nfor i in range(1, opt.n_scales):\n    opt.scales.append(opt.scales[-1] * opt.scale_step)\nopt.arch = \'{}-{}\'.format(opt.model, opt.model_depth)\nopt.mean = get_mean(opt.norm_value, dataset=opt.mean_dataset)\nopt.std = get_std(opt.norm_value)\n\nprint(opt)\nwith open(os.path.join(opt.result_path, \'opts.json\'), \'w\') as opt_file:\n    json.dump(vars(opt), opt_file)\n\ntorch.manual_seed(opt.manual_seed)\n\nmodel, parameters = generate_model(opt)\nprint(model)\npytorch_total_params = sum(p.numel() for p in model.parameters() if\n                           p.requires_grad)\nprint(""Total number of trainable parameters: "", pytorch_total_params)\n\nif opt.no_mean_norm and not opt.std_norm:\n    norm_method = Normalize([0, 0, 0], [1, 1, 1])\nelif not opt.std_norm:\n    norm_method = Normalize(opt.mean, [1, 1, 1])\nelse:\n    norm_method = Normalize(opt.mean, opt.std)\n\n\nspatial_transform = Compose([\n    #Scale(opt.sample_size),\n    Scale(112),\n    CenterCrop(112),\n    ToTensor(opt.norm_value), norm_method\n    ])\ntemporal_transform = TemporalCenterCrop(opt.sample_duration)\n#temporal_transform = TemporalBeginCrop(opt.sample_duration)\n#temporal_transform = TemporalEndCrop(opt.sample_duration)\ntarget_transform = ClassLabel()\nvalidation_data = get_validation_set(\n    opt, spatial_transform, temporal_transform, target_transform)\ndata_loader = torch.utils.data.DataLoader(\n    validation_data,\n    batch_size=1,\n    shuffle=False,\n    num_workers=opt.n_threads,\n    pin_memory=True)\nval_logger = Logger(os.path.join(opt.result_path, \'val.log\'), [\'epoch\', \'loss\', \'acc\'])\n\nif opt.resume_path:\n    print(\'loading checkpoint {}\'.format(opt.resume_path))\n    checkpoint = torch.load(opt.resume_path)\n    assert opt.arch == checkpoint[\'arch\']\n\n    opt.begin_epoch = checkpoint[\'epoch\']\n    model.load_state_dict(checkpoint[\'state_dict\'])\n\nrecorder = []\n\nprint(\'run\')\n\nmodel.eval()\n\nbatch_time = AverageMeter()\ntop1 = AverageMeter()\ntop5 = AverageMeter()\n\nend_time = time.time()\nfor i, (inputs, targets) in enumerate(data_loader):\n    if not opt.no_cuda:\n        targets = targets.cuda(async=True)\n    #inputs = Variable(torch.squeeze(inputs), volatile=True)\n    inputs = Variable(inputs, volatile=True)\n    targets = Variable(targets, volatile=True)\n    outputs = model(inputs)\n    recorder.append(outputs.data.cpu().numpy().copy())\n    #outputs = torch.unsqueeze(torch.mean(outputs, 0), 0)\n    prec1, prec5 = calculate_accuracy(outputs, targets, topk=(1, 5))\n\n    top1.update(prec1, inputs.size(0))\n    top5.update(prec5, inputs.size(0))\n\n    batch_time.update(time.time() - end_time)\n    end_time = time.time()\n\n    print(\'[{0}/{1}]\\t\'\n          \'Time {batch_time.val:.5f} ({batch_time.avg:.5f})\\t\'\n          \'prec@1 {top1.avg:.5f} prec@5 {top5.avg:.5f}\'.format(\n              i + 1,\n              len(data_loader),\n              batch_time=batch_time,\n              top1 =top1,\n              top5=top5))\n\nvideo_pred = [np.argmax(np.mean(x, axis=0)) for x in recorder]\nprint(video_pred)\n\nwith open(\'annotation_Something/categories.txt\') as f:\n    lines = f.readlines()\n    categories = [item.rstrip() for item in lines]\n\nname_list = [x.strip().split()[0] for x in open(\'annotation_Something/testlist01.txt\')]\norder_dict = {e:i for i, e in enumerate(sorted(name_list))}\nreorder_output = [None] * len(recorder)\nreorder_pred = [None] * len(recorder)\noutput_csv = []\nfor i in range(len(recorder)):\n    idx = order_dict[name_list[i]]\n    reorder_output[idx] = recorder[i]\n    reorder_pred[idx] = video_pred[i]\n    output_csv.append(\'%s;%s\'%(name_list[i],\n                               categories[video_pred[i]]))\n\n    with open(\'something_predictions.csv\',\'w\') as f:\n        f.write(\'\\n\'.join(output_csv))\n\n\n\nprint(\'-----Evaluation is finished------\')\nprint(\'Overall Prec@1 {:.05f}% Prec@5 {:.05f}%\'.format(top1.avg, top5.avg))\n'"
main.py,8,"b'import os\nimport sys\nimport json\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.optim import lr_scheduler\n\nfrom opts import parse_opts\nfrom model import generate_model\nfrom mean import get_mean, get_std\nfrom spatial_transforms import *\nfrom temporal_transforms import *\nfrom target_transforms import ClassLabel, VideoID\nfrom target_transforms import Compose as TargetCompose\nfrom dataset import get_training_set, get_validation_set, get_test_set\nfrom utils import *\nfrom train import train_epoch\nfrom validation import val_epoch\nimport test\n\n\n\nif __name__ == \'__main__\':\n    opt = parse_opts()\n    if opt.root_path != \'\':\n        opt.video_path = os.path.join(opt.root_path, opt.video_path)\n        opt.annotation_path = os.path.join(opt.root_path, opt.annotation_path)\n        opt.result_path = os.path.join(opt.root_path, opt.result_path)\n        if not os.path.exists(opt.result_path):\n            os.makedirs(opt.result_path)\n        if opt.resume_path:\n            opt.resume_path = os.path.join(opt.root_path, opt.resume_path)\n        if opt.pretrain_path:\n            opt.pretrain_path = os.path.join(opt.root_path, opt.pretrain_path)\n    opt.scales = [opt.initial_scale]\n    for i in range(1, opt.n_scales):\n        opt.scales.append(opt.scales[-1] * opt.scale_step)\n    opt.arch = \'{}\'.format(opt.model)\n    opt.mean = get_mean(opt.norm_value, dataset=opt.mean_dataset)\n    opt.std = get_std(opt.norm_value)\n    opt.store_name = \'_\'.join([opt.dataset, opt.model, str(opt.width_mult) + \'x\',\n                               opt.modality, str(opt.sample_duration)])\n    print(opt)\n    with open(os.path.join(opt.result_path, \'opts.json\'), \'w\') as opt_file:\n        json.dump(vars(opt), opt_file)\n\n    torch.manual_seed(opt.manual_seed)\n\n    model, parameters = generate_model(opt)\n    print(model)\n\n    # Egogesture, with ""no-gesture"" training, weighted loss\n    # class_weights = torch.cat((0.012*torch.ones([1, 83]), 0.00015*torch.ones([1, 1])), 1)\n    criterion = nn.CrossEntropyLoss()\n\n    # # nvgesture, with ""no-gesture"" training, weighted loss\n    # class_weights = torch.cat((0.04*torch.ones([1, 25]), 0.0008*torch.ones([1, 1])), 1)\n    # criterion = nn.CrossEntropyLoss(weight=class_weights, size_average=False)\n\n    # criterion = nn.CrossEntropyLoss()\n    if not opt.no_cuda:\n        criterion = criterion.cuda()\n\n    if opt.no_mean_norm and not opt.std_norm:\n        norm_method = Normalize([0, 0, 0], [1, 1, 1])\n    elif not opt.std_norm:\n        norm_method = Normalize(opt.mean, [1, 1, 1])\n    else:\n        norm_method = Normalize(opt.mean, opt.std)\n\n    if not opt.no_train:\n        assert opt.train_crop in [\'random\', \'corner\', \'center\']\n        if opt.train_crop == \'random\':\n            crop_method = MultiScaleRandomCrop(opt.scales, opt.sample_size)\n        elif opt.train_crop == \'corner\':\n            crop_method = MultiScaleCornerCrop(opt.scales, opt.sample_size)\n        elif opt.train_crop == \'center\':\n            crop_method = MultiScaleCornerCrop(\n                opt.scales, opt.sample_size, crop_positions=[\'c\'])\n        spatial_transform = Compose([\n            #RandomHorizontalFlip(),\n            #RandomRotate(),\n            #RandomResize(),\n            crop_method,\n            #MultiplyValues(),\n            #Dropout(),\n            #SaltImage(),\n            #Gaussian_blur(),\n            #SpatialElasticDisplacement(),\n            ToTensor(opt.norm_value), norm_method\n        ])\n        temporal_transform = TemporalRandomCrop(opt.sample_duration, opt.downsample)\n        target_transform = ClassLabel()\n        training_data = get_training_set(opt, spatial_transform,\n                                         temporal_transform, target_transform)\n        train_loader = torch.utils.data.DataLoader(\n            training_data,\n            batch_size=opt.batch_size,\n            shuffle=True,\n            num_workers=opt.n_threads,\n            pin_memory=True)\n        train_logger = Logger(\n            os.path.join(opt.result_path, opt.store_name + \'_train.log\'),\n            [\'epoch\', \'loss\', \'prec1\', \'prec5\', \'lr\'])\n        train_batch_logger = Logger(\n            os.path.join(opt.result_path, \'train_batch.log\'),\n            [\'epoch\', \'batch\', \'iter\', \'loss\', \'prec1\', \'prec5\', \'lr\'])\n\n        if opt.nesterov:\n            dampening = 0\n        else:\n            dampening = opt.dampening\n        optimizer = optim.SGD(\n            parameters,\n            lr=opt.learning_rate,\n            momentum=opt.momentum,\n            dampening=dampening,\n            weight_decay=opt.weight_decay,\n            nesterov=opt.nesterov)\n        scheduler = lr_scheduler.ReduceLROnPlateau(\n            optimizer, \'min\', patience=opt.lr_patience)\n    if not opt.no_val:\n        spatial_transform = Compose([\n            Scale(opt.sample_size),\n            CenterCrop(opt.sample_size),\n            ToTensor(opt.norm_value), norm_method\n        ])\n        #temporal_transform = LoopPadding(opt.sample_duration)\n        temporal_transform = TemporalCenterCrop(opt.sample_duration, opt.downsample)\n        target_transform = ClassLabel()\n        validation_data = get_validation_set(\n            opt, spatial_transform, temporal_transform, target_transform)\n        val_loader = torch.utils.data.DataLoader(\n            validation_data,\n            batch_size=8,\n            shuffle=False,\n            num_workers=opt.n_threads,\n            pin_memory=True)\n        val_logger = Logger(\n            os.path.join(opt.result_path, opt.store_name + \'_val.log\'), [\'epoch\', \'loss\', \'prec1\', \'prec5\'])\n\n    best_prec1 = 0\n    if opt.resume_path:\n        print(\'loading checkpoint {}\'.format(opt.resume_path))\n        checkpoint = torch.load(opt.resume_path)\n        assert opt.arch == checkpoint[\'arch\']\n        best_prec1 = checkpoint[\'best_prec1\']\n        opt.begin_epoch = checkpoint[\'epoch\']\n        model.load_state_dict(checkpoint[\'state_dict\'])\n\n\n    print(\'run\')\n    for i in range(opt.begin_epoch, opt.n_epochs + 1):\n    # for i in range(opt.begin_epoch, opt.begin_epoch + 10):\n        if not opt.no_train:\n            adjust_learning_rate(optimizer, i, opt)\n            train_epoch(i, train_loader, model, criterion, optimizer, opt,\n                        train_logger, train_batch_logger)\n            state = {\n                \'epoch\': i,\n                \'arch\': opt.arch,\n                \'state_dict\': model.state_dict(),\n                \'optimizer\': optimizer.state_dict(),\n                \'best_prec1\': best_prec1\n                }\n            save_checkpoint(state, False, opt)\n            \n        if not opt.no_val:\n            validation_loss, prec1 = val_epoch(i, val_loader, model, criterion, opt,\n                                        val_logger)\n            is_best = prec1 > best_prec1\n            best_prec1 = max(prec1, best_prec1)\n            state = {\n                \'epoch\': i,\n                \'arch\': opt.arch,\n                \'state_dict\': model.state_dict(),\n                \'optimizer\': optimizer.state_dict(),\n                \'best_prec1\': best_prec1\n                }\n            save_checkpoint(state, is_best, opt)\n\n\n    if opt.test:\n        spatial_transform = Compose([\n            Scale(int(opt.sample_size / opt.scale_in_test)),\n            CornerCrop(opt.sample_size, opt.crop_position_in_test),\n            ToTensor(opt.norm_value), norm_method\n        ])\n        # temporal_transform = LoopPadding(opt.sample_duration, opt.downsample)\n        temporal_transform = TemporalRandomCrop(opt.sample_duration, opt.downsample)\n        target_transform = VideoID()\n\n        test_data = get_test_set(opt, spatial_transform, temporal_transform,\n                                 target_transform)\n        test_loader = torch.utils.data.DataLoader(\n            test_data,\n            batch_size=40,\n            shuffle=False,\n            num_workers=opt.n_threads,\n            pin_memory=True)\n        test.test(test_loader, model, opt, test_data.class_names)\n\n\n\n\n'"
mean.py,0,"b""def get_mean(norm_value=255, dataset='activitynet'):\n    assert dataset in ['activitynet', 'kinetics']\n\n    if dataset == 'activitynet':\n        return [\n            114.7748 / norm_value, 107.7354 / norm_value, 99.4750 / norm_value\n        ]\n    elif dataset == 'kinetics':\n        # Kinetics (10 videos for each class)\n        return [\n            110.63666788 / norm_value, 103.16065604 / norm_value,\n            96.29023126 / norm_value\n        ]\n\n\ndef get_std(norm_value=255):\n    # Kinetics (10 videos for each class)\n    return [\n        38.7568578 / norm_value, 37.88248729 / norm_value,\n        40.02898126 / norm_value\n    ]\n"""
model.py,3,"b'import torch\nfrom torch import nn\n\nfrom models import c3d, squeezenet, mobilenet, shufflenet, mobilenetv2, shufflenetv2, resnext, resnet, resnetl\nimport pdb\n\ndef generate_model(opt):\n    assert opt.model in [\'c3d\', \'squeezenet\', \'mobilenet\', \'resnext\', \'resnet\', \'resnetl\',\n                         \'shufflenet\', \'mobilenetv2\', \'shufflenetv2\']\n\n\n    if opt.model == \'c3d\':\n        from models.c3d import get_fine_tuning_parameters\n        model = c3d.get_model(\n            num_classes=opt.n_classes,\n            sample_size=opt.sample_size,\n            sample_duration=opt.sample_duration)\n    elif opt.model == \'squeezenet\':\n        from models.squeezenet import get_fine_tuning_parameters\n        model = squeezenet.get_model(\n            version=opt.version,\n            num_classes=opt.n_classes,\n            sample_size=opt.sample_size,\n            sample_duration=opt.sample_duration)\n    elif opt.model == \'shufflenet\':\n        from models.shufflenet import get_fine_tuning_parameters\n        model = shufflenet.get_model(\n            groups=opt.groups,\n            width_mult=opt.width_mult,\n            num_classes=opt.n_classes)\n    elif opt.model == \'shufflenetv2\':\n        from models.shufflenetv2 import get_fine_tuning_parameters\n        model = shufflenetv2.get_model(\n            num_classes=opt.n_classes,\n            sample_size=opt.sample_size,\n            width_mult=opt.width_mult)\n    elif opt.model == \'mobilenet\':\n        from models.mobilenet import get_fine_tuning_parameters\n        model = mobilenet.get_model(\n            num_classes=opt.n_classes,\n            sample_size=opt.sample_size,\n            width_mult=opt.width_mult)\n    elif opt.model == \'mobilenetv2\':\n        from models.mobilenetv2 import get_fine_tuning_parameters\n        model = mobilenetv2.get_model(\n            num_classes=opt.n_classes,\n            sample_size=opt.sample_size,\n            width_mult=opt.width_mult)\n    elif opt.model == \'resnext\':\n        assert opt.model_depth in [50, 101, 152]\n        from models.resnext import get_fine_tuning_parameters\n        if opt.model_depth == 50:\n            model = resnext.resnext50(\n                num_classes=opt.n_classes,\n                shortcut_type=opt.resnet_shortcut,\n                cardinality=opt.resnext_cardinality,\n                sample_size=opt.sample_size,\n                sample_duration=opt.sample_duration)\n        elif opt.model_depth == 101:\n            model = resnext.resnext101(\n                num_classes=opt.n_classes,\n                shortcut_type=opt.resnet_shortcut,\n                cardinality=opt.resnext_cardinality,\n                sample_size=opt.sample_size,\n                sample_duration=opt.sample_duration)\n        elif opt.model_depth == 152:\n            model = resnext.resnext152(\n                num_classes=opt.n_classes,\n                shortcut_type=opt.resnet_shortcut,\n                cardinality=opt.resnext_cardinality,\n                sample_size=opt.sample_size,\n                sample_duration=opt.sample_duration)\n    elif opt.model == \'resnetl\':\n        assert opt.model_depth in [10]\n\n        from models.resnetl import get_fine_tuning_parameters\n\n        if opt.model_depth == 10:\n            model = resnetl.resnetl10(\n                num_classes=opt.n_classes,\n                shortcut_type=opt.resnet_shortcut,\n                sample_size=opt.sample_size,\n                sample_duration=opt.sample_duration)\n    elif opt.model == \'resnet\':\n        assert opt.model_depth in [10, 18, 34, 50, 101, 152, 200]\n        from models.resnet import get_fine_tuning_parameters\n        if opt.model_depth == 10:\n            model = resnet.resnet10(\n                num_classes=opt.n_classes,\n                shortcut_type=opt.resnet_shortcut,\n                sample_size=opt.sample_size,\n                sample_duration=opt.sample_duration)\n        elif opt.model_depth == 18:\n            model = resnet.resnet18(\n                num_classes=opt.n_classes,\n                shortcut_type=opt.resnet_shortcut,\n                sample_size=opt.sample_size,\n                sample_duration=opt.sample_duration)\n        elif opt.model_depth == 34:\n            model = resnet.resnet34(\n                num_classes=opt.n_classes,\n                shortcut_type=opt.resnet_shortcut,\n                sample_size=opt.sample_size,\n                sample_duration=opt.sample_duration)\n        elif opt.model_depth == 50:\n            model = resnet.resnet50(\n                num_classes=opt.n_classes,\n                shortcut_type=opt.resnet_shortcut,\n                sample_size=opt.sample_size,\n                sample_duration=opt.sample_duration)\n        elif opt.model_depth == 101:\n            model = resnet.resnet101(\n                num_classes=opt.n_classes,\n                shortcut_type=opt.resnet_shortcut,\n                sample_size=opt.sample_size,\n                sample_duration=opt.sample_duration)\n        elif opt.model_depth == 152:\n            model = resnet.resnet152(\n                num_classes=opt.n_classes,\n                shortcut_type=opt.resnet_shortcut,\n                sample_size=opt.sample_size,\n                sample_duration=opt.sample_duration)\n        elif opt.model_depth == 200:\n            model = resnet.resnet200(\n                num_classes=opt.n_classes,\n                shortcut_type=opt.resnet_shortcut,\n                sample_size=opt.sample_size,\n                sample_duration=opt.sample_duration)\n\n\n\n    if not opt.no_cuda:\n        model = model.cuda()\n        model = nn.DataParallel(model, device_ids=None)\n        pytorch_total_params = sum(p.numel() for p in model.parameters() if\n                               p.requires_grad)\n        print(""Total number of trainable parameters: "", pytorch_total_params)\n\n        if opt.pretrain_path:\n            print(\'loading pretrained model {}\'.format(opt.pretrain_path))\n            pretrain = torch.load(opt.pretrain_path, map_location=torch.device(\'cpu\'))\n            # print(opt.arch)\n            # print(pretrain[\'arch\'])\n            # assert opt.arch == pretrain[\'arch\']\n            model = modify_kernels(opt, model, opt.pretrain_modality)\n            model.load_state_dict(pretrain[\'state_dict\'])\n            \n\n            if opt.model in  [\'mobilenet\', \'mobilenetv2\', \'shufflenet\', \'shufflenetv2\']:\n                model.module.classifier = nn.Sequential(\n                                nn.Dropout(0.5),\n                                nn.Linear(model.module.classifier[1].in_features, opt.n_finetune_classes))\n                model.module.classifier = model.module.classifier.cuda()\n            elif opt.model == \'squeezenet\':\n                model.module.classifier = nn.Sequential(\n                                nn.Dropout(p=0.5),\n                                nn.Conv3d(model.module.classifier[1].in_channels, opt.n_finetune_classes, kernel_size=1),\n                                nn.ReLU(inplace=True),\n                                nn.AvgPool3d((1,4,4), stride=1))\n                model.module.classifier = model.module.classifier.cuda()\n            else:\n                model.module.fc = nn.Linear(model.module.fc.in_features, opt.n_finetune_classes)\n                model.module.fc = model.module.fc.cuda()\n\n            model = modify_kernels(opt, model, opt.modality)\n        else:\n            model = modify_kernels(opt, model, opt.modality)\n\n        parameters = get_fine_tuning_parameters(model, opt.ft_portion)\n        return model, parameters\n    else:\n        if opt.pretrain_path:\n            print(\'loading pretrained model {}\'.format(opt.pretrain_path))\n            pretrain = torch.load(opt.pretrain_path)\n\n            model = modify_kernels(opt, model, opt.pretrain_modality)\n            model.load_state_dict(pretrain[\'state_dict\'])\n\n            \n\n            if opt.model in  [\'mobilenet\', \'mobilenetv2\', \'shufflenet\', \'shufflenetv2\']:\n                model.module.classifier = nn.Sequential(\n                                nn.Dropout(0.9),\n                                nn.Linear(model.module.classifier[1].in_features, opt.n_finetune_classes)\n                                )\n            elif opt.model == \'squeezenet\':\n                model.module.classifier = nn.Sequential(\n                                nn.Dropout(p=0.5),\n                                nn.Conv3d(model.module.classifier[1].in_channels, opt.n_finetune_classes, kernel_size=1),\n                                nn.ReLU(inplace=True),\n                                nn.AvgPool3d((1,4,4), stride=1))\n            else:\n                model.module.fc = nn.Linear(model.module.fc.in_features, opt.n_finetune_classes)\n\n            model = modify_kernels(opt, model, opt.modality)\n            parameters = get_fine_tuning_parameters(model, opt.ft_begin_index)\n            return model, parameters\n        else:\n            model = modify_kernels(opt, model, opt.modality)\n\n    return model, model.parameters()\n\n\ndef _construct_depth_model(base_model):\n    # modify the first convolution kernels for Depth input\n    modules = list(base_model.modules())\n\n    first_conv_idx = list(filter(lambda x: isinstance(modules[x], nn.Conv3d),\n                                 list(range(len(modules)))))[0]\n    conv_layer = modules[first_conv_idx]\n    container = modules[first_conv_idx - 1]\n\n    # modify parameters, assume the first blob contains the convolution kernels\n    motion_length = 1\n    params = [x.clone() for x in conv_layer.parameters()]\n    kernel_size = params[0].size()\n    new_kernel_size = kernel_size[:1] + (1*motion_length,  ) + kernel_size[2:]\n    new_kernels = params[0].data.mean(dim=1, keepdim=True).expand(new_kernel_size).contiguous()\n\n    new_conv = nn.Conv3d(1, conv_layer.out_channels, conv_layer.kernel_size, conv_layer.stride,\n                         conv_layer.padding, bias=True if len(params) == 2 else False)\n    new_conv.weight.data = new_kernels\n    if len(params) == 2:\n        new_conv.bias.data = params[1].data # add bias if neccessary\n    layer_name = list(container.state_dict().keys())[0][:-7] # remove .weight suffix to get the layer name\n\n    # replace the first convlution layer\n    setattr(container, layer_name, new_conv)\n\n    return base_model\n\ndef _construct_rgbdepth_model(base_model):\n    # modify the first convolution kernels for RGB-D input\n    modules = list(base_model.modules())\n\n    first_conv_idx = list(filter(lambda x: isinstance(modules[x], nn.Conv3d),\n                           list(range(len(modules)))))[0]\n    conv_layer = modules[first_conv_idx]\n    container = modules[first_conv_idx - 1]\n    # modify parameters, assume the first blob contains the convolution kernels\n    motion_length = 1\n    params = [x.clone() for x in conv_layer.parameters()]\n    kernel_size = params[0].size()\n    new_kernel_size = kernel_size[:1] + (1 * motion_length,) + kernel_size[2:]\n    new_kernels = torch.mul(torch.cat((params[0].data, params[0].data.mean(dim=1,keepdim=True).expand(new_kernel_size).contiguous()), 1), 0.6)\n    new_kernel_size = kernel_size[:1] + (3 + 1 * motion_length,) + kernel_size[2:]\n    new_conv = nn.Conv3d(4, conv_layer.out_channels, conv_layer.kernel_size, conv_layer.stride,\n                         conv_layer.padding, bias=True if len(params) == 2 else False)\n    new_conv.weight.data = new_kernels\n    if len(params) == 2:\n        new_conv.bias.data = params[1].data  # add bias if neccessary\n    layer_name = list(container.state_dict().keys())[0][:-7]  # remove .weight suffix to get the layer name\n\n    # replace the first convolution layer\n    setattr(container, layer_name, new_conv)\n    return base_model\n\ndef _modify_first_conv_layer(base_model, new_kernel_size1, new_filter_num):\n    modules = list(base_model.modules())\n    first_conv_idx = list(filter(lambda x: isinstance(modules[x], nn.Conv3d),\n                                               list(range(len(modules)))))[0]\n    conv_layer = modules[first_conv_idx]\n    container = modules[first_conv_idx - 1]\n \n    new_conv = nn.Conv3d(new_filter_num, conv_layer.out_channels, kernel_size=(new_kernel_size1,7,7),\n                         stride=(1,2,2), padding=(1,3,3), bias=False)\n    layer_name = list(container.state_dict().keys())[0][:-7]\n\n    setattr(container, layer_name, new_conv)\n    return base_model\n\ndef modify_kernels(opt, model, modality):\n    if modality == \'RGB\' and opt.model not in [\'c3d\', \'squeezenet\', \'mobilenet\',\'shufflenet\', \'mobilenetv2\', \'shufflenetv2\']:\n        print(""[INFO]: RGB model is used for init model"")\n        model = _modify_first_conv_layer(model,3,3) ##### Check models trained (3,7,7) or (7,7,7)\n    elif modality == \'Depth\':\n        print(""[INFO]: Converting the pretrained model to Depth init model"")\n        model = _construct_depth_model(model)\n        print(""[INFO]: Done. Flow model ready."")\n    elif modality == \'RGB-D\':\n        print(""[INFO]: Converting the pretrained model to RGB+D init model"")\n        model = _construct_rgbdepth_model(model)\n        print(""[INFO]: Done. RGB-D model ready."")\n    modules = list(model.modules())\n    first_conv_idx = list(filter(lambda x: isinstance(modules[x], nn.Conv3d),\n                                               list(range(len(modules)))))[0]\n    #conv_layer = modules[first_conv_idx]\n    #if conv_layer.kernel_size[0]> opt.sample_duration:\n     #   model = _modify_first_conv_layer(model,int(opt.sample_duration/2),1)\n    return model\n'"
offline_test.py,8,"b'import argparse\nimport time\nimport os\nimport sys\nimport json\nimport shutil\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport itertools\nimport torch\nfrom torch.autograd import Variable\nfrom sklearn.metrics import confusion_matrix\nfrom torch.nn import functional as F\n\nfrom opts import parse_opts\nfrom model import generate_model\nfrom mean import get_mean, get_std\nfrom spatial_transforms import *\nfrom temporal_transforms import *\nfrom target_transforms import ClassLabel, VideoID\nfrom target_transforms import Compose as TargetCompose\nfrom dataset import get_training_set, get_validation_set, get_test_set, get_online_data\nfrom utils import Logger\nfrom train import train_epoch\nfrom validation import val_epoch\nimport test\nfrom utils import AverageMeter, calculate_precision, calculate_recall\nimport pdb\nfrom sklearn.metrics import confusion_matrix\n\ndef plot_cm(cm, classes, normalize = True):\n    import seaborn as sns\n    if normalize:\n        cm = cm.astype(\'float\') / cm.sum(axis=1)[:, np.newaxis]\n        print(""Normalized confusion matrix"")\n    else:\n        print(\'Confusion matrix, without normalization\')\n\n    ax= plt.subplot()\n    sns.heatmap(cm, annot=False, ax = ax); #annot=True to annotate cells\n\n    # labels, title and ticks\n    ax.set_xlabel(\'Predicted labels\');ax.set_ylabel(\'True labels\'); \n    plt.xticks(rotation=\'vertical\')\n    plt.yticks(rotation=\'horizontal\')\n    \n\n\ndef calculate_accuracy(outputs, targets, topk=(1,)):\n    maxk = max(topk)\n    batch_size = targets.size(0)\n    _, pred = outputs.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(targets.view(1, -1).expand_as(pred))\n    ret = []\n    for k in topk:\n        correct_k = correct[:k].float().sum().item()\n        ret.append(correct_k / batch_size)\n\n    return ret\n\n\nopt = parse_opts_offline()\nif opt.root_path != \'\':\n    opt.video_path = os.path.join(opt.root_path, opt.video_path)\n    opt.annotation_path = os.path.join(opt.root_path, opt.annotation_path)\n    opt.result_path = os.path.join(opt.root_path, opt.result_path)\n    if opt.resume_path:\n        opt.resume_path = os.path.join(opt.root_path, opt.resume_path)\n    if opt.pretrain_path:\n        opt.pretrain_path = os.path.join(opt.root_path, opt.pretrain_path)\nopt.scales = [opt.initial_scale]\nfor i in range(1, opt.n_scales):\n    opt.scales.append(opt.scales[-1] * opt.scale_step)\nopt.arch = \'{}-{}\'.format(opt.model, opt.model_depth)\nopt.mean = get_mean(opt.norm_value)\nopt.std = get_std(opt.norm_value)\n\nprint(opt)\nwith open(os.path.join(opt.result_path, \'opts.json\'), \'w\') as opt_file:\n    json.dump(vars(opt), opt_file)\n\ntorch.manual_seed(opt.manual_seed)\n\nmodel, parameters = generate_model(opt)\nprint(model)\npytorch_total_params = sum(p.numel() for p in model.parameters() if\n                           p.requires_grad)\nprint(""Total number of trainable parameters: "", pytorch_total_params)\n\nif opt.no_mean_norm and not opt.std_norm:\n    norm_method = Normalize([0, 0, 0], [1, 1, 1])\nelif not opt.std_norm:\n    norm_method = Normalize(opt.mean, [1, 1, 1])\nelse:\n    norm_method = Normalize(opt.mean, opt.std)\n\n\nspatial_transform = Compose([\n    #Scale(opt.sample_size),\n    Scale(112),\n    CenterCrop(112),\n    ToTensor(opt.norm_value), norm_method\n    ])\ntemporal_transform = TemporalCenterCrop(opt.sample_duration)\n#temporal_transform = TemporalBeginCrop(opt.sample_duration)\n#temporal_transform = TemporalEndCrop(opt.sample_duration)\ntarget_transform = ClassLabel()\ntest_data = get_test_set(\n    opt, spatial_transform, temporal_transform, target_transform)\n\ntest_loader = torch.utils.data.DataLoader(\n            test_data,\n            batch_size=opt.batch_size,\n            shuffle=False,\n            num_workers=opt.n_threads,\n            pin_memory=True)\ntest_logger = Logger(os.path.join(opt.result_path, \'test.log\'),\n [ \'top1\', \'top5\', \'precision\', \'recall\'])\n\n\nif opt.resume_path:\n    print(\'loading checkpoint {}\'.format(opt.resume_path))\n    checkpoint = torch.load(opt.resume_path)\n    assert opt.arch == checkpoint[\'arch\']\n\n    opt.begin_epoch = checkpoint[\'epoch\']\n    model.load_state_dict(checkpoint[\'state_dict\'])\n\n\n#test.test(test_loader, model, opt, test_data.class_names)\n\n\n\nrecorder = []\n\nprint(\'run\')\n\nmodel.eval()\n\nbatch_time = AverageMeter()\ntop1 = AverageMeter()\ntop5 = AverageMeter()\nprecisions = AverageMeter() #\nrecalls = AverageMeter()\n\ny_true = []\ny_pred = []\nend_time = time.time()\nfor i, (inputs, targets) in enumerate(test_loader):\n    if not opt.no_cuda:\n        targets = targets.cuda(async=True)\n    #inputs = Variable(torch.squeeze(inputs), volatile=True)\n    with torch.no_grad():\n        inputs = Variable(inputs)\n        targets = Variable(targets)\n        outputs = model(inputs)\n        if not opt.no_softmax_in_test:\n            outputs = F.softmax(outputs)\n        recorder.append(outputs.data.cpu().numpy().copy())\n    y_true.extend(targets.cpu().numpy().tolist())\n    y_pred.extend(outputs.argmax(1).cpu().numpy().tolist())\n\n    #outputs = torch.unsqueeze(torch.mean(outputs, 0), 0)\n    #pdb.set_trace()\n    # print(outputs.shape, targets.shape)\n    if outputs.size(1) <= 4:\n\n        prec1= calculate_accuracy(outputs, targets, topk=(1,))\n        precision = calculate_precision(outputs, targets) #\n        recall = calculate_recall(outputs,targets)\n\n\n        top1.update(prec1[0], inputs.size(0))\n        precisions.update(precision, inputs.size(0))\n        recalls.update(recall,inputs.size(0))\n\n        batch_time.update(time.time() - end_time)\n        end_time = time.time()\n\n        \n        \n        print(\'[{0}/{1}]\\t\'\n              \'Time {batch_time.val:.5f} ({batch_time.avg:.5f})\\t\'\n              \'prec@1 {top1.avg:.5f} \\t\'\n              \'precision {precision.val:.5f} ({precision.avg:.5f})\\t\'\n              \'recall {recall.val:.5f} ({recall.avg:.5f})\'.format(\n                  i + 1,\n                  len(test_loader),\n                  batch_time=batch_time,\n                  top1 =top1,\n                  precision = precisions,\n                  recall = recalls))\n    else:\n\n        prec1, prec5 = calculate_accuracy(outputs, targets, topk=(1,5))\n        precision = calculate_precision(outputs, targets) #\n        recall = calculate_recall(outputs,targets)\n\n\n        top1.update(prec1, inputs.size(0))\n        top5.update(prec5, inputs.size(0))\n        precisions.update(precision, inputs.size(0))\n        recalls.update(recall,inputs.size(0))\n\n        batch_time.update(time.time() - end_time)\n        end_time = time.time()\n        print(\'[{0}/{1}]\\t\'\n              \'Time {batch_time.val:.5f} ({batch_time.avg:.5f})\\t\'\n              \'prec@1 {top1.avg:.5f} prec@5 {top5.avg:.5f}\\t\'\n              \'precision {precision.val:.5f} ({precision.avg:.5f})\\t\'\n              \'recall {recall.val:.5f} ({recall.avg:.5f})\'.format(\n                  i + 1,\n                  len(test_loader),\n                  batch_time=batch_time,\n                  top1 =top1,\n                  top5=top5,\n                  precision = precisions,\n                  recall = recalls))\ntest_logger.log({\n        \'top1\': top1.avg,\n        \'top5\': top5.avg,\n        \'precision\':precisions.avg,\n        \'recall\':recalls.avg\n    })\n\nprint(\'-----Evaluation is finished------\')\nprint(\'Overall Prec@1 {:.05f}% Prec@5 {:.05f}%\'.format(top1.avg, top5.avg))\n'"
online_test.py,9,"b'import os\nimport glob\nimport json\nimport pandas as pd\nimport csv\nimport torch\nfrom torch.autograd import Variable\nfrom torch.nn import functional as F\n\nfrom opts import parse_opts_online\nfrom model import generate_model\nfrom mean import get_mean, get_std\nfrom spatial_transforms import *\nfrom temporal_transforms import *\nfrom target_transforms import ClassLabel\nfrom dataset import get_online_data\nfrom utils import  AverageMeter, LevenshteinDistance, Queue\n\nimport pdb\nimport numpy as np\nimport datetime\n\n\ndef weighting_func(x):\n    return (1 / (1 + np.exp(-0.2 * (x - 9))))\n\n\nopt = parse_opts_online()\n\n\ndef load_models(opt):\n    opt.resume_path = opt.resume_path_det\n    opt.pretrain_path = opt.pretrain_path_det\n    opt.sample_duration = opt.sample_duration_det\n    opt.model = opt.model_det\n    opt.model_depth = opt.model_depth_det\n    opt.width_mult = opt.width_mult_det\n    opt.modality = opt.modality_det\n    opt.resnet_shortcut = opt.resnet_shortcut_det\n    opt.n_classes = opt.n_classes_det\n    opt.n_finetune_classes = opt.n_finetune_classes_det\n\n    if opt.root_path != \'\':\n        opt.video_path = os.path.join(opt.root_path, opt.video_path)\n        opt.annotation_path = os.path.join(opt.root_path, opt.annotation_path)\n        opt.result_path = os.path.join(opt.root_path, opt.result_path)\n        if opt.resume_path:\n            opt.resume_path = os.path.join(opt.root_path, opt.resume_path)\n        if opt.pretrain_path:\n            opt.pretrain_path = os.path.join(opt.root_path, opt.pretrain_path)\n\n    opt.scales = [opt.initial_scale]\n    for i in range(1, opt.n_scales):\n        opt.scales.append(opt.scales[-1] * opt.scale_step)\n    opt.arch = \'{}\'.format(opt.model)\n    opt.mean = get_mean(opt.norm_value)\n    opt.std = get_std(opt.norm_value)\n\n    print(opt)\n    with open(os.path.join(opt.result_path, \'opts_det.json\'), \'w\') as opt_file:\n        json.dump(vars(opt), opt_file)\n\n    torch.manual_seed(opt.manual_seed)\n\n    detector, parameters = generate_model(opt)\n\n    if opt.resume_path:\n        opt.resume_path = os.path.join(opt.root_path, opt.resume_path)\n        print(\'loading checkpoint {}\'.format(opt.resume_path))\n        checkpoint = torch.load(opt.resume_path)\n        #assert opt.arch == checkpoint[\'arch\']\n\n        detector.load_state_dict(checkpoint[\'state_dict\'])\n\n    print(\'Model 1 \\n\', detector)\n    pytorch_total_params = sum(p.numel() for p in detector.parameters() if\n                               p.requires_grad)\n    print(""Total number of trainable parameters: "", pytorch_total_params)\n\n    opt.resume_path = opt.resume_path_clf\n    opt.pretrain_path = opt.pretrain_path_clf\n    opt.sample_duration = opt.sample_duration_clf\n    opt.model = opt.model_clf\n    opt.model_depth = opt.model_depth_clf\n    opt.width_mult = opt.width_mult_clf\n    opt.modality = opt.modality_clf\n    opt.resnet_shortcut = opt.resnet_shortcut_clf\n    opt.n_classes = opt.n_classes_clf\n    opt.n_finetune_classes = opt.n_finetune_classes_clf\n    if opt.root_path != \'\':\n        opt.video_path = os.path.join(opt.root_path, opt.video_path)\n        opt.annotation_path = os.path.join(opt.root_path, opt.annotation_path)\n        opt.result_path = os.path.join(opt.root_path, opt.result_path)\n        if opt.resume_path:\n            opt.resume_path = os.path.join(opt.root_path, opt.resume_path)\n        if opt.pretrain_path:\n            opt.pretrain_path = os.path.join(opt.root_path, opt.pretrain_path)\n\n    opt.scales = [opt.initial_scale]\n    for i in range(1, opt.n_scales):\n        opt.scales.append(opt.scales[-1] * opt.scale_step)\n    opt.arch = \'{}\'.format(opt.model)\n    opt.mean = get_mean(opt.norm_value)\n    opt.std = get_std(opt.norm_value)\n\n    print(opt)\n    with open(os.path.join(opt.result_path, \'opts_clf.json\'), \'w\') as opt_file:\n        json.dump(vars(opt), opt_file)\n\n    torch.manual_seed(opt.manual_seed)\n    classifier, parameters = generate_model(opt)\n\n    if opt.resume_path:\n        print(\'loading checkpoint {}\'.format(opt.resume_path))\n        checkpoint = torch.load(opt.resume_path)\n#        assert opt.arch == checkpoint[\'arch\']\n\n        classifier.load_state_dict(checkpoint[\'state_dict\'])\n\n    print(\'Model 2 \\n\', classifier)\n    pytorch_total_params = sum(p.numel() for p in classifier.parameters() if\n                               p.requires_grad)\n    print(""Total number of trainable parameters: "", pytorch_total_params)\n\n    return detector, classifier\n\n\ndetector, classifier = load_models(opt)\n\nif opt.no_mean_norm and not opt.std_norm:\n    norm_method = Normalize([0, 0, 0], [1, 1, 1])\nelif not opt.std_norm:\n    norm_method = Normalize(opt.mean, [1, 1, 1])\nelse:\n    norm_method = Normalize(opt.mean, opt.std)\n\nspatial_transform = Compose([\n    Scale(112),\n    CenterCrop(112),\n    ToTensor(opt.norm_value), norm_method\n])\n\ntarget_transform = ClassLabel()\n\n## Get list of videos to test\nif opt.dataset == \'egogesture\':\n    subject_list = [\'Subject{:02d}\'.format(i) for i in [2, 9, 11, 14, 18, 19, 28, 31, 41, 47]]\n    test_paths = []\n    for subject in subject_list:\n        for x in glob.glob(os.path.join(opt.video_path, subject, \'*/*/rgb*\')):\n            test_paths.append(x)\nelif opt.dataset == \'nvgesture\':\n    df = pd.read_csv(os.path.join(opt.video_path, \'nvgesture_test_correct_cvpr2016_v2.lst\'), delimiter=\' \', header=None)\n    test_paths = []\n    for x in df[0].values:\n        test_paths.append(os.path.join(opt.video_path, x.replace(\'path:\', \'\'), \'sk_color_all\'))\n\nprint(\'Start Evaluation\')\ndetector.eval()\nclassifier.eval()\n\nlevenshtein_accuracies = AverageMeter()\nvideoidx = 0\nfor path in test_paths[:]:\n    if opt.dataset == \'egogesture\':\n        opt.whole_path = os.path.join(*path.rsplit(os.sep, 4)[1:])\n    elif opt.dataset == \'nvgesture\':\n        opt.whole_path = os.path.join(*path.rsplit(os.sep, 5)[1:])\n\n    videoidx += 1\n    active_index = 0\n    passive_count = 0\n    active = False\n    prev_active = False\n    finished_prediction = None\n    pre_predict = False\n\n    cum_sum = np.zeros(opt.n_classes_clf, )\n    clf_selected_queue = np.zeros(opt.n_classes_clf, )\n    det_selected_queue = np.zeros(opt.n_classes_det, )\n    myqueue_det = Queue(opt.det_queue_size, n_classes=opt.n_classes_det)\n    myqueue_clf = Queue(opt.clf_queue_size, n_classes=opt.n_classes_clf)\n\n    print(\'[{}/{}]============\'.format(videoidx, len(test_paths)))\n    print(path)\n    opt.sample_duration = max(opt.sample_duration_clf, opt.sample_duration_det)\n    temporal_transform = TemporalRandomCrop(opt.sample_duration, opt.downsample)\n    test_data = get_online_data(\n        opt, spatial_transform, None, target_transform)\n\n    test_loader = torch.utils.data.DataLoader(\n        test_data,\n        batch_size=opt.batch_size,\n        shuffle=False,\n        num_workers=opt.n_threads,\n        pin_memory=True)\n\n    results = []\n    prev_best1 = opt.n_classes_clf\n    dataset_len = len(test_loader.dataset)\n    for i, (inputs, targets) in enumerate(test_loader):\n        if not opt.no_cuda:\n            targets = targets.cuda()\n        ground_truth_array = np.zeros(opt.n_classes_clf + 1, )\n        with torch.no_grad():\n            inputs = Variable(inputs)\n            targets = Variable(targets)\n            if opt.modality_det == \'RGB\':\n                inputs_det = inputs[:, :-1, -opt.sample_duration_det:, :, :]\n            elif opt.modality_det == \'Depth\':\n                inputs_det = inputs[:, -1, -opt.sample_duration_det:, :, :].unsqueeze(1)\n            elif opt.modality_det == \'RGB-D\':\n                inputs_det = inputs[:, :, -opt.sample_duration_det:, :, :]\n\n            outputs_det = detector(inputs_det)\n            outputs_det = F.softmax(outputs_det, dim=1)\n            outputs_det = outputs_det.cpu().numpy()[0].reshape(-1, )\n\n            # enqueue the probabilities to the detector queue\n            myqueue_det.enqueue(outputs_det.tolist())\n\n            if opt.det_strategy == \'raw\':\n                det_selected_queue = outputs_det\n            elif opt.det_strategy == \'median\':\n                det_selected_queue = myqueue_det.median\n            elif opt.det_strategy == \'ma\':\n                det_selected_queue = myqueue_det.ma\n            elif opt.det_strategy == \'ewma\':\n                det_selected_queue = myqueue_det.ewma\n\n            prediction_det = np.argmax(det_selected_queue)\n            prob_det = det_selected_queue[prediction_det]\n\n            #### State of the detector is checked here as detector act as a switch for the classifier\n            if prediction_det == 1:\n                if opt.modality_clf == \'RGB\':\n                    inputs_clf = inputs[:, :-1, :, :, :]\n                elif opt.modality_clf == \'Depth\':\n                    inputs_clf = inputs[:, -1, :, :, :].unsqueeze(1)\n                elif opt.modality_clf == \'RGB-D\':\n                    inputs_clf = inputs[:, :, :, :, :]\n                inputs_clf = torch.Tensor(inputs_clf.numpy()[:,:,::2,:,:])\n                outputs_clf = classifier(inputs_clf)\n                outputs_clf = F.softmax(outputs_clf, dim=1)\n                outputs_clf = outputs_clf.cpu().numpy()[0].reshape(-1, )\n\n                # Push the probabilities to queue\n                myqueue_clf.enqueue(outputs_clf.tolist())\n                passive_count = 0\n\n                if opt.clf_strategy == \'raw\':\n                    clf_selected_queue = outputs_clf\n                elif opt.clf_strategy == \'median\':\n                    clf_selected_queue = myqueue_clf.median\n                elif opt.clf_strategy == \'ma\':\n                    clf_selected_queue = myqueue_clf.ma\n                elif opt.clf_strategy == \'ewma\':\n                    clf_selected_queue = myqueue_clf.ewma\n\n            else:\n                outputs_clf = np.zeros(opt.n_classes_clf, )\n                # Push the probabilities to queue\n                myqueue_clf.enqueue(outputs_clf.tolist())\n                passive_count += 1\n\n        if passive_count >= opt.det_counter or i == (dataset_len -2):\n            active = False\n        else:\n            active = True\n\n        # one of the following line need to be commented !!!!\n        if active:\n            active_index += 1\n            cum_sum = ((cum_sum * (active_index - 1)) + (\n                        weighting_func(active_index) * clf_selected_queue)) / active_index  # Weighted Aproach\n            # cum_sum = ((cum_sum * (x-1)) + (1.0 * clf_selected_queue))/x #Not Weighting Aproach\n\n            best2, best1 = tuple(cum_sum.argsort()[-2:][::1])\n            if float(cum_sum[best1] - cum_sum[best2]) > opt.clf_threshold_pre:\n                finished_prediction = True\n                pre_predict = True\n\n        else:\n            active_index = 0\n\n        if active == False and prev_active == True:\n            finished_prediction = True\n        elif active == True and prev_active == False:\n            finished_prediction = False\n\n        if finished_prediction == True:\n            best2, best1 = tuple(cum_sum.argsort()[-2:][::1])\n            if cum_sum[best1] > opt.clf_threshold_final:\n                if pre_predict == True:\n                    if best1 != prev_best1:\n                        if cum_sum[best1] > opt.clf_threshold_final:\n                            results.append(((i * opt.stride_len) + opt.sample_duration_clf, best1))\n                            print(\'Early Detected - class : {} with prob : {} at frame {}\'.format(best1, cum_sum[best1],\n                                                                                                  (\n                                                                                                              i * opt.stride_len) + opt.sample_duration_clf))\n                else:\n                    if cum_sum[best1] > opt.clf_threshold_final:\n                        if best1 == prev_best1:\n                            if cum_sum[best1] > 5:\n                                results.append(((i * opt.stride_len) + opt.sample_duration_clf, best1))\n                                print(\'Late Detected - class : {} with prob : {} at frame {}\'.format(best1,\n                                                                                                     cum_sum[best1], (\n                                                                                                                 i * opt.stride_len) + opt.sample_duration_clf))\n                        else:\n                            results.append(((i * opt.stride_len) + opt.sample_duration_clf, best1))\n\n                            print(\'Late Detected - class : {} with prob : {} at frame {}\'.format(best1, cum_sum[best1],\n                                                                                                 (\n                                                                                                             i * opt.stride_len) + opt.sample_duration_clf))\n\n                finished_prediction = False\n                prev_best1 = best1\n\n            cum_sum = np.zeros(opt.n_classes_clf, )\n\n        if active == False and prev_active == True:\n            pre_predict = False\n\n        prev_active = active\n\n\n    if opt.dataset == \'egogesture\':\n        target_csv_path = os.path.join(opt.video_path,\n                                       \'labels-final-revised1\',\n                                       opt.whole_path.rsplit(os.sep, 2)[0],\n                                       \'Group\' + opt.whole_path[-1] + \'.csv\').replace(\'Subject\', \'subject\')\n        true_classes = []\n        with open(target_csv_path) as csvfile:\n            readCSV = csv.reader(csvfile, delimiter=\',\')\n            for row in readCSV:\n                true_classes.append(int(row[0]) - 1)\n    elif opt.dataset == \'nvgesture\':\n        true_classes = []\n        with open(\'./annotation_nvGesture/vallistall.txt\') as csvfile:\n            readCSV = csv.reader(csvfile, delimiter=\' \')\n            for row in readCSV:\n                if row[0] == opt.whole_path:\n                    if row[1] != \'26\':\n                        true_classes.append(int(row[1]) - 1)\n    if len(results) != 0:\n        predicted = np.array(results)[:, 1]\n    else:\n        predicted = []\n    true_classes = np.array(true_classes)\n    levenshtein_distance = LevenshteinDistance(true_classes, predicted)\n    levenshtein_accuracy = 1 - (levenshtein_distance / len(true_classes))\n    if levenshtein_distance < 0:  # Distance cannot be less than 0\n        levenshtein_accuracies.update(0, len(true_classes))\n    else:\n        levenshtein_accuracies.update(levenshtein_accuracy, len(true_classes))\n\n    print(\'predicted classes: \\t\', predicted)\n    print(\'True classes :\\t\\t\', true_classes)\n    print(\'Levenshtein Accuracy = {} ({})\'.format(levenshtein_accuracies.val, levenshtein_accuracies.avg))\n\nprint(\'Average Levenshtein Accuracy= {}\'.format(levenshtein_accuracies.avg))\n\nprint(\'-----Evaluation is finished------\')\nwith open(""./results/online-results.log"", ""a"") as myfile:\n    myfile.write(""{}, {}, {}, {}, {}, {}"".format(datetime.datetime.now(),\n                                    opt.resume_path_clf, \n                                    opt.model_clf,\n                                    opt.width_mult_clf,\n                                    opt.modality_clf,\n                                    levenshtein_accuracies.avg))'"
online_test_video.py,10,"b'import os\r\nimport glob\r\nimport json\r\nimport pandas as pd\r\nimport numpy as np\r\nimport csv\r\nimport torch\r\nimport time\r\nfrom torch.autograd import Variable\r\nfrom PIL import Image\r\nimport cv2\r\nfrom torch.nn import functional as F\r\n\r\nfrom opts import parse_opts_online\r\nfrom model import generate_model\r\nfrom mean import get_mean, get_std\r\nfrom spatial_transforms import *\r\nfrom temporal_transforms import *\r\nfrom target_transforms import ClassLabel\r\nfrom dataset import get_online_data\r\nfrom utils import  AverageMeter, LevenshteinDistance, Queue\r\n\r\nimport pdb\r\nimport numpy as np\r\nimport datetime\r\n\r\n\r\n###Pretrained RGB models\r\n##Google Drive\r\n#https://drive.google.com/file/d/1V23zvjAKZr7FUOBLpgPZkpHGv8_D-cOs/view?usp=sharing\r\n##Baidu Netdisk\r\n#https://pan.baidu.com/s/114WKw0lxLfWMZA6SYSSJlw code:p1va\r\n\r\ndef weighting_func(x):\r\n    return (1 / (1 + np.exp(-0.2 * (x - 9))))\r\n\r\n\r\nopt = parse_opts_online()\r\n\r\n\r\ndef load_models(opt):\r\n    opt.resume_path = opt.resume_path_det\r\n    opt.pretrain_path = opt.pretrain_path_det\r\n    opt.sample_duration = opt.sample_duration_det\r\n    opt.model = opt.model_det\r\n    opt.model_depth = opt.model_depth_det\r\n    opt.width_mult = opt.width_mult_det\r\n    opt.modality = opt.modality_det\r\n    opt.resnet_shortcut = opt.resnet_shortcut_det\r\n    opt.n_classes = opt.n_classes_det\r\n    opt.n_finetune_classes = opt.n_finetune_classes_det\r\n\r\n    if opt.root_path != \'\':\r\n        opt.video_path = os.path.join(opt.root_path, opt.video_path)\r\n        opt.annotation_path = os.path.join(opt.root_path, opt.annotation_path)\r\n        opt.result_path = os.path.join(opt.root_path, opt.result_path)\r\n        if opt.resume_path:\r\n            opt.resume_path = os.path.join(opt.root_path, opt.resume_path)\r\n        if opt.pretrain_path:\r\n            opt.pretrain_path = os.path.join(opt.root_path, opt.pretrain_path)\r\n\r\n    opt.scales = [opt.initial_scale]\r\n    for i in range(1, opt.n_scales):\r\n        opt.scales.append(opt.scales[-1] * opt.scale_step)\r\n    opt.arch = \'{}\'.format(opt.model)\r\n    opt.mean = get_mean(opt.norm_value)\r\n    opt.std = get_std(opt.norm_value)\r\n\r\n    print(opt)\r\n    with open(os.path.join(opt.result_path, \'opts_det.json\'), \'w\') as opt_file:\r\n        json.dump(vars(opt), opt_file)\r\n\r\n    torch.manual_seed(opt.manual_seed)\r\n\r\n    detector, parameters = generate_model(opt)\r\n    detector = detector.cuda()\r\n    if opt.resume_path:\r\n        opt.resume_path = os.path.join(opt.root_path, opt.resume_path)\r\n        print(\'loading checkpoint {}\'.format(opt.resume_path))\r\n        checkpoint = torch.load(opt.resume_path)\r\n\r\n        detector.load_state_dict(checkpoint[\'state_dict\'])\r\n\r\n    print(\'Model 1 \\n\', detector)\r\n    pytorch_total_params = sum(p.numel() for p in detector.parameters() if\r\n                               p.requires_grad)\r\n    print(""Total number of trainable parameters: "", pytorch_total_params)\r\n\r\n    opt.resume_path = opt.resume_path_clf\r\n    opt.pretrain_path = opt.pretrain_path_clf\r\n    opt.sample_duration = opt.sample_duration_clf\r\n    opt.model = opt.model_clf\r\n    opt.model_depth = opt.model_depth_clf\r\n    opt.width_mult = opt.width_mult_clf\r\n    opt.modality = opt.modality_clf\r\n    opt.resnet_shortcut = opt.resnet_shortcut_clf\r\n    opt.n_classes = opt.n_classes_clf\r\n    opt.n_finetune_classes = opt.n_finetune_classes_clf\r\n    if opt.root_path != \'\':\r\n        opt.video_path = os.path.join(opt.root_path, opt.video_path)\r\n        opt.annotation_path = os.path.join(opt.root_path, opt.annotation_path)\r\n        opt.result_path = os.path.join(opt.root_path, opt.result_path)\r\n        if opt.resume_path:\r\n            opt.resume_path = os.path.join(opt.root_path, opt.resume_path)\r\n        if opt.pretrain_path:\r\n            opt.pretrain_path = os.path.join(opt.root_path, opt.pretrain_path)\r\n\r\n    opt.scales = [opt.initial_scale]\r\n    for i in range(1, opt.n_scales):\r\n        opt.scales.append(opt.scales[-1] * opt.scale_step)\r\n    opt.arch = \'{}\'.format(opt.model)\r\n    opt.mean = get_mean(opt.norm_value)\r\n    opt.std = get_std(opt.norm_value)\r\n\r\n    print(opt)\r\n    with open(os.path.join(opt.result_path, \'opts_clf.json\'), \'w\') as opt_file:\r\n        json.dump(vars(opt), opt_file)\r\n\r\n    torch.manual_seed(opt.manual_seed)\r\n    classifier, parameters = generate_model(opt)\r\n    classifier = classifier.cuda()\r\n    if opt.resume_path:\r\n        print(\'loading checkpoint {}\'.format(opt.resume_path))\r\n        checkpoint = torch.load(opt.resume_path)\r\n\r\n        classifier.load_state_dict(checkpoint[\'state_dict\'])\r\n\r\n    print(\'Model 2 \\n\', classifier)\r\n    pytorch_total_params = sum(p.numel() for p in classifier.parameters() if\r\n                               p.requires_grad)\r\n    print(""Total number of trainable parameters: "", pytorch_total_params)\r\n\r\n    return detector, classifier\r\n\r\n\r\ndetector, classifier = load_models(opt)\r\n\r\nif opt.no_mean_norm and not opt.std_norm:\r\n    norm_method = Normalize([0, 0, 0], [1, 1, 1])\r\nelif not opt.std_norm:\r\n    norm_method = Normalize(opt.mean, [1, 1, 1])\r\nelse:\r\n    norm_method = Normalize(opt.mean, opt.std)\r\n\r\nspatial_transform = Compose([\r\n    Scale(112),\r\n    CenterCrop(112),\r\n    ToTensor(opt.norm_value), norm_method\r\n])\r\n\r\nopt.sample_duration = max(opt.sample_duration_clf, opt.sample_duration_det)\r\nfps = """"\r\ncap = cv2.VideoCapture(opt.video)\r\nnum_frame = 0\r\nclip = []\r\nactive_index = 0\r\npassive_count = 0\r\nactive = False\r\nprev_active = False\r\nfinished_prediction = None\r\npre_predict = False\r\ndetector.eval()\r\nclassifier.eval()\r\ncum_sum = np.zeros(opt.n_classes_clf, )\r\nclf_selected_queue = np.zeros(opt.n_classes_clf, )\r\ndet_selected_queue = np.zeros(opt.n_classes_det, )\r\nmyqueue_det = Queue(opt.det_queue_size, n_classes=opt.n_classes_det)\r\nmyqueue_clf = Queue(opt.clf_queue_size, n_classes=opt.n_classes_clf)\r\nresults = []\r\nprev_best1 = opt.n_classes_clf\r\nspatial_transform.randomize_parameters()\r\nwhile cap.isOpened():\r\n    t1 = time.time()\r\n    ret, frame = cap.read()\r\n    if num_frame == 0:\r\n        cur_frame = cv2.resize(frame,(320,240))\r\n        cur_frame = Image.fromarray(cv2.cvtColor(cur_frame,cv2.COLOR_BGR2RGB))\r\n        cur_frame = cur_frame.convert(\'RGB\')\r\n        for i in range(opt.sample_duration):\r\n            clip.append(cur_frame)\r\n        clip = [spatial_transform(img) for img in clip]\r\n    clip.pop(0)\r\n    _frame = cv2.resize(frame,(320,240))\r\n    _frame = Image.fromarray(cv2.cvtColor(_frame,cv2.COLOR_BGR2RGB))\r\n    _frame = _frame.convert(\'RGB\')\r\n    _frame = spatial_transform(_frame)\r\n    clip.append(_frame)\r\n    im_dim = clip[0].size()[-2:]\r\n    try:\r\n        test_data = torch.cat(clip, 0).view((opt.sample_duration, -1) + im_dim).permute(1, 0, 2, 3)\r\n    except Exception as e:\r\n        pdb.set_trace()\r\n        raise e\r\n    inputs = torch.cat([test_data],0).view(1,3,opt.sample_duration,112,112)\r\n    num_frame += 1\r\n\r\n\r\n    ground_truth_array = np.zeros(opt.n_classes_clf + 1, )\r\n    with torch.no_grad():\r\n        inputs = Variable(inputs)\r\n        inputs_det = inputs[:, :, -opt.sample_duration_det:, :, :]\r\n        outputs_det = detector(inputs_det)\r\n        outputs_det = F.softmax(outputs_det, dim=1)\r\n        outputs_det = outputs_det.cpu().numpy()[0].reshape(-1, )\r\n        # enqueue the probabilities to the detector queue\r\n        myqueue_det.enqueue(outputs_det.tolist())\r\n\r\n        if opt.det_strategy == \'raw\':\r\n            det_selected_queue = outputs_det\r\n        elif opt.det_strategy == \'median\':\r\n            det_selected_queue = myqueue_det.median\r\n        elif opt.det_strategy == \'ma\':\r\n            det_selected_queue = myqueue_det.ma\r\n        elif opt.det_strategy == \'ewma\':\r\n            det_selected_queue = myqueue_det.ewma\r\n        prediction_det = np.argmax(det_selected_queue)\r\n\r\n        prob_det = det_selected_queue[prediction_det]\r\n        \r\n        #### State of the detector is checked here as detector act as a switch for the classifier\r\n        if prediction_det == 1:\r\n            inputs_clf = inputs[:, :, :, :, :]\r\n            inputs_clf = torch.Tensor(inputs_clf.numpy()[:,:,::1,:,:])\r\n            outputs_clf = classifier(inputs_clf)\r\n            outputs_clf = F.softmax(outputs_clf, dim=1)\r\n            outputs_clf = outputs_clf.cpu().numpy()[0].reshape(-1, )\r\n            # Push the probabilities to queue\r\n            myqueue_clf.enqueue(outputs_clf.tolist())\r\n            passive_count = 0\r\n\r\n            if opt.clf_strategy == \'raw\':\r\n                clf_selected_queue = outputs_clf\r\n            elif opt.clf_strategy == \'median\':\r\n                clf_selected_queue = myqueue_clf.median\r\n            elif opt.clf_strategy == \'ma\':\r\n                clf_selected_queue = myqueue_clf.ma\r\n            elif opt.clf_strategy == \'ewma\':\r\n                clf_selected_queue = myqueue_clf.ewma\r\n\r\n        else:\r\n            outputs_clf = np.zeros(opt.n_classes_clf, )\r\n            # Push the probabilities to queue\r\n            myqueue_clf.enqueue(outputs_clf.tolist())\r\n            passive_count += 1\r\n    \r\n    if passive_count >= opt.det_counter:\r\n        active = False\r\n    else:\r\n        active = True\r\n\r\n    # one of the following line need to be commented !!!!\r\n    if active:\r\n        active_index += 1\r\n        cum_sum = ((cum_sum * (active_index - 1)) + (weighting_func(active_index) * clf_selected_queue)) / active_index  # Weighted Aproach\r\n        #cum_sum = ((cum_sum * (active_index-1)) + (1.0 * clf_selected_queue))/active_index #Not Weighting Aproach\r\n        best2, best1 = tuple(cum_sum.argsort()[-2:][::1])\r\n        if float(cum_sum[best1] - cum_sum[best2]) > opt.clf_threshold_pre:\r\n            finished_prediction = True\r\n            pre_predict = True\r\n\r\n    else:\r\n        active_index = 0\r\n    if active == False and prev_active == True:\r\n        finished_prediction = True\r\n    elif active == True and prev_active == False:\r\n        finished_prediction = False\r\n\r\n    if finished_prediction == True:\r\n        #print(finished_prediction,pre_predict)\r\n        best2, best1 = tuple(cum_sum.argsort()[-2:][::1])\r\n        if cum_sum[best1] > opt.clf_threshold_final:\r\n            if pre_predict == True:\r\n                if best1 != prev_best1:\r\n                    if cum_sum[best1] > opt.clf_threshold_final:\r\n                        results.append(((i * opt.stride_len) + opt.sample_duration_clf, best1))\r\n                        print(\'Early Detected - class : {} with prob : {} at frame {}\'.format(best1, cum_sum[best1],\r\n                                                                                              (\r\n                                                                                                          i * opt.stride_len) + opt.sample_duration_clf))\r\n            else:\r\n                if cum_sum[best1] > opt.clf_threshold_final:\r\n                    if best1 == prev_best1:\r\n                        if cum_sum[best1] > 5:\r\n                            results.append(((i * opt.stride_len) + opt.sample_duration_clf, best1))\r\n                            print(\'Late Detected - class : {} with prob : {} at frame {}\'.format(best1,\r\n                                                                                                 cum_sum[best1], (\r\n                                                                                                             i * opt.stride_len) + opt.sample_duration_clf))\r\n                    else:\r\n                        results.append(((i * opt.stride_len) + opt.sample_duration_clf, best1))\r\n\r\n                        print(\'Late Detected - class : {} with prob : {} at frame {}\'.format(best1, cum_sum[best1],\r\n                                                                                             (\r\n                                                                                                         i * opt.stride_len) + opt.sample_duration_clf))\r\n\r\n            finished_prediction = False\r\n            prev_best1 = best1\r\n\r\n        cum_sum = np.zeros(opt.n_classes_clf, )\r\n    \r\n    if active == False and prev_active == True:\r\n        pre_predict = False\r\n\r\n    prev_active = active\r\n    elapsedTime = time.time() - t1\r\n    fps = ""(Playback) {:.1f} FPS"".format(1/elapsedTime)\r\n\r\n    if len(results) != 0:\r\n        predicted = np.array(results)[:, 1]\r\n        prev_best1 = -1\r\n    else:\r\n        predicted = []\r\n\r\n    print(\'predicted classes: \\t\', predicted)\r\n\r\n    cv2.putText(frame, fps, (0, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (38, 0, 255), 1, cv2.LINE_AA)\r\n    cv2.imshow(""Result"", frame)\r\n\r\n    if cv2.waitKey(1)&0xFF == ord(\'q\'):\r\n        break\r\ncv2.destroyAllWindows()\r\n\r\n'"
online_test_wo_detector.py,7,"b'import os\nimport glob\nimport json\nimport pandas as pd\nimport csv\nimport torch\nfrom torch.autograd import Variable\nfrom torch.nn import functional as F\n\nfrom opts import parse_opts_online\nfrom model import generate_model\nfrom mean import get_mean, get_std\nfrom spatial_transforms import *\nfrom temporal_transforms import *\nfrom target_transforms import ClassLabel\nfrom dataset import get_online_data\nfrom utils import  AverageMeter, LevenshteinDistance, Queue\n\nimport pdb\nimport numpy as np\nimport datetime\n\n\ndef weighting_func(x):\n    return (1 / (1 + np.exp(-0.2 * (x - 9))))\n\n\nopt = parse_opts_online()\n\n\nopt = parse_opts_online()\n\n\ndef load_models(opt):\n    opt.resume_path = opt.resume_path_clf\n    opt.pretrain_path = opt.pretrain_path_clf\n    opt.sample_duration = opt.sample_duration_clf\n    opt.model = opt.model_clf\n    opt.model_depth = opt.model_depth_clf\n    opt.width_mult = opt.width_mult_clf\n    opt.modality = opt.modality_clf\n    opt.resnet_shortcut = opt.resnet_shortcut_clf\n    opt.n_classes = opt.n_classes_clf\n    opt.n_finetune_classes = opt.n_finetune_classes_clf\n    if opt.root_path != \'\':\n        opt.video_path = os.path.join(opt.root_path, opt.video_path)\n        opt.annotation_path = os.path.join(opt.root_path, opt.annotation_path)\n        opt.result_path = os.path.join(opt.root_path, opt.result_path)\n        if opt.resume_path:\n            opt.resume_path = os.path.join(opt.root_path, opt.resume_path)\n        if opt.pretrain_path:\n            opt.pretrain_path = os.path.join(opt.root_path, opt.pretrain_path)\n\n    opt.scales = [opt.initial_scale]\n    for i in range(1, opt.n_scales):\n        opt.scales.append(opt.scales[-1] * opt.scale_step)\n    opt.arch = \'{}\'.format(opt.model)\n    opt.mean = get_mean(opt.norm_value)\n    opt.std = get_std(opt.norm_value)\n\n    print(opt)\n    with open(os.path.join(opt.result_path, \'opts_clf.json\'), \'w\') as opt_file:\n        json.dump(vars(opt), opt_file)\n\n    torch.manual_seed(opt.manual_seed)\n    classifier, parameters = generate_model(opt)\n\n    if opt.resume_path:\n        print(\'loading checkpoint {}\'.format(opt.resume_path))\n        checkpoint = torch.load(opt.resume_path)\n#        assert opt.arch == checkpoint[\'arch\']\n\n        classifier.load_state_dict(checkpoint[\'state_dict\'])\n\n    print(\'Model 2 \\n\', classifier)\n    pytorch_total_params = sum(p.numel() for p in classifier.parameters() if\n                               p.requires_grad)\n    print(""Total number of trainable parameters: "", pytorch_total_params)\n\n    return classifier\n\n\nclassifier = load_models(opt)\n\n\nif opt.no_mean_norm and not opt.std_norm:\n    norm_method = Normalize([0, 0, 0], [1, 1, 1])\nelif not opt.std_norm:\n    norm_method = Normalize(opt.mean, [1, 1, 1])\nelse:\n    norm_method = Normalize(opt.mean, opt.std)\n\nspatial_transform = Compose([\n    Scale(112),\n    CenterCrop(112),\n    ToTensor(opt.norm_value), norm_method\n])\n\ntarget_transform = ClassLabel()\n\n## Get list of videos to test\nif opt.dataset == \'egogesture\':\n    subject_list = [\'Subject{:02d}\'.format(i) for i in [2, 9, 11, 14, 18, 19, 28, 31, 41, 47]]\n    test_paths = []\n    for subject in subject_list:\n        for x in glob.glob(os.path.join(opt.video_path, subject, \'*/*/rgb*\')):\n            test_paths.append(x)\nelif opt.dataset == \'nvgesture\':\n    df = pd.read_csv(os.path.join(opt.video_path, \'nvgesture_test_correct_cvpr2016_v2.lst\'), delimiter=\' \', header=None)\n    test_paths = []\n    for x in df[0].values:\n        test_paths.append(os.path.join(opt.video_path, x.replace(\'path:\', \'\'), \'sk_color_all\'))\n\nprint(\'Start Evaluation\')\nclassifier.eval()\n\n\nlevenshtein_accuracies = AverageMeter()\nvideoidx = 0\nfor path in test_paths[:]:\n    if opt.dataset == \'egogesture\':\n        opt.whole_path = os.path.join(*path.rsplit(os.sep, 4)[1:])\n    elif opt.dataset == \'nvgesture\':\n        opt.whole_path = os.path.join(*path.rsplit(os.sep, 5)[1:])\n\n    videoidx += 1\n    active_index = 0\n    passive_count = 0\n    active = False\n    prev_active = False\n    finished_prediction = None\n    pre_predict = False\n\n    cum_sum = np.zeros(opt.n_classes_clf, )\n    clf_selected_queue = np.zeros(opt.n_classes_clf, )\n    det_selected_queue = np.zeros(opt.n_classes_det, )\n    myqueue_det = Queue(opt.det_queue_size, n_classes=opt.n_classes_det)\n    myqueue_clf = Queue(opt.clf_queue_size, n_classes=opt.n_classes_clf)\n\n    print(\'[{}/{}]============\'.format(videoidx, len(test_paths)))\n    print(path)\n    opt.sample_duration = max(opt.sample_duration_clf, opt.sample_duration_det)\n    temporal_transform = TemporalRandomCrop(opt.sample_duration, opt.downsample)\n    test_data = get_online_data(\n        opt, spatial_transform, None, target_transform)\n\n    test_loader = torch.utils.data.DataLoader(\n        test_data,\n        batch_size=opt.batch_size,\n        shuffle=False,\n        num_workers=opt.n_threads,\n        pin_memory=True)\n\n    results = []\n    prev_best1 = opt.n_classes_clf\n    dataset_len = len(test_loader.dataset)\n    for i, (inputs, targets) in enumerate(test_loader):\n        if not opt.no_cuda:\n            targets = targets.cuda()\n        ground_truth_array = np.zeros(opt.n_classes_clf + 1, )\n        with torch.no_grad():\n            inputs = Variable(inputs)\n            targets = Variable(targets)\n            if opt.modality_clf == \'RGB\':\n                inputs_clf = inputs[:, :-1, :, :, :]\n            elif opt.modality_clf == \'Depth\':\n                inputs_clf = inputs[:, -1, :, :, :].unsqueeze(1)\n            elif opt.modality_clf == \'RGB-D\':\n                inputs_clf = inputs[:, :, :, :, :]\n            inputs_clf = torch.Tensor(inputs_clf.numpy()[:, :, ::2, :, :])\n            outputs_clf = classifier(inputs_clf)\n            outputs_clf = F.softmax(outputs_clf, dim=1)\n            outputs_clf = outputs_clf.cpu().numpy()[0].reshape(-1, )\n\n            #pdb.set_trace()\n            #### State of the detector is checked here as detector act as a switch for the classifier\n            if  np.argmax(outputs_clf) != opt.n_classes_clf-1:\n                # Push the probabilities to queue\n                myqueue_clf.enqueue(outputs_clf.tolist())\n                passive_count = 0\n\n                if opt.clf_strategy == \'raw\':\n                    clf_selected_queue = outputs_clf\n                elif opt.clf_strategy == \'median\':\n                    clf_selected_queue = myqueue_clf.median\n                elif opt.clf_strategy == \'ma\':\n                    clf_selected_queue = myqueue_clf.ma\n                elif opt.clf_strategy == \'ewma\':\n                    clf_selected_queue = myqueue_clf.ewma\n\n            else:\n                outputs_clf = np.zeros(opt.n_classes_clf, )\n                # Push the probabilities to queue\n                myqueue_clf.enqueue(outputs_clf.tolist())\n                passive_count += 1\n\n        if passive_count >= opt.det_counter or i == (dataset_len -2):\n            active = False\n        else:\n            active = True\n\n        # one of the following line need to be commented !!!!\n        if active:\n            active_index += 1\n            cum_sum = ((cum_sum * (active_index - 1)) + (\n                        weighting_func(active_index) * clf_selected_queue)) / active_index  # Weighted Aproach\n            # cum_sum = ((cum_sum * (x-1)) + (1.0 * clf_selected_queue))/x #Not Weighting Aproach\n\n            best2, best1 = tuple(cum_sum.argsort()[-2:][::1])\n            if float(cum_sum[best1] - cum_sum[best2]) > opt.clf_threshold_pre:\n                finished_prediction = True\n                pre_predict = True\n\n        else:\n            active_index = 0\n\n        if active == False and prev_active == True:\n            finished_prediction = True\n        elif active == True and prev_active == False:\n            finished_prediction = False\n\n        if finished_prediction == True:\n            best2, best1 = tuple(cum_sum.argsort()[-2:][::1])\n            if cum_sum[best1] > opt.clf_threshold_final:\n                if pre_predict == True:\n                    if best1 != prev_best1:\n                        if cum_sum[best1] > opt.clf_threshold_final:\n                            results.append(((i * opt.stride_len) + opt.sample_duration_clf, best1))\n                            print(\'Early Detected - class : {} with prob : {} at frame {}\'.format(best1, cum_sum[best1],\n                                                                                                  (\n                                                                                                              i * opt.stride_len) + opt.sample_duration_clf))\n                else:\n                    if cum_sum[best1] > opt.clf_threshold_final:\n                        if best1 == prev_best1:\n                            if cum_sum[best1] > 5:\n                                results.append(((i * opt.stride_len) + opt.sample_duration_clf, best1))\n                                print(\'Late Detected - class : {} with prob : {} at frame {}\'.format(best1,\n                                                                                                     cum_sum[best1], (\n                                                                                                                 i * opt.stride_len) + opt.sample_duration_clf))\n                        else:\n                            results.append(((i * opt.stride_len) + opt.sample_duration_clf, best1))\n\n                            print(\'Late Detected - class : {} with prob : {} at frame {}\'.format(best1, cum_sum[best1],\n                                                                                                 (\n                                                                                                             i * opt.stride_len) + opt.sample_duration_clf))\n\n                finished_prediction = False\n                prev_best1 = best1\n\n            cum_sum = np.zeros(opt.n_classes_clf, )\n\n        if active == False and prev_active == True:\n            pre_predict = False\n\n        prev_active = active\n\n\n    if opt.dataset == \'egogesture\':\n        target_csv_path = os.path.join(opt.video_path,\n                                       \'labels-final-revised1\',\n                                       opt.whole_path.rsplit(os.sep, 2)[0],\n                                       \'Group\' + opt.whole_path[-1] + \'.csv\').replace(\'Subject\', \'subject\')\n        true_classes = []\n        with open(target_csv_path) as csvfile:\n            readCSV = csv.reader(csvfile, delimiter=\',\')\n            for row in readCSV:\n                true_classes.append(int(row[0]) - 1)\n    elif opt.dataset == \'nvgesture\':\n        true_classes = []\n        with open(\'./annotation_nvGesture/vallistall.txt\') as csvfile:\n            readCSV = csv.reader(csvfile, delimiter=\' \')\n            for row in readCSV:\n                if row[0] == opt.whole_path:\n                    if row[1] != \'26\':\n                        true_classes.append(int(row[1]) - 1)\n    if len(results) != 0:\n        predicted = np.array(results)[:, 1]\n    else:\n        predicted = []\n    true_classes = np.array(true_classes)\n    levenshtein_distance = LevenshteinDistance(true_classes, predicted)\n    levenshtein_accuracy = 1 - (levenshtein_distance / len(true_classes))\n    if levenshtein_distance < 0:  # Distance cannot be less than 0\n        levenshtein_accuracies.update(0, len(true_classes))\n    else:\n        levenshtein_accuracies.update(levenshtein_accuracy, len(true_classes))\n\n    print(\'predicted classes: \\t\', predicted)\n    print(\'True classes :\\t\\t\', true_classes)\n    print(\'Levenshtein Accuracy = {} ({})\'.format(levenshtein_accuracies.val, levenshtein_accuracies.avg))\n\nprint(\'Average Levenshtein Accuracy= {}\'.format(levenshtein_accuracies.avg))\n\nprint(\'-----Evaluation is finished------\')\nwith open(""./results/online-results.log"", ""a"") as myfile:\n    myfile.write(""{}, {}, {}, {}, {}, {}"".format(datetime.datetime.now(),\n                                    opt.resume_path_clf,\n                                    opt.model_clf,\n                                    opt.width_mult_clf,\n                                    opt.modality_clf,\n                                    levenshtein_accuracies.avg))\n'"
opts.py,0,"b'import argparse\n\n\ndef parse_opts():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--root_path\', default=\'/root/data/ActivityNet\', type=str, help=\'Root directory path of data\')\n    parser.add_argument(\'--video_path\', default=\'video_kinetics_jpg\', type=str, help=\'Directory path of Videos\')\n    parser.add_argument(\'--annotation_path\', default=\'kinetics.json\', type=str, help=\'Annotation file path\')\n    parser.add_argument(\'--result_path\', default=\'results\', type=str, help=\'Result directory path\')\n    parser.add_argument(\'--store_name\', default=\'model\', type=str, help=\'Name to store checkpoints\')\n    parser.add_argument(\'--modality\', default=\'RGB\', type=str, help=\'Modality of generated model. RGB, Flow or RGBFlow\')\n    parser.add_argument(\'--pretrain_modality\', default=\'RGB\', type=str, help=\'Modality of the pretrain model. RGB, Flow or RGBFlow\')\n    parser.add_argument(\'--dataset\', default=\'kinetics\', type=str, help=\'Used dataset (activitynet | kinetics | ucf101 | hmdb51)\')\n    parser.add_argument(\'--n_classes\', default=400, type=int, help=\'Number of classes (activitynet: 200, kinetics: 400, ucf101: 101, hmdb51: 51)\')\n    parser.add_argument(\'--n_finetune_classes\', default=400, type=int, help=\'Number of classes for fine-tuning. n_classes is set to the number when pretraining.\')\n    parser.add_argument(\'--sample_size\', default=112, type=int, help=\'Height and width of inputs\')\n    parser.add_argument(\'--sample_duration\', default=16, type=int, help=\'Temporal duration of inputs\')\n    parser.add_argument(\'--downsample\', default=1, type=int, help=\'Downsampling. Selecting 1 frame out of N\')\n    parser.add_argument(\'--initial_scale\', default=1.0, type=float, help=\'Initial scale for multiscale cropping\')\n    parser.add_argument(\'--n_scales\', default=5, type=int, help=\'Number of scales for multiscale cropping\')\n    parser.add_argument(\'--scale_step\', default=0.84089641525, type=float, help=\'Scale step for multiscale cropping\')\n    parser.add_argument(\'--train_crop\', default=\'corner\', type=str, help=\'Spatial cropping method in training. random is uniform. corner is selection from 4 corners and 1 center.  (random | corner | center)\')\n    parser.add_argument(\'--learning_rate\', default=0.04, type=float, help=\'Initial learning rate (divided by 10 while training by lr scheduler)\')\n    parser.add_argument(\'--lr_steps\', default=[15, 25, 35, 45, 60, 50, 200, 250], type=float, nargs=""+"", metavar=\'LRSteps\', help=\'epochs to decay learning rate by 10\') # [15, 30, 37, 50, 200, 250]\n    parser.add_argument(\'--momentum\', default=0.9, type=float, help=\'Momentum\')\n    parser.add_argument(\'--dampening\', default=0.9, type=float, help=\'dampening of SGD\')\n    parser.add_argument(\'--weight_decay\', default=1e-3, type=float, help=\'Weight Decay\')\n    parser.add_argument(\'--mean_dataset\', default=\'activitynet\', type=str, help=\'dataset for mean values of mean subtraction (activitynet | kinetics)\')\n    parser.add_argument(\'--no_mean_norm\', action=\'store_true\', help=\'If true, inputs are not normalized by mean.\')\n    parser.set_defaults(no_mean_norm=False)\n    parser.add_argument(\'--std_norm\', action=\'store_true\', help=\'If true, inputs are normalized by standard deviation.\')\n    parser.set_defaults(std_norm=False)\n    parser.add_argument(\'--nesterov\', action=\'store_true\', help=\'Nesterov momentum\')\n    parser.set_defaults(nesterov=False)\n    parser.add_argument(\'--optimizer\', default=\'sgd\', type=str, help=\'Currently only support SGD\')\n    parser.add_argument(\'--lr_patience\', default=10, type=int, help=\'Patience of LR scheduler. See documentation of ReduceLROnPlateau.\')\n    parser.add_argument(\'--batch_size\', default=128, type=int, help=\'Batch Size\')\n    parser.add_argument(\'--n_epochs\', default=250, type=int, help=\'Number of total epochs to run\')\n    parser.add_argument(\'--begin_epoch\', default=1, type=int, help=\'Training begins at this epoch. Previous trained model indicated by resume_path is loaded.\')\n    parser.add_argument(\'--n_val_samples\', default=3, type=int, help=\'Number of validation samples for each activity\')\n    parser.add_argument(\'--resume_path\', default=\'\', type=str, help=\'Save data (.pth) of previous training\')\n    parser.add_argument(\'--pretrain_path\', default=\'\', type=str, help=\'Pretrained model (.pth)\')\n    parser.add_argument(\'--ft_portion\', default=\'complete\', type=str, help=\'The portion of the model to apply fine tuning, either complete or last_layer\')\n    parser.add_argument(\'--no_train\', action=\'store_true\', help=\'If true, training is not performed.\')\n    parser.set_defaults(no_train=False)\n    parser.add_argument(\'--no_val\', action=\'store_true\', help=\'If true, validation is not performed.\')\n    parser.set_defaults(no_val=False)\n    parser.add_argument(\'--test\', action=\'store_true\', help=\'If true, test is performed.\')\n    parser.set_defaults(test=False)\n    parser.add_argument(\'--test_subset\', default=\'val\', type=str, help=\'Used subset in test (val | test)\')\n    parser.add_argument(\'--scale_in_test\', default=1.0, type=float, help=\'Spatial scale in test\')\n    parser.add_argument(\'--crop_position_in_test\', default=\'c\', type=str, help=\'Cropping method (c | tl | tr | bl | br) in test\')\n    parser.add_argument(\'--no_softmax_in_test\', action=\'store_true\', help=\'If true, output for each clip is not normalized using softmax.\')\n    parser.set_defaults(no_softmax_in_test=False)\n    parser.add_argument(\'--no_cuda\', action=\'store_true\', help=\'If true, cuda is not used.\')\n    parser.set_defaults(no_cuda=False)\n    parser.add_argument(\'--n_threads\', default=16, type=int, help=\'Number of threads for multi-thread loading\')\n    parser.add_argument(\'--checkpoint\', default=10, type=int, help=\'Trained model is saved at every this epochs.\')\n    parser.add_argument(\'--no_hflip\', action=\'store_true\', help=\'If true holizontal flipping is not performed.\')\n    parser.set_defaults(no_hflip=False)\n    parser.add_argument(\'--norm_value\', default=1, type=int, help=\'If 1, range of inputs is [0-255]. If 255, range of inputs is [0-1].\')\n    parser.add_argument(\'--model\', default=\'resnet\', type=str, help=\'(resnet | preresnet | wideresnet | resnext | densenet | \')\n    parser.add_argument(\'--version\', default=1.1, type=float, help=\'Version of the model\')\n    parser.add_argument(\'--model_depth\', default=18, type=int, help=\'Depth of resnet (10 | 18 | 34 | 50 | 101)\')\n    parser.add_argument(\'--resnet_shortcut\', default=\'B\', type=str, help=\'Shortcut type of resnet (A | B)\')\n    parser.add_argument(\'--wide_resnet_k\', default=2, type=int, help=\'Wide resnet k\')\n    parser.add_argument(\'--resnext_cardinality\', default=32, type=int, help=\'ResNeXt cardinality\')\n    parser.add_argument(\'--groups\', default=3, type=int, help=\'The number of groups at group convolutions at conv layers\')\n    parser.add_argument(\'--width_mult\', default=1.0, type=float, help=\'The applied width multiplier to scale number of filters\')\n    parser.add_argument(\'--manual_seed\', default=1, type=int, help=\'Manually set random seed\')\n    parser.add_argument(\'--train_validate\', action=\'store_true\', help=\'If true, test is performed.\')\n    parser.set_defaults(train_validate=False)\n    args = parser.parse_args()\n\n    return args\n\n\ndef parse_opts_online():\n    # Real-time test arguments with detector and classifier architecture\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--root_path\', default=\'/root/data/ActivityNet\', type=str, help=\'Root directory path of data\')\n    parser.add_argument(\'--video_path\', default=\'video_kinetics_jpg\', type=str, help=\'Directory path of Videos\')\n    parser.add_argument(\'--video\', default=\'data2/EgoGesture/videos/Subject02/Scene1/Color/rgb1.avi\', type=str, help=\'Directory path of test Videos\')\n    parser.add_argument(\'--whole_path\', default=\'video_kinetics_jpg\', type=str, help=\'The whole path of Videos\')\n    parser.add_argument(\'--annotation_path\', default=\'kinetics.json\', type=str, help=\'Annotation file path\')\n    parser.add_argument(\'--result_path\', default=\'results\', type=str, help=\'Result directory path\')\n    parser.add_argument(\'--store_name\', default=\'model\', type=str, help=\'Name to store checkpoints\')\n    parser.add_argument(\'--modality\', default=\'RGB\', type=str, help=\'Modality of input data. RGB, Flow or RGBFlow\')\n    parser.add_argument(\'--modality_det\', default=\'RGB\', type=str, help=\'Modality of input data. RGB, Flow or RGBFlow\')\n    parser.add_argument(\'--modality_clf\', default=\'RGB\', type=str, help=\'Modality of input data. RGB, Flow or RGBFlow\')\n    parser.add_argument(\'--dataset\', default=\'kinetics\', type=str,\n                        help=\'Used dataset (activitynet | kinetics | ucf101 | hmdb51)\')\n    parser.add_argument(\'--n_classes_det\', default=400, type=int,\n                        help=\'Number of classes (activitynet: 200, kinetics: 400, ucf101: 101, hmdb51: 51)\')\n    parser.add_argument(\'--n_finetune_classes_det\', default=400, type=int,\n                        help=\'Number of classes for fine-tuning. n_classes is set to the number when pretraining.\')\n    parser.add_argument(\'--n_classes_clf\', default=400, type=int,\n                        help=\'Number of classes (activitynet: 200, kinetics: 400, ucf101: 101, hmdb51: 51)\')\n    parser.add_argument(\'--n_finetune_classes_clf\', default=400, type=int,\n                        help=\'Number of classes for fine-tuning. n_classes is set to the number when pretraining.\')\n\n    parser.add_argument(\'--n_classes\', default=400, type=int,\n                        help=\'Number of classes (activitynet: 200, kinetics: 400, ucf101: 101, hmdb51: 51)\')\n    parser.add_argument(\'--n_finetune_classes\', default=400, type=int,\n                        help=\'Number of classes for fine-tuning. n_classes is set to the number when pretraining.\')\n    parser.add_argument(\'--sample_size\', default=112, type=int, help=\'Height and width of inputs\')\n    parser.add_argument(\'--sample_duration_det\', default=16, type=int, help=\'Temporal duration of inputs\')\n    parser.add_argument(\'--sample_duration_clf\', default=16, type=int, help=\'Temporal duration of inputs\')\n    parser.add_argument(\'--sample_duration\', default=16, type=int, help=\'Temporal duration of inputs\')\n\n    parser.add_argument(\'--initial_scale\', default=1.0, type=float, help=\'Initial scale for multiscale cropping\')\n    parser.add_argument(\'--n_scales\', default=5, type=int, help=\'Number of scales for multiscale cropping\')\n    parser.add_argument(\'--scale_step\', default=0.84089641525, type=float, help=\'Scale step for multiscale cropping\')\n    parser.add_argument(\'--train_crop\', default=\'corner\', type=str,\n                        help=\'Spatial cropping method in training. random is uniform. corner is selection from 4 corners and 1 center.  (random | corner | center)\')\n    parser.add_argument(\'--learning_rate\', default=0.1, type=float,\n                        help=\'Initial learning rate (divided by 10 while training by lr scheduler)\')\n    parser.add_argument(\'--lr_steps\', default=[10, 20, 30, 40, 100], type=float, nargs=""+"", metavar=\'LRSteps\',\n                        help=\'epochs to decay learning rate by 10\')\n    parser.add_argument(\'--momentum\', default=0.9, type=float, help=\'Momentum\')\n    parser.add_argument(\'--dampening\', default=0.9, type=float, help=\'dampening of SGD\')\n    parser.add_argument(\'--weight_decay\', default=1e-3, type=float, help=\'Weight Decay\')\n    parser.add_argument(\'--mean_dataset\', default=\'activitynet\', type=str,\n                        help=\'dataset for mean values of mean subtraction (activitynet | kinetics)\')\n    parser.add_argument(\'--no_mean_norm\', action=\'store_true\', help=\'If true, inputs are not normalized by mean.\')\n    parser.set_defaults(no_mean_norm=False)\n    parser.add_argument(\'--std_norm\', action=\'store_true\', help=\'If true, inputs are normalized by standard deviation.\')\n    parser.set_defaults(std_norm=False)\n    parser.add_argument(\'--nesterov\', action=\'store_true\', help=\'Nesterov momentum\')\n    parser.set_defaults(nesterov=False)\n    parser.add_argument(\'--optimizer\', default=\'sgd\', type=str, help=\'Currently only support SGD\')\n    parser.add_argument(\'--lr_patience\', default=10, type=int,\n                        help=\'Patience of LR scheduler. See documentation of ReduceLROnPlateau.\')\n    parser.add_argument(\'--batch_size\', default=128, type=int, help=\'Batch Size\')\n    parser.add_argument(\'--n_epochs\', default=200, type=int, help=\'Number of total epochs to run\')\n    parser.add_argument(\'--begin_epoch\', default=1, type=int,\n                        help=\'Training begins at this epoch. Previous trained model indicated by resume_path is loaded.\')\n    parser.add_argument(\'--n_val_samples\', default=3, type=int, help=\'Number of validation samples for each activity\')\n    parser.add_argument(\'--resume_path_det\', default=\'\', type=str, help=\'Save data (.pth) of previous training\')\n    parser.add_argument(\'--resume_path_clf\', default=\'\', type=str, help=\'Save data (.pth) of previous training\')\n    parser.add_argument(\'--resume_path\', default=\'\', type=str, help=\'Save data (.pth) of previous training\')\n    parser.add_argument(\'--pretrain_path_det\', default=\'\', type=str, help=\'Pretrained model (.pth)\')\n    parser.add_argument(\'--pretrain_path_clf\', default=\'\', type=str, help=\'Pretrained model (.pth)\')\n    parser.add_argument(\'--pretrain_path\', default=\'\', type=str, help=\'Pretrained model (.pth)\')\n\n    parser.add_argument(\'--ft_begin_index\', default=0, type=int, help=\'Begin block index of fine-tuning\')\n    parser.add_argument(\'--no_train\', action=\'store_true\', help=\'If true, training is not performed.\')\n    parser.set_defaults(no_train=False)\n    parser.add_argument(\'--no_val\', action=\'store_true\', help=\'If true, validation is not performed.\')\n    parser.set_defaults(no_val=False)\n    parser.add_argument(\'--test\', action=\'store_true\', help=\'If true, test is performed.\')\n    parser.set_defaults(test=True)\n    parser.add_argument(\'--test_subset\', default=\'val\', type=str, help=\'Used subset in test (val | test)\')\n    parser.add_argument(\'--scale_in_test\', default=1.0, type=float, help=\'Spatial scale in test\')\n    parser.add_argument(\'--crop_position_in_test\', default=\'c\', type=str,\n                        help=\'Cropping method (c | tl | tr | bl | br) in test\')\n    parser.add_argument(\'--no_softmax_in_test\', action=\'store_true\',\n                        help=\'If true, output for each clip is not normalized using softmax.\')\n    parser.set_defaults(no_softmax_in_test=False)\n    parser.add_argument(\'--no_cuda\', action=\'store_true\', help=\'If true, cuda is not used.\')\n    parser.set_defaults(no_cuda=False)\n    parser.add_argument(\'--n_threads\', default=4, type=int, help=\'Number of threads for multi-thread loading\')\n    parser.add_argument(\'--checkpoint\', default=10, type=int, help=\'Trained model is saved at every this epochs.\')\n    parser.add_argument(\'--no_hflip\', action=\'store_true\', help=\'If true holizontal flipping is not performed.\')\n    parser.set_defaults(no_hflip=False)\n    parser.add_argument(\'--norm_value\', default=1, type=int,\n                        help=\'If 1, range of inputs is [0-255]. If 255, range of inputs is [0-1].\')\n\n    parser.add_argument(\'--model_det\', default=\'resnet\', type=str,\n                        help=\'(resnet | preresnet | wideresnet | resnext | densenet | \')\n    parser.add_argument(\'--model_depth_det\', default=18, type=int, help=\'Depth of resnet (10 | 18 | 34 | 50 | 101)\')\n    parser.add_argument(\'--resnet_shortcut_det\', default=\'B\', type=str, help=\'Shortcut type of resnet (A | B)\')\n    parser.add_argument(\'--wide_resnet_k_det\', default=2, type=int, help=\'Wide resnet k\')\n    parser.add_argument(\'--resnext_cardinality_det\', default=32, type=int, help=\'ResNeXt cardinality\')\n\n    parser.add_argument(\'--model\', default=\'resnet\', type=str,\n                        help=\'(resnet | preresnet | wideresnet | resnext | densenet | \')\n    parser.add_argument(\'--model_depth\', default=18, type=int, help=\'Depth of resnet (10 | 18 | 34 | 50 | 101)\')\n    parser.add_argument(\'--resnet_shortcut\', default=\'B\', type=str, help=\'Shortcut type of resnet (A | B)\')\n    parser.add_argument(\'--wide_resnet_k\', default=2, type=int, help=\'Wide resnet k\')\n    parser.add_argument(\'--resnext_cardinality\', default=32, type=int, help=\'ResNeXt cardinality\')\n\n    parser.add_argument(\'--model_clf\', default=\'resnet\', type=str,\n                        help=\'(resnet | preresnet | wideresnet | resnext | densenet | \')\n    parser.add_argument(\'--model_depth_clf\', default=18, type=int, help=\'Depth of resnet (10 | 18 | 34 | 50 | 101)\')\n    parser.add_argument(\'--resnet_shortcut_clf\', default=\'B\', type=str, help=\'Shortcut type of resnet (A | B)\')\n    parser.add_argument(\'--wide_resnet_k_clf\', default=2, type=int, help=\'Wide resnet k\')\n    parser.add_argument(\'--resnext_cardinality_clf\', default=32, type=int, help=\'ResNeXt cardinality\')\n\n    parser.add_argument(\'--width_mult\', default=1.0, type=float, help=\'The applied width multiplier to scale number of filters\')\n    parser.add_argument(\'--width_mult_det\', default=1.0, type=float, help=\'The applied width multiplier to scale number of filters\')\n    parser.add_argument(\'--width_mult_clf\', default=1.0, type=float, help=\'The applied width multiplier to scale number of filters\')\n    \n    parser.add_argument(\'--manual_seed\', default=1, type=int, help=\'Manually set random seed\')\n    parser.add_argument(\'--det_strategy\', default=\'raw\', type=str, help=\'Detector filter (raw | median | ma | ewma)\')\n    parser.add_argument(\'--det_queue_size\', default=1, type=int, help=\'Detector queue size\')\n    parser.add_argument(\'--det_counter\', default=1, type=float, help=\'Number of consequtive detection\')\n    parser.add_argument(\'--clf_strategy\', default=\'raw\', type=str, help=\'Classifier filter (raw | median | ma | ewma)\')\n    parser.add_argument(\'--clf_queue_size\', default=1, type=int, help=\'Classifier queue size\')\n    parser.add_argument(\'--clf_threshold_pre\', default=1, type=float, help=\'Cumulative sum threshold to prepredict\')\n    parser.add_argument(\'--clf_threshold_final\', default=1, type=float,\n                        help=\'Cumulative sum threshold to predict at the end\')\n    parser.add_argument(\'--stride_len\', default=1, type=int, help=\'Stride Lenght of video loader window\')\n    parser.add_argument(\'--ft_portion\', default=\'complete\', type=str, help=\'The portion of the model to apply fine tuning, either complete or last_layer\')\n    parser.add_argument(\'--groups\', default=3, type=int, help=\'The number of groups at group convolutions at conv layers\')\n    parser.add_argument(\'--downsample\', default=1, type=int, help=\'Downsampling. Selecting 1 frame out of N\')\n    \n    args = parser.parse_args()\n\n    return args\n\n'"
spatial_transforms.py,8,"b'import random\nimport math\nimport numbers\nimport collections\nimport numpy as np\nimport torch\nimport cv2\nimport scipy.ndimage\nfrom PIL import Image, ImageOps\ntry:\n    import accimage\nexcept ImportError:\n    accimage = None\n\n\nclass Compose(object):\n    """"""Composes several transforms together.\n    Args:\n        transforms (list of ``Transform`` objects): list of transforms to compose.\n    Example:\n        >>> transforms.Compose([\n        >>>     transforms.CenterCrop(10),\n        >>>     transforms.ToTensor(),\n        >>> ])\n    """"""\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img):\n        for t in self.transforms:\n            img = t(img)\n        return img\n\n    def randomize_parameters(self):\n        for t in self.transforms:\n            t.randomize_parameters()\n\n\nclass ToTensor(object):\n    """"""Convert a ``PIL.Image`` or ``numpy.ndarray`` to tensor.\n    Converts a PIL.Image or numpy.ndarray (H x W x C) in the range\n    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n    """"""\n\n    def __init__(self, norm_value=255):\n        self.norm_value = norm_value\n\n    def __call__(self, pic):\n        """"""\n        Args:\n            pic (PIL.Image or numpy.ndarray): Image to be converted to tensor.\n        Returns:\n            Tensor: Converted image.\n        """"""\n        if isinstance(pic, np.ndarray):\n            # handle numpy array\n            img = torch.from_numpy(pic.transpose((2, 0, 1)))\n            # backward compatibility\n            return img.float().div(self.norm_value)\n\n        if accimage is not None and isinstance(pic, accimage.Image):\n            nppic = np.zeros(\n                [pic.channels, pic.height, pic.width], dtype=np.float32)\n            pic.copyto(nppic)\n            return torch.from_numpy(nppic)\n\n        # handle PIL Image\n        if pic.mode == \'I\':\n            img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n        elif pic.mode == \'I;16\':\n            img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n        else:\n            img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n        # PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK\n        if pic.mode == \'YCbCr\':\n            nchannel = 3\n        elif pic.mode == \'I;16\':\n            nchannel = 1\n        else:\n            nchannel = len(pic.mode)\n        img = img.view(pic.size[1], pic.size[0], nchannel)\n        # put it from HWC to CHW format\n        # yikes, this transpose takes 80% of the loading time/CPU\n        img = img.transpose(0, 1).transpose(0, 2).contiguous()\n        if isinstance(img, torch.ByteTensor):\n            return img.float().div(self.norm_value)\n        else:\n            return img\n\n    def randomize_parameters(self):\n        pass\n\n\nclass Normalize(object):\n    """"""Normalize an tensor image with mean and standard deviation.\n    Given mean: (R, G, B) and std: (R, G, B),\n    will normalize each channel of the torch.*Tensor, i.e.\n    channel = (channel - mean) / std\n    Args:\n        mean (sequence): Sequence of means for R, G, B channels respecitvely.\n        std (sequence): Sequence of standard deviations for R, G, B channels\n            respecitvely.\n    """"""\n\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        """"""\n        Args:\n            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n        Returns:\n            Tensor: Normalized image.\n        """"""\n        # TODO: make efficient\n        for t, m, s in zip(tensor, self.mean, self.std):\n            t.sub_(m).div_(s)\n        return tensor\n\n    def randomize_parameters(self):\n        pass\n\n\nclass Scale(object):\n    """"""Rescale the input PIL.Image to the given size.\n    Args:\n        size (sequence or int): Desired output size. If size is a sequence like\n            (w, h), output size will be matched to this. If size is an int,\n            smaller edge of the image will be matched to this number.\n            i.e, if height > width, then image will be rescaled to\n            (size * height / width, size)\n        interpolation (int, optional): Desired interpolation. Default is\n            ``PIL.Image.BILINEAR``\n    """"""\n\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        assert isinstance(size,\n                          int) or (isinstance(size, collections.Iterable) and\n                                   len(size) == 2)\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (PIL.Image): Image to be scaled.\n        Returns:\n            PIL.Image: Rescaled image.\n        """"""\n        if isinstance(self.size, int):\n            w, h = img.size\n            if (w <= h and w == self.size) or (h <= w and h == self.size):\n                return img\n            if w < h:\n                ow = self.size\n                oh = int(self.size * h / w)\n                return img.resize((ow, oh), self.interpolation)\n            else:\n                oh = self.size\n                ow = int(self.size * w / h)\n                return img.resize((ow, oh), self.interpolation)\n        else:\n            return img.resize(self.size, self.interpolation)\n\n    def randomize_parameters(self):\n        pass\n\n\nclass CenterCrop(object):\n    """"""Crops the given PIL.Image at the center.\n    Args:\n        size (sequence or int): Desired output size of the crop. If size is an\n            int instead of sequence like (h, w), a square crop (size, size) is\n            made.\n    """"""\n\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (PIL.Image): Image to be cropped.\n        Returns:\n            PIL.Image: Cropped image.\n        """"""\n        w, h = img.size\n        th, tw = self.size\n        x1 = int(round((w - tw) / 2.))\n        y1 = int(round((h - th) / 2.))\n        return img.crop((x1, y1, x1 + tw, y1 + th))\n\n    def randomize_parameters(self):\n        pass\n\n\nclass CornerCrop(object):\n\n    def __init__(self, size, crop_position=None):\n        self.size = size\n        if crop_position is None:\n            self.randomize = True\n        else:\n            self.randomize = False\n        self.crop_position = crop_position\n        self.crop_positions = [\'c\', \'tl\', \'tr\', \'bl\', \'br\']\n\n    def __call__(self, img):\n        image_width = img.size[0]\n        image_height = img.size[1]\n\n        if self.crop_position == \'c\':\n            th, tw = (self.size, self.size)\n            x1 = int(round((image_width - tw) / 2.))\n            y1 = int(round((image_height - th) / 2.))\n            x2 = x1 + tw\n            y2 = y1 + th\n        elif self.crop_position == \'tl\':\n            x1 = 0\n            y1 = 0\n            x2 = self.size\n            y2 = self.size\n        elif self.crop_position == \'tr\':\n            x1 = image_width - self.size\n            y1 = 0\n            x2 = image_width\n            y2 = self.size\n        elif self.crop_position == \'bl\':\n            x1 = 0\n            y1 = image_height - self.size\n            x2 = self.size\n            y2 = image_height\n        elif self.crop_position == \'br\':\n            x1 = image_width - self.size\n            y1 = image_height - self.size\n            x2 = image_width\n            y2 = image_height\n\n        img = img.crop((x1, y1, x2, y2))\n\n        return img\n\n    def randomize_parameters(self):\n        if self.randomize:\n            self.crop_position = self.crop_positions[random.randint(\n                0,\n                len(self.crop_positions) - 1)]\n\n\nclass RandomHorizontalFlip(object):\n    """"""Horizontally flip the given PIL.Image randomly with a probability of 0.5.""""""\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (PIL.Image): Image to be flipped.\n        Returns:\n            PIL.Image: Randomly flipped image.\n        """"""\n        if self.p < 0.5:\n            return img.transpose(Image.FLIP_LEFT_RIGHT)\n        return img\n\n    def randomize_parameters(self):\n        self.p = random.random()\n\n\nclass MultiScaleCornerCrop(object):\n    """"""Crop the given PIL.Image to randomly selected size.\n    A crop of size is selected from scales of the original size.\n    A position of cropping is randomly selected from 4 corners and 1 center.\n    This crop is finally resized to given size.\n    Args:\n        scales: cropping scales of the original size\n        size: size of the smaller edge\n        interpolation: Default: PIL.Image.BILINEAR\n    """"""\n\n    def __init__(self,\n                 scales,\n                 size,\n                 interpolation=Image.BILINEAR,\n                 crop_positions=[\'c\', \'tl\', \'tr\', \'bl\', \'br\']):\n        self.scales = scales\n        self.size = size\n        self.interpolation = interpolation\n\n        self.crop_positions = crop_positions\n\n    def __call__(self, img):\n        min_length = min(img.size[0], img.size[1])\n        crop_size = int(min_length * self.scale)\n\n        image_width = img.size[0]\n        image_height = img.size[1]\n\n        if self.crop_position == \'c\':\n            center_x = image_width // 2\n            center_y = image_height // 2\n            box_half = crop_size // 2\n            x1 = center_x - box_half\n            y1 = center_y - box_half\n            x2 = center_x + box_half\n            y2 = center_y + box_half\n        elif self.crop_position == \'tl\':\n            x1 = 0\n            y1 = 0\n            x2 = crop_size\n            y2 = crop_size\n        elif self.crop_position == \'tr\':\n            x1 = image_width - crop_size\n            y1 = 0\n            x2 = image_width\n            y2 = crop_size\n        elif self.crop_position == \'bl\':\n            x1 = 0\n            y1 = image_height - crop_size\n            x2 = crop_size\n            y2 = image_height\n        elif self.crop_position == \'br\':\n            x1 = image_width - crop_size\n            y1 = image_height - crop_size\n            x2 = image_width\n            y2 = image_height\n\n        img = img.crop((x1, y1, x2, y2))\n\n        return img.resize((self.size, self.size), self.interpolation)\n\n    def randomize_parameters(self):\n        self.scale = self.scales[random.randint(0, len(self.scales) - 1)]\n        self.crop_position = self.crop_positions[random.randint(\n            0,\n            len(self.scales) - 1)]\n\n\nclass MultiScaleRandomCrop(object):\n\n    def __init__(self, scales, size, interpolation=Image.BILINEAR):\n        self.scales = scales\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, img):\n        min_length = min(img.size[0], img.size[1])\n        crop_size = int(min_length * self.scale)\n\n        image_width = img.size[0]\n        image_height = img.size[1]\n\n        x1 = self.tl_x * (image_width - crop_size)\n        y1 = self.tl_y * (image_height - crop_size)\n        x2 = x1 + crop_size\n        y2 = y1 + crop_size\n\n        img = img.crop((x1, y1, x2, y2))\n\n        return img.resize((self.size, self.size), self.interpolation)\n\n    def randomize_parameters(self):\n        self.scale = self.scales[random.randint(0, len(self.scales) - 1)]\n        #self.scale = 1\n        self.tl_x = random.random()\n        self.tl_y = random.random()\n\n\n\n\n\n\n\n\nclass SpatialElasticDisplacement(object):\n\n    def __init__(self, sigma=3.0, alpha=1.0, order=3, cval=0, mode=""constant""):\n        self.alpha = alpha\n        self.sigma = sigma\n        self.order = order\n        self.cval = cval\n        self.mode = mode\n\n    def __call__(self, img):\n        if self.p < 0.65:\n            is_PIL = isinstance(img, Image.Image)\n            if is_PIL:\n                img = np.asarray(img)\n\n            image = img\n            image_first_channel = np.squeeze(image[..., 0])\n            indices_x, indices_y = self._generate_indices(image_first_channel.shape, alpha=self.alpha, sigma=self.sigma)\n            ret_image = (self._map_coordinates(\n                image,\n                indices_x,\n                indices_y,\n                order=self.order,\n                cval=self.cval,\n                mode=self.mode))\n\n            if is_PIL:\n                return Image.fromarray(ret_image)\n            else:\n                return ret_image\n        else:\n            return img\n\n    def _generate_indices(self, shape, alpha, sigma):\n        assert (len(shape) == 2),""shape: Should be of size 2!""\n        dx = scipy.ndimage.gaussian_filter((np.random.rand(*shape) * 2 - 1), sigma, mode=""constant"", cval=0) * alpha\n        dy = scipy.ndimage.gaussian_filter((np.random.rand(*shape) * 2 - 1), sigma, mode=""constant"", cval=0) * alpha\n\n        x, y = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), indexing=\'ij\')\n        return np.reshape(x+dx, (-1, 1)), np.reshape(y+dy, (-1, 1))\n\n    def _map_coordinates(self, image, indices_x, indices_y, order=1, cval=0, mode=""constant""):\n        assert (len(image.shape) == 3),""image.shape: Should be of size 3!""\n        result = np.copy(image)\n        height, width = image.shape[0:2]\n        for c in range(image.shape[2]):\n            remapped_flat = scipy.ndimage.interpolation.map_coordinates(\n                image[..., c],\n                (indices_x, indices_y),\n                order=order,\n                cval=cval,\n                mode=mode\n            )\n            remapped = remapped_flat.reshape((height, width))\n            result[..., c] = remapped\n        return result\n\n    def randomize_parameters(self):\n       self.p = random.random()\n\n\nclass RandomRotate(object):\n\n    def __init__(self):\n        self.interpolation = Image.BILINEAR\n\n    def __call__(self, img):\n        im_size = img.size\n        ret_img = img.rotate(self.rotate_angle, resample=self.interpolation)\n\n        return ret_img\n\n    def randomize_parameters(self):\n        self.rotate_angle = random.randint(-10, 10)\n\n\nclass RandomResize(object):\n\n    def __init__(self):\n        self.interpolation = Image.BILINEAR\n\n    def __call__(self, img):\n        im_size = img.size\n        ret_img = img.resize((int(im_size[0]*self.resize_const),\n                              int(im_size[1]*self.resize_const)))\n\n        return ret_img\n\n    def randomize_parameters(self):\n        self.resize_const = random.uniform(0.9, 1.1)\n\n\n\nclass Gaussian_blur(object):\n\n    def __init__(self, radius=0.0):\n        self.radius = radius\n\n    def __call__(self, img):\n        if self.p < 0.2:\n            blurred = ndimage.gaussian_filter(img, sigma=(5, 5, 0), order=0)\n            return blurred\n        else:\n            return img\n\n    def randomize_parameters(self):\n        self.p = random.random()\n        self.radius = random.uniform(0.0, 0.1)\n\n\nclass SaltImage(object):\n    def __init__(self, ratio=100):\n        self.ratio = ratio\n\n    def __call__(self, img):\n        is_PIL = isinstance(img, Image.Image)\n        if is_PIL:\n            img = np.asarray(img)\n\n        if self.p < 0.10:\n            data_final = []\n            img = img.astype(np.float)\n            img_shape = img.shape\n            noise = np.random.randint(self.ratio, size=img_shape)\n            img = np.where(noise == 0, 255, img)\n\n            if is_PIL:\n                return Image.fromarray(img.astype(np.uint8))\n            else:\n                return img\n        else:\n            return img\n\n    def randomize_parameters(self):\n        self.p = random.random()\n        self.ratio = random.randint(80, 120)\n\n\nclass Dropout(object):\n\n    def __init__(self, ratio=100):\n        self.ratio = ratio\n\n    def __call__(self, img):\n        is_PIL = isinstance(img, Image.Image)\n        if is_PIL:\n            img = np.asarray(img)\n\n        if self.p < 0.10:\n            data_final = []\n            img = img.astype(np.float)\n            img_shape = img.shape\n            noise = np.random.randint(self.ratio, size=img_shape)\n            img = np.where(noise == 0, 0, img)\n            if is_PIL:\n                return Image.fromarray(img.astype(np.uint8))\n            else:\n                return img\n        else:\n            return img\n\n    def randomize_parameters(self):\n        self.p = random.random()\n        self.ratio = random.randint(30, 50)\n\n\nclass MultiplyValues():\n\n    def __init__(self, value=0.2, per_channel=False):\n        self.value = value\n        self.per_channel = per_channel\n\n    def __call__(self, img):\n        is_PIL = isinstance(img, Image.Image)\n        if is_PIL:\n            img = np.asarray(img)\n\n        image = img.astype(np.float64)\n        image *= self.sample\n        image = np.where(image > 255, 255, image)\n        image = np.where(image < 0, 0, image)\n        image = image.astype(np.uint8)\n\n        if is_PIL:\n            return Image.fromarray(image)\n        else:\n            return image\n\n    def randomize_parameters(self):\n        self.sample = random.uniform(1.0 - self.value, 1.0 + self.value)\n'"
speed_gpu.py,14,"b'import time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.autograd import Variable\nimport torch.onnx\nimport onnx\nfrom onnx import optimizer\nimport os\n\nimport tensorrt as trt\nimport numpy as np\nimport pycuda.autoinit\nimport pycuda.driver as cuda \nimport time\n\n\nfrom utils import AverageMeter, calculate_accuracy\nfrom models import squeezenet, shufflenetv2, shufflenet, mobilenet, mobilenetv2, c3d, resnetl\n\n\nmodel_folder = \'results\'\nmodel_name = \'onnx_model\'\nonnx_model = \'results/ortho_model_shufflenetv2_unet_1120x224_best2.onnx\'\n# onnx_model = os.path.join(model_folder, model_name + \'.onnx\')\n\n\n# # model = shufflenet.get_model(groups=3, width_mult=0.5, num_classes=600)#1\n# # model = shufflenetv2.get_model( width_mult=0.25, num_classes=600, sample_size = 112)#2\n# # model = mobilenet.get_model( width_mult=0.5, num_classes=600, sample_size = 112)#3\n# # model = mobilenetv2.get_model( width_mult=0.2, num_classes=600, sample_size = 112)#4\n# # model = shufflenet.get_model(groups=3, width_mult=1.0, num_classes=600)#5\n# # model = shufflenetv2.get_model( width_mult=1.0, num_classes=600, sample_size = 112)#6\n# # model = mobilenet.get_model( width_mult=1.0, num_classes=600, sample_size = 112)#7\n# # model = mobilenetv2.get_model( width_mult=0.45, num_classes=600, sample_size = 112)#8\n# # model = shufflenet.get_model(groups=3, width_mult=1.5, num_classes=600)#9\n# # model = shufflenetv2.get_model( width_mult=1.5, num_classes=600, sample_size = 112)#10\n# # model = mobilenet.get_model( width_mult=1.5, num_classes=600, sample_size = 112)#11\n# # model = mobilenetv2.get_model( width_mult=0.7, num_classes=600, sample_size = 112)#12\n# # model = shufflenet.get_model(groups=3, width_mult=2.0, num_classes=600)#13\n# model = shufflenetv2.get_model( width_mult=2.0, num_classes=600, sample_size = 112)#14\n# # model = mobilenet.get_model( width_mult=2.0, num_classes=600, sample_size = 112)#15\n# # model = mobilenetv2.get_model( width_mult=1.0, num_classes=600, sample_size = 112)#16\n# # model = squeezenet.get_model( version=1.1, num_classes=600, sample_size = 112, sample_duration = 8)\n# # model = resnetl.resnetl10(num_classes=2, sample_size = 112, sample_duration = 8)\n# # model = model.cuda()\n# # model = nn.DataParallel(model, device_ids=None)  \n# print(model)\n\n# # create the imput placeholder for the model\n# # input_placeholder = torch.randn(1, 3, 16, 112, 112)\n# input_placeholder = torch.randn(1, 3, 1120, 224)\n\n\n# # export\n# torch.onnx.export(model, input_placeholder, onnx_model)\n# print(\'{} exported!\'.format(onnx_model))\n\n\n\nTRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n\ndef build_engine(model_path):\n    with trt.Builder(TRT_LOGGER) as builder, \\\n        builder.create_network() as network, \\\n        trt.OnnxParser(network, TRT_LOGGER) as parser: \n        builder.max_workspace_size = 1<<30\n        builder.max_batch_size = 1\n        with open(model_path, ""rb"") as f:\n            parser.parse(f.read())\n        engine = builder.build_cuda_engine(network)\n        return engine\n\ndef alloc_buf(engine):\n    # host cpu mem\n    h_in_size = trt.volume(engine.get_binding_shape(0))\n    h_out_size = trt.volume(engine.get_binding_shape(1))\n    h_in_dtype = trt.nptype(engine.get_binding_dtype(0))\n    h_out_dtype = trt.nptype(engine.get_binding_dtype(1))\n    in_cpu = cuda.pagelocked_empty(h_in_size, h_in_dtype)\n    out_cpu = cuda.pagelocked_empty(h_out_size, h_out_dtype)\n    # allocate gpu mem\n    in_gpu = cuda.mem_alloc(in_cpu.nbytes)\n    out_gpu = cuda.mem_alloc(out_cpu.nbytes)\n    stream = cuda.Stream()\n    return in_cpu, out_cpu, in_gpu, out_gpu, stream\n\n\ndef inference(engine, context, inputs, out_cpu, in_gpu, out_gpu, stream):\n    # async version\n    # with engine.create_execution_context() as context:  # cost time to initialize\n    # cuda.memcpy_htod_async(in_gpu, inputs, stream)\n    # context.execute_async(1, [int(in_gpu), int(out_gpu)], stream.handle, None)\n    # cuda.memcpy_dtoh_async(out_cpu, out_gpu, stream)\n    # stream.synchronize()\n\n    # sync version\n    cuda.memcpy_htod(in_gpu, inputs)\n    context.execute(1, [int(in_gpu), int(out_gpu)])\n    cuda.memcpy_dtoh(out_cpu, out_gpu)\n    return out_cpu\n\nif __name__ == ""__main__"":\n    # inputs = np.random.random((1, 3, 16, 112, 112)).astype(np.float32)\n    inputs = np.random.random((1, 3, 1120, 224)).astype(np.float32)\n    engine = build_engine(onnx_model)\n    context = engine.create_execution_context()\n    for _ in range(10):\n        t1 = time.time()\n        in_cpu, out_cpu, in_gpu, out_gpu, stream = alloc_buf(engine)\n        res = inference(engine, context, inputs.reshape(-1), out_cpu, in_gpu, out_gpu, stream)\n        print(res)\n        print(""cost time: "", time.time()-t1)\n\n\n# tensorrt docker image: docker pull nvcr.io/nvidia/tensorrt:19.09-py3 (See: https://ngc.nvidia.com/catalog/containers/nvidia:tensorrt/tags)\n# NOTE: cuda driver >= 418\n\n\n\n\n\n# batch_time = AverageMeter()\n# input_var = Variable(torch.randn(1, 3, 8, 112, 112).cuda())\n# end_time = time.time()\n\n# for i in range(10000):\n\n#   output = model(input_var)\n#   batch_time.update(time.time() - end_time)\n#   end_time = time.time()\n#   print(""Current average time: "", batch_time.avg, ""Speed (vps): "", 1 / (batch_time.avg / 1.0) )\n\n# print(""Average time for GPU: "", batch_time.avg, ""Speed (vps): "", 1 / (batch_time.avg / 1.0))\n\n\n\n\n\n\n\n\n\n# import time\n# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n# from torch import optim\n# from torch.autograd import Variable\n\n# from utils import AverageMeter, calculate_accuracy\n# from models import squeezenet, shufflenetv2, shufflenet, mobilenet, mobilenetv2, c3d, resnetl\n\n# try:\n#     from apex.parallel import DistributedDataParallel as DDP\n#     from apex.fp16_utils import *\n#     from apex import amp, optimizers\n#     from apex.multi_tensor_apply import multi_tensor_applier\n# except ImportError:\n#     raise ImportError(""Please install apex from https://www.github.com/nvidia/apex to run this example."")\n\n\n# # model = shufflenet.get_model(groups=3, width_mult=0.5, num_classes=600)#1\n# # model = shufflenetv2.get_model( width_mult=0.25, num_classes=600, sample_size = 112)#2\n# # model = mobilenet.get_model( width_mult=0.5, num_classes=600, sample_size = 112)#3\n# # model = mobilenetv2.get_model( width_mult=0.2, num_classes=600, sample_size = 112)#4\n# # model = shufflenet.get_model(groups=3, width_mult=1.0, num_classes=600)#5\n# # model = shufflenetv2.get_model( width_mult=1.0, num_classes=600, sample_size = 112)#6\n# # model = mobilenet.get_model( width_mult=1.0, num_classes=600, sample_size = 112)#7\n# # model = mobilenetv2.get_model( width_mult=0.45, num_classes=600, sample_size = 112)#8\n# # model = shufflenet.get_model(groups=3, width_mult=1.5, num_classes=600)#9\n# # model = shufflenetv2.get_model( width_mult=1.5, num_classes=600, sample_size = 112)#10\n# # model = mobilenet.get_model( width_mult=1.5, num_classes=600, sample_size = 112)#11\n# # model = mobilenetv2.get_model( width_mult=0.7, num_classes=600, sample_size = 112)#12\n# # model = shufflenet.get_model(groups=3, width_mult=2.0, num_classes=600)#13\n# # model = shufflenetv2.get_model( width_mult=2.0, num_classes=600, sample_size = 112)#14\n# # model = mobilenet.get_model( width_mult=2.0, num_classes=600, sample_size = 112)#15\n# # model = mobilenetv2.get_model( width_mult=1.0, num_classes=600, sample_size = 112)#16\n# # model = squeezenet.get_model( version=1.1, num_classes=600, sample_size = 112, sample_duration = 8)\n# model = resnetl.resnetl10(num_classes=2, sample_size = 112, sample_duration = 8)\n# model = model.cuda()\n# #model = nn.DataParallel(model, device_ids=None)\t\n# optimizer = optim.SGD(\n#             model.parameters(),\n#             lr=0.001)\n# print(model)\n\n# print(""\\nCUDNN VERSION: {}\\n"".format(torch.backends.cudnn.version()))\n# assert torch.backends.cudnn.enabled, ""Amp requires cudnn backend to be enabled.""\n\n# model, optimizer = amp.initialize(model, optimizer,\n#                                       opt_level=\'O3\',\n#                                       keep_batchnorm_fp32=True,\n#                                       loss_scale=None\n#                                       )\n\n# batch_time = AverageMeter()\n# input_var = Variable(torch.randn(1, 3, 8, 112, 112).cuda())\n# end_time = time.time()\n\n# for i in range(10000):\n\n# \toutput = model(input_var)\n# \tbatch_time.update(time.time() - end_time)\n# \tend_time = time.time()\n# \tprint(""Current average time: "", batch_time.avg, ""Speed (vps): "", 1 / (batch_time.avg / 1.0) )\n\n# print(""Average time for GPU: "", batch_time.avg, ""Speed (vps): "", 1 / (batch_time.avg / 1.0))\n'"
target_transforms.py,0,"b""import random\nimport math\n\n\nclass Compose(object):\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, target):\n        dst = []\n        for t in self.transforms:\n            dst.append(t(target))\n        return dst\n\n\nclass ClassLabel(object):\n\n    def __call__(self, target):\n        return target['label']\n\n\nclass VideoID(object):\n\n    def __call__(self, target):\n        return target['video_id']\n"""
temporal_transforms.py,0,"b'import random\nimport math\n\n\nclass LoopPadding(object):\n\n    def __init__(self, size, downsample):\n        self.size = size\n        self.downsample = downsample\n\n    def __call__(self, frame_indices):\n        vid_duration  = len(frame_indices)\n        clip_duration = self.size * self.downsample\n        out = frame_indices\n\n        for index in out:\n            if len(out) >= clip_duration:\n                break\n            out.append(index)\n\n        selected_frames = [out[i] for i in range(0, clip_duration, self.downsample)]\n\n        return out\n\n\nclass TemporalBeginCrop(object):\n    """"""Temporally crop the given frame indices at a beginning.\n\n    If the number of frames is less than the size,\n    loop the indices as many times as necessary to satisfy the size.\n\n    Args:\n        size (int): Desired output size of the crop.\n    """"""\n\n    def __init__(self, size, downsample):\n        self.size = size\n        self.downsample = downsample\n\n    def __call__(self, frame_indices):\n        vid_duration  = len(frame_indices)\n        clip_duration = self.size * self.downsample\n\n        out = frame_indices[:clip_duration]\n\n        for index in out:\n            if len(out) >= clip_duration:\n                break\n            out.append(index)\n\n        selected_frames = [out[i] for i in range(0, clip_duration, self.downsample)]\n\n        return selected_frames\n\n\nclass TemporalCenterCrop(object):\n    """"""Temporally crop the given frame indices at a center.\n\n    If the number of frames is less than the size,\n    loop the indices as many times as necessary to satisfy the size.\n\n    Args:\n        size (int): Desired output size of the crop.\n    """"""\n\n    def __init__(self, size, downsample):\n        self.size = size\n        self.downsample = downsample\n\n    def __call__(self, frame_indices):\n        """"""\n        Args:\n            frame_indices (list): frame indices to be cropped.\n        Returns:\n            list: Cropped frame indices.\n        """"""\n        vid_duration  = len(frame_indices)\n        clip_duration = self.size * self.downsample\n\n        center_index = len(frame_indices) // 2\n        begin_index = max(0, center_index - (clip_duration // 2))\n        end_index = min(begin_index + clip_duration, vid_duration)\n\n        out = frame_indices[begin_index:end_index]\n\n        for index in out:\n            if len(out) >= clip_duration:\n                break\n            out.append(index)\n\n        selected_frames = [out[i] for i in range(0, clip_duration, self.downsample)]\n\n        return selected_frames\n\n\nclass TemporalRandomCrop(object):\n    """"""Temporally crop the given frame indices at a random location.\n\n    If the number of frames is less than the size,\n    loop the indices as many times as necessary to satisfy the size.\n\n    Args:\n        size (int): Desired output size of the crop.\n    """"""\n\n    def __init__(self, size, downsample):\n        self.size = size\n        self.downsample = downsample\n\n    def __call__(self, frame_indices):\n        """"""\n        Args:\n            frame_indices (list): frame indices to be cropped.\n        Returns:\n            list: Cropped frame indices.\n        """"""\n\n        vid_duration  = len(frame_indices)\n        clip_duration = self.size * self.downsample\n\n        rand_end = max(0, vid_duration - clip_duration - 1)\n        begin_index = random.randint(0, rand_end)\n        end_index = min(begin_index + clip_duration, vid_duration)\n\n        out = frame_indices[begin_index:end_index]\n\n        for index in out:\n            if len(out) >= clip_duration:\n                break\n            out.append(index)\n\n        selected_frames = [out[i] for i in range(0, clip_duration, self.downsample)]\n\n        return selected_frames\n'"
test.py,6,"b""import torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport time\nimport os\nimport sys\nimport json\n\nfrom utils import AverageMeter\n\n\ndef calculate_video_results(output_buffer, video_id, test_results, class_names):\n    video_outputs = torch.stack(output_buffer)\n    average_scores = torch.mean(video_outputs, dim=0)\n    sorted_scores, locs = torch.topk(average_scores, k=10)\n\n    video_results = []\n    for i in range(sorted_scores.size(0)):\n        video_results.append({\n            'label': class_names[int(locs[i])],\n            'score': float(sorted_scores[i])\n        })\n\n    test_results['results'][video_id] = video_results\n\n\ndef test(data_loader, model, opt, class_names):\n    print('test')\n\n    model.eval()\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n\n    end_time = time.time()\n    output_buffer = []\n    previous_video_id = ''\n    test_results = {'results': {}}\n    for i, (inputs, targets) in enumerate(data_loader):\n        data_time.update(time.time() - end_time)\n\n        with torch.no_grad():\n            inputs = Variable(inputs)\n        outputs = model(inputs)\n        if not opt.no_softmax_in_test:\n            outputs = F.softmax(outputs, dim=1)\n\n        for j in range(outputs.size(0)):\n            if not (i == 0 and j == 0) and targets[j] != previous_video_id:\n                calculate_video_results(output_buffer, previous_video_id,\n                                        test_results, class_names)\n                output_buffer = []\n            output_buffer.append(outputs[j].data.cpu())\n            previous_video_id = targets[j]\n\n        if (i % 100) == 0:\n            with open(\n                    os.path.join(opt.result_path, '{}.json'.format(\n                        opt.test_subset)), 'w') as f:\n                json.dump(test_results, f)\n\n        batch_time.update(time.time() - end_time)\n        end_time = time.time()\n\n        print('[{}/{}]\\t'\n              'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n              'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'.format(\n                  i + 1,\n                  len(data_loader),\n                  batch_time=batch_time,\n                  data_time=data_time))\n    with open(\n            os.path.join(opt.result_path, '{}.json'.format(opt.test_subset)),\n            'w') as f:\n        json.dump(test_results, f)"""
test_models.py,7,"b'import argparse\nimport time\nimport os\nimport sys\nimport json\nimport shutil\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\nfrom sklearn.metrics import confusion_matrix\nfrom torch.nn import functional as F\n\nfrom opts import parse_opts\nfrom model import generate_model\nfrom dataset import get_training_set, get_validation_set, get_test_set\nfrom mean import get_mean, get_std\nfrom spatial_transforms import *\nfrom temporal_transforms import *\nfrom target_transforms import ClassLabel, VideoID\nfrom target_transforms import Compose as TargetCompose\nfrom dataset import get_training_set, get_validation_set, get_test_set\nfrom utils import Logger\nfrom train import train_epoch\nfrom validation import val_epoch\nimport test\nfrom utils import AverageMeter\n\n\n""""""\ndef calculate_accuracy(outputs, targets, topk=(1,)):\n    maxk = max(topk)\n    batch_size = targets.size(0)\n\n    _, pred = outputs.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(targets.view(1, -1).expand_as(pred))\n    ret = []\n    for k in topk:\n        correct_k = correct[:k].float().sum().data[0]\n        ret.append(correct_k / batch_size)\n\n    return ret\n""""""\n\n\n\ndef calculate_accuracy(outputs, targets, topk=(1,)):\n    maxk = max(topk)\n    batch_size = targets.size(0)\n\n    _, pred = outputs.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(targets.view(1, -1).expand_as(pred))\n    ret = []\n    for k in topk:\n        correct_k = correct[:k].float().sum().data[0]\n        ret.append(correct_k / batch_size)\n\n    return ret\n\n\nopt = parse_opts()\nif opt.root_path != \'\':\n    opt.video_path = os.path.join(opt.root_path, opt.video_path)\n    opt.annotation_path = os.path.join(opt.root_path, opt.annotation_path)\n    opt.result_path = os.path.join(opt.root_path, opt.result_path)\n    if opt.resume_path:\n        opt.resume_path = os.path.join(opt.root_path, opt.resume_path)\n    if opt.pretrain_path:\n        opt.pretrain_path = os.path.join(opt.root_path, opt.pretrain_path)\nopt.scales = [opt.initial_scale]\nfor i in range(1, opt.n_scales):\n    opt.scales.append(opt.scales[-1] * opt.scale_step)\nopt.arch = \'{}-{}\'.format(opt.model, opt.model_depth)\nopt.mean = get_mean(opt.norm_value, dataset=opt.mean_dataset)\nopt.std = get_std(opt.norm_value)\n\nprint(opt)\nwith open(os.path.join(opt.result_path, \'opts.json\'), \'w\') as opt_file:\n    json.dump(vars(opt), opt_file)\n\ntorch.manual_seed(opt.manual_seed)\n\nmodel, parameters = generate_model(opt)\nprint(model)\npytorch_total_params = sum(p.numel() for p in model.parameters() if\n                           p.requires_grad)\nprint(""Total number of trainable parameters: "", pytorch_total_params)\n\nif opt.no_mean_norm and not opt.std_norm:\n    norm_method = Normalize([0, 0, 0], [1, 1, 1])\nelif not opt.std_norm:\n    norm_method = Normalize(opt.mean, [1, 1, 1])\nelse:\n    norm_method = Normalize(opt.mean, opt.std)\n\n\nspatial_transform = Compose([\n    #Scale(opt.sample_size),\n    Scale(112),\n    CenterCrop(112),\n    ToTensor(opt.norm_value), norm_method\n    ])\ntemporal_transform = TemporalCenterCrop(opt.sample_duration)\n#temporal_transform = TemporalBeginCrop(opt.sample_duration)\n#temporal_transform = TemporalEndCrop(opt.sample_duration)\ntarget_transform = ClassLabel()\nvalidation_data = get_validation_set(\n    opt, spatial_transform, temporal_transform, target_transform)\ndata_loader = torch.utils.data.DataLoader(\n    validation_data,\n    batch_size=1,\n    shuffle=False,\n    num_workers=opt.n_threads,\n    pin_memory=True)\nval_logger = Logger(os.path.join(opt.result_path, \'val.log\'), [\'epoch\', \'loss\', \'acc\'])\n\nif opt.resume_path:\n    print(\'loading checkpoint {}\'.format(opt.resume_path))\n    checkpoint = torch.load(opt.resume_path)\n    assert opt.arch == checkpoint[\'arch\']\n\n    opt.begin_epoch = checkpoint[\'epoch\']\n    model.load_state_dict(checkpoint[\'state_dict\'])\n\nrecorder = []\n\nprint(\'run\')\n\nmodel.eval()\n\nbatch_time = AverageMeter()\ntop1 = AverageMeter()\ntop5 = AverageMeter()\n\nend_time = time.time()\nfor i, (inputs, targets) in enumerate(data_loader):\n    if not opt.no_cuda:\n        targets = targets.cuda(async=True)\n    #inputs = Variable(torch.squeeze(inputs), volatile=True)\n    inputs = Variable(inputs, volatile=True)\n    targets = Variable(targets, volatile=True)\n    outputs = model(inputs)\n    recorder.append(outputs.data.cpu().numpy().copy())\n    #outputs = torch.unsqueeze(torch.mean(outputs, 0), 0)\n    prec1, prec5 = calculate_accuracy(outputs, targets, topk=(1, 5))\n\n    top1.update(prec1, inputs.size(0))\n    top5.update(prec5, inputs.size(0))\n\n    batch_time.update(time.time() - end_time)\n    end_time = time.time()\n\n    print(\'[{0}/{1}]\\t\'\n          \'Time {batch_time.val:.5f} ({batch_time.avg:.5f})\\t\'\n          \'prec@1 {top1.avg:.5f} prec@5 {top5.avg:.5f}\'.format(\n              i + 1,\n              len(data_loader),\n              batch_time=batch_time,\n              top1 =top1,\n              top5=top5))\n\nvideo_pred = [np.argmax(np.mean(x, axis=0)) for x in recorder]\nprint(video_pred)\n\nwith open(\'annotation_Something/categories.txt\') as f:\n    lines = f.readlines()\n    categories = [item.rstrip() for item in lines]\n\nname_list = [x.strip().split()[0] for x in open(\'annotation_Something/testlist01.txt\')]\norder_dict = {e:i for i, e in enumerate(sorted(name_list))}\nreorder_output = [None] * len(recorder)\nreorder_pred = [None] * len(recorder)\noutput_csv = []\nfor i in range(len(recorder)):\n    idx = order_dict[name_list[i]]\n    reorder_output[idx] = recorder[i]\n    reorder_pred[idx] = video_pred[i]\n    output_csv.append(\'%s;%s\'%(name_list[i],\n                               categories[video_pred[i]]))\n\n    with open(\'something_predictions.csv\',\'w\') as f:\n        f.write(\'\\n\'.join(output_csv))\n\n\n\nprint(\'-----Evaluation is finished------\')\nprint(\'Overall Prec@1 {:.05f}% Prec@5 {:.05f}%\'.format(top1.avg, top5.avg))\n'"
train.py,2,"b""import torch\nfrom torch.autograd import Variable\nimport time\nimport os\nimport sys\n\nfrom utils import *\n\n\ndef train_epoch(epoch, data_loader, model, criterion, optimizer, opt,\n                epoch_logger, batch_logger):\n    print('train at epoch {}'.format(epoch))\n\n    model.train()\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    end_time = time.time()\n    for i, (inputs, targets) in enumerate(data_loader):\n        data_time.update(time.time() - end_time)\n\n        if not opt.no_cuda:\n            targets = targets.cuda()\n        inputs = Variable(inputs)\n        targets = Variable(targets)\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n\n        losses.update(loss.data, inputs.size(0))\n        prec1, prec5 = calculate_accuracy(outputs.data, targets.data, topk=(1,5))\n        top1.update(prec1, inputs.size(0))\n        top5.update(prec5, inputs.size(0))\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        batch_time.update(time.time() - end_time)\n        end_time = time.time()\n\n        batch_logger.log({\n            'epoch': epoch,\n            'batch': i + 1,\n            'iter': (epoch - 1) * len(data_loader) + (i + 1),\n            'loss': losses.val.item(),\n            'prec1': top1.val.item(),\n            'prec5': top5.val.item(),\n            'lr': optimizer.param_groups[0]['lr']\n        })\n        if i % 10 ==0:\n            print('Epoch: [{0}][{1}/{2}]\\t lr: {lr:.5f}\\t'\n                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                  'Prec@1 {top1.val:.5f} ({top1.avg:.5f})\\t'\n                  'Prec@5 {top5.val:.5f} ({top5.avg:.5f})'.format(\n                      epoch,\n                      i,\n                      len(data_loader),\n                      batch_time=batch_time,\n                      data_time=data_time,\n                      loss=losses,\n                      top1=top1,\n                      top5=top5,\n                      lr=optimizer.param_groups[0]['lr']))\n\n    epoch_logger.log({\n        'epoch': epoch,\n        'loss': losses.avg.item(),\n        'prec1': top1.avg.item(),\n        'prec5': top5.avg.item(),\n        'lr': optimizer.param_groups[0]['lr']\n    })\n\n    #if epoch % opt.checkpoint == 0:\n    #    save_file_path = os.path.join(opt.result_path,\n    #                                  'save_{}.pth'.format(epoch))\n    #    states = {\n    #        'epoch': epoch + 1,\n    #        'arch': opt.arch,\n    #        'state_dict': model.state_dict(),\n    #        'optimizer': optimizer.state_dict(),\n    #    }\n    #    torch.save(states, save_file_path)\n"""
utils.py,1,"b'import csv\nimport torch\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\nimport shutil\nimport numpy as np\n\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\nclass Logger(object):\n\n    def __init__(self, path, header):\n        self.log_file = open(path, \'w\')\n        self.logger = csv.writer(self.log_file, delimiter=\'\\t\')\n\n        self.logger.writerow(header)\n        self.header = header\n\n    def __del(self):\n        self.log_file.close()\n\n    def log(self, values):\n        write_values = []\n        for col in self.header:\n            assert col in values\n            write_values.append(values[col])\n\n        self.logger.writerow(write_values)\n        self.log_file.flush()\n\n\n\nclass Queue:\n    # Constructor creates a list\n    def __init__(self, max_size, n_classes):\n        self.queue = list(np.zeros((max_size, n_classes), dtype=float).tolist())\n        self.max_size = max_size\n        self.median = None\n        self.ma = None\n        self.ewma = None\n\n    # Adding elements to queue\n    def enqueue(self, data):\n        self.queue.insert(0, data)\n        self.median = self._median()\n        self.ma = self._ma()\n        self.ewma = self._ewma()\n        return True\n\n    # Removing the last element from the queue\n    def dequeue(self):\n        if len(self.queue) > 0:\n            return self.queue.pop()\n        return (""Queue Empty!"")\n\n    # Getting the size of the queue\n    def size(self):\n        return len(self.queue)\n\n    # printing the elements of the queue\n    def printQueue(self):\n        return self.queue\n\n    # Average\n    def _ma(self):\n        return np.array(self.queue[:self.max_size]).mean(axis=0)\n\n    # Median\n    def _median(self):\n        return np.median(np.array(self.queue[:self.max_size]), axis=0)\n\n    # Exponential average\n    def _ewma(self):\n        weights = np.exp(np.linspace(-1., 0., self.max_size))\n        weights /= weights.sum()\n        average = weights.reshape(1, self.max_size).dot(np.array(self.queue[:self.max_size]))\n        return average.reshape(average.shape[1], )\n\n\ndef LevenshteinDistance(a, b):\n    # This is a straightforward implementation of a well-known algorithm, and thus\n    # probably shouldn\'t be covered by copyright to begin with. But in case it is,\n    # the author (Magnus Lie Hetland) has, to the extent possible under law,\n    # dedicated all copyright and related and neighboring rights to this software\n    # to the public domain worldwide, by distributing it under the CC0 license,\n    # version 1.0. This software is distributed without any warranty. For more\n    # information, see <http://creativecommons.org/publicdomain/zero/1.0>\n    ""Calculates the Levenshtein distance between a and b.""\n    n, m = len(a), len(b)\n    if n > m:\n        # Make sure n <= m, to use O(min(n,m)) space\n        a, b = b, a\n        n, m = m, n\n\n    current = range(n + 1)\n    for i in range(1, m + 1):\n        previous, current = current, [i] + [0] * n\n        for j in range(1, n + 1):\n            add, delete = previous[j] + 1, current[j - 1] + 1\n            change = previous[j - 1]\n            if a[j - 1] != b[i - 1]:\n                change = change + 1\n            current[j] = min(add, delete, change)\n    if current[n]<0:\n        return 0\n    else:\n        return current[n]\n\n\ndef load_value_file(file_path):\n    with open(file_path, \'r\') as input_file:\n        value = float(input_file.read().rstrip(\'\\n\\r\'))\n\n    return value\n\n\ndef calculate_accuracy(output, target, topk=(1,)):\n    """"""Computes the precision@k for the specified values of k""""""\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n\n\ndef calculate_precision(outputs, targets):\n\n    _, pred = outputs.topk(1, 1, True)\n    pred = pred.t()\n    return  precision_score(targets.view(-1), pred.view(-1), average = \'macro\')\n\n\ndef calculate_recall(outputs, targets):\n\n    _, pred = outputs.topk(1, 1, True)\n    pred = pred.t()\n    return  recall_score(targets.view(-1), pred.view(-1), average = \'macro\')\n\n\ndef save_checkpoint(state, is_best, opt):\n    torch.save(state, \'%s/%s_checkpoint.pth\' % (opt.result_path, opt.store_name))\n    if is_best:\n        shutil.copyfile(\'%s/%s_checkpoint.pth\' % (opt.result_path, opt.store_name),\'%s/%s_best.pth\' % (opt.result_path, opt.store_name))\n\n\ndef adjust_learning_rate(optimizer, epoch, opt):\n    """"""Sets the learning rate to the initial LR decayed by 10 every 30 epochs""""""\n    lr_new = opt.learning_rate * (0.1 ** (sum(epoch >= np.array(opt.lr_steps))))\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr_new\n        #param_group[\'lr\'] = opt.learning_rate\n\n\n'"
validation.py,2,"b""import torch\nfrom torch.autograd import Variable\nimport time\nimport sys\n\nfrom utils import *\n\n\ndef val_epoch(epoch, data_loader, model, criterion, opt, logger):\n    print('validation at epoch {}'.format(epoch))\n\n    model.eval()\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    end_time = time.time()\n    for i, (inputs, targets) in enumerate(data_loader):\n        data_time.update(time.time() - end_time)\n\n        if not opt.no_cuda:\n            targets = targets.cuda()\n        with torch.no_grad():\n            inputs = Variable(inputs)\n            targets = Variable(targets)\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        prec1, prec5 = calculate_accuracy(outputs.data, targets.data, topk=(1,5))\n        top1.update(prec1, inputs.size(0))\n        top5.update(prec5, inputs.size(0))\n\n        losses.update(loss.data, inputs.size(0))\n\n        batch_time.update(time.time() - end_time)\n        end_time = time.time()\n\n        if i % 10 ==0:\n          print('Epoch: [{0}][{1}/{2}]\\t'\n              'Time {batch_time.val:.5f} ({batch_time.avg:.5f})\\t'\n              'Data {data_time.val:.5f} ({data_time.avg:.5f})\\t'\n              'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n              'Prec@1 {top1.val:.5f} ({top1.avg:.5f})\\t'\n              'Prec@5 {top5.val:.5f} ({top5.avg:.5f})'.format(\n                  epoch,\n                  i + 1,\n                  len(data_loader),\n                  batch_time=batch_time,\n                  data_time=data_time,\n                  loss=losses,\n                  top1=top1,\n                  top5=top5))\n\n    logger.log({'epoch': epoch,\n                'loss': losses.avg.item(),\n                'prec1': top1.avg.item(),\n                'prec5': top5.avg.item()})\n\n    return losses.avg.item(), top1.avg.item()"""
datasets/egogesture.py,2,"b'import torch\nimport torch.utils.data as data\nfrom PIL import Image\nfrom spatial_transforms import *\nimport os\nimport math\nimport functools\nimport json\nimport copy\nfrom numpy.random import randint\nimport numpy as np\nimport random\nimport glob\n\nimport pdb\n\n\ndef pil_loader(path, modality):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, \'rb\') as f:\n        #print(path)\n        with Image.open(f) as img:\n            if modality == \'RGB\':\n                return img.convert(\'RGB\')\n            elif modality == \'Flow\':\n                return img.convert(\'L\')\n            elif modality == \'Depth\':\n                return img.convert(\'L\') # 8-bit pixels, black and white check from https://pillow.readthedocs.io/en/3.0.x/handbook/concepts.html\n\n\ndef accimage_loader(path, modality):\n    try:\n        import accimage\n        return accimage.Image(path)\n    except IOError:\n        # Potentially a decoding problem, fall back to PIL.Image\n        return pil_loader(path)\n\n\ndef get_default_image_loader():\n    from torchvision import get_image_backend\n    if get_image_backend() == \'accimage\':\n        return accimage_loader\n    else:\n        return pil_loader\n\n\ndef video_loader(video_dir_path, frame_indices, modality, sample_duration, image_loader):\n    video = []\n    if modality == \'RGB\':\n        for i in frame_indices:\n            image_path = os.path.join(video_dir_path, \'{:06d}.jpg\'.format(i))\n            if os.path.exists(image_path):\n                video.append(image_loader(image_path, modality))\n            else:\n                print(image_path, ""------- Does not exist"")\n                return video\n    elif modality == \'Depth\':\n\n        for i in frame_indices:\n            image_path = os.path.join(video_dir_path.rsplit(os.sep,2)[0] , \'Depth\',\'depth\' + video_dir_path[-1], \'{:06d}.jpg\'.format(i) )\n            if os.path.exists(image_path):\n                video.append(image_loader(image_path, modality))\n            else:\n                print(image_path, ""------- Does not exist"")\n                return video\n    elif modality == \'RGB-D\':\n        for i in frame_indices: # index 35 is used to change img to flow\n            image_path = os.path.join(video_dir_path, \'{:06d}.jpg\'.format(i))\n            image_path_depth = os.path.join(video_dir_path.rsplit(os.sep,2)[0] , \'Depth\',\'depth\' + video_dir_path[-1], \'{:06d}.jpg\'.format(i) )\n    \n            image = image_loader(image_path, \'RGB\')\n            image_depth = image_loader(image_path_depth, \'Depth\')\n            if os.path.exists(image_path):\n                video.append(image)\n                video.append(image_depth)\n            else:\n                print(image_path, ""------- Does not exist"")\n                return video\n    return video\n\ndef get_default_video_loader():\n    image_loader = get_default_image_loader()\n    return functools.partial(video_loader, image_loader=image_loader)\n\n\ndef load_annotation_data(data_file_path):\n    with open(data_file_path, \'r\') as data_file:\n        return json.load(data_file)\n\n\ndef get_class_labels(data):\n    class_labels_map = {}\n    index = 0\n    for class_label in data[\'labels\']:\n        class_labels_map[class_label] = index\n        index += 1\n    return class_labels_map\n\n\ndef get_video_names_and_annotations(data, subset):\n    video_names = []\n    annotations = []\n\n    for key, value in data[\'database\'].items():\n        this_subset = value[\'subset\']\n        if this_subset in subset:\n            label = value[\'annotations\'][\'label\']\n            video_names.append(key.split(\'_\')[0])\n            annotations.append(value[\'annotations\'])\n\n    return video_names, annotations\n\n\ndef make_dataset(root_path, annotation_path, subset, n_samples_for_each_video,\n                 sample_duration):\n    if type(subset)==list:\n        subset = subset\n    else:\n        subset =  [subset]\n    data = load_annotation_data(annotation_path)\n    video_names, annotations = get_video_names_and_annotations(data, subset)\n    class_to_idx = get_class_labels(data)\n    idx_to_class = {}\n    for name, label in class_to_idx.items():\n        idx_to_class[label] = name\n\n    dataset = []\n    list_subset = \'\'\n    for x in subset:\n        list_subset += x+\',\' \n    print(""[INFO]: EgoGesture Dataset - "" + list_subset + "" is loading..."")\n    for i in range(len(video_names)):\n        if i % 1000 == 0:\n            print(\'dataset loading [{}/{}]\'.format(i, len(video_names)))\n\n        video_path = os.path.join(root_path, video_names[i])\n        \n        if not os.path.exists(video_path):\n            print(video_path + "" does not exist"")\n            continue\n\n        #### Add more frames from start end end\n        begin_t = int(float(annotations[i][\'start_frame\']))\n        end_t = int(float(annotations[i][\'end_frame\']))\n        n_frames = end_t - begin_t + 1\n        sample = {\n            \'video\': video_path,\n            \'segment\': [begin_t, end_t],\n            \'n_frames\': n_frames,\n            \'video_id\': i\n        }\n        if len(annotations) != 0:\n            sample[\'label\'] = class_to_idx[annotations[i][\'label\']]\n        else:\n            sample[\'label\'] = -1\n\n        if n_samples_for_each_video == 1:\n            sample[\'frame_indices\'] = list(range(begin_t, end_t + 1))\n            dataset.append(sample)\n        else:\n            if n_samples_for_each_video > 1:\n                step = max(1,\n                           math.ceil((n_frames - 1 - sample_duration) /\n                                     (n_samples_for_each_video - 1)))\n            else:\n                step = sample_duration\n            for j in range(1, n_frames, step):\n                sample_j = copy.deepcopy(sample)\n                sample_j[\'frame_indices\'] = list(\n                    range(j, min(n_frames + 1, j + sample_duration)))\n                dataset.append(sample_j)\n    return dataset, idx_to_class\n\n\nclass EgoGesture(data.Dataset):\n    """"""\n    Args:\n        root (string): Root directory path.\n        spatial_transform (callable, optional): A function/transform that  takes in an PIL image\n            and returns a transformed version. E.g, ``transforms.RandomCrop``\n        temporal_transform (callable, optional): A function/transform that  takes in a list of frame indices\n            and returns a transformed version\n        target_transform (callable, optional): A function/transform that takes in the\n            target and transforms it.\n        loader (callable, optional): A function to load an video given its path and frame indices.\n     Attributes:\n        classes (list): List of the class names.\n        class_to_idx (dict): Dict with items (class_name, class_index).\n        imgs (list): List of (image path, class_index) tuples\n    """"""\n\n    def __init__(self,\n                 root_path,\n                 annotation_path,\n                 subset,\n                 n_samples_for_each_video=1,\n                 spatial_transform=None,\n                 temporal_transform=None,\n                 target_transform=None,\n                 sample_duration=16,\n                 modality=\'RGB\',\n                 get_loader=get_default_video_loader):\n\n        if subset == \'training\':\n            subset = [\'training\', \'validation\']\n        self.data, self.class_names = make_dataset(\n            root_path, annotation_path, subset, n_samples_for_each_video,\n            sample_duration)\n\n        self.spatial_transform = spatial_transform\n        self.temporal_transform = temporal_transform\n        self.target_transform = target_transform\n        self.modality = modality\n        self.sample_duration = sample_duration\n        self.loader = get_loader()\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n        Returns:\n            tuple: (image, target) where target is class_index of the target class.\n        """"""\n\n        path = self.data[index][\'video\']\n\n        frame_indices = self.data[index][\'frame_indices\']\n\n        if self.temporal_transform is not None:\n            frame_indices = self.temporal_transform(frame_indices)\n\n        clip = self.loader(path, frame_indices, self.modality, self.sample_duration)\n        oversample_clip =[]\n        if self.spatial_transform is not None:\n            self.spatial_transform.randomize_parameters()\n            clip = [self.spatial_transform(img) for img in clip]\n        \n        im_dim = clip[0].size()[-2:]\n        clip = torch.cat(clip, 0).view((self.sample_duration, -1) + im_dim).permute(1, 0, 2, 3)\n        \n     \n        target = self.data[index]\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        \n        return clip, target\n\n    def __len__(self):\n        return len(self.data)\n\n\n'"
datasets/egogesture_online.py,3,"b'import torch\nimport torch.utils.data as data\nfrom PIL import Image\nfrom spatial_transforms import *\nfrom temporal_transforms import *\nimport os\nimport math\nimport functools\nimport json\nimport copy\nfrom numpy.random import randint\nimport numpy as np\nimport random\n\nimport pdb\n\n\ndef pil_loader(path, modality):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, \'rb\') as f:\n        #print(path)\n        with Image.open(f) as img:\n            if modality == \'RGB\':\n                return img.convert(\'RGB\')\n            elif modality == \'Flow\':\n                return img.convert(\'L\')\n            elif modality == \'Depth\':\n                return img.convert(\'L\') # 8-bit pixels, black and white check from https://pillow.readthedocs.io/en/3.0.x/handbook/concepts.html\n\n\ndef accimage_loader(path, modality):\n    try:\n        import accimage\n        return accimage.Image(path)\n    except IOError:\n        # Potentially a decoding problem, fall back to PIL.Image\n        return pil_loader(path)\n\n\ndef get_default_image_loader():\n    from torchvision import get_image_backend\n    if get_image_backend() == \'accimage\':\n        return accimage_loader\n    else:\n        return pil_loader\n\n\ndef video_loader(video_dir_path, frame_indices, modality, sample_duration, image_loader):\n    \n    video = []\n    if modality == \'RGB\':\n        for i in frame_indices:\n            image_path = os.path.join(video_dir_path, \'{:06d}.jpg\'.format(i))\n            if os.path.exists(image_path):\n                \n                video.append(image_loader(image_path, modality))\n            else:\n                print(image_path, ""------- Does not exist"")\n                return video\n    elif modality == \'Depth\':\n\n        for i in frame_indices:\n            image_path = os.path.join(video_dir_path.rsplit(os.sep,2)[0] , \'Depth\',\'depth\' + video_dir_path[-1], \'{:06d}.jpg\'.format(i) )\n            if os.path.exists(image_path):\n                video.append(image_loader(image_path, modality))\n            else:\n                print(image_path, ""------- Does not exist"")\n                return video\n    elif modality == \'RGB-D\':\n        for i in frame_indices: # index 35 is used to change img to flow\n            image_path = os.path.join(video_dir_path, \'{:06d}.jpg\'.format(i))\n            image_path_depth = os.path.join(video_dir_path.rsplit(os.sep,2)[0] , \'Depth\',\'depth\' + video_dir_path[-1], \'{:06d}.jpg\'.format(i) )\n            \n\n            image = image_loader(image_path, \'RGB\')\n            image_depth = image_loader(image_path_depth, \'Depth\')\n\n            if os.path.exists(image_path):\n                video.append(image)\n                video.append(image_depth)\n            else:\n                print(image_path, ""------- Does not exist"")\n                return video\n    \n    return video\n\ndef get_default_video_loader():\n    image_loader = get_default_image_loader()\n    return functools.partial(video_loader, image_loader=image_loader)\n\n\ndef load_annotation_data(data_file_path):\n    with open(data_file_path, \'r\') as data_file:\n        return json.load(data_file)\n\n\ndef get_class_labels(data):\n    class_labels_map = {}\n    index = 0\n    for class_label in data[\'labels\']:\n        class_labels_map[class_label] = index\n        index += 1\n    return class_labels_map\n\n\ndef get_annotation(data, whole_path):\n    annotation = []\n    print(""@@@@@@@@@@@@@@"", whole_path)\n\n    for key, value in data[\'database\'].items():\n        if key.split(\'_\')[0] == whole_path:\n            annotation.append(value[\'annotations\'])\n\n    return  annotation\n\n\ndef make_dataset( annotation_path, video_path , whole_path,sample_duration, n_samples_for_each_video, stride_len):\n    \n    data = load_annotation_data(annotation_path)\n    whole_video_path = os.path.join(video_path, whole_path)\n    annotation = get_annotation(data, whole_path)\n    class_to_idx = get_class_labels(data)\n    idx_to_class = {}\n    for name, label in class_to_idx.items():\n        idx_to_class[label] = name\n\n    dataset = []\n    print(""[INFO]: Videot  is loading..."", whole_video_path, whole_path)\n    import glob\n\n    n_frames = len(glob.glob(whole_video_path + \'/*.jpg\'))\n\n    if not os.path.exists(whole_video_path):\n        print(whole_video_path , "" does not exist"")\n    label_list = []\n    for i in range(len(annotation)):\n        begin_t = int(annotation[i][\'start_frame\'])\n        end_t = int(annotation[i][\'end_frame\'])\n        for j in range(begin_t,end_t+1):\n            label_list.append(class_to_idx[annotation[i][\'label\']])\n\n    label_list = np.array(label_list)\n    for _ in range(1,n_frames+1 - sample_duration,stride_len):\n        \n        sample = {\n                \'video\': whole_video_path,\n                \'index\': _ ,\n                \'video_id\' : _ \n\n            }\n        \n        # print(range(_    - int(sample_duration/8), _   ))\n        counts = np.bincount(label_list[np.array(list(range(_    - int(sample_duration/8), _   )))])\n        sample[\'label\'] = np.argmax(counts)\n        if n_samples_for_each_video == 1:\n            sample[\'frame_indices\'] = list(range(_ , _ + sample_duration))\n            dataset.append(sample)\n        else:\n            if n_samples_for_each_video > 1:\n                step = max(1,\n                            math.ceil((n_frames - 1 - sample_duration) /\n                                    (n_samples_for_each_video - 1)))\n            else:\n                step = sample_duration\n            for j in range(sample_duration, n_frames, step):\n                sample_j = copy.deepcopy(sample)\n                sample_j[\'frame_indices\'] = list(\n                    range(j, min(n_frames + 1, j + sample_duration)))\n                dataset.append(sample_j)\n\n    return dataset, idx_to_class\n\n\n\n\nclass EgoGestureOnline(data.Dataset):\n    """"""\n    Args:\n        root (string): Root directory path.\n        spatial_transform (callable, optional): A function/transform that  takes in an PIL image\n            and returns a transformed version. E.g, ``transforms.RandomCrop``\n        temporal_transform (callable, optional): A function/transform that  takes in a list of frame indices\n            and returns a transformed version\n        target_transform (callable, optional): A function/transform that takes in the\n            target and transforms it.\n        loader (callable, optional): A function to load an video given its path and frame indices.\n     Attributes:\n        classes (list): List of the class names.\n        class_to_idx (dict): Dict with items (class_name, class_index).\n        imgs (list): List of (image path, class_index) tuples\n    """"""\n\n    def __init__(self,\n                 annotation_path,\n                 video_path,\n                 whole_path,\n                 n_samples_for_each_video=1,\n                 spatial_transform=None,\n                 temporal_transform=None,\n                 target_transform=None,\n                 sample_duration=16,\n                 modality=\'RGB\',\n                 stride_len = None,\n                 get_loader=get_default_video_loader):\n        self.data, self.class_names = make_dataset(\n         annotation_path, video_path, whole_path, sample_duration,n_samples_for_each_video, stride_len)\n        \n        self.spatial_transform = spatial_transform\n        self.temporal_transform = temporal_transform\n        self.target_transform = target_transform\n        self.modality = modality\n        self.sample_duration = sample_duration\n        self.loader = get_loader()\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n        Returns:\n            tuple: (image, target) where target is class_index of the target class.\n        """"""\n\n        path = self.data[index][\'video\']\n\n        frame_indices = self.data[index][\'frame_indices\']\n\n\n        if self.temporal_transform is not None:\n            frame_indices = self.temporal_transform(frame_indices)\n        \n        clip = self.loader(path, frame_indices, self.modality, self.sample_duration)\n        oversample_clip =[]\n        if self.spatial_transform is not None:\n            self.spatial_transform.randomize_parameters()\n            clip = [self.spatial_transform(img) for img in clip]\n\n        im_dim = clip[0].size()[-2:]\n        try:\n            clip = torch.cat(clip, 0).view((self.sample_duration, -1) + im_dim).permute(1, 0, 2, 3)\n        except Exception as e:\n            pdb.set_trace()\n            raise e\n        \n        \n        # clip = torch.stack(clip, 0).permute(1, 0, 2, 3)\n        target = self.data[index]\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n        \n        return clip, target\n\n    def __len__(self):\n        return len(self.data)\n\n\n# from target_transforms import ClassLabel, VideoID\n# from target_transforms import Compose as TargetCompose\n\n\n# norm_method = Normalize([0, 0, 0], [1, 1, 1])\n\n\n# spatial_transform = Compose([\n#     #Scale(opt.sample_size),\n#     Scale(112),\n#     CenterCrop(112),\n#     ToTensor(), norm_method\n#     ])\n# temporal_transform = TemporalCenterCrop(8)\n# #temporal_transform = TemporalBeginCrop(opt.sample_duration)\n# #temporal_transform = TemporalEndCrop(opt.sample_duration)\n# target_transform = ClassLabel()\n\n\n# vl = VideoLoader(\n# annotation_path = \'/usr/home/kop/MyRes3D-Ahmet/annotation_EgoGesture/egogesturebinary.json\',\n# video_path = \'/data2/EgoGesture/images\',\n# whole_path = \'Subject01/Scene1/Color/rgb1\',\n# n_samples_for_each_video=1,\n# spatial_transform=spatial_transform,\n# temporal_transform=None,\n# target_transform=target_transform,\n# sample_duration=8,\n# modality=\'RGB\',\n# get_loader=get_default_video_loader)\n\n\n\n'"
datasets/jester.py,2,"b'import torch\nimport torch.utils.data as data\nfrom PIL import Image\nimport os\nimport math\nimport functools\nimport json\nimport copy\nfrom numpy.random import randint\nimport numpy as np\nimport random\n\nfrom utils import load_value_file\n\n\ndef pil_loader(path):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, \'rb\') as f:\n        with Image.open(f) as img:\n            return img.convert(\'RGB\')\n\n\ndef accimage_loader(path):\n    try:\n        import accimage\n        return accimage.Image(path)\n    except IOError:\n        # Potentially a decoding problem, fall back to PIL.Image\n        return pil_loader(path)\n\n\ndef get_default_image_loader():\n    from torchvision import get_image_backend\n    if get_image_backend() == \'accimage\':\n        return accimage_loader\n    else:\n        return pil_loader\n\n\ndef video_loader(video_dir_path, frame_indices, sample_duration, image_loader):\n    video = []\n    for i in frame_indices:\n        image_path = os.path.join(video_dir_path, \'{:05d}.jpg\'.format(i))\n        if os.path.exists(image_path):\n            video.append(image_loader(image_path))\n        else:\n            return video\n\n    return video\n\n\ndef get_default_video_loader():\n    image_loader = get_default_image_loader()\n    return functools.partial(video_loader, image_loader=image_loader)\n\n\ndef load_annotation_data(data_file_path):\n    with open(data_file_path, \'r\') as data_file:\n        return json.load(data_file)\n\n\ndef get_class_labels(data):\n    class_labels_map = {}\n    index = 0\n    for class_label in data[\'labels\']:\n        class_labels_map[class_label] = index\n        index += 1\n    return class_labels_map\n\n\ndef get_video_names_and_annotations(data, subset):\n    video_names = []\n    annotations = []\n\n    for key, value in data[\'database\'].items():\n        this_subset = value[\'subset\']\n        if this_subset == subset:\n            label = value[\'annotations\'][\'label\']\n            #video_names.append(\'{}/{}\'.format(label, key))\n            video_names.append(key)\n            annotations.append(value[\'annotations\'])\n\n    return video_names, annotations\n\n\ndef make_dataset(root_path, annotation_path, subset, n_samples_for_each_video,\n                 sample_duration):\n    data = load_annotation_data(annotation_path)\n    video_names, annotations = get_video_names_and_annotations(data, subset)\n    class_to_idx = get_class_labels(data)\n    idx_to_class = {}\n    for name, label in class_to_idx.items():\n        idx_to_class[label] = name\n\n    dataset = []\n    for i in range(len(video_names)):\n        if i % 1000 == 0:\n            print(\'dataset loading [{}/{}]\'.format(i, len(video_names)))\n\n        video_path = os.path.join(root_path, video_names[i])\n        if not os.path.exists(video_path):\n            print(video_path)\n            continue\n\n        n_frames_file_path = os.path.join(video_path, \'n_frames\')\n        n_frames = int(load_value_file(n_frames_file_path))\n        if n_frames <= 0:\n            continue\n\n        begin_t = 1\n        end_t = n_frames\n        sample = {\n            \'video\': video_path,\n            \'segment\': [begin_t, end_t],\n            \'n_frames\': n_frames,\n            #\'video_id\': video_names[i].split(\'/\')[1]\n            \'video_id\': video_names[i]\n        }\n        if len(annotations) != 0:\n            sample[\'label\'] = class_to_idx[annotations[i][\'label\']]\n        else:\n            sample[\'label\'] = -1\n\n        if n_samples_for_each_video == 1:\n            sample[\'frame_indices\'] = list(range(1, n_frames + 1))\n            dataset.append(sample)\n        else:\n            if n_samples_for_each_video > 1:\n                step = max(1,\n                           math.ceil((n_frames - 1 - sample_duration) /\n                                     (n_samples_for_each_video - 1)))\n            else:\n                step = sample_duration\n            for j in range(1, n_frames, step):\n                sample_j = copy.deepcopy(sample)\n                sample_j[\'frame_indices\'] = list(\n                    range(j, min(n_frames + 1, j + sample_duration)))\n                dataset.append(sample_j)\n\n    return dataset, idx_to_class\n\n\nclass Jester(data.Dataset):\n    """"""\n    Args:\n        root (string): Root directory path.\n        spatial_transform (callable, optional): A function/transform that  takes in an PIL image\n            and returns a transformed version. E.g, ``transforms.RandomCrop``\n        temporal_transform (callable, optional): A function/transform that  takes in a list of frame indices\n            and returns a transformed version\n        target_transform (callable, optional): A function/transform that takes in the\n            target and transforms it.\n        loader (callable, optional): A function to load an video given its path and frame indices.\n     Attributes:\n        classes (list): List of the class names.\n        class_to_idx (dict): Dict with items (class_name, class_index).\n        imgs (list): List of (image path, class_index) tuples\n    """"""\n\n    def __init__(self,\n                 root_path,\n                 annotation_path,\n                 subset,\n                 n_samples_for_each_video=1,\n                 spatial_transform=None,\n                 temporal_transform=None,\n                 target_transform=None,\n                 sample_duration=16,\n                 get_loader=get_default_video_loader):\n        self.data, self.class_names = make_dataset(\n            root_path, annotation_path, subset, n_samples_for_each_video,\n            sample_duration)\n\n        self.spatial_transform = spatial_transform\n        self.temporal_transform = temporal_transform\n        self.target_transform = target_transform\n        self.sample_duration = sample_duration\n        self.loader = get_loader()\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n        Returns:\n            tuple: (image, target) where target is class_index of the target class.\n        """"""\n        path = self.data[index][\'video\']\n        frame_indices = self.data[index][\'frame_indices\']\n        if self.temporal_transform is not None:\n           frame_indices = self.temporal_transform(frame_indices)\n        clip = self.loader(path, frame_indices, self.sample_duration)\n        if self.spatial_transform is not None:\n            self.spatial_transform.randomize_parameters()\n            clip = [self.spatial_transform(img) for img in clip]\n        im_dim = clip[0].size()[-2:]\n        clip = torch.stack(clip, 0).permute(1, 0, 2, 3)\n\n        target = self.data[index]\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return clip, target\n\n    def __len__(self):\n        return len(self.data)\n'"
datasets/kinetics.py,2,"b'import torch\nimport torch.utils.data as data\nfrom PIL import Image\nimport os\nimport math\nimport functools\nimport json\nimport copy\nfrom numpy.random import randint\nimport numpy as np\nimport random\nimport cv2\n\nfrom utils import load_value_file\n\n\ndef pil_loader(path):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, \'rb\') as f:\n        with Image.open(f) as img:\n            return img.convert(\'RGB\')\n\n\ndef accimage_loader(path):\n    try:\n        import accimage\n        return accimage.Image(path)\n    except IOError:\n        # Potentially a decoding problem, fall back to PIL.Image\n        return pil_loader(path)\n\n\ndef get_default_image_loader():\n    from torchvision import get_image_backend\n    if get_image_backend() == \'accimage\':\n        return accimage_loader\n    else:\n        return pil_loader\n\n\ndef video_loader(video_dir_path, frame_indices, sample_duration, image_loader):\n    # print(frame_indices)\n    cap = cv2.VideoCapture(video_dir_path)\n    video = []\n    cap.set(1, frame_indices[0])\n    for _ in frame_indices:\n        ret, frame = cap.read()\n        if ret:\n            pil_frame = Image.fromarray(frame)\n            video.append(pil_frame)\n        else:\n            break\n\n    cap.release()\n    \n\n    # Loop as many times for short videos\n    for frame in video:\n        if len(video) >= sample_duration:\n            break\n        video.append(frame)\n    \n    if len(video) == 0: # give an empty clip\n        for _ in range(sample_duration):\n            video.append(Image.new(\'RGB\', (320, 180)))\n\n    return video\n\n\ndef get_default_video_loader():\n    image_loader = get_default_image_loader()\n    return functools.partial(video_loader, image_loader=image_loader)\n\n\ndef load_annotation_data(data_file_path):\n    with open(data_file_path, \'r\') as data_file:\n        return json.load(data_file)\n\n\ndef get_class_labels(data):\n    class_labels_map = {}\n    index = 0\n    for class_label in data[\'labels\']:\n        class_labels_map[class_label] = index\n        index += 1\n    return class_labels_map\n\n\ndef get_video_names_annotations_framenum(data, subset):\n    video_names = []\n    annotations = []\n    framenum    = []\n\n    for key, value in data[\'database\'][subset].items():\n        this_subset = value[\'subset\']\n        framenum.append(value[\'n_frames\'])\n        if this_subset == subset:\n            if subset == \'testing\':\n                video_names.append(\'test/{}\'.format(key))\n            else:\n                label = value[\'annotations\'][\'label\']\n                video_names.append(\'{}/{}\'.format(label, key))\n                annotations.append(value[\'annotations\'])\n\n    return video_names, annotations, framenum\n\n\ndef make_dataset(root_path, annotation_path, subset, n_samples_for_each_video,\n                 sample_duration):\n    data = load_annotation_data(annotation_path)\n    video_names, annotations, framenum = get_video_names_annotations_framenum(data, subset)\n    class_to_idx = get_class_labels(data)\n    idx_to_class = {}\n    for name, label in class_to_idx.items():\n        idx_to_class[label] = name\n\n    dataset = []\n    for i in range(len(video_names)):\n        if i % 1000 == 0:\n            print(\'dataset loading [{}/{}]\'.format(i, len(video_names)))\n\n        video_path = os.path.join(root_path, video_names[i])\n        if not os.path.exists(video_path):\n            print(video_path)\n            continue\n\n        n_frames = framenum[i]\n        if n_frames <= 0:\n            continue\n\n        begin_t = 1\n        end_t = n_frames\n        sample = {\n            \'video\': video_path,\n            \'segment\': [begin_t, end_t],\n            \'n_frames\': n_frames,\n            \'video_id\': video_names[i][:-14].split(\'/\')[1]\n        }\n        if len(annotations) != 0:\n            sample[\'label\'] = class_to_idx[annotations[i][\'label\']]\n        else:\n            sample[\'label\'] = -1\n\n        if n_samples_for_each_video == 1:\n            sample[\'frame_indices\'] = list(range(1, n_frames + 1))\n            dataset.append(sample)\n        else:\n            if n_samples_for_each_video > 1:\n                step = max(1,\n                           math.ceil((n_frames - 1 - sample_duration) /\n                                     (n_samples_for_each_video - 1)))\n            else:\n                step = sample_duration\n            for j in range(1, n_frames, step):\n                sample_j = copy.deepcopy(sample)\n                sample_j[\'frame_indices\'] = list(\n                    range(j, min(n_frames + 1, j + sample_duration)))\n                dataset.append(sample_j)\n\n    return dataset, idx_to_class\n\n\nclass Kinetics(data.Dataset):\n    """"""\n    Args:\n        root (string): Root directory path.\n        spatial_transform (callable, optional): A function/transform that  takes in an PIL image\n            and returns a transformed version. E.g, ``transforms.RandomCrop``\n        temporal_transform (callable, optional): A function/transform that  takes in a list of frame indices\n            and returns a transformed version\n        target_transform (callable, optional): A function/transform that takes in the\n            target and transforms it.\n        loader (callable, optional): A function to load an video given its path and frame indices.\n     Attributes:\n        classes (list): List of the class names.\n        class_to_idx (dict): Dict with items (class_name, class_index).\n        imgs (list): List of (image path, class_index) tuples\n    """"""\n\n    def __init__(self,\n                 root_path,\n                 annotation_path,\n                 subset,\n                 n_samples_for_each_video=1,\n                 spatial_transform=None,\n                 temporal_transform=None,\n                 target_transform=None,\n                 sample_duration=16,\n                 get_loader=get_default_video_loader):\n        self.data, self.class_names = make_dataset(\n            root_path, annotation_path, subset, n_samples_for_each_video,\n            sample_duration)\n\n        self.spatial_transform = spatial_transform\n        self.temporal_transform = temporal_transform\n        self.target_transform = target_transform\n        self.sample_duration = sample_duration\n        self.loader = get_loader()\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n        Returns:\n            tuple: (image, target) where target is class_index of the target class.\n        """"""\n        path = self.data[index][\'video\']\n\n        frame_indices = self.data[index][\'frame_indices\']\n        if self.temporal_transform is not None:\n           frame_indices = self.temporal_transform(frame_indices)\n        clip = self.loader(path, frame_indices, self.sample_duration)\n        if self.spatial_transform is not None:\n            self.spatial_transform.randomize_parameters()\n            clip = [self.spatial_transform(img) for img in clip]\n        clip = torch.stack(clip, 0).permute(1, 0, 2, 3)\n\n        target = self.data[index]\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return clip, target\n\n    def __len__(self):\n        return len(self.data)\n'"
datasets/nv.py,2,"b'import torch\nimport torch.utils.data as data\nfrom PIL import Image\nfrom spatial_transforms import *\nimport os\nimport math\nimport functools\nimport json\nimport copy\nfrom numpy.random import randint\nimport numpy as np\nimport random\n\nfrom utils import load_value_file\nimport pdb\n\n\ndef pil_loader(path, modality):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, \'rb\') as f:\n        #print(path)\n        with Image.open(f) as img:\n            if modality == \'RGB\':\n                return img.convert(\'RGB\')\n            elif modality == \'Depth\':\n                return img.convert(\'L\') # 8-bit pixels, black and white check from https://pillow.readthedocs.io/en/3.0.x/handbook/concepts.html\n\n\ndef accimage_loader(path, modality):\n    try:\n        import accimage\n        return accimage.Image(path)\n    except IOError:\n        # Potentially a decoding problem, fall back to PIL.Image\n        return pil_loader(path)\n\n\ndef get_default_image_loader():\n    from torchvision import get_image_backend\n    if get_image_backend() == \'accimage\':\n        return accimage_loader\n    else:\n        return pil_loader\n\n\ndef video_loader(video_dir_path, frame_indices, modality, sample_duration, image_loader):\n    \n    video = []\n    if modality == \'RGB\':\n        for i in frame_indices:\n            image_path = os.path.join(video_dir_path, \'{:05d}.jpg\'.format(i))\n            if os.path.exists(image_path):\n                \n                video.append(image_loader(image_path, modality))\n            else:\n                print(image_path, ""------- Does not exist"")\n                return video\n    elif modality == \'Depth\':\n\n        for i in frame_indices:\n            image_path = os.path.join(video_dir_path.replace(\'color\',\'depth\'), \'{:05d}.jpg\'.format(i) )\n            if os.path.exists(image_path):\n                video.append(image_loader(image_path, modality))\n            else:\n                print(image_path, ""------- Does not exist"")\n                return video\n    elif modality == \'RGB-D\':\n        for i in frame_indices: # index 35 is used to change img to flow\n            image_path = os.path.join(video_dir_path, \'{:05d}.jpg\'.format(i))\n\n            \n            image_path_depth = os.path.join(video_dir_path.replace(\'color\',\'depth\'), \'{:05d}.jpg\'.format(i) )\n\n            \n            image = image_loader(image_path, \'RGB\')\n            image_depth = image_loader(image_path_depth, \'Depth\')\n\n            if os.path.exists(image_path):\n                video.append(image)\n                video.append(image_depth)\n            else:\n                print(image_path, ""------- Does not exist"")\n                return video\n    \n    return video\n\ndef get_default_video_loader():\n    image_loader = get_default_image_loader()\n    return functools.partial(video_loader, image_loader=image_loader)\n\n\ndef load_annotation_data(data_file_path):\n    with open(data_file_path, \'r\') as data_file:\n        return json.load(data_file)\n\n\ndef get_class_labels(data):\n    class_labels_map = {}\n    index = 0\n    for class_label in data[\'labels\']:\n        class_labels_map[class_label] = index\n        index += 1\n    return class_labels_map\n\n\ndef get_video_names_and_annotations(data, subset):\n    video_names = []\n    annotations = []\n\n    for key, value in data[\'database\'].items():\n        this_subset = value[\'subset\']\n        if this_subset == subset:\n            label = value[\'annotations\'][\'label\']\n            #video_names.append(\'{}/{}\'.format(label, key))\n            video_names.append(key.split(\'^\')[0])\n            annotations.append(value[\'annotations\'])\n\n    return video_names, annotations\n\n\ndef make_dataset(root_path, annotation_path, subset, n_samples_for_each_video,\n                 sample_duration):\n    data = load_annotation_data(annotation_path)\n    video_names, annotations = get_video_names_and_annotations(data, subset)\n    class_to_idx = get_class_labels(data)\n    idx_to_class = {}\n    for name, label in class_to_idx.items():\n        idx_to_class[label] = name\n\n    dataset = []\n    print(""[INFO]: NV Dataset - "" + subset + "" is loading..."")\n    for i in range(len(video_names)):\n        if i % 1000 == 0:\n            print(\'dataset loading [{}/{}]\'.format(i, len(video_names)))\n\n        video_path = os.path.join(root_path, video_names[i])\n        \n        if not os.path.exists(video_path):\n            continue\n\n        \n\n        begin_t = int(annotations[i][\'start_frame\'])\n        end_t = int(annotations[i][\'end_frame\'])\n        n_frames = end_t - begin_t + 1\n        sample = {\n            \'video\': video_path,\n            \'segment\': [begin_t, end_t],\n            \'n_frames\': n_frames,\n            #\'video_id\': video_names[i].split(\'/\')[1]\n            \'video_id\': i\n        }\n        if len(annotations) != 0:\n            sample[\'label\'] = class_to_idx[annotations[i][\'label\']]\n        else:\n            sample[\'label\'] = -1\n\n        if n_samples_for_each_video == 1:\n            sample[\'frame_indices\'] = list(range(begin_t, end_t + 1))\n            dataset.append(sample)\n        else:\n            if n_samples_for_each_video > 1:\n                step = max(1,\n                           math.ceil((n_frames - 1 - sample_duration) /\n                                     (n_samples_for_each_video - 1)))\n            else:\n                step = sample_duration\n            for j in range(1, n_frames, step):\n                sample_j = copy.deepcopy(sample)\n                sample_j[\'frame_indices\'] = list(\n                    range(j, min(n_frames + 1, j + sample_duration)))\n                dataset.append(sample_j)\n\n    return dataset, idx_to_class\n\n\nclass NV(data.Dataset):\n    """"""\n    Args:\n        root (string): Root directory path.\n        spatial_transform (callable, optional): A function/transform that  takes in an PIL image\n            and returns a transformed version. E.g, ``transforms.RandomCrop``\n        temporal_transform (callable, optional): A function/transform that  takes in a list of frame indices\n            and returns a transformed version\n        target_transform (callable, optional): A function/transform that takes in the\n            target and transforms it.\n        loader (callable, optional): A function to load an video given its path and frame indices.\n     Attributes:\n        classes (list): List of the class names.\n        class_to_idx (dict): Dict with items (class_name, class_index).\n        imgs (list): List of (image path, class_index) tuples\n    """"""\n\n    def __init__(self,\n                 root_path,\n                 annotation_path,\n                 subset,\n                 n_samples_for_each_video=1,\n                 spatial_transform=None,\n                 temporal_transform=None,\n                 target_transform=None,\n                 sample_duration=16,\n                 modality=\'RGB\',\n                 get_loader=get_default_video_loader):\n        self.data, self.class_names = make_dataset(\n            root_path, annotation_path, subset, n_samples_for_each_video,\n            sample_duration)\n\n        self.spatial_transform = spatial_transform\n        self.temporal_transform = temporal_transform\n        self.target_transform = target_transform\n        self.modality = modality\n        self.sample_duration = sample_duration\n        self.loader = get_loader()\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n        Returns:\n            tuple: (image, target) where target is class_index of the target class.\n        """"""\n\n        path = self.data[index][\'video\']\n\n        frame_indices = self.data[index][\'frame_indices\']\n\n        if self.temporal_transform is not None:\n            frame_indices = self.temporal_transform(frame_indices)\n        clip = self.loader(path, frame_indices, self.modality, self.sample_duration)\n        oversample_clip =[]\n        if self.spatial_transform is not None:\n            self.spatial_transform.randomize_parameters()\n            clip = [self.spatial_transform(img) for img in clip]\n    \n        im_dim = clip[0].size()[-2:]\n        clip = torch.cat(clip, 0).view((self.sample_duration, -1) + im_dim).permute(1, 0, 2, 3)\n        \n     \n        target = self.data[index]\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return clip, target\n\n    def __len__(self):\n        return len(self.data)\n\n\n'"
datasets/nv_online.py,2,"b'import torch\nimport torch.utils.data as data\nfrom PIL import Image\nfrom spatial_transforms import *\nfrom temporal_transforms import *\nimport os\nimport math\nimport functools\nimport json\nimport copy\nfrom numpy.random import randint\nimport numpy as np\nimport random\n\nfrom utils import load_value_file\nimport pdb\n\n\ndef pil_loader(path, modality):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, \'rb\') as f:\n        #print(path)\n        with Image.open(f) as img:\n            if modality == \'RGB\':\n                return img.convert(\'RGB\')\n            elif modality == \'Depth\':\n                return img.convert(\'L\') # 8-bit pixels, black and white check from https://pillow.readthedocs.io/en/3.0.x/handbook/concepts.html\n\n\ndef accimage_loader(path, modality):\n    try:\n        import accimage\n        return accimage.Image(path)\n    except IOError:\n        # Potentially a decoding problem, fall back to PIL.Image\n        return pil_loader(path)\n\n\ndef get_default_image_loader():\n    from torchvision import get_image_backend\n    if get_image_backend() == \'accimage\':\n        return accimage_loader\n    else:\n        return pil_loader\n\n\ndef video_loader(video_dir_path, frame_indices, modality, sample_duration, image_loader):\n    \n    video = []\n    if modality == \'RGB\':\n        for i in frame_indices:\n            image_path = os.path.join(video_dir_path, \'{:05d}.jpg\'.format(i))\n            if os.path.exists(image_path):\n                \n                video.append(image_loader(image_path, modality))\n            else:\n                print(image_path, ""------- Does not exist"")\n                return video\n    elif modality == \'Depth\':\n\n        for i in frame_indices:\n            image_path = os.path.join(video_dir_path.replace(\'color\',\'depth\'), \'{:05d}.jpg\'.format(i) )\n            if os.path.exists(image_path):\n                video.append(image_loader(image_path, modality))\n            else:\n                print(image_path, ""------- Does not exist"")\n                return video\n    elif modality == \'RGB-D\':\n        for i in frame_indices:\n            image_path = os.path.join(video_dir_path, \'{:05d}.jpg\'.format(i))\n            image_path_depth = os.path.join(video_dir_path.replace(\'color\',\'depth\'), \'{:05d}.jpg\'.format(i) )\n\n            \n            image = image_loader(image_path, \'RGB\')\n            image_depth = image_loader(image_path_depth, \'Depth\')\n\n            if os.path.exists(image_path):\n                video.append(image)\n                video.append(image_depth)\n            else:\n                print(image_path, ""------- Does not exist"")\n                return video\n    return video\n\ndef get_default_video_loader():\n    image_loader = get_default_image_loader()\n    return functools.partial(video_loader, image_loader=image_loader)\n\n\ndef load_annotation_data(data_file_path):\n    with open(data_file_path, \'r\') as data_file:\n        return json.load(data_file)\n\n\ndef get_class_labels(data):\n    class_labels_map = {}\n    index = 0\n    for class_label in data[\'labels\']:\n        class_labels_map[class_label] = index\n        index += 1\n    return class_labels_map\n\n\ndef get_annotation(data, whole_path):\n    annotation = []\n\n    for key, value in data[\'database\'].items():\n        if key.split(\'^\')[0] == whole_path:\n            annotation.append(value[\'annotations\'])\n\n    return  annotation\n\n\ndef make_dataset( annotation_path, video_path , whole_path,sample_duration, n_samples_for_each_video, stride_len):\n    \n    data = load_annotation_data(annotation_path)\n    whole_video_path = os.path.join(video_path,whole_path)\n    annotation = get_annotation(data, whole_path)\n    class_to_idx = get_class_labels(data)\n    idx_to_class = {}\n    for name, label in class_to_idx.items():\n        idx_to_class[label] = name\n\n    dataset = []\n    print(""[INFO]: Videot  is loading..."")\n    import glob\n\n    n_frames = len(glob.glob(whole_video_path + \'/*.jpg\'))\n    \n    if not os.path.exists(whole_video_path):\n        print(whole_video_path , "" does not exist"")\n    label_list = []\n    for i in range(len(annotation)):\n        begin_t = int(annotation[i][\'start_frame\'])\n        end_t = int(annotation[i][\'end_frame\'])\n        for j in range(begin_t,end_t+1):\n            label_list.append(class_to_idx[annotation[i][\'label\']])\n\n    label_list = np.array(label_list)\n    for _ in range(1,n_frames+1 - sample_duration,stride_len):\n        \n        sample = {\n                \'video\': whole_video_path,\n                \'index\': _ ,\n                \'video_id\' : _ \n\n            }\n        ## Different strategies to set true label of overlaping frames\n        # counts = np.bincount(label_list[np.array(list(range(_    - int(sample_duration/4), _ )))])\n        sample[\'label\'] = 0 #np.argmax(counts)\n        if n_samples_for_each_video == 1:\n            sample[\'frame_indices\'] = list(range(_ , _ + sample_duration))\n            dataset.append(sample)\n        else:\n            if n_samples_for_each_video > 1:\n                step = max(1,\n                            math.ceil((n_frames - 1 - sample_duration) /\n                                    (n_samples_for_each_video - 1)))\n            else:\n                step = sample_duration\n            for j in range(sample_duration, n_frames, step):\n                sample_j = copy.deepcopy(sample)\n                sample_j[\'frame_indices\'] = list(\n                    range(j, min(n_frames + 1, j + sample_duration)))\n                dataset.append(sample_j)\n\n    return dataset, idx_to_class\n\n\n\n\nclass NVOnline(data.Dataset):\n    """"""\n    Args:\n        root (string): Root directory path.\n        spatial_transform (callable, optional): A function/transform that  takes in an PIL image\n            and returns a transformed version. E.g, ``transforms.RandomCrop``\n        temporal_transform (callable, optional): A function/transform that  takes in a list of frame indices\n            and returns a transformed version\n        target_transform (callable, optional): A function/transform that takes in the\n            target and transforms it.\n        loader (callable, optional): A function to load an video given its path and frame indices.\n     Attributes:\n        classes (list): List of the class names.\n        class_to_idx (dict): Dict with items (class_name, class_index).\n        imgs (list): List of (image path, class_index) tuples\n    """"""\n\n    def __init__(self,\n                 annotation_path,\n                 video_path,\n                 whole_path,\n                 n_samples_for_each_video=1,\n                 spatial_transform=None,\n                 temporal_transform=None,\n                 target_transform=None,\n                 sample_duration=16,\n                 modality=\'RGB\',\n                 stride_len = None,\n                 get_loader=get_default_video_loader):\n\n        self.data, self.class_names = make_dataset(\n         annotation_path, video_path, whole_path, sample_duration,n_samples_for_each_video, stride_len)\n        \n        self.spatial_transform = spatial_transform\n        self.temporal_transform = temporal_transform\n        self.target_transform = target_transform\n        self.modality = modality\n        self.sample_duration = sample_duration\n        self.loader = get_loader()\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n        Returns:\n            tuple: (image, target) where target is class_index of the target class.\n        """"""\n\n        path = self.data[index][\'video\']\n\n        frame_indices = self.data[index][\'frame_indices\']\n\n        if self.temporal_transform is not None:\n            frame_indices = self.temporal_transform(frame_indices)\n        \n        clip = self.loader(path, frame_indices, self.modality, self.sample_duration)\n        oversample_clip =[]\n        if self.spatial_transform is not None:\n            self.spatial_transform.randomize_parameters()\n            clip = [self.spatial_transform(img) for img in clip]\n\n        im_dim = clip[0].size()[-2:]\n        clip = torch.cat(clip, 0).view((self.sample_duration, -1) + im_dim).permute(1, 0, 2, 3)\n        \n        target = self.data[index]\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n        \n        return clip, target\n\n    def __len__(self):\n        return len(self.data)\n\n\n'"
datasets/ucf101.py,2,"b'import torch\nimport torch.utils.data as data\nfrom PIL import Image\nimport os\nimport math\nimport functools\nimport json\nimport copy\nimport random\nfrom numpy.random import randint\n\nfrom utils import load_value_file\n\n\ndef pil_loader(path):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, \'rb\') as f:\n        with Image.open(f) as img:\n            return img.convert(\'RGB\')\n\n\ndef accimage_loader(path):\n    try:\n        import accimage\n        return accimage.Image(path)\n    except IOError:\n        # Potentially a decoding problem, fall back to PIL.Image\n        return pil_loader(path)\n\n\ndef get_default_image_loader():\n    from torchvision import get_image_backend\n    if get_image_backend() == \'accimage\':\n        return accimage_loader\n    else:\n        return pil_loader\n\n\ndef video_loader(video_dir_path, frame_indices, image_loader):\n    video = []\n    for i in frame_indices:\n        image_path = os.path.join(video_dir_path, \'image_{:05d}.jpg\'.format(i))\n        if os.path.exists(image_path):\n            video.append(image_loader(image_path))\n        else:\n            return video\n\n    return video\n\n\ndef get_default_video_loader():\n    image_loader = get_default_image_loader()\n    return functools.partial(video_loader, image_loader=image_loader)\n\n\ndef load_annotation_data(data_file_path):\n    with open(data_file_path, \'r\') as data_file:\n        return json.load(data_file)\n\n\ndef get_class_labels(data):\n    class_labels_map = {}\n    index = 0\n    for class_label in data[\'labels\']:\n        class_labels_map[class_label] = index\n        index += 1\n    return class_labels_map\n\n\ndef get_video_names_and_annotations(data, subset):\n    video_names = []\n    annotations = []\n\n    for key, value in data[\'database\'].items():\n        this_subset = value[\'subset\']\n        if this_subset == subset:\n            label = value[\'annotations\'][\'label\']\n            video_names.append(\'{}/{}\'.format(label, key))\n            annotations.append(value[\'annotations\'])\n\n    return video_names, annotations\n\n\ndef make_dataset(root_path, annotation_path, subset, n_samples_for_each_video,\n                 sample_duration):\n    data = load_annotation_data(annotation_path)\n    video_names, annotations = get_video_names_and_annotations(data, subset)\n    class_to_idx = get_class_labels(data)\n    idx_to_class = {}\n    for name, label in class_to_idx.items():\n        idx_to_class[label] = name\n\n    dataset = []\n    for i in range(len(video_names)):\n        if i % 1000 == 0:\n            print(\'dataset loading [{}/{}]\'.format(i, len(video_names)))\n\n        video_path = os.path.join(root_path, video_names[i])\n        if not os.path.exists(video_path):\n            continue\n\n        n_frames_file_path = os.path.join(video_path, \'n_frames\')\n        n_frames = int(load_value_file(n_frames_file_path))\n        if n_frames <= 0:\n            continue\n\n        begin_t = 1\n        end_t = n_frames\n        sample = {\n            \'video\': video_path,\n            \'segment\': [begin_t, end_t],\n            \'n_frames\': n_frames,\n            \'video_id\': video_names[i].split(\'/\')[1]\n        }\n        if len(annotations) != 0:\n            sample[\'label\'] = class_to_idx[annotations[i][\'label\']]\n        else:\n            sample[\'label\'] = -1\n\n        if n_samples_for_each_video == 1:\n            sample[\'frame_indices\'] = list(range(1, n_frames + 1))\n            dataset.append(sample)\n        else:\n            if n_samples_for_each_video > 1:\n                step = max(1,\n                           math.ceil((n_frames - 1 - sample_duration) /\n                                     (n_samples_for_each_video - 1)))\n            else:\n                step = sample_duration\n            for j in range(1, n_frames, step):\n                sample_j = copy.deepcopy(sample)\n                sample_j[\'frame_indices\'] = list(\n                    range(j, min(n_frames + 1, j + sample_duration)))\n                dataset.append(sample_j)\n\n    return dataset, idx_to_class\n\n\nclass UCF101(data.Dataset):\n    """"""\n    Args:\n        root (string): Root directory path.\n        spatial_transform (callable, optional): A function/transform that  takes in an PIL image\n            and returns a transformed version. E.g, ``transforms.RandomCrop``\n        temporal_transform (callable, optional): A function/transform that  takes in a list of frame indices\n            and returns a transformed version\n        target_transform (callable, optional): A function/transform that takes in the\n            target and transforms it.\n        loader (callable, optional): A function to load an video given its path and frame indices.\n     Attributes:\n        classes (list): List of the class names.\n        class_to_idx (dict): Dict with items (class_name, class_index).\n        imgs (list): List of (image path, class_index) tuples\n    """"""\n\n    def __init__(self,\n                 root_path,\n                 annotation_path,\n                 subset,\n                 n_samples_for_each_video=1,\n                 spatial_transform=None,\n                 temporal_transform=None,\n                 target_transform=None,\n                 sample_duration=16,\n                 get_loader=get_default_video_loader):\n        self.data, self.class_names = make_dataset(\n            root_path, annotation_path, subset, n_samples_for_each_video,\n            sample_duration)\n\n        self.spatial_transform = spatial_transform\n        self.temporal_transform = temporal_transform\n        self.target_transform = target_transform\n        self.loader = get_loader()\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n        Returns:\n            tuple: (image, target) where target is class_index of the target class.\n        """"""\n        path = self.data[index][\'video\']\n\n        frame_indices = self.data[index][\'frame_indices\']\n        if self.temporal_transform is not None:\n            frame_indices = self.temporal_transform(frame_indices)\n        clip = self.loader(path, frame_indices)\n        if self.spatial_transform is not None:\n            self.spatial_transform.randomize_parameters()\n            clip = [self.spatial_transform(img) for img in clip]\n        clip = torch.stack(clip, 0).permute(1, 0, 2, 3)\n\n        target = self.data[index]\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return clip, target\n\n    def __len__(self):\n        return len(self.data)\n'"
models/c3d.py,5,"b'""""""\nThis is the c3d implementation with batch norm.\n\nReferences\n----------\n[1] Tran, Du, et al. ""Learning spatiotemporal features with 3d convolutional networks."" \nProceedings of the IEEE international conference on computer vision. 2015.\n""""""\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom functools import partial\n\n\nclass C3D(nn.Module):\n    def __init__(self,\n                 sample_size,\n                 sample_duration,\n                 num_classes=600):\n\n        super(C3D, self).__init__()\n        self.group1 = nn.Sequential(\n            nn.Conv3d(3, 64, kernel_size=3, padding=1),\n            nn.BatchNorm3d(64),\n            nn.ReLU(),\n            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(1, 2, 2)))\n        self.group2 = nn.Sequential(\n            nn.Conv3d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm3d(128),\n            nn.ReLU(),\n            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)))\n        self.group3 = nn.Sequential(\n            nn.Conv3d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm3d(256),\n            nn.ReLU(),\n            nn.Conv3d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm3d(256),\n            nn.ReLU(),\n            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)))\n        self.group4 = nn.Sequential(\n            nn.Conv3d(256, 512, kernel_size=3, padding=1),\n            nn.BatchNorm3d(512),\n            nn.ReLU(),\n            nn.Conv3d(512, 512, kernel_size=3, padding=1),\n            nn.BatchNorm3d(512),\n            nn.ReLU(),\n            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)))\n        self.group5 = nn.Sequential(\n            nn.Conv3d(512, 512, kernel_size=3, padding=1),\n            nn.BatchNorm3d(512),\n            nn.ReLU(),\n            nn.Conv3d(512, 512, kernel_size=3, padding=1),\n            nn.BatchNorm3d(512),\n            nn.ReLU(),\n            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(2, 2, 2), padding=(0, 1, 1)))\n\n        last_duration = int(math.floor(sample_duration / 16))\n        last_size = int(math.ceil(sample_size / 32))\n        self.fc1 = nn.Sequential(\n            nn.Linear((512 * last_duration * last_size * last_size) , 2048),\n            nn.ReLU(),\n            nn.Dropout(0.5))\n        self.fc2 = nn.Sequential(\n            nn.Linear(2048, 2048),\n            nn.ReLU(),\n            nn.Dropout(0.5))\n        self.fc = nn.Sequential(\n            nn.Linear(2048, num_classes))         \n\n        \n\n    def forward(self, x):\n        out = self.group1(x)\n        out = self.group2(out)\n        out = self.group3(out)\n        out = self.group4(out)\n        out = self.group5(out)\n        out = out.view(out.size(0), -1)\n        out = self.fc1(out)\n        out = self.fc2(out)\n        out = self.fc(out)\n        return out\n\n\ndef get_fine_tuning_parameters(model, ft_portion):\n    if ft_portion == ""complete"":\n        return model.parameters()\n\n    elif ft_portion == ""last_layer"":\n        ft_module_names = []\n        ft_module_names.append(\'fc\')\n\n        parameters = []\n        for k, v in model.named_parameters():\n            for ft_module in ft_module_names:\n                if ft_module in k:\n                    parameters.append({\'params\': v})\n                    break\n            else:\n                parameters.append({\'params\': v, \'lr\': 0.0})\n        return parameters\n\n    else:\n        raise ValueError(""Unsupported ft_portion: \'complete\' or \'last_layer\' expected"")\n\n\ndef get_model(**kwargs):\n    """"""\n    Returns the model.\n    """"""\n    model = C3D(**kwargs)\n    return model\n\n\nif __name__ == \'__main__\':\n    model = get_model(sample_size = 112, sample_duration = 16, num_classes=600)\n    model = model.cuda()\n    model = nn.DataParallel(model, device_ids=None)\n    print(model)\n\n    input_var = Variable(torch.randn(8, 3, 16, 112, 112))\n    output = model(input_var)\n    print(output.shape)\n'"
models/mobilenet.py,3,"b'\'\'\'MobileNet in PyTorch.\n\nSee the paper ""MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications""\nfor more details.\n\'\'\'\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef conv_bn(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv3d(inp, oup, kernel_size=3, stride=stride, padding=(1,1,1), bias=False),\n        nn.BatchNorm3d(oup),\n        nn.ReLU(inplace=True)\n    )\n\n\nclass Block(nn.Module):\n    \'\'\'Depthwise conv + Pointwise conv\'\'\'\n    def __init__(self, in_planes, out_planes, stride=1):\n        super(Block, self).__init__()\n        self.conv1 = nn.Conv3d(in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=in_planes, bias=False)\n        self.bn1 = nn.BatchNorm3d(in_planes)\n        self.conv2 = nn.Conv3d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn2 = nn.BatchNorm3d(out_planes)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        return out\n\n\nclass MobileNet(nn.Module):\n    def __init__(self, num_classes=600, sample_size=224, width_mult=1.):\n        super(MobileNet, self).__init__()\n\n        input_channel = 32\n        last_channel = 1024\n        input_channel = int(input_channel * width_mult)\n        last_channel = int(last_channel * width_mult)\n        cfg = [\n        # c, n, s\n        [64,   1, (2,2,2)],\n        [128,  2, (2,2,2)],\n        [256,  2, (2,2,2)],\n        [512,  6, (2,2,2)],\n        [1024, 2, (1,1,1)],\n        ]\n\n        self.features = [conv_bn(3, input_channel, (1,2,2))]\n        # building inverted residual blocks\n        for c, n, s in cfg:\n            output_channel = int(c * width_mult)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                self.features.append(Block(input_channel, output_channel, stride))\n                input_channel = output_channel\n        # make it nn.Sequential\n        self.features = nn.Sequential(*self.features)\n\n        # building classifier\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(last_channel, num_classes),\n        )\n\n\n    def forward(self, x):\n        x = self.features(x)\n        x = F.avg_pool3d(x, x.data.size()[-3:])\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n\ndef get_fine_tuning_parameters(model, ft_portion):\n    if ft_portion == ""complete"":\n        return model.parameters()\n\n    elif ft_portion == ""last_layer"":\n        ft_module_names = []\n        ft_module_names.append(\'classifier\')\n\n        parameters = []\n        for k, v in model.named_parameters():\n            for ft_module in ft_module_names:\n                if ft_module in k:\n                    parameters.append({\'params\': v})\n                    break\n            else:\n                parameters.append({\'params\': v, \'lr\': 0.0})\n        return parameters\n\n    else:\n        raise ValueError(""Unsupported ft_portion: \'complete\' or \'last_layer\' expected"")\n    \n\ndef get_model(**kwargs):\n    """"""\n    Returns the model.\n    """"""\n    model = MobileNet(**kwargs)\n    return model\n\n\n\nif __name__ == \'__main__\':\n    model = get_model(num_classes=600, sample_size = 112, width_mult=1.)\n    model = model.cuda()\n    model = nn.DataParallel(model, device_ids=None)\n    print(model)\n\n    input_var = Variable(torch.randn(8, 3, 16, 112, 112))\n    output = model(input_var)\n    print(output.shape)\n'"
models/mobilenetv2.py,4,"b'\'\'\'MobilenetV2 in PyTorch.\n\nSee the paper ""MobileNetV2: Inverted Residuals and Linear Bottlenecks"" for more details.\n\'\'\'\nimport torch\nimport math\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\n\n\ndef conv_bn(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv3d(inp, oup, kernel_size=3, stride=stride, padding=(1,1,1), bias=False),\n        nn.BatchNorm3d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\ndef conv_1x1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv3d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm3d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n\n        hidden_dim = round(inp * expand_ratio)\n        self.use_res_connect = self.stride == (1,1,1) and inp == oup\n\n        if expand_ratio == 1:\n            self.conv = nn.Sequential(\n                # dw\n                nn.Conv3d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm3d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv3d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm3d(oup),\n            )\n        else:\n            self.conv = nn.Sequential(\n                # pw\n                nn.Conv3d(inp, hidden_dim, 1, 1, 0, bias=False),\n                nn.BatchNorm3d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # dw\n                nn.Conv3d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm3d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv3d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm3d(oup),\n            )\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, num_classes=1000, sample_size=224, width_mult=1.):\n        super(MobileNetV2, self).__init__()\n        block = InvertedResidual\n        input_channel = 32\n        last_channel = 1280\n        interverted_residual_setting = [\n            # t, c, n, s\n            [1,  16, 1, (1,1,1)],\n            [6,  24, 2, (2,2,2)],\n            [6,  32, 3, (2,2,2)],\n            [6,  64, 4, (2,2,2)],\n            [6,  96, 3, (1,1,1)],\n            [6, 160, 3, (2,2,2)],\n            [6, 320, 1, (1,1,1)],\n        ]\n\n        # building first layer\n        assert sample_size % 16 == 0.\n        input_channel = int(input_channel * width_mult)\n        self.last_channel = int(last_channel * width_mult) if width_mult > 1.0 else last_channel\n        self.features = [conv_bn(3, input_channel, (1,2,2))]\n        # building inverted residual blocks\n        for t, c, n, s in interverted_residual_setting:\n            output_channel = int(c * width_mult)\n            for i in range(n):\n                stride = s if i == 0 else (1,1,1)\n                self.features.append(block(input_channel, output_channel, stride, expand_ratio=t))\n                input_channel = output_channel\n        # building last several layers\n        self.features.append(conv_1x1x1_bn(input_channel, self.last_channel))\n        # make it nn.Sequential\n        self.features = nn.Sequential(*self.features)\n\n        # building classifier\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(self.last_channel, num_classes),\n        )\n\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = F.avg_pool3d(x, x.data.size()[-3:])\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.kernel_size[2] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm3d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                n = m.weight.size(1)\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n\n\ndef get_fine_tuning_parameters(model, ft_portion):\n    if ft_portion == ""complete"":\n        return model.parameters()\n\n    elif ft_portion == ""last_layer"":\n        ft_module_names = []\n        ft_module_names.append(\'classifier\')\n\n        parameters = []\n        for k, v in model.named_parameters():\n            for ft_module in ft_module_names:\n                if ft_module in k:\n                    parameters.append({\'params\': v})\n                    break\n            else:\n                parameters.append({\'params\': v, \'lr\': 0.0})\n        return parameters\n\n    else:\n        raise ValueError(""Unsupported ft_portion: \'complete\' or \'last_layer\' expected"")\n\n    \ndef get_model(**kwargs):\n    """"""\n    Returns the model.\n    """"""\n    model = MobileNetV2(**kwargs)\n    return model\n\n\nif __name__ == ""__main__"":\n    model = get_model(num_classes=600, sample_size=112, width_mult=1.)\n    model = model.cuda()\n    model = nn.DataParallel(model, device_ids=None)\n    print(model)\n\n\n    input_var = Variable(torch.randn(8, 3, 16, 112, 112))\n    output = model(input_var)\n    print(output.shape)\n\n\n'"
models/resnet.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport math\nfrom functools import partial\n\n__all__ = [\n    \'ResNet\', \'resnet10\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\n    \'resnet152\', \'resnet200\'\n]\n\n\ndef conv3x3x3(in_planes, out_planes, stride=1):\n    # 3x3x3 convolution with padding\n    return nn.Conv3d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=1,\n        bias=False)\n\n\ndef downsample_basic_block(x, planes, stride):\n    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n    zero_pads = torch.Tensor(\n        out.size(0), planes - out.size(1), out.size(2), out.size(3),\n        out.size(4)).zero_()\n    if isinstance(out.data, torch.cuda.FloatTensor):\n        zero_pads = zero_pads.cuda()\n\n    out = Variable(torch.cat([out.data, zero_pads], dim=1))\n\n    return out\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm3d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3x3(planes, planes)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm3d(planes)\n        self.conv2 = nn.Conv3d(\n            planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm3d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self,\n                 block,\n                 layers,\n                 sample_size,\n                 sample_duration,\n                 shortcut_type=\'B\',\n                 num_classes=400):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv3d(\n            3,\n            64,\n            kernel_size=7,\n            stride=(1, 2, 2),\n            padding=(3, 3, 3),\n            bias=False)\n        self.bn1 = nn.BatchNorm3d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0], shortcut_type)\n        self.layer2 = self._make_layer(\n            block, 128, layers[1], shortcut_type, stride=2)\n        self.layer3 = self._make_layer(\n            block, 256, layers[2], shortcut_type, stride=2)\n        self.layer4 = self._make_layer(\n            block, 512, layers[3], shortcut_type, stride=2)\n        last_duration = int(math.ceil(sample_duration / 16))\n        last_size = int(math.ceil(sample_size / 32))\n        self.avgpool = nn.AvgPool3d(\n            (last_duration, last_size, last_size), stride=1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                m.weight = nn.init.kaiming_normal(m.weight, mode=\'fan_out\')\n            elif isinstance(m, nn.BatchNorm3d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            if shortcut_type == \'A\':\n                downsample = partial(\n                    downsample_basic_block,\n                    planes=planes * block.expansion,\n                    stride=stride)\n            else:\n                downsample = nn.Sequential(\n                    nn.Conv3d(\n                        self.inplanes,\n                        planes * block.expansion,\n                        kernel_size=1,\n                        stride=stride,\n                        bias=False), nn.BatchNorm3d(planes * block.expansion))\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef get_fine_tuning_parameters(model, ft_portion):\n    if ft_portion == ""complete"":\n        return model.parameters()\n\n    elif ft_portion == ""last_layer"":\n        ft_module_names = []\n        ft_module_names.append(\'classifier\')\n\n        parameters = []\n        for k, v in model.named_parameters():\n            for ft_module in ft_module_names:\n                if ft_module in k:\n                    parameters.append({\'params\': v})\n                    break\n            else:\n                parameters.append({\'params\': v, \'lr\': 0.0})\n        return parameters\n\n    else:\n        raise ValueError(""Unsupported ft_portion: \'complete\' or \'last_layer\' expected"")\n\n\ndef resnet10(**kwargs):\n    """"""Constructs a ResNet-18 model.\n    """"""\n    model = ResNet(BasicBlock, [1, 1, 1, 1], **kwargs)\n    return model\n\n\ndef resnet18(**kwargs):\n    """"""Constructs a ResNet-18 model.\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    return model\n\n\ndef resnet34(**kwargs):\n    """"""Constructs a ResNet-34 model.\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    return model\n\n\ndef resnet50(**kwargs):\n    """"""Constructs a ResNet-50 model.\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    return model\n\n\ndef resnet101(**kwargs):\n    """"""Constructs a ResNet-101 model.\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    return model\n\n\ndef resnet152(**kwargs):\n    """"""Constructs a ResNet-101 model.\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    return model\n\n\ndef resnet200(**kwargs):\n    """"""Constructs a ResNet-101 model.\n    """"""\n    model = ResNet(Bottleneck, [3, 24, 36, 3], **kwargs)\n    return model\n'"
models/resnetl.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport math\nfrom functools import partial\n\n__all__ = [\n    \'ResNetL\', \'resnetl10\'\n]\n\n\ndef conv3x3x3(in_planes, out_planes, stride=1):\n    # 3x3x3 convolution with padding\n    return nn.Conv3d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=1,\n        bias=False)\n\n\ndef downsample_basic_block(x, planes, stride):\n    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n    zero_pads = torch.Tensor(\n        out.size(0), planes - out.size(1), out.size(2), out.size(3),\n        out.size(4)).zero_()\n    if isinstance(out.data, torch.cuda.FloatTensor):\n        zero_pads = zero_pads.cuda()\n\n    out = Variable(torch.cat([out.data, zero_pads], dim=1))\n\n    return out\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm3d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3x3(planes, planes)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm3d(planes)\n        self.conv2 = nn.Conv3d(\n            planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm3d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNetL(nn.Module):\n\n    def __init__(self,\n                 block,\n                 layers,\n                 sample_size,\n                 sample_duration,\n                 shortcut_type=\'B\',\n                 num_classes=400):\n        self.inplanes = 16\n        super(ResNetL, self).__init__()\n        self.conv1 = nn.Conv3d(\n            3,\n            16,\n            kernel_size=7,\n            stride=(1, 2, 2),\n            padding=(3, 3, 3),\n            bias=False)\n        self.bn1 = nn.BatchNorm3d(16)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 16, layers[0], shortcut_type)\n        self.layer2 = self._make_layer(\n            block, 32, layers[1], shortcut_type, stride=2)\n        self.layer3 = self._make_layer(\n            block, 64, layers[2], shortcut_type, stride=2)\n        self.layer4 = self._make_layer(\n            block, 128, layers[3], shortcut_type, stride=2)\n        last_duration = int(math.ceil(sample_duration / 16))\n        last_size = int(math.ceil(sample_size / 32))\n        self.avgpool = nn.AvgPool3d(\n            (last_duration, last_size, last_size), stride=1)\n        self.fc = nn.Linear(128 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                m.weight = nn.init.kaiming_normal_(m.weight, mode=\'fan_out\')\n            elif isinstance(m, nn.BatchNorm3d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            if shortcut_type == \'A\':\n                downsample = partial(\n                    downsample_basic_block,\n                    planes=planes * block.expansion,\n                    stride=stride)\n            else:\n                downsample = nn.Sequential(\n                    nn.Conv3d(\n                        self.inplanes,\n                        planes * block.expansion,\n                        kernel_size=1,\n                        stride=stride,\n                        bias=False), nn.BatchNorm3d(planes * block.expansion))\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef get_fine_tuning_parameters(model, ft_portion=""complete""):\n    if ft_portion == ""complete"":\n        return model.parameters()\n    elif ft_portion == ""last_layer"":\n        ft_module_names = []\n        ft_module_names.append(\'fc\')\n\n        parameters = []\n        for k, v in model.named_parameters():\n            for ft_module in ft_module_names:\n                if ft_module in k:\n                    parameters.append({\'params\': v})\n                    break\n            else:\n                parameters.append({\'params\': v, \'lr\': 0.0})\n        return parameters\n\n    else:\n        raise ValueError(""Unsupported ft_portion: \'complete\' or \'last_layer\' expected"")\n\n\n\ndef resnetl10(**kwargs):\n    """"""Constructs a resnetl-10 model.\n    """"""\n    model = ResNetL(BasicBlock, [1, 1, 1, 1], **kwargs)\n    return model\n'"
models/resnext.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport math\nfrom functools import partial\n\n__all__ = [\'ResNeXt\', \'resnext50\', \'resnext101\']\n\n\ndef conv3x3x3(in_planes, out_planes, stride=1):\n    # 3x3x3 convolution with padding\n    return nn.Conv3d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=1,\n        bias=False)\n\n\ndef downsample_basic_block(x, planes, stride):\n    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n    zero_pads = torch.Tensor(\n        out.size(0), planes - out.size(1), out.size(2), out.size(3),\n        out.size(4)).zero_()\n    if isinstance(out.data, torch.cuda.FloatTensor):\n        zero_pads = zero_pads.cuda()\n\n    out = Variable(torch.cat([out.data, zero_pads], dim=1))\n\n    return out\n\n\nclass ResNeXtBottleneck(nn.Module):\n    expansion = 2\n\n    def __init__(self, inplanes, planes, cardinality, stride=1,\n                 downsample=None):\n        super(ResNeXtBottleneck, self).__init__()\n        mid_planes = cardinality * int(planes / 32)\n        self.conv1 = nn.Conv3d(inplanes, mid_planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm3d(mid_planes)\n        self.conv2 = nn.Conv3d(\n            mid_planes,\n            mid_planes,\n            kernel_size=3,\n            stride=stride,\n            padding=1,\n            groups=cardinality,\n            bias=False)\n        self.bn2 = nn.BatchNorm3d(mid_planes)\n        self.conv3 = nn.Conv3d(\n            mid_planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm3d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNeXt(nn.Module):\n\n    def __init__(self,\n                 block,\n                 layers,\n                 sample_size,\n                 sample_duration,\n                 shortcut_type=\'B\',\n                 cardinality=32,\n                 num_classes=400):\n        self.inplanes = 64\n        super(ResNeXt, self).__init__()\n        self.conv1 = nn.Conv3d(\n            3,\n            64,\n            kernel_size=7,\n            stride=(1, 2, 2),\n            padding=(3, 3, 3),\n            bias=False)\n        #self.conv1 = nn.Conv3d(\n        #    3,\n        #    64,\n        #    kernel_size=(3,7,7),\n        #    stride=(1, 2, 2),\n        #    padding=(1, 3, 3),\n        #    bias=False)\n        self.bn1 = nn.BatchNorm3d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 128, layers[0], shortcut_type,\n                                       cardinality)\n        self.layer2 = self._make_layer(\n            block, 256, layers[1], shortcut_type, cardinality, stride=2)\n        self.layer3 = self._make_layer(\n            block, 512, layers[2], shortcut_type, cardinality, stride=2)\n        self.layer4 = self._make_layer(\n            block, 1024, layers[3], shortcut_type, cardinality, stride=2)\n        last_duration = int(math.ceil(sample_duration / 16))\n        #last_duration = 1\n        last_size = int(math.ceil(sample_size / 32))\n        self.avgpool = nn.AvgPool3d(\n            (last_duration, last_size, last_size), stride=1)\n        self.fc = nn.Linear(cardinality * 32 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                m.weight = nn.init.kaiming_normal(m.weight, mode=\'fan_out\')\n            elif isinstance(m, nn.BatchNorm3d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self,\n                    block,\n                    planes,\n                    blocks,\n                    shortcut_type,\n                    cardinality,\n                    stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            if shortcut_type == \'A\':\n                downsample = partial(\n                    downsample_basic_block,\n                    planes=planes * block.expansion,\n                    stride=stride)\n            else:\n                downsample = nn.Sequential(\n                    nn.Conv3d(\n                        self.inplanes,\n                        planes * block.expansion,\n                        kernel_size=1,\n                        stride=stride,\n                        bias=False), nn.BatchNorm3d(planes * block.expansion))\n\n        layers = []\n        layers.append(\n            block(self.inplanes, planes, cardinality, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, cardinality))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef get_fine_tuning_parameters(model, ft_portion):\n    if ft_portion == ""complete"":\n        return model.parameters()\n\n    elif ft_portion == ""last_layer"":\n        ft_module_names = []\n        ft_module_names.append(\'classifier\')\n\n        parameters = []\n        for k, v in model.named_parameters():\n            for ft_module in ft_module_names:\n                if ft_module in k:\n                    parameters.append({\'params\': v})\n                    break\n            else:\n                parameters.append({\'params\': v, \'lr\': 0.0})\n        return parameters\n\n    else:\n        raise ValueError(""Unsupported ft_portion: \'complete\' or \'last_layer\' expected"")\n\n\ndef resnext50(**kwargs):\n    """"""Constructs a ResNet-50 model.\n    """"""\n    model = ResNeXt(ResNeXtBottleneck, [3, 4, 6, 3], **kwargs)\n    return model\n\n\ndef resnext101(**kwargs):\n    """"""Constructs a ResNet-101 model.\n    """"""\n    model = ResNeXt(ResNeXtBottleneck, [3, 4, 23, 3], **kwargs)\n    return model\n\n\ndef resnext152(**kwargs):\n    """"""Constructs a ResNet-101 model.\n    """"""\n    model = ResNeXt(ResNeXtBottleneck, [3, 8, 36, 3], **kwargs)\n    return model\n'"
models/shufflenet.py,5,"b'\'\'\'ShuffleNet in PyTorch.\n\nSee the paper ""ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices"" for more details.\n\'\'\'\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\ndef conv_bn(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv3d(inp, oup, kernel_size=3, stride=stride, padding=(1,1,1), bias=False),\n        nn.BatchNorm3d(oup),\n        nn.ReLU(inplace=True)\n    )\n\n\ndef channel_shuffle(x, groups):\n    \'\'\'Channel shuffle: [N,C,H,W] -> [N,g,C/g,H,W] -> [N,C/g,g,H,w] -> [N,C,H,W]\'\'\'\n    batchsize, num_channels, depth, height, width = x.data.size()\n    channels_per_group = num_channels // groups\n    # reshape\n    x = x.view(batchsize, groups, \n        channels_per_group, depth, height, width)\n    #permute\n    x = x.permute(0,2,1,3,4,5).contiguous()\n    # flatten\n    x = x.view(batchsize, num_channels, depth, height, width)\n    return x\n\n\n\nclass Bottleneck(nn.Module):\n    def __init__(self, in_planes, out_planes, stride, groups):\n        super(Bottleneck, self).__init__()\n        self.stride = stride\n        self.groups = groups\n        mid_planes = out_planes//4\n        if self.stride == 2:\n            out_planes = out_planes - in_planes\n        g = 1 if in_planes==24 else groups\n        self.conv1    = nn.Conv3d(in_planes, mid_planes, kernel_size=1, groups=g, bias=False)\n        self.bn1      = nn.BatchNorm3d(mid_planes)\n        self.conv2    = nn.Conv3d(mid_planes, mid_planes, kernel_size=3, stride=stride, padding=1, groups=mid_planes, bias=False)\n        self.bn2      = nn.BatchNorm3d(mid_planes)\n        self.conv3    = nn.Conv3d(mid_planes, out_planes, kernel_size=1, groups=groups, bias=False)\n        self.bn3      = nn.BatchNorm3d(out_planes)\n        self.relu     = nn.ReLU(inplace=True)\n\n        if stride == 2:\n            self.shortcut = nn.AvgPool3d(kernel_size=(2,3,3), stride=2, padding=(0,1,1))\n\n\n    def forward(self, x):\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = channel_shuffle(out, self.groups)\n        out = self.bn2(self.conv2(out))\n        out = self.bn3(self.conv3(out))\n\n        if self.stride == 2:\n            out = self.relu(torch.cat([out, self.shortcut(x)], 1))\n        else:\n            out = self.relu(out + x)\n\n        return out\n\n\nclass ShuffleNet(nn.Module):\n    def __init__(self,\n                 groups,\n                 width_mult=1,\n                 num_classes=400):\n        super(ShuffleNet, self).__init__()\n        self.num_classes = num_classes\n        self.groups = groups\n        num_blocks = [4,8,4]\n\n        # index 0 is invalid and should never be called.\n        # only used for indexing convenience.\n        if groups == 1:\n            out_planes = [24, 144, 288, 567]\n        elif groups == 2:\n            out_planes = [24, 200, 400, 800]\n        elif groups == 3:\n            out_planes = [24, 240, 480, 960]\n        elif groups == 4:\n            out_planes = [24, 272, 544, 1088]\n        elif groups == 8:\n            out_planes = [24, 384, 768, 1536]\n        else:\n            raise ValueError(\n                """"""{} groups is not supported for\n                   1x1 Grouped Convolutions"""""".format(num_groups))\n        out_planes = [int(i * width_mult) for i in out_planes]\n        self.in_planes = out_planes[0]\n        self.conv1   = conv_bn(3, self.in_planes, stride=(1,2,2))\n        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n        self.layer1  = self._make_layer(out_planes[1], num_blocks[0], self.groups)\n        self.layer2  = self._make_layer(out_planes[2], num_blocks[1], self.groups)\n        self.layer3  = self._make_layer(out_planes[3], num_blocks[2], self.groups)\n\n        # building classifier\n        self.classifier = nn.Sequential(\n                            nn.Dropout(0.2),\n                            nn.Linear(out_planes[3], self.num_classes)\n                            )\n\n    def _make_layer(self, out_planes, num_blocks, groups):\n        layers = []\n        for i in range(num_blocks):\n            stride = 2 if i == 0 else 1\n            layers.append(Bottleneck(self.in_planes, out_planes, stride=stride, groups=groups))\n            self.in_planes = out_planes\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.maxpool(out)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = F.avg_pool3d(out, out.data.size()[-3:])\n        out = out.view(out.size(0), -1)\n        out = self.classifier(out)\n        return out\n\ndef get_fine_tuning_parameters(model, ft_portion):\n    if ft_portion == ""complete"":\n        return model.parameters()\n\n    elif ft_portion == ""last_layer"":\n        ft_module_names = []\n        ft_module_names.append(\'classifier\')\n\n        parameters = []\n        for k, v in model.named_parameters():\n            for ft_module in ft_module_names:\n                if ft_module in k:\n                    parameters.append({\'params\': v})\n                    break\n            else:\n                parameters.append({\'params\': v, \'lr\': 0.0})\n        return parameters\n\n    else:\n        raise ValueError(""Unsupported ft_portion: \'complete\' or \'last_layer\' expected"")\n\n\ndef get_model(**kwargs):\n    """"""\n    Returns the model.\n    """"""\n    model = ShuffleNet(**kwargs)\n    return model\n\n\nif __name__ == ""__main__"":\n    model = get_model(groups=3, num_classes=600, width_mult=1)\n    model = model.cuda()\n    model = nn.DataParallel(model, device_ids=None)\n    print(model)\n\n    input_var = Variable(torch.randn(8, 3, 16, 112, 112))\n    output = model(input_var)\n    print(output.shape)\n\n\n'"
models/shufflenetv2.py,6,"b'\'\'\'ShuffleNetV2 in PyTorch.\n\nSee the paper ""ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design"" for more details.\n\'\'\'\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom collections import OrderedDict\nfrom torch.nn import init\nimport math\n\n\ndef conv_bn(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv3d(inp, oup, kernel_size=3, stride=stride, padding=(1,1,1), bias=False),\n        nn.BatchNorm3d(oup),\n        nn.ReLU(inplace=True)\n    )\n\ndef conv_1x1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv3d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm3d(oup),\n        nn.ReLU(inplace=True)\n    )\n\ndef channel_shuffle(x, groups):\n    \'\'\'Channel shuffle: [N,C,H,W] -> [N,g,C/g,H,W] -> [N,C/g,g,H,w] -> [N,C,H,W]\'\'\'\n    batchsize, num_channels, depth, height, width = x.data.size()\n    channels_per_group = num_channels // groups\n    # reshape\n    x = x.view(batchsize, groups, \n        channels_per_group, depth, height, width)\n    #permute\n    x = x.permute(0,2,1,3,4,5).contiguous()\n    # flatten\n    x = x.view(batchsize, num_channels, depth, height, width)\n    return x\n    \nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        oup_inc = oup//2\n        \n        if self.stride == 1:\n            #assert inp == oup_inc\n        \tself.banch2 = nn.Sequential(\n                # pw\n                nn.Conv3d(oup_inc, oup_inc, 1, 1, 0, bias=False),\n                nn.BatchNorm3d(oup_inc),\n                nn.ReLU(inplace=True),\n                # dw\n                nn.Conv3d(oup_inc, oup_inc, 3, stride, 1, groups=oup_inc, bias=False),\n                nn.BatchNorm3d(oup_inc),\n                # pw-linear\n                nn.Conv3d(oup_inc, oup_inc, 1, 1, 0, bias=False),\n                nn.BatchNorm3d(oup_inc),\n                nn.ReLU(inplace=True),\n            )                \n        else:                  \n            self.banch1 = nn.Sequential(\n                # dw\n                nn.Conv3d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm3d(inp),\n                # pw-linear\n                nn.Conv3d(inp, oup_inc, 1, 1, 0, bias=False),\n                nn.BatchNorm3d(oup_inc),\n                nn.ReLU(inplace=True),\n            )        \n    \n            self.banch2 = nn.Sequential(\n                # pw\n                nn.Conv3d(inp, oup_inc, 1, 1, 0, bias=False),\n                nn.BatchNorm3d(oup_inc),\n                nn.ReLU(inplace=True),\n                # dw\n                nn.Conv3d(oup_inc, oup_inc, 3, stride, 1, groups=oup_inc, bias=False),\n                nn.BatchNorm3d(oup_inc),\n                # pw-linear\n                nn.Conv3d(oup_inc, oup_inc, 1, 1, 0, bias=False),\n                nn.BatchNorm3d(oup_inc),\n                nn.ReLU(inplace=True),\n            )\n          \n    @staticmethod\n    def _concat(x, out):\n        # concatenate along channel axis\n        return torch.cat((x, out), 1)        \n\n    def forward(self, x):\n        if self.stride == 1:\n            x1 = x[:, :(x.shape[1]//2), :, :, :]\n            x2 = x[:, (x.shape[1]//2):, :, :, :]\n            out = self._concat(x1, self.banch2(x2))\n        elif self.stride == 2:\n            out = self._concat(self.banch1(x), self.banch2(x))\n\n        return channel_shuffle(out, 2)\n\n\nclass ShuffleNetV2(nn.Module):\n    def __init__(self, num_classes=600, sample_size=112, width_mult=1.):\n        super(ShuffleNetV2, self).__init__()\n        assert sample_size % 16 == 0\n        \n        self.stage_repeats = [4, 8, 4]\n        # index 0 is invalid and should never be called.\n        # only used for indexing convenience.\n        if width_mult == 0.25:\n            self.stage_out_channels = [-1, 24,  32,  64, 128, 1024]\n        elif width_mult == 0.5:\n            self.stage_out_channels = [-1, 24,  48,  96, 192, 1024]\n        elif width_mult == 1.0:\n            self.stage_out_channels = [-1, 24, 116, 232, 464, 1024]\n        elif width_mult == 1.5:\n            self.stage_out_channels = [-1, 24, 176, 352, 704, 1024]\n        elif width_mult == 2.0:\n            self.stage_out_channels = [-1, 24, 224, 488, 976, 2048]\n        else:\n            raise ValueError(\n                """"""{} groups is not supported for\n                       1x1 Grouped Convolutions"""""".format(num_groups))\n\n        # building first layer\n        input_channel = self.stage_out_channels[1]\n        self.conv1 = conv_bn(3, input_channel, stride=(1,2,2))\n        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n        \n        self.features = []\n        # building inverted residual blocks\n        for idxstage in range(len(self.stage_repeats)):\n            numrepeat = self.stage_repeats[idxstage]\n            output_channel = self.stage_out_channels[idxstage+2]\n            for i in range(numrepeat):\n                stride = 2 if i == 0 else 1\n                self.features.append(InvertedResidual(input_channel, output_channel, stride))\n                input_channel = output_channel\n                \n        # make it nn.Sequential\n        self.features = nn.Sequential(*self.features)\n\n        # building last several layers\n        self.conv_last      = conv_1x1x1_bn(input_channel, self.stage_out_channels[-1])\n    \n\t    # building classifier\n        self.classifier = nn.Sequential(\n                            nn.Dropout(0.2),\n                            nn.Linear(self.stage_out_channels[-1], num_classes)\n                            )\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.maxpool(out)\n        out = self.features(out)\n        out = self.conv_last(out)\n        out = F.avg_pool3d(out, out.data.size()[-3:])\n        out = out.view(out.size(0), -1)\n        out = self.classifier(out)\n        return out\n\n\ndef get_fine_tuning_parameters(model, ft_portion):\n    if ft_portion == ""complete"":\n        return model.parameters()\n\n    elif ft_portion == ""last_layer"":\n        ft_module_names = []\n        ft_module_names.append(\'classifier\')\n\n        parameters = []\n        for k, v in model.named_parameters():\n            for ft_module in ft_module_names:\n                if ft_module in k:\n                    parameters.append({\'params\': v})\n                    break\n            else:\n                parameters.append({\'params\': v, \'lr\': 0.0})\n        return parameters\n\n    else:\n        raise ValueError(""Unsupported ft_portion: \'complete\' or \'last_layer\' expected"")\n\n\ndef get_model(**kwargs):\n    """"""\n    Returns the model.\n    """"""\n    model = ShuffleNetV2(**kwargs)\n    return model\n   \n\nif __name__ == ""__main__"":\n    model = get_model(num_classes=600, sample_size=112, width_mult=1.)\n    model = model.cuda()\n    model = nn.DataParallel(model, device_ids=None)\n    print(model)\n\n    input_var = Variable(torch.randn(8, 3, 16, 112, 112))\n    output = model(input_var)\n    print(output.shape)\n'"
models/squeezenet.py,6,"b'\'\'\'SqueezeNet in PyTorch.\n\nSee the paper ""SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size"" for more details.\n\'\'\'\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom functools import partial\n\n__all__ = [\'SqueezeNet\', \'squeezenet1_0\', \'squeezenet1_1\']\n\n\nclass Fire(nn.Module):\n\n    def __init__(self, inplanes, squeeze_planes,\n                 expand1x1_planes, expand3x3_planes,\n                 use_bypass=False):\n        super(Fire, self).__init__()\n        self.use_bypass = use_bypass\n        self.inplanes = inplanes\n        self.relu = nn.ReLU(inplace=True)\n        self.squeeze = nn.Conv3d(inplanes, squeeze_planes, kernel_size=1)\n        self.squeeze_bn = nn.BatchNorm3d(squeeze_planes)\n        self.expand1x1 = nn.Conv3d(squeeze_planes, expand1x1_planes,\n                                   kernel_size=1)\n        self.expand1x1_bn = nn.BatchNorm3d(expand1x1_planes)\n        self.expand3x3 = nn.Conv3d(squeeze_planes, expand3x3_planes,\n                                   kernel_size=3, padding=1)\n        self.expand3x3_bn = nn.BatchNorm3d(expand3x3_planes)\n\n    def forward(self, x):\n        out = self.squeeze(x)\n        out = self.squeeze_bn(out)\n        out = self.relu(out)\n\n        out1 = self.expand1x1(out)\n        out1 = self.expand1x1_bn(out1)\n        \n        out2 = self.expand3x3(out)\n        out2 = self.expand3x3_bn(out2)\n\n        out = torch.cat([out1, out2], 1)\n        if self.use_bypass:\n        \tout += x\n        out = self.relu(out)\n\n        return out\n\n\nclass SqueezeNet(nn.Module):\n\n    def __init__(self,\n                 sample_size,\n                 sample_duration,\t\t\t\n    \t         version=1.1,\n    \t         num_classes=600):\n        super(SqueezeNet, self).__init__()\n        if version not in [1.0, 1.1]:\n            raise ValueError(""Unsupported SqueezeNet version {version}:""\n                             ""1.0 or 1.1 expected"".format(version=version))\n        self.num_classes = num_classes\n        last_duration = int(math.ceil(sample_duration / 16))\n        last_size = int(math.ceil(sample_size / 32))\n        if version == 1.0:\n            self.features = nn.Sequential(\n                nn.Conv3d(3, 96, kernel_size=7, stride=(1,2,2), padding=(3,3,3)),\n                nn.BatchNorm3d(96),\n                nn.ReLU(inplace=True),\n                nn.MaxPool3d(kernel_size=3, stride=2, padding=1),\n                Fire(96, 16, 64, 64),\n                Fire(128, 16, 64, 64, use_bypass=True),\n                nn.MaxPool3d(kernel_size=3, stride=2, padding=1),\n                Fire(128, 32, 128, 128),\n                Fire(256, 32, 128, 128, use_bypass=True),\n                nn.MaxPool3d(kernel_size=3, stride=2, padding=1),\n                Fire(256, 48, 192, 192),\n                Fire(384, 48, 192, 192, use_bypass=True),\n                Fire(384, 64, 256, 256),\n                nn.MaxPool3d(kernel_size=3, stride=2, padding=1),\n                Fire(512, 64, 256, 256, use_bypass=True),\n            )\n        if version == 1.1:\n            self.features = nn.Sequential(\n                nn.Conv3d(3, 64, kernel_size=3, stride=(1,2,2), padding=(1,1,1)),\n                nn.BatchNorm3d(64),\n                nn.ReLU(inplace=True),\n                nn.MaxPool3d(kernel_size=3, stride=2, padding=1),\n                Fire(64, 16, 64, 64),\n                Fire(128, 16, 64, 64, use_bypass=True),\n                nn.MaxPool3d(kernel_size=3, stride=2, padding=1),\n                Fire(128, 32, 128, 128),\n                Fire(256, 32, 128, 128, use_bypass=True),\n                nn.MaxPool3d(kernel_size=3, stride=2, padding=1),\n                Fire(256, 48, 192, 192),\n                Fire(384, 48, 192, 192, use_bypass=True),\n                nn.MaxPool3d(kernel_size=3, stride=2, padding=1),\n                Fire(384, 64, 256, 256),\n                Fire(512, 64, 256, 256, use_bypass=True),\n            )\n        # Final convolution is initialized differently form the rest\n        final_conv = nn.Conv3d(512, self.num_classes, kernel_size=1)\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.5),\n            final_conv,\n            nn.ReLU(inplace=True),\n            nn.AvgPool3d((last_duration, last_size, last_size), stride=1)\n        )\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                m.weight = nn.init.kaiming_normal_(m.weight, mode=\'fan_out\')\n            elif isinstance(m, nn.BatchNorm3d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x.view(x.size(0), -1)\n\n\ndef get_fine_tuning_parameters(model, ft_portion):\n    if ft_portion == ""complete"":\n        return model.parameters()\n\n    elif ft_portion == ""last_layer"":\n        ft_module_names = []\n        ft_module_names.append(\'classifier\')\n\n        parameters = []\n        for k, v in model.named_parameters():\n            for ft_module in ft_module_names:\n                if ft_module in k:\n                    parameters.append({\'params\': v})\n                    break\n            else:\n                parameters.append({\'params\': v, \'lr\': 0.0})\n        return parameters\n\n    else:\n        raise ValueError(""Unsupported ft_portion: \'complete\' or \'last_layer\' expected"")\n\n    \ndef get_model(**kwargs):\n    """"""\n    Returns the model.\n    """"""\n    model = SqueezeNet(**kwargs)\n    return model\n\n\nif __name__ == \'__main__\':\n    model = SqueezeNet(version=1.1, sample_size = 112, sample_duration = 16, num_classes=600)\n    model = model.cuda()\n    model = nn.DataParallel(model, device_ids=None)\n    print(model)\n\n    input_var = Variable(torch.randn(8, 3, 16, 112, 112))\n    output = model(input_var)\n    print(output.shape)\n'"
thop/__init__.py,0,b'from .utils import profile'
thop/count_hooks.py,11,"b'import argparse\n\nimport torch\nimport torch.nn as nn\n\nmultiply_adds = 1\n\n\ndef count_conv2d(m, x, y):\n\t# TODO: add support for pad and dilation\n\tx = x[0]\n\n\tcin = m.in_channels\n\tcout = m.out_channels\n\tkh, kw = m.kernel_size\n\tbatch_size = x.size()[0]\n\n\tout_w = y.size(2) // m.stride[0]\n\tout_h = y.size(3) // m.stride[1]\n\n\t# ops per output element\n\t# kernel_mul = kh * kw * cin\n\t# kernel_add = kh * kw * cin - 1\n\tkernel_ops = multiply_adds * kh * kw * cin // m.groups\n\tbias_ops = 1 if m.bias is not None else 0\n\tops_per_element = kernel_ops + bias_ops\n\n\t# total ops\n\t# num_out_elements = y.numel()\n\toutput_elements = batch_size * out_w * out_h * cout\n\ttotal_ops = output_elements * ops_per_element\n\n\t# in case same conv is used multiple times\n\tm.total_ops += torch.Tensor([int(total_ops)])\n\n\ndef count_conv3d(m, x, y):\n\t# TODO: add support for pad and dilation\n\tx = x[0]\n\n\tcin = m.in_channels\n\tcout = m.out_channels\n\tkd, kh, kw = m.kernel_size\n\tbatch_size = x.size()[0]\n    \n\tout_d = y.size(2) // m.stride[0]\n\tout_w = y.size(3) // m.stride[1]\n\tout_h = y.size(4) // m.stride[2]\n\n\t# ops per output element\n\t# kernel_mul = kh * kw * cin\n\t# kernel_add = kh * kw * cin - 1\n\tkernel_ops = multiply_adds * kd * kh * kw * cin // m.groups\n\tbias_ops = 1 if m.bias is not None else 0\n\tops_per_element = kernel_ops + bias_ops\n\n\t# total ops\n\t# num_out_elements = y.numel()\n\toutput_elements = batch_size * out_d * out_w * out_h * cout\n\ttotal_ops = output_elements * ops_per_element\n\n\t# in case same conv is used multiple times\n\tm.total_ops += torch.Tensor([int(total_ops)])\n\n\ndef count_bn2d(m, x, y):\n\tx = x[0]\n\n\tnelements = x.numel()\n\ttotal_sub = nelements\n\ttotal_div = nelements\n\ttotal_ops = total_sub + total_div\n\n\tm.total_ops += torch.Tensor([int(total_ops)])\n\n\ndef count_relu(m, x, y):\n\tx = x[0]\n\n\tnelements = x.numel()\n\ttotal_ops = nelements\n\n\tm.total_ops += torch.Tensor([int(total_ops)])\n\n\ndef count_softmax(m, x, y):\n\tx = x[0]\n\n\tbatch_size, nfeatures = x.size()\n\n\ttotal_exp = nfeatures\n\ttotal_add = nfeatures - 1\n\ttotal_div = nfeatures\n\ttotal_ops = batch_size * (total_exp + total_add + total_div)\n\n\tm.total_ops += torch.Tensor([int(total_ops)])\n\n\ndef count_maxpool(m, x, y):\n\tkernel_ops = torch.prod(torch.Tensor([m.kernel_size])) - 1\n\tnum_elements = y.numel()\n\ttotal_ops = kernel_ops * num_elements\n\n\tm.total_ops += torch.Tensor([int(total_ops)])\n\n\ndef count_avgpool(m, x, y):\n\ttotal_add = torch.prod(torch.Tensor([m.kernel_size])) - 1\n\ttotal_div = 1\n\tkernel_ops = total_add + total_div\n\tnum_elements = y.numel()\n\ttotal_ops = kernel_ops * num_elements\n\n\tm.total_ops += torch.Tensor([int(total_ops)])\n\n\ndef count_linear(m, x, y):\n\t# per output element\n\ttotal_mul = m.in_features\n\ttotal_add = m.in_features - 1\n\tnum_elements = y.numel()\n\ttotal_ops = (total_mul + total_add) * num_elements\n\n\tm.total_ops += torch.Tensor([int(total_ops)])\n'"
thop/utils.py,5,"b'import logging\n\nimport torch\nimport torch.nn as nn\nfrom .count_hooks import *\n\nregister_hooks = {\n\tnn.Conv2d: count_conv2d,\n\tnn.Conv3d: count_conv3d,\n\tnn.BatchNorm2d: count_bn2d,\n\tnn.BatchNorm3d: count_bn2d,\n\tnn.ReLU: count_relu,\n\tnn.ReLU6: count_relu,\n\tnn.MaxPool1d: count_maxpool,\n\tnn.MaxPool2d: count_maxpool,\n\tnn.MaxPool3d: count_maxpool,\n\tnn.AvgPool1d: count_avgpool,\n\tnn.AvgPool2d: count_avgpool,\n\tnn.AvgPool3d: count_avgpool,\n\tnn.Linear: count_linear,\n\tnn.Dropout: None,\n}\n\n\ndef profile(model, input_size, custom_ops={}):\n\tdef add_hooks(m):\n\t\tif len(list(m.children())) > 0:\n\t\t\treturn\n\n\t\tm.register_buffer(\'total_ops\', torch.zeros(1))\n\t\tm.register_buffer(\'total_params\', torch.zeros(1))\n\n\t\tfor p in m.parameters():\n\t\t\tm.total_params += torch.Tensor([p.numel()])\n\n\t\tm_type = type(m)\n\t\tfn = None\n\n\t\tif m_type in custom_ops:\n\t\t\tfn = custom_ops[m_type]\n\t\telif m_type in register_hooks:\n\t\t\tfn = register_hooks[m_type]\n\t\telse:\n\t\t\tlogging.warning(""Not implemented for "", m)\n\n\t\tif fn is not None:\n\t\t\tlogging.info(""Register FLOP counter for module %s"" % str(m))\n\t\t\tm.register_forward_hook(fn)\n\n\tmodel.eval()\n\tmodel.apply(add_hooks)\n\n\tx = torch.zeros(input_size)\n\tmodel(x)\n\n\ttotal_ops = 0\n\ttotal_params = 0\n\tfor m in model.modules():\n\t\tif len(list(m.children())) > 0: # skip for non-leaf module\n\t\t\tcontinue\n\t\ttotal_ops += m.total_ops\n\t\ttotal_params += m.total_params\n\n\ttotal_ops = total_ops.item()\n\ttotal_params = total_params.item()\n\n\treturn total_ops, total_params'"
utils/ego_prepare.py,0,"b'from __future__ import print_function, division\nimport sys\nimport glob \nimport pandas as pd\nimport csv\nimport os\n\npath_to_dataset = \'/usr/home/kop/datasets/EgoGesture\'\n\npaths = sorted(glob.glob(os.path.join(path_to_dataset,\'labels-final-revised1/*/*/*\' )))\n########################################################\n##################### SUBJECT LIST #####################\n########################################################\nsubject_ids = [\'{num:02d}\'.format(num=i) for i in range(1,51)]\n\nsubject_ids_train = [\'{num:02d}\'.format(num=i) for i in [3, 4, 5, 6, 8, 10, 15, 16, 17, 20, 21, 22, 23,\\\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t 25, 26, 27, 30, 32, 36, 38, 39, 40, 42, 43, 44,\\\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t  45, 46, 48, 49, 50]]\nsubject_ids_val = [\'{num:02d}\'.format(num=i) for i in [1, 7, 12, 13, 24, 29, 33, 34, 35, 37]]\nsubject_ids_test = [\'{num:02d}\'.format(num=i) for i in [ 2, 9, 11, 14, 18, 19, 28, 31, 41, 47]]\n\n########################################################\n\ndef create_trainlist( subset ,file_name, class_types = \'all\'):\n\t\n\t\n\tfolder1 = \'Color\'\n\tfolder2 = \'rgb\'\n\tif subset == \'training\': # Check subject id for validation/train split\n\t\tsubjects_to_process = subject_ids_train\n\telif subset == \'validation\':\n\t\tsubjects_to_process = subject_ids_val\n\telif subset == \'testing\':\n\t\tsubjects_to_process = subject_ids_test\n\telse:\n\t\traise(ValueError(""Subset cannot be no other than training, validation, testing""))\n\n\n\tprint(""Preparing Lines"")\n\tnew_lines = []\n\tfor path in paths:\n\t\tdf = pd.read_csv(path,index_col = False, header = None)\n\t\tx = path.rsplit(os.sep,4)\n\t\tsubject = x[2]\n\t\tif subject[-2:] in subjects_to_process:\n\n\t\t\tindex = x[-1].split(\'.\')[0][-1]\n\t\t\tfolder_path = os.path.join(subject.title(), x[3],\'{}\'.format(folder1),\\\n\t\t\t\t\'{}\'.format(folder2)+ index)\n\t\t\t\n\n\t\t\tfull_path = os.path.join(\'/\'+x[0],\'images\',folder_path)\n\t\t\tn_images = len(sorted(glob.glob(full_path + \'/*\')))\n\t\t\tdf_val = df.values\n\t\t\tstart = 1\n\t\t\tend = df_val[1,1] -1 \n\t\t\tlen_lines = df_val.shape[0]\n\t\t\tfor i in range(len_lines):\n\t\t\t\tline = df_val[i,:]\n\t\t\t\tif class_types == \'all\':\n\t\t\t\t\tif (line[1] - start) >= 8:# Some action starts right away so I do not add None LABEL\n\t\t\t\t\t\tnew_lines.append(folder_path + \' \' + str(84)+ \' \' + str(start)+ \' \' + str(line[1]-1))\n\t\t\t\t\tnew_lines.append(folder_path + \' \' + str(line[0])+ \' \' + str(line[1])+ \' \' + str(line[2]))\n\t\t\t\telif class_types == \'all_but_None\':\n\t\t\t\t\tnew_lines.append(folder_path + \' \' + str(line[0])+ \' \' + str(line[1])+ \' \' + str(line[2]))\n\t\t\t\telif class_types == \'binary\':\n\t\t\t\t\tif (line[1] - start) >= 8:# Some action starts right away so I do not add None LABEL\n\t\t\t\t\t\tnew_lines.append(folder_path + \' \' + \'1\' + \' \' + str(start)+ \' \' + str(line[1]-1))\n\t\t\t\t\tnew_lines.append(folder_path + \' \' + \'2\' + \' \' + str(line[1])+ \' \' + str(line[2]))\n\t\t\t\t\n\t\t\t\tstart = line[2]+1\n\t\t\tif (n_images - start >8):\n\t\t\t\t# Class 84 is None(Non Gesture) class\n\t\t\t\tif class_types == \'all\':\n\t\t\t\t\tnew_lines.append(folder_path + \' \' + \'84\'+ \' \' + str(start)+ \' \' + str(n_images))\n\t\t\t\telif class_types == \'binary\':\n\t\t\t\t\tnew_lines.append(folder_path + \' \' + \'1\' + \' \' + str(start)+ \' \' + str(n_images))\n\t\telse:\n\t\t\tcontinue\n\n\tprint(""Writing to the file ..."")\n\tfile_path = os.path.join(\'annotation_EgoGesture\',file_name)\n\twith open(file_path, \'w\') as myfile:\n\t    for new_line in new_lines:\n\t    \tmyfile.write(new_line)\n\t    \tmyfile.write(\'\\n\')\n\tprint(""Scuccesfully wrote file to:"",file_path)\n\nif __name__ == \'__main__\':\n\t# This file helps to index videos in the dataset by creating a .txt file where every line is a video clip\n\t# has the gesture\n\t# The format of each line is as following: <path to the folder> <class index> <start frame> <end frame>\n    subset = sys.argv[1]\n    file_name = sys.argv[2]\n    class_types = sys.argv[3]\n    create_trainlist(subset, file_name, class_types)\n\n    # HOW TO RUN:\n    # python ego_prepare.py training trainlistall.txt all\n'"
utils/egogesture_json.py,0,"b""from __future__ import print_function, division\nimport os\nimport sys\nimport json\nimport pandas as pd\n\ndef convert_csv_to_dict(csv_path, subset, labels):\n    data = pd.read_csv(csv_path, delimiter=' ', header=None)\n    keys = []\n    key_labels = []\n    key_start_frame = []\n    key_end_frame = []\n    for i in range(data.shape[0]):\n        row = data.ix[i, :]\n        class_name = labels[row[1]-1]\n        basename = str(row[0])\n        start_frame = str(row[2])\n        end_frame = str(row[3])\n\n        keys.append(basename)\n        key_labels.append(class_name)\n        key_start_frame.append(start_frame)\n        key_end_frame.append(end_frame)\n        \n    database = {}\n    for i in range(len(keys)):\n        key = keys[i] \n        if key in database: # need this because I have the same folder several  times \n            key = key + '_' + str(i) \n        \n        database[key] = {}\n        database[key]['subset'] = subset\n        label = key_labels[i]\n        start_frame = key_start_frame[i]\n        end_frame = key_end_frame[i]\n\n        database[key]['annotations'] = {'label': label, 'start_frame':start_frame, 'end_frame':end_frame}\n    \n    return database\n\ndef load_labels(label_csv_path):\n    data = pd.read_csv(label_csv_path, delimiter=' ', header=None)\n    labels = []\n    for i in range(data.shape[0]):\n        labels.append(data.ix[i, 1])\n    return labels\n\ndef convert_egogesture_csv_to_activitynet_json(label_csv_path, train_csv_path, \n                                           val_csv_path, dst_json_path):\n    labels = load_labels(label_csv_path)\n    train_database = convert_csv_to_dict(train_csv_path, 'training', labels)\n    val_database = convert_csv_to_dict(val_csv_path, 'validation', labels)\n    test_database = convert_csv_to_dict(test_csv_path, 'testing', labels)\n    \n    dst_data = {}\n    dst_data['labels'] = labels\n    dst_data['database'] = {}\n    dst_data['database'].update(train_database)\n    dst_data['database'].update(val_database)\n    dst_data['database'].update(test_database)\n\n    with open(dst_json_path, 'w') as dst_file:\n        json.dump(dst_data, dst_file)\n\nif __name__ == '__main__':\n    csv_dir_path = sys.argv[1]\n    class_types = sys.argv[2]\n    if class_types == 'all':\n        class_ind_file = 'classIndAll.txt'\n    elif class_types == 'all_but_None':\n        class_ind_file = 'classIndAllbutNone.txt'\n    elif class_types == 'binary':\n        class_ind_file = 'classIndBinary.txt'\n\n\n    label_csv_path = os.path.join(csv_dir_path, class_ind_file)\n    train_csv_path = os.path.join(csv_dir_path, 'trainlist'+ class_types + '.txt')\n    val_csv_path = os.path.join(csv_dir_path, 'vallist'+ class_types + '.txt')\n    test_csv_path = os.path.join(csv_dir_path, 'testlist'+ class_types + '.txt')\n    dst_json_path = os.path.join(csv_dir_path, 'egogesture' + class_types + '.json')\n    \n    convert_egogesture_csv_to_activitynet_json(label_csv_path, train_csv_path,\n                                               val_csv_path, dst_json_path)\n    print('Successfully wrote to json : ', dst_json_path)\n    # HOW TO RUN:\n    # python egogesture_json.py '../annotation_EgoGesture' all\n"""
utils/eval_kinetics.py,0,"b'import json\n\nimport numpy as np\nimport pandas as pd\n\nclass KINETICSclassification(object):\n    GROUND_TRUTH_FIELDS = [\'database\', \'labels\']\n    PREDICTION_FIELDS = [\'results\', \'version\', \'external_data\']\n\n    def __init__(self, ground_truth_filename=None, prediction_filename=None,\n                 ground_truth_fields=GROUND_TRUTH_FIELDS,\n                 prediction_fields=PREDICTION_FIELDS,\n                 subset=\'validation\', verbose=False, top_k=1,\n                 check_status=True):\n        if not ground_truth_filename:\n            raise IOError(\'Please input a valid ground truth file.\')\n        if not prediction_filename:\n            raise IOError(\'Please input a valid prediction file.\')\n        self.subset = subset\n        self.verbose = verbose\n        self.gt_fields = ground_truth_fields\n        self.pred_fields = prediction_fields\n        self.top_k = top_k\n        self.ap = None\n        self.hit_at_k = None\n        self.check_status = check_status\n        # Retrieve blocked videos from server.\n        if self.check_status:\n            self.blocked_videos = get_blocked_videos()\n        else:\n            self.blocked_videos = list()\n        # Import ground truth and predictions.\n        self.ground_truth, self.activity_index = self._import_ground_truth(\n            ground_truth_filename)\n        self.prediction = self._import_prediction(prediction_filename)\n\n        if self.verbose:\n            print(\'[INIT] Loaded annotations from {} subset.\'.format(subset))\n            nr_gt = len(self.ground_truth)\n            print(\'\\tNumber of ground truth instances: {}\'.format(nr_gt))\n            nr_pred = len(self.prediction)\n            print(\'\\tNumber of predictions: {}\'.format(nr_pred))\n\n    def _import_ground_truth(self, ground_truth_filename):\n        """"""Reads ground truth file, checks if it is well formatted, and returns\n           the ground truth instances and the activity classes.\n\n        Parameters\n        ----------\n        ground_truth_filename : str\n            Full path to the ground truth json file.\n\n        Outputs\n        -------\n        ground_truth : df\n            Data frame containing the ground truth instances.\n        activity_index : dict\n            Dictionary containing class index.\n        """"""\n        with open(ground_truth_filename, \'r\') as fobj:\n            data = json.load(fobj)\n        # Checking format\n        # if not all([field in data.keys() for field in self.gt_fields]):\n            # raise IOError(\'Please input a valid ground truth file.\')\n\n        # Initialize data frame\n        activity_index, cidx = {}, 0\n        video_lst, label_lst = [], []\n        for videoid, v in data[\'database\'][self.subset].items():\n            if self.subset != v[\'subset\']:\n                continue\n            if videoid in self.blocked_videos:\n                continue\n            this_label = v[\'annotations\'][\'label\']\n            if this_label not in activity_index:\n                activity_index[this_label] = cidx\n                cidx += 1\n            video_lst.append(videoid[:-14])\n            label_lst.append(activity_index[this_label])\n        ground_truth = pd.DataFrame({\'video-id\': video_lst,\n                                     \'label\': label_lst})\n        ground_truth = ground_truth.drop_duplicates().reset_index(drop=True)\n        return ground_truth, activity_index\n\n    def _import_prediction(self, prediction_filename):\n        """"""Reads prediction file, checks if it is well formatted, and returns\n           the prediction instances.\n\n        Parameters\n        ----------\n        prediction_filename : str\n            Full path to the prediction json file.\n\n        Outputs\n        -------\n        prediction : df\n            Data frame containing the prediction instances.\n        """"""\n        with open(prediction_filename, \'r\') as fobj:\n            data = json.load(fobj)\n        # Checking format...\n        # if not all([field in data.keys() for field in self.pred_fields]):\n            # raise IOError(\'Please input a valid prediction file.\')\n\n        # Initialize data frame\n        video_lst, label_lst, score_lst = [], [], []\n        for videoid, v in data[\'results\'].items():\n            if videoid in self.blocked_videos:\n                continue\n            for result in v:\n                label = self.activity_index[result[\'label\']]\n                video_lst.append(videoid)\n                label_lst.append(label)\n                score_lst.append(result[\'score\'])\n        prediction = pd.DataFrame({\'video-id\': video_lst,\n                                   \'label\': label_lst,\n                                   \'score\': score_lst})\n        return prediction\n\n    def evaluate(self):\n        """"""Evaluates a prediction file. For the detection task we measure the\n        interpolated mean average precision to measure the performance of a\n        method.\n        """"""\n        hit_at_k = compute_video_hit_at_k(self.ground_truth,\n                                          self.prediction, top_k=self.top_k)\n        # avg_hit_at_k = compute_video_hit_at_k(\n            # self.ground_truth, self.prediction, top_k=self.top_k, avg=True)\n        if self.verbose:\n            print(\'[RESULTS] Performance on ActivityNet untrimmed video \'\n                   \'classification task.\')\n            # print \'\\tMean Average Precision: {}\'.format(ap.mean())\n            print(\'\\tError@{}: {}\'.format(self.top_k, 1.0 - hit_at_k))\n            #print \'\\tAvg Hit@{}: {}\'.format(self.top_k, avg_hit_at_k)\n        # self.ap = ap\n        self.hit_at_k = hit_at_k\n        # self.avg_hit_at_k = avg_hit_at_k\n\n################################################################################\n# Metrics\n################################################################################\n\ndef compute_video_hit_at_k(ground_truth, prediction, top_k=3, avg=False):\n    """"""Compute accuracy at k prediction between ground truth and\n    predictions data frames. This code is greatly inspired by evaluation\n    performed in Karpathy et al. CVPR14.\n\n    Parameters\n    ----------\n    ground_truth : df\n        Data frame containing the ground truth instances.\n        Required fields: [\'video-id\', \'label\']\n    prediction : df\n        Data frame containing the prediction instances.\n        Required fields: [\'video-id, \'label\', \'score\']\n\n    Outputs\n    -------\n    acc : float\n        Top k accuracy score.\n    """"""\n    video_ids = np.unique(ground_truth[\'video-id\'].values)\n    avg_hits_per_vid = np.zeros(video_ids.size)\n    for i, vid in enumerate(video_ids):\n        pred_idx = prediction[\'video-id\'] == vid\n        if not pred_idx.any():\n            continue\n        this_pred = prediction.loc[pred_idx].reset_index(drop=True)\n        # Get top K predictions sorted by decreasing score.\n        sort_idx = this_pred[\'score\'].values.argsort()[::-1][:top_k]\n        this_pred = this_pred.loc[sort_idx].reset_index(drop=True)\n        # Get labels and compare against ground truth.\n        pred_label = this_pred[\'label\'].tolist()\n        gt_idx = ground_truth[\'video-id\'] == vid\n        gt_label = ground_truth.loc[gt_idx][\'label\'].tolist()\n        avg_hits_per_vid[i] = np.mean([1 if this_label in pred_label else 0\n                                       for this_label in gt_label])\n        if not avg:\n            avg_hits_per_vid[i] = np.ceil(avg_hits_per_vid[i])\n    return float(avg_hits_per_vid.mean())\n'"
utils/eval_ucf101.py,0,"b'import json\n\nimport numpy as np\nimport pandas as pd\n\nclass UCFclassification(object):\n\n    def __init__(self, ground_truth_filename=None, prediction_filename=None,\n                 subset=\'validation\', verbose=False, top_k=1):\n        if not ground_truth_filename:\n            raise IOError(\'Please input a valid ground truth file.\')\n        if not prediction_filename:\n            raise IOError(\'Please input a valid prediction file.\')\n        self.subset = subset\n        self.verbose = verbose\n        self.top_k = top_k\n        self.ap = None\n        self.hit_at_k = None\n        # Import ground truth and predictions.\n        self.ground_truth, self.activity_index = self._import_ground_truth(\n            ground_truth_filename)\n        self.prediction = self._import_prediction(prediction_filename)\n\n        if self.verbose:\n            print(\'[INIT] Loaded annotations from {} subset.\'.format(subset))\n            nr_gt = len(self.ground_truth)\n            print(\'\\tNumber of ground truth instances: {}\'.format(nr_gt))\n            nr_pred = len(self.prediction)\n            print(\'\\tNumber of predictions: {}\'.format(nr_pred))\n\n    def _import_ground_truth(self, ground_truth_filename):\n        """"""Reads ground truth file, checks if it is well formatted, and returns\n           the ground truth instances and the activity classes.\n\n        Parameters\n        ----------\n        ground_truth_filename : str\n            Full path to the ground truth json file.\n\n        Outputs\n        -------\n        ground_truth : df\n            Data frame containing the ground truth instances.\n        activity_index : dict\n            Dictionary containing class index.\n        """"""\n        with open(ground_truth_filename, \'r\') as fobj:\n            data = json.load(fobj)\n        # Checking format\n        # if not all([field in data.keys() for field in self.gt_fields]):\n            # raise IOError(\'Please input a valid ground truth file.\')\n\n        # Initialize data frame\n        activity_index, cidx = {}, 0\n        video_lst, label_lst = [], []\n        for videoid, v in data[\'database\'].items():\n            if self.subset != v[\'subset\']:\n                continue\n            this_label = v[\'annotations\'][\'label\']\n            if this_label not in activity_index:\n                activity_index[this_label] = cidx\n                cidx += 1\n            video_lst.append(videoid)\n            label_lst.append(activity_index[this_label])\n        ground_truth = pd.DataFrame({\'video-id\': video_lst,\n                                     \'label\': label_lst})\n        ground_truth = ground_truth.drop_duplicates().reset_index(drop=True)\n        return ground_truth, activity_index\n\n    def _import_prediction(self, prediction_filename):\n        """"""Reads prediction file, checks if it is well formatted, and returns\n           the prediction instances.\n\n        Parameters\n        ----------\n        prediction_filename : str\n            Full path to the prediction json file.\n\n        Outputs\n        -------\n        prediction : df\n            Data frame containing the prediction instances.\n        """"""\n        with open(prediction_filename, \'r\') as fobj:\n            data = json.load(fobj)\n        # Checking format...\n        # if not all([field in data.keys() for field in self.pred_fields]):\n            # raise IOError(\'Please input a valid prediction file.\')\n\n        # Initialize data frame\n        video_lst, label_lst, score_lst = [], [], []\n        for videoid, v in data[\'results\'].items():\n            for result in v:\n                label = self.activity_index[result[\'label\']]\n                video_lst.append(videoid)\n                label_lst.append(label)\n                score_lst.append(result[\'score\'])\n        prediction = pd.DataFrame({\'video-id\': video_lst,\n                                   \'label\': label_lst,\n                                   \'score\': score_lst})\n        return prediction\n\n    def evaluate(self):\n        """"""Evaluates a prediction file. For the detection task we measure the\n        interpolated mean average precision to measure the performance of a\n        method.\n        """"""\n        hit_at_k = compute_video_hit_at_k(self.ground_truth,\n                                          self.prediction, top_k=self.top_k)\n        if self.verbose:\n            print(\'[RESULTS] Performance on ActivityNet untrimmed video \'\n                   \'classification task.\')\n            print(\'\\tError@{}: {}\'.format(self.top_k, 1.0 - hit_at_k))\n            #print \'\\tAvg Hit@{}: {}\'.format(self.top_k, avg_hit_at_k)\n        self.hit_at_k = hit_at_k\n\n################################################################################\n# Metrics\n################################################################################\ndef compute_video_hit_at_k(ground_truth, prediction, top_k=3):\n    """"""Compute accuracy at k prediction between ground truth and\n    predictions data frames. This code is greatly inspired by evaluation\n    performed in Karpathy et al. CVPR14.\n\n    Parameters\n    ----------\n    ground_truth : df\n        Data frame containing the ground truth instances.\n        Required fields: [\'video-id\', \'label\']\n    prediction : df\n        Data frame containing the prediction instances.\n        Required fields: [\'video-id, \'label\', \'score\']\n\n    Outputs\n    -------\n    acc : float\n        Top k accuracy score.\n    """"""\n    video_ids = np.unique(ground_truth[\'video-id\'].values)\n    avg_hits_per_vid = np.zeros(video_ids.size)\n    for i, vid in enumerate(video_ids):\n        pred_idx = prediction[\'video-id\'] == vid\n        if not pred_idx.any():\n            continue\n        this_pred = prediction.loc[pred_idx].reset_index(drop=True)\n        # Get top K predictions sorted by decreasing score.\n        sort_idx = this_pred[\'score\'].values.argsort()[::-1][:top_k]\n        this_pred = this_pred.loc[sort_idx].reset_index(drop=True)\n        # Get labels and compare against ground truth.\n        pred_label = this_pred[\'label\'].tolist()\n        gt_idx = ground_truth[\'video-id\'] == vid\n        gt_label = ground_truth.loc[gt_idx][\'label\'].tolist()\n        avg_hits_per_vid[i] = np.mean([1 if this_label in pred_label else 0\n                                       for this_label in gt_label])\n    return float(avg_hits_per_vid.mean())\n'"
utils/jester_json.py,0,"b""from __future__ import print_function, division\nimport os\nimport sys\nimport json\nimport pandas as pd\n\ndef convert_csv_to_dict(csv_path, subset, labels):\n    data = pd.read_csv(csv_path, delimiter=' ', header=None)\n    keys = []\n    key_labels = []\n    for i in range(data.shape[0]):\n        row = data.iloc[i, :]\n        class_name = labels[row[1]-1]\n        basename = str(row[0])\n        \n        keys.append(basename)\n        key_labels.append(class_name)\n        \n    database = {}\n    for i in range(len(keys)):\n        key = keys[i]\n        database[key] = {}\n        database[key]['subset'] = subset\n        label = key_labels[i]\n        database[key]['annotations'] = {'label': label}\n    \n    return database\n\ndef load_labels(label_csv_path):\n    data = pd.read_csv(label_csv_path, delimiter=' ', header=None)\n    labels = []\n    for i in range(data.shape[0]):\n        labels.append(data.iloc[i, 1])\n    return labels\n\ndef convert_jester_csv_to_activitynet_json(label_csv_path, train_csv_path, \n                                           val_csv_path, dst_json_path):\n    labels = load_labels(label_csv_path)\n    train_database = convert_csv_to_dict(train_csv_path, 'training', labels)\n    val_database = convert_csv_to_dict(val_csv_path, 'validation', labels)\n    \n    dst_data = {}\n    dst_data['labels'] = labels\n    dst_data['database'] = {}\n    dst_data['database'].update(train_database)\n    dst_data['database'].update(val_database)\n\n    with open(dst_json_path, 'w') as dst_file:\n        json.dump(dst_data, dst_file)\n\nif __name__ == '__main__':\n    csv_dir_path = sys.argv[1]\n\n    label_csv_path = os.path.join(csv_dir_path, 'classInd.txt')\n    train_csv_path = os.path.join(csv_dir_path, 'trainlist.txt')\n    val_csv_path = os.path.join(csv_dir_path, 'vallist.txt')\n    dst_json_path = os.path.join(csv_dir_path, 'jester.json')\n    \n    convert_jester_csv_to_activitynet_json(label_csv_path, train_csv_path,\n                                               val_csv_path, dst_json_path)\n\n"""
utils/kinetics_json.py,0,"b'from __future__ import print_function, division\nimport os\nimport cv2\nimport sys\nimport json\nimport pandas as pd\n\ndef convert_csv_to_dict(csv_path, dataset_path, subset):\n    data = pd.read_csv(csv_path)\n    keys = []\n    key_labels = []\n    for i in range(data.shape[0]):\n        row = data.iloc[i, :]\n        basename = \'%s_%s_%s.mp4\' % (row[\'youtube_id\'],\n                                 \'%06d\' % row[\'time_start\'],\n                                 \'%06d\' % row[\'time_end\'])\n        keys.append(basename)\n        if subset != \'testing\':\n            key_labels.append(row[\'label\'])\n\n    database = {}\n    for i in range(len(keys)):\n        key = keys[i]\n        database[key] = {}\n        database[key][\'subset\'] = subset\n        if subset != \'testing\':\n            label = key_labels[i]\n            database[key][\'annotations\'] = {\'label\': label}\n        else:\n            database[key][\'annotations\'] = {}\n        # Add n_frames\n        video_path = os.path.join(dataset_path, label, key)\n        cap = cv2.VideoCapture(video_path)\n        database[key][\'n_frames\'] = int(cap.get(7)) # Returns the number of frames in the video\n        cap.release()\n\n    cap.release()\n    return database\n\ndef load_labels(train_csv_path):\n    data = pd.read_csv(train_csv_path)\n    return data[\'label\'].unique().tolist()\n\ndef convert_kinetics_csv_to_activitynet_json(train_csv_path, val_csv_path, test_csv_path, dataset_path, dst_json_path):\n    labels = load_labels(val_csv_path)\n    val_database = convert_csv_to_dict(val_csv_path, dataset_path, \'validation\')\n    train_database = convert_csv_to_dict(train_csv_path, dataset_path, \'training\')\n\n    dst_data = {}\n    dst_data[\'labels\'] = labels\n    dst_data[\'database\'] = {""training"": train_database, ""validation"": val_database}\n\n    with open(dst_json_path, \'w\') as dst_file:\n        json.dump(dst_data, dst_file)\n\nif __name__==""__main__"":\n  train_csv_path = sys.argv[1]\n  val_csv_path = sys.argv[2]\n  dataset_path = sys.argv[3]\n  dst_json_path = sys.argv[4]\n\n  convert_kinetics_csv_to_activitynet_json(\n    train_csv_path, val_csv_path, dataset_path, dst_json_path)\n'"
utils/n_frames_jester.py,0,"b'from __future__ import print_function, division\nimport os\nimport sys\nimport subprocess\n\ndef class_process(dir_path):\n    if not os.path.isdir(dir_path):\n        return\n\n    for file_name in os.listdir(dir_path):\n        video_dir_path = os.path.join(dir_path, file_name)\n        image_indices = []\n        for image_file_name in os.listdir(video_dir_path):\n            if \'00\' not in image_file_name:\n                continue\n            image_indices.append(int(image_file_name[0:4]))\n\n        if len(image_indices) == 0:\n            print(\'no image files\', video_dir_path)\n            n_frames = 0\n        else:\n            image_indices.sort(reverse=True)\n            n_frames = len(image_indices)\n            print(video_dir_path, n_frames)\n        with open(os.path.join(video_dir_path, \'n_frames\'), \'w\') as dst_file:\n            dst_file.write(str(n_frames))\n\n\nif __name__==""__main__"":\n    dir_path = sys.argv[1]\n    class_process(dir_path)\n'"
utils/n_frames_kinetics.py,0,"b'from __future__ import print_function, division\nimport os\nimport sys\nimport subprocess\n\ndef class_process(dir_path, class_name):\n  class_path = os.path.join(dir_path, class_name)\n  if not os.path.isdir(class_path):\n    return\n\n  for file_name in os.listdir(class_path):\n    video_dir_path = os.path.join(class_path, file_name)\n    image_indices = []\n    for image_file_name in os.listdir(video_dir_path):\n      if \'image\' not in image_file_name:\n        continue\n      image_indices.append(int(image_file_name[6:11]))\n\n    if len(image_indices) == 0:\n      print(\'no image files\', video_dir_path)\n      n_frames = 0\n    else:\n      image_indices.sort(reverse=True)\n      n_frames = image_indices[0]\n      print(video_dir_path, n_frames)\n    with open(os.path.join(video_dir_path, \'n_frames\'), \'w\') as dst_file:\n      dst_file.write(str(n_frames))\n\n\nif __name__==""__main__"":\n  dir_path = sys.argv[1]\n  for class_name in os.listdir(dir_path):\n    class_process(dir_path, class_name)\n\n  class_name = \'test\'\n  class_process(dir_path, class_name)\n'"
utils/n_frames_ucf101_hmdb51.py,0,"b'from __future__ import print_function, division\nimport os\nimport sys\nimport subprocess\n\ndef class_process(dir_path, class_name):\n  class_path = os.path.join(dir_path, class_name)\n  if not os.path.isdir(class_path):\n    return\n\n  for file_name in os.listdir(class_path):\n    video_dir_path = os.path.join(class_path, file_name)\n    image_indices = []\n    for image_file_name in os.listdir(video_dir_path):\n      if \'image\' not in image_file_name:\n        continue\n      image_indices.append(int(image_file_name[6:11]))\n\n    if len(image_indices) == 0:\n      print(\'no image files\', video_dir_path)\n      n_frames = 0\n    else:\n      image_indices.sort(reverse=True)\n      n_frames = image_indices[0]\n      print(video_dir_path, n_frames)\n    with open(os.path.join(video_dir_path, \'n_frames\'), \'w\') as dst_file:\n      dst_file.write(str(n_frames))\n\n\nif __name__==""__main__"":\n  dir_path = sys.argv[1]\n  for class_name in os.listdir(dir_path):\n    class_process(dir_path, class_name)\n'"
utils/nv_json.py,0,"b""from __future__ import print_function, division\nimport os\nimport sys\nimport json\nimport pandas as pd\n\ndef convert_csv_to_dict(csv_path, subset, labels):\n    data = pd.read_csv(csv_path, delimiter=' ', header=None)\n    keys = []\n    key_labels = []\n    key_start_frame = []\n    key_end_frame = []\n    for i in range(data.shape[0]):\n        row = data.ix[i, :]\n        class_name = labels[row[1]-1]\n        \n        basename = str(row[0])\n        start_frame = str(row[2])\n        end_frame = str(row[3])\n\n        keys.append(basename)\n        key_labels.append(class_name)\n        key_start_frame.append(start_frame)\n        key_end_frame.append(end_frame)\n        \n    database = {}\n    for i in range(len(keys)):\n        key = keys[i]  \n        if key in database: # need this because I have the same folder 3  times\n            key = key + '^' + str(i) \n        database[key] = {}\n        database[key]['subset'] = subset\n        label = key_labels[i]\n        start_frame = key_start_frame[i]\n        end_frame = key_end_frame[i]\n\n        database[key]['annotations'] = {'label': label, 'start_frame':start_frame, 'end_frame':end_frame}\n    \n    return database\n\ndef load_labels(label_csv_path):\n    data = pd.read_csv(label_csv_path, delimiter=' ', header=None)\n    labels = []\n    for i in range(data.shape[0]):\n        labels.append(str(data.ix[i, 1]))\n    return labels\n\ndef convert_nv_csv_to_activitynet_json(label_csv_path, train_csv_path, \n                                           val_csv_path, dst_json_path):\n    labels = load_labels(label_csv_path)\n    train_database = convert_csv_to_dict(train_csv_path, 'training', labels)\n    val_database = convert_csv_to_dict(val_csv_path, 'validation', labels)\n    \n    dst_data = {}\n    dst_data['labels'] = labels\n    dst_data['database'] = {}\n    dst_data['database'].update(train_database)\n    dst_data['database'].update(val_database)\n    with open(dst_json_path, 'w') as dst_file:\n        json.dump(dst_data, dst_file)\n\nif __name__ == '__main__':\n    csv_dir_path = sys.argv[1]\n    for class_type in ['all', 'all_but_None', 'binary']:\n\n        if class_type == 'all':\n            class_ind_file = 'classIndAll.txt'\n        elif class_type == 'all_but_None':\n            class_ind_file = 'classIndAllbutNone.txt'\n        elif class_type == 'binary':\n            class_ind_file = 'classIndBinary.txt'\n\n\n        label_csv_path = os.path.join(csv_dir_path, class_ind_file)\n        train_csv_path = os.path.join(csv_dir_path, 'trainlist'+ class_type + '.txt')\n        val_csv_path = os.path.join(csv_dir_path, 'vallist'+ class_type + '.txt')\n        dst_json_path = os.path.join(csv_dir_path, 'nv' + class_type + '.json')\n\n        convert_nv_csv_to_activitynet_json(label_csv_path, train_csv_path,\n                                               val_csv_path, dst_json_path)\n        print('Successfully wrote to json : ', dst_json_path)\n    # HOW TO RUN:\n    # python nv_json.py '../annotation_nvGesture'\n"""
utils/nv_prepare.py,0,"b'import os\nimport cv2\nimport numpy as np\nimport glob\nimport sys\nfrom subprocess import call\n\ndataset_path = ""/data2/nvGesture""\ndef load_split_nvgesture(file_with_split = \'./nvgesture_train_correct.lst\',list_split = list()):\n    file_with_split = os.path.join(dataset_path,file_with_split)\n    params_dictionary = dict()\n    with open(file_with_split,\'rb\') as f:\n          dict_name  = file_with_split[file_with_split.rfind(\'/\')+1 :]\n          dict_name  = dict_name[:dict_name.find(\'_\')]\n\n          for line in f:\n            params = line.decode().split(\' \')\n            params_dictionary = dict()\n\n            params_dictionary[\'dataset\'] = dict_name\n\n            path = params[0].split(\':\')[1]\n            for param in params[1:]:\n                    parsed = param.split(\':\')\n                    key = parsed[0]\n                    if key == \'label\':\n                        # make label start from 0\n                        label = int(parsed[1]) - 1 \n                        params_dictionary[\'label\'] = label\n                    elif key in (\'depth\',\'color\',\'duo_left\'):\n                        #othrwise only sensors format: <sensor name>:<folder>:<start frame>:<end frame>\n                        sensor_name = key\n                        #first store path\n                        params_dictionary[key] = path + \'/\' + parsed[1] \n                        #store start frame\n                        params_dictionary[key+\'_start\'] = int(parsed[2])\n\n                        params_dictionary[key+\'_end\'] = int(parsed[3])\n                        \n        \n            params_dictionary[\'duo_right\'] = params_dictionary[\'duo_left\'].replace(\'duo_left\', \'duo_right\')\n            params_dictionary[\'duo_right_start\'] = params_dictionary[\'duo_left_start\']\n            params_dictionary[\'duo_right_end\'] = params_dictionary[\'duo_left_end\']          \n\n            params_dictionary[\'duo_disparity\'] = params_dictionary[\'duo_left\'].replace(\'duo_left\', \'duo_disparity\')\n            params_dictionary[\'duo_disparity_start\'] = params_dictionary[\'duo_left_start\']\n            params_dictionary[\'duo_disparity_end\'] = params_dictionary[\'duo_left_end\']                  \n\n            list_split.append(params_dictionary)\n \n    return list_split\n\ndef create_list(example_config, sensor,  class_types = \'all\'):\n\n    folder_path = example_config[sensor] + \'_all\'\n    n_images = len(glob.glob(os.path.join(folder_path, \'*.jpg\')))\n\n    label = example_config[\'label\']+1\n    start_frame = example_config[sensor+\'_start\']\n    end_frame = example_config[sensor+\'_end\']\n\n    \n    frame_indices = np.array([[start_frame,end_frame]])\n    len_lines = frame_indices.shape[0]\n    start = 1\n    for i in range(len_lines):\n        line = frame_indices[i,:]\n        if class_types == \'all\':\n            if (line[0] - start) >= 8:# Some action starts right away so I do not add None LABEL\n                new_lines.append(folder_path + \' \' + \'26\'+ \' \' + str(start)+ \' \' + str(line[0]-1))\n            new_lines.append(folder_path + \' \' + str(label)+ \' \' + str(line[0])+ \' \' + str(line[1]))\n        elif class_types == \'all_but_None\':\n            new_lines.append(folder_path + \' \' + str(label)+ \' \' + str(line[0])+ \' \' + str(line[1]))\n        elif class_types == \'binary\':\n            if (line[0] - start) >= 8:# Some action starts right away so I do not add None LABEL\n                new_lines.append(folder_path + \' \' + \'1\' + \' \' + str(start)+ \' \' + str(line[0]-1))\n            new_lines.append(folder_path + \' \' + \'2\' + \' \' + str(line[0])+ \' \' + str(line[1]))\n        start = line[1]+1\n\n    if (n_images - start >4):\n        if class_types == \'all\':\n            new_lines.append(folder_path + \' \' + \'26\'+ \' \' + str(start)+ \' \' + str(n_images))\n        elif class_types == \'binary\':\n            new_lines.append(folder_path + \' \' + \'1\' + \' \' + str(start)+ \' \' + str(n_images))\n\ndef extract_frames(sensors=[""color"", ""depth""]):\n    """"""Extract frames of .avi files.\n    \n    Parameters\n    ----------\n    modalities: list of str, [""color"", ""depth"", ""duo_left"", ""duo_right"", ""duo_disparity""]\n    """"""\n    for vt in sensors:\n        files = glob.glob(os.path.join(dataset_path, \n                                       ""Video_data"",\n                                       ""*"", ""*"", \n                                       ""sk_"" + vt + "".avi"")) # this line should be updated according to the full path \n        for file in files:\n            print(""Extracting frames for "", file)\n            directory = file.split(""."")[0] + ""_all""\n            if not os.path.exists(directory):\n                os.makedirs(directory)\n            call([""ffmpeg"", ""-i"",  file, os.path.join(directory, ""%05d.jpg""), ""-hide_banner""]) \n       \n    \nif __name__ == ""__main__"":\n    sensors = [""color"", ""depth""]\n    subset = sys.argv[1]\n    file_name = sys.argv[2]\n    class_types = sys.argv[3]\n\n    sensors = [""color""]\n    file_lists = dict()\n    if subset == \'training\':\n        file_list = ""./nvgesture_train_correct_cvpr2016_v2.lst""\n    elif subset == \'validation\':\n        file_list = ""./nvgesture_test_correct_cvpr2016_v2.lst""\n    \n\n    subset_list = list()\n\n    load_split_nvgesture(file_with_split = file_list,list_split = subset_list)\n\n    new_lines = [] \n    print(""Processing Traing List"")\n    for sample_name in subset_list:\n        create_list(example_config = sample_name, sensor = sensors[0], class_types = class_types)\n\n\n    print(""Writing to the file ..."")\n    file_path = os.path.join(\'annotation_nvGesture\',file_name)\n    with open(file_path, \'w\') as myfile:\n        for new_line in new_lines:\n            myfile.write(new_line)\n            myfile.write(\'\\n\')\n    print(""Scuccesfully wrote file to:"",file_path)\n    \n    extract_frames(sensors=sensors)\n'"
utils/ucf101_json.py,0,"b""from __future__ import print_function, division\nimport os\nimport sys\nimport json\nimport pandas as pd\n\ndef convert_csv_to_dict(csv_path, subset):\n    data = pd.read_csv(csv_path, delimiter=' ', header=None)\n    keys = []\n    key_labels = []\n    for i in range(data.shape[0]):\n        row = data.ix[i, :]\n        slash_rows = data.ix[i, 0].split('/')\n        class_name = slash_rows[0]\n        basename = slash_rows[1].split('.')[0]\n        \n        keys.append(basename)\n        key_labels.append(class_name)\n        \n    database = {}\n    for i in range(len(keys)):\n        key = keys[i]\n        database[key] = {}\n        database[key]['subset'] = subset\n        label = key_labels[i]\n        database[key]['annotations'] = {'label': label}\n    \n    return database\n\ndef load_labels(label_csv_path):\n    data = pd.read_csv(label_csv_path, delimiter=' ', header=None)\n    labels = []\n    for i in range(data.shape[0]):\n        labels.append(data.ix[i, 1])\n    return labels\n\ndef convert_ucf101_csv_to_activitynet_json(label_csv_path, train_csv_path, \n                                           val_csv_path, dst_json_path):\n    labels = load_labels(label_csv_path)\n    train_database = convert_csv_to_dict(train_csv_path, 'training')\n    val_database = convert_csv_to_dict(val_csv_path, 'validation')\n    \n    dst_data = {}\n    dst_data['labels'] = labels\n    dst_data['database'] = {}\n    dst_data['database'].update(train_database)\n    dst_data['database'].update(val_database)\n\n    with open(dst_json_path, 'w') as dst_file:\n        json.dump(dst_data, dst_file)\n\nif __name__ == '__main__':\n    csv_dir_path = sys.argv[1]\n\n    for split_index in range(1, 4):\n        label_csv_path = os.path.join(csv_dir_path, 'classInd.txt')\n        train_csv_path = os.path.join(csv_dir_path, 'trainlist0{}.txt'.format(split_index))\n        val_csv_path = os.path.join(csv_dir_path, 'testlist0{}.txt'.format(split_index))\n        dst_json_path = os.path.join(csv_dir_path, 'ucf101_0{}.json'.format(split_index))\n\n        convert_ucf101_csv_to_activitynet_json(label_csv_path, train_csv_path,\n                                               val_csv_path, dst_json_path)\n"""
utils/video_accuracy.py,0,"b""from eval_ucf101 import UCFclassification\nfrom eval_kinetics import KINETICSclassification\n\n\n\n# ucf_classification = UCFclassification('../annotation_UCF101/ucf101_01.json',\n#                                        '../results/val.json',\n#                                        subset='validation', top_k=1)\n# ucf_classification.evaluate()\n# print(ucf_classification.hit_at_k)\n\n\nkinetics_classification = KINETICSclassification('../annotation_Kinetics/kinetics.json',\n                                       '../results/val.json',\n                                       subset='validation',\n                                       top_k=1,\n                                       check_status=False)\nkinetics_classification.evaluate()\nprint(kinetics_classification.hit_at_k)\n"""
utils/video_jpg.py,0,"b'from __future__ import print_function, division\nimport os\nimport sys\nimport subprocess\n\n\nif __name__==""__main__"":\n  dir_path = sys.argv[1]\n  dst_dir_path = sys.argv[2]\n\n  for file_name in os.listdir(dir_path):\n    if \'.mp4\' not in file_name:\n      continue\n    name, ext = os.path.splitext(file_name)\n    dst_directory_path = os.path.join(dst_dir_path, name)\n\n    video_file_path = os.path.join(dir_path, file_name)\n    try:\n      if os.path.exists(dst_directory_path):\n        if not os.path.exists(os.path.join(dst_directory_path, \'image_00001.jpg\')):\n          subprocess.call(\'rm -r {}\'.format(dst_directory_path), shell=True)\n          print(\'remove {}\'.format(dst_directory_path))\n          os.mkdir(dst_directory_path)\n        else:\n          continue\n      else:\n        os.mkdir(dst_directory_path)\n    except:\n      print(dst_directory_path)\n      continue\n    cmd = \'ffmpeg -i {} -vf scale=-1:360 {}/image_%05d.jpg\'.format(video_file_path, dst_directory_path)\n    print(cmd)\n    subprocess.call(cmd, shell=True)\n    print(\'\\n\')\n'"
utils/video_jpg_kinetics.py,0,"b'from __future__ import print_function, division\nimport os\nimport sys\nimport subprocess\n\ndef class_process(dir_path, dst_dir_path, class_name):\n  class_path = os.path.join(dir_path, class_name)\n  if not os.path.isdir(class_path):\n    return\n\n  dst_class_path = os.path.join(dst_dir_path, class_name)\n  if not os.path.exists(dst_class_path):\n    os.mkdir(dst_class_path)\n\n  for file_name in os.listdir(class_path):\n    if \'.mp4\' not in file_name:\n      continue\n    name, ext = os.path.splitext(file_name)\n    dst_directory_path = os.path.join(dst_class_path, name)\n\n    video_file_path = os.path.join(class_path, file_name)\n    try:\n      if os.path.exists(dst_directory_path):\n        if not os.path.exists(os.path.join(dst_directory_path, \'image_00001.jpg\')):\n          subprocess.call(\'rm -r \\""{}\\""\'.format(dst_directory_path), shell=True)\n          print(\'remove {}\'.format(dst_directory_path))\n          os.mkdir(dst_directory_path)\n        else:\n          continue\n      else:\n        os.mkdir(dst_directory_path)\n    except:\n      print(dst_directory_path)\n      continue\n    cmd = \'ffmpeg -i \\""{}\\"" -vf scale=-1:240 \\""{}/image_%05d.jpg\\""\'.format(video_file_path, dst_directory_path)\n    print(cmd)\n    subprocess.call(cmd, shell=True)\n    print(\'\\n\')\n\nif __name__==""__main__"":\n  dir_path = sys.argv[1]\n  dst_dir_path = sys.argv[2]\n\n  for class_name in os.listdir(dir_path):\n    class_process(dir_path, dst_dir_path, class_name)\n\n  class_name = \'test\'\n  class_process(dir_path, dst_dir_path, class_name)\n'"
utils/video_jpg_ucf101_hmdb51.py,0,"b'from __future__ import print_function, division\nimport os\nimport sys\nimport subprocess\n\ndef class_process(dir_path, dst_dir_path, class_name):\n  class_path = os.path.join(dir_path, class_name)\n  if not os.path.isdir(class_path):\n    return\n\n  dst_class_path = os.path.join(dst_dir_path, class_name)\n  if not os.path.exists(dst_class_path):\n    os.mkdir(dst_class_path)\n\n  for file_name in os.listdir(class_path):\n    if \'.avi\' not in file_name:\n      continue\n    name, ext = os.path.splitext(file_name)\n    dst_directory_path = os.path.join(dst_class_path, name)\n\n    video_file_path = os.path.join(class_path, file_name)\n    try:\n      if os.path.exists(dst_directory_path):\n        if not os.path.exists(os.path.join(dst_directory_path, \'image_00001.jpg\')):\n          subprocess.call(\'rm -r \\""{}\\""\'.format(dst_directory_path), shell=True)\n          print(\'remove {}\'.format(dst_directory_path))\n          os.mkdir(dst_directory_path)\n        else:\n          continue\n      else:\n        os.mkdir(dst_directory_path)\n    except:\n      print(dst_directory_path)\n      continue\n    cmd = \'ffmpeg -i \\""{}\\"" -vf scale=-1:240 \\""{}/image_%05d.jpg\\""\'.format(video_file_path, dst_directory_path)\n    print(cmd)\n    subprocess.call(cmd, shell=True)\n    print(\'\\n\')\n\nif __name__==""__main__"":\n  dir_path = sys.argv[1]\n  dst_dir_path = sys.argv[2]\n\n  for class_name in os.listdir(dir_path):\n    class_process(dir_path, dst_dir_path, class_name)\n'"
