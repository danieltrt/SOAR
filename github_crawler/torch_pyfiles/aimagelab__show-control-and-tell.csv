file_path,api_count,code
config.py,0,"b""# COCO root path\ncoco_root = 'datasets/coco/'\n\n# Flickr30k root path\nflickr_root = 'datasets/flickr/'\n\n# Flickr30kEntities root path\nflickr_entities_root = 'datasets/flickr/Flickr30kEntities/'\n"""
test_region_sequence.py,4,"b'from speaksee.data import TextField, ImageDetectionsField\nfrom data import COCOControlSequenceField\nfrom data import FlickrDetectionField, FlickrControlSequenceField\nfrom data.dataset import COCOEntities, FlickrEntities\nfrom models import ControllableCaptioningModel\nfrom models import ControllableCaptioningModel_NoVisualSentinel, ControllableCaptioningModel_SingleSentinel\nfrom speaksee.data import DataLoader, DictionaryDataset, RawField\nfrom speaksee.evaluation import Bleu, Meteor, Rouge, Cider, Spice\nfrom speaksee.evaluation import PTBTokenizer\nfrom utils import NWNounAligner\nfrom config import *\nimport torch\nimport random\nimport numpy as np\nimport itertools\nimport argparse\nimport os\nfrom tqdm import tqdm\n\nrandom.seed(1234)\ntorch.manual_seed(1234)\ndevice = torch.device(\'cuda\')\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--exp_name\', default=\'ours\', type=str,\n                    help=\'model name: ours | ours_without_visual_sentinel | ours_with_single_sentinel\')\nparser.add_argument(\'--dataset\', default=\'coco\', type=str, help=\'dataset: coco | flickr\')\nparser.add_argument(\'--sample_rl\', action=\'store_true\', help=\'test the model with cider optimization\')\nparser.add_argument(\'--sample_rl_nw\', action=\'store_true\', help=\'test the model with cider + nw optimization\')\nparser.add_argument(\'--batch_size\', default=16, type=int, help=\'batch size\')\nparser.add_argument(\'--nb_workers\', default=0, type=int, help=\'number of workers\')\nopt_test = parser.parse_args()\nprint(opt_test)\n\nassert(opt_test.exp_name in [\'ours\', \'ours_without_visual_sentinel\', \'ours_with_single_sentinel\'])\n\nif not opt_test.sample_rl and not opt_test.sample_rl_nw:\n    exp_name =\'%s_%s\' % (opt_test.exp_name, opt_test.dataset)\n    print(\'Loading \\""%s\\"" model trained with cross-entropy loss.\' % opt_test.exp_name)\nif opt_test.sample_rl:\n    exp_name = \'%s_%s_%s\' % (opt_test.exp_name, opt_test.dataset, \'rl\')\n    print(\'Loading \\""%s\\"" model trained with CIDEr optimization.\' % opt_test.exp_name)\nif opt_test.sample_rl_nw:\n    exp_name = \'%s_%s_%s\' % (opt_test.exp_name, opt_test.dataset, \'rl_nw\')\n    print(\'Loading \\""%s\\"" model trained with CIDEr + NW optimization.\' % opt_test.exp_name)\nsaved_data = torch.load(\'saved_models/%s/%s.pth\' % (opt_test.exp_name, exp_name))\nopt = saved_data[\'opt\']\n\nif opt_test.dataset == \'coco\':\n    image_field = ImageDetectionsField(detections_path=os.path.join(coco_root, \'coco_detections.hdf5\'), load_in_tmp=False)\n\n    det_field = COCOControlSequenceField(detections_path=os.path.join(coco_root, \'coco_detections.hdf5\'),\n                                         classes_path=os.path.join(coco_root, \'object_class_list.txt\'),\n                                         pad_init=False, padding_idx=-1, all_boxes=False, fix_length=20)\n\n    text_field = TextField(init_token=\'<bos>\', eos_token=\'<eos>\', lower=True, remove_punctuation=True, fix_length=20)\n\n    dataset = COCOEntities(image_field, det_field, text_field,\n                           img_root=\'\',\n                           ann_root=os.path.join(coco_root, \'annotations\'),\n                           entities_file=os.path.join(coco_root, \'coco_entities.json\'),\n                           id_root=os.path.join(coco_root, \'annotations\'))\n\n    test_dataset = COCOEntities(image_field, det_field, RawField(),\n                                img_root=\'\',\n                                ann_root=os.path.join(coco_root, \'annotations\'),\n                                entities_file=os.path.join(coco_root, \'coco_entities.json\'),\n                                id_root=os.path.join(coco_root, \'annotations\'),\n                                filtering=True)\n\n    nw_aligner = NWNounAligner(pre_comp_file=os.path.join(coco_root, \'%s_noun_glove.pkl\' % opt_test.dataset), normalized=True)\n\nelif opt_test.dataset == \'flickr\':\n    image_field = FlickrDetectionField(detections_path=os.path.join(flickr_root, \'flickr30k_detections.hdf5\'))\n\n    det_field = FlickrControlSequenceField(detections_path=os.path.join(flickr_root, \'flickr30k_detections.hdf5\'),\n                                           pad_init=False, padding_idx=-1, fix_length=20)\n\n    text_field = TextField(init_token=\'<bos>\', eos_token=\'<eos>\', lower=True, remove_punctuation=True, fix_length=20)\n\n    dataset = FlickrEntities(image_field, text_field, det_field,\n                             img_root=\'\',\n                             ann_file=os.path.join(flickr_root, \'flickr30k_annotations.json\'),\n                             entities_root=flickr_entities_root)\n\n    test_dataset = FlickrEntities(image_field, RawField(), det_field,\n                                  img_root=\'\',\n                                  ann_file=os.path.join(flickr_root, \'flickr30k_annotations.json\'),\n                                  entities_root=flickr_entities_root)\n\n    nw_aligner = NWNounAligner(pre_comp_file=os.path.join(flickr_root, \'%s_noun_glove.pkl\' % opt_test.dataset), normalized=True)\n\nelse:\n    raise NotImplementedError\n\ntrain_dataset, val_dataset, _ = dataset.splits\ntext_field.build_vocab(train_dataset, val_dataset, min_freq=5)\n\nif opt_test.exp_name == \'ours\':\n    model = ControllableCaptioningModel(20, len(text_field.vocab), text_field.vocab.stoi[\'<bos>\'],\n                                        h2_first_lstm=opt.h2_first_lstm, img_second_lstm=opt.img_second_lstm).to(device)\nelif opt_test.exp_name == \'ours_without_visual_sentinel\':\n    model = ControllableCaptioningModel_NoVisualSentinel(20, len(text_field.vocab), text_field.vocab.stoi[\'<bos>\'],\n                                                         h2_first_lstm=opt.h2_first_lstm,\n                                                         img_second_lstm=opt.img_second_lstm).to(device)\nelif opt_test.exp_name == \'ours_with_single_sentinel\':\n    model = ControllableCaptioningModel_SingleSentinel(20, len(text_field.vocab), text_field.vocab.stoi[\'<bos>\'],\n                                                       h2_first_lstm=opt.h2_first_lstm,\n                                                       img_second_lstm=opt.img_second_lstm).to(device)\nelse:\n    raise NotImplementedError\n\n_, _, test_dataset = test_dataset.splits\ntest_dataset = DictionaryDataset(test_dataset.examples, test_dataset.fields, \'image\')\ndataloader_test = DataLoader(test_dataset, batch_size=opt_test.batch_size, num_workers=opt_test.nb_workers)\n\nmodel.eval()\nmodel.load_state_dict(saved_data[\'state_dict\'])\n\npredictions = []\ngt_captions = []\nmax_len = 20\n\nwith tqdm(desc=\'Test\', unit=\'it\', total=len(iter(dataloader_test))) as pbar:\n    for it, (keys, values) in enumerate(iter(dataloader_test)):\n        detections = keys\n        _, _, ctrl_det_seqs_test, _, captions = values\n        for i in range(detections.size(0)):\n            ctrl_det_seqs_test_i = ctrl_det_seqs_test[i].numpy()\n            ctrl_det_seqs_sum = np.sum(np.abs(ctrl_det_seqs_test_i), axis=-1)\n            _, unique_indexes, unique_inverse = np.unique(ctrl_det_seqs_sum, axis=0, return_index=True, return_inverse=True)\n            ctrl_det_seqs_test_unique = ctrl_det_seqs_test_i[unique_indexes]\n            this_captions = [[captions[i][ii] for ii in range(len(unique_inverse)) if unique_inverse[ii] == jj] for jj in range(ctrl_det_seqs_test_unique.shape[0])]\n\n            detections_i, ctrl_det_seqs_test_unique = detections[i].to(device), torch.tensor(ctrl_det_seqs_test_unique).float().to(device)\n            detections_i = detections_i.unsqueeze(0).expand(ctrl_det_seqs_test_unique.size(0), detections_i.size(0), detections_i.size(1))\n\n            out, _ = model.beam_search((detections_i, ctrl_det_seqs_test_unique), eos_idxs=[text_field.vocab.stoi[\'<eos>\'], -1], beam_size=5)\n\n            out = out[0].data.cpu().numpy()\n            for o, caps in zip(out, this_captions):\n                predictions.append(np.expand_dims(o, axis=0))\n                gt_captions.append(caps)\n\n        pbar.update()\n\npredictions = np.concatenate(predictions, axis=0)\ngen = {}\ngts = {}\nscores_nw = []\n\nprint(""Computing sequence contrallabity results."")\nfor i, cap in enumerate(predictions):\n    pred_cap = text_field.decode(cap, join_words=False)\n    pred_cap = \' \'.join([k for k, g in itertools.groupby(pred_cap)])\n\n    score_nw = 0.\n    for c in gt_captions[i]:\n        score = nw_aligner.score(c, pred_cap)\n        score_nw += score\n    scores_nw.append(score_nw / len(gt_captions[i]))\n\n    gts[i] = gt_captions[i]\n    gen[i] = [pred_cap]\n\n\ngts_t = PTBTokenizer.tokenize(gts)\ngen_t = PTBTokenizer.tokenize(gen)\n\nval_bleu, _ = Bleu(n=4).compute_score(gts_t, gen_t)\nmethod = [\'Blue_1\', \'Bleu_2\', \'Bleu_3\', \'Bleu_4\']\nfor metric, score in zip(method, val_bleu):\n    print(metric, score)\n\nval_meteor, _ = Meteor().compute_score(gts_t, gen_t)\nprint(\'METEOR\', val_meteor)\n\nval_rouge, _ = Rouge().compute_score(gts_t, gen_t)\nprint(\'ROUGE_L\', val_rouge)\n\nval_cider, _ = Cider().compute_score(gts_t, gen_t)\nprint(\'CIDEr\', val_cider)\n\nval_spice, _ = Spice().compute_score(gts_t, gen_t)\nprint(\'SPICE\', val_spice)\n\nprint(\'NW Alignment Score\', np.mean(scores_nw))\n'"
test_region_set.py,8,"b'from speaksee.data import TextField, ImageDetectionsField\nfrom data import COCOControlSetField, FlickrDetectionField, FlickrControlSetField\nfrom data.dataset import COCOEntities, FlickrEntities\nfrom models import ControllableCaptioningModel\nfrom models import ControllableCaptioningModel_NoVisualSentinel, ControllableCaptioningModel_SingleSentinel\nfrom speaksee.data import DataLoader, DictionaryDataset, RawField\nfrom speaksee.evaluation import Bleu, Meteor, Rouge, Cider, Spice\nfrom speaksee.evaluation import PTBTokenizer\nfrom utils import NounIoU\nfrom utils import SinkhornNet\nfrom config import *\nimport torch\nimport random\nimport numpy as np\nimport itertools\nimport argparse\nimport os\nimport munkres\nfrom tqdm import tqdm\n\nrandom.seed(1234)\ntorch.manual_seed(1234)\ndevice = torch.device(\'cuda\')\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--exp_name\', default=\'ours\', type=str,\n                    help=\'model name: ours | ours_without_visual_sentinel | ours_with_single_sentinel\')\nparser.add_argument(\'--dataset\', default=\'coco\', type=str, help=\'dataset: coco | flickr\')\nparser.add_argument(\'--sample_rl\', action=\'store_true\', help=\'test the model with cider optimization\')\nparser.add_argument(\'--sample_rl_nw\', action=\'store_true\', help=\'test the model with cider + nw optimization\')\nparser.add_argument(\'--batch_size\', default=16, type=int, help=\'batch size\')\nparser.add_argument(\'--nb_workers\', default=0, type=int, help=\'number of workers\')\nopt_test = parser.parse_args()\nprint(opt_test)\n\nassert(opt_test.exp_name in [\'ours\', \'ours_without_visual_sentinel\', \'ours_with_single_sentinel\'])\n\nif not opt_test.sample_rl and not opt_test.sample_rl_nw:\n    exp_name =\'%s_%s\' % (opt_test.exp_name, opt_test.dataset)\n    print(\'Loading \\""%s\\"" model trained with cross-entropy loss.\' % opt_test.exp_name)\nif opt_test.sample_rl:\n    exp_name = \'%s_%s_%s\' % (opt_test.exp_name, opt_test.dataset, \'rl\')\n    print(\'Loading \\""%s\\"" model trained with CIDEr optimization.\' % opt_test.exp_name)\nif opt_test.sample_rl_nw:\n    exp_name = \'%s_%s_%s\' % (opt_test.exp_name, opt_test.dataset, \'rl_nw\')\n    print(\'Loading \\""%s\\"" model trained with CIDEr + NW optimization.\' % opt_test.exp_name)\nsaved_data = torch.load(\'saved_models/%s/%s.pth\' % (opt_test.exp_name, exp_name))\nopt = saved_data[\'opt\']\n\nsaved_data_sinkhorn = torch.load(\'saved_models/sinkhorn_network/sinkhorn_network_%s.pth\' % opt_test.dataset)\nopt_sinkhorn = saved_data_sinkhorn[\'opt\']\n\nif opt_test.dataset == \'coco\':\n    image_field = ImageDetectionsField(detections_path=os.path.join(coco_root, \'coco_detections.hdf5\'), load_in_tmp=False)\n\n    det_field = COCOControlSetField(detections_path=os.path.join(coco_root, \'coco_detections.hdf5\'),\n                                    classes_path=os.path.join(coco_root, \'object_class_list.txt\'),\n                                    img_shapes_path=os.path.join(coco_root, \'coco_img_shapes.json\'),\n                                    precomp_glove_path=os.path.join(coco_root, \'object_class_glove.pkl\'),\n                                    fix_length=opt_sinkhorn.max_len, max_detections=20)\n\n    text_field = TextField(init_token=\'<bos>\', eos_token=\'<eos>\', lower=True, remove_punctuation=True, fix_length=20)\n\n    dataset = COCOEntities(image_field, det_field, text_field,\n                           img_root=\'\',\n                           ann_root=os.path.join(coco_root, \'annotations\'),\n                           entities_file=os.path.join(coco_root, \'coco_entities.json\'),\n                           id_root=os.path.join(coco_root, \'annotations\'))\n\n    test_dataset = COCOEntities(image_field, det_field, RawField(),\n                                img_root=\'\',\n                                ann_root=os.path.join(coco_root, \'annotations\'),\n                                entities_file=os.path.join(coco_root, \'coco_entities.json\'),\n                                id_root=os.path.join(coco_root, \'annotations\'),\n                                filtering=True)\n\n    noun_iou = NounIoU(pre_comp_file=os.path.join(coco_root, \'%s_noun_glove.pkl\' % opt_test.dataset))\n\nelif opt_test.dataset == \'flickr\':\n    image_field = FlickrDetectionField(detections_path=os.path.join(flickr_root, \'flickr30k_detections.hdf5\'))\n\n    det_field = FlickrControlSetField(detections_path=os.path.join(flickr_root, \'flickr30k_detections.hdf5\'),\n                                      classes_path=os.path.join(flickr_root, \'object_class_list.txt\'),\n                                      img_shapes_path=os.path.join(flickr_root, \'flickr_img_shapes.json\'),\n                                      precomp_glove_path=os.path.join(flickr_root, \'object_class_glove.pkl\'),\n                                      fix_length=opt_sinkhorn.max_len)\n\n    text_field = TextField(init_token=\'<bos>\', eos_token=\'<eos>\', lower=True, remove_punctuation=True, fix_length=20)\n\n    dataset = FlickrEntities(image_field, text_field, det_field,\n                             img_root=\'\',\n                             ann_file=os.path.join(flickr_root, \'flickr30k_annotations.json\'),\n                             entities_root=flickr_entities_root)\n\n    test_dataset = FlickrEntities(image_field, RawField(), det_field,\n                                  img_root=\'\',\n                                  ann_file=os.path.join(flickr_root, \'flickr30k_annotations.json\'),\n                                  entities_root=flickr_entities_root)\n\n    noun_iou = NounIoU(pre_comp_file=os.path.join(flickr_root, \'%s_noun_glove.pkl\' % opt_test.dataset))\n\nelse:\n    raise NotImplementedError\n\ntrain_dataset, val_dataset, _ = dataset.splits\ntext_field.build_vocab(train_dataset, val_dataset, min_freq=5)\n\nsinkhorn_net = SinkhornNet(opt_sinkhorn.max_len, opt_sinkhorn.n_iters, opt_sinkhorn.tau).to(device)\n\nif opt_test.exp_name == \'ours\':\n    model = ControllableCaptioningModel(20, len(text_field.vocab), text_field.vocab.stoi[\'<bos>\'],\n                                        h2_first_lstm=opt.h2_first_lstm, img_second_lstm=opt.img_second_lstm).to(device)\nelif opt_test.exp_name == \'ours_without_visual_sentinel\':\n    model = ControllableCaptioningModel_NoVisualSentinel(20, len(text_field.vocab), text_field.vocab.stoi[\'<bos>\'],\n                                                         h2_first_lstm=opt.h2_first_lstm,\n                                                         img_second_lstm=opt.img_second_lstm).to(device)\nelif opt_test.exp_name == \'ours_with_single_sentinel\':\n    model = ControllableCaptioningModel_SingleSentinel(20, len(text_field.vocab), text_field.vocab.stoi[\'<bos>\'],\n                                                       h2_first_lstm=opt.h2_first_lstm,\n                                                       img_second_lstm=opt.img_second_lstm).to(device)\nelse:\n    raise NotImplementedError\n\n_, _, test_dataset = test_dataset.splits\ntest_dataset = DictionaryDataset(test_dataset.examples, test_dataset.fields, \'image\')\ndataloader_test = DataLoader(test_dataset, batch_size=opt_test.batch_size, num_workers=opt_test.nb_workers)\n\nmodel.eval()\nmodel.load_state_dict(saved_data[\'state_dict\'])\n\nsinkhorn_net.eval()\nsinkhorn_net.load_state_dict(saved_data_sinkhorn[\'state_dict\'])\n\npredictions = []\ngt_captions = []\nmax_len = 20\n\nwith tqdm(desc=\'Test\', unit=\'it\', total=len(iter(dataloader_test))) as pbar:\n    for it, (keys, values) in enumerate(iter(dataloader_test)):\n        detections = keys\n        det_seqs_txt, det_seqs_vis, det_seqs_pos, det_seqs_all, captions = values\n        for i in range(detections.size(0)):\n            det_seqs_all_i = det_seqs_all[i].numpy()\n            if opt_test.dataset == \'coco\':\n                det_seqs_all_sum = np.sum(np.abs(det_seqs_all_i), axis=-1)\n            elif opt_test.dataset == \'flickr\':\n                det_seqs_all_sum = np.sum(np.abs(det_seqs_vis[i].numpy()), axis=-1)\n            else:\n                raise NotImplementedError\n            _, unique_indices, unique_inverse = np.unique(det_seqs_all_sum, axis=0, return_index=True, return_inverse=True)\n            det_seqs_vis_unique = det_seqs_vis[i][unique_indices]\n            det_seqs_txt_unique = det_seqs_txt[i][unique_indices]\n            det_seqs_pos_unique = det_seqs_pos[i][unique_indices]\n            det_seqs_all_unique = det_seqs_all_i[unique_indices]\n            this_captions = [[captions[i][ii] for ii in range(len(unique_inverse)) if unique_inverse[ii] == jj] for jj in range(det_seqs_all_unique.shape[0])]\n\n            det_seqs_perm = torch.cat((det_seqs_txt_unique, det_seqs_vis_unique, det_seqs_pos_unique), dim=-1).to(device)\n            matrices = sinkhorn_net(det_seqs_perm)\n            matrices = torch.transpose(matrices, 1, 2)\n\n            if isinstance(matrices, torch.Tensor):\n                matrices = matrices.detach().cpu().numpy()\n            m = munkres.Munkres()\n            det_seqs_recons = np.zeros(det_seqs_all_unique.shape)\n\n            for j, matrix in enumerate(matrices):\n                seqs = []\n                ass = m.compute(munkres.make_cost_matrix(matrix))\n                perm_matrix = np.zeros_like(matrix)\n                for a in ass:\n                    perm_matrix[a] = 1\n\n                perm = np.reshape(det_seqs_all_unique[j], (det_seqs_all_unique.shape[1], -1))\n                recons = np.dot(perm_matrix, perm)\n                recons = np.reshape(recons, det_seqs_all_unique.shape[1:])\n                recons = recons[np.sum(recons, (1, 2)) != 0]\n\n                last = recons.shape[0] - 1\n                det_seqs_recons[j, :recons.shape[0]] = recons\n                det_seqs_recons[:, last + 1:] = recons[last:last+1]\n\n\n            detections_i, det_seqs_recons = detections[i].to(device), torch.tensor(det_seqs_recons).float().to(device)\n            detections_i = detections_i.unsqueeze(0).expand(det_seqs_recons.size(0), detections_i.size(0), detections_i.size(1))\n            out, _ = model.beam_search((detections_i, det_seqs_recons), eos_idxs=[text_field.vocab.stoi[\'<eos>\'], -1],\n                                       beam_size=5, out_size=1)\n\n            out = out[0].data.cpu().numpy()\n\n            for o, caps in zip(out, this_captions):\n                predictions.append(np.expand_dims(o, axis=0))\n                gt_captions.append(caps)\n\n        pbar.update()\n\npredictions = np.concatenate(predictions, axis=0)\ngen = {}\ngts = {}\nscores_iou = []\n\nprint(""Computing set contrallabity results."")\nfor i, cap in enumerate(predictions):\n    pred_cap = text_field.decode(cap, join_words=False)\n    pred_cap = \' \'.join([k for k, g in itertools.groupby(pred_cap)])\n\n    gts[i] = gt_captions[i]\n    gen[i] = [pred_cap]\n\n    score_iou = 0.\n    for c in gt_captions[i]:\n        score = noun_iou.score(c, pred_cap)\n        score_iou += score\n\n    scores_iou.append(score_iou / len(gt_captions[i]))\n\ngts_t = PTBTokenizer.tokenize(gts)\ngen_t = PTBTokenizer.tokenize(gen)\n\nval_bleu, _ = Bleu(n=4).compute_score(gts_t, gen_t)\nmethod = [\'Blue_1\', \'Bleu_2\', \'Bleu_3\', \'Bleu_4\']\nfor metric, score in zip(method, val_bleu):\n    print(metric, score)\n\nval_meteor, _ = Meteor().compute_score(gts_t, gen_t)\nprint(\'METEOR\', val_meteor)\n\nval_rouge, _ = Rouge().compute_score(gts_t, gen_t)\nprint(\'ROUGE_L\', val_rouge)\n\nval_cider, _ = Cider().compute_score(gts_t, gen_t)\nprint(\'CIDEr\', val_cider)\n\nval_spice, _ = Spice().compute_score(gts_t, gen_t)\nprint(\'SPICE\', val_spice)\n\nprint(\'Noun IoU\', np.mean(scores_iou))\n'"
train.py,20,"b'from speaksee.data import TextField, ImageDetectionsField\nfrom data import COCOControlSequenceField\nfrom data.dataset import COCOEntities, PairedDataset\nfrom models import ControllableCaptioningModel\nfrom speaksee.data import DataLoader, RawField\nfrom speaksee import evaluation\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.nn import NLLLoss\nfrom utils import NWNounAligner\nfrom config import *\nimport torch\nimport random\nimport argparse\nimport itertools\nimport numpy as np\nimport os\nfrom tqdm import tqdm\n\nrandom.seed(1234)\ntorch.manual_seed(1234)\ndevice = torch.device(\'cuda\')\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--exp_name\', default=\'show_control_and_tell\', type=str, help=\'experiment name\')\nparser.add_argument(\'--nb_workers\', default=0, type=int, help=\'number of workers\')\nparser.add_argument(\'--batch_size\', default=100, type=int, help=\'batch size\')\nparser.add_argument(\'--lr\', default=5e-4, type=float, help=\'initial learning rate\')\nparser.add_argument(\'--step_size\', default=3, type=int, help=\'learning rate schedule step size\')\nparser.add_argument(\'--gamma\', default=0.8, type=float, help=\'learning rate schedule gamma\')\nparser.add_argument(\'--h2_first_lstm\', default=1, type=int, help=\'h2 as input to the first lstm\')\nparser.add_argument(\'--img_second_lstm\', default=0, type=int, help=\'img vector as input to the second lstm\')\nparser.add_argument(\'--sample_rl\', action=\'store_true\', help=\'reinforcement learning with cider optimization\')\nparser.add_argument(\'--sample_rl_nw\', action=\'store_true\', help=\'reinforcement learning with cider + nw optimization\')\nopt = parser.parse_args()\nprint(opt)\n\nimage_field = ImageDetectionsField(detections_path=os.path.join(coco_root, \'coco_detections.hdf5\'), load_in_tmp=False)\n\ndet_field = COCOControlSequenceField(detections_path=os.path.join(coco_root, \'coco_detections.hdf5\'),\n                                     classes_path=os.path.join(coco_root, \'object_class_list.txt\'),\n                                     pad_init=False, padding_idx=-1, all_boxes=False, fix_length=20)\n\ntext_field = TextField(init_token=\'<bos>\', eos_token=\'<eos>\', lower=True, remove_punctuation=True, fix_length=20)\n\ndataset = COCOEntities(image_field, det_field, text_field,\n                       img_root=\'\',\n                       ann_root=os.path.join(coco_root, \'annotations\'),\n                       entities_file=os.path.join(coco_root, \'coco_entities.json\'),\n                       id_root=os.path.join(coco_root, \'annotations\'))\n\ntrain_dataset, val_dataset, _ = dataset.splits\ntext_field.build_vocab(train_dataset, val_dataset, min_freq=5)\n\ntest_dataset = COCOEntities(image_field, det_field, RawField(),\n                            img_root=\'\',\n                            ann_root=os.path.join(coco_root, \'annotations\'),\n                            entities_file=os.path.join(coco_root, \'coco_entities.json\'),\n                            id_root=os.path.join(coco_root, \'annotations\'),\n                            filtering=True)\n\n_, val_dataset, _ = test_dataset.splits\n\nif opt.sample_rl or opt.sample_rl_nw:\n    train_dataset.fields[\'text\'] = RawField()\n    train_dataset_raw = PairedDataset(train_dataset.examples, {\'image\': image_field, \'detection\': det_field, \'text\': RawField()})\n    ref_caps_train = list(train_dataset_raw.text)\n    cider_train = evaluation.Cider(evaluation.PTBTokenizer.tokenize(ref_caps_train))\n\ndataloader_train = DataLoader(train_dataset, batch_size=opt.batch_size, shuffle=False, num_workers=opt.nb_workers)\n\nval_dataset.fields[\'text\'] = RawField()\ndataloader_val = DataLoader(val_dataset, batch_size=16, num_workers=opt.nb_workers)\n\nmodel = ControllableCaptioningModel(20, len(text_field.vocab), text_field.vocab.stoi[\'<bos>\'],\n                                    h2_first_lstm=opt.h2_first_lstm, img_second_lstm=opt.img_second_lstm).to(device)\n\noptim = Adam(model.parameters(), lr=opt.lr)\nscheduler = StepLR(optim, step_size=opt.step_size, gamma=opt.gamma)\nloss_fn = NLLLoss()\nloss_fn_gate = NLLLoss(ignore_index=-1)\n\nstart_epoch = 0\nbest_cider = .0\npatience = 0\nif opt.sample_rl or opt.sample_rl_nw:\n    saved_data = torch.load(\'saved_models/%s_best.pth\' % opt.exp_name)\n    print(""Loading from epoch %d, with validation CIDER %.02f"" % (saved_data[\'epoch\'], saved_data[\'val_cider\']))\n    start_epoch = saved_data[\'epoch\'] + 1\n    model.load_state_dict(saved_data[\'state_dict\'])\n    best_cider = saved_data[\'best_cider\']\n\nif opt.sample_rl_nw:\n    nw_aligner = NWNounAligner(pre_comp_file=os.path.join(coco_root, \'coco_noun_glove.pkl\'), normalized=True)\n\n\nfor e in range(start_epoch, start_epoch+100):\n    if not opt.sample_rl and not opt.sample_rl_nw:\n        # Training with cross-entropy\n        model.train()\n        running_loss = .0\n        running_loss_gate = .0\n        with tqdm(desc=\'Epoch %d - train\' % e, unit=\'it\', total=len(iter(dataloader_train))) as pbar:\n            for it, (detections, ctrl_det_seqs, ctrl_det_gts, _, _, captions) in enumerate(iter(dataloader_train)):\n                detections, ctrl_det_seqs = detections.to(device), ctrl_det_seqs.to(device)\n                ctrl_det_gts, captions = ctrl_det_gts.to(device), captions.to(device)\n\n                out, gate = model((detections, ), (captions, ctrl_det_seqs))\n\n                optim.zero_grad()\n                captions = captions[:, 1:].contiguous()\n                out = out[:, :-1].contiguous()\n                loss_cap = loss_fn(out.view(-1, len(text_field.vocab)), captions.view(-1))\n                loss_gate = loss_fn_gate(gate.view(-1, 2), ctrl_det_gts.view(-1).long())\n                loss = loss_cap + 4*loss_gate\n\n                loss.backward()\n                optim.step()\n\n                running_loss += loss_cap.item()\n                running_loss_gate += loss_gate.item()\n                pbar.set_postfix(loss_cap=running_loss / (it+1), loss_gate=running_loss_gate / (it+1))\n                pbar.update()\n\n        scheduler.step()\n    else:\n        # Baseline captions\n        model.eval()\n        baselines = []\n        with tqdm(desc=\'Epoch %d - baseline\' % e, unit=\'it\', total=len(iter(dataloader_train))) as pbar:\n            with torch.no_grad():\n                for it, (detections, ctrl_det_seqs, ctrl_det_gts, ctrl_det_seqs_test, _, caps_gt) in enumerate(iter(dataloader_train)):\n                    detections, ctrl_det_seqs_test = detections.to(device), ctrl_det_seqs_test.to(device)\n                    outs_baseline, _ = model.test(detections, ctrl_det_seqs_test)\n\n                    caps_baseline = text_field.decode(outs_baseline.cpu().numpy(), join_words=False)\n\n                    bas = []\n                    for i, bas_i in enumerate(caps_baseline):\n                        bas_i = \' \'.join([k for k, g in itertools.groupby(bas_i)])\n                        bas.append(bas_i)\n                    baselines.append(bas)\n                    pbar.update()\n\n        # Training with self-critical\n        model.train()\n        running_loss = .0\n        running_loss_gate = .0\n        running_loss_nw = .0\n        running_reward = .0\n        running_reward_nw = .0\n        with tqdm(desc=\'Epoch %d - train\' % e, unit=\'it\', total=len(iter(dataloader_train))) as pbar:\n            for it, (detections, ctrl_det_seqs, ctrl_det_gts, ctrl_det_seqs_test, _, caps_gt) in enumerate(iter(dataloader_train)):\n                detections, ctrl_det_seqs = detections.to(device), ctrl_det_seqs.to(device)\n                ctrl_det_gts, ctrl_det_seqs_test = ctrl_det_gts.to(device), ctrl_det_seqs_test.to(device)\n                outs, log_probs = model.sample_rl(detections, ctrl_det_seqs_test)\n                optim.zero_grad()\n\n                caps_gen = text_field.decode(outs[0].detach().cpu().numpy(), join_words=False)\n\n                gts = []\n                gen = []\n                scores = []\n                scores_baseline = []\n                if not opt.sample_rl_nw:\n                    for i, (gts_i, gen_i) in enumerate(zip(caps_gt, caps_gen)):\n                        gen_i = \' \'.join([k for k, g in itertools.groupby(gen_i)])\n                        gts.append([gts_i, ])\n                        gen.append(gen_i)\n                else:\n                    for i, (gts_i, bas_i, gen_i) in enumerate(zip(caps_gt, baselines[it], caps_gen)):\n                        gen_i = \' \'.join([k for k, g in itertools.groupby(gen_i)])\n                        gts.append([gts_i, ])\n                        gen.append(gen_i)\n                        scores.append((1 + nw_aligner.score(gts_i, gen_i)) / 2.)\n                        scores_baseline.append((1 + nw_aligner.score(gts_i, bas_i)) / 2.)\n\n                caps_gt = evaluation.PTBTokenizer.tokenize(gts)\n                caps_gen = evaluation.PTBTokenizer.tokenize(gen)\n                caps_baseline = evaluation.PTBTokenizer.tokenize(baselines[it])\n\n                reward = cider_train.compute_score(caps_gt, caps_gen)[1].astype(np.float32)\n                reward_baseline = cider_train.compute_score(caps_gt, caps_baseline)[1].astype(np.float32)\n                reward = torch.from_numpy(reward).to(device)\n                reward_baseline = torch.from_numpy(reward_baseline).to(device)\n\n                if not opt.sample_rl_nw:\n                    loss_cap = -(torch.mean(log_probs[0], -1) + torch.mean(log_probs[1], -1)) * (reward - reward_baseline)\n                    loss_cap = loss_cap.mean()\n                    loss = loss_cap\n                    loss.backward()\n                    optim.step()\n\n                    running_loss += loss_cap.item()\n                    running_reward += torch.mean(reward - reward_baseline).item()\n                    pbar.set_postfix(loss=running_loss / (it + 1), reward=running_reward / (it + 1))\n                    pbar.update()\n                else:\n                    reward_nw = torch.cat(scores).to(device)\n                    reward_nw_baseline = torch.cat(scores_baseline).to(device)\n\n                    loss_cap = -(torch.mean(log_probs[0], -1) + torch.mean(log_probs[1], -1)) * (reward - reward_baseline)\n                    loss_cap = loss_cap.mean()\n                    loss_nw = -(torch.mean(log_probs[0], -1) + torch.mean(log_probs[1], -1)) * (reward_nw - reward_nw_baseline)\n                    loss_nw = loss_nw.mean()\n                    loss = loss_cap + 4*loss_nw\n                    loss.backward()\n                    optim.step()\n\n                    running_loss += loss_cap.item()\n                    running_loss_nw += loss_nw.item()\n                    running_reward += torch.mean(reward - reward_baseline).item()\n                    running_reward_nw += torch.mean(reward_nw - reward_nw_baseline).item()\n                    pbar.set_postfix(loss=running_loss / (it + 1), loss_nw=running_loss_nw / (it + 1),\n                                     reward=running_reward / (it + 1), reward_nw=running_reward_nw / (it + 1))\n                    pbar.update()\n\n    # Validation with CIDEr\n    gen = []\n    gts = []\n    max_len = 20\n    model.eval()\n    with tqdm(desc=\'Test\', unit=\'it\', total=len(iter(dataloader_val))) as pbar:\n        with torch.no_grad():\n            for it, (detections, ctrl_det_seqs, ctrl_det_gts, ctrl_det_seqs_test, _, captions) in enumerate(iter(dataloader_val)):\n                detections, ctrl_det_seqs = detections.to(device), ctrl_det_seqs.to(device)\n                ctrl_det_seqs_test = ctrl_det_seqs_test.to(device)\n                out, gate = model.test(detections, ctrl_det_seqs_test)\n\n                caps_gen = text_field.decode(out, join_words=False)\n                for i, (gts_i, gen_i) in enumerate(zip(captions, caps_gen)):\n                    gen_i = \' \'.join([k for k, g in itertools.groupby(gen_i)])\n                    gen.append(gen_i)\n                    gts.append([gts_i, ])\n                pbar.update()\n\n    gts = evaluation.PTBTokenizer.tokenize(gts)\n    gen = evaluation.PTBTokenizer.tokenize(gen)\n\n    val_bleu, _ = evaluation.Bleu(n=4).compute_score(gts, gen)\n    method = [\'Blue_1\', \'Bleu_2\', \'Bleu_3\', \'Bleu_4\']\n    for metric, score in zip(method, val_bleu):\n        print(metric, score)\n\n    val_meteor, _ = evaluation.Meteor().compute_score(gts, gen)\n    print(\'METEOR\', val_meteor)\n\n    val_rouge, _ = evaluation.Rouge().compute_score(gts, gen)\n    print(\'ROUGE_L\', val_rouge)\n\n    val_cider, _ = evaluation.Cider().compute_score(gts, gen)\n    print(\'CIDEr\', val_cider)\n\n    saved_data = {\n        \'epoch\': e,\n        \'opt\': opt,\n        \'val_cider\': val_cider,\n        \'patience\': patience,\n        \'best_cider\': best_cider,\n        \'state_dict\': model.state_dict(),\n        \'optimizer\': optim.state_dict(),\n        \'scheduler\': scheduler.state_dict(),\n    }\n\n    if not os.path.exists(\'saved_models/\'):\n        os.makedirs(\'saved_models/\')\n\n    if val_cider >= best_cider:\n        best_cider = val_cider\n        best_srt = \'best_rl\' if opt.sample_rl else \'best\'\n        best_srt = \'best_rl_nw\' if opt.sample_rl_nw else best_srt\n        patience = 0\n        saved_data[\'best_cider\'] = best_cider\n        saved_data[\'patience\'] = patience\n        torch.save(saved_data, \'saved_models/%s_%s.pth\' % (opt.exp_name, best_srt))\n    else:\n        patience += 1\n        saved_data[\'patience\'] = patience\n    torch.save(saved_data, \'saved_models/%s_last.pth\' % opt.exp_name)\n\n    if patience == 5:\n        print(\'patience ended.\')\n        break\n'"
data/__init__.py,0,"b'from .dataset import COCOEntities, FlickrEntities\nfrom .field import COCOControlSequenceField, COCOControlSetField\nfrom .field import FlickrDetectionField, FlickrControlSequenceField, FlickrControlSetField\n'"
data/dataset.py,0,"b'import os\nimport json\nimport numpy as np\nimport re\nimport xml.etree.ElementTree\nfrom speaksee.data import field, Example, PairedDataset, COCO\nfrom speaksee.utils import nostdout\nfrom itertools import groupby\nimport pickle as pkl\n\n\nclass COCOEntities(PairedDataset):\n    def __init__(self, image_field, det_field, text_field, img_root, ann_root, entities_file, id_root=None,\n                 use_restval=True, filtering=False):\n        roots = dict()\n        roots[\'train\'] = {\n            \'img\': os.path.join(img_root, \'train2014\'),\n            \'cap\': os.path.join(ann_root, \'captions_train2014.json\')\n        }\n        roots[\'val\'] = {\n            \'img\': os.path.join(img_root, \'val2014\'),\n            \'cap\': os.path.join(ann_root, \'captions_val2014.json\')\n        }\n        roots[\'test\'] = {\n            \'img\': os.path.join(img_root, \'val2014\'),\n            \'cap\': os.path.join(ann_root, \'captions_val2014.json\')\n        }\n        roots[\'trainrestval\'] = {\n            \'img\': (roots[\'train\'][\'img\'], roots[\'val\'][\'img\']),\n            \'cap\': (roots[\'train\'][\'cap\'], roots[\'val\'][\'cap\'])\n        }\n\n        if id_root is not None:\n            ids = {}\n            ids[\'train\'] = np.load(os.path.join(id_root, \'coco_train_ids.npy\'))\n            ids[\'val\'] = np.load(os.path.join(id_root, \'coco_dev_ids.npy\'))\n            ids[\'test\'] = np.load(os.path.join(id_root, \'coco_test_ids.npy\'))\n            ids[\'trainrestval\'] = (\n                ids[\'train\'],\n                np.load(os.path.join(id_root, \'coco_restval_ids.npy\')))\n\n            if use_restval:\n                roots[\'train\'] = roots[\'trainrestval\']\n                ids[\'train\'] = ids[\'trainrestval\']\n        else:\n            ids = None\n\n        if not filtering:\n            dataset_path = \'saved_data/coco/coco_entities_precomp.pkl\'\n        else:\n            dataset_path = \'saved_data/coco/coco_entities_filtered_precomp.pkl\'\n        if not os.path.isfile(dataset_path):\n            with nostdout():\n                train_examples, val_examples, test_examples = COCO.get_samples(roots, ids)\n\n            self.train_examples, self.val_examples, self.test_examples = self.get_samples([train_examples, val_examples, test_examples], entities_file, filtering)\n            pkl.dump((self.train_examples, self.val_examples, self.test_examples), open(dataset_path, \'wb\'), -1)\n        else:\n            self.train_examples, self.val_examples, self.test_examples = pkl.load(open(dataset_path, \'rb\'))\n\n        examples = self.train_examples + self.val_examples + self.test_examples\n        super(COCOEntities, self).__init__(examples, {\'image\': image_field, \'detection\': det_field, \'text\': text_field})\n\n    @property\n    def splits(self):\n        train_split = PairedDataset(self.train_examples, self.fields)\n        val_split = PairedDataset(self.val_examples, self.fields)\n        test_split = PairedDataset(self.test_examples, self.fields)\n        return train_split, val_split, test_split\n\n    @classmethod\n    def get_samples(cls, samples, entities_file, filtering=False):\n        train_examples = []\n        val_examples = []\n        test_examples = []\n\n        with open(entities_file, \'r\') as fp:\n            visual_chunks = json.load(fp)\n\n        for id_split, samples_split in enumerate(samples):\n            for s in samples_split:\n                id_image = str(int(s.image.split(\'/\')[-1].split(\'_\')[-1].split(\'.\')[0]))\n                caption = s.text.lower().replace(\'\\t\', \' \').replace(\'\\n\', \'\')\n\n                words = caption.strip().split(\' \')\n                caption_fixed = []\n                for w in words:\n                    if w not in field.TextField.punctuations and w != \'\':\n                        caption_fixed.append(w)\n\n                det_classes = [None for _ in caption_fixed]\n                caption_fixed = \' \'.join(caption_fixed)\n\n                for p in field.TextField.punctuations:\n                    caption_fixed = caption_fixed.replace(p, \'\')\n\n                if id_image in visual_chunks:\n                    if caption in visual_chunks[id_image]:\n                        chunks = visual_chunks[id_image][caption]\n                        for chunk in chunks:\n                            words = chunk[0].split(\' \')\n                            chunk_fixed = []\n                            for w in words:\n                                if w not in field.TextField.punctuations and w != \'\':\n                                    chunk_fixed.append(w)\n                            chunk_fixed = \' \'.join(chunk_fixed)\n                            for p in field.TextField.punctuations:\n                                chunk_fixed = chunk_fixed.replace(p, \'\')\n\n                            sub_str = \' \'.join([\'_\' for _ in chunk_fixed.split(\' \')])\n                            sub_cap = caption_fixed.replace(chunk_fixed, sub_str).split(\' \')\n                            for i, w in enumerate(sub_cap):\n                                if w == \'_\':\n                                    det_classes[i] = chunk[1]\n\n                        example = Example.fromdict({\'image\': s.image,\n                                                    \'detection\': (s.image, tuple(det_classes)),\n                                                    \'text\': caption_fixed})\n\n                        det_classes_set = [x[0] for x in groupby(det_classes) if x[0] is not None]\n                        chunks_filtered = list(set([c[1] for c in chunks]))\n                        if len(det_classes_set) < len(chunks_filtered):\n                            pass\n                        else:\n                            if id_split == 0:\n                                train_examples.append(example)\n                            elif id_split == 1:\n                                if filtering:\n                                    if \'_\' not in example.detection[1]:\n                                        val_examples.append(example)\n                                else:\n                                    val_examples.append(example)\n                            elif id_split == 2:\n                                if filtering:\n                                    if \'_\' not in example.detection[1]:\n                                        test_examples.append(example)\n                                else:\n                                    test_examples.append(example)\n\n        return train_examples, val_examples, test_examples\n\n\nclass FlickrEntities(PairedDataset):\n    def __init__(self, image_field, text_field, det_field, img_root, ann_file, entities_root,\n                 precomp_file=\'saved_data/flickr/flickr_entities_precomp.pkl\'):\n        if os.path.isfile(precomp_file):\n            with open(precomp_file, \'rb\') as pkl_file:\n                self.train_examples, self.val_examples, self.test_examples = pkl.load(pkl_file)\n        else:\n            self.train_examples, self.val_examples, self.test_examples = self.get_samples(ann_file, img_root, entities_root)\n            with open(precomp_file, \'wb\') as pkl_file:\n                pkl.dump((self.train_samples, self.val_samples, self.test_samples), pkl_file, protocol=pkl.HIGHEST_PROTOCOL)\n\n        examples = self.train_examples + self.val_examples + self.test_examples\n        super(FlickrEntities, self).__init__(examples, {\'image\': image_field, \'detection\': det_field, \'text\': text_field})\n\n    @property\n    def splits(self):\n        train_split = PairedDataset(self.train_examples, self.fields)\n        val_split = PairedDataset(self.val_examples, self.fields)\n        test_split = PairedDataset(self.test_examples, self.fields)\n        return train_split, val_split, test_split\n\n    def get_samples(self, ann_file, img_root, entities_root):\n        def _get_sample(d):\n            filename = d[\'filename\']\n            split = d[\'split\']\n            xml_root = xml.etree.ElementTree.parse(os.path.join(entities_root, \'Annotations\',\n                                                                filename.replace(\'.jpg\', \'.xml\'))).getroot()\n            det_dict = dict()\n            id_counter = 1\n            for obj in xml_root.findall(\'object\'):\n                obj_names = [o.text for o in obj.findall(\'name\')]\n                if obj.find(\'bndbox\'):\n                    bbox = tuple(int(o.text) for o in obj.find(\'bndbox\'))\n                    for obj_name in obj_names:\n                        if obj_name not in det_dict:\n                            det_dict[obj_name] = {\'id\': id_counter, \'bdnbox\': [bbox]}\n                            id_counter += 1\n                        else:\n                            det_dict[obj_name][\'bdnbox\'].append(bbox)\n\n            bdnboxes = [[] for _ in range(id_counter - 1)]\n            for it in det_dict.values():\n                bdnboxes[it[\'id\'] - 1] = tuple(it[\'bdnbox\'])\n            bdnboxes = tuple(bdnboxes)\n\n            captions = [l.strip() for l in open(os.path.join(entities_root, \'Sentences\',\n                                                             filename.replace(\'.jpg\', \'.txt\')), encoding=""utf-8"").readlines()]\n            outputs = []\n            for c in captions:\n                matches = prog.findall(c)\n                caption = []\n                det_ids = []\n\n                for match in matches:\n                    for i, grp in enumerate(match):\n                        if i in (0, 2):\n                            if grp != \'\':\n                                words = grp.strip().split(\' \')\n                                for w in words:\n                                    if w not in field.TextField.punctuations and w != \'\':\n                                        caption.append(w)\n                                        det_ids.append(0)\n                        elif i == 1:\n                            words = grp[1:-1].strip().split(\' \')\n                            obj_name = words[0].split(\'#\')[-1].split(\'/\')[0]\n                            words = words[1:]\n                            for w in words:\n                                if w not in field.TextField.punctuations and w != \'\':\n                                    caption.append(w)\n                                    if obj_name in det_dict:\n                                        det_ids.append(det_dict[obj_name][\'id\'])\n                                    else:\n                                        det_ids.append(0)\n\n                caption = \' \'.join(caption)\n                if caption != \'\' and np.sum(np.asarray(det_ids)) > 0:\n                    example = Example.fromdict({\'image\': os.path.join(img_root, filename),\n                                                \'detection\': (os.path.join(img_root, filename), bdnboxes, det_ids),\n                                                \'text\': caption})\n                    outputs.append([example, split])\n\n            return outputs\n\n        train_samples = []\n        val_samples = []\n        test_samples = []\n\n        prog = re.compile(r\'([^\\[\\]]*)(\\[[^\\[\\]]+\\])([^\\[\\]]*)\')\n        dataset = json.load(open(ann_file, \'r\'))[\'images\']\n\n        samples = []\n        for d in dataset:\n            samples.extend(_get_sample(d))\n\n        for example, split in samples:\n            if split == \'train\':\n                train_samples.append(example)\n            elif split == \'val\':\n                val_samples.append(example)\n            elif split == \'test\':\n                test_samples.append(example)\n\n        return train_samples, val_samples, test_samples\n'"
data/field.py,2,"b""import torch\nimport numpy as np\nimport h5py\nimport pickle as pkl\nimport warnings\nimport json\nfrom itertools import groupby\nfrom speaksee.data import RawField\n\n\nclass COCOControlSequenceField(RawField):\n    def __init__(self, postprocessing=None, detections_path=None, classes_path=None,\n                 padding_idx=0, fix_length=None, all_boxes=True, pad_init=True, pad_eos=True, dtype=torch.float32,\n                 max_detections=20, max_length=100, sorting=False):\n        self.max_detections = max_detections\n        self.max_length = max_length\n        self.detections_path = detections_path\n        self.padding_idx = padding_idx\n        self.fix_length = fix_length\n        self.all_boxes = all_boxes\n        self.sorting = sorting\n        self.eos_token = padding_idx if pad_eos else None\n        self.dtype = dtype\n\n        self.classes = ['__background__']\n        with open(classes_path, 'r') as f:\n            for object in f.readlines():\n                self.classes.append(object.split(',')[0].lower().strip())\n\n        super(COCOControlSequenceField, self).__init__(None, postprocessing)\n\n    def get_detections_inside(self, det_boxes, query):\n        cond1 = det_boxes[:, 0] >= det_boxes[query, 0]\n        cond2 = det_boxes[:, 1] >= det_boxes[query, 1]\n        cond3 = det_boxes[:, 2] <= det_boxes[query, 2]\n        cond4 = det_boxes[:, 3] <= det_boxes[query, 3]\n        cond = cond1 & cond2 & cond3 & cond4\n        return np.nonzero(cond)[0]\n\n    def _fill(self, cls_seq, det_features, det_boxes, selected_classes, most_probable_dets, max_len):\n        det_sequences = np.zeros((self.fix_length, self.max_detections, det_features.shape[-1]))\n        for j, cls in enumerate(cls_seq[:max_len]):\n            if cls == '_':\n                det_sequences[j, :det_features.shape[0]] = most_probable_dets\n            else:\n                seed_detections = [i for i, c in enumerate(selected_classes) if c == cls]\n                if self.all_boxes:\n                    det_ids = np.unique(np.concatenate([self.get_detections_inside(det_boxes, d) for d in seed_detections]))\n                else:\n                    det_ids = np.unique(seed_detections)\n                det_sequences[j, :len(det_ids)] = np.take(det_features, det_ids, axis=0)[:self.max_detections]\n\n        if not self.sorting:\n            last = len(cls_seq[:max_len])\n            det_sequences[last:] = det_sequences[last-1]\n\n        return det_sequences.astype(np.float32)\n\n    def preprocess(self, x):\n        image = x[0]\n        det_classes = x[1]\n        max_len = self.fix_length + (self.eos_token, self.eos_token).count(None) - 2\n\n        id_image = int(image.split('/')[-1].split('_')[-1].split('.')[0])\n        try:\n            f = h5py.File(self.detections_path, 'r')\n            det_cls_probs = f['%s_cls_prob' % id_image][()]\n            det_features = f['%s_features' % id_image][()]\n            det_boxes = f['%s_boxes' % id_image][()]\n        except KeyError:\n            warnings.warn('Could not find detections for %d' % id_image)\n            det_cls_probs = np.random.rand(10, 2048)\n            det_features = np.random.rand(10, 2048)\n            det_boxes = np.random.rand(10, 4)\n\n        most_probable_idxs = np.argsort(np.max(det_cls_probs, -1))[::-1][:self.max_detections]\n        most_probable_dets = det_features[most_probable_idxs]\n\n        selected_classes = [self.classes[np.argmax(det_cls_probs[i][1:])+1] for i in range(len(det_cls_probs))]\n\n        cls_seq = []\n        for i, cls in enumerate(det_classes):\n            if cls is not None:\n                cls_seq.append(cls)\n            else:\n                cls_ok = next((c for c in det_classes[i+1:] if c is not None), '_')\n                cls_seq.append(cls_ok)\n\n        cls_seq_gt = np.asarray([int(a != b) for (a, b) in zip(cls_seq[:-1], cls_seq[1:])] + [0, ])\n        cls_seq_gt = cls_seq_gt[:max_len]\n        cls_seq_gt = np.concatenate([cls_seq_gt, [self.eos_token, self.eos_token]])\n        cls_seq_gt = np.concatenate([cls_seq_gt, [self.padding_idx]*max(0, self.fix_length - len(cls_seq_gt))])\n        cls_seq_gt = cls_seq_gt.astype(np.float32)\n\n        cls_seq_test = [x[0] for x in groupby(det_classes) if x[0] is not None]\n        if self.sorting:\n            cls_seq_test.sort()\n            det_sequences_test = self._fill(cls_seq_test, det_features, det_boxes, selected_classes, most_probable_dets, max_len)\n            return det_sequences_test\n        else:\n            det_sequences = self._fill(cls_seq, det_features, det_boxes, selected_classes, most_probable_dets, max_len)\n            det_sequences_test = self._fill(cls_seq_test, det_features, det_boxes, selected_classes, most_probable_dets, max_len)\n\n            cls_seq_test = ' '.join(cls_seq_test)\n\n            return det_sequences, cls_seq_gt, det_sequences_test, cls_seq_test # , id_image\n\n\nclass COCOControlSetField(RawField):\n    def __init__(self, postprocessing=None, detections_path=None, classes_path=None, img_shapes_path=None,\n                 precomp_glove_path=None, fix_length=20, max_detections=20):\n        self.fix_length = fix_length\n        self.detections_path = detections_path\n        self.max_detections = max_detections\n\n        self.classes = ['__background__']\n        with open(classes_path, 'r') as f:\n            for object in f.readlines():\n                self.classes.append(object.split(',')[0].lower().strip())\n\n        with open(precomp_glove_path, 'rb') as fp:\n            self.vectors = pkl.load(fp)\n\n        with open(img_shapes_path, 'r') as fp:\n            self.img_shapes = json.load(fp)\n\n        super(COCOControlSetField, self).__init__(None, postprocessing)\n\n    def preprocess(self, x):\n        image = x[0]\n        det_classes = x[1]\n\n        id_image = int(image.split('/')[-1].split('_')[-1].split('.')[0])\n        try:\n            f = h5py.File(self.detections_path, 'r')\n            det_cls_probs = f['%s_cls_prob' % id_image][()]\n            det_features = f['%s_features' % id_image][()]\n            det_boxes = f['%s_boxes' % id_image][()]\n        except KeyError:\n            warnings.warn('Could not find detections for %d' % id_image)\n            det_cls_probs = np.random.rand(10, 2048)\n            det_features = np.random.rand(10, 2048)\n            det_boxes = np.random.rand(10, 4)\n\n        selected_classes = [self.classes[np.argmax(det_cls_probs[i][1:]) + 1] for i in range(len(det_cls_probs))]\n        width, height = self.img_shapes[str(id_image)]\n\n        cls_seq = [x[0] for x in groupby(det_classes) if x[0] is not None]\n        det_sequences_visual_all = np.zeros((self.fix_length, self.max_detections, det_features.shape[-1]))\n\n        det_sequences_visual = np.zeros((self.fix_length, det_features.shape[-1]))\n        det_sequences_word = np.zeros((self.fix_length, 300))\n        det_sequences_position = np.zeros((self.fix_length, 4))\n\n        cls_seq = cls_seq[:self.fix_length]\n        cls_seq.sort()\n\n        for j, cls in enumerate(cls_seq):\n            cls_w = cls.split(',')[0].split(' ')[-1]\n            if cls_w in self.vectors:\n                det_sequences_word[j] = self.vectors[cls_w]\n            seed_detections = [i for i, c in enumerate(selected_classes) if c == cls]\n            det_ids = np.unique(seed_detections)\n            det_sequences_visual_all[j, :len(det_ids)] = np.take(det_features, det_ids, axis=0)[:self.max_detections]\n\n            det_sequences_visual[j] = det_features[det_ids[0]]\n            bbox = det_boxes[det_ids[0]]\n            det_sequences_position[j, 0] = (bbox[2] - bbox[0] / 2) / width\n            det_sequences_position[j, 1] = (bbox[3] - bbox[1] / 2) / height\n            det_sequences_position[j, 2] = (bbox[2] - bbox[0]) / width\n            det_sequences_position[j, 3] = (bbox[3] - bbox[1]) / height\n\n        return det_sequences_word.astype(np.float32), det_sequences_visual.astype(np.float32), \\\n               det_sequences_position.astype(np.float32), det_sequences_visual_all.astype(np.float32)\n\n\nclass FlickrDetectionField(RawField):\n    def __init__(self, preprocessing=None, postprocessing=None, detections_path=None):\n        self.max_detections = 100\n        self.detections_path = detections_path\n\n        super(FlickrDetectionField, self).__init__(preprocessing, postprocessing)\n\n    def preprocess(self, x, avoid_precomp=False):\n        image_id = int(x.split('/')[-1].split('.')[0])\n        try:\n            precomp_data = h5py.File(self.detections_path, 'r')['%d_features' % image_id][()]\n        except KeyError:\n            warnings.warn('Could not find detections for %d' % image_id)\n            precomp_data = np.random.rand(10, 2048)\n\n        delta = self.max_detections - precomp_data.shape[0]\n        if delta > 0:\n            precomp_data = np.concatenate([precomp_data, np.zeros((delta, precomp_data.shape[1]))], axis=0)\n        elif delta < 0:\n            precomp_data = precomp_data[:self.max_detections]\n\n        return precomp_data.astype(np.float32)\n\n\nclass FlickrControlSequenceField(RawField):\n    def __init__(self, postprocessing=None, detections_path=None,\n                 padding_idx=0, fix_length=None, pad_init=True, pad_eos=True, dtype=torch.float32,\n                 max_detections=20, max_length=100):\n        self.detections_path = detections_path\n        self.max_detections = max_detections\n        self.max_length = max_length\n        self.detections_path = detections_path\n        self.padding_idx = padding_idx\n        self.fix_length = fix_length\n        self.eos_token = padding_idx if pad_eos else None\n        self.dtype = dtype\n\n        super(FlickrControlSequenceField, self).__init__(None, postprocessing)\n\n    @staticmethod\n    def _bb_intersection_over_union(boxA, boxB):\n        xA = max(boxA[0], boxB[0])\n        yA = max(boxA[1], boxB[1])\n        xB = min(boxA[2], boxB[2])\n        yB = min(boxA[3], boxB[3])\n\n        interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n\n        boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n        boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n        iou = interArea / (boxAArea + boxBArea - interArea)\n        return iou\n\n    def _fill(self, cls_seq, det_features, bbox_ids, most_probable_dets, max_len):\n        det_sequences = np.zeros((self.fix_length, self.max_detections, det_features.shape[-1]))\n        for j, cls in enumerate(cls_seq[:max_len]):\n            if cls == '_':\n                det_sequences[j, :det_features.shape[0]] = most_probable_dets\n            else:\n                det_ids = bbox_ids[cls]\n                det_sequences[j, :len(det_ids)] = np.take(det_features, det_ids, axis=0)[:self.max_detections]\n\n        last = len(cls_seq[:max_len])\n        det_sequences[last:] = det_sequences[last-1]\n        return det_sequences.astype(np.float32)\n\n    def preprocess(self, x, avoid_precomp=False):\n        image = x[0]\n        gt_bboxes = x[1]\n        det_classes = x[2]\n        max_len = self.fix_length + (self.eos_token, self.eos_token).count(None) - 2\n\n        id_image = image.split('/')[-1].split('.')[0]\n\n        try:\n            f = h5py.File(self.detections_path, 'r')\n            det_features = f['%s_features' % id_image][()]\n            det_cls_probs = f['%s_cls_prob' % id_image][()]\n            det_bboxes = f['%s_boxes' % id_image][()]\n        except KeyError:\n            warnings.warn('Could not find detections for %d' % id_image)\n            det_features = np.random.rand(10, 2048)\n            det_cls_probs = np.random.rand(10, 2048)\n            det_bboxes = np.random.rand(10, 4)\n\n        det_classes = [d-1 if d > 0 else None for d in det_classes]\n\n        most_probable_idxs = np.argsort(np.max(det_cls_probs, -1))[::-1][:self.max_detections]\n        most_probable_dets = det_features[most_probable_idxs]\n\n        cls_seq = []\n        for i, cls in enumerate(det_classes):\n            if cls is not None:\n                cls_seq.append(cls)\n            else:\n                cls_ok = next((c for c in det_classes[i + 1:] if c is not None), '_')\n                cls_seq.append(cls_ok)\n\n        cls_seq_gt = np.asarray([int(a != b) for (a, b) in zip(cls_seq[:-1], cls_seq[1:])] + [0, ])\n        cls_seq_gt = cls_seq_gt[:max_len]\n        cls_seq_gt = np.concatenate([cls_seq_gt, [self.eos_token, self.eos_token]])\n        cls_seq_gt = np.concatenate([cls_seq_gt, [self.padding_idx] * max(0, self.fix_length - len(cls_seq_gt))])\n        cls_seq_gt = cls_seq_gt.astype(np.float32)\n\n        cls_seq_test = [x[0] for x in groupby(det_classes) if x[0] is not None]\n\n        bbox_ids = dict()\n        for i, cls in enumerate(set(cls_seq_test)):\n            id_boxes = []\n            for k, bbox in enumerate(gt_bboxes[cls]):\n                id_bbox = -1\n                iou_max = 0\n                for ii, det_bbox in enumerate(det_bboxes):\n                    iou = self._bb_intersection_over_union(bbox, det_bbox)\n                    if iou_max < iou:\n                        id_bbox = ii\n                        iou_max = iou\n                id_boxes.append(id_bbox)\n            bbox_ids[cls] = id_boxes\n\n        det_sequences = self._fill(cls_seq, det_features, bbox_ids, most_probable_dets, max_len)\n        det_sequences_test = self._fill(cls_seq_test, det_features, bbox_ids, most_probable_dets, max_len)\n\n        cls_seq_test = [str(c) for c in cls_seq_test]\n        cls_seq_test = ' '.join(cls_seq_test)\n\n        return det_sequences, cls_seq_gt, det_sequences_test, cls_seq_test\n\n\nclass FlickrControlSetField(RawField):\n    def __init__(self, postprocessing=None, detections_path=None, classes_path=None, img_shapes_path=None,\n                 precomp_glove_path=None, fix_length=20, max_detections=20):\n        self.fix_length = fix_length\n        self.detections_path = detections_path\n        self.max_detections = max_detections\n\n        self.classes = ['__background__']\n        with open(classes_path, 'r') as f:\n            for object in f.readlines():\n                self.classes.append(object.split(',')[0].lower().strip())\n\n        with open(precomp_glove_path, 'rb') as fp:\n            self.vectors = pkl.load(fp)\n\n        with open(img_shapes_path, 'r') as fp:\n            self.img_shapes = json.load(fp)\n\n        super(FlickrControlSetField, self).__init__(None, postprocessing)\n\n    @staticmethod\n    def _bb_intersection_over_union(boxA, boxB):\n        xA = max(boxA[0], boxB[0])\n        yA = max(boxA[1], boxB[1])\n        xB = min(boxA[2], boxB[2])\n        yB = min(boxA[3], boxB[3])\n\n        interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n\n        boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n        boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n        iou = interArea / (boxAArea + boxBArea - interArea)\n        return iou\n\n    def preprocess(self, x):\n        image = x[0]\n        gt_bboxes = x[1]\n        det_classes = x[2]\n\n        id_image = image.split('/')[-1].split('.')[0]\n        try:\n            f = h5py.File(self.detections_path, 'r')\n            det_cls_probs = f['%s_cls_prob' % id_image][()]\n            det_features = f['%s_features' % id_image][()]\n            det_bboxes = f['%s_boxes' % id_image][()]\n        except KeyError:\n            warnings.warn('Could not find detections for %d' % id_image)\n            det_cls_probs = np.random.rand(10, 2048)\n            det_features = np.random.rand(10, 2048)\n            det_bboxes = np.random.rand(10, 4)\n\n        det_classes = [d - 1 if d > 0 else None for d in det_classes]\n        selected_classes = [self.classes[np.argmax(det_cls_probs[i][1:]) + 1] for i in range(len(det_cls_probs))]\n        width, height = self.img_shapes[str(id_image)]\n\n        cls_seq = [x[0] for x in groupby(det_classes) if x[0] is not None]\n        det_sequences_visual_all = np.zeros((self.fix_length, self.max_detections, det_features.shape[-1]))\n\n        det_sequences_visual = np.zeros((self.fix_length, det_features.shape[-1]))\n        det_sequences_word = np.zeros((self.fix_length, 300))\n        det_sequences_position = np.zeros((self.fix_length, 4))\n\n        cls_seq = cls_seq[:self.fix_length]\n        cls_seq.sort()\n\n        for j, cls in enumerate(cls_seq):\n            id_boxes = []\n            for k, bbox in enumerate(gt_bboxes[cls]):\n                id_bbox = -1\n                iou_max = 0\n                for ii, det_bbox in enumerate(det_bboxes):\n                    iou = self._bb_intersection_over_union(bbox, det_bbox)\n                    if iou_max < iou:\n                        id_bbox = ii\n                        iou_max = iou\n                id_boxes.append(id_bbox)\n\n            id_boxes.sort()\n\n            cls_w = selected_classes[id_boxes[0]].split(',')[0].split(' ')[-1]\n            if cls_w in self.vectors:\n                det_sequences_word[j] = self.vectors[cls_w]\n\n            det_sequences_visual_all[j, :len(id_boxes)] = np.take(det_features, id_boxes, axis=0)[:self.max_detections]\n            det_sequences_visual[j] = det_features[id_boxes[0]]\n\n            bbox = det_bboxes[id_boxes[0]]\n            det_sequences_position[j, 0] = (bbox[2] - bbox[0] / 2) / width\n            det_sequences_position[j, 1] = (bbox[3] - bbox[1] / 2) / height\n            det_sequences_position[j, 2] = (bbox[2] - bbox[0]) / width\n            det_sequences_position[j, 3] = (bbox[3] - bbox[1]) / height\n\n        return det_sequences_word.astype(np.float32), det_sequences_visual.astype(np.float32), \\\n               det_sequences_position.astype(np.float32), det_sequences_visual_all.astype(np.float32)\n\n"""
models/CaptioningModel.py,14,"b""import torch\nfrom torch import nn\nfrom torch import distributions\nimport functools\nimport operator\n\n\nclass CaptioningModel(nn.Module):\n    def __init__(self, seq_len):\n        self.seq_len = seq_len\n        super(CaptioningModel, self).__init__()\n\n    def init_weights(self):\n        raise NotImplementedError\n\n    def init_state(self, b_s, device):\n        raise NotImplementedError\n\n    def step(self, t, state, prev_outputs, images, seqs, *args, mode='teacher_forcing'):\n        raise NotImplementedError\n\n    def forward(self, statics, seqs, *args):\n        device = statics[0].device\n        b_s = statics[0].size(0)\n        seq_len = seqs[0].size(1)\n        state = self.init_state(b_s, device)\n        outs = None\n\n        outputs = []\n        for t in range(seq_len):\n            outs, state = self.step(t, state, outs, statics, seqs, *args, mode='teacher_forcing')\n            outputs.append(outs)\n\n        outputs = list(zip(*outputs))\n        outputs = tuple(torch.cat([oo.unsqueeze(1) for oo in o], 1) for o in outputs)\n        return outputs\n\n    def test(self, statics, *args):\n        device = statics[0].device\n        b_s = statics[0].size(0)\n        state = self.init_state(b_s, device)\n        outs = None\n\n        outputs = []\n        for t in range(self.seq_len):\n            outs, state = self.step(t, state, outs, statics, None, *args, mode='feedback')\n            outs = tuple(torch.max(o, -1)[1] for o in outs)\n            outputs.append(outs)\n\n        outputs = list(zip(*outputs))\n        outputs = tuple(torch.cat([oo.unsqueeze(1) for oo in o], 1) for o in outputs)\n        return outputs\n\n    def sample_rl(self, statics, *args):\n        device = statics[0].device\n        b_s = statics[0].size(0)\n        state = self.init_state(b_s, device)\n\n        outputs = []\n        log_probs = []\n        for t in range(self.seq_len):\n            prev_outputs = outputs[-1] if t > 0 else None\n            outs, state = self.step(t, state, prev_outputs, statics, None, *args, mode='feedback')\n            outputs.append([])\n            log_probs.append([])\n            for out in outs:\n                distr = distributions.Categorical(logits=out)\n                sample = distr.sample()\n                outputs[-1].append(sample)\n                log_probs[-1].append(distr.log_prob(sample))\n\n        outputs = list(zip(*outputs))\n        outputs = tuple(torch.cat([oo.unsqueeze(1) for oo in o], 1) for o in outputs)\n        log_probs = list(zip(*log_probs))\n        log_probs = tuple(torch.cat([oo.unsqueeze(1) for oo in o], 1) for o in log_probs)\n        return outputs, log_probs\n\n    def _select_beam(self, input, selected_beam, cur_beam_size, beam_size, b_s, reduced=True):\n        if not isinstance(input, list) and not isinstance(input, tuple):\n            return self._select_beam_i(input, selected_beam, cur_beam_size, beam_size, b_s, reduced=reduced)\n\n        new_input = []\n        for i, s in enumerate(input):\n            if isinstance(s, tuple) or isinstance(s, list):\n                new_state_i = []\n                for ii, ss in enumerate(s):\n                    new_state_ii = self._select_beam_i(ss, selected_beam, cur_beam_size, beam_size, b_s,\n                                                       reduced=reduced)\n                    new_state_i.append(new_state_ii)\n                new_input.append(tuple(new_state_i))\n            else:\n                new_state_i = self._select_beam_i(s, selected_beam, cur_beam_size, beam_size, b_s, reduced=reduced)\n                new_input.append(new_state_i)\n        return list(new_input)\n\n    def _select_beam_i(self, input, selected_beam, cur_beam_size, beam_size, b_s, reduced=True):\n        input_shape = input.shape\n        if reduced:\n            input_shape = input_shape[1:]\n        else:\n            input_shape = input_shape[2:]\n        input_exp_shape = (b_s, cur_beam_size) + input_shape\n        output_exp_shape = (b_s, beam_size) + input_shape\n        input_red_shape = (b_s * beam_size,) + input_shape\n        selected_beam_red_size = (b_s, beam_size) + tuple(1 for _ in range(len(input_exp_shape) - 2))\n        selected_beam_exp_size = (b_s, beam_size) + input_exp_shape[2:]\n        input_exp = input.view(input_exp_shape)\n        selected_beam_exp = selected_beam.view(selected_beam_red_size).expand(selected_beam_exp_size)\n        out = torch.gather(input_exp, 1, selected_beam_exp)\n        if reduced:\n            out = out.view(input_red_shape)\n        else:\n            out = out.view(output_exp_shape)\n        return out\n\n    def beam_search(self, statics, eos_idxs, beam_size, out_size=1, *args):\n        device = statics[0].device\n        b_s = statics[0].size(0)\n        state = self.init_state(b_s, device)\n\n        outputs = []\n        log_probs = []\n        selected_outs = None\n\n        for t in range(self.seq_len):\n            outs_logprob, state = self.step(t, state, selected_outs, statics, None, *args, mode='feedback')\n\n            if t == 0:\n                n_outs = len(outs_logprob)\n                cur_beam_size = 1\n                seq_logprob = statics[0].data.new_zeros([b_s, 1] + [1]*n_outs)\n                seq_masks = [statics[0].data.new_ones((b_s, beam_size))] * n_outs\n            else:\n                cur_beam_size = beam_size\n\n            old_seq_logprob = seq_logprob\n            outs_logprob = [ol.view([b_s, cur_beam_size] + [1]*i + [-1] + [1]*(n_outs-i-1)) for i, ol in enumerate(outs_logprob)]\n            seq_logprob = seq_logprob + functools.reduce(operator.add, outs_logprob)\n\n            # Mask sequence if it reaches EOS\n            if t > 0:\n                masks = [(so.view(b_s, cur_beam_size) != idx).float() for idx, so in zip(eos_idxs, selected_outs)]\n                seq_masks = [sm * m for (sm, m) in zip(seq_masks, masks)]\n                outs_logprob = [ol.squeeze() * sm.unsqueeze(-1) for (ol, sm) in zip(outs_logprob, seq_masks)]\n                old_seq_logprob = old_seq_logprob.expand_as(seq_logprob).contiguous()\n                old_seq_logprob[:, :, 1:] = -999\n                seq_mask_full = torch.clamp(torch.sum(torch.cat([sm.unsqueeze(0) for sm in seq_masks]), 0), 0, 1)\n                seq_mask_full = seq_mask_full.view(list(seq_mask_full.shape) + [1] * n_outs)\n                seq_logprob = seq_mask_full*seq_logprob + old_seq_logprob*(1-seq_mask_full)\n\n            selected_logprob, selected_idx = torch.sort(seq_logprob.view(b_s, -1), -1, descending=True)\n            selected_logprob, selected_idx = selected_logprob[:, :beam_size], selected_idx[:, :beam_size]\n\n            _div = functools.reduce(operator.mul, seq_logprob.shape[2:], 1)\n            selected_beam = selected_idx / _div\n            selected_outs = []\n            for i in range(n_outs):\n                if i == 0:\n                    selected_idx = selected_idx - selected_beam * _div\n                else:\n                    selected_idx = selected_idx - selected_outs[-1] * _div\n                _div = functools.reduce(operator.mul, seq_logprob.shape[3+i:], 1)\n                selected_outs.append(selected_idx / _div)\n\n            # Update states, statics and seq_mask\n            state = self._select_beam(state, selected_beam, cur_beam_size, beam_size, b_s)\n            statics = self._select_beam(statics, selected_beam, cur_beam_size, beam_size, b_s)\n            seq_masks = self._select_beam(seq_masks, selected_beam, beam_size, beam_size, b_s, reduced=False)\n            outputs = self._select_beam(outputs, selected_beam, cur_beam_size, beam_size, b_s, reduced=False)\n            outputs.append([so.unsqueeze(-1) for so in selected_outs])\n            seq_logprob = selected_logprob.view([b_s, beam_size] + [1]*n_outs)\n\n            outs_logprob = [ol.view(b_s, cur_beam_size, -1) for ol in outs_logprob]\n            this_word_logprob = self._select_beam(outs_logprob, selected_beam, cur_beam_size, beam_size, b_s, reduced=False)\n            this_word_logprob = [torch.gather(o, 2, selected_outs[i].unsqueeze(-1)) for i, o in enumerate(this_word_logprob)]\n            log_probs.append(this_word_logprob)\n            selected_outs = [so.view(-1) for so in selected_outs]\n\n        # Sort result\n        seq_logprob, sort_idxs = torch.sort(seq_logprob.view(b_s, beam_size, 1), 1, descending=True)\n        outputs = list(zip(*outputs))\n        outputs = [torch.cat(o, -1) for o in outputs]\n        outputs = [torch.gather(o, 1, sort_idxs.expand(b_s, beam_size, self.seq_len)) for o in outputs]\n        log_probs = list(zip(*log_probs))\n        log_probs = [torch.cat(lp, -1) for lp in log_probs]\n        log_probs = [torch.gather(lp, 1, sort_idxs.expand(b_s, beam_size, self.seq_len)) for lp in log_probs]\n\n        outputs = [o.contiguous()[:, :out_size] for o in outputs]\n        log_probs = [lp.contiguous()[:, :out_size] for lp in log_probs]\n        if out_size == 1:\n            outputs = [o.squeeze(1) for o in outputs]\n            log_probs = [lp.squeeze(1) for lp in log_probs]\n        return outputs, log_probs\n\n\n"""
models/__init__.py,0,b'from .CaptioningModel import CaptioningModel as _CaptioningModel\nfrom .controllable_captioning import ControllableCaptioningModel\nfrom .controllable_captioning_no_visual_sentinel import ControllableCaptioningModel_NoVisualSentinel\nfrom .controllable_captioning_single_sentinel import ControllableCaptioningModel_SingleSentinel\n'
models/controllable_captioning.py,27,"b""from __future__ import division\nfrom __future__ import absolute_import\nimport torch\nfrom torch import nn\nfrom torch import distributions\nimport torch.nn.functional as F\nfrom models import _CaptioningModel\n\n\nclass ControllableCaptioningModel(_CaptioningModel):\n    def __init__(self, seq_len, vocab_size, bos_idx, det_feat_size=2048, input_encoding_size=1000, rnn_size=1000, att_size=512,\n                 h2_first_lstm=True, img_second_lstm=False):\n        super(ControllableCaptioningModel, self).__init__(seq_len)\n        self.vocab_size = vocab_size\n        self.bos_idx = bos_idx\n        self.det_feat_size = det_feat_size\n        self.input_encoding_size = input_encoding_size\n        self.rnn_size = rnn_size\n        self.att_size = att_size\n        self.h2_first_lstm = h2_first_lstm\n        self.img_second_lstm = img_second_lstm\n\n        self.embed = nn.Embedding(vocab_size, input_encoding_size)\n\n        if self.h2_first_lstm:\n            self.W1_is = nn.Linear(det_feat_size + rnn_size + input_encoding_size, rnn_size)\n        else:\n            self.W1_is = nn.Linear(det_feat_size + input_encoding_size, rnn_size)\n        self.W1_hs = nn.Linear(rnn_size, rnn_size)\n\n        self.att_va = nn.Linear(det_feat_size, att_size, bias=False)\n        self.att_ha = nn.Linear(rnn_size, att_size, bias=False)\n        self.att_a = nn.Linear(att_size, 1, bias=False)\n\n        self.att_sa = nn.Linear(rnn_size, att_size, bias=False)\n        self.att_s = nn.Linear(att_size, 1, bias=False)\n\n        if self.h2_first_lstm:\n            self.lstm_cell_1 = nn.LSTMCell(det_feat_size + rnn_size + input_encoding_size, rnn_size)\n        else:\n            self.lstm_cell_1 = nn.LSTMCell(det_feat_size + input_encoding_size, rnn_size)\n\n        if self.img_second_lstm:\n            self.lstm_cell_2 = nn.LSTMCell(rnn_size + det_feat_size + det_feat_size, rnn_size)\n        else:\n            self.lstm_cell_2 = nn.LSTMCell(rnn_size + det_feat_size, rnn_size)\n\n        self.out_fc = nn.Linear(rnn_size, vocab_size)\n        self.s_fc = nn.Linear(rnn_size, det_feat_size)\n\n        if self.h2_first_lstm:\n            self.W1_ig = nn.Linear(det_feat_size + rnn_size + input_encoding_size, rnn_size)\n        else:\n            self.W1_ig = nn.Linear(det_feat_size + input_encoding_size, rnn_size)\n        self.W1_hg = nn.Linear(rnn_size, rnn_size)\n        self.att_ga = nn.Linear(rnn_size, att_size, bias=False)\n        self.att_g = nn.Linear(att_size, 1, bias=False)\n\n        self.init_weights()\n\n    def init_weights(self):\n        nn.init.xavier_normal_(self.embed.weight)\n        nn.init.xavier_normal_(self.out_fc.weight)\n        nn.init.constant_(self.out_fc.bias, 0)\n\n        nn.init.xavier_normal_(self.s_fc.weight)\n        nn.init.constant_(self.s_fc.bias, 0)\n\n        nn.init.xavier_normal_(self.W1_is.weight)\n        nn.init.constant_(self.W1_is.bias, 0)\n\n        nn.init.xavier_normal_(self.W1_hs.weight)\n        nn.init.constant_(self.W1_hs.bias, 0)\n\n        nn.init.xavier_normal_(self.att_va.weight)\n        nn.init.xavier_normal_(self.att_ha.weight)\n        nn.init.xavier_normal_(self.att_a.weight)\n        nn.init.xavier_normal_(self.att_sa.weight)\n        nn.init.xavier_normal_(self.att_s.weight)\n\n        nn.init.xavier_normal_(self.lstm_cell_1.weight_ih)\n        nn.init.orthogonal_(self.lstm_cell_1.weight_hh)\n        nn.init.constant_(self.lstm_cell_1.bias_ih, 0)\n        nn.init.constant_(self.lstm_cell_1.bias_hh, 0)\n\n        nn.init.xavier_normal_(self.lstm_cell_2.weight_ih)\n        nn.init.orthogonal_(self.lstm_cell_2.weight_hh)\n        nn.init.constant_(self.lstm_cell_2.bias_ih, 0)\n        nn.init.constant_(self.lstm_cell_2.bias_hh, 0)\n\n        nn.init.xavier_normal_(self.W1_ig.weight)\n        nn.init.constant_(self.W1_ig.bias, 0)\n        nn.init.xavier_normal_(self.W1_hg.weight)\n        nn.init.constant_(self.W1_hg.bias, 0)\n        nn.init.xavier_normal_(self.att_ga.weight)\n        nn.init.xavier_normal_(self.att_g.weight)\n\n\n    def init_state(self, b_s, device):\n        h0_1 = torch.zeros((b_s, self.rnn_size), requires_grad=True).to(device)\n        c0_1 = torch.zeros((b_s, self.rnn_size), requires_grad=True).to(device)\n        h0_2 = torch.zeros((b_s, self.rnn_size), requires_grad=True).to(device)\n        c0_2 = torch.zeros((b_s, self.rnn_size), requires_grad=True).to(device)\n        ctrl_det_idxs = torch.zeros((b_s, ), requires_grad=True).long().to(device)\n        return (h0_1, c0_1), (h0_2, c0_2), ctrl_det_idxs\n\n    def step(self, t, state, prev_outputs, statics, seqs, *args, mode='teacher_forcing'):\n        assert (mode in ['teacher_forcing', 'feedback'])\n        bos_idx = self.bos_idx\n        detections = statics[0]\n        b_s = detections.size(0)\n\n        detections_mask = (torch.sum(detections, -1, keepdim=True) != 0).float()\n        image_descriptor = torch.sum(detections, 1) / torch.sum(detections_mask, 1)\n        state_1, state_2, ctrl_det_idxs = state\n\n        if mode == 'teacher_forcing':\n            it = seqs[0][:, t]\n            det_curr = seqs[1][:, t]\n        elif mode == 'feedback':\n            if t == 0:\n                it = detections.data.new_full((b_s,), bos_idx).long()\n            else:\n                it = prev_outputs[0]\n                ctrl_det_idxs = ctrl_det_idxs + prev_outputs[1]\n                ctrl_det_idxs = torch.clamp(ctrl_det_idxs, 0, statics[1].shape[1]-1)\n\n            det_curr = torch.gather(statics[1], 1, ctrl_det_idxs.view((b_s, 1, 1, 1)).expand((b_s, 1) + statics[1].shape[2:])).squeeze(1)\n\n        xt = self.embed(it)\n\n        if self.h2_first_lstm:\n            input_1 = torch.cat([state_2[0], image_descriptor, xt], 1)\n        else:\n            input_1 = torch.cat([image_descriptor, xt], 1)\n\n        s_gate = torch.sigmoid(self.W1_is(input_1) + self.W1_hs(state_1[0]))\n        state_1 = self.lstm_cell_1(input_1, state_1)\n\n        s_t = s_gate * torch.tanh(state_1[1])\n        fc_sentinel = self.s_fc(s_t).unsqueeze(1)\n\n        regions = torch.cat([fc_sentinel, det_curr], 1)\n        regions_mask = (torch.sum(regions, -1, keepdim=True) != 0).float()\n\n        det_weights = torch.tanh(self.att_va(det_curr) + self.att_ha(state_1[0]).unsqueeze(1))\n        det_weights = self.att_a(det_weights)\n        sent_weights = torch.tanh(self.att_sa(s_t) + self.att_ha(state_1[0])).unsqueeze(1)\n        sent_weights = self.att_s(sent_weights)\n        att_weights = torch.cat([sent_weights, det_weights], 1)\n\n        att_weights = F.softmax(att_weights, 1)  # (b_s, n_regions, 1)\n        att_weights = regions_mask * att_weights\n        att_weights = att_weights / torch.sum(att_weights, 1, keepdim=True)\n        att_detections = torch.sum(regions * att_weights, 1)\n\n        if self.img_second_lstm:\n            input_2 = torch.cat([state_1[0], att_detections, image_descriptor], 1)\n        else:\n            input_2 = torch.cat([state_1[0], att_detections], 1)\n        state_2 = self.lstm_cell_2(input_2, state_2)\n        out = F.log_softmax(self.out_fc(state_2[0]), dim=-1)\n\n        g_gate = torch.sigmoid(self.W1_ig(input_1) + self.W1_hg(state_1[0]))\n        g_t = g_gate * torch.tanh(state_1[1])\n        gate_weights = torch.tanh(self.att_ga(g_t) + self.att_ha(state_1[0])).unsqueeze(1)\n        gate_weights = self.att_g(gate_weights)\n        gate_weights = torch.cat([gate_weights, torch.sum(regions_mask[:, 1:] * det_weights, 1, keepdim=True)], 1)\n        gate_weights = F.log_softmax(gate_weights, 1).squeeze(-1)  # (b_s, 2)\n\n        return (out, gate_weights), (state_1, state_2, ctrl_det_idxs)\n\n    def test(self, detections, ctrl_det_seqs_test):\n        return super(ControllableCaptioningModel, self).test((detections, ctrl_det_seqs_test))\n\n    def sample_rl(self, detections, ctrl_det_seqs_test):\n        return super(ControllableCaptioningModel, self).sample_rl((detections, ctrl_det_seqs_test))\n"""
models/controllable_captioning_no_visual_sentinel.py,22,"b""from __future__ import division\nfrom __future__ import absolute_import\nimport torch\nfrom torch import nn\nfrom torch import distributions\nimport torch.nn.functional as F\nfrom models import _CaptioningModel\n\n\nclass ControllableCaptioningModel_NoVisualSentinel(_CaptioningModel):\n    def __init__(self, seq_len, vocab_size, bos_idx, det_feat_size=2048, input_encoding_size=1000, rnn_size=1000, att_size=512,\n                 h2_first_lstm=True, img_second_lstm=False):\n        super(ControllableCaptioningModel_NoVisualSentinel, self).__init__(seq_len)\n        self.vocab_size = vocab_size\n        self.bos_idx = bos_idx\n        self.det_feat_size = det_feat_size\n        self.input_encoding_size = input_encoding_size\n        self.rnn_size = rnn_size\n        self.att_size = att_size\n        self.h2_first_lstm = h2_first_lstm\n        self.img_second_lstm = img_second_lstm\n\n        self.embed = nn.Embedding(vocab_size, input_encoding_size)\n\n        self.att_va = nn.Linear(det_feat_size, att_size, bias=False)\n        self.att_ha = nn.Linear(rnn_size, att_size, bias=False)\n        self.att_a = nn.Linear(att_size, 1, bias=False)\n\n        if self.h2_first_lstm:\n            self.lstm_cell_1 = nn.LSTMCell(det_feat_size + rnn_size + input_encoding_size, rnn_size)\n        else:\n            self.lstm_cell_1 = nn.LSTMCell(det_feat_size + input_encoding_size, rnn_size)\n\n        if self.img_second_lstm:\n            self.lstm_cell_2 = nn.LSTMCell(rnn_size + det_feat_size + det_feat_size, rnn_size)\n        else:\n            self.lstm_cell_2 = nn.LSTMCell(rnn_size + det_feat_size, rnn_size)\n\n        self.out_fc = nn.Linear(rnn_size, vocab_size)\n\n        if self.h2_first_lstm:\n            self.W1_ig = nn.Linear(det_feat_size + rnn_size + input_encoding_size, rnn_size)\n        else:\n            self.W1_ig = nn.Linear(det_feat_size + input_encoding_size, rnn_size)\n        self.W1_hg = nn.Linear(rnn_size, rnn_size)\n        self.att_ga = nn.Linear(rnn_size, att_size, bias=False)\n        self.att_g = nn.Linear(att_size, 1, bias=False)\n\n        self.init_weights()\n\n    def init_weights(self):\n        nn.init.xavier_normal_(self.embed.weight)\n        nn.init.xavier_normal_(self.out_fc.weight)\n        nn.init.constant_(self.out_fc.bias, 0)\n\n        nn.init.xavier_normal_(self.att_va.weight)\n        nn.init.xavier_normal_(self.att_ha.weight)\n        nn.init.xavier_normal_(self.att_a.weight)\n\n        nn.init.xavier_normal_(self.lstm_cell_1.weight_ih)\n        nn.init.orthogonal_(self.lstm_cell_1.weight_hh)\n        nn.init.constant_(self.lstm_cell_1.bias_ih, 0)\n        nn.init.constant_(self.lstm_cell_1.bias_hh, 0)\n\n        nn.init.xavier_normal_(self.lstm_cell_2.weight_ih)\n        nn.init.orthogonal_(self.lstm_cell_2.weight_hh)\n        nn.init.constant_(self.lstm_cell_2.bias_ih, 0)\n        nn.init.constant_(self.lstm_cell_2.bias_hh, 0)\n\n        nn.init.xavier_normal_(self.W1_ig.weight)\n        nn.init.constant_(self.W1_ig.bias, 0)\n        nn.init.xavier_normal_(self.W1_hg.weight)\n        nn.init.constant_(self.W1_hg.bias, 0)\n        nn.init.xavier_normal_(self.att_ga.weight)\n        nn.init.xavier_normal_(self.att_g.weight)\n\n    def init_state(self, b_s, device):\n        h0_1 = torch.zeros((b_s, self.rnn_size), requires_grad=True).to(device)\n        c0_1 = torch.zeros((b_s, self.rnn_size), requires_grad=True).to(device)\n        h0_2 = torch.zeros((b_s, self.rnn_size), requires_grad=True).to(device)\n        c0_2 = torch.zeros((b_s, self.rnn_size), requires_grad=True).to(device)\n        ctrl_det_idxs = torch.zeros((b_s, ), requires_grad=True).long().to(device)\n        return (h0_1, c0_1), (h0_2, c0_2), ctrl_det_idxs\n\n    def step(self, t, state, prev_outputs, statics, seqs, *args, mode='teacher_forcing'):\n        assert (mode in ['teacher_forcing', 'feedback'])\n        bos_idx = self.bos_idx\n        detections = statics[0]\n        b_s = detections.size(0)\n\n        detections_mask = (torch.sum(detections, -1, keepdim=True) != 0).float()\n        image_descriptor = torch.sum(detections, 1) / torch.sum(detections_mask, 1)\n        state_1, state_2, ctrl_det_idxs = state\n\n        if mode == 'teacher_forcing':\n            it = seqs[0][:, t]\n            det_curr = seqs[1][:, t]\n        elif mode == 'feedback':\n            if t == 0:\n                it = detections.data.new_full((b_s,), bos_idx).long()\n            else:\n                it = prev_outputs[0]\n                ctrl_det_idxs = ctrl_det_idxs + prev_outputs[1]\n                ctrl_det_idxs = torch.clamp(ctrl_det_idxs, 0, statics[1].shape[1]-1)\n\n            det_curr = torch.gather(statics[1], 1, ctrl_det_idxs.view((b_s, 1, 1, 1)).expand((b_s, 1) + statics[1].shape[2:])).squeeze(1)\n\n        xt = self.embed(it)\n\n        if self.h2_first_lstm:\n            input_1 = torch.cat([state_2[0], image_descriptor, xt], 1)\n        else:\n            input_1 = torch.cat([image_descriptor, xt], 1)\n\n        state_1 = self.lstm_cell_1(input_1, state_1)\n\n        regions = det_curr\n        regions_mask = (torch.sum(regions, -1, keepdim=True) != 0).float()\n\n        det_weights = torch.tanh(self.att_va(det_curr) + self.att_ha(state_1[0]).unsqueeze(1))\n        det_weights = self.att_a(det_weights)\n        att_weights = det_weights\n\n        att_weights = F.softmax(att_weights, 1)  # (b_s, n_regions, 1)\n        att_weights = regions_mask * att_weights\n        att_weights = att_weights / torch.sum(att_weights, 1, keepdim=True)\n        att_detections = torch.sum(regions * att_weights, 1)\n\n        if self.img_second_lstm:\n            input_2 = torch.cat([state_1[0], att_detections, image_descriptor], 1)\n        else:\n            input_2 = torch.cat([state_1[0], att_detections], 1)\n        state_2 = self.lstm_cell_2(input_2, state_2)\n        out = F.log_softmax(self.out_fc(state_2[0]), dim=-1)\n\n        g_gate = torch.sigmoid(self.W1_ig(input_1) + self.W1_hg(state_1[0]))\n        g_t = g_gate * torch.tanh(state_1[1])\n        gate_weights = torch.tanh(self.att_ga(g_t) + self.att_ha(state_1[0])).unsqueeze(1)\n        gate_weights = self.att_g(gate_weights)\n        gate_weights = torch.cat([gate_weights, torch.sum(regions_mask * det_weights, 1, keepdim=True)], 1)\n        gate_weights = F.log_softmax(gate_weights, 1).squeeze(-1)\n\n        return (out, gate_weights), (state_1, state_2, ctrl_det_idxs)\n\n    def test(self, detections, ctrl_det_seqs_test):\n        return super(ControllableCaptioningModel_NoVisualSentinel, self).test((detections, ctrl_det_seqs_test))\n\n    def sample_rl(self, detections, ctrl_det_seqs_test):\n        return super(ControllableCaptioningModel_NoVisualSentinel, self).sample_rl((detections, ctrl_det_seqs_test))\n"""
models/controllable_captioning_single_sentinel.py,24,"b""from __future__ import division\nfrom __future__ import absolute_import\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom models import _CaptioningModel\n\n\nclass ControllableCaptioningModel_SingleSentinel(_CaptioningModel):\n    def __init__(self, seq_len, vocab_size, bos_idx, det_feat_size=2048, input_encoding_size=1000, rnn_size=1000, att_size=512,\n                 h2_first_lstm=True, img_second_lstm=False):\n        super(ControllableCaptioningModel_SingleSentinel, self).__init__(seq_len)\n        self.vocab_size = vocab_size\n        self.bos_idx = bos_idx\n        self.det_feat_size = det_feat_size\n        self.input_encoding_size = input_encoding_size\n        self.rnn_size = rnn_size\n        self.att_size = att_size\n        self.h2_first_lstm = h2_first_lstm\n        self.img_second_lstm = img_second_lstm\n\n        self.embed = nn.Embedding(vocab_size, input_encoding_size)\n\n        if self.h2_first_lstm:\n            self.W1_is = nn.Linear(det_feat_size + rnn_size + input_encoding_size, rnn_size)\n        else:\n            self.W1_is = nn.Linear(det_feat_size + input_encoding_size, rnn_size)\n        self.W1_hs = nn.Linear(rnn_size, rnn_size)\n\n        self.att_va = nn.Linear(det_feat_size, att_size, bias=False)\n        self.att_ha = nn.Linear(rnn_size, att_size, bias=False)\n        self.att_a = nn.Linear(att_size, 1, bias=False)\n\n        self.att_sa = nn.Linear(rnn_size, att_size, bias=False)\n        self.att_s = nn.Linear(att_size, 1, bias=False)\n\n        if self.h2_first_lstm:\n            self.lstm_cell_1 = nn.LSTMCell(det_feat_size + rnn_size + input_encoding_size, rnn_size)\n        else:\n            self.lstm_cell_1 = nn.LSTMCell(det_feat_size + input_encoding_size, rnn_size)\n\n        if self.img_second_lstm:\n            self.lstm_cell_2 = nn.LSTMCell(rnn_size + det_feat_size + det_feat_size, rnn_size)\n        else:\n            self.lstm_cell_2 = nn.LSTMCell(rnn_size + det_feat_size, rnn_size)\n\n        self.out_fc = nn.Linear(rnn_size, vocab_size)\n        self.s_fc = nn.Linear(rnn_size, det_feat_size)\n\n        self.init_weights()\n\n    def init_weights(self):\n        nn.init.xavier_normal_(self.embed.weight)\n        nn.init.xavier_normal_(self.out_fc.weight)\n        nn.init.constant_(self.out_fc.bias, 0)\n\n        nn.init.xavier_normal_(self.s_fc.weight)\n        nn.init.constant_(self.s_fc.bias, 0)\n\n        nn.init.xavier_normal_(self.W1_is.weight)\n        nn.init.constant_(self.W1_is.bias, 0)\n\n        nn.init.xavier_normal_(self.W1_hs.weight)\n        nn.init.constant_(self.W1_hs.bias, 0)\n\n        nn.init.xavier_normal_(self.att_va.weight)\n        nn.init.xavier_normal_(self.att_ha.weight)\n        nn.init.xavier_normal_(self.att_a.weight)\n        nn.init.xavier_normal_(self.att_sa.weight)\n        nn.init.xavier_normal_(self.att_s.weight)\n\n        nn.init.xavier_normal_(self.lstm_cell_1.weight_ih)\n        nn.init.orthogonal_(self.lstm_cell_1.weight_hh)\n        nn.init.constant_(self.lstm_cell_1.bias_ih, 0)\n        nn.init.constant_(self.lstm_cell_1.bias_hh, 0)\n\n        nn.init.xavier_normal_(self.lstm_cell_2.weight_ih)\n        nn.init.orthogonal_(self.lstm_cell_2.weight_hh)\n        nn.init.constant_(self.lstm_cell_2.bias_ih, 0)\n        nn.init.constant_(self.lstm_cell_2.bias_hh, 0)\n\n    def init_state(self, b_s, device):\n        h0_1 = torch.zeros((b_s, self.rnn_size), requires_grad=True).to(device)\n        c0_1 = torch.zeros((b_s, self.rnn_size), requires_grad=True).to(device)\n        h0_2 = torch.zeros((b_s, self.rnn_size), requires_grad=True).to(device)\n        c0_2 = torch.zeros((b_s, self.rnn_size), requires_grad=True).to(device)\n        ctrl_det_idxs = torch.zeros((b_s, ), requires_grad=True).long().to(device)\n        return (h0_1, c0_1), (h0_2, c0_2), ctrl_det_idxs\n\n    def step(self, t, state, prev_outputs, statics, seqs, *args, mode='teacher_forcing'):\n        assert (mode in ['teacher_forcing', 'feedback'])\n        bos_idx = self.bos_idx\n        detections = statics[0]\n        b_s = detections.size(0)\n\n        detections_mask = (torch.sum(detections, -1, keepdim=True) != 0).float()\n        image_descriptor = torch.sum(detections, 1) / torch.sum(detections_mask, 1)\n        state_1, state_2, ctrl_det_idxs = state\n\n        if mode == 'teacher_forcing':\n            it = seqs[0][:, t]\n            det_curr = seqs[1][:, t]\n        elif mode == 'feedback':\n            if t == 0:\n                it = detections.data.new_full((b_s,), bos_idx).long()\n            else:\n                it = prev_outputs[0]\n                ctrl_det_idxs = ctrl_det_idxs + prev_outputs[1]\n                ctrl_det_idxs = torch.clamp(ctrl_det_idxs, 0, statics[1].shape[1]-1)\n\n            det_curr = torch.gather(statics[1], 1, ctrl_det_idxs.view((b_s, 1, 1, 1)).expand((b_s, 1) + statics[1].shape[2:])).squeeze(1)\n\n        xt = self.embed(it)\n\n        if self.h2_first_lstm:\n            input_1 = torch.cat([state_2[0], image_descriptor, xt], 1)\n        else:\n            input_1 = torch.cat([image_descriptor, xt], 1)\n\n        s_gate = torch.sigmoid(self.W1_is(input_1) + self.W1_hs(state_1[0]))\n        state_1 = self.lstm_cell_1(input_1, state_1)\n\n        s_t = s_gate * torch.tanh(state_1[1])\n        fc_sentinel = self.s_fc(s_t).unsqueeze(1)\n\n        regions = torch.cat([fc_sentinel, det_curr], 1)\n        regions_mask = (torch.sum(regions, -1, keepdim=True) != 0).float()\n\n        det_weights = torch.tanh(self.att_va(det_curr) + self.att_ha(state_1[0]).unsqueeze(1))\n        det_weights = self.att_a(det_weights)\n        sent_weights = torch.tanh(self.att_sa(s_t) + self.att_ha(state_1[0])).unsqueeze(1)\n        sent_weights = self.att_s(sent_weights)\n        att_weights = torch.cat([sent_weights, det_weights], 1)\n\n        att_weights = F.softmax(att_weights, 1)\n        att_weights = regions_mask * att_weights\n        att_weights = att_weights / torch.sum(att_weights, 1, keepdim=True)\n        att_detections = torch.sum(regions * att_weights, 1)\n\n        if self.img_second_lstm:\n            input_2 = torch.cat([state_1[0], att_detections, image_descriptor], 1)\n        else:\n            input_2 = torch.cat([state_1[0], att_detections], 1)\n        state_2 = self.lstm_cell_2(input_2, state_2)\n        out = F.log_softmax(self.out_fc(state_2[0]), dim=-1)\n\n        gate_weights = torch.cat([sent_weights, torch.sum(regions_mask[:, 1:] * det_weights, 1, keepdim=True)], 1)\n        gate_weights = F.log_softmax(gate_weights, 1).squeeze(-1)\n\n        return (out, gate_weights), (state_1, state_2, ctrl_det_idxs)\n\n    def test(self, detections, ctrl_det_seqs_test):\n        return super(ControllableCaptioningModel_SingleSentinel, self).test((detections, ctrl_det_seqs_test))\n\n    def sample_rl(self, detections, ctrl_det_seqs_test):\n        return super(ControllableCaptioningModel_SingleSentinel, self).sample_rl((detections, ctrl_det_seqs_test))\n    """
utils/__init__.py,0,b'from .nw_noun_aligner import NWNounAligner\nfrom .noun_iou import NounIoU\nfrom .sinkhorn_network import SinkhornNet\n'
utils/noun_iou.py,5,"b""import torch\nimport torch.nn.functional as F\nimport pickle as pkl\nimport munkres\n\n\nclass NounIoU(object):\n    def __init__(self, pre_comp_file):\n        self.pre_comp_file = pre_comp_file\n        self.munkres = munkres.Munkres()\n\n        with open(self.pre_comp_file, 'rb') as fp:\n            self.vectors = pkl.load(fp)\n\n    def prep_seq(self, seq):\n        seq = seq.split(' ')\n        seq = [w for w in seq if w in self.vectors]\n        return seq\n\n    def score(self, seq_gt, seq_pred):\n        seq_gt = self.prep_seq(seq_gt)\n        seq_pred = self.prep_seq(seq_pred)\n        m, n = len(seq_gt), len(seq_pred)  # length of two sequences\n\n        if m == 0:\n            return 1.\n        if n == 0:\n            return 0.\n\n        similarities = torch.zeros((m, n))\n        for i in range(m):\n            for j in range(n):\n                a = self.vectors[seq_gt[i]]\n                b = self.vectors[seq_pred[j]]\n                a = torch.from_numpy(a)\n                b = torch.from_numpy(b)\n                similarities[i, j] = torch.mean(F.cosine_similarity(a.unsqueeze(0), b.unsqueeze(0))).unsqueeze(-1)\n\n        similarities = (similarities + 1) / 2\n        similarities = similarities.numpy()\n        ass = self.munkres.compute(munkres.make_cost_matrix(similarities))\n\n        intersection_score = .0\n        for a in ass:\n            intersection_score += similarities[a]\n        iou_score = intersection_score / (m + n - intersection_score)\n\n        return iou_score\n"""
utils/nw_noun_aligner.py,5,"b""from speaksee import GloVe\nimport torch\nimport torch.nn.functional as F\nimport pickle as pkl\n\n\nclass NWNounAligner(object):\n    def __init__(self, match_award=1, mismatch_penalty=-1, gap_penalty=-1, pre_comp_file=None, normalized=False):\n        self.match_award = match_award\n        self.mismatch_penalty = mismatch_penalty\n        self.gap_penalty = gap_penalty\n        self.pre_comp_file = pre_comp_file\n        self.normalized = normalized\n        if self.pre_comp_file is not None:\n            with open(self.pre_comp_file, 'rb') as fp:\n                self.vectors = pkl.load(fp)\n        else:\n            self.vectors = GloVe()\n\n    @staticmethod\n    def zeros(shape):\n        retval = []\n        for x in range(shape[0]):\n            retval.append([])\n            for y in range(shape[1]):\n                retval[-1].append(0)\n        return retval\n\n    def prep_seq(self, seq):\n        seq = seq.split(' ')\n        seq = [w for w in seq if w in self.vectors]\n        return seq\n\n    def match_score(self, alpha, beta):\n        if alpha == beta:\n            return self.match_award\n        elif alpha == '-' or beta == '-':\n            return self.gap_penalty\n        else:\n            a = self.vectors[alpha]\n            b = self.vectors[beta]\n            if self.pre_comp_file is not None:\n                a = torch.from_numpy(a)\n                b = torch.from_numpy(b)\n\n            return torch.mean(F.cosine_similarity(a.unsqueeze(0), b.unsqueeze(0))).unsqueeze(-1)\n\n    def finalize(self, align1, align2):\n        align1 = align1[::-1]  # reverse sequence 1\n        align2 = align2[::-1]  # reverse sequence 2\n\n        # calcuate identity, score and aligned sequeces\n        symbol = []\n        score = 0\n        identity = 0\n        for i in range(0, len(align1)):\n            # if two AAs are the same, then output the letter\n            if align1[i] == align2[i]:\n                symbol.append(align1[i])\n                identity = identity + 1\n                score += self.match_score(align1[i], align2[i])\n\n            # if they are not identical and none of them is gap\n            elif align1[i] != align2[i] and align1[i] != '-' and align2[i] != '-':\n                score += self.match_score(align1[i], align2[i])\n                symbol.append(' ')\n\n            # if one of them is a gap, output a space\n            elif align1[i] == '-' or align2[i] == '-':\n                symbol.append(' ')\n                score += self.gap_penalty\n\n        return score\n\n    def score(self, seq1, seq2):\n        seq1 = self.prep_seq(seq1)\n        seq2 = self.prep_seq(seq2)\n        m, n = len(seq1), len(seq2)  # length of two sequences\n\n        # Generate DP table and traceback path pointer matrix\n        score = self.zeros((m + 1, n + 1))  # the DP table\n\n        # Calculate DP table\n        for i in range(0, m + 1):\n            score[i][0] = self.gap_penalty * i\n        for j in range(0, n + 1):\n            score[0][j] = self.gap_penalty * j\n        for i in range(1, m + 1):\n            for j in range(1, n + 1):\n                match = score[i - 1][j - 1] + self.match_score(seq1[i - 1], seq2[j - 1])\n                delete = score[i - 1][j] + self.gap_penalty\n                insert = score[i][j - 1] + self.gap_penalty\n                score[i][j] = max(match, delete, insert)\n\n        # Traceback and compute the alignment\n        align1, align2 = [], []\n        i, j = m, n  # start from the bottom right cell\n        while i > 0 and j > 0:  # end toching the top or the left edge\n            score_current = score[i][j]\n            score_diagonal = score[i - 1][j - 1]\n            score_up = score[i][j - 1]\n            score_left = score[i - 1][j]\n\n            if score_current == score_diagonal + self.match_score(seq1[i - 1], seq2[j - 1]):\n                align1.append(seq1[i - 1])\n                align2.append(seq2[j - 1])\n                i -= 1\n                j -= 1\n            elif score_current == score_left + self.gap_penalty:\n                align1.append(seq1[i - 1])\n                align2.append('-')\n                i -= 1\n            elif score_current == score_up + self.gap_penalty:\n                align1.append('-')\n                align2.append(seq2[j - 1])\n                j -= 1\n\n        # Finish tracing up to the top left cell\n        while i > 0:\n            align1.append(seq1[i - 1])\n            align2.append('-')\n            i -= 1\n        while j > 0:\n            align1.append('-')\n            align2.append(seq2[j - 1])\n            j -= 1\n\n        nw_score = torch.zeros((1,)) + self.finalize(align1, align2)\n        if self.normalized:\n            nb_nouns = max(m, n)\n            if nb_nouns > 0:\n                return nw_score / nb_nouns\n            else:\n                return nw_score\n        else:\n            return nw_score\n"""
utils/sinkhorn_network.py,6,"b'import torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nclass SinkhornNet(nn.Module):\n    def __init__(self, N, n_iters, tau):\n        super(SinkhornNet, self).__init__()\n        self.n_iters = n_iters\n        self.tau = tau\n\n        self.W1_txt = nn.Linear(300, 128)\n        self.W1_vis = nn.Linear(2048, 512)\n        self.W2_vis = nn.Linear(512, 128)\n        self.W_fc_pos = nn.Linear(260, 256)\n        self.W_fc = nn.Linear(256, N)\n        self.init_weights()\n\n    def init_weights(self):\n        nn.init.xavier_normal_(self.W1_txt.weight)\n        nn.init.constant_(self.W1_txt.bias, 0)\n        nn.init.xavier_normal_(self.W1_vis.weight)\n        nn.init.constant_(self.W1_vis.bias, 0)\n        nn.init.xavier_normal_(self.W2_vis.weight)\n        nn.init.constant_(self.W2_vis.bias, 0)\n        nn.init.xavier_normal_(self.W_fc_pos.weight)\n        nn.init.constant_(self.W_fc_pos.bias, 0)\n        nn.init.xavier_normal_(self.W_fc.weight)\n        nn.init.constant_(self.W_fc.bias, 0)\n\n    def sinkhorn(self, x):\n        x = torch.exp(x / self.tau)\n\n        for _ in range(self.n_iters):\n           x = x / (10e-8 + torch.sum(x, -2, keepdim=True))\n           x = x / (10e-8 + torch.sum(x, -1, keepdim=True))\n\n        return x\n\n    def forward(self, seq):\n        x_txt = seq[:, :, :300]\n        x_vis = seq[:, :, 300:2348]\n        x_pos = seq[:, :, 2348:]\n        x_txt = F.relu(self.W1_txt(x_txt))\n        x_vis = F.relu(self.W1_vis(x_vis))\n        x_vis = F.relu(self.W2_vis(x_vis))\n        x = torch.cat((x_txt, x_vis, x_pos), dim=-1)\n        x = F.relu(self.W_fc_pos(x))\n\n        x = torch.tanh(self.W_fc(x))\n\n        return self.sinkhorn(x)\n'"
